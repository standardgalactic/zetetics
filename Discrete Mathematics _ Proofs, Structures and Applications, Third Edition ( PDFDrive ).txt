
Discrete
Mathematics


A TAYLOR & FRANCIS BOOK
CRC Press is an imprint of the
Taylor & Francis Group, an informa business
Boca Raton   London   New York

Taylor & Francis
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2009 by Taylor & Francis Group, LLC
Taylor & Francis is an Informa business
No claim to original U.S. Government works
Version Date: 20131121
International Standard Book Number-13: 978-1-4398-1281-5 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts 
have been made to publish reliable data and information, but the author and publisher cannot assume 
responsibility for the validity of all materials or the consequences of their use. The authors and publishers 
have attempted to trace the copyright holders of all material reproduced in this publication and apologize to 
copyright holders if permission to publish in this form has not been obtained. If any copyright material has 
not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmit-
ted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, 
including photocopying, microfilming, and recording, or in any information storage or retrieval system, 
without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.
com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood 
Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and 
registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, 
a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used 
only for identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Contents
Contents
v
Preface to the Third Edition
ix
Preface to the Second Edition
xi
Preface to the First Edition
xiii
List of Symbols
xvii
Chapter 1: Logic
1
1.1
Propositions and Truth Values
1
1.2
Logical Connectives and Truth Tables
2
1.3
Tautologies and Contradictions
13
1.4
Logical Equivalence and Logical Implication
15
1.5
The Algebra of Propositions
22
1.6
Arguments
26

vi
Contents
1.7
Formal Proof of the Validity of Arguments
29
1.8
Predicate Logic
35
1.9
Arguments in Predicate Logic
45
Chapter 2: Mathematical Proof
50
2.1
The Nature of Proof
50
2.2
Axioms and Axiom Systems
51
2.3
Methods of Proof
55
2.4
Mathematical Induction
69
Chapter 3: Sets
79
3.1
Sets and Membership
79
3.2
Subsets
85
3.3
Operations on Sets
91
3.4
Counting Techniques
100
3.5
The Algebra of Sets
104
3.6
Families of Sets
111
3.7
The Cartesian Product
122
3.8
Types and Typed Set Theory
134
Chapter 4: Relations
154
4.1
Relations and Their Representations
154
4.2
Properties of Relations
164
4.3
Intersections and Unions of Relations
171

Contents
vii
4.4
Equivalence Relations and Partitions
175
4.5
Order Relations
188
4.6
Hasse Diagrams
198
4.7
Application: Relational Databases
205
Chapter 5: Functions
220
5.1
Deﬁnitions and Examples
220
5.2
Composite Functions
238
5.3
Injections and Surjections
246
5.4
Bijections and Inverse Functions
260
5.5
More on Cardinality
270
5.6
Databases: Functional Dependence and Normal Forms
277
Chapter 6: Matrix Algebra
291
6.1
Introduction
291
6.2
Some Special Matrices
294
6.3
Operations on Matrices
296
6.4
Elementary Matrices
308
6.5
The Inverse of a Matrix
318
Chapter 7: Systems of Linear Equations
331
7.1
Introduction
331
7.2
Matrix Inverse Method
337
7.3
Gauss–Jordan Elimination
342
7.4
Gaussian Elimination
355

viii
Contents
Chapter 8: Algebraic Structures
361
8.1
Binary Operations and Their Properties
361
8.2
Algebraic Structures
370
8.3
More about Groups
379
8.4
Some Families of Groups
384
8.5
Substructures
396
8.6
Morphisms
404
8.7
Group Codes
418
Chapter 9: Introduction to Number Theory
436
9.1
Divisibility
437
9.2
Prime Numbers
449
9.3
Linear Congruences
460
9.4
Groups in Modular Arithmetic
473
9.5
Public Key Cryptography
479
Chapter 10: Boolean Algebra
492
10.1 Introduction
492
10.2 Properties of Boolean Algebras
496
10.3 Boolean Functions
503
10.4 Switching Circuits
520
10.5 Logic Networks
529
10.6 Minimization of Boolean Expressions
536

Contents
ix
Chapter 11: Graph Theory
548
11.1 Deﬁnitions and Examples
548
11.2 Paths and Cycles
561
11.3 Isomorphism of Graphs
575
11.4 Trees
582
11.5 Planar Graphs
591
11.6 Directed Graphs
600
Chapter 12: Applications of Graph Theory
611
12.1 Introduction
611
12.2 Rooted Trees
612
12.3 Sorting
626
12.4 Searching Strategies
643
12.5 Weighted Graphs
652
12.6 The Shortest Path and Travelling Salesman Problems
660
12.7 Networks and Flows
673
References and Further Reading
687
Hints and Solutions to
Selected Exercises
692
Index
798


Preface to the Third Edition
The most obvious change from the ﬁrst two editions of this text is its title.
We believe that the new title, Discrete Mathematics: Proofs, Structures and
Applications, provides a better description of the book. This book was originally
published under the title Discrete Mathematics for New Technology, a title
we were never entirely comfortable with for two reasons. Firstly, it was not
really clear which ‘new technology’ was being referred to and, furthermore, one
decade’s new technology is frequently the next decade’s obsolete technology.
Secondly, although we had originally conceptualised the text as providing the
discrete mathematical background for undergraduate computer science students,
it was apparent that the book had a much wider readership including mathematics
undergraduates, education students, practising scientists and others.
Our philosophy had always been to provide, so far as we were able, a rigorous
and accessible exposition of the mathematics and not to tie the text too closely to
any application domain or community. Perhaps this is part of the reason for the
wide readership that the book has enjoyed. We have maintained this approach in
the current edition. So, whilst we believe that the book continues to provide much
of the core mathematical underpinning for computer science, we hope others will
continue to ﬁnd the text accessible, informative and enjoyable.
In the eight years since the publication of the second edition, we have continued
to received feedback on the text from users.
The feedback has remained
complimentary about the clarity of our exposition and, since correcting known
errors for the second edition, there have been few comments pointing out errors
or suggesting ways in which the text could be improved. Nevertheless, through
our own use of the text, we have continued to log errors of substance or style and
we have corrected these in the current edition.
ix

x
Preface to the Third Edition
The principal changes in this new edition are an expanded chapter 1 and a new
chapter 9 on number theory. The revised chapter 1 includes a new section on the
formal proof of the validity of arguments in propositional logic. This means that
we now consider formal proofs ﬁrst in the context of propositional logic before
moving on to predicate logic.
The new chapter 9 covers elementary number
theory and congruences. This allows us to explore in a little more depth some
of the groups that arise in modular arithmetic. The signiﬁcant application that
we explore is the so-called public key encryption scheme, called RSA encryption,
that underpins much of the secure transmission of data on the internet. Although
the mathematics behind the RSA system is reasonably straightforward, it does
provide a practical, secure and widely used means of encrypting data.
As
one of our reviewers noted, this encryption scheme represents a premier ‘new
technology’.
We wish to acknowledge, with thanks, colleagues who have commented on
previous versions of the text.
John Taylor is also grateful to the University
of Brighton for a sabbatical period which was devoted in part to writing the
new chapter 9 and the accompanying solutions manual. Nevertheless, as in the
previous editions, any remaining shortcomings are ours and we have no one to
blame for them but each other.
RG and JT
June 2009

Preface to the Second Edition
In the nine years since the publication of the ﬁrst edition, we have received
feedback on the text from a number of users, both teachers and students. Most
have been complimentary about the clarity of our exposition, some have pointed
out errors of detail or historical accuracy and others have suggested ways in which
the text could be improved. In this edition we have attempted to retain the style
of exposition, correct the (known) errors and implement various improvements
suggested by users.
When writing the ﬁrst edition, we took a conscious decision not to root the
mathematical development in a particular method or language that was current
within the formal methods community.
Our priority was to give a thorough
treatment of the mathematics as we felt this was likely to be more stable over
time than particular methodologies. In a discipline like computing which evolves
rapidly and where the future direction is uncertain, a secure grounding in theory
is important.
We have continued with this philosophy in the second edition.
Thus, for example, Z made no appearance in the ﬁrst edition, and the object
constraint language (OCL) or the B method make no appearance in this edition.
Although the discipline of computing has indeed changed considerably since
the publication of the ﬁrst edition, the core mathematical requirements of the
undergraduate curricula have remained surprisingly constant. For example, in
the UK, the computing benchmark for undergraduate courses, published by the
Quality Assurance Agency for Higher Education (QAA) in April 2000, requires
undergraduate programmes to present ‘coherent underpinning theory’.
In the
USA, the joint ACM/IEEE Computer Society Curriculum 2001 project lists
‘Discrete Structures’ (sets, functions, relations, logic, proof, counting, graphs
and trees) as one of the 14 knowledge areas in the computing curriculum ‘to
emphasize the dependency of computing on discrete mathematics’.
xi

xii
Preface to the Second Edition
In this edition we have included a new section on typed set theory and
subsequently we show how relations and functions ﬁt into the typed world. We
have also introduced a speciﬁcation approach to mathematical operations, via
signatures, preconditions and postconditions. Computing undergraduates will be
familiar with types from the software design and implementation parts of their
course and we hope our use of types will help tie together the mathematical
underpinnings more closely with software development practice.
For the
mathematicians using the text, this work has a payoff in providing a framework
in which Russell’s paradox can be avoided, for example.
The principal shortcoming reported by users of the ﬁrst edition was the inclusion
of relatively few exercises at a routine level to develop and reinforce the
mathematical concepts introduced in the text. In the second edition, we have
added many new exercises (and solutions) which we hope will enhance the
usefulness of the text to teachers and students alike. Also included are a number
of new examples designed to reinforce the concepts introduced.
We wish to acknowledge, with thanks, our colleagues who have commented
on and thus improved various drafts of additional material included in the
second edition. In particular, we thank Paul Courtney, Gerald Gallacher, John
Howse, Brian Spencer and our reviewers for their knowledgeable and thoughtful
comments. We would also like to thank those—most notably Peter Kirkegaard—
who spotted errors in the ﬁrst edition or made suggestions for improving the text.
Nevertheless, any remaining shortcomings are ours and we have no one to blame
for them but each other.
RG and JT
April 2001

Preface to the First Edition
This book aims to present in an accessible yet rigorous way the core mathematics
requirement for undergraduate computer science students at British universities
and polytechnics. Selections from the material could also form a one- or two-
semester course at freshman–sophomore level at American colleges. The formal
mathematical prerequisites are covered by the GCSE in the UK and by high-
school algebra in the USA. However, the latter part of the text requires a certain
level of mathematical sophistication which, we hope, will be developed during
the reading of the book.
Over 30 years ago the discipline of computer science hardly existed, except as
a subdiscipline of mathematics.
Computers were seen, to a large extent, as
the mathematician’s tool. As a result, the machines spent a large proportion of
their time cranking through approximate numerical solutions to algebraic and
differential equations and the mathematics ‘appropriate’ for the computer scientist
was the theory of equations, calculus, numerical analysis and the like.
Since that time computer science has become a discipline in its own right and has
spawned its own subdisciplines. The nature and sophistication of both hardware
and software have changed dramatically over the same time period. Perhaps less
public, but no less dramatic, has been the parallel development of undergraduate
computer science curricula and the mathematics which underpins it. Indeed, the
whole relationship between mathematics and computer science has changed so
that mathematics is now seen more as the servant of computer science than vice
versa as was the case formerly.
Various communities and study groups on both sides of the Atlantic have studied
and reported upon the core mathematics requirements for computer scientists
educated and trained at various levels.
The early emphasis on continuous
xiii

xiv
Preface to the First Edition
mathematics in general, and numerical methods in particular, has disappeared.
There is now wide agreement that the essential mathematics required for computer
scientists comes from the area of ‘discrete mathematics’. There is, however, less
agreement concerning the detailed content and emphasis of a core mathematics
course.
Discrete mathematics encompasses a very wide range of mathematical topics and
we have necessarily been selective in our choice of material. Our starting point
was a report of the M2 Study Group of the 1986 Undergraduate Mathematics
Teaching Conference held at the University of Nottingham.
Their report,
published in 1987, suggested an outline syllabus for a ﬁrst-year mathematics
course for computer science undergraduates.
All the topic areas (with the
exception of probability theory) suggested in the outline are covered in this text.
We have also been inﬂuenced in our selection of material by various courses at
the freshman–sophomore level offered by institutions in the USA.
Ultimately the selection, presentation and emphasis of the material in this book
were based on our own judgements. We have attempted to include the essential
mathematical material required by undergraduate computer scientists in a ﬁrst
course. However, one of our key aims is to develop in students the rigorous
logical thinking which, we believe, is essential if computer science graduates are
to adapt to the demands of their rapidly developing discipline. Our approach is
informal. We have attempted to keep prerequisites to an absolute minimum and
to maintain a level of discussion within the reach of the student. In the process,
we have not sacriﬁced the mathematical rigour which we believe to be important
if mathematics is to be used in a meaningful way.
Our priority has been to give a sound and thorough treatment of the mathematics.
We also felt that it was important to place the theory in context by including
a selection of the more salient applications. It is our belief that mathematical
applications can be readily assimilated only when a ﬁrm mathematical foundation
has been laid.
Too frequently, students are exposed to concepts requiring
mathematical background before the background has been adequately provided.
We hope this text will provide such a foundation.
In order to keep the book within manageable proportions and still provide some
applications, we have been forced to omit certain topics such as ﬁnite state
machines and formal languages. Although such topics are relevant to computer
scientists and others, we felt that they were not central to the mathematical core
of the text. We believe that the book will provide a sound background for readers
who wish to explore these and other areas.
As our writing of the text progressed and its content was discussed with
colleagues, we became increasingly conscious that we were presenting material

Preface to the First Edition
xv
which lies at the very foundation of mathematics itself.
It seems likely that
discrete mathematics will become an increasingly important part of mathematics
curricula at all levels in the coming years. Given our emphasis on a sound and
thorough development of mathematical concepts, this text would be appropriate
for undergraduate mathematicians following a course in discrete mathematics.
The ﬁrst half of the book could also be recommended reading for the aspiring
mathematics undergraduate in the summer before he or she enters university.
The approximate interdependence of the various parts of the text is shown in the
diagram below. There are various sections which are concerned largely with
applications (or further development) of the theory and which may be omitted
without jeopardizing the understanding of later material. The most notable of
these are §§4.7, 5.5, 5.6 and 8.7.
We wish to acknowledge with thanks our families, friends and colleagues for
their encouragement. In particular we would like to thank Dr Paul Milican, Paul
Douglas and Alice Tomiˇc for their advice and comments on various parts of the

xvi
Preface to the First Edition
manuscript. Our reviewers provided many helpful comments and suggestions for
which we are grateful. If the text contains any errors or stylistic misjudgements,
we can only blame each other. The technical services staff at Richmond College
and Jim Revill and Al Troyano at IOP Publishing also deserve our thanks for their
patience with us during the development of this text. Last, but not least, we wish
to thank Pam Taylor for providing (at short notice) the ideas and sketches for the
cartoons.
RG and JT
July 1990

List of Symbols
The following is a list of symbols introduced in this book together with their
interpretations and the section where each is deﬁned.
Symbol
Interpretation
Section
¯p
negation of the proposition p
1.2
p ∧q
conjunction of the propositions p and q
1.2
p ∨q
inclusive disjunction of the propositions p and q
1.2
p ⊻q
exclusive disjunction of the propositions p and q
1.2
p →q
conditional proposition ‘if p then q’
1.2
p ↔q
biconditional proposition ‘p if and only if q’
1.2
t
tautology
1.3
f
contradiction
1.3
P ≡Q
logical equivalence of P and Q
1.4
P ⊢Q
the proposition P logically implies the proposi-
tion Q
1.4
P(x)
propositional function with variable x
1.4
∀
the universal quantiﬁer
1.8
∃
the existential quantiﬁer
1.8
¬
negation of a propositional function or of a
quantiﬁed propositional function
1.8
P ⇒Q
Q is logically implied by P in conjunction with
axioms and theorems which apply to the system
2.3
P ⇔Q
P ⇒Q and Q ⇒P
2.3
a ∈A
the element a belongs to the set A
3.1
a /∈A
the element a does not belong to the set A
3.1
∅
the empty set
3.1
|A|
the cardinality of the set A
3.1
xvii

xviii
List of Symbols
A ⊆B
the set A is a subset of the set B
3.2
A ⊂B
the set A is a proper subset of the set B
3.2
A ̸⊆B
the set A is not a subset of the set B
3.2
A ⊇B
the set A is a superset of the set B
3.2
U
the universal set
3.2
N
the set of natural numbers
3.2
Z
the set of integers
3.2
Q
the set of rational numbers
3.2
R
the set of real numbers
3.2
C
the set of complex numbers
3.2
E
the set of even numbers
3.2
O
the set of odd numbers
3.2
Z+
the set of positive integers
3.2
Z−
the set of negative integers
3.6
Q+
the set of positive rational numbers
3.2
R+
the set of positive real numbers
3.2
E+
the set of positive even numbers
5.4
O+
the set of positive odd numbers
5.5
A ∩B
the intersection of the sets A and B
3.3
A ∪B
the union of the sets A and B
3.3
¯A
the complement of the set A
3.3
A −B
the difference of the sets A and B
3.3
n
\
r=1
Ar
the intersection of the sets A1, A2, . . . , An
3.3
n
[
r=1
Ar
the union of the sets A1, A2, . . . , An
3.3
A ∗B
the symmetric difference of the sets A and B
3.5
P(A)
the power set of the set A
3.6
\
i∈I
Ai
the intersection of the family of sets {Ai : i ∈
I}
3.6
[
i∈I
Ai
the union of the family of sets {Ai : i ∈I}
3.6
A × B
the Cartesian product of the sets A and B
3.7
X2
the Cartesian product X × X
3.7
a R b
the element a is related to the element b
4.1
a
±
R b
the element a is not related to the element b
4.1
IA
the identity relation on the set A
4.1

List of Symbols
xix
UA
the universal relation on the set A
4.1
R−1
the inverse relation of the relation R
4.1
S ◦R
the composite of the relations R and S
4.3
[x]
the equivalence class of the element x
4.4
a ≡n b
a is congruent modulo n to b, i.e. a−b = kn for
some integer k (see also section 9.3)
4.4
+n
addition modulo n
4.4
×n
multiplication modulo n
4.4
Zn
the set of equivalence classes under congruence
modulo n, i.e. {[0], [1], . . . , [n −1]}
4.4
⌊x⌋
the integer part of the real number x, i.e. the
largest integer less than or equal to x
4.4
[a, b)
the half-open interval {x ∈R : a ⩽x < b}
4.4
(a, b]
the half-open interval {x ∈R : a < x ⩽b}
4.4
[a, b]
the closed interval {x ∈R : a ⩽x ⩽b}
4.5
(a, b)
the open interval {x ∈R : a < x < b}
4.5
n|m
n divides m
4.5
f : A →B
a function f from the set A to the set B, i.e. a
function with domain A and codomain B
5.1
f(a)
the image of the element a under the function f
5.1
f : a 7→b
for the function f the image of the element a is b
5.1
idA
the
identity
function
with
domain
and
codomain A
5.1
im(f)
the image set of the function f, i.e. the subset of
the codomain of f which contains the images of
all elements in the domain
5.1
f(C)
the image of the set C under the function f
5.1
f −1(D)
the inverse image of the set D under the
function f
5.1
f ◦g
the composite of the functions f and g, where
f ◦g(x) = f[g(x)]
5.2
iC
inclusion function of a subset C in a set A
5.2
f|C
restriction of the function f to a subset C of its
domain
5.2
P
the set of prime numbers
5.5
ℵ0
the cardinality of Z+
5.5
c
the cardinality of R
5.5
aij
the element in the matrix A occupying the ith
row and jth column
6.1
[aij]
the matrix with (i, j)-entry aij
6.1
Om×n
the m × n zero matrix
6.2
In
the n × n identity matrix
6.2
AT
the transpose of the matrix A
6.2
A ∼B
the matrix A is row-equivalent to the matrix B
6.4

xx
List of Symbols
A−1
the multiplicative inverse of the matrix A
6.5
(A B)
the partitioned matrix with submatrices A and B
6.5
(A b)
the augmented matrix of a system of linear
equations with matrix of coefﬁcients A
7.3
e
the identity with respect to a binary operation
8.1
(S, ∗)
the algebraic structure with underlying set S and
binary operation ∗
8.2
A∗
the set of all strings over the alphabet A
8.2
λ
the empty string
8.2
(G, ∗)
the group with underlying set G and binary
operation ∗
8.3
Dn
the dihedral group of degree n
8.4
Sn
the symmetric group of degree n
8.4
(G1, ∗) ⩽(G2, ◦)
the group (G1, ∗) is a subgroup of the group
(G2, ◦)
8.5
Cn
the group of rotations of a regular n-sided
polygon
8.5
|g|
the order of an element g ∈G of a group (G, ∗)
8.5
(G1, ∗) ∼= (G2, ◦)
the groups (G1, ∗) and (G2, ◦) are isomorphic
8.6
ker f
the kernel of a morphism f : G1 →G2 where
(G1, ∗) and (G2, ◦) are groups
8.6
d(x, y)
the distance between the binary words x and y
8.7
Bn
the set of binary words of length n
8.7
w(x)
the weight of the binary word x
8.7
x ⊕y
the n bit word whose ith bit is the sum modulo
2 of the ith bits of the n bit words x and y
8.7
b|a
the integer b divides the integer a
9.1
gcd(a, b)
the greatest common divisor of a and b
9.1
a ≡b mod n
a is congruent to b modulo n, i.e. a−b = kn for
some integer k (see also section 4.4)
9.3
φ(n)
the number of integers a where 1 ⩽a ⩽n
which are coprime to n
9.5
(B, ⊕, ∗,¯, 0, 1)
the Boolean algebra with underlying set B,
binary
operations
⊕
and
∗,
complement
operation¯, and identities 0 and 1 under ⊕and ∗
respectively
10.1
¯b
the complement of the element b ∈B, the
underlying set of a Boolean algebra
10.1
me1 e2...en
the minterm x1e1x2e2 . . . xnen where e1 = 0 or
1 (i = 1, 2, . . . , n) and
xi
ei =
(
¯xi
if ei = 0
xi
if e1 = 1
10.3

List of Symbols
xxi
Me1 e2...en
the maxterm x1e1 ⊕x2e2 ⊕· · · ⊕xnen where
e1 = 0 or 1 (i = 1, 2, . . . , n) and
xi
ei =
(
¯xi
if ei = 0
xi
if e1 = 1
10.3
the switch denoted by A
10.4
¯S
a switch which is always in the opposite state to
another switch S
10.4
AND-gate
10.5
OR-gate
10.5
NOT-gate
10.5
NAND-gate
10.5
NOR-gate
10.5
δ(e)
the set of vertices incident to the edge e of a
graph
11.1
Cn
the cycle graph with n vertices
11.1
Wn
the wheel graph with n vertices
11.1
deg(v)
the degree of the vertex v of a graph
11.1
Kn
the complete graph with n vertices
11.1
Kn,m
the complete bipartite graph on n and m vertices
11.1
A(Γ)
the adjacency matrix for the graph Γ
11.1
Γ ⩽Σ
the graph Γ is a subgraph of the graph Σ
11.1
Γ + Σ
the sum of the graphs Γ and Σ
11.1
Γ ∪Σ
the union of the graphs Γ and Σ
11.1
Γ ∼= Σ
the graphs Γ and Σ are isomorphic
11.3
E(v, w)
the set of edges joining the vertices v and w of a
graph
11.3
δ(e)
the ordered pair of initial and ﬁnal vertices of the
(directed) edge e of a directed graph
11.6
(T, v∗)
the rooted tree with root v∗
12.2
(L, {v}, R)
the binary tree with root v, left subtree L and
right subtree R
12.2
a ⩽b
a R b where a, b ∈A and A is a totally ordered
set under the order relation R
12.3
w(e)
the weight of the edge e of a weighted graph
12.5
w(Γ′)
the weight of the subgraph Γ′ of a weighted
graph Γ
12.5
w(v1, v2)
the weight of the unique edge joining vertices v1
and v2 of a complete weighted graph
12.6


Chapter 1
Logic
Logic is used to establish the validity of arguments. It is not so much concerned
with what the argument is about but more with providing rules so that the general
form of the argument can be judged as sound or unsound. The rules which logic
provides allow us to assess whether the conclusion drawn from stated premises
is consistent with those premises or whether there is some faulty step in the
deductive process which claims to support the validity of the conclusion.
1.1
Propositions and Truth Values
A proposition is a declarative statement which is either true or false, but not both
simultaneously. (Propositions are sometimes called ‘statements’.) Examples of
propositions are:
1.
This rose is white.
2.
Triangles have four vertices.
3.
3 + 2 = 4.
4.
6 < 24.
5.
Tomorrow is my birthday.
Note that the same proposition may sometimes be true and sometimes false
depending on where and when it was stated and by whom. Whilst proposition 5 is
true when stated by anyone whose birthday is tomorrow, it is false when stated by
anyone else. Further, if anyone for whom it is a true statement today states it on
any other day, it will then be false. Similarly, the truth or falsity of proposition 1
depends on the context in which the proposition was stated.
1

2
Logic
Exclamations, questions and demands are not propositions since they cannot be
declared true or false. Thus the following are not propositions:
6.
Keep off the grass.
7.
Long live the Queen!
8.
Did you go to Jane’s party?
9.
Don’t say that.
The truth (T) or falsity (F) of a proposition is called truth value. Proposition 4
has a truth value of true (T) and propositions 2 and 3 have truth values of false (F).
The truth values of propositions 1 and 5 depend on the circumstances in which the
statement was uttered. Sentences 6–9 are not propositions and therefore cannot
be assigned truth values.
Propositions are conventionally symbolized using the letters p, q, r, . . . .
Any
of these may be used to symbolize speciﬁc propositions, e.g. p: Manchester is
in Scotland, q: Mammoths are extinct. We also use these letters to stand for
arbitrary propositions, i.e. as variables for which any particular proposition may
be substituted.
1.2
Logical Connectives and Truth Tables
The propositions 1–5 considered in §1.1 are simple propositions since they make
only a single statement.
In this section we look at how simple propositions
can be combined to form more complicated propositions called compound
propositions.
The devices which are used to link pairs of propositions are
called logical connectives and the truth value of any compound proposition
is completely determined by (a) the truth values of its component simple
propositions, and (b) the particular connective, or connectives, used to link them.
Before we look at the most commonly used connectives we ﬁrst look at an
operation which can be performed on a single proposition. This operation is called
negation and it has the effect of reversing the truth value of the proposition. We
state the negation of a proposition by preﬁxing it by ‘It is not the case that. . . ’.
This is not the only way of negating a proposition but what is important is that the
negation is false in all circumstances that the proposition is true, and true in all
circumstances that the proposition is false.
We can summarize this in a table. If p symbolizes a proposition ¯p (or ¬p or −p
or ∼p) symbolizes the negation of p. The following table shows the relationship

Logical Connectives and Truth Tables
3

4
Logic
between the truth values of p and those of ¯p.
p
¯p
T
F
F
T
The left-hand column gives all possible truth values for p and the right-hand
column gives the corresponding truth values of ¯p, the negation of p. A table
which summarizes truth values of propositions in this way is called a truth table.
There are several alternative ways of stating the negation of a proposition. If we
consider the proposition ‘All dogs are ﬁerce’, some examples of its negation are:
It is not the case that all dogs are ﬁerce.
Not all dogs are ﬁerce.
Some dogs are not ﬁerce.
Note that the proposition ‘No dogs are ﬁerce’ is not the negation of ‘All dogs are
ﬁerce’. Remember that to be the negation, the second statement must be false in
all circumstances that the ﬁrst is true and vice versa. This is clearly not the case
since ‘All dogs are ﬁerce’ is false if just one dog is not ﬁerce. However, ‘No dogs
are ﬁerce’ is not true in this case. (See §1.8.)
Whilst negation is an operation which involves only a single proposition, logical
connectives are used to link pairs of propositions.
We shall consider ﬁve
commonly used logical connectives: conjunction, inclusive disjunction, exclusive
disjunction, the conditional and biconditional.
Conjunction
Two simple propositions can be combined by using the word ‘and’ between
them. The resulting compound proposition is called the conjunction of its two
component simple propositions. If p and q are two propositions p ∧q (or p. q)
symbolizes the conjunction of p and q. For example:
p : The sun is shining.
q : Pigs eat turnips.
p ∧q : The sun is shining and pigs eat turnips.
The following truth table gives the truth values of p ∧q (read as ‘p and q’) for

Logical Connectives and Truth Tables
5
each possible pair of truth values of p and q.
p
q
p ∧q
T
T
T
T
F
F
F
T
F
F
F
F
From the table it can be seen that the conjunction p ∧q is true only when both p
and q are true. Otherwise the conjunction is false.
Linking two propositions using ‘and’ is not the only way of forming a
conjunction. The following are also conjunctions of p and q even though they
have nuances which are slightly different from when the two propositions are
joined using ‘and’.
The sun shines but pigs eat turnips.
Although the sun shines, pigs eat turnips.
The sun shines whereas pigs eat turnips.
All give the sense that they are true only when each simple component is true.
Otherwise they would be judged as false.
Disjunction
The word ‘or’ can be used to link two simple propositions.
The compound
proposition so formed is called the disjunction of its two component simple
propositions.
In logic we distinguish two different types of disjunction, the
inclusive and exclusive forms. The word ‘or’ in natural language is ambiguous in
conveying which type of disjunction we mean. We return to this point after we
have considered the two forms.
Given the two propositions p and q, p∨q symbolizes the inclusive disjunction of
p and q. This compound proposition is true when either or both of its components
are true and is false otherwise. Thus the truth table for p ∨q is given by:
p
q
p ∨q
T
T
T
T
F
T
F
T
T
F
F
F

6
Logic
The exclusive disjunction of p and q is symbolized by p ⊻q. This compound
proposition is true when exactly one (i.e. one or other, but not both) of its
components is true. The truth table for p ⊻q is given by:
p
q
p ⊻q
T
T
F
T
F
T
F
T
T
F
F
F
When two simple propositions are combined using ‘or’, context will often provide
the clue as to whether the inclusive or exclusive sense is intended. For instance,
‘Tomorrow I will go swimming or play golf’ seems to suggest that I will not
do both and therefore points to an exclusive interpretation. On the other hand,
‘Applicants for this post must be over 25 or have at least 3 years relevant
experience’ suggests that applicants who satisfy both criteria will be considered,
and that ‘or’ should therefore be interpreted inclusively.
Where context does not resolve the ambiguity surrounding the word ‘or’, the
intended sense can be made clear by afﬁxing ‘or both’ to indicate an inclusive
reading, or by afﬁxing ‘but not both’ to make clear the exclusive sense. Where
there is no clue as to which interpretation is intended and context does not make
this clear, then ‘or’ is conventionally taken in its inclusive sense.
Conditional Propositions
The conditional connective (sometimes called implication) is symbolized by →
(or by ⊃). The linguistic expression of a conditional proposition is normally
accepted as utilizing ‘if . . . then . . . ’ as in the following example:
p : I eat breakfast.
q : I don’t eat lunch.
p →q : If I eat breakfast then I don’t eat lunch.
Alternative expressions for p →q in this example are:
I eat breakfast only if I don’t eat lunch.
Whenever I eat breakfast, I don’t eat lunch.
That I eat breakfast implies that I don’t eat lunch.

Logical Connectives and Truth Tables
7
The following is the truth table for p →q:
p
q
p →q
T
T
T
T
F
F
F
T
T
F
F
T
Notice that the proposition ‘if p then q’ is false only when p is true and q is
false, i.e. a true statement cannot imply a false one. If p is false, the compound
proposition is true no matter what the truth value of q. To clarify this, consider
the proposition: ‘If I pass my exams then I will get drunk’. This statement says
nothing about what I will do if I don’t pass my exams. I may get drunk or I may
not, but in either case you could not accuse me of having made a false statement.
The only circumstances in which I could be accused of uttering a falsehood is if I
pass my exams and don’t get drunk.
In the conditional proposition p →q, the proposition p is sometimes called the
antecedent and q the consequent. The proposition p is said to be a sufﬁcient
condition for q and q a necessary condition for p.
Biconditional Propositions
The biconditional connective is symbolized by ↔, and expressed by ‘if and only
if . . . then . . . ’. Using the previous example:
p : I eat breakfast.
q : I don’t eat lunch.
p ↔q : I eat breakfast if and only if I don’t eat lunch (or alternatively, ‘If and
only if I eat breakfast, then I don’t eat lunch’).
The truth table for p ↔q is given by:
p
q
p ↔q
T
T
T
T
F
F
F
T
F
F
F
T
Note that for p ↔q to be true, p and q must both have the same truth values, i.e.
both must be true or both must be false.

8
Logic
Examples 1.1
1.
Consider the following propositions:
p : Mathematicians are generous.
q : Spiders hate algebra.
Write the compound propositions symbolized by:
(i)
p ∨¯q
(ii)
(q ∧p)
(iii)
¯p →q
(iv)
¯p ↔¯q.
Solution
(i)
Mathematicians are generous or spiders don’t hate algebra (or both).
(ii)
It is not the case that spiders hate algebra and mathematicians are
generous.
(iii)
If mathematicians are not generous then spiders hate algebra.
(iv)
Mathematicians are not generous if and only if spiders don’t hate algebra.
(As we have seen, these are not unique solutions and there are acceptable
alternatives.)
2.
Let p be the proposition ‘Today is Monday’ and q be ‘I’ll go to London’.
Write the following propositions symbolically.
(i)
If today is Monday then I won’t go to London.
(ii)
Today is Monday or I’ll go to London, but not both.
(iii)
I’ll go to London and today is not Monday.
(iv)
If and only if today is not Monday then I’ll go to London.
Solution
(i)
p →¯q
(ii)
p ⊻q
(iii)
q ∧¯p
(iv)
¯p ↔q.
3.
Construct truth tables for the following compound propositions.
(i)
¯p ∨q

Logical Connectives and Truth Tables
9
(ii)
¯p ∧¯q
(iii)
¯q →p
(iv)
¯p ↔¯q.
Solution
(i)
p
q
¯p
¯p ∨q
T
T
F
T
T
F
F
F
F
T
T
T
F
F
T
T
Note that the truth table is built up in stages. The ﬁrst two columns give
the usual combinations of possible truth values of p and q. The third
column gives, for each truth value of p, the truth value of ¯p. When p
is true, ¯p is false and vice versa. The last column combines the truth
values in columns 3 and 2 using the inclusive disjunction connective.
The compound proposition ¯p ∨q is true when at least one of its two
components is true. This is the case in row 1 (where q is true), row 3 (¯p
and q are both true) and row 4 (¯p is true). In the second row, ¯p and q are
both false and hence ¯p ∨q is false.
(ii)
p
q
¯p
¯q
¯p ∧¯q
T
T
F
F
F
T
F
F
T
F
F
T
T
F
F
F
F
T
T
T
Here we ﬁrst obtain truth values for ¯p and ¯q by reversing the
corresponding truth values of p and q respectively. Now ¯p ∧¯q is only
true when both ¯p and ¯q are true, i.e. in row 4. In all other cases ¯p ∧¯q is
false.
(iii)
p
q
¯q
¯q →p
T
T
F
T
T
F
T
T
F
T
F
T
F
F
T
F

10
Logic
(iv)
p
q
¯p
¯q
¯p ↔¯q
T
T
F
F
T
T
F
F
T
F
F
T
T
F
F
F
F
T
T
T
We can construct truth tables for compound propositions involving more than two
simple propositions as in the following example.
4.
Construct truth tables for:
(i)
p →(q ∧r)
(ii)
(¯p ∨q) ↔¯r.
Solution
(i)
p
q
r
q ∧r
p →(q ∧r)
T
T
T
T
T
T
T
F
F
F
T
F
T
F
F
T
F
F
F
F
F
T
T
T
T
F
T
F
F
T
F
F
T
F
T
F
F
F
F
T
The ﬁrst three columns list all possible combinations of truth values for
p, q and r. Since each proposition can take two truth values there are
23 = 8 possible combinations of truth values for the three propositions.
Column 4 gives truth values of q∧r by comparing the truth values of q and
r individually in columns 2 and 3. Considering the pairs of truth values in
columns 1 and 4 gives the truth values for p →(q ∧r). Remember that
this compound proposition is false only when p is true and q ∧r is false,
i.e. in rows 2, 3 and 4.

Logical Connectives and Truth Tables
11
(ii)
Again we build up the truth table column by column to obtain the
following:
p
q
r
¯p
¯r
¯p ∨q
(¯p ∨q) ↔¯r
T
T
T
F
F
T
F
T
T
F
F
T
T
T
T
F
T
F
F
F
T
T
F
F
F
T
F
F
F
T
T
T
F
T
F
F
T
F
T
T
T
T
F
F
T
T
F
T
F
F
F
F
T
T
T
T
Exercises 1.1
1.
Consider the propositions:
p : Max is sulking.
q : Today is my birthday.
Write in words the compound propositions given by:
(i)
¯p ∧q
(ii)
p ∨q
(iii)
¯p →q
(iv)
q ↔p.
2.
Consider the propositions:
p : Mary laughs.
q : Sally cries.
r : Jo shouts.
Write in words the following compound propositions:
(i)
p →(q ⊻r)
(ii)
(r ∧q) ↔p
(iii)
(p →¯q) ∧(r →q)
(iv)
p ∨(¯q ∨¯r)
(v)
(p ∨r) ↔¯q.

12
Logic
3.
Let p, q and r denote the following propositions:
p : Bats are blind.
q : Gnats eat grass.
r : Ants have long teeth.
Express the following compound propositions symbolically.
(i)
If bats are blind then gnats don’t eat grass.
(ii)
If and only if bats are blind or gnats eat grass then ants don’t have
long teeth.
(iii)
Ants don’t have long teeth and, if bats are blind, then gnats don’t
eat grass.
(iv)
Bats are blind or gnats eat grass and, if gnats don’t eat grass, then
ants don’t have long teeth.
4.
Draw a truth table and determine for what truth values of p and q the
proposition ¯p ∨q is false.
5.
Draw the truth table for the propositions:
(i)
¯p →q
(ii)
¯q ∧p
(iii)
(p ∨q) →(p ∧q)
(iv)
(p →q) ⊻¯q
(v)
¯p ↔(p ∧q)
(vi)
(¯p ∧q) ⊻(p ∨¯q).
6.
Consider the two propositions:
p : John is rich.
q : John is dishonest.
Under what circumstances is the compound proposition ‘If John is honest
then he is not rich’ false?
7.
Given the three propositions p, q and r, construct truth tables for:
(i)
(p ∧q) →¯r
(ii)
(p ⊻r) ∧¯q
(iii)
p ∧(¯q ∨r)
(iv)
p →(¯q ∨¯r)
(v)
(p ∨q) ↔(r ∨p).

Tautologies and Contradictions
13
1.3
Tautologies and Contradictions
There are certain compound propositions which have the surprising property that
they are always true no matter what the truth value of their simple components.
Similarly, there are others which are always false regardless of the truth values of
their components. In both cases, this property is a consequence of the structure of
the compound proposition.
Deﬁnition 1.1
A tautology is a compound proposition which is true no matter what the
truth values of its simple components.
A contradiction is a compound proposition which is false no matter what
the truth values of its simple components.
We shall denote a tautology by t and a contradiction by f.
Examples 1.2
1.
Show that p ∨¯p is a tautology.
Solution
Constructing the truth table for p ∨¯p, we have:
p
¯p
p ∨¯p
T
F
T
F
T
T
Note that p ∨¯p is always true (no matter what proposition is substituted
for p) and is therefore a tautology.
2.
Show that (p ∧q) ∨(p ∧q) is a tautology.

14
Logic
Solution
The truth table for (p ∧q) ∨(p ∧q) is given below.
p
q
p ∧q
p ∧q
(p ∧q) ∨(p ∧q)
T
T
T
F
T
T
F
F
T
T
F
T
F
T
T
F
F
F
T
T
The last column of the truth table contains only the truth value T and hence we
can deduce that (p ∧q) ∨(p ∧q) is a tautology.
Note that, in the last example, we could have appealed to the result obtained in
the ﬁrst one where we showed that the inclusive disjunction of any proposition
and its negation is a tautology. In example 1.2.2 we have a proposition p ∧q
and its negation (p ∧q). Hence, by the previous result, the inclusive disjunction
(p ∧q) ∨(p ∧q) is a tautology.
The proposition (p ∧q) ∨(p ∧q) is said to be a substitution instance of the
proposition p ∨¯p. The former proposition is obtained from the latter simply by
substituting p∧q for p throughout. Clearly any substitution instance of a tautology
is itself a tautology so that one way of establishing that a proposition is a tautology
is to show that it is a substitution instance of another proposition which is known
to be a tautology.
Example 1.3
Show that (p ∧¯q) ∧(¯p ∨q) is a contradiction.
Solution
p
q
¯q
p ∧¯q
¯p
¯p ∨q
(p ∧¯q) ∧(¯p ∨q)
T
T
F
F
F
T
F
T
F
T
T
F
F
F
F
T
F
F
T
T
F
F
F
T
F
T
T
F

Logical Equivalence and Logical Implication
15
The last column shows that (p ∧¯q) ∧(¯p ∨q) is always false, no matter what the
truth values of p and q. Hence (p ∧¯q) ∧(¯p ∨q) is a contradiction.
Just as any substitution instance of a tautology is also a tautology, so any
substitution instance of a contradiction is also a contradiction. For instance, using
a truth table, we can show that p ∧¯p is a contradiction. Since (p →q) ∧(p →q)
is a substitution instance of p ∧¯p, we can deduce that this compound proposition
is also a contradiction.
Exercises 1.2
Determine whether each of the following is a tautology, a contradiction or neither:
1.
p →(p ∨q)
2.
(p →q) ∧(¯p ∨q)
3.
(p ∨q) ↔(q ∨p)
4.
(p ∧q) →p
5.
(p ∧q) ∧(p ∨q)
6.
(p →q) →(p ∧q)
7.
(¯p ∧q) ∧(p ∨¯q)
8.
(p →¯q) ∨(¯r →p)
9.
[p →(q ∧r)] ↔[(p →q) ∧(p →r)]
10.
[(p ∨q) →¯r] ⊻(¯p ∨¯q).
1.4
Logical Equivalence and Logical Implication
Two propositions are said to be logically equivalent if they have identical truth
values for every set of truth values of their components. Using P and Q to denote

16
Logic
(possibly) compound propositions, we write P ≡Q if P and Q are logically
equivalent.
As with tautologies and contradictions, logical equivalence is a
consequence of the structures of P and Q.
Example 1.4
Show that ¯p ∨¯q and p ∧q are logically equivalent, i.e. that (¯p ∨¯q) ≡(p ∧q).
Solution
We draw up the truth table for ¯p ∨¯q and also for p ∧q.
p
q
¯p
¯q
¯p ∨¯q
p ∧q
p ∧q
T
T
F
F
F
T
F
T
F
F
T
T
F
T
F
T
T
F
T
F
T
F
F
T
T
T
F
T
Comparing the columns for ¯p ∨¯q and for p ∧q we note that the truth values are
the same. Each is true except in the case where p and q are both true. Hence ¯p ∨¯q
and p ∧q are logically equivalent propositions.
Note that if two compound propositions are logically equivalent, then the
compound proposition formed by joining them using the biconditional connective
must be a tautology, i.e. if P ≡Q then P ↔Q is a tautology. This is so because
two logically equivalent propositions are either both true or both false. In either
of these cases the biconditional is true.
The converse is also the case, i.e. if P ↔Q is a tautology, then P ≡Q. This
follows from the fact that the biconditional P ↔Q is only true when P and Q
both have the same truth values.
In example 1.4, we showed that ¯p ∨¯q and p ∧q are logically equivalent by
constructing their truth tables and comparing truth values. An alternative method
would have been to show that (¯p∨¯q) ↔(p ∧q) is a tautology and to deduce from
this the logical equivalence of ¯p ∨¯q and p ∧q.
Example 1.5
Show that the following two propositions are logically equivalent.

Logical Equivalence and Logical Implication
17
(i)
If it rains tomorrow then, if I get paid, I’ll go to Paris.
(ii)
If it rains tomorrow and I get paid then I’ll go to Paris.
Solution
Deﬁne the following simple propositions:
p : It rains tomorrow.
q : I get paid.
r : I’ll go to Paris.
We are required to show the logical equivalence of p →(q →r) and (p∧q) →r.
We can do this in one of two ways:
(a)
establish that p →(q →r) and (p ∧q) →r have the same truth values,
or
(b)
establish that [p →(q →r)] ↔[(p ∧q) →r] is a tautology.
Using the ﬁrst method we complete the truth table for p →(q →r) and (p∧q) →
r.
p
q
r
q →r
p →(q →r)
p ∧q
(p ∧q) →r
T
T
T
T
T
T
T
T
T
F
F
F
T
F
T
F
T
T
T
F
T
T
F
F
T
T
F
T
F
T
T
T
T
F
T
F
T
F
F
T
F
T
F
F
T
T
T
F
T
F
F
F
T
T
F
T
Since the truth values of p →(q →r) and (p ∧q) →r are the same for each
set of truth values of p, q and r, we can deduce the logical equivalences of these
compound propositions. Completing one further column of the truth table for
[p →(q →r)] ↔[(p ∧q) →r] would show this to be a tautology and would
establish the logical equivalence of the two propositions by the second method.
Another structure-dependent relation which may exist between two propositions
is that of logical implication.
A proposition P is said to logically imply a
proposition Q if, whenever P is true, then Q is also true.
Note that the converse does not apply, i.e. Q may also be true when P is false.
For logical implication all we insist on is that Q is never false when P is true. We

18
Logic
shall symbolize logical implication by ⊢so that ‘P logically implies Q’ is written
P ⊢Q.
Example 1.6
Show that q ⊢(p ∨q).
Solution
We must show that, whenever q is true, then p ∨q is true. Constructing the truth
table gives:
p
q
p ∨q
T
T
T
T
F
T
F
T
T
F
F
F
From a comparison of the second and third columns we note that, whenever q is
true (ﬁrst and third rows), p ∨q is also true. Note that p ∨q is also true when
q is false (second row) but this has no relevance in establishing that q logically
implies p ∨q.
We showed that ‘P ≡Q’ and ‘P ↔Q is a tautology’ mean exactly the same. A
similar line of argument can be used to establish that ‘P ⊢Q’ and ‘P →Q is a
tautology’ are identical statements. If we have P ⊢Q then Q is never false when
P is true. Since this is the only situation where P →Q would be false then we
must have P →Q is a tautology. Conversely, if P →Q is a tautology then the
truth of P guarantees the truth of Q and hence we have P ⊢Q.
Example 1.7
Show that (p ↔q) ∧q logically implies p.
Solution
As with example 1.5 we can show that [(p ↔q) ∧q] ⊢p in one of two ways. We
can either show that p is always true when (p ↔q) ∧q is true or we can show
that [(p ↔q) ∧q] →p is a tautology.

Logical Equivalence and Logical Implication
19
The truth table for (p ↔q) ∧q is given by:
p
q
p ↔q
(p ↔q) ∧q
T
T
T
T
T
F
F
F
F
T
F
F
F
F
T
F
Comparing the fourth column with the ﬁrst, we see that p is true whenever
(p ↔q) ∧q is true (ﬁrst row only). Therefore [(p ↔q) ∧q] ⊢p.
Alternatively, we could complete a further column of of the truth table for
[(p ↔q) ∧q] →p and show this to be a tautology.
More about conditionals
Given the conditional proposition p →q, we deﬁne the following:
(a)
the converse of p →q :
q →p
(b)
the inverse of p →q :
¯p →¯q
(c)
the contrapositive of p →q :
¯q →¯p.
The following truth table gives values of the conditional together with those for
its converse, inverse and contrapositive.
p
q
p →q
q →p
¯p →¯q
¯q →¯p
T
T
T
T
T
T
T
F
F
T
T
F
F
T
T
F
F
T
F
F
T
T
T
T
From the table we note the following useful result: a conditional proposition
p →q and its contrapositive ¯q →¯p are logically equivalent, i.e. (p →q) ≡
(¯q →¯p).
Note that a conditional proposition is not logically equivalent to either its converse
or inverse.
However, the converse and inverse of a proposition are logically
equivalent to each other.

20
Logic
Example 1.8
State the converse, inverse and contrapositive of the proposition ‘If Jack plays his
guitar then Sara will sing’.
Solution
We deﬁne:
p: Jack plays his guitar
q: Sara will sing
so that:
p →q: If Jack plays his guitar then Sara will sing.
Converse:
q →p: If Sara will sing then Jack plays his guitar.
Inverse:
¯p →¯q: If Jack doesn’t play his guitar then Sara won’t sing.
Contrapositive:
¯q →¯p: If Sara won’t sing then Jack doesn’t play his guitar.
As we have shown, ‘If Jack plays his guitar then Sara will sing’ and ‘If Sara won’t
sing then Jack doesn’t play his guitar’ are equivalent propositions.
Exercises 1.3
1.
Prove that (p →q) ≡(¯p ∨q).
2.
Prove that (p ∧q) and (p →¯q) are logically equivalent propositions.
3.
Prove that (p ⊻q) ≡(p ⊻¯q).
4.
Prove that p logically implies (q →p).
5.
Prove that (¯q →¯p) ⊢(p →q).
6.
Prove the following logical implications:
(i)
(p ∧q) ⊢q
(ii)
(p ∧q) ⊢p
(iii)
[(p →q) ∧p] ⊢q
(iv)
[(p →q) ∧(p ∨r)] ⊢(q ∨r)
(v)
p ⊢(q →p)
(vi)
[(p ∨q) ∧¯q] ⊢p.
7.
Prove that the exclusive disjunction of p and q is logically equivalent to
the negation of the biconditional proposition p ↔q.

Logical Equivalence and Logical Implication
21
8.
Show that the biconditional proposition p ↔q is logically equivalent to
the conjunction of the two conditional propositions p →q and q →p.
(Thus, in the biconditional p ↔q, proposition p is a necessary and
sufﬁcient condition for q and q is a necessary and sufﬁcient condition
for p.)
9.
Establish the following logical equivalences:
(i)
(p →q) ≡(p ∧¯q)
(ii)
(p ↔q) ≡(p ∧¯q) ∧(q ∧¯p)
(iii)
(p ∨q) ≡(¯p ∧¯q)
(iv)
(p ⊻q) ≡(p ∧¯q) ∧(q ∧¯p).
(These results show that any compound proposition involving the
disjunctive (either form), conditional or biconditional connectives can
be written in a logically equivalent form involving only negation and
conjunction.)
10.
Consider a new connective, denoted by |, where p|q is deﬁned by the
following truth table:
p
q
p|q
T
T
F
T
F
T
F
T
T
F
F
T
Show that:
(i)
¯p ≡(p|p)
(ii)
(p ∧q) ≡(p|q)|(p|q).
Use the results for exercise 1.3.9 above to deduce that a proposition
involving any of the ﬁve familiar connectives can be written in a logically
equivalent form which uses only the connective denoted by |.
11.
State the converse, inverse and contrapositive of the proposition: ‘If it’s
not Sunday then the supermarket is open until midnight’.

22
Logic
1.5
The Algebra of Propositions
The following is a list of some important logical equivalences, all of which can be
veriﬁed using one of the techniques described in §1.4. (In fact we demonstrated
two of these laws in §1.4.) These are often referred to as ‘replacement laws’
because, as we shall see later, there are situations where it is useful to substitute
one proposition for another logically equivalent form. These laws hold for any
simple propositions p, q and r and also for any substitution instance of them. We
give the name of each law and also, because we shall need to refer to them later,
the accepted abbreviation for identifying each one. Recall that we use t to denote
a tautology and f to denote a contradiction.
Replacement Laws
Idempotent laws (Idem)
p ∧p ≡p
p ∨p ≡p.
Commutative laws (Comm)
p ∧q ≡q ∧p
p ∨q ≡q ∨p
p ⊻q ≡q ⊻p
p ↔q ≡q ↔p.
Associative laws (Assoc)
(p ∧q) ∧r ≡p ∧(q ∧r)
(p ∨q) ∨r ≡p ∨(q ∨r)
(p ⊻q) ⊻r ≡p ⊻(q ⊻r)
(p ↔q) ↔r ≡p ↔(q ↔r).
Distributive laws (Dist)
p ∧(q ∨r) ≡(p ∧q) ∨(p ∧r)
p ∨(q ∧r) ≡(p ∨q) ∧(p ∨r).
Involution law (Invol)
¯¯p ≡p.

The Algebra of Propositions
23
De Morgan’s† laws (De M)
p ∨q ≡¯p ∧¯q
p ∧q ≡¯p ∨¯q.
Identity laws (Ident)
p ∨f ≡p
p ∧t ≡p
p ∨t ≡t
p ∧f ≡f.
Complement laws (Comp)
p ∨¯p ≡t
p ∧¯p ≡f
¯f ≡t
¯t ≡f.
Transposition law (Trans)
p →q ≡¯q →¯p
.
Material Implication law (Impl)
p →q ≡¯p ∨q
.
Material Equivalence laws (Equiv)
p ↔q ≡(p →q) ∧(q →p)
p ↔q ≡(p ∧q) ∨(¯p ∧¯q)
.
† Named after the British mathematician Augustus de Morgan (1806–71) who became the ﬁrst
professor of the new University of London in 1828 and the ﬁrst president of the London Mathematical
Society in 1865.

24
Logic
Exportation law (Exp)
(p ∧q) →r ≡p →(q →r)
.
The Duality Principle
Given any compound proposition P involving only the connectives denoted by ∧
and ∨, the dual of that proposition is obtained by replacing ∧by ∨, ∨by ∧, t by
f and f by t. For example, the dual of (p ∧q) ∨¯p is (p ∨q) ∧¯p. The dual of
(p ∨f) ∧q is (p ∧t) ∨q.
Notice that we have not stated how to obtain the dual of a compound proposition
containing connectives other than conjunction and inclusive disjunction. This
does not matter since we have shown that propositions containing the other
connectives can all be written in a logically equivalent form involving only
negation and conjunction (see exercise 1.3.9).
The duality principle states that, if two propositions are logically equivalent, then
so are their duals. The principle is evident in several of the laws of the algebra
of propositions stated above. In many cases the logical equivalences are stated in
pairs where one member of the pair is the dual of the other.
Substitution Rule
Suppose that we have two logically equivalent propositions P1 and P2, so that
P1 ≡P2. Suppose also that we have a compound proposition Q in which P1
appears.
The substitution rule says that we may replace P1 by P2 and the

The Algebra of Propositions
25
resulting proposition is logically equivalent to Q. Thus substituting a logically
equivalent proposition for another in a compound proposition does not alter the
truth value of that proposition.
Although we have not formally proved the substitution rule, it is clearly
reasonable if we consider the truth table. Substituting truth values of P2 for P1
makes no difference to the truth table since, if P1 and P2 are logically equivalent,
they have the same truth values for each set of truth values of their components.
The substitution rule and the replacement laws give us a means of establishing
logical equivalences between propositions without drawing up a truth table. We
demonstrate this in the following example.
Example 1.9
Prove that (¯p ∧q) ∨(p ∨q) ≡¯p.
Solution
(¯p ∧q) ∨(p ∨q) ≡(¯p ∧q) ∨(¯p ∧¯q)
(De M)
≡¯p ∧(q ∨¯q)
(Dist)
≡¯p ∧t
(Comp)
≡¯p.
(Ident)
Exercise 1.4
1.
Prove each of the following logical equivalences using the method of
example 1.9.
(i)
(p ∧p) ∨(¯p ∨¯p) ≡t.
(ii)
(p ∧q) ∧q ≡p ∧q.
(iii)
p →q ≡p ∧¯q.
(iv)
(p ∧q) →r ≡(¯p ∨¯q) ∨r.
(v)
q ∧[(p ∨q) ∧(¯q ∧¯p)] ≡q ∧(q ∨p).
2.
Use the method of example 1.9 to show that p ∧(q ∨¯p) is logically
equivalent to p ∧q. State the dual of each of these two propositions and
show that the two dual propositions are also logically equivalent.

26
Logic
1.6
Arguments
An argument consists of a set of propositions called premises together with
another proposition, purported to follow from the premises, called the conclusion.
We say that the argument is valid if the conjunction of the premises logically
implies the conclusion. Otherwise the argument is said to be invalid. Thus if we
have premises P1, P2, . . . , Pn and a conclusion Q, then the argument is valid if
(P1 ∧P2 ∧· · · ∧Pn) ⊢Q, i.e. if (P1 ∧P2 ∧· · · ∧Pn) →Q is a tautology. What
this means (see §1.4) is that whenever P1, P2, . . . , Pn are all true, then Q must be
true. This makes sense since it ensures that, in a valid argument, a set of premises
all of which are true cannot lead to a false conclusion.
Examples 1.10
1.
Test the validity of the following argument: ‘If you insulted Bob then I’ll
never speak to you again. You insulted Bob so I’ll never speak to you
again.’
Solution
We deﬁne: p: You insulted Bob.
q: I’ll never speak to you again.
The premises in this argument are:
p →q and p.
The conclusion is:
q.

Arguments
27
We must therefore investigate the truth table for [(p →q) ∧p] →q. If this
compound proposition is a tautology, then the argument is valid. Otherwise it is
not.
p
q
p →q
(p →q) ∧p
[(p →q) ∧p] →q
T
T
T
T
T
T
F
F
F
T
F
T
T
F
T
F
F
T
F
T
This shows that the argument is valid.
2.
Test the validity of the following argument: ‘If you are a mathematician
then you are clever. You are clever and rich. Therefore if you are rich
then you are a mathematician.’
Solution
Deﬁne: p: You are a mathematician.
q: You are clever.
r: You are rich.
The premises are:
p →q and q ∧r.
The conclusion is:
r →p.
We must test whether or not [(p →q) ∧(q ∧r)] →(r →p) is a tautology.
p q r p→q q ∧r (p→q) ∧(q ∧r) r→p [(p→q) ∧(q ∧r)]→(r→p)
T T T
T
T
T
T
T
T T F
T
F
F
T
T
T F T
F
F
F
T
T
T F F
F
F
F
T
T
F T T
T
T
T
F
F
F T F
T
F
F
T
T
F F T
T
F
F
F
T
F F F
T
F
F
T
T
From the last column we see that [(p →q)∧(q∧r)] →(r →p) is not a tautology
and hence the argument is not valid.

28
Logic
Exercise 1.5
Test the validity of the following arguments.
1.
If you gamble you’re stupid.
You’re not stupid therefore you don’t
gamble.
2.
If I leave college then I’ll get a job in a bank. I’m not leaving college so
I won’t get a job in a bank.
3.
James is either a policeman or a footballer. If he’s a policeman then he
has big feet. James hasn’t got big feet so he’s a footballer.
4.
If I could swim I’d come sailing with you. I can’t swim so I’m not coming
sailing with you.
5.
If you ﬁnd this difﬁcult then you’re stupid or you haven’t done your
homework. You’ve done your homework and you’re not stupid therefore
you won’t ﬁnd this difﬁcult.
6.
You can go out if and only if you do the washing up. If you go out
then you won’t watch television. Therefore you either watch television or
wash up but not both.
7.
If I graduate in June then I’ll go on holiday in the summer. In the summer
I’ll get a job or I’ll go on holiday. I won’t go on holiday in the summer
so I won’t graduate in June.
8.
If there are clouds in the sky then the sun doesn’t shine and if the sun
doesn’t shine then the temperature falls. The temperature isn’t falling so
there are no clouds in the sky.
9.
I shall be a lawyer or a banker (but not both). If I become a lawyer then I
shall never be rich. Therefore I shall be rich only if I become a banker.
10.
If you are eligible for admission then you must be under 25 and if you are
not under 25 then you do not qualify for a scholarship. Therefore if you
qualify for a scholarship, you are eligible for admission.

Formal Proof of the Validity of Arguments
29
1.7
Formal Proof of the Validity of Arguments
Whilst a truth table will always establish whether or not an argument is valid, the
method can become tediously lengthy. If there are more than a small number of
premises, the number of columns in the table becomes large. Furthermore, the
number of rows increases exponentially with the number of simple propositions
deﬁned. An argument involving n simple propositions will require 2n rows in its
truth table.
An alternative method of validating an argument consists of constructing a
sequence of propositions starting with the premises.
Propositions may then
be added to the sequence, but only if their truth is guaranteed by the truth of
propositions already included in the list. The object in constructing the sequence
is to add propositions which will ﬁnally justify adding the conclusion of the
argument. When this is achieved, the formal proof of validity is complete. At
this point we have a list of propositions all of which are known to be true given
the truth of the premises. In particular, the truth of the argument’s conclusion is
guaranteed by the truth of the premises and this is, of course, the condition for the
argument to be valid.
How do we recognise which propositions may be added to the list? Clearly, if a
proposition is already included, we may add any other proposition to which it is
logically equivalent. Therefore the list of logical equivalences given in section 1.5
will be useful. We may also use the substitution rule given in §1.5. If a compound
proposition is already in the list, we may add one where part of that proposition
is replaced by a logically equivalent form since this substitution does not alter the
truth value.
A battery of logical equivalences however, will not be sufﬁcient in itself for
us to establish the truth of the argument’s conclusion assuming the truth of the
premises. Remember that, if a proposition P logically implies a proposition Q,
then Q is true whenever P is true. This means that we can add to the list any
proposition which is logically implied by an earlier proposition in the sequence.
We can extend this principle. Recall that the conjunction of propositions P1 ∧
P2 ∧. . . ∧Pn is true only when each of the conjuncts P1, P2, ... Pn is true. Now
if (P1 ∧P2 ∧. . . ∧Pn) ⊢Q, then Q is true whenever P1 ∧P2 ∧. . . ∧Pn is true,
i.e. when P1, P2, . . .and Pn are all true. This means that we can add to our list
any proposition which is logically implied by the conjunction of a set of earlier
propositions in the sequence.
The relationship (P1 ∧P2 ∧. . . ∧Pn) ⊢Q means that we can regard Q as the
conclusion of a valid argument with premises P1, P2, . .. Pn. (This was exactly
our deﬁnition of a valid argument.) Hence a justiﬁcation for adding a proposition

30
Logic
to the sequence is that it is the conclusion of a valid argument whose premises are
already included in the list. It will therefore be useful for us to have a list of valid
‘mini-arguments’. Then, when we spot any substitution instance of the premises
in our list, we know that we can add the corresponding conclusion if we so wish.
The following is a list of useful valid arguments to which we can appeal when
justifying the addition of propositions to the list constituting our formal proof.
These are often referred to as ‘rules of inference’ and there are nine which will
prove sufﬁcient for our needs. We list these rules in the table below.
Rules of Inference
Name of rule
Premises
Conclusion
Simpliﬁcation (Simp)
P ∧Q
P
Addition (Add)
P
P ∨Q
Conjunction (Conj)
P, Q
P ∧Q
Disjunctive syllogism (DS)
P ∨Q, ¬P
Q
Modus ponens (MP)
P, P →Q
Q
Modus tollens (MT)
P →Q, ¯Q
¯P
Hypothetical syllogism (HS)
P →Q, Q →R
P →R
Absorption (Abs)
P →Q
P →(P ∧Q)
Constructive dilemma (CD)
P →Q, R →S, P ∨R
Q ∨S
These arguments are valid no matter what propositions are substituted for P, Q
and R. For instance an argument with premises (p ∧¯q) →(q ∨r) and p ∧¯q and
with conclusion q ∨r is valid because it is a substitution instance of the modus
ponens rule. In that rule we simply substitute p ∧¯q for P and q ∨r for Q. What
these rules give us is, in a sense, ‘patterns’ for certain valid arguments.
There follow some examples to illustrate the construction of a formal proof.
Examples 1.11
1.
Construct a formal proof of the validity of the following argument.
Premises :
p →q, p ∧r
Conclusion :
q

Formal Proof of the Validity of Arguments
31
We commence the sequence, as always, with the premises:
1.
p →q
(premise)
2.
p ∧r
(premise)
Our aim is to be able to justify (in terms of the ‘truth guarantee’ criterion)
the addition of the conclusion to the list. This can be done in two steps.
The Simpliﬁcation rule applied to the second premise allows us to add
p. Now that we have p, the Modus Ponens rule applied to this together
with the ﬁrst premise justiﬁes the addition of q, the conclusion of the
argument. This completes the proof.
We summarise these two steps below.
3.
p
(2. Simp)
4.
q
(1, 3. MP)
Note that the propositions in the sequence are numbered so that they can
be referred to later and also that a justiﬁcation must be provided for the
addition of each proposition to the list.
Putting the steps together, the complete formal proof is the following.
1.
p →q
(premise)
2.
p ∧r
(premise)
3.
p
(2. Simp)
4.
q
(1, 3. MP)
2.
Provide a formal proof of the validity of the following argument:
Premises :
p →¯q, q ∨r
Conclusion :
p →r
Note that, if we could add ¯q →r to our sequence, we could apply the
Hypothetical Syllogism rule to this and the ﬁrst premise and thereby
justify the addition of the conclusion.
But how can we sanction the
addition of ¯q →r? The second premise provides the clue. If we replace
q by ¯¯q (justiﬁed by the Involution law), the second premise becomes
¯¯q ∨r. Now we can use the Material Implication law to justify the logical
equivalence of ¯¯q ∨r and ¯q →r. Then the conclusion p →r follows from
p →¯q and ¯q →r by Hypothetical Syllogism. This completes the proof.
The formal proof is given below.

32
Logic
1.
p →¯q
(premise)
2.
q ∨r
(premise)
3.
¯¯q ∨r
(2. Invol)
4.
¯q →r
(3. Impl)
5.
¯q →r
(2, 3. HS)
3.
Construct a formal proof of the following argument.
Premises :
p →q, r →s, ¯q, r
Conclusion :
¯p ∧s
As with the previous example, we shall often ﬁnd it useful to work
backwards by asking ourselves what needs to be added to the list to justify
adding the conclusion. Here the conclusion is a conjunction. If the list
were to contain each of the conjuncts ¯p and s, then we have a rule of
inference (Conjunction) which will allow us to add ¯p∧s. In this example,
each of the conjuncts follows directly from a rule of inference applied to
a pair of premises. The proof is given below.
1.
p →q
(premise)
2.
r →s
(premise)
3.
¯q
(premise)
4.
r
(premise)
5.
¯p
(1, 3. MT)
6.
s
(2, 4. MP)
7.
¯p ∧s
(5, 6. Conj)
4.
Give a formal proof for the following argument:
Premises :
p ∧(q ∨r), ¯p ∨¯q
Conclusion :
r ∨q
The proof here is a little longer and, again, it helps to work backwards.
The conclusion here is the inclusive disjunction of r and q. Note that
the Addition rule would allow us to add the proposition r ∨q if our list
contained simply the proposition r. So let’s concentrate on how we might
justify the inclusion of r.
Now the only source of the proposition r is the disjunction q∨r in the ﬁrst
premise, so need to extract this out. This can be achieved by applying the
Commutative law (so that q ∨r becomes the ﬁrst of the two conjuncts)
followed by the Simpliﬁcation rule.
Our proof so far is as follows:

Formal Proof of the Validity of Arguments
33
1.
p ∧(q ∨r)
(premise)
2.
¯p ∨¯q
(premise)
3.
(q ∨r) ∧p
(1. Comm)
4.
q ∨r
(3. Simp)
Now, how can we ‘extract’ r from q ∨r? The Disjunctive Syllogism rule
provides the clue—we need ¯q and we can then use this rule to justify the
addition of r. To justify the addition of ¯q, we could use premise 2 along
with Disjunctive Syllogism again. But for this we need ¯¯p or (justiﬁed by
the Involution law) its logically equivalent form p. This can be obtained
from premise 1 using the Simpliﬁcation law.
We now have the rest of the steps necessary to complete the proof. The
full proof is given below.
1.
p ∧(q ∨r)
(premise)
2.
¯p ∨¯q
(premise)
3.
(q ∨r) ∧p
(1. Comm)
4.
q ∨r
(3. Simp)
5.
p
(1. Simp)
6.
¯¯p
(5. Inv)
7.
¯q
(7, 2. DS)
8.
r
(7, 4. DS)
9.
r ∨s
(8. Add)
There are often several alternative formal proofs of a valid argument. For
instance, the following is an alternative to the formal proof above.
1.
p ∧(q ∨r)q
(premise)
2.
¯p ∨¯q
(premise)
3.
(p ∧q) ∨(p ∧r)
(1. Dist)
4.
p ∧q
(2. De M)
5.
p ∧r
(3. Simp)
6.
r ∧p
(5. Comm)
7.
r
(6. Simp)
8.
r ∨s
(8. Add)
Remember that, using a truth table, we were able to establish the validity
or invalidity of an argument. Our method of formal proof is really only
useful for showing that an argument is valid. If we are unable to ﬁnd a
formal proof of the validity of an argument we cannot be sure whether
this is because no such proof exists (because the argument is not valid) or
because we have simply failed to ﬁnd a proof for an argument which is,
in fact, valid.

34
Logic
Exercise 1.6
1.
Provide a formal proof for each of the following arguments.
(i)
Premises:
(p ∧q) →(r ∧s), p, q
Conclusion:
s
(ii)
Premises:
p →¯q, q ∨r
Conclusion:
p →r
(iii)
Premises:
p →¯q, p ∧r, q ∨r
Conclusion:
r
(iv)
Premises:
p ∨q, r →¯q, ¯p, (¯r ∧q) →s
Conclusion:
s
(v)
Premises:
(p →q) ∧(¯p →¯r), s ∧p
Conclusion:
q
(vi)
Premises:
(p ∨q) →(r ∧s), r →t, ¯t
Conclusion:
¯p
(vii)
Premises:
(p ∨q) ∧(q ∨r), ¯q
Conclusion:
p ∧r
2.
Symbolise the following arguments and provide a formal proof of the
validity of each one.
(i)
The murder was committed by A or by both B and C. Therefore
the murder was committed by A or B.
(ii)
I won’t play golf only if I go swimming. I won’t go swimming.
Therefore I’ll play golf.
(iii)
If Ed goes to the party then I won’t go. Either I’ll go to the party
or I’ll stay home and watch TV. Therefore if Ed goes to the party
then I’ll stay home and watch TV.
(iv)
If the summer is hot or wet then my garden ﬂourishes. The summer
is hot and windy. Therefore my garden ﬂourishes.
(v)
People are happy if and only if they are charitable. Nobody is both
happy and charitable. Hence people are unhappy and uncharitable.

Predicate Logic
35
(vi)
If you go to college or get a good job then you will be successful
and respected. You go to college. Therefore you will be respected.
(vii)
If I eat cheese I get sick and if I drink wine I get sick. If I go to
Ira’s I eat cheese or I drink wine. I’m going to Ira’s. Therefore I
shall get sick.
1.8
Predicate Logic
Consider the following argument: ‘Everyone who has green eyes is not to be
trusted. Bill has green eyes. Therefore Bill is not to be trusted.’ Expressing this
in our propositional notation would give us an argument with premises p and q
and a conclusion r. Our notation gives us no means of showing that different
propositions are making statements about the same thing.
Two propositions
as similar as ‘Bill has green eyes’ and ‘Jeff has green eyes’ would have to be
symbolized by p and q. We have as yet no means of expressing the fact that both
propositions are about ‘green eyes’.
A predicate describes a property of one or several objects or individuals.
Examples of predicates might be:
(a)
...is red.
(b)
...has long teeth.
(c)
...enjoys standing on his head.
(d)
...has spiky leaves.
(e)
...cannot be tolerated under any circumstances.
The space in front of these predicates can be ﬁlled in with the names of objects
or individuals where appropriate to form a proposition which may be true or
false in the usual way. For instance (a) could be preﬁxed by ‘that door’, ‘this
ﬂower’, ‘your nose’ or any other object. Propositions of this kind consist of a
subject together with a predicate describing whatever property the subject is said
to possess.
We shall symbolize these propositions in a different way from before so as to
distinguish their two component parts. We shall use capital letters to refer to
predicates, so that we might deﬁne:
R : is red.
T : has long teeth.

36
Logic
H : enjoys standing on his head.
Lower-case letters will be used to denote particular objects or individuals. For
instance:
r : this rose.
j : James.
We can then form simple propositions as follows:
R(r) : This rose is red.
R(j) : James is red.
H(j) : James enjoys standing on his head.
Notice that the attribute symbol is written to the left of the symbol representing
the particular object or individual. If R is the predicate ‘is red’, we can write R(x)
to denote ‘x is red’ where x can be replaced by any object or individual. Note that
R(x) is not itself a proposition since it cannot be declared true or false. However,
it becomes a proposition once x is replaced by a particular object or individual.
The letter x is a variable which serves as a place marker to indicate where we
may substitute the names of objects or individuals in order to form propositions.
For this reason, R(x) is called a propositional function.
We can negate propositional functions.
If R(x) denotes ‘x is red’ then the
negation of R(x), denoted by ¬R(x) (or R(x)), is the propositional function
interpreted as ‘x is not red’.
Substituting a particular ‘value’ for x in a propositional function is not the only
way of converting it to a proposition. This can also be achieved through the use
of quantiﬁers.
The Universal Quantiﬁer
Consider the proposition ‘All rats are grey’.
One way in which we could
paraphrase this proposition is: ‘For every x, if x is a rat, then x is grey’. This gives
us a way of symbolizing the proposition using the predicate symbols described
earlier. Suppose we deﬁne:
R(x) : x is a rat.
G(x) : x is grey.

Predicate Logic
37
We denote ‘for every x’ by ∀x and we can then write ‘All rats are grey’ as:
∀x[R(x) →G(x)].
The symbol ∀is called the universal quantiﬁer. The quantiﬁed variable ∀x is
read as ‘for all x’ or ‘for every x’.
Example 1.12
Symbolize the proposition ‘Every day I go jogging’.
Solution
Deﬁne the following:
D(x) : x is a day.
J(x) : x is when I go jogging.
Then ‘Every day I go jogging’ can be paraphrased ‘For every x, if x is a day, then
x is when I go jogging’. We can express this proposition symbolically by:
∀x[D(x) →J(x)].
The Existential Quantiﬁer
Consider the proposition ‘Some rats are grey’. Here we assert that there is at least
one rat which is grey. We could paraphrase this proposition as ‘There exists at
least one x such that x is a rat and x is grey’. Thus if we deﬁne:
R(x) : x is a rat
G(x) : x is grey
and denote ‘there exists at least one x’ by ∃x, then ‘Some rats are grey’ can be
written:
∃x[R(x) ∧G(x)].
The symbol ∃is called the existential quantiﬁer and ∃x is read as ‘there exists
at least one x’ or ‘for some x’.
Example 1.13
1.
Symbolize ‘Some people think of no one but themselves’.

38
Logic
Solution
Deﬁne:
P(x) : x is a person
N(x) : x thinks of no one but himself.
Then ‘Some people think of no one but themselves’ can be written:
∃x[P(x) ∧N(x)].
2.
Symbolize ‘Some of the children didn’t apologize’.
Solution
Deﬁne:
C(x) : x is a child
A(x) : x apologized.
Then ‘Some of the children didn’t apologize’ can be written using the negation of
A(x) thus:
∃x[C(x) ∧¬A(x)].
3.
Symbolize the proposition ‘Nobody likes cheats’.
Solution
Deﬁne:
P(x) : x is a person
C(x) : x likes cheats.
What we want to say here is that there does not exist an x where x is a person and
x likes cheats. We can symbolize this by negating the existential quantiﬁer thus:
¬∃x[P(x) ∧C(x)].
Note that we can use the connectives ∧, ∨, →, etc between the propositional
functions P(x), C(x) even though these are not propositions. Thus if we deﬁne:
P(x) : x is a person
C(x) : x cheats
T(x) : x talks loudly
then the expression
∀x[P(x) →{T(x) ∧C(x)}]

Predicate Logic
39
symbolizes ‘Everybody cheats and talks loudly’.
In example 1.13.2 we symbolized the proposition ‘Some of the children didn’t
apologize’ as ∃x[C(x) ∧¬A(x)]. There is a sense in which this proposition
seems to refer to some particular group of children which the speaker has in mind
rather than children in general. The predicate ‘is a child’ in this example seems
to mean ‘is a member of a particular group of children’. This particular group of
children is called the universe of discourse and we can consider the variable x to
be restricted to members of this set.
If we deﬁne the universe of discourse carefully, we can shorten the proposition
∃x[C(x) ∧¬A(x)] to the simple form ∃x[¬A(x)] where it is understood that x
belongs to the particular group of children that the speaker has in mind. The
expression ∃x[¬A(x)] then states that, within this universe of discourse, at least
one x exists who didn’t apologize.
When the universe of discourse is not speciﬁed it is assumed to be the complete
universe of objects or individuals referred to in the proposition. ‘All rats are grey’
is assumed to be a statement about all rats in the universe unless the context makes
it clear that some subset of these is intended.
In determining the truth value of quantiﬁed propositions, it is important that we
are clear about the universe of discourse. For instance, the proposition ‘Some of
the children didn’t apologize’ may be true in one universe of discourse but false
in another.
Example 1.14
Deﬁne the following:
F(x) : x is greater than ﬁve
E(x) : x is an even number
N(x) : x is negative.
Consider the following universes of discourse:
(i)
integers (i.e. whole numbers)
(ii)
real numbers
(iii)
negative integers.
Determine the truth values of each of the following propositions in each universe
of discourse.

40
Logic
(a)
∃xF(x)
(b)
∀xN(x)
(c)
∀x[F(x) ∧E(x)]
(d)
∃x[¬N(x)].
Solution
(a)
This proposition states that there exists an x which is greater than ﬁve.
This is true for the universe of integers and for the universe of real
numbers. It is false if x is restricted to negative integers.
(b)
The proposition here is ‘For every x, x is negative’. This is false for
integers and for real numbers but it is true for the universe of negative
integers.
(c)
Here we have ‘For every x, x is greater than ﬁve and even’. This is false
in all three universes.
(d)
This proposition is ‘There exists an x which is not negative’. This is true
for integers and for real numbers but is false for negative integers.
Two-Place Predicates
Consider the predicate ‘is heavier than’. In order to convert this predicate into a
proposition, the names of two objects or individuals are necessary. For instance,
using this predicate, we may form the proposition ‘A brick is heavier than a
hamster’. The predicate ‘is heavier than’ is an example of a two-place predicate.
If H denotes this predicate, then H(x, y) denotes the propositional function ‘x is
heavier than y’.
Two-place predicates can be quantiﬁed using the universal and existential
quantiﬁers. However, two quantiﬁers are necessary to produce a proposition from
a two-variable propositional function. The quantiﬁed expressions ∀xF(x, y) and
∃xF(x, y) are not propositions but propositional functions of the single variable
y.
Suppose we have:
P(x, y) : x + y = 7
where the universe of discourse for each variable is the real numbers.
The
following propositions are possible:

Predicate Logic
41
1.
∀x ∃yP(x, y)
2.
∃y ∀xP(x, y)
3.
∀y ∃xP(x, y)
4.
∃x ∀yP(x, y)
5.
∀y ∀xP(x, y)
6.
∀x ∀yP(x, y)
7.
∃y ∃xP(x, y)
8.
∃x ∃yP(x, y).
Note that the propositions are read from left to right and that the order of
quantiﬁed variables is important. Consider for instance propositions 1 and 2. The
ﬁrst states that, for every x, there exists at least one y such that, x + y = 7. This
is clearly true. On the other hand, proposition 2 states that there exists at least one
y such that, for every x, x + y = 7. This is not true since a single y value cannot
be found for every x. Each value of x needs a different value of y to balance the
equation x + y = 7. Thus the propositions ∀x ∃yP(x, y) and ∃y ∀xP(x, y) are
not equivalent statements. For similar reasons, propositions 3 and 4 are also not
equivalent.
The propositions ∀x ∀yF(x, y) and ∀y ∀xF(x, y) are equivalent for any
propositional function F(x, y), i.e. they have identical truth values.
In
the example above, 5 and 6 are equivalent (false) propositions.
Similarly
∃x ∃yF(x, y) and ∃y ∃xF(x, y) are equivalent propositions for any propositional
function F(x, y). Hence 7 and 8 are equivalent (true) propositions.
Negation of Quantiﬁed Propositional Functions
The proposition ∀xF(x) states that, for all x in the universe of discourse, x
has the property deﬁned by the predicate F. The negation of this proposition,
¬∀xF(x), states that ‘It is not the case that all x have the property deﬁned by F’,
i.e. there is at least one x that does not have the property F. This is symbolized by
∃x[¬F(x)]. So, for any propositional function F(x), the propositions ¬∀xF(x)
and ∃x[¬F(x)] have the same truth values and are therefore equivalent, i.e.
¬∀xF(x) ≡∃x[¬F(x)].
Similarly, the negation of ∃xF(x), symbolized by ¬∃xF(x), states that there
does not exist an x within the universe of discourse that has the property deﬁned
by F. This is the same as saying that, for all x, x does not have the property F,
i.e. ∀x[¬F(x)]. Thus we have
¬∃xF(x) ≡∀x[¬F(x)]
for all propositional functions F(x).
These two equivalences are known as the Rules of Quantiﬁcation Denial
(abbreviated as QD). The rules are summarised below.

42
Logic
Rules of Quantiﬁcation Denial
Where a universe of discourse is deﬁned for the variable x, then for any
propositional function F(x):
¬∀xF(x) ≡∃x[¬F(x)]
and
¬∃xF(x) ≡∀x[¬F(x)].
We can also show that
¬∃x[¬F(x)] ≡∀xF(x)
since
¬∃x[¬F(x)] ≡∀x[¬¬F(x)]
(by the second result above)
≡∀xF(x)
(by the involution law).
Similarly we can show that
¬∀x[¬F(x)] ≡∃xF(x).
For doubly quantiﬁed propositional functions, equivalences can be established by
repeated applications of the rules above. For instance:
¬∃y ∀xP(x, y) ≡∀y[¬∀xP(x, y)]
≡∀y ∃x[¬P(x, y)].
The negation of other similar propositions can be obtained in the same way.
Example 1.15
We deﬁne the following on the universe of men.
M(x) : x is mortal.
C(x) : x lives in the city.
Symbolize the negations of the following propositions changing the quantiﬁer.
(i)
All men are immortal.
(ii)
Some men live in the city.

Predicate Logic
43
Solution
(i)
The proposition given can be symbolized by ∀x[¬M(x)]. The negation
of this proposition is given by
¬∀x[¬M(x)] ≡∃xM(x).
The resulting proposition is ‘Some men are mortal’.
(ii)
‘Some men live in the city’ is symbolized by ∃xC(x). Its negation is
¬∃xC(x) ≡∀x[¬C(x)].
That is, ‘All men live out of the city’.
Exercises 1.7
1.
Suppose the following predicates and individuals are deﬁned:
m : Maria
s : Maria’s son
C : works in the city
B : rides a bicycle
F : is a chicken farmer.
Symbolize the following:
(i)
Maria works in the city and her son is a chicken farmer.
(ii)
If Maria rides a bicycle then her son works in the city.
(iii)
Maria works in the city and rides a bicycle but her son is not a
chicken farmer.
(iv)
Everyone who works in the city is a chicken farmer.
(v)
Everyone who works in the city and doesn’t ride a bicycle is a
chicken farmer.
(vi)
Some people who work in the city and ride a bicycle are not
chicken farmers.
(vii)
If no-one working in the city rides a bicycle then Maria doesn’t
work in the city and her son is not a chicken farmer.
(viii) No chicken farmers work in the city and ride a bicycle.
2.
Translate the following into symbolic form using one-place predicates.
Deﬁne predicates used and, where necessary, deﬁne the universe of
discourse.

44
Logic
(i)
All babies cry a lot.
(ii)
Nobody can ignore him.
(iii)
Some students can’t write a good essay.
(iv)
Not everybody approves of capital punishment.
(v)
There are people who have had a university education and live in
poverty.
(vi)
Every time it rains I forget my umbrella.
(vii)
All of my friends believe in nuclear disarmament.
(viii) All Fred’s children are rude or stupid.
(ix)
Somebody set off the ﬁre alarm and everybody left the building.
(x)
Not all rats are dirty and carry disease.
(xi)
Everybody who doesn’t like snails has no taste.
(xii)
Some toys are dangerous and no child should be given them.
3.
Translate the following into symbolic form using two-place predicates.
(i)
Everybody loves somebody.
(ii)
Somebody loves everybody.
(iii)
Everyone is taller than Sam.
(iv)
All elephants love buns.
4.
Negate each of the following propositions and use the Rules of
Quantiﬁcation Denial to change the quantiﬁer. Express the result as a
reasonable English sentence.
(i)
Everybody likes strawberry jam.
(ii)
There are birds that cannot ﬂy.
(iii)
Sometimes I think you are lazy.
(iv)
Nobody leaves without my permission.
5.
Consider the following predicates:
P(x, y) : x > y
Q(x, y) : x ⩽y
R(x) : x −7 = 2
S(x) : x > 9.
If the universe of discourse is the real numbers, give the truth value of
each of the following propositions:
(i)
∃xR(x)
(ii)
∀y[¬S(y)]
(iii)
∀x ∃yP(x, y)
(iv)
∃y ∀xQ(x, y)

Arguments in Predicate Logic
45
(v)
∀x ∀y[P(x, y) ∨Q(x, y)]
(vi)
∃xS(x) ∧¬∀xR(x)
(vii)
∃y ∀x[S(y) ∧Q(x, y)]
(viii) ∀x ∀y[{R(x) ∧S(y)} →Q(x, y)].
1.9
Arguments in Predicate Logic
We return to the argument at the beginning of §1.8: ‘Everyone who has green
eyes is not to be trusted. Bill has green eyes. Therefore Bill is not to be trusted.’
If we deﬁne the following on the universe of all human beings:
G(x) : x has green eyes
T(x) : x can be trusted
b : Bill
then the premises of this argument are:
∀x[G(x) →¬T(x)]
and
G(b)
and the conclusion is:
¬T(b).
Remember that to establish the validity of an argument, we must show that,
whenever all the premises are true, then the conclusion must be true. As before,
we shall do this in steps. Assuming the premises to be true will allow us to
deduce other true propositions which in turn allow us to guarantee the truth of
the conclusion. We shall once again use our Replacement Laws and Rules of
Inference (see §1.5 and §1.7). However, with arguments expressed in predicate
logic, we shall need to use the truth of quantiﬁed propositions to draw conclusions
about the truth of propositions relating to speciﬁc members of the universe of
discourse, and vice versa. We therefore need the following four rules which will
allow us to do this.
1. Universal Speciﬁcation (US)
This rule states that if the proposition ∀xF(x) is true, then we can deduce that the
proposition F(a) is true for every a in the universe of discourse.

46
Logic
2. Universal Generalization (UG)
If the proposition F(a) is true for every a in the universe of discourse, then we
can conclude that ∀xF(x) is true.
3. Existential Speciﬁcation (ES)
If ∃xF(x) is true, then there is an element a in the universe of discourse such that
F(a) is true.
We must be very careful in interpreting this rule. The element a is not arbitrary.
It is one of the elements in the universe which has the property F. That at least
one such element exists is guaranteed by the truth of ∃xF(x).
4. Existential Generalization (EG)
If F(a) is true for some element a belonging to the universe of discourse then
∃xF(x) is true.
Examples 1.16
1.
Show that the following is a valid argument: ‘Everyone who has green
eyes is not to be trusted. Bill has green eyes. Therefore Bill is not to be
trusted.’
Solution
With a universe of discourse of ‘people’, we have established that, if b denotes
‘Bill’, the premises are:
∀x[G(x) →¬T(x)]
and
G(b)
and the conclusion is:
¬T(b).
Assuming the truth of the premises, we must establish the truth of the conclusion.
We do this as follows:
1.
∀x[G(x) →¬T(x)]
(premise)
2.
G(b)
(premise)
3.
G(b) →¬T(b)
(1. US)
4.
¬T(b)
(2, 3. MP)

Arguments in Predicate Logic
47
The truth of each of the propositions 1–4 is guaranteed for the reason given. We
have shown that the truth of the premises guarantees the truth of the conclusion
and hence that the argument is valid.
2.
Show that the following is a valid argument: ‘All students go to parties.
Some students drink too much. Therefore some people who drink too
much go to parties.’
Solution
Once again, we take our universe of discourse as ‘people’.
Deﬁne:
S(x) : x is a student
D(x) : x drinks too much
P(x) : x goes to parties.
The premises are:
∀x[S(x) →P(x)]
and
∃x[S(x) ∧D(x)]
and the conclusion is:
∃x[D(x) ∧P(x)].
We proceed as follows:
1.
∃x[S(x) ∧D(x)]
(premise)
2.
∀x[S(x) →P(x)]
(premise)
3.
S(a) ∧D(a)
(1. ES)
4.
S(a) →P(a)
(2. US)
5.
S(a)
(3. Simp)
6.
P(a)
(4, 5. MP)
7.
D(a) ∧S(a)
(3. Comm)
8.
D(a)
(7. Simp)
9.
D(a) ∧P(a)
(6, 8. Conj)
10.
∃x[D(x) ∧P(x)]
(9. EG)
Note that the a in line 2 is not arbitrary but is an element in the universe which
has the properties deﬁned by S and D. The a in line 3 is the same individual for
whom we can state S(a) →P(a) because we have ∀x[S(x) →P(x)], where
S(x) →P(x) holds for all x in the universe of discourse and hence for a.
3.
Show that the following is a valid argument: ‘Everyone shouts or cries.
Not everyone cries. So some people shout and don’t cry.’

48
Logic
Solution
With our universe of discourse as ‘people’, we deﬁne the following:
S(x) : x shouts
C(x) : x cries.
The premises of the argument are: ∀x[S(x) ∨C(x)] and ¬∀xC(x).
Note that we cannot apply either rule of speciﬁcation to the second premise in its
current negated form. We therefore use the Rule of Quantiﬁcation Denial to write
it in the logically equivalent form: ∃x¬C(x).
We can now validate the argument as follows.
1.
∀x[S(x) ∨C(x)]
(premise)
2.
¬∀xC(x)
(premise)
3.
∃x¬C(x)
(2. QD)
4.
¬C(a)
(3. ES)
5.
S(a) ∨C(a)
(1. US)
6.
C(a) ∨S(a)
(5. Comm)
7.
S(a)
(4, 5. DS)
8.
S(a) ∧¬C(a)
(7, 4. Conj)
9.
∃x[S(x) ∧¬C(x)]
(8. EG)
Note that it was necessary to use the rule of existential speciﬁcation on the second
premise before using universal speciﬁcation on the ﬁrst. This is because if we
ﬁrst state S(a) ∨C(a), a is an arbitrary member of the universe. But the property
¬C(x) applies only to certain individuals in the universe, so we cannot assume
that it applies to an arbitrary individual a. In other words, an a for which ¬C(a)
is true (and the premise asserts that there is at least one such a) must also be one
for which S(a) ∨C(a) is true, since the latter property is true for any individual
in the universe.
Exercises 1.8
Establish the validity of the following arguments.
1.
Some monkeys eat bananas. All monkeys are primates. Therefore some
primates eat bananas.
2.
All cars are dangerous weapons. No dangerous weapons should be given
to children. Therefore cars should not be given to children.

Arguments in Predicate Logic
49
3.
No reasonable man approves of wars. Jack approves of wars. Therefore
Jack is not a reasonable man.
4.
All gamblers are bound for ruin.
No one bound for ruin is happy.
Therefore no gamblers are happy.
5.
All computer scientists are clever or wealthy. No computer scientist is
wealthy. Therefore all computer scientists are clever or witty.
6.
All those who eat apples have strong teeth. All those who don’t eat apples
are unhealthy. Betty hasn’t strong teeth. Therefore Betty is unhealthy.
7.
Some alligators are friendly and sociable.
All alligators which are
friendly live in a zoo. Therefore some alligators which live in a zoo are
sociable.
8.
All problems are difﬁcult and frustrating.
Some problems are
challenging. Hence some problems are frustrating and challenging.
9.
All animals with scales are dragons. Some animals which are not dragons
have sharp claws. So there are animals without scales which have sharp
claws.
10.
Everyone who is forty is fat or foolish. No one is foolish and no one is
fat. So no one is forty.

Chapter 2
Mathematical Proof
2.1
The Nature of Proof
The discipline of mathematics is characterized by the concept of proof. In this
chapter we consider the nature of mathematical proof, some of the different
techniques of proof and how a proof should be constructed and written down.
What do mathematicians mean by ‘proof’? The popular view of a mathematical
proof is probably that of a sequence of steps, almost certainly written mainly in
symbols, where each step follows logically from an earlier part of the proof and
where the last line is the statement being proved. Associated with this image is
probably the notion that a proof is the absolute and rigorous test of mathematical
truth. Surprisingly perhaps, this is not quite the view of many mathematicians,
although there is by no means unanimity of opinion amongst the mathematical
professionals themselves. Many hold a more sociological view of the role of a
proof. They see it as essentially an explanation and communication of ideas; a
line of argument sufﬁcient to convince a fellow mathematician of the validity of
the particular result. As the great English mathematician Godfrey Hardy wrote:
‘Strictly speaking there is no such thing as mathematical proof; ... [they are]
rhetorical ﬂourishes designed to affect psychology, ... devices to stimulate the
imagination of students.’
Which, then, is the ‘correct’ view of the nature and signiﬁcance of the proof of
a mathematical theorem? Probably the best answer is: both! The word ‘proof’
is used to cover a wide spectrum of styles. At one extreme we have very formal
proofs which are rather like the logical arguments considered in chapter 1. Each
step follows from the premises or a previous step by the laws of logic. Indeed, it is
possible to write out such a proof using only symbols and no words but, needless
50

Axioms and Axiom Systems
51
to say, this is likely to be very difﬁcult to follow. Away from the formal end of
the spectrum are proofs which are more ‘reader-friendly’. A less formal proof
may use a mixture of words, symbols and diagrams of one kind or another. Most
proofs found in mathematical textbooks (and research papers, for that matter) are
not formal. They aim to communicate the essential reasons why a particular result
holds rather than dwelling on rigorous step-by-step detail.
Any proof exposes certain lines of reasoning to scrutiny by others.
As such
the mathematical community sets certain standards concerning what should be
regarded as an acceptable proof and what should not. Vague descriptions are not
allowed. Arguments which are clear and coherent, although somewhat informal,
are acceptable even if they gloss over some minor details. It goes without saying
that any proof must be ‘correct’ in that it must not contain any logical errors.
Something which is not sanctioned in mathematics is the drawing of conclusions
based on large numbers of observations. However many times we square an even
number and discover that the result is even, this does not constitute a proof that
the square of an even number is even. It may, however, strengthen our belief that
this is so and encourage us to search for a valid proof. Making judgements about
facts on the basis of observation is known as inductive reasoning. The type of
reasoning where a conclusion is drawn by logical inference is called deductive
reasoning. For mathematicians, the latter is the only form of reasoning which is
acceptable in a proof.
2.2
Axioms and Axiom Systems
To understand more fully what is meant by a proof, formal or informal, we
need to look brieﬂy at the nature of modern mathematics. Most mathematicians
would agree that their subject has as its mode of operation what is known as the
‘axiomatic method’. The use of the axiomatic method was introduced by Euclid
in about 300 BC (although the modern view of the nature of axioms differs in
important ways from Euclid’s).
A mathematical theory, such as set theory, number theory, group theory or
whatever, consists of various components of which the most important are the
following:
1.
Undeﬁned terms.
2.
Axioms.
3.
Deﬁnitions.
4.
Theorems.
5.
Proofs.

52
Mathematical Proof
Of these, you probably have a reasonable idea of what we mean by 3, 4 and 5.
That we need to have undeﬁned terms in mathematics may come as a surprise, but
a little reﬂection should indicate why these are necessary.
Suppose we wish to write the deﬁnitive work on, say, set theory. Where do we
begin? The obvious starting point is to say precisely what a set is, so we begin:
Deﬁnition 1: A set is . .. —what? The problem is that, if we attempt to deﬁne
‘set’, we need to do so in terms of something else (a collection, perhaps?), but
now the ‘something else’ is undeﬁned. If we try to deﬁne the something else, we
have to do so in terms of something else again, but then the ‘something else again’
is undeﬁned, and so on. Clearly, we want to avoid an inﬁnite string of deﬁnitions
(otherwise we could never begin the theory proper) or circularity in our deﬁnitions
(‘a set is a collection; a collection is a set’). This forces us to have some terms
which are left undeﬁned. Of course, we can still explain in an intuitive way what
we have in mind when using the undeﬁned terms, but this intuitive explanation is
not strictly part of the theory itself.
Item 2 in our list above—axioms—also needs some clariﬁcation. Just as we
cannot deﬁne every term which is to be used in a mathematical theory, so we
cannot prove every statement about the theory for much the same reason. In order
to have somewhere to begin, we need to make some statements which will not be
proved. These statements are called axioms. They represent, in a sense, the basic
properties of the theory, its ‘building blocks’.
Note that the truth or falsity of the axioms is not considered; they are merely
statements about the undeﬁned terms which serve to ‘get the theory going’†.
However, they must be consistent amongst themselves in the sense that it must
be possible for them all to be true simultaneously. Axioms which contradict
each other are not acceptable.
When it comes to applying a mathematical
theory, the undeﬁned terms are given interpretations and the axioms then become
propositions which are either true or false. Of course, a mathematical theory can
only sensibly be applied if the interpretations of the axioms to the situation under
consideration are true propositions.
An axiomatic theory develops by making deﬁnitions and proving theorems.
Deﬁnitions are introduced for the convenience of not having to refer everything
back to undeﬁned terms.
A theorem is a statement about various terms of
the system which follows from the axioms using the kind of logical reasoning
introduced in chapter 1. The ﬁrst theorems are proved directly from the axioms;
† As assumptions about undeﬁned terms, the axioms have neither meaning nor truth.
Because
mathematics is built from these foundations, it too has no meaning! It was this consequence of the
axiomatic method that Bertrand Russell had in mind when he wrote: ‘Mathematics may be deﬁned
as the subject in which we never know what we are talking about nor whether what we are saying is
true.’

Axioms and Axiom Systems
53
more theorems are then proved using these and so on. The theory spreads out
further and further ‘away from’ the original axioms, but ultimately rests solely on
them. Theorems and their proofs form the heart of (pure) mathematics.
The axiomatic description of mathematics does seem to imply that the subject is
somewhat mechanical. For instance, it should be possible to program a computer
with a system of axioms and the rules of logic and then set it off proving theorems.
Why, then, has this not been successfully achieved? The missing ingredient (in
both the computer and axiomatic description of mathematics) is human intuition.
Usually a theorem originates in a conjecture—a belief that a certain result holds.
Such a belief may arise from the observation of many situations where this was
so and none where it was not. On the other hand, many important mathematical
conjectures were just ‘hunches’—intuitive beliefs that such-and-such must be the
case. However it arose, for a conjecture to be promoted to a theorem, a proof
must be supplied in which a justiﬁcation for the conjecture is given. Here again
intuition plays an important part in indicating which line of reasoning might lead
to a proof. So although the axiomatic method gives a coherent explanation of
what mathematics is on a formal level, it does not describe or explain at all the
process of doing mathematics. Perhaps only psychologists can hope to do that!
To illustrate these general ideas, consider the following example of an axiom
system. The example is not one which would be of very much interest for two
reasons. Firstly, the axioms are not sufﬁciently ‘rich’ for us to be able to prove
anything very interesting about the system, and secondly, the system does not have
many worthwhile applications. In other words, the example is neither particularly
interesting in its own right, nor in terms of its applicability. However, we hope
that it will serve to clarify the preceding remarks.
Example 2.1
Undeﬁned terms: ‘blub’, ‘glug’ and ‘to lie on’.
Axioms:
A1. Every blub lies on at least one glug.
A2. For every glug, there are exactly two blubs which lie on it.
A3. There are exactly ﬁve blubs.
Figure 2.1 gives an interpretation of the axiom system with blubs represented as
points and glugs as lines, with the obvious interpretation of ‘to lie on’. Note
that, in this interpretation, each of the axioms is a true proposition. A speciﬁc
interpretation of the undeﬁned terms such that the axioms are true propositions is
called a model of the axiom system.
In this interpretation there are ﬁve glugs, G1, G2, . . . , G5. Is this always the case?

54
Mathematical Proof
Figure 2.1
In other words, given any interpretation of the axioms, are there always ﬁve glugs?
If we prove, from the axioms, that there are exactly ﬁve glugs then the answer
must be ‘yes’. However, the answer is in fact ‘no’—ﬁgure 2.2 gives an alternative
model which has 10 glugs.
Figure 2.2
We can, however, prove from the axioms that there are at least three glugs. This
means that any model of the system must have at least three glugs.
Theorem
There exist at least three glugs.
Proof
Let B1 be a blub. (Axiom A3 guarantees the existence of a blub.) By axiom A1,
B1 lies on some glug, G1 say, and, by A2, there is another blub, B2 say, which
also lies on G1.
There is another blub B3 which is different from B1 and B2 (A3) and B3 lies
on some glug G2 (A1). G2 must be different from G1 because G1 cannot have
three blubs lying on it (A2). Axiom A2 tells us that there is another blub lying on

Methods of Proof
55
G2. There are two possibilities: the other blub of G2 is either B1 or B2, or it is
different from B1 and B2.
In the ﬁrst case, there are still two blubs not lying on a glug. In the second case,
there is another blub B4 lying on G2 which still leaves one blub ‘glugless’. In
either case there is at least one blub B5 which does not lie on either G1 or G2.
Axiom A1 tells us that there must be a third glug on which B5 lies. Furthermore
this glug must be different from G1 and G2 by axiom A2.
Therefore there are at least three glugs.
□
Having proved our ﬁrst theorem about blubs and glugs, we could go on and use
it to prove further theorems—see exercise 2.1.1. Since the ‘blub–glug axiom
system’ is too restrictive to be of much interest, we shall not dwell on it further
here. However, blubs and glugs will reappear in chapter 10 ‘disguised’ as the
vertices and edges of graphs. (See exercise 10.1.18 for another model of this
axiom system.)
Exercises 2.1
These questions refer to the ‘blub–glug axiom system’ described in example 2.1.
1.
Prove that there exists a blub which lies on (at least) two different glugs.
(Hint: the theorem of example 2.1 may be of use here.)
2.
Give a model of the axiom system which has more than 10 glugs.
Introduce a new axiom to the system which rules out your model. Can
you prove from the axioms of the new system that there are at most 10
glugs? (Avoid, if you can, a new axiom which simply says that there are
at most 10 glugs.)
2.3
Methods of Proof
As we have seen, formal mathematics is based on the axiomatic method.
Beginning with undeﬁned terms and axioms, it develops by proving theorems
using the rules of logic. In this section we consider the essential features of a
proof and we outline some methods of proof.

56
Mathematical Proof
To set the scene more precisely, suppose that we are given a system of axioms,
A1, A2, . . . , An. A theorem is a statement about the terms of the system which
is logically implied by the conjunction of the axioms. We can therefore deﬁne a
theorem in the system formally as a proposition T such that
(A1 ∧A2 ∧· · · ∧An) ⊢T.
Recall that P ⊢Q if Q is true whenever P is true. In any model of the axiom
system, the axioms have interpretations which are true propositions so that every
theorem has an interpretation which is a true proposition. Thus theorems are
propositions which are true in every model of the axiom system.
What then constitutes a proof of a theorem?
Informally, a proof is a valid
argument in which the theorem is the conclusion. The premises may be axioms or
other theorems which have already been proved. Although it must be possible to
prove any theorem with only the axioms as premises, this is clearly uneconomical.
Once a theorem has been proved, it can be used in conjunction with the axioms to
prove other theorems. Hence to prove theorem T we must show that
(A1 ∧A2 ∧· · · ∧An ∧T1 ∧T2 ∧· · · ∧Tm) ⊢T
where the Ai (i = 1, 2, . . . , n) are axioms and the Tj (j = 1, 2, . . . , m) are
theorems which have already been proved. We do this by assuming the truth of
the axioms (and hence of the theorems) and showing that this guarantees the truth
of T.
Many theorems are, strictly speaking, quantiﬁed propositional functions of the
form ∀xT(x), where x is a member of a speciﬁed universe of discourse. To
prove such a theorem, we in fact prove the proposition which is the universal
speciﬁcation of ∀xT(x), i.e. T(a) for every a in the universe of discourse. Having
shown that the truth of T(a) follows from the axioms and theorems for any
arbitrary a in the universe of discourse, we can then apply universal generalization
and conclude that ∀xT(x) is also true (see §1.9).
Before outlining some techniques of proof, there is a piece of notation which
needs clarifying. The symbol ⇒, read as ‘implies that’, is used between two
propositions where the second ‘follows logically’ from the ﬁrst. (If P ⇒Q
and also Q ⇒P, we write P ⇔Q.) What we mean by P ⇒Q is that Q is
logically implied by the conjunction of P and other statements about the terms of
the system, such as axioms and theorems. Hence P ⇒Q is just shorthand for
(A1 ∧A2 ∧· · · ∧An ∧T1 ∧T2 ∧· · · ∧Tm ∧P) ⊢Q, where the Ai and Tj are
axioms and proved theorems respectively. In a proof, these axioms and theorems
may not be referred to explicitly when it can be assumed that those to whom the
proof is addressed have some background knowledge of the system. For example,
we can write: if x is an arbitrary real number, x2 −2 < 2 ⇒−2 < x < 2. Note

Methods of Proof
57
that the truth of the second proposition is not a direct logical consequence of the
truth of x2 −2 < 2 alone. It is also dependent on certain axioms and theorems of
the real numbers, such as: for all real numbers a, b, c, if a < b, then a+c < b+c.
Where the real numbers are concerned, many properties reﬂected in axioms and
theorems are so familiar that we apply them without thought. In a proof, the
statement P ⇒Q conveys to the reader that the truth of Q follows from the truth
of P and the conjunction of other true propositions with which it is assumed he
or she is familiar.
Deciding what to justify explicitly and what to assume as background knowledge
is part of the art of proof writing. If too much detail is included, the reader will
experience a ‘can’t see the wood for the trees’ feeling and the overall structure
will be hard to discern. Similarly, too heavy an emphasis on symbols may cause
difﬁculty in understanding the proof. Instead of concentrating on minute levels of
detail, it is more useful to explain the important steps, employing a judicious blend
of natural language and symbols. Of course, sufﬁcient detail needs to be given to
enable the reader to follow the argument and to verify its validity. Exactly how
much detail needs to be supplied will depend on such factors as the mathematical
sophistication of the intended audience and how novel the approach is.
We now give some examples of methods by which a mathematical statement may
be proved. The list is by no means exhaustive but it does give some of the more
common techniques. We shall come across plenty of other examples of proofs in
later chapters.
Direct Proof of a Conditional Proposition
Many mathematical conjectures can be expressed in the form P →Q, i.e. as a
conditional proposition. Their proof therefore consists of showing that
(A1 ∧A2 ∧· · · ∧An ∧T1 ∧T2 ∧· · · ∧Tm) ⊢(P →Q)
where the Ai and Tj are axioms and theorems as before. This is equivalent to
showing that
(A1 ∧A2 ∧· · · ∧An ∧T1 ∧T2 ∧· · · ∧Tm) →(P →Q)
is a tautology and,
by the logical equivalence of R →(P →Q) and
(R ∧P) →Q, that
(A1 ∧A2 ∧· · · ∧An ∧T1 ∧T2 ∧· · · ∧Tm ∧P) →Q
is a tautology or that
(A1 ∧A2 ∧· · · ∧An ∧T1 ∧T2 ∧· · · ∧Tm ∧P) ⊢Q

58
Mathematical Proof
i.e. that P ⇒Q. So, for a direct proof of a theorem of the form P →Q,
we assume the truth of the axioms and hence of any proved theorems. We also
assume the truth of P and show that the truth of Q necessarily follows.
Examples 2.2
1.
Prove that, for every integer n, if n is even, then n2 is even. (The integers
are the ‘whole’ numbers.)
Proof
Let n be an even integer. Then 2 is a factor of n, so n can be expressed as n = 2m
for some integer m. It follows that n2 = (2m)2 = 4m2. Now 4m2 can be written
as 2(2m2) where 2m2 is also an integer. Therefore n2 is even. This concludes
the proof.
□
Note that we have omitted reasons for certain steps. For instance no speciﬁc
reason was given for the fact that (2m)2 = 4m2. This is because it is assumed
that this step is obvious. However, in a more formal proof, missing details would
have to be supplied.
The proof can be written using more mathematical notation.
This gives the
following more concise, but still acceptable, version.
Proof
Let n be an integer.
Then
n = 2m
for some integer m
n2 = (2m)2
⇒
= 4m2
= 2(2m)2
⇒
n2 is an even integer.
□
Strictly speaking, what we are asked to prove here is the proposition: ∀x[P(x) →
Q(x)], where P(x) is ‘x is even’, Q(x) is ‘x2 is even’ and the universe of
discourse is the integers. What we have in fact proved is the proposition which
is the universal speciﬁcation of this quantiﬁed proposition: P(n) →Q(n) for
any n in the universe of discourse. The assumption P(n) is true is that n is an

Methods of Proof
59
arbitrary even integer, i.e. a ‘typical’ even integer. The proof shows that the truth
of Q(n) follows from this assumption and therefore that P(n) →Q(n) for any n
in the universe of discourse. Universal generalization allows us to conclude that
∀x[P(x) →Q(x)].
2.
Prove that, if n and m are integers and 3 is a factor of both n and m,
then 3 is a factor of any number of the form nx + my where x and y are
integers.
Proof
We are required to prove the conditional proposition
[R(n) ∧R(m)] →Q(n, m)
where R(n) is ‘3 is a factor of n’ and R(m) is ‘3 is a factor of m’ and m and n
are arbitrary integers. The proposition Q(n, m) is given by ‘3 is a factor of any
number of the form nx+my, where x and y are integers’. (We are using universal
speciﬁcation here as in the last example.)
We make the assumption that R(n) ∧R(m) is a true proposition, i.e. that n and
m are arbitrary integers such that 3 is a factor of each.
From the truth of ‘3 is a factor of n’ we can deduce that n = 3p for some integer
p. Similarly, from ‘3 is a factor of m’ we can write m = 3q for some integer
q. Hence nx + my = 3px + 3qy = 3(px + qy). Since px + qy is an integer,
we conclude that nx + my can be written as three times an integer and hence is
divisible by three.
□
This argument may be summarized more symbolically as follows.
Proof
Let m and n be integers both divisible by 3.
Then
3 is a factor of n ⇒n = 3p, where p is an integer
and
3 is a factor of m ⇒m = 3q, where q is an integer.
Hence
nx + my = 3px + 3qy
= 3(px + qy)

60
Mathematical Proof
⇒
nx + my is divisible by three.
□
3.
What is wrong with the following ‘proof’ that 1 = 2?
‘Proof’
We shall ‘prove’ the conditional proposition: ‘For x a real number, if
x = 2, then x = 1’.
x = 2
x −1 = 1
⇒
(x −1)2 = 1
⇒
= x −1
x2 −2x + 1 = x −1
⇒
x2 −2x = x −2
⇒
x(x −2) = x −2
⇒
x(x −2)
x −2
= x −2
x −2
⇒
x = 1.
⇒
This example shows that great care needs to be taken when constructing proofs.
Each step seems to follow logically from the previous ones, yet there is clearly a
ﬂaw in the argument somewhere because it is claiming to prove an absurdity.
The error, in fact, comes right at the end of the proof when both sides of the
equation are divided by x −2. This division is not allowed because x = 2 and
division by zero is not a valid operation. The correct conclusion to draw from the
equation x(x −2) = x −2 is: either x = 2 or x = 1.
Proof of a Conditional Proposition using the Contrapositive
Recall that the contrapositive ¯Q →¯P is logically equivalent to the conditional
proposition P →Q. Hence, if we can establish the truth of the contrapositive,
we can deduce that the conditional is also true. This constitutes an indirect proof
of P →Q although we may use a direct proof of ¯Q →¯P since this is itself
a conditional proposition. We assume the truth of ¯Q (together with the relevant
axioms and theorems) and we establish the truth of ¯P.

Methods of Proof
61
Examples 2.3
1.
By proving the contrapositive, prove that, for every integer n, if n2 is
even, then n is even.
Proof
The proposition to be proved is P(n) →Q(n), where P(n) is ‘n2 is even’,
Q(n) is ‘n is even’ and n is an arbitrary integer. The contrapositive is ¬Q(n) →
¬P(n): if n is odd then n2 is odd. We prove this directly by assuming the truth
of ‘n is odd’ and showing that the truth of ‘n2 is odd’ follows.
Let n be an odd integer.
Then
n = 2m + 1
where m is an integer
n2 = (2m + 1)2
⇒
= 4m2 + 4m + 1
= 2(2m2 + 2m) + 1
where 2m2 + 2m is an integer
⇒
n2 is odd.
□
2.
Prove that, if m and n are positive integers such that mn = 100, then
either m ⩽10 or n ⩽10.
Proof
We shall again prove the contrapositive but we must be a little careful.
The
proof required is that of P(m, n) →Q(m, n) where P(m, n) is ‘m and n are
arbitrary positive integers such that mn = 100’ and Q(m, n) is the inclusive
disjunction of the two propositions ‘m ⩽10’ and ‘n ⩽10’. By De Morgan’s
laws (p ∨q) = ¯p ∧¯q so that the negation of Q(m, n) is ‘m > 10 and n > 10’.
The contrapositive ¬Q(m, n) →¬P(m, n) is therefore ‘If m and n are arbitrary
integers such that m > 10 and n > 10, then mn ̸= 100’.
Let m and n be positive integers.
Then
m > 10
and
n > 10
mn > 100
⇒

62
Mathematical Proof
mn ̸= 100
⇒
and the theorem is proved.
□
3.
The following is given as an example of a common incorrect attempt
at a proof. The result to be proved is: If x and y are real numbers,
x2 + y2 ⩾2xy.
Suppose that x and y are arbitrary real numbers such that
x2 + y2 ⩾2xy.
Then
x2 −2xy + y2 ⩾0
(x −y)2 ⩾0.
⇒
Since this is clearly always true, we can conclude that x2 + y2 ⩾2xy.
What has been proved here is P →t, where P is ‘x2 + y2 ⩾2xy for
arbitrary real numbers x and y’ and t is a tautology. But P →t is not
logically equivalent to P and so this does not constitute a valid proof of
P.
Proof by Contradiction
Using a truth table we can readily establish the logical equivalence of P and
¯P →f, where f is a contradiction (a proposition which is always false). Hence
to prove a theorem T we can instead prove the conditional proposition ¯T →f.
This can be achieved using a direct proof which assumes the truth of axioms and
theorems as usual and also assumes the truth of ¯T (i.e. the falsity of T). We then
show that this implies a proposition which is patently false, i.e. a contradiction.
Usually, the contradiction takes the form of the conjunction of a proposition and
its negation, Q ∧¯Q. (Recall that Q ∧¯Q ≡f.) We can then deduce that ¯T →f is
true and hence that the theorem T is true.
This method of indirect proof is frequently referred to as ‘proof by contradiction’
or as reductio ad absurdum.

Methods of Proof
63
Examples 2.4
1.
Prove that
√
2 is not rational. (A rational number is one which can be
written in the form p/q where q ̸= 0 and p and q are integers.)
Proof
The proof of this theorem is a well known example of proof by contradiction. We
assume that
√
2 is rational and show that this leads to a contradiction.
Suppose that
√
2 is rational, i.e.
√
2 = m/n where m and n are integers and
n ̸= 0. We may assume that the fraction m/n is in its ‘lowest terms’, i.e. that m
and n have no common factors. If they do have common factors we simply cancel
them.
Now
√
2 = m/n
2 = m2/n2
⇒
2n2 = m2
⇒
m2 is even
⇒
m is even
(see example 2.3.1)
⇒
m = 2p
for some integer p
⇒
m2 = 4p2.
⇒
Substituting this result into the equation 2n2 = m2 gives
2n2 = 4p2
n2 = 2p2
⇒
n2 is even
⇒
n is even.
⇒
We have now shown that both m and n are even, i.e. that they have a common
factor 2.
But m and n have no common factors because any such factors
were cancelled at the beginning. Hence we have deduced the conjunction of a
proposition and its negation, i.e. a contradiction, and this proves the theorem. □
2.
Prove that there are inﬁnitely many prime numbers. (A prime number
is a positive integer greater than 1 which has no factors other than 1 and
itself. We explore the properties of prime numbers in Chapter 9.2.)
The following is Euclid’s proof of the theorem. It is generally regarded
as a classic example of proof by contradiction.

64
Mathematical Proof
Proof
Suppose that there are only a ﬁnite number, n say, of prime numbers. This means
that we can list all the prime numbers as follows: P1, P2, . . . , Pn.
Consider the product of this complete list of prime numbers: Q = P1P2 . . . Pn.
Now Q + 1 = P1P2 . . . Pn + 1.
The integer Q+1 is not prime since it is different from P1, P2, . . . , Pn. Therefore
Q + 1 must be divisible by some prime number, say P, where P is one of the Pi,
i = 1, 2, . . . , n. But Q is divisible by P and so clearly P cannot be a factor
of Q + 1 and here is our contradiction. We conclude that our assumption that
the number of prime numbers is ﬁnite is false and deduce that there are inﬁnitely
many primes.
□
Proof of a Biconditional Proposition
To prove a biconditional proposition P ↔Q, we usually appeal to the logical
equivalence of P ↔Q and [(P →Q) ∧(Q →P)]. Commonly, therefore,
the proof of a biconditional involves two distinct parts, one proving the result
P →Q and the other proving Q →P. It is fairly commonly the case that one of
the conditionals will be relatively more straightforward to prove than the other.
Examples 2.5
1.
Prove that, for any integers x and y, the product xy is even if and only if
x is even or y is even.
Proof
We ﬁrst prove that, if x is even or y is even then xy is even, using a direct proof.
Suppose x is even, i.e. x = 2n, for some integer n. Then xy = 2ny so xy is even.
If y is even, an identical argument shows that xy is even. The word ‘similarly’
is used to indicate this and to save us having to repeat the argument. We write:
similarly, if y is even, then xy is even.

Methods of Proof
65
We now prove the converse: if xy is even then x is even or y is even. We shall
use a direct proof of the contrapositive: if x and y are odd, then xy is odd.
Now x is odd and y is odd
x = 2n + 1, y = 2m + 1
for some integers m and n.
⇒
Then
xy = (2n + 1)(2m + 1)
= 4mn + 2n + 2m + 1
= 2(2mn + n + m) + 1
xy is odd.
⇒
This completes the proof.
□
2.
Prove that 3x2 −7x + 4 = x2 + 3x −8 if and only if x = 2 or x = 3.
Proof
3x2 −7x + 4 = x2 + 3x −8
2x2 −10x + 12 = 0
⇒
x2 −5x + 6 = 0
⇒
(x −2)(x −3) = 0
⇒
x = 2
or
x = 3.
⇒
To prove the converse we can write:
x = 2
or
x = 3
(x −2)(x −3) = 0
⇒
x2 −5x + 6 = 0
⇒
2x2 −10x + 12 = 0
⇒
3x2 −7x + 4 = x2 + 3x −8.
⇒
Note that the steps in the second part of the proof are exactly the same as those in
the ﬁrst in reverse. We can therefore summarize both parts of the proof as follows:
3x2 −7x + 4 = x2 + 3x −8
2x2 −10x + 12 = 0
⇔

66
Mathematical Proof
x2 −5x + 6 = 0
⇔
(x −2)(x −3) = 0
⇔
⇔
x = 2
or
x = 3.
□
The methods of proof which we have considered so far all have a similar structure.
In each case we start by assuming the truth of one particular proposition. We
then show that the truth of another proposition follows given certain background
knowledge, i.e. axioms and theorems already proved. We summarize each of
these methods of proof in the table below.
Method of proof
Assume
Deduce
Direct proof of P →Q
P; background knowledge
Q
Proof of P →Q using
¯Q; background knowledge
¯P
the contrapositive
Proof of P by
¯P; background knowledge
A contradiction, f
contradiction
Proof of the
(a) P; background knowledge
Q
biconditional P ↔Q
and
(b) Q; background knowledge
P
Use of Counter-Examples
Many mathematical conjectures take the form ‘all As are Bs’ or ‘all objects with
property A have property B’. This could be rewritten as the universally quantiﬁed
conditional propositional function ∀x[A(x) →B(x)], where A(x) is ‘x is an (or
has the property) A’ and B(x) is ‘x is a (or has the property) B’. The proof of
the conjecture could then take one of the forms described above.
The proposition could also be written ∀xB(x) where x is restricted to the universe
of discourse of As (or objects having the property A).
As we have already
remarked, the inability to ﬁnd an x which has not the property B does not
constitute a proof of the theorem. However many x we ﬁnd which have the
property B, this is no guarantee that we have failed to ﬁnd an elusive x which
does not have this property. However, if the universe of discourse is ﬁnite, then
(given time if it is large) we can examine every element to check that it has the
property in question. If no element fails the test then the theorem is proved. This
is called proof by exhaustion because it exhausts all the possibilities for x.

Methods of Proof
67
On the other hand, to disprove a conjecture of the form ∀xB(x), we need ﬁnd
only one member of the universe of discourse which does not have the property
B. We can justify this logically. To disprove ∀xB(x) we must prove the negation
¬∀xB(x). As we have seen (§1.8), this is equivalent to ∃x¬B(x), i.e. there is at
least one member of the universe which does not have the property B. To prove
this, all we need to do is to demonstrate that such an individual exists. This is the
essence of what is sometimes called ‘proof by counter-example’ (although a more
accurate title would be ‘disproof by counter-example’).
Example 2.6
Prove or disprove the proposition: for all positive integers n,
f(n) = n2 −n + 17
is prime.
Solution
We begin by trying a few positive integer values: f(1) = 17, f(2) = 19,
f(3) = 23, f(4) = 29, f(5) = 37.
In each of these f(n) is prime, so we might be tempted to suspect that f(n) is
always prime and to wonder how this conjecture might be proved. A few more

68
Mathematical Proof
examples might show some pattern developing and give us some insight into a
possible method of proof: f(6) = 47, f(7) = 59, f(8) = 73, f(9) = 89,
f(10) = 107.
Since all of these are prime, our conjecture seems well founded and we may feel
sufﬁciently conﬁdent to commence the attempt to ﬁnd a valid proof. However, a
little thought together with a degree of mathematical insight will save us wasting
our time. It is not too difﬁcult to see that f(n) could not be prime for every
positive integer n. An obvious counter-example is:
f(17) = 172 −17 + 17
= 17 × 17.
(For centuries mathematicians have attempted to ﬁnd a formula which will
generate only prime numbers. Pierre de Fermat (1601–65) thought that he had
cracked the problem with the formula 22n + 1 where n is any integer.
For
n = 0, 1, 2, 3, 4 the formula generates the integers 3, 5, 17, 257 and 65 537 all
of which are prime. However n = 5 gives 4 294 967 297 which has a factor 641.
Fermat’s conjecture was therefore disproved, although not until nearly 100 years
after his death when Euler discovered this counter-example.)
Exercises 2.2
1.
Prove that the sum of two consecutive integers is odd.
2.
Prove that, if n is an integer, n2 is odd if and only if n is odd.
3.
Prove directly that the product of two consecutive integers is even. Use
this result to prove that, if the quadratic equation x2 + ax + b = 0 has
solutions which are consecutive integers, then a is odd and b is even.
4.
Prove that, if both solutions of x2 + ax + b = 0 are even integers, then a
and b are both even integers.
5.
Prove that, if m and n and positive integers such that m is a factor of n
and n is a factor of m, then m = n.
6.
By proving the contrapositive, prove that, if n2 is not divisible by 5, then
n is not divisible by 5.
7.
Use proof by contradiction to prove that 1 +
√
2 is not rational.

Mathematical Induction
69
8.
Prove or disprove that, if a, b and c are integers such that a is a factor of
b + c, then a is a factor of b or a is a factor of c.
9.
Prove that, for any integer n, if n −2 is divisible by four, then n2 −4 is
divisible by 16.
10.
Prove that the smallest factor greater than 1 of any integer is prime.
2.4
Mathematical Induction
Despite its title, the method of proof known as ‘mathematical induction’ is not an
inductive proof! It could not be so because, as we have already pointed out, the
only acceptable mathematical proofs employ deductive reasoning. Induction has
a role in providing us with information as to what is likely to be true and hence
what is a reasonable conjecture. The problem with any proof is that we need to
know the result before we can commence proving it.
Many mathematical conjectures concern properties of the positive integers.
Consider, for example, the following problem: ﬁnd a formula for the sum of
the ﬁrst n odd integers. A useful starting point might be to write down the sums
for some small values of n and see if this gives us any idea as to what might be a
possible conjecture.
For n = 1, the sum is 1.
For n = 2, the sum is 1 + 3 = 4.
For n = 3, the sum is 1 + 3 + 5 = 9.
For n = 4, the sum is 1 + 3 + 5 + 7 = 16.
At this stage we notice that, so far, for each value of n, the sum is n2. We try a
few more to see if our conjecture is well founded.
For n = 5, the sum is 16 + 9 = 25.
For n = 6, the sum is 25 + 11 = 36.
Inductive reasoning leads us to the conjecture that the sum of the ﬁrst n odd
positive integers is n2. We must now ﬁnd a proof, based on deduction, that this is
true for all positive integers n.
Mathematical induction is appropriate for proving that a result holds for all
positive integers. It consists of the following steps:

70
Mathematical Proof
(a)
Prove that the conjecture holds for n = 1.
(b)
Prove that, for all k ⩾1, if the result holds for n = k, then it must also
hold for n = k + 1. This is known as the inductive step.
To prove the conditional proposition in (b), we call upon the techniques outlined
in the previous section. However, the inductive step is most commonly established
using a direct proof. We assume that the result holds for n = k. (This assumption
is sometimes known as the inductive hypothesis.) From this we deduce that it
also holds for n = k + 1. Because it holds for n = 1, the inductive step allows
us to deduce that it holds for n = 2, n = 3, etc. The ‘principle of mathematical
induction’ allows us to conclude that the result therefore holds for all positive
integers n. (This principle is usually taken as an axiom of the positive integers.)
Principle of Mathematical Induction
Let S(n) be a proposition concerning a positive integer n. If
(a)
S(1) is true, and
(b)
for every k ⩾1, the truth of S(k) implies the truth of S(k + 1),
then S(n) is true for all positive integers n.
An analogy to the process of mathematical induction is an inﬁnite line of ﬁreworks
connected together so that each is set off by the previous one in the line. Although
it has been arranged that the kth ﬁrework will ignite the (k+1)st, nothing happens
until we light the ﬁrst ﬁrework in the line. This sets off the second, which sets off
the third and so on to the end of the (inﬁnite) line.
Let us now subject our conjecture, that the sum of the ﬁrst n odd positive integers
is n2, to proof by mathematical induction.
Examples 2.7
1.
Prove that the sum of the ﬁrst n odd positive integers is n2.
Proof
We want to prove that
1 + 3 + 5 + · · ·
←−−−n terms−−−→= n2.

Mathematical Induction
71
Note that the last term in the sequence is 2n −1 so that we may write our
conjecture as
1 + 3 + 5 + · · · + (2n −1) = n2.
We follow the steps:
(a)
Prove that the conjecture is true for n = 1.
The sum of the ﬁrst one odd integer is 1 and, for n = 1, 1 = n2. So the conjecture
holds for n = 1.
(b)
Assume that the conjecture is true for n = k where k ⩾1 and show that
this implies the truth of the conjecture for n = k + 1.
Suppose that 1+3+5+· · ·+(2k−1) = k2. Adding the next odd integer, 2k+1,
to each side of the equation, we have
1 + 3 + 5 + · · · + (2k −1) + (2k + 1) = k2 + (2k + 1)
= (k + 1)2.
The left-hand side of this equation is the sum of the ﬁrst k + 1 odd numbers and
we have shown, using the inductive hypothesis, that this sum is (k + 1)2. Hence
we have shown that, if the conjecture holds for n = k, then it also holds for
n = k + 1. But we have shown that it holds for n = 1, and, by the principle of
mathematical induction, it therefore holds for all positive integers n.
□
2.
Prove that, for every positive integer n, the expression 2n+2 + 32n+1 is
divisible by 7.
Proof
Let f(n) = 2n+2 + 32n+1.
For n = 1, we have f(1) = 23 + 33 = 8 + 27 = 35 which is divisible by 7.
Hence the result holds for n = 1.
Assume that, for some integer k ⩾1,
f(k) = 2k+2 + 32k+1 = 7a
where a is an integer. (This is the inductive hypothesis.)
Now
f(k + 1) = 2(k+1)+2 + 32(k+1)+1

72
Mathematical Proof
= 2k+3 + 32k+3
= 2 × 2k+2 + 32 × 32k+1
= 2 × 2k+2 + 9 × 32k+1.
At this point we need to use the inductive hypothesis, 2k+2 + 32k+1 = 7a, to
substitute for either 2k+2 or 32k+1 (it doesn’t matter which).
So, substituting 32k+1 = 7a −2k+2 gives
f(k + 1) = 2 × 2k+2 + 9(7a −2k+2)
= 9 × 7a + 2 × 2k+2 −9 × 2k+2
= 7(9a −2k+2)
= 7b
where b = 9a −2k+2.
Since b is an integer, we can conclude that f(k + 1) is divisible by 7. This
completes the inductive step.
Applying the principle of mathematical induction we deduce that 2n+2 + 32n+1
is divisible by 7 for all positive integers n.
□
3.
What is wrong with the following ‘proof’ by induction?
Conjecture: All computers are the same price.
‘Proof’: Let S(n) denote the proposition ‘any group of n computers are
the same price’.
Clearly S(1) is true.
Assume the truth of S(k), i.e. that any group of k computers are the
same price, and consider any collection of k + 1 (distinct) computers
denoted by C1, C2, . . . , Ck, Ck+1. By the inductive hypothesis, all of
C1, C2, . . . , Ck are the same price and also C2, . . . , Ck, Ck+1 are the
same price. Therefore all of C1, C2, . . . , Ck, Ck+1 are the same price.
Since C1, C2, . . . , Ck, Ck+1 was any collection of k + 1 computers, we
have established the inductive step. Hence all computers are the same
price by mathematical induction.
Solution
Empirical evidence shows that the ‘proved’ statement is false, so the proof
contains some error which must be in the inductive step.

Mathematical Induction
73
The inductive step relies implicitly on the two groups of computers consisting of
C1, C2, . . . , Ck and C2, . . . , Ck, Ck+1 having members in common so that the
‘same price’ property can be transferred from the ﬁrst group to the second. If
k ⩾2 this is indeed the case, so the inductive step is valid for k ⩾2. The
problem is that the implication ‘if S(1) is true then S(2) is true’ does not hold as
the two groups in question, C1 and C2, do not have members in common.
The ‘proof’ is not valid because we have not established the inductive step for
every k ⩾1.
Variations on the Principle of Mathematical Induction
There are various modiﬁcations which we can make to the inductive principle.
Suppose, for example, that we wish to prove that a proposition S(n) is true for
all integers greater than or equal to some ﬁxed integer N. The following simple
modiﬁcation to the principle of induction would achieve this.
(a)
Prove that S(N) is true.
(b)
Prove that, for every integer k ⩾N, if S(k) is true, then S(k + 1) is true.
This is just the standard method of proof by induction except that we ‘begin’ at
N instead of 1.
Note that, even when required to prove S(n) for all positive integers, sometimes
it can be simpler to begin the induction at n = 0 rather than n = 1. If we do
begin with n = 0, we have in fact proved slightly more than was required, but no
one would quibble with that! In example 2.7.2 for instance, f(0) = 22 + 3 = 7,
which is clearly divisible by seven. Continuing with the inductive hypothesis and
inductive step as in the example would have shown that 2n+2 + 32n+1 is divisible
by 7 for all positive integers n and also for n = 0.
A more substantial modiﬁcation of the inductive method is provided by the so-
called ‘second principle of induction’. The essence of this is that, when we come
to the inductive step, we assume that S(r) is true for all positive integers r less
than or equal to k, rather than just for k itself.

74
Mathematical Proof
Second Principle of Induction
Let S(n) be a proposition concerning a positive integer n. If
(a)
S(1) is true, and
(b)
for every k ⩾1, the truth of S(r) for all r ⩽k implies the truth of
S(k + 1),
then S(n) is true for all positive integers.
This second principle of induction may at ﬁrst appear to be more general than the
ﬁrst because we are allowed to assume rather more in order to deduce the truth of
S(k + 1). However, if we let T(n) be the proposition ‘S(r) is true for all positive
integers r ⩽n’ then the two parts of the second principle are:
(a)
T(1) is true, and
(b)
for every k ⩾1, the truth of T(k) implies the truth of T(k + 1).
This is just the (ﬁrst) principle of induction for the proposition T(n). Thus the
second principle is no more general than the ﬁrst although it may be simpler to
use in the proofs of certain results.
We summarize the proof of S(n) (where n is a positive integer) using each of the
two principles of induction in the table below.
Assume
Deduce
Proof of S(n) using the
(a) Background knowledge
S(1)
(ﬁrst) principle of
and
induction
(b) S(k); background knowledge
S(k + 1)
Proof of S(n) using the
(a) Background knowledge
S(1)
second principle of
and
induction
(b) S(1), S(2), . . . , S(k);
S(k + 1)
background knowledge
Example 2.8
Prove that every positive integer greater than 1 is either prime or can be expressed
as a product of prime numbers.

Mathematical Induction
75
(This is part of a result which has the grand name ‘the fundamental theorem of
arithmetic’. The complete statement of this theorem goes on to say that, for any
given positive integer, its expression as the product of primes is unique apart from
the order in which the prime factors are written.)
Proof
Since the proposition involves integers greater than 1, we begin the induction with
n = 2. The proposition clearly holds for n = 2 since 2 is itself a prime number.
Now suppose that every integer greater than 1 and less than or equal to k is either
prime or can be expressed as the product of prime numbers. Consider the integer
k + 1. There are two possibilities: either it is prime or it is composite (not prime).
If it is prime then there is nothing to prove.
If, on the other hand, k + 1 is composite, then it can be written as k + 1 = rs
where 2 ⩽r ⩽k and 2 ⩽s ⩽k. Now, by our inductive hypothesis, r and s
are prime or can be written as products of prime numbers: r = p1p2 . . . pt and
s = q1q2 . . . qu where the pi (i = 1, 2, . . . , t) and qj (j = 1, 2, . . . , u) are prime
numbers.
Hence
k + 1 = rs
= p1p2 . . . ptq1q2 . . . qu
so that k+1 can be expressed as the product of prime numbers. The result follows
by the second principle of induction.
□
Inductive Deﬁnitions
The use of the inductive principles is not conﬁned entirely to proofs of
propositions about the positive integers; they can also be used to deﬁne
mathematical objects or properties which depend upon the positive integers.
Consider the following sequence of ‘Fibonacci numbers’†:
1, 1, 2, 3, 5, 8, 13, 21, . . . .
† Named after the Italian, Leonardo of Pisa (born c. 1170 and know as Fibonacci), who was reputed
to have used the sequence to model the increase in a population of rabbits over time. Unfortunately,
the model proved to be inaccurate; unrestrained rabbit populations increase more rapidly than the
Fibonacci numbers! However, over the years many ‘occurrences’ of this sequence have been noticed
in nature, art and architecture.

76
Mathematical Proof
Each number in the sequence after the ﬁrst two is the sum of the two preceding
numbers. Denoting the nth Fibonacci number by fn, we can deﬁne the sequence
as follows:
f1 = 1,
f2 = 1
and, for n ⩾3,
fn = fn−1 + fn−2.
This is an example of an inductive deﬁnition; we can think of it as a means of
making precise the ‘.. .’ in the sequence of Fibonacci numbers deﬁned above.
The astute reader will have noticed that the inductive deﬁnition does not quite
conform to the principles of induction stated above.
To begin the inductive
deﬁnition, we need to deﬁne the ﬁrst two Fibonacci numbers, rather than only
the ﬁrst one. The following describes the general form of an inductive deﬁnition
of some mathematical object or property An which depends on a positive integer
n.
Inductive Deﬁnition
To deﬁne An for all positive integers:
(a)
Deﬁne explicitly Ak for k = 1, 2, . . . , r.
(b)
For k > r, deﬁne Ak in terms of A1, . . . , Ak−1.
To prove propositions about some object or involving some property which has
been deﬁned inductively, it is natural to use mathematical induction. We end
this chapter with an inductive proof (using the second principle) of a property
of Fibonacci numbers. Note that we need to begin the proof with an explicit
veriﬁcation of the result for n = 1 and n = 2. (Why is this?)
Example 2.9
Let fn denote the nth Fibonacci number deﬁned inductively above. Prove that
fn < 2n.
Proof
First note that f1 = 1 < 2 = 21 and f2 = 1 < 4 = 22, so the proposition is true
for n = 1 and n = 2.

Mathematical Induction
77
Now suppose that fr < 2r for all positive integers r ⩽k. Then for k ⩾2,
fk+1 = fk + fk−1
(the inductive deﬁnition of fn)
< 2k + 2k−1
(by the inductive hypothesis)
< 2k + 2k
(since 2k−1 < 2k)
= 2 × 2k
= 2k+1.
This completes the inductive step. We conclude that fn < 2n for all positive
integers n.
□
Exercises 2.3
1.
Prove that, for all positive integers n,
1 + 2 + · · · + n = 1
2n(n + 1).
2.
Prove that 2n > n for all positive integers n.
3.
Prove that, for all positive integers n, 5n −1 is divisible by 4.
4.
Prove that, for all non-negative integers n,
1 + x + x2 + x3 + · · · + xn = xn+1 −1
x −1
where x is a real number, x ̸= 1.
5.
Prove that the sum of the squares of the ﬁrst n positive integers is
n(n + 1)(2n + 1)
6
.
6.
Prove that the sum of the cubes of the ﬁrst n positive integers is
·n(n + 1)
2
¸2
.

78
Mathematical Proof
7.
For all positive integers n, An is deﬁned inductively as follows:
A1 = 3
An = An−1 + 3
for n ⩾2.
Prove that An = 3n.
8.
Prove that, for all integers n ⩾4, n! > 2n. (n! = n(n −1)(n −2) . . . 1.)
9.
Prove that the sum of the ﬁrst n even integers is n(n + 1).
10.
Prove that, for all positive integers n, 42n+1 + 3n+2 is divisible by 13.
11.
An is deﬁned inductively as follows:
A1 = 6
A2 = 11
An = 3An−1 −2An−2
for n ⩾3.
Prove that, for n ⩾1, An = 5 × 2n−1 + 1.
12.
Show that, for the Fibonacci sequence,
f 2
n+2 −f 2
n+1 = fnfn+3
n = 1, 2, . . . .

Chapter 3
Sets
3.1
Sets and Membership
The notion of a ‘set’ is one of the basic concepts of mathematics—some would
say the basic concept. Those who have encountered sets in their previous study of
mathematics may be tempted to skip this chapter, regarding sets as rather trivial
objects. Our advice is: don’t! Set theory is non-trivial and we shall be using
set-theoretic terminology and concepts throughout the book.
We shall make no attempt to give a precise deﬁnition of a set†. However, we can
describe what we mean by the term: a set is to be thought of as any collection
of objects whatsoever.
The objects can also be anything and they are called
elements of the set. The elements contained in a given set need not have anything
in common (other than the obvious common attribute that they all belong to the
given set). Equally, there is no restriction on the number of elements allowed in
a set; there may be an inﬁnite number, a ﬁnite number or even no elements at
all. There is, however, one restriction we insist upon: given a set and an object,
we should be able to decide (in principle at least—it may be difﬁcult in practice)
whether or not the object belongs to the set. Clearly a concept as general as this
has many familiar examples as well as many frivolous ones.
† In §2.2 we explained why undeﬁned terms are necessary in mathematics. In an axiomatic treatment
of set theory, it is usual for ‘set’ to be undeﬁned.
79

80
Sets
Examples 3.1
1.
A set could be deﬁned to contain Picasso, the Eiffel Tower and the number
π. This is a (rather strange) ﬁnite set.
2.
The set containing all the positive, even integers is clearly an inﬁnite set.
3.
Consider the ‘set’ containing the 10 best songs of all time. This is not
allowed unless we give a precise deﬁnition of ‘best’. Your best? Mine?
Without being more precise this fails the condition that we should be able
to decide whether an element belongs to the set.
Notation
We shall generally use upper-case letters to denote sets and lower-case letters
to denote elements. (This convention will sometimes be violated, for example
when the elements of a particular set are themselves sets.) The symbol ∈denotes
‘belongs to’ or ‘is an element of’. Thus
a ∈A means (the element) a belongs to (the set) A
and
a /∈A means ¬(a ∈A) or a does not belong to A.
Deﬁning Sets
Sets can be deﬁned in various ways. The simplest is by listing the elements
enclosed between curly brackets or ‘braces’ { }. The two (well deﬁned) sets
in examples 3.1 could be written:
A = {Picasso, Eiffel Tower, π}
B = {2, 4, 6, 8, . . .}.
In the second of these we clearly cannot list all the elements. Instead we list
enough elements to establish a pattern and use ‘. ..’
to indicate that the list
continues indeﬁnitely. Other examples are the following.
For a ﬁxed positive integer n, Cn = {1, 2, . . . , n}, the set of the ﬁrst n positive
integers. Again we use ‘. . . ’ to indicate that there are elements in the list which
we have omitted to write, although in this case only ﬁnitely many are missing.

Sets and Membership
81
D = { }, the empty set (or null set), which contains no elements. This set is
usually denoted ∅.
Listing the elements of a set is impractical except for small sets or sets where
there is a pattern to the elements such as B and Cn above. An alternative is to
deﬁne the elements of a set by a property or predicate (see chapter 1). More
precisely, if P(x) is a single-variable propositional function, we can form the set
whose elements are all those objects a (and only those) for which P(a) is a true
proposition. A set deﬁned in this way is denoted
A = {x : P(x)}.
(This is read: the set of all x such that P(x) (is true).)
Note that ‘within A’—that is, if we temporarily regard A as the universe of
discourse—the quantiﬁed propositional function ∀xP(x) is a true statement.
Examples 3.2
1.
The set B above could be deﬁned as B = {n : n is an even, positive
integer}, or B = {n : n = 2m, where m > 0 and m is an integer}, or,
with a slight change of notation, B = {2m : m > 0 and m is an integer}.
Note that although the propositional functions used are different, the same
elements are generated in each case.
2.
The set Cn above could be deﬁned as Cn = {p : p is an integer and
1 ⩽p ⩽n}.
3.
The set {1, 2} could alternatively be deﬁned as {x : x2 −3x + 2 = 0}.
We say that {1, 2} is the solution set of the equation x2 −3x + 2 = 0.
4.
The empty set ∅can be deﬁned in this way using any propositional
function P(x) which is true for no objects x.
For instance, rather
frivolously,
∅= {x : x is a green rabbit with long purple ears}.
5.
X = {x : x is an honest politician} is not a set unless we deﬁne ‘honest’
more precisely.

82
Sets
Equality of Sets
Two sets are deﬁned to be equal if and only if they contain the same elements; that
is, A = B if ∀x[x ∈A ↔x ∈B] is a true proposition, and conversely. The order
in which elements are listed is immaterial. Also, it is the standard convention to
disregard repeats of elements in a listing. Thus the following all deﬁne the same
set:
{1, −1
2, 1066, π}
{−1
2, π, 1066, 1}
{1, −1
2, −1
2, π, 1066, −1
2, 1}.
We should perhaps note here that there is only one empty set; or, put another way,
all empty sets are equal. This is because any two empty sets contain precisely the
same elements: none!
Also, if P(x) and Q(x) are propositional functions which are true for precisely
the same objects x, then the sets they deﬁne are equal, i.e.
{x : P(x)} = {x : Q(x)}.
For example, {x : (x −1)2 = 4} = {x : (x + 1)(x −3) = 0}, since the two
propositional functions P(x) : (x −1)2 = 4 and Q(x) : (x + 1)(x −3) = 0 are
true for precisely the same values of x, namely −1 and 3.
Deﬁnition 3.1
If A is a ﬁnite set its cardinality, |A|, is the number of (distinct) elements
which it contains.
If A has an inﬁnite number of elements, we say it has inﬁnite cardinality†,
and write |A| = ∞.
Other notations commonly used for the cardinality of A are n(A), #(A) and ¯¯A.
† There is a more sophisticated approach to cardinality of inﬁnite sets which allows different inﬁnite
sets to have different cardinality. Thus ‘different sizes’ of inﬁnite sets can be distinguished! In this
theory the set of integers has different cardinality from the set of numbers, for example. See §5.5 for
more details of how this distinction can be made.

Sets and Membership
83
Examples 3.3
1.
|∅| = 0 since ∅contains no elements.
2.
|{π, 2, Attila the Hun}| = 3.
3.
If X = {0, 1, . . . , n} then |X| = n + 1.
4.
|{2, 4, 6, 8, . . .}| = ∞.
Although cardinality appears to be a simple enough concept, determining the
cardinality of a given set may be difﬁcult in practice. This is particularly the
case when some or all of the elements of the given set are themselves sets. This is
a perfectly valid construction: the elements of a set can be anything, so certainly
they can be sets.
For example, let X = {{1, 2}}. Then X contains only a single element, namely
the set {1, 2}, so |X| = 1. It is clearly important to distinguish between the
set {1, 2} (which has cardinality 2) and X, the set which has {1, 2} as its only
element. Similarly, the sets ∅and {∅} are different. The latter is non-empty
since it contains a single element—namely ∅. Thus |{∅}| = 1.
Examples 3.4
1.
Let A = {1, {1, 2}}. Note that A has two elements, the number 1 and the
set {1, 2}. Therefore, |A| = 2.
2.
Similarly,
|{1, 2, {1, 2}}| = 3,
|{∅, {1, 2}}| = 2,
|{∅, {∅}}| = 2,
|{∅, {∅}, {1, 2}}| = 3,
|{∅, {∅, {∅}}}| = 2, etc.

84
Sets
Exercises 3.1
1.
List the elements of each of the following sets, using the ‘...’ notation
where necessary:
(i)
{x : x is an integer and −3 < x < 4}
(ii)
{x : x is a positive (integer) multiple of three}
(iii)
{x : x = y2 and y is an integer}
(iv)
{x : (3x −1)(x + 2) = 0}
(v)
{x : x ⩾0 and (3x −1)(x + 2) = 0}
(vi
{x : x is an integer and (3x −1)(x + 2) = 0}
(vii)
{x : x is a positive integer and (3x −1)(x + 2) = 0}
(viii) {x : 2x is a positive integer}.
2.
Let X = {0, 1, 2}. List the elements of each of the following sets:
(i)
{z : z = 2x and x ∈X}
(ii)
{z : z = x + y where x ∈X and y ∈X}
(iii)
{z : x = z + y where x ∈X and y ∈X}
(iv)
{z : z ∈X or −z ∈X}
(v)
{z : z2 ∈X}
(vi)
{z : z is an integer and z2 ∈X}.
3.
Determine the cardinality of each of the following sets:
(i)
{x : x is an integer and 1/8 < x < 17/2}
(ii)
{x : √x is an integer}
(iii)
{x : x2 = 1 or 2x2 = 1}
(iv)
{a, b, c, {a, b, c}}
(v)
{a, {b, c}, {a, b, c}}
(vi)
{{a, b, c}, {a, b, c}}
(vii)
{a, {a}, {{a}}, {{{a}}}}
(viii) {∅, {∅}, {{∅}}}.
4.
Use the notation {x : P(x)}, where P(x) is a propositional function, to
describe each of the following sets.
(i)
{1, 2, 3, 4, 5}.
(ii)
{3, 6, 9, 12, 15, . . . , 27, 30}.
(iii)
{1, 3, 5, 7, 9, 11, . . .}.
(iv)
{2, 3, 5, 7, 11, 13, 17, 19, 23, . . .}.
(v)
{a, e, i, o, u}.

Subsets
85
(vi)
The set of integers which can be written as the sum of the squares
of two integers.
(vii)
The set of all integers less than 1000 which are perfect squares.
(viii) The set of all numbers that are an integer multiple of 13.
(ix)
{Afghanistan, Albania, Algeria, . . . , Zambia, Zimbabwe}.
(x)
{Love’s Labour’s Lost, The Comedy of Errors, The Two Gentlemen
of Verona, . . . , The Tempest, The Winter’s Tale, The Famous
History of the Life of King Henry VIII}.
3.2
Subsets
Deﬁnition 3.2
The set A is a subset of the set B, denoted A ⊆B, if every element of A
is also an element of B. Symbolically, A ⊆B if ∀x[x ∈A →x ∈B] is
true, and conversely.
If A is a subset of B, we say that B is a superset of A, and write B ⊇A.
Clearly every set B is a subset of itself, B ⊆B. (This is because, for any given
x, x ∈B →x ∈B is ‘automatically’ true.) Any other subset of B is called a
proper subset of B. The notation A ⊂B is used to denote ‘A is a proper subset
of B’. Thus A ⊂B if and only if A ⊆B and A ̸= B.
It should also be noted that ∅⊆A for every set A. In this case deﬁnition 3.2 is
satisﬁed in a vacuous way—the empty set has no elements, so certainly each of
them belongs to A. Alternatively, for any object x, the proposition x ∈∅is false
so the conditional (x ∈∅) →(x ∈A) is true.
Examples 3.5
1.
{2, 4, 6, . . .} ⊆{1, 2, 3, . . .} ⊆{0, 1, 2, . . .}. Of course, we could have
used the proper subset symbol ⊂to link these three sets instead.

86
Sets
2.
Similarly: {women} ⊆{people} ⊆{mammals} ⊆{creatures};
{War and Peace} ⊆{novels} ⊆{works of ﬁction};
{Mona Lisa} ⊆{paintings} ⊆{works of art}; etc.
Again, in each of these we could have used ⊂instead.
3.
Let X = {1, {2, 3}}. Then {1} ⊆X but {2, 3} is not a subset of X,
which we can denote by {2, 3} ̸⊆X. However, {2, 3} is an element
of X, so {{2, 3}} ⊆X. Care clearly needs to be taken to distinguish
between set membership and subset, particularly when a set has elements
which are themselves sets.
To prove that two sets are equal, A = B, it is sufﬁcient (and frequently very
convenient) to show that each is a subset of the other, A ⊆B and B ⊆A.
Essentially, this follows from the following logical equivalence of compound
propositions:
(P ↔Q) ≡[(P →Q) ∧(Q →P)].
We know that A = B if ∀x(x ∈A ↔x ∈B) is a true proposition. In chapter 2
we noted that to prove that a biconditional P ↔Q is true, it is sufﬁcient to
prove both conditionals P →Q and Q →P are true. It follows that to prove
∀x(x ∈A ↔x ∈B) it is sufﬁcient to prove both ∀x(x ∈A →x ∈B) and
∀x(x ∈B →x ∈A). But ∀x(x ∈A →x ∈B) is true precisely when A ⊆B
and similarly ∀x(x ∈B →x ∈A) is true precisely when B ⊆A. In summary:
Theorem 3.1
Two sets A and B are equal if and only if A ⊆B and B ⊆A.
Examples 3.6
1.
Show that {x : 2x2 + 5x −3 = 0} ⊆{x : 2x2 + 7x + 2 = 3/x}.
Solution
Let A = {x : 2x2 + 5x −3 = 0} and B = {x : 2x2 + 7x + 2 = 3/x}.
We need to show that every element of A is an element of B. The equation
2x2 + 5x −3 = 0 has solutions x = 1
2 and x = −3, so A = { 1
2, −3}.

Subsets
87
When x = 1
2, 2x2 + 7x + 2 = 1
2 + 7
2 + 2 = 6 = 3/x, so 1
2 ∈B.
When x = −3, 2x2 + 7x + 2 = 18 −21 + 2 = −1 = 3/x, so −3 ∈B.
Therefore every element of A is an element of B, so A ⊆B.
2.
Let A = {{1}, {2}, {1, 2}} and let B be the set of all non-empty subsets
of {1, 2}. Show that A = B.
Solution
A ⊆B since each of the three elements of A is a non-empty subset of {1, 2} and
therefore an element of B.
B ⊆A since every non-empty subset of {1, 2} (i.e. every element of B) is
contained in A.
Using theorem 3.1, we conclude that A = B.
3.
Prove that if A ⊆B and C = {x : x ∈A ∨x ∈B}, then C = B.
Solution
Let A ⊆B. We will show that B ⊆C and C ⊆B.
Let x ∈B. Then x ∈A ∨x ∈B is true, so x ∈C. Thus every element of B also
belongs to C, so B ⊆C.
Now let x ∈C. Then either x ∈A or x ∈B (or both). However, if x ∈A then it
follows that x ∈B also, since A ⊆B. Therefore in either case we can conclude
x ∈B. This shows that every element of C also belongs to B, so C ⊆B.
We have now shown B ⊆C and C ⊆B, so theorem 3.1 allows us to conclude
that B = C.
Since the concept of a set is such a broad one, it is usual to restrict attention to only
those sets which are relevant in a particular context. For example, we would surely
wish to discount sets such as {Genghis Khan, Queen Boadicea, Attila the Hun}
in a study of expert systems! It is convenient to deﬁne some universal set which
contains as subsets all sets relevant to the current task or study. Anything outside
the universal set is simply not considered. The universal set is not something
ﬁxed for all time—we can change it to suit different contexts. The universal set is

88
Sets
frequently denoted U . The universal set is, of course, essentially the universe of
discourse introduced in chapter 1.
Some special sets of numbers which are frequently used as universal sets are the
following.
N = {0, 1, 2, 3, . . .} the set of natural numbers.
Z = {. . . , −2, −1, 0, 1, 2, . . .} the set of integers†.
Q = {p/q : p, q ∈Z and q ̸= 0} the set of fractions or rational numbers.
R = the set of real numbers; real numbers can be thought of as corresponding
to points on a number line or as numbers written as (possibly inﬁnite)
decimals.
C = {x + iy : x, y ∈R and i2 = −1} the set of complex numbers.
Clearly the following subset relations hold amongst these sets:
N ⊆Z ⊆Q ⊆R ⊆C.
Also frequently used are Z+, Q+ and R+, the sets of positive integers, rational
numbers and real numbers respectively. Note that N is not equal to Z+ since 0
belongs to the former but not the latter. In addition, we shall sometimes use E and
O to denote the sets of even and odd integers respectively:
E = {2n : n ∈Z} = {. . . , −4, −2, 0, 2, 4, . . .}
† The notation Z comes from the German word for numbers: Zahlen.

Subsets
89
O = {2n + 1 : n ∈Z} = {. . . , −3, −1, 1, 3, 5, . . .}.
If a universal set has been deﬁned the notation {x : P(x)} means the set of all x in
the universal set satisfying the property P(x). Therefore if our current universal
set is Z then X = {x : 2x2 + 3x −2 = 0} is the set {−2}, but if U is Q or R
then X = {−2, 1
2}. In the former case we would probably make the restriction
more explicit and write
X = {x : x ∈Z and 2x2 + 3x −2 = 0}
or, using a slight but useful abuse of the notation,
X = {x ∈Z : 2x2 + 3x −2 = 0}.
Exercises 3.2
1.
State whether each of the following statements is true or false.
(i)
2 ∈{1, 2, 3, 4, 5}
(v)
∅⊆{∅, {∅}}
(ii)
{2} ∈{1, 2, 3, 4, 5}
(vi)
{∅} ⊆{∅, {∅}}
(iii)
2 ⊆{1, 2, 3, 4, 5}
(vii)
0 ∈∅
(iv)
{2} ⊆{1, 2, 3, 4, 5}
(viii)
{1, 2, 3, 4, 5} = {5, 4, 3, 2, 1}.
2.
(i)
List all the subsets of:
(a)
{a, b}
(b)
{a, b, c}
(c)
{a}.
Can you conjecture how many subsets a set with n elements will
have?
(ii)
Does the empty set have any subsets? Explain your answer. Is
your answer consistent with your conjecture from part (i)?
3.
In each of the following cases state whether x ∈A, x ⊆A, both or
neither:
(i)
x = {1};
A = {1, 2, 3}
(ii)
x = {1};
A = {{1}, {2}, {3}}
(iii)
x = {1};
A = {1, 2, {1, 2}}
(iv)
x = {1, 2};
A = {1, 2, {1, 2}}
(v)
x = {1};
A = {{1, 2, 3}}
(vi)
x = 1;
A = {{1}, {2}, {3}}.

90
Sets
4.
Given that X = {1, 2, 3, 4}, list the elements of each of the following
sets:
(i)
{A : A ⊆X and |A| = 2}
(ii)
{A : A ⊆X and |A| = 1}
(iii)
{A : A is a proper subset of X}
(iv)
{A : A ⊆X and 1 ∈A}.
5.
Let U = {x : x is an integer and 2 ⩽x ⩽10}. In each of the following
cases, determine whether A ⊆B, B ⊆A, both or neither:
(i)
A = {x : x is odd}
B = {x : x is a multiple of 3}
(ii)
A = {x : x is even}
B = {x : x2 is even}
(iii)
A = {x : x is even}
B = {x : x is a power of 2}
(iv)
A = {x : 2x + 1 > 7}
B = {x : x2 > 20}
(v)
A = {x : √x ∈Z}
B = {x : x is a power of 2 or 3}
(vi)
A = {x : √x ⩽2}
B = {x : x is a perfect square}
(vii)
A = {x : x2 −3x + 2 = 0}
B = {x : x + 7 is a perfect square}.
6.
In each of the following cases, prove that A ⊆B:
(i)
A = {x : 2x2 + 5x = 3}
B = {x : 2x2 + 17x + 27 = 18/x}
(ii)
A = {x : x is a positive integer and x is even}
B = {x : x is a positive integer and x2 is even}
(iii)
A = {x : x is an integer and x is a multiple of 6}
B = {x : x is an integer and x is a multiple of 3}.
7.
Let A be any set and P(x) be any propositional function.
(i)
Prove that B = {x : x ∈A and P(x)} is a subset of A.
If B ⊂A what can you deduce about P(x)?
If A = B what can you deduce about P(x)?
(ii)
Prove that A is a subset of C = {x : x ∈A or P(x)}.
If A ⊂C what can you deduce about P(x)?
If A = C what can you deduce about P(x)?
8.
Prove that, if A ⊆B and C = {x : x ∈A ∧x ∈B}, then C = A.
9.
Prove that, if A and B have no elements in common and C = {x :
x ∈A ∧x ∈B}, then C = ∅.
10.
(i)
Prove that, if A ⊆B and B ⊆C, then A ⊆C.
(ii)
Deduce that, if A ⊆B, B ⊆C and C ⊆A, then A = B = C.

Operations on Sets
91
11.
Given that A = {1, 2, 3, 4}, determine the cardinality of each of the
following sets:
(i)
{B : B ⊆A and |B| = 2}
(ii)
{B : B ⊆A and 1 ∈B}
(iii)
{B : B ⊆A and {1, 2} ⊆B}
(iv)
{B : B ⊆A and {1, 2} ⊂B}.
12.
(Russell’s paradox†.)
Consider the ‘set’ R of all sets which are not
elements of themselves. That is,
R = {A : A is a set and A /∈A}.
Find a set which is an element of R. Can you ﬁnd a set which is not an
element of R?
Explain why R is not a well deﬁned set. (Hint: is R itself an element of
R?)
3.3
Operations on Sets
The Venn-Euler diagram‡ is a useful visual representation of sets. In such a
diagram sets are represented as regions in the plane and elements which belong
to a given set are placed inside the region representing it. Frequently all the sets
in the diagram are placed inside a box which represents the universal set. If an
element belongs to more than one set in the diagram, the two regions representing
the sets concerned must overlap and the element is placed in the overlapping
region.
In this way the picture represents the relationships between the sets
concerned.
For example, if A ⊆B the region representing A may be enclosed inside the
region representing B to ensure that every element in the region representing A is
also inside that representing B; see ﬁgure 3.1.
† Bertrand Russell, celebrated mathematician, logician, philosopher, politician, peace campaigner,
Nobel laureate, etc!, communicated this paradox to his fellow mathematician Frege in 1902 just as
Frege had completed a major work in set theory. This and other paradoxes which struck at the heart
of set theory created turmoil in the foundations of mathematics at the time.
‡ These diagrams are more commonly called just ‘Venn diagrams’ after John Venn, the nineteenth-
century English mathematician. In fact, diagrams such as ﬁgure 3.1 are more properly called ‘Euler
diagrams’ after Leonhard Euler who ﬁrst introduced them in 1761. Although both Venn and Euler
had precise rules for constructing their diagrams, today the term ‘Venn diagram’ is used informally to
denote any diagram that represents sets by regions in the plane.

92
Sets
Figure 3.1
Example 3.7
The sets A = {Ann, Alan, Fred, Jack, Mark, Mary, Ruth}
B = {Ann, Janet, Margaret, Mary, Ruth}
C = {Margaret, Mark, Mary, Matthew, Molly}
can be represented by the Venn-Euler diagram shown in ﬁgure 3.2.
Figure 3.2
Given sets A and B we can deﬁne two new sets as follows.

Operations on Sets
93
The intersection of A and B is the set of all elements which belong both to A
and B—it is denoted A ∩B.
The union of A and B is the set of all elements which belong to A or to B or
to both—it is denoted A ∪B.
Symbolically:
A ∩B = {x : x ∈A and x ∈B}
A ∪B = {x : x ∈A or x ∈B or both}.
There are obvious connections between intersection of sets and conjunction
of propositions, and between union of sets and (inclusive) disjunction of
propositions. If A and B are deﬁned by propositional functions P(x) and Q(x)
respectively, then
A ∩B = {x : P(x) ∧Q(x)}
and
A ∪B = {x : P(x) ∨Q(x)}.
These sets can best be visualized by the following Venn-Euler diagrams
(ﬁgures 3.3 and 3.4 respectively) where the regions representing intersection and
union are shaded.
Figure 3.3
Clearly we can extend the deﬁnitions of intersection and union to more than two
sets. Let A1, A2, . . . , An be sets.
Their intersection is:
n
\
r=1
Ar = A1 ∩A2 ∩· · · ∩An
= {x : x ∈A1 and x ∈A2 and . . . and x ∈An}
= {x : x belongs to each set Ar, for r = 1, 2, . . . , n}.

94
Sets
Figure 3.4
Their union is:
n
[
r=1
Ar = A1 ∪A2 ∪· · · ∪An
= {x : x ∈A1 or x ∈A2 or .. . or x ∈An}
= {x : x belongs to at least one set Ar, r = 1, . . . , n}.
Sets A and B are said to be disjoint if they have no elements in common; that is,
if A ∩B = ∅. In a Venn-Euler diagram this may be represented by drawing the
regions representing the two sets to be non-overlapping, as in ﬁgure 3.5.
Figure 3.5
Given a set A, another set we can deﬁne is its complement which consists of
all those elements in U which do not belong to A. The complement of A is
denoted ¯A (or A′ or Ac).
Of course, it is important that a universal set has
already been deﬁned; otherwise the complement will not be a well deﬁned set.
There is an obvious connection between complement and negation; namely, if
A = {x : P(x)} then ¯A = {x : ¬P(x)}. The Venn-Euler diagram shown in
ﬁgure 3.6 illustrates the complement.
Related to the complement of a set is the difference or relative complement of

Operations on Sets
95
Figure 3.6
two sets A and B, denoted A −B or A \ B. This set contains all the elements of
A which do not belong to B:
A −B = {x : x ∈A and x /∈B} = A ∩¯B.
Note that the complement of A is given by ¯A = U −A. The difference A −B is
illustrated in ﬁgure 3.7.
Figure 3.7
Examples 3.8
1.
Let U = {1, 2, 3, . . . , 10} = {n : n ∈Z+ and n ⩽10},
A = {n ∈U : 1 ⩽n < 7},
B = {n ∈U : n is a multiple of 3}.
Then A = {1, 2, 3, 4, 5, 6} and B = {3, 6, 9}.

96
Sets
Therefore:
A ∩B = {3, 6}
A ∪B = {1, 2, 3, 4, 5, 6, 9}
A −B = {1, 2, 4, 5}
B −A = {9}
¯A = {7, 8, 9, 10}
¯B = {1, 2, 4, 5, 7, 8, 10}
A ∪B = {7, 8, 10} = ¯A ∩¯B
A ∩B = {1, 2, 4, 5, 7, 8, 9, 10} = ¯A ∪¯B
A −B = {3, 6, 7, 8, 9, 10} = ¯A ∪B.
2.
(i)
For each of the following, draw a Venn-Euler diagram and shade
the region corresponding to the indicated set.
(a)
A −(B ∩C)
(b)
(A −B) ∪(A −C).
(ii)
Show that A −(B ∩C) = (A −B) ∪(A −C) for all sets A, B
and C.
Solution
(i)
(a)
The region representing A −(B ∩C) is that part of A that lies
outside B ∩C. This is represented by the following diagram.
(b)
In the following diagram, the regions representing A−B and A−C
are shaded differently. Then (A−B)∪(A−C) is the region which
has either shading.

Operations on Sets
97
(ii)
In the diagrams above, the region representing A −(B ∩C) in part (a)
is the same as that representing (A −B) ∪(A −C) in part (b). This
suggests that the two sets are equal. However, a pair of diagrams does not
constitute a proof, so we now prove this using the technique suggested by
theorem 3.1.
Let A, B and C be sets.
First we show A −(B ∩C) ⊆(A −B) ∪(A −C).
Let x ∈A −(B ∩C). Then x ∈A and x /∈B ∩C. Hence x ∈A
and either x /∈B or x /∈C (or both). Therefore either x ∈A and
x /∈B or x ∈A and x /∈C (or both). It follows that x ∈A −B or
x ∈A −C (or both). Hence x ∈(A −B) ∪(A −C). We have shown
that if x ∈A −(B ∩C) then x ∈(A −B) ∪(A −C). Therefore
A −(B ∩C) ⊆(A −B) ∪(A −C).
Secondly we must show that (A −B) ∪(A −C) ⊆A −(B ∩C).
Let x ∈(A −B) ∪(A −C). Then x ∈A −B or x ∈A −C (or both)
so x ∈A and x /∈B or x ∈A and x /∈C (or both). Hence x ∈A and
either x /∈B or x /∈C (or both) which implies x ∈A and x /∈B ∩C.
Therefore x ∈A−(B∩C). We have shown that if x ∈(A−B)∪(A−C)
then x ∈A −(B ∩C). Therefore (A −B) ∪(A −C) ⊆A −(B ∩C).
Finally, since we have shown that each set is a subset of the other, we
may conclude (A −B) ∪(A −C) = A −(B ∩C).
Exercises 3.3
1.
Draw Venn-Euler diagrams and shade the regions representing each of
the following sets:
(i)
¯A ∩B
(ii)
¯A ∪B
(iii)
(A ∩B) ∪(A ∪B)
(iv)
A ∩(B ∪C)
(v)
A ∪(B ∩C)
(vi)
(A ∩B) −C
(vii)
A −(B ∩C)
(viii) (A ∪B) −C
(ix)
A −(B ∪C)

98
Sets
(x)
(A −B) ∩(A −C).
2.
Let U = {n : n ∈N ∧n < 10}, A = {2, 4, 6, 8}, B = {2, 3, 5, 7},
C = {1, 4, 9}. Deﬁne (for example, by listing elements) each of the
following sets.
(i)
A ∩B
(vi)
A ∩(B ∪C)
(ii)
A ∪B
(vii)
¯B ∪B
(iii)
A −B
(viii)
¯B ∩B
(iv)
B ∩C
(ix)
A ∪C
(v)
¯A ∩B
(x)
(A −C) −B.
3.
Consider the sets A, B, C, D and E represented by the following Venn-
Euler diagram. (The sets C and E are represented by shaded regions.)
For each of the following pairs of sets X and Y , state whether X ⊆Y ,
Y ⊆X, X ∩Y = ∅or none of these.
(i)
X = A ∪B
Y = C
(ii)
X = A ∩B
Y = D
(iii)
X = A ∩B
Y = C
(iv)
X = E
Y = A ∩D
(v)
X = B ∩C
Y = C ∪D
(vi)
X = A ∩E
Y = D ∪E
(vii)
X = C ∪E
Y = A ∪D
(viii)
X = C −B
Y = D ∪E
(ix)
X = A ∪D
Y = B ∩E
(x)
X = A −E
Y = A −D.
4.
Let U = {1, 2, 3, . . . , 9, 10} and deﬁne sets A, B, C and D as follows.
A = {2, 4, 6, 8, 10}
B = {3, 4, 5, 6}
C = {7, 8, 9, 10}
D = {1, 3, 5, 7, 9}.
List the elements of each of the following sets.

Operations on Sets
99
(i)
A ∪B
(vii)
¯B ∩¯C
(ii)
A ∩D
(viii)
A −(B ∩¯C)
(iii)
B ∪C
(ix)
(A −B) ∪(D −C)
(iv)
A ∩(B ∪D)
(x)
D −C
(v)
B ∪( ¯A ∩¯D)
(xi)
( ¯A ∪¯B) −(A ∪B)
(vi)
(C ∩D) ∪B
(xii)
( ¯C −A) ∩(A −¯C).
5.
Let U = {n ∈Z : 1 ⩽n ⩽12}, A = {n : n is a divisor of 12},
B = {n : n is a prime number} and C = {n : n is odd}. (Recall that 1
is not a prime number.)
(i)
Describe in words each of the following sets:
(a)
A ∩B
(b)
A ∩B ∩C
(c)
B ∩¯C
(d)
A −C.
(ii)
List the elements of each of the following sets:
(a)
A ∪B
(b)
B ∩C
(c)
A ∪C
(d)
C −A
(e)
A ∩B.
6.
Given that A = {x : P(x)}
B = {x : Q(x)}
C = {x : R(x)}
deﬁne each of the following sets in terms of P(x), Q(x) and R(x) (and
logical connectives):
(i)
A ∩¯B
(ii)
A ∪B
(iii)
A ∩(B ∪¯C)
(iv)
A −B
(v)
A −(B ∪C)
(vi)
¯A −¯B.
7.
For each of the following, draw two Venn-Euler diagrams.
On one
diagram shade the region represented by the set on the left-hand side of
the equality and on the other diagram shade the region represented by the
set on the right-hand side of the equality. Then prove that the identity for
all sets A, B and C.
(i)
A −B = A −(A ∩B)

100
Sets
(ii)
A ∩(B −C) = (A ∩B) −C
(iii)
(A ∪B) −C = (A −C) ∪(B −C)
(iv)
A ∪(B −C) = (A ∪B) −( ¯A ∩C)
(v)
(A −B) −C = A −(B ∪C).
3.4
Counting Techniques
Some quite complex mathematical results rely for their proofs on counting
arguments: counting the numbers of elements of various sets, the number of ways
in which a certain outcome can be achieved, etc. Although counting may appear
to be a rather elementary exercise, in practice it can be extremely complex and
rather subtle. Mathematicians have devised a number of techniques and results to
deal with counting problems in a branch of the subject called enumeration theory.
One of the simplest counting results is the following, which says that to count
the total number of elements of two disjoint sets A and B, we simply count the
elements of A, count the elements of B and add them.
Counting Principle 1
If A and B are disjoint sets, then
|A ∪B| = |A| + |B|.
In many applications, of course, more than two sets are involved. The above
principle easily generalizes to the following, which can be proved formally using
mathematical induction (see chapter 2).
Counting Principle 2
If A1, A2, . . . , An are sets, no pair of which have elements in common,
then
|A1 ∪A2 ∪· · · ∪An| = |A1| + |A2| + · · · + |An|.

Counting Techniques
101
Frequently, the sets whose elements are to be counted will not satisfy the rather
stringent condition of the counting principles above—that any pair of them be
disjoint.
However, in these situations it is often possible to divide the set
under consideration into subsets which do satisfy the conditions of the counting
principles. One of the simplest results which can be proved in this way is the
following.
Theorem 3.2 (The inclusion–exclusion principle)
If A and B are ﬁnite sets then
|A ∪B| = |A| + |B| −|A ∩B|.
Proof
We can divide A ∪B into its subsets A −B, A ∩B and B −A which satisfy the
condition of counting principle 2; see ﬁgure 3.8.
Figure 3.8
Therefore, by counting principle 2,
|A ∪B| = |A −B| + |A ∩B| + |B −A|.
(1)
The sets A and B can themselves be split into disjoint subsets A −B, A ∩B and
B −A, A ∩B respectively. Thus
|A| = |A −B| + |A ∩B|
(2)
and
|B| = |B −A| + |A ∩B|.
(3)
It is now a simple exercise to combine equations (1), (2) and (3) to produce the
desired result.
□

102
Sets
The inclusion–exclusion principle is so called because to count the elements of
A ∪B we could have added the number of elements of A and the number of
elements of B, in which case we have included the elements of A ∩B twice:
once as elements of A and once as elements of B. To obtain the correct number
of elements in A∪B, we would then need to exclude those in A∩B once, so that
overall they are just counted once.
There are corresponding identities for more than two sets. The result for three
sets is theorem 3.3, the proof of which we leave as an exercise.
Theorem 3.3
If A, B and C are ﬁnite sets, then
|A ∪B ∪C| = |A| + |B| + |C| −|A ∩B| −|B ∩C|
−|C ∩A| + |A ∩B ∩C|.
Example 3.9
Each of the 100 students in the ﬁrst year of Utopia University’s Computer
Science Department studies at least one of the subsidiary subjects: mathematics,
electronics and accounting.
Given that 65 study mathematics, 45 study
electronics, 42 study accounting, 20 study mathematics and electronics, 25 study
mathematics and accounting, and 15 study electronics and accounting, ﬁnd the
number who study:
(i)
all three subsidiary subjects;
(ii)
mathematics and electronics but not accounting;
(iii)
only electronics as a subsidiary subject.
Solution
Let U = {students in the ﬁrst year of Utopia’s Computer Science Department}
M = {students studying mathematics}
E = {students studying electronics}
A = {students studying accounting}.
We are given the following information: |U | = 100, |M| = 65, |E| = 45,
|A| = 42, |M ∩E| = 20, |M ∩A| = 25, |E ∩A| = 15. Also, since every student
takes at least one of three subjects as a subsidiary, U = M ∪E ∪A.

Counting Techniques
103
Let |M ∩E ∩A| = x. Figure 3.9 shows the cardinalities of the various disjoint
subsets of U . These are calculated as follows, beginning with the innermost
region representing M ∩E ∩A and working outwards in stages.
Figure 3.9
By Counting Principle 1,
|M ∩A| = |M ∩A ∩E| + |(M ∩A) −E|
so
|(M ∩A) −E| = |M ∩A| −|M ∩A ∩E| = 25 −x.
Similarly
|(M ∩E) −A| = |M ∩E| −|M ∩E ∩A| = 20 −x
and
|(A ∩E) −M| = |A ∩E| −|M ∩E ∩A| = 15 −x.
Now consider set M. By Counting Principle 2,
|M| = |M −(A ∪E)| + |(M ∩A) −E| + |(M ∩E) −A| + |M ∩E ∩A|
so
|M −(A ∪E)| = |M| −|(M ∩A) −E| −|(M ∩E) −A| −|M ∩E ∩A|
= 65 −(25 −x) −(20 −x) −x
= 20 + x.
Similarly
|A −(M ∪E)| = |A| −|(A ∩M) −E| −|(A ∩E) −M| −|M ∩E ∩A|
= 42 −(25 −x) −(15 −x) + x
= 2 + x

104
Sets
and
|E −(M ∪A)| = |E| −|(E ∩M) −A| −|(E ∩A) −M| −|M ∩E ∩A|
= 45 −(20 −x) −(15 −x) + x
= 10 + x.
Now, using Counting Principle 2 again, |M ∪A ∪E| = 100 is the sum of the
cardinalities of its seven disjoint subsets, so:
100 = (20 + x) + (2 + x) + (10 + x) + (25 −x)
+ (20 −x) + (15 −x) + x
100 = 92 + x
⇒
x = 8.
⇒
We could now re-draw ﬁgure 3.9 showing the cardinality of each disjoint subset
of M ∪A ∪E. However, this is not necessary to answer the three parts of the
question.
(i)
Eight students study all three subsidiary subjects.
(ii)
The number of students who study mathematics and electronics but not
accounting is |(M ∩E) −A| = 20 −x = 20 −8 = 12.
(iii)
The number of students who study only electronics as a subsidiary subject
is |E −(M ∪A)| = 10 + x = 10 + 8 = 18.
3.5
The Algebra of Sets
From example 3.8.1 and exercise 3.3.7 above, it is clear that the intersection,
union, complement (and hence difference) operations on sets are related to one
another. For instance,
A ∩B = ¯A ∪¯B
for the sets deﬁned in example 3.8.1. In fact, this equation holds for all sets.
We give below the basic identities connecting the operations of intersection,
union and complement. Compare these with the laws for propositions given in
§1.5. Given the connection between operations on sets and logical connectives
between propositions, each of the set theory laws listed below can be derived
from the corresponding logical equivalence between compound propositions. The
following laws hold for all sets A, B and C.

The Algebra of Sets
105
Idempotent laws
A ∩A = A
A ∪A = A.
Commutative laws
A ∩B = B ∩A
A ∪B = B ∪A.
Associative laws
A ∩(B ∩C) = (A ∩B) ∩C
A ∪(B ∪C) = (A ∪B) ∪C.
Absorption laws
A ∩(A ∪B) = A
A ∪(A ∩B) = A.
Distributive laws
A ∩(B ∪C) = (A ∩B) ∪(A ∩C)
A ∪(B ∩C) = (A ∪B) ∩(A ∪C).
Involution law
¯¯A = A.
De Morgan’s laws
(A ∪B) = ¯A ∩¯B
(A ∩B) = ¯A ∪¯B.

106
Sets
Identity laws
A ∪∅= A
A ∩U = A
A ∪U = U
A ∩∅= ∅.
Complement laws
A ∪¯A = U
A ∩¯A = ∅
¯∅= U
¯
U = ∅.
Although these laws can be derived from the corresponding equivalences between
propositions, they are probably best illustrated using Venn-Euler diagrams. For
example, the second of the distributive laws is illustrated by the Venn-Euler
diagram in ﬁgure 3.10. The Venn-Euler diagram of ﬁgure 3.10(a) shows the set
A ∪(B ∩C). In ﬁgure 3.10(b), the two sets A ∪B and A ∪C are shaded
differently, so the double shading represents their intersection (A∪B)∩(A∪C).
The regions shaded in ﬁgure 3.10(a) and doubly shaded in ﬁgure 3.10(b) are the
same, indicating that the two sets are equal.
The other laws may be illustrated similarly. For example, ﬁgure 3.11 explains the
ﬁrst of De Morgan’s laws.
In ﬁgure 3.11(a) the complement of A ∪B is shaded and in ﬁgure 3.11(b) ¯A and
¯B are shaded, the double shading representing ¯A ∩¯B. The double-shaded area in
(b) is the same as the shaded area in (a) indicating that the two sets represented
are equal.
The Duality Principle
Just as a compound proposition involving the connectives ∧, ∨and negation has
a dual proposition, so, too, does a statement about sets which involves ∩, ∪and
complement. The dual of such a statement is obtained by interchanging ∩and
∪everywhere and interchanging ∅and U everywhere in the original statement.
For example, the dual of
(A ∩∅) ∪(B ∩U ) ∪¯B = U

The Algebra of Sets
107
Figure 3.10
is
(A ∪U ) ∩(B ∪∅) ∩¯B = ∅.
For each of the laws of the algebra of sets, its dual is also a law. This suggests the
following duality principle for sets which, although not a mathematical theorem,
is extremely useful.

108
Sets
Figure 3.11
Duality Principle for Sets
If a statement about sets is true for all sets then its dual statement is
necessarily true for all sets also.
Exercises 3.4
1.
For each of the following set identities, draw a pair of Venn-Euler
diagrams (as in ﬁgures 3.10 and 3.11) to illustrate the identity.
(i)
A ∩(B ∪C) = (A ∩B) ∪(A ∩C)
(ii)
A −B = B ∪¯A
(iii)
A ∩B = ¯A ∪¯B
(iv)
(A −B) ∩C = (A ∩C) −B
(v)
(A −B) ∪(B −A) = (A ∪B) −(B ∩A).
2.
For each of the following nine sets, draw a Venn-Euler diagram and shade
the region corresponding to the set. Use your diagrams to identify which
sets are equal.
(i)
A ∪B
(iv)
A −(A ∩B)
(vii)
(A −B) ∪(B −A)
(ii)
A −B
(v)
A ∩¯B
(viii)
(A ∪B) −(A ∩B)
(iii)
A ∪B
(vi)
B ∪¯A
(ix)
A ∩B.

The Algebra of Sets
109
3.
For each of the following four sets, draw a Venn-Euler diagram and shade
the region corresponding to the set. Use your diagrams to identify which
sets are equal.
(i)
(A ∩B) ∪(A ∩C)
(iii)
(A ∩C) −B
(ii)
(A −B) ∩C
(iv)
A ∩(B ∪C).
4.
The laws for the algebra of sets can sometimes be used to give proofs
of set identities that are simpler than showing each set is a subset of the
other (the method used in example 3.8.2(ii)). For example, the following
is an alternative proof of the result in example 3.8.2(ii).
Proof
For all sets A, B and C:
A −(B ∩C) = A ∩(B ∩C)
(deﬁnition of difference)
= A ∩( ¯B ∪¯C)
(De Morgan’s law)
= (A ∩¯B) ∪(A ∩¯C)
(distributive law)
= (A −B) ∪(B −C)
(deﬁnition of difference).
In a similar way, prove each of the identities given in exercises 3.3.7.
5.
The symmetric difference A ∗B of sets A and B is deﬁned by:
A ∗B = (A −B) ∪(B −A).
(i)
Using the laws for the algebra of sets, show that, for every set A,
A ∗∅= A and A ∗A = ∅.
(ii)
Draw Venn-Euler diagrams to illustrate the identity
A ∩(B ∗C) = (A ∩B) ∗(A ∩C).
(This is called the distributive law: we say that intersection is
distributive over symmetric difference.)
(iii)
Find a counter-example to the proposition that, for all sets A, B
and C,
A ∪(B ∗C) = (A ∪B) ∗(A ∪C).
(This shows that union is not distributive over symmetric
difference.)
6.
(i)
Find sets A and B such that A ∈B and A ⊆B.

110
Sets
(ii)
Find sets A, B and C such that (A ∈B and B ⊆C) and (A ⊆B
and B ∈C).
7.
Write down the dual of each of the following statements:
(i)
¯A ∩¯B = (A ∪B)
(ii)
A ∪(B ∩U ) = (A ∪∅) ∪B
(iii)
A ∩B = ∅.
Note that if statement (iii) is true its dual may not also be true. Explain
why this fact does not violate the principle of duality.
8.
Use theorem 3.2, the counting principles and the algebra of sets to prove
theorem 3.3.
9.
Given that |A| = 55, |B| = 40, |C| = 80, |A∩B| = 20, |A∩B∩C| = 17,
|B ∩C| = 24, and |A ∪C| = 100, ﬁnd:
(i)
|A ∩C|
(ii)
|C −B|
(iii)
|(B ∩C) −(A ∩B ∩C)|.
Draw a Venn-Euler diagram and mark on it the cardinalities of the sets
corresponding to each region of the diagram.
If |U | = 150 ﬁnd |A ∪B ∪C|.
10.
In a survey of 1000 households, 275 owned a home computer, 455 a
video, 405 two cars, and 265 households owned neither a home computer,
nor a video, nor two cars. Given that 145 households owned both a home
computer and a video, 195 both a video and two cars, and 110 both two
cars and a home computer, ﬁnd the number of households surveyed which
owned:
(i)
a home computer, a video and two cars;
(ii)
a video only;
(iii)
two cars, a video but not a home computer;
(iv)
a video, a home computer but not two cars.
11.
In a certain village, there are three sports clubs: the soccer club, the rugby
club and the cricket club.
Everyone who belongs to the cricket club
also belongs to the soccer club or rugby club (or both). The following
additional information is known:
42 people belong to the soccer club;
45 people belong to the rugby club;

Families of Sets
111
7 people belong to both the soccer and rugby clubs;
11 people belong to both the soccer and cricket clubs;
28 people belong to both the rugby and cricket clubs;
twice as many people belong only to the soccer club as belong only to
the rugby club.
Find the number of people in the village who belong to
(i)
all three clubs
(ii)
the cricket club
(iii)
only the soccer club.
3.6
Families of Sets
In section 3.3, we deﬁned the intersection and union of a collection of n sets as
follows:
n
\
r=1
Ar = A1 ∩A2 ∩· · · ∩An = {x : x ∈Ar for each r = 1, 2, . . . , n}
and
n
[
r=1
Ar = A1 ∪A2 ∪· · · ∪An = {x : x ∈Ar for some r = 1, 2, . . . , n}.
In this section we turn our attention to more general ‘families’ or ‘collections’ of
sets which will include the case where there are inﬁnitely many sets in the family.
By a family or collection of sets, we really mean a set of sets, although the terms
‘family of sets’ or ‘collection of sets’ are both in widespread use and we shall use
the three terms interchangeably. Before we can consider intersections and unions
of arbitrary families of sets, we need ﬁrst to describe carefully what we mean by
such a family.
In the examples above, we have deﬁned the intersection and union of the
family (or set) of sets {A1, A2, . . . , An}. In this family, the integers 1, 2, . . . , n
serve as labels to distinguish the various sets in the collection.
In principle,
any collection of labels would be suitable;
for example, if we were to
choose Alice, Bob, . . . , Nina as labels, then we could write the family as
{AAlice, ABob, . . . , ANina}. In practice, the labels 1, 2, . . . , n are usually preferable.
Whatever labels we choose form an indexing set or labelling set I for the

112
Sets
collection.
For the collection {A1, A2, . . . , An}, the indexing set is I
=
{1, 2, . . . , n} and we can write the family as
{Ai : i ∈I} = {A1, A2, . . . , An}.
Using this idea of indexing set, we can deﬁne more general families of sets.
For example, any collection of sets that has Z+ as the indexing set will contain
inﬁnitely many sets, one corresponding to each positive integer:
{Ar : r ∈Z+} = {A1, A2, A3, . . .}.
If the set of real numbers R is the indexing set then the resulting family of sets
{Ar : r ∈R} also contains inﬁnitely many sets, but this time we cannot list them
even in an inﬁnite list (see §5.5 for further details of the quantitative difference
between the inﬁnite sets Z and R).
An arbitrary family of sets is of the form F = {Ai : i ∈I} where I is any
(indexing) set; in such a collection F, there is exactly one set Ar for each element
r of the indexing set I. Recall that the indexing set is just a collection of labels for
the sets in the family F. It is now straightforward to modify the deﬁnition given
at the beginning of the section and deﬁne the intersection and union of the family
F as follows:
\
i∈I
Ai = {x : x ∈Ai for all i ∈I}
[
i∈I
Ai = {x : x ∈Ai for some i ∈I}.
Examples 3.10
1.
The deﬁnitions given above for intersection and union of arbitrary
families of sets include as special cases our previous deﬁnitions for ﬁnite
collections of sets. For example, let I = {1, 2}. A corresponding family
of sets is {A1, A2}. Now
\
i∈I
Ai = {x : x ∈Ai for i = 1 and i = 2} = {x : x ∈A1 and x ∈A2}
= A1 ∩A2
[
i∈I
Ai = {x : x ∈Ai for i = 1 or i = 2} = {x : x ∈A1 or x ∈A2}
= A1 ∪A2.
So the deﬁnitions above agree with our previous deﬁnitions of
intersection and union of two sets.

Families of Sets
113
2.
Let I = Z+ = {1, 2, 3, . . .}, and for each i ∈Z+ let Ai = {i}. Thus
A1 = {1}, A2 = {2}, etc. Therefore
\
i∈Z+
Ai = ∅
and
[
i∈Z+
Ai = {1, 2, 3, . . .} = Z+.
When the indexing set is Z+ we frequently write
∞
\
i=1
Ai
for
\
i∈Z+
Ai
and
∞
[
i=1
Ai
for
[
i∈Z+
Ai.
3.
Let I = Z+ and for each n ∈Z+ let An = {k ∈Z : k ⩽n}. Thus:
A1 = {k ∈Z : k ⩽1} = {. . . , −3, −2, −1, 0, 1}
A2 = {k ∈Z : k ⩽2} = {. . . , −3, −2, −1, 0, 1, 2}
A3 = {k ∈Z : k ⩽3} = {. . . , −3, −2, −1, 0, 1, 2, 3}, etc.
Then
∞
\
n=1
An = {k ∈Z : k ⩽n for all n ∈Z+} = {k ∈Z : k ⩽1} = A1.
Note that the family satisﬁes A1 ⊆A2 ⊆A3 ⊆· · · ⊆An ⊆An+1 ⊆
· · · . Whenever this is the case, we have
∞
T
n=1
An = A1.
Now
∞
[
n=1
An = {k ∈Z : k ⩽n for some n ∈Z+}.
Note that every integer k satisﬁes k ⩽n for some n ∈Z+: if k > 0
we may take n = k, and if k ⩽0 we may take n = 1. Therefore every
integer k belongs to the union of the family so
∞
[
n=1
An = Z.
4.
Let I = R, the set of real numbers, and for each m ∈I let Am be the set
of points in the plane which lie on the line of gradient m which passes
through the origin (0, 0)—ﬁgure 3.12. That is,
Am = {(x, y) : x and y are real numbers and y = mx}.

114
Sets
Figure 3.12
Note that in this case we cannot list the sets in the family {Am : m ∈R},
even in an inﬁnite list.
This is because the real numbers themselves
cannot be listed in an inﬁnite list x1, x2, x3, . . . (see §5.5).
Then
\
m∈I
Am = {(0, 0)}
since the origin (0, 0) is the unique point common to all such lines.
The union
[
m∈I
Am
is the whole plane except the positive and negative parts of the y-axis.
Points on the y-axis (except the origin) do not occur in the union because
none of the lines Am are vertical. The union of the sets Am can also be
deﬁned by
{(x, y) : x and y are real numbers and x ̸= 0} ∪{(0, 0)}.
Power Set
Given any set A we can deﬁne the set consisting of all subsets of A. Called the
‘power set of A’, this is almost certainly the most widely used and important
example of a family of sets.

Families of Sets
115
Deﬁnition 3.3
Let A be any set. The power set of A, denoted P(A), is the set of all
subsets of A:
P(A) = {B : B ⊆A}.
Notice that the power set of any set A contains ∅and A since both are subsets of
A. In particular the power set is necessarily non-empty.
The following theorem shows how the power set is related to subset, intersection
and union.
Theorem 3.4
For all sets A and B:
(i)
A ⊆B if and only if P(A) ⊆P(B).
(ii)
P(A) ∩P(B) = P(A ∩B).
(iii)
P(A) ∪P(B) ⊆P(A ∪B).
Proof
We shall prove part (i) as an illustration; the proofs of parts (ii) and (iii) are left as
exercises.
To prove the biconditional statement we prove the two conditional statements:
A ⊆B ⇒P(A) ⊆P(B)
and
P(A) ⊆P(B) ⇒A ⊆B.
Firstly, suppose A ⊆B. We must show that P(A) ⊆P(B), so let X ∈P(A).
This means X ⊆A. Since A ⊆B, it follows from exercise 3.2.10(i) that X ⊆B,
which means that X ∈P(B). Since X ∈P(A) implies X ∈P(B), we
conclude that P(A) ⊆P(B), which completes the ﬁrst half of the proof.
To prove the converse statement, suppose P(A) ⊆P(B). Since A ∈P(A), it
follows that A ∈P(B). This means that A ⊆B, which completes the proof. □

116
Sets
Examples 3.11
1.
P(∅) = {∅}
P{a} = {∅, {a}}
P{a, b} = {∅, {a}, {b}, {a, b}}
P{a, b, c} = {∅, {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, {a, b, c}}.
2.
Let A = {1, 2, 3} and B = {1, 2}. Determine whether each of the
following is true or false and give a brief justiﬁcation.
(i)
B ∈P(A)
(ii)
B ∈A
(iii)
A ∈P(A)
(iv)
A ⊆P(A)
(v)
B ⊆P(A)
(vi)
{{1}, B} ⊆P(A)
(vii)
∅∈P(A)
(viii) ∅⊆P(A).
Solution
(i)
True: B is a subset of A so B is an element of its power set.
(ii)
False: B is a set but the elements of A are numbers, so B is not an element
of A.
(iii)
True: since A ⊆A it follows that A ∈P(A). In fact, as noted above,
this is the case for any set A.
(iv)
False: the elements of A are numbers whereas the elements of P(A)
are sets (namely subsets of A). Hence the elements of A cannot also be
elements of P(A), so A ̸⊆P(A).
(v)
False: for the same reasons as given in part (iv).
(vi)
True: {1} ∈P(A) (since {1} ⊆A) and B ∈P(A) (part (i)) so
each element of the set {{1}, B} is also an element of P(A); hence
{{1}, B} ⊆P(A).
(vii)
True: since ∅⊆A, we have ∅∈P(A).
(viii)
True: ∅⊆X for every set X and P(A) is certainly a set, so ∅⊆P(A).
3.
Again we emphasize that great care should be taken to use ∈and ⊆
correctly. For instance, if a ∈A then {a} ⊆A so {a} ∈P(A). There is
particular scope for confusion when x and {x} are both elements of a set
X.
Let A = {1, 2, {1}}.
Then 1 ∈A so {1} ⊆A, and therefore
{1} ∈P(A). In this case {1} ∈A as well, so {{1}} ∈P(A). In

Families of Sets
117
fact
P(A) = {∅, {1}, {2}, {{1}}, {1, 2}, {1, {1}}, {2, {1}}, A}.
Recall that 1, {1}, {{1}} are all different. The ﬁrst is a number, the
second a set whose only element is a number, and the third a set whose
only element is a set. Clearly we could continue in this way to produce
an inﬁnite sequence of different sets:
{1}, {{1}}, {{{1}}}, . . . .
Each set in this sequence (except the ﬁrst) could be deﬁned as the set
whose single element is the previous set in the sequence. More precisely,
if we deﬁne
X1 = {1}
and
Xn+1 = {Xn}
for n = 1, 2, 3, . . .
then the sequence X1, X2, X3, . . . is identical with the sequence of sets
above. As a ﬁnal step, let
X =
[
n∈Z+
Xn =
∞
[
n=1
Xn.
Note that X is a well deﬁned set: given x we can decide deﬁnitely
whether x ∈X or x /∈X. If x is of the form {· · · {1} · · · }, where a
ﬁnite number of braces appear, then x ∈X; otherwise x /∈X.
We could deﬁne this union X directly as follows:
X = {x : x = {1} or x = {y} where y ∈X}.
This is an example of a recursively deﬁned set—that is, one deﬁned
partially in terms of itself. Of course, we cannot deﬁne a set completely
in terms of itself, which is why we also need x = {1} as part of the
deﬁnition.
The idea of recursion—deﬁning something partially in terms of itself—is
important in mathematics and computer science, both theoretically and
practically. In computing, for example, many high-level programming
languages allow procedures to call themselves—such procedures are
called recursive.
The sets given in example 3.11.1 above suggest that, if A is ﬁnite and |A| = n
then |P(A)| = 2n. To prove this let A be the set {a1, a2, . . . , an}. We can form a
subset of A by considering each element ai in turn and either including it or not in

118
Sets
the subset. For each element there are two choices (either include it or don’t) and
the choice for each element is independent of the choices for the other elements,
so there are 2n choices altogether. Each of these 2n choices gives a different
subset and every subset of A can be obtained in this way. We have proved the
following theorem (which can also be proved by mathematical induction).
Theorem 3.5
If |A| = n then |P(A)| = 2n.
Some authors use 2A to denote the power set: then theorem 3.5 takes the elegant
form |2A| = 2|A|.
Partitions of a Set
It is sometimes important to divide a set into non-intersecting subsets.
For
instance, in §3.4, this device was frequently used when counting elements of sets.
Such a division of a set into non-intersecting subsets is called a ‘partition’ of the
set. It is closely related to the important notion of an equivalence relation on a set,
which is introduced in the next chapter.
Deﬁnition 3.4
Let A be a set. A partition of A is a family (i.e. a set) {Si : i ∈I} of
non-empty subsets of A such that:
(i)
S
i∈I
Si = A, and
(ii)
Si ∩Sj = ∅if i ̸= j, for all i, j ∈I.
The ﬁrst condition says that the sets Si in the family ‘ﬁll out’ all of A. The
second condition says that, for any pair of sets Si, Sj in the partition, these sets
are disjoint. Whenever the second condition is satisﬁed we say that the sets Si,
i ∈I, are pairwise disjoint. It is useful to visualize the elements Si of the
partition as non-overlapping ‘blocks’ which ﬁt together to form A rather like the
pieces of a jigsaw puzzle—see ﬁgure 3.13. Using this analogy, the ﬁrst condition

Families of Sets
119
of deﬁnition 3.4 says that there are no missing pieces to the jigsaw puzzle and
the second condition says that the pieces ﬁt together ‘snugly’ with no overlaps
between pieces. Clearly these are exactly the properties required of the pieces of
a jigsaw puzzle.
Figure 3.13
Perhaps it is worth pointing out that pairwise disjoint is a stronger condition than
requiring the intersection of the whole family to be the empty set. For example,
if A = {1, 2}, B = {2, 3} and C = {3, 4} then the family {A, B, C} is not
pairwise disjoint since A∩B ̸= ∅, for example. However, A∩B ∩C = ∅, since
there is no element common to all three sets.
Examples 3.12
1.
{{1}, {2, 3}, {4, 5, 6}} is a partition of {1, 2, 3, 4, 5, 6}.
2.
Each of the following is a partition of Z, the set of integers.
(i)
{Z−, {0}, Z+}, where Z−and Z+ are the sets of negative and
positive integers respectively.
(ii)
{E, O}, where E = {. . . , −4, −2, 0, 2, 4, 6, . . .}, the set of even
integers, and O = {. . . , −3, −1, 1, 3, 5, 7, . . .}, the set of odd
integers.
(iii)
{{n} : n ∈Z}.
Clearly, for any set A we can form a partition in this way by taking the
sets in the partition to be all the singleton subsets of A. (A singleton set
is simply a set with only one element.)
3.
For each real number α, let Lα be the set of points in the plane which lie
on the vertical line through the point (α, 0):
Lα = {(x, y) : x = α and y is a real number}

120
Sets
= {(α, y) : y ∈R}.
The family of these sets, {Lα : α ∈R}, is a partition of the plane: every
point of the plane lies on one of the lines Lα and any two of the lines are
disjoint.
Figure 3.14
Exercises 3.5
1.
List the elements of P(A) in the following cases:
(i)
A = {a, b, c, d}
(ii)
A = {{1}, {1, 2}}
(iii)
A = {{1}, {1, 2}, {1, 2, 3}}
(iv)
A = P{1, 2}
(v)
A = P(∅).
2.
Let A = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.
Determine whether each of the
following is a partition of A. If the set is not a partition, explain why
not.
(i)
{1, 2, {3, 4}, {5, 6}, {7, 8}, {9, 10}}
(ii)
{{1, 2}, {3, 4}, {5, 6}, {7, 8}, {9, 10}}
(iii)
{{1, 3, 5, 7, 9}, {2, 4, 8}, {10}}
(iv)
{{1, 5}, {2, 6, 10}, {3}, {4, 7, 9}, {8}}
(v)
{{2, 8, 10}, {1, 6}, {3, 4, 5}, {7, 8, 9}}.
3.
Which of the following are partitions of the set {2, 3, 7, 9, 10}?
(i)
{{2, 3}, {3, 7, 9}, {10}}
(ii)
{{2, 10}, {3, 7}, {9}}
(iii)
{{2, 3, 4}, {7, 9, 10}}
(iv)
{{2}, {3}, {7}, {9}, {10}}

Families of Sets
121
(v)
{2, 3, 7, 9, 10}
(vi)
{{2, 3, 7, 9, 10}}
(vii)
{{10, 3}, {7, 2}}
(viii) {{2, 9, 10}, {3, 7}, ∅}.
4.
(i)
How many partitions are there of the set {a, b, c, d}?
(ii)
Find all the partitions, if any, of the empty set ∅.
5.
Let {Am : m ∈R} be the family of sets deﬁned in example 3.10.4—that
is, Am is the set of points in the plane lying on the line y = mx.
Is {Am : m ∈R} a partition of the plane? Explain your answer.
6.
Which of the following families of sets are partitions of the set Z of
integers? Explain your answers.
(i)
{{n, n + 1} : n ∈Z}
(ii)
{{−n, n} : n ∈Z+}
(iii)
{{n, n2, n3} : n ∈Z}
(iv)
{{2n : n ∈Z}, {2n + 1 : n ∈Z}}.
7.
Which of the following are partitions of R, the set of real numbers?
Explain your answers.
(i)
{In : n ∈Z}, where In = {x ∈R : n ⩽x ⩽n + 1}.
(ii)
{Jn : n ∈Z}, where Jn = {x ∈R : n ⩽x < n + 1}.
(iii)
{Kn : n ∈Z}, where Kn = {x ∈R : n < x < n + 1}.
8.
Deﬁne a sequence of sets X0, X1, X2, . . . by X0 = ∅and, for n > 0,
Xn+1 = Xn ∪{Xn}.
List the elements of X1, X2, and X3.
What is the cardinality of Xn?
Give a recursive deﬁnition for the union
X =
∞
[
n=0
Xn.
(This sequence of sets was invented/discovered in the 1920s by the
mathematician, and later theoretical computer scientist, John von
Neumann (1903–57)†. His idea was to start with only the empty set and
† To some extent this discovery of von Neumann’s was anticipated some 20 years earlier by Bertrand
Russell.

122
Sets
‘create’ the natural numbers. Von Neumann deﬁned the natural number
n to be the set Xn.)
9.
Prove parts (ii) and (iii) of theorem 3.4.
Find sets A and B such that P(A) ∪P(B) is a proper subset of
P(A ∪B).
10.
Use the Principle of Mathematical Induction to prove theorem 3.5.
3.7
The Cartesian Product‡
The order in which the elements of a (ﬁnite) set are listed is immaterial; in
particular, {x, y} = {y, x}. In some circumstances, however, order is signiﬁcant.
For instance, in coordinate geometry the points with coordinates (1, 2) and (2, 1),
respectively, are distinct. We therefore wish to deﬁne, in the context of sets,
something akin to the coordinates of points used in analytical geometry.
In order to deal with situations where order is important, we deﬁne the ordered
pair (x, y) of objects x and y, to be such that
(x, y) = (x′, y′)
if and only if x = x′ and y = y′.
(*)
With this deﬁnition it is clear that (x, y) and (y, x) are different (unless x = y),
so the order is signiﬁcant. It could be argued, with justiﬁcation, that we have not
really deﬁned the ordered pair, but merely listed a property which we desire of it.
Those who are concerned about the way we have plucked the ordered pair out of
thin air, as it were, should note that (x, y) can be deﬁned in terms of (unordered)
sets considered earlier. (See exercise 3.6.1 for a way of doing this.) We have
not formally deﬁned the ordered pair in this way because the particular choice
of deﬁnition (and there is more than one way of deﬁning (x, y)) is unimportant.
What is signiﬁcant about the ordered pair is precisely the property (*) above.
We are now in a position to deﬁne the Cartesian product of two sets, a concept
which is fundamental to several later chapters.
‡ Named after the French mathematician and philosopher Ren´e Descartes (1596–1650), the founder
of analytical geometry.

The Cartesian Product
123
Deﬁnition 3.5
The Cartesian product, X × Y , of two sets X and Y is the set of all
ordered pairs (x, y) where x belongs to X and y belongs to Y :
X × Y = {(x, y) : x ∈X and y ∈Y }.
When X = Y , it is usual to denote X × X by X2. This is read as ‘X two’ and
not ‘X squared’.
Note that, if either X or Y (or both) is the empty set then X ×Y is also the empty
set. For example, if X = ∅then there are no elements x to place in the ﬁrst
position of the ordered pair (x, y), so there are no ordered pairs in X × Y .
If X and Y are both non-empty, then X ×Y = Y ×X if and only if X = Y . The
implication in one direction is obvious; if X = Y then clearly X × Y = Y × X.
For the converse, we prove its contrapositive: if X ̸= Y then X × Y ̸= Y × X.
Now, if X ̸= Y then either there exists an element x∗which belongs to X but not
to Y , or there exists an element y∗which belongs to Y but not to X (or both). In
the former case, choose any element y ∈Y —we can make such a choice since
we are assuming that Y in non-empty. Now the ordered pair (x∗, y) belongs to
X × Y , but does not belong to Y × X since x∗/∈Y . In the latter case we choose
an element x ∈X; then (x, y∗) belongs to X × Y but not to Y × X, since in this
case y∗/∈X. Therefore in either case we can ﬁnd an element which belongs to
X × Y but not to Y × X, so the sets are not equal.
Examples 3.13
1.
If X = {1, 2} and Y = {a, b, c} then
X × Y = {(1, a), (1, b), (1, c), (2, a), (2, b), (2, c)}.
The elements of the sets X, Y
and X × Y
can be represented
systematically on a single Venn-Euler diagram, as in ﬁgure 3.15.
2.
If X = Y = R, the set of real numbers, then X×Y = R×R = R2 which
is the coordinate geometry representation of the (two-dimensional) plane.
The corresponding diagram to ﬁgure 3.15 in this case is the plane with its
usual rectangular coordinate axes. A point P in the plane is represented
by an ordered pair (x, y) of real numbers—its coordinates.

124
Sets
Figure 3.15
3.
Let X = {main courses offered by a certain restaurant} and Y
=
{desserts offered by the same restaurant}. The X × Y is the set of all
(two-course) meals which can be ordered at the restaurant.
Diagrams such as ﬁgure 3.15 above and the coordinate geometry picture of the
plane R2 = R × R are useful ways of visualizing Cartesian products.
We
can mimic these to obtain a pictorial way of representing an arbitrary Cartesian
product X × Y , given in ﬁgure 3.16. The sets X and Y are drawn as one-
dimensional regions, rather than the usual two-dimensional ones in a Venn-Euler
diagram. That is, X and Y are drawn as line segments, with elements belonging to
them placed on the line segment. It is convenient to draw these lines perpendicular
to one another with the line representing X horizontal. The Cartesian product is
then represented as the rectangular region which lies above X and to the right of
Y , and the ordered pair (x, y) is placed in this rectangle at the point vertically
above x and horizontally to the right of y.
This type of diagram is useful for visualizing the intersections and unions of
Cartesian products, and it also indicates other properties of the Cartesian product
which are perhaps not so apparent from the ordered pair deﬁnition. For example,
if we choose an element x∗∈X and keep it ﬁxed, then the set
{x∗} × Y = {(x∗, y) : y ∈Y }
is a ‘copy’ of Y in the sense that for every y ∈Y there corresponds one and
only one element (x∗, y) ∈{x∗} × Y . This subset {x∗} × Y of X × Y can be
visualized in ﬁgure 3.16 as the vertical line in X × Y which lies above the point
in X representing the element x∗. We shall consider this kind of correspondence
in more detail in chapter 5.

The Cartesian Product
125
Figure 3.16
The
ordered
pair
(x, y)
may
be
generalized
to
an
ordered
n-tuple
(x1, x2, . . . , xn) with the property that
(x1, x2, . . . , xn) = (x′
1, x′
2, . . . , x′
n)
if and only if x1 = x′
1, x2 = x′
2, . . . , xn = x′
n.
Again we should note that the ordered n-tuple can be deﬁned formally in terms of
(unordered) sets. In particular, if ordered pairs have already been deﬁned in terms
of (unordered) sets (as indicated in exercise 3.6.1 below), then ordered n-tuples
can be deﬁned inductively using ordered pairs. (See exercise 3.6.7 for the details.)
The Cartesian product of n sets is now a natural generalization of the case of two
sets.

126
Sets
Deﬁnition 3.6
The Cartesian product of the sets X1, X2, . . . , Xn is
X1 × X2 × · · · × Xn
= {(x1, x2, . . . , xn) : x1 ∈X1 and x2 ∈X2 and . . . and xn ∈Xn}
= {(x1, x2, . . . , xn) : xi ∈Xi for i = 1, 2, . . . , n}.
Again we write Xn (which is read ‘X n’ rather than ‘X to the (power) n’) in
the case where Xi = X for i = 1, 2, . . . , n. For the general case, the Cartesian
product X1 × X2 × · · · × Xn is sometimes abbreviated
n×
r=1 Xr.
Examples 3.14
1.
If A = {1, 2}, B = {a, b} and C = {α, β} then
A × B × C = {(1, a, α), (1, a, β), (1, b, α), (1, b, β), (2, a, α),
(2, a, β), (2, b, α), (2, b, β)}.
It is harder to picture the Cartesian product of three sets, A × B × C, in a
diagram similar to ﬁgure 3.16 for two sets, but clearly its elements could
be displayed in a three-dimensional region. Of course, for the Cartesian
product of n sets, an n-dimensional region would be required, which is
even more difﬁcult to visualize!
2.
As in the case of two sets, if any one (or more) of the sets Xr (for
r = 1, 2, . . . , n) is empty then so, too, is their Cartesian product
n×
r=1 Xr.
For instance, if Xj = ∅then there is no element xj to place in the jth
position of the ordered n-tuple, so there can be no ordered n-tuples at all.
3.
If X1 = X2 = · · · = Xn = R, then the Cartesian product Rn is the
set of all n-tuples of real numbers (xx, x2, . . . , xn). The set Rn is a
coordinate representation of real n-dimensional space, which again is

The Cartesian Product
127
somewhat (!) difﬁcult to visualize. One of the reasons why ordered n-
tuples are important is that they provide a framework for studying and
understanding ‘n-dimensional sets’, whether in mathematics, computer
science or elsewhere.
Of course, for the case n = 3, the set R3 is (or can be identiﬁed
with) three-dimensional space familiar to those who have studied three-
dimensional geometry.
4.
We can extend example 3.13.3 by adding starters to the menu!
Let
V
= {starters offered by a certain restaurant} and, as before, X =
{main courses offered by the restaurant}, Y = {desserts offered by the
restaurant}. Then an ordered triple (v, x, y) ∈V × X × Y comprises
a starter v, main course x and dessert y and so represents a three course
meal. Therefore the Cartesian product V × X × Y represents the set of
all three course meals offered by the restaurant.
If X and Y are ﬁnite sets with |X| = n and |Y | = m, then it is clear from the
‘coordinate grid’ diagram of X × Y (see ﬁgure 3.15) that the Cartesian product
has nm elements. That is,
|X × Y | = |X| × |Y |.
This result clearly generalizes to the following for n sets, which may be proved
formally using mathematical induction.
Theorem 3.6
If X1, X2, . . . , Xn are ﬁnite sets then
|X1 × X2 × · · · × Xn| = |X1| × |X2| × · · · × |Xn|.
We now turn to the question of how the Cartesian product operation behaves with
respect to the other set theory operations such as intersection and union. Before
we consider the general situation, let’s look at two examples to see what is likely
to happen in general.

128
Sets
Examples 3.15
1.
Let A = {a, b, c, d}, X = {x, y, z} and Y = {y, z, t}. Then
X ∩Y = {y, z}
so
A × (X ∩Y ) = {(a, y), (a, z), (b, y), (b, z), (c, y), (c, z), (d, y), (d, z)}.
Now
A × X = {(a, x), (a, y), (a, z), (b, x), (b, y), (b, z), (c, x),
(c, y), (c, z), (d, x), (d, y), (d, z)},
and
A × Y = {(a, y), (a, z), (a, t), (b, y), (b, z), (b, t), (c, y),
(c, z), (c, t), (d, y), (d, z), (d, t)}.
Therefore
(A × X) ∩(A × Y ) = {(a, y), (a, z), (b, y), (b, z), (c, y), (c, z),
(d, y), (d, z)}.
Therefore, for the sets in this example,
A × (X ∩Y ) = (A × X) ∩(A × Y ),
so we may wish to investigate whether this identity is true for all sets A,
X and Y .
2.
To investigate whether a similar identity may hold for unions, consider the
sets A = {a, b}, X = {x, y} and Y = {y, z}. Then X ∪Y = {x, y, z},
so
A × (X ∪Y ) = {(a, x), (a, y), (a, z), (b, x), (b, y), (b, z)}
= {(a, x), (a, y), (b, x), (b, y)}
∪{(a, y), (a, z), (b, y), (b, z)}
= (A × X) ∪(A × Y ).
The results suggested by these examples do in fact hold for all sets A, X and Y .
We list below identities which indicate how the Cartesian product behaves with
respect to the intersection and union operations.

The Cartesian Product
129
Theorem 3.7
(i)
For all sets A, X and Y
A × (X ∩Y ) = (A × X) ∩(A × Y )
and
(X ∩Y ) × A = (X × A) ∩(Y × A).
(This says that the Cartesian product is distributive over
intersection.)
(ii)
For all sets A, X and Y
A × (X ∪Y ) = (A × X) ∪(A × Y )
and
(X ∪Y ) × A = (X × A) ∪(Y × A).
(This says that the Cartesian product is distributive over union.)
Proof
We shall prove the ﬁrst identity in part (i) only—the others are left as exercises
(3.6.9).
Let (a, x) ∈A × (X ∩Y ). By the deﬁnition of the Cartesian product, this means
that a ∈A and x ∈(X ∩Y ). Thus x ∈X, so (a, x) belongs to A × X; and
x ∈Y , so (a, x) belongs to A×Y as well. Therefore (a, x) ∈(A×X)∩(A×Y ),
which proves that A × (X ∩Y ) ⊆(A × X) ∩(A × Y ).
To prove the subset relation the other way round as well, let
(a, x) ∈(A × X) ∩(A × Y ).
Then (a, x) ∈(A × X), so a ∈A and x ∈X; and (a, x) ∈(A × Y ), so
a ∈A and x ∈Y . Therefore a ∈A and x ∈(X ∩Y ) which means that
the ordered pair (a, x) belongs to the Cartesian product A × (X ∩Y ). Hence
(A × X) ∩(A × Y ) ⊆A × (X ∩Y ).
The conclusion that the sets A × (X ∩Y ) and (A × X) ∩(A × Y ) are equal now
follows, since each is a subset of the other.
□

130
Sets
Figure 3.17 illustrates the identity proved above. The sets X and Y are both drawn
as vertical line segments, which are kept distinct to avoid confusion over where
one begins and the other ends. This means that it is more difﬁcult to represent
their intersection adequately—we have indicated X ∩Y by a thickened line on
both X and Y . The Cartesian products A × X and A × Y are shaded differently,
the region of double shading representing (A × X) ∩(A × Y ). It is clear from
the diagram that this doubly shaded region corresponds to the Cartesian product
A × (X ∩Y ).
Figure 3.17
Finally, we state how the Cartesian product behaves with respect to the subset
relationship. The proof of theorem 3.8 is left as an exercise (3.6.12). Before
attempting to prove this, it is advisable to draw a ‘coordinate grid’ diagram to
represent the situation.
Theorem 3.8
(i)
For all sets A, B and X, A ⊆B implies (A × X) ⊆(B × X).
(ii)
If X is non-empty, then (A × X) ⊆(B × X) implies A ⊆B.

The Cartesian Product
131
Exercises 3.6
1.
(Kuratowski’s deﬁnition† of the ordered pair.) If (x, y) is deﬁned by
(x, y) = {{x}, {x, y}}, show that
(x, y) = (a, b)
if and only if x = a and y = b.
2.
In each of the following cases list the elements of X × Y , and draw a
‘coordinate grid’ diagram similar to ﬁgure 3.15:
(i)
X = {1, 2, 3, 4}
Y = {a, b}
(ii)
X = {1, 2}
Y = {a, b, c, d, e}
(iii)
X = {(1, 2)}
Y = {a, b, c, d, e}.
3.
Let A = {1, 2, 3, 4}, B = {3, 4, 5}, X = {a, b}, Y = {b, c, d}. List the
elements of each of the following sets.
(i)
(A ∩B) × (X ∩Y )
(ii)
(A × X) ∩(B × Y )
(iii)
(A × Y ) ∩(B × X)
(iv)
(A ∩X) × Y
(v)
(A ∩B) × (X ∪Y )
(vi)
(A × X) ∪(B × Y ).
4.
In a simple library catalogue, each book has just four properties or
attributes: title, author, class number, publication date. We assume that
each book has a single author (co-authored books, such as this one, are
listed only by their ﬁrst named author) and that the class number is a
positive decimal (for example, 314.25).
Further, we assume that the
library holds at most one copy of any book.
Then each book in the
library’s stock corresponds to a unique quadruple of the form
(title, author, class number, year of publication).
Let C
(for ‘collection’) denote the set of all such quadruples
corresponding to the books held in the library’s collection. Then C ⊆
T × A × R+ × Z where T is the set of all titles of books in the library’s
collection and A denotes the set of all authors of books in the library’s
collection. Informally, we can think of C as ‘being’ the set of all books in
† Named after the twentieth-century Polish mathematician Kazimierz Kuratowski, whose name is
also associated with a theorem about planar graphs—see chapter 11.

132
Sets
the library’s collection. Of course, this is not strictly correct since a book
is not an ordered quadruple; however, there is an ‘exact correspondence’
between real books in the collection and quadruples in the set C. (The
precise nature of such an ‘exact correspondence’ will be made clear in
chapter 5.)
(i)
Explain brieﬂy why C is a proper subset of T × A × R+ × Z.
(ii)
Let D = {n ∈Z : there exists (t, a, x, n) ∈C}.
Describe in words what the set D represents in terms of the
library’s collection. What is the signiﬁcance for the library of the
smallest element of D?
(iii)
Let S = {(t, a, x, n) ∈C : a = Shakespeare}.
Describe
informally in words the set S.
(iv)
Deﬁne formally (in a similar manner to the set S deﬁned in part
(iii)) the set of all books in C authored by ‘Garnier’.
(v)
Suppose {(t, a, x, n) ∈C : x = 514.3} = ∅. What does this tell
us about the library’s collection?
(vi)
Suppose {(t, a, x, n) ∈C : t = ‘Crime and Punishment’ ∧a =
‘Dostoyevsky’} ̸= ∅. What does this tell us about the library’s
collection?
Note:
Representing objects by n-tuples corresponding to various
attributes is a useful and extremely common way of organizing data. See
sections 4.7 and 5.6 for a brief introduction to relational databases which
are founded on the use of n-tuples in this way.
5.
If X × Y = X × Z does it necessarily follow that Y = Z? Explain your
answer.
6.
Let
[0, 1] = {x ∈R : 0 ⩽x ⩽1}
(0, 1) = {x ∈R : 0 < x < 1}
[0, 1) = {x ∈R : 0 ⩽x < 1}
(0, 1] = {x ∈R : 0 < x ⩽1}.
Describe (geometrically) each of the following sets:
(i)
[0, 1] × [0, 1]
(ii)
(0, 1) × (0, 1)
(iii)
[0, 1) × (0, 1]
(iv)
[0, 1] × (0, 1).

The Cartesian Product
133
7.
(i)
Deﬁning the ordered triple (x, y, z) in terms of ordered pairs by
(x, y, z) = ((x, y), z)
show that (x, y, z) = (a, b, c) if and only if x = a, y = b and
z = c.
(ii)
If, for n ⩾3, ordered n-tuples are deﬁned inductively by
(x1, x2, . . . , xn) = ((x1, x2, . . . , xn−1), xn)
show that (x1, x2, . . . , xn) = (y1, y2, . . . , yn) if and only if xi =
yi for each i = 1, 2, . . . , n.
8.
Let A = {a, b} and X = {1, 2, 3}.
(i)
List all the non-empty subsets of A and all the non-empty subsets
of X.
(ii)
List all the non-empty subsets of A × X which are of the form
B × Y for some B ⊆A and some Y ⊆X.
(iii)
Write down a subset of A × X that is not of the form B × Y for
some B ⊆A and some Y ⊆X.
9.
Prove the identities omitted from the proof of theorem 3.7. That is, for all
sets A, X and Y :
(i)
(X ∩Y ) × A = (X × A) ∩(Y × A)
(ii)
A × (X ∪Y ) = (A × X) ∪(A × Y )
(iii)
(X ∪Y ) × A = (X × A) ∪(Y × A).
10.
Using theorem 3.7 and the laws for the algebra of sets, show that, for all
sets A, B, X and Y ,
(i)
(A ∩B) × (X ∩Y ) = (A × X) ∩(A × Y ) ∩(B × X) ∩(B × Y )
(ii)
(A ∪B) × (X ∪Y ) = (A × X) ∪(A × Y ) ∪(B × X) ∪(B × Y ).
Draw diagrams to represent these identities. (Hint: for clarity in the
diagram representing identity (ii), it is best to draw the sets A and B
as if disjoint, and the sets X and Y also as if disjoint.)
11.
(i)
Prove that, for all sets A, B, X and Y ,
(A ∩B) × (X ∩Y ) = (A × X) ∩(B × Y )
= (A × Y ) ∩(B × X).

134
Sets
Draw a diagram to represent each of these identities.
(ii)
Find sets A, B, X and Y such that
(A ∪B) × (X ∪Y ) ̸= (A × X) ∪(B × Y ).
12.
Prove theorem 3.8.
13.
Prove that, for non-empty sets A, B, X and Y ,
(A × B) ⊆(X × Y )
if and only if A ⊆X and B ⊆Y .
14.
Prove each of the following identities, and draw diagrams to illustrate
each:
(i)
(A −B) × X = (A × X) −(B × X)
(ii)
(A −B) × (X −Y ) = (A × X) −[(A × Y ) ∪(B × X)].
3.8
Types and Typed Set Theory
In software engineering,
the notion of ‘types’ plays an important role
in the various phases of software development:
speciﬁcation, design and
implementation. Objects of different types behave differently and have different
operations associated with them. In a software system managing a library, for
example, objects classiﬁed as being of the type ‘book’ clearly have rather different
properties than objects classiﬁed as being of the type ‘borrower’. Similarly, in
programming languages, variables need to be declared to be of type ‘integer’,
‘real’, ‘string’ and so forth, again because these types have different properties.
In this section we will introduce types from a mathematical point of view and
consider how we can formally deﬁne various operations on a type.
Consider the set of integers Z. Various operations are deﬁned on this set, such as
addition, subtraction, multiplication and so on. In other words, given two integers
n and m, we can deﬁne n + m, n −m, n × m, etc. This is rather obvious,
but note that the subset relation is not deﬁned on the set of integers; if m and
n are integers, then n ⊆m is meaningless. Similarly, the operations deﬁned
in chapter 1—conjunction, disjunction, implication, etc—are not deﬁned on the
set of integers; if n and m are integers then n ∧m, n ∨m and n →m are all
meaningless.

Types and Typed Set Theory
135
Each operation deﬁned on Z has a ‘signature’ which describes the ‘inputs’ and
‘outputs’ of the operation. The operations of addition, subtraction, multiplication
each take two integers as ‘input’ and give a single integer as ‘output’.
For
example, we could input 2 and 5 into the addition operation and obtain output
7; similarly inputting 2 and 5 into the subtraction or multiplication operation
would give output −3 or 10 respectively. We say that each of these operations
has signature
Integer , Integer →Integer
reﬂecting the fact that two integers are required for input and a single integer is
the result of performing the operation.
Some integer operations take as their input a single integer. For example, the
‘negation’ operation or the ‘square’ operation both operate on a single integer.
Input 3 into the ‘negation’ operation and the output is −3; similarly, input −2 and
the output is −(−2) = 2. Or, for the ‘square’ operation, input 3 (or −3) and the
output is 9. Each of these operations thus has signature
Integer →Integer .
We can now informally deﬁne the ‘type’ Integer to comprise the set of integers
Z together with the operations that are deﬁned on integers and their signatures.
In general, a type T has a set of allowed values that variables of the type can
take, together with a collection of operations in which variables of the type can
participate as inputs or ‘arguments’.
Other ‘standard’ types include the following.
Real
The type of the real numbers.
Boolean
The type of logical expressions (propositions and propositional
functions).
String
The type of strings of characters (such as ‘agxp nyt’ or ‘Hello Paul!
How are you?’).
Usually, (mathematical) operations have as input one or more arguments of the
same type, but the output is frequently of a different type to the input(s). For
example, if n and m are integers then n + m and n ⩽m are both meaningful
expressions but of a different nature: the value of n + m is another integer but
the value of n ⩽m is either ‘true’ or ‘false’. More precisely, ‘n ⩽m’ is a
proposition or propositional function (depending on whether the integers n and
m are given speciﬁed values) and so is of type Boolean . Therefore the ‘less than
or equal’ operation has signature
Integer , Integer →Boolean .

136
Sets
Examples 3.16
1.
We summarize some (but not all) of the operations deﬁned on the type
Integer , together with their signatures.
Addition
+
: Integer , Integer →Integer .
The notation
+
means that addition is an ‘inﬁx’ operation where
the sign of the operation comes in between the two integer ‘arguments’.
The two ‘underscores’ on either side of the addition sign represent
placeholders which will be ﬁlled by the two integer input values.
Subtraction and multiplication have the same signature as addition.
Subtraction
−
: Integer , Integer →Integer
Multiplication
×
: Integer , Integer →Integer .
As noted above, negation takes a single integer as argument and returns
the integer with the sign changed. For example, the negation of 2 is
−2, the negation of −5 is 5, the negation of 0 is 0 (=−0). Negation
is a ‘preﬁx’ operation because the operation sign precedes the input
argument; it has the following signature.
Negation
−
: Integer →Integer .
Note that negation and subtraction are different operations because they
have different signatures. Some people very sensibly use different words
for the two operations: they say ‘minus’ for subtraction and ‘negative’
for negation.
2 −3
‘two minus three’
−3
‘negative three’
2 −(−3)
‘two minus negative three’.
Each of the order operations < (less than), ⩽(less than or equal to), >
(greater than) and ⩾(greater than or equal to) are inﬁx operations with
the same signature. For example ‘less than or equal to’ has signature:
⩽
: Integer , Integer →Boolean .
The operation ‘divides’ means ‘is a factor of’ or ‘goes exactly into’. For
example, 6 divides 48 but 6 does not divide 15. What is the signature of
the operation? It takes two integers as arguments, m and n say, and the
result is either true or false. Hence the expression ‘m divides n’ has a

Types and Typed Set Theory
137
Boolean value (true or false) depending on whether m is or is not a factor
of n. Therefore the operation has the following signature. (Note that m|n
is read as ‘m divides n’.)
Divides
|
: Integer , Integer →Boolean .
The ‘absolute value’ or ‘modulus’ operation, | |, takes a single integer
as input and returns a non-negative integer as output. If the input value
is greater than or equal to zero then the operation ‘leaves it alone’; if
the input value is negative then the operation returns the corresponding
positive value. For example, |3| = 3, | −3| = 3, |0| = 0, etc. The
absolute value operation has signature:
| | : Integer →Integer .
2.
Recall that Boolean is the type of logical expressions and we know
that such expressions can have one of two values, T (true) or F (false).
Therefore the set of values of the Boolean type is {T, F}. Some of the
operations deﬁned on the type Boolean , together with their signatures are
given below. The values returned by these operations are deﬁned by the
truth tables given in chapter 1.
Negation
¬
: Boolean →Boolean
Conjunction
∧
: Boolean , Boolean →Boolean
Disjunction
∨
: Boolean , Boolean →Boolean
Exclusive disjunction
⊻
: Boolean , Boolean →Boolean
Conditional
→
: Boolean , Boolean →Boolean
Biconditional
↔
: Boolean , Boolean →Boolean .
3.
The type Real has the set of real numbers R as its set of values. Many
of the operations deﬁned on the type Integer are also deﬁned on the type
Real . Some of the operations deﬁned on Real are given below, together
with their signatures. We shall meet other operations on the type later.
Addition
+
: Real , Real →Real
Subtraction
−
: Real , Real →Real
Multiplication
×
: Real , Real →Real
Division
/
: Real , Real →Real
Negation
−
: Real →Real
Less than or equal to
⩽
: Real , Real →Boolean
Less than
<
: Real , Real →Boolean

138
Sets
Greater than or equal to
⩾
: Real , Real →Boolean
Greater than
>
: Real , Real →Boolean
Square
2 : Real →Real
Square root
√
: Real →Real .
Typed Set Theory
Typed set theory is a more restricted version of set theory than the version
considered in the previous sections of this chapter. In typed set theory, all the
elements of a set are required to have the same type.
Thus, for example, a
set containing an element of type Real and an element of type String is not
permitted. The notation for describing typed sets is slightly different from the
notation we used previously for ‘untyped’ sets. If T is a type then the notation
x : T means ‘x is of type T ’. It is similar to x ∈A (‘x belongs to A’) but
gives more information as it indicates the operations in which x may participate
as an argument. We use the notation {x : T | P(x)} to deﬁne the set of all
elements of type T for which the propositional function P(x) is true. We read
{x : T | P(x)} as ‘the set of all x of type T such that P(x) (is true)’. For
example, {x : Real | x2 = 2} = {−
√
2,
√
2}.
In typed set theory every element and every set is required to have a speciﬁed
type. Any set whose elements are all of type T itself has type Set [T ] which
simply indicates that it is a set of ‘things’ of type T . Thus a set of the form
{x : T | P(x)} has type Set [T ].
Examples 3.17
1.
Let A = {n : Integer |−2 ⩽n ⩽3} and B = {n : Real | −2 ⩽n ⩽3}.
Then A has type Set [Integer ] and B has type Set [Real ]. Also note that
A is ﬁnite whereas B is inﬁnite: A = {−2, −1, 0, 1, 2, 3} but B contains
all real numbers between −2 and 3 (inclusive) and it is impossible to list
them.
2.
We assume a type Person has been deﬁned which is the type of all people,
living or deceased. Although we shall not consider them here, we can
imagine some of the operations deﬁned on this type: motherOf, age,
gender, maritalStatus, etc. Given this type, we can deﬁne various sets
of people; for example:
A = {x : Person | x is/was UK Prime Minister during

Types and Typed Set Theory
139
part of the period 1970–99}
= {Blair, Callaghan, Heath, Major, Thatcher, Wilson}
B = {x : Person | x is/was US President during
part of the period 1970–99}
= {Bush, Carter, Clinton, Ford, Nixon, Reagan}
C = {x : Person | x was born on 29 February}
D = {x : Person | x has Spanish nationality}.
Each of these sets has type Set [Person ].
3.
Let A = {1, 2, 3}; then A : Set [Integer ]. What is the type of P(A), the
power set of A? Recall that the elements of P(A) are the subsets of A:
P(A) = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.
The elements of P(A) have type Set [Integer ] so P(A) itself has type
Set [Set [Integer ]] because it is a set of sets of integers.
4.
Consider the informal deﬁnition of a set as ‘the set of all cities in
Canada.’ In order to be able to deﬁne this as a typed set, we need to
assume the existence of a type City , say, which is the type of cities.
(We could imagine some of the operations that might be deﬁned on this
type: Population( ) : City →Integer, Mayor( ) : City →Person ,
Country( ) : City →Nation , and so on.) Provided City is a deﬁned
type, we can then deﬁne:
A = {x : City | x is in Canada}
= {Ottawa, Montreal, Vancouver, . . .} : Set [City ].
Operations on Typed Sets
The usual set theory operations—intersection, union, complement, and so on—
are deﬁned on typed sets. However, only sets of the same type can ‘participate
in’ these operations. For example, if we were to attempt to form the union of a
set of integers with a set of people, say, the result would not be a well formed
typed set because its elements would not all be of the same type. For a ﬁxed type
T , the signatures of the standard operations on the type Set [T ] are given below.
Intersection, union and difference are inﬁx operations which take as arguments
two sets of the same type and produce another set of the same type. Thus their
signatures are the following.
Intersection
∩
: Set [T ], Set [T ] →Set [T ]

140
Sets
Union
∪
: Set [T ], Set [T ] →Set [T ]
Difference
−
: Set [T ], Set [T ] →Set [T ].
Subset
If A : Set [T ] and B : Set [T ] then A ⊆B is either true or false. (Note that ‘subset’
behaves rather like ‘less than or equal’ in this respect.) Therefore ‘subset’ takes
two sets of the same type as arguments and returns a Boolean expression so it has
signature:
⊆
: Set [T ], Set [T ] →Boolean .
Membership
For x ∈A to be deﬁned, we require x and A to have appropriate types. More
precisely, we require x : T and A : Set [T ] so that the types ‘match’. Given this,
what is the type of the statement x ∈A? As with subset, ‘A ⊆B’, the expression
‘x ∈A’ is either true or false depending on whether or not x really is a member
of the set A. Therefore set membership has signature
∈
: T , Set [T ] →Boolean .
This means that the placeholder to the left of the membership symbol ∈can take a
value of type T and the placeholder on the right takes a value of type Set [T ]. Note
that the set membership operation is unusual for a mathematical operation in that
the types of its two inputs are necessarily different, T and Set [T ] respectively†.
Empty Set
What is the type of the empty set ∅? Given the signature of ‘subset’ deﬁned
above, we can only compare sets of the same type:
writing ∅
⊆
{1, 2, 3} implies that the empty set ∅must be of type
Set [Integer ],
writing ∅⊆{1.23, −19.857, π} implies that ∅is of type Set [Real ],
† It is interesting to note that, in typed set theory, Russell’s paradox disappears. (See exercise 3.2.12
for a discussion of Russell’s paradox.) Indeed, in Principia Mathematica, their monumental work on
the foundations of mathematics, Russell and Whitehead use a theory of types to avoid the paradox.
The reason the paradox does not occur in typed set theory is that we are unable to form the set that
gives rise to the difﬁculty. If A : Set [T ] then A /∈A, which is just shorthand for ¬(A ∈A), is not
an allowed expression because it does not conform to the signature of /∈(or ∈). Hence we are unable
to form the (typed) set R in exercise 3.2.12 that gives rise to the paradox.

Types and Typed Set Theory
141
writing ∅⊆{Blair, Callaghan, Heath, Major, Thatcher, Wilson} implies that
∅is of type Set [Person ], etc.
There seems to be a difﬁculty here since we have stated that each set must have
a unique type. The way around this problem is to deﬁne one empty set of each
type. Thus there is an empty set of integers (containing no integers) which has
type Set [Integer ], an empty set of real numbers (containing no real numbers)
which has type Set [Real ], an empty set of people (containing no people) which
has type Set [Person ], and so on. We shall continue to use ∅to denote each of
these empty sets. It should usually be clear from the context which empty set is
being represented by ∅.
Using a single notation to stand for several different concepts is called
overloading the notation. Actually, we do this all the time in mathematics. For
example, we use a single symbol + to represent addition of integers, real numbers,
matrices (see chapter 6), elements of an Abelian group (see chapter 8), and so
forth. Similarly we use the symbol −to represent both subtraction and negation
of integers and of real numbers, and the difference of sets, etc. So using ∅to
denote the empty set of each type should not cause any difﬁculty.
Power Set
In example 3.17.3, we saw that if A is of type Set [Integer ] then its power set
P(A) is of type Set [Set [Integer ]]. This generalizes to sets of any type. If a set
A has type Set [T ] then any subset also has type Set [T ]; therefore the elements of
P(A) have type Set [T ] so P(A) itself has type Set [Set [T ]]. Since the power set
operation takes a single set A as input and produces a single set P(A) as output,
it has signature
P( ) : Set [T ] →Set [Set [T ]].
Cardinality
For ﬁnite sets†, the cardinality operation takes a set as argument and returns an
integer value, namely the number of elements in the set. Hence cardinality has
signature:
| | : Set [T ] →Integer .
† To include inﬁnite sets, we would need to augment the Integer type by adding a special symbol
∞to produce a new type Integer ∗whose set of values is Z ∪{∞}. Then cardinality would have
signature | | : Set [T ] →Integer ∗.

142
Sets
Type Checking
Notice that, according to the signatures of ∩, ∪and −deﬁned above, we can only
form the union, intersection and difference of sets that are of the same type. Thus,
for example, if A : Set [S ] and B : Set [T ] then A ∩B is meaningless in typed
set theory unless S = T . Similarly, if x : S and A : Set [T ] then x ∈A is also
meaningless unless S = T . (Actually, this is not quite true. As we shall see
shortly, it is possible for S to be a ‘subtype’ of T and, in this case, A ∩B and
x ∈A are properly deﬁned—see example 3.18.3.)
In fact, this phenomenon occurs in other situations in mathematics, even if we
have not formally deﬁned types. For example, if m and n are integers then the
expression (m ⩽n) + 3 is meaningless because the ﬁrst argument of the addition
operation is m ⩽n which is of type Boolean whereas addition requires two
integers (or two reals) as arguments.
In each of these cases, there is an operation whose arguments do not match the
signature of the operation. Type checking an expression means verifying that,
for each operation in the expression, the types of its arguments agree with those
speciﬁed by the signature of the operation. For example, if the expression includes
∩then both its arguments must be sets of the same type; if it includes + then both
arguments must be integers (or both real numbers), and so on.
Examples 3.18
Suppose the following type declarations have been made:
k, n, m : Integer
x, y : Real
P, Q : Boolean
Anne, Brian : Person .
For each of the following statements or terms, decide whether it is meaningful
(in other words, whether it ‘type checks’) and, if so, what is the type of the
expression. (Assume the ‘obvious’ operations are deﬁned on the type Person .)
1.
n ⩾m.
‘Greater than or equal’ is an inﬁx operation that takes two integers as
arguments, which is what we have here. Thus the expression type checks
(i.e. it is meaningful). The expression has type Boolean .
2.
(n ⩾m) + k.

Types and Typed Set Theory
143
This does not type check so is not meaningful. We noted above that
n ⩾m has type Boolean but + does not take a Boolean type as either
argument.
3.
n + x.
Surprisingly, perhaps, this does not type check because addition (as given
in examples 3.16) is deﬁned between two integers or two real numbers.
Addition either has signature
+
: Integer , Integer →Integer or
+
: Real , Real →Real but the given expression attempts to add an
integer to a real.
However n+x clearly ought to be a meaningful expression—we have not
previously had any difﬁculty in adding, say, 7 and 4.32! The way round
this difﬁculty is to regard Integer as a subtype of Real . This means that
we may substitute an integer value in any expression that requires a real
argument. Clearly this can always be done: it amounts to regarding the
integer 3, for example, as a real number 3.00. With this convention the
expression n + x is meaningful and has type Real .
4.
Anne IsOlderThan Brian.
Assuming that IsOlderThan is an inﬁx operation with signature
IsOlderThan
: Person , Person →Boolean
then the expression type checks and gives a Boolean result.
5.
n + Age(Anne).
This is meaningful provided Age has signature Person →Integer so
that both n and Age(Anne) are of type Integer ; then the expression
has type Integer . (Alternatively, we could deﬁne Age to have signature
Person →Real , then the expression would also have type Real .)
6.
(x < y) ∨(P →Q).
This type checks as both x < y and P →Q have type Boolean . The
expression has type Boolean .
7.
Age(Anne) + 5 = Brian.
This is not meaningful since Age(Anne) + 5 : Integer and Brian : Person
but equality is only deﬁned for values of the same type.
Note that,
however, Age(Anne) + 5 = Age(Brian) is meaningful and has type

144
Sets
Boolean because now both sides of the = sign have the same type:
Integer .
8.
(x + y) ↔(Brian IsSonOf Anne).
We assume IsSonOf has signature
IsSonOf
: Person , Person
→
Boolean . Then the given expression does not type check since x + y :
Real and Brian IsSonOf Anne : Boolean but ↔requires both arguments
to have type Boolean .
Deﬁning Operations: Preconditions and Postconditions
So far we have only deﬁned the signature of various operations deﬁned on Integer ,
Boolean , Set [T ] and so on. We have not deﬁned the behaviour of any of the
operations. This may not seem very important because we are all agreed what
addition means for integers or intersection for sets and so on. However, being
able to deﬁne precisely what an operation achieves is extremely important in
software speciﬁcation.
To build a piece of software, it is vital to be able to
deﬁne exactly what each component should do. Unfortunately, there are many
examples of software failures because this precise speciﬁcation stage has not been
properly completed. We shall describe a way of specifying the behaviour of an
operation using logical expressions as ‘preconditions’ and ‘postconditions’ for
the operation. To keep the discussion as simple as possible, we shall restrict our
examples to familiar mathematical operations.
A precondition is a condition that must be fulﬁlled before an operation can be
invoked and a postcondition is a condition that is fulﬁlled as a result of the
operation being invoked. We can think of the precondition and postcondition
as deﬁning a contract between the operation and any user of the operation. To
satisfy the contract, the user is ‘obliged’ to supply values to the operation which
satisfy the precondition; the operation is then ‘required’ to return a value which
satisﬁes the postcondition. Note also that such a ‘contract’ only speciﬁes what an
operation should do and not how it should do it.
Examples 3.19
1.
Consider the operation of division of real numbers. The operation takes
two real numbers x and y as input and produces a single real number x/y
as output. Therefore division has signature
/
: Real , Real →Real .

Types and Typed Set Theory
145
However, division by zero is meaningless so x/y is only deﬁned when
y ̸= 0. If we are to ‘feed in’ two real numbers to the division operation,
we had better ensure that the second of them is non-zero. This deﬁnes the
precondition: y ̸= 0.
Provided the precondition is satisﬁed, the result of performing the
division operation on real numbers x, y is the real number which is ‘x
divided by y’. How can we deﬁne this number without simply asserting
that it is x divided by y? For example, if we input π and 4, what is the
property that the output number π/4 must satisfy? Suppose we have a
‘division operation machine’ that gives answers rounded to three decimal
places; we input π and 4 and the machine returns the answer 0.792. Is
the machine functioning correctly? The simplest test is to multiply the
output by 4: 0.792 × 4 = 3.168 which is not the value of π correct to
three decimal places. Therefore the division machine is faulty. In general,
assuming that the operation of multiplication has been deﬁned, the real
number r that is the result of dividing x by y is deﬁned by the equation
x = r × y. This equation is the postcondition.
We can now give the full description of the division operation. It has three
parts: signature, precondition and postcondition.
/
: x : Real , y : Real →r : Real
precondition
y ̸= 0
postcondition
x = r × y.
Note that we have extended the usual signature expression by adding
labels for the input and output variables. This is so that we can refer
to particular inputs and the output in the precondition and postcondition.
For example, the precondition must state that it is the second of the two
arguments that does not take a zero value. Hence we need to be able to
distinguish between the two input variables.
This speciﬁcation of division is the required contract between the
operation and its user. The user is obliged to ‘feed in’ real numbers x
and y satisfying the precondition y ̸= 0; then the operation will keep
its side of the contract by producing a real number r (which is the value
of x/y) satisfying the postcondition: x = r × y. The contract does not
specify how the operation will calculate the value r = x/y—provided the
result satisﬁes the postcondition, any method of calculation is acceptable.
If we were required to implement the division operation as a software
routine then the method of calculation would be important in terms of the
speed of the operation and accuracy of the output. However, as far as the
speciﬁcation of the operation is concerned, these issues are not relevant.

146
Sets
2.
Consider the ‘square root’ operation deﬁned on real numbers. Since the
operation takes a real number as input and returns a real number value, it
has signature
√
: Real →Real .
What should be the precondition(s) and postcondition(s)?
Imagine a
square root machine as a ‘black box’; we input a real number into the
machine and out comes another real number.
To determine the precondition, consider what real numbers we are
allowed to ‘feed into’ the machine without ‘breaking it’ (we imagine that
feeding in an illegal value is likely to break the machine). We cannot feed
in a negative number since the square root of a negative real number is not
deﬁned (in the context of the type Real ). But this is the only restriction—
any other real number is an allowed input. Therefore the precondition is
x ⩾0 where x is the input value.
To determine the postcondition, suppose we feed in the value x and the
real number r is the resulting output. What test(s) would need to be
carried out on the output r in order to determine whether the machine was
working properly? For example, suppose we feed in 7 and the machine
(working to three decimal places, as before) outputs the answer 2.615. Is
it working correctly? Since 2.6152 = 2.615 × 2.615 = 6.838 to three
decimal places (which is not equal to 7), we conclude the machine is not
functioning properly.
In general, if we ‘feed in’ x (satisfying the precondition x ⩾0) the
output r should satisfy r2 = x if the machine is working properly and
this equation forms part of the postcondition.
We are assuming here
that the ‘square’ operation has been deﬁned and properly speciﬁed—see
example 3(i) below. If the square operation has not been speciﬁed then
we would need to use the equation r×r = x in the postcondition in place
of r2 = x.
In fact there is another condition which must be satisﬁed. It is a common
but erroneous belief that √x means ‘the positive or negative square root
of x’. In fact, the symbol √means ‘the non-negative square root of’. For
example,
√
4 = 2 and not ±2. (The equation x2 = 4 has two solutions,
namely 2 =
√
4 and −2 = −
√
4, and these are frequently summarized
as ±
√
4 which is probably the cause of the error.) This means that there

Types and Typed Set Theory
147
is another part of the postcondition which says that the output should be
non-negative: r ⩾0.
Putting all the pieces together gives the following speciﬁcation of the
square root operation.
√
: x : Real →r : Real
precondition
x ⩾0
postcondition
r ⩾0 ∧r2 = x.
3.
The following are some further speciﬁcations of operations given with
somewhat briefer explanation.
In general, each speciﬁcation has
a signature, precondition and postcondition although sometimes no
precondition is required.
(i)
square
2 : x : Real →r : Real
precondition
There is no precondition since we are allowed
to ‘feed in’ any real number to the ‘square
operation’
postcondition
r = x × x.
(ii)
absolute value
| | : x : Real →r : Real
precondition
none (from now on we shall simply miss
out the precondition part if there is no
precondition)
postcondition
(x ⩾0 →r = x) ∧(x < 0 →r = −x).
(iii)
‘ﬂoor’ or ‘integer part’
⌊⌋: x : Real →n : Integer .
The ﬂoor or integer part of a real number is the largest integer
that is less than or equal to the given real number. For example,
⌊8.74⌋= 8, ⌊π⌋= 3, ⌊−2.38⌋= −3, ⌊4⌋= 4, etc.
postcondition
(n ⩽x) ∧(n + 1 > x).
The postcondition uses ⩽and > each with one Integer and one
Real argument. We have deﬁned these operations to have signature
Real , Real →Real (or Integer , Integer →Integer ). Therefore, in
the postcondition, we are assuming that Integer is a subtype of
Real and that, in each operation, we are substituting a value of
type Integer for the ﬁrst argument of the operation with signature
Real , Real →Real .
(iv)
intersection
∩
: A : Set [T ], B : Set [T ] →C : Set [T ]
postcondition
for all x : T , x ∈C ↔(x ∈A ∧x ∈B).

148
Sets
The output set should be the intersection of the two input sets,
C = A ∩B, and the postcondition deﬁnes this intersection. In this
case, to capture the required deﬁning property of A ∩B, we need
to quantify over all elements of type T .
(v)
union
∪
: A : Set [T ], B : Set [T ] →C : Set [T ]
postcondition
for all x : T , x ∈C ↔(x ∈A ∨x ∈B).
(vi)
difference
−
: A : Set [T ], B : Set [T ] →C : Set [T ]
postcondition
for all x : T , x ∈C ↔(x ∈A ∧x /∈B).
(vii)
empty set
It may seem odd to think of the empty set as an operation at all.
However, for every type T , there is an empty set of type Set [T ].
We can think of the empty set operation as delivering an empty
set of type Set [T ] ‘automatically’ without having ﬁrst to receive
an input. This means that there is no input type and hence no
precondition. We can deﬁne:
empty set
∅:→C : Set [T ]
postcondition
for all x : T , ¬(x ∈C).
The postcondition characterizes the empty set as the set that
contains no element of type T . Any operation like this that has
no input type is called a constant.
(viii) subset
⊆
: A : Set [T ], B : Set [T ] →Boolean
postcondition
for all x : T , A ⊆B ↔(x ∈A →x ∈B).
(ix)
set equality
=
: A : Set [T ], B : Set [T ] →Boolean
postcondition
A = B ↔(A ⊆B ∧B ⊆A).
(x)
husband of
HusbandOf( ) : p : Person →q : Person .
In the following speciﬁcation of HusbandOf we assume the
operations IsFemale, IsMarried and IsMarriedTo have clear
meanings (and have been properly speciﬁed). See exercise 3.7.6
for the speciﬁcation of further operations on the type Person .
precondition
IsFemale(p) ∧IsMarried(p)
postcondition
¬IsFemale(q) ∧p IsMarriedTo q.

Types and Typed Set Theory
149
Exercises 3.7
1.
Suppose the following type declarations have been made.
k, m, n : Integer
x, y, z : Real
P, Q : Boolean .
Where necessary, assume that Integer is a subtype of Real ; for example,
the division operation for real numbers
/
: Real , Real →Real can
take integer arguments (although the result will always be of type Real ).
(a)
State the type of each of the following terms.
(i)
m + (k × n)
(iv)
P ↔Q
(ii)
x ⩽(y −z)
(v)
P ∨(x ̸= y)
(iii)
m −n = 2 × k
(vi)
(n/k) + z.
(b)
Determine
whether
each
of
the
following
expressions
is
meaningful (that is, ‘type checks’).
(i)
x × (k/n)
(iv)
(n = m) →(P ∨Q)
(ii)
P ∧(x ⩾y)
(v)
¬(m < k)
(iii)
(P ∧x) ⩾y
(vi)
(¬m) < k.
2.
Consider the type Person of all human beings (living and deceased).
Various operations are to be deﬁned on the type. Deﬁne the signature
of each of the following operations. In some cases you will need to make
choices regarding the meaning of the operation. You may assume the
existence of other types that you require. For example, suppose Name
is an operation which returns the family name of a person. You may
assume the existence of the type String , which is the type of a person’s
family name, and then deﬁne Name to have the following signature:
Name( ) : Person →String .
Operation
Comment
Height
Gives a person’s height in metres in the form
Height(Jack) = 1.913.
DateOfBirth
Assume that Date is a deﬁned type.
YearOfBirth
Age
Gives the age in years (i.e. age last birthday).
Mother
Gives a person’s mother.
IsOlderThan
An
inﬁx
operation;
for
particular
p
and
q,
p IsOlderThan q gives the truth value of ‘p is
older than q’.

150
Sets
CitizenOf
Gives the country of the person’s nationality. Assume
that Nation is a deﬁned type.
What
happens
if
we
allow
dual
(or
multiple)
nationality?
Children
Gives the set of children of a particular person.
IsTallerThan
Qualiﬁcations
Siblings
Gives the set of siblings of a person.
3.
When quantifying over propositional functions, it is often necessary
to specify the types of the variables.
We use the following obvious
generalization of the notation introduced in chapter 1.
∀x : T , P(x)
means ‘for all x of type T , P(x) (is true)’
∃x : T , P(x)
means ‘there exists (at least one) x of type
T such that P(x) (is true)’.
Determine whether of not each of the following statements is true or false.
(In some cases you will need to make assumptions about the operations
involved—see question 2 above.)
(i)
∃n : Integer , n2 = 2
(ii)
∃x : Real , x2 = 2
(iii)
∀n : Integer , n −1 < n
(iv)
∀x : Real , ∃n : Integer , n > x
(v)
∀x : Person , Mother(x) IsOlderThan x
(vi)
∃x : Real , x2 < 0
(vii)
∃x : Person , IsQueen(x)
(viii) ∃x ∃y : Person , Age(x) = Age(y)
(ix)
∀n : Integer , ∃m : Integer , m > n
(x)
∀n : Integer , (n < 0) ∨(n ⩾0).
4.
Determine whether or not each of the following statements is true. Note
that, for a statement to be true, it must ﬁrst be meaningful (that is, it must
type check).
(i)
∀n : Integer , n < n + 1
(ii)
∀x : Person , x ⩾0
(iii)
∀n : Integer , n + 1 > 0
(iv)
∃n : Integer , n + 1 > 0
(v)
∀x : Real , x2 ⩾0
(vi)
∃x : Real , x2 = 3
(vii)
∀x : Person , ∃n : Integer , Age(x) = n
(viii) ∃n : Integer , ∀x : Person , Age(x) = n

Types and Typed Set Theory
151
(ix)
∃n ∃m : Integer , (n < m) ∧(n2 > m2)
(x)
∀P ∀Q : Boolean , (P ↔Q) ∨(P ↔¬Q) (Hint: draw up a truth
table)
(xi)
∀x : Real , ∃n : Integer , n ⩾x
(xii)
∀x : Real , (x2 < 0) →(x < 0).
5.
Suppose that, on the type Integer , the operations of addition and
multiplication are given and are fully speciﬁed. Suppose also that there
is an operation IsPositive with signature
IsPositive( ) : Integer →Boolean
such that IsPositive(n) is true when n is greater than zero and false
otherwise.
Write down complete speciﬁcations for each of the following operations.
That
is,
write
down
preconditions
(if
any
are
necessary)
and
postconditions which deﬁne the operation.
To begin with, you may
only use addition, multiplication and IsPositive (together with equality).
However, once an operation has been deﬁned, it may then be used in
subsequent speciﬁcations. In many cases there is more than one way of
correctly specifying the operation.
(i)
subtraction
−
: Integer , Integer →Integer
(ii)
negation
−
: Integer →Integer
(iii)
greater than
>
: Integer , Integer →Boolean
(iv)
is negative
IsNegative( ) : Integer →Boolean
(v)
less than
<
: Integer , Integer →Boolean
(vi)
reciprocal
1/
: Integer →Real
(vii)
greater than or equal to
⩾
: Integer , Integer →Boolean
(viii)
less than or equal to
⩽
: Integer , Integer →Boolean
(ix)
even
IsEven( ) : Integer →Boolean
(x)
odd
IsOdd( ) : Integer →Boolean
(xi)
mod
mod
: Integer , Integer →Integer
The operation ‘n mod k’ gives the remainder when n is divided
by k. For example, 2 mod 3 = 2, 4 mod 3 = 1, 38 mod 3 = 2,
180 mod 3 = 0, 52 mod 5 = 2, 17 mod 7 = 3, etc.
(xii)
divides
|
: Integer , Integer →Boolean .
Recall that n|m is true if n is a factor of m (that is, ‘n goes
exactly into m’) and is false otherwise.

152
Sets
6.
In this question, you may assume the type Person has the following
operations already speciﬁed.
IsMarried(p)
p is married
IsFemale(p)
p is female
IsChildOf(p, q)
p is a child of q
IsMarriedTo(p, q)
p is married to q
(i)
Write down the signatures of IsMarried, IsFemale, IsChildOf and
IsMarriedTo.
(ii)
The operation WifeOf(p) is to be deﬁned as returning the wife of
p. Write down informal (in English) and formal preconditions and
postconditions for WifeOf. In your formal version, you may use
any of the operations above, but no others.
(iii)
Sons(p) is to be deﬁned as returning the set (which may be empty)
of sons of p : Person . Write down the signature, informal (in
English) and formal preconditions and postconditions for Sons,
again using only the operations above.
(iv)
An operation f is deﬁned on the set of people as follows:
signature
f( ) : p : Person →q : Person
precondition
none
postcondition
(q = f(p)) ↔(IsMale(q) ∧IsChildOf(p, q))
In ordinary English, describe what output the function f produces.
(v)
Write a formal speciﬁcation for the function FatherInLaw, that is
to return a person’s father-in-law (the father of his or her spouse).
7.
A type Pet is to be deﬁned as the type of all living domestic pets. In this
question, assume the existence of the types Integer , Real , Boolean and
Person .
(i)
Assuming that every pet has one and only one (human) owner but
a person may own more than one pet, write down the signature of
each of the following operations:
(a)
OwnerOf( )
this gives the owner of a pet
(b)
Owns( )
this gives the pets owned by a person.
(ii)
If
a
:
Pet ,
what
is
the
relationship
between
a
and
Owns(OwnerOf(a))?

Types and Typed Set Theory
153
(iii)
The operation hasPet
: Person
→Boolean returns true if
the person owns at least one pet and false otherwise.
Give a
formal speciﬁcation of hasPet in terms of preconditions and/or
postconditions.
(iv)
An operation f has signature a : Pet , b : Pet →Boolean . It has
no precondition but has the following postcondition:
postcondition
f(a, b) ↔∃p : Person p = OwnerOf(a) ∧p = OwnerOf(b).
Describe in simple terms the meaning of this operation f.

Chapter 4
Relations
4.1
Relations and Their Representations
The mathematical notion of a relation, like that of a set, is a very general one.
It is one of the key concepts of mathematics and examples of relations occur
throughout the subject. Three special types of relation are particularly important:
functions, equivalence relations and order relations. Functions are the subject
of the next chapter; equivalence and order relations are considered later in this
chapter. We begin, though, with a look at the general concept of a relation and
various ways of visualizing relations.
In §1.8 we considered two-place predicates such as ‘is heavier than’. A two-
place predicate requires two variables to convert it into a propositional function.
For example, if H is the predicate ‘is heavier than’, then H(x, y) denotes the
propositional function ‘x is heavier than y’.
We can think of a two-variable
propositional function as deﬁning some kind of relationship between its two
variables. Given objects a and b, the proposition H(a, b) is true if and only if
the objects are related in the appropriate way.
The ﬁrst thing to note is that, in a two-variable propositional function F(x, y),
the order of the variables may be signiﬁcant. For speciﬁc objects a and b, F(a, b)
and F(b, a) may have different truth values. This is the case for the propositional
functions ‘x is heavier than y’, ‘x is the mother of y’ or ‘x is greater than y’, for
instance. Therefore, the set of objects for which F(a, b) is a true proposition will
be a set of ordered pairs. It is also important to realize that the two variables x and
y may represent different kinds of object. For example, consider the propositional
function C(x, y): x is the capital city of y. Here x is the name of a city but y is
the name of a country, so the set of ordered pairs (a, b) for which C(a, b) is a true
154

Relations and Their Representations
155
proposition is a subset of the Cartesian product A × B, where A = {cities} and
B = {countries}.
The following mathematical deﬁnition of a ‘relation’ is surprisingly simple and
very general. Some authors refer to this as a binary relation because it relates two
objects. (There is a generalization of this which relates n objects—see exercise
4.1.11.)
Deﬁnition 4.1
Let A and B be sets. A relation from A to B (or between A and B) is a
subset of the Cartesian product A × B.
The ﬁrst thing to notice is that a relation as we have deﬁned it is a set; namely a
set of ordered pairs. If R is a relation from A to B, we say that a ∈A is related
to b ∈B if (a, b) ∈R. Thus the relation R itself is simply the set of all related
pairs of elements. For the most part we shall adopt the commonly used notation
and write a R b to denote ‘a is related to b’, and a
±
R b to denote (a, b) /∈R or ‘a
is not related to b’. If A = B it is also common to refer to R as a relation on A.
Examples 4.1
1.
Let A = {cities of the world}, B = {countries of the world} and
R = {(a, b) : a is the capital city of b}. Thus a R b denotes ‘a is the
capital city of b’.
Examples are: (Paris) R (France), (Moscow) R (Russia), (Tirana) R
(Albania), etc.
Also
we
have:
(London)
±
R (Zimbabwe),
(Naples)
±
R (Italy),
(New York)
±
R (United States), etc.
2.
Let A = B = {1, 2, 3, 4, 5, 6} and R = {(a, b) : a divides b}. Since A is
a small ﬁnite set we can list the elements of the relation:
R = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 2), (2, 4), (2, 6),
(3, 3), (3, 6), (4, 4), (5, 5), (6, 6)}.
We represent R diagrammatically in ﬁgure 4.1 by plotting its elements on
the coordinate grid diagram of the Cartesian product A × B = A2.

156
Relations
Figure 4.1
3.
Let A = B = Z+, the set of positive integers, and let a R b denote ‘a has
the same parity as b’; that is, either a and b are both even or they are both
odd. More precisely
R = {(a, b) : a −b is an integer multiple of 2}.
Then
1 R 1, 1 R 3, 1 R 5, . . .
2 R 2, 2 R 4, 2 R 6, . . .
3 R 1, 3 R 3, 3 R 5, . . .
4 R 2, 4 R 4, 4 R 6, . . . etc.
A picture for this relation is ﬁgure 4.2, where again we have plotted the
elements of R on the diagram for A × B.
There are various ways of representing relations visually, particularly relations
between ﬁnite sets. In ﬁgures 4.1 and 4.2, the elements of R are marked on the
coordinate grid diagram of the Cartesian product A × B. Diagrams such as these
show clearly R as a subset of A × B, but are not so good at showing additional
properties of the relation.
An alternative for ﬁnite sets is to represent A and B as two side-by-side Venn-
Euler diagrams with the elements arranged vertically. An arrow is drawn from
a ∈A to b ∈B whenever a R b. We refer to this as the arrow diagram of the
relation. For example, the arrow diagram for the relation deﬁned in example 4.1.2
above is given in ﬁgure 4.3.
Unfortunately ﬁgure 4.3 does not show very clearly at a glance which elements
are related to which. For sets larger than {1, 2, 3, 4, 5, 6} diagrams of this type

Relations and Their Representations
157
Figure 4.2
Figure 4.3
would become too cluttered to be of much use. However, for relations on a set
(i.e. where A = B), there is a slight modiﬁcation we can make which clariﬁes
the diagram. Instead of listing the elements of A twice, once in each Venn-Euler
diagram, we can represent each element of A once by a point in the plane. A
directed arrow is still drawn from a to b if and only if aRb. The resulting diagram
(see ﬁgure 4.4) is an example of a directed graph† or digraph and is called the
directed graph of the relation. We shall study graphs and directed graphs in
greater detail in chapters 10 and 11.
If two elements a and b are such that a R b and b R a, we will usually connect
† More precisely, ﬁgure 4.4 is the diagram of a directed graph—see chapter 10.

158
Relations
Figure 4.4
their points in the directed graph by a single bi-directional arrow, rather than two
directed arrows. (See the diagram in exercise 4.1.5.)
A third way to represent a relation is by a ‘binary matrix’.
Let A
=
{a1, a2, . . . , an} and B = {b1, b2, . . . , bm} be ﬁnite sets and let R be a relation
from A to B. The binary matrix of R is a rectangular array of zeros and ones
with n rows and m columns. The rows correspond to the elements of A (in the
order listed above) and the columns correspond to the elements of B (again, in
the order listed above). At the intersection of the ith row and jth column we place
a one if ai R bj or a zero if ai
±
R bj. For example, the binary matrix representing
the relation R on A = {1, 2, 3, 4, 5, 6} given by a R b if and only if a divides b
(example 4.1.2) is the following:








b1 = 1
b2 = 2
b3 = 3
b4 = 4
b5 = 5
b6 = 6
a1 = 1
1
1
1
1
1
1
a2 = 2
0
1
0
1
0
1
a3 = 3
0
0
1
0
0
1
a4 = 4
0
0
0
1
0
0
a5 = 5
0
0
0
0
1
0
a6 = 6
0
0
0
0
0
1








.
We have taken the elements of A = B in increasing order so that row i represents
the number i and column j represents the number j. The zero at the intersection of
the ﬁfth row and second column means that a5
±
R b2—that is, 5 does not divide 2.

Relations and Their Representations
159
Similarly the one at the intersection of the second row and fourth column means
that a2 R b4—that is, 2 divides 4.
Normally we will not write a1, . . . , a6 to label the rows and b1, . . . , b6 to label the
columns as we have here, provided it is clearly understood which rows correspond
to which elements of A and which columns correspond to which elements of
B. Several of the properties of the relation R which we consider later can be
deduced from properties of its binary matrix. Matrix algebra itself is the subject
of chapter 6.
Note that the binary matrix of a relation on a ﬁnite set A is square—that is, it
has an equal number of rows and columns. The number of rows or columns is of
course |A|, the cardinality of A.
Relations and Types
We now consider brieﬂy how typed sets introduced in section 3.8 ﬁt in with the
theory of relations. If A and B are typed sets and R is a relation from A to B,
then R should also have a speciﬁed type. Before we can determine the type of
R, we must ﬁrst deﬁne the type of the Cartesian product A × B (because R is a
subset of A × B).
Given elements a of type S and b of type T , we deﬁne the ordered pair (a, b) to
have type S × T . Symbolically,
a : S , b : T →(a, b) : S × T .
For example, if n : Integer and x : Real then (n, x) : Integer × Real . Now if
A : Set [S ] and B : Set [T ] then their Cartesian product A × B is a set containing
ordered pairs (a, b) of type S × T . Therefore A × B has type Set [S × T ]. In
symbols,
A : Set [S ], B : Set [T ] →A × B : Set [S × T ].
Now suppose that R is a relation from A to B. Then R ⊆A × B so R has the
same type as A × B, namely Set [S × T ]. To summarize:
if R is a relation from A : Set [S ] to B : Set [T ] then R : Set [S × T ].
In examples 4.1:
the relation in example 1 has type Set [City × Country ];
the relation in example 2 has type Set [Integer × Integer ];
the relation in example 3 has type Set [Integer × Integer ].

160
Relations
Exercises 4.1
1.
For each of the following relations R on a set A, draw:
(a)
its coordinate grid diagram,
(b)
its directed graph, and
(c)
its binary matrix.
(i)
A = {1, 2, 3, 4, 5, 6, 7, 8};
a R b if and only if a < b.
(ii)
A = {1, 2, 3, 4, 5, 6, 7, 8};
a R b if and only if a = b.
(iii)
A = {1, 2, 3, 4, 5, 6, 7, 8};
a R b if and only if a ⩽b.
(iv)
A = {1, 2, 3, 4, 5, 6, 7, 8};
a R b if and only if a/b ∈Z.
(v)
A = {a, b, c, d, e};
R = {(a, b), (a, c), (a, e), (b, c), (c, a), (c, d), (d, e),
(e, c), (e, d)}.
(vi)
A = {a, b, c, d, e, f};
x R y if and only if x and y are both vowels or x and y are both
consonants.
(vii)
A = {1, 2, 3, 4, 5, 6, 7, 8};
a R b if and only if a = 2b.
(viii) A = {1, 2, 3, 4, 5, 6, 7, 8};
a R b if an only if a = 2nb for some n = Z+.
(ix)
A = P{1, 2, 3}, the power set of {1, 2, 3};
a R b if and only if a ⊆b.
(x)
A = P{1, 2, 3}, the power set of {1, 2, 3};
a R b if and only if a ⊂b.
2.
The binary matrices MR and MS for two relations R and S respectively
on the set A = {1, 2, 3, 4, 5} are given below.
MR =






0
0
0
0
0
1
0
1
0
1
1
1
0
1
0
1
1
1
0
1
1
0
0
1
0






and
MS =






1
1
1
1
1
0
0
0
0
0
1
1
1
0
0
1
0
0
1
1
0
0
0
0
0






.

Relations and Their Representations
161
(i)
List the elements of R and S.
(ii)
Draw the directed graphs of R and S.
3.
A relation R on the set A = {a, b, c, d, e} has the directed graph shown
in the diagram below.
(i)
List the elements of R.
(ii)
Write down the binary matrix of R.
4.
A
relation
R
between
the
sets
A
=
{1, 2, 3}
and
B
=
P(A)
=
{∅, {1}, {2}, {3}, {1, 2}, {2, 3}, {1, 3}, {1, 2, 3}} has the
following binary matrix. (The row and columns of the matrix correspond
to the elements of A and B as they are listed respectively.)


0
1
0
0
1
0
1
1
0
0
1
0
1
1
0
1
0
0
0
1
0
1
1
1

.
List the elements of R and deﬁne a R b in words or symbols.
5.
Each of the four football teams A, B, C, D in a mini-league plays
every other team both at home and away.
A relation R on the set
S = {A, B, C, D} is deﬁned by:
X R Y
if and only if X beat Y when X played at home.
The following diagram is the directed graph of R.
List the elements of R, and write down its binary matrix.
6.
For each of the following relations R on the (ordered) set A
=
{a, b, c, d, e}, whose binary matrix is given, list the elements of R and
draw its directed graph:

162
Relations
(i)






0
1
1
1
1
0
0
1
1
1
0
0
0
1
1
0
0
0
0
1
0
0
0
0
0






(ii)






1
0
0
0
0
1
1
0
0
0
1
1
1
0
0
1
1
1
1
0
1
1
1
1
1






(iii)






1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1






.
What is the connection between the relations in parts (i) and (ii)?
7.
Let A = {rivers in the world}, B = {towns or cities in the world} and
deﬁne a relation R from A to B by
a R b
if and only if a ﬂows through b.
(i)
Describe in words each of the following sets:
(a)
{b ∈B : (Thames) R b}
(b)
{a ∈A : a R (London)}.
(ii)
Describe in words each of the following propositions:
(a)
¬(∃x ∈A, x R (Toronto))
(b)
∀a ∈A (a R (Washington, DC) →a = Potomac).
(iii)
Deﬁne each of the following sets symbolically.
(a)
The set of rivers which ﬂow through Paris.
(b)
The set of rivers which ﬂow through some town or city.
(iv)
Write symbolically each of the following statements:
(a)
All rivers ﬂow through some town or city.
(b)
All towns or cities have a river ﬂowing through them.
(c)
The River Nile ﬂows through more than one town or city.
8.
Let A be any (ﬁnite) set.
(i)
The identity relation IA on A is the relation of equality deﬁned

Relations and Their Representations
163
by: aIAb if and only if a = b.
Describe (a) the directed graph and (b) the binary matrix of IA.
(ii)
The universal relation UA on A is the relation deﬁned by: a UA b
for all a, b ∈A.
Describe (a) the directed graph and (b) the binary matrix of UA.
9.
(i)
How many relations are there from {a, b, c} to {1, 2, 3, 4}? (Do
not try to list them all!)
(ii)
More generally, if |A| = n and |B| = m, how many relations are
there from A to B?
10.
Given a relation R from set A to set B, its inverse relation R−1 is the
relation from B to A deﬁned by
x R−1 y
if and only if y R x.
(a)
Let A = {1, 2, 3, 4} and let R be the relation on A deﬁned by
R = {(1, 2), (1, 4), (2, 2), (2, 3), (3, 4), (4, 3), (4, 4)}.
(i)
List the elements of R−1.
(ii)
Draw the directed graphs of both R and R−1.
(iii)
Write down the binary matrices of both R and R−1.
(b)
Let R be a relation on a set A.
(i)
Describe the connection between the directed graphs of R
and its inverse R−1.
(ii)
Describe the connection between the binary matrices of R
and its inverse R−1.
11.
An n-ary relation between sets A1, A2, . . . , An is deﬁned to be a subset
of the Cartesian product A1 × A2 × · · · × An. If A = A1 = A2 = · · · =
An we refer to an n-ary relation on A. Let A = {1, 2, . . . , 6} and deﬁne a
3-ary relation R on A by (x, y, z) ∈R if and only if x < y and y divides
z.
List the elements of R.
12.
Determine the type of each of the relations deﬁned in questions 1, 4, 5
and 8 above.
13.
(i)
Let A : Set [S ], B : Set [T ] and let R be a relation from A to B.
What is the type of the inverse relation R−1?

164
Relations
(ii)
Deﬁne the type of the relation R deﬁned in question 7 above.
Describe the inverse relation R−1 in words and deﬁne its type.
4.2
Properties of Relations
Up to now we have not justiﬁed our assertion, made at the beginning of this
chapter, that relations are important in mathematics. Indeed, if all we were able
to do with relations between sets were to draw diagrams to represent them, the
concept of a relation would not be very signiﬁcant. Its importance is mainly due
to special kinds of relation which satisfy additional properties. The two of these
which we shall study in this chapter—equivalence relations and order relations—
are both relations on a set, so we look now at some of the properties which a
relation on a set may have.
Deﬁnitions 4.2
Let R be a relation on set A. We say that R is:
(i)
reﬂexive if and only if a R a for every a ∈A;
(ii)
symmetric if and only if a R b implies b R a for every a, b, ∈A;
(iii)
anti-symmetric if and only if a R b and b R a implies a = b for
every a, b ∈A;
(iv)
transitive if and only if a R b and b R c implies a R c for every
a, b, c, ∈A.
Note that to prove that a relation R on a set A satisﬁes one of these four properties,
we need to show that the appropriate property is satisﬁed by an arbitrary element
or elements of A. For example, to prove that R is symmetric, we need to show
that a R b →b R a for arbitrary elements a, b ∈A. However, to show that R
does not satisfy one of the properties, we need to ﬁnd a particular element or
elements of A that show this. These particular elements are a counter-example to
the property (see §2.3). For example, to show that R is not symmetric, we need
to ﬁnd particular elements a, b ∈A such that a R b but b
±
R a.

Properties of Relations
165
Examples 4.2
1.
Let R be the relation on the set of real numbers deﬁned by
x R y
if and only if x ⩽y.
Then:
(i)
R is reﬂexive because x ⩽x for every x ∈R;
(ii)
R is not symmetric because, for example, 1 ⩽2 but 2 
 1, so
x R y does not imply y R x;
(iii)
R is anti-symmetric: if x ⩽y and y ⩽x then it follows that x = y;
(iv)
R is transitive because if x ⩽y and y ⩽z then it follows that
x ⩽z.
2.
Let A = {a, b, c, d} and R = {(a, a), (a, b), (a, c), (b, a), (b, b), (b, c),
(b, d), (d, d)}.
The relation R satisﬁes none of the properties of deﬁnition 4.2:
R is not reﬂexive since c
±
Rc; therefore it is not true that xRx for every
x ∈A;
R is not symmetric since, for example, a R c but c
±
R a;
R is not anti-symmetric since a R b and b R a but a ̸= b;
R is not transitive since a R b and b R d but a
±
R d.
3.
Let A = Z+ × Z+ and R be the relation on A deﬁned by (a, b) R (c, d)
if and only if a + d = b + c. Show that R is reﬂexive, symmetric and
transitive, but not anti-symmetric.
Solution
For all positive integers a and b, a + b = b + a, so (a, b) R (a, b) for every
(a, b) ∈A, Therefore R is reﬂexive.
R is symmetric since if (a, b) R (c, d) then a + d = b + c which implies that
c + b = d + a, so (c, d) R (a, b).
To show that R is transitive, suppose (a, b)R(c, d) and (c, d)R(e, f). This means
that
a + d = b + c
and
c + f = d + e.
Adding these equations gives
a + d + c + f = b + c + d + e

166
Relations
so
a + f = b + e
which means that (a, b) R (e, f).
Therefore (a, b) R (c, d) and (c, d) R (e, f) implies that (a, b) R (e, f), so R is
transitive.
Finally, to show that R is not anti-symmetric we need to ﬁnd a counter-example;
that is, we need to ﬁnd elements (a, b) and (c, d) of A such that (a, b) R (c, d) and
(c, d) R (a, b) but (a, b) ̸= (c, d). Clearly the elements (1, 2) and (2, 3) will do.
We now consider how we can recognize whether a relation satisﬁes any of these
properties given its directed graph or its binary matrix. Firstly, if R is a reﬂexive
relation on a ﬁnite set A then a R a for every a ∈A. This means that, in the
directed graph of R, there is a directed arrow from every point to itself. The
directed graph of the relation in example 4.1.2 (ﬁgure 4.4) has this property. In
the binary matrix of a reﬂexive relation R, there is a one in every position along
the diagonal which runs from the top left to the bottom right of the matrix. (This
diagonal is called the ‘leading diagonal’ of the matrix.)
If R is symmetric then every arrow connecting different points in its directed
graph is bidirectional. This is because an arrow from a to b means that a R b; but
if R is symmetric this implies that b R a as well, so the arrow must also go from
b to a. The binary matrix of a symmetric relation is symmetric about its leading
diagonal; whatever appears at the intersection of the ith row and jth column also
appears at the intersection of the jth row and ith column.
For an anti-symmetric relation, the directed graph is such that there are no
bidirectional arrows connecting different points.
This is because for distinct
elements a and b of A we cannot have both aRb and bRa. The property satisﬁed
by the binary matrix of an anti-symmetric relation is slightly less obvious. A
relation being anti-symmetric means that, for distinct elements a and b, if a R b
then b
±
Ra and if bRa then a
±
Rb. Thus in the binary matrix, if there is a one at the
intersection of the ith row and jth column (where i ̸= j) then there must be a zero
at the intersection of the jth row and ith column. However, since it is possible to
have a
±
R b and b
±
R a, there could be zeros in both of these positions.
The properties satisﬁed by the directed graph and binary matrix of a transitive
relation are less obvious still.
For the graph, however, we can describe the
property reasonably easily. If three points are such that there are arrows from
the ﬁrst to the second and from the second to the third, then there must also be an
arrow from the ﬁrst to the third.

Properties of Relations
167
Examples 4.3
1.
Consider the directed graph given in ﬁgure 4.5 of a relation R on the set
A = {a, b, c, d, e}.
Figure 4.5
From this diagram we can see that:
(i)
R is not reﬂexive, since there is no arrow from c to itself, for
example.
(ii)
R is symmetric, but not anti-symmetric, since every arrow
connecting distinct points is bidirectional.
(iii)
R is not transitive since, for instance, there are arrows from a to d,
and from d to b, but not from a to b.
2.
A relation R on a four-element set A has the following binary matrix:




1
0
1
0
1
1
0
0
0
0
1
1
0
1
0
1



.
Which of the properties of deﬁnitions 4.2 does R satisfy?
Solution
Firstly, it is clear that R is reﬂexive since there are only ones along the leading
diagonal.

168
Relations
R is not symmetric because the matrix is not symmetric about the leading
diagonal. For instance, there is a one in row 1, column 3, but a zero in row 3,
column 1.
We have to look a bit harder to see that R is anti-symmetric; except for the leading
diagonal wherever a one appears in row i, column j, a zero appears in row j,
column i. Note that sometimes a zero appears in both these places; for example,
in row 1, column 4 and row 4, column 1.
With some trial and error, we can also spot that R is not transitive. If we label
the elements of the set a1, a2, a3 and a4, in that order, then a1 R a3 and a3 R a4
but a1
±
R a4. We leave it as an exercise to discover whether there are any other
counter-examples to transitivity.
Exercises 4.2
1.
For each of the binary relations on a set deﬁned in exercises 4.1,
determine whether the relation is:
(i)
reﬂexive;
(ii)
symmetric;
(iii)
anti-symmetric;
(iv)
transitive.
2.
Let A be any set of living human beings. For each of the following
relations on A, deﬁned by a two-place predicate, determine which, if any,
of the four properties of deﬁnitions 4.2 is/are satisﬁed:
(i)
‘is the mother of’.
(ii)
‘is the brother of’.
(iii)
‘is the sibling of’.
(iv)
‘is at least as tall as’.
(v)
‘is taller than’.
(vi)
‘is the same age (in years) as’.
(vii)
‘is the same gender as’.
(viii) ‘is an ancestor of’.
(ix)
‘is married to’. (Is this affected by whether the people come from
a monogamous or polygamous society?)
(x)
‘is an acquaintance of’.
3.
Let A = {a, b, c, d, e}.
For each of the following relations R on

Properties of Relations
169
A, determine which of the four properties (reﬂexive, symmetric, anti-
symmetric, transitive) are satisﬁed by the relation. Justify your answers.
(i)
R = {(a, a), (a, b), (a, c), (b, a), (b, b), (b, c), (c, a), (c, b),
(c, c)}.
(ii)
R = {(a, a), (b, b), (c, c), (d, d), (e, e), (a, b), (b, c)}.
(iii)
R = {(a, a), (a, d), (b, b), (c, c), (d, d), (d, e), (e, a), (e, e)}.
(iv)
R = {(a, b), (b, c), (c, d), (d, e), (e, a)}.
(v)
R = {(a, b), (b, a), (b, d), (d, a), (c, e), (e, c), (e, e)}.
4.
For each of the following relations, determine which of the four properties
are satisﬁed by the relation. Justify your answers.
(i)
A = {1, 2, 3, 4, 5, 6, 7, 8}
n R m if and only if n = 2km
for some k ∈Z.
(ii)
A = {1, 2, 3, 4, 5, 6, 7, 8}
n R m if and only if n ⩽m.
(iii)
A = {1, 2, 3, 4, 5, 6, 7, 8}
n R m if and only if n ̸= m.
(iv)
A = P({1, 2, 3})
B R C if and only if B ⊆C.
(v)
A = P({1, 2, 3})
B R C if and only if |B| = |C|.
5.
Let A be any non-empty set and R = ∅be the empty relation on A.
Which, if any, of the four properties deﬁned in deﬁnitions 4.2 is/are
satisﬁed by R?
If A itself is empty, which, if any, of the properties are satisﬁed?
6.
Is it possible for a relation to be both symmetric and anti-symmetric? If
so, how is it possible?
7.
Let R = {(a, a), (a, b), (a, c), (b, b), (b, c)} be a relation on the set
{a, b, c, d}. What is the minimum number of elements which need to
be added to R in order that it becomes:
(i)
reﬂexive;
(ii)
symmetric;
(iii)
anti-symmetric;
(iv)
transitive?
8.
Let A = {a, b, c, d}. For each of the following, deﬁne a relation R on A
which satisﬁes the given properties. Try to keep the examples simple.
(i)
R is reﬂexive and transitive but not symmetric.
(ii)
R is both symmetric and anti-symmetric.
(iii)
R is symmetric but not reﬂexive and not transitive.
9.
Determine which, if any, of the four properties in deﬁnitions 4.2 is

170
Relations
satisﬁed by each of the following relations on the set Z+ of positive
integers.
(i)
n R m if and only if n −m is a multiple of 3.
(ii)
n R m if and only if n = 3km for some k ∈Z+.
(iii)
n R m if and only if n ̸= m.
(iv)
n R m if and only if n/m is an integer.
10.
Let A be the set of all lines in the plane R2. Which of our four properties
is satisﬁed by the following relations?
(i)
l1 R l2 if and only if l1 is parallel to l2.
(ii)
l1 R l2 if and only if l1 is perpendicular to l2.
(iii)
(For those readers who have studied elementary coordinate
geometry.) l1 R l2 if and only if the product of the gradients of
l1 and l2 is equal to one.
11.
(i)
Given an example of a relation which is symmetric and transitive
but not reﬂexive.
(ii)
The following argument is sometimes given to show that a relation
which is symmetric and transitive must also be reﬂexive.
Suppose R is a symmetric and transitive relation on a set A. Let
a be any element of A. By the symmetric property, a R b implies
bRa. Since aRb and bRa we can deduce aRa from the transitive
property. Because a was an arbitrary element, we have proved that
a R a for every a ∈A; therefore R is reﬂexive.
The example in (i) shows that this argument must be false. What
is wrong with it?
12.
Let A be any set of propositions not all of which have the same truth
values. Deﬁne a relation R on A by:
p R q
if and only if p →q is true.
Which of the four properties is satisﬁed by this relation?
13.
For each of the four properties, reﬂexive, symmetric, anti-symmetric
and transitive, if a relation R on a set A satisﬁes the property does its
inverse relation R−1 necessarily satisfy the property as well? (The inverse
relation is deﬁned in exercise 4.1.10.)
Show also that R is symmetric if and only if R = R−1.

Intersections and Unions of Relations
171
4.3
Intersections and Unions of Relations
Since a relation R between A and B is simply a set—a subset of the Cartesian
product A × B—we can deﬁne intersections and unions of relations.
Let R and S be two relations from a set A to a set B. Both their intersection R∩S
and their union R ∪S are subsets of A × B also. That is, both the intersection
and the union of two relations from A to B are also relations from A to B.
The situation is not quite so clear when R and S are relations between different
pairs of sets. Suppose R is a relation from A to B, and S is a relation from C to
D. Since R and S are both sets of ordered pairs, so, too, are their intersection and
union. Thus R ∩S and R ∪S are both relations. However, it is not immediately
apparent exactly which sets are related by R ∩S and which sets are related by
R ∪S. We leave consideration of this situation to exercise 4.3.3.
A natural question to ask is: if R and S are both relations from A to B, which
(if any) of their properties are inherited by R ∩S and R ∪S? Since the four
properties of relations deﬁned in the previous section are for relations on a set, we
shall further restrict our attention to the case where R and S are both relations on
the same set A.
Consider ﬁrst the reﬂexive property. If R and S are both reﬂexive then (a, a) ∈R
and (a, a) ∈S for every a ∈A. Thus (a, a) belongs to R ∩S and to R ∪S for
every a ∈A, so the intersection and union of R and S are both also reﬂexive.

172
Relations
Secondly, suppose that R and S are both symmetric. Then so, too, are R ∩S
and R ∪S. We show this for the intersection only—the argument for the union
is similar (see exercise 4.3.4(i)). Let a, b ∈A be such that a(R ∩S)b or, in set
notation, (a, b) ∈R ∩S. Then a R b and a S b. Since R and S are both symmetric
this implies b R a and b S a, which means that (b, a) ∈R ∩S. Thus a(R ∩S)b
implies b(R ∩S)a, so R ∩S is symmetric.
The situation for anti-symmetry is more complicated. If R and S are both anti-
symmetric, then an argument along the lines of that above for symmetry shows
that the intersection R ∩S is also anti-symmetric. However, the union need not
be anti-symmetric. In order to demonstrate this we need to produce a counter-
example; that is, an example of anti-symmetric relations R and S whose union
R ∪S is not anti-symmetric. A simple example is the following. Let A = {a, b},
R = {(a, b)} and S = {(b, a)}. The relations R and S are both anti-symmetric
since we never have x related to y, and y related to x for different elements x and
y. However, the union R ∪S = {(a, b), (b, a)} is not anti-symmetric because a is
related to b, b is related to a, but a and b are not equal.
The situation for transitivity is similar to anti-symmetry: namely, if R and S are
both transitive then R ∩S is also transitive, but R ∪S need not be. The proof
that R ∩S is transitive is similar to the proof for symmetry and we again leave
it as an exercise (4.3.4(ii)). The simplest possible counter-example which shows
that R ∪S need not be transitive requires a three-element set A. (Why is this the
simplest case?) Let A = {a, b, c}, R = {(a, b)} and S = {(b, c)}. Then R and
S are both transitive in a rather trivial way: we never have different elements x,
y and z such that x is related to y and y is related to z, so the relations cannot
fail to be transitive. (The transitive property is a conditional: if x R y and y R z
then x R z. Recall that a conditional proposition p →q is true whenever p is
false. Thus if x R y and y R z is false for all x, y, z ∈A then R has the transitive
property.) However, R ∪S = {(a, b), (b, c)} is not transitive since a is related to
b and b is related to c, but a is not related to c.
We summarize these considerations in the following theorem.

Intersections and Unions of Relations
173
Theorem 4.1
Let R and S be two relations on the same set as A.
(i)
If R and S are both reﬂexive then so, too, are R ∩S and R ∪S.
(ii)
If R and S are both symmetric then so, too, are R ∩S and R ∪S.
(iii)
If R and S are both anti-symmetric then so, too, is R∩S but R∪S
need not be anti-symmetric.
(iv)
If R and S are both transitive, then so, too, is R∩S but R∪S need
not be transitive.
Exercises 4.3
1.
Two relations R and S on the set A = {a, b, c, d} are deﬁned by:
R = {(a, b), (a, c), (a, d), (b, b), (b, c), (c, a), (c, d)}
S = {(a, b), (a, c), (c, b), (c, d), (d, a)}.
(i)
Find R ∩S and R ∪S.
(ii)
Draw the directed graphs of R, S, R ∩S and R ∪S.
(iii)
Write down the binary matrices of R, S, R ∩S and R ∪S.
2.
Let R and S be relations on a set A.
(i)
Explain how the directed graphs of R ∩S and R ∪S are related to
the directed graphs of R and S.
(ii)
Explain how the binary matrices of R ∩S and R ∪S are related to
the binary matrices of R and S.
3.
Let R1 be a relation from A1 to B1 and let R2 be a relation from A2 to
B2. Show that R1 ∩R2 and R1 ∪R2 are both relations from A1 ∪A2 to
B1 ∪B2.
4.
Let R and S be relations on the same set A. Prove that:
(i)
if R and S are both symmetric then so is R ∪S;
(ii)
if R and S are both transitive then so is R ∩S.
Questions 5–11 refer to the composite of two relations which is deﬁned
as follows. Let R be a relation from A to B, and S be a relation from B

174
Relations
to C. The composite of R and S is the relation S◦R from A to C deﬁned
by a(S ◦R)c if and only if there exists an element b ∈B such that a R b
and b S c. This is illustrated in the following diagram.
5.
Let A = {1, 2, 3, 4} and deﬁne two relations R and S on A by:
R = {(1, 3), (2, 2), (3, 1), (3, 4), (4, 2)}
S = {(1, 2), (2, 3), (3, 4), (4, 1)}.
(i)
List the elements of the relations S ◦R and R ◦S.
(ii)
List the elements of the relations R−1, S−1, (S ◦R)−1 and
(R ◦S)−1.
(iii)
List the elements of the relations R−1 ◦S−1 and S−1 ◦R−1.
(iv)
What do you notice about the relations in parts (ii) and (iii)? Can
you prove the general result that this suggests?
6.
A relation R on the set A = {a, b, c, d, e, f, g, h} has the following
directed graph.
(i)
List the elements of R.
(ii)
List the elements of R ◦R.
(iii)
Draw the directed graph of the relation R ◦R.

Equivalence Relations and Partitions
175
7.
Let R and S be the relations on a set A of people deﬁned by:
x R y
if and only if x is the mother of y;
x S y
if and only if x is the father of y.
Describe the relations (i) S ◦R, and (ii) R ◦S.
8.
Let R be the relation on Z+ deﬁned by
n R m
if and only if m = n2.
Describe the relation R2 = R ◦R on Z+.
9.
Let R be a relation from A to B and let IA and IB be the identity relations
on A and B respectively. (See exercise 4.1.8.) Show that: (i) R ◦IA = R,
and (ii) IB ◦R = R.
10.
Let R be a relation from A to B, S a relation from B to C, and T a
relation from C to D. Show that (T ◦S) ◦R = T ◦(S ◦R).
11.
Let R be a relation from A to B and let S be a relation from B to C.
Describe the relationship between the types of R, S and S ◦R.
4.4
Equivalence Relations and Partitions
One of the most important types of relation is an equivalence relation on a set. In
this section we deﬁne the notion of an equivalence relation and explore the close
connection between equivalence relations and partitions of a set.
Consider the relation R on the set of living people deﬁned by: x R y if and only if
x resides in the same country as y. Assuming each person is resident in only one
country, the relation satisﬁes three obvious properties:
x resides in the same country as x; that is, R is reﬂexive;
if x resides in the same country as y, then y resides in the same country as
x; that is, R is symmetric;

176
Relations
if x resides in the same country as y, and y resides in the same country as z,
then x resides in the same country as z; that is, R is transitive.
Any given element x is related to everyone who lives in the same country as x
and to no one else. Therefore the relation subdivides the set of living people
into subsets according to their countries of residence. This is an example of an
equivalence relation, which we now deﬁne formally.
Deﬁnition 4.3
A relation R on a set A is an equivalence relation if R is reﬂexive,
symmetric and transitive.
Examples 4.4
1.
Let A = R, the set of real numbers, and deﬁne a relation R on A by
x R y
if and only if x2 = y2.
Then:
R is reﬂexive since x2 = x2 for every real number x;
R is symmetric since x2 = y2 implies y2 = x2;
R is transitive since x2 = y2 and y2 = z2 implies x2 = z2.
Therefore R is an equivalence relation.
2.
Let A = R2 −{(0, 0)}, the set of points in the plane except the origin†,
and deﬁne a relation R on A by
(a, b) R (c, d)
if and only if (a, b) and (c, d) both lie on the
same straight line through the origin.
Clearly R is both reﬂexive and symmetric. Also it is not difﬁcult to see
that R is transitive: if (a, b) and (c, d) both lie on the same straight line
through the origin, and similarly (c, d) and (e, f) both lie on the same
straight line through the origin, then so, too, do (a, b) and (e, f).
† For obvious reasons the set A is often referred to as the punctured plane.

Equivalence Relations and Partitions
177
Therefore R is an equivalence relation.
3.
Let A = Z, the set of integers, and deﬁne a relation R on A by
n R m
if and only if n = 2km for some integer k.
Show that R is an equivalence relation.
Solution
Firstly, R is reﬂexive since n = 20n for every integer n.
Secondly, if n = 2km then m = 2−kn so n R m implies m R n; therefore R is
symmetric.
Thirdly, suppose n R m and m R p; then there exist integers k and l such that n =
2km and m = 2lp. Combining these two equations gives n = 2k2lp = 2k+lp
where k + l is an integer. Thus n R m and m R p implies n R p so R is transitive.
4.
Consider the relation R deﬁned on Z+ by
n R m
if and only if n divides m.
R is not an equivalence relation. To show this we only need to show
that one of the three properties is not satisﬁed by R. Clearly R is not
symmetric since, for example, 2 divides 4 but 4 does not divide 2.
(Note, however, that R is both reﬂexive and transitive. In fact R is also
anti-symmetric because if n divides m and m divides n then n = m.
Of course, these facts are not important in showing that R is not an
equivalence relation.)
As we mentioned in chapter 3, there is a close connection between partitions of
a set and equivalence relations on the set. Recall that a partition of a set A is
a family of non-empty subsets which are pairwise disjoint and whose union is
all of A (see deﬁnition 3.4). Suppose R is an equivalence relation on A. We
can form subsets by grouping together in the same subset all elements which are
related. We shall see that the properties of the equivalence relation guarantee that
the subsets formed in this way form a partition of A. These subsets are called the
‘equivalence classes’ of the relation which we now deﬁne formally.

178
Relations
Deﬁnition 4.4
Let R be an equivalence relation on a set A, and let x ∈A.
The
equivalence class of x, denoted [x], is the set of all elements of A to which
x is related:
[x] = {y ∈A : x R y}.
Note that, since R is symmetric, the equivalence class of x is also equal to
{y ∈A : y R x}. In other words, the equivalence class of x can equally well be
deﬁned either as the set of elements that are related to x or as the set of elements
to which x is related. Sometimes, if we need to emphasize the relation R, we refer
to the R-equivalence class of x which we denote by [x]R.
Examples 4.5
1.
Let R be the equivalence relation on Z+ deﬁned in example 4.1.3 by
n R m if and only if n −m is divisible by 2. Then:
[1] = {1, 3, 5, 7, 9, . . .}
[2] = {2, 4, 6, 8, 10, . . .}
[3] = {1, 3, 5, 7, 9, . . .}
[4] = {2, 4, 6, 8, 10, . . .} etc.
In this example there are clearly only two different equivalence classes—
the sets of even and odd positive integers respectively. Note that these
two equivalence classes form a partition of Z+.
2.
Let R be the equivalence relation deﬁned on the set of integers Z by nRm
if and only if n2 = m2.
For each integer n, only n R n and n R (−n) so the equivalence class of
n contains two integers, namely n and its negative:
[n] = {n, −n}.
There is one exception: since 0 equals its negative, the equivalence class
of 0 contains only itself, [0] = {0}.
3.
Let R be the relation on the punctured plane R2 −{(0, 0)} deﬁned in
example 4.4.2 by (a, b) R (c, d) if and only if (a, b) and (c, d) lie on the
same straight line through the origin.

Equivalence Relations and Partitions
179
Let (x, y) be any point in R2 −{(0, 0)}.
The equivalence class of
(x, y) is the set of all points (except the origin which is not an element
of the punctured plane itself) which lie on the line through (0, 0) and
(x, y). In this case we can visualize the equivalence classes geometrically.
There are inﬁnitely many different classes in this example; one for each
(punctured) line through the origin.
4.
Let A be any non-empty set of people and deﬁne a relation R on A by
x R y if and only if x is the same height as y (measured, let us say, to the
nearest centimetre). Then R is an equivalence relation on A.
For any person in the set, his or her equivalence class is the set of all
people (in the set A) who are the same height.
It might seem from the deﬁnition that there are as many equivalence classes as
there are elements of the set A. However, the above examples show that there are
many fewer distinct (i.e. unequal) equivalence classes in general. This is because
if two elements are related then their equivalence classes are equal. To see this,
suppose that R is an equivalence relation on A and xRy for two elements x and y
of A. We wish to show that [x] = [y]. Let z ∈[x]; then x R z by deﬁnition. Since
xRy and R is symmetric we know that yRx also. Thus yRx and xRz; it follows
that y R z, by the transitivity of R, so z ∈[y]. This shows that [x] ⊆[y]. The
proof that [y] ⊆[x] is similar, so we can conclude that [x] = [y]. The converse is
also true; namely, if [x] = [y] then x is related to y. This is very easy to prove:
y ∈[y] since R is reﬂexive, so y ∈[x] which means x R y by deﬁnition. We have
proved the following result.
Theorem 4.2
Let R be an equivalence relation on A and x, y ∈A. Then [x] = [y] if and
only if x R y.
The observation made in theorem 4.2 paves the way for us to prove the result
mentioned above: the family of equivalence classes of an equivalence relation on
a set form a partition of the set.

180
Relations
Theorem 4.3
Let R be an equivalence relation on a non-empty set A. The family of
distinct R-equivalence classes is a partition of A.
Proof
A partition must be a family of non-empty sets—see deﬁnition 3.4. This is clearly
satisﬁed here.
Since R is reﬂexive, x ∈[x] for every x ∈A.
Therefore,
every equivalence class is non-empty. This also shows that the ﬁrst property of
a partition is satisﬁed. Since x ∈[x] for every x ∈A, it also follows that every
element of A belongs to some equivalence class, namely its own. Therefore the
union of all the equivalence classes contains all the elements of A.
The second property of a partition, that the members of the family of subsets
should be pairwise disjoint, sometimes causes confusion because equivalence
classes need not be disjoint according to theorem 4.2. The point here is that it
is the family of distinct equivalence classes which we are considering†. So we
must show that any two distinct classes are disjoint. In fact it is easier to prove
the contrapositive: if two classes have elements in common (are not disjoint) then
they are equal (are not distinct).
So suppose [x] ∩[y] ̸= ∅. Then we may choose an element z in the intersection.
Thus x R z since z ∈[x] and y R z since z ∈[y]. The symmetric and transitive
properties of R imply that x R y, so we conclude [x] = [y] by theorem 4.2.
We have now checked both properties of a partition, so the proof is complete. □
Examples 4.6
1.
Recall from example 3.19.3(iii) that the ‘ﬂoor’ or ‘integer part’ of a real
number is the largest integer that is less than or equal to the given real
number. For example,
⌊2.4⌋= 2
⌊−3.8⌋= −4
⌊
√
10⌋= 3 etc.
† From a theoretical point of view we do not need to emphasize that we are considering the family of
distinct classes as this is taken care of automatically. Since the family of equivalence classes is a set
of classes, our usual convention for sets, that we disregard any repeated elements, applies.

Equivalence Relations and Partitions
181
Let R be the relation on the set R of real numbers deﬁned by x R y if and
only if ⌊x⌋= ⌊y⌋. It is straightforward to check that R is an equivalence
relation. Consider 1
2 ∈R: since ⌊1
2⌋= 0, the equivalence class of 1
2 is
[ 1
2] = {x ∈R : ⌊x⌋= 0} = {x ∈R : 0 ⩽x < 1}.
This set, called a half-open interval, is denoted [0, 1). Similarly,
[2.4] = {x ∈R : ⌊x⌋= ⌊2.4⌋= 2} = {x ∈R : 2 ⩽x < 3} = [2, 3).
In fact every equivalence class is a half-open interval of the form [n, n+1)
for some integer n, so the partition of R by equivalence classes is
{[n, n + 1) : n ∈Z}.
2.
A relation R is deﬁned on the set R of real numbers by
x R y if and only if (x = 0 = y) ∨(xy > 0).
We leave it as an exercise to check that R is an equivalence relation. (This
is not difﬁcult although, since there are two cases in the deﬁnition of xRy,
the proof of each property requires consideration of cases.) What are the
equivalence classes? Since the equivalence classes form a partition of R,
we can adopt the following simple strategy for ﬁnding all the equivalence
classes:
(I)
choose any a ∈R and ﬁnd its equivalence class [a];
(II)
choose b /∈[a] and ﬁnd [b];
(III)
choose c /∈[a] ∪[b] and ﬁnd [c];
(IV)
continue in this way, at each stage choosing x ∈R that does
not belong to any existing equivalence class, until it is no longer
possible to choose such an x.
So we ﬁrst select a = 1, say. Since 1 ̸= 0 we have 1 R y ⇔1 × y > 0 ⇔
y > 0, so
[1] = {y ∈R : y > 0} = R+, the set of positive real numbers.
Next we must choose b /∈R+; so let b = 0. Now 0 R y ⇔y = 0 so the
equivalence class is a singleton set
[0] = {0}.
Now we must select c /∈R+ ∪{0}; so let c = −1. Again −1 ̸= 0 so
−1 R y ⇔−1 × y > 0 ⇔−y > 0 ⇔y < 0. Therefore
[−1] = {y ∈R : y < 0} = R−, the set of negative real numbers.

182
Relations
Since every real number belongs to one of these equivalence classes
[1] = R+, [0] = {0} or [−1] = R−, we have found all the (distinct)
classes. Hence the partition of R into equivalence classes is
{R+, {0}, R−}.
3.
Deﬁne a relation R on P({1, 2, 3}), the power set of {1, 2, 3}, by A R B
if and only if |A| = |B|. For example, {1, 2} R {2, 3}, {1, 2}
±
R {3}, etc.
It is easy to verify that R is an equivalence relation on P({1, 2, 3})—
see exercise 4.2.4(v). For A ⊆{1, 2, 3}, the equivalence class of A
contains all those subsets of {1, 2, 3} with the same cardinality. Hence
the equivalence classes are:
[∅] = {∅}
[{1}] = {{1}, {2}, {3}}
[{1, 2}] = {{1, 2}, {1, 3}, {2, 3}}
[{1, 2, 3}] = {{1, 2, 3}}.
More generally, let F be any family of ﬁnite sets and deﬁne a relation R
on F by A R B if and only if |A| = |B|. Then again R is an equivalence
relation on F. Given A ∈F, its equivalence class [A] is the set of those
sets in F with the same cardinality as A. Therefore the equivalence
relation partitions F into subfamilies of sets with the same cardinality.
We have noted the connection between equivalence relations and partitions in
one ‘direction’ only—from equivalence relations to partitions. That is, given an
equivalence relation on a set we have deﬁned a partition by equivalence classes.
However, we can proceed in the other direction as well. If we are given a partition
of a set we can use it to deﬁne an equivalence relation in such a way that the
equivalence classes are the original subsets which make up the partition. This is
easily done. Given a partition {Si : i ∈I} of a set A, we deﬁne a relation R on
A by
x R y
if and only if x and y belong to the same subset Si of the
partition.
The properties of the partition mean that the relation is well deﬁned: given two
elements x and y of the set, each belongs to precisely one of the sets Si so we can
determine with certainty whether x R y or x
±
R y. It should be clear that R is an
equivalence relation. Every element of A belongs to the same subset as itself so
R is reﬂexive. The statements that guarantee that R is symmetric and transitive
are equally as trite!

Equivalence Relations and Partitions
183
The equivalence classes of this relation coincide with the original subsets of the
partition. An equivalence class [x] contains all those elements which belong to
the same subset in the partition as x, which means that [x] must be equal to the
subset containing x. Thus every equivalence class is one of the ‘original’ subsets
Si.
We can summarize the preceding remarks in the following theorem.
Theorem 4.4
Let {Si : i ∈I} be a partition of a set A. Then xRy if and only if x, y ∈Si
for some i ∈I deﬁnes an equivalence relation R on A whose equivalence
classes are precisely the sets Si in the partition.
Theorems 4.3 and 4.4 mean that we can pass freely back and forth between
equivalence relations and partitions. In a sense they are two aspects of the same
phenomenon.
Modulo Arithmetic
Let n be a ﬁxed positive integer. A relation, called congruence modulo n, is
deﬁned on the set Z of integers by
a ≡n b
if and only if a −b = kn for some k ∈Z.
Alternative notations for a ≡n b are a ≡b mod n or simply a ≡b if the value
of n has already been established. If a ≡n b we say that a is congruent to b
modulo n. Thus a is congruent to b modulo n if n divides their difference or,
equivalently, if a and b have the same remainder after division by n.
We leave it as a straightforward exercise to show that congruence modulo n is an
equivalence relation—see exercise 4.4.10.
Example 4.7
Before considering the general case we look ﬁrst at a speciﬁc example, that of
congruence modulo 5. Since we are ﬁxing n = 5 in this example, we write a ≡b
as an abbreviation for a ≡5 b.

184
Relations
In this case a ≡b if and only if a −b = 5k for some integer k; that is, if and only
if there exists an integer k such that a = 5k + b. Therefore
[p] = {q ∈Z : q = 5k + p, for some k ∈Z}.
All of the equivalence classes are inﬁnite; some of them are listed below:
[0] = {. . . , −10, −5, 0, 5, 10, 15, . . .}
[1] = {. . . , −9, −4, 1, 6, 11, 16, . . .}
[2] = {. . . , −8, −3, 2, 7, 12, 17, . . .}
[3] = {. . . , −7, −2, 3, 8, 13, 18, . . .}
[4] = {. . . , −6, −1, 4, 9, 14, 19, . . .}.
Clearly these ﬁve are all the distinct equivalence classes, since every integer is
contained in one of these. For instance
· · · = [−8] = [−3] = [2] = [7] = [12] = · · ·
since
· · · −8 ≡−3 ≡2 ≡7 ≡12 · · · .
Returning to the general case, it can be shown that the relation of congruence
modulo n partitions Z into the n distinct equivalence classes
[0], [1], [2], . . . , [n −1]†.
Let Zn = {[0], [1], [2], . . . , [n −1]} denote the set of equivalence classes. We can
deﬁne the arithmetic operations of addition and multiplication on the set Zn by
[a] +n [b] = [a + b]
and
[a] ×n [b] = [a. b].
This is not quite as straightforward as it might seem; hidden in these deﬁnitions is
a potential problem. The crucial point is that a given equivalence class has many
different ‘names’. For example, in the case of congruence modulo 5, we saw that
[−8], [−3], [2], [7], [12], etc. are different notations for the same equivalence class.
The potential problem with the deﬁnitions above of addition and multiplication on
Zn is that using different labels for the equivalence classes may produce different
results.
† To emphasize the role of n, these classes are sometimes denoted [0]n, [1]n, [2]n, . . . , [n −1]n.

Equivalence Relations and Partitions
185
Before we consider the general case, let us look again at the example of n = 5.
In this case [−8] = [2] and [4] = [19] so we would hope that [−8] +5 [4] and
[2]+5[19] would deﬁne the same equivalence class, and [−8]×5[4] and [2]×5[19]
would similarly deﬁne the same class. Now from the deﬁnition of addition on Zn,
[−8]+5 [4] = [−4] and [2]+5 [19] = [21]. However, all is well since −4 ≡5 21 so
[−4] = [21]; the more convenient name for this particular class is [1]. Similarly
[−8] ×5 [4] = [−32] and [2] ×5 [19] = [38], but again these two classes are equal
since −32 ≡5 38.
Returning once more to the general case, to show that there really is no problem
with our deﬁnitions of addition and multiplication on Zn, we need to prove:
if [a] = [a′] and [b] = [b′] then [a + b] = [a′ + b′] and [ab] = [a′b′].
The actual proof is not difﬁcult and is left as an exercise—see exercise 4.4.11.
We now have a well deﬁned ‘arithmetic modulo n’. The arithmetic of these sets
Zn is important to mathematicians as they are all examples of a mathematical
structure called a ring. In computer science the systems of arithmetic modulo 2,
modulo 8 and modulo 16 have some importance.
Some of these ‘ﬁnite arithmetics’ as we may call them have some slightly unusual
properties. For instance, the product of two non-zero elements (i.e. classes other
than [0]) may sometimes be zero, [0]. In Z6, for example, [3]6 ×6 [4]6 = [0]6. We
leave it as an investigation to determine for what values of n this can occur. (See
exercise 4.4.12.)
We conclude this section with the tables for addition and multiplication in the set
Z5.
Addition
+5
[0]
[1]
[2]
[3]
[4]
[0]
[0]
[1]
[2]
[3]
[4]
[1]
[1]
[2]
[3]
[4]
[0]
[2]
[2]
[3]
[4]
[0]
[1]
[3]
[3]
[4]
[0]
[1]
[2]
[4]
[4]
[0]
[1]
[2]
[3]
Multiplication
×5
[0]
[1]
[2]
[3]
[4]
[0]
[0]
[0]
[0]
[0]
[0]
[1]
[0]
[1]
[2]
[3]
[4]
[2]
[0]
[2]
[4]
[1]
[3]
[3]
[0]
[3]
[1]
[4]
[2]
[4]
[0]
[4]
[3]
[2]
[1]

186
Relations
Exercises 4.4
1.
A relation R on the set of integers Z is deﬁned by n R m if and only
if |n| = |m|. Show that R is an equivalence relation and determine the
corresponding equivalence classes.
2.
(i)
A relation R on the set of real numbers R is deﬁned by xRy if and
only if ⌊2x⌋= ⌊2y⌋.
(a)
Verify that R is an equivalence relation.
(b)
Determine the equivalence classes of 1/4 and 1/2.
(c)
Describe the partition of R into the equivalence classes of
R.
(ii)
An equivalence relation S is deﬁned on R by x S y if and only if
⌊3x⌋= ⌊3y⌋. Determine the partition of R into the equivalence
classes of S.
3.
Let A be any non-empty set of people.
(i)
A relation R on A is deﬁned by P R Q if and only if P and Q are
the same age (in years). Show that R is an equivalence relation on
A and describe the equivalence classes of R.
(ii)
A second relation S on A is deﬁned by P S Q if and only if P and
Q were born in the same country. Given that S is an equivalence
relation on A, describe the equivalence classes of S.
4.
Show that both the identity relation IA and the universal relation UA, as
deﬁned in exercise 4.1.8, are equivalence relations on a set A. What are
the corresponding equivalence classes?
5.
Verify that each of the following are equivalence relations on the plane
R2 and describe the equivalence classes:
(i)
(x1, y1) R (x2, y2) if and only if x1 = x2.
(ii)
(x1, y1) R (x2, y2) if and only if x1 + y1 = x2 + y2.
(iii)
(x1, y1) R (x2, y2) if and only if x2
1 + y2
1 = x2
2 + y2
2.
6.
A relation R on Z+ × Z+ is deﬁned by
(m, n) R (p, q)
if and only if m + q = n + p.
Show that R is an equivalence relation and describe the equivalence
classes of (1, 1), (2, 1), (3, 1), (1, 2) and (1, 3).

Equivalence Relations and Partitions
187
How are the set of equivalence classes and the set of integers related?
7.
Verify that xRy if and only if (x−y) ∈Z deﬁnes an equivalence relation
on the set Q of rational numbers. Describe the equivalence classes of 2,
1
4 and −1
4.
8.
How many different equivalence relations are there on the sets (i)
{a, b, c}, and (ii) {a, b, c, d}?
9.
Let A = {n ∈Z : n ⩾2} = {2, 3, 4, 5, 6, . . .}. For n ∈A, let P(n)
denote the smallest prime number that divides n and let Q(n) denote the
largest prime number that divides n. For example:
P(14) = 2, P(15) = 3, P(16) = 2, P(17) = 17, P(18) = 2, . . .
Q(14) = 7, Q(15) = 5, Q(16) = 2, Q(17) = 17, Q(18) = 3, . . . .
(i)
Show that
n R m
if and only if P(n) = P(m)
deﬁnes an equivalence relation on A. List the ﬁrst few elements of
the equivalence classes of 2, 3 and 5.
(ii)
It is given that
n S m
if and only if Q(n) = Q(m)
also deﬁnes an equivalence relation on A.
List the ﬁrst few
elements of the equivalence classes of 2, 3 and 5.
10.
Show that, for a ﬁxed positive integer n, the relation of congruence
modulo n is an equivalence relation on Z.
Show also that a ≡n b if and only if a and b have the same remainder
after division by n.
11.
Show that addition and multiplication on Zn is well deﬁned. That is,
prove:
if
[a] = [a′]
and
[b] = [b′]
then
[a + b] = [a′ + b′]
and
[ab] = [a′b′].
12.
Draw up addition and multiplication tables for the set Zn for n = 3, 4, 6
and 7.

188
Relations
For which of these values of n do there exist non-zero elements [a]n and
[b]n such that [a]n ×n [b]n = [0]n?
What is the general condition on n such that there do not exist non-zero
elements [a]n and [b]n in Zn such that [a]n ×n [b]n = [0]n?
13.
Let A be any set of propositions and deﬁne R by
p R q
if and only if p ↔q is true.
Show that R is an equivalence relation on A. What are the equivalence
classes?
14.
A relation R on a set A is reﬂexive and satisﬁes the ‘circular’ property
if x R y and y R z then z R x, for all x, y, z, ∈A.
Show that R is an equivalence relation on A.
15.
Let R and S be equivalence relations on a set A. Show that R ◦S is an
equivalence relation on A if and only if R ◦S = S ◦R.
(The composite R ◦S of two relations is deﬁned in exercises 4.3.)
4.5
Order Relations
Many sets have a natural ordering of their elements. Probably the most familiar
example is the set of real numbers ordered by ‘magnitude’.
We are used to
statements such as 3 ⩽π, −4 < −3, 2 <
√
8 < 3, and x2 > 0 for every
non-zero x ∈R. Similarly, any family of sets is ordered by ‘inclusion’: if A ⊆B
we may regard A as being ‘smaller’ than B. As a non-mathematical example, a
set of people could be ordered by age or by height.
Unlike equivalence relations, there are various different types of order relation.
Consider the relations on R deﬁned by x < y and x ⩽y, respectively. These
have different properties; for example, the latter is reﬂexive, but the former is not.
There is another, perhaps less obvious, difference. Given any two real numbers x
and y, at least one of the statements x ⩽y and y ⩽x is valid, but this is not true
of the statements x < y and y < x.
The most general order relation we consider is called a ‘partial order’ which we
deﬁne as follows.

Order Relations
189
Deﬁnition 4.5
A partial order on a set is a relation which is reﬂexive†, anti-symmetric
and transitive.
A set together with a partial order is called a partially ordered set or,
somewhat less elegantly, a poset.
Examples 4.8
1.
The relation R on the set of real numbers deﬁned by x R y if and only if
x ⩽y is a partial order (see example 4.2.1).
However, the relation S deﬁned by x S y if and only if x < y is not a
partial order, since it is not reﬂexive.
2.
Let F be any family of sets and deﬁne a relation R on F by A R B
if and only if A ⊆B. Every set is a subset of itself so R is reﬂexive.
The anti-symmetry property is precisely the property we frequently use
to prove that two sets are equal: see theorem 3.1. The transitivity of ⊆is
dealt with in exercise 3.2.10(i). Therefore R is a partial order.
Again note that the strict inclusion of subsets, ⊂, is not a partial order
because it is not reﬂexive.
3.
The ‘divisibility’ relation on the set of positive integers Z+, deﬁned by
n R m if and only if n divides m, is a partial order. (Note: n divides m is
frequently written n|m.)
4.
The relation on the set of English words deﬁned by ‘the word w1 is related
to the word w2 if w1 = w2 or w1 comes before w2 in a dictionary’ is a
partial ordering. This is the usual alphabetical (or lexicographic) ordering
of words.
5.
A relation R is deﬁned on R2 by
(x1, y1) R (x2, y2)
if and only if either x1 < x2 or both x1 = x2
and y1 ⩽y2.
† Some authors do not require a partial order to be reﬂexive, although it is much more common to
include the condition.

190
Relations
Show that R is a partial order on R2.
Solution
Every (x, y) ∈R2 is related to itself by the second part of the deﬁnition of
R : x = x and y ⩽y. Hence R is reﬂexive.
To prove the anti-symmetric and transitive properties, it helps to note that
(x1, y1) R (x2, y2) implies x1 ⩽x2.
Suppose (x1, y1) R (x2, y2) and (x2, y2) R (x1, y1). Then x1 ⩽x2 and x2 ⩽x1,
so x1 = x2. This means that we must also have y1 ⩽y2 and y2 ⩽y1, so y1 = y2
as well. Hence (x1, y2) = (x2, y2) which shows that R is anti-symmetric.
Finally, to prove that R is transitive, suppose that (x1, y1) R (x2, y2) and
(x2, y2) R (x3, y3). Then x1 ⩽x2 and x2 ⩽x3. If x1 < x2 or x2 < x3 (or
both), then x1 < x3 which means that (x1, y1)R(x3, y3). The other possibility is
that x1 = x2 and x2 = x3. In this case we must have y1 ⩽y2 and y2 ⩽y3, since
(x1, y1) R (x2, y2) and (x2, y2) R (x3, y3). Therefore x1 = x3 and y1 ⩽y3 so
again (x1, y1) R (x3, y3). In both cases we may conclude that (x1, y1) R (x3, y3)
which shows that R is transitive.
This partial order may seem a little strange at ﬁrst, It is, however, very similar
to the alphabetical (lexicographic) ordering of words. To compare two ordered
pairs, we ﬁrst compare their initial elements; if these are unequal then we know
how the ordered pairs are related. If the ﬁrst elements are equal, then we need
to look at the second elements of the ordered pairs to see how they are related.
(Of course in the case of English words we may need to continue this process and
consider the third letter, the fourth, etc, until we can order the words.)
There is also a geometric way of visualizing this partial order. Let P1 and P2 be
the points in the plane with coordinates (x1, y1) and (x2, y2) respectively. Then
(x1, y1) R (x2, y2) if and only if either P1 is to the left of P2 or the points are on
the same vertical level and P1 is below (or coincides with) P2.
The following theorem says that any subset of a partially ordered set is
automatically a partially ordered set. It gives a way of generating many more
examples of partially ordered sets. The proof is straightforward and we leave it as
an exercise.

Order Relations
191
Theorem 4.5
Let R be a partial order on a set A, and let B be any subset of A. Then
S = R ∩(B × B) is a partial order on B.
Although the deﬁnition of the relation S looks somewhat technical, it is the
obvious relation on B. For b1, b2 ∈B we have b1 S b2 if and only if b1 R b2.
Therefore elements of B are related by S in exactly the same way as they are
related by R, when we consider them as elements of A. This relation S is called
the restriction of R to B, and we say that B inherits the relation S from the
relation R on A.
Maximal and Minimal Elements
According to theorem 4.5, any subset of the real numbers is partially ordered by
the relation ⩽. Some sets of real numbers ordered in this way will have a greatest
element and some will not, and similarly for the smallest or least element. For
example, the set of integers has no greatest or least element, but the set of positive
integers has a least element, namely 1, but no greatest element.
Clearly an ﬁnite subset of R will have both a greatest and a least element with
respect to this order. An inﬁnite subset of R, however, may or may not have a
greatest and/or least element. For example, the open interval
(0, 1) = {x ∈R : 0 < x < 1}
which contains an inﬁnite number of elements, has no greatest or least element.
However, its companion closed interval
[0, 1] = {x ∈R : 0 ⩽x ⩽1}
which also contains an inﬁnite number of elements, does have a greatest and a
least element, namely 1 and 0 respectively.
We should not be led by the case of subsets of R to believe that every ﬁnite poset
has a (single) greatest and a (single) least element. Consider the set of all proper
subsets of {a, b, c} ordered by inclusion ⊆. The least element is ∅but there is
no (single) greatest element since there are three different two-element subsets
of {a, b, c}. This example indicates that we should be more precise about our
meaning of greatest and least elements. The following deﬁnition of greatest and

192
Relations
least elements is the obvious one if we regard a R b to mean in some sense ‘a is
less than (or equal to) b’. The greatest element of a poset, for example, is then the
element which is ‘bigger’ than all the other elements.
Deﬁnition 4.6
Let R be a partial order on a set A. The greatest element of A (if it exists)
is the element α such that a R α for every a ∈A.
Similarly, the least element of A (if it exists) is the element β such that
β R a for every a ∈A.
Returning to the example of the proper subsets of {a, b, c} ordered by inclusion,
we can verify that there is no greatest element according to our deﬁnition.
However, each of the two-element subsets can be regarded as the ‘largest possible’
in the sense that there are no subsets which are ‘bigger’ than these. We formalize
this idea in the deﬁnition of ‘maximal’ elements.
Deﬁnition 4.7
Let A be a poset, with order relation R. An element x of A is maximal if,
for every a ∈A, x R a implies x = a.
Similarly, an element y is minimal if, for every a ∈A, aRy implies a = y.
If we regard a R b as meaning ‘a is less than or equal to b’ in whatever sense, then
an element is maximal if there is no ‘greater’ element in the set, i.e. the element
is related only to itself. Similarly an element is minimal if there is no ‘smaller’
element in the set, i.e. no other element is related to it.
Examples 4.9
1.
Consider again the proper subsets of {a, b, c} ordered by inclusion. In
this case there are three different maximal elements {a, b}, {b, c} and
{a, c}. There is a single minimal element, namely the least element ∅.

Order Relations
193
2.
Let A = {2, 3, 4, 5, 6, 7, 8}, ordered by divisibility: x R y if and only if x
divides y.
There are four minimal elements, 2, 3, 5 and 7. If a divides 2, where
a ∈A, then a = 2; and similarly for 3, 5 and 7.
The elements 5, 6, 7 and 8 are all maximal. For a ∈A, if 5 divides a then
a = 5; and similarly for 6, 7 and 8.
Note that, with this ordering, A has no greatest or least element. Clearly
the only candidates for a least element are the minimal elements, none of
which is the least element. For example, since 2
±
R 3 it is not true that
2 R a for every a ∈A, so 2 is not the least element. Also 3
±
R 2, 5
±
R 2 and
7
±
R 2 so neither 3 nor 5 nor 7 is the least element. Similar remarks apply
to the maximal elements, so there is no greatest element.
We have seen that a partially ordered set may have several minimal and/or
maximal elements. It can, however, have at most one greatest element and at
most one least element. That is, if a poset A has a greatest element α, then α
is unique; and similarly for a least element β. (We have, in fact, been tacitly
assuming this by referring to the greatest and least elements.) It is easy to see,
for example, that A has at most one least element: suppose β and β′ are two least
elements. Then β R β′ since β is a least element, and β′ R β, since β′ is a least
element. Therefore β = β′ (by anti-symmetry), so there is only one least element.
The same kind of argument clearly works for the greatest element as well.
The following theorem clariﬁes the connection between least and minimal
elements and between greatest and maximal elements.
Theorem 4.6
Let A be a poset with partial order relation R.
If A has a greatest element α, then α is maximal and there are no other
maximal elements.
Similarly, if A has a least element β, then β is minimal and there are no
other minimal elements.

194
Relations
Proof
We prove the proposition for the greatest element only; the proof for the least
element is similar.
Let α be the greatest element and suppose α R a where a ∈A. Since α is the
greatest element we also know that aRα. Therefore a = α, by the anti-symmetric
property, so α is a maximal element.
Suppose, now, that x is a maximal element. Since α is the greatest element, we
have x R α. By the maximal property of x this implies x = α, so α is the only
maximal element.
□
We have seen that, in a partially ordered set, there may be elements a and b such
that neither a R b nor b R a†. For our most familiar order relation, ⩽on R, this
cannot occur. A partial order such as this, where every pair of elements is related
(at least one way round), is called a ‘total order’.
Deﬁnition 4.8
A total order (or linear order) on a set A is a partial order R which
satisﬁes the following dichotomy law.
For every pair a, b ∈A, either a R b or b R a (or both).
Note that there is a certain amount of redundancy in the deﬁnition of a total order
in that the reﬂexive condition (which is included in the statement that R is a partial
order) follows from the dichotomy law. This is because if we let b = a then
this last condition implies a R a for every a ∈A. Thus a total order could be
deﬁned slightly more efﬁciently as a relation which is anti-symmetric, transitive
and satisﬁes the dichotomy law.
Examples 4.10
1.
The relation ⩽on R is a total order. Any subset of a totally ordered set is
also totally ordered by the same relation (exercise: prove this). Thus the
relation ⩽is a total order on any set of real numbers.
† Historically, orders where this cannot occur were studied before partial orders. The term partial
order was therefore required to emphasize the possibility that two elements need not be related.

Order Relations
195
2.
The alphabetical ordering of English words is a total order, as is the
related order on R2 given in example 4.8.5.
3.
Let F be any family of ﬁnite sets such that no two sets have the same
cardinality and R the relation on F deﬁned by A R B if and only if
|A| ⩽|B|. (Exercise: verify that this is a partial order.)
That this is a total order essentially follows from the fact that |A| is an
integer and ⩽is a total order on the set of integers.
4.
Let R be the relation on R2 deﬁned by (x1, y1) R (x2, y2) if and only
if x1 ⩽x2 and y1 ⩽y2. Again, we leave it as an exercise (4.5.3(i)) to
show that R is a partial order. It is not, however, a total order because, for
example, (0, 1) and (1, 0) are not related.
5.
Let A = {1, 2, 3, 4, 6, 12}, the set of factors of 12, ordered by divisibility.
A is not a totally ordered set because, for example, 2 and 3 are not related.
However, A does have subsets which are totally ordered by the inherited
relation of divisibility.
For example, the subsets {1, 2, 4, 12} and
{1, 3, 6, 12}—and of course any subsets of these—are totally ordered.
Subsets such as those in the last example are sufﬁciently important to be given a
name: a subset of a partially ordered set which is totally ordered by the inherited
relation is called a chain. Note that a chain may be ﬁnite, as in the previous
example, or inﬁnite in length; for example, the set {1, 2, 4, 8, . . . , 2k, . . .} is a
chain in Z+ ordered by divisibility. Of course, in a totally ordered set, every
non-empty subset is a chain.
Exercises 4.5
1.
Verify that the divisibility relation, nRm if and only if n divides m, n|m,
is a partial order on the set of positive integers. What is the least element?
2.
A class of students who have been studying relations has proposed
(incorrectly) that each of the following relations R on set A is a
partial order. For each relation, determine which property or properties
(reﬂexive, anti-symmetric, transitive) the relation fails to satisfy.

196
Relations
(i)
A = P({1, 2, 3});
B R C if and only if B ⊂C.
(ii)
A = P({1, 2, 3});
B R C if and only if |B| ⩽|C|.
(iii)
A = Z;
n R m if and only if n2 ⩽m2.
(iv)
A = R × R;
(x1, x2) R (y1, y2) if and only if x1 ⩽y1.
(v)
A is any non-empty set of people no two of whom are both the
same age and the same height; P R Q if and only if (age(P) ⩽
age(Q)) ∨(height(P) ⩽height(Q)).
(vi)
A = Z × Z;
(n, m) R (p, q) if and only if
n ⩽p ∨m ⩽q.
3.
(i)
Show that the relation R on the plane R2 deﬁned by
(x1, y1) R (x2, y2)
if and only if x1 ⩽x2 and y1 ⩽y2
is a partial order.
(ii)
More generally, show that if R is a partial order on a set A then the
relation R × R deﬁned by
(x1, y1)(R × R)(x2, y2)
if and only if x1 R x2 and y1 R y2
is a partial order on the Cartesian product A × A.
4.
Prove theorem 4.5.
5.
Show that, if F is any family of ﬁnite sets such that no two sets have the
same cardinality, then the relation R deﬁned on F by
A R B
if and only if |A| ⩽|B|
is a total order. (See example 4.10.3.)
Describe the maximal and minimal elements.
6.
Let A be a set of people. Under what circumstances does the relation
deﬁned by
x R y
if and only if x is younger than or the same age as y
deﬁne a partial order on A? (Assume, say, that age is measured to the
nearest day.)
In the situation where R is a partial order, show that it is in fact a total
order, and describe the greatest and least elements.

Order Relations
197
7.
Let F be a non-empty family of ﬁnite sets. A relation R is deﬁned on F
by:
A R B
if and only if A = B or |A| < |B|.
(i)
Show that R is a partial order on F. Describe the maximal and
minimal elements.
(ii)
Is R a total order in the case where F = P({1, 2, 3})? Explain
your answer.
8.
Show that
n R m
if and only if n = 2km for some k ∈Z
deﬁnes an equivalence relation on Z+ but
n S m
if and only if n = 2km for some k ∈N
deﬁnes a partial order relation on Z+.
9.
A strict order relation on a set A is a transitive relation which satisﬁes
the following trichotomy law.
For every a, b ∈A exactly one of the following three conditions hold:
a R b,
b R a,
a = b.
(i)
Show that the strict inequality relation < is a strict order on R.
(ii)
More generally, show that if R is a total order on A then the
relation R∗, deﬁned by x R∗y if and only if x R y and x ̸= y,
is a strict order on A.
10.
Let R be a strict order relation on A. Show that the relation R# deﬁned
by
x R# y
if and only if either x R y or x = y
is a total order on A. (Compare with question 9(ii) above.)
11.
Let A be a poset with order relation R, and let a1, a2, . . . , an be elements
of A such that a1 R a2, a2 R a3, . . . , an−1 R an, an R a1.
Show that a1 = a2 = · · · = an.
12.
Prove that the inverse relation R−1 of a partial order R is a partial order.
(See exercise 4.1.10 for the deﬁnition of R−1.) Prove also that an element

198
Relations
is maximal with respect to R if and only if it is minimal with respect to
R−1.
If R is a total order, is R−1 necessarily a total order as well?
If R is a strict order, is R−1 necessarily a strict order as well?
13.
A total order R on a set A is said to be a well ordering if every non-empty
subset of A has a least element with respect to R.
(i)
Show that every total order on a ﬁnite set A is a well ordering.
(ii)
Find an example of an inﬁnite set with a well ordering.
(iii)
Show that the usual (total) order relation ⩽is not a well ordering
on either the set Z of integers or on the set R+ of positive real
numbers.
4.6
Hasse Diagrams
Consider the set {1, 2, 3, 4, 5, 6} ordered by divisibility. The directed graph of
the relation was given in ﬁgure 4.4 above. Although the diagram is not very
complicated, it is apparent that for sets much larger than this the directed graph
would become too complex to be of much use. Since the partial order is reﬂexive
and transitive, we can obtain much the same information in a modiﬁed version of
the directed graph, called a ‘Hasse diagram’.
Let A be a ﬁnite set partially ordered by the relation R. We say that b covers a if
a R b and there is no element c such that a R c and c R b. More formally, b covers
a if a R b and, for all x ∈A, a R x and x R b implies either a = x or x = b.
The Hasse diagram of a ﬁnite poset can now be deﬁned as follows. The elements
of the set are represented as points in the plane and the points representing a and
b are joined by a rising line if and only if b covers a.
The Hasse diagram for {1, 2, 3, 4, 5, 6} ordered by divisibility is given in
ﬁgure 4.6 which is clearer and less complicated than its directed graph (ﬁgure 4.4).
We can still reconstruct the relation R from the Hasse diagram, given that we
know that R is a partial order. Every element is certainly related to itself since
R is reﬂexive. If a ̸= b then a R b if and only if there is a sequence of rising
lines connecting a to b. (We hope this is self-evident: a formal proof of this last

Hasse Diagrams
199
Figure 4.6
statement uses the transitivity of R and mathematical induction.) For example,
from ﬁgure 4.6 we can see that 1 R 6 either because there are rising lines from 1
to 2 and from 2 to 6, or because there are rising lines from 1 to 3 and from 3 to 6.
Examples 4.11
1.
The Hasse diagram for P{a, b, c} ordered by inclusion is given in
ﬁgure 4.7. We leave it as an exercise to draw the directed graph of this
relation as a comparison.
Figure 4.7
2.
A partial order R on a set A has the Hasse diagram in ﬁgure 4.8. List the
elements of R.
Solution
Since R is a partial order, every element of the set is related to itself.

200
Relations
Figure 4.8
For each element p of the set, we can ﬁnd all elements q (different from p) such
that pRq by following the lines upwards from the point p. For example, beginning
at the point b we ﬁnd:
b R b, b R r, b R x, b R y, b R s, b R z and b R t.
Following this procedure for each element in turn enables us to list the elements
of R:
R = {(a, a), (a, r), (a, x), (a, y), (b, b), (b, r), (b, s), (b, t), (b, x),
(b, y), (b, z), (r, r), (r, x), (r, y), (s, s), (s, y), (s, z), (t, t),
(t, z), (x, x), (y, y), (z, z)}.
3.
Draw a Hasse diagram of the partial order relation R on A
=
{a, b, c, p, q, x, y} given by
R = {(a, a), (b, b), (c, c), (p, p), (q, q), (x, x), (y, y), (a, p), (b, q),
(c, q), (x, a), (x, b), (x, p), (x, q), (y, b), (y, c), (y, q)}.
Solution
First note that p and q are maximal elements since the only ordered pairs in R
of the form (p, ) or (q, ) are (p, p) and (q, q). Similarly x and y are minimal
elements since the only ordered pairs in R of the form ( , x) or ( , y) are (x, x)
and (y, y). The element a is neither maximal nor minimal since x R a and a R p;
similarly neither b nor c is maximal or minimal. Hence we may arrange the
points representing the elements of A as shown in ﬁgure 4.9(a) with the maximal
elements at the top and the minimal elements at the bottom of the diagram.
Now a covers x since xRa but there is no element t ∈A such that xRt and tRa;
hence we join the points corresponding to x and a. Similarly p covers a so we join
the corresponding points. However, p does not cover x because x R a and a R p.
Continuing in this way, we obtain the Hasse diagram shown in ﬁgure 4.9(b).

Hasse Diagrams
201
Figure 4.9
4.
Show that neither of the conﬁgurations in ﬁgure 4.10 can occur anywhere
in the Hasse diagram of a poset.
Figure 4.10
Solution
In (a) the line joining a to c should not occur because a R b and b R c
which means that c does not cover a. Of course, a is related to c by the
transitive property.
The conﬁguration in ﬁgure 4.10(b) cannot occur for a similar reason. The
right-hand part of the diagram implies that a R b and b R d (assuming
transitivity), so d does not cover a.
The line joining a to d should
therefore be deleted.
5.
Let A = {a1, a2, . . . , an} be a ﬁnite set with a total order R. Then every
pair of elements is related, so given x, y ∈A either we can get from x
to y or we can get from y to x by a sequence of rising lines in the Hasse
diagram.
This means that, in the Hasse diagram, the elements are arranged in a
single vertical line as in ﬁgure 4.11. This diagram explains why a total
order is sometimes called a linear order.

202
Relations
Figure 4.11
The Hasse diagram of a ﬁnite poset shows clearly the greatest and least elements
(if these exist) as well as the maximal and minimal elements.
The least element has the property that every element can be reached from it by a
sequence of rising lines. For example, the empty set in ﬁgure 4.7 has this property.
The greatest element has the corresponding property that it can be reached from
every element by a sequence of rising lines. In ﬁgure 4.7, the set {a, b, c} has this
property.
Minimal elements are those which have no lines rising to them. They usually
occur at the bottom of the Hasse diagram with only rising lines coming from them,
although a minimal element could be represented by an isolated point, connected
to no lines at all if the element is related to nothing but itself. The dual property for
maximal elements is that they have no rising lines from them. They usually appear
at the top of the diagram, but again could be represented by isolated points. For
example, in ﬁgure 4.8, the minimal elements are a and b, and the maximal ones
are x, y and z.
A consideration of the possibilities for the Hasse diagram of a ﬁnite poset suggests
that there must be at least one minimal and at least one maximal element. This
we now prove.
Theorem 4.7
Let A be a ﬁnite non-empty poset. Then A contains at least one minimal
element and if there is only one it is the least element.
Similarly A must contain at least one maximal element and if there is only
one it is the greatest element.

Hasse Diagrams
203
Proof
As usual, we shall prove the ﬁrst part only since the proof of the second is similar.
Choose any element a1 of A. If a1 is minimal we are ﬁnished. Otherwise there
exists an element a2 such that a2 R a1. Either a2 is minimal or there exists a3
such that a3 R a2. Since A is ﬁnite this sequence of elements a1, a2, a3, . . . must
terminate at some element ak which must therefore be minimal.
Now suppose that A has a unique minimal element, β say. Let a1 be any element
of A different from β.
Then a1 is not minimal so, by the ﬁrst part of the
proof, there exists a sequence of elements a1, a2, a3, . . . , with each related to the
previous one, which must terminate at the minimal element β. Therefore β R a1,
for every a1 ∈A, so β is the least element.
□
Finally, we note that it is easy to identify chains—totally ordered subsets—from
the Hasse diagram of a poset. In the diagram a chain is seen as any part which
resembles ﬁgure 4.11. That is, a chain is a portion of the diagram consisting of a
single line with no branches. From ﬁgure 4.8 we can identify the chains; they are
the following subsets of {a, b, r, s, t, x, y, z}:
{a, r, x}, {a, r, y}, {b, r, x}, {b, r, y}, {b, s, y}, {b, s, z}, {b, t, z}
and, of course, any subsets of these.
Exercises 4.6
1.
Draw Hasse diagrams for each of the following sets under the divisibility
relation: n R m if and only if n divides m:
(i)
{1, 2, 3, 4, 6, 12}
(ii)
{1, 2, 4, 5, 10, 20}
(iii)
{1, 2, 4, 8, 16, 32}
(iv)
{1, 2, 3, 5, 6, 10, 15, 30}.
In each case identify the longest chain(s) (i.e. the chain(s) with the
greatest number of elements).
2.
The Hasse diagram of a partial order R on the set {a, b, c, d, e, f, g, h, i}
is given in the diagram below. List the elements of R and identify the

204
Relations
maximal and minimal elements.
3.
Let A be a poset with three elements. How many different kinds of Hasse
diagrams of A are possible? Hence ﬁnd the total number of different
partial orders that can be deﬁned on a set with three elements.
Repeat for a four-element poset.
4.
Draw the Hasse diagram for the set of non-empty proper subsets of
{a, b, c, d} ordered by inclusion.
Identify the maximal and minimal
elements and the chain(s) of greatest length.
5.
Let A = {0, 1, 2} × {2, 5, 8}
= {(0, 2), (0, 5), (0, 8), (1, 2), (1, 5), (1, 8), (2, 2), (2, 5), (2, 8)}.
A partial order relation R on A is deﬁned by
(a, b) R (c, d)
if and only if (a + b) divides (c + d).
(i)
Draw a Hasse diagram for the poset A.
(ii)
What are the maximal and minimal elements of the poset A? Does
A have a greatest and/or a least element?
6.
Let S be the set of non-empty subsets of {a, b, c}. A partial order relation
R on S is deﬁned by
A R B
if and only if either (A = {a} and a /∈B) or (A ⊆B).
(i)
Draw a Hasse diagram for the poset S.
(ii)
What are the maximal and minimal elements of the poset S? Does
S have a greatest and/or a least element?
7.
Let A = {n ∈Z : 2 ⩽n ⩽12}. A partial order relation R on A is
deﬁned by
a R b
if and only if either (a divides b) or (a is prime and a < b).

Application: Relational Databases
205
(i)
Draw a Hasse diagram for the poset A.
(ii)
Identify the least element and the maximal elements.
8.
A partial order relation R on A = {a, b, c, d, e, f, g} has the directed
graph given below. Draw its Hasse diagram.
4.7
Application: Relational Databases
The advent of relatively cheap computers has made ours the ‘information age’—
large quantities of information are routinely stored, retrieved and manipulated
electronically. A computer system designed to store and handle information is
called a database system. The software which controls the manipulation of the
stored data is called a database management system or DBMS.
There are many different ways of representing data; for example, as lists, tables,
diagrams of various kinds, and so on.
Any representation of data inevitably
imposes some kind of structure on them.
All database management systems
are designed assuming that data have a particular type of structure and the way
that a DBMS manipulates the stored data depends on a theoretical model of

206
Relations
the data themselves. Thus database management systems can be classiﬁed into
various types, the most common being relational, network (or ‘CODASYL’) and
hierarchical. More recently, object-oriented and deductive (or logical) database
systems have been developed to handle more sophisticated data structures and
relationships between data.
In this section we shall consider brieﬂy relational database systems which are
based on the mathematics of relations. The relational model was ﬁrst proposed by
E F Codd in a paper in 1970. Although the idea was initially greeted with some
scepticism, its potential was soon appreciated and most new database systems
developed in recent years have been relational. Networks and hierarchical systems
remain important largely because signiﬁcant quantities of data are still stored in
such systems.
Almost invariably a data item comprises, and is classiﬁed into, several parts. For
instance, an entry in an address book might be classiﬁed in one of the following
ways:
1.
Name, address, telephone number.
2.
Family name, ﬁrst name, address, telephone number.
3.
Family name, ﬁrst name, street number and name, town/city, county/state,
postcode/ZIP code, telephone area code, telephone number.
Each part of a data item is called an ‘attribute’. We shall always assume that data
items have a speciﬁed set of attributes and each attribute has a speciﬁed type.
Thus each data item itself has a deﬁned type, sometimes called the ‘record type’
of the item. Each of the three classiﬁcations above deﬁnes a different record
type. In the ﬁrst classiﬁcation, suppose that attribute ‘name’ has type String ,
attribute ‘address’ has type Address (a deﬁned type which may have both Integer
and String components) and attribute ‘telephone number’ has type Integer . Then
any corresponding data item has type String × Address × Integer (see §4.1). A
collection of data items all of the same type is called a ‘table’ or, sometimes,
a ‘relation’. A table deﬁned in this way (a set of items all of which have the
same type) is said to be in ﬁrst normal form and we shall assume all our sets
of data items have this property. Note that, since a table is a set of items, the
order in which they appear is not signiﬁcant. It is also worth noting that a table is
conceptual and need not correspond to any actual ﬁle stored on a computer disk or
other media. Any useful collection of data items of the same type may be deﬁned
as a table.

Application: Relational Databases
207
Deﬁnition 4.9
Data are classiﬁed into components (or ‘headings’) called attributes (or
ﬁelds). Each attribute has a speciﬁed type and this deﬁnes the type of
a data item, sometimes called its record type. A record instance is an
actual data item of a particular type and a table is a set of record instances
of the same type.
A detailed discussion of suitable guidelines for choosing record types for
particular kinds of data is beyond the scope of this book. However, it should be
clear that a record type with many attributes is more ﬂexible than one with fewer
attributes. This is illustrated in the following example, which we shall develop
further below.
Example 4.12
A charity, Goodworks, wishes to set up a database holding information regarding
its donors, their names, addresses, and telephone numbers, the details of their
contributions, and so on.
Goodworks initially decides to set up the database with a single table and to
classify the data using ﬁve attributes with the following names and types.
DONOR
NAME : String

208
Relations
DONOR
ADDRESS : Address
DONOR
TELEPHONE : Integer
DONATION
AMOUNT : Currency
DONATION
DATE : Date
Hence record instances have type String ×Address ×Integer ×Currency ×Date .
Table 4.1 shows ﬁve (ﬁctitious) record instances showing part of the table (in the
sense of deﬁnition 4.9) that comprises Goodworks’ database of donors.
Table 4.1
DONOR
NAME
DONOR
ADDRESS
DONOR
DONATION
DONATION
TELEPHONE
AMOUNT
DATE
Smith, A
33 New Street
9612-3993
£100
June 1996
Great Oldtown
XP3 9NJ
Smith, A
33 New Street
9612-3993
£250
Dec. 2000
Great Oldtown
XP3 9NJ
Smith, A
33 New Street
9612-3993
£150
July 2001
Great Oldtown
XP3 9NJ
Thomas, N
2A Oaks Road
2468-9753
£500
May 1994
Suburbia
Bigcity
BC3 5NR
Thomas, N
2A Oaks Road
2468-9753
£350
Oct. 1998
Suburbia
Bigcity
BC3 5NR
Since a donor’s complete address is labelled by a single attribute, it would be
difﬁcult to extract geographical information from this table. For instance, suppose
Goodworks is launching a special campaign in ‘Bigcity’ and wishes to write to all
past donors who live there. Obtaining a list of all such donors would be a difﬁcult
task from this table because the donor’s town or city has not been deﬁned as a
separate attribute. The required list of past donors whose address is in Bigcity
could be obtained more easily if the single attribute DONOR
ADDRESS were
replaced, say, by the following set of attributes: STREET, CITY, POSTCODE. (Here
we may assume that STREET contains all of the address coming before the town
or city name.) It would then be relatively straightforward to pick out the record
instances whose CITY attribute has the value ‘Bigcity’.

Application: Relational Databases
209
This example illustrates an important guideline for deﬁning attributes: each
potentially useful individual piece of information in a record instance should be
speciﬁed by an attribute. Of course this is not a precise rule for deﬁning attributes
because the meaning of ‘potentially useful individual piece of information’ will
depend on the context of the data usage. In the example above, if geographical
information is of no interest (and is never likely to be of any interest), it may be
perfectly acceptable to specify the donors’ addresses using a single attribute.
In the relational model of databases, all data is held in tables. The columns of a
table are headed by attribute names; the types of these attributes deﬁne the record
type of the table. Each row of a table represents a record instance of the given
type.
In general, suppose a table R has n attributes A1, A2, . . . , An. Informally, we say
that R has ‘attribute set’ or ‘attribute type’ (A1, A2, . . . , An). Each attribute Ai
has a particular type T i and a corresponding set of data entries. In example 4.12,
the attribute DONOR
NAME has type String and the corresponding set of data
entries would be the set of all the names of past donors to Goodworks. Similarly
the set of all donations made corresponds to the attribute DONATION
AMOUNT,
which has type Currency and so on. Let Xi denote the set corresponding to the
attribute Ai; thus Ai denotes the name of the attribute and Xi the set of values
attained by the attribute. The sets Xi are time dependent and may change as new
record instances are added to or existing record instances are deleted from the
table.
With this notation, a given record instance is an n-tuple (x1, x2, . . . , xn) where
each xi belongs to the set Xi, corresponding to the attribute Ai, and has type T i.
This ensures that all record instances have the same type T 1 × T 2 × · · · × T n. A
table is a collection of record instances of the same type; that is, a set of n-tuples
(x1, x2, . . . , xn). Recall that the set of all n-tuples (x1, x2, . . . , xn), where xi ∈
Xi for i = 1, 2, . . . , n, is the Cartesian product X1 × X2 × · · · × Xn. Therefore
a table R is just a subset of this Cartesian product R ⊆X1 × X2 × · · · × Xn.
This is the deﬁnition of an n-ary relation between the sets X1, X2, . . . , Xn. To
summarize: in the relational model of database system:
each table is just an n-ary relation, R ⊆X1 × X2 × · · · × Xn;
if each xi has type T i then each record instance has type T 1 × T 2 × · · · × T n;
each Xi has type Set [T i] and the table R has type Set [T 1 × T 2 × · · · × T n].
Example 4.13
For future reference, we label the attributes in example 4.12 as follows:
A1 : DONOR
NAME

210
Relations
A2 : DONOR
ADDRESS
A3 : DONOR
TELEPHONE
A4 : DONATION
AMOUNT
A5 : DONATION
DATE.
For each attribute Ai we suppose that there is a set Xi of individual data
items for the corresponding attribute.
Then a record instance is a 5-tuple
(x1, x2, x3, x4, x5) where xi ∈Xi for i = 1, . . . , 5.
With this record type there is likely to be a certain amount of duplication of
information. For instance, a donor’s name, address and telephone number are
recorded for every donation he or she makes. Apart from being a wasteful use of
the storage medium, this can cause problems in updating the record ﬁle. Suppose,
for example, that Ms A Smith, who has made three donations, moves to a new
address. To update the table, the new address would need to be changed in each
of her three record instances.
For these reasons it may be more sensible to split the data into two separate tables,
one with attributes
A1, A2, A3; DONOR
NAME, DONOR
ADDRESS, DONOR
TELEPHONE
and the other with the attributes
A1, A4, A5; DONOR
NAME, DONATION
AMOUNT, DONATION
DATE.
In this way much of the duplication of information in the original database is
avoided and most updating tasks are achieved more simply. The database would
now consist of two (related) tables, one a subset of X1 × X2 × X3 and the other
a subset of X1 × X4 × X5. Of course the attribute DONOR
NAME serves to link
the two tables.
Table 4.2 FILE ONE
DONOR
NAME
DONOR
ADDRESS
DONOR
TELEPHONE
Smith, A
33 New Street
9612-3993
Great Oldtown XP3 9NJ
Thomas, N
2A Oaks Road
2468-9753
Suburbia
Bigcity BC3 5NR
Tables 4.2 and 4.3 show how the information contained in table 4.1 would be
split into these two new tables, which we call FILE ONE and FILE TWO.
Individual record instances in FILE ONE have type String × Address × Integer

Application: Relational Databases
211
Table 4.3 FILE TWO
DONOR
NAME
DONATION
AMOUNT
DONATION
DATE
Smith, A
£100
June 1996
Smith, A
£250
Dec. 2000
Smith, A
£150
July 2001
Thomas, N
£500
May 1994
Thomas, N
£350
Oct. 1998
so FILE ONE itself has type Set [String × Address × Integer ]. Similarly record
instances in FILE TWO have type String × Currency × Date so FILE TWO
itself has type Set [String × Currency × Date ].
Example 4.13 indicates why it may be desirable to organize a database into several
related tables. This poses the question of how we can specify the relationship
between the different tables in the database and, in particular, how we can access
related record instances from different tables. Organizing Goodworks’ database
into two related tables, as suggested above, should not prevent us from obtaining
record instances with all ﬁve attributes, A1, A2, A3, A4, A5, as listed in table 4.1.
Before we turn to a consideration of how this can be achieved, we are now in a
position to deﬁne formally a relational database.
Deﬁnition 4.10
Let A1, A2, . . . , An be a collection of attributes and suppose with each Ai
there is associated a set Xi of data items. Each data item associated with
the attribute Ai has type T i, so Xi has type Set [T i].
A relational database with attributes A1, A2, . . . , An is a collection (or
set) of relations, each of which is a relation between some (possibly all) of
the sets Xi (i = 1, 2, . . . , n). Each relation R ⊆Xi1 × Xi2 × · · · × Xim
is called a table; its elements are called record instances and each record
instance has type T i1 × T i2 × · · · × T im.
Example 4.14
According to the formal deﬁnition, the (modiﬁed) Goodworks database deﬁned in
example 4.13 consists of two 3-ary relations, one a relation between X1, X2, X3

212
Relations
and the other a relation between X1, X4, X5. The database thus contains two
tables.
Record instances in a table can be accessed by a ‘key’. This is simply a set
of attributes whose values uniquely specify a record instance, but no proper
subset has this property of uniquely specifying record instances. In other words,
specifying the values of the attributes in the key determines the values of the other
attributes not belonging to the key, but specifying the values of only a subset of
these attributes would not necessarily determine the values of the other attributes.
In practice there may be several possible choices of key. A set of attributes which
could serve as a key is called a candidate key. One of these is selected to be used
as the actual key—it is called the primary key. In other words, the candidate
keys are the potential keys and the primary key is the one actually chosen to act
as key.
In the Goodworks database, {DONOR
NAME} is a candidate key to FILE ONE,
with attributes (DONOR
NAME,
DONOR
ADDRESS,
DONOR
TELEPHONE),
provided no two donors have the same name. If this is the case, each record
instance can be identiﬁed solely by the donor’s name. If, on the other hand, there
were two different donors with (exactly) the same name, then {DONOR
NAME}
would not be a key, but the attribute set {DONOR
NAME, DONOR
TELEPHONE}
might well be a key.
We now turn to the kind of information which can be obtained from a relational
database. Two basic types of operation which can be performed are the extraction
of a list of all record instances which satisfy a certain set of criteria and the
creation of new tables (relations) from the existing ones in the database. We
consider ﬁve fundamental operations which can be combined together to provide
most of the classes of information commonly required by database users.
Selection
The process of selection lists all record instances from a table which satisfy a
given set of criteria. The following commands are examples of selection.
1.
List all name and address records for customers whose address is in City
X.
2.
List all customers whose bank account is overdrawn and whose overdraft
exceeds their agreed limit.
3.
List the names and occupations of all students who graduated in 2000.

Application: Relational Databases
213
In order to be able execute these instructions the corresponding attributes need to
have been deﬁned. For example, as we have already explained, for command 1
above to be executable, the city of a customer’s address needs to be a separate
attribute. Similarly, account balance and overdraft limit both need to be attributes
of the appropriate table in order to be able to execute command 2. Likewise, year
of graduation must be one of the attributes if command 3 is to be performed.
Let R be a table with attributes A1, A2, . . . , Am and let ai be a speciﬁed value for
the attribute Ai. We wish to select all those record instances in R whose attribute
Ai has the value ai. This can be described mathematically as follows.
The table is an m-ary relation between the sets X1, . . . , Xm; that is, R ⊆
(X1 × X2 × · · · × Xm). The selection process is nothing other than deﬁning
the subset of R consisting of all m-tuples (x1, . . . , xm) whose xi entry is the
speciﬁed value ai:
{(x1, . . . , xm) ∈R : xi = ai}.
Selecting from R all record instances with the property that several attributes have
certain speciﬁed values also corresponds mathematically to deﬁning a subset of
R. For instance, to list all record instances whose attribute Ai has value ai, Aj
has value aj and Ak has value ak, we need to deﬁne the following subset of R:
{(x1, . . . , xm) ∈R : xi = ai, xj = aj and xk = ak}.
We can regard the selection process as deﬁning new tables, namely subsets
of given tables in the database. These new tables would probably have only
temporary existence; they would not be added to the collection of tables which
constitute the (theoretical model of the) database. It should be noted that a new
table obtained by selection has the same record type as the original table.
Selection can be described simply as follows.
The new table is obtained by
picking out those rows (record instances) which have the corresponding attribute
values. Since entire rows are selected to form the new table, it is clear that it must
have the same record type as the original table.
For example, the command ‘SELECT ALL DONATIONS MADE AFTER
DECEMBER 1996’, when performed on the part of the record ﬁle represented
in table 4.3, picks out the second, third and ﬁfth rows, as indicated in table 4.4.
Actually, this operation is a slight generalization of that deﬁned above. We have
implicitly assumed that the set of dates corresponding to the DONATION
DATE
attribute are ordered in the obvious way and we are selecting all those record
instances whose DONATION
DATE value is greater than some speciﬁed value,
namely December 1996.

214
Relations
Table 4.4
DONOR
NAME
DONATION
AMOUNT
DONATION
DATE
Smith, A
£100
June 1996
Smith, A
£250
Dec. 2000
Smith, A
£150
July 2001
Thomas, N
£500
May 1994
Thomas, N
£350
Oct. 1998
Projection
Whereas selection picks out certain rows from a table, the next operation we
describe, ‘projection’, picks out certain columns. Since the columns correspond
to attributes, it is clear that the resulting table has fewer attributes than the original.
We can formally describe projection as follows. Let R be a table with attributes
A1, . . . , Ap and let B1, . . . , Bq be attributes with q ⩽p such that each attribute
Bi is also an attribute of R; that is, each Bi = Aj for some j. Projection deﬁnes
a new table with attributes B1, . . . , Bq whose record instances comprise the Bi
attributes of each of the record instances of R.
Example 4.15
Consider again the Goodworks FILE TWO with attributes (A1, A2, A3) =
DONOR
NAME, DONATION
AMOUNT, DONATION
DATE),
part of which is
represented in table 4.3.
Projection onto the attributes (A1, A2)
=
(DONOR
NAME, DONATION
AMOUNT) produces the new table whose record
entries consist only of the name and donation amount attribute values. Thus
projection onto A1, A2 ‘forgets’ the donation date. This is illustrated in table 4.5.
Note that only the ﬁrst two columns of table 4.3 have been selected, but that the
new table has the same number of rows (record instances) as the original.
Mathematically, the projection operation, like selection, produces a new relation.
Since this is most easily described in terms of certain naturally deﬁned functions
on Cartesian products, we shall leave the mathematical description to the next
chapter which deals with functions: see §5.6.

Application: Relational Databases
215
Table 4.5
DONOR
NAME
DONATION
AMOUNT
Smith, A
£100
Smith, A
£250
Smith, A
£150
Thomas, N
£500
Thomas, N
£350
Natural Join
Suppose Goodworks has organized its database into the two tables, FILE ONE
and FILE TWO, as described in example 4.13. How is it then possible to obtain
a list of, say, all donor names, telephone numbers and donation amounts? The
problem is that the donor telephone numbers and the donation amounts are held in
different tables. We need a method of joining the tables together to produce a new
table with all three required attributes: DONOR
NAME, DONOR
TELEPHONE
and DONATION
AMOUNT.
Since the two tables also contain respectively
DONOR
ADDRESS and DONATION
DATE, the result of the ‘joining’ will produce
a table whose record type also includes these attributes. This is not a problem,
however, since we can then project the joined table onto the required record
type. In fact joining the two tables will produce the original ﬁve-attribute table
introduced in example 4.12.
The ‘natural join’ has the following mathematical basis. Suppose R and S are
tables with attributes A1, . . . , Ap, B1, . . . , Bq and A1, . . . , Ap, C1, . . . , Cr. Note
that we allow the possibility that p = 0 which represents the case where R and S
have no attributes in common. Their natural join is a new table with attributes
A1, . . . , Ap, B1, . . . , Bq, C1, . . . , Cr. The record instances which comprise the
natural join are all the (p + q + r)-tuples (x1, . . . , xp, y1, . . . , yq, z1, . . . , zr) with
the property that (x1, . . . , xp, y1, . . . , yq) ∈R and (x1, . . . , xp, z1, . . . , zr) ∈S.
Note that for simplicity we have listed the common attributes in R and S at
the beginning of the record type. In practice, this need not be the case, but to
describe the more general situation is notationally more complex. In set notation
the natural join of R and S is
{(x1, . . . , xp, y1, . . . , yq, z1, . . . , zp) : (x1, . . . , xp, y1, . . . , yq) ∈R
and (x1, . . . , xp, z1, . . . , zr) ∈S}.

216
Relations
Example 4.16
To obtain a list of all Goodworks donors, their addresses and their donations,
we ﬁrst need to join the tables FILE ONE with attribute type (A1, A2, A3)
and FILE TWO with attribute type (A1, A4, A5). This produces the table with
attribute type (A1, A2, A3, A4, A5) shown in table 4.1.
Then we project this
joined table onto (A1, A2, A4). The resulting table is shown below in table 4.6.
Table 4.6
DONOR
NAME
DONOR
ADDRESS
DONATION
AMOUNT
Smith, A
33 New Street
£100
Great Oldtown
XP3 9NJ
Smith, A
33 New Street
£250
Great Oldtown
XP3 9NJ
Smith, A
33 New Street
£150
Great Oldtown
XP3 9NJ
Thomas, N
2A Oaks Road
£500
Suburbia
Bigcity
BC3 5NR
Thomas, N
2A Oaks Road
£350
Suburbia
Bigcity
BC3 5NR
Union and Difference
Given two tables R and S of the same record type their union and difference
are both simply the usual (typed) set theory union R ∪S and difference R −S
respectively. Thus R ∪S is the table which contains all record instances in R
or in S (but does not list the repeats twice). It corresponds to pasting the table
representing S under that representing R and then deleting the repeated rows, if
any. The difference R−S is the table which contains all the record instances in R
which do not appear in S. The need for R and S to have the same set of attributes
is evident in both cases.

Application: Relational Databases
217
Exercises 4.7
The exercises refer to the following relational database of a ﬁctitious college
which contains information concerning its students, their current courses, etc.
Attributes:
A1 = ID
NUMBER
A2 = STUDENT
NAME
A3 = DATE
OF
BIRTH
A4 = DATE
OF
ENTRY
A5 = MAJOR
DISCIPLINE
B1 = CURRENT
COURSE #1
B2 = CURRENT
COURSE #2
B3 = CURRENT
COURSE #3
B4 = CURRENT
COURSE #4.
Tables:
PERSONAL
Attributes: (A1, A2, A3, A4)
DISCIPLINE
Attributes: (A1, A2, A5)
CURRENT COURSE
Attributes: (A1, B1, B2, B3, B4).
Parts of three tables, PERSONAL, DISCIPLINE and CURRENT COURSE, are
given below.
PERSONAL
M1452
Adams, K
23/06/71
1990
F3286
Johnson, D
15/12/69
1989
F5419
Kirby, F
29/07/63
1990
M3415
Singer, R
03/10/71
1989
F0278
Williams, L
19/03/70
1989
DISCIPLINE
M1452
Adams, K
CompSci
F3286
Johnson, D
Psyc
F5419
Kirby, F
Math/Econ
M3415
Singer, R
Hist
F0278
Williams, L
CompSci/Math
CURRENT COURSE
M1452
Comp100
Math150
Bus 105
Econ110
F3286
Psyc250
Psyc280
Psyc281
Soc 200
F5419
Math100
Math150
Econ110
Econ120
M3415
Hist210
Hist220
Lit 200
Stat120
F0278
Comp210
Comp230
Math205
Math215

218
Relations
1.
List the tables which result from performing each of the following
operations.
(i)
Select from PERSONAL those record instances who entered the
college in 1989.
(ii)
Project PERSONAL onto (A2, A3).
(iii)
Perform the natural join of PERSONAL and DISCIPLINE.
(iv)
Perform the natural join of PERSONAL and CURRENT
COURSE and then project the result onto (A2, B1, B2, B3, B4).
(v)
Using * as a ‘wildcard’ which can represent any number, select
from PERSONAL those record instances whose A1 attribute is
F**** and project the result onto (A1, A2).
2.
Select from CURENT COURSE those students with B3 attribute value
equal to ‘Econ110’.
Why does this selection not list all students who are currently
taking course Econ110?
Explain how to obtain a list from
CURRENT COURSE of those students currently taking Econ110.
3.
Perform the natural join of PERSONAL and DISCIPLINE and then
perform the natural join of the result with CURRENT COURSE.
Perform the natural join of PERSONAL with the result of performing the
natural join of DISCIPLINE and CURRENT COURSE.
Is the natural join operation associative in general? Justify your answer.
4.
Explain how the following lists of information can be obtained using the
operations described in the text. (Where necessary use * as a ‘wildcard’
which can stand for any character or number—see question 1(v) above
for an example.)
(i)
A list of student names and current courses.
(ii)
A list of student ID numbers and names for those students who
entered the college in 1990.
(iii)
A list of student names and current courses for those students
whose major disciplines include CompSci.
(iv)
A list of names, major disciplines and current courses of all
students.

Application: Relational Databases
219
(v)
A list of ID numbers, dates of entry and current courses of those
students born in 1971.

Chapter 5
Functions
5.1
Deﬁnitions and Examples
In this chapter we consider another of the central concepts of modern
mathematics, that of a function or mapping. Although functions have been used in
mathematics for several centuries, it is only comparatively recently that a rigorous
and generally accepted deﬁnition of the concept has emerged. When historians
come to write the history of mathematics in the second half of the twentieth
century, the rise in importance of functions of various kinds will almost certainly
be one of their major themes.
Like many of the concepts which we deal with in this book, that of a function is
both simple and very general. Instead of giving the deﬁnition immediately, we
shall begin with a notion with which you may very well be familiar from your
previous studies—that of a (real) variable. Traditionally labelled x, a variable is
often associated with expressions such as
x2 + 4x −7,
1/(x + 1)3,
sin x,
log x,
etc.
Expressions like these are frequently denoted f(x) and called ‘a function of (the
variable) x’. In such cases there is generally the assumption (which is often only
implicit) that the variable x refers to an ‘arbitrary’ real number, although it may
be subject to some restrictions, such as it must be positive. For us this idea of a
function is both too restrictive and somewhat incomplete, although it does point
towards a simpler and more general deﬁnition. The essence of the examples above
is that we can calculate (in principle, at least) the value of the expression for any
(allowed) value of the variable x. More important than an expression itself is the
fact that it provides a ‘rule’ for calculating its value given any value of x. Two
220

Deﬁnitions and Examples
221
different expressions f(x) and g(x) may give the same values for all real numbers
x, and we would regard the two expressions as deﬁning the same function. A
simple example of this is provided by the expressions f(x) = x2 + 4x −7 and
g(x) = (x + 2)2 −11.
In this book we will need to use functions where the ‘variable’ is not a real
number, nor even a number, but an element of some given set A. Thus it may
also be somewhat misleading to refer to a rule for ‘calculating’ the value of an
expression. With these points in mind, the following is a reasonable working
deﬁnition.
Working Deﬁnition
Let A and B be two sets. A function f from A to B, written f : A →B,
is a rule which associates to each a ∈A a unique element f(a) ∈B.
This is a very general deﬁnition, which includes the examples above as well as
many non-numerical examples. It is quite common to visualize the function rule
as being encapsulated in a ‘function machine’. This is a ‘black box’, illustrated
below, which has the property that if an element a ∈A is fed into the machine, it
produces as output the associated element f(a) ∈B.
Examples 5.1
1.
Let A = {a, b, c, d, e}, B = {α, β, γ, δ} and deﬁne a function f : A →
B by f(a) = β, f(b) = α, f(c) = f(d) = f(e) = δ.
An arrow diagram such as ﬁgure 5.1 is a useful way of visualizing a
function like this, where the sets A and B are ﬁnite.
The sets are
represented as regions of the plane and an arrow is drawn from each
element of A to its associated element of B. (Compare this with the
arrow diagram of a relation—ﬁgure 4.3.)
2.
The expression x2 + 4x −7 referred to above is not a function on its own
according to the working deﬁnition because the sets A and B have not

222
Functions
Figure 5.1
been speciﬁed. However, the expression can be used to deﬁne a function
f : R →R which associates to each real number x the real number
f(x) = x2 + 4x −7.
For example, 2 is associated with f(2) = 22 + 4 × 2 −7 = 5, −4 is
associated with f(−4) = (−4)2 + 4 × (−4) −7 = −7, and so on.
Note: we emphasize that a function is more than just the rule of association—we
should always specify the two sets involved as well.
3.
The second expression given above, 1/(x+1)3, cannot be used to deﬁne a
function from R to R. This is because 1/(x + 1)3 is not deﬁned for every
real number x. When x has the value −1, the expression is undeﬁned.
(Division by zero is not allowed: 1/0 is meaningless.)
However, since −1 is the only ‘troublesome’ element of R in this respect,
there is a function
f : R −{−1} →R
deﬁned by
f(x) = 1/(x + 1)3.
(Recall that R −{−1} = {x ∈R : x ̸= −1}.) The function f associates
1 with f(1) = 1/23 = 1/8, −4 with f(−4) = 1/(−3)3 = −1/27, etc.
An alternative approach is to view the association f(x) = 1/(x + 1)3 as
deﬁning a ‘partial function’ R →R. In a partial function, f(a) need not
be deﬁned for every a ∈A. See exercise 5.1.12 for details.
This example underlines the importance of the sets A and B in our
working deﬁnition. We have chosen the largest possible subset of R for
the set A, but we could have been more restrictive and deﬁned a different
function using the same expression, say g : R+ →R, g(x) = 1/(x+1)3.
4.
Another non-numerical example is the following. Let A = {living human
beings} and B = {human beings, living or dead}. A function f from A

Deﬁnitions and Examples
223
to B could be deﬁned by associating to each person his or her mother.
Symbolically,
f : A →B,
f(p) = the mother of p.
One of the drawbacks with our working deﬁnition is that it begs an important
question: what do we mean by a rule? Intuitively, a rule in the sense of the
working deﬁnition is some method of speciﬁcation whereby, given any a ∈A,
the element f(a) ∈B can be determined, at least in principle. If A is a ﬁnite
set this could be achieved by tabulating its elements alongside their associated
elements of B.
For instance, the function deﬁned in example 5.1.1 could be tabulated as follows.
A
B
a
β
b
α
c
δ
d
δ
e
δ
A more concise way of doing the same thing would be to list the pairs (a, f(a))
for each element a of A. The list (a, β), (b, α), (c, δ), (d, δ), (e, δ) completely
speciﬁes the rule of association which deﬁnes the function from A to B. Since
each member of this list is an ordered pair, we are simply deﬁning a subset of the
Cartesian product A × B.
In this example we have used the given rule of association to deﬁne the subset of
A × B consisting of all pairs (a, f(a)) where a ∈A. Changing our perspective
slightly, we can regard a subset of the Cartesian product as deﬁning a rule of
association itself. In other words specifying a rule is nothing more or less than
specifying a subset of A × B. This leads us then to our formal deﬁnition of a
function.

224
Functions
Deﬁnition 5.1
Let A and B be sets. A function f from A to B, written f : A →B, is a
subset f ⊆(A × B) which satisﬁes:
(∗)
for each a ∈A there exists a unique b ∈B such that (a, b) ∈f.
The set A is called the domain, and the set B the codomain, of f. If
(a, b) ∈f the element b ∈B is called the image of a ∈A and is written
b = f(a), or f : a 7→b†.
A function is also called a mapping or a transformation.
The condition (∗) on the subset f of A × B corresponds to the condition in the
working deﬁnition that each a ∈A is associated with a unique element f(a) ∈B.
Recall from chapter 4 that a relation from a set A to a set B is a subset of A × B.
According to deﬁnition 5.1, therefore, a function f : A →B is just a special kind
of relation from A to B—one which satisﬁes the property (∗).
Deﬁnition 5.2
Two functions f : A →B and g : A′ →B′ are equal if:
(i)
A = A′,
(ii)
B = B′, and
(iii)
f(a) = g(a) for all elements a belonging to A = A′.
Although this is the most common (and probably the most useful) deﬁnition of
equality of functions, it is slightly at odds with what we might expect. Since the
functions f : A →B and g : A′ →B′ are both sets (subsets of A×B and A′×B′,
respectively), we ought perhaps to deﬁne them to be equal if they contain the
same elements (see §3.1). Now f contains an ordered pair (a, b) for every a ∈A
(and similarly for g). Therefore, if the sets f and g contain the same elements,
it follows that A = A′ and f(a) = g(a) for every a ∈A = A′. However,
† Note that the barred arrow, 7→, is used exclusively for denoting the image of an element to avoid
confusion with the function itself, f : A →B.

Deﬁnitions and Examples
225
the second condition of deﬁnition 5.2 is not implied by the equality of the sets f
and g. Suppose, for example, that as sets f = g = {(a, 1), (b, 2), (c, 3)}. Then
A = A′ = {a, b, c}, f(a) = g(a) = 1, f(b) = g(b) = 2 and f(c) = g(c) = 3.
However, all that can be said about the codomains, B and B′, is that they must
both contain the elements 1, 2 and 3; they need not be equal.
We shall see later that it is highly desirable to impose the additional condition for
equality of two functions that their codomains be equal.
Examples 5.2
1.
The function f : A →B deﬁned informally in example 5.1.1 can now be
deﬁned formally as the set f ⊆A × B, where
f = {(a, β), (b, α), (c, δ), (d, δ), (e, δ)}.
2.
Similarly the function f : R →R of example 5.1.2 is deﬁned formally as
the set
f = {(x, y) ∈R × R : y = x2 + 4x −7}.
3.
Again in the same way, the function f : A →B given in example 5.1.4
is deﬁned as the set
f = {(a, b) ∈A × B : b is the mother of a}
where A = {living humans} and B = {humans, living or dead}.
4.
Other familiar functions can be deﬁned in this manner. For instance,
the ‘square’ and ‘cube’ functions f and g from R to R are respectively
deﬁned as the sets
f = {(x, y) ∈R × R : y = x2}
and
g = {(x, y) ∈R × R : y = x3}.
5.
Let A be any set. The identity function idA : A →A is deﬁned by
idA = {(x, x) : x ∈A}.
The identity function is simply the identity relation on A—see
exercise 4.1.8(i). Less formally we could write the identity function as
idA : A →A, idA(x) = x, for all x ∈A.
6.
We might attempt to deﬁne the ‘square root’ function R →R as the set
f = {(x, y) ∈R × R : x = y2}.

226
Functions
The reason for deﬁning this set is, of course, that if y = √x then y2 = x.
However, it should be emphasized that this subset of R × R is not a
function; it fails condition (∗) of deﬁnition 5.1 on two counts. (It is, of
course, a relation from R to R.)
Firstly, it is not true that for each x ∈R there exists an element y ∈R
such that (x, y) ∈f. If x = −1 for instance, there is no y with the
required property (namely that y2 = −1).
Secondly, even when there does exist the required element y it is (usually)
not unique. Consider x = 4 for example. In this case there are two
corresponding elements (x, y) of f such that y2 = 4; namely, (4, 2) and
(4, −2).
7.
In some instances it may be difﬁcult or even impossible in practice to
compute f(a) for some elements a of the domain of a function f. For
example, deﬁne a function f : Z+ →{0, 1, 2, 3, 4, 5, 6, 7, 8, 9} by
f(n) = the digit in the nth decimal place in the expansion of π.
More formally,
f = {(n, m) : m = the digit in the nth decimal place in the
expansion of π}.
Although the value of π has been calculated to many million decimal
places†, for very large values of n it may still be impractical to calculate
f(n). For example, what is f(1010) or f(1020)?
The informal description of a function as a rule which associates f(x) to x is
too appealing to drop altogether and we shall continue to use it. Thus we shall
frequently use expressions like ‘the function f : A →B deﬁned by b = f(a), or
a 7→f(a)’. You should be able to reinterpret this in terms of the formal deﬁnition,
if necessary.
Diagrams such as ﬁgure 5.1 above will continue to be useful visual aids, even
when the sets involved are arbitrary. Although it seems very obvious and natural
to us now, the ‘arrow notation’ for a function is comparatively recent. It only
became widely used after the development of category theory beginning in the
late 1940s.
† You may wonder why anyone should ever be interested in computing several million decimal
places of any number! However, there are some interesting questions concerning the randomness
or otherwise of the distribution of digits in the decimal expansion of π; the actual expansion has been
calculated in order to provide evidence for or against various possible answers to these questions.
Also, calculations such as these are used to test the performance of high powered ‘super computers’.

Deﬁnitions and Examples
227
You are almost certainly familiar with the notion of the graph of a function
f : R →R. This is the curve drawn in the plane R2 = R × R consisting of
all the points (x, y) such that y = f(x). However, according to deﬁnition 5.1,
this set of points is the function f itself. In other words the graph of f : R →R
is just a pictorial representation of the set f. From our point of view, there is little
distinction to be made between the function itself and its graph.
It should be noted, however, that not every curve in the (x, y)-plane is the graph
of some function f : A →R, where A ⊆R. A circle is a simple example of
a curve which is not the graph of a function. Consider, for instance, the circle
centred at the origin (0, 0) with radius 1; its equation is x2 + y2 = 1 (ﬁgure 5.2).
For each value of x (strictly) between −1 and 1, there correspond two values of
y. For example, if x = 1
2 the corresponding values of y are
√
3/2 and −
√
3/2.
Therefore the condition (∗) of deﬁnition 5.1 is violated.
Figure 5.2
Given a curve in the (x, y)-plane, it is easy to see whether it is the graph of some
function f : A →R. Given a ∈A, there exists a unique y ∈R such that
y = f(a) if and only if the vertical line through x = a meets the curve exactly
once. The ‘problem’ with the example of the circle above is that the vertical line
through x = 1
2, for instance, meets the curve twice. This leads us to the following
‘test for functionhood’.

228
Functions
Vertical line test
A curve in the (x, y)-plane is the graph of some function f : A →R where
A ⊆R if and only if the following condition is satisﬁed.
(#)
Every vertical line in the plane meets the curve at most once.
If the condition (#) is satisﬁed, then the domain A of the function f is the
set of points a ∈R such that the vertical line through a meets the curve.
There are two features of the deﬁnition of a function which sometimes cause
confusion, both of which are illustrated by the function f : A →B deﬁned in
example 5.1.1. The ﬁrst is that two (or more) elements of the domain may have
the same image in the codomain. In the case of our function we have f(c) =
f(d) = f(e) = δ. Secondly, not every element of the codomain need necessarily
be the image of some element of the domain. Again, for our function f, there is
no x ∈A such that f(x) = γ, so γ is not the image of any element of the domain.
Whether or not either of these actually occurs for a given function is easily
observed in the ‘arrow diagram’ of a function—see ﬁgure 5.3.
We shall consider these points in more detail in §5.3. For now, the second point
leads us to make the next deﬁnition.
Deﬁnition 5.3
Let f : A →B be a function. The image set of f (or range† of f) is the
set
im(f) = {b ∈B : (a, b) ∈f for some a ∈A}.
Note that im(f) is a subset of B, the codomain of f; it should not be confused
with f(a), the image of an element a ∈A. The image of an element (of A) is an
element (of B), but the image set of the function is a set, namely the set of all the
images of elements of the domain:
im(f) = {f(a) : a ∈A}.
† Unfortunately the term ‘range’ is used differently by different authors. Some use it as we have and
others use it to mean codomain. For this reason we shall avoid using the word.

Deﬁnitions and Examples
229
Figure 5.3
For the function deﬁned in example 5.1.1, im(f) = {α, β, δ}. Thus im(f) may
be a proper subset of the codomain of f. Figure 5.4 should help you understand
the deﬁnition of im(f).
Figure 5.4
Examples 5.3
1.
The image set of f : Z+ →Z+, n 7→2n is
{2n : n ∈Z+} = {2, 4, 8, 16, 32, . . .}.
This function can be represented visually by a modiﬁed version of the
‘arrow diagram’—see ﬁgure 5.5.
Figure 5.5

230
Functions
Similarly, the image set of f : Z+ →R, n 7→2−n is
{2−n : n ∈Z+} = {1/2, 1/4, 1/8, 1/16, 1/32, . . .}.
2.
Let f and g be the square and cube functions R →R deﬁned by
f(x) = x2 and g(x) = x3 respectively. (The formal deﬁnitions are
given in example 5.2.4.) Then
im(f) = {y ∈R : y = x2 for some x ∈R} = {x2 : x ∈R}.
We show that im(f) = R+ ∪{0} = {y ∈R : y ⩾0}, by proving that
im(f) ⊆R+ ∪{0} and R+ ∪{0} ⊆im(f). (Recall that this is frequently
how we prove two sets are equal—see theorem 3.1.)
Let y ∈im(f). Then, by deﬁnition, y = f(x) = x2 for some x ∈R, so
y ⩾0. Hence im(f) ⊆R+ ∪{0}.
Now let y ∈R+∪{0}. To show y ∈im(f), we need to ﬁnd a real number
x such that f(x) = y. Since y ⩾0, its square root is a real number. So
let x = √y ∈R. Then
f(x) = f(√y) = (√y)2 = y
so y ∈im(f). Hence R+ ∪{0} ⊆im(f).
Since im(f) ⊆R+ ∪{0} and R+ ∪{0} ⊆im(f), we conclude that
im(f) = R+ ∪{0}.
For the cube function,
im(g) = {y ∈R : y = x3 for some x ∈R} = {x3 : x ∈R}.
In this case, however, im(g) = R. Since im(g) is clearly a subset of R, we
need to show that R ⊆im(g). Note that
3√y ∈R for every real number
y. Therefore, given y ∈R, let x =
3√y; then
g(x) = g( 3√y) = ( 3√y)3 = y
so y ∈im(g). Hence im(g) ⊆R, so we conclude im(g) = R.
3.
Find the image set of the function f : R →R deﬁned by
f(x) =
3x
x2 + 1.

Deﬁnitions and Examples
231
Solution
By deﬁnition y ∈im(f) if and only if
y =
3x
x2 + 1
for some x ∈R.
Now this equation is equivalent to
yx2 + y = 3x
or
yx2 −3x + y = 0.
Regarding this as a quadratic equation in x and using the quadratic formula, we
have, provided y ̸= 0,
x = 3 ±
p
9 −4y2
2y
.
In order that this has a real solution we require y ̸= 0 and
9 −4y2 ⩾0.
Hence
y2 ⩽9/4
(and y ̸= 0)
which means
−3/2 ⩽y ⩽3/2
(and y ̸= 0).
Therefore, provided −3/2 ⩽y ⩽3/2, y ̸= 0, there exists a real number x such
that y = f(x). The value y = 0 is a special case, but clearly f(0) = 0, so
0 ∈im(f).
Hence
im(f) = [−3/2, 3/2] = {y ∈R : −3/2 ⩽y ⩽3/2}.
Finding the image set of a function f : A →R, where A is a subset of R, involves
determining the real numbers y such that the equation y = f(x) has a solution
for some x ∈A. This was the method adopted in the last example; we were able
to ﬁnd im(f) because the equation y = f(x) had a fairly simple form. In general,
however, it may be rather more difﬁcult to ﬁnd im(f) in such cases.
If we are given (or can determine) the graph of f : A →R, then the image set of
the function can be found in a simple geometric way. For each element a of A, its
image f(a) can be determined from the graph by drawing a vertical line through

232
Functions
Figure 5.6
a until it meets the graph and then drawing a horizontal line from the point on the
graph to the y-axis—see ﬁgure 5.6.
We now see that the image set of f is the set of points on the y-axis which arise
from the graph in this way. In other words, the image set of f is the set of points
on the y-axis such that the horizontal line through the point meets the graph of f
in at least one point. Of course if the horizontal line through y meets the graph
more than once, there is more than one element of A which has image equal to y.
Example 5.4
Let
f : R →R,
x 7→
3x
x2 + 1
be the function considered in example 5.3.3. We have given its graph in ﬁgure 5.7,
from which it is easy to see that im(f) = [−3/2, 3/2].
Figure 5.7

Deﬁnitions and Examples
233
Functions and Types
We now consider brieﬂy how functions ﬁt in the theory of typed sets introduced
in §3.8. In fact, since a function is a special kind of relation, we have essentially
deﬁned the type of a function in chapter 4. Suppose A and B are typed sets, where
A : Set [S ] and B : Set [T ] and suppose f : A →B is a function. According to
the formal deﬁnition 5.1, f ⊆A × B so f has type Set [S × T ]. Thus, for
example, a function f : Z →Z has type Set [Integer × Integer ], a function
g : R →R has type Set [Real × Real ], a function h : R →P(Z) has type
Set [Real × Set [Integer ]], and so on.
Exercises 5.1
1.
Three functions f, g and h are deﬁned as follows.
f : R →R
f(x) = x2 −5
g : Z →R
g(x) =
5x
x2 −2
h : R →Z
h(x) = ⌊x⌋.
(See example 4.6.1.)
Find the value of each of the following.
(i)
f(3)
(v)
h(−3.7)
(ii)
g(3)
(vi)
g(h(3.7))
(iii)
h(3.7)
(vii)
f(a + 1)
(iv)
f(
√
2)
(viii)
g(a2).
2.
Let A = {1, 2, 3, 4}. Two functions f, g : P(A) →P(A) are deﬁned
by f(X) = A −X and g(X) = X ∪{1}. Find the value of each of the
following.
(i)
f({1, 2})
(v)
g({1, 2})
(ii)
f({4})
(vi)
g({4})
(iii)
f(A)
(vii)
g(A)
(iv)
f(∅)
(viii)
g(∅).
3.
Which of the following subsets of Z × Z are functions Z →Z? Justify
your answers.
(i)
{(n, 2n) : n ∈Z}
(ii)
{(2n, n) : n ∈Z}

234
Functions
(iii)
{(n, n3) : n ∈Z}
(iv)
{(n3, n) : n ∈Z}
(v)
{(n, n + 4) : n ∈Z}
(vi)
{(n + 4, n) : n ∈Z}
(vii)
{(n, 2n) : n ∈Z}
(viii) {(n, m) : n ∈Z and m = an for some a ∈Z}.
4.
Which of the following subsets f of A×B are functions A →B? Justify
your answers.
(i)
A = B = {human beings, living or dead}
f = {(a, b) ∈A × B : a is a parent of b}
(ii)
A = B = {human beings, living or dead}
f = {(a, b) ∈A × B : b is a parent of a}
(iii)
A = {countries of the world}, B = {cities of the world}
f = {(a, b) ∈A × B : b is the capital city of a}
(iv)
A = B = {living human beings}
f = {(a, b) ∈A × B : b is married to a}.
5.
Let A be any non-empty set and P(A) its power set. Which of the
following subsets of A × P(A) are functions A →P(A)? Justify your
answers.
(i)
f = {(a, B) : a ∈B}
(ii)
f = {(a, B) : B = {a}}
(iii)
f = {(a, B) : B ̸= ∅}
(iv)
f = {(a, B) : B ∪{a} = A}.
6.
Let A = {1, 2, 3, 4, 5, 6, 7, 8, 9}. For each of the following functions
f : A →A deﬁned informally:
(a)
list the images f(1), f(2), . . . , f(9),
(b)
write down the image set of the function,
(c)
list the elements of f as a subset of A × A.
(i)
f(x) = the larger of x and 4 (and f(4) = 4)
(ii)
f(x) = the smaller of x + 4 and 9 (and f(5) = 9)
(iii)
f(x) = the smallest prime number which (exactly) divides x + 1
(iv)
f(x) = |x −3| + 1
(v)
f(x) = |2x −9|
(vi)
f(x) = x2 + x
x + 1

Deﬁnitions and Examples
235
(vii)
f(x) =
(
4
if x ⩽5
3
if x > 5
(viii) f(x) =
(
2
if x2 ⩽2x
3
if x2 > 2x.
7.
Determine the image set of each of the following functions:
(i)
f : R →R, x 7→x2 + 2
(ii)
f : R →R, x 7→(x + 2)2
(iii)
f : R →R, x 7→1/(x2 + 2)
(iv)
f : R →R, x 7→x4
(v)
f : R →R, x 7→(x + 2)/(x2 + 5)
(vi)
f : R →R, x 7→
√
x2 + 1.
8.
Describe the image set of each of the following functions.
(i)
A = P{a, b, c, d}, the power set of {a, b, c, d}
f : A →Z, f(C) = |C|.
(ii)
f : Z →Z, f(n) = n2.
(iii)
A = {countries of the world}, B = {cities of the world}
f : A →B, f(X) = the capital city of X.
(iv)
A = P{a, b, c, d}, f : A →A, f(X) = X ∩{a}.
(v)
A = P{a, b, c, d}, f : A →A, f(X) = X ∪{a}.
9.
Determine the type of each of the functions deﬁned in questions 1, 2, 6,
7 and 8 above.
10.
Let f : A →B be a function and C a subset of A. The image of C
is the set denoted by f(C) = {f(c) : c ∈C}. Thus f(C) is the set of
all images of elements of C; in particular f(A) = im(f) and, if a ∈A,
f{a} = {f(a)}. (See the diagram below.)
Determine f(C) in each of the following cases.
(i)
f : R →R, f(x) = x2; C = [−3, 2] = {x ∈R : −3 ⩽x ⩽2}.

236
Functions
(ii)
f : R →R, f(x) = 2/x; C = (0, 8] = {x ∈R : 0 < x ⩽8}.
(iii)
f : Z →Z, f(x) = 2x; C = {n ∈Z : −1 ⩽n ⩽6}.
(iv)
f : R2 →R, f(x, y) = x2 + y2;
C = [−2, 3] × [−1, 2] = {(x, y) ∈R2 : −2 ⩽x ⩽3 and
−1 ⩽y ⩽2}.
(v)
f : {English words} →Z+, f(w) = the number of letters in w;
C = {mathematics, is, a, fascinating, subject}.
(vi)
f : A →B is any function; C = ∅.
11.
Let f : A →B be a function and D a subset of B. The inverse image of
D is the set f −1(D) = {a ∈A : f(a) ∈D}. Thus f −1(D) is the set of
all elements of A whose image lies in D. Note that f −1 is not necessarily
a function so that the inverse image is a different concept from that of the
image deﬁned in the previous question. (See the diagram below.)
Find f −1(D) in each of the following cases.
(i)
f : R →R, f(x) = x2; D = [4, 9] = {x ∈R : 4 ⩽x ⩽9}.
(ii)
f : R →R, f(x) = x2; D = [−9, −4]
= {x ∈R : −9 ⩽x ⩽−4}.
(iii)
f : R →R, f(x) = x2; D = [−4, 9] = {x ∈R : −4 ⩽x ⩽9}.
(iv)
f : Z →R, f(x) = 2x; D = {n ∈Z : 0 ⩽n ⩽10}.
(v)
f : R2 →R, f(x, y) = x2 + y2; D = [0, 1].
(vi)
f : A →B is any function; D = ∅.
(vii)
f : A →B is any function; D = B.
12.
A partial function f from A to B is a ‘function’ in which f(a) is
not deﬁned for every a ∈A. A partial function is sometimes denoted
f : A ↛B. In example 5.1.3, the rule f(x) = 1/(x + 1)3 deﬁnes a
partial function f : R ↛R because f(−1) is not deﬁned. For a partial
function f : A ↛B, the set A is called the source of f and the set of
elements of A for which f(a) is deﬁned, {a ∈A : f(a) is deﬁned}, is
the domain of f.
Sometimes, to emphasize that a function is not partial; i.e. f(a) is deﬁned
for all a ∈A, we say f is a total function. So a total function is what

Deﬁnitions and Examples
237
we have previously called simply a function and we will always use the
unqualiﬁed term ‘function’ to mean total function.
Explain why each of the following is a partial function and determine the
domain in each case.
(i)
f : {1, 2, 3, 4, 5, 6, 7, 8, 9} ↛{1, 2, 3, 4, 5, 6, 7, 8, 9},
f(n) = n + 3.
(ii)
f : {1, 2, 3, 4, 5, 6, 7, 8, 9} ↛{1, 2, 3, 4, 5, 6, 7, 8, 9},
f(n) = 2n −5.
(iii)
f : {1, 2, 3, 4, 5, 6, 7, 8, 9} ↛{1, 2, 3, 4, 5, 6, 7, 8, 9},
f(n) = n2.
(iv)
f : {1, 2, 3, 4, 5, 6, 7, 8, 9} ↛{1, 2, 3, 4, 5, 6, 7, 8, 9},
f(n) = √n.
(v)
f : R ↛R, f(x) = 1/x.
(vi)
f : Z ↛Z, f(n) = 1/n.
(vii)
f : Z ↛Z, f(n) = n/4.
(viii) f : Z ↛Z, f(n) = 4/n.
(ix)
f : Z+ × Z+ ↛Z+, f(n, m) = n/m.
(x)
f : Z ↛Z, f(n) = 2n.
13.
Classify each of the following as (a) a total function, (b) a partial function
or (c) not a function (either partial or total). Give brief reasons for your
answers.
(i)
f : Z →Z, f(n) = n/2.
(ii)
A = {countries of the world};
f : A →A, f(X) = countries sharing a border with X.
(iii)
A = {countries of the world};
f : A →P(A), f(X) = the set of countries sharing a border
with X.
(iv)
f : R →R, f(x) = √x.
(v)
f : R →R, f(x) = ±√x.
(vi)
f : R+ →R+, f(x) = √x.
14.
Let A and B be ﬁnite sets such that |A| = n and |B| = m. How many
different functions are there from A to B?
15.
Let f : A →B be a function. Deﬁne a relation R on its domain A by:
x R y
if and only if f(x) = f(y).
Show that R is an equivalence relation on A, and describe the equivalence
classes.

238
Functions
16.
Let f : A →B be a function. Under what circumstances is
g = {(b, a) : (a, b) ∈f}
a function B →A? (This question is considered in §5.4.)
17.
(i)
If R is an equivalence relation on a set A, is R necessarily a
function A →A? Justify your answer.
(ii)
If R is a partial order relation on a set A, is R necessarily a function
A →A? Justify your answer.
5.2
Composite Functions
Let f : A →B and g : B →C be functions. If x is an element of A then
y = f(x) belongs to B. Therefore g(y) = g(f(x)) is an element of C. We can
use the association x 7→g(f(x)) to deﬁne a function from A to C, called the
composite of f and g, denoted g ◦f†. The composite g ◦f can be represented
by the diagram in ﬁgure 5.8(i).
Alternatively, if we think of the functions f and g being represented by function
machines, then the composite g ◦f has a function machine that is obtained by
connecting the output of f to the input of g. This is represented by ﬁgure 5.8(ii).
Example 5.5
Let A = {a, b, c, d, e}, B = {α, β, γ, δ} and C = {1, 2, 3, 4, 5, 6}.
Let
f : A →B be the function deﬁned in example 5.1.1 and g : B →C be the
function deﬁned by
α 7→3,
β 7→5,
γ 7→1,
δ 7→5.
Then the composite function, g ◦f : A →C, is given by
a 7→5,
b 7→3,
c 7→5,
d 7→5,
e 7→5.
† Thus the composite g ◦f is the function ‘f followed by g’. This is an instance where notation
can cause some confusion; the function f is written after g but ‘acts’ before it. Some authors avoid
this ‘problem’ with the notation by writing the function on the right; that is, they write xf instead of
f(x). Written in this notation g(f(x)) becomes xfg, so the composite function ‘f followed by g’ is
denoted fg.

Composite Functions
239
(i)
(ii)
Figure 5.8
This example is illustrated by ﬁgure 5.9. The diagram shows very clearly that the
composite function g ◦f is ‘f followed by g’.
Figure 5.9
So far we have considered only the informal deﬁnition of g ◦f. However, we
should be able to make the notion precise in terms of our Cartesian product
deﬁnition of a function.
According to the formal deﬁnition, the function g ◦f should be the subset of
the Cartesian product A × C consisting of all those elements (x, z) such that
z = g ◦f(x). If we let y = f(x) ∈B then (x, y) ∈f and (y, z) ∈g. Therefore
we may formally deﬁne the composite function as follows.

240
Functions
Deﬁnition 5.4
Let f : A →B and g : B →C be functions. The composite function
g ◦f : A →C is
g ◦f = {(x, z) ∈A × C : (x, y) ∈f and (y, z) ∈g for some y ∈B}.
Note that the composite g ◦f of two arbitrary functions may not exist.
In
deﬁnition 5.4, the domain of g equals the codomain of f. It is usual to deﬁne the
composite function only when the sets ‘match up’ in this way. However, this is
slightly more restrictive than is strictly necessary and we can widen the conditions
under which g ◦f is deﬁned as follows. Let f : A →B and g : B′ →C be
functions and let a ∈A. In order that g(f(a)) be deﬁned, we require that f(a)
belong to B′, the domain of g. Now to deﬁne g ◦f it is necessary (and sufﬁcient)
that g(f(a)) be deﬁned for all a ∈A. Hence g ◦f is deﬁned if and only if the
image set of f is a subset of the domain of g. Of course this condition is satisﬁed
if B = B′, which is the condition given in deﬁnition 5.4. (Figure 5.11 below may
help you to visualize the situation described here.)
Examples 5.6
1.
The formal deﬁnition of the composite function g ◦f in example 5.5 is
g ◦f = {(a, 5), (b, 3), (c, 5), (d, 5), (e, 5)}.
2.
Let f and g be the functions R →R deﬁned by f(x) = x + 2 and
g(x) = 1/(x2 + 1) respectively.
Then g ◦f : R →R is deﬁned by
g ◦f(x) = g(f(x))
= g(x + 2)
=
1
(x + 2)2 + 1
=
1
x2 + 4x + 5.
Similarly,
f ◦g(x) = f(g(x))

Composite Functions
241
= f(1/(x2 + 1))
=
1
x2 + 1 + 2
= 2x2 + 3
x2 + 1 .
This example illustrates that, in general, f ◦g ̸= g ◦f. Of course, given
two functions f and g, it is quite possible for g ◦f to be deﬁned but f ◦g
not to be deﬁned. (See exercise 5.2.7.)
3.
Three functions, f, g and h, are deﬁned by f : Z+ →R, f(x) =
2
x + 1,
g : Z →Z, g(x) = x2 + 3 and h : R →R, h(x) = 3x + 2.
Determine which of the following composite functions are deﬁned.
(i)
g ◦f
(ii)
f ◦g
(iii)
h ◦f
(iv)
f ◦h
(v)
g ◦h
(vi)
h ◦g.
Solution
(i)
Since f(2) = 2/3, we have that 2/3 ∈im(f) but 2/3 /∈Z. Therefore
im(f) ̸⊆Z, so g ◦f is not deﬁned.
(ii)
For all x ∈Z, g(x) = x2 + 3 ⩾3 (since x2 ⩾0). Hence the image set
of g is a subset of Z+, the domain of f. Therefore f ◦g is deﬁned.
(iii)
Since im(f) ⊆R, h ◦f is deﬁned.
(iv)
The composite f ◦h is not deﬁned: h(1/2) = 7/2 /∈Z+ so im(h) ̸⊆Z+.
(v)
The same reasoning as in part (iv) shows that g ◦h is not deﬁned.
(vi)
Since im(g) ⊆Z ⊆R, it follows that the composite h ◦g is deﬁned.
4.
Consider f : R →(R+ ∪{0}), f(x) = x2 and g : (R+ ∪{0}) →R,
g(x) = √x. Determine the composite functions g ◦f and f ◦g.
Solution
The function g◦f : R →R is g◦f(x) = g(x2) =
√
x2. Note that
√
x2 is positive
(or zero). If x is positive (or zero), then
√
x2 is just x. However, if x is negative,
then
√
x2 = −x. For instance, if x = −2, then
√
x2 =
√
4 = 2 = −(−2).

242
Functions
In other words,
g ◦f(x) =
(
x
if x ⩾0
−x
if x < 0.
This is called the modulus function, and is denoted x 7→|x|. Its graph is given
in ﬁgure 5.10.
Figure 5.10
The function f ◦g : (R+ ∪{0}) →(R+ ∪{0}) is f ◦g(x) = (√x)2. Since x is
positive (or zero) here, (√x)2 is just x itself. In other words, f ◦g is the identity
function on R+ ∪{0}.
Theorem 5.1
Let f : A →B and g : B →C be functions. Then
im(g ◦f) ⊆im(g).
Proof
Let c ∈im(g ◦f). Then there exists a ∈A such that (g ◦f)(a) = g(f(a)) = c.
Now let b = f(a) ∈B; then g(b) = c, so c ∈im(g). Therefore im(g ◦f) ⊆
im(g).
□
Theorem 5.1 is probably best visualized by ﬁgure 5.11.

Composite Functions
243
Figure 5.11
Exercises 5.2
1.
Let f, g and h be the functions deﬁned in exercise 5.1.1:
f : R →R
f(x) = x2 −5
g : Z →R
g(x) =
5x
x2 −2
h : R →Z
h(x) = ⌊x⌋.
Find the value of each of the following.
(i)
(f ◦f)(2)
(v)
(h ◦g)(3)
(ii)
(g ◦h)(2.5)
(vi)
(h ◦f)(1.5)
(iii)
(f ◦g)(2)
(vii)
(f ◦h)(1.5)
(iv)
(h ◦h)(3.7)
(viii)
(g ◦h)(2).
2.
Let A = {Anna Karenina, Crime and Punishment, Sons and Lovers,
War and Peace}, B = {Dostoyevsky, Lawrence, Tolstoy, Zola}, and
C = {America, England, France, Russia}.
Deﬁne two functions f : A →B and g : B →C by
f :
Anna Karenina 7→Tolstoy
f : Crime and Punishment 7→Dostoyevsky
f :
Sons and Lovers 7→Lawrence
f :
War and Peace 7→Tolstoy
g :
Dostoyevsky 7→Russia
g :
Lawrence 7→England

244
Functions
g :
Tolstoy 7→Russia
g :
Zola 7→France.
(i)
Deﬁne the composite function g ◦f in a similar way, and draw a
diagram to represent the composite function.
(ii)
Write down (in words) rules which deﬁne each of the functions
f, g and g ◦f.
3.
Let f, g : R →R be deﬁned by f(x) = 4x −1 and g(x) = x2 + 1. Find:
(i)
f(2)
(vi)
(g ◦g)(2)
(ii)
g(2)
(vii)
(f ◦g ◦f)(3)
(iii)
(g ◦f)(2)
(viii)
(g ◦f ◦g)(3)
(iv)
(f ◦g)(2)
(ix)
(g ◦f)(x)
(v)
(f ◦f)(2)
(x)
(f ◦g)(x).
4.
Let f, g and h be functions R →R deﬁned respectively by
f(x) = 2x + 1,
g(x) = 1/(x2 + 1),
and
h(x) =
p
x2 + 1.
Find expressions for each of the following:
(i)
(g ◦f)(1)
(vi)
(g ◦f)(x)
(ii)
(f ◦g)(1)
(vii)
(g ◦h)(x)
(iii)
(g ◦h)(2)
(viii)
(f ◦f)(x)
(iv)
(h ◦f)(3)
(ix)
((f ◦g) ◦h)(x)
(v)
(f ◦g)(x)
(x)
(f ◦(g ◦h))(x).
5.
Let A = {humans, living or dead} and let f and g be the functions
A →A deﬁned by f(x) = the father of x and g(x) = the mother of
x, respectively.
Describe the composite functions f ◦f, f ◦g, g ◦f and g ◦g.
6.
Let f : A →B be any function. Show that f ◦idA = f and idB ◦f = f.
7.
Let f : A →B and g : C →D be two functions. What are the most
general conditions under which both composites g ◦f and f ◦g can be
deﬁned?
8.
(Associativity of composition.) Let f : A →B, g : B →C, h : C →D
be functions. Explain why (h◦g)◦f = h◦(g◦f). (Hint: ﬁnd expressions
for ((h ◦g) ◦f)(x) and (h ◦(g ◦f))(x).)
9.
Let A = {a, b, c}. Deﬁne a function f : A →A, which is not the identity
function on A, such that f ◦f = f.

Composite Functions
245
10.
Let f : R →R be the function f(x) = ⌊x⌋, where ⌊x⌋is the largest
integer less than or equal to x (see example 4.6.1).
(i)
Show that f ◦f = f.
(ii)
Show that f(x + k) = f(x) + k for all x ∈R if and only if k ∈Z.
(iii)
For what values of x is f(2x) = 2f(x)?
11.
Given f : R →R, x 7→(2x + 1), deﬁne f [n] : R →R inductively by
f [1] = f,
f [n] = f [n−1] ◦f
for n > 1.
Prove that f [n](x) = 2nx + (2n −1).
12.
Consider the function f : Z+ →Z+, x 7→x + 2.
(i)
Show that there are inﬁnitely many different functions g : Z+ →
Z+ such that g ◦f = idZ+.
(ii)
Show that there is no function h : Z+ →Z+ such that f ◦h =
idZ+.
(iii)
Evaluate f [n](x). (See exercise 5.2.11 above.)
13.
In each of the following, deﬁne the composite function g ◦f:
(i)
f : R →R,
f(x) =
(
x2 + x
if x ⩾0
1/x
if x < 0
g : R →R,
g(x) =
(√x + 1
if x ⩾0
1/x
if x < 0
(ii)
f : R →R,
f(x) =
(
x −2
if x ⩾1
x3
if x < 1
g : R →R,
g(x) =
(
(x + 4)/3
if x ⩾0
|x + 1|
if x < 0.
14.
Let C be a subset of A. The function iC : C →A, c 7→c, is called the
inclusion of C in A. (Thus iC is the same as the identity function on C
except that its codomain is A.)
(i)
Deﬁne iC as a subset of C × A.
Now let f : A →B be a function. The function f|C : C →B, c 7→f(c),
is called the restriction of f to C. (Thus f|C is similar to f except that
it has domain C.)
(ii)
Show that f|C = f ∩(C × B).
(iii)
Using the informal deﬁnitions, show that f|C = f ◦iC.

246
Functions
15.
Let f : A →B and g : C →D be functions. If either f or g is a
partial function or if the image set of f is not a subset of the domain of
g, im(f) ̸⊆C, then their composite g ◦f may be a partial function (see
exercise 5.1.12). For each of the following pairs of functions, determine
whether f, g and the composite g ◦f are partial or total functions. If g ◦f
is partial, determine its domain.
(i)
f : {1, 2, 3, 4, 5} →{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, f(x) = 2x
g : {1, 2, 3, 4, 5} →{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, g(x) = x + 3
(ii)
f : Z →Z, f(x) = 2x;
g : Z →Z, g(x) = x + 3
(iii)
f : Z →R, f(x) =
1
x2 + 1;
g : Z →Z, g(x) = 2x + 3
(iv)
f : R →R, f(x) =
1
x2 + 1;
g : R →R, g(x) = √x
(v)
f : Z →Z, f(x) = x
2 ;
g : R →R, g(x) = 2x
(vi)
f : R →R, f(x) = 2x;
g : Z →Z, g(x) = x
2
(vii)
f : Z →Z, f(x) = 2x;
g : Z →Z, g(x) = x
2
(viii)
f : R →R, f(x) = x2;
g : R →R, g(x) = √1 −x.
16.
Determine the type of each of the functions (f, g and g ◦f) deﬁned in
question 15.
5.3
Injections and Surjections
In this section we consider two special kinds of functions: ‘injections’ and
‘surjections’. Recall from §5.1 that a function f : A →B can be such that:
(i)
different elements of the domain may have the same image in the
codomain;
(ii)
there may be elements of the codomain which are not the image of any
element of the domain.
Both these possibilities are exhibited by the square function f : R →R,
f(x) = x2. For instance, both 2 and −2 have the same image (namely 4) and

Injections and Surjections
247
any negative real number does not belong to the image of f (because x2 ⩾0 for
all real numbers x).
A function where the ﬁrst possibility does not occur is called ‘injective’ and
a function where the second possibility does not occur is called ‘surjective’.
The two cases are represented in ﬁgure 5.12. The function f : {a, b, c, d} →
{α, β, γ, δ, ε} illustrated in ﬁgure 5.12(a) is injective but not surjective. It is injec-
tive because no two elements of the domain have the same image; it is not surjec-
tive because the element δ of the codomain is not the image of any element of the
domain. On the other hand, the function g : {a, b, c, d, e} →{α, β, γ, δ}, illus-
trated in ﬁgure 5.12(b), is surjective but not injective. It is surjective because every
element of the codomain is the image of at least one element of the domain; it is
not injective because the two elements a and b of the domain have the same image.
Figure 5.12
The following are the formal deﬁnitions of injective and surjective, given in terms
of the Cartesian product deﬁnition of a function.
Deﬁnition 5.5
Let f : A →B be a function.
(i)
We say that f is injective or is an injection† if the following is
satisﬁed for all elements a, a′ ∈A:
if (a, b), (a′, b′) ∈f and a ̸= a′ then b ̸= b′.
(ii)
We say that f is surjective or is a surjection† if for every b ∈B
there exists a ∈A such that (a, b) ∈f.
† Some authors use the term ‘one-to-one function’ for an injective function and ‘onto function’ for
a surjective function.

248
Functions
Written less formally, a function f is injective if, for all a, a′ ∈A,
if a ̸= a′ then f(a) ̸= f(a′).
However, to prove that a given function is injective it is generally easier to use the
equivalent contrapositive statement. That is, for all a, a′ ∈A,
if f(a) = f(a′) then a = a′.
Of course, to show a function is not injective we need to ﬁnd a counter-example
to the general condition. In other words, we need to ﬁnd two different elements a
and a′ of A which have the same image, f(a) = f(a′).
The second part of the deﬁnition can be rephrased simply to say that f : A →B
is surjective if its image set equals its codomain, i.e. im(f) = B.
Examples 5.7
1.
We have seen that f : R →R, x 7→x2, is neither injective nor surjective.
2.
Let
f : {1, 2, 3, 4, 5} →{1, 2, 3, 4, 5, 6},
be deﬁned by
1 7→4, 2 7→6, 3 7→1, 4 7→3, 5 7→5,
and let
g : {2, 4, 6, 8, 10, 12} →{2, 3, 5, 7, 11},
be deﬁned by
2 7→11, 4 7→2, 6 7→5, 8 7→3, 10 7→5, 12 7→7.

Injections and Surjections
249
The function f is injective because each element of the domain has a
different image. In other words, the following situation does not occur in
the arrow diagram of f.
The element 2 in the codomain is not the image of any element of the
domain, 2 /∈im(f). Therefore f is not surjective.
Now consider g. There do exist two different elements of the domain
with the same image, g(6) = 5 = g(10), so g is not injective. Since
each element of the codomain is ‘hit by an arrow’— 2 = g(4), 3 = g(8),
5 = g(6), 7 = g(12) and 11 = g(2)—it follows that g is surjective.
3.
Consider f : R →R deﬁned by f(x) = 3x −7. Show that f is both
injective and surjective.
Solution
To show that f is an injection we prove that, for all real numbers x and y,
f(x) = f(y) implies x = y.
Now
f(x) = f(y)
3x −7 = 3y −7
⇒
3x = 3y
⇒
x = y
⇒
so f is injective.
To show that f is a surjection, let y be any element of the codomain R. We need
to ﬁnd x ∈R such that f(x) = y. Let x = (y + 7)/3. Then x ∈R and
f(x) = f((y + 7)/3)
= 3 × y + 7
3
−7

250
Functions
= y + 7 −7
= y
so f is surjective.
In the proof of surjectivity above, we seem to have plucked the value x =
(y + 7)/3 ‘out of thin air’ as it were. In fact, to discover that (y + 7)/3 is the
appropriate value of x, we let y = f(x) and solve this to ﬁnd x, as follows:
y = 3x −7
⇒
y + 7 = 3x
⇒
x = (y + 7)/3.
However, this process of ‘working backwards’ to discover the appropriate value
of x to consider is not strictly part of the proof of the surjectivity of f.
Clearly this proof can be generalized to show that any linear function f : R →R,
f(x) = ax+b (where a and b are ﬁxed real numbers with a ̸= 0), is both injective
and surjective.
4.
Let A = {countries of the world}, B = {cities of the world} and de-
ﬁne f : A →B by f(x) = the capital city of X (exercise 5.1.4(iii)).
Then f is injective since (we suppose) different countries have differ-
ent capital cities. Since there are cities which are not the capitals of
any countries, f is not surjective. For example, New York /∈im(f),
Birmingham /∈im(f), etc.
With the same sets A and B deﬁne g : B →A by g(C) = the country
to which C belongs. This function is a surjection because (again we sup-
pose) every country contains at least one city within its borders. However,
g is not injective as several cities may belong to the same country. For
example, f(Paris) = f(Nice) = France, f(Ottawa) = f(Vancouver) =
Canada, etc.
5.
Let X and Y be non-empty sets and X × Y their Cartesian product. The
functions
p1 : X × Y →X, p1(x, y) = x
and
p2 : X × Y →Y, p2(x, y) = y
are called the natural projections of X × Y onto X and of X × Y onto
Y respectively. Both are clearly surjective and, provided X and Y are not
singleton sets, neither is injective.
Consider a function f : A →B where A and B are subsets of R. Just as we can
‘read off’ im(f) from the graph of the function, we can also tell from the graph
whether or not the function is injective or surjective.

Injections and Surjections
251
Suppose that f is not injective. Then there are two elements a1 and a2 in A such
that f(a1) = f(a2) = b, say. This means that the horizontal line at height b meets
the graph at points corresponding to x = a1 and x = a2 on the x-axis. This
situation is illustrated in ﬁgure 5.13.
Figure 5.13
If, on the other hand, f is injective then this situation never occurs. In other words,
a horizontal line through any point b ∈B on the y-axis will not meet the graph in
more than one point.
We saw in §5.2 that im(f) is represented by the region of the y-axis consisting
of those points such that a horizontal line through a point meets the graph
somewhere. Therefore f is surjective (i.e. im(f) = B) if and only if every
horizontal line through a point of B meets the graph at least once.
These considerations are summarized in the following theorem.
Theorem 5.2
Let f : A →B be a function, where A and B are subsets of R. Then:
(i)
f is injective if and only if every horizontal line through a point of
B on the y-axis meets the graph of f at most once;
(ii)
f is surjective if and only if every horizontal line through a point
of B on the y-axis meets the graph of f at least once.

252
Functions
Example 5.8
The graphs of four functions A →B are given below. Determine whether each
function is injective and/or surjective.
Solution
The function with graph (a) is injective since each horizontal line drawn through
B meets the graph at most once. It is not surjective because, for instance, a
horizontal line through any negative element of B does not meet the graph at all.
Graph (b) is the graph of a surjective but not injective function. Every horizontal
line through B meets the graph somewhere, but the horizontal line at the same
height as the horizontal portion of the graph meets the graph more than once—it
meets it in inﬁnitely many points in fact.
Similar arguments show that the function represented by graph (c) is both injective
and surjective, and the function represented by graph (d) is neither injective nor
surjective.

Injections and Surjections
253
The examples above illustrate that the injective and surjective properties are
independent of one another.
A function may be injective but not surjective,
surjective but not injective, both or neither.
Theorem 5.3
Let f : A →B and G : B →C be two functions.
(i)
If f and g are both injective then so, too, is the composite g ◦f.
(ii)
If f and g are both surjective then so, too, is the composite g ◦f.
Proof
(i)
Suppose f and g are injections. Let a, a′ ∈A, b = f(a) and b′ = f(a′).
Then
g ◦f(a) = g ◦f(a′)
g(f(a)) = g(f(a′))
⇒
g(b) = g(b′)
⇒
b = b′
(since g is injective)
⇒
f(a) = f(a′)
(since f(a) = b, f(a′) = b′)
⇒
a = a′
(since f is injective).
⇒
Hence g ◦f is an injection.
(ii)
Suppose f and g are surjections and let c ∈C. Since g is surjective, there
exists b ∈B such that g(b) = c, and since f is surjective, there exists
a ∈A such that f(a) = b. Therefore there exists a ∈A such that
g ◦f(a) = g(f(a)) = g(b) = c
so g ◦f is surjective.
□
It is reasonable to ask whether the converse of each part of theorem 5.3 is also
true: if g ◦f is injective (surjective), does it follow that f and g are necessarily
injective (surjective)? The answer to both questions is ‘no’, as the following
example shows.

254
Functions
Example 5.9
Let A = {a1, a2}, B = {b1, b2, b3} and C = {c1, c2}, and deﬁne
f : A →B
by
f(a1) = b1, f(a2) = b2
and
g : B →C
by
g(b1) = c1, g(b2) = g(b3) = c2.
Figure 5.14 illustrates these functions.
Figure 5.14
Clearly f is injective but not surjective and g is surjective but not injective.
However, the composite function g ◦f : A →C, which is given by
g ◦f(a1) = c1
and
g ◦f(a2) = c2
is both injective and surjective.
This example suggests the following which is a partial converse to theorem 5.3.
Theorem 5.4
Let f : A →B and g : B →C be two functions:
(i)
If the composite g ◦f is injective then so, too, is f.
(ii)
If the composite g ◦f is surjective then so, too, is g.

Injections and Surjections
255
Proof
In both cases we prove the contrapositive statement.
(i)
The contrapositive is: if f is not injective then g ◦f is not injective.
Suppose that f is not injective. Then there exist a, a′ ∈A such that
a ̸= a′ but f(a) = f(a′). Hence g ◦f(a) = g ◦f(a′) as well, so the
composite function is also not injective.
(ii)
The contrapositive statement here is: if g is not surjective then g ◦f is not
surjective. Suppose that g is not surjective. Then im(g) is a proper subset
of C. Since im(g ◦f) ⊆im(g) (theorem 5.1), it follows that im(g ◦f) is
also a proper subset of C, so g ◦f is not surjective either.
□
The existence of an injection or a surjection from one set to another also
has implications for the cardinalities of the sets concerned.
Suppose A =
{a1, a2, . . . , an} and B = {b1, b2, . . . , bm} are ﬁnite sets and f : A →B
is an injection.
Then, assuming we have not listed any element of A twice,
the elements f(a1), f(a2), . . . , f(an) are all different, so B contains at least n
elements. Now suppose instead that f : A →B is a surjection. Then the list of
elements f(a1), f(a2), . . . , f(an) must include every element of B at least once
(but may contain repeats), so B contains at most n elements. We have proved the
following theorem.
Theorem 5.5
Let f : A →B be a function between ﬁnite sets.
(i)
If f is injective then |A| ⩽|B|.
(ii)
If f is surjective then |A| ⩾|B|.
Exercises 5.3
1.
For each of the following functions F determine whether or not F is (a)
injective, (b) surjective. Justify your answers.
(i)
F : {a, b, c, d, e, f} →{a, b, c, d, e, f},
a 7→f, b 7→b, c 7→d, d 7→e, e 7→b, f 7→c.

256
Functions
(ii)
F : {a, b, c, d, e, f} →{a, b, c, d, e, f},
a 7→f, b 7→e, c 7→d, d 7→c, e 7→b, f 7→a.
(iii)
F : {a, b, c, d, e} →{a, b, c, d, e, f, g},
a 7→b, b 7→e, c 7→f, d 7→c, e 7→a.
(iv)
F : {a, b, c, d, e, f, g} →{a, b, c, d, e},
a 7→e, b 7→c, c 7→d, d 7→a, e 7→d, f 7→e, g 7→a.
(v)
F : {a, b, c, d, e, f} →{a, b, c, d, e, f, g},
a 7→b, b 7→e, c 7→d, d 7→b, e 7→a, f 7→g.
2.
For each of the following functions f determine whether or not f is (a)
injective, (b) surjective. Justify your answers. Hint: in some cases it may
help to evaluate f(n) for a few values of n.
(i)
f : Z →Z, f(n) = n −6.
(ii)
f : Z →Z, f(n) = 3n −5.
(iii)
f : Z →Z, f(n) = n2.
(iv)
f : Z →Z, f(n) = n3.
(v)
f : Z →Z, f(n) = n2 + n.
(vi)
f : Z →Z, f(n) = (−1)n.
(vii)
f : Z →Z, f(n) = n + (−1)n.
(viii) f : Z →Z, f(n) =
(
n
if n ⩾0
n −1
if n < 0.
(ix)
f : Z →Z, f(n) =



n
if n is even
n + 1
2
if n is odd.
(x)
f : Z →Z, f(n) =
(
n −1
if n is even
n + 1
if n is odd.
3.
Each of the following is the graph of a function A →B (where A
and B are subsets of R). Determine whether or not each function is:
(a) injective; (b) surjective.

Injections and Surjections
257
4.
Determine whether or not each of the following functions is (a) injective,
(b) surjective. Justify your answers.
(i)
A = {1, 2, 3}, B = P(A); f : A →B, f(x) = {x}.
(ii)
A = any non-empty set, B = P(A); f : A →B, f(x) = {x}.
(iii)
A = {1, 2, 3, 4}, B = P(A); f : B →B, f(X) = X ∩{1, 2}.
(iv)
A = {1, 2, 3, 4}, B = P(A); f : B →B, f(X) = X ∪{1, 2}.
(v)
A = {1, 2, 3, 4}, B = P(A); f : B →B, f(X) = A −X.
(vi)
A = {1, 2, 3, 4}, B = P(A); f : B →A, f(X) = smallest
element in X.
5.
Determine whether or not each of the following functions is (a) injective,
(b) surjective. Justify your answers.
(i)
A = B = Z5 = {[0], [1], [2], [3], [4]},
f([n]) = [n2].
(ii)
A = B = Z5 = {[0], [1], [2], [3], [4]},
f([n]) = [n3].
(iii)
A = B = Z5 = {[0], [1], [2], [3], [4]},
f([n]) = [2n + 3].
(iv)
A = B = Z5 = {[0], [1], [2], [3], [4]},
f([n]) = [5n + 3].
(v)
A = B = Z6 = {[0], [1], [2], [3], [4], [5]},
f([n]) = [n2].
(vi)
A = B = Z6 = {[0], [1], [2], [3], [4], [5]},
f([n]) = [n3].
(vii)
A = B = Z6 = {[0], [1], [2], [3], [4], [5]},
f([n]) = [2n + 3].
(viii)
A = B = Z6 = {[0], [1], [2], [3], [4], [5]},
f([n]) = [5n + 3].

258
Functions
6.
Determine whether each of the following real-valued functions is
injective, surjective, both or neither.
(i)
f : R →R, f(x) = x2 + 4.
(ii)
f : R −{1} →R −{1}, f(x) = x/(x −1).
(iii)
f : R →R, f(x) = 2x.
(iv)
f : R →R, f(x) = |x|.
(v)
f : R →R+ ∪{0}, f(x) = x + |x|.
(vi)
f : R2 →R, f(x, y) = xy.
(vii)
f : R →R2, f(x) = (x, x2).
(viii) f : R2 →R2, f(x, y) = (x + y, x −y).
(ix)
f : R2 →R2, f(x, y) = (x + y, x2 + y2).
(x)
f : R2 →R2, f(x, y) = (x −y, x2 −y2).
7.
For each of the following functions f : A →B:
(a)
Determine what conditions, if any, must be placed on the sets A
and/or B to ensure that f is injective.
(b)
Determine what conditions, if any, must be placed on the sets A
and/or B to ensure that f is surjective.
(i)
A is a non-empty set of people, B = {n ∈Z : 0 ⩽n ⩽100}
f : A →B, f(p) = age last birthday of p.
(ii)
A is a non-empty set of cities, B is a non-empty set of countries
f : A →B, f(X) = country containing city X.
(iii)
A is a non-empty set of countries, B is a non-empty set of cities
f : A →B, f(X)
= city with the largest population (in
thousands) in country X.
(iv)
A = {n ∈Z : a ⩽n ⩽b}, B = {n ∈Z : c ⩽n ⩽d}
f : A →B, f(n) = n.
(v)
A = {n ∈Z : a ⩽n ⩽b}, B = {n ∈Z : c ⩽n ⩽d}
f : A →B, f(n) = n + 10.
8.
Let f : A →B be a function and let C1, C2 be subsets of A. Prove that:
(i)
f(C1 ∪C2) = f(C1) ∪f(C2)
(ii)
f(C1 ∩C2) ⊆f(C1) ∩f(C2)
(iii)
f is injective if and only if, for all subsets C1, C2 of A,
f(C1 ∩C2) = f(C1) ∩f(C2).
(See exercise 5.1.10 for the deﬁnition of f(C) where C ⊆A.)

Injections and Surjections
259
9.
Let f : A →B be a function. Prove each of the following.
(i)
For all subsets C of A, C ⊆f −1(f(C)).
(ii)
If f is injective then C = f −1(f(C)) for all subsets C of A.
(iii)
If C = f −1(f(C)) for all subsets C of A then f is injective.
(iv)
If f is surjective then f(f −1(D)) = D for all subsets D of B.
(See exercise 5.1.11 for the deﬁnition of f −1(D) where D ⊆B.)
10.
Let A1 and A2 be non-empty sets and f1 : A1 →B1 and f2 : A2 →B2
be two functions. Deﬁne F = (f1 × f2) : (A1 × A2) →(B1 × B2) by
F(a1, a2) = (f1(a1), f2(a2)).
Prove that:
(i)
F is injective if and only if f1 and f2 are both injective;
(ii)
F is surjective if and only if f1 and f2 are both surjective.
11.
Let A and B be sets and f : A →B a function. Deﬁne Pf : P(A) →
P(B) by (Pf)(C) = f(C), where C ⊆A.
Prove that:
(i)
if f is injective then Pf is injective;
(ii)
if f is surjective then Pf is surjective.
Are the converse statements true?
12.
Let f : A →B be a function and let C be a subset of A.
(i)
Show that if f is injective then so, too, is the restriction f|C. (See
exercise 5.2.14.)
(ii)
Under what conditions is f|C surjective?
13.
Let X1 × X2 × · · · × Xn be the Cartesian product of non-empty sets
X1, X2, . . . , Xn.
(i)
Show that, for each i = 1, 2, . . . , n, the natural projection
pi : X1 × X2 × · · · × Xn →Xi,
(x1, x2, . . . , xn) 7→xi
is surjective.
(ii)
If one of the sets Xj is empty, is the natural projection still
surjective?

260
Functions
(iii)
Let {j1, j2, . . . , jm} be a set of positive integers such that 1 ⩽
j1 < j2 < · · · < jm ⩽n. Show that the natural projection
X1 × X2 × · · · × Xn →Xj1 × Xj2 × · · · × Xjm
deﬁned by
(x1, x2, . . . , xn) 7→(xj1, xj2, . . . , xjm)
is a surjection.
5.4
Bijections and Inverse Functions
In the previous sections we deﬁned two special kinds of functions: injections and
surjections. Functions which are both injective and surjective have interesting and
important properties; they are the subjects of this section.
Deﬁnition 5.6
A function f : A →B is bijective or is a bijection if it is both injective
and surjective.
The terms one-to-one correspondence and one-to-one onto function are also
used for ‘bijection’.
Examples 5.10
1.
In example 5.7.3 we proved that the function f : R →R deﬁned by
f(x) = 3x −7 is a bijection.
2.
Show that f : R+ ∪{0} →R+ ∪{0}, x 7→x2, is a bijection.
Solution
If x1 and x2 are both non-negative then x 2
1 = x 2
2 implies x1 = x2, so f is
injective. If y ⩾0 then √y is also a non-negative real number and f(√y) = y,
so f is surjective.

Bijections and Inverse Functions
261
Note that this example underlines again the importance of the domain and
codomain in the deﬁnition of a function. We have already seen in example 5.7.1
that f : R →R, x 7→x2, is neither injective nor surjective. Thus the properties
of a function depend crucially on its domain and codomain as well as the ‘rule of
association’. In particular, the statement ‘the function f(x) = x2 is bijective’ is
ambiguous at best and meaningless at worst.
3.
Let E+ = {2n : n ∈Z+} denote the set of even positive integers and
consider the function f : Z+ →E+ deﬁned by f(n) = 2n.
Now f(n) = f(n′) ⇒2n = 2n′ ⇒n = n′, so f is injective, and
if m ∈E+ then n = m/2 ∈Z+ and f(n) = m, so f is surjective.
Therefore f is a bijection.
4.
Let f be the function R2 →R2 deﬁned by f(x, y) = (2x −3y, x −2y).
Show that f ◦f = idR2 and deduce that f is a bijection.
Solution
Let (x, y) ∈R2. Then
(f ◦f)(x, y) = f(2x −3y, x −2y)
= (2(2x −3y) −3(x −2y), (2x −3y) −2(x −2y))
= (x, y).
Therefore f ◦f = idR2. We can use this property to prove that f is both injective
and surjective. Let (x, y), (x′, y′) ∈R2. Then:
f(x, y) = f(x′, y′)
f(f(x, y)) = f(f(x′, y′))
⇒
(f ◦f)(x, y) = (f ◦f)(x′, y′)
⇒
(x, y) = (x′, y′)
⇒
so f is injective.
To show that f is surjective, let (a, b) ∈R2 and deﬁne (x, y) = f(a, b). Then
f(x, y) = f(f(a, b)) = (f ◦f)(a, b) = (a, b)
so f is surjective.
The properties of injections and surjections given in theorems 5.2, 5.3 and 5.5
immediately imply the following results.

262
Functions
Theorem 5.6
Let f : A →B be a function where A and B are subsets of R. Then f is
bijective if and only if every horizontal line through a point of B meets the
graph of f exactly once.
Theorem 5.7
(i)
The composite of two bijections is a bijection.
(ii)
If f : A →B is a bijection, where A and B are ﬁnite sets, then
|A| = |B|.
Note that the converse of (i) is false—if a composite function g ◦f is bijective,
it does not follow that both f and g need be bijective. A counter-example to this
is provided by the functions in example 5.9. Theorem 5.4 gives the most general
result in the reverse direction—if g ◦f is a bijection then f is injective and g is
surjective.
There is a kind of converse to (ii). If A and B are ﬁnite sets with the same
cardinality, then there exists a bijection from A to B which can easily be deﬁned
as follows. Suppose |A| = |B| = n; list the elements of A and B respectively
as {a1, . . . , an} and {b1, . . . , bn}. Then a bijection f is given by f(ai) = bi for
i = 1, . . . , n. Clearly if n ⩾2 there is more than one choice of bijection A →B;
in fact there are n! = n(n −1)(n −2) . . . 2.1 different such bijections—see
exercise 5.4.7.
Theorem 5.7(ii) implies that there is no bijection from a ﬁnite set to a proper
subset. By contrast, example 5.10.3 gives a bijection from the (inﬁnite) set of
positive integers to a proper subset—the (inﬁnite) set E+ of even positive integers.
In fact every inﬁnite set has the property that there exists a bijection from itself to
some proper subset. This property can therefore be used to characterize inﬁnite
sets without having to refer to numbers of elements: a set A is inﬁnite if and only
if there exists a proper subset B and a bijection A →B.
The argument used in example 5.10.4 to prove that f is bijective generalizes to
any function f : A →A such that f ◦f = idA. We state this as a theorem.

Bijections and Inverse Functions
263
Theorem 5.8
Let f : A →A be a function such that f ◦f = idA. Then f is a bijection.
Proof
Suppose f ◦f = idA. Let a, b ∈A. Then:
f(a) = f(b)
f(f(a)) = f(f(b))
⇒
(f ◦f)(a) = (f ◦f)(b)
⇒
a = b
⇒
so f is injective.
Let c ∈A and deﬁne a = f(c). Then
f(a) = f(f(c)) = (f ◦f)(c) = c
so f is surjective.
□
We now turn to the question raised in exercise 5.1.16: given a function f : A →
B, under what circumstances does g = {(b, a) : (a, b) ∈f} deﬁne a function?
We can think of g as ‘reversing the arrows’ in the arrow diagram of f: if b = f(a)
then a = g(b). See ﬁgure 5.15.
Figure 5.15
Example 5.2.6 indicates that this will not in general deﬁne a function. The square
function f from R to R is formally deﬁned as {(x, y) ∈R × R : y = x2}.
However, the relation g = {(y, x) ∈R × R : y = x2} is not a function as we
explained in example 5.2.6.
Returning to the general situation, let f : A →B be a function and deﬁne the
relation g = {(b, a) : (a, b) ∈f}. Now, according to deﬁnition 5.1, g is a function

264
Functions
if for each b ∈B there exists a unique a ∈A such that (b, a) ∈g or, equivalently,
(a, b) ∈f. The existence of some a ∈A with the required property for each
element of B is precisely the requirement that f is surjective. Furthermore, an
element a ∈A such that (a, b) ∈f is unique if and only if f is injective. This
is because the existence of two elements a, a′ ∈A such that (a, b), (a′, b) ∈f is
equivalent to f(a) = f(a′) for two different elements of A. These arguments can
be summarized as follows.
Theorem 5.9
Let f : A →B be a function. The relation g = {(b, a) ∈B × A : (a, b) ∈
f} is a function from B to A if and only if f is a bijection.
Deﬁnition 5.7
If f : A →B is a bijection then the function g : B →A deﬁned by
g(b) = a if and only if f(a) = b is called the inverse function of f and is
denoted f −1.
Theorem 5.10
Let f : A →B be a bijection and let f −1 : B →A be its inverse. Then
f −1 ◦f = idA and f ◦f −1 = idB.
Proof
Let a ∈A and suppose b = f(a). Then a = f −1(b), so
(f −1 ◦f)(a) = f −1(f(a)) = f −1(b) = a.
Hence f −1 ◦f = idA.

Bijections and Inverse Functions
265
Now let b′ ∈B and suppose a′ = f −1(b′). Then b′ = f(a′), so
(f ◦f −1)(b′) = f(f −1(b′)) = f(a′) = b′.
Hence f ◦f −1 = idB.
□
Examples 5.11
1.
For any set A the identity function on A, idA, is its own inverse function.
2.
We can now deﬁne the ‘square root’ function as the inverse of the
bijection f : R+ ∪{0} →R+ ∪{0}, x 7→x2 deﬁned in example 5.10.2.
Since, for non-negative real numbers, y = x2 if and only if x = √y, the
inverse function is
f −1 : R+ ∪{0} →R+ ∪{0} deﬁned by f −1(y) = √y.
3.
Let f : R →R be deﬁned by f(x) = 5x + 8. Show that f is a bijection
and ﬁnd its inverse.
Solution
If we can ﬁnd the inverse function f −1 then f must be a bijection, by theorem 5.9.
To ﬁnd f −1 we simply use its deﬁnition: if y = f(x) then x = f −1(y).
Now
y = f(x)
y = 5x + 8
⇒
y −8 = 5x
⇒
(y −8)/5 = x
⇒
x = f −1(y) = (y −8)/5.
⇒
Therefore the inverse function is f −1 : R →R, f −1(y) = (y −8)/5.
4.
In example 5.10.4, we showed that f : R2 →R2, f(x, y) = (2x−3y, x−
2y) is a bijection. Find its inverse.
Solution
In example 5.10.4 we showed that f ◦f = idR2. It follows from theorem 5.10
that f −1 = f, so
f −1 : R2 →R2,
f −1(x, y) = (2x −3y, x −2y).

266
Functions
5.
Show that f : R −{1} →R −{2} deﬁned by
f(x) =
2x
x −1
is bijective and ﬁnd its inverse.
Solution
Again we show that f is a bijection by ﬁnding its inverse. We let y = f(x) and
solve for x to ﬁnd x = f −1(y).
Now
y =
2x
x −1
y(x −1) = 2x
⇒
yx −2x = y
⇒
x(y −2) = y
⇒
x =
y
y −2.
⇒
Therefore we deﬁne a function
g : R −{2} →R −{1},
g(y) =
y
y −2.
It is a routine matter to check that (g ◦f)(x) = x for all x ∈R −{1} and
(f ◦g)(y) = y for all y ∈R −{2}. Therefore f is bijective and f −1 = g.
Exercises 5.4
1.
Determine which (if any) of the following functions Z →Z is bijective
and, for each bijection, ﬁnd its inverse.
(i)
f : Z →Z, f(n) = n −17.
(ii)
f : Z →Z, f(n) = 2n + 8.
(iii)
f : Z →Z, f(n) = (n −1)(n + 3).
(iv)
f : Z →Z, f(n) = n + 5.
(v)
f : Z →Z, f(n) =
(
n −1
if n is even
n + 1
if n is odd.

Bijections and Inverse Functions
267
2.
(i)
Let f : Z5 →Z5, f([n]) = [3n + 1] and g : Z5 →Z5,
g([n]) = [2n + 3]. Find g ◦f and f ◦g. Deduce that f is a
bijection and identify f −1.
(ii)
Let f : Z5 →Z5 be given by f([n]) = [4n + 3]. Find f ◦f. What
can you deduce about f?
(iii)
Let f : Z5 →Z5 be given by f([n]) = [n3]. Evaluate f([0]),
f([1]), f([2]), f([3]) and f([4]). Hence determine f −1.
3.
Show that each of the following is a bijection and ﬁnd its inverse.
(i)
f : R →R, f(x) = 5x + 3
8
.
(ii)
f : R −{−1} →R −{3}, f(x) =
3x
x + 1.
(iii)
f : [1, 3] →[−2, 2], f(x) = 2x −4.
(iv)
f : R+ →(0, 1), f(x) =
1
x + 1.
(v)
f : R2 →R2, f(x, y) = (y, x).
(vi)
f : R2 →R2, f(x, y) = (2x −1, 5y + 3).
(vii)
f : R2 →R2, f(x, y) = (2x −y, x −2y).
(viii) f : R →R, f(x) = (2x + 3)3.
(ix)
f : Z+ →Z, f(n) =
(
n/2
if n is even
(1 −n)/2
if n is odd.
(x)
f : Z+ × {0, 1} →Z, f(n, m) =
(
n −1
if m = 0
−n
if m = 1.
4.
(i)
Let A be any (non-empty) set and let C be a subset of A. A
function δC is deﬁned by
δC : A →{0, 1},
δC(a) =
(
0
if a /∈C
1
if a ∈C.
(a)
Let A = {a, b, c, d, e} and C = {b, d, e}. Evaluate δC(x)
for each x ∈A.

268
Functions
(b)
Under what circumstances is δC injective?
(c)
Under what circumstances is δC surjective?
(ii)
Let A = P{a, b} and B = {0, 1} × {0, 1} = {(0, 0), (0, 1),
(1, 0), (1, 1)}. A function f : A →B is deﬁned by f(C) =
(δC(a), δC(b)). Show that f is a bijection.
(iii)
Suppose |X| = n. How can the function f deﬁned in part (ii) be
generalized to a bijection P(X) →{0, 1}n?
5.
For each of the following pairs, A and B, of subsets of R, ﬁnd an explicit
bijection f : A →B.
(i)
A = [0, 1],
B = [1, 3].
(ii)
A = (0, 1),
B = R+.
(iii)
A = R+,
B = R.
(iv)
A = (0, 1),
B = R.
(v)
A = Z,
B = Z+.
6.
Let f and g be functions R →R and k ∈R. The functions f + g, f ∗g
and kf : R →R are deﬁned respectively by
(f + g)(x) = f(x) + g(x)
(f ∗g)(x) = f(x)g(x)
(kf)(x) = kf(x).
(i)
Prove that, if k ̸= 0, then kf is a bijection if and only if f is a
bijection.
(ii)
Find bijections f and g such that neither f + g nor f ∗g is a
bijection.
7.
(i)
Prove that if |A| = n then there are n! = n(n −1)(n −2) . . . 2.1
different bijections A →A.
(ii)
Suppose |A| = n and |B| = m, where n ⩽m. Show that there
are m!/(m −n)! = m(m −1)(m −2) . . . (m −n + 1) different
injections A →B.
(iii)
Counting the number of surjections A →B in the case where
|A| = n ⩾m = |B| is much harder.
The number of such
surjections is S(n, m)×m!, where S(n, m) is a so-called ‘Stirling
number of the second kind’.
(Unfortunately, there is no easy
formula for S(n, m). Stirling numbers crop up in various counting
problems such as this.)
In some special cases, however, the number of surjections A →B
can be identiﬁed. Show that:

Bijections and Inverse Functions
269
(a)
if m = 1, there is only one surjection A →B;
(b)
if m = 2, there are 2n −2 surjections A →B;
(c)
if m = n−1, there are 1
2n(n−1)×m! surjections A →B.
8.
Let A and B be ﬁnite sets with the same number of elements and let
f : A →B be a function. Prove that f is injective if and only if f is
surjective.
(Note: this result is useful if we need to show that a given function
f : A →B is a bijection where |A| = |B|. All we are required to
do is either show that f is an injection or show that f is a surjection.)
9.
Let f : A →B be a function. Show that f is bijective if and only if
f(A −C) = B −f(C), for every subset C of A.
10.
(If you are not familiar with the theory of matrices, you should read
chapter 6 before attempting this question.)
Let X = {x : x is a 2 × 1 column matrix/vector}. Thus X is just R2 in
another notation.
Let A be a 2 × 2 matrix, and deﬁne f : X →X by f(x) = Ax.
Show that f is a bijection if and only if A is non-singular.
11.
If f : A →B is injective but not surjective then deﬁning the inverse
f −1 : B →A by f −1(b) = a if and only if b = f(a) deﬁnes a partial
function. This is illustrated in the diagrams below.
Determine the inverse of each of the following functions. In each case,
state whether the inverse is a partial function or a total function; if the
inverse function is partial, state its domain.
(i)
f : {1, 2, 3, 4, 5} →{1, 2, 3, 4, 5, 6, 7, 8}, f : x 7→x + 2.
(ii)
f : {1, 2, 3, 4, 5} →{0, 1, 2, 3, 4}, f : x 7→x2 −1
x + 1 .

270
Functions
(iii)
f : R →R, f : x 7→4x −8.
(iv)
f : Z →Z, f : x 7→2x + 1.
(v)
f : Z →Z+, f(n) =
(
n2 + 1
if n < 0
n2
if n ⩾0.
5.5
More on Cardinality
In this section we introduce brieﬂy a theory of cardinality due to Cantor† which
enables inﬁnite sets of different cardinality to be deﬁned. This theory, which is
in essence very simple, caused great controversy in the mathematical community
when it was introduced by Cantor in the 1870s and 1880s. As the material in this
section is not used elsewhere in the book, it may safely be omitted. However, the
ideas that Cantor introduced are indeed of great importance to mathematics and
we hope you will ﬁnd them stimulating. As Hallett (1984) says: ‘Cantor was the
founder of the mathematical theory of the inﬁnite, and so one might with justice
call him the founder of modern mathematics’.
With the advertisement over, we now turn to the theory. The starting point is
theorem 5.7(ii): if there exists a bijection between ﬁnite sets then those sets
must have the same cardinality. Since the notion of bijection is a purely set-
theoretic one, it does not require the sets involved to be ﬁnite. We can therefore
use theorem 5.7(ii) to deﬁne cardinality for inﬁnite sets. More precisely, it deﬁnes
the notion of two sets (ﬁnite or inﬁnite) having the same cardinality.
Deﬁnition 5.8
Two (ﬁnite or inﬁnite) sets A and B are said to have the same cardinality,
written |A| = |B|, if there exists a bijection f : A →B.
† Georg Cantor was born in St Petersburg in 1845, but spent most of his life in Germany. He was the
ﬁrst person to provide a satisfactory theory of the inﬁnite. One of the ﬁercest critics of Cantor’s theory
was his former teacher, Leopold Kronecker, who Cantor believed was responsible for his failure to be
appointed professor at the University of Berlin. Possibly the attacks of Kronecker and others led to the
nervous breakdowns Cantor suffered. Although he also received praise from contemporaries, notably
David Hilbert, Cantor was plagued by self-doubt and eventually died in 1918 in a mental institution.

More on Cardinality
271
Example 5.10.3 shows that there is a bijection between the sets of positive
integers Z+ and positive even integers E+; therefore the two sets have the same
cardinality. Thus an inﬁnite set may have the same cardinality as a proper subset!
In fact, as we remarked in §5.4, this property characterizes inﬁnite sets. Any set
which has the same cardinality as Z+ is said to have cardinality ℵ0. (This is read
as ‘aleph nought’ or ‘aleph zero’; the symbol ℵis the ﬁrst letter of the Hebrew
alphabet). Thus |A| = ℵ0 if there exists a bijection Z+ →A.
Any set with cardinality ℵ0 is said to be countably inﬁnite. The reason for the
terminology is the following. Suppose |A| = ℵ0. Then, by deﬁnition, there is a
bijection f : Z+ →A. If we denote f(n) ∈A by an, we can regard the bijection
f as ‘listing’ or ‘counting’ the elements of A as a1, a2, . . . , an, . . . . Since this
listing or counting of the elements of A is an inﬁnite process, we say that A is
countably inﬁnite.
Examples 5.12
1.
The set P of prime numbers is countably inﬁnite.
In chapter 2 we
presented Euclid’s proof that P is an inﬁnite set.
We can deﬁne the
required bijection f : Z+ →P as follows. List the elements of Z+
and P in increasing order:
Now deﬁne f : Z+ →P by f(n) = pn, the nth prime number in the list.
Although there is no (known) formula for f(n), we can in principle ﬁnd
f(n) for any positive integer n. Thus our description of f does deﬁne a
function, which is clearly bijective. Hence |P| = ℵ0.
2.
The set Q = {p/q : p, q ∈Z and q ̸= 0} of rational numbers also has
cardinality ℵ0. (In view of the fact that between any two integers there
are inﬁnitely many rationals, this result is, at ﬁrst sight, rather surprising.)
We need to deﬁne a bijection Z+ →Q; again our deﬁnition is descriptive.
Firstly, note that we can list the rational numbers in a two-dimensional

272
Functions
array as follows. (Ignore the arrows for the present.)
This array clearly includes every rational number, but it also has many
repeats. For example 0 = 0/2 = 0/3 = · · · ; 1/2 = 2/4 = 3/6 = · · · ;
etc.
To deﬁne a bijection f : Z+ →Q, begin with f(1) = 0 and follow the
arrows around the array, ignoring any elements of Q which have already
been encountered. Now deﬁne f(n) to be the nth rational obtained in this
way.
Thus we have f(1) = 0, f(2) = 1, f(3) = −1, f(4) = −2,
f(5) = −1/2, f(6) = 1/2, f(7) = 2, f(8) = 3, f(9) = 1/3,
f(10) = −1/3, f(11) = −3, etc.
As in the previous example, it would be virtually impossible to deﬁne
a formula for f(n). However, our description does deﬁne a function
f : Z+ →Q which is bijective because, in deﬁning f, we skip over
those rational numbers which we have already encountered.
Therefore |Q| = ℵ0.
At this point you may be beginning to think that all inﬁnite sets have cardinality
ℵ0. In fact, this is not the case. Using his now-famous ‘diagonal argument’,
Cantor proved that the set of real numbers is not countably inﬁnite.
Theorem 5.11
The set R is not countably inﬁnite.

More on Cardinality
273
Proof
The proof is by contradiction.
We ﬁrst represent each real number x as a (non-terminating) decimal numeral
x0.x1x2 . . . xn . . . . Here x0 is an integer and, for i ⩾1, each xi is an integer in
the range 0 ⩽xi ⩽9. If the usual decimal of x terminates we simply add a whole
string of zeros to the end.
For example, 37 1
4 = 37.250 00 . . . , π = 3.141 59 . . . , and so on. There is a
problem with this, however—the decimal numeral for x is not necessarily unique
due to the possibility of recurring nines. For instance, 0.5000 . . . = 0.4999 . . . . If
we adopt the convention that the decimal numeral is not to end in recurring nines,
then every x ∈R does have a unique decimal expansion.
Now suppose there does exist a bijection f : Z+ →R. Thus we can list the
elements of R in some inﬁnite list:
f(1) = a0.a1a2a3a4 . . .
f(2) = b0.b1b2b3b4 . . .
f(3) = c0.c1c2c3c4 . . .
...
...
Of course, we soon run out of letters of the alphabet, but that does not alter the
principle. To obtain a contradiction, we are going to deﬁne a real number x
(between 0 and 1) which is not equal to f(n) for any n ∈Z+. The existence
of such an x shows that f is not surjective.
In order to deﬁne x, we specify its decimal numeral 0.x1x2x3 . . . as follows. If
the nth decimal place of f(n) is 4, we deﬁne xn = 3; if the nth decimal place of
f(n) is not equal to 4, we deﬁne xn = 4. The choice of 3 and 4 here is more or
less arbitrary. What is important is that xn is deﬁned to be different from the nth
decimal place of f(n).
To clarify the deﬁnition of x, suppose that the ﬁrst few elements in out list are:
f(1) =
3.2178 . . .
f(2) = −1.6422 . . .
f(3) = 13.0187 . . .
f(4) = −0.9876 . . . .
In this case the decimal numeral of x would begin 0.4344. ... (The second decimal
place of x has the value 3 because the second decimal place of f(2) is 4.) Note
that the nth decimal places of f(n) and x are different in each case.

274
Functions
This deﬁnes a real number x. Now x is not equal to f(n), for any n ∈Z+,
because it differs from f(n) at least in the nth decimal place. (Of course, x and
f(n) will usually differ in many other decimal places as well.) Therefore x does
not appear anywhere in the list, which contradicts the surjectivity of f.
Hence there is no bijection f : Z+ →R.
□
The cardinality of R is usually denoted c, which stands for ‘continuum’. We now
know that there are at least two different inﬁnite cardinalities, ℵ0 and c.
In fact Cantor’s diagonal argument, given in the proof of the previous theorem,
can be modiﬁed to show that, for any set A, the cardinality of A differs from the
cardinality of its power set P(A); see exercise 5.5.2. We are already familiar
with this in the case of ﬁnite sets, for which |P(A)| = 2|A| (theorem 3.5).
Knowing this, Cantor was able to determine an inﬁnite sequence of different
inﬁnite cardinalities:
|Z+|, |P(Z+)|, |P(P(Z+))|, |P(P(P(Z+)))|, . . . .
Although it is beyond the scope of the present chapter, it is possible to show that
c is the second cardinality in this sequence, in other words, that the sets R and
P(Z+) have the same cardinality. An interesting question is whether there is any
cardinality ‘in between’ ℵ0 and c. To be more precise, extending theorem 5.5 (i)
to arbitrary sets, we can deﬁne an order relation on cardinalities by
α ⩽β
if and only if
there exists sets A and B such that |A| = α, |B| = β
and there exists an injection A →B.
Then the question is: does there exist a set S such that
ℵ0 < |S| < c ?
Cantor believed, although he was unable to prove, that the answer to the question
is ‘no’. This conjecture—that there is no such set S—became known as Cantor’s
continuum hypothesis, and its proof (or disproof) was much sought†. Eventually,
in 1938,the Austrian logician Kurt G¨odel showed that it is impossible to prove the
† In 1900, David Hilbert, whom many regard as the leading mathematician of his day, addressed
the Second International Congress of Mathematicians in Paris.
In order to anticipate the future
development of mathematics, Hilbert outlined 23 problems whose solutions would make signiﬁcant
progress in the subject. In the ﬁrst of these problems, Hilbert asked for a resolution of the continuum
hypothesis. (Hilbert himself had tried unsuccessfully to prove Cantor’s hypothesis.)
Hilbert was one of the earliest champions of Cantor’s work on the inﬁnite. At the height of the
controversy surrounding Cantor’s theory, he wrote: ‘No one shall drive us from the paradise which
Cantor has created for us’.

More on Cardinality
275
continuum hypothesis is false using the usual axioms about sets. Much later, in
1963, the American Paul Cohen showed that the continuum hypothesis cannot be
proved to be true either using the usual axioms about sets. Therefore the truth or
falsity of the continuum hypothesis is undecidable in axiomatic set theory. One
can therefore choose whether to assume its truth or its falsity. More precisely, we
can choose to include the continuum hypothesis or its negation as an additional
axiom of set theory. This seems somewhat paradoxical. It means that there are
two different versions of set theory—one where the continuum hypothesis is ‘true’
and one where it is ‘false’. (In fact, it is now known that there are other statements
of this type so there are many different versions of set theory! Fortunately they
only differ in rather esoteric aspects—the properties of sets developed in chapter 3
are common to all the different set theories.)
Having deﬁned a hierarchy of inﬁnite cardinalities, Cantor set about deﬁning
arithmetic operations for them. That is, Cantor deﬁned addition, multiplication
and exponentiation of (inﬁnite) cardinalities. This might sound like an extremely
difﬁcult task, but in fact the deﬁnitions are very obvious! Recall the following
facts about ﬁnite sets A and B.
1.
If A ∩B = ∅then |A ∪B| = |A| + |B|. (Counting principle 1, p 94.)
2.
|A × B| = |A| × |B|. (See theorem 3.6.)
3.
If C = {functions A →B} then |C| = |B||A|. (Exercise 5.1.14.)
Cantor simply—and boldly!—used these three facts to deﬁne addition,
multiplication and exponentiation for arbitrary cardinalities.
Deﬁnition 5.9
Let A and B be (ﬁnite or inﬁnite) sets and let |A| = α and |B| = β. Then
(i)
α + β = |A ∪B|, provided A ∩B = ∅;
(ii)
αβ = |A × B|;
(iii)
βα = |C|, where C is the set of all functions A →B.
We need to check that these deﬁnitions make sense. For addition to be well
deﬁned, for example, we need to show that α + β depends only on α and β and
not on the particular sets A and B used in the deﬁnition. That is, if |A| = |A′| and
|B| = |B′|, where A ∩B and A′ ∩B′ are both empty, then |A ∪B| = |A′ ∪B′|.
Similar remarks apply to the deﬁnition of αβ and βα.
The proofs of these
necessary facts are beyond the scope of this book, and we shall have to be content
with assuming them.

276
Functions
Provided we accept that these deﬁnitions are indeed well deﬁned, some interesting
results can be obtained. We conclude the section with a small selection of these.
Examples 5.13
1.
ℵ0 + ℵ0 = ℵ0.
Proof
We have seen in example 5.10.3 that the set of even positive integers E+ has
cardinality ℵ0. A similar argument shows that O+, the set of odd positive integers,
also has the cardinality ℵ0. Since E+ ∩O+ = ∅,
ℵ0 + ℵ0 = |E+ ∪O+| = |Z+| = ℵ0.
2.
ℵ0ℵ0 = ℵ0.
Proof
Using a similar argument to the one given in example 5.12.2 (which showed
|Q| = ℵ0), we can prove that |Z+ × Z+| = ℵ0, from which the result follows by
deﬁnition.
3.
2ℵ0 = ℵ1.
Proof
Recall that ℵ1 = |P(Z+)| and 2ℵ0 = |C|, where C is the set of functions
Z+ →A, for some set A with two elements. Since we can choose A to be
any set with two elements, we might as well take A = {0, 1}.
Given X ⊆Z+ deﬁne a function fX : Z+ →{0, 1} by fX(n) = 0 if n /∈X
and fX(n) = 1 if n ∈X. We can now deﬁne a function F : P(Z+) →C by
F(X) = fX.
To prove 2ℵ0 = ℵ1, we need to show that F is a bijection. This is most easily
done by deﬁning its inverse. So we deﬁne G : C →P(Z+) as follows. If f :
Z+ →{0, 1} is a function, deﬁne a subset Xf = {n ∈Z+ : f(n) = 1} ⊆Z+;
now deﬁne G(f) = Xf.
A little thought should convince you that G = F −1. Hence F is a bijection, so
2ℵ0 = ℵ1.

Databases: Functional Dependence and Normal Forms
277
Exercises 5.5
1.
Determine the cardinality of each of the following sets:
(i)
{n ∈Z : n ⩾106}
(ii)
(0, 1) = {x ∈R : 0 < x < 1}
(iii)
Z+ × {0, 1}
(iv)
Z+ × Z+ × Z+.
2.
Use modiﬁcation of Cantor’s diagonal argument (theorem 5.11) to show
that there is no bijection A →P(A), for any set A.
(Hint: suppose that there is a bijection f : A →P(A). Then, for every
x ∈A, f(x) is a subset of A; call it Ax. Now consider the subset B of
A deﬁned by B = {x ∈A : x /∈Ax}. Show, by contradiction, that B is
not the image of any x ∈A.)
3.
Show that, for any set A, |P(A)| = 2|A|. (Hint: see example 5.13.3.)
4.
Prove each of the following.
(i)
ℵ0 + k = ℵ0, for any k ∈Z+.
(ii)
(ℵ0)2 = ℵ0. (Note that this is not the same as example 5.13.2,
which shows ℵ0ℵ0 = ℵ0.
By deﬁnition 5.9(iii), (ℵ0)2 is the
cardinality of the set of functions {0, 1} →Z+.)
5.
Show that α2
=
αα for any cardinality α.
(See the remark in
exercise 5.5.4(ii) above.)
5.6
Databases: Functional Dependence and Normal Forms
In §4.7 we introduced some of the basic concepts of and operations on relational
databases. The purpose of this section is to use some of the ideas of this chapter
to develop further the database concepts. In particular we consider the notion of
functional dependence and normal forms as applied to relational databases.
Firstly, however, we reconsider from the point of view of functions the operation
of projection introduced in §4.7. The following example shows that the projection
operation on tables can be viewed as a function.

278
Functions
Example 5.14
Let A1, A2, A3, A4 be attributes and let R be a table with attribute type
(A1, A2, A3, A4). Recall that this means that R ⊆(X1 × X2 × X3 × X4) where
each Xi is the set of data items corresponding to the attribute Ai. The elements
(x1, x2, x3, x4) ∈R are called record instances.
Projection of R onto (A2, A4) produces a new table, S say, with attribute type
(A2, A4) whose record instances consist of just the (A2, A4) values of the record
instances of R. (Recall that a table is an abstract concept and need not correspond
to a ﬁle stored on any medium. Thus projection does not produce a new ﬁle stored
on disk or tape.) The table S is
S = {(a2, a4) : x2 = a2 and x4 = a4 for some (x1, x2, x3, x4) ∈R}.
Recall from exercise 5.3.13 that there is a natural projection function
p : X1 × X2 × X3 × X4 →X2 × X4,
p(x1, x2, x3, x4) = (x2, x4).
It should be clear that the table S is simply p(R), the image of R under the natural
projection p.
The general situation is conceptually no more difﬁcult than the previous example.
However, the general case is more complicated to describe because the notation
is necessarily more complex. Let R be a table of attribute type (A1, A2, . . . , An).
Let I = {i1, i2, . . . , ik} be a set of indices such that 1 ⩽i1 < i2 < · · · < ik ⩽n.
The natural projection function with index set I is the function
pI : X1 × X2 × · · · × Xn →Xi1 × Xi2 × · · · × Xik
deﬁned by pI(x1, x2, . . . , xn) = (xi1, xi2, . . . , xik). Projection of the table R
onto attribute type (Ai1, Ai2, . . . , Aik) deﬁnes the new table S = pI(R). The
record instances of S are the k-tuples (xi1, xi2, . . . , xik) obtained from the n-
tuples of R by deleting those xj where j does not belong to the index set I. In
example 5.14, the index set is I = {2, 4}.
Functional Dependence
The idea of functional dependence between attributes is of fundamental
importance to relational database theory in general and the deﬁnition of various
normal forms in particular. As the terminology suggests, functional dependence

Databases: Functional Dependence and Normal Forms
279
between attributes is closely connected with the concept of a function. Informally,
in a table R, an attribute Aj is said to be ‘functionally dependent’ on an attribute
Ai if record instances with the same Ai value but different Aj values never
occur. This means that, as far as the table R is concerned, specifying a value
of Ai uniquely determines a value of Aj. In this situation we also say that Ai
‘functionally determines’ Aj.
The precise connection between functional dependence and the concept of a
function can be described as follows. Let Ai and Aj be attributes of a table R
and let I = {i, j}. For convenience we suppose i < j. Natural projection of
R onto attribute set {Ai, Aj} produces the relation S = pI(R) comprising all
ordered pairs (ai, aj) such that there is some element (x1, . . . , xn) ∈R with
xi = ai and xj = aj. Thus S is a subset of the Cartesian product Xi × Xj. The
attribute Ai functionally determines the attribute Aj if and only if the relation
S is a function Xi →Xj. Brieﬂy, Ai functionally determines Aj (and Aj is
functionally dependent on Ai) if projection onto {Ai, Aj} deﬁnes a relation
which is a function Xi →Xj.
Example 5.15
Let X1 = {α1, α2, α3}, X2 = {β1, β2, β3, β4}, X3 = {γ1, γ2} and X4 =
{δ1, δ2, δ3, δ4, δ5}. A table R of type {A1, A2, A3, A4} is deﬁned below.

280
Functions
A1
A2
A3
A4
α1
β1
γ2
δ1
α1
β2
γ2
δ3
α1
β3
γ2
δ5
α2
β4
γ1
δ1
α3
β3
γ2
δ2
α3
β1
γ2
δ4
α3
β2
γ2
δ4
Projecting R onto the attribute set {A1, A2} gives the relation
S = {(α1, β1), (α1, β2), (α1, β3), (α2, β4), (α3, β3), (α3, β1), (α3, β2)}.
The relation S is not a function X1 →X2 since, for example, α1 S β1 and
α1 S β2. Therefore the attribute A2 is not functionally dependent on attribute A1.
Specifying the value of A1 to be α1, for example, does not determine the value of
A2—it could be β1, β2 or β3.
Now project R onto attribute set {A1, A3}.
This produces the relation T =
{(α1, γ2), (α2, γ1), (α3, γ2)}. Now T is a function X1 →X3 given in function
notation by
α1 7→γ2
α2 7→γ1
α3 7→γ2.
Therefore A3 is functionally dependent on A1.
We leave it as an exercise (5.6.1) to determine all of the remaining functional
dependences (if any) in R.
The notion of functional dependence readily generalizes to sets of attributes but
again is notationally more complicated to describe. Informally, a set of attributes
{Aj1, . . . , Ajm} is functionally dependent on a set of attributes {Ai1, . . . , Aik} if
specifying the values of each of the attributes Ai1, . . . , Aik uniquely determines
the values of each of the attributes Aj1, . . . , Ajm.
For the relation R deﬁned in example 5.15, it is not too difﬁcult to see
that {A1, A2} functionally determines {A4}. Projecting R onto {A1, A2, A4}
produces the relation
{(α1, β1, δ1), (α1, β2, δ3), (α1, β3, δ5), (α2, β4, δ1),(α3, β3, δ2),
(α3, β1, δ4), (α3, β2, δ4)}.

Databases: Functional Dependence and Normal Forms
281
Therefore, specifying the values of A1 and A2 determines the value of A4. We
may regard this relation as deﬁning a function
{(α1, β1), (α1, β2), (α1, β3), (α2, β4), (α3, β3), (α3, β1), (α3, β2)} →X4
given by
(α1, β1) 7→δ1
(α1, β2) 7→δ3
(α1, β3) 7→δ5
(α2, β4) 7→δ1
(α3, β3) 7→δ2
(α3, β1) 7→δ4
(α3, β2) 7→δ4.
Note that the domain of this function,
{(α1, β1), (α1, β2), (α1, β3), (α2, β4), (α3, β3), (α3, β1), (α3, β2)},
is the projection of R onto {A1, A2} and the codomain X4 is the projection of R
onto {A4}.
To give a formal deﬁnition of functional dependence between sets of attributes,
let I = {i1, i2, . . . , ik} and J = {j1, j2, . . . , jm} be disjoint sets of indices.
For convenience we suppose i1 < i2 < · · · < ik < j1 < j2 < · · · <
jm. Then I ∪J = {i1, . . . , ik, j1, . . . , jm} where the indices are written in
increasing order. Let SI∪J = pI∪J(R) be the projection of R onto the set of
attributes AI∪J = {Ai1, . . . , Aik, Aj1, . . . , Ajm}. Then AJ = {Aj1, . . . , Ajm}
is functionally dependent on AI = {Ai1, . . . , Aik} if the set SI∪J is a function
from SI = pI(R) to SJ = pJ(R). Again we also say that AI functionally
determines AJ in this situation. In the example above I = {1, 2} and J = {4}.
What this formal deﬁnition means is that for every k-tuple (ai1, . . . , aik) ∈SI =
pI(R) there exists a unique m-tuple (aj1, . . . , ajm) ∈SJ = pJ(R) such that
the (k + m)-tuple (ai1, . . . , aik, aj1, . . . , ajm) belongs to SI∪J. In other words,
within the table R, the values of the attributes Ai1, . . . , Aik uniquely determine
the values of the attributes Aj1, . . . , Ajm.
The notation needed to describe this general situation may obscure the general
concept being described. Hopefully, a further example will help to clarify the
situation.
Example 5.16
A relation R of type (A1, A2, A3, A4, A5) is deﬁned by the following table.

282
Functions
A1
A2
A3
A4
A5
α1
β1
γ1
δ1
ε1
α1
β1
γ1
δ2
ε1
α1
β2
γ2
δ1
ε1
α1
β3
γ1
δ2
ε2
α1
β3
γ1
δ1
ε2
α2
β1
γ3
δ1
ε3
α3
β1
γ2
δ1
ε1
α3
β1
γ2
δ2
ε1
α3
β4
γ4
δ2
ε3
It is not too difﬁcult to see that no single attribute is functionally dependent on
any other single attribute. For instance, A2 is not functionally dependent on A1
since in rows 2 and 3 there are record instances with the same A1 value (namely
α1) but different A2 values (β1 and β2 respectively). From the point of view of
functions, the projection of R onto {A1, A2} is the relation
S = {(α1, β1), (α1, β2), (α1, β3), (α2, β1), (α3, β1), (α3, β4)}.
The set S is not a function from X1 = {α1, α2, α3} to X2 = {β1, β2, β3, β4}
because there are ordered pairs with the same ﬁrst element but different second
elements.
Using similar arguments, it is not too difﬁcult to see that no single attribute
functionally determines any other single attribute. An equivalent way of viewing
this is to say that no projection of R onto a pair of attributes produces a function.
However, the pair of attributes {A1, A2} functionally determines the pair of
attributes {A3, A5}. (This implies that {A1, A2} functionally determines {A3}
and also that {A1, A2} functionally determines {A5}.
Why?)
To see that
{A1, A2} functionally determines {A3, A5}, consider the projection of R onto
{A1, A2, A3, A5} which gives the following relation.
A1
A2
A3
A5
α1
β1
γ1
ε1
α1
β2
γ2
ε1
α1
β3
γ1
ε2
α2
β1
γ3
ε3
α3
β1
γ2
ε1
α3
β4
γ4
ε3

Databases: Functional Dependence and Normal Forms
283
If we let pij denote the projection of R onto {Ai, Aj} then the set deﬁned by this
table deﬁnes a function from
p12(R) = {(α1, β1), (α1, β2), (α1, β3), (α2, β1), (α3, β1), (α3, β4)}
to
p35(R) = {(γ1, ε1), (γ2, ε1), (γ1, ε2), (γ3, ε3), (γ4, ε3)}.
In the usual notation for a function, we can write this function p12(R) →p35(R)
as follows:
(α1, β1) 7→(γ1, ε1)
(α1, β2) 7→(γ2, ε1)
(α1, β3) 7→(γ1, ε2)
(α2, β1) 7→(γ3, ε3)
(α3, β1) 7→(γ2, ε1)
(α3, β4) 7→(γ4, ε3).
Hence {A1, A2} functionally determines {A3, A5}. Note that this function is not
bijective. This means that {A3, A5} does not functionally determine {A1, A2}.
Note also that projection onto {A1, A2, A3, A5} does not deﬁne a function
between the ‘complete’ Cartesian products X1 ×X2 →X3×X5. This is because
there exist ordered pairs—(α2, β2), for example—which belong to X1 × X2 but
do not belong to p12(R). In the terminology of exercise 5.1.12, projection onto
{A1, A2, A3, A5} deﬁnes a partial function X1 × X2 →X3 × X5.
The notion of a key can also be deﬁned quite succinctly using functional
dependence.
Recall that a candidate key is a set of attributes whose values
uniquely specify a record instance but no proper subset of the candidate key has
this property. For this to be the case, specifying the values of the attributes in the
candidate key uniquely determines the values of the attributes not in the candidate
key. In other words a candidate key is a set of attributes which functionally
determines each attribute of the table but no proper subset of the candidate key
functionally determines each attribute. It will be useful to distinguish between
those attributes in a table R which belong to some candidate key and those which
do not. An attribute of R which does belong to a candidate key is called prime
and an attribute which belongs to no candidate key is called non-prime.
Normal Forms
The various normal forms for tables are designed to avoid problems of redundancy
and inconsistency in the data. A given table will almost inevitably be updated
during its lifetime in the database, usually many times. Record instances will be
modiﬁed or deleted and new record instances will be added to the table. In order
to be able to update tables as appropriate and to prevent this causing anomalies in

284
Functions
the data, it is necessary to avoid certain kinds of functional dependence between
the various attributes of the tables.
To understand what kind of updating anomalies can occur consider the following
example.
Example 5.17
A company, which manufactures components, stores the data relating to its prices,
customers and the customers’ orders in a single table of attribute type (ORDER #,
PART #, CUSTOMER
NAME, CUSTOMER
ADDRESS, PART
PRICE, QUANTITY,
DATE). Whenever an order arrives at the company, it is assigned an order number
and the information it contains is entered as one or more record instances. We
suppose that a customer may order several different components at a time so that
a single order number may have associated with it several different part numbers.
Given the attribute type of the company’s table, this means that a given customer
order may be recorded as several different record instances in the table, one for
each part ordered. Thus {ORDER #} alone would not be a candidate key for
the table, but the pair {ORDER #, PART #} would be a candidate key.
We
suppose that {ORDER #, PART #} has been chosen as the primary key for the
table. This is indicated by printing the key attributes in bold type, so that we
denote the type of this record ﬁle by (ORDER #, PART #, CUSTOMER
NAME,
CUSTOMER
ADDRESS, PART
PRICE, QUANTITY, DATE). In fact, {ORDER #,
PART #} is the only candidate key for this record ﬁle so there is no choice to be
made.
It should be clear that the company is not being very sensible in recording data
referring to its products, its customers and the orders which it receives in a single
table. There are several problems with tables which contain data referring to
different kinds of entities. (We are not suggesting that this scenario is realistic. It
merely serves to highlight some of the problems which can arise and which the
normal forms are designed to eliminate.)
The ﬁrst problem is the considerable amount of duplication of the data being
stored.
For instance the component prices and the customers’ names and
addresses will be recorded many times in different record instances which is
clearly wasteful of the storage medium and causes updating problems. When
a customer’s address or the price of a particular part changes, the company has
a choice of whether or not to update all the relevant record instances. Clearly
such updating may involve a considerable amount of work as these details need
to be changed in many different record instances.
If, however, the company
chooses not to update, then an inconsistency will result with, for example, the
same component having different prices in different record instances or the same

Databases: Functional Dependence and Normal Forms
285
customer having different addresses in different record instances. Again there are
clearly problems associated with this choice. What would result if, for instance,
the company wished to produce a list of its customers’ names and addresses?
Projection onto {CUSTOMER
NAME, CUSTOMER
ADDRESS} would produce a
table where the same customer appears with more than one address. Similarly,
projection onto {PART #, PART
PRICE} would produce a less than helpful table.
There are other perhaps less obvious, but no less serious, problems with the
linking of customer-related and product-related information in a single table. If
the company produces a new product, it has no means of including information in
the table about the part number or price for the new item until one of its customers
places an order for the particular component. Similarly, if for whatever reason the
information concerning a particular order is deleted from the table, then so too are
the data relating to a part number and price. If the record instance being deleted is
the last to contain information about a particular component, then that information
will be lost completely from the table.
As we have said, it is precisely to avoid the kinds of updating problems indicated
in example 5.17 that the different normal forms for tables have been introduced.
Since any table in a relational database is, we suppose, in ﬁrst normal form (see
§4.7), we begin by deﬁning the second normal form.
Deﬁnition 5.10
A table R is in second normal form if no non-prime attribute is
functionally dependent on a proper subset of any candidate key.
We should note that this deﬁnition is not universally accepted. Some authors
deﬁne normal forms in terms of the chosen primary key rather than in terms of
all possible candidate keys. Thus they deﬁne a prime attribute to be one which
appears in the primary key and a table to be in second normal form if no non-
prime attribute is functionally dependent on a proper subset of the primary key.
The deﬁnition of a candidate key says that no proper subset of it functionally
determines every attribute. However, it may happen that a proper subset of a
key functionally determines some attribute and it is precisely this situation that
the second normal form rules out. Of course, a table is automatically in second
normal form if the only candidate keys are single attributes. We can think of
the second normal form as eliminating ‘partial dependences’. Every non-prime

286
Functions
attribute must be functionally dependent on a complete candidate key and not any
‘part’ of it.
Example 5.18
Consider the table introduced in example 5.17. This is not in second normal
form. Recall that the only candidate key is {ORDER #, PART #}. The non-
prime attribute PART
PRICE is functionally dependent on a proper subset of the
key, namely {PART #}.
(We are assuming that the part price does not vary
from order to order or from customer to customer.)
Similarly the attributes
CUSTOMER
NAME and CUSTOMER
ADDRESS are functionally dependent on the
proper subset {ORDER #} of the key.
In order to ‘normalize’ we need to split the table into more than one
table.
(Normalizing a table means replacing it with tables which are in the
appropriate normal form.)
One way of obtaining tables in second normal
form is to divide the original table into three new tables of attribute types
(ORDER #, CUSTOMER
NAME, CUSTOMER
ADDRESS, DATE), (ORDER #,
PART #, QUANTITY) and (PART #, PART
PRICE) respectively. The primary
key for each table is indicated in bold type. (Again in each case there is, in fact,
only one candidate key.) It should be clear that each of these tables is in second
normal form. The ﬁrst and third have single-attribute keys and must therefore be

Databases: Functional Dependence and Normal Forms
287
in second normal form. For the second table, the quantity ordered clearly depends
on both the order number and the part number so it, too, is in second normal form.
One of the problems indicated in example 5.17 has not been resolved in
example 5.18 by splitting the table into three.
Since the customer-related
information is still linked with order number in the ﬁrst of the new tables, a
particular customer’s name and address will be duplicated many times.
If a
customer changes address, this information will have to be changed in many
different record instances. The problem here is essentially that there remains
what is often called a ‘transitive dependence’ in the table. The attribute ORDER #
functionally determines CUSTOMER
NAME and the attribute CUSTOMER
NAME
functionally determines CUSTOMER
ADDRESS (assuming that each customer has
only one address.) The third normal form is designed to eliminate such ‘hidden’
or ‘transitive’ dependences as these.
Deﬁnition 5.11
A table R is in third normal form if
(i)
it is in second normal form, and
(ii)
whenever a non-prime attribute is functionally dependent on a set
of attributes, the set of attributes contains a candidate key as a
subset.

288
Functions
Suppose {Ai : i ∈I} is a set of attributes which does not contain a candidate
key. For a table in third normal form the set {Ai : i ∈I} does not functionally
determine any attribute Aj. (Note that we suppose here that Aj /∈{Ai : i ∈I}—
the deﬁnition of functional dependence applies to disjoint sets of attributes.) If
{Ai : i ∈I} does functionally determine Aj then the table contains the following
‘transitive dependence’: any key K functionally determines {Ai : i ∈I}
(since K functionally determines every attribute) and {Ai : i ∈I} functionally
determines Aj (and, of course, K functionally determines Aj). Thus the third
normal form rules out this kind of ‘transitive dependence’.
It should be noted, however, that it is only this kind of transitive dependence which
is ruled out by the third normal form. A table in third normal form can contain
some transitive dependence if it has more then one candidate key. Suppose K1
and K2 are candidate keys for a table R and suppose Aj is an attribute which does
not belong to K1 ∪K2. Then K1 functionally determines K2 (since K1 is a key),
K2 functionally determines Aj (since K2 is a key) and, of course, K1 functionally
determines Aj. Writing X 99K Y to stand for ‘X functionally determines Y ’ we
have:
K1 99K K2 99K Aj
and
K1 99K Aj.
Example 5.19
Consider the table above with attribute type (ORDER #, CUSTOMER
NAME,
CUSTOMER
ADDRESS,
DATE) and primary key {ORDER #} introduced in
example 5.18. (Recall that {ORDER #} is the unique candidate key.) This is
not in third normal form. The attribute CUSTOMER
ADDRESS is functionally
dependent on {CUSTOMER
NAME} which does not contain the key as a subset.
This table could be ‘normalized’ into third normal form by splitting it into two
tables, one of attribute type (ORDER #, CUSTOMER
NAME, DATE) and the other
of attribute type (CUSTOMER
NAME, CUSTOMER
ADDRESS) with the keys
indicated in bold type as usual.
The remaining two tables in example 5.18—those of attribute types (ORDER #,
PART #, QUANTITY) and (PART #, PART
PRICE) respectively—are in third
normal form. Thus to normalize into third normal form the original table of
attribute type (ORDER #, PART #, CUSTOMER
NAME, CUSTOMER
ADDRESS,
PART
PRICE, QUANTITY, DATE) deﬁned in example 5.17, we could split it into
four tables with the following attribute types:
(ORDER #, CUSTOMER
NAME, DATE)
(CUSTOMER
NAME, CUSTOMER
ADDRESS)
(ORDER #, PART #, QUANTITY)
(PART #, PART
PRICE).

Databases: Functional Dependence and Normal Forms
289
Each of these tables is in third normal form. They suffer from none of the updating
problems mentioned in example 5.17. Using the operations, such as selection,
projection, and natural join, introduced in §4.7, we can still create tables with
attribute sets such as (ORDER #, CUSTOMER
NAME, CUSTOMER
ADDRESS,
DATE) or (ORDER #, CUSTOMER
NAME, PART #, PART
PRICE, QUANTITY).
We should mention in conclusion that this is not the end of the story as far as
normal forms are concerned. It is generally regarded as a ‘rule of thumb’, but not
a rigidly applied principle, that a database designer should aim for a collection
of tables which are in third normal form. There are, however, ‘higher’ normal
forms—Boyce–Codd normal form and the fourth and ﬁfth normal forms. Boyce–
Codd normal form is similar to but slightly more restrictive than the third normal
form. The fourth and ﬁfth normal forms are designed to eliminate certain kinds
of so-called ‘multidependences’ amongst the attributes. A consideration of these
normal forms (and when and why a database designer should aim for ‘higher
normalization’) is beyond the scope of our brief excursion into relational database
theory. The interested reader should consult a more specialized book. (A selection
of possible titles for further reading is given in the list of references.)
Exercises 5.6
1.
This question refers to the table deﬁned in example 5.15.
(i)
Determine the functional dependences between single attributes.
(ii)
Find all the candidate keys for the table.
(iii)
Is the table in (a) second, or (b) third normal form?
2.
This question refers to the table R deﬁned in example 5.16.
(i)
Show that K = {A1, A2, A4} is a candidate key for R.
(ii)
Is R in (a) second, or (b) third normal form?
3.
This question refers to the tables PERSONAL, DISCIPLINE and
CURRENT COURSE introduced in exercises 4.7 (but not the parts of
the tables displayed there). In each case {A1} = {ID
NUMBER} is the
only candidate key to the table.
(i)
Assuming that no two students have exactly the same name, are
these tables in third normal form?

290
Functions
(ii)
If two students do have exactly the same name, are these tables in
third normal form?
4.
Determine whether each of the following tables is in third normal form.
You may assume that there is a unique candidate key in each case. If
the table is not in third normal form, split the data into two or more
tables which are in third normal form. (Where appropriate, state any
assumptions you need to make about various functional dependences
between attributes or sets of attributes.)
(i)
A table stores employee-related information and has attribute
type {EMPLOYEE #,
EMPLOYEE
NAME,
DEPARTMENT #,
JOB
DESCRIPTION,
WORK
LOCATION}.
You may assume
that each employee works for only one department and each
department is located in only one place.
(ii)
A travel agency records information about its customers’ ﬂight
bookings in a table of attribute type {PASSENGER
NAME,
FLIGHT #,
AIRLINE,
DATE,
EMBARKATION,
DESTINATION,
CLASS}.
(iii)
A hospital uses three tables to maintain its patient-related
information. The name and attribute type of each table is given
below.
PATIENT HISTORY:
{PATIENT #, PATIENT
NAME, ADMISSION
DATE, DISCHARGE
DATE, CONDITION}.
PATIENT CURRENT:
{PATIENT #, PATIENT
NAME, ADMISSION
DATE, CONDITION,
CONSULTANT #, CONSULTANT
NAME, CONSULTANT
PHONE,
WARD #}.
TREATMENT CURRENT:
{PATIENT #, DRUG, QUANTITY, DAILY
COST}.
5.
Recast the deﬁnition of functional dependence given on page 281 in terms
of partial functions. (See exercise 5.1.12 for the deﬁnition of a partial
function.)

Chapter 6
Matrix Algebra
6.1
Introduction
It is often convenient to present certain kinds of data in tabular form. For example,
the following table shows, in a concise way, the percentage marks obtained by ﬁve
students (denoted by A–E) in three examinations.
Examination
Discrete
Data
Operating
Student
maths
structures
systems
A
72
68
60
B
35
48
42
C
61
60
76
D
84
82
90
E
53
62
51
Here, the data values (the examination marks) are positioned in the table
according to two variable factors: student and examination subject.
A two-dimensional rectangular array of numbers, such as that in the table above, is
called a matrix (plural—matrices) and the numbers which constitute the matrix
are called its elements (or entries). A matrix may have elements which are other
than numbers, for example sets or variables. We deﬁne the dimension (or order)
of a matrix to be the number of rows and columns (in that order) which it contains.
291

292
Matrix Algebra
Thus a matrix having m rows and n columns is said to have dimension m × n
(read as ‘m by n’). Since it has ﬁve rows and three columns, the matrix above has
dimension 5 × 3.
Whilst a matrix may be presented as a table with row and column headings, it
may also be shown as a rectangular array of elements enclosed in brackets. In this
form, the matrix above would be presented as:






72
68
60
35
48
42
61
60
76
84
82
90
53
62
51






.
Example 6.1
State the dimension of each of the following matrices.
(i)
µ
2
4
−1
5
¶
(ii)
µ
−2
14
1
0
3
−2
¶
(iii)
¡
3
1
4
2
−9
¢
(iv)


1
5
7

.
Solution
(i)
This matrix has two rows and two columns. Its dimension is therefore
2 × 2.
(ii)
The dimension of this matrix 2 × 3.
(iii)
This is a 1 × 5 matrix.
(iv)
This matrix has dimension 3 × 1.
It is conventional to refer to a matrix using an upper-case letter. For instance:
A =


2
1
−4
3
7
0


B =
µ
1
0
3
−4
¶
.

Introduction
293
The elements within the matrix are usually denoted by the corresponding lower-
case letter to which two subscripts are attached. The ﬁrst identiﬁes the row within
which the element lies and the second its column. For example, a31 denotes the
element in matrix A lying in the third row and ﬁrst column. For the matrix A
above, a31 = 7. Similarly a22 = 3, a12 = 1, etc. For the matrix B, b12 = 0,
b22 = −4, and so on.
In general, aij denotes the element in matrix A occupying the ith row and jth
column. This element is often referred to as the (i, j)-entry or the (i, j)-element
of A. An alternative notation is to denote the matrix A by [aij] where A = [aij]
refers to the matrix A whose (i, j)-entry is aij. Note the important distinction
between [aij] and aij. The former refers to the whole matrix whereas the latter,
for different values of i and j, denotes the individual elements within the matrix.
Equal Matrices
Matrices are equal if they are identical in every respect—that is, they have the
same dimension and the same elements in the same positions. Put more formally:
let A = [aij] and B = [bij]; then A = B if and only if A and B have the same
dimension and aij = bij for all values of i and j.
Example 6.2
Find all the values of a, b, c and d if
µ
3a
2b
c
−d
¶
=
µ
10
6
−4
1
¶
.
Solution
Since the condition that the matrices have the same dimension is satisﬁed (both
are 2 × 2 matrices), all we must ensure is that corresponding elements are equal.
Therefore
3a = 10
2b = 6
c = −4
−d = 1.
Solving these simple equations gives
a = 10
3
b = 3
c = −4
d = −1.

294
Matrix Algebra
6.2
Some Special Matrices
It is convenient to distinguish several different ‘families’ of matrices. A list of
these together with their deﬁning characteristics is given below.
A square matrix is one having the same number of rows as it has columns. The
following are examples of square matrices:
µ
2
3
−1
4
¶


3
2
0
4
5
−1
2
−3
7

.
A column matrix (or column vector) is a matrix having only one column.
Examples are:
µ
2
1
¶




−1
4
2
10



.
A row matrix (or row vector) is a matrix having only one row, for example:
¡
7
1
−2
¢
¡
3
2
1
0
−1
¢
.
Row and column vectors are often (but not always) denoted by lower-case bold
letters. In handwriting, lower case letters with a line or tilde underneath (the
printer’s notation for bold print) are used. Thus we may write:
u =
¡
2
4
3
−6
¢
or
u
˜
=
¡
2
4
3
−6
¢
.
A row vector is sometimes written with its elements separated by commas, for
example u = (2, 4, 6, −2, 2).
A zero matrix (or null matrix) is one where every element is zero. So for a zero
matrix A, aij = 0 for all values of i and j. Since different zero matrices exist
for every possible dimension it is usual to denote a zero matrix by Om×n where
m × n is the dimension of the matrix. For example,
O2×3 =
µ 0
0
0
0
0
0
¶
and
O2×2 =
µ 0
0
0
0
¶
.
A diagonal matrix is a square matrix where all of the elements are zero except
possibly those occupying the positions diagonally from the top-left corner to the
bottom right corner.
In any square matrix, these elements constitute what is

Some Special Matrices
295
termed the leading diagonal or principal diagonal. Thus A = [aij] is a diagonal
matrix if aij = 0 for i ̸= j. Examples of diagonal matrices are:
µ
3
0
0
2
¶


−2
0
0
0
1
0
0
0
4




5
0
0
0
−4
0
0
0
0

.
An identity matrix (or unit matrix) is a diagonal matrix whose leading diagonal
elements are all 1. Note that this means that an identity matrix is necessarily
square. Thus if A = [aij] is an identity matrix, then aij = 0 for i ̸= j and aii = 1.
An identity matrix is often denoted by I when its dimension is clear from the
context or irrelevant to the discussion. When it is necessary to distinguish identity
matrices of different dimensions, we denote the identity matrix of dimension n×n
by In. For example,
I2 =
µ 1
0
0
1
¶
and
I4 =




1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1



.
A matrix A = [aij] is symmetric if it is square and aij = aji for all values of i
and j. (This means that the ‘symmetry’ of a symmetric matrix is about the leading
diagonal.) The following are symmetric matrices:
µ
3
−1
−1
2
¶




5
−1
3
7
−1
−9
2
5
3
2
6
0
7
5
0
2



.
Exercises 6.1
1.
Give the truth values of the following propositions:
(i)
{A : A is an identity matrix} ⊂{B : B is a symmetric matrix}.
(ii)
{B : B is a symmetric matrix} ⊂{C : C is a diagonal matrix}.
(iii)
µ
1
1
1
1
¶
∈{A : A is an identity matrix}.
(iv)
{D : D is a square matrix} ⊂{C : C is a diagonal matrix}.
(v)
{A : A is an identity matrix} ⊂{C : C is a diagonal matrix}.
2.
Write the 3 × 2 matrices A, B and C deﬁned as follows:
A = [aij]
B = [bij]
C = [cij]
where aij = i −j, bij = i −2j, cij = 4i + 3j.

296
Matrix Algebra
3.
Write down the matrix A which has dimension 4 × 4, is symmetric and
has the following properties: aii = i2, a13 = a24 = 0, a14 = 3,
a12 = a23 = a11 + a22, a34 = a23 −a14.
4.
Give an example of a matrix which is both a row matrix and a column
matrix.
5.
If
A = [aij] =
µ
x + y
10
2x −y
4
¶
ﬁnd x and y if a11 = a22 and a12 = 1
2a21.
6.
The transpose of a matrix A (denoted by AT) is the matrix obtained by
interchanging the rows and columns of A. For example, if
A =
µ
2
3
1
−2
4
3
¶
then
AT =


2
−2
3
4
1
3

.
In general, if A = [aij], then AT = [aji], i.e. the (i, j)-entry of AT is aji,
the (j, i)-entry of A.
(i)
Write down the transpose of each of the following matrices:
(a)


−1
2
3


(b)
µ
4
0
−1
3
1
3
¶
(c)
µ
−1
6
3
4
0
7
−1
3
¶
.
(ii)
Prove that A is a symmetric matrix if and only if A = AT. (This is
often given as the deﬁnition of a symmetric matrix.)
6.3
Operations on Matrices
Multiplication of a Matrix by a Scalar
A scalar is simply a real number; for instance, 6, −2, 3 and 0.672 are all scalars.
To multiply a matrix by a scalar, we multiply every element of the matrix by that
number. For instance, if A is the matrix given by
A =
µ 2
3
−2
1
−4
6
¶

Operations on Matrices
297
then multiplication of A by the scalar 3 gives
3 × A = 3A =
µ
6
9
−6
3
−12
18
¶
.
We say that multiplication by a scalar is ‘commutative’ (see deﬁnition 8.3 on
page 364); i.e. k × A = A × k = kA for any matrix A and scalar k.
The formal deﬁnition of multiplication of a matrix by a scalar is given below.
Deﬁnition 6.1
If A = [aij] is any matrix and k is a scalar, then the product kA is the
matrix given by kA = [kaij], i.e. the (i, j)-entry of kA is k times the
(i, j)-entry of A.
Note that multiplication of a matrix by a scalar −1 results in another matrix whose
elements are the same as those for A but with the opposite sign. For instance, for
the matrix A given above
−1A =
µ
−2
−3
2
−1
4
−6
¶
.
In general if A = [aij], then −1A = [−aij]. We normally denote the matrix −1A
by −A.
Addition of Matrices
Unlike real numbers, it is not always possible to add two matrices. Only matrices
having the same dimension can be added and, where this condition is satisﬁed,
we say that the matrices are conformable for addition. Given two matrices A
and B which have the same dimension, the result of adding A and B is simply the
matrix whose elements are the sums of the elements in corresponding positions
of A and B. For instance, if
A =


3
−1
2
7
5
4


and
B =


4
2
−1
1
−6
0



298
Matrix Algebra
then
A + B =


3
−1
2
7
5
4

+


4
2
−1
1
−6
0


=


3 + 4
−1 + 2
2 + (−1)
7 + 1
5 + (−6)
4 + 0


=


7
1
1
8
−1
4

.
We can deﬁne matrix addition in general as follows.
Deﬁnition 6.2
Let A = [aij] and B = [bij] be matrices of the same dimension. Then
A + B = C where C = [cij] and cij = aij + bij for all values of i and j.
Note that, for any m × n matrix A, we have A + Om×n = A and also
A + (−A) = Om×n. Because of the latter property, the matrix −A is sometimes
referred to as the additive inverse of A. (See chapter 8 for a detailed explanation
of the concept of inverses.)
We can use our deﬁnition of scalar multiplication together with the deﬁnition of
matrix addition to deﬁne matrix subtraction. We have already noted that −A
means −1A, i.e. the result of multiplying the matrix A by −1. So B −A is just
the sum of B and −A, i.e. the sum of B an the additive inverse of A. This can be
found according to the rules of matrix addition provided that the condition that A
and B have the same dimension is satisﬁed.
Examples 6.3
1.
If
A =
µ
−1
0
2
4
5
−3
¶
and
B =
µ
2
4
7
1
−6
−1
¶
ﬁnd A −B.

Operations on Matrices
299
Solution
A −B = A + (−B)
=
µ
−1
0
2
4
5
−3
¶
+
µ
−2
−4
−7
−1
6
1
¶
=
µ
−3
−4
−5
3
11
−2
¶
.
2.
If
A =


2
7
1
4
3
−2


and
B =


−1
0
0
4
−3
1


ﬁnd 3A −2B.
Solution
3A −2B = 3A + 2(−B)
= 3


2
7
1
4
3
−2

+ 2


1
0
0
−4
3
−1


=


6
21
3
12
9
−6

+


2
0
0
−8
6
−2


=


8
21
3
4
15
−8

.
(We could also have written 3A −2B = 3A + (−2)B and obtained the same
result.)
It is a simple matter to show that matrix addition and scalar multiplication have
the following properties (provided, of course, that the appropriate matrices have
the same dimension and are therefore conformable for addition).
(a)
A + B = B + A (commutative law).
(b)
A + (B + C) = (A + B) + C (associative law).
(c)
k(A + B) = kA + kB (i.e. multiplication by a scalar is distributive over
matrix addition).
(d)
k(lA) = (kl)A, where k and l are scalars.

300
Matrix Algebra
Multiplication of Matrices
Given the way in which we have deﬁned matrix addition, you might expect
that matrix multiplication is carried out in an analogous way—by multiplying
the elements in corresponding positions in matrices with the same dimension.
However, this is not the case. If it were, then matrix algebra would be worthy
of little attention since the rules governing it would be very much like the
familiar algebra of real numbers.
Matrix multiplication lacks the intuitive
appeal of matrix addition and, for the time being, we will concentrate on how
matrices are multiplied rather than attempting a justiﬁcation as to why they are
multiplied in this way. However, be assured that matrix multiplication does have
signiﬁcant applications in mathematics and elsewhere. We shall consider one
such application in the next chapter when we deal with solving systems of linear
equations. Matrix multiplication is also used in chapter 10.
We ﬁrst consider the simplest case of multiplication of two matrices—that of
multiplying a row matrix by a column matrix. For the moment, we assume that
the following two conditions must be satisﬁed:
(a)
each matrix has the same number of elements;
(b)
the row matrix is placed to the left of the column matrix in forming the
product.
Thus, if
u = (u1 u2 . . . un)
and
v =





v1
v2
...
vn





then the product uv is deﬁned as follows:
uv = (u1 u2 . . . un)





v1
v2
...
vn





= (u1v1 + u2v2 + · · · + unvn)
i.e. uv is the 1 × 1 matrix whose single element is calculated by multiplying
together the ﬁrst elements in u and v, the second elements in u and v, etc and
summing the results.

Operations on Matrices
301
Example 6.4
Where possible calculate the matrix product uv in each of the following cases:
(i)
u =
¡ 4
2
3 ¢
v =


1
0
5


(ii)
u =
¡
−1
2
4
3
¢
v =


−2
1
7


(iii)
u =
¡ −1
2
4
−2 ¢
v =




3
−1
2
2



.
Solution
(i)
Here u is a row matrix, v is a column matrix and both have the same
number of elements. Therefore the product uv exists and
uv =
¡
4
2
3
¢


1
0
5


= (4 × 1 + 2 × 0 + 3 × 5)
= (19).
(ii)
Since u and v do not have the same number of elements, we cannot form
the product uv.
(iii)
uv =
¡
−1
2
4
−2
¢




3
−1
2
2




= ((−1) × 3 + 2 × (−1) + 4 × 2 + (−2) × 2)
= (−1).
Matrix multiplication in general consists of repeated applications of the operation
we have just described for the multiplication of a row matrix by a column matrix.
When multiplying two matrices A and B to form the product AB, we repeatedly
‘multiply’ a row of A by a column of B as we did in the examples above. We
perform this operation on every combination of a row from A together with a

302
Matrix Algebra
column from B. Beginning with the ﬁrst row of A we multiply it by each column
of B in turn beginning with the ﬁrst. The scalars which result are the elements
comprising the ﬁrst row of the product matrix. We then repeat the process with
the second row of A together with each column of B giving the second row of
AB. The process continues until the ﬁnal row of A has been multiplied by each
column of B.
The process is illustrated below.





a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn










b11
b12
. . .
b1r
b21
b22
. . .
b2r
...
...
...
...
bn1
bn2
. . .
bnr





1st row of A
1st column of B
=





c11





(1, 1)-entry of AB





a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn










b11
b12
. . .
b1r
b21
b22
. . .
b2r
...
...
...
...
bn1
bn2
. . .
bnr





1st row of A
2nd column of B
=





c11
c12





(1, 2)-entry of AB.
In general:








a11
a12
. . .
a1n
...
...
...
ai1
ai2
. . .
ain
...
...
...
am1
am2
. . .
amn















b11
. . .
bj1
. . .
b1r
b21
. . .
bj2
. . .
b2r
...
· · ·
...
bn1
. . .
bjn
. . .
bnr







ith row of A
jth column of B

Operations on Matrices
303
=








...
. . .
cij
. . .
...








(i, j)-entry of AB.
Note that the sequence of operations which we have described requires that A
must have the same number of elements in a row as B has in a column or,
equivalently, A must have the same number of columns as B has rows. Also
the (i, j)-entry in AB is the result of multiplying the ith row of A by the jth
column of B.
Example 6.5
Given the matrices
A =
µ
3
1
−2
2
4
3
¶
and
B =


2
1
1
−3
4
−1
1
3
0


calculate C = AB.
Solution
Since A has the same number of elements in a row as B has in a column, the
product AB exists. We perform our ‘row times column’ operation taking each
row of A with each column of B.
We take the ﬁrst row of A and the ﬁrst column of B:
µ
3
1
−2
2
4
3
¶ 

2
1
1
−3
4
−1
1
3
0

.
We perform our multiplication operation and get:
3 × 2 + 1 × (−3) + (−2) × 1 = 1.
This scalar is c11, the (1, 1) entry of C:


1

.

304
Matrix Algebra
Repeating this operation with the ﬁrst row of A and second column of B we obtain
c12, where
c12 = 3 × 1 + 1 × 4 + (−2) × 3 = 1.
Continuing in this way we calculate systematically all the elements in C. Thus
C =
µ
3
1
−2
2
4
3
¶ 

2
1
1
−3
4
−1
1
3
0


=
µ 3.2 + 1.(−3) + (−2).1
3.1 + 1.4 + (−2).3
3.1 + 1.(−1) + (−2).0
2.2 + 4.(−3) + 3.1
2.1 + 4.4 + 3.3
2.1 + 4.(−1) + 3.0
¶
=
µ
1
1
2
−5
27
−2
¶
.
We have already said that, for the matrix product AB to exist, the number of
columns of A must be the same as the number of rows of B. Where this condition
is satisﬁed, what will be the dimension of the product AB? Each row of A, when
multiplied by all the columns of B, produces a row of AB. So AB must have the
same number of rows as A. Further, given a row of A, the ‘row times column’
operation is performed on each column of B. So the number of elements within a
row of AB must be the same as the number of columns of B. In other words, the
number of columns of AB is the same as the number of columns of B.
We can state this more formally as follows:
If A has dimension m × n and B has dimension p × q, then the matrix product
AB exists if and only if n = p. The dimension of AB is then m × q.
Example 6.6
For each of the following pairs of matrices A and B, state whether the matrix
product AB exists. If it does exist, state the dimension of AB.
(i)
A =


2
3
−1
2
5
4


B =
µ
4
1
2
3
2
1
¶
(ii)
A =
µ
−1
0
0
0
¶
B =


4
0
1
2
3
−1


(iii)
A =
¡
1
4
2
−5
¢
B =




2
0
4
1
3
−3
−1
−1



.

Operations on Matrices
305
Solution
(i)
The matrix A has dimension 3 × 2 and B has dimension 2 × 3. Since
A has the same number of columns as B has rows, AB exists and has
dimension 3 × 3. Evaluation of the product gives
AB =


17
8
7
2
3
0
32
13
14

.
(ii)
The matrices A and B have dimensions 2 × 2 and 3 × 2 respectively.
Matrix A has two columns but B has three rows and hence the product
AB does not exist (although BA does).
(iii)
The dimension of A is 1 × 4 and that of B is 4 × 2. Thus AB exists and
has dimension 1 × 2. Evaluation of the product gives
AB =
¡
29
3
¢
.
We now give a formal deﬁnition of matrix multiplication. The notation is a little
cumbersome but it should be clear that it summarizes exactly those steps detailed
in the examples above.
Deﬁnition 6.3
If A = [aij] is an m × n matrix and B = [bij] is an n × r matrix, then
AB = C where C = [cij] has dimension m × r and
cij = ai1b1j + ai2b2j + · · · + ainbnj
=
n
X
k=1
aikbkj.
It is important to note that, in general, AB ̸= BA.
We say that matrix
multiplication is ‘not commutative’ (see deﬁnition 8.3). In fact the dimensions
of A and B could well be such that, although one of these two products exists, the
other does not (see example 6.6(ii)). Even if A and B are such that AB and BA
both exist, in general these two products may not be equal. This means ‘multiply

306
Matrix Algebra
A by B’ is ambiguous when A and B are matrices since it is not clear which of
the two products AB or BA is intended. Where the product AB is required, we
say that B is to be pre-multiplied by A (or that A is to be post-multiplied by B).
If the product BA is required, we say that A is to be pre-multiplied by B (or that
B is to be post-multiplied by A).
Matrix multiplication has the following properties provided, of course, that the
appropriate matrix sums and products exist. It is easy to demonstrate that these
properties hold for speciﬁc matrices but their proof for general matrices is a
tedious and lengthy exercise.
(a)
(AB)C = A(BC)
(associative law);
(b)
A(B + C) = AB + AC
(distributive law);
(B + C)D = BD + CD
(c)
k(AB) = (kA)B = A(kB)
where k is a scalar.
Note that property (a) means that we can write this matrix product as ABC
without fear of ambiguity.
Exercises 6.2
1.
If
A =
µ
2
−3
0
−1
¶
and
B =
µ
4
−1
2
−1
¶
ﬁnd:
(i)
A + 2B
(ii)
3A −6B
(iii)
AB
(iv)
A2 (i.e. AA)
(v)
BA
(vi)
A(BA)
(vii)
(AB)A
(viii)
A(A −B)
(ix)
ATB
(x)
(AB)T
(xi)
BTAT.
(See exercise 6.1.6 for the deﬁnition of AT, the transpose of A.)
2.
If
A =
¡ 1
3
2 ¢
B =


2
4
3
3
−1
0


C =
µ
2
3
−1
4
1
−1
¶
ﬁnd the following, if they exist. If any matrix product does not exist,
explain why it does not.

Operations on Matrices
307
(i) AB
(ii) BA
(iii) AC
(iv) CA
(v) BC
(vi) CB
(vii)
ABC.
3.
If
A =
µ
0
1
2
0
¶
ﬁnd a matrix B such that AB = BA.
4.
Let A be any 2 × 2 matrix. Show that AI2 = I2A = A, where I2 is the
2 × 2 identity matrix.
5.
Let A be any n × n matrix. Show that AIn = InA = A. (Use the
deﬁnition of matrix multiplication.)
6.
Let A be any m × n matrix. Show that AIn = ImA = A.
7.
If
A =


1
−3
2
2
1
−3
4
−3
−1


B =


1
4
1
2
1
1
1
−2
1


C =


2
1
−1
3
−2
−1
2
−5
−1


show that AB = AC. (But note that B ̸= C, i.e. the ‘cancellation law’
does not hold for matrix multiplication in general.)
8.
Let A, B and C be matrices such that A has dimension m × n, B
has dimension p × q and C has dimension r × s.
For each of the
following, write down the condition(s) for the product to exist and state
the dimension of the product:
(i) ABC
(ii) CBA
(iii) (A + B)C.
9.
Find a counter-example to show that, if A and B are matrices and
AB = O (where O is the zero matrix of appropriate dimension), then
we cannot conclude that either A or B is a zero matrix.
10.
If A = [aij] is a diagonal matrix of dimension n × n (so that aij = 0 if
i ̸= j) and B = [bij] is another diagonal matrix of dimension n × n, ﬁnd
the products AB and BA. Use your result to write down the products
AB and BA where
A =


2
0
0
0
3
0
0
0
−3


B =


−4
0
0
0
−1
0
0
0
5

.
If A = [aij] is a diagonal matrix, write down the matrix A2. What would
you expect the result to be for An? Prove this result by mathematical
induction.

308
Matrix Algebra
11.
In exercise 6.1.6 we deﬁned AT, the transpose of matrix A. Prove each
of the following properties of the transpose:
(i)
(AT)T = A
(ii)
(A + B)T = AT + BT
(iii)
(AB)T = BTAT.
12.
Show that, if A is a square matrix, then A + AT is a symmetric matrix.
(Recall that a matrix B is symmetric if and only if B = BT—see
exercise 6.1.6.)
6.4
Elementary Matrices
An elementary matrix is one which can be obtained from an identity matrix by
performing only one of the following operations on that identity matrix:
(R1)
interchanging two rows;
(R2)
multiplying the elements of one row by a non-zero real number;
(R3)
adding to the elements of one row, any multiple of the corresponding
elements of another row;
(C1)
interchanging two columns;
(C2)
multiplying the elements of one column by a non-zero real number;
(C3)
adding to the elements of one column any multiple of the corresponding
elements of another column.
Notice that (R1), (R2) and (R3) describe operations which are applied to rows
of the identity matrix whilst (C1), (C2) and (C3) describe operations applied to
its columns. The operations (R1), (R2) and (R3), when applied to any matrix
(not necessarily an identity matrix), are called elementary row operations (or
elementary row transformations). Operations (C1), (C2) and (C3) are called
elementary column operations (or elementary column transformations).
An elementary matrix is always square since it is obtained from another square
matrix (an identity matrix) by one of the operations described above, none of
which alter the dimension of the matrix.

Elementary Matrices
309
Example 6.7
State whether or not each of the following is an elementary matrix:
(i)
µ
7
3
2
1
4
1
¶
(ii)


0
0
1
0
1
0
1
0
0


(iii)
µ
1
5
0
1
¶
(iv)
µ
4
0
0
−3
¶
(v)


5
0
0
0
1
0
0
1
1

.
Solution
(i)
This matrix is not square and therefore cannot be an elementary matrix.
(ii)
Starting from the identity matrix
I3 =


1
0
0
0
1
0
0
0
1


the matrix


0
0
1
0
1
0
1
0
0


can be obtained by interchanging the ﬁrst and third rows (or by
interchanging the ﬁrst and third columns). Hence it is an elementary
matrix.

310
Matrix Algebra
(iii)
The matrix
µ
1
5
0
1
¶
can be obtained from
I2 =
µ
1
0
0
1
¶
by adding ﬁve times the second row to the ﬁrst row (or by adding ﬁve
times the ﬁrst column to the second). In either case this involves one
elementary row or column operation and hence
µ
1
5
0
1
¶
is an elementary matrix.
(iv)
In order to obtain
µ
4
0
0
−3
¶
from I2 we would need to perform two elementary row or column
operations: either multiply the ﬁrst row by 4 and the second by −3
or multiply the ﬁrst column by 4 and the second by −3.
No single
elementary row or column operation produces the required result and
hence
µ
4
0
0
−3
¶
is not an elementary matrix.
(v)
The matrix


5
0
0
0
1
0
0
1
1


cannot be obtained from I3 by means of any single elementary row or
column operation and hence it is not an elementary matrix.
In the examples above, each of the elementary matrices could be formed in two
ways—either by a single elementary row operation or by a single elementary
column operation. It is not difﬁcult to see that this is the case for all elementary
matrices.
Note that any identity matrix In is itself an elementary matrix since it can be
regarded as being derived from In by multiplying any row or column by 1.

Elementary Matrices
311
What is interesting about elementary matrices is their effect upon another matrix
of appropriate dimension when the two are multiplied together. Consider the
elementary matrix
E =


3
0
0
0
1
0
0
0
1


obtained from I3 by multiplying the ﬁrst row by 3. Observe what happens when
we multiply this matrix by another matrix, say,
A =


2
−3
1
2
4
6


so that the elementary matrix is on the left (i.e. we pre-multiply A by the
elementary matrix):
EA =


3
0
0
0
1
0
0
0
1




2
−3
1
2
4
6

=


6
−9
1
2
4
6

.
The product matrix is simply the matrix obtained from A by multiplying the ﬁrst
row by 3. In this case pre-multiplication of A by the elementary matrix E effects
the same elementary row operation on A as was necessary on I3 to produce the
elementary matrix itself.
The following matrices can be obtained from I3 by multiplying a row by a non-
zero constant k:


k
0
0
0
1
0
0
0
1




1
0
0
0
k
0
0
0
1




1
0
0
0
1
0
0
0
k

.
It is easy to show that, if one of these elementary matrices is multiplied by any
3 × n matrix A, with the elementary matrix on the left of the product, then the
result is the matrix A with the corresponding row multiplied by k.
Does this result generalize to elementary matrices obtained by other elementary
row operations? Consider the following elementary matrix E, obtained from I4
by interchanging rows 1 and 2:




0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
1



.

312
Matrix Algebra
If
A =




7
4
2
−1
3
1
2
0
−3
6
−3
2
1
−2
4
3




then
EA =




0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
1








7
4
2
−1
3
1
2
0
−3
6
−3
2
1
−2
4
3




=




3
1
2
0
7
4
2
−1
−3
6
−3
2
1
−2
4
3



.
The effect of pre-multiplication by the elementary matrix is to interchange rows
1 and 2 of A, the same elementary row operation by which the elementary matrix
was obtained from the appropriate identity matrix.
In a similar way we can check the effect of an elementary matrix formed by
the addition of k times one row to another when multiplied by a matrix A of
appropriate dimension (with the elementary matrix on the left). The product is
again a matrix which is the result of applying the same elementary row operation
to A.
We now generalize these results in a useful theorem, which we state without proof.
Theorem 6.1
(i)
Consider an elementary matrix E formed from In by an
elementary row operation. If A is any matrix of dimension n × m,
the matrix product EA is the matrix resulting from performing the
same elementary row operation on A.
(ii)
Consider an elementary matrix F formed from In by an elementary
column operation. If A is any matrix of dimension m × n, the
matrix product AF is the matrix resulting from performing the
same elementary column operation on A.

Elementary Matrices
313
The theorem conﬁrms that elementary row operations on a matrix can be effected
by pre-multiplication by the appropriate elementary matrix. It also states that we
can effect elementary column operations by post-multiplication by the elementary
matrix formed from the appropriate identity matrix by the same column operation.
As you read this chapter and the next, you will no doubt notice that we tend to
neglect elementary column operations in favour of elementary row operations.
There is no particular reason for this other than convention. The uses to which we
put row operations could equally well be served by column operations.
Examples 6.8
1.
Given
A =


3
4
2
−1
5
4
6
1
8


ﬁnd a matrix E so that the product EA is given by
EA =


3
4
2
11
7
20
6
1
8

.
Solution
We note that the product EA is the result of the following elementary row
operation on A: add twice the third row to the second.
Therefore E is the
elementary matrix obtained from I3 by this same elementary row operation.
Therefore
E =


1
0
0
0
1
2
0
0
1

.
We can check that E is the required matrix:
EA =


1
0
0
0
1
2
0
0
1




3
4
2
−1
5
4
6
1
8


=


3
4
2
11
7
20
6
1
8

.

314
Matrix Algebra
2.
Given
A =


2
1
3
−2
0
−4


ﬁnd F so that
AF =


1
2
−2
3
−4
0

.
Solution
The product AF is the result of the elementary column operation ‘interchange
columns 1 and 2’ on the matrix A. Therefore F is the elementary matrix obtained
from I2 by interchanging the ﬁrst and second columns. Thus
F =
µ
0
1
1
0
¶
.
Checking that AF gives the required matrix:
AF =


2
1
3
−2
0
−4


µ
0
1
1
0
¶
=


1
2
−2
3
−4
0

.
3.
Given
A =


3
0
−4
−4
2
1
−2
−1
5


and
B =


−4
2
1
−6
0
8
−2
−1
5


ﬁnd a matrix P such that PA = B.
Solution
The matrix B is not the result of any one elementary row operation on the matrix
A so P is not an elementary matrix. However, B can be considered as the result
of applying the following two elementary row operations to A: (i) interchange
rows 1 and 2, then (ii) multiply (the new) row 2 by −2.
Pre-multiplication of A by the elementary matrix
E1 =


0
1
0
1
0
0
0
0
1



Elementary Matrices
315
will effect the interchange of the ﬁrst and second rows. Pre-multiplication of the
result, namely E1A, by the elementary matrix
E2 =


1
0
0
0
−2
0
0
0
1


will have the effect of multiplying the second row of E1A by −2. Thus
E2(E1A) =


−4
2
1
−6
0
8
−2
−1
5

.
Using the associative property of matrix multiplication, we have
E2(E1A) = (E2E1)A
so that
P = E2E1
=


1
0
0
0
−2
0
0
0
1




0
1
0
1
0
0
0
0
1


=


0
1
0
−2
0
0
0
0
1

.
Note that the result PA could also have been obtained by the following two
elementary row operations on A: (i) multiply row 1 by −2, then (ii) interchange
rows 1 and 2.
In this case the corresponding elementary matrices are
E3 =


−2
0
0
0
1
0
0
0
1


and
E4 =


0
1
0
1
0
0
0
0
1



316
Matrix Algebra
and therefore
P = E4E3 =


0
1
0
1
0
0
0
0
1




−2
0
0
0
1
0
0
0
1


=


0
1
0
−2
0
0
0
0
1


as before.
In these examples we have seen how one matrix may be obtained from another by
a sequence of one or more elementary row operations and how this is equivalent
to pre-multiplication of the matrix by the appropriate elementary matrices. In
general, if a matrix B can be obtained from matrix A by a ﬁnite sequence of
elementary row operations, we say that B is row-equivalent to A and we write
A ∼B.
Exercises 6.3
1.
State whether or not each of the following is an elementary matrix.
For each elementary matrix state the alternative elementary row and
elementary column operations by which it is formed from the appropriate
identity matrix.
(i)
µ
1
1
0
1
¶
(ii)




0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0



(iii)




1
0
−3
0
0
1
0
0
0
0
1
0
0
0
0
1




(iv)
µ
1
0
0
4
¶
(v)
µ
−1
0
0
−1
¶
(vi)


1
0
0
0
1
0
0
0
0


(vii)


1
0
0
0
1
0
0
0
1

.
2.
If
A =


2
−4
1
−1
2
3
0
1
−2


and
B =


0
1
−2
−1
2
3
2
−4
1



Elementary Matrices
317
(i)
ﬁnd an elementary matrix E1 such that E1A = B
(ii)
ﬁnd an elementary matrix E2 such that E2B = A
(iii)
evaluate E1E2 and E2E1.
3.
If
A =


1
−1
3
2
−4
4


and
B =


1
−1
5
0
−4
4


(i)
ﬁnd an elementary matrix E1 such that A = E1B
(ii)
ﬁnd an elementary matrix E2 such that E2A = B
(iii)
evaluate E1E2 and E2E1.
4.
If
A =


2
4
−2
1
3
7
−1
1
1


and
B =


2
−8
−2
1
−6
7
−1
−2
1


(i)
ﬁnd a matrix F1 such that AF1 = B
(ii)
ﬁnd a matrix F2 such that BF2 = A
(iii)
evaluate F1F2 and F2F1.
5.
If
A =


2
−1
2
7
0
4
1
6
2


and
B =


7
0
4
2
−1
2
1
6
2


ﬁnd an elementary matrix E1 so that E1A = B.
If
C =


3 1
2
0
2
2
−1
2
1
6
2


ﬁnd an elementary matrix E2 such that E2B = C. Hence ﬁnd a matrix
Q such that QA = C.
6.
If
A =
µ
2
4
1
4
¶
ﬁnd elementary matrices E1, E2 and E3 so that E3E2E1A = I2.
7.
Is multiplication of elementary matrices commutative? Why or why not?
8.
Show that the relation deﬁned by ARB if and only if A is row-equivalent
to B is an equivalence relation on the set of m × n matrices.

318
Matrix Algebra
6.5
The Inverse of a Matrix
Consider the matrices
A =
µ
1
−2
−1
3
¶
and
B =
µ
3
2
1
1
¶
.
Then
AB =
µ
1
−2
−1
3
¶ µ
3
2
1
1
¶
=
µ
1
0
0
1
¶
the 2 × 2 identity matrix. Also
BA =
µ
3
2
1
1
¶ µ
1
−2
−1
3
¶
=
µ
1
0
0
1
¶
.
Here we have a matrix B which, when multiplied by A on the left or on the
right, gives the 2 × 2 identity matrix. The matrix B is said to be the ‘inverse’
(or ‘multiplicative inverse’, to give it its full name) of A and we write B = A−1
where A−1 denotes the inverse of A.
(Remember that in §6.3 we referred to −A as the additive inverse of the matrix
A. The word ‘inverse’ is therefore ambiguous. However, in matrix algebra, the
term is conventionally taken to refer to the multiplicative inverse rather than the
additive inverse. If the latter is intended, then its full title must be used.)
We now give a formal deﬁnition of the inverse of a matrix.
Deﬁnition 6.4
Let A be a square matrix of dimension n × n. A matrix B such that
AB = BA = In is called the inverse of A and we write B = A−1.
Note that the following points are implied in the deﬁnition.
(a)
The inverse is deﬁned only for a square matrix.
(b)
Since A is a square matrix, B must be a square matrix with the same
dimension as A, so that both the products AB and BA exist.
(c)
If B is the inverse of A, then A is the inverse of B. Hence the inverse of
A−1 is A, i.e. (A−1)−1 = A.
It is not clear whether the inverse of a square matrix can always be found. As
we shall see, some square matrices do not have an inverse. Such matrices are

The Inverse of a Matrix
319
called singular matrices. Those matrices which do have an inverse are called
non-singular matrices (or invertible matrices).
Although matrix multiplication is not in general commutative, it can be shown
that for square matrices, if AB = I, then BA = I, so that to establish that B is
the inverse of A only one of the two products needs to be evaluated.
Whilst the deﬁnition tells us what the inverse of a matrix is, it does not give us
any idea as to how to ﬁnd it for a given matrix, nor even how to discover whether
a matrix has an inverse. There are a number of methods for ﬁnding the inverse of
a square matrix where it exists. We shall develop a method based on elementary
matrices. To do this we shall need some simple theorems.
In exercise 6.2.5 we showed that if In is the n × n identity matrix and A is any
n × n matrix then AIn = InA = A. We can use this result to prove the following
important theorem which guarantees that, where the inverse of a square matrix
exists, that inverse is unique. (Actually we have been assuming that this is the
case by referring to ‘the’ inverse of a square matrix.)
Theorem 6.2
A non-singular matrix has only one inverse, i.e. if B and C are both inverses
of a matrix A, then B = C.
Proof
Suppose that B and C are both inverses of A.
Since B is an inverse of A, we have
AB = BA = I.
The matrix C is also an inverse of A so that
AC = CA = I.
We multiply both sides of the equation BA = I on the right by C. This gives
(BA)C = IC
B(AC) = C
(since matrix multiplication is associative)
⇒
BI = C
(since AC = I)
⇒

320
Matrix Algebra
⇒
B = C.
□
The next theorem guarantees the existence of inverses for all elementary matrices.
Theorem 6.3
Every elementary matrix has an inverse (i.e. is non-singular) and the inverse
of an elementary matrix is also an elementary matrix.
Proof
This theorem follows from the fact that, for every elementary row operation, there
is another elementary row operation which ‘undoes’ its effect. Suppose we have a
matrix A to which we apply an elementary row operation which results in matrix
B. Then there is another elementary row operation which, when applied to B,
results in A. These two elementary row operations are said to be inverses of each
other. For instance, if we perform the operation ‘multiply row 2 by k’ (k ̸= 0)
on matrix A to produce matrix B, then the operation ‘multiply row 2 by 1/k’
performed on B results in A. For the elementary row operation ‘add k times row
i to row j’ the inverse elementary row operation is ‘add (−k) times row i to row
j’. The elementary row operation ‘interchange two rows’ is its own inverse.
Since each elementary row operation corresponds to pre-multiplication by an
elementary matrix, it follows from the above that, for every elementary matrix,
there is another elementary matrix which ‘reverses’ the elementary row operation
performed by the ﬁrst.
Let E1 be an elementary matrix obtained from In by applying an elementary row
operation. Applying the inverse of this elementary row operation to In produces
another elementary matrix E2.
Since applying an elementary row operation
followed by its inverse has no net effect E2E1In = In, and therefore E2E1 = In.
Thus E2 is the inverse of E1 (and E1 is the inverse of E2).
□
Example 6.9
Find the inverse of the elementary matrix
E1 =


1
0
0
0
1
0
0
−3
1

.

The Inverse of a Matrix
321
Proof
The elementary matrix E1 performs the elementary row operation ‘add (−3)
times the second row to the third row’. The inverse operation is ‘add 3 times
the second row to the third row’ which corresponds to the elementary matrix
E2 =


1
0
0
0
1
0
0
3
1


and E2 is the inverse of E1.
We can check this result by conﬁrming that
E2E1 = I3 (or E1E2 = I3).
We need to prove one more simple theorem before we can establish a method for
ﬁnding the inverse of a non-singular square matrix.
Theorem 6.4
If A and B are non-singular matrices of dimension n × n, then AB is
non-singular and (AB)−1 = B−1A−1.
Proof
Consider the matrix product (AB)(B−1A−1). Using the associative property of
matrix multiplication (twice), we have:
(AB)(B−1A−1) = A(BB−1)A−1
= AInA−1
= AA−1
= In.
Hence B−1A−1 is the inverse of AB, i.e. (AB)−1 = B−1A−1.
□
This theorem can be extended to cover the inverse of the product of any ﬁnite
number of non-singular matrices to give the result
(A1A2 . . . An)−1 = An
−1An−1
−1 . . . A1
−1
(see exercise 6.4.1).

322
Matrix Algebra
We are now in a position to devise a method for ﬁnding the inverse of a non-
singular matrix.
Suppose that the n × n matrix A is row-equivalent to In, i.e. using a ﬁnite
number of elementary row operations on A, we can obtain In. This means that
pre-multiplication of A by a ﬁnite number of elementary matrices, one for each
elementary row operation, results in In. Thus we have:
(EmEm−1 . . . E2E1)A = In
so that
A−1 = EmEm−1 . . . E1
= (EmEm−1 . . . E1)In.
Thus whatever elementary row operations are necessary to reduce A to the
identity matrix, the same operations performed in the same order on the
appropriate identity matrix will result in the inverse of A.
Notice that A will have an inverse provided that A is row-equivalent to In. If A
cannot be reduced to In by a ﬁnite sequence of elementary row operations then
our method breaks down because we cannot write the matrix equation
(EmEm−1 . . . E2E1)A = In.
Although we shall not prove it, it is the case that where A is not row-equivalent
to In then A has no inverse. We state this formally in the following theorem.
Theorem 6.5
If A is an n × n matrix, A−1 exists if and only if A is row-equivalent to In.
Example 6.10
1.
Find the inverse of
A =
µ
2
4
1
4
¶
(see exercise 6.3.6).

The Inverse of a Matrix
323
Solution
We perform a sequence of elementary row operations on A with the object of
reducing this matrix to I2. We then perform the same sequence of operations on
I2. The result is A−1.
The operations on A are shown below. The elementary row operation used at
each stage and the row to which it is applied are shown in square brackets after
the matrix which results. We denote the ith row by Ri so that, for instance,
R2 →(R2 −R1) indicates that the second row has been transformed by having
the ﬁrst row subtracted from it:
A =
µ
2
4
1
4
¶
∼
µ 1
0
1
4
¶
[R1 →(R1 −R2)]
∼
µ
1
0
0
4
¶
[R2 →(R2 −R1)]
∼
µ
1
0
0
1
¶
[R2 →(R2 ÷ 4)].
These three elementary row operations ‘reduce’ A to I2. We now perform these
operations, in the same order, on I2:
I2 =
µ
1
0
0
1
¶
∼
µ
1
−1
0
1
¶
[R1 →(R1 −R2)]
∼
µ
1
−1
−1
2
¶
[R2 →(R2 −R1)]
∼
Ã
1
−1
−1
4
1
2
!
[R2 →(R2 ÷ 4)].
Thus
A−1 =
Ã
1
−1
−1
4
1
2
!
.
We can check this:
AA−1 =
µ
2
4
1
4
¶ Ã
1
−1
−1
4
1
2
!
=
µ 1
0
0
1
¶
.

324
Matrix Algebra
Alternatively
A−1A =
Ã
1
−1
−1
4
1
2
! µ
2
4
1
4
¶
=
µ 1
0
0
1
¶
.
So we have AA−1 = A−1A = I2 as required.
2.
Find the inverse of
A =
µ
6
2
4
1
¶
.
Solution
Rather than perform row operations on A followed by the same sequence on I2,
we may as well do both simultaneously. The usual way of presenting this is to
write the matrices A and I2 side by side thus:
µ
6
2
1
0
4
1
0
1
¶
.
This matrix, denoted by (A I2), is an example of a partitioned matrix. It is
a 2 × 4 matrix partitioned into two 2 × 2 blocks and each block is termed a
submatrix of the partition. We now perform elementary row operations on (A I2)
until we obtain (I2 A−1):
(A I2) =
µ
6
2
1
0
4
1
0
1
¶
∼
µ
−2
0
1
−2
4
1
0
1
¶
[R1 →(R1 −2R2)]
∼
Ã
1
0
−1
2
1
4
1
0
1
!
[R1 →(R1 ÷ (−2))]
∼
Ã
1
0
−1
2
1
0
1
2
−3
!
[R2 →(R2 −4R1)].
Hence
A−1 =
Ã
−1
2
1
2
−3
!
.
We can check that AA−1 = I2 (or A−1A = I2) as in the last example.

The Inverse of a Matrix
325
In general, for a non-singular matrix A, there will be many different sequences of
elementary row operations which will reduce A to In and it does not matter which
of these we use. Any sequence of elementary row operations which reduces A to
In will, when applied to In, result in the inverse of A. It is useful, however,
to develop a systematic method of utilizing elementary row operations which will
reduce a square matrix to the appropriate identity matrix. Provided that the matrix
concerned is row-equivalent to In the following steps will always reduce an n×n
matrix to In.
1.
Obtain a one in the top left-hand corner of the matrix A either by
(a)
dividing (or multiplying) the ﬁrst row by a suitable constant, or
(b)
if the top left-hand element is zero, interchanging the ﬁrst row with
another row which has a non-zero element as its ﬁrst entry and then
performing step (a).
2.
Subtract a suitable multiple of the ﬁrst row from every other row so as to
obtain zero in every ﬁrst column entry apart from row 1.
The ﬁrst column now has zeros in every row except the ﬁrst where the element is
a one. We now work on the second column with the object of obtaining a one in
the second row and zeros elsewhere.
3.
Divide (or multiply) the second row by a suitable constant so as to
produce a one in the second column. If this is not possible because the
(2, 2)-entry is zero, interchange row 2 with a row below it which does
not have zero in the second column and then divide (or multiply) by a
suitable constant.
4.
Subtract a suitable multiple of the second row from every other row so
that the second column consists of zeros apart from a one in row 2.
The second column now has zeros in every row except the second where the
element is a one.
The process continues in the same way operating on each column in turn so as
to produce a one on the leading diagonal and zeros everywhere else. Applying
these steps to the partitioned matrix (A I) ﬁnally results in (I A−1) so long as A
is non-singular. A ﬂowchart for this algorithm is given in ﬁgure 6.1.
We illustrate these steps in the examples below.

326
Matrix Algebra
Figure 6.1
Examples 6.11
1.
Find the inverse of
A =


2
2
−6
−1
1
2
−3
5
3

.
Solution
(A I3) =


2
2
−6
1
0
0
−1
1
2
0
1
0
−3
5
3
0
0
1



The Inverse of a Matrix
327
∼


1
1
−3
1
2
0
0
−1
1
2
0
1
0
−3
5
3
0
0
1


[R1 →(R1 ÷ 2)]
∼



1
1
−3
1
2
0
0
0
2
−1
1
2
1
0
0
8
−6
3
2
0
1



[R2 →(R2 + R1)]
[R3 →(R3 + 3R1)]
∼



1
1
−3
1
2
0
0
0
1
−1
2
1
4
1
2
0
0
8
−6
3
2
0
1



[R2 →(R2 ÷ 2)]
∼



1
0
−5
2
1
4
−1
2
0
0
1
−1
2
1
4
1
2
0
0
0
−2
−1
2
−4
1



[R1 →(R1 −R2)]
[R3 →(R3 −8R2)]
∼



1
0
−5
2
1
4
−1
2
0
0
1
−1
2
1
4
1
2
0
0
0
1
1
4
2
−1
2



[R3 →(R3 ÷ (−2))]
∼



1
0
0
7
8
9
2
−5
4
0
1
0
3
8
3
2
−1
4
0
0
1
1
4
2
−1
2



[R1 →(R1 + 5
2R3)]
[R2 →(R2 + 1
2R3)].
Thus
A−1 =



7
8
9
2
−5
4
3
8
3
2
−1
4
1
4
2
−1
2


.
We can conﬁrm the result by checking that AA−1 = I3 (or A−1A = I3). Since
it is only too easy to make errors in the process of ﬁnding a matrix inverse, it is
always wise to do this.
2.
Find the inverse of
A =


1
1
1
2
−1
3
4
1
5

.
Solution
(A I3) =


1
1
1
1
0
0
2
−1
3
0
1
0
4
1
5
0
0
1



328
Matrix Algebra
∼


1
1
1
1
0
0
0
−3
1
−2
1
0
0
−3
1
−4
0
1


[R2 →(R2 −2R1)]
[R3 →(R3 −4R1)]
∼



1
1
1
1
0
0
0
1
−1
3
2
3
−1
3
0
0
−3
1
−4
0
1



[R2 →(R2 ÷ (−3))]
∼



1
0
4
3
1
3
1
3
0
0
1
−1
3
2
3
−1
3
0
0
0
0
−2
−1
1



[R1 →(R1 −R2)]
[R3 →(R3 + 3R2)].
No further sequence of elementary row operations will complete the conversion
of the matrix A to I3. The matrix A is not row-equivalent to I3. Therefore, by
theorem 6.5, A does not have an inverse and is a singular matrix.
You are probably beginning to realize that ﬁnding the inverse of even a 3 × 3
matrix can be a tedious and error-prone activity. Using this method for inverting
a 10 × 10 matrix is not for the faint-hearted! Fortunately, computer programs
are available which will invert matrices of considerable dimension, e.g. Derive,
Maple, Mathematica, etc.
Exercises 6.4
1.
Prove by mathematical induction that
(A1A2 . . . An)−1 = An
−1An−1
−1 . . . A2
−1A1
−1.
2.
Find the inverse (if it exists) of each of the following matrices:
(i)
µ
5
−3
−3
2
¶
(ii)
µ
−4
8
−1
3
¶
(iii)
µ
4
2
6
3
¶
(iv)


1
1
0
1
0
0
1
1
1


(v)


1
2
3
4
5
6
7
8
9

(vi)


2
3
−2
1
2
−1
−2
1
0

(vii)


4
2
1
3
−1
0
2
−4
2


(viii)


0
2
2
2
2
0
1
4
3

.

The Inverse of a Matrix
329
3.
If A, B and C are square matrices of the same dimension, prove that, if
A is non-singular and AB = AC, then B = C. Show that the matrix A
in exercise 6.2.7 is singular so that the above result does not necessarily
hold for A.
4.
(i)
Show that if A and B are square matrices of the same dimension
and A is non-singular, then
(A−1BA)2 = A−1B2A.
(ii)
Let
A =
µ
2
1
3
1
¶
and
B =
µ
4
−2
3
−1
¶
.
Calculate A−1BA. Using the result in (i), calculate A−1B2A and
hence A−1B4A.
(iii)
Prove that (A−1BA)n = A−1BnA for all positive integers n.
5.
Let
A =
µ
a
b
c
d
¶
.
Show that, if ad −bc ̸= 0, then
A−1 =
1
ad −bc
µ
d
−b
−c
a
¶
.
Use this result to write down the inverses of
(i)
µ
2
−3
4
1
¶
(ii)
µ
3
7
1
1
¶
.
6.
Suppose that A is a non-singular diagonal matrix of dimension n×n with
(non-zero) diagonal elements aii (i = 1, . . . , n). Determine A−1.
Hence write down the inverse of



4
7
0
0
0
−5
3
0
0
0
4
9


.
7.
A matrix which is its own inverse is said to be involutary, i.e. A is an
involutary matrix if A2 = I. Prove that an n × n matrix A is involutary
if and only if (In −A)(In + A) = On×n.

330
Matrix Algebra
8.
Suppose that partitioned matrices A and B are as follows:
A =
µ
4
2
2
1
3
1
−3
0
−2
3
¶
and
B =






2
−1
4
0
3
−1
3
−2
0
−1
4
−2
1
0
1






so that
A = (A1 A2)
and
B =
µ B1
B2
B3
B4
¶
.
Evaluate AB and show that
AB = (A1B1 + A2B3
A1B2 + A2B4).
Show also that
BT =
µ B1T
B3T
B2T
B4T
¶
.

Chapter 7
Systems of Linear Equations
7.1
Introduction
A linear equation in n variables (or unknowns) x1, x2, . . . , xn is one which can
be expressed in the form
a1x1 + a2x2 + · · · + anxn = b
(1)
where a1, a2, . . . , an and b are real numbers. The constants a1, a2, . . . , an are
called the coefﬁcients of the variables x1, x2, . . . , xn respectively.
Examples of linear equations are:
3x1 + 2x2 −5x3 = 7,
3y = 7x −4z + 2,
5x1 −2x3 = 4x2 + 3x4.
A linear equation which is expressed in the form of (1) above, with the variables
on the left-hand side of the equation and the constant term on the right, is said to
be written in standard form. Of the examples above, only the ﬁrst is in standard
form.
A solution of the linear equation
a1x1 + a2x2 + · · · + anxn = b
is an ordered n-tuple (c1, c2, . . . , cn) such that x1 = c1, x2 = c2, . . . , xn = cn
satisﬁes the equation, i.e. such that a1c1 + a2c2 + · · · + ancn = b.
331

332
Systems of Linear Equations
For the simplest linear equation, ax = b, there are three possibilities concerning
solutions.
1.
If a ̸= 0, the equation has the single solution x = b/a.
2.
If a = 0 and b ̸= 0, then the equation has no solution.
3.
If a = 0 and b = 0, then any real number is a solution of the equation,
i.e. the equation has an inﬁnite number of solutions.
For the general linear equation (1) with more than one variable, we have two
possibilities. If a1 = a2 = · · · = an = 0, b ̸= 0, the equation will have no
solution. Otherwise a single linear equation in more than one variable will have
an inﬁnite number of solutions. For example, if we take the linear equation
3x −2y + z = 4
some solutions are:
x = 0
y = 0
z = 4
i.e. (0, 0, 4),
x = 1
y = 1
2
z = 2
i.e. (1, 1
2, 2),
x = 2
y = 1
z = 0
i.e. (2, 1, 0).
The solution set of a linear equation is the set of all possible solutions. For a
linear equation in n variables, this is a set of ordered n-tuples. Apart from the
trivial case referred to above, for a linear equation with two or more variables this
will be an inﬁnite set.
We can interpret linear equations in two and three variables geometrically. The
solution set of the equation a1x + a2y = b (where a1 and a2 are not both zero) is
represented by all points on a line in two-dimensional space, R2.
The general linear equation in three variables a1x + a2y + a3z = b has a
solution set which, so long as it is not empty, deﬁnes a plane in three-dimensional

Introduction
333
space, R3.
A system of linear equations is simply several linear equations involving the
same variables. In general a system consisting of m linear equations in n variables
can be written:
a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
...
...
...
am1x1 + am2x2 + · · · + amnxn = bm.
If b1 = b2 = · · · = bm = 0, then the system of linear equations is said to be
homogeneous. If any of these constants is not zero, then the system is said to be
non-homogeneous. For example,
6x + 2y −z = 7
4x + y + z = 2
is a non-homogeneous system of two linear equations in the three variables x, y
and z. The system
2x1 −x2 + x3 −x4 = 0
x2 = 2x1 −4x3 −x4
3x1 + 5x2 −2x3 = 0
is recognizable as a homogeneous system of three linear equations in the four
variables x1, x2, x3 and x4 once the second equation has been written in standard
form:
2x1 −x2 −4x3 −x4 = 0.
A solution of a system of linear equations is an ordered n-tuple deﬁning values
of the variables which satisfy each equation in the system. For instance, the
system
3x −2y + z = −3
x + y + z =
5
x −2y −z = −9
y + z =
6
has a solution (−1, 2, 4).

334
Systems of Linear Equations
In general, just as for the single linear equation ax = b, a system of linear
equations may have none, one or many solutions. A system which has no solution
is called inconsistent.
A system which has one or many solutions is called
consistent.
A convenient way to represent a system of linear equations is in matrix form.
Consider the following general system of m equations in the n variables
x1, x2, . . . , xn:
a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
...
...
...
am1x1 + am2x2 + · · · + amnxn = bm.
This system can be represented by the equivalent matrix equation:





a11 a12 . . . a1n
a12 a22 . . . a2n
...
...
...
am1 am2 . . . amn










x1
x2
...
xn




=





b1
b2
...
bm




.
Multiplying together the matrices on the left-hand side of the equation gives a
matrix of dimension m × 1 whose elements are the left-hand side of the system
of equations. Equating the elements of this matrix with those in the matrix on
the right-hand side of the matrix equation gives each of the m equations in the
system.
If we let
A = [aij]
x =





x1
x2
...
xn





and
b =





b1
b2
...
bm





then we can write the matrix equation as
Ax = b.
The matrix A is often referred to as the matrix of coefﬁcients. A solution of the
system corresponds to any column matrix x which satisﬁes the matrix equation.
The elements of this column matrix are values of x1, x2, . . . , xn which satisfy all
the equations in the system.

Introduction
335
Example 7.1
Write the following system of linear equations in matrix form:
3x1 + 2x2 −x3 =
7
x1 −3x2 −2x3 = −5
2x1 + x2
=
4
6x2 + 7x3 = 12.
Solution
The equivalent matrix equation is




3
2
−1
1
−3
−2
2
1
0
0
6
7






x1
x2
x3

=




7
−5
4
12



.
Thus the system can be written in the form Ax = b, where
A =




3
2
−1
1
−3
−2
2
1
0
0
6
7




x =


x1
x2
x3


b =




7
−5
4
12



.
That
x =


1
2
0


(i.e. x1 = 1, x2 = 2, x3 = 0) is a solution of the system can be checked by
substitution into the matrix equation.
We saw earlier that the linear equation ax = b has none, one or inﬁnitely many
solutions. The situation is exactly the same for any system of linear equations.
We can justify this assertion for a system of two equations in two variables:
a11x + a12y = b1
a21x + a22y = b2.
As long as the coefﬁcients of x and y are not both zero, each of these equations
represents a line in the (x, y)-plane and the solution of the system can be

336
Systems of Linear Equations
interpreted as a point or points which are common to both lines. There are three
possibilities.
(a)
The lines are parallel and do not meet at all. In this case the system has
no solution and is inconsistent. This situation is illustrated in the diagram
below.
(b)
The lines cross and therefore have one point in common. In this case the
system has one solution.
(c)
The lines coincide so that all points on one line are common to the other.
The system then has an inﬁnite number of solutions.
It is clearly not possible for two lines to have more than one point in common
unless they have all points in common.
We now prove this result for the general system of linear equations. We show that,
for a consistent set of equations, either there is one solution or there are inﬁnitely
many solutions.

Matrix Inverse Method
337
Theorem 7.1
Every system of linear equations has no solution, one solution or inﬁnitely
many solutions.
Proof
We shall come across examples of each of the three cases during the course of
this chapter so, to prove the theorem, we must show that there are no other
possibilities. This means that we must show that a system of linear equations
having more then one solution cannot have a ﬁnite number of solutions.
Suppose that the system of equations represented in matrix form by
Ax = b
has two different solutions x = u and x = v so that
Au = b
and
Av = b.
Consider the matrix tu + (1 −t)v where t is any real number. Now
A[tu + (1 −t)v] = tAu + (1 −t)Av
= tb + (1 −t)b
= b.
This shows that, if u and v are solutions to the system, then so is tu + (1 −t)v
for any value of t. This proves the result, i.e. that a system having more than one
solution has an inﬁnite number of solutions.
□
We now consider some methods for solving systems of linear equations.
7.2
Matrix Inverse Method
Suppose that we have a system of m linear equations in n variables expressed in
matrix form Ax = b. Now consider the case where m = n, i.e. we have the same
number of equations as variables. The matrix A is then square and of dimension

338
Systems of Linear Equations
n × n. If A is non-singular so that A−1 exists, we can pre-multiply the matrix
equation above by A−1 to give:
A−1Ax = A−1b
Inx = A−1b
⇒
x = A−1b.
⇒
Equating the elements in the matrices on each side of the last equation gives
the values of x1, x2, . . . , xn which satisfy all the equations in the system, i.e.
a solution to the system of equations.
Example 7.2
Solve the system of equations
2x + 2y −6z =
4
−x + y + 2z =
3
−3x + 5y + 3z = −1.
Solution
The system of equations can be written in matrix form
Ax = b
where
A =


2
2
−6
−1
1
2
−3
5
3


x =


x
y
z


and
b =


4
3
−1

.
If A is non-singular, a solution of the system is given by
x = A−1b.
In example 6.10.1 we found that
A−1 =




7
8
9
2
−5
4
3
8
3
2
−1
4
1
4
2
−1
2





Matrix Inverse Method
339
so that x = A−1b is equivalent to


x
y
z

=




7
8
9
2
−5
4
3
8
3
2
−1
4
1
4
2
−1
2






4
3
−1

.
Multiplying the matrices on the right-hand side we have


x
y
z

=




146
8
50
8
15
2




and a solution of the system is
x = 146
8 = 73
4 , y = 50
8 = 25
4 , z = 15
2 ,
i.e. (x, y, z) =
¡ 73
4 , 25
4 , 15
2
¢
.
This can be conﬁrmed by substituting these values of x, y and z into each equation
and checking that they satisfy every equation in the system.
We have already shown that a system of linear equations may have none, one or
inﬁnitely many solutions. It is reasonable therefore to ask whether the solution
obtained by the method above is the only solution or whether it is just one of an
inﬁnite number still to be found. It is not difﬁcult to prove that, when A is non-
singular, the solution x = A−1b is the only solution. This we do in the following
theorem.
Theorem 7.2
If A is a non-singular square matrix, then the system of linear equations
represented by Ax = b has a unique solution given by x = A−1b.
Proof
Suppose that x and y are both solutions so that
Ax = b
and
Ay = b.

340
Systems of Linear Equations
We have
Ax = Ay
A−1Ax = A−1Ay
⇒
x = y.
⇒
We have already seen that x = A−1b is a solution and we can therefore conclude
that this is the unique solution of the equation.
□
Example 7.3
Solve the homogeneous system of equations
2x + 2y −6z = 0
−x + y + 2z = 0
−3x + 5y + 3z = 0.
Solution
In matrix form the system of equations is given by Ax = b where
A =


2
2
−6
−1
1
2
−3
5
3


x =


x
y
z


b =


0
0
0

.
The solution is x = A−1b where A−1 is as in the last example. So we have
x =




7
8
9
2
−5
4
3
8
3
2
−1
4
1
4
2
−1
2






0
0
0


=


0
0
0

.
Thus the solution of the system is (0, 0, 0).
This result is fairly obvious when we look at the original set of equations. It is
clear that any set of homogeneous equations will always have a solution where all
variables are zero. This solution is called the trivial solution. For a homogeneous

Matrix Inverse Method
341
system of n equations in n variables where the matrix of coefﬁcients is non-
singular, the implication of theorem 7.2 is that the only solution is the trivial one.
We now have a method for solving a system of n linear equations in n variables
so long as A, the matrix of coefﬁcients, is non-singular. However, the method is
computationally inefﬁcient because, as we have already noted, inverting a matrix
can involve a lot of arithmetic operations. Also, what happens if A is not square
or is square but singular? In either of these cases, the method fails.
The method based on ﬁnding the inverse of the matrix of coefﬁcients, although
useful, has obvious limitations. In the next section we consider another method
which can be applied to any system of linear equations and which therefore does
not suffer from the disadvantages of the method described above.
Exercises 7.1
1.
Find the inverse of the matrix
A =
µ
2
−1
1
2
¶
.
Hence solve the system of equations
2x −y = 6
x + 2y = 8.
2.
Find the inverse of the matrix
A =


2
2
1
1
−1
−1
1
3
3

.
Hence solve the system of equations
2x + 2y + z = 4
x −y −z = 1
x + 3y + 3z = 1.
Write down the solution of the system
2x + 2y + z = 0
x −y −z = 0
x + 3y + 3z = 0.

342
Systems of Linear Equations
3.
Solve the system of linear equations
3x + 2y + z =
4
x −y + 2z =
8
6x −3y −z = −4.
7.3
Gauss†–Jordan‡ Elimination
In order to explain the method we need two deﬁnitions.
Deﬁnition 7.1
A matrix is said to be in row echelon form if all the following are true.
(a)
The ﬁrst (i.e. furthest to the left) non-zero element in every row is
a 1.
(b)
In each row the ‘leading 1’ is further to the right than the leading 1
in any preceding row.
(c)
Every row of zeros is below all non-zero rows.
(The ﬁrst non-zero element in any row of a matrix is termed the leading element
of that row.)
Examples of matrices in row echelon form are:




1
3
−2
5
0
0
1
4
0
0
0
1
0
0
0
0






1
−1
0
0
1
2
0
0
1




0
1
−1
4
0
0
1
−2
0
0
0
0

.
† Carl Friedrich Gauss (1777–1855), a German, is considered by many to be the greatest
mathematician of all time. He was a child prodigy and, when he was 20, gave the ﬁrst proof of
the fundamental theorem of algebra. Much of his extensive research he never published. As a result, a
great deal of the mathematics published in the mid nineteenth century was subsequently found to have
been discovered earlier by Gauss.
‡ Wilhelm Jordan (1842–99) was a surveyor and professor of geodesy at the Karlsruhe Technical
College in Germany. However, this method is often mistakenly attributed to Camille Jordan (1838–
1921), an engineer who published material in many branches of mathematics, particularly group
theory.

Gauss–Jordan Elimination
343
The following matrices are not in row echelon form, the ﬁrst because the leading
element in the second row is not a one and the second because the leading 1 in
the second row is to the left of the leading 1 in the ﬁrst row. The third matrix fails
because a row of zeros (the third row) is above a non-zero row.


1
1
3
0
2
0
0
0
1




0
1
2
1
2
−1
0
0
1






1
0
0
3
0
0
1
−2
0
0
0
0
0
0
0
1



.
Deﬁnition 7.2
A matrix is said to be in reduced row echelon form if it is in row echelon
form and every column which contains a leading 1 contains zeros for all its
other elements.

344
Systems of Linear Equations
Examples of reduced row echelon matrices are:


1
0
0
−4
0
1
0
6
0
0
1
5




1
0
0
0
1
0
0
0
1






0
1
−1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0



.
You are probably familiar with the method used in the example below to solve a
system of two linear equations in two variables.

Gauss–Jordan Elimination
345
Example 7.4
Solve the system of equations
2x −2y = 6
4x + y = 7.
Solution
We label the equations (e1) and (e2) so that we can refer to them:
2x −2y = 6
(e1)
4x + y = 7.
(e2)
We can eliminate x from (e2) by subtracting from it twice (e1). This gives the two
equations
2x −2y =
6
(e1)
5y = −5.
(e2 −2e1)
Dividing the second of these equations by 5 gives y = −1. Substitution of y = −1
into (e1) gives 2x = 4 so that x = 2.
This method essentially consists of applying certain ‘allowed’ operations to the
equations within the system to produce another system having the same solution
but which is easier to solve. Two or more systems of equations having the same
solution(s) are called equivalent systems. The following operations on a system
of linear equations are ‘permitted’ in that they produce another system which is
equivalent:
(a)
interchanging two equations;
(b)
multiplying (or dividing) one equation by a non-zero constant;
(c)
adding to one equation a multiple of another equation.
These operations are precisely the elementary row operations (§6.4) but applied
to the equations in a system rather than the rows of a matrix.
Now suppose that we have the following linear system to solve:
x + y + z =
2
2x −2y −z =
2
3x + y −2z = −2.

346
Systems of Linear Equations
We represent the system by the following partitioned matrix:


1
1
1
2
2
−2
−1
2
3
1
−2
−2

.
This is the matrix of coefﬁcients which we denoted by A but with an extra column
consisting of the constant terms on the right-hand sides of the equations. It is
called the augmented matrix (or the augmented matrix of coefﬁcients) and is
denoted by (A b). Each equation in the system can be reconstructed from the
augmented matrix.
Each permitted operation on the equations of the system corresponds to an
elementary row operation on the augmented matrix.
Each elementary row
operation produces a row-equivalent matrix representing an equivalent system
of equations.
In other words, row-equivalent augmented matrices represent
equivalent systems of equations. Now suppose that we can reduce the matrix
to reduced row echelon form using elementary row operations. The result might
be something like


1
0
0
a
0
1
0
b
0
0
1
c


where a, b and c are constants. This matrix represents the system x = a, y = b,
z = c, i.e. the solution of the original system.
This suggests a useful method for solving a system of linear equations. We write
down the augmented matrix and, by applying elementary row operations to it,
we reduce it (if possible) to reduced row echelon form. The solution(s) of the
system can then be read off from this matrix. This process is called Gauss–
Jordan elimination. Note that the method does not depend on being able to
invert any matrix nor on the number of equations or variables. Hence it is general
enough to apply to any system of linear equations.
Example 7.5
Solve the following system of linear equations:
x + y + z =
2
2x −2y −z =
2
3x + y −2z = −2.

Gauss–Jordan Elimination
347
Solution
Starting from the augmented matrix we reduce it by elementary row operations as
follows:


1
1
1
2
2
−2
−1
2
3
1
−2
−2


(augmented matrix)
∼


1
1
1
2
0
−4
−3
−2
0
−2
−5
−8


[R2 →(R2 −2R1)]
[R3 →(R3 −3R1)]
∼



1
1
1
2
0
1
3
4
1
2
0
−2
−5
−8



[R2 →(R2 ÷ (−4))]
∼



1
0
1
4
3
2
0
1
3
4
1
2
0
0
−7
2
−7



[R1 →(R1 −R2)]
[R3 →(R3 + 2R2)]
∼



1
0
1
4
3
2
0
1
3
4
1
2
0
0
1
2



[R3 →(R3 ÷ (−7
2))]
∼



1
0
0
1
0
1
0
−1
0
0
1
2



[R1 →(R1 −1
4R3)]
[R2 →(R2 −3
4R3)].
Thus the solution is x = 1, y = −1, z = 2. These can be checked by substitution
into the three equations.
The systematic steps required to reduce the augmented matrix to reduced row
echelon form are exactly those which we employed to ﬁnd the inverse of a matrix
A when we used elementary row operations to convert the partitioned matrix
(A I) to (I A−1). The sequence of steps is given on page 325 and illustrated in
the ﬂowchart in ﬁgure 6.1. In fact, if A (the matrix of coefﬁcients) is a square non-
singular matrix, the process of Gauss–Jordan elimination will inevitably result in
a reduced row echelon form which consists of the appropriate identity matrix with
an extra column on the right-hand side.
We now see what happens if A does not have an inverse, either because (a) A is a
square singular matrix, or (b) A is not a square matrix.

348
Systems of Linear Equations
Example 7.6
Solve the system of linear equations
2x + 2y + z = 4
x −y −z = 2
3x + y
= 6.
Solution


2
2
1
4
1
−1
−1
2
3
1
0
6


(augmented matrix)
∼



1
1
1
2
2
1
−1
−1
2
3
1
0
6



[R1 →(R1 ÷ 2)]
∼



1
1
1
2
2
0
−2
−3
2
0
0
−2
−3
2
0



[R2 →(R2 −R1)]
[R3 →(R3 −3R1)]
∼



1
1
1
2
2
0
1
3
4
0
0
−2
−3
2
0



[R2 →(R2 ÷ (−2))]
∼



1
0
−1
4
2
0
1
3
4
0
0
0
0
0



[R1 →(R1 −R2)]
[R3 →(R3 + 2R2)].
This is now in reduced row echelon form and represents the system of equations
x −1
4z = 2
y + 3
4z = 0.
The last equation which would normally be expected to give us the value for z,
corresponds to 0z = 0. This is satisﬁed by any value of z and therefore z can be
chosen arbitrarily. Writing z = t, where t is a parameter which can take any real
value, we have
x = 2 + 1
4t
y = −3
4t

Gauss–Jordan Elimination
349
and the system has an inﬁnite number of solutions corresponding to all possible
values of the parameter t. We can write the solution set as
©
(x, y, z) : x = 2 + 1
4t, y = −3
4t, z = t, where t ∈R
ª
=
©¡
2 + 1
4t, −3
4t, t
¢
: t ∈R
ª
.
In the example above, we can see from the reduced row echelon matrix that A, the
matrix of coefﬁcients, is singular since it has not been possible to reduce that part
of the augmented matrix corresponding to A to the identity matrix. (Remember
that any non-singular square matrix is row-equivalent to the appropriate identity
matrix.) However, it does not matter that A is singular; the method has still
enabled us to solve the system.
Example 7.7
Solve the linear system
2x1 −x2 + 5x3 + 3x4 = 5
x1 + x2 + 4x3 + 3x4 = 7
x1
+ 3x3 + 2x4 = 4
x2 + x3 + x4 = 3.
Solution




2
−1
5
3
5
1
1
4
3
7
1
0
3
2
4
0
1
1
1
3




(augmented matrix)
∼





1
−1
2
5
2
3
2
5
2
1
1
4
3
7
1
0
3
2
4
0
1
1
1
3





[R1 →(R1 ÷ 2)]
∼





1
−1
2
5
2
3
2
5
2
0
3
2
3
2
3
2
9
2
0
1
2
1
2
1
2
3
2
0
1
1
1
3





[R2 →(R2 −R1)]
[R3 →(R3 −R1)]

350
Systems of Linear Equations
∼





1
−1
2
5
2
3
2
5
2
0
1
1
1
3
0
1
2
1
2
1
2
3
2
0
1
1
1
3





[R2 →(R2 × 2
3)]
∼




1
0
3
2
4
0
1
1
1
3
0
0
0
0
0
0
0
0
0
0




[R1 →(R1 + 1
2R2)]
[R3 →(R3 −1
2R2)]
[R4 →(R4 −R2)].
This is equivalent to the system
x1 + 3x3 + 2x4 = 4
x2 + x3 + x4 = 3.
In this example two variables, x3 and x4, can be assigned arbitrary values.
Writing x3 = t, x4 = u, we have
x1 = 4 −3t −2u
x2 = 3 −t −u.
As in the last example, the system has an inﬁnite number of solutions and the
solution set can be written
{(x1, x2, x3, x4) : x1 = 4 −3t −2u, x2 = 3 −t −u, x3 = t, x4 = u; t, u ∈R}
= {(4 −3t −2u, 3 −t −u, t, u) : t, u ∈R}.
Example 7.8
Solve the linear system
x + 2y −3z = 0
3x −y + z = 3
2x −3y + 4z = 1.
Solution


1
2
−3
0
3
−1
1
3
2
−3
4
1


(augmented matrix)

Gauss–Jordan Elimination
351
∼


1
2
−3
0
0
−7
10
3
0
−7
10
1


[R2 →(R2 −3R1)]
[R3 →(R3 −2R1)]
∼



1
2
−3
0
0
1
−10
7
−3
7
0
−7
10
1



[R2 →(R2 ÷ (−7))]
∼



1
0
−1
7
6
7
0
1
−10
7
−3
7
0
0
0
−2



[R1 →(R1 −2R2)]
[R3 →(R3 + 7R2)]
∼



1
0
−1
7
6
7
0
1
−10
7
−3
7
0
0
0
1



[R3 →(R3 ÷ (−2))]
∼



1
0
−1
7
0
0
1
−10
7
0
0
0
0
1



[R1 →(R1 −6
7R3)]
[R2 →(R2 + 3
7R3)].
The bottom row of the reduced row echelon matrix represents the equation
0x + 0y + 0z = 1, which clearly has no solution. Hence the system of equations
is inconsistent. (Note that A is again a singular square matrix.)
In each of the last three examples, A, the matrix of coefﬁcients, was a square
singular matrix and so any process of applying elementary row operations to the
augmented matrix will not reduce that part of the matrix to an identity matrix.
Hence a system of equations where A is singular will not have a unique solution.
Depending on the form of the reduced row echelon matrix, such a system has
either an inﬁnite number of solutions or no solution at all.
The method of Gauss–Jordan elimination can be used just as easily in the case
where A is not a square matrix, i.e. where there are more equations than variables
or more variables than equations. We consider each of these cases in the two
examples below.
Example 7.9
Solve the system of equations
2x −3y = −4
x + 2y =
5
−4x + 6y =
8.

352
Systems of Linear Equations
Solution


2
−3
−4
1
2
5
−4
6
8


(augmented matrix)
∼



1
−3
2
−2
1
2
5
−4
6
8



[R1 →(R1 ÷ 2)]
∼



1
−3
2
−2
0
7
2
7
0
0
0



[R2 →(R2 −R1)]
[R3 →(R3 + 4R1)]
∼



1
−3
2
−2
0
1
2
0
0
0



[R2 →(R2 × 2
7)]
∼


1
0
1
0
1
2
0
0
0


[R1 →(R1 + 3
2R2)].
Thus we have x = 1, y = 2, the last row of the matrix providing no information
other than 0x + 0y = 0!
Where there are more equations than variables, there may be none, one or an
inﬁnite number of solutions. However, if the number of variables exceeds the
number of equations, then there will be none or an inﬁnite number of solutions.
The following example shows why this is the case.
Example 7.10
Solve the linear system
3x −2y + z = −4
x + y + 2z =
2.
Solution
µ 3
−2
1
−4
1
1
2
2
¶
(augmented matrix)

Gauss–Jordan Elimination
353
∼
Ã
1
−2
3
1
3
−4
3
1
1
2
2
!
[R1 →(R1 ÷ 3)]
∼
Ã
1
−2
3
1
3
−4
3
0
5
3
5
3
10
3
!
[R2 →(R2 −R1)]
∼
Ã
1
−2
3
1
3
−4
3
0
1
1
2
!
[R2 →(R2 × 3
5)]
∼
µ
1
0
1
0
0
1
1
2
¶
[R1 →(R1 + 2
3R2)].
An equivalent system is therefore given by
x + z = 0
y + z = 2.
Writing z = t, we have x = −t, y = 2 −t and the system has an inﬁnite number
of solutions because t can be chosen arbitrarily.
If we consider the possible ﬁnal forms of the reduced row echelon matrix when
we have more variables than equations, it is clear that such a system cannot have a
unique solution. The system will either be inconsistent or have an inﬁnite number
of solutions. When we interpret the two equations in example 7.10 geometrically,
we can see why this is the case. The two equations represent planes in three-
dimensional space. There are three possibilities:
(a)
the planes intersect in a line and there are an inﬁnite number of solutions
which can be expressed in terms of one parameter;
(b)
the equations represent the same plane and there are an inﬁnite number
of solutions which can be expressed in terms of two parameters;
(c)
the planes are parallel and do not intersect at all, in which case there are
no solutions.
A ﬁnal word is in order about homogeneous systems of linear equations. We have
seen that such a system always has at least one solution—the trivial solution where
all the variables are zero. However, a homogeneous system is just a special case
of a general system of linear equations and the results that we have just deduced
for linear systems apply no less to homogeneous linear systems. For instance, we
have stated that if A, the matrix of coefﬁcients, is a square singular matrix, then
the system has no solution or an inﬁnite number of solutions. Applying this result
to homogeneous systems, we deduce that if A is a singular square matrix, the
homogeneous system must have an inﬁnite number of solutions since we know
that it has at least one.

354
Systems of Linear Equations
We give below a table summarizing the number of solutions which a system
of linear equations may have for the four possible states of A, the matrix of
coefﬁcients.
Homogeneous
Non-homogeneous
linear system
linear system
A is m × m, non-singular
Trivial solution only
Unique solution
A is m × m, singular
Inﬁnite number of solutions
None or inﬁnite number
of solutions
A is m × m, m > n
One or inﬁnite number of
None, one or inﬁnite
solutions
number of solutions
A is m × n, m < n
Inﬁnite number of solutions
None or inﬁnite number
of solutions
There are methods for determining from the augmented matrix (A b) exactly how
many solutions a linear system of equations has, but these are beyond the scope
of this book.
Exercises 7.2
Use Gauss–Jordan elimination to solve the following systems of linear equations.
1.
x −y −2z =
2
2x + y −z =
7
x −3y + 2z = −12.
Hence write down the solution of
x −y −2z = 0
2x + y −z = 0
x −3y + 2z = 0.
2.
2x1 + 7x2 + 3x3 = 14
x1 + 5x2 + 3x3 = 13
x1 + 4x2 + 2x3 = 8.

Gaussian Elimination
355
3.
2x1 + x2 −x3 =
2
4x1 −x2 −x3 = −2
3x1 + 3x2 −2x3 =
6.
4.
x1 −x2 + 2x3 = 0
2x1 + 3x2 −x3 = 0.
5.
x1 + 3x2 + x3
= 1
x1 + 2x2 + x3 + x4 = 5
−x2
+ x4 = 4
x1 + x2 + x3 + 2x4 = 9.
6.
x −y + z = 0
2x −y
= 0
3x + 2y −7z = 0.
7.
2x1 + 4x2 −x3 + x4 = 2
3x1 −x2
+ 2x4 = 3
x1 + 2x2 + 3x3 −x4 = 5.
7.4
Gaussian Elimination
In Gauss–Jordan elimination the object is to reduce the augmented matrix of
coefﬁcients to reduced row echelon form by applying elementary row operations.
Then any solution can be ‘read off’ directly from the ﬁnal matrix.
In practice it is not necessary to reduce the augmented matrix right down to
reduced row echelon form. If a matrix in row echelon form is obtained, the
solutions to the system (if any) can be calculated easily. The steps necessary
to obtain the row echelon form of the matrix are the same as those required to
obtain the reduced row echelon form except that, having obtained a ‘leading 1’ in
any row, suitable multiples of that row are subtracted only from the rows below it
in order to obtain zeros below the leading 1 in any column. The method, which is
called Gaussian elimination is illustrated in the examples below.

356
Systems of Linear Equations
Example 7.11
Use Gaussian elimination to solve the following system of linear equations:
x + y + z =
2
2x −2y −z =
2
3x + y −2z = −2.
(In example 7.5 we solved this system using Gauss–Jordan elimination.)
Solution


1
1
1
2
2
−2
−1
2
3
1
−2
−2


(augmented matrix)
∼


1
1
1
2
0
−4
−3
−2
0
−2
−5
−8


[R2 →(R2 −2R1)]
[R3 →(R3 −3R1)]
∼



1
1
1
2
0
1
3
4
1
2
0
−2
−5
−8



[R2 →(R2 × (−1
4))]

Gaussian Elimination
357
∼



1
1
1
2
0
1
3
4
1
2
0
0
−7
2
−7



[R3 →(R3 + 2R2)]
∼



1
1
1
2
0
1
3
4
1
2
0
0
1
2



[R3 →(R3 × (−2
7))].
This gives the system of equations
x + y +
z = 2
(i)
y + 3
4z = 1
2
(ii)
z = 2
(iii)
which is equivalent to the original system.
From equation (iii) we have z = 2. Substitution of this value into equation (ii)
gives
y + 3
2 = 1
2
so that
y = −1.
Substitution of these values for y and z into equation (i) gives
x −1 + 2 = 2
so that
x = 1.
Thus the solution of the system is (1, −1, 2).
Example 7.12
Use Gaussian elimination to solve the following system of linear equations:
2x + 2y + z = 4
x −y −z = 2
3x + y
= 6.
(In example 7.6 we solved this system using Gauss–Jordan elimination.)
Solution


2
2
1
4
1
−1
−1
2
3
1
0
6


(augmented matrix)

358
Systems of Linear Equations
∼



1
1
1
2
2
1
−1
−1
2
3
1
0
6



[R1 →(R1 ÷ 2)]
∼



1
1
1
2
2
0
−2
−3
2
0
0
−2
−3
2
0



[R2 →(R2 −R1)]
[R3 →(R3 −3R1)]
∼



1
1
1
2
2
0
1
3
4
0
0
−2
−3
2
0



[R2 →(R2 ÷ (−2))]
∼



1
1
1
2
2
0
1
3
4
0
0
0
0
0



[R3 →(R3 + 2R2)].
This represents the system of equations
x + y + 1
2z = 2
y + 3
4z = 0
0z = 0.
Writing z = t, we have
y + 3
4t = 0
(from the second equation) so that
y = −3
4t.
Also
x −3
4t + 1
2t = 2
(from the ﬁrst equation) so that
x = 2 + 1
4t.
In a consistent set of equations, Gaussian elimination results in a ﬁnal matrix from
which the value of at least one of the variables can be read off directly or assigned
a parameter if the number of solutions is inﬁnite. The values of the other variables
must be calculated by a systematic method of ‘back-substitution’. This usually
involves fewer arithmetic operations than are involved in the extra elementary
row operations necessary to obtain the reduced row echelon form rather than the
row echelon form. For this reason, Gaussian elimination is usually preferred to
Gauss–Jordan elimination as a method for solving systems of linear equations.

Gaussian Elimination
359
Exercises 7.3
1.
Find the solution(s) (if any) of the following systems of linear equations
using Gauss–Jordan elimination:
(i)
3x −y + z = 10
x + y −z = −2
−x + 2y + 2z =
0
(ii)
2x1 + x2 + 8x3 = 14
x2 + 2x3 = 6
x1 + 3x2 + 5x3 = 10
(iii)
x + 2y −4z = 4
x + 3y −6z = 7
2x + 3y −5z = 9
(iv)
3x + 2y −z = 4
4x −2y + 7z = 3
x + 4y −2z = 3
(v)
x + y + z = 0
−2x −y + z = 0
3x + 2y + 2z = 0.
2.
Find the solution(s), if any, of the following systems of linear equations,
using Gaussian elimination:
(i)
x1 + x2 + 4x3 =
2
4x1 + 3x2 + 15x3 =
0
2x1 + x2 + 7x3 = −4
(ii)
x + y −z = 0
2x + y + 2z = 4
x −2y + z = 8
(iii)
x + y −z = 8
y −3z = 2
2x + y + z = 14
(iv)
x + 2y + z = 0
3x −y −z = 0
2x −3y + 2z = 0.

360
Systems of Linear Equations
3.
Find the solution(s), if any, of the following systems of linear equations:
(i)
x1 + x2 −3x3 =
3
2x1 −3x2 −x3 = −9
(ii)
x + y = 0
2x + y = 1
x −2y = 8
(iii)
2x −y −z = 1
x −2y + 3z = 7
−x + y −z = 2
(iv)
x1 + x2 + x3 =
3
2x1 −x2 −x3 =
0
−x1 + 3x2 −2x3 =
0
x2 −2x3 = −1
(v)
x + y −z = 0
3x −2y + 2z = 0
2x −y + z = 0
(vi)
3x1 −2x2 + x3 =
7
2x1 −2x2 + 3x3 = −1
−2x1
+ 4x3 = −1.

Chapter 8
Algebraic Structures
8.1
Binary Operations and Their Properties
Very often in mathematics we are interested in combining the elements of some
set. We have come across many such examples in earlier chapters of this book.
In chapter 1 propositions were combined to form new propositions using logical
connectives. The operations of union and intersection of sets introduced in chapter
3 each combine two sets to give a third denoted by A∪B and A∩B respectively.
In chapter 5 we looked at composition of functions. Given functions f and g
such that the image of f is a subset of the domain of g, we deﬁned the composite
function g ◦f. Other examples are the addition and multiplication of matrices
and the familiar arithmetic operations of addition, subtraction, multiplication and
division of real numbers.
The essential feature of each of these examples is a rule which allows two
members of a speciﬁed set to be combined. For our purposes in this chapter we
shall require that the rule must provide a means of combining any two elements
and the result must itself be a member of the set. A rule which satisﬁes these
criteria is called a ‘binary operation’.
Of the examples given above, whether or not a particular rule is a binary operation
depends critically upon the set in question. For example, addition is a binary
operation on the set of positive integers; any two positive integers can be added
and the result is also a positive integer. On the other hand subtraction is not a
binary operation on this set because, given any two positive integers m and n, the
result m −n is not always a positive integer. Subtraction is a binary operation on
the set of all integers, however.
361

362
Algebraic Structures
For some binary operations the order of combining two elements matters and for
some it does not. For instance, for the set of integers, it is always the case that
m + n = n + m. This is not so for subtraction where in general the results of
m−n and n−m are different. For this reason a binary operation must be viewed
as acting, not just on a pair of elements of a set, but on an ordered pair.
In summary, a binary operation has two essential ingredients: a set and a rule for
combining any ordered pair of its elements so that the result is also a member of
the set.
Deﬁnition 8.1(a)
A binary operation ∗on a non-empty set S is a rule for combining any
two elements x, y ∈S to give an element z ∈S where z is denoted by
x ∗y.
Notice from the deﬁnition that a binary operation is simply a function which
assigns an element of S to every ordered pair of elements (x, y) where x and
y belong to S. The set of these ordered pairs is, of course, the Cartesian product
S×S. This leads us to an alternative, rather more succinct, deﬁnition given below.
Deﬁnition 8.1(b)
A binary operation on a non-empty set S is a function f : S × S →S. If
x and y are elements of S, we denote f(x, y) by x ∗y.
The condition that x∗y must belong to S (or equivalently that the codomain of the
function f is S) is called the closure property of the binary operation and, when
this condition holds, we say that S is closed under the operation ∗. (In some texts,
closure is not a required property of a binary operation, which is then deﬁned as
a function f : S × S →T, where S and T are non-empty sets and normally
S ⊆T.)

Binary Operations and Their Properties
363
Examples 8.1
1.
The operations of addition, subtraction and multiplication are each binary
operations on Z, the set of integers. Division is not a binary operation on
Z since, for instance, 3 ÷ 4 does not result in a member of Z, i.e Z is not
closed under division.
2.
Addition, subtraction and multiplication are all binary operations on Q,
the set of rational numbers. Division is still not a binary operation on Q
because x ÷ 0 is not deﬁned for any x ∈Q.
3.
If S = P(A), the power set of a set A (i.e. the set of all subsets of A),
then the operations denoted by ∪and ∩are each binary operations on S.
4.
A binary operation on a ﬁnite set may be deﬁned using a table showing
the result of applying that operation to any ordered pair of elements of
the set. For example, if S = {a, b, c, d}, we can deﬁne a binary operation
∗on S by the following table.
∗
a
b
c
d
a
a
b
c
d
b
d
c
a
b
c
c
b
a
a
d
d
b
c
a
The convention for interpreting the table is that b ∗d, for example, is
deﬁned to be the element at the intersection of the row labelled ‘b’ and
the column headed by ‘d’, so that b∗d = b. Similarly c∗d = a, d∗c = c,
c ∗c = a, and so on. A table which deﬁnes a binary operation in this way
is called a Cayley table†.
We now consider some deﬁnitions which will enable us to distinguish the
properties of certain binary operations. The ﬁrst relates to the fact that a binary
operation ∗combines pairs of elements of a set, so that there are two different
readings which we could give to the expression a ∗b ∗c. We could interpret
it as (a ∗b) ∗c, i.e. combine a with b ﬁrst and then combine the result with c.
Alternatively, we could perform the operation in the order a ∗(b ∗c), combining
† Named after Arthur Cayley (1821–95), the English mathematician who, during his professorship at
Cambridge University, successfully brought about a change in the regulations so that women could be
admitted to the university.

364
Algebraic Structures
a with the result of b ∗c. For some binary operations, for example subtraction on
the real numbers, the two interpretations give different results. For others, such as
addition, the method of grouping the three elements makes no difference. Binary
operations which have the latter property are termed ‘associative’.
Deﬁnition 8.2
A binary operation ∗on a set S is said to be associative if, for all
x, y, z ∈S,
(x ∗y) ∗z = x ∗(y ∗z).
For a binary operation which is not associative, an expression involving the
combination of more than two elements must include brackets to indicate which
elements are to be combined ﬁrst. For an associative binary operation we can
write x ∗y ∗z without fear of ambiguity.
Recall that we have deﬁned a binary operation on an ordered pair (x, y). Implicit
in the deﬁnition is that, if x and y are elements of a set, x ∗y and y ∗x are also
elements of the set. However, they may not be equal because the ordered pairs
(x, y) and (y, x) are not equal (unless y = x). Binary operations, such as addition
of real numbers, for which x ∗y = y ∗x are said to be ‘commutative’.
Deﬁnition 8.3
A binary operation ∗on a set S is said to be commutative if, for all
x, y ∈S,
x ∗y = y ∗x.
For certain binary operations there is an element within the set which is neutral
in the sense that, when it is combined with any member of the set, it leaves that
element unchanged. For the real numbers under addition, zero has this property:
x + 0 = 0 + x = x for all real numbers x. Such an element, if it exists, is called
an ‘identity element’.

Binary Operations and Their Properties
365
Deﬁnition 8.4
Let ∗be a binary operation on a set S. An element e ∈S with the property
that
x ∗e = e ∗x = x
for all x ∈S is called an identity element (or just an identity) for the
operation ∗.
Notice that for e to be an identity element, both the equations x ∗e = x and
e ∗x = x must be satisﬁed for all elements x of the set S. The element 0 is not
an identity for the integers under subtraction because, if x ∈Z, x −0 = x but
0 −x = −x.
Our ﬁnal property relates only to binary operations for which an identity element
exists.
Deﬁnition 8.5
Let ∗be a binary operation on the set S and suppose that there is an identity
element e ∈S. Let x be an element of S. An inverse of x is an element
y ∈S such that
x ∗y = y ∗x = e.
Where the inverse is unique, we write y = x−1. (Note that in this case, x
is also the inverse of y, i.e. x = y−1.)
The fact that we write x−1 as a generic symbol for the inverse of x may be a little
confusing given the more familiar interpretation of x−1 as meaning 1/x (x ̸= 0).
(In fact 1/x is the inverse of x if S is the set of non-zero real numbers and ∗is
the binary operation of multiplication.) We must therefore be careful to interpret
x−1 as the inverse of the element x with respect to the binary operation currently
under consideration.

366
Algebraic Structures
Examples 8.2
1.
The binary operation of addition on Z is associative and commutative.
The identity element is 0 since 0 ∈Z and
x + 0 = 0 + x = x
for all x ∈Z. The inverse of any element x is −x since
x + (−x) = (−x) + x = 0
and for any x ∈Z, (−x) ∈Z. Hence for the set of integers under addition
we can write x−1 = −x.
2.
Multiplication on Z is also associative and commutative. The identity
element is 1. Do any elements of Z have inverses? Consider the element
4, for example. To ﬁnd the inverse of 4 we must ﬁnd an element a ∈Z
such that
4 × a = a × 4 = 1.
The only number a which satisﬁes these equations is 1
4 which does not
belong to Z. Thus 4 does not have an inverse. The only elements of Z
which do have inverses under multiplication are 1 and −1. Since each of
these is its own inverse, we say that these elements are self-inverse.
For multiplication on Q (the set of rational numbers), all elements except
0 have inverses. The inverse of any non-zero element x is 1/x and we
can therefore write the familiar x−1 = 1/x.
3.
Let S = P(A), the power set of a set A. We saw in chapter 3 that the
binary operation of union of sets is both associative and commutative.
The identity element is ∅since ∅∈S and
X ∪∅= ∅∪X = X
for any X ∈S. The only member of S which has an inverse is ∅, which
is self-inverse. (The power set P(A) together with set intersection is
considered in exercise 8.1.4.)
4.
The binary operation deﬁned by the Cayley table in example 8.1.4 is not
associative since, for instance
(b ∗d) ∗a = b ∗a = d
but
b ∗(d ∗a) = b ∗d = b.

Binary Operations and Their Properties
367
Neither is the operation commutative, e.g. b ∗a ̸= a ∗b. That no identity
element exists can be readily veriﬁed from the table (see exercise 8.1.3)
and, since there is no identity, there can be no inverses.
In the examples of binary operations described above, if an identity existed at all,
there was only one element of the set which satisﬁed the necessary criteria. This
is no accident. An identity element may or may not exist for a given set and binary
operation but, where it does exist, it is unique. (In fact, we have been anticipating
this result by referring to the identity.)
Theorem 8.1
Let ∗be a binary operation on a set S. If an identity element exists, then it
is unique.
Proof
Let e1 and e2 be identity elements in S under the operation ∗. Since e2 is an
identity,
e1 ∗e2 = e2 ∗e1 = e1.
But e1 is also an identity, so
e2 ∗e1 = e1 ∗e2 = e2.
This establishes that e1 = e2, so the identity element is unique.
□
We can also show that, for an associative binary operation, the inverse of an
element, where it exists is unique.
Theorem 8.2
Let ∗be an associative binary operation on a set S which has identity
element e under ∗. For any element which has an inverse, the inverse is
unique.

368
Algebraic Structures
Proof
Suppose that an element x ∈S has inverses y and z so that
y ∗x = x ∗y = e
z ∗x = x ∗z = e.
Now
y = y ∗e
= y ∗(x ∗z)
= (y ∗x) ∗z
(by associativity)
= e ∗z
= z.
Hence the inverse of x is unique.
□
Notice that associativity of the binary operation was essential to the proof of this
theorem. If the binary operation is not associative, the uniqueness of any inverse
cannot be guaranteed (see exercise 8.1.9).
Exercises 8.1
1.
In each of the examples below, state whether x ∗y deﬁnes a binary
operation on the set S given. If it does not, explain why not.
(i)
x ∗y = x −y, S = R+.
(ii)
x ∗y = z where z < x + y, S = Z.
(iii)
x ∗y = xy, S = R+.
(iv)
x ∗y
= the least common multiple of x and y, S
=
{1, 2, 3, 4, 6, 8, 12, 24} (the set of divisors of 24).
(v)
x∗y = the greatest common factor of x and y, S as deﬁned in (iv).
(vi)
x ∗y = x + y, S = {all matrices}.
2.
Consider the binary operation of subtraction on the set of real numbers.
(a)
Is the operation associative?
(b)
Is the operation commutative?
(c)
Does an identity element exist and, if so, which elements have
inverses?

Binary Operations and Their Properties
369
3.
Suppose that S is a ﬁnite set and a binary operation is deﬁned on S by
a Cayley table (as in example 8.1.4). How can you tell from the Cayley
table:
(i)
whether ∗is commutative on S?
(ii)
whether an identity exists?
4.
Consider S = P(A), the set of all subsets of a set A, together with the
binary operation of intersection, ∩.
(i)
Is ∩commutative on S?
(ii)
What is the identity element?
(iii)
Which elements, if any, have inverses? What are their inverses?
5.
Let S be a set together with a binary operation ∗. Suppose that an identity
element exists and, for all x, y, z ∈S,
x ∗(y ∗z) = (x ∗z) ∗y.
Show that ∗is commutative and associative.
6.
How many distinct binary operations can be deﬁned on a set with
(i)
two elements,
(ii)
three elements,
(iii)
four elements,
(iv)
n elements?
7.
Let S = {a, b, c} and ∗be a commutative binary operation on S. Let a be
the identity element and suppose that every element has a unique inverse.
Draw the Cayley tables for all the binary operations which satisfy these
criteria. Are any of these operations associative?
8.
Let S = P(A), the power set of a set A. Let
X ∗Y = (X −Y ) ∪(Y −X)
for all X, Y ∈S. (This operation is called the symmetric difference of
X and Y ; see exercise 3.4.5.)
(i)
Show that ∗is a binary operation on S.
(ii)
Is ∗commutative?
(iii)
Is ∗associative?
(iv)
Is there an identity element? If so, what is it?
(v)
If there is an identity element, what is the inverse of an element
X ∈S?

370
Algebraic Structures
9.
The following is the Cayley table for a binary operation ∗on the set
{a, b, c, d}. Note that a is the identity element and that c and d are both
inverses for b.
∗
a
b
c
d
a
a
b
c
d
b
b
d
a
a
c
c
a
b
d
d
d
a
b
c
Show that ∗is not associative on {a, b, c, d} (cf. theorem 8.2).
8.2
Algebraic Structures
An algebraic structure consists of one or more sets together with one or more
operations which enable members of the sets to be combined in some way. What
is important about a particular algebraic structure is that many of its properties
are predictable from the characteristics of the operation or operations involved.
This means that we can classify algebraic structures into families whose members
have many features in common. Identiﬁcation of a given algebraic structure as
belonging to a particular family of structures allows us to conclude that it has
the properties characteristic of all members of the family. To illustrate the point:
you may know nothing about a lory. However, if you are told that it is a type
of parrot, then you may reasonably assume that it has amongst its attributes all
those which are characteristic of parrots. So it is with algebraic structures. If a
particular structure can be identiﬁed as a ‘group’ then it can be assumed to have
all the properties characteristic of groups.
The algebraic structures with which we shall concern ourselves here are those
which consist of a single set S together with a single binary operation for
combining members of the set. We shall denote such a structure by (S, ∗) to
emphasize that the structure has two essential components—a set and a binary
operation on that set.
Properties of the binary operation provide the axioms
deﬁning the different families by which these structures are classiﬁed.
Semigroups
For our ﬁrst class of algebraic structures we require of the binary operation
only that it be associative.
Algebraic structures with this property are called
‘semigroups’.

Algebraic Structures
371
Deﬁnition 8.6
Let S be a non-empty set and let ∗be a binary operation deﬁned on S. The
structure (S, ∗) is a semigroup if the operation ∗is associative on S, i.e. if,
for every x, y, z ∈S,
(x ∗y) ∗z = x ∗(y ∗z).
If the operation is also commutative, then the structure (S, ∗) is called an
abelian† (or commutative) semigroup.
Examples 8.3
1.
The structures (N, +), (N, ×), (R, +), (R, ×) are all abelian semigroups.
2.
Let A denote a non-empty set of symbols.
Such a set is called an
alphabet. Some examples of alphabets are:
(a)
A = {α, β, γ, δ, φ, π}
(b)
A = {a, b, c, d, . . . , x, y, z}
(c)
A = {×, +, −, ÷, /, £, $, %, &, q}.
Given an alphabet A, we deﬁne a string (or word) over A to be a ﬁnite
ordered sequence of symbols from A.
The length of a string is the
number of symbols which it contains. Thus, if A = {a, b, c, d}, then
abbc, dcad and abcd are all strings of length 4. The string bccadaa has
length 7, and so on.
Now suppose that A is an alphabet and consider A∗, the set of all strings
over A.
(Note that A∗is an inﬁnite set.)
We deﬁne the operation
of concatenation on the elements of A∗as follows.
If x and y are
two elements of A∗(i.e. two strings over the alphabet A), then the
concatenation of x and y, denoted by x ∗y, is the string obtained by
juxtaposing x and y so that x is on the left and y on the right. Thus for
the set A∗of strings over the set A = {a, b, c, d}, we have, for example,
abd ∗cabc = abdcabc
baaa ∗ccbabb = baaaccbabb.
† Named after Niels Henrik Abel (1802–29), a Norwegian mathematician who contributed to the
theory of equations and inﬁnite series. A year after his premature death from tuberculosis, he was
honoured with the award of the Grand Prize in Mathematics by the Royal Academy of France.

372
Algebraic Structures
For any given alphabet A, the operation of concatenation on A∗is a
binary operation and it is clear from the deﬁnition that this operation is
associative. The structure (A∗, ∗), where ∗represents concatenation, is
therefore a semigroup. It is called the free semigroup generated by A.
Note that concatenation is commutative only when A has just a single
element.
3.
Suppose that S = {a, b, c} and a binary operation is deﬁned on S by the
following Cayley table.
∗
a
b
c
a
a
b
c
b
a
b
c
c
a
b
c
The structure (S, ∗) is in fact a semigroup, but to check that this is so we
have to show that
(x ∗y) ∗z = x ∗(y ∗z)
for all x, y, z ∈S. Often when a binary operation is deﬁned by a Cayley
table, establishing associativity involves checking that this equation holds
for all possible choices of x, y and z. This can be a long and arduous
process! However, in this case, notice that
x ∗y = y
for all x, y ∈S. Hence
(x ∗y) ∗z = y ∗z = z
x ∗(y ∗z) = x ∗z = z
for all x, y, z ∈S, so ∗is associative. For this structure we can save
ourselves the trouble of testing all 27 equations.
Monoids
The single restriction on the binary operation of semigroups does not give them
enough structure for many interesting properties to emerge. So for our next family
of algebraic structures we add a second condition to that of associativity—the
existence of an identity. (Remember that theorem 8.1 guarantees that there can
be only one identity.) Algebraic structures having these two properties are called
‘monoids’.

Algebraic Structures
373
Deﬁnition 8.7
A monoid is a semigroup (S, ∗) which has an identity element.
If ∗is also commutative, the monoid is called an abelian monoid (or
commutative monoid).
Examples 8.4
1.
(Z+, ×) is an abelian monoid with identity element 1. The structure
(Z+, +) is not a monoid because there is no identity element (0 /∈Z+).
The structures (Z, ×) and (Z, +) are each abelian monoids with identity
elements 1 and 0 respectively.
2.
In example 8.3.2 we deﬁned the operation of concatenation on strings
of symbols. Suppose we add the empty string (i.e. string containing no
symbols) to the set A∗. Denoting the empty string by λ, we have
x ∗λ = λ ∗x = x
for all x ∈A∗∪{λ}. The structure (A∗∪{λ}, ∗) is a monoid and it is
called the free monoid generated by A.
3.
The structure (S, ∗) deﬁned in example 8.3.3 is not a monoid since there
is no identity element.
4.
If S = P(A), where A is any set, then (S, ∪) is an abelian monoid
with identity element ∅. Also (S, ∩) is an abelian monoid with identity
element A.
Groups
Many of the most important and interesting examples of algebraic structures
involving a single binary operation satisfy a third condition in addition to the
two deﬁning a monoid. This is that each element of the set has an inverse element
with respect to the operation. Adding this condition to those for a monoid deﬁnes
the class of algebraic structures known generically as ‘groups’.

374
Algebraic Structures
Deﬁnition 8.8
A group is a monoid (S, ∗) in which every element has an inverse, i.e. the
pair (S, ∗) satisﬁes the following three conditions:
(G1)
∗is associative on S;
(G2)
an identity element exists;
(G3)
every element of S has an inverse.
Predictably, a group in which the binary operation is commutative is called
an abelian group (or commutative group).
Remember that we proved (theorem 8.2) that, for a set with an associative binary
operation, the inverse of any element is unique. When applied to a group (S, ∗),
the theorem guarantees the existence of a unique inverse for every element of S.
Examples 8.5
1.
The structure (Z, +) is a group. The identity element is 0 and the inverse
of any z ∈Z is −z. Since addition is commutative, (Z, +) is an abelian
group.
2.
The structure (R+, ×) is an abelian group with identity element 1. The
inverse of x is 1/x.
3.
The monoid (A∗∪{λ}, ∗) deﬁned in example 8.4.2 is not a group
because, for any non-empty string x, we cannot ﬁnd another string y so
that
x ∗y = y ∗x = λ
where λ is the empty string. Thus no element in the set A∗∪{λ}, other
than λ itself, has an inverse under concatenation. (Exercise 8.2.12 shows
how a group can be deﬁned from an alphabet.)
4.
Consider Z together with the binary operation deﬁned by
x ∗y = x + y + 1.
Is the structure (Z, ∗) a group?

Algebraic Structures
375
Testing ﬁrst for associativity: for any x, y, z ∈Z we have
(x ∗y) ∗z = (x + y + 1) ∗z
= (x + y + 1) + z + 1
= x + y + z + 2
and
x ∗(y ∗z) = x ∗(y + z + 1)
= x + (y + z + 1) + 1
= x + y + z + 2.
Thus ∗is associative on Z.
Is there an identity element? If so, the identity e must satisfy
e ∗x = x ∗e = x
for any x ∈Z.
Now
e ∗x
and
x ∗e = x
x + e + 1 = x
⇔
e = −1.
⇔
Since −1 ∈Z and x ∗(−1) = (−1) ∗x = x for all x ∈Z, −1 is the
identity element under operation ∗.
What about inverses? For x, y ∈Z
x ∗y = e
and
y ∗x = e
x + y + 1 = −1
⇔
y = −2 −x.
⇔
For every x ∈Z, (−2 −x) ∈Z, so every element has an inverse.
Since (G1), (G2) and (G3) are satisﬁed, (Z, ∗) is a group.
5.
In chapter 4 we looked at modulo arithmetic. For ﬁxed integer n, we
deﬁned the equivalence relation ‘congruence modulo n’ on the set Z of
integers:
a ≡n b
if and only if a −b = kn for some k ∈Z.

376
Algebraic Structures
We found that this relation partitioned Z into the set of equivalence
classes
Zn = {[0], [1], [2], . . . , [n −1]}.
Consider n = 5 (see example 4.7). The table for +5 addition modulo 5
(given on page 185), is the following.
+5
[0]
[1]
[2]
[3]
[4]
[0]
[0]
[1]
[2]
[3]
[4]
[1]
[1]
[2]
[3]
[4]
[0]
[2]
[2]
[3]
[4]
[0]
[1]
[3]
[3]
[4]
[0]
[1]
[2]
[4]
[4]
[0]
[1]
[2]
[3]
Is the set Z5 together with addition modulo 5 a group?
That the operation is associative follows from the associativity of ordinary
addition of integers. If we do not appeal to this property, we are faced
with no alternative but to test all possible equations of the form
(x +5 y) +5 z = x +5 (y +5 z)
for all x, y, z ∈Z5. (How many such equations are there?)
From the table above we can see that the identity element is [0] and that
every element has an inverse. For example, [1]−1 = [4], [3]−1 = [2].
Hence Z5 with addition modulo 5 is a group. (See exercise 8.2.4 for
consideration of the group properties of Z5 under multiplication modulo
5.)
Exercises 8.2
1.
Show that the set of all 2×2 matrices with real elements together with the
binary operation of matrix addition is a group. Why is this set together
with matrix multiplication not a group?
Show that matrix multiplication on the set of all 2 × 2 non-singular
matrices is a binary operation. Prove that the set of all 2 × 2 non-singular
matrices forms a group under matrix multiplication.

Algebraic Structures
377
2.
If ∗is a binary operation on a set S, then an element x ∈S is said to be
idempotent if x ∗x = x. Prove that a group has only one idempotent
element.
3.
Show that the set
Z6 = {[0], [1], [2], [3], [4], [5]}
together with addition modulo 6 (denoted by +6) is a group.
Is Z6
together with multiplication modulo 6 (denoted by ×6) a group?
4.
Show that the set
Z5 = {[0], [1], [2], [3], [4]}
under multiplication modulo 5 is not a group but that
Z5 −{[0]} = {[1], [2], [3], [4]}
is a group under this operation.
Is Z4 −{[0]} under multiplication modulo 4 a group?
Under what circumstances will the set Zn −{[0]} under multiplication
modulo n be a group? (Cf. exercise 4.4.12.)
5.
Let P = {p ∈Z+ :
p is prime and p ⩽13}. A binary operation ∗is
deﬁned on P by
p ∗q = greatest prime divisor of p + q −2.
Construct a Cayley table for P under the operation ∗and show that P
has an identity element with respect to ∗. Is (P, ∗) a group? Justify your
answer.
6.
Let S be a non empty set and ∗a binary operation deﬁned by
x ∗y = x
for all x, y ∈S. Show that (S, ∗) is a semigroup. Is (S, ∗) a monoid?
Why or why not?
7.
Suppose that the binary operations ∗and ◦are deﬁned on the sets S and
T respectively and that (S, ∗) and (T, ◦) are both groups. Deﬁne the
operation . on the Cartesian product S × T as follows:
(s1, t1).(s2, t2) = (s1 ∗s2, t1 ◦t2)

378
Algebraic Structures
for all s1, s2 ∈S and t1, t2 ∈T.
Show that . is a binary operation on S × T and that (S × T, .) is a group.
What is the inverse of a typical element (s, t) of S × T? (The algebraic
structure (S × T, .) is called the external direct product of (S, ∗) and
(T, ◦). In this exercise you are required to show that the external direct
product of two groups is itself a group.)
8.
Consider the structure (N, ∗) where ∗is the binary operation deﬁned by
x ∗y =
(
x
if x ⩾y
y
if x < y
where x, y ∈N.
Show that (N, ∗) is a semigroup. Is (N, ∗) a monoid? Why or why not?
Deﬁne the binary operation ◦on N by
x ◦y =
(
x
if x ⩽y
y
if x > y.
Is (N, ◦) a semigroup? Is (N, ◦) a monoid?
9.
Consider the set of 2 × 2 matrices of the form
µ
a
0
0
b
¶
where
a, b ∈R,
together
with
the
binary
operation
of
matrix
multiplication. Is this structure
(a)
a semigroup,
(b)
a monoid,
(c)
a group?
10.
Show that the set of all 2 × 2 matrices of the form
µ
1
n
0
1
¶
where n ∈Z is a group under the operation of matrix multiplication.
What is the identity? What is the inverse of
µ 1
4
0
1
¶
?

More about Groups
379
11.
Let M denote the set of real 2 × 2 matrices of the form
µ
x
y
−y
x
¶
where x and y are not both zero. Show that M is a group under the
operation of matrix multiplication.
12.
Let A be a ﬁnite alphabet and let ¯A be the set of symbols of the form ¯a
where a ∈A, i.e. ¯A = {¯a : a ∈A}. Let B = A ∪¯A and let F(A) be
the subset of B∗∪{λ} consisting of those strings which do not contain
pairs of symbols of the form a¯a or ¯aa. Deﬁne the binary operation ∗on
F(A) to be concatenation of strings followed by the successive removal
of all substrings of the form a¯a or ¯aa. For example
ab ∗¯bca = ab¯bca = aca
db¯a ∗a¯bc ¯da = db¯aa¯bc ¯da = db¯bc ¯da = dc ¯da.
Assuming that the operation is associative, show that (F(A), ∗) is a
group. This is called the free group generated by A.
8.3
More about Groups
We now concentrate our attention on groups, the most important of our three
algebraic structures and historically the ﬁrst to be studied abstractly.
Of the three families of structures which we consider in this chapter, the class
of groups is the most widely studied, has the most interesting structure and is
the most extensively applied. In addition to its signiﬁcance within mathematics
itself, group theory has applications in ﬁelds as diverse as physics, chemistry and
linguistics. In the last section of this chapter we look at how groups are utilized
in coding theory.
The foundations of group theory were laid in the nineteenth century by the French
mathematician Galois†.
The subject is now a well developed component of
† Evariste Galois (1811–32) was born in Paris and had a short but eventful life. He twice failed the
entrance examination to the L’Ecole Polytechnique although, in his late teens, he made discoveries
which contributed signiﬁcantly to the theory of equations. His political activities led to a six-month
spell in prison and, shortly after his release, he was killed in a duel. Although not recognized in his
lifetime, Galois is now regarded as one of the greatest of mathematical geniuses

380
Algebraic Structures
abstract algebra and many books are devoted exclusively to the subject. We shall
be able to do no more than prove some basic theorems about groups and look at
some important examples of groups. In the following sections, we shall also look
at some relations amongst groups themselves.
In this section and those which follow, we shall adopt the convention of omitting
the symbol ∗when writing expressions involving an unspeciﬁed binary operation.
We shall only include this symbol where to omit it results in an ambiguous
expression, for instance when we need to distinguish between two binary
operations. Instead of x ∗y we shall write xy. We also deﬁne ‘powers’ of x
as follows. If n ∈Z+,
xn = x ∗x ∗· · · ∗x
←−−−n terms−−−→
and if n ∈Z−
xn = (x−1)|n| = x−1 ∗x−1 ∗· · · ∗x−1
←−−−−−−|n| terms−−−−−−→.
Predictably, we shall deﬁne x0 = e, the identity element.
This ‘multiplicative notation’ has the advantage of convenience and brevity but
the disadvantage that, for those of us who have studied any algebra, xy is already
established in our minds as meaning ‘x multiplied by y’. We must therefore
be careful not to make assumptions which may be true for the operation of
multiplication but not necessarily so for the binary operation under consideration.
For example, we cannot assume xy = yx unless the binary operation is known
to be commutative. We must also be careful with the ‘laws of indices’. It is not
difﬁcult to show that the following hold for the elements of a group:
(x−1)n = (xn)−1 = x−n
for all n ∈Z
xmxn = xm+n
= xnxm
for all m, n ∈Z
(xm)n = xmn
= (xn)m
for all m, n ∈Z.
(xy)n = xnyn
for all n ∈Z only for a
However,
commutative binary operation.
Where the binary operation is addition, it is usual to adopt the notation normally
associated with that operation. The inverse of an element x is denoted by −x and
x + x + · · · + x
←−−−−n terms−−−−→
is written n.x or nx. For an additive group, the analogues of the ‘laws of indices’
listed above are

More about Groups
381
n(−x) = −(nx)
= (−n)x
for all n ∈Z
mx + nx = (m + n)x = nx + mx
for all m, n ∈Z
n(mx) = (nm)x
= m(nx)
for all m, n ∈Z
n(x + y) = nx + ny
for all n ∈Z since addition
and
is commutative.
We shall denote a group by (G, ∗) rather than (S, ∗) in order to emphasize that
we are referring to a group rather than some other algebraic structure.
Perhaps the most obvious characteristic of any group (G, ∗) is its ‘size’, that is
the number of elements in the underlying set G. This is termed the ‘order’ of the
group (G, ∗).
Deﬁnition 8.9
The order of a group (G, ∗) is the cardinality of the set G. It is denoted by
|G| (see deﬁnition 3.1).
We now prove some useful theorems about the properties of groups.
Theorem 8.3
If (G, ∗) is a group, then the left and right cancellation laws hold; that is, if
a, x, y ∈G, then
(a)
ax = ay implies that x = y (left cancellation law), and
(b)
xa = ya implies that x = y (right cancellation law).
Proof
Suppose that ax = ay.
Since (G, ∗) is a group, then the element a has an inverse a−1. ‘Multiplying’ on
the left by a−1 gives

382
Algebraic Structures
a−1(ax) = a−1(ay)
(a−1a)x = (a−1a)y
(by associativity)
⇒
ex = ey
(where e is the identity)
⇒
x = y.
⇒
We have proved that the left cancellation law holds in a group. A similar proof
establishes that the right cancellation law also holds.
□
These cancellation laws, as they apply to addition and multiplication of non-zero
real numbers, are a familiar feature of elementary algebra. For example, from the
equation 3x = 3y we can deduce that x = y. We can make the same deduction
from the equation x + 2 = y + 2.
The next theorem also has a familiar application in elementary algebra. The linear
equations a + x = b and ax = b have unique solutions for x as long as a ̸= 0. (If
a = 0, the ﬁrst equation has a unique solution but the second does not.) The need
to solve equations such as these arises frequently and we might therefore ask,
given a binary operation ∗, under what circumstances does the ‘linear’ equation
a ∗x = b have a unique solution? That such an equation does not always have a
unique solution is easy enough to demonstrate. Consider, for example, the binary
operation deﬁned in example 8.1.4. The equation c ∗x = a has two solutions and
the equation c ∗x = d has none. For the members of the group, however, we can
prove that every such equation has a unique solution.
Theorem 8.4
If (G, ∗) is a group and a, b ∈G, then
(a)
the equation ax = b has a unique solution x = a−1b, and
(b)
the equation ya = b has a unique solution y = ba−1.
Proof
(a)
Suppose we have ax = b.

More about Groups
383
Pre-multiplying this equation by a−1 gives
a−1(ax) = a−1b
(a−1a)x = a−1b
⇒
ex = a−1b
⇒
x = a−1b.
⇒
Thus x = a−1b is a solution of the equation. We must now show that this
is the only solution.
Suppose that x1 and x2 are both solutions of ax = b. Then we have
ax1 = ax2
x1 = x2
(by the left cancellation law).
⇒
Hence x = a−1b is the unique solution.
The proof of (b) is similar.
□
A useful consequence of each of these two theorems is their implication for the
Cayley table of a group (G, ∗) with a ﬁnite number of elements. The second
theorem guarantees that every element appears exactly once in every row and
column. To see why this is so, consider an arbitrary element a ∈G. Any element
g ∈G appears in the row corresponding to a if the equation ax = g has a solution
for some x ∈G. In this case g is in the column corresponding to x as shown in
ﬁgure 8.1.
Figure 8.1
Theorem 8.4 states that this equation has a unique solution for each g ∈G. Hence
every element of G appears just once in the row corresponding to the element
a and, since a was chosen arbitrarily, we can deduce that every element of G
appears exactly once in every row. A similar argument can be used to show that
each element appears exactly once in every column. Theorem 8.3 can also be
used to establish this result which we summarize below.

384
Algebraic Structures
Theorem 8.5
If (G, ∗) is a ﬁnite group (i.e. one with ﬁnite order), its Cayley table is
such that every element of G appears once and only once in every row and
column.
Since we have not established the truth of the converse statement, we cannot use
this property of the Cayley table to show that (S, ∗) is a group although the fact
that this criterion is not satisﬁed is often useful in proving that a structure is not a
group. In fact, if the Cayley table for a binary operation on a ﬁnite set S is such
that there is an identity element and every element of the set appears once and
only once in every row and column, then (S, ∗) is a group if and only if ∗is an
associative operation (see exercise 8.3.3). However, as we have seen, establishing
associativity for a binary operation deﬁned by a Cayley table can be a tedious
process (see example 8.3.3).
We now turn our attention to some important families of groups.
8.4
Some Families of Groups
Cyclic Groups
Consider the group deﬁned by the following Cayley table.
∗
e
a
b
c
e
e
a
b
c
a
a
b
c
e
b
b
c
e
a
c
c
e
a
b
For this group we have a1 = a, a2 = b, a3 = c, a4 = e, from which we can
deduce that every element of {e, a, b, c} can be written in the form an for some
integer n. For any given element, this representation is not unique. For instance,
we could write b = a2 = a6 = a−2 and so on. In fact there are an inﬁnite number
of ways of representing each element of the set as a ‘power’ of a. The point is that
every element of {e, a, b, c} can be written as an for some integer n and, where
this is the case, we say that a is a ‘generator’ of the group.

Some Families of Groups
385
It is reasonable to ask whether any other element is also a generator of the group.
We can conﬁrm that the element c is a generator but that b is not because bn = e
if n is an even integer and bn = b if n is odd. A group which has at least one
generator is said to be ‘cyclic’.
Deﬁnition 8.10
A group (G, ∗) is said to be cyclic if there exists an element a ∈G such
that, for each g ∈G, g = an for some n ∈Z. The group (G, ∗) is said to
be generated by a and a is called a generator of (G, ∗).
In the notation for an additive group (one where the binary operation is addition),
the element a is a generator if, for all g ∈G, g = na for some integer n. A cyclic
group is necessarily abelian because, given g1, g2 ∈G, we have g1 = ar and

386
Algebraic Structures
g2 = as for some r, s ∈Z so that
g1g2 = aras
= ar+s
= as+r
= g2g1.
Examples 8.6
1.
Show that the group (Z, +) is cyclic with generator 1.
Solution
The identity element is 0 and the inverse of the element 1 is −1. For any
element n ∈Z where n > 0 we have
n = 1 + 1 + · · · + 1
←−−−n terms−−−→
= n.1.
If n < 0,
n = (−1) + (−1) + · · · + (−1)
←−−−−−−−−|n| terms−−−−−−−−→
= |n|.(−1)
= n.1.
If n = 0 then
n = 0.1 = n.1.
Hence (Z, +) is a cyclic group and 1 is a generator.
(A similar line of argument will show that −1 is also a generator of the
group.)
2.
Show that Z7
= {[0], [1], [2], [3], [4], [5], [6]} together with addition
modulo 7 is a cyclic group with generator [2].

Some Families of Groups
387
Solution
1.[2] = [2]
2.[2] = [2] +7 [2] = [4]
3.[2] = [4] +7 [2] = [6]
4.[2] = [6] +7 [2] = [1]
5.[2] = [1] +7 [2] = [3]
6.[2] = [3] +7 [2] = [5]
7.[2] = [5] +7 [2] = [0].
Hence every element of Z7 can be written as n.[2] for some integer n and
so (Z7, +7) is a cyclic group with generator [2].
It is easy to verify that all elements of Z7 except [0] are generators of the
group (Z7, +7).
Dihedral Groups
Consider an equilateral triangle with vertices numbered 1,2 and 3 positioned as
shown in the diagram below.
Now consider all the possible transformations of this triangle which result in an
interchange of the positions of the vertices. For instance, if the triangle is rotated
anti-clockwise through 120◦about its ‘centre’, we obtain:

388
Algebraic Structures
Reﬂection of the triangle in the line joining the uppermost vertex to the midpoint
of the opposite side gives:
The set of all these transformations is called the set of symmetries of the
equilateral triangle. There are six such symmetries, three involving rotations
and three involving reﬂections in the lines L1, L2 and L3 as shown in ﬁgure 8.2.
Figure 8.2
(Note that these lines are ﬁxed in the plane and do not move when the triangle is
rotated or reﬂected.)
Table 8.1 gives the position of the vertices of the triangle after each of the
transformations has been effected, given the starting position indicated.
Consider the set T = {r0, r1, r2, m1, m2, m3} and the operation ∗where a ∗b =
ab means ‘perform transformation a followed by transformation b’. Thus r1 ∗m1
means ‘rotate the triangle through 120◦anti-clockwise and then reﬂect the result
in L1’. Figure 8.3 shows the result of combining these two transformations.
Figure 8.3

Some Families of Groups
389
Table 8.1
Symmetry
Result of transformation
r0: rotation through 0◦anti-clockwise
r1: rotation through 120◦anti-clockwise
r2: rotation through 240◦anti-clockwise
m1: reﬂection in L1
m2: reﬂection in L2
m3: reﬂection in L3
The result is equivalent to the single transformation m2 and we can write r1m1 =
m2. The operation ∗is not commutative since, for example m1r1 = m3; see
ﬁgure 8.4.
Figure 8.4

390
Algebraic Structures
The Cayley table for the set T under the operation ∗is given below.
∗
r0
r1
r2
m1
m2
m3
r0
r0
r1
r2
m1
m2
m3
r1
r1
r2
r0
m2
m3
m1
r2
r2
r0
r1
m3
m1
m2
m1
m1
m3
m2
r0
r2
r1
m2
m2
m1
m3
r1
r0
r2
m3
m3
m2
m1
r2
r1
r0
It is clear that ∗is a binary operation on T and we can show that (T, ∗) is a
non-abelian group. The identity is r0 and each element has an inverse. We have
the usual problem with associativity. However, since each transformation can be
regarded as a function mapping the triangular region of the plane to itself, the
operation ∗is then composition of functions which we have already shown to be
associative (see exercise 5.2.8).
The group (T, ∗) is often denoted by D3 (the operation being understood as that
of combining transformations). It is referred to as the group of symmetries of
the equilateral triangle or the dihedral group of degree 3.
A similar group of symmetries exists for any regular polygon. The dihedral group
of degree n is the group of symmetries of a regular n-sided polygon. It has 2n
elements and is denoted by Dn.
Groups of Permutations
Deﬁnition 8.11
Suppose that S is a non-empty set. A permutation of S is a bijection from
S to S.
The usual way of deﬁning a speciﬁc bijection would be to show the effect of the
mapping on every element of S. For example, if S = {1, 2, 3, 4}, we could deﬁne
a bijection p1 by
p1(1) = 2
p1(2) = 4
p1(3) = 3
p1(4) = 1.

Some Families of Groups
391
A more convenient way of representing p1 is by using an array in which the
elements of S occupy the ﬁrst row and their corresponding images the second
row. For the bijection p1 deﬁned above, we write
p1 =
·
1
2
3
4
p1(1)
p1(2)
p1(3)
p1(4)
¸
=
·
1
2
3
4
2
4
3
1
¸
.
In the same way, we might deﬁne another permutation p2 by
p2 =
·
1
2
3
4
2
3
4
1
¸
.
This is equivalent to:
p2(1) = 2
p2(2) = 3
p2(3) = 4
p2(4) = 1.
Note that the order in which the elements of S are listed in the ﬁrst row is
immaterial. What is important is that below each element is its image under the
appropriate bijection. Thus we could equally well write
p1 =
·
2
1
4
3
4
2
1
3
¸
or
p2 =
·
4
3
2
1
1
4
3
2
¸
.
Consider now the set A = {1, 2, 3} and let S3 be the set of all permutations of A.
(We use the notation S3 for this set to emphasize that it is the set of permutations
of a set with three elements.) It is not difﬁcult to establish (see exercise 5.4.7) that
S3 has six elements p1, p2, . . . , p6 deﬁned as follows:
p1 =
·
1
2
3
1
2
3
¸
p2 =
·
1
2
3
2
3
1
¸
p3 =
·
1
2
3
3
1
2
¸
p4 =
·
1
2
3
1
3
2
¸
p5 =
·
1
2
3
3
2
1
¸
p6 =
·
1
2
3
2
1
3
¸
.
There is a natural binary operation which can be deﬁned on S3, that of
composition of functions. Thus pipj (pi, pj ∈S3) denotes composition of the
bijections pi and pj in the order pi followed by pj. (This notation is convenient for
our purpose but it is at odds with our usual notation for composition of functions.
Remember that for functions f1 and f2, (f1 ◦f2)(x) is interpreted as f1[f2(x)],
i.e. perform f2 followed by f1. Thus if a ∈A,
(pipj)(a) = pj[pi(a)]
= (pj ◦pi)(a).)

392
Algebraic Structures
The operation is clearly a binary operation since the composition of bijections on
S is itself a bijection on S (see theorem 5.7(i)). Consider for example p3p5. In
array form we write
p3p5 =
·
1
2
3
3
1
2
¸ ·
1
2
3
3
2
1
¸
.
To obtain the array representing the bijection p3p5 we must ﬁnd the effect of the
bijection on each member of A. Take the element 1 for instance. From the array
for p3 we see that 1 7→3. The array for p5 gives 3 7→1. Therefore under the
bijection p3p5 the image of 1 is 1. We show this below:
p3p5 =
=
·
1
2
3
1
?
?
¸
.
Repeating this process with the remaining elements of A we have
p3p5 =
· 1
2
3
3
1
2
¸ · 1
2
3
3
2
1
¸
=
·
1
2
3
1
3
2
¸
.
This is the array representing p4 and so we can write
p3p5 = p4.
Completing the Cayley table for (S3, ∗) gives:
∗
p1
p2
p3
p4
p5
p6
p1
p1
p2
p3
p4
p5
p6
p2
p2
p3
p1
p5
p6
p4
p3
p3
p1
p2
p6
p4
p5
p4
p4
p6
p5
p1
p3
p2
p5
p5
p4
p6
p2
p1
p3
p6
p6
p5
p4
p3
p2
p1
That the structure (S3, ∗) is a non-abelian group can easily be veriﬁed.
The
identity is p1 and inverses are given by p1−1 = p1, p2−1 = p3, p3−1 = p2,
p4−1 = p4, p5−1 = p5, p6−1 = p6. Associativity follows from the associativity

Some Families of Groups
393
of composition of functions. The set A, on which the bijections were deﬁned, has
three elements. The set of permutations of A, denoted by S3, has six elements.
If S = {1, 2, . . . , n}, so that |S| = n, then the set of permutations, Sn, would
have n(n −1)(n −2) . . . 1 = n! elements. This is so because, in deﬁning a
bijection from S to S, the ﬁrst element of S can be mapped to any one of the
n elements of S, the second element of S to any one of the remaining n −1
elements, and so on. This gives n! possible bijections in all (see exercise 5.4.7).
For any positive integer n, (Sn, ∗), where ∗denotes composition of bijections, is
a group called the symmetric group of degree n. It is usually referred to simply
as Sn, the operation being understood as that of composition of bijections.
Exercises 8.3
1.
Show that for any group (G, ∗),
(ab)−1 = b−1a−1
for all a, b ∈G. (This is sometimes known as the ‘shoes and socks’
theorem. Can you suggest why?)
Deduce that, if a ∈G, (a−1)n = (an)−1 for all n ∈Z.
(Note that theorem 6.4 is the ‘shoes and socks’ theorem applied to the
group of non-singular n × n matrices under multiplication.)
2.
The following is part of the Cayley table of a ﬁnite group. Complete the
table.
∗
e
p
q
r
s
t
e
e
p
q
r
s
t
p
p
q
e
s
q
q
r
r
t
e
p
s
s
t
t

394
Algebraic Structures
3.
The binary operation ∗is deﬁned on the set S = {e, a, b, c, d} by the
following Cayley table.
∗
e
a
b
c
d
e
e
a
b
c
d
a
a
e
d
b
c
b
b
c
e
d
a
c
c
d
a
e
b
d
d
b
c
a
e
Use this table to show that the converse of theorem 8.5 does not hold.
4.
Consider a (non-square) rectangle with vertices numbered 1, 2, 3 and 4
positioned as shown in the diagram below.
The rectangle has four symmetries:
r0:
rotation through 0◦about the centre
r1:
rotation through 180◦about the centre
m1:
reﬂection in the line L1
m2:
reﬂection in the line L2.
Draw the Cayley table for the composition of these transformations and
show that the set {r0, r1, m1, m2} together with this binary operation is
a group. (This group is known as the Klein four-group.)
5.
Let (G, ∗) be a ﬁnite group with order n. Show that, for every element
g ∈G, there exists an integer m ⩽n such that gm = e.
6.
Draw up the Cayley table for D4, the symmetries of a square under
composition of transformations. Establish that the group properties hold
for D4.
7.
Prove that a cyclic group with only one generator cannot have more than
two elements.

Some Families of Groups
395
8.
Show that the group (Z6, +6) is cyclic and identify all its generators.
9.
Show that the set of rotational symmetries of an equilateral triangle
{r0, r1, r2} is a group under composition of rotations. Is this a cyclic
group? If so, what are the generators?
10.
Let (G, ∗) be a group. Show that if x2 = e for all x ∈G, then (G, ∗) is
abelian.
11.
We have shown (exercise 8.2.10) that the set of all 2 × 2 matrices of the
form
µ 1
n
0
1
¶
where n ∈Z, is a group under matrix multiplication. Show that this is a
cyclic group.
12.
Find all the subsets of Z10 which form a group under the operation of
×10. Identify the generators of any of these groups which are cyclic.
13.
(a)
For each of the following values of n, ﬁnd the largest subset of Zn
which forms a group under ×n.
(i)
n = 6;
(ii)
n = 7;
(iii)
n = 8;
(iv)
n = 9.
(b)
Given a set S such that S ⊆Zn, how should the elements of S be
chosen so that (S, ×n) is a group with the greatest possible order?
14.
(a)
Let C5 = {e, g, g2, g3, g4} be the cyclic group of order 5 (so that
g5 = e). Which elements of C5 generate the group?
(b)
Repeat (a) for the cyclic groups C6 and C9.
(c)
Generalize the results of (a) and (b).
In other words, which
of the elements of the cyclic group of order n,
Cn
=
{e, g, g2, g3, . . . , gn−1} generate the group?

396
Algebraic Structures
8.5
Substructures
We have shown that the set of symmetries of an equilateral triangle T
=
{r0, r1, r2, m1, m2, m3} is a group under composition of transformations. In
exercise 8.3.9 we saw that the subset {r0, r1, r2} is also a group under the same
binary operation. We have come across other examples of ‘a group within a
group’. For instance, (Z, +) and (R, +) are each groups and Z is a subset of
R. Where one group is contained within another, we refer to the former as a
‘subgroup’ of the latter.
Deﬁnition 8.12
Let (G, ∗) be a group. If H ⊆G and (H, ∗) is itself a group, we say that
(H, ∗) is a subgroup of (G, ∗) and we write (H, ∗) ⩽(G, ∗).
Note that, in order to be a subgroup, the subset H must be a group under the same
binary operation as that deﬁned for the group (G, ∗).
Every group (G, ∗) with two or more elements has at least two subgroups. Since
G ⊆G, (G, ∗) is a subgroup of itself. Also, if e is the identity element, {e} ⊆G
and ({e}, ∗) is a group and is therefore a subgroup of (G, ∗). These two subgroups
are called improper (or trivial) subgroups. All other subgroups (if any exist) are
called proper subgroups.
Examples 8.7
1.
It can be readily veriﬁed that the set
Z7 −{[0]} = {[1], [2], [3], [4], [5], [6]}
is a group under multiplication modulo 7 (see exercise 8.2.4). The Cayley
table for the set {[1], [2], [4]} under multiplication modulo 7 is given
below.
×7
[1]
[2]
[4]
[1]
[1]
[2]
[4]
[2]
[2]
[4]
[1]
[4]
[4]
[1]
[2]

Substructures
397
From the table we can see that {[1], [2], [4]} is also a group under
multiplication modulo 7 and is therefore a subgroup of (Z7 −{[0]}, ×7).
2.
We denote by Cn the group of rotations of a regular n-sided polygon
under composition of rotations. (The group considered in exercise 8.3.9 is
C3.) This group is cyclic and, for all positive integers n, Cn is a subgroup
of Dn, the dihedral group of degree n.
Given a group (G, ∗) and a set H where H ⊆G, it is useful to have a set of
criteria for determining whether (H, ∗) is a subgroup of (G, ∗). The following
theorem provides a set of such criteria. The proof is simple and is therefore left
as an exercise (8.4.8).
Theorem 8.6 (Subgroup test)
If (G, ∗) is a group and H is a non-empty subset of G, then (H, ∗) is a
group if and only if
(a)
ab ∈H for all a, b ∈H (i.e. H is closed under ∗), and
(b)
for all a ∈H, a−1 ∈H.
The theorem states that, if H ̸= ∅and H ⊆G, to establish that (H, ∗) is a
subgroup of (G, ∗), we need only ensure that H is closed under ∗and that the
inverse of every element of H is also a member of H.
In fact, if H is a ﬁnite non-empty subset of G, all that is necessary to establish
that (H, ∗) is a subgroup of (G, ∗) is to show that H is closed under the operation
∗. The second condition, that every element belonging to H has an inverse which
belongs to H, is automatically satisﬁed. Since this is less obvious than the result
of theorem 8.6 we give a proof.
Theorem 8.7 (Finite subgroup test)
Let (G, ∗) be group and H ⊆G, where H is ﬁnite and non-empty. If H is
closed under ∗, then (H, ∗) is a subgroup of (G, ∗).

398
Algebraic Structures
Proof
We are given that ab ∈H for all a, b ∈H. To apply the result of theorem 8.6 we
must show that a−1 ∈H for all a ∈H.
Now, if a ∈H, then an ∈H for all n ∈Z+ by the closure property. Since
H is a ﬁnite set, this apparently inﬁnite collection of terms must contain some
duplicates. In particular ar = as for some r, s ∈Z+. Without loss of generality
we can assume that r > s and, since ar−s ∈H, we can write this equation as
asar−s = as.
Applying the left cancellation law (these elements belong to the group (G, ∗)),
we have
ar−s = e.
(This establishes that the identity, e, belongs to H.)
Since r > s, r −s −1 ⩾0 so ar−s−1 ∈H and
aar−s−1 = ar−s
= e.
Thus the inverse of a is ar−s−1. This shows that every element of H has an
inverse in H and the theorem is proved.
□
A third test is given for subgroups in exercise 8.4.9.
The subgroup tests provide a useful means of proving that a particular structure is
a group. If H ⊆G where (G, ∗) is known to be group, then to prove that (H, ∗)
is a group it is sufﬁcient to show that the appropriate subgroup conditions apply.
Example 8.8.1 illustrates this.
Examples 8.8
1.
Consider the set
A =
½µ
1
n
0
1
¶
: n ∈Z
¾
under the operation ∗of matrix multiplication. Now A is a non-empty
subset of the set of all 2 × 2 non-singular matrices and we have shown
this to be a group under matrix multiplication (exercise 8.2.1). To show
that (A, ∗) is a group we simply apply theorem 8.6.

Substructures
399
The set A is closed under matrix multiplication since
µ 1
n
0
1
¶ µ 1
m
0
1
¶
=
µ 1
m + n
0
1
¶
for all m, n ∈Z.
The inverse of
µ
1
n
0
1
¶
is
µ
1
−n
0
1
¶
which is an element of A.
Hence (A, ∗) is a subgroup of the group of all 2×2 non-singular matrices
under multiplication and so is a group.
2.
We have already established that the structure (R+, ×) is a group. The
structure (Q+, ×) is a subgroup of (R+, ×) because:
(a)
Q+ ⊆R+ and Q+ is non-empty;
(b)
for any a, b ∈Q+, ab ∈Q+, i.e. Q+ is closed under multi-
plication;
(c)
for any a ∈Q+, a−1 = 1/a ∈Q+.
Given any element a ∈G, where (G, ∗) is a group, it is reasonable to ask what
is the smallest subgroup which contains a. By ‘smallest’ we mean the subgroup
which is contained within any other subgroup of which the element a is a member.
Clearly if the subgroup contains a, by the closure property it must contain
a2, a3, . . . , i.e. it must contain all positive powers of a. The identity, a0, must
also be included and, since the subgroup contains a, it must also contain a−1, the
inverse of a. Applying the closure property again, the subgroup must also contain
the following: (a−1)2 = a−2, (a−1)3 = a−3, . . . . To summarize, any subgroup
(G, ∗) which contains the element a must contain at least all elements of the form
an where n is an integer. (These elements may not be distinct. Indeed, if G is
ﬁnite they certainly will not be.) This is the essence of the proof of the following
theorem.
Theorem 8.8
Let (G, ∗) be a group and let a ∈G. Let H = {an : n ∈Z}. Then (H, ∗)
is a subgroup of (G, ∗) and, if (H′, ∗) is any other subgroup containing a,
then H ⊆H′.
The group (H, ∗) is called the cyclic subgroup of (G, ∗) generated by a. Note

400
Algebraic Structures
that (H, ∗) may not be a proper subgroup of (G, ∗). If a = e, the identity, then
H = {e} so that (H, ∗) is an improper subgroup of (G, ∗). Also if (G, ∗) is cyclic
and a is a generator then H = G and (H, ∗) is again an improper subgroup.
Example 8.9
Consider the group (Z6, +6). Find the cyclic subgroup generated by the element
[2].
Solution
The subgroup must contain all elements of the form n × [2] where n is an integer:
n = 0;
0 × [2] = [0], the identity element
n = 1;
1 × [2] = [2]
n = −1;
−1 × [2] = [4]
n = 2;
2 × [2] = [2] +6 [2] = [4]
n = −2;
−2 × [2] = 2(−1 × [2]) = [4] +6 [4] = [2].
It is clear that for all integers n, n×[2] gives one of [0], [2], [4]. Thus [2] generates
the subgroup ({[0], [2], [4]}, +6) and this is the smallest subgroup containing [2].
In a similar way we can verify that [3] generates the subgroup ({[0], [3]}, +6)
whilst [1] and [5] generate the group (Z6, +6) itself. The element [4] generates
the same subgroup as does [2].
The ‘powers’ of an element a of a group (G, ∗) may all be distinct, i.e. am ̸= an
for any integers m, n where m ̸= n. On the other hand there may be distinct
integers m and n such that am = an. In this case we have am−n = e and there is
a power of a which gives the identity. The smallest positive value of r such that
ar = e is called the ‘order’ of the element a.
Deﬁnition 8.13
If (G, ∗) is a group with identity element e, the order of an element a ∈G
is the least positive integer r such that ar = e. If no such integer exists then
a is said to be of inﬁnite order. If the order of a is n we write |a| = n.

Substructures
401
If (G, ∗) is a ﬁnite group then the powers of any element a ∈g cannot be distinct
and hence every element has ﬁnite order.
Example 8.10
Find the order of each element of the group (G, ∗) deﬁned by the following table.
∗
e
a
b
c
e
e
a
b
c
a
a
e
c
b
b
b
c
e
a
c
c
b
a
e
Solution
Clearly the order of the identity element of any group is 1. (In fact, the identity
element is the only element with order 1.) Since a2 = e, the order of a is 2, i.e.
|a| = 2. Also b2 = e and c2 = e so that the orders of b and c are also 2.
The deﬁnitions for a subsemigroup and submonoid are similar to that for a
subgroup and we include them here for completeness.
Deﬁnition 8.14
Let (S, ∗) be a semigroup and let T ⊆S, where T ̸= ∅. The structure
(T, ∗) is a subsemigroup of (S, ∗) if (T, ∗) is itself a semigroup.
Given a non-empty set T where T ⊆S and (S, ∗) is a semigroup, the only
criterion necessary to establish that (T, ∗) is a semigroup is that T be closed
under the operation ∗so that ∗is a binary operation on T. If this is so, ∗will be an
associative binary operation because it ‘inherits’ this property from the semigroup
(S, ∗).

402
Algebraic Structures
Deﬁnition 8.15
Let (S, ∗) be a monoid with identity e. If T ⊆S and (T, ∗) is itself a
monoid with identity e, then (T, ∗) is a submonoid of (S, ∗).
To test whether (T, ∗) is a submonoid, we therefore need to establish that three
criteria are satisﬁed:
(a)
T ⊆S;
(b)
T is closed under ∗;
(c)
T contains the identity element, e.
Examples 8.11
1.
The structure (Z+, +) is a semigroup.
If E+ = {2, 4, 6, . . .} then
(E+, +) is a subsemigroup of (Z+, +) since E+ ⊆Z+ and E+ is closed
under addition.
2.
The structure (Z+, ×) is a monoid with identity element 1. If O+ =
{1, 3, 5, . . .} then (O+, ×) is a submonoid of (Z+, ×).
3.
Let A = {a, b}. Consider (A∗, ∗), the free semigroup generated by A
(see example 8.3.2). Let X = {x : x ∈A∗and x has a as its ﬁrst
symbol}. X is clearly closed under the operation of concatenation since,
if two strings having a as their ﬁrst symbol are concatenated, the resulting
string will also have a as its ﬁrst symbol. Hence (X, ∗) is a subsemigroup
of (A∗, ∗). Note that (X, ∗) is not a submonoid of (A∗∪{λ}, ∗) (the free
monoid generated by A), since λ is not a member of X.
Exercises 8.4
1.
Let (M, ∗) be an abelian monoid.
Show that the set of idempotent
elements of M is a submonoid under ∗.
(The element x ∈M is
idempotent if x2 = x.)
2.
Find all the proper subgroups of each of the following groups:
(i)
(Z7, +7)

Substructures
403
(ii)
(Z8, +8)
(iii)
(Z10, +10)
(iv)
(Z12, +12).
3.
Show that:
(i)
the set {3z : z ∈Z} together with addition forms a subgroup of
(Z, +);
(ii)
the set {nz : z ∈Z} together with addition forms a subgroup of
(Z, +) for any integer n.
4.
Find all the proper subgroups of (S3, ∗), the group of permutations of a
set with three elements (see §8.4).
5.
Determine whether or not ({[0], [3], [6]}, +9) is a subgroup of (Z9, +9).
6.
Find all the cyclic subgroups of D4, the dihedral group of degree 4. Find
also a non-cyclic proper subgroup of D4. (See exercise 8.3.6.)
7.
Given a group (G, ∗), the centre is deﬁned to be the set {a ∈G : ag = ga
for all g ∈G}, i.e. the subset of G containing all elements which
commute with every element of G.
(i)
Show that the centre is a subgroup of (G, ∗).
(ii)
Find the centre of D3, the dihedral group of degree 3.
(iii)
Find the centre of D4, the dihedral group of degree 4.
8.
Prove theorem 8.6.
9.
Let (G, ∗) be a group and let H ⊆G where H ̸= ∅. Prove that (H, ∗) is
a subgroup of (G, ∗) if and only if ab−1 ∈H for all a, b ∈H.
10.
Prove that, if (H, ∗) and (K, ∗) are both subgroups of the group (G, ∗),
then so is (H ∩K, ∗). Is (H ∪K, ∗) necessarily a subgroup of (G, ∗)?
Justify your answer.
11.
Consider the set Z7 −{[0]}
=
{[1], [2], [3], [4], [5], [6]} under
multiplication modulo 7.
Find all the proper subgroups of the group
(Z7 −{[0]}, ×7).
12.
Consider the set T = {A, B, C, D} where
A =
µ
1
0
0
1
¶
B =
µ
0
1
1
0
¶
C =
µ
0
−1
−1
0
¶
D =
µ −1
0
0
−1
¶
.

404
Algebraic Structures
Show that matrix multiplication is a binary operation on this set and hence
that T together with this operation is a subgroup of the set of all non-
singular 2 × 2 matrices under multiplication.
13.
Prove that every subgroup of a cyclic group is also cyclic.
14.
The following is a well-known result in group theory, whose proof is
beyond the scope of the current chapter.
Lagrange’s theorem.† Let G be a ﬁnite group and let H be a subgroup
of G. Then the order of H is a factor of the order of G.
Use Lagrange’s theorem to prove that, if G is a group with order n and
g ∈G then gn = e.
8.6
Morphisms
Isomorphism
In §8.4 we considered examples of three important families of groups—cyclic
groups, dihedral groups and groups of permutations. The Cayley tables for the
dihedral group D3 and for S3, the group of permutations of a set with three
elements, are reproduced below.
D3
S3
∗
r0
r1
r2
m1
m2
m3
r0
r0
r1
r2
m1
m2
m3
r1
r1
r2
r0
m2
m3
m1
r2
r2
r0
r1
m3
m1
m2
m1
m1
m3
m2
r0
r2
r1
m2
m2
m1
m3
r1
r0
r2
m3
m3
m2
m1
r2
r1
r0
∗
p1
p2
p3
p4
p5
p6
p1
p1
p2
p3
p4
p5
p6
p2
p2
p3
p1
p5
p6
p4
p3
p3
p1
p2
p6
p4
p5
p4
p4
p6
p5
p1
p3
p2
p5
p5
p4
p6
p2
p1
p3
p6
p6
p5
p4
p3
p2
p1
† Named after the Italian-born mathematician Joseph-Louis Lagrange (1736-1813). Lagrange lived
before the development of abstract groups. He actually proved a result about polynomials which was
later recognised to be a special case of this theorem.

Morphisms
405
Comparison of these tables leads to the rather surprising observation that the two
are identical apart from the labelling of the elements. Wherever r2 appears in the
ﬁrst table, p3 appears in the second; wherever m3 is positioned in the ﬁrst, p6 is
found in the second, and so on. Had we called the transformations p1, p2, . . . , p6
instead of r0, r1, r2, m1, m2, m3 respectively, the two tables would have looked
identical. When two ﬁnite groups are related in this way we say that they are
‘isomorphic’.
It is important to appreciate that being isomorphic does not mean that groups
are ‘equal’. In our example, the two sets, however their elements are labelled, are
different and the two binary operations are not the same. However, there is clearly
a very close relationship between isomorphic groups in that their structure is the
same even if the elements are not and we must somehow describe this relationship
in mathematical terms.
What we mean by saying that the Cayley tables are ‘identical apart from the
naming of the elements’ is that there exists a one-to-one correspondence between
the elements of D3 and the elements of S3 so that corresponding elements occupy
the same positions in their respective tables. This one-to-one correspondence is a
bijective function which has the property of preserving the group structure. Such a
function is called an ‘isomorphism’. Put more formally: given two groups (G, ∗)
and (G′, ◦), an isomorphism is a bijective function f : G →G′ which is such that
the image of g1 ∗g2 is that element of G′ which is the result of the operation ◦
applied to the images of g1 and g2. It is this important property of the isomorphism
which ensures that the structures of isomorphic groups are the same.
We summarize these ideas in ﬁgure 8.5 and in formal deﬁnition which applies not
only to ﬁnite groups but also to inﬁnite ones.
Figure 8.5

406
Algebraic Structures
Deﬁnition 8.16
An isomorphism from the group (G, ∗) to the group (G′, ◦) is a bijective
function f : G →G′ such that
f(g1 ∗g2) = f(g1) ◦f(g2)
for all g1, g2 ∈G.
If such a function exists, we say that (G, ∗) is
isomorphic to (G′, ◦) and we write (G, ∗) ∼= (G′, ◦).
An isomorphism from D3 to S3 is deﬁned by f : r0 7→p1, r1 7→p2, r2 7→p3,
m1 7→p4, m2 7→p5, m3 7→p6.
More generally, we might ask whether the group of all permutations of a set with
n elements is isomorphic with the dihedral group of degree n. The answer is no,
because the orders of these groups are not equal for n > 3, so that there does not
exist a bijection Dn →Sn. For the dihedral group of degree n, |Dn| = 2n
whereas |Sn| = n!.
Not every permutation of n elements corresponds to a
symmetry of an n-sided regular polygon.
Examples 8.12
1.
Consider the groups (R, +) and (R+, ×). Show that the function f :
R →R+, where f(x) = 2x, deﬁnes an isomorphism from (R, +) to
(R+, ×).
Solution
We have to show two things:
(a)
that f is a bijection;
(b)
that, if x, y, ∈R, then f(x + y) = f(x) × f(y).
Perhaps the easiest way to conﬁrm that f is a bijection is to plot the graph of
y = f(x) for x ∈R. This is given in ﬁgure 8.6.
Since any horizontal line through the positive part of the y-axis meets the graph
exactly once, by theorem 5.6, the function f is bijective.

Morphisms
407
Figure 8.6
Also
f(x + y) = 2x+y
= 2x × 2y
= f(x) × f(y).
We have shown that f is an isomorphism from (R, +) to (R+, ×).
(Note that f : R →R+ where f(x) = ax is an isomorphism from (R, +) to
(R+, ×) for any a ∈R+, a ̸= 1.)
2.
Show that the groups (Z4, +4) and (Z5 −{[0]}, ×5) are isomorphic.
Solution
The Cayley tables for each of these groups are given below.
+4
[0]
[1]
[2]
[3]
[0]
[0]
[1]
[2]
[3]
[1]
[1]
[2]
[3]
[0]
[2]
[2]
[3]
[0]
[1]
[3]
[3]
[0]
[1]
[2]
×5
[1]
[2]
[3]
[4]
[1]
[1]
[2]
[3]
[4]
[2]
[2]
[4]
[1]
[3]
[3]
[3]
[1]
[4]
[2]
[4]
[4]
[3]
[2]
[1]
Our task is to ﬁnd an isomorphism between the two groups—that is, we must
ﬁnd a bijection f : Z4 →Z5 −{[0]} such that
f(x +4 y) = f(x) ×5 f(y)
for all x, y ∈Z4.

408
Algebraic Structures
Comparison of the Cayley tables and a little trial and error reveals that there are
two bijections which have the required properties. These are:
f : Z4 →Z5 −{[0]}
f([0]) = [1], f([1]) = [2],
f([2]) = [4], f([3]) = [3]
and
g : Z4 →Z5 −{[0]}
g([0]) = [1], g([1]) = [3],
g([2]) = [4], g([3]) = [2].
Both of these functions map the ﬁrst Cayley table to the second and hence both
are isomorphisms from (Z4, +4) to (Z5 −{[0]}, ×5).
These examples illustrate the fact that there may be more than one isomorphism
between isomorphic groups.
However, to establish that two groups are
isomorphic, all that is necessary is to ﬁnd one such function.
Determining whether or not two groups are isomorphic necessarily involves a
certain amount of trial and error and can therefore be time consuming, especially
if the order of the groups is large.
A certain amount of guesswork can be
eliminated by using known properties of isomorphic groups. Some of these are
listed in the theorem below.
Theorem 8.9
If f : G1 →G2 is an isomorphism between the groups (G1, ∗) and (G2, ◦)
then:
(1)
if e is the identity in (G1, ∗), f(e) is the identity in (G2, ◦);
(2)
(G1, ∗) is abelian if and only if (G2, ◦) is abelian;
(3)
if a−1 is the inverse of a in (G1, ∗), then f(a−1) is the inverse of
f(a) in (G2, ◦), i.e. f(a−1) = [f(a)]−1;
(4)
the inverse function f −1 : G2 →G1 deﬁnes an isomorphism from
(G2, ◦) to (G1, ∗);
(5)
if (H1, ∗) is a subgroup of (G1, ∗), then (H2, ◦) where H2 =
{f(a) : a ∈H1} is a subgroup of (G2, ◦) and (H1, ∗) ∼= (H2, ◦);
(6)
(G1, ∗) is cyclic if and only if (G2, ◦) is cyclic;
(7)
if a ∈G1 then |a| = |f(a)|.

Morphisms
409
It is not difﬁcult to prove that the properties listed apply to isomorphic groups and
that if any one of them fails, then the two groups in question are not isomorphic.
Therefore, to show that two groups are not isomorphic we look for a group-
theoretic property which holds for one but not for the other. To show that two
groups are isomorphic, however, it is not enough to show that they have common
properties. We must actually ﬁnd an isomorphism. These points are summarized
in the following ‘isomorphism principle’.
Isomorphism principle
To show that two groups are isomorphic, an isomorphism from one to the
other must be found; to show that two groups are not isomorphic, a group-
theoretic property must be found which one group has but the other does
not.
In attempting to establish whether or not two groups are isomorphic, the
properties listed above can be useful in determining which bijections are potential
isomorphisms and which are not.
Particularly helpful in this respect is the
property concerning orders of elements. Given two groups (G1, ∗) and (G2, ◦)
a useful ﬁrst step in the search for an isomorphism is to write down the orders
of the elements of each. At this stage we can at least see whether a bijective
function with order-preserving properties is possible. If it is not then we can
deduce immediately that the groups are not isomorphic. If it is possible to deﬁne
such a function then, in our search for an isomorphism, some bijections can be
eliminated by the fact that the orders of each element and its image must be
equal. If an order-preserving bijective function exists this is not sufﬁcient for us to
conclude that the groups are isomorphic, since the property f(x∗y) = f(x)◦f(y)
must also be satisﬁed. However, order-preserving bijections are the only possible
candidates for isomorphisms.
Examples 8.13
1.
The group (G, ∗) is deﬁned as in example 8.10. Determine whether this
group is isomorphic with (Z4, +4).

410
Algebraic Structures
Solution
The Cayley table for (Z4, +4) is the following.
+4
[0]
[1]
[2]
[3]
[0]
[0]
[1]
[2]
[3]
[1]
[1]
[2]
[3]
[0]
[2]
[2]
[3]
[0]
[1]
[3]
[3]
[0]
[1]
[2]
For this group the orders of the elements are given by |[0]| = 1, |[1]| = 4,
|[2]| = 2, |[3]| = 4.
In example 8.10 we found that the orders of the elements of (G, ∗) were as
follows: |e| = 1, |a| = 2, |b| = 2, |c| = 2.
We can see immediately that there is no bijection possible where the order of each
element is the same as that of its image. Hence the groups are not isomorphic.
2.
Show
that
the
groups
(Z4, +4)
and
({[1], [3], [7], [9]}, ×10)
are
isomorphic.
Solution
The Cayley table for {[1], [3], [7], [9]} under multiplication modulo 10 is given
below.
×10
[1]
[3]
[7]
[9]
[1]
[1]
[3]
[7]
[9]
[3]
[3]
[9]
[1]
[7]
[7]
[7]
[1]
[9]
[3]
[9]
[9]
[7]
[3]
[1]
We ﬁrst write down the orders of the elements of each group so that we can decide
which bijective functions are possible isomorphisms. We saw in the example
above that, for (Z4, +4), the orders of the elements are: |[0]| = 1, |[1]| = 4,
|[2]| = 2, |[3]| = 4.
For the second group, orders of elements are as follows: |[1]| = 1, |[3]| = 4,
|[7]| = 4, |[9]| = 2.
From these we can deduce that any isomorphism f : Z4 →{[1], [3], [7], [9]} must
be such that f([0]) = [1], and f([2]) = [9]. For the other two elements of Z4

Morphisms
411
there are two possibilities f([1]) = [3] and f([3]) = [7] or f([1]) = [7] and
f([3]) = [2]. Hence there are two isomorphism candidates, the functions f and g
deﬁned below.
f : Z4 →{[1], [3], [7], [9]}
f([0]) = [1], f([1]) = [3],
f([2]) = [9], f([3]) = [7].
f : Z4 →{[1], [3], [7], [9]}
f([0]) = [1], f([1]) = [7],
f([2]) = [9], f([3]) = [3].
In fact each of these functions is an isomorphism but this must be established as
before.
3.
Determine whether the groups D3 and (Z6, +6) are isomorphic.
Solution
Applying theorem 8.9(2) we can say immediately that these groups are not
isomorphic since (Z6, +6) is abelian whereas D3 is not.
Isomorphisms are deﬁned in exactly the same way between monoids and
semigroups.
Indeed, since any group is automatically a monoid and a
semigroup, all of the group isomorphisms considered above could be regarded as
isomorphisms between two monoids or between two semigroups. In the example
below we consider an isomorphism between two monoids which are not groups.
Example 8.14
Show that, if A = {a, b}, then the monoids (P(A), ∪) and (P(A), ∩) are
isomorphic.
Solution
If A = {a, b} then P(A) = {∅, {a}, {b}, {a, b}}.
The Cayley tables for (P(A), ∪) and (P(A), ∩) are given below.
∪
∅
{a}
{b}
{a, b}
∅
∅
{a}
{b}
{a, b}
{a}
{a}
{a}
{a, b}
{a, b}
{b}
{b}
{a, b}
{b}
{a, b}
{a, b}
{a, b}
{a, b}
{a, b}
{a, b}

412
Algebraic Structures
∩
∅
{a}
{b}
{a, b}
∅
∅
∅
∅
∅
{a}
∅
{a}
∅
{a}
{b}
∅
∅
{b}
{b}
{a, b}
∅
{a}
{b}
{a, b}
A bijective function which maps the ﬁrst table to the second is
f : P(A) →P(A)
f(∅) = {a, b}, f({a, b}) = ∅,
f({a}) = {a}, f({b}) = {b}.
Thus the two monoids are isomorphic.
An alternative, and perhaps more natural, isomorphism is given by g(X) = ¯X for
all X ∈P(A).
Morphisms
For two groups (G, ∗) and (G′, ◦) to be isomorphic, we need to be able to deﬁne
a function f : G →G′ which is bijective and which also preserves the structure
of the group. Dropping the bijective condition deﬁnes a more general concept
of structure-preserving function called a ‘morphism’ (or ‘homomorphism’).
Morphisms are deﬁned not only between pairs of groups but between any two
algebraic structures of the types considered in this chapter.
Deﬁnition 8.17
Given two algebraic structures (A, ∗) and (B, ◦), a morphism from (A, ∗)
to (B, ◦) is a function f : A →B such that
f(a1 ∗a2) = f(a1) ◦f(a2)
for all a1, a2 ∈A.
A morphism need not be surjective so that there may be elements of B which are
not the image of any element of A. If it is surjective, a morphism is called an
epimorphism. Similarly a morphism need not be injective so that there may be

Morphisms
413
elements of B which are the image of more than one element of A. An injective
morphism is called a monomorphism. An isomorphism is a morphism which is
both surjective and injective.
What is important about morphisms between two algebraic structures (A, ∗) and
(B, ◦) is that many of the properties of A under the operation ∗are preserved in
the image set f(A) under the operation ◦. In particular, if (A, ∗) is a member
of a particular family of structures then so too is (f(A), ◦). This is stated in the
following theorem the proof of which is left as an exercise (8.5.2).
Theorem 8.10
Let (A, ∗) and (B, ◦) be algebraic structures and let f : A →B be a
morphism.
(a)
If (A, ∗) is a semigroup then so too is (f(A), ◦).
(b)
If (A, ∗) is a monoid then so too is (f(A), ◦).
(c)
If (A, ∗) is a group then so too is (f(A), ◦).
We saw that when there exists an isomorphism between two algebraic structures
(A, ∗) and (B, ◦), these structures could be regarded as ‘essentially the same’.
Dropping the bijective condition means that some of the likenesses between the
two structures are lost.
For a bijective morphism, (B, ◦) can be regarded as
a perfect copy of (A, ∗) whereas if the morphism lacks the bijective property
(f(A), ◦) lacks some of the details of (A, ∗).
The following theorem lists some of the general properties of all morphisms.
Theorem 8.11
Let (A, ∗) and (B, ◦) be algebraic structures and let f : A →B be a
morphism from (A, ∗) to (B, ◦). Then:
(1)
if e is the identity in (A, ∗), f(e) is the identity in (f(A), ◦);
(2)
if (A, ∗) is abelian then (f(A), ◦) is abelian;
(3)
if a−1 is the inverse of a in (A, ∗) then f(a−1) is the inverse of
f(a) in (f(A), ◦), i.e. f(a−1) = [f(a)]−1;
(4)
if (A, ∗) is a cyclic group then so too is (f(A), ◦).

414
Algebraic Structures
The structure (f(A), ◦) is called the morphic image of (A, ∗). If the morphism
is surjective then (f(A), ◦) can be replaced by (B, ◦) in each of the statements
above. An analogy which is sometimes used to illustrate the relationship between
a structure and its morphic image is that between a colour photograph and the
person appearing in it. Some characteristics of the individual can be obtained
from the photograph, for example the colour of their hair. Others, such as their
height and weight, cannot be determined.
Examples 8.15
1.
Consider the groups (Z, +) and ({[0], [1]}, +2). Show that the function
f : Z →{[0], [1]} where
f(x) =
(
[0]
if x is even,
[1]
if x is odd
deﬁnes a morphism from (Z, +) to ({[0], [1]}, +2).
Solution
We must show that, for all a, b ∈Z
f(a + b) = f(a) +2 f(b).
Clearly the values of each side of this equation depend on whether a and b are odd
or even. All the four possible cases are considered in the table below. The last two
columns show that, in each case, the equation above is satisﬁed and hence that f
deﬁnes a morphism from (Z, +) to ({[0], [1]}, +2).
a
b
a + b
f(a)
f(b)
f(a + b)
f(a) +2 f(b)
Even
Even
Even
[0]
[0]
[0]
[0]
Even
Odd
Odd
[0]
[1]
[1]
[1]
Odd
Even
Odd
[1]
[0]
[1]
[1]
Odd
Odd
Even
[1]
[1]
[0]
[0]
Note that f is surjective (and is therefore an epimorphism) but is not injective.
2.
Consider the group (Z, +). Let f : Z →Z be deﬁned by f(x) = 2x.
Show that f is a morphism from (Z, +) to (Z, +).

Morphisms
415
Solution
Here we must show that f(x + y) = f(x) + f(y) for all x, y ∈Z.
We have
f(x + y) = 2(x + y)
= 2x + 2y
= f(x) + f(y).
Hence f is a morphism. The function is injective (i.e. is a monomorphism) but is
not surjective. The image set is the set E of even integers (including zero) and we
can conﬁrm the result of theorem 8.10(c). The set E is a group under the operation
of addition.
The following is an example of a morphism between monoids.
Example 8.16
Consider the alphabet A = {a, b, c} and let (A∗∪{λ}, ∗) be the free monoid
generated by A (see example 8.4.2). A function f : A∗∪{λ} →N is deﬁned by
f(x) = the length of the string x.
Show that f is a morphism from (A∗∪{λ}, ∗) to (N, +).
Solution
Again we must show that
f(x ∗y) = f(x) + f(y)
where x and y are strings and ∗represents concatenation of strings. Since the
number of symbols in two concatenated strings is clearly equal to the sum of the
number of symbols in each string, the equation holds and f deﬁnes a morphism
from (A∗∪{λ}, ∗) to (N, +). This function is surjective (but not injective) and
the morphism is therefore an epimorphism.

416
Algebraic Structures
Exercises 8.5
1.
Prove theorem 8.9.
2.
Prove theorem 8.10.
3.
Let A be the set of all 2 × 2 matrices of the form
µ
1
n
0
1
¶
where n ∈Z.
(In example 8.8.1 we showed that A is a group under matrix
multiplication.) Show that the function f : A →Z deﬁned by
f
·µ
1
n
0
1
¶¸
= n
is an isomorphism from (A, ∗) (where ∗denotes matrix multiplication) to
(Z, +).
4.
Which of the following functions f : R →R deﬁnes a morphism from
(R, +) to (R, +)?
(i)
f(x) = x −3,
(ii)
f(x) = 5x,
(iii)
f(x) = x2,
(iv)
f(x) = x/2,
(v)
f(x) = |x|,
(vi)
f(x) = 2x.
Which morphisms are isomorphisms? (An isomorphism from a group to
itself is called an automorphism.)
5.
Which of the following functions f : R →R −{0} are morphisms from
(R, +) to (R −{0}, ×)? Which morphisms are isomorphisms?
(i)
f(x) = 2x,
(ii)
f(x) = 5,
(iii)
f(x) = 1,
(iv)
f(x) = 3−x.
6.
Prove that, if f : A →B deﬁnes a morphism from the algebraic structure
(A, ∗) to the structure (B, ◦), then f(eA) = eB where eA is the identity in

Morphisms
417
(A, ∗) and eB is the identity in (f(A), ◦) (assuming that these identities
exist).
7.
Let T = {A, B, C, D} where
A =
µ 1
0
0
1
¶
B =
µ
0
1
−1
0
¶
C =
µ
−1
0
0
−1
¶
D =
µ
0
−1
1
0
¶
.
Show that T is a group under matrix multiplication. Show that this group
is isomorphic to (Z5 −{[0]}, ×5).
8.
Let (G, ∗) be a group and let g be a particular element of G. Show that
the function f : G →G, where f(x) = g−1xg, deﬁnes an isomorphism
from (G, ∗) to itself. (This is called an inner automorphism.)
9.
(i)
Let R∗be the set of non-zero real numbers. Show that the function
f : R∗→R∗deﬁned by f(x) = x2 is a morphism from the group
(R∗, ×) to itself.
(ii)
Let (G, ∗) be a group. Show that the function f : G →G deﬁned
by f(x) = x2 is a morphism from (G, ∗) to itself if and only if
(G, ∗) is abelian.
10.
Let (G1, ∗), (G2, ◦) and (G3, ·) be groups.
Let f : G1 →G2 and
g : G2 →G3 deﬁne morphisms from (G1, ∗) to (G2, ◦) and from (G2, ◦)
to (G3, ·) respectively. Show that g ◦f : G1 →G3 deﬁnes a morphism
from (G1, ∗) to (G3, ·).
11.
Let (G, ∗) be a group and let a function f : G →G be deﬁned by
f(x) = x−1 where x−1 denotes the inverse of x with respect to the
operation ∗. Show that f is an isomorphism from (G, ∗) to itself if and
only if G is abelian.
12.
Let A = {a}. Show that:
(i)
the free semigroup generated by A is isomorphic to (Z+, +);
(ii)
the free monoid generated by A is isomorphic to (N, +);
(iii)
the free group generated by A is isomorphic to (Z, +).
13.
Show that the set of matrices T
=
{A, B, C, D} deﬁned in
exercise 8.4.12 is isomorphic to the set of symmetries of a rectangle under
composition of transformations. (It is another manifestation of the Klein
four-group.)

418
Algebraic Structures
14.
Let (G1, ∗) and (G2, ◦) be two groups with identities e1 and e2
respectively. The kernel of a morphism f : G1 →G2, denoted by ker f,
is the set {g ∈G1 : f(g) = e2}, i.e. the set of elements of G1 which map
to the identity in G2.
(a)
For each pair of groups and the morphism given, ﬁnd the kernel of
the morphism:
(i)
(Z, +), (Z, +); f : Z →Z where f(x) = 7x;
(ii)
(R, +), (R, +); f : Z →Z where f(x) = 7x;
(iii)
(R, +), (R, +); f : R →R where f(x) = 0;
(iv)
(R, +), (R+, ×); f : R →R+ where f(x) = 2x;
(v)
(Z, +), (Z6, +6); f : Z →Z6 where f(x) = [x] (the
modulo 6 equivalence class of x);
(vi)
(Z6, +6), (Z6, +6); f : Z6 →Z6 where f([x]) = [2x].
(b)
Show that:
(i)
f is a monomorphism if and only if ker f = {e1};
(ii)
(ker f, ∗) is a subgroup of (G1, ∗);
(iii)
if x ∈ker f and g ∈G1, then g−1xg ∈ker f. (A subgroup
N, which is such that if x ∈N and g ∈G then g−1xg ∈N,
is called a normal subgroup of G.)
8.7
Group Codes
Many of the applications of modern technology involve the communication of
data from one point to another. The two points may be relatively close to each
other, as in the case of data transfer from one memory location to another in
a computer.
On the other hand, telecommunication via satellite involves the
transmission of data over many thousands of miles. In either case, however, the
essential features of the system are the same. There is a communication channel
along which data are transmitted where, ideally, the data received at one end of
the channel are identical to those sent at the other.
For our purposes we shall assume that all relevant data can be represented by a
string of digits each of which is either zero or one, i.e. a word over the alphabet
{0, 1}. We shall refer to such words as binary words and their digits as bits.
However much we would like our transmission system to be completely reliable,

Group Codes
419
it is inevitable that faults will develop from time to time and that there will be
interference (known as ‘noise’) from external sources. These may cause an error
in transmission so that a received word is different from that transmitted. It is
important therefore to be able to detect when a received word is in error and,
if possible, to determine what was the word actually sent. If the latter is not
possible, then at least the detection of an error could lead to a request for the data
to be retransmitted.
We shall make the following assumptions about transmission of errors.
(a)
They take the form of the conversion of 1 to 0 or 0 to 1 in one or more of
the bits which comprise the transmitted word.
(b)
The conversions of 1 to 0 and 0 to 1 are equally likely.
(c)
Errors in individual bits occur independently of each other.
(d)
An error is equally likely in each of the bits which comprise the
transmitted word.
(e)
For n < m, n errors are more likely than m errors so that for an
incorrectly transmitted word, the most likely number of errors is one.
We now consider some examples to illustrate the essential features of error
detection and correction.
Suppose that words to be transmitted through a communication channel are all
the members of the set of binary words of length 3. We denote this set B3, i.e.
B3 = {000, 001, 010, 100, 110, 101, 011, 111}. Suppose that the word 010 is
transmitted and that an error occurs in the third digit so that the word received
is 011. There is no way of detecting this error because 011 is a member of the
set of words which we might expect to receive. Further, if we cannot detect an
error, there is certainly no chance of our correcting it. This example highlights
one property which is essential if we are to detect errors at all—an incorrectly
transmitted word must not be a member of the set of words which we are
expecting to receive.
The words in the example above are in a sense ‘too close together’. Any error
results in another member of the set. Suppose instead that words for transmission
are members of the set {111, 100, 001, 010}. In this case an error in the third digit
of the transmitted word 010 will be detected because when we receive 011 we
know that this could not have been transmitted. However, even though we know
that an error has occurred, we cannot determine where it is. On the assumption
that one error is the most probable, the word transmitted is equally likely to be
111, 001 or 010. We can detect the single error but we cannot correct it. Note
that two errors cannot be detected because errors in any two digits would result in
another member of the set.
The members of the set {111, 100, 001, 010} are still too close together for even

420
Algebraic Structures
a single error to be corrected. Suppose that we transmit only members of the set
{000, 111}. Now errors in one or two digits can be detected and, if a single error
occurs, we can correct it. For example, if we receive 011 we will assume that the
word closest to it, 111, was transmitted. However, two errors cannot be corrected.
If 000 undergoes two transmission errors and is received as 011, we shall, on the
assumption of one error, incorrectly assume that the word sent was 111. For this
set of words, we can detect two errors but we cannot correct them.
The examples above illustrate that error detection is easier than error correction.
However, both depend upon the words in the set of possible transmitted words
being sufﬁciently different from one another. The following deﬁnition gives a
means by which we can measure the difference between two individual words.
Deﬁnition 8.18
Let x and y be binary words of length n. The distance (or Hamming
distance†) between x and y, denoted by d(x, y), is the number of digits in
which x and y differ.
For example, if x = 001101 and y = 111110, the two words have different ﬁrst,
second, ﬁfth and sixth digits. Hence d(x, y) = 4.
It is easy enough to show that the distance has the following properties for all
binary words x, y and z of length n:
(a)
d(x, y) ⩾0,
(b)
d(x, y) = 0 if and only if x = y,
(c)
d(x, y) = d(y, x),
(d)
d(x, z) ⩽d(x, y) + d(y, z).
(Any function d : X × X →R+ ∪{0} having these properties is called a metric
on the set X. Hence distance is a metric on the set Bn.)
For successful error detection and correction it is desirable that the distance
between individual words in the set of possible transmitted words be as large
as possible.
† Named after Richard Hamming who pioneered the ﬁeld of error detection and correction in
transmitted data in the 1950s.

Group Codes
421
In practice the detection and correction of errors are carried out by coding words
before transmission. Generally this involves adding one or more bits to the end
of the word. These are called check digits and they act as checks on the validity
of some or all of the digits of the received word. Thus any transmitted binary
word of length n consists of m digits called information bits which carry the
information to be sent and r = n −m check digits which provide the means
for detecting and correcting errors. Denoting by Bn the set of all binary words
of length n, we can view the coding mechanism as a function E : Bm →Bn.
Such a function is called an encoding function and the members of its image set
are called codewords. Since each codeword must correspond to a unique word
Bm, an encoding function must be injective. For each encoding function there is a
decoding function D : Bn →Bm∪{‘error’} which maps a codeword y ∈Bn to
x ∈Bm, where E(x) = y. Since m < n the set of codewords is a proper subset
of Bn so that there are elements of the domain of D which are not codewords. If
a word w′ is received which falls into this category then D(w′) = D(w) where
w is the codeword ‘nearest to’ w′ in the sense that it differs from w′ in the fewest
digits. This is called ‘nearest-neighbour decoding’. If the ‘nearest neighbour’ is
not unique, then D(w′) = ‘error’. If the data cannot be retransmitted, then one
of the set of nearest neighbours may be chosen arbitrarily and decoded.
A coding/decoding procedure which consists of an encoding function E : Bm →
Bn and a decoding function D : Bn →Bm∪{‘error’} is called an (m, n) block
code. Such a code is said to be systematic if, given x ∈Bm, the ﬁrst m digits of
E(x) are, in the same order, those of x itself. In this brief introduction to coding
theory we shall restrict our discussion to systematic block codes.

422
Algebraic Structures
The simplest encoding function involves the addition of a single digit to the end
of a word where the digit is chosen to make the number of ones in the codeword
even. Such a code is an (m, m + 1) systematic block code and it is called an even
parity check code. The encoding function E : Bm →Bm+1 is such that, for
example, if m = 4, E(0011) = 00110 and E(1000) = 10001. Odd parity check
codes can also be used. For an even parity check code using one check digit, one
error can be detected because it will result in an odd number of ones. However,
the error cannot be corrected because it is not possible to tell which digit is at
fault.
A code in which any combination of k or fewer errors can be detected is said to
be k-error detecting and a code in which any combination of k or fewer errors
can be corrected is called k-error correcting. Even and odd parity check codes
are 1-error detecting and 0-error correcting.
We have seen how the ability to detect and correct errors is dependent upon
the distance between codewords. For codes involving check digits, the distance
between each pair of codewords is not necessarily the same so that the factor
determining the error-detecting and error-correcting capabilities of the code is
the minimum of all the distances between pairs of codewords. The minimum
distance of a code is deﬁned to be the minimum of all the distances between
distinct pairs of codewords.
The following two theorems give criteria for determining the capability for error
detection in a code.
Theorem 8.12
A code is k-error detecting if and only if the minimum distance is at least
k + 1.
Proof
Any number of errors in a codeword can be detected so long as they do not result
in another codeword. If the minimum distance between codewords is k + 1, then
any number of errors fewer than k + 1 will not result in a codeword and so will
be detected. Hence k or fewer errors can be detected and so the code is k-error
detecting. A similar line of argument establishes the converse.
□

Group Codes
423
Theorem 8.13
A code is k-error correcting if and only if the minimum distance is at least
2k + 1.
Proof
We ﬁrst prove that, if a code is k-error correcting, then the minimum distance
between any pair of codewords is at least 2k + 1. We use proof by contradiction:
we assume that the code is k-error correcting and that there exists a pair of words
whose distance is less than 2k + 1. Denoting these by w and x, we have
d(w, x) ⩽2k.
However, since the code is k-error correcting, we can certainly detect k errors, so,
by the last theorem,
d(w, x) ⩾k + 1.
So we have
k + 1 ⩽d(w, x) ⩽2k
i.e. w and x differ in at least k + 1 digits and at most 2k digits.
Now suppose that w undergoes k transmission errors and is received as w′ so that
d(w, w′) = k. Suppose also that each of these k errors occurs in one of the digits
in which w and x differ. This means that w′ and x differ by at most 2k −k = k
digits, so
d(w′, x) ⩽k
and w′ is at least as close to x as it is to w. Hence w′ cannot be correctly decoded.
Thus the code is not k-error correcting and we have a contradiction.
We now prove that, if the minimum distance is 2k + 1, then the code is k-error
correcting. Suppose that a codeword w is transmitted with k or fewer errors and
is received as w′. For any other codeword x we have
d(w, w′) + d(w′, x) ⩾d(w, x)
so
d(w′, x) ⩾d(w, x) −d(w, w′).
But
d(w, x) ⩾2k + 1
and
d(w, w′) ⩽k

424
Algebraic Structures
so that
d(w′, x) ⩾(2k + 1) −(k)
and hence
d(w′, x) ⩾k + 1.
From this we can conclude that the distance between the received word w′ and
any other codeword is greater than the distance between w′ and w so that w′ will
be correctly decoded as w.
□
Example 8.17
Consider the encoding function E : B2 →B6 deﬁned as follows:
E(00) = 001000
E(01) = 010100
E(10) = 100010
E(11) = 110001.
Find how many errors the code can detect and correct.
Solution
The distances between pairs of codewords are given below:
d(001000, 010100) = 3
d(001000, 100010) = 3
d(001000, 110001) = 4
d(010100, 100010) = 4
d(010100, 110001) = 3
d(100010, 110001) = 3.
The minimum distance is three and so the code is k-error detecting where
k + 1 = 3. The code is therefore 2-error detecting.
The code is k-error correcting where 2k + 1 = 3. This gives k = 1 and so only
one error can be corrected.
In order to appreciate the importance of groups in coding theory we need to deﬁne
a binary operation on the set of n bit words, Bn. We do this as follows.
Deﬁnition 8.19
Let x and y be codewords of length n such that the ith digit of x is xi and
the ith digit of y is yi. The sum of x and y, denoted x ⊕y, is the n bit
word whose ith digit is xi +2 yi, where +2 denotes addition modulo 2.

Group Codes
425
Thus the sum of two codewords is obtained by applying modulo 2 addition to
corresponding bits. For example
1011001 ⊕1000111 = 0011110
and
111001 ⊕110011 = 001010.
Note that the modulo 2 sum of two bits is 0 if the bits are the same and 1 if they
are different. The distance between two words x and y could therefore be deﬁned
as the number of ones in x ⊕y.
Deﬁnition 8.20
The weight of a word x, denoted by w(x), is the number of ones which it
contains.
For example, w(101101) = 4 and w(011110111) = 7.
The distance between two n bit binary words x and y is given by
d(x, y) = w(x ⊕y).
A code for which the set of codewords is a group under the operation ⊕is called
a group code. It is a simple matter to show that the set Bn of all n bit binary
words is a group under this operation (see exercise 8.6.1). However, this is not a
particularly useful set of codewords because, as we saw earlier, the words are too
close together for any error detection to be possible. We have shown that the error-
detecting and error-correcting capabilities of a code can be determined from the
minimum distance between codewords. For an arbitrary code, determining this
minimum distance involves comparing the distance between all possible pairs of
codewords—a daunting prospect if the number of codewords is large! For a group
code, however, we can show that the minimum distance is equal to the minimum
weight of all non-zero codewords.
Theorem 8.14
The minimum distance of a group code is the minimum weight of all non-
zero codewords.

426
Algebraic Structures
Proof
Let n be the minimum weight of all non-zero codewords so that there exists a
codeword z such that w(z) = n and, for any other codeword x, w(x) ⩾n.
Suppose that d is the minimum distance of the code so that there exist distinct
codewords v and w such that
d(v, w) = d
that is,
w(v ⊕w) = d.
Now v ⊕w is also a codeword by the closure property of the binary operation ⊕
and we therefore have
w(v ⊕w) ⩾n
so that
d ⩾n.
Denoting by 0 the word whose bits are all zero, we have
x ⊕x = 0
for any codeword x. Now 0 is the identity under ⊕and so is a codeword.
Therefore
d(0, z) ⩾d.
But
d(0, z) = w(z)
= n
so that
n ⩾d.
We have shown that d ⩾n and also that n ⩾d, and we can therefore conclude
that n = d.
□
An encoding function E : Bm →Bn (n > m) which encodes a word by
appending check digits can most easily be described using an m × n matrix
G whose entries are zeros and ones. Such a matrix is known as a generator
matrix for the code. To encode an m bit word, we view that word as a 1 × m
matrix and post-multiply this row matrix by the matrix G with all additions and
multiplications carried out modulo 2. For a systematic code we require that the
ﬁrst m bits of the codeword are the same as the m bits of the word to be encoded.
In this case it is necessary that the ﬁrst m columns of G constitute the identity
matrix Im.

Group Codes
427
Example 8.18
Consider the generator matrix
G =


1
0
0
1
0
1
0
1
0
1
1
0
0
0
1
0
1
1

.
An encoding function E : B3 →B6 is deﬁned by E(x) = xG for any x ∈B3.
For instance
E(011) =
¡
0
1
1
¢


1
0
0
1
0
1
0
1
0
1
1
0
0
0
1
0
1
1

=
¡
0
1
1
1
0
1
¢
so that 011 is encoded as 011101. As another example, consider
E(100) =
¡
1
0
0
¢


1
0
0
1
0
1
0
1
0
1
1
0
0
0
1
0
1
1

=
¡
1
0
0
1
0
1
¢
.
In general, for any three-bit word with digits x1, x2 and x3
E(x1x2x3) =
¡
x1
x2
x3
¢


1
0
0
1
0
1
0
1
0
1
1
0
0
0
1
0
1
1


=
¡
x1
x2
x3
x1 +2 x2
x2 +2 x3
x1 +2 x3
¢
.
If x1x2x3 encodes as w1w2w3w4w5w6 we have
w1 = x1
w2 = x2
w3 = x3
w4 = x1 +2 x2
w5 = x2 +2 x3
w6 = x1 +2 x3
so that
w4 = w1 +2 w2
w5 = w2 +2 w3
w6 = w1 +2 w3.
Thus the last three digits of the codeword act as parity checks on different pairs of
information bits. An error in any one of the six bits of a codeword will uniquely
determine which of these three equations is not satisﬁed. For example, for an
error in the fourth bit, the ﬁrst equation alone will not hold; an error in the ﬁrst bit
will result in the ﬁrst and third equations not holding.
Since 0 +2 0 = 1 +2 1 = 0, the three equations above can be written
w1 +2 w2
+2 w4
= 0
w2 +2 w3
+2 w5
= 0
w1
+2 w3
+2 w6 = 0

428
Algebraic Structures
which is equivalent to the matrix equation


1
1
0
1
0
0
0
1
1
0
1
0
1
0
1
0
0
1










w1
w2
w3
w4
w5
w6








=


0
0
0

.
The matrix
H =


1
1
0
1
0
0
0
1
1
0
1
0
1
0
1
0
0
1


is called a parity check matrix. For any (correctly transmitted) codeword w the
equation
HwT =


0
0
0


is always satisﬁed.
Notice the relationship between the generator matrix G and H, the parity check
matrix. The matrix G can be regarded as the partitioned matrix (I3 F) where
F =


1
0
1
1
1
0
0
1
1

.
The matrix H is the partitioned matrix (F T I3) where F T denotes the transpose
of F.
We generalize this result in the following theorem.
Theorem 8.15
Let G be an m × n generator matrix such that G = (Im F) where F is an
(m×r) matrix and m = n−r. Let an encoding function E : Bm →Bn be
deﬁned by E(x) = xG for any x ∈Bm. Then for any codeword w ∈Bn,
HwT = Or×1
where
H = (F T Ir).

Group Codes
429
Proof
If w is a codeword, then w = xG for some x ∈Bm.
H(xG)T = H(GTxT)
(see exercise 6.2.11 (iii))
= (HGT)xT
= (F T Ir)
Ã
Im
F T
!
xT
= (F T +2 F T)xT
(see exercise 6.4.9)
= Or×mxT
= Or×1.
■
The converse of this theorem also holds (see exercise 8.6.9).
Example 8.19
The generator matrix
G =




1
0
0
0
1
1
1
0
1
0
0
0
1
1
0
0
1
0
1
1
0
0
0
0
1
0
0
1




deﬁnes an encoding function E : B4 →B7. For example,
E(1101) =
¡ 1
1
0
1 ¢




1
0
0
0
1
1
1
0
1
0
0
0
1
1
0
0
1
0
1
1
0
0
0
0
1
0
0
1




=
¡
1
1
0
1
1
0
1
¢
.
Now G = (I4 F) where
F =




1
1
1
0
1
1
1
1
0
0
0
1




so that the parity check matrix corresponding to G is given by
H = (F T I3)
=


1
0
1
0
1
0
0
1
1
1
0
0
1
0
1
1
0
1
0
0
1

.

430
Algebraic Structures
For the codeword 1101101 we have
H( 1
1
0
1
1
0
1 )T =


1
0
1
0
1
0
0
1
1
1
0
0
1
0
1
1
0
1
0
0
1












1
1
0
1
1
0
1










=


0
0
0


as expected.
We have seen that for any positive integer n, the set of all elements of Bn is a
group under bit-wise addition modulo 2 with identity O1×n. We now show that
the set of all codewords where the encoding function is deﬁned by a generator
matrix is a group under this operation.
Theorem 8.16
Let E : Bm →Bn be an encoding function such that E(x) = xG where
G is a generator matrix. Then the set of codewords E(Bm) is a group
under bit-wise addition modulo 2.
Proof
Given x1, x2 ∈Bm, we have
E(x1 ⊕x2) = (x1 ⊕x2)G
= x1G ⊕x2G
= E(x1) ⊕E(x2).
Therefore E is a morphism from (Bm, ⊕) to (Bn, ⊕). Since (Bm, ⊕) is a group,
we can conclude that (E(Bm), ⊕) is a group (see theorem 8.10). (It is in fact a
subgroup of (Bn, ⊕).)
□
We noted earlier that the error-detecting and error-correcting capabilities of a code
depend on the minimum distance of the code. For a group code, the minimum

Group Codes
431
distance is the minimum weight of a non-zero codeword. We now use these
results to show how to tell from the parity check matrix how many errors can
be detected or corrected.
Suppose that H is an r × n matrix with columns denoted by h1, h2, . . . , hn, and
suppose that for k of these columns the sum of the elements in the corresponding
rows is zero. We will denote these columns by hi1, hi2, . . . , hik. Now the n digit
word w which has ones as its i1, i2, . . . , ik digits and zeros elsewhere must be
such that HwT = 0 and so is a codeword. To illustrate why this is so, consider
the following matrix H where
H =


h11
h12
h13
h14
h15
h21
h22
h23
h24
h25
h31
h32
h33
h34
h35

.
Suppose that the ﬁrst, third and fourth columns of H sum to zero so that
h11 +2 h13 +2 h14 = 0
h21 +2 h23 +2 h24 = 0
h31 +2 h33 +2 h34 = 0.
Now consider a word w with digits w1, w2, w3, w4, w5 where w1 = w3 = w4 =
1 and w2 = w5 = 0. Then
HwT =


h11w1 +2 h12w2 +2 h13w3 +2 h14w4 +2 h15w5
h21w1 +2 h22w2 +2 h23w3 +2 h24w4 +2 h25w5
h31w1 +2 h32w2 +2 h33w3 +2 h34w4 +2 h35w5


=


h11w1 +2 h13w3 +2 h14w4
h21w1 +2 h23w3 +2 h24w4
h31w1 +2 h33w3 +2 h34w4


since w2 = w5 = 0
=


h11 +2 h13 +2 h14
h21 +2 h23 +2 h24
h31 +2 h33 +2 h34


since w1 = w3 = w4 = 1
=


0
0
0

.
Hence 10110 is a codeword.
The converse is also true—if a codeword has ones only in its i1, i2, . . . , ik
positions then i1, i2, . . . , ik columns of H must sum to zero.
This result enables us to determine the minimum weight of a code deﬁned by a
generator matrix G or, equivalently, by a parity check matrix H. It is simply the

432
Algebraic Structures
minimum number of columns of H which sum to zero. Since such a code is a
group code, the minimum weight is equal to the minimum distance and from this
we can determine the error-detecting and error-correcting capabilities of the code.
Example 8.20
Suppose a group code is deﬁned by the encoding function E : B4 →B7 where
E(x) = xG for any x ∈B4 and G is the generator matrix
G =




1
0
0
0
1
1
1
0
1
0
0
0
1
1
0
0
1
0
1
1
0
0
0
0
1
1
0
1



.
The parity check matrix corresponding to G is
H =


1
0
1
1
1
0
0
1
1
1
0
0
1
0
1
1
0
1
0
0
1

.
To ﬁnd the minimum weight of the code we must ﬁnd the minimum number of
columns of H which sum to zero. For two columns to sum to zero the entries
in each must be identical. Since there is no column of zeros (in which case the
minimum weight of the code would be one) and no identical columns (from which
we would conclude that the minimum weight was two), the minimum weight is
at least three. That it is three can be conﬁrmed by adding columns 1, 2 and 5,
although these are not the only three columns which sum to zero. The minimum
weight of this code is three and hence the minimum distance between codewords
is three.
From this we deduce that the code is 2-error detecting and 1-error
correcting.
Given a systematic code deﬁned by a parity check (or generator) matrix, decoding
a received word w involves the calculation of HwT. This quantity is called
the syndrome of w. If the syndrome has entries which are all zero, then we
may reasonably conclude that the word was correctly transmitted and decoding
involves the selection of the ﬁrst m information bits.
What if the syndrome has one or more entries which are not zero? In this case
we know that at least one transmission error has occurred. Suppose that there
is one error and it is in the ith digit. Denoting the received word by wr and the
transmitted word by wt, these two words differ in that wr is wt with 1 added
(modulo 2, of course) to its ith digit. If we deﬁne e to be the binary word with all
digits zero except for the ith, then e is called the error pattern and wr = wt ⊕e.

Group Codes
433
Deﬁnition 8.21
If an n bit word wt is transmitted and an n bit word wr is received, the error
pattern is the binary word e with digits e1, e2, . . . , en where
ei =
(
0
if the ith digits of wr and wt are the same
1
if the ith digits of wr and wt are different.
For a transmission error in only the ith digit of wt, the syndrome HwrT is given
by
Hwr
T = H(wt ⊕e)T
= H(wt
T ⊕eT)
= Hwt
T ⊕HeT
= HeT
(since wt is a codeword)
= ith column of H.
So for a single error the syndrome tells us exactly which digit is in error.
Example 8.21
Suppose we have an encoding function E : B3 →B6 with parity check matrix
given by
H =


1
1
0
1
0
0
1
1
1
0
1
0
1
0
1
0
0
1

.
Suppose that a word 100001 is received. What is most likely to be the word which
was transmitted?
Solution
We ﬁrst compute the syndrome
HwT =


1
1
0
1
0
0
1
1
1
0
1
0
1
0
1
0
0
1










1
0
0
0
0
1








=


1
1
0

.

434
Algebraic Structures
Since HwT is not zero, 100001 is not a codeword and could not have been
transmitted. The result is in fact the second column of H from which we conclude
that there is an error in the second digit of the received word and that 110001 was
actually transmitted. This is then decoded as 110.
Suppose that the syndrome is neither zero nor a column of H. In this case we
conclude that there is an error in more than one digit and for a single-error-
correcting code the received word cannot be decoded reliably.
In this brief introduction to coding theory we have been concerned mainly
with codes which are single error correcting.
These are important because,
for an incorrectly transmitted word, one error is more likely than several.
Where the possibility of multiple errors is not small (e.g. in transmission from
a spacecraft) codes which have more sophisticated error-detecting and error-
correcting capabilities are used. These also involve theory from the realm of
abstract algebra.
Exercises 8.6
1.
Show that the set Bn is a group under operation ⊕of bit-wise addition
modulo 2.
2.
An encoding function E : B3 →B9 is deﬁned by E(x1x2x3)
=
x1x2x3x1x2x3x1x2x3. (This is an example of a triple-repetition block
code.) What is the generator matrix for this code? What is the maximum
number of errors which the code will (i) detect and (ii) correct?
3.
An encoding function E : B3 →B6 is deﬁned by the generator matrix
G =


1
0
0
1
0
0
0
1
0
1
1
0
0
0
1
0
1
1


so that E(x) = xG for any x ∈B3.
Find a parity check matrix. The words listed below are received at the end
of a communication channel. For each one calculate the syndrome and
indicate whether the word is likely to have been correctly transmitted:
(i) 111001
(ii) 101011
(iii) 001011
(iv) 101101
(v) 011111.

Group Codes
435
4.
An encoding function E : B2 →B5 is deﬁned as follows:
E(00) = 00000
E(01) = 01111
E(10) = 10101
E(11) = 11010.
Show that this is a group code. Find the minimum distance of the code
and hence the maximum number of errors which the code can (i) detect
and (ii) correct.
5.
A parity check matrix for a systematic (m, n) block code is given by
H =


1
0
1
1
1
0
0
1
1
1
0
0
1
0
0
1
1
1
0
0
1

.
What are the values of m and n?
Find the corresponding generator
matrix.
6.
Find a generator matrix for the (m, m + 1) even parity check code. What
is the corresponding parity check matrix for this code?
7.
Given the parity check matrix
H =


1
1
0
1
0
0
0
1
1
0
1
0
1
0
1
0
0
1


calculate the syndrome for each of the following received words and
indicate which are likely to have been correctly transmitted. For those
which contain an error, ﬁnd, if possible, the word which was likely to
have been transmitted:
(i) 011001
(ii) 111000
(iii) 001100
(iv) 111110.
8.
Consider an encoding function E : Bm →Bn deﬁned by an m × n
generator matrix G. Let r = n−m be the number of check digits in each
codeword. Show that, for ﬁxed r, the largest number of information bits
for a single-error-correcting code is 2r −r −1.
For given r (r ∈Z+, r > 1) a (2r −r −1, 2r −1) single-error-correcting
block code is called a Hamming code. Write down parity check matrices
for the (1, 3) and (4, 7) Hamming codes. (Note that these matrices are
unique only up to a reordering of the ﬁrst 2r −r −1 columns.)
9.
State the converse of theorem 8.15. (The converse is true but the proof is
beyond the scope of this book.)

Chapter 9
Introduction to Number Theory
Number theory is concerned with the properties of the integers; it is sometimes
referred to simply as ‘arithmetic’. As such, it deals with some of the most familiar
mathematical objects. Despite this, number theory is certainly not trivial. There
are many deep and beautiful results in number theory. Gauss was said to have
described mathematics as the ‘Queen of the Sciences’ and number theory as the
‘Queen of Mathematics’.
Number theory has sometimes been thought of as an archetypal branch of pure
mathematics—very interesting in its own right, but with little practical use
outside mathematics.
Indeed, G H Hardy who was a number theorist and a
leading mathematician of the ﬁrst half of the 20th century, wrote in his book,
A Mathematician’s Apology, ‘I have never done anything ‘useful’. No discovery
of mine has made, or is likely to make, directly or indirectly, for good or ill, the
least difference to the amenity of the world.’
However, results in elementary number theory have been used to devise
encryption algorithms that are very secure. The particular ‘public key’ properties
of these algorithms make them ideal for encryption of data sent via the internet
and, in so doing, enable e-commerce to be viable.
Hardy also claimed that
pure mathematics does no harm to the world: ‘no one has yet discovered any
warlike purpose to be served by the theory of numbers ...’. But with the arrival
of encryption systems based on number theory, the factorisation of some large
positive integers is now considered a military secret.
The point here is that
applications of even the ‘purest’ part of mathematics can arise in unexpected
places.
We will consider public key encryption algorithms based on number
theory in section 9.5.
436

Divisibility
437
9.1
Divisibility
We begin with the simple process of dividing one integer by another. Early on
in school mathematics, we learn to ‘divide and take remainders’. For example,
we learn something that we might phrase as ‘23 divided by 5 goes 4 times with a
remainder of 3’. This is simply saying that
23 = 4 × 5 + 3.
Here we say that dividing 23 by 5 gives quotient 4 and remainder 3.
We can do this for any positive integers a and b, a result known as the Division
Algorithm .
Theorem 9.1 (The Division Algorithm)
Let a, b ∈Z+. Then there exist unique q, r ∈N such that
a = qb + r
and
0 ⩽r < b.
Here q is called the quotient and r is called the remainder.
Proof
We ﬁrst prove the existence of the positive integers q and r. If a < b then we may
write a = 0×b+a where 0 ⩽a < b and the result follows with q = 0 and r = a.
Now suppose that a ⩾b.
Consider the set
S = {a −nb : n ∈Z+} = {a −b, a −2b, a −3b, . . .}.
In the case above where a = 23 and b = 5 the set S is
S = {18, 13, 8, 3, −2, −7, . . .}.
It is clear that S always contains some non-negative integers; for example, the
ﬁrst listed element a −b ⩾0. Therefore S contains a least non-negative element
which is of the form a−qb for some q ∈Z+. Call this least non-negative element
r. Then r = a −qb so a = qb + r where r ⩾0 and q, r ∈N.

438
Introduction to Number Theory
We are not quite ﬁnished with the existence part as we still need to show that
r < b. Suppose that r ⩾b. Then a −(q + 1)b < a −qb = r and
a −(q + 1)b = a −qb −b = r −b ⩾0.
Hence a −(q + 1)b belongs to S, is non-negative and is smaller than r,
contradicting the fact that r was the least non-negative element of S. Therefore
r < b, as required. This establishes the existence of q, r ∈N.
Now we turn to the uniqueness of q and r. Suppose that
a = qb + r
and
0 ⩽r < b
and
a = q′b + r′
and
0 ⩽r′ < b.
Subtracting gives
0 = (q −q′)b + (r −r′)
so
r −r′ = (q′ −q)b.
Suppose that q′ ̸= q. Then without loss of generality we may suppose that q′ > q.
Then we have
r −r′ = (q′ −q)b > b.
This is impossible since both r and r′ lie between 0 and b −1 inclusive so the
greatest value of their difference is b−1. Hence their difference cannot be greater
than b. Therefore q′ = q from which it also follows that r′ = r. Hence q and r
are unique.
□
Examples 9.1
1.
With a = 37 and b = 8 we have 37 = 4 × 8 + 5 so q = 4 and r = 5.
Although we would not use this to ﬁnd the quotient and remainder, the
set S used in the proof of the Division Algorithm is
S = {37 −8n : n ∈Z+} = {29, 21, 13, 5, −3, −11, . . .}.
The smallest non-negative value in this set is the 4th element 5 =
37 −4 × 8, so q = 4 and r = 5.
2.
In fact the division algorithm is also valid when the integer a is negative
but in this case the quotient q will also be negative. For example, if

Divisibility
439
a = −37 and b = 5 we have −37 = −8 × 5 + 3 so q = −8 and
r = 3.
When the remainder is zero in theorem 9.1, we say that b is a divisor or factor
of a. Thus, for example, the divisors of 36 are: 1, 2, 3, 4, 6, 9, 12, 18 and 36. In
general, ﬁnding all the divisors of a large number is very time consuming and
somewhat tedious.
Deﬁnition 9.1
Let a, b ∈Z+. If a = qb for some q ∈Z+ (that is, if r = 0 in theorem
9.1) then we say that a is a multiple of b. In this case we also say that b is
a factor of a or b is a divisor of a. We also say that b divides a, which is
denoted b|a.
In chapter 4, we claimed that the divisibility relation is a partial order on Z+.
See exercise 4.5.1 on page 195. This is captured in the second part of theorem
9.2 below.
The ﬁrst part says that divisibility is ‘preserved’ by taking linear
combinations. The proof of the ﬁrst part is left as an exercise; for the proof of
the second part, see the solution to exercise 4.5.1 on page 723.
Theorem 9.2
Let a, b, c ∈Z+.
(a)
If c|a and c|b then c|(ma + nb) for all m, n ∈Z+.
(b)
The divisibility relation on Z+ satisﬁes the following conditions.
Reﬂexive:
a|a for all a ∈Z+.
Anti-symmetric:
For all a, b ∈Z+, if a|b and b|a then a = b.
Transitive:
For all a, b, c ∈Z+, if a|b and b|c then a|c.
The notion of the greatest common divisor of two positive integers is probably a
familiar one. For example, consider 36 and 30. Now 36 has divisors 1, 2, 3, 4, 6,
12, 18 and 36 and 30 has divisors 1, 2, 3, 5, 6, 10, 15 and 30. Hence the divisors
they have in common are 1, 2, 3 and 6 so their greatest common divisor is 6.

440
Introduction to Number Theory
Deﬁnitions 9.2
Let a, b, c, d ∈Z+.
If d|a and d|b then we say that d is a common divisor (or common factor)
of a and b.
If d is a common divisor of a and b and, for all other common divisors c,
c ⩽d, then d is called the greatest common divisor (or greatest common
factor) of a and b, written d = gcd(a, b).
Example 9.2
Let a = 24 and b = 44. Then a has divisors 1, 2, 3, 4, 6, 8, 12, 24 and b has
divisors 1, 2, 4, 11, 22, 44 so gcd(24, 44) = 4.
For large integers a and b, ﬁnding their greatest common divisor by listing
the divisors of each and selecting the largest is time consuming and tedious.
Fortunately, there is a more efﬁcient method, called Euclid’s algorithm which
is based on the following simple result.
Theorem 9.3
Let a, b ∈Z+. If a = qb + r where q, r ∈N and 0 ⩽r < b (as in the
Division Algorithm) then gcd(a, b) = gcd(b, r).
Proof
By theorem 9.2 (a), any common divisor of b and r also divides a = qb + r.
Similarly, any common divisor of a and b also divides r = a −qb. Therefore the
pair a, b and the pair b, r have exactly the same common divisors and therefore
these pairs have the same greatest common divisor.
□
Before we describe Euclid’s greatest common divisor algorithm in general we
work through an example to show how we can apply the Division Algorithm
repeatedly to ﬁnd the greatest common divisor of two integers.

Divisibility
441
Example 9.3
We will ﬁnd the greatest common divisor of 2016 and 273 by repeated application
of the division algorithm (theorem 9.1) together with theorem 9.3.
Applying the division algorithm to a = 2016 and b = 273 gives
2016 = 7 × 273 + 105.
By theorem 9.3, gcd(2016, 273) = gcd(273, 105) so applying the division
algorithm once has considerably reduced the size of the numbers involved.
Next we apply the division algorithm to 273 and the remainder found at the
previous stage, 105:
273 = 2 × 105 + 63.
Hence, applying theorem 9.3 again,
gcd(2016, 273) = gcd(273, 105) = gcd(105, 63).
Applying the division algorithm again with 105 and the remainder found at the
previous stage, 63:
105 = 1 × 63 + 42
so
gcd(2016, 273) = gcd(273, 105) = gcd(105, 63) = gcd(63, 42).
Continuing in the same way repeatedly applying the division algorithm we soon
obtain a zero remainder:
63 = 1 × 42 + 21
42 = 2 × 21 + 0
From theorem 9.3,
gcd(2016, 273) = gcd(273, 105)
= gcd(105, 63)
= gcd(63, 42)
= gcd(42, 21) = 21.

442
Introduction to Number Theory
In the previous example, we repeatedly applied the division algorithm where the
numbers ‘a’ and ‘b’ at one stage are replaced by ‘b’and the remainder ‘r’ at the
next stage. We continue until a remainder of zero is obtained. (It is clear that the
sequence of remainders must ‘reach’ zero since each remainder is non-negative
and strictly less than its predecessor: rk ⩾0 and rk < rk−1.) Then the greatest
common divisor of the original pair of numbers is the last non-zero remainder.
This is Euclid’s greatest common divisor algorithm, which we summarise in the
following theorem. Figure 9.1 describes Euclid’s algorithm using a ﬂowchart.
Theorem 9.4 (Euclid’s greatest common divisor algorithm)
Let a, b ∈Z+ and suppose b < a. Repeatedly apply the division algorithm
as follows until rn = 0:
a
=
q1b + r1
where
0 ⩽r1 < b
b
=
q2r1 + r2
where
0 ⩽r2 < r1
r1
=
q3r2 + r3
where
0 ⩽r3 < r2
...
rn−3
=
qn−1rn−2 + rn−1
where
0 ⩽rn−1 < rn−2
rn−2
=
qnrn−1 + rn
where
rn = 0.
Then gcd(a, b) = rn−1.
Example 9.4
Usually, when working through Euclid’s algorithm, all the applications of the
division algorithm are set out in one go. For example, using Euclid’s greatest
common divisor algorithm to calculate d
=
gcd(15 300, 3510) we would
normally proceed as follows.
15 300 = 4 × 3510 + 1260
3510 = 2 × 1260 + 990
1260 = 1 × 990 + 270
990 = 3 × 270 + 180
270 = 1 × 180 + 90
180 = 2 × 90 + 0.
Hence gcd(15 300, 3510) = 90.

Divisibility
443
r = 0 ?
Input n, m
Find q : largest integer
such that xxxx
a = max{n, m}
b = min{n, m}
Start
Set r = qb
a
gcd(n, m) = b
Yes
No
Set a = current value of b
and b = current value of r
Stop
qb
a
£
r
qb
a
=
-
Figure 9.1 Euclid’s gcd algorithm
Euclid’s algorithm actually gives us something extra. By working ‘backwards’
through the algorithm, we can express gcd(a, b) as a linear combination of a
and b. We illustrate this using the working through of the algorithm given in
example 9.3 above.
Example 9.5
In example 9.3 above, we found gcd(2016, 273) = 21.
First we re-write the penultimate application of the division algorithm, 63 =
1 × 42 + 21 making the gcd 21 the subject:
21 = 63 −1 × 42.

444
Introduction to Number Theory
Now, from the previous application of the division algorithm, 105 = 1 × 63 + 42,
we can replace 42 with 105 −1 × 63 and simplify:
21 = 63 −1 × (105 −1 × 63) = 2 × 63 −105.
Next use the previous application of the division algorithm, 273 = 2 × 105 + 63,
to replace 63 with 273 −2 × 105 and simplify:
21 = 2 × (273 −2 × 105) −105 = 2 × 273 −5 × 105.
Finally, the ﬁrst step in Euclid’s algorithm, 2016 = 7 × 273 + 105 allows us to
replace 105:
21 = 2 × 273 −5 × (2016 −7 × 273) = 37 × 273 −5 × 2016.
Hence by working ‘backwards’ through Euclid’s algorithm,
successively
replacing the remainders and simplifying, we have found
21 = 37 × 273 −5 × 2016.
Clearly the process described in the previous example can always be carried
through so that gcd(a, b) can always be expressed in the form ma + nb for some
integers m and n. This result is known as B´ezout’s identity. Note that in B´ezout’s
identity, one of the integers m and n will be positive and the other negative—why
is this?
Theorem 9.5 (B´ezout’s identity)
Let a, b ∈Z+. Then there exist integers m and n such that
gcd(a, b) = ma + nb.
Example 9.6
Express gcd(15 300, 3510) in the form m × 15 300 + n × 3510 where m, n ∈Z.

Divisibility
445
Solution
In example 9.4, we calculated gcd(15 300, 3510) = 90.
Working backwards through the application of Euclid’s algorithm given in
example 9.4, successively replacing remainders and simplifying, gives the
following.
90 = 270 −180
= 270 −(990 −3 × 270)
= −990 + 4 × 270
= −990 + 4 × (1260 −990)
= 4 × 1260 −5 × 990
= 4 × 1260 −5 × (3510 −2 × 1260)
= −5 × 3510 + 14 × 1260
= −5 × 3510 + 14 × (15 300 −4 × 3510)
= 14 × 15 300 −61 × 3510.
Hence gcd(15 300, 3510) = 90 = 14 × 15 300 −61 × 3510.
B´ezout’s identity says that gcd(a, b) can be expressed as an ‘integer linear
combination’ of a and b. An obvious question to ask is: what other integers
can be expressed in this way as ma + nb for some integers m and n? Clearly
any multiple of the greatest common divisor can be written in this way. The next
theorem says that, in fact, only multiples of gcd(a, b) can be expressed as an
‘integer linear combination’ of a and b.
Theorem 9.6
Let a, b ∈Z+ and let d = gcd(a, b).
An integer c can be expressed in the form c = sa + tb for some s, t ∈Z if
and only if d divides c.
Hence d = gcd(a, b) is the smallest positive integer that can be expressed
as sa + tb where s, t ∈Z.
Proof
Let a, b ∈Z+ and let d = gcd(a, b).

446
Introduction to Number Theory
Suppose that c = sa + tb for some s, t ∈Z. Since d divides both a and b, it
follows from theorem 9.2 (a) that d divides c, d|c.
Conversely suppose d divides c.
Then c = kd for some integer k.
From
theorem 9.5 we know that d can be written as d = ma + nb for some m, n ∈Z.
Hence c = kd = k(ma + nb) = (km)a + (kb)d where ka, kb ∈Z, as required.
Finally, since d divides every positive integer in the set {sa + tb : s, t ∈Z}, it
follows that d is the smallest positive integer in the set.
□
Deﬁnition 9.3
Two integers a and b are coprime (or relatively prime) if gcd(a, b) = 1.
Examples 9.7
1.
Consider 42 and 55. Since 42 has factors 1, 2, 3, 6, 7, 14, 21, 42 and 55
has factors 1, 5, 11, 55, their only common divisor is 1, Hence 42 and 55
are coprime.
2.
The integers 2016 and 273 are not coprime. Since 2016 = 3 × 672 and
273 = 3 × 91, it follows that 3 is a common divisor of 2016 and 273 so
their greatest common divisor is not 1.
In fact, using Euclid’s algorithm, we found in example 9.3 that
gcd(2016, 273) = 21.
As an important consequence of theorems 9.5 and 9.6 we have the following.
Theorem 9.7
Let a, b ∈Z+. Then a and b are coprime if and only if there exist integers
m and n such that
ma + nb = 1.

Divisibility
447
Exercises 9.1
1.
(i)
Use Euclid’s greatest common divisor algorithm to calculate:
(a)
gcd(21 600, 2970)
(b)
gcd(1485, 1745)
(c)
gcd(851, 2679)
(d)
gcd(13 376, 7980).
(ii)
Use the results of part (i) to express:
(a)
gcd(21 600, 2970) in the form 21 600m + 2970n where
m, n ∈Z;
(b)
gcd(1485, 1745) in the form 1485m+1745n where m, n ∈
Z;
(c)
gcd(851, 2679) in the form 851m+2679n where m, n ∈Z;
(d)
gcd(13 376, 7980) in the form 13 376m + 7980n where
m, n ∈Z.
2.
Prove each of the following where a, b, c, d, m ∈Z+.
(i)
If a|b and b|c then a|c.
(ii)
If a|b and c|d then ac|bd.
(iii)
a|b if and only if ma|mb.
3.
Prove theorem 9.2 (a).
4.
Show that, in B´ezout’s identity (theorem 9.5), the integers n and m are
not uniquely determined by a and b. Speciﬁcally, using the result of
example 9.5, ﬁnd integers s ̸= 37 and t ̸= −5 such that
21 = 273s + 2016t.
5.
Let a and b be positive integers.
Show that every integer c can be expressed as c = sa + tb for some
integers s and t if and only if a and b are coprime.

448
Introduction to Number Theory
6.
Using the results of exercise 1 (i) (d) and theorem 9.6, determine which,
if any, of the following integers can be expressed as 13 376s + 7980t for
some integers s and t:
226, 228, 230.
7.
Determine which of the following pairs of integers are coprime:
(i)
57 and 45,
(ii)
54 and 101,
(iii)
112 and 117.
8.
We can extend the deﬁnition of greatest common divisor of two positive
integers to a ﬁnite set of positive integers: gcd(a1, a2, . . . , an) is the
smallest positive integer d such that d|a1, d|a2, . . . , d|an.
(i)
It can be shown that
gcd(a1, a2, . . . , an) = gcd(gcd(a1, a2), a3, . . . , an).
In particular, we have gcd(a, b, c) = gcd(gcd(a, b), c).
Use this result to calculate:
(a)
gcd(63, 165, 297),
(b)
gcd(1092, 1155, 2002),
(c)
gcd(72, 48, 108, 54).
(ii)
Generalise theorem 9.2 (a) to the case of k integers, a1, a2, . . . , ak.
That is, suppose that c|a1, c|a2, .. . , c|ak. Prove that, for any
integers n1, n2, . . . , nk,
c|(n1a1 + n2a2 + . . . + nkak).
9.
Let (an) = (1, 1, 2, 3, 5, 8, 13, 21 . . .) be the Fibonacci sequence deﬁned
by a1 = a2 = 1 and, for n ⩾3, an = an−1 + an−2.
(i)
Apply Euclid’s greatest common divisor algorithm to calculate
gcd(21, 13).
(ii)
Describe what happens in general when Euclid’s greatest common
divisor algorithm is used to calculate gcd(an, an−1).

Prime Numbers
449
(iii)
Use the result of part (i) to ﬁnd integers r and s such that 1 =
21r + 13s.
(iv)
(Harder) Use the result of part (iii) to conjecture a general result
which expresses 1 = ran + san−1 for some integers r and s.
Prove your conjecture using mathematical induction.
10.
Let a and b be consecutive positive integers.
Show that a and b are
coprime.
11.
Let a and b be coprime positive integers. Using theorem 9.7, prove that,
for all positive integers c, if a|c and b|c then ab|c.
Show that this result is false if a and b are not coprime.
9.2
Prime Numbers
We begin with what is probably a familiar deﬁnition.
Deﬁnition 9.4
An integer p is prime if p > 1 and the only divisors of p are 1 and p itself.
An integer n > 1 that is not prime is called composite.
Note that 1 is not prime; 2 is the smallest prime and all other primes are odd. The
ﬁrst few primes are: 2, 3, 5, 7, 11, 13, 17, 19, . . . . Later in this section we will ﬁnd
all the primes less than 100.
In two important ways, prime numbers play an analogous role in number theory
to that played by atoms in molecular chemistry. Firstly, primes are ‘indivisible’ in
the sense that they cannot be ‘broken down’ into the product of smaller positive
integers. Secondly, as we shall see, primes are the ‘building blocks’ of the positive
integers in the sense that every positive integer (except 1) can expressed as a
product of primes.

450
Introduction to Number Theory
The fact that primes are ‘indivisible’ has certain consequences for the divisibility
relation that we now explore by considering two (related) questions. Suppose p is
prime.
1.
Which positive integers a are coprime with p?
2.
If p divides a product of integers a × b, what may we deduce?
To explore the ﬁrst question, let p = 7. It is clear that, if a is a multiple of 7, then
a and 7 are not coprime. What about those integers greater than 1 that are not
divisible by 7:
2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, . . . .
Each of these is coprime with 7. This would suggest that an integer a > 1 that is
not a multiple of p is coprime with p.
To answer the second question, again let p = 7. Now 7 divides 126 and we can
express 126 as a product of two integers in a number of ways:
126 = 2 × 63 = 3 × 42 = 6 × 21 = 9 × 14 = 18 × 7.
In each of these ways of writing 126 as a product, 7 divides one of the factors.
Similarly, 7|1155 and however we write 1155 as a × b where a, b ∈Z+, we
ﬁnd that 7 divides one of the terms. For example, 1155 = 15 × 77 and 7|77;
1155 = 55 × 21 and 7|21, and so forth.
These considerations are formalised in the following theorem.
Theorem 9.8
Let p be prime.
(a)
For all a ∈Z+, either p|a or a and p are coprime.
(b)
For all a, b ∈Z+, if p|ab then p|a or p|b.
Proof
(a)
Consider the greatest common divisor of a and p. By deﬁnition gcd(a, p)
is a positive integer that divides p (as well as a). Since p is prime its only
divisors are 1 and p, so the only possibilities for the greatest common
divisor are gcd(a, p) = 1 or gcd(a, p) = p.

Prime Numbers
451
If gcd(a, p) = p then p|a since, by deﬁnition, the greatest common
divisor gcd(a, p) divides a.
If gcd(a, p) = 1 than a and p are coprime, again by deﬁnition.
(b)
Let p|ab.
Suppose that p does not divide a. Then we need to show that p|b.
By part (a), we have gcd(a, p) = 1. Therefore, from theorem 9.7, there
exist m, n ∈Z such that ma + np = 1. Multiplying this equation by b
gives
b = mab + npb
and we consider each of the terms in this expression. By our assumption
p|ab so that p|mab. Clearly p|npb. Therefore p divides each term so p
divides mab + npb = b, as required.
□
We claimed above that the prime numbers form the ‘building blocks’ from which
all integers can be ‘constructed’ using multiplication in the sense that every
integer n > 1 can be written as a product of primes. To see this, let n be an
integer. If n is not prime then it has a factor a such that 1 < a < n so can be
expressed as n = ab. Then consider a and b in turn. Each is either prime or
can itself be factorised. Continuing in this way we eventually reach only prime
factors. This is illustrated in the following examples.
Examples 9.8
1.
Consider 120. First we may write 120 = 10×12. Then each of the factors
also factorises, 10 = 2 × 5 and 12 = 3 × 4, so 120 = (2 × 5) × (3 × 4).
The only composite number in this expression is 4 = 2 × 2 so we obtain
120 = 2 × 5 × 3 × 2 × 2 = 23 × 3 × 5.
Note that it does not matter how we complete the factorisation.
For
example, the following is an alternative way of achieving the (same)
prime factorisation of 120:
120 = 3×40 = 3×2×20 = 3×2×5×4 = 3×2×5×2×2 = 23×3×5.
2.
Repeating the same process with 675 gives
675 = 5 × 135 = 5 × 5 × 27 = 52 × 33.

452
Introduction to Number Theory
The statement that this can always be done in an ‘essentially’ unique way is called
the Fundamental Theorem of Arithmetic. By ‘essentially unique’ we mean unique
apart from the order in which the primes are written.
Theorem 9.9 (The Fundamental Theorem of Arithmetic)
Every integer n > 1 has a factorisation
n = pe1
1 pe2
2 . . . pek
k
where p1, p2, . . . , pk are distinct primes and e1, e2, . . . , ek are positive
integers.
This factorisation is essentially unique; that is, it is unique up to the
ordering of the primes.
The fundamental theorem gives a way of calculating the greatest common divisor,
as illustrated in the following example.
Example 9.9
Find gcd(1350, 2205).
First we ﬁnd the prime factorisation of 1350 and 2205.
1350 = 5 × 270 = 5 × 27 × 10 = 5 × 33 × 2 × 5 = 2 × 33 × 52
2205 = 5 × 441 = 5 × 3 × 147 = 5 × 3 × 3 × 49 = 32 × 5 × 72.
The greatest common divisor is then the product of those prime powers common
to both expressions,
gcd(1350, 2205) = 32 × 5 = 45.
In other words, for each prime that is common to both factorisations, we chose
the smaller of its powers in the two expressions. For example, 33 is a divisor of
1350 and 32 is a divisor of 2205. Hence the largest power of 3 that is a common
divisor is 32.

Prime Numbers
453
Fermat Numbers and Mersenne Numbers
Numbers of the form Fn = 22n + 1 are called Fermat numbers.
Fermat
conjectured that Fn is prime for all n > 0. The ﬁrst few Fermat numbers are:
F0
=
220 + 1
=
21 + 1 = 3
which is prime,
F1
=
221 + 1
=
22 + 1 = 5
which is prime,
F2
=
222 + 1
=
24 + 1 = 17
which is prime,
F3
=
223 + 1
=
28 + 1 = 257
which is prime,
F4
=
224 + 1
=
216 + 1 = 65537
which is prime.
In 1732 Euler showed that the next Fermat number
F5 = 225 = 232 + 1 = 4294967297 = 641 × 6700417
is composite, thus proving that Fermat’s conjecture is false.
However integers of the form 2n −1 have been considered as possible candidates
for (large) prime numbers. We show that a necessary condition for 2n −1 to be
prime is that n itself is prime. In other words, for all n ∈Z+,
2n −1 is prime ⇒n is prime.
This is most easily proved by proving the contrapositive:
n is not prime ⇒2n −1 is not prime.
Suppose that n is not prime. Then n = rs where 1 < r < n and 1 < s < n.
Hence 2n = 2rs = (2r)s. We know from exercise 2.3.4 that, for x ̸= 1,
1 + x + x2 + . . . + xn−1 = xn −1
x −1 .
Using this result with x = 2r and n = s gives
1 + 2r + (2r)2 + . . . + (2r)s−1 = (2r)s −1
2r −1
= 2n −1
2r −1
so, multiplying both sides by 2r −1, gives
(2r −1)(1 + 2r + (2r)2 + . . . + (2r)s−1) = 2n −1.
This last equation expresses 2n −1 as the product of two integers, namely 2r −1
and 1 + 2r + (2r)2 + . . . + (2r)s−1. Hence 2n −1 is not prime, as required.
Integers of the form Mp = 2p−1 where p is prime are called Mersenne numbers.

454
Introduction to Number Theory
We consider whether the converse of the result above is true. In other words, if
p is prime, is it necessarily the case that Mp = 2p −1 is prime? The ﬁrst few
Mersenne numbers are:
M2
=
22 −1
=
3
which is prime,
M3
=
23 −1
=
7
which is prime,
M5
=
25 −1
=
31
which is prime,
M7
=
27 −1
=
127
which is prime.
However M11 = 211 −1 = 2047 = 23 × 89, so the converse of the statement
is false; in other words, not all Mersenne numbers are prime. However, over 40
Mersenne primes have been discovered. For example the 39th Mersenne prime is
M13466917 ≈104053946; hence, this prime number has over 4 million digits!
There is a collaborative project of volunteers called GIMPS—the Great Internet
Mersenne Prime Search—that is dedicated to ﬁnding Mersenne primes. Mersenne
primes with over ten million digits are now known.
Primality Testing and Factorisation
There are two practical questions associated with prime factorisation.
(a)
How do we determine whether or not a given integer n is prime?
(b)
How do we ﬁnd the prime factorisation of a given integer n?
The ﬁrst simple observation that restricts the amount of work required to test
whether a given integer n is prime is provided by the following theorem. This
says ﬁrstly that we only need to test whether n has prime factors and secondly
that we only need to consider factors up to √n in size.
Theorem 9.10
An integer n > 1 is composite if and only if it is divisible by a prime
number p ⩽√n.
Proof
If n is divisible by a prime p ⩽√n then clearly n is composite.

Prime Numbers
455
Conversely, suppose that n is composite. Then n = ab where 1 < a < n and
1 < b < n. At least one of a and b is less than or equal to √n. (How would you
prove this?) Suppose a ⩽√n. If a itself is prime we are ﬁnished. If a is not
prime then it is divisible by some prime p ⩽a ⩽√n which also divides n.
□
Example 9.10
We can verify that 101 is prime by showing that it is not divisible by any of the
primes p ⩽
√
101. There are only four such primes, 2, 3, 5, and 7 and it is easy
to check that each of these does not divide 101.
For very large integers, this method is inefﬁcient. However there are other much
more sophisticated ways of determining whether an integer is prime (which are
based on some very advanced number theory). These techniques are beyond the
scope of this text.
The second problem mentioned above—ﬁnding the prime factors of a large
integer n—is much harder than primality testing. For very large integers it is
practically impossible to factorise them in a reasonable length of time even using
the most powerful computers. We will return to this later.
We conclude this section with a method of ﬁnding all the primes p in a particular
range 1 < p ⩽N, known as the ‘Sieve of Eratosthenes’.
Sieve of Eratosthenes
First list the integers 2, 3, 4, . . . , N. Then
•
circle 2 and strike out all multiples of 2.
•
circle 3 (the ﬁrst number not stuck out) and strike out all remaining
multiples of 3.
•
circle 5 (the ﬁrst number not stuck out) and strike out all remaining
multiples of 5.
Continue until every integer in the list is either circled or struck out. The
primes are the circled numbers.

456
Introduction to Number Theory
Example 9.11
We use the Sieve of Eratosthenes to ﬁnd all primes up to 100.
First list all the integers 2, 3, 4, . . . , 100.
12
14
13
11
17
16
15
19
18
20
22
24
23
21
27
26
25
29
28
30
32
34
33
31
37
36
35
39
38
40
42
44
43
41
47
46
45
49
48
50
52
54
53
51
57
56
55
59
58
60
62
64
63
61
67
66
65
69
68
70
72
74
73
71
77
76
75
79
78
80
82
84
83
81
87
86
85
89
88
90
92
94
93
91
97
96
95
99
98
100
2
4
3
7
6
5
9
8
10
First circle 2 and then strike out all multiples of 2.
12
14
13
11
17
16
15
19
18
20
22
24
23
21
27
26
25
29
28
30
32
34
33
31
37
36
35
39
38
40
42
44
43
41
47
46
45
49
48
50
52
54
53
51
57
56
55
59
58
60
62
64
63
61
67
66
65
69
68
70
72
74
73
71
77
76
75
79
78
80
82
84
83
81
87
86
85
89
88
90
92
94
93
91
97
96
95
99
98
100
2
4
3
7
6
5
9
8
10
Next circle 3 and then strike out all multiples of 3 that have not already been
struck out.

Prime Numbers
457
12
14
13
11
17
16
15
19
18
20
22
24
23
21
27
26
25
29
28
30
32
34
33
31
37
36
35
39
38
40
42
44
43
41
47
46
45
49
48
50
52
54
53
51
57
56
55
59
58
60
62
64
63
61
67
66
65
69
68
70
72
74
73
71
77
76
75
79
78
80
82
84
83
81
87
86
85
89
88
90
92
94
93
91
97
96
95
99
98
100
2
4
3
7
6
5
9
8
10
Then circle 5 and then strike out all multiples of 5 that have not already been
struck out.
12
14
13
11
17
16
15
19
18
20
22
24
23
21
27
26
25
29
28
30
32
34
33
31
37
36
35
39
38
40
42
44
43
41
47
46
45
49
48
50
52
54
53
51
57
56
55
59
58
60
62
64
63
61
67
66
65
69
68
70
72
74
73
71
77
76
75
79
78
80
82
84
83
81
87
86
85
89
88
90
92
94
93
91
97
96
95
99
98
100
2
4
3
7
6
5
9
8
10
Repeating for 7 gives the following table.
12
14
13
11
17
16
15
19
18
20
22
24
23
21
27
26
25
29
28
30
32
34
33
31
37
36
35
39
38
40
42
44
43
41
47
46
45
49
48
50
52
54
53
51
57
56
55
59
58
60
62
64
63
61
67
66
65
69
68
70
72
74
73
71
77
76
75
79
78
80
82
84
83
81
87
86
85
89
88
90
92
94
93
91
97
96
95
99
98
100
2
4
3
7
6
5
9
8
10

458
Introduction to Number Theory
Note that, at this stage, only three integers were struck out: 49 = 7 × 7,
77 = 7 × 11 and 91 = 7 × 13. In fact, the smallest number struck out when
the prime p is circled is p2 since all smaller integers that are not prime have a
factor smaller than p and so have been struck out at an earlier stage.
Therefore, since the next number to be circled is 11 and 112 = 121 is greater than
100, no further numbers will be struck out. Hence all the remaining numbers may
be circled which gives the following list of all the prime numbers less than 100.
12
14
13
11
17
16
15
19
18
20
22
24
23
21
27
26
25
29
28
30
32
34
33
31
37
36
35
39
38
40
42
44
43
41
47
46
45
49
48
50
52
54
53
51
57
56
55
59
58
60
62
64
63
61
67
66
65
69
68
70
72
74
73
71
77
76
75
79
78
80
82
84
83
81
87
86
85
89
88
90
92
94
93
91
97
96
95
99
98
100
2
4
3
7
6
5
9
8
10
Exercises 9.2
1.
(i)
Find the prime factorisation of
(a)
1008
(b)
792
(c)
3276
(d)
22 680.
(ii)
Use the results of part (a) to determine
(a)
gcd(1008, 792)
(b)
gcd(1008, 3276)
(c)
gcd(3276, 22 680)
(d)
gcd(1008, 792, 3276).

Prime Numbers
459
2.
(i)
Find the ﬁrst sequence of three consecutive composite positive
integers.
(ii)
Find the ﬁrst sequence of ﬁve consecutive composite positive
integers.
(iii)
Prove that, for each integer n ⩾2, the sequence of n −1 positive
integers
n! + 2, n! + 3, n! + 4, . . . , n! + n
are all composite.
3.
Show that the condition that p is prime is a necessary condition for each
part of theorem 9.8.
4.
Prove the following generalisation of theorem 9.8 (b).
Let p be prime and let a1, a2, . . . , an ∈Z+. If p|(a1a2 . . . an) then p|ai
for some i = 1, 2, . . . , n.
5.
For which prime numbers p is p2 + 2 also prime?
6.
Goldbach’s conjecture states that: every even integer n ⩾4 is the sum
of two prime numbers.
Verify Goldbach’s conjecture for even integers n in the range 4 ⩽n ⩽
30.
Note. The status of Goldbach’s conjecture is unknown. In other words,
no proof and no counter-example has been found.
7.
Let p be prime and let a and b be positive integers.
(i)
Show that, if gcd(a, p2)
=
p and gcd(b, p2)
=
p then
gcd(ab, p4) = p2.
(ii)
Show that, if gcd(a, p2)
=
p and gcd(b, p3)
=
p2 then
gcd(ab, p4) = p3.
(iii)
If gcd(a, b) = p what are the possible values of gcd(a2, b)?
(iv)
If gcd(a, b) = p what are the possible values of gcd(a3, b)?
8.
In example 2.4.1 we proved that
√
2 is irrational. Use the Fundamental
Theorem of Arithmetic to prove that, if n is a positive integer that is not
a perfect square then √n is irrational.
Hint. Prove the contrapositive: if √n is rational then n is a perfect square.

460
Introduction to Number Theory
9.3
Linear Congruences
We introduced the relation of ‘congruence module n’ in chapter 4.
For
convenience, we repeat below the deﬁnition given on page 183.
Deﬁnition 9.5
Let n be a positive integer.
For integers a and b, we say that a is congruent to b modulo n, denoted
a ≡b mod n, if their difference a −b is a multiple of n:
a ≡b
mod n if and only if a −b = kn for some k ∈Z.
Other notations for a ≡b mod n are a ≡b (mod n) or a ≡n b.
Note that another way of phrasing the deﬁnition is to say that a and b are congruent
modulo n is that n divides their difference:
a ≡b
mod n if and only if n|(a −b).
In this chapter, we will prefer the notation a ≡b mod n rather than a ≡n b
which we used in chapter 4. The notation a ≡b mod n is more standard in
number theory where, for example, we may be interested in considering various
kinds of ‘equations’ modulo n. In chapter 4, we were interested in congruence
from as a kind of relation so the notation a ≡n b was more in tune with the general
relation notation a R b.
One reason for considering modulo arithmetic – that is, working modulo n for
some n – is that some problems that involve the use of very large integers may
be simpliﬁed in this way. Actually, we do this all the time in ‘everyday life’. For
example, what day of the week will it be in 70772 days time? Since 70772 ≡2
mod 7, the answer is that it will be 2 days later than today; thus, if today is
Tuesday then it will be Thursday in 70772 days time. However, none of us will
be alive to see it since 70772 days is over 193 years!
Theorem 9.11
Let n be a ﬁxed positive integer. Then, for all a, b ∈Z, a ≡b mod n if
and only if a and b have the same remainder after division by n.

Linear Congruences
461
Proof
From the Division Algorithm (or, more properly, the generalization that allows
negative integers—see example 9.1.2) there exist integers q, q′, r, r′ such that
a = qn + r where 0 ⩽r < n,
b = q′n + r′ where 0 ⩽r′ < n.
Therefore
a −b = (q −q′)n + (r −r′) where −n < r −r′ < n.
Suppose a ≡b mod n. Then n divides a −b so n also divides (a −b) −(q −
q′)n = r −r′. But −n < r −r′ < n and the only integer in this range that is a
multiple of n is 0. Therefore r −r′ = 0 so r = r′. Hence a and b have the same
remainder after division by n.
Conversely, suppose r = r′ above; that is, a and b have the same remainder after
division by n. Then a −b = (q −q′)n where q −q′ ∈Z. Hence n divides a −b,
n|(a −b), so a ≡b mod n.
□
Theorem 9.12
Let n ∈Z. If n is a perfect square then n is congruent to 0 or 1 modulo 4:
n = m2 for some m ∈N
⇒
n ≡0
mod 4 or n ≡1
mod 4.
Proof
For any integer m, we have m ≡0, 1, 2 or 3 mod 4.
If
m ≡0 mod 4
then
m2 ≡0 mod 4.
If
m ≡1 mod 4
then
m2 ≡1 mod 4.
If
m ≡2 mod 4
then
m2 ≡0 mod 4.
If
m ≡3 mod 4
then
m2 ≡1 mod 4.
Therefore, if n = m2, for some integer m then
n ≡0
mod 4 or n ≡1
mod 4.
□

462
Introduction to Number Theory
Example 9.12
636802 is not a perfect square since 636802 = 159200 × 4 + 2 so 636802 ≡2
mod 4.
Note that we are using the contrapositive of theorem 9.12 here: if n ̸≡0 and
n ̸≡1 mod 4 then n is not a perfect square.
Recall from chapter 4 that congruence modulo n is an equivalence relation and
the equivalence class of a ∈Z is
[a] = {b ∈Z : b ≡a
mod n}
= {. . . , a −2n, a −n, a, a + n, a + 2n, a + 3n, . . .}.
The equivalence class [a] is often called the congruence class of a. For a general
n, there are n distinct congruence classes:
[0] = {. . . , −2n, −n, 0, n, 2n, . . .}
[1] = {. . . , −2n + 1, −n + 1, 1, n + 1, 2n + 1, . . .}
[2] = {. . . , −2n + 2, −n + 2, 2, n + 2, 2n + 2, . . .}
...
[n −1] = {. . . , −n −1, −1, n −1, 2n + 1, 3n + 1, . . .}.
The set of congruence classes modulo n is denoted
Zn = {[0], [1], [2], . . . , [n −1]}.
We also deﬁned the arithmetic operations of addition and multiplication on the
elements of Zn. Subtraction is similarly deﬁned. If [a] and [b] are congruence
classes modulo n then
[a] +n [b] = [a + b]
[a] −n [b] = [a −b]
[a] ×n [b] = [ab] .
The following theorem shows that these deﬁnitions are valid; in other words, the
deﬁnitions do not depend on the labels of the equivalence classes. See pages 184

Linear Congruences
463
and 185 for a more detailed discussion of this point. The proof of parts (i) and (iii)
of theorem 9.13 were left as an exercise in chapter 4—see exercise 4.4.11—and
the proof of part (ii) is similar.
Theorem 9.13
Let n ∈Z. If a ≡a′ mod n and b ≡b′ mod n then
(i)
a + b ≡a′ + b′ mod n,
(ii)
a −b ≡a′ −b′ mod n,
(iii)
ab ≡a′b′ mod n.
Examples 9.13
With a little thought, it is possible to evaluate apparently difﬁcult looking
expressions modulo n without resorting to calculator or computer. The following
examples illustrate this.
1.
Evaluate 310 mod 14.
Solution
Rather than trying to evaluate 310 (which is 59 049), then dividing it
by 14 and identifying the remainder, instead we build up to 310 by ﬁrst
evaluating smaller powers.
First note that 33 = 27 ≡13 mod 14. Now 13 ≡−1 mod 14 so we
have 33 ≡−1 mod 14.
From this it follows that 39 ≡(33)3 ≡(−1)3 ≡−1 mod 14.
Hence 310 ≡39 × 3 ≡−1 × 3 ≡−3 ≡11 mod 14.
2.
Evaluate 27 × 13 × 9 mod 31.
Solution
Again we begin by evaluating the simpler expression 27 × 13 mod 31
but here, too, a little thought can save effort.

464
Introduction to Number Theory
Note that 27 ≡−4 mod 31 so, as in the previous example, we can
replace a larger positive value with a smaller negative one which will
simplify the subsequent calculation. Hence we have 27×13 ≡−4×13 =
−52 ≡10 mod 31.
Therefore 27 × 13 × 9 ≡10 × 9 = 90 ≡−3 ≡28 mod 31.
In this last step we have again switched between equivalent positive and
negative values where it is convenient to do so. For example, since 90 is
three less than a multiple of 31, we can immediately see that 90 ≡−3
mod 31 and then adding 31 gives −3 ≡28 mod 31.
3.
Evaluate 5302 mod 31.
Solution
The expression 5302 mod 31 looks daunting but something rather nice
occurs that makes the evaluation very easy.
First note that 52 = 25 ≡−6 mod 31 so that 53 ≡(−6)×5 = −30 ≡1
mod 31.
(Note that we could have obtained this directly since 53 = 125 =
(4 × 31) + 1 ≡1 mod 31.)
Since 53 ≡1 mod 31, it follows that any power of 53 is also congruent
to 1 modulo 31. In particular 5300 = (53)100 ≡1100 ≡1 mod 31.
Finally we have
5302 = 52 × 5300 ≡25 × 1 ≡25
mod 31.
Equations in Modular Arithmetic
Let n ∈Z. We shall consider ‘linear congruence equations’ or, simply, ‘linear
congruences’
ax ≡b
mod n
where 0 < a < n and 0 ⩽b < n and we seek solutions x in the range 0 ⩽x < n.
We begin by considering some simple examples where we can ﬁnd the solutions
by trial and error.

Linear Congruences
465
Examples 9.14
1.
Consider the congruence 2x ≡4 mod 5.
Working modulo 5 we have 2 × 0 ≡0, 2 × 1 ≡2, 2 × 2 ≡4, 2 × 3 ≡
1, 2×4 ≡3. Hence the congruence has a unique solution x ≡2 mod 5.
2.
Consider the congruence 2x ≡4 mod 6.
Since 2×0 ≡0, 2×1 ≡2, 2×2 ≡4, 2×3 ≡0, 2×4 ≡2, 2×5 ≡4,
the congruence has two solutions x ≡2 mod 6 and x ≡5 mod 6.
3.
Consider the congruence 2x ≡5 mod 6.
From the calculations above, the congruence has no solution.
4.
Consider the congruence 6x ≡12 mod 30.
Clearly x ≡2 mod 30 is a solution. A little trial and error shows that
x ≡7, 12, 17, 22, 27 mod 30 are also solutions.
The previous examples show that a linear congruence ax ≡b mod n may have
no solution, a unique solution or several solutions. Compare this situation with
‘ordinary’ arithmetic where the equation ax = b has a unique solution (provided
a ̸= 0). The following theorem gives the general situation.
Theorem 9.14 (Solving linear congruences)
Let a, b and n be integers such that 0 < a < n and 0 ⩽b < n and let
d = gcd(a, n). Then the congruence
ax ≡b
mod n
has a solution if and only if d divides b.
If d divides b and x0 is a solution of the congruence in the range 0 ⩽x0 <
n/d, then there are exactly d solutions in the range 0 ⩽x0 < n, namely
x0, x0 + n
d , x0 + 2n
d , . . . , x0 + (d −1)n
d .

466
Introduction to Number Theory
Proof
Let d = gcd(a, n). By theorem 9.6,
d|b ⇔b = sa + tn for some s, t ∈Z
⇔sa ≡b
mod n for some s ∈Z
⇔ax ≡b
mod n has a solution.
For the second part, suppose that d divides b. Suppose also that x0 is a solution of
ax ≡b mod n. Then ax0 −b = kn for some k ∈Z. Therefore, for any s ∈Z,
x = x0 + sn
d ⇒ax −b = ax0 + asn
d −b
⇒ax −b = kn + asn
d
⇒ax −b = n
³
k + sa
d
´
⇒ax ≡b
mod n
since a
d ∈Z.
Therefore
x0, x0 + n
d , x0 + 2n
d , . . . , x0 + (d −1)n
d
are all solutions of ax ≡b mod n and these are all different mod n.
□
Examples 9.15
We consider how the solutions to examples 9.14 above ﬁt into the framework of
this theorem.
1.
2x ≡4 mod 5.
Since gcd(2, 5) = 1 and 1|4 there is a solution which is unique since
d = gcd(2, 5) = 1.
2.
2x ≡4 mod 6.
Since gcd(2, 6) = 2 and 2|4 there is a solution. By the second part of
theorem 9.14, there are two solutions since gcd(2, 6) = 2.
x0 = 2 is a solution in 0 ⩽x0 < 6/2 = 3. The other solution is
2 + 6
2 = 5.

Linear Congruences
467
3.
2x ≡5 mod 6.
Since gcd(2, 6) = 2 and 2 does not divide 5, there is no solution.
An important special case of theorem 9.14 is when a and n are coprime. Then
d = 1, so that d|b for all b ∈Z+ and the congruence has a unique solution. This
is captured in the following theorem.
Theorem 9.15
If a and n are coprime and 0 < a < n then the congruence ax ≡b
mod n has a unique solution in the range 0 ⩽x < n.
Note that, if n is prime then theorem 9.15 says that any congruence ax ≡b
mod n, where 0 < a < n, has a unique solution in the range 0 ⩽x < n.
Suppose the congruence ax ≡b mod n has one or more solutions. How do we
ﬁnd the solution(s)? For small values of n, this can be done by testing each of the
values x in the range 0 ⩽x < n/d where d = gcd(a, n). For larger values of n,
however, this is tedious.
The following theorem allows us to replace a congruence with one involving
smaller numbers.
Theorem 9.16
(a)
Suppose m divides each of a, b and n and let a′ = a/m, b′ = b/m
and n′ = n/m. Then
ax ≡b
mod n if and only if a′x ≡b′
mod n′.
(b)
Suppose a and n are coprime and m divides both a and b. Let
a′ = a/m and b′ = b/m. Then
ax ≡b
mod n if and only if a′x ≡b′
mod n.

468
Introduction to Number Theory
Proof
(a)
Suppose m divides a, b and n. Recall that ax ≡b mod n if and only
if the difference ax −b is a multiple of n:
ax ≡b
mod n ⇔ax −b = qn for some q ∈Z.
In this case, dividing by m gives
a
mx −b
m = q n
m
which is precisely a′x −b′ = qn′.
This last equation says that the
difference a′x−b′ is a multiple of n′ which, by deﬁnition, means a′x ≡b′
mod n′.
We could summarise this argument more symbolically as follows:
ax ≡b
mod n ⇔ax −b = qn for some q ∈Z
⇔
a
mx −b
m = q n
m where q ∈Z
⇔a′x −b′ = qn′ where q ∈Z
⇔a′x ≡b′
mod n′.
(b)
Suppose that a and n are coprime and m divides both a and b. As in part
(a) we have,
ax ≡b
mod n
⇒
ax −b = qn
for some q ∈Z
⇒
a
mx −b
m = qn
m
where q ∈Z
⇒
a′x −b′ = qn
m
where q ∈Z
⇒
m|(nq)
since a′, b′ ∈Z.
Since m divides a and a and n are coprime so m and n are also coprime.
(This is because any common factor of m and n would also be a common
factor of a and n.) Therefore, by theorem 9.7, there exist integers r and s
such that 1 = rm+sn. Multiplying by q gives q = rmq+snq. We know
from the reasoning above that m|(qn) and clearly we also have m|rmq.
Therefore m|q.
Let q/m = q′ ∈Z. Then the equation a′x −b′ = qn/m above becomes
a′x −b′ = m′n. Hence a′x ≡b′ mod n.
The converse is straightforward. If a′x ≡b′ mod n then a′x −b′ = qn
for some q ∈Z. Multiplying by m gives ax −b = qmn, so ax ≡b
mod n.
□

Linear Congruences
469
Examples 9.16
We use theorems 9.14, 9.15 and 9.16 to help us solve a number of linear
congruences.
1.
20x ≡12 mod 14.
Solution
Since gcd(20, 14) = 2 and 2 divides 12, the congruence has two solutions
by theorem 9.14.
Now 2|20, 2|12 and 2|14 so, by part (a) of theorem 9.16, we may ‘divide
through by 2’ and replace the original congruence with
10x ≡6
mod 7.
In this new congruence, 10 and 7 are coprime and 2 divides both 10 and
6. By part (b) of theorem 9.16, we may replace it with
5x ≡3
mod 7.
This congruence has a unique solution in the range 0 ⩽x < 7 by
theorem 9.15 since 7 is prime. This solution is easy to ﬁnd by trial and
error:
5 × 0 = 0,
5 × 1 = 1,
5 × 2 = 10 ≡3,
so x = 2 is the unique solution of 5x ≡3 mod 7.
Therefore, x = 2 is also a solution of the original congruence 20x ≡12
mod 14 in the range
0 ⩽x <
14
gcd(20, 14) = 14
2 = 7.
By theorem 9.14, the (only) other solution is x = 2 + 7 = 9.
2.
18x ≡36 mod 42.
Solution
Since gcd(18, 42) = 6 and 6|36 the congruence has six solutions by
theorem 9.14.

470
Introduction to Number Theory
Now 6|18, 6|36 and 6|42 so, by theorem 9.16 (a), we may replace the
original congruence with
3x ≡6
mod 7.
In this new congruence, 3 and 7 are coprime and 3|3, 3|6. Hence by
theorem 9.16 (b) we may replace it with
x ≡2
mod 7.
This clearly has the solution x = 2 which is the unique solution in the
range 0 ⩽x < 7.
Therefore, x = 2 is also a solution of 18x ≡36 mod 42 in the range
0 ⩽x < 42
6 = 7.
By theorem 9.14, the remaining solutions in the range 0 ⩽x < 42 are
obtained by adding multiples of 42/6 = 7. The full list of solutions is
therefore
x = 2, 2 + 7 = 9, 2 + 2 × 7 = 16, 2 + 3 × 7 = 23,
2 + 4 × 7 = 30, 2 + 5 × 7 = 37.
3.
14x = 149 mod 201.
Solution
Since gcd(14, 201) = 1 so that 14 and 201 are coprime, there is a unique
solution in the range 0 ⩽x < 201 by theorem 9.15.
Now gcd(14, 149) = 1 so no further simpliﬁcation is possible.
How do we solve 14x = 149 mod 201 without resorting to trial and
error? Answer: use a little cunning!
The idea is we do two things in an attempt to ﬁnd a solution. Firstly, ﬁnd
an ‘approximate’ solution; that is, ﬁnd a multiple of 14 that is close to
149. Secondly, in order to be able to ‘adjust’ the approximate solution
to obtain an exact solution, we ﬁnd a multiple of 14 that is close to zero
modulo 201. In other words, we ﬁnd a multiple of 14 that is close to the
modulus 201.
For the ﬁrst part, note that 14×11 = 154 which gets ‘close’ to the desired
value of 149.

Linear Congruences
471
Also 14 × 14 = 196 ≡−5 mod 201, which is close to zero. On this
occasion, the desired right-hand side 149 is simply the sum of the two
values we have obtained: 149 = 154+(−5). Hence the solution is easily
obtained:
14 × 11 + 14 × 14 ≡154 −5 = 149
mod 201
⇒
14 × (11 + 14) ≡149
mod 201
⇒
14 × 25 ≡149
mod 201.
Therefore the solution is x = 25.
4.
14x = 156 mod 201.
Solution
This is similar to the previous example but the ‘adjustment’ of the
approximate solution to become an actual solution requires a little more
work. From the previous example, the approximate solution is provided
by
14 × 11 = 154.
We now need to ﬁnd a way of increasing the approximate solution by 2
to obtain the exact solution.
We also know that
14 × 14 ≡−5
mod 201.
This is a small negative value; taking the ‘next’ multiple gives a small
positive value,
14 × 15 ≡9
mod 201.
The idea now is to combine these two ‘small’ values, −5 and 9, to obtain
the difference 2 between the approximate solution 14 × 11 = 154 and
the actual solution 14x = 156 mod 201. The ﬁrst step is to write the
required value 2 as a linear combination of −5 and 9:
2 = 3 × 9 + 5 × (−5).
Working modulo 201 we can replace 9 with 14 × 15 and we can replace
−5 with 14 × 14. This gives
2 ≡3 × (14 × 15) + 5 × (14 × 14)
mod 201
⇒
2 ≡14 × (3 × 15) + 14 × (5 × 14)
mod 201
⇒
2 ≡14 × 45 + 14 × 70
mod 201
⇒
2 ≡14 × 115
mod 201.

472
Introduction to Number Theory
This is exactly the adjustment we need to make to our approximate
solution 14 × 11 = 154. Therefore
156
≡
154 + 2
≡
14 × 11 + 14 × 115
≡
14 × (11 + 115)
≡
14 × 126
mod 201,
so the required solution is x = 126.
Exercises 9.3
1.
Show that the last decimal digit of a perfect square cannot be 2, 3, 7 or 8.
Is 3 190 493 a perfect square?
2.
(i)
Determine the congruence classes modulo 6.
(ii)
Draw up tables for addition, subtraction and multiplication for the
set Z6 of congruence classes modulo 6.
3.
Show that, in Z3 = {[0], [1], [2]}, [1] = [4] but [21] ̸= [24].
This example shows that we cannot deﬁne exponentiation of congruence
classes by [a][b] = [ab].
4.
Using similar techniques to those given in examples 9.13, evaluate each
of the following.
(i)
11 × 19 mod 23,
(ii)
39 mod 23,
(iii)
33 × 17 mod 23,
(iv)
512 mod 23.
5.
Find the last decimal digit of each of the following.
(i)
234593
(ii)
291475,
(iii)
1! + 2! + 3! + · · · + 10!
6.
Suppose a ≡b mod n and k ∈Z. Does it follow that:

Groups in Modular Arithmetic
473
(i)
ak ≡bk mod n ?
(ii)
ka ≡kb mod n ?
In each case either give a proof or give a counter-example.
7.
Find all the solutions of each of the following congruences by trial and
error; that is, by an exhaustive search through the values 0, 1, . . . , n −1
(where n is the modulus). Compare this with what happens when we
solve these equations in ‘ordinary arithmetic’; that is, when we solve
them over the real numbers R.
(i)
3x ≡4 mod 6
(ii)
3x ≡4 mod 7
(iii)
x2 ≡2 mod 5
(iv)
x2 + 2 ≡0 mod 6
(v)
x2 + 2 ≡0 mod 7
(vi)
x2 ≡x mod 6.
8.
For each of the following linear congruences, determine whether a
solution exists.
If a solution exists, use the techniques illustrated in
examples 9.16 to determine all the solutions in the range 0 ⩽x < n
(where n is the modulus).
(i)
12x ≡15 mod 22
(ii)
5x ≡1 mod 11
(iii)
19x ≡42 mod 50
(iv)
18x ≡42 mod 50
(v)
65x ≡27 mod 169
(vi)
65x ≡39 mod 169
(vii)
16x ≡301 mod 595
(viii) 20x ≡101 mod 637.
9.4
Groups in Modular Arithmetic
In this section we consider brieﬂy the construction and properties of groups with
operation addition modulo n or multiplication modulo n. This builds on some of
the examples and exercises in chapter 8.

474
Introduction to Number Theory
Addition Modulo n
In example 8.5.5, we saw that the set Z5 = {[0], [1], [2], [3], [4]} of congruence
classes modulo 5 is a group under the operation of addition modulo 5, +5. The
Cayley table for this group is given on page 376.
Clearly the congruence class [1] is a generator for Z5 since each [n] ∈Z5 can be
expressed as
[n] = n.[1] = [1] + [1] + · · · + [1]
←−−−−−n terms −−−−−→
.
Hence (Z5, +5) is a cyclic group.
In this case, every non-zero congruence class is a generator. For example, [2] is a
generator since
1.[2] = [2]
2.[2] = [2] +5 [2] = [4]
3.[2] = [4] +5 [2] = [1]
4.[2] = [1] +3 [2] = [3]
5.[2] = [3] +5 [2] = [0].
Hence every element of Z5 can be written as n.[2] for some integer n.
Example 9.17
We now consider addition modulo 6. The set of congruence classes is
Z6 = {[0], [1], [2], [3], [4], [5]}
and the Cayley table for +6, addition modulo 6, is the following.
+5
[0]
[1]
[2]
[3]
[4]
[5]
[0]
[0]
[1]
[2]
[3]
[4]
[5]
[1]
[1]
[2]
[3]
[4]
[5]
[0]
[2]
[2]
[3]
[4]
[5]
[0]
[1]
[3]
[3]
[4]
[5]
[0]
[1]
[2]
[4]
[4]
[5]
[0]
[1]
[2]
[3]
[5]
[5]
[0]
[1]
[2]
[3]
[4]

Groups in Modular Arithmetic
475
It is clear that (Z6, +6) is a group:
•
+6 is a binary operation on Z6 since only elements of Z6 appear in the
table.
•
+6 is associative—this follows from the fact that ordinary addition of
integers is associative (see example 8.5.5).
•
[0] is the identity element since, for all [n] ∈Z6,
[0] +6 [n] = [n] = [n] +6 [0].
•
Each [n] ∈Z6 has inverse [n]−1 = [6 −n] ∈Z6 since
[n] +6 [6 −n] = [6] = [0].
For example, [4]−1 = [2] since [4] +6 [2] = [0].
As with Z5, it is clear that [1] is a generator for Z6 so (Z6, +6) is a cyclic group.
In this case, however, the congruence class [2] is not a generator. To see this note
that:
1.[2] = [2]
2.[2] = [2] +6 [2] = [4]
3.[2] = [4] +6 [2] = [0].
Hence [2] has order 3 and generates the subgroup {[0], [2], [4]} of Z6.
The general situation for addition modulo n is captured by the following theorem.
Theorem 9.17
Let n > 1 be a ﬁxed integer. Then the set of congruence classes modulo n,
Zn = {[0], [1], [2], . . . , [n −1]},
forms a cyclic group under addition modulo n, +n, with generator [1].
Multiplication Modulo n
Suppose we wish to ‘build’ a group S whose elements are congruence classes
using multiplication modulo n (for some positive integer n). Clearly [1] is the

476
Introduction to Number Theory
identity element under multiplication modulo n since, for any congruence class
[k],
[1] ×n [k] = [1.k] = [k].
It is also clear that S cannot contain the congruence class [0]. This is because, for
all congruence classes [k], we have
[0] ×n [k] = [0]
so [0] does not have an inverse under multiplication modulo n. (Any inverse
under multiplication would need to satisfy [k] ×n [0] = [1].) Thus we will always
exclude [0] from S and only consider the set of non-zero congruence classes,
denoted
Z∗
n = {[1], [2], . . . , [n −1]}.
We begin by exploring some examples.
Examples 9.18
1.
The Cayley table for Z∗
5 = {[1], [2], [3], [4]} under multiplication modulo
5 is the given below.
×5
[1]
[2]
[3]
[4]
[1]
[1]
[2]
[3]
[4]
[2]
[2]
[4]
[1]
[3]
[3]
[3]
[1]
[4]
[2]
[4]
[4]
[3]
[2]
[1]
It is clear that Z∗
5 = {[1], [2], [3], [4]} is a group:
•
×5 is a binary operation on Z∗
5 since only elements of Z∗
5 appear
in the table.
•
×5 is associative since ordinary multiplication of integers is
associative.
•
[1] is the identity element (as noted above).
•
Each [n] ∈Z∗
5 has an inverse:
[1]−1 = [1]
since
[1] ×6 [1] = [1]
[2]−1 = [3]
since
[2] ×6 [3] = [1]
[3]−1 = [2]
since
[3] ×6 [2] = [1]
[4]−1 = [4]
since
[4] ×6 [4] = [1].

Groups in Modular Arithmetic
477
2.
We now consider Z∗
6 = {[1], [2], [3], [4], [5]} under multiplication modulo
6. The Cayley table is given below.
×5
[1]
[2]
[3]
[4]
[5]
[1]
[1]
[2]
[3]
[4]
[5]
[2]
[2]
[4]
[0]
[2]
[4]
[3]
[3]
[0]
[3]
[0]
[3]
[4]
[4]
[2]
[0]
[4]
[2]
[5]
[5]
[4]
[3]
[2]
[1]
In this case, the operation ×6 is not a binary operation on the set Z∗
6.
For example, [2], [3] ∈Z∗
6 but [2] ×6 [3] = [0] which is not an element
of Z∗
6. Similarly, [3], [4] ∈Z∗
6 but [3] ×6 [4] = [0] ̸∈Z∗
6. There are
three ‘problem elements’: [2], [3] and [4]. Suppose we remove these three
elements from Z∗
6. This gives the set which we denote
U(6) = Z∗
6 −{[2], [3], [4]} = {[0], [5]}.
The Cayley table for (U(6), ×6) is the following.
×5
[1]
[5]
[1]
[1]
[5]
[5]
[5]
[1]
Although this may appear somewhat trivial, at least multiplication
modulo 6 is a binary operation on U(6) since only elements of U(6)
appear in the table. Furthermore (U(6), ×6) is a group as can be readily
veriﬁed.
Note that the ‘problem elements’ that were removed—[2], [3] and [4] in
this case—are those congruence classes [k] where gcd(k, 6) > 1. For
example, gcd(4, 6) = 2 > 1. Equivalently, the elements that are retained
in U(6) are the classes [k] where k and the modulus 6 are coprime:
U(6) = {[k] ∈Z∗
6 : gcd(k, 6) = 1}.
3.
To explore whether the patterns identiﬁed in the previous two examples
hold more generally, we now consider multiplication modulo 12. The

478
Introduction to Number Theory
Cayley table for Z∗
12 = {[1], [2], , 3], . . . , [11]} is given below.
×12
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[1]
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[2]
[2]
[4]
[6]
[8]
[10]
[0]
[2]
[4]
[6]
[8]
[10]
[3]
[3]
[6]
[9]
[0]
[3]
[6]
[9]
[0]
[3]
[6]
[9]
[4]
[4]
[8]
[0]
[4]
[8]
[0]
[4]
[8]
[0]
[4]
[8]
[5]
[5]
[10]
[3]
[8]
[1]
[6]
[11]
[4]
[9]
[2]
[7]
[6]
[6]
[0]
[6]
[0]
[6]
[0]
[6]
[0]
[6]
[0]
[6]
[7]
[7]
[2]
[9]
[4]
[11]
[6]
[1]
[8]
[3]
[10]
[5]
[8]
[8]
[4]
[0]
[8]
[4]
[0]
[8]
[4]
[0]
[8]
[4]
[9]
[9]
[6]
[3]
[0]
[9]
[6]
[3]
[0]
[9]
[6]
[3]
[10]
[10]
[8]
[6]
[4]
[2]
[0]
[10]
[8]
[6]
[4]
[2]
[11]
[11]
[10]
[9]
[8]
[7]
[6]
[5]
[4]
[3]
[2]
[1]
The ‘problem elements’ are again those where [0] appears in the table
as this indicates where ×12 fails to be a binary operation. For example,
[9], [4] ∈Z∗
12 but [9] ×12 [4] = [0] ̸∈Z∗
12. The problem elements are
[2], [3], [4], [6], [8], [9] and [10] which are those congruence classes [k]
where gcd(k, 12) > 1.
Let U(12) be the set of congruence classes [k] where k is coprime with
the modulus 12,
U(12) = {[k] ∈Z∗
12 : gcd(k, 12) = 1} = {[1], [5], [7], [11]}.
The Cayley table for (U(12), ×12) is given below
×12
[1]
[5]
[7]
[11]
[1]
[1]
[5]
[7]
[11]
[5]
[5]
[1]
[11]
[7]
[7]
[7]
[11]
[1]
[5]
[11]
[11]
[7]
[5]
[1]
Clearly (U(12), ×12) is a group in which each element is self-inverse.
The following theorem summarises the situation for multiplication modulo n.
This theorem gives a solution to the questions posed in exercise 8.2.4 and
exercise 8.3.13.

Public Key Cryptography
479
Theorem 9.18
Let n > 1 be a ﬁxed integer. Let
U(n) = {[k] ∈Z∗
n : gcd(k, n) = 1}
be the set of non-zero congruence classes [k] modulo n where k and n are
coprime. Then (U(n), ×n) is a group.
In particular, if p is prime, then
Z∗
p = {[1], [2], . . . , [p −1]}
is a group under multiplication modulo p.
9.5
Public Key Cryptography
Cryptography, or encryption, is the process of converting a message, or
plaintext, into an unintelligible sequence of characters, or ciphertext, which
would be meaningless if intercepted by a third party. The ﬁrst use of cryptography
dates back over two millennia and, over the centuries, many methods, simple
and complex, mechanical and mathematical, have been used to devise and to
attempt to break encryption schemes. In this chapter, we shall describe one of
the more recent encryption schemes, based on simple number theory, that is now
in widespread use.
Although the terms such as ‘code making’ and ‘code breaking’ are in common
everyday use for converting messages into ciphertext and vice versa, encryption
has quite a different purpose from the codes introduced in the previous chapter.
There the emphasis was on introducing redundancy into the encoded words so
that errors could be detected and, if possible, corrected. In the present case the
emphasis is ensuring that the message words are sufﬁciently scrambled so that
their meaning is hidden from an ‘enemy’ who may intercept the message.
Suppose Alice wants to send a message M to Bob which she wishes to keep secret
from Eve who is eavesdropping†. Alice encrypts the message to give an encrypted
message M ′ which she transmits. When Bob receives M ′, he decrypts it to obtain
the original message M. The general scheme is represented in ﬁgure 9.2.
† It is quite common that the protagonists in this scenario are called Alice, Bob and Eve respectively.

480
Introduction to Number Theory
Alice
M´
M´
M
Bob
Eve
M
encryption
decryption
Figure 9.2 Encrypting and decrypting a message
Example 9.19
Suppose Alice encrypts her message by shifting each letter three places along
the alphabet. This encryption method is often known as a ‘Caesar cypher’ after
Julius Caesar who is reported to have used it to send messages which had military
signiﬁcance.
Thus, for example, the plaintext message ‘send help’ encrypts to the
ciphertext ‘vhqg khos’. To decrypt the message Bob needs to shift each letter
back three places (or, equivalently, shift along 23 places). So Bob can decrypt
the encrypted message ‘vhqg khos’ to obtain the original message ‘send
help’.
Although very straightforward, the previous example illustrates some general
points. To encrypt a message, Alice uses an encryption algorithm (here ‘shift
along n places’) and an encryption key (here n = 3) which is a parameter
that speciﬁes how the algorithm operates.
To decrypt the message that he
receives, Bob also uses a decryption algorithm (say, ‘shift along m places’) and a
decryption key (here m = 23).
The disadvantage with this cypher (apart from being very easy to ‘crack’!) is
that Bob needs to know Alice’s encryption key n = 3 to obtain his decryption
key m = 26 −n = 23. Furthermore, knowing the encryption key allows the
decryption key to be calculated. Thus if Eve obtains the encryption key, she can
decrypt the encrypted message M ′. This means that Alice and Bob have to agree
in advance what key they are going to use and then ensure that this information is
kept secret.
Until the advent of public key encryption systems, this was a key difﬁculty (excuse
the pun) of cryptography: the sender and receiver needed to agree, in advance, on
the encryption key and this information needed to be kept secret. Agreeing or
communicating the key could provide a signiﬁcant security challenge in its own
right before the secure communication of any messages could be attempted.

Public Key Cryptography
481
The idea behind public key cryptography is that the encryption key can be made
public but the decryption key is kept secret. For this to be effective, of course, it
must be the case that knowing the encryption key does not allow the decryption
key to be calculated. In this scheme, if Alice wishes to send a message to Bob,
she encrypts it using Bob’s public key that he publishes to the world. Only Bob,
who knows the secret decryption key, can decrypt the message. This solves the
problem of Alice and Bob having to agree a key in advance. This scheme is
illustrated in ﬁgure 9.3.
Alice
M´
M´
M
Bob
Eve
M
Bob’s public
encryption key
B
Bob’s private
decryption key
B
Figure 9.3 Encrypting and decrypting a message using public and private keys
We shall describe a public key encryption system based on number theory, but
ﬁrst we need a little theory.
Euler’s Phi Function
Deﬁnition 9.6
Euler’s phi function φ : Z+ →Z+ is deﬁned by
φ(n) = number of integers a where 1 ⩽a ⩽n which are coprime to n.
=
¯¯{a ∈Z+ : 1 ⩽a ⩽n and gcd(a, n) = 1}
¯¯ .
Example 9.20
φ(6) = 2 since 6 is coprime only with 1 and 5 in the range 1 ⩽a ⩽6.
φ(7) = 6 since 7 is coprime with every integer in the range 1 ⩽a < 7.
φ(8) = 4 since 8 is coprime only with 1, 3, 5 and 7 in the range 1 ⩽a ⩽8.

482
Introduction to Number Theory
Generalising the result φ(7) = 6 above, if p is prime then every integer in the
range 1 ⩽a < p is coprime with p. This gives the following theorem.
Theorem 9.19
If p is prime then φ(p) = p −1.
In theorem 9.18 in the previous section, we described the group U(n) under
multiplication modulo n. Since U(n) is the set of non-zero congruence classes
[k] modulo n where k and n are coprime, we have |U(n)| = φ(n). Using the
properties of groups we can establish the following theorem.
Theorem 9.20: Euler’s Theorem
If a and n are coprime then aφ(n) ≡1 mod n.
Proof
From exercise 8.4.14 we know that if G is a group of order N, |G| = N, and
g ∈G then gN = e.
We apply this result to U(n) where |U(n)| = φ(n). If a is coprime with n then the
congruence class [a] belongs to U(n). Hence [a]φ(n) = [1], the identity element
of U(n), by the result above. In other words
aφ(n) ≡1
mod n,
as required.
□
An immediate consequence of theorems 9.19 and 9.20 is the following.
Theorem 9.21: Fermat’s little theorem
If p is prime then ap−1 ≡1 mod p for all 1 ⩽a ⩽p −1.

Public Key Cryptography
483
The last theoretical result we need to describe our encryption–decryption system
is the following.
Theorem 9.22
If m and n are coprime then φ(mn) = φ(m)φ(n).
Proof
If m = 1 or n = 1 the result is trivial since φ(1) = 1, so suppose m > 1 and
n > 1.
First arrange the mn integers 1, 2, . . . , mn into an array with n rows and m
columns as follows.
1
2
3
. . .
m
m + 1
m + 2
m + 3
. . .
2m
. . .
. . .
. . .
. . .
(n −1)m + 1
(n −1)m + 2
(n −1)m + 3
. . .
nm
Now φ(mn) is the number of integers a in this array such that gcd(a, mn) = 1.
Since m and n are coprime, gcd(a, mn) = 1 if and only if a and m are coprime
and a and n are coprime. Thus we need to count the number of integers in the
array that are coprime with both m and n.
In any column, all of the integers are congruent modulo m. Therefore φ(m) of
the columns contain only integers coprime with m and the remaining m −φ(m)
columns contain only integers a with gcd(a, m) > 1.
Any column of integers coprime with m comprises integers of the form
c, m + c, 2m + c, . . . , (n −1)m + c.
Since m and n are coprime, these are all different mod n.
Therefore such
a column contains φ(n) integers that are coprime with n.
Therefore there
are φ(m)φ(n) integers in the array that are coprime with both m and n, so
φ(mn) = φ(m)φ(n).
□
Example 9.21
Calculate φ(56).

484
Introduction to Number Theory
Note that 56 = 7×8 where 7 and 8 are coprime. Also φ(7) = 6 (since 7 is prime)
and φ(8) = 4 (example 9.20 above). Therefore φ(56) = φ(7)φ(8) = 6 × 4 = 24.
This is clearly simpler than calculating φ(56) directly. In fact, the 24 integers that
are coprime with 56 are:
1, 3, 5, 9, 11, 13, 15, 17, 19, 23, 25, 27, 29, 31, 33, 37, 39, 41, 43, 45, 47, 51, 53, 55.
RSA Encryption
RSA† encryption is based on the fact that factorising very large numbers is so
time consuming that, for sufﬁciently large integers, it is practically impossible
even using the fastest supercomputers.
The process is as follows. Bob chooses two very large prime numbers p and q (say,
80 digits or more) but keeps these private. Let n = pq. Then n is a very large
number; it will be made public but, given the practical impossibility in factorising
very large numbers, its factors p and q remain private to Bob.
Now
φ(n) = (p −1)(q −1)
by theorems 9.19 and 9.22. Bob can use this result to evaluate φ(n) easily since
he knows the factorisation n = pq. However, without knowing this factorisation,
it will not be possible for anyone (Eve, for example) to evaluate φ(n).
To illustrate the point but with relatively small numbers, suppose we are required
to evaluate φ(640 817).
Without any further information, this seems very
daunting—we need to ﬁnd how many integers a in the range 1 ⩽a ⩽640 816
are coprime with 640 817. However, if we are Bob, we constructed n = 640 817
in the ﬁrst place so we will know that n factorises as
n = 773 × 829
which are both prime. Hence we are able to evaluate φ(640 817) easily:
φ(640 817) = 772 × 828 = 639 216.
Bob then chooses e such that e and φ(n) are coprime. Bob also calculates d such
that
de ≡1
mod φ(n)
† This is named after the three people credited with discovering the technique in 1977, Ron Rivest,
Adi Shamir, Leonard Adleman. However it is now believed that this scheme was known earlier to
cryptographers at the UK Government Communications Headquarters, GCHQ.

Public Key Cryptography
485
but keeps d private. It will not be possible for anyone else to calculate d since,
as we have seen, it is not practically possible to ﬁnd φ(n) without knowing the
factorisation n = pq which only Bob knows.
Bob’s public encryption key is the pair (n, e).
Encoding
Suppose Alice has a message M which is an integer in the range 0 ⩽M < n.
In practice letters are represented as numerical values, for example using their
ASCII code. Hence any text message may be split into a sequence of integers so
it is sufﬁcient to be able to encrypt integer values. Then M is encoded as M ′
where
M ′ ≡M e
mod n.
Note that the encoding only involves the public key (e, n). The message M ′ is
transmitted.
Decoding
Bob receives M ′. Note that, since de ≡1 mod φ(n), we have de = kφ(n) + 1
for some k ∈Z. Now
(M ′)d = (M)ed
= (M)kφ(n)+1
= ((M)φ(n))k × M
≡1k × M
mod n
(by theorem 9.20)
≡M
mod n.
Therefore, Bob calculates (M ′)d mod N to recover the message M. Bob’s
private decryption key is the integer d.
Summary
Encoding rule:
M ′ ≡M e mod n where e and φ(n) are coprime;
n and e are made public.
Decoding rule:
M ≡(M ′)d mod n where d satisﬁes de ≡1 mod φ(n);
d and φ(n) are kept private.
Examples 9.22
1.
Our ﬁrst example is a ‘toy example’ with small primes just to illustrate
the process of encryption and decryption. Let p = 7 and q = 13 so that
n = pq = 91 and φ(n) = 6 × 12 = 72.

486
Introduction to Number Theory
Firstly we choose an encryption power e = 5, which is coprime with
φ(n), so that the encryption rule is
M ′ ≡M 5
mod 143.
Since φ(n) = 72, the decryption power d satisﬁes
5d ≡1
mod 72.
Since
5 × 14 = 70 ≡−2
mod 72
and
5 × 15 = 75 ≡3
mod 72,
adding the two equations gives
5 × 29 ≡1
mod 72.
Hence the decryption power is d = 29 and the decryption rule is
M = (M ′)29
mod 91.
Let M = 31. We will encrypt M to obtain M ′ and then decrypt M ′ to
obtain the original message M. Encryption is straightforward:
M ′ = 315 = 28 629 151 ≡5
mod 91.
To decrypt the encrypted message M ′ = 5 we need to evaluate 529
mod 91. We will do this in stages, as follows.
(M ′)3
=
125
≡
34
mod 91
⇒
(M ′)6
=
342 = 1156
≡
64
mod 91
⇒
(M ′)12
=
642 = 4096
≡
1
mod 91
⇒
(M ′)24
≡
1
mod 91
⇒
(M ′)29
≡
55 = 53 × 52
=
34 × 25 = 850
≡
31
mod 31.
Hence we have retrieved the original message M = 31.
Suppose instead we had tried the encryption rule M ′ ≡M 15 mod 143.
Then the decryption power d satisﬁes
15d ≡1
mod 72.

Public Key Cryptography
487
Since gcd(15, 72) = 3 and 3 does not divide 1, this congruence has no
solution by theorem 9.14. This illustrates the necessity of selecting the
encryption power e that is coprime with φ(n).
2.
Let p = 89 and q = 97. These are still not very large primes but will
illustrate the process in perhaps a slightly more realistic manner. Then
n = pq = 89 × 97 = 8633.
Now φ(n) = (p −1)(q −1) = 88 × 96 = 8448. We need an integer
e coprime to 8448. Choose e = 7, say. The public encryption key is
(n, e) = (8633, 7).
To ﬁnd the private decryption key d, we need to solve
de ≡1
mod φ(n); in other words 7d ≡1 mod 8448. Now 7×1207 = 8449 ≡
1 mod 8448, so d = 1207.
Therefore we have the following (public) encryption and (private)
decryption scheme.
Encryption: M ′ ≡M 7 mod 8633.
Decryption: M ≡(M ′)1207 mod 8633.
For example, suppose a message is numerically represented as M = 359.
This encodes to
3597 = 768 530 557 342 240 919 ≡1030
mod 8633.
To decode we need to calculate 10301207 mod 8633. This may seem
difﬁcult but this can be calculated in stages. For example, one way of
doing this is the following.
10304
≡4583
mod 8633
⇒
103012
= (10304)3 ≡45833 ≡7131
mod 8633
⇒
103060
= (103012)5 ≡71315 ≡5479
mod 8633
⇒
1030300
= (103060)5 ≡54795 ≡5870
mod 8633
⇒
10301200
= (1030300)4 ≡58704 ≡2036
mod 8633
⇒
10301207
= 10301200 × 10307
≡2036 × 3564 ≡359
mod 8633.
Hence 10301207 ≡359 mod 8633 which decodes the message sent.
3.
This time we will be a little more ambitious and chose the two primes to
be p = 199 and q = 211. Hence n = pq = 199 × 211 = 41 989. Then
φ(n) = (p −1)(q −1) = 198 × 210 = 41 580.

488
Introduction to Number Theory
Finally we choose the encryption power to be e = 13 so that the
encryption rule is
M ′ = M 13
mod 41 989.
Firstly, we will encrypt the message M = 2171. We do this, ﬁrstly, by
repeated squaring as follows.
M 2 = 21712 = 4 713 241
≡10 473
mod 41 989
⇒
M 4 = 10 4732 = 109 683 729
≡8461
mod 41 989
⇒
M 8 = 84612 = 71 588 521
≡39 265
mod 41 989.
Therefore
M 13 = M 8 × M 4 × M
= 39 265 × 8461 × 2171
= 721 252 149 215
≡74
mod 41 989.
Therefore M = 2171 encrypts to M ′ = 74.
Assuming that we are the receiver of the encrypted message, we shall
now decrypt M ′ to retrieve the original message M. First we need to ﬁnd
the decryption power d that satisﬁes ed ≡1 mod φ(n). In our case this
is
13d ≡1
mod 41 580.
Now
3198 × 13 = 41 574 ≡−6
mod 41 580
so
3199 × 13 = 41 574 ≡7
mod 41 580.
Hence
(3198 + 3199) × 13 = 6397 × 13 ≡= 1
mod 41 580.
Therefore d = 6397 so the decryption rule is
M = (M ′)6397
mod 41 989.
We therefore need to evaluate
746397
mod 41 989

Public Key Cryptography
489
which seems extremely daunting but, with persistence, can be achieved
by repeatedly squaring as follows.
(M ′)2
=
742 = 5476
⇒
(M ′)4
=
54762 = 29 986 576
≡
6430
mod 41 989
⇒
(M ′)8
=
64302 = 41 344 900
≡
27 724
mod 41 989
⇒
(M ′)16
=
27 7242 = 768 620 176
≡
11 531
mod 41 989
⇒
(M ′)32
=
11 5312 = 132 963 961
≡
26 787
mod 41 989
⇒
(M ′)64
=
26 7872 = 717 543 369
≡
35 337
mod 41 989
⇒
(M ′)128
=
35 3372 = 1248 703 569
≡
34 687
mod 41 989
⇒
(M ′)256
=
34 6872 = 1203 187 969
≡
35 163
mod 41 989
⇒
(M ′)512
=
35 1632 = 1236 436 569
≡
28 475
mod 41 989
⇒
(M ′)1024
=
28 4752 = 810 825 625
≡
18 035
mod 41 989
⇒
(M ′)2048
=
18 0352 = 325 261 225
≡
14 431
mod 41 989
⇒
(M ′)4096
=
14 4312 = 208 253 761
≡
30 310
mod 41 989.
Now we need to write the decryption power 6397 as a sum of powers of
2; essentially, we need to write 6397 as a binary numeral. We have
6397 = 4096 + 2048 + 128 + 64 + 32 + 16 + 8 + 4 + 1
so that
(M ′)6397 = (M ′)4096 × (M ′)2048 × (M ′)128 × (M ′)64
× (M ′)32 × (M ′)16 × (M ′)8 × (M ′)4 × M ′.

490
Introduction to Number Theory
Again we need to evaluate this in stages, as follows.
(M ′)6144
=
(M ′)4096 × (M ′)2048
=
30 310 × 14 431 = 43 7403 610
≡
4197
mod 41 989
⇒
(M ′)6272
=
(M ′)6144 × (M ′)128
=
4197 × 34 687 = 145 581 339
≡
5476
mod 41 989
⇒
(M ′)6336
=
(M ′)6272 × (M ′)64
=
5476 × 35 337 = 193 505 412
≡
20 100
mod 41 989
⇒
(M ′)6368
=
(M ′)6336 × (M ′)32
=
20 100 × 26 787 = 538 418 700
≡
35 742
mod 41 989
⇒
(M ′)6384
=
(M ′)6368 × (M ′)16
=
35 742 × 11 531 = 412 141 002
≡
18 967
mod 41 989
⇒
(M ′)6392
=
(M ′)6384 × (M ′)8
=
18 967 × 27 724 = 525 841 108
≡
12 861
mod 41 989
⇒
(M ′)6397
=
(M ′)6392 × (M ′)4 × M ′
=
12 861 × 6430 × 74 = 6119 521 020
≡
2171
mod 41 989.
Hence we have successfully decrypted M ′ = 94 to obtain the original
message M = 2171.
Exercises 9.4
1.
Evaluate each of the following, where φ is Euler’s phi function.
(i)
φ(24)
(ii)
φ(30)
(iii)
φ(31)
(iv)
φ(32)
(v)
φ(170)
(vi)
φ(195)

Public Key Cryptography
491
(vii)
φ(323)
(viii) φ(385)
2.
(i)
Evaluate φ(9), φ(25) and φ(49).
(ii)
Conjecture the value of φ(p2) where p is prime.
Prove your
conjecture.
3.
An RSA code is designed with p = 11 and q = 19 (so that n = pq =
209).
The encryption rule for a message M, is M ′ ≡M 7 mod 209, where
1 ⩽M ⩽209.
(i)
Encrypt each of the following ‘messages’.
(a)
M = 7
(b)
M = 14
(c)
M = 195
(ii)
Find the integer d (in the range 1 ⩽d ⩽φ(n)) such that the
decryption rule is
M ≡(M ′)d
mod 209.
Hence, or otherwise, decrypt each of the following ‘messages’.
(a)
M ′ = 3
(b)
M ′ = 174
(c)
M ′ = 20
4.
For each of the following RSA encryption schemes, M −→M ′, ﬁnd the
decryption scheme; that is, ﬁnd d so that M = (M ′)d mod n.
(i)
M ′ = M 13 mod 55
(ii)
M ′ = M 17 mod 57
(iii)
M ′ = M 7 mod 143
(iv)
M ′ = M 11 mod 247

Chapter 10
Boolean Algebra
10.1
Introduction
In chapter 3 we noted the strong similarity between the algebra of sets and that
of propositions. In particular, each of the laws listed in §3.5 has a counterpart
in §1.5 to which it bears more than a passing resemblance. For example, De
Morgan’s laws for the propositions p and q are given by p ∨q ≡¯p ∧¯q and
p ∧q ≡¯p ∨¯q. For the sets A and B these laws take the form (A ∪B) = ¯A ∩¯B
and (A ∩B) = ¯A∪¯B. In this chapter we shall see that the laws common to these
two systems are attributable to their relationship to an algebraic structure known
as a ‘Boolean algebra’ and that the properties which they share are those which
are common to all Boolean algebras.
The idea of a Boolean algebra was ﬁrst developed by George Boole† in the
middle of the nineteenth century. Boole was chieﬂy concerned with the algebra
of propositions but, in recent years, the subject has been extended and is now a
signiﬁcant component of abstract algebra. An important application of Boolean
algebra is in the analysis of electronic circuits and hence in the design of a range
of digital devices such as computers, telephone systems and electronic control
systems.
† Boole’s book The Laws of Thought published in 1854 was an attempt to formalize the process of
logical thinking.
492

Introduction
493
Deﬁnition 10.1
A Boolean algebra consists of a set B together with three operations
deﬁned on that set. These are:
(a)
a binary operation denoted by ⊕referred to as the sum (or join);
(b)
a binary operation denoted by ∗referred to as the product (or
meet);
(c)
an operation which acts on a single element of B, denoted by ¯,
where, for any element b ∈B, the element ¯b ∈B is called the
complement of b. (An operation which acts on a single member
of a set S and which results in a member of S is called a unary
operation.)
The following axioms apply to the set B together with the
operations ⊕, ∗and¯.
B1.
Distinct identity elements belonging to B exist for each of the
binary operations ⊕and ∗and we denote these by 0 and 1
respectively. Thus we have
b ⊕0 = 0 ⊕b = b
b ∗1 = 1 ∗b = b
for all b ∈B.

494
Boolean Algebra
Deﬁnition 10.1 (continued)
B2.
The operations ⊕and ∗are associative, that is
(a ⊕b) ⊕c = a ⊕(b ⊕c)
(a ∗b) ∗c = a ∗(b ∗c)
for all a, b, c ∈B.
B3.
The operations ⊕and ∗are commutative, that is
a ⊕b = b ⊕a
a ∗b = b ∗a
for all a, b ∈B.
B4.
The operation ⊕is distributive over ∗and the operation ∗is
distributive over ⊕, that is
a ⊕(b ∗c) = (a ⊕b) ∗(a ⊕c)
a ∗(b ⊕c) = (a ∗b) ⊕(a ∗c)
for all a, b, c ∈B.
B5.
For all b ∈B, b ⊕¯b = 1 and b ∗¯b = 0.
A Boolean algebra with underlying set B, binary operations ⊕and ∗, complement
operation¯, and identity elements 0 and 1 is denoted by (B, ⊕, ∗,¯, 0, 1).
There are a number of alternative notations for the sum and product operations.
Some authors use ∨and ∧, others use + and ×. However, all of these symbols
tend to have connotations associated with their use for speciﬁc operations and
we therefore prefer to use the more neutral symbols ⊕and ∗for general binary
operations.
A casual glance at axiom B5 may lead you to conclude that ¯b is the inverse of b.
This is not so. Remember that, if b−1 is the inverse of b, then b ∗b−1 gives the
identity with respect to the operation ∗. However, b ⊕¯b gives the identity with
respect to ∗and b∗¯b gives the identity with respect to ⊕, so that ¯b is not the inverse
of b with respect to either operation.

Introduction
495
One ﬁnal word of caution: note that 0 and 1 are used here as symbols for the two
identity elements and not for the numbers which they conventionally symbolize.
We must therefore be careful not to make assumptions which are true for the
integers 0 and 1 but not necessarily so for identities in general.
Examples 10.1
1.
The simplest Boolean algebra (and also the one of most interest to
computer scientists, as we shall see later) consists of the set B = {0, 1}
together with the binary operations ⊕and ∗and complement operation¯
deﬁned by the following tables.
⊕
0
1
0
0
1
1
1
1
∗
0
1
0
0
0
1
0
1
b
¯b
0
1
1
0
We leave it as an exercise to verify that the axioms B1–B5 hold.
2.
Let S be a non-empty set and consider P(S), the power set of S, together
with the binary operations of union and intersection and the operation of
complementation, where, for all A ∈P(S), ¯A = S −A.
We established the following results in sections 3.5 and 3.6:
(a)
the operations ∪and ∩are associative;
(b)
the operations ∪and ∩are commutative;
(c)
the operation ∪is distributive over ∩and ∩is distributive over ∪;
(d)
the sets ∅and S belong to P(S) and
A ∪∅= ∅∪A = A
A ∩S = S ∩A = A
for all A ∈P(S). Thus ∅and S are the identities for ∪and ∩
respectively;
(e)
for any A ∈P(S), ¯A ∈P(S) and A ∪¯A = S and A ∩¯A = ∅.
Since these are precisely the axioms B1–B5 we can conclude that
(P(S), ∪, ∩,¯, ∅, S) is a Boolean algebra.
The sum and product
operations are union and intersection respectively, and we can write
0 = ∅and 1 = S for the two identities.

496
Boolean Algebra
In fact we can replace P(S) by any non-empty family of sets which is
closed under the operations of union, intersection and complementation.
The resulting structure is also a Boolean algebra.
3.
Let B be a set of propositions which is closed under the operations of
conjunction, disjunction and negation and where equality of propositions
is interpreted as their logical equivalence.
In chapter 1 we showed that the operations ∨and ∧are associative,
commutative and that each is distributive over the other. If we denote
a contradiction (a proposition which is always false) by f and a tautology
(a proposition which is always true) by t then f and t must belong to B.
This is so because, for any proposition p belonging to B, p ∧¯p ≡f and
p ∨¯p ≡t belong to B by the closure properties. Further, for any p ∈B,
we have
p ∨f ≡f ∨p ≡p
p ∧t ≡t ∧p ≡p
so that f and t are the identities for the binary operations ∨and
∧respectively.
All contradictions are logically equivalent as are all
tautologies so that t and f are unique elements of B.
The structure (B, ∨, ∧,¯, f, t) satisﬁes the axioms B1–B5 and is therefore
a Boolean algebra.
The operations ∨and ∧correspond to ⊕and ∗
respectively and for the identity elements we have 0 = f and 1 = t.
10.2
Properties of Boolean Algebras
In chapters 1 and 3 we considered the duality principle as it applied to the algebras
of propositions and of sets. We shall now see that this principle applies to all
Boolean algebras.
Given any proposition about a Boolean algebra, we deﬁne its dual to be the
proposition obtained by substituting ⊕for ∗, ∗for ⊕, 0 for 1 and 1 for 0.

Properties of Boolean Algebras
497
For example, given the elements a, b of a Boolean algebra, the dual of
(a ⊕b) ∗a ∗¯b = 0
is
(a ∗b) ⊕a ⊕¯b = 1.
Each of the Boolean algebra axioms B1–B5 is actually a pair of axioms. Within
a pair, each axiom is the dual of the other. Now suppose that, using the axioms,
we can prove some theorem about a Boolean algebra. It follows that the dual of
that theorem can also be proved by using, in the same sequence, the duals of the
axioms used to prove the ﬁrst. This is the essence of the duality principle. Every
time we prove a theorem about a Boolean algebra, we can, by appealing to this
principle, simply state that the theorem which is the dual also holds. In Boolean
algebra duality gives us ‘two theorems for the price of one’.
Duality Principle
For any theorem about a Boolean algebra, the dual is also a theorem.
We will now use the axioms and the duality principle to prove some theorems
about the general Boolean algebra (B, ⊕, ∗,¯, 0, 1). The ﬁrst result is simply a
restatement of theorem 8.1. This theorem says that, for any binary operation, if
an identity element exists, then it is unique.
Theorem 10.1
The identity elements 0 and 1 are unique.
We now show that the complement of any element of a Boolean algebra (as
deﬁned in axiom B5) is also unique.
Theorem 10.2
Given an element b ∈B, there is only one element ¯b ∈B such that
b ⊕¯b = 1 and b ∗b = 0 (i.e. which satisﬁes axiom B5).

498
Boolean Algebra
Proof
Suppose that ¯b1 and ¯b2 are both complements of an element b of a Boolean algebra
(B, ⊕, ∗,¯, 0, 1). This means that
b ⊕¯b1 = ¯b1 ⊕b = 1
b ⊕¯b2 = ¯b2 ⊕b = 1
b ∗¯b1 = ¯b1 ∗b = 0
b ∗¯b2 = ¯b2 ∗b = 0.
Thus we have
¯b1 = ¯b1 ∗1
(axiom B1)
= ¯b1 ∗(b ⊕¯b2)
= (¯b1 ∗b) ⊕(¯b1 ∗¯b2)
(axiom B4)
= 0 ⊕(¯b1 ∗¯b2)
= 0 ⊕(¯b2 ∗¯b1)
(axiom B3)
= (¯b2 ∗b) ⊕(¯b2 ∗¯b1)
= ¯b2 ∗(b ⊕¯b1)
(axiom B4)
= ¯b2 ∗1
= ¯b2
(axiom B1).
We have shown that ¯b1 = ¯b2 and so we can conclude that the complement is
unique.
□
As we have already noted, the laws common to the algebra of sets and
propositions are examples of general results which apply to all Boolean algebras.
In fact some authors prefer to include some or all of these in the list of Boolean
algebra axioms. This is not necessary since, as we shall now see, all can be proved
using only the axioms B1–B5.
Theorem 10.3 Idempotent laws
For all b ∈B
b ⊕b = b
and
b ∗b = b.
Proof
For all b ∈B we have
b = b ⊕0
(axiom B1)

Properties of Boolean Algebras
499
= b ⊕(b ∗¯b)
(axiom B5)
= (b ⊕b) ∗(b ⊕¯b)
(axiom B4)
= (b ⊕b) ∗1
(axiom B5)
= b ⊕b
(axiom B1).
We have proved that b ⊕b = b. The result b ∗b = b follows by the duality
principle.
□
Theorem 10.4 Identity laws
For all b ∈B
1 ⊕b = b ⊕1 = 1
and
0 ∗b = b ∗0 = 0.
Proof
The proof of this theorem is left as an exercise (see exercise 10.1.5).
□
Theorem 10.5 Absorption laws
For all b1, b2 ∈B
b1 ⊕(b1 ∗b2) = b1
and
b1 ∗(b1 ⊕b2) = b1.
Proof
For all b1, b2 ∈B
b1 ⊕(b1 ∗b2) = (b1 ∗1) ⊕(b1 ∗b2)
(axiom B1)
= b1 ∗(1 ⊕b2)
(axiom B4)
= b1 ∗1
(theorem 10.4)
= b1
(axiom B1).
By the duality principle, we have
b1 ∗(b1 ⊕b2) = b1.
□

500
Boolean Algebra
Theorem 10.6 Involution law
For all b ∈B,¯¯b = b.
Proof
Since b ⊕¯b = ¯b ⊕b = 1 and b ∗¯b = ¯b ∗b = 0, it follows that b is the complement
of ¯b. We have already proved (theorem 10.2) that the complement of any element
is unique so that ¯¯b = b.
□
Theorem 10.7 De Morgan’s laws
For all b1, b2 ∈B
(b1 ⊕b2) = ¯b1 ∗¯b2
and
(b1 ∗b2) = ¯b1 ⊕¯b2.
Proof
(b1 ⊕b2) ⊕(¯b1 ∗¯b2) = [(b1 ⊕b2) ⊕¯b1] ∗[(b1 ⊕b2) ⊕¯b2]
(axiom B4)
= [¯b1 ⊕(b1 ⊕b2)] ∗[(b1 ⊕b2) ⊕¯b2]
(axiom B3)
= [(¯b1 ⊕b1) ⊕b2] ∗[b1 ⊕(b2 ⊕¯b2)]
(axiom B2)
= (1 ⊕b2) ∗(b1 ⊕1)
(axiom B5)
= 1 ∗1
(theorem 10.4)
= 1
(axiom B1).
We have proved that (b1 ⊕b2) ⊕(¯b1 ∗¯b2) = 1 so that ¯b1 ∗¯b2 is the complement
of b1 ⊕b2, i.e. (b1 ⊕b2) = ¯b1 ∗¯b2.
That (b1 ∗b2) = ¯b1 ⊕¯b2 follows from the duality principle.
□
Theorem 10.8
¯0 = 1
and
¯1 = 0.

Properties of Boolean Algebras
501
Proof
The proof is left as an exercise (see exercise 10.1.6).
□
Exercises 10.1
1.
Evaluate the following for the Boolean algebra ({0, 1}, ⊕, ∗,¯, 0, 1) as
deﬁned in example 10.1.1.
(i)
(0 ⊕1) ∗0
(ii)
0 ∗¯1
(iii)
(1 ∗1) ⊕(0 ∗¯0)
(iv)
¯1 ⊕[(0 ∗1) ∗1]
(v)
[(0 ∗1) ∗1) ∗(¯1 ⊕1)] ⊕1
(vi)
[1 ⊕(¯1 ∗1)] ∗(¯0 ⊕0)
(vii)
[(1 ∗1) ⊕¯0] ∗[(1 ⊕0) ∗1].
2.
Consider the set of real numbers R, together with the binary operations
of addition and multiplication. Which of the Boolean algebra axioms B1,
B2, B3, B4 are not satisﬁed? Is it possible to deﬁne a unary operation on
R so that axiom B5 holds?
3.
Let B = {1, 2, 3, 5, 6, 10, 15, 30}, i.e. the set of divisors of 30. Binary
operations denoted by ⊕and ∗and a unary operation denoted by ¯ are
deﬁned as follows: for all b1, b2 ∈B
b1 ⊕b2 = the least common multiple of b1 and b2
b1 ∗b2 = the highest common factor of b1 and b2
¯b1 = 30/b1.
What are the identity elements with respect to ⊕and ∗? Show that B
together with the three operations is a Boolean algebra.
4.
Let B be the set of divisors of 24, so that B = {1, 2, 3, 4, 6, 8, 12, 24}.
For all b1, b2 ∈B deﬁne
b1 ⊕b2 = the least common multiple of b1 and b2
b1 ∗b2 = the highest common factor of b1 and b2
¯b1 = 24/b1.
Is B together with these three operations a Boolean algebra?

502
Boolean Algebra
Suppose B is the set of divisors of 42 with the operations ⊕, ∗and ¯
deﬁned appropriately. Is B together with these operations a Boolean
algebra? What about the set of divisors of 45?
5.
Prove the identity laws (theorem 10.4), i.e. for all b ∈B,
1 ⊕b = b ⊕1 = 1
and
0 ∗b = b ∗0 = 0.
6.
Prove theorem 10.8, i.e. that ¯0 = 1 and ¯1 = 0.
7.
Although the associativity properties are usually given as two of the
axioms of a Boolean algebra, they can be proved from the other axioms.
Show this. You may wish to use the absorption laws (theorem 10.5) but
this is legitimate since they can be proved without utilizing axiom B2.
(Hint: Show that, for any b1, b2, b3 ∈B [(b1⊕b2)⊕b3]∗[b1⊕(b2⊕b3)] =
(b1⊕b2)⊕b3 and also that [(b1⊕b2)⊕b3]∗[b1⊕(b2⊕b3)] = b1⊕(b2⊕b3).)
8.
Let (B, ⊕, ∗,¯, 0, 1) be a Boolean algebra. Prove the following results
hold for all b1, b2, b3 ∈B using the Boolean algebra axioms and any
theorem given in §10.2. State the dual of each result:
(i)
(b1 ⊕b2) ∗¯b1 ∗¯b2 = 0
(ii)
b1 ⊕[(¯b2 ⊕b1) ∗b2] = 1
(iii)
(b1 ⊕b2) ∗(¯b1 ⊕¯b2) = (b1 ∗¯b2) ⊕(¯b1 ∗b2)
(iv)
b1 ∗(¯b1 ⊕b2) = b1 ∗b2
(v)
(b1 ⊕b2 ⊕b3) ∗(b1 ⊕b2) = b1 ⊕b2
(vi)
(b1 ⊕b2) ∗(b1 ⊕b2) = b1 ⊕b2
(vii)
b1 ⊕[b1 ∗(b2 ⊕1)] = b1.
9.
Prove that, in any Boolean algebra (B, ⊕, ∗,¯, 0, 1), b1 ∗¯b2 = 0 if and
only if b1 ∗b2 = b1.
10.
Prove the following cancellation law.
Let (B, ⊕, ∗,¯, 0, 1) be a Boolean algebra and let b1, b2, b3 ∈B. If
b1 ∗b2 = b1 ∗b3 and ¯b1 ∗b2 = ¯b1 ∗b3 then b2 = b3.
Why are both conditions necessary rather than just one of them?
11.
A relation R is deﬁned on the underlying set B of a Boolean algebra
(B, ⊕, ∗,¯, 0, 1) as follows:
b1 R b2
if and only if
b1 ∗b2 = b1.

Boolean Functions
503
(i)
Show that R is a partial order on the set B. (Partial orders are
deﬁned in §4.5.)
(ii)
Show that b1 ∗b2 = b1 if and only if b1 ⊕b2 = b2.
10.3
Boolean Functions
We are already familiar with the idea of a real variable, that is one whose range
of possible values is the set of real numbers or some subset of it. The idea of a
‘Boolean variable’ is similar. It is a variable whose range of possible ‘values’ is
the underlying set B of a Boolean algebra (B, ⊕, ∗,¯, 0, 1).
Deﬁnitions 10.2
(a)
Given a Boolean algebra (B, ⊕, ∗,¯, 0, 1), a Boolean variable is a
variable to which can be assigned elements of the set B.
(b)
Given a Boolean variable x, the complement of x denoted by ¯x, is
a variable which is such that ¯x = ¯b whenever x = b for any b ∈B.
(c)
A literal is a Boolean variable x or its complement ¯x.
A useful notation for distinguishing literals is to write x1 for the variable x and
x0 for ¯x, the complement of x. The two literals associated with the variable x can
then be deﬁned by
xe =
(
¯x
if e = 0
x
if e = 1.
Just as real variables can be combined to form algebraic expressions using such
operations as addition, subtraction, multiplication, etc, so Boolean variables
can be combined to form Boolean expressions.
The operations appearing in
Boolean expressions are of course the Boolean operations of sum, product and
complement. A Boolean expression is deﬁned recursively as follows.

504
Boolean Algebra
Deﬁnition 10.3
Given a Boolean algebra (B, ⊕, ∗,¯, 0, 1), the following are Boolean
expressions in the n Boolean variables x1, x2, . . . , xn:
(a)
the identity elements 0 and 1;
(b)
the Boolean variables x1, x2, . . . , xn;
(c)
(X⊕Y ), (X∗Y ) and ¯X, where X and Y are Boolean expressions.
The following are examples of Boolean expressions: x1⊕¯x2, (x1 ∗x2)⊕(x2∗x3),
(x1 ⊕x2) ∗¯x1, 1 ∗x1 ∗(¯x2 ⊕0).
Note that a Boolean expression in the n variables x1, x2, . . . , xn may not
necessarily contain all n of the variables.
From this point on we will adopt the common practice of omitting the symbol ∗
in Boolean expressions, although we shall continue to include the symbol ⊕. We
shall write x1x2 for x1 ∗x2, x1(x2 ⊕x3) for x1 ∗(x2 ⊕x3), x12 for x1 ∗x1,
etc. This is analogous to the convention we adopted in chapter 8 or to that of
dropping the multiplication sign in algebraic expressions so that xy is interpreted
as ‘x multiplied by y’. As we shall see, Boolean expressions can be lengthy and
omitting this symbol is a notational convenience which makes them easier to write
and to read.
We shall also follow the convention of evaluating products before sums thereby
rendering the use of certain brackets unnecessary.
For example x1x2 ⊕x3
is taken to mean (x1 ∗x2) ⊕x3; similarly, x1x2 ⊕x3x1 is interpreted as
(x1∗x2)⊕(x3∗x1), and so on. Again there is the obvious analogy with evaluating
algebraic expressions where the rule is that multiplication/division is performed
before addition/subtraction. As always, terms which are enclosed in brackets are
evaluated before any others.
In the notation which we shall now use the examples of Boolean expressions given
above are written: x1 ⊕¯x2, x1x2 ⊕x2x3, (x1 ⊕x2)¯x1, 1x1(¯x2 ⊕0).
We have already seen in §10.2 and in exercise 10.1.8 that often the same Boolean
expression can be expressed in a number of different forms. This should come
as no great surprise since it is a familiar feature of the algebra of real variables.
If x and y are real variables x2 + 2xy and (x + y)2 −y2 represent equivalent
expressions in that one can be derived from the other using the rules of elementary
algebra. The situation is the same for Boolean expressions. If one expression

Boolean Functions
505
can be derived from another using the ‘rules’ of Boolean algebra then the two
expressions are equivalent. Equivalent algebraic expressions are such that their
value is the same for any set of values of the variables. So it is with equivalent
Boolean expressions. Substitution of the same elements for the variables gives
the same result.
Deﬁnition 10.4
Two Boolean expressions are said to be equivalent (or equal) if one can be
obtained from the other by a ﬁnite sequence of applications of the Boolean
algebra axioms.
For instance, x1(¯x1 ⊕x2) and x1x2 are equivalent Boolean expressions (see
exercise 10.1.8(iv)) and we can write x1(¯x1 ⊕x2) = x1x2.
Given a Boolean algebra (B, ⊕, ∗,¯, 0, 1), a Boolean expression can be used to
deﬁne a function. As with functions of a real variable, the expression provides
a ‘rule’ for evaluating the function for any element of its domain. For example,
consider the Boolean expression (x1 ⊕x2)¯x1. This deﬁnes a function, f, of the
two variables x1 and x2 as follows:
f(x1, x2) = (x1 ⊕x2)¯x1.
The domain of the function is B × B and its codomain is B.
The following is the formal deﬁnition of a Boolean function.
Deﬁnition 10.5
Given a Boolean algebra (B, ⊕, ∗,¯, 0, 1),
a Boolean function of
the n variables x1, x2, . . . , xn is a function f : Bn →B such that
f(x1, x2, . . . , xn) is a Boolean expression.
It follows from the deﬁnition that equivalent Boolean expressions deﬁne the same
function. For example, consider the two functions
f : B2 →B
f(x1, x2) = x1(¯x1 ⊕x2)

506
Boolean Algebra
g : B2 →B
g(x1, x2) = x1x2.
We have x1(¯x1 ⊕x2) = x1x2 (see exercise 10.1.8), and so the functions f and g
are equal.
Since a particular Boolean expression may have a number of equivalent forms,
there arises the question of how we can decide whether or not two Boolean
expressions are equivalent and hence whether or not two Boolean functions are
equal. Of course we could attempt to derive one from the other using the axioms
and any theorems proved from them. However, this can be time consuming and,
where we are unable to derive one expression from the other, we cannot be sure
whether this is because we have not applied the correct sequence of steps or
because the two expressions are not equivalent. If the latter is the case then all our
efforts are bound to fail. Fortunately there is an alternative method of establishing
the equivalence of two Boolean expressions but, before we can consider this, we
need some more deﬁnitions.
Deﬁnition 10.6
A minterm (or complete product) in the n variables x1, x2, . . . , xn is a
Boolean expression which has the form of the product of each Boolean
variable or its complement. Thus a minterm consists of the product of n
literals, one corresponding to each variable.
For example, there are eight possible minterms in the three variables x1, x2, x3.
These are:
x1x2x3
x1x2¯x3
x1¯x2x3
x1¯x2¯x3
¯x1x2x3
¯x1x2¯x3
¯x1¯x2x3
¯x1¯x2¯x3.
Using the deﬁnition of xe given above, we denote a minterm in the n variables
x1, x2, . . . , xn by me1e2...en where
me1e2...en = x1
e1x2
e2 . . . xn
en.
For example, we have
m10110 = x1
1x2
0x3
1x4
1x5
0
= x1¯x2x3x4¯x5
and
m0111 = x1
0x2
1x3
1x4
1

Boolean Functions
507
= ¯x1x2x3x4.
For the n variables x1, x2, . . . , xn there are 2n possible minterms since each of
the n literals in the minterm can take one of two forms, the respective variable
or its complement. Of these 2n minterms, no two are equivalent. This can be
veriﬁed by appropriate substitution of the values 0 or 1 for each variable. Given
two minterms, it is always possible to assign the value 0 or 1 to each variable so
that evaluating each minterm gives a different result. For example, consider
m010 = ¯x1x1¯x3
and
m111 = x1x2x3.
Substituting x1 = 0, x2 = 1, x3 = 0, we have m010 = 1 and m111 = 0. Thus
m010 and m111 are not equivalent Boolean expressions.
Theorem 10.9
Of the 2n minterms in the variables x1, x2, . . . , xn, no two are equivalent
Boolean expressions.
Proof
We ﬁrst note that 00 = ¯0 = 1 and 11 = 1 so that, if xi = ei, xiei = 1. This
means that, given a minterm
m = me1e2...en = x1
e1x2
e2 . . . xn
en
substituting xi = ei for i = 1, 2, . . . , n gives the product of n terms all of which
are equal to 1 and so the minterm is equal to 1.
Now any other minterm contains at least one literal which is the complement of a
literal contained in m and so substitution of the values xi = ei (i = 1, 2, . . . , n)
as above results in a product which contains at least one zero.
Hence, by
theorem 10.4, the product is zero.
We have shown that for any two distinct minterms there is at least one set of values
of the variables for which the minterms have different values. We can therefore
conclude that no two distinct minterms are equivalent.
□

508
Boolean Algebra
Deﬁnition 10.7
A maxterm (or complete sum) in the n variables x1, x2, . . . , xn is a
Boolean expression which has the form of the sum of each Boolean variable
or its complement. Thus a maxterm consists of the sum of n literals.
We denote a maxterm in the n variables x1, x2, . . . , xn by
Me1e2...en = x1
e1 ⊕x2
e2 ⊕· · · ⊕xn
en
where xiei is deﬁned as before.
Thus
M11010 = x1 ⊕x2 ⊕¯x3 ⊕x4 ⊕¯x5
and
M0011 = ¯x1 ⊕¯x2 ⊕x3 ⊕x4.
As with minterms, there are 2n possible maxterms in n variables and (by the
duality principle) no two of these are equivalent Boolean expressions.
We now come to an important theorem which will enable us to decide whether two
Boolean functions are equal without having to go to the trouble of showing that
the Boolean expression deﬁning one of the functions can be transformed to the
Boolean expression representing the other using the axioms. What the theorem
shows is that any Boolean expression in n variables can be written uniquely as the
sum of some or all of the 2n minterms in these n variables. A function which is
deﬁned by a Boolean expression in this form is said to be in disjunctive normal
form. Since it consists of a sum of minterms, it is sometimes called the minterm
form. It is also referred to as the canonical (or complete) sum-of-products form.
Before we consider the theorem we will look at an example where we derive
the disjunctive normal form of a Boolean expression by applying the axioms and
theorems of §10.2. However, although this is a perfectly acceptable method of
obtaining the disjunctive normal form, it is not the easiest method and therefore
not the one which we shall ultimately adopt.
Example 10.2
Write the following Boolean expression in the three variables x1, x2, x3 in
disjunctive normal form: x1x2(x1 ⊕x3).

Boolean Functions
509
Solution
x1x2(x1 ⊕x3) = x1x2x1 ⊕x1x2x3
(axiom B4)
= x1x1x2 ⊕x1x2x3
(axiom B3)
= x1x2 ⊕x1x2x3
(idempotent laws)
= x1x21 ⊕x1x2x3
(axiom B1)
= x1x2(x3 ⊕¯x3) ⊕x1x2x3
(axiom B5)
= x1x2x3 ⊕x1x2¯x3 ⊕x1x2x3
(axiom B4)
= x1x2x3 ⊕x1x2¯x3
(axiom B3 and idempotent laws).
This is the disjunctive normal form since it consists of the sum of the two
minterms x1x2x3 and x1x2¯x3.
We now consider the theorem which guarantees the existence of a disjunctive
normal form for any non-zero Boolean function.
Theorem 10.10
Every Boolean function f(x1, x2, . . . , xn), which is not identically zero,
can be written as the sum of all possible Boolean expressions of the form
f(e1, e2, . . . , en)x1
e1x2
e2, . . . xn
en
where xiei has the usual interpretation. Thus we can write
f(x1, x2, . . . , xn) = ⊕
(e) f(e1, e2, . . . , en)x1
e1x2
e2, . . . xn
en
= ⊕
(e) f(e1, e2, . . . , en)me1e2...en
where (e) denotes all possible n-tuples (e1, e2, . . . , en) where ei = 0 or 1
(i = 1, 2, . . . , n). There are 2n of these.
Proof
The proof of this theorem, although not difﬁcult, is rather long. We therefore give
only an outline and leave the interested reader to ﬁll in the details.

510
Boolean Algebra
We prove ﬁrst that the theorem holds for a function of one variable f(x), i.e. that
f(x) = f(0)¯x ⊕f(1)x.
Now, because it is deﬁned by a Boolean expression in one variable, a function of
one variable must take one of the following forms:
(a)
f(x) = 0 or f(x) = 1
(b)
f(x) = x
(c)
f(x) = ¯x
(d)
f(x) consists of the sums and products of terms which are themselves
sums or products of x, ¯x and the identity elements of B.
It can be shown using the axioms that
0¯x ⊕0x = 0
so that, if f(x) = 0,
f(x) = 0¯x ⊕0x
= f(0)¯x ⊕f(1)x.
Similarly, if f(x) = 1,
f(x) = 1¯x ⊕1x
= f(0)¯x ⊕f(1)x.
Also, if f(x) = x, we have
f(x) = 0¯x ⊕1x
= f(0)¯x ⊕f(1)x
and, if f(x) = ¯x,
f(x) = 1¯x ⊕0x
= f(0)¯x ⊕f(1)x.
Thus the theorem holds for f(x) in each of the cases (a), (b) and (c) above. That it
also holds for case (d) can be established by taking f1(x) and f2(x) to be any two
functions for which the theorem holds and showing that the theorem also holds
for f1(x)f2(x) and for f1(x)⊕f2(x). This proves that the theorem applies to any
Boolean function of one variable.
Now consider a function of n variables f(x1, x2, . . . , xn). If we regard this as a
function of the single variable x1 and apply the theorem, we have
f(x1, x2, . . . , xn) = [f(0, x2, . . . , xn)¯x1] ⊕[f(1, x2, . . . , xn)x1].

Boolean Functions
511
We now regard f(0, x2, . . . , xn) and f(1, x2, . . . , xn) as functions of the single
variable x2 so that applying the theorem again gives
f(0, x2, . . . , xn) = [f(0, 0, x3, . . . , xn)¯x2] ⊕[f(0, 1, x3, . . . , xn)x2]
and
f(1, x2, . . . , xn) = [f(1, 0, x3, . . . , xn)¯x2] ⊕[f(1, 1, x3, . . . , xn)x2].
Thus we have
f(x1, x2, . . . , xn) = [f(0, 0, x3, . . . , xn)¯x1¯x2] ⊕[f(0, 1, x3, . . . , xn)¯x1x2]
⊕[f(1, 0, x3, . . . , xn)x1¯x2] ⊕[f(1, 1, x3, . . . , xn)x1x2].
Continuing in this way, dealing with each variable in turn, gives the result
f(x1, x2, . . . , xn) = ⊕
(e) f(e1, e2, . . . , en)x1
e1x2
e2 . . . xn
en.
□
This result looks a little formidable but it is really quite simple although the
notation may make it seem less so. The value of f(e1, e2, . . . , en) is obtained
by substituting xi = ei in the Boolean expression deﬁning the function. Since
the ei values are either zero or one, f(e1, e2, . . . , en) will be either zero or one.
The theorem shows that a Boolean function can be written as a sum of terms,
each of which is the product of f(e1, e2, . . . , en) and the corresponding minterm
for every set of values of the ei. If f(e1, e2, . . . , en) = 1, the product is simply
the minterm and if f(e1, e2, . . . , en) = 0, then the product is zero. Hence the
minterms which appear in the disjunctive normal form are those of the form
x1
e1x2
e2 . . . xn
en
for which f(e1, e2, . . . , en) = 1.
Some examples may help to clarify this.
Examples 10.3
1.
Write the Boolean function f(x1, x2) = x1 ⊕x2 in disjunctive normal
form.
Solution
Theorem 10.10 states that f(x1, x2) = x1 ⊕x2 can be written as follows:
f(x1, x2) = f(0, 0)¯x1¯x2 ⊕f(0, 1)¯x1x2 ⊕f(1, 0)x1¯x2 ⊕f(1, 1)x1x2.

512
Boolean Algebra
We use a table (with an obvious analogy to a truth table) to calculate f(e1, e2) for
the various possible assignments of 0 and 1 to the variables e1 and e2.
e1
e2
f(e1, e2)
0
0
0
0
1
1
1
0
1
1
1
1
So we have
f(x1, x2) = 0¯x1¯x2 ⊕1¯x1x2 ⊕1x1¯x2 ⊕1x1x2
= ¯x1x2 ⊕x1¯x2 ⊕x1x2.
From this example we can see that expressing a Boolean function in disjunctive
normal form consists of ﬁnding the values of e1, e2, . . . , en for which
f(e1, e2, . . . , en) = 1 and writing down the sum of the corresponding minterms.
2.
Write f(x1, x2, x3) = x2x3 ⊕x3x1 in disjunctive normal form.
Solution
Using a table to calculate f(e1, e2, e3) for all possible values of e1, e2, e3 we have
the following.
e1
e2
e3
e2e3
e3e1
e2e3 ⊕e3e1
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
1
1
1
0
1
1
0
0
0
0
0
1
0
1
0
1
1
1
1
0
0
0
0
1
1
1
1
1
1
Now f(e1, e2, e3) = 1 when
(a)
e1 = 0, e2 = 1, e3 = 1
(b)
e1 = 1, e2 = 0, e3 = 1
(c)
e1 = 1, e2 = 1, e3 = 1.

Boolean Functions
513
The minterms x1e1x2e2x3e3 corresponding to each of these are
(a)
¯x1x2x3
(b)
x1¯x2x3
(c)
x1x2x3.
The disjunctive normal form is therefore given by
f(x1, x2, x3) = ¯x1x2x3 ⊕x1¯x2x3 ⊕x1x2x3.
We now show that there is only one disjunctive normal form for any given Boolean
function.
Theorem 10.11
The disjunctive normal form of a given Boolean function is unique (up to
reordering of the minterms in the expression).
Proof
The method of proof is by contradiction.
Suppose that the function f(x1, x2, . . . , xn) can be written in disjunctive normal
form in two ways, so that
f(x1, x2, . . . , xn) = P1 ⊕P2 ⊕· · · ⊕Pr
= Q1 ⊕Q2 ⊕· · · ⊕Qs
where Pi and Qj (i = 1, 2, . . . , r and j = 1, 2, . . . , s) are terms of the form
x1
e1x2
e2 . . . xn
en.
We will assume, without loss of generality, that r ⩾s.
Now, if the two disjunctive normal forms are not equal then at least one of the Pi
must be different from every Qj. Let us suppose that Pm has this property.
Since Pm is different from all the Qj, Pm and Qj must be such that one contains
xk while the other contains ¯xk for some value of k. The expression PmQj then
contains the product xk¯xk and hence
PmQj = 0.

514
Boolean Algebra
Now this is true for each j = 1, 2, . . . , s so that
PmQ1 ⊕PmQ2 ⊕· · · ⊕PmQs = 0
Pm(Q1 ⊕Q2 ⊕· · · ⊕Qs) = 0
(axiom B4)
⇒
Pmf(x1, x2, . . . , xn) = 0.
⇒
But
Pmf(x1, x2, . . . , xn) = Pm(P1 ⊕P2 ⊕· · · ⊕Pr)
= PmP1 ⊕PmP2 ⊕· · · ⊕PmPr
(axiom B4)
= PmPm
(since all the Pi are different)
= Pm
(idempotent laws).
Thus if there are two disjunctive normal forms for f(x1, x2, . . . , xn) we have
Pmf(x1, x2, . . . , xn) = 0
Pmf(x1, x2, . . . , xn) = Pm.
This contradiction shows that the disjunctive normal form must be unique.
□
It is this theorem which gives us a useful method for establishing the equality or
otherwise of two Boolean functions. We simply write the expressions deﬁning the
two functions in disjunctive normal form and compare the results. The disjunctive
normal form is the ‘ﬁngerprint’ of a Boolean function. Two or more Boolean
functions are equal if and only if they have the same disjunctive normal form.
Example 10.4
Show that f(x1, x2) = x1 ⊕x2 and g(x1, x2) = ¯x1x2 ⊕x1 are equal functions.
Solution
We shall write each function in disjunctive normal form and, if these are the same,
theorem 10.11 allows us to conclude that the two expressions are equivalent and
hence that the functions are equal.
Consider ﬁrst f(x1, x2) = x1 ⊕x2. In example 10.3.1 we derived the disjunctive
formal form for this function:
f(x1, x2) = ¯x1x2 ⊕x1¯x2 ⊕x1x2.

Boolean Functions
515
We now write g(x1, x2) = ¯x1x2 ⊕x1 in disjunctive normal form. We have the
following table.
e1
e2
¯e1
¯e1e2
¯e1e2 ⊕e1
0
0
1
0
0
0
1
1
1
1
1
0
0
0
1
1
1
0
0
1
So
g(x1, x2) = ¯x1x2 ⊕x1¯x2 ⊕x1x2
= f(x1, x2).
Hence the two functions are equal.
Note that, to prove that two functions f(x1, x2, . . . , xn) and g(x1, x2, . . . , xn)
are equal, it is sufﬁcient to prove that f(e1, e2, . . . , en) = g(e1, e2, . . . , en) for
all possible n-tuples (e1, e2, . . . , en). Thus a Boolean function is completely
determined by the values that it takes for the 2n combinations of zeros and ones
that can be substituted for e1, e2, . . . , en.
Example 10.5
A Boolean function f(x1, x2) is such that f(0, 0) = 1, f(0, 1) = 0, f(1, 0) = 1,
f(1, 1) = 0. Find a Boolean expression for this function.
Solution
We are given values of f(e1, e2) for all possible choices of e1 and e2.
The
minterms which appear in the disjunctive normal form are those of the form
x1e1x2e2 for which f(e1, e2) = 1, i.e. ¯x1¯x2 and x1¯x2. So a Boolean expression
for the function is
f(x1, x2) = ¯x1¯x2 ⊕x1¯x2.
Theorem 10.10 shows that it is possible to write a Boolean function as the sum of
minterms. By applying the duality principle to this theorem and to theorem 10.11,
we see that it is also possible to write such a function uniquely as the product of
maxterms.

516
Boolean Algebra
Theorem 10.12 (Dual of theorem 10.10)
Every Boolean function f(x1, x2, . . . , xn), which is not identically one,
can be written as the product of all possible Boolean expressions of the
form
f(¯e1, ¯e2, . . . , ¯en) ⊕x1
e1 ⊕x2
e2 ⊕· · · ⊕xn
en
or equivalently, as the product of all Boolean expressions of the form
f(e1, e2, . . . , en) ⊕x1
¯e1 ⊕x2
¯e2 ⊕· · · ⊕xn
¯en.
Therefore
f(x1, x2, . . . , xn) = ∗
(e) f(e1, e2, . . . , en) ⊕x1
¯e1 ⊕x2
¯e2 ⊕· · · ⊕xn
¯en
= ∗
(e) f(e1, e2, . . . , en) ⊕M¯e1¯e2...¯en
where (e) denotes all possible n-tuples (e1, e2, . . . , en) where ei = 0 or 1
(i = 1, 2, . . . , n).
A function which is written in this form is said to be in conjunctive normal
form. It is also referred to as the maxterm form or the canonical (or complete)
product-of-sums form.
Theorem 10.13 (Dual of theorem 10.11)
The conjunctive normal form of a given Boolean function is unique up to a
reordering of maxterms.
Example 10.6
Express f(x1, x2) = x1(x1 ⊕x2) in conjunctive normal form.

Boolean Functions
517
Solution
As with the examples on disjunctive normal forms, we ﬁrst evaluate f(e1, e2) for
all values of e1, e2.
e1
e2
e1 ⊕e2
e1(e1 ⊕e2)
0
0
0
0
0
1
1
0
1
0
1
1
1
1
1
1
Thus
f(x1, x2) = (0 ⊕x1 ⊕x2)(0 ⊕x1 ⊕¯x2)(1 ⊕¯x1 ⊕x2)(1 ⊕¯x1 ⊕¯x2).
Now 0 ⊕b = b and 1 ⊕b = 1 for any b ∈B, where B is the underlying set of a
Boolean algebra, so that
f(x1, x2) = (x1 ⊕x2)(x1 ⊕¯x2) ∗1 ∗1
= (x1 ⊕x2)(x1 ⊕¯x2).
This is the conjunctive normal form of f(x1, x2).
When expressing a function f(x1, x2, . . . , xn) in disjunctive normal form the
minterms me1e2...en which are present are those for which f(e1, e2, . . . , en) = 1.
From the example above we can see that the maxterms M¯e1¯e2...¯en which appear
in the conjunctive normal form are those for which f(e1, e2, . . . , en) = 0.
Example 10.7
Express f(x1, x2, x3) = (¯x1 ⊕x2)(¯x1 ⊕¯x3) in conjunctive normal form.

518
Boolean Algebra
Solution
e1
e2
e3
¯e1
¯e3
¯e1 ⊕e2
¯e1 ⊕¯e3
(¯e1 ⊕e2)(¯e1 ⊕¯e3)
0
0
0
1
1
1
1
1
0
0
1
1
0
1
1
1
0
1
0
1
1
1
1
1
0
1
1
1
0
1
1
1
1
0
0
0
1
0
1
0
1
0
1
0
0
0
0
0
1
1
0
0
1
1
1
1
1
1
1
0
0
1
0
0
The following values of e1, e2, e3 give f(e1, e2, e3) = 0:
(a)
e1 = 1, e2 = 0, e3 = 0
(b)
e1 = 1, e2 = 0, e3 = 1
(c)
e1 = 1, e2 = 1, e3 = 1.
The corresponding maxterms M¯e1¯e2¯e3 are
(a)
¯x1 ⊕x2 ⊕x3
(b)
¯x1 ⊕x2 ⊕¯x3
(c)
¯x1 ⊕¯x2 ⊕¯x3.
Hence, in conjunctive normal form,
f(x1, x2, x3) = (¯x1 ⊕x2 ⊕x3)(¯x1 ⊕x2 ⊕¯x3)(¯x1 ⊕¯x2 ⊕¯x3).
Conjunctive normal forms can be used in exactly the same way as disjunctive
normal forms for proving whether or not Boolean functions are equal. However,
disjunctive normal forms tend to be preferred because of their signiﬁcance in the
application of Boolean algebra to the design of electronic circuits.
Remember that, given the n Boolean variables x1, x2, . . . , xn, there are 2n
possible minterms in these n variables.
Now for any Boolean function (not
identically zero) f(x1, x2, . . . , xn) can be expressed uniquely as the sum of some
or all of these minterms. A set containing 2n elements has 22n possible subsets
including the empty set and the set itself (see theorem 3.5). From this we can
deduce that there are 22n possible selections of minterms and hence there are just
22n distinct Boolean functions of n variables. These include the function which

Boolean Functions
519
is identically zero and cannot be expressed as the sum of minterms and which
can therefore be thought of as the function in which all minterms are absent. The
function in which all 2n minterms are present is f(x1, x2, . . . , xn) = 1.
Exercises 10.2
1.
Express each of the following Boolean functions in disjunctive normal
form and hence state which of the functions are equal:
(i)
f(x1, x2) = ¯x1x2 ⊕x1¯x2
(ii)
f(x1, x2) = x1
(iii)
f(x1, x2) = x1(¯x1 ⊕x2)
(iv)
f(x1, x2) = x1x2
(v)
f(x1, x2) = (x1 ⊕x2)(x1 ⊕¯x2)
(vi)
f(x1, x2, x3) = x2(x1x3 ⊕¯x1)
(vii)
f(x1, x2, x3) = x1 ⊕¯x2 ⊕x3
(viii) f(x1, x2, x3) = x2(¯x1 ⊕x3)
(ix)
f(x1, x2, x3) = x3 ⊕¯x1x2
(x)
f(x1, x2, x3) = (x1 ⊕x2 ⊕x3)(¯x1 ⊕x3).
2.
Express each of the following Boolean functions in both disjunctive and
conjunctive normal forms:
(i)
f(x1, x2, x3) = x1 ⊕x2 ⊕¯x3
(ii)
f(x1, x2, x3) = x1x2 ⊕¯x3 ⊕x1
(iii)
f(x1, x2, x3) = 1(x2 ⊕x3)x1
(iv)
f(x1, x2, x3) = ¯x1x2 ⊕x1x3.
3.
Let F be the set of all Boolean functions in the n variables x1, x2, . . . , xn.
(We showed that there are 22n such functions.) Suppose that for each
fi ∈F (i = 1, . . . , 22n) a Boolean expression deﬁning fi is Ei. We
deﬁne the following operations on the set F:
(a)
¯fi = ¯Ei for all fi ∈F
(b)
fifj = EiEj for all fi, fj ∈F
(c)
fi ⊕fj = Ei ⊕Ej for all fi, fj ∈F.
Show that there exist identity elements f0, f1 ∈F such that
f0 ⊕fi = fi ⊕f0 = f1
fif1 = f1fi = fi

520
Boolean Algebra
for all fi ∈F.
Show that (F, ⊕, ∗,¯, f0, f1) is a Boolean algebra.
4.
Let (B, ⊕, ∗,¯, 0, 1) be a Boolean algebra and let the binary operation ◦
be deﬁned on B by
b1 ◦b2 = b1¯b2 ⊕¯b1b2
for all b1, b2 ∈B. Show that (B, ◦) is an abelian group with identity 0.
What is the inverse of an arbitrary element b ∈B?
5.
Given the Boolean algebra (B, ⊕, ∗,¯, 0, 1), an atom is a non-zero
element a ∈B such that, for all b ∈B, either ba = a or ba = 0.
(i)
Let S = {j, k, l, m}. What are the atoms of the Boolean algebra
(P(S), ∪, ∩,¯, ∅, S)?
(ii)
Prove that, if a1 and a2 are atoms of the Boolean algebra
(B, ⊕, ∗,¯, 0, 1) and a1a2 ̸= 0, then a1 = a2.
10.4
Switching Circuits
Many electronic devices such as computers, telephone systems, trafﬁc and train
control systems employ as part of their circuitry items known as switches. A
switch may be viewed as a connection within the circuit such that, when the switch
is closed, electric current may pass through it but, when it is open, no current can
pass through that point of the circuit. A switch is an example of a two-state
device, the two states being ‘on’ and ‘off’. A circuit which incorporates one or
more switches is known as a switching circuit. Diagrammatically, we shall show
a switch as follows:
Now imagine that we have a circuit (assumed to include a suitable power source)
which contains a switch A. We denote the state of the switch by the variable x
where x = 0 if A is open and x = 1 if A is closed.
Consider now a circuit which contains two switches A1 and A2 connected as
shown in the diagram below.

Switching Circuits
521
Switches connected to each other in this way are said to be in series. It is clear that
current will ﬂow across this section of a circuit only if both switches A1 and A2
are closed. Let x1 and x2 be variables denoting the states of switches A1 and A2
respectively. (In each case 0 denotes open and 1 denotes closed.) Let f(x1, x2)
be a function which has the value 1 for values of x1 and x2 which allow current
to ﬂow and 0 otherwise. Thus f : {0, 1}2 →{0, 1} and the value of f(x1, x2)
for all possible values of x1 and x2 is given in the table below.
x1
x2
f(x1, x2)
0
0
0
0
1
0
1
0
0
1
1
1
We can now see that f is the familiar function f(x1, x2) = x1x2 where
x1 and x2 are variables whose domain is the two-element Boolean algebra
({0, 1}, ⊕, ∗,¯, 0, 1).
Two switches may alternatively be connected in parallel, the arrangement shown
in the diagram below.
For current to ﬂow around a circuit containing a power source and only two
switches connected in this way, it is necessary that one or both of the switches
are closed. Deﬁning x1 and x2 as before and g(x1, x2) exactly as we deﬁned
f(x1, x2) for switches in series, we have the following table.
x1
x2
g(x1, x2)
0
0
0
0
1
1
1
0
1
1
1
1
Thus
g(x1, x2)
=
x1 ⊕x2
deﬁned
on
the
same
Boolean
algebra
({0, 1}, ⊕, ∗,¯, 0, 1).
Functions such as the two we have considered describe the behaviour of a circuit
according to the states of the switches which are incorporated into that circuit.

522
Boolean Algebra
Such functions are called switching functions. Given n switches A1, A2, . . . , An
whose states are deﬁned by the n variables x1, x2, . . . , xn (xi = 0 or 1, i =
1, 2, . . . , n), a switching function f : {0, 1}n →{0, 1} describes the behaviour
of the circuit for all the 2n possible states of the switches. As we have seen in
the examples above, f can be represented by a Boolean expression and hence is a
Boolean function.
In the following examples we look at switching functions for more complicated
switching circuits.
Examples 10.8
1.
Deﬁne the switching function f for the circuit incorporating the following
arrangement of switches.
Solution
Let x1, x2, x3 denote the states of the switches A1, A2 and A3 respectively. Let
f1(x1, x2) denote the behaviour of the part of the circuit containing the switches
A1 and A2. Since these are connected in series f1(x1, x2) = x1x2. If f2(x3)
denotes the behaviour of the portion of the circuit containing switch A3, then
clearly f2(x3) = x3.
Now the two switches A1 and A2 are connected in parallel to the switch A3 and
so, if f(x1, x2, x3) denotes the behaviour of the circuit containing this system of
switches, we have
f(x1, x2, x3) = f1(x1, x2) ⊕f2(x3)
= x1x2 ⊕x3.
2.
Deﬁne the switching function f for the circuit incorporating the following
system of switches.

Switching Circuits
523
Solution
Let x1, x2, x3, x4 be the variables denoting the states of the switches A1, A2,
A3 and A4 respectively. Then f(x1, x2, x3, x4) is the switching function for the
circuit.
Switches A1 and A2 are connected in series so that f1(x1, x2) = x1x2 is the
switching function for these two switches. Switches A3 and A4 are connected in
parallel, therefore f2(x3, x4) = x3 ⊕x4 is the appropriate switching function.
The section of the circuit containing A1 and A2 is connected in parallel to the
section containing A3 and A4 so that
f(x1, x2, x3, x4) = f1(x1, x2) ⊕f2(x3, x4)
= x1x2 ⊕x3 ⊕x4.
3.
Consider the circuit employing the same arrangement of switches as in
example 2. However, suppose that the switches A1 and A3 are such that
they go on or off together. In this case their states are always identical
and we can use the single variable x1 to denote the state of each of them.
Suppose also that switches A2 and A4 are such that when one is on the
other is off and vice versa. If x2 describes the state of A2, then we can
use ¯x2 to describe the state of A4.
On the diagram we will use the same letter for switches which are always
in the same state so that the same variable can be used. If S denotes a
switch we shall use ¯S to label a switch which is always in the opposite
state to S. So for this example the circuit diagram is as follows.

524
Boolean Algebra
The switching function for this circuit is
f(x1, x2) = x1x2 ⊕x1 ⊕¯x2.
We have already seen that a given Boolean expression can be written in a number
of equivalent forms so that the functions corresponding to each of these are equal.
Applied to switching functions this means that two different arrangements of
switches may have equal switching functions. This implies that the behaviour of
the two circuits (in terms of whether or not current ﬂows around them) is identical
given that the state of corresponding switches incorporated in them is the same.
For example, consider the following two systems of switches.
Let x1 and x2 describe the states of switches A1 and A2 respectively and
let f1(x1, x2) be the switching function for the circuit (a) and f2(x1, x2) the
switching function for (b).
Using the same technique as in the previous examples, we have
f1(x1, x2) = x1x2(x1¯x2 ⊕¯x1 ⊕x2)
f2(x1, x2) = x1x2.
Although it is not immediately obvious, the functions f1 and f2 are equal. This
can be veriﬁed either by rewriting each in disjunctive (or conjunctive) normal
form, or by drawing up a table and evaluating each function for the four possible
assignments of 0 and 1 to the variables x1 and x2.
Since their switching functions are equal, the behaviour of the circuits is identical
for any set of states of the switches A1 and A2. However, it is clear that the second
circuit is very much simpler than the ﬁrst and also that it is likely to be cheaper to
construct and more reliable. In constructing a circuit which is required to behave
in a certain way, it is important to be able to recognize whether a particular design
is the simplest among all those which are possible. We return to this problem
later.
We give one further example to illustrate a familiar practical application of a
switching circuit.

Switching Circuits
525
Example 10.9
A light bulb located over a ﬂight of stairs is controlled by two wall switches, one
at the top of the stairs and the other at the bottom. The switches are such that
when the state of either one is reversed, the state of the light is reversed, i.e. it
goes on if it was off and off if it was on. Design a circuit which will achieve this.
Solution
First note that the wall switches are not necessarily the switches in the circuit
although they will each control one or more circuit switches.
Let us ﬁrst draw a table showing what is required of the two wall switches S1 and
S2. We shall arbitrarily suppose that initially both are up and the light is off, i.e.
no current ﬂows. When either one or the other is down, current must ﬂow through
the circuit, but when both are down there must be no current.
S1
S2
Current
Up
Up
No
Up
Down
Yes
Down
Up
Yes
Down
Down
No
Suppose that S1 and S2 control switches A1 and A2 in the circuit and that when a
wall switch is up, the corresponding circuit switch is open. Using the variables x1
and x2 to denote the state of the circuit switches in the usual way and f(x1, x2)
to denote whether or not current ﬂows through the circuit, the table above is
equivalent to the following.
x1
x2
f(x1, x2)
0
0
0
0
1
1
1
0
1
1
1
0
From this table we can express f(x1, x2) in terms of a Boolean expression in
disjunctive normal form:
f(x1, x2) = ¯x1x2 ⊕x1¯x2.

526
Boolean Algebra
The circuit for which this is the switching function incorporates switches ¯A1 and
A2 in series connected in parallel to switches A1 and ¯A2 in series. The switching
system is therefore as shown in the following diagram.
Note that we could equally well have expressed f(x1, x2) in conjunctive normal
form thus:
f(x1, x2) = (¯x1 ⊕¯x2)(x1 ⊕x2).
Thus an alternative circuit for achieving the same effect would be as follows.
In both these cases some device is necessary so that a single wall switch controls
two circuit switches. The exact nature of such a device need not concern us here.

Switching Circuits
527
Exercises 10.3
1.
Deﬁne a switching function for each of the following systems of switches.
2.
For each of the following functions, draw the diagram of a system of
switches for which it is the switching function:
(i)
f(x1, x2) = (x1 ⊕x2)¯x1
(ii)
f(x1, x2) = x1x2(x1 ⊕x2)
(iii)
f(x1, x2, x3) = (x1 ⊕¯x2 ⊕x3)¯x1
(iv)
f(x1, x2, x3) = (x1 ⊕x3)(¯x2 ⊕¯x1)
(v)
f(x1, x2, x3) = ¯x1(x2x3 ⊕¯x2x1).
3.
Write down the switching function for each of the following systems of
switches. By writing the function in disjunctive normal form, design

528
Boolean Algebra
an equivalent system of switches, that is one having an equal switching
function.
4.
Suppose that a light bulb is located as in example 10.9 but that it is
controlled by three switches rather than two. Reversing the state of any
one of the switches reverses the state of the light. Design a switching
system which will achieve this.
5.
The central heating system in a small house is to be controlled by three
thermostats, one located in each of three rooms. The thermostats are
preset at 15◦C but in the interests of economy it is desired that the central
heating be on only if the temperature in at least two of the rooms falls
below 15◦C; otherwise the heating must be off.
Design a switching system, to be operated by the thermostats, which
will allow current to ﬂow round a circuit (thereby activating the heating
system) only when the temperature in at least two of the rooms falls below
15◦C.
Do you think that your circuit is the simplest one which will achieve the
desired effect? If not, try and design a simpler one.
6.
A simple burglar alarm system consists of a master switch and two
movement sensors. When the master switch is on and either (or both)
of the sensors is activated by movement within the room in which it is

Logic Networks
529
located, an alarm bell rings until the master switch is turned off. If the
master switch is not turned on, then the bell does not ring, whatever the
state of the sensors.
Design a switching circuit incorporating switches activated by the
movement sensors and the master switch which will achieve this effect.
Do you think that the circuit you have designed is the simplest one? If
not, try to design a simpler one.
10.5
Logic Networks
In this section we deal with ‘logic gates’. These are electronic devices which may
be viewed as the basic functional components of a digital computer. A logic gate
is an electronic component, incorporated within a circuit, which operates on one
or more inputs to produce one output. Each input and each output can take one
of two values (normally low and high voltage) which are denoted by 0 and 1.
Because of the ‘two-value’ nature of the input and output variables, a logic gate
is an example of a binary device. It also falls in the category of combinational
devices because the output value depends only on the input values. (This is in
contrast to a sequential device where the output value is also determined by such
factors as the time or the past history of the circuit.)
The three most important types of logic gates are the AND-gate, the OR-gate and
the NOT-gate (or inverter). These gates are so named because of their association
(which will become obvious) with the corresponding logical connective. We use
xi to represent the value of the input(s) to a gate and the variable z to represent its
output. The following table summarizes the operation of each of the three gates.
AND-gate
OR-gate
NOT-gate
Circuit symbol
Input/output
table
x1
x2
z
0
0
0
0
1
0
1
0
0
1
1
1
x1
x2
z
0
0
0
0
1
1
1
0
1
1
1
1
x
z
0
1
1
0
Boolean
expression
z = x1x2
z = x1 ⊕x2
z = ¯x

530
Boolean Algebra
The AND-gate and the OR-gate each have two inputs and one output. For the
AND-gate the output z has the value 1 only when the two input values are 1.
The value of z is 0 otherwise. Viewing x1, x2 and z as variables whose domain
is the underlying set {0, 1} of the Boolean algebra ({0, 1}, ∗, ⊕,¯, 0, 1) we have
z = x1x2.
The OR-gate has output value 1 only if either or both of the input values are 1 and
therefore the Boolean expression for z is given by z = x1 ⊕x2.
In contrast to the other two gates the NOT-gate has only one input. The gate
has the effect of reversing the value of the input variable so that the Boolean
expression for z is given by z = ¯x.
Comparison of the input/output tables with the truth tables for conjunction,
disjunction and negation should make it clear why these gates are named as they
are.
Within a circuit, a number of these gates may be linked together, the output from
one gate acting as the input to one or more others. Such a circuit is termed a
logic network. We can describe the output of the system of gates by a Boolean
expression in terms of the various input variables.
Examples 10.10
1.
Give the Boolean expression for the output of the following system of
gates.
Solution
The output from the AND-gate having x1 and x2 as inputs is x1x2 and that from
the AND-gate having x3 and x4 as inputs is x3x4. The output from the OR-gate
is therefore x1x2 ⊕x3x4 and this is the input to the NOT-gate. The ﬁnal output is
therefore
x1x2 ⊕x3x4.

Logic Networks
531
We show these stages on the following diagram.
From the Boolean expression for the output, we can determine its value for each
of the 16 possible sets of values of the input variables.
2.
Design a system of logic gates with input variables x1, x2 and x3 which
will produce an output deﬁned by the Boolean expression ¯x1x2 ⊕x1x3.
Solution
The ﬁnal output can be achieved by an OR-gate whose input values are ¯x1x2 and
x1x3. The ﬁrst of these expressions is the output of an AND-gate with inputs ¯x1
and x2. The second is the output of an AND-gate with inputs x1 and x3.
Thus we have the following diagram.
The input variable x1 branches, one branch passing through the NOT-gate and the
other through the AND-gate having the variable x3 as its other input. The point
at which the circuit branches is shown as a ﬁlled-in circle on the circuit diagram.
3.
Determine the Boolean expression for the output of the following system
of gates.
Solution
In this example the output from the inverter branches and one branch is used as
one input to the ﬁnal OR-gate. Proceeding as in the ﬁrst example we have the

532
Boolean Algebra
following diagram.
In all these examples the output is represented by a Boolean expression in terms of
the input variables. We can, as usual, regard this Boolean expression as deﬁning
a function of the variables representing the states of the inputs. As we have
seen, a particular function may be deﬁned by a number of equivalent Boolean
expressions. It follows therefore that, given any network of logic gates, there may
be a number of equivalent networks. By ‘equivalent’ networks we mean that,
given any set of values of the input variables, the output of any of these networks
is the same.
For example, it is a simple matter to establish the equivalence of the two Boolean
expressions.
(x1 ⊕x2)(¯x1 ⊕¯x2)
and
x1¯x2 ⊕¯x1x2.
(Note that these are the conjunctive and disjunctive normal forms of the
expression.) Thus the two logic networks whose outputs can be described by
these expressions are equivalent. These are as follows.
Since the two Boolean expressions are equivalent, the output is the same for any
given set of values of the input variables x1 and x2.
The fact that a number of different logic networks may be equivalent raises again
the question of how we may determine, given a particular Boolean expression

Logic Networks
533
describing the output, the simplest network which will do the job. By ‘simplest’
we mean the one with the fewest logic gates. We turn to this problem in the next
section.
Exercises 10.4
1.
Give a Boolean expression describing the output of each of the following
logic networks.
2.
Design a logic network for each of the following so that the output is
described by the Boolean expression given:

534
Boolean Algebra
(i)
(x1 ⊕x2)(¯x1 ⊕x3)
(ii)
¯x1x2 ⊕x3x2 ⊕x1
(iii)
x1x3 ⊕¯x1 ⊕x2¯x3
(iv)
x1x2x3 ⊕¯x1¯x2¯x3
(v)
(x1 ⊕¯x2 ⊕x3)(¯x1 ⊕x3)x2.
3.
The following circuits have more than one output. Write the Boolean
expression for each of the outputs in terms of the input variables.
4.
The NAND-gate is shown symbolically as the following.
The Boolean expression for its output is x1x2.
For each of the following, design a logic circuit which utilizes only
NAND-gates so that the output is deﬁned by the given Boolean
expression:
(i)
x1 ⊕x2
(ii)
x1x2
(iii)
¯x1.

Logic Networks
535
Deduce that, for any Boolean expression describing the output, a logic
network can be designed using only NAND-gates.
(You might ﬁnd
exercise 1.3.10 helpful here.)
5.
The NOR-gate is shown symbolically as the following.
The Boolean expression for its output is (x1 ⊕x2).
Repeat the last exercise substituting NOR-gate for NAND-gate.
6.
(For readers familiar with binary arithmetic.) Suppose that x and y are
single-digit binary numerals. The following table gives the sum of x and
y as a two-digit binary numeral for all combinations of values of x and y.
x
y
Binary numeral for
the sum x + y
0
0
00
0
1
01
1
0
01
1
1
10
Design a logic network that has outputs z1 and z2 so that the binary
number whose ﬁrst and second digits are z1 and z2 (read from left to
right) represents the sum of the input variables x and y. (A logic network
designed to add two single-digit binary numbers in this way is called a
half-adder.)
7.
(Also for readers familiar with binary arithmetic and who have
successfully solved the last problem.) A full-adder is a logic network
which has three inputs x1, x2 and x3 and two outputs z1 and z2, the ﬁrst
and second digits respectively of the binary sum of x1, x2 and x3.

536
Boolean Algebra
(i)
Draw a table showing the values of the output variables z1 and z2
for each set of values of the input variables x1, x2 and x3.
(ii)
Design a logic network which will achieve the output described.
In practice, addition of binary numerals with several digits is achieved
by using a half-adder to add the two least signiﬁcant (rightmost) digits
and using the ‘carry digit’ as input to a full-adder along with the next
two digits to be added. The carry-digit from this sum is fed into the next
full-adder along with the two digits third from the right in the summands,
and so on. The diagram below shows this process applied to the addition
of the binary numeral with digits x1, x2, x3 (x3 being the least signiﬁcant
digit) to another with digits y1, y2, y3. The result is the binary numeral
with digits z1, z2, z3, z4.
10.6
Minimization of Boolean Expressions
We now consider the following problem—given a Boolean expression (which
may represent the output of a logic network or a switching function), what is
the ‘simplest’ equivalent expression?
Where the design of switching circuits
and logic networks is concerned the question is an important one since it has
implications for the cost of producing the circuit and for its efﬁciency of operation.

Minimization of Boolean Expressions
537
Of all the circuits for which a given Boolean expression describes the output, the
cheapest to produce is the one having the fewest logic elements (gates or switches)
and also the fewest inputs to these elements. An additional advantage for a circuit
having as few logic elements as possible is that there is less chance that it will
develop a fault.
For our purposes, given a Boolean expression, the ‘simplest’ Boolean expression
equivalent to it will satisfy the following criteria:
(a)
it will be expressed as the sum of terms which are themselves the product
of literals;
(b)
no other equivalent Boolean expression in this form contains fewer terms;
(c)
of all the equivalent Boolean expressions in this form which have the
same number of terms, none has fewer literals (each literal being counted
every time it occurs).
Where these criteria are satisﬁed by a Boolean expression, we say that it is in
minimal form (or just minimal).
There may be several equivalent Boolean
expressions which satisfy all three criteria so that the minimal form is not
necessarily unique.
The technique which we shall use to obtain the minimal form of a Boolean expres-
sion is to start with the disjunctive normal form. This is the sum of terms which
are the product of literals and so at least satisﬁes the ﬁrst criterion. We shall then
aim to reduce the number of terms as far as possible so that the second criterion is
satisﬁed and then to reduce the number of literals. We now consider a systematic
method by which this ‘pruning’ of the disjunctive normal form can be achieved.
Karnaugh Maps
A Karnaugh map is a diagrammatic representation of a Boolean expression in
disjunctive normal form. It consists of a rectangle divided into subrectangles
referred to as cells where each cell may be taken to represent a minterm. For
a given number of variables the cells within the Karnaugh map represent all the
possible minterms which may appear in the disjunctive normal form of a Boolean
expression. The minterms are allocated to the cells in such a way that adjacent
cells represent minterms in which all the literals are identical except for one which
is complemented in one cell but not in an adjacent one. Thus movement around
the map from cell to cell (up or down, to left or right, but not diagonally) gives a
sequence of minterms where each is different from the last by only one literal.
The minterms represented by the cells in a Karnaugh map for a Boolean
expression in the two variables x1 and x2 are shown in the diagram below.

538
Boolean Algebra
x2
¯x2
x1
x1x2
x1¯x2
¯x1
¯x1x2
¯x1¯x2
Written in each cell is the corresponding minterm. Notice that the requirement
that adjacent cells differ by just one literal also applies at the edges of the map if
we view the rightmost column of cells as being adjacent to the left-hand column
and also the top and bottom rows as being adjacent.
In using the map it is
important to realize that the right and left edges are to be regarded as contiguous
and so are the top and bottom edges.
The following is a layout for a Karnaugh map for three variables x1, x2 and x3.
x2x3
¯x2x3
¯x2¯x3
x2¯x3
x1
a
b
c
d
¯x1
e
f
g
h
The cell labelled a represents the minterm x1x2x3, cell c represents x1¯x2¯x3, f
represents ¯x1¯x2x3, etc. There is more than one way of constructing a Karnaugh
map so that the necessary criteria are satisﬁed. An alternative for three variables
is given below.
x3
¯x3
x1x2
a
d
x1¯x2
b
c
¯x1¯x2
f
g
¯x1x2
e
h
A Boolean expression given as the sum of minterms (i.e. in disjunctive normal
form) is represented on the Karnaugh map by placing a one in each cell
corresponding to a minterm which is present.
For example, the Boolean
expression x1x2 ⊕¯x1x2 in the two variables x1 and x2 is represented by
x2
¯x2
x1
1
¯x1
1

Minimization of Boolean Expressions
539
The Boolean expression in the three variables x1, x2 and x3 given by
x1¯x2x3 ⊕x1¯x2¯x3 ⊕¯x1x2x3 ⊕¯x1x2¯x3
is represented by
x2x3
¯x2x3
¯x2¯x3
x2¯x3
x1
1
1
¯x1
1
1
It is important to realize that only Boolean expressions which are the sum of
minterms can be represented on a Karnaugh map. However, this need not concern
us since we know that any Boolean expression can be written in disjunctive
normal form (i.e. as the sum of minterms) and hence we can represent any
Boolean expression on a Karnaugh map of appropriate dimensions.
To appreciate how a Karnaugh map will help us to accomplish the process of
minimization, note that two adjacent ones (horizontally or vertically) in the map
imply that the Boolean expression contains the sum of two minterms which differ
only in that one variable is replaced by its complement. Where this is the case,
this variable can be eliminated. For example, in the Karnaugh map above, the two
adjacent ones in the top row indicate that the Boolean expression represented in
the map contains the sum of x1¯x2x3 and x1¯x2¯x3. Now
x1¯x2x3 ⊕x1¯x2¯x3 = x1¯x2(x3 ⊕¯x3)
(axiom B4)
= x1¯x2.
Thus we can replace two terms consisting of a total of six literals by one term
consisting of two literals.
We can proceed further: the map has another pair of adjacent ones—those in the
bottom row. (Remember that the right and left edges are regarded as coincident.)
Applying the same technique we have
¯x1x2x3 ⊕¯x1x2¯x3 = ¯x1x2(x3 ⊕¯x3)
= ¯x1x2.
There are no further adjacent ones and so we have
x1¯x2x3 ⊕x1¯x2¯x3 ⊕¯x1x2x3 ⊕¯x1x2¯x3 = x1¯x2 ⊕¯x1x2.
In fact this is a minimal form of the original Boolean expression.

540
Boolean Algebra
This idea can be extended to larger rectangular groups of ones. Consider, for
example, the following Karnaugh map.
x2x3
¯x2x3
¯x2¯x3
x2¯x3
x1
1
1
¯x1
1
1
Here we have
x1¯x2x3 ⊕x1¯x2¯x3 ⊕¯x1¯x2x3 ⊕¯x1¯x2¯x3
= x1¯x2(x3 ⊕¯x3) ⊕¯x1¯x2(x3 ⊕¯x3)
= x1¯x2 ⊕¯x1¯x2
= ¯x2(x1 ⊕¯x1)
= ¯x2.
Similarly, we can show that the rectangular grouping of ones in the Karnaugh map
below represents a Boolean expression which is equivalent to the single term x1.
x2x3
¯x2x3
¯x2¯x3
x2¯x3
x1
1
1
1
1
¯x1
Thus a group of four ones arranged in a rectangular block (either 2 × 2, 1 × 4
or 4 × 1) allows replacement of four terms by one and the elimination of two
variables. In a similar way we can show that a group of eight ones arranged in any
rectangular block indicates that the eight corresponding minterms can be replaced
by a single term in which three variables have been eliminated. In all these cases
the variable or variables which remain are those which appear unchanged in all
cells constituting the block.
It is important to note that only blocks of 2, 4, 8, . . . cells lead to replacement of
the appropriate number of minterms by a single term, so it is these blocks that
we must look for in a Karnaugh map. Further, the larger the rectangular block,
the greater the reduction in terms and so we must utilize the larger blocks where
we have a choice. (In practice Karnaugh maps become too unwieldy for Boolean
expressions in more than about four variables and other techniques for obtaining
the minimal form, such as the Quine–McCluskey algorithm, are more appropriate.
See, for instance, Gersting (2006).)
Given the criteria for a minimal form of a Boolean expression, our priorities in
attempting to simplify an expression given as the sum of minterms are ﬁrstly

Minimization of Boolean Expressions
541
to reduce the number of terms and secondly to reduce the number of literals.
Therefore on the Karnaugh map we must aim to group into rectangles all cells
containing a one and to use the least possible number of rectangular blocks which
include all the marked cells. Since each block results in one term this ensures
the minimum number of terms. Furthermore, each cell containing a one must be
included in the largest possible block so that the number of literals appearing in
the resulting term is as small as possible.
Given a Karnaugh map, the following sequence of steps normally enables
identiﬁcation of a minimal representation of a Boolean expression. Although the
method usually gives the minimal form, it is not absolutely foolproof and, having
applied it, it is wise to check that there is no other way of grouping the ones,
which results in fewer terms or in the same number of terms but fewer literals
(see exercise 10.5.5).
(1)
Isolate any ones in the map which are not adjacent to any other ones. The
terms corresponding to these cells cannot be reduced and will therefore
appear unchanged in the minimal form.
(2)
Locate any ones that are adjacent to only one other cell containing a
one and circle the pair.
For each of these pairs, the two minterms
corresponding to the cells can be represented by a single term consisting
of the literals common to both.
(3)
Locate any ones which can be allocated to a block of four in only one way
and circle that block. The corresponding four terms can be represented
by one term consisting of the common literals.
(4)
Locate any ones which can be allocated to a block of eight and circle that
block. The corresponding eight terms can be represented by one term
consisting of the common literals.
(5)
For any cells containing a one that remain, form the largest possible
rectangular groups so that there are as few groups as possible and so that
all cells containing a one are enclosed in at least one block.
Note that the process allows for a one to be included in more than one block.
This simply means that the term corresponding to that cell is considered as being
repeated in the original Boolean expression. Since, for any b belonging to the
underlying set B of a Boolean algebra, we have b ⊕b = b (idempotent law), the
minterms in a Boolean expression in disjunctive normal form can be repeated any
number of times and the resulting expression is equivalent.
We now work through some examples to illustrate the method.

542
Boolean Algebra
Examples 10.11
1.
Find a minimal form of the Boolean expression
x1x2x3 ⊕x1¯x2x3 ⊕x1¯x2¯x3 ⊕¯x1¯x2¯x3 ⊕¯x1x2¯x3.
Solution
The Karnaugh map for the expression given is as follows.
x2x3
¯x2x3
¯x2¯x3
x2¯x3
x1
1
1
1
¯x1
1
1
There are no ones with no ‘neighbours’ so we proceed to the second step and
identify all ones which can be paired with only one other one. These are in the
top left and bottom right cells. Thus we have the following.
There are no blocks of four ones so all we have left to do is to allocate the
remaining one to a block of two. This can be done in either of two ways:
(a)
(b)

Minimization of Boolean Expressions
543
Thus we have two alternative minimal forms of the Boolean expression. They are
x1x3 ⊕¯x1¯x3 ⊕¯x2¯x3
(corresponding to the grouping in (a)) and
x1x3 ⊕¯x1¯x3 ⊕x1¯x2
(corresponding to the grouping in (b)).
2.
Find a minimal representation of
x1x2¯x3¯x4 ⊕x1x2x3¯x4 ⊕¯x1x2¯x3¯x4 ⊕¯x1¯x2x3x4
⊕¯x1¯x2¯x3x4 ⊕x1¯x2x3x4 ⊕x1¯x2¯x3x4 ⊕x1¯x2x3¯x4.
Solution
The Karnaugh map is as follows.
x3x4
¯x3x4
¯x3¯x4
x3¯x4
x1x2
1
1
¯x1x2
1
¯x1¯x2
1
1
x1¯x2
1
1
1
Again there are no isolated ones so we look for those for which there is only one
possible ‘partner’. There is one of these, in the second row.

544
Boolean Algebra
A unique block of four ones exists in the bottom left-hand corner and the
remaining ones can be blocked into a pair.
Thus a minimal representation is
¯x2x4 ⊕x2¯x3¯x4 ⊕x1x3¯x4.
3.
Find a minimal representation of the Boolean expression
x1x2¯x3x4 ⊕¯x1x2¯x3x4 ⊕¯x1x2¯x3¯x4 ⊕¯x1x2x3¯x4
⊕¯x1¯x2x3x4 ⊕¯x1¯x2¯x3x4 ⊕¯x1¯x2¯x3¯x4 ⊕x1¯x2¯x3¯x4.
Solution
Dealing ﬁrstly with the pairs we have the following.
All marked cells have been allocated to a block and the minimal representation is
¯x1¯x2x4 ⊕x2¯x3x4 ⊕¯x1x2¯x4 ⊕¯x2¯x3¯x4.

Minimization of Boolean Expressions
545
We might have been tempted in this example to select the block of four ones as
follows.
However, this would have meant that ﬁve terms would have occurred in the
reduced expression even if we had ‘paired’ each of the remaining ones with an
adjacent cell. This is not the minimal representation because it has a greater
number of terms than our ﬁrst solution.
4.
Find a minimal representation of the Boolean expression
x1x2x3x4 ⊕x1x2¯x3x4 ⊕¯x1x2¯x3x4 ⊕¯x1x2¯x3¯x4
⊕¯x1¯x2¯x3¯x4 ⊕¯x1¯x2¯x3x4 ⊕x1¯x2¯x3x4 ⊕x1¯x2x3¯x4.
Solution
The Karnaugh map is as follows.
x3x4
¯x3x4
¯x3¯x4
x3¯x4
x1x2
1
1
¯x1x2
1
1
¯x1¯x2
1
1
x1¯x2
1
1
Firstly we circle the isolated one in the bottom right-hand corner. Then we pair
the ones that have only a single adjacent one. This occurs in the top left-hand

546
Boolean Algebra
corner only.
For all the other ones there is more than one way of assigning it to a ‘pair’, so we
leave these for the time being.
Now we look for blocks of four—there are two of these.
All the ones are now covered and so a minimal representation is
x1x2x4 ⊕¯x3x4 ⊕¯x1¯x3 ⊕x1¯x2x3¯x4.
Exercises 10.5
1.
Find a minimal representation for each of the following Boolean
expressions:
(i)
x1x2x3 ⊕¯x1¯x2x3 ⊕x1¯x2¯x3 ⊕¯x1¯x2¯x3
(ii)
x1x2x3 ⊕x1¯x2x3 ⊕x1¯x2¯x3 ⊕x1x2¯x3 ⊕¯x1x2x3
(iii)
x1¯x2x3¯x4 ⊕x1¯x2x3x4 ⊕¯x1x2x3x4 ⊕¯x1¯x2¯x3¯x4
(iv)
¯x1¯x2x3x4 ⊕¯x1¯x2x3¯x4 ⊕x1¯x2x3¯x4 ⊕x1x2x3¯x4 ⊕x1x2¯x3x4 ⊕
¯x1x2x3¯x4

Minimization of Boolean Expressions
547
(v)
x1x2x3x4 ⊕x1¯x2x3x4 ⊕x1¯x2x3¯x4 ⊕x1¯x2¯x3¯x4 ⊕¯x1x2x3¯x4 ⊕
¯x1¯x2¯x3¯x4.
2.
Find a minimal form of each of the following Boolean expressions by
ﬁrst writing the expression in disjunctive normal form and then using a
Karnaugh map:
(i)
x1(x2x3 ⊕¯x3)
(ii)
(x1 ⊕x2)(¯x2 ⊕x3)
(iii)
(x1 ⊕x2 ⊕x3)(¯x1 ⊕x3)
(iv)
(x1 ⊕¯x2 ⊕x3)(¯x1 ⊕x2 ⊕x3)(x1 ⊕x2).
3.
For each of the Boolean expressions in exercise 10.4.2 obtain a minimal
form of the given expression and sketch the corresponding logic network.
Compare with the circuits obtained in exercise 10.4.2.
4.
For exercises 10.3.5 and 10.3.6, design the ‘simplest’ switching circuit
which will achieve the desired effect where ‘simplest’ means that the
Boolean expression describing the circuit is in minimal form. Compare
with the circuits previously obtained.
5.
Draw the Karnaugh map for the Boolean expression
x1x2x3x4 ⊕¯x1x2x3x4 ⊕¯x1¯x2x3x4 ⊕x1¯x2x3x4 ⊕x1x2¯x3x4
⊕¯x1¯x2¯x3x4 ⊕x1¯x2¯x3x4 ⊕x1x2¯x3¯x4 ⊕¯x1x2¯x3¯x4 ⊕¯x1¯x2¯x3¯x4
⊕x1x2x3¯x4 ⊕¯x1x2x3¯x4.
Using the method of grouping ones as described in the text, simplify this
expression as far as you can. Show that the method does not lead to the
minimal form of this Boolean expression, i.e. show that there is a better
blocking of the marked cells which gives an expression with fewer terms.

Chapter 11
Graph Theory
11.1
Deﬁnitions and Examples
Although generally regarded as one of the more modern branches of mathematics,
graph theory actually dates back to 1736. In that year Leonhard Euler† published
the ﬁrst paper on what is now called graph theory. In the paper, Euler developed
a theory which solved the so-called K¨onigsberg Bridge problem (see §11.2).
Surely few other branches of the subject can be given as precise a ‘birthday’ as
this. However, it must be said that, as a mature subject, graph theory is indeed
modern. It came of age, so to speak, exactly 200 years after Euler’s paper with the
publication in 1936 of the ﬁrst text in graph theory. (The ﬁrst 200 years of graph
theory are beautifully outlined in Biggs et al (1976) which includes extracts from
many of the original papers concerned with the development of graph theory.)
Like many of the concepts we have considered, the idea of a graph is very simple.
It is probably due to its simplicity that graph theory has found many applications
in recent years in ﬁelds as diverse as chemistry, computer science, economics,
electronics and linguistics.
Before we begin by explaining what a graph is, perhaps we should say what it is
not. The term ‘graph’ as used in this chapter and the next does not mean the graph
of a function (considered in chapter 5). It is unfortunate that the same term has
† Euler (1707–83) was born in Switzerland and spent most of his long life in Russia (St Petersburg)
and Prussia (Berlin). He was the most proliﬁc mathematician of all time, his collected works ﬁlling
more than 70 volumes. Like many of the very great mathematicians of his era, Euler contributed to
almost every branch of pure and applied mathematics. He is also responsible, more than any other
person, for much of the mathematical notation in use today.
548

Deﬁnitions and Examples
549
two quite different meanings, although it is usually clear from the context which
meaning is intended.
What, then, is a ‘graph’? Intuitively, a graph is simply a collection of points,
called ‘vertices’, and a collection of lines, called ‘edges’, each of which joins
either a pair of points or a single point to itself. A familiar example, which serves
as a useful analogy, is a road map which shows towns as vertices and the roads
joining them as edges.
For mathematical purposes we require a more precise deﬁnition. In order to deﬁne
a graph, we ﬁrst need to specify the set of its vertices and the set of its edges. Then
we need to say, in precise mathematical terms, which edges join which vertices.
An edge is deﬁned as having a vertex at each end, so we need to associate with
every edge of the graph its endpoint vertices. The endpoints of an edge are either
a pair of vertices (if the edge joins two different vertices) or a single vertex (if
the edge joins a vertex to itself). Thus for every edge e of a graph we deﬁne a
set {v1, v2} of vertices which speciﬁes that e joins vertices v1 and v2, where of
course we need to allow the possibility that v1 = v2. Now this set {v1, v2}, which
we denote by δ(e), is a subset of the set of vertices. Therefore δ(e) is an element
of the power set of the vertex set. This leads us to the following formal deﬁnition.
Its rather technical nature should not be allowed to obscure the essentially simple
concept that is being described.
Deﬁnition 11.1
An undirected graph Γ comprises:
(i)
a ﬁnite non-empty set V of vertices,
(ii)
a ﬁnite set E of edges, and
(iii)
a function δ : E →P(V ) such that, for every edge e, δ(e) is a
one- or two-element subset of V .
The edge e is said to join the element(s) of δ(e).
Generally we shall use the term ‘graph’ without qualiﬁcation to mean an
undirected graph. If we need to emphasize a speciﬁc graph Γ we will write VΓ, EΓ
and δΓ for the sets V and E and the function δ : E →P(V ) respectively.
As we have explained, this function δ is merely a formal way of specifying
the ends of edges.
In this case where an edge e joins a vertex to itself, the

550
Graph Theory
Figure 11.1
set δ(e) will contain a single element.
Consider, for example, the graph Γ
represented in ﬁgure 11.1. Clearly Γ has vertex set {v1, v2, v3, v4} and edges
set {e1, e2, e3, e4, e5}. The function δ : E →P(v) is deﬁned by
δ : e1 7→{v1}
δ : e2 7→{v1, v2}
δ : e3 7→{v1, v3}
δ : e4 7→{v2, v3}
δ : e5 7→{v2, v3}.
This simply indicates that e1 joins vertex v1 to itself, e2 joins vertices v1 and v2,
etc.
We emphasize that an edge may join a vertex to itself, as in the case of e1, and a
vertex may be connected to no edges at all, as in the case of v4. Also note that a
given pair of vertices may be joined by more than one edge; in this example the
edges e4 and e5 both connect the vertices v2 and v3.
Unfortunately there are many variations on the deﬁnition of a graph.
Some
authors use a deﬁnition which excludes the possibility of multiple edges in
their graphs; that is, several edges connecting the same pair of vertices. Other
deﬁnitions exclude the possibility of loops—edges which join a vertex to itself.
We shall call a graph which satisﬁes both these restrictions—that has no loops or
multiple edges—a simple graph†. The terminology of graph theory is distinctly
non-standard. When consulting other texts you are strongly advised to check very
carefully the author’s deﬁnitions and terminology.
There is one restriction which we have placed on a graph which, though common,
is not universal; namely, that the sets of vertices and edges are ﬁnite. If either (or
† Those authors who deﬁne a graph to be what we are calling a simple graph frequently use the term
multigraph to denote the more general concept (the one which we have called graph).

Deﬁnitions and Examples
551
both) of these are inﬁnite Γ is usually called an inﬁnite graph, although we shall
not consider these.
We should emphasize at the outset that a graph and a diagram representing it are
not the same thing. As we have deﬁned it, a graph consists of two sets together
with a function. Figure 11.1 itself is not a graph but a pictorial representation
of one. Whilst diagrams are extremely helpful in understanding the properties of
graphs, some care needs to be taken in interpreting them. The most signiﬁcant
point to make is that a given graph may be represented by two diagrams which
appear very different. For instance, the two diagrams in ﬁgure 11.2 represent the
same graph Γ as can be observed by writing down the function δ : E →P(V ).
The diagram in ﬁgure 11.2(a) indicates why this graph is sometimes called the
cycle graph with seven vertices, denoted by C7. Clearly for every positive integer
n there is a corresponding graph Cn, the cycle graph with n vertices and n edges.
For each n, the diagram representing Cn can be drawn as a circle with n vertices
around its circumference. It should be clear that Cn is simple if and only if n ⩾3.
Figure 11.2
When ﬁrst studying graph theory, one thing soon becomes apparent: there are
initially many more deﬁnitions than theorems.
This is probably because to
say, or prove, anything signiﬁcant about graphs requires reasonably developed
terminology. We collect together below a few basic deﬁnitions.

552
Graph Theory
Deﬁnitions 11.2
(i)
A pair of vertices v and w are adjacent if there exists an edge e
joining them. In this case we say both v and w are incident to e
and also that e is incident to v and to w.
(ii)
The edges e1, e2, . . . , en are adjacent if they have at least one
vertex in common.
(iii)
The degree or valency, deg(v), of a vertex v is the number of edges
which are incident to v. (Unless stated otherwise, a loop joining v
to itself counts two towards the degree of v.) A graph in which
every vertex has the same degree r is called regular (with degree
r) or simply r-regular.
Examples 11.1
1.
Let Γ be the graph illustrated in ﬁgure 11.1. The vertices v1 and v2
are adjacent, because the edge e2 joins them. Similarly v1 and v3 are
adjacent, as are v2 and v3. The vertex v4 is adjacent to no other vertex.
Edges e1, e2 and e3 are adjacent, since they all meet at vertex v1.
Similarly e2, e4, e5 are adjacent, as are e3, e4, e5.
Note that only pairs of vertices may be adjacent, but any number of edges
can be adjacent.
The degrees of the four vertices are given in the following table.
Vertex
Degree
v1
4
v2
3
v3
3
v4
0
2.
A well known 3-regular simple graph is Petersen’s graph. Two diagrams
representing this graph are given in ﬁgure 11.3. (We have omitted to label
the vertices and edges for the sake of clarity of the diagram.)

Deﬁnitions and Examples
553
Figure 11.3
In drawing diagrams of graphs we only allow edges to meet at vertices. It is not
always possible to draw diagrams in the plane satisfying this property (see §11.5),
so we may need to indicate that one edge passes underneath another as we have
done in ﬁgure 11.3.
Deﬁnitions 11.3
(i)
A null graph (or totally disconnected graph) is one whose edge
set is empty. (Pictorially, a null graph is just a collection of points.)
(ii)
A complete graph is a simple graph in which every pair of distinct
vertices is joined by an edge.
(iii)
A bipartite graph is a graph where the vertex set has a partition
{V1, V2} such that every edge joins a vertex of V1 to a vertex of V2.
(iv)
A complete bipartite graph is a bipartite graph such that every
vertex of V1 is joined to every vertex of V2 by a unique edge.
Examples 11.2
1.
Since a complete graph is simple there are no loops and each pair of
distinct vertices is joined by a unique edge. Clearly a complete graph is
uniquely speciﬁed by the number of its vertices.

554
Graph Theory
The complete graph Kn with n vertices can be described as follows.
It has vertex set V = {v1, v2, . . . , vn} and edge set E = {eij : 1 ⩽i <
j ⩽n} with the function δ given by
δ(eij) = {vi, vj}.
The graph Kn is clearly regular with degree n −1, since every vertex is
connected, by a unique edge, to each of the other n −1 vertices.
The complete graphs with three, four and ﬁve vertices are illustrated in
ﬁgure 11.4.
Figure 11.4
2.
Let Γ be a bipartite graph where the vertex set V has the partition
{V1, V2}. Note that Γ need not be simple. All that is required is that
each edge must join a vertex of V1 to a vertex of V2. Given v1 ∈V1
and v2 ∈V2, there may be more than one edge joining them or no edge
joining them. Clearly, though, there are no loops in Γ.
A complete bipartite graph is completely speciﬁed by |V1| and |V2|. The
complete bipartite graph on n and m vertices, denoted Kn,m, has
|V1| = n and |V2| = m. It is necessarily simple.
Figure 11.5 shows two bipartite graphs. In each case the vertices of V1
are indicated by full circles and the vertices of V2 by crosses. The graph
in (b) is the complete bipartite graph, K3,3.
We have noted that a graph Γ may be represented by diagrams that appear very
different. An alternative way of representing a graph, one which is easier for
computer representation, for instance, is by its ‘adjacency matrix’ which we now
deﬁne.

Deﬁnitions and Examples
555
Figure 11.5
Deﬁnition 11.4
Let Γ be a graph with vertex set {v1, v2, . . . , vn}. The adjacency matrix
of Γ is the n × n matrix A = A(Γ) such that aij is the number† of distinct
edges joining vi and vj.
The adjacency matrix is necessarily symmetric as the number of edges joining
vi and vj is the same as the number joining vj and vi. The degree of vertex vi
is easily determined from the adjacency matrix. If there are no loops at vi then
its degree is the sum of the entries in the ith column (or ith row) of the matrix.
Since every loop counts twice in the degree, when summing the entries in the ith
column (or ith row) the diagonal element aii must be doubled to obtain the degree
of vi.
Examples 11.3
1.
The following is the adjacency matrix A of the graph represented in
ﬁgure 11.1:
A =




1
1
1
0
1
0
2
0
1
2
0
0
0
0
0
0



.
† Unfortunately this is another instance where terminology varies. For some authors the adjacency
matrix is a binary matrix with aij = 0 if vi and vj are not adjacent and aij = 1 if they are
adjacent, regardless of the number of edges connecting them. For simple graphs, of course, there is
no distinction since there can be at most one edge joining any pair of vertices.

556
Graph Theory
Note that V = {v1, v2, v3, v4} and the rows and columns of A refer to the
vertices in the order listed. Just as for the binary matrix of a relation (see
§4.1), we must always be clear which rows and columns refer to which
vertices.
Two properties of the graph are immediately apparent from the matrix.
Firstly, by considering the leading diagonal we note that there is only
one loop—from v1 to itself. Secondly, the last row (or column) of zeros
indicates that v4 is an isolated vertex connected to no vertices at all
(including itself).
The degrees of the vertices are easily calculated from the matrix as
follows:
deg(v1) = 2 × 1 + 1 + 1 = 4
deg(v2) = 1 + 2 = 3
deg(v3) = 1 + 2 = 3
deg(v4) = 0.
2.
The null graph with n vertices has the n × n zero matrix On×n as its
adjacency matrix, since there are no edges whatsoever.
3.
A complete graph has adjacency matrix with zeros along the leading
diagonal (since there are no loops) and ones everywhere else (since every
vertex is joined to every other by a unique edge).
There is one more piece of terminology which we wish to introduce in this section.
The notion of a ‘subgraph’ of a graph is probably more or less self-evident. The
formal deﬁnition is the following. (Compare with deﬁnition 8.12 of a subgroup,
for example.)
Deﬁnition 11.5
A graph Σ is a subgraph of the graph Γ, denoted Σ ⩽Γ, if VΣ ⊆VΓ,
EΣ ⊆EΓ and δΣ(e) = δΓ(e), for every edge e of Σ.
The condition that δΣ(e) = δΓ(e), for every edge e of Σ, means only that the
edges of the subgraph Σ must join the same vertices as they do in Γ. Intuitively,

Deﬁnitions and Examples
557
Σ is a subgraph of Γ if we can obtain a diagram for Σ by erasing some of the
vertices and/or edges from a diagram of Γ. Of course, if we erase a vertex we
must also erase all edges incident to it.
Example 11.4
Graphs Γ and Σ have vertex sets VΓ
=
{v1, v2, v3, v4, v5} and VΣ
=
{v1, v2, v4, v5} and respective adjacency matrices






1
1
0
1
1
1
0
2
1
0
0
2
0
0
1
1
1
0
0
1
1
0
1
1
0






and




1
1
0
1
1
0
0
0
0
0
0
1
1
0
1
0



.
Figure 11.6 indicates that we can regard Σ as a subgraph of Γ.
Figure 11.6
Exercise 11.1
1.
Draw diagrams to represent the complete graphs K2 and K6 and the
complete bipartite graphs K2,5 and K4,4.
2.
Draw diagrams to represent each of the graphs whose adjacency matrix
is given below. Write down the degree of each vertex, and state whether
the graph is (a) simple; (b) regular.
(i)




1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1





558
Graph Theory
(ii)








0
1
0
0
0
1
1
0
1
0
0
0
0
1
0
1
0
1
0
0
1
0
1
0
0
0
0
1
0
1
1
0
1
0
1
0








(iii)






1
2
0
2
1
2
1
2
0
1
0
2
1
2
1
2
0
2
1
1
1
1
1
1
0






.
3.
Copy ﬁgure 11.3(a) of Petersen’s graph and label the vertices and edges.
(i)
Write out explicitly the function δ : E →P(V ).
(ii)
Write down the adjacency matrix for the graph.
4.
Is it possible for a graph to be both null and complete? If so, how is it
possible? If not, why not?
5.
(i)
For each of the graphs in ﬁgure 11.5, label the vertices and edges
and write down the adjacency matrix of the graph.
(ii)
Let Γ be any bipartite graph whose vertex set is partitioned into
the subsets {v1, v2, . . . , vp} and {w1, w2, . . . , wq}. What can you
say about the adjacency matrix of Γ?
6.
(i)
Is Petersen’s graph bipartite? Justify your answer.
(ii)
For which values of n is the cycle graph Cn bipartite?
(iii)
Devise an algorithm for testing whether a graph is bipartite given
a diagram of the graph.
7.
(i)
Prove the following well known result about graphs.
The Handshaking Lemma
In any graph, the sum of the vertex degrees is twice the number of
edges,
X
v∈V
deg(v) = 2 × |E|.
(ii)
Deduce that, in any graph, the number of vertices with odd degree
is even.
8.
The degree sequence of a graph is the sequence of its vertex degrees

Deﬁnitions and Examples
559
arranged in non-decreasing order. For example, the degree sequence of
the graph shown in ﬁgure 11.1 is (0, 3, 3, 4).
(i)
Write down the degree sequence of each of the graphs illustrated
in ﬁgures 11.2–11.6.
(ii)
Describe the degree sequence of
(a)
a null graph with n vertices;
(b)
the complete graph Kn;
(c)
an r-regular graph with n vertices;
(d)
the complete bipartite graph Kn,m where n ⩽m.
(iii)
What information about a graph may be deduced from:
(a)
the number of entries in its degree sequence?
(b)
the sum of the entries of its degree sequence?
9.
For each of the following sequences, either draw the diagram of a graph
with the given sequence as its degree sequence or explain why no graph
has the given sequence as its degree sequence.
(i)
(2, 2, 2, 2, 3, 3, 4)
(ii)
(1, 2, 2, 2, 3, 3)
(iii)
(1, 2, 2, 2, 2, 3)
(iv)
(2, 2, 2, 3, 3, 3, 3)
(v)
(2, 2, 2, 2, 3, 3, 3).
10.
For each of the following matrices, draw the diagram of a graph with the
given matrix as its adjacency matrix and write down the degree sequence
of the graph.
(i)






0
1
0
1
1
1
0
1
0
0
0
1
0
1
1
1
0
1
0
1
1
0
1
1
0






(ii)








0
1
0
0
1
1
1
0
1
0
0
1
0
1
0
1
0
1
0
0
1
0
1
1
1
0
0
1
0
1
1
1
1
1
1
0








.
11.
For n ⩾2, the wheel graph Wn is the graph obtained from the cycle
graph Cn by adding a single new vertex and joining it to each existing
vertex of Cn by a unique edge. Diagrams of the graphs of W5 and W6
are given below.

560
Graph Theory
Describe
(i)
the degree sequence
(ii)
the adjacency matrix
of Wn.
12.
Prove that, for n ⩾2, the complete graph Kn contains Kn−1 as a
subgraph.
13.
Show that, if Γ is a simple graph with n vertices, then |E| ⩽1
2n(n −1).
(Hint: think of Kn.)
14.
Let Γ and Σ be two graphs with disjoint vertex and edge sets. The union
of Γ and Σ is the graph denoted Γ ∪Σ whose vertex and edge sets are
respectively the unions of the vertex and edge sets of Γ and Σ with the
obvious function δ. The sum, Γ + Σ, of Γ and Σ is obtained by taking
the union of Γ and Σ and then joining each vertex of Γ to each vertex of
Σ by a unique edge.
What is the sum of (i) two null graphs, (ii) two complete graphs?
15.
Explain why any simple graph with n vertices may be regarded as a
subgraph of the complete graph Kn.
16.
Let Γ be a graph without any loops and with vertex and edge sets
{v1, v2, . . . , vn} and {e1, e2, . . . , em} respectively.
The incidence
matrix of Γ is the m × n matrix B such that bij = 1 if the edge ei
is incident with the vertex vj, and bij = 0 otherwise.
(i)
With a suitable choice of labelling if necessary, write down the
incidence matrices of the graphs represented in ﬁgures 11.1, 11.2,
11.3 and 11.4.
(ii)
What can be said about the sum of the entries in each row?
(iii)
What information is provided by the sum of the entries of each
column?

Paths and Cycles
561
17.
Draw a diagram to represent the graph whose incidence matrix is:
(i)












1
1
0
0
0
1
1
0
0
0
1
0
0
0
1
1
0
0
1
0
0
1
0
0
1
0
1
0
1
0
0
0
0
1
1
0
0
0
1
1












(ii)












1
1
0
0
0
0
1
0
0
1
0
0
0
1
0
1
0
0
0
1
0
0
1
0
0
0
1
1
0
0
0
0
1
0
0
1
0
0
0
1
1
0
0
0
0
0
1
1












.
18.
This question refers to the ‘blubs and glugs’ axiom system introduced in
chapter 2.
(i)
Show that the axiom system may be modelled by a graph Γ with
‘blubs’ interpreted as vertices and ‘glugs’ as edges. What special
properties must Γ satisfy in order that it is a model for the system?
(ii)
Give an alternative model for the axiom system as a bipartite
graph with ‘blubs’ and ‘glugs’ interpreted as the elements of the
sets of vertices V1 and V2 respectively.
In this model what is
the interpretation of ‘lies on’? What special properties must the
bipartite graph satisfy in order to serve as a model for the system?
11.2
Paths and Cycles
Using the analogy of a road map, we can consider various types of ‘journeys’
in a graph. For instance, if the graph actually represents a network of roads
connecting various towns, one question we might ask is: is there a journey,
beginning and ending at the same town, which visits every other town just once
without traversing the same road more than once? As usual, we begin with some
deﬁnitions.

562
Graph Theory
Deﬁnitions 11.6
(i)
An edge sequence of length n in a graph Γ is a sequence of
(not necessarily distinct) edges e1, e2, . . . , en such that ei and
ei+1 are adjacent for i = 1, 2, . . . , n −1. The edge sequence
determines a sequence of vertices (again, not necessarily distinct)
v0, v1, v2, . . . , vn−1, vn where δ(ei) = {vi−1, vi}. We say v0 is
the initial vertex and vn the ﬁnal vertex of the edge sequence.
(ii)
A path is an edge sequence in which all the edges are distinct. If
in addition all the vertices are distinct (except possibly v0 = vn)
the path is called simple.
(iii)
An edge sequence is closed if v0 = vn. A closed simple path
containing at least one edge is called a cycle or a circuit.
An edge sequence is any ﬁnite sequence of edges which can be traced on the
diagram of the graph without removing pen from paper. It may repeat edges,

Paths and Cycles
563
go round loops several times, etc. Edge sequences are too general to be of very
much use which is why we have deﬁned paths. In a path we are not allowed to
‘travel along’ the same edge more than once. If, in addition, we do not ‘visit’
the same vertex more than once (which rules out loops), then the path is simple.
The edge sequence or path is closed if we begin and end the ‘journey’ at the same
place.
Examples 11.5
1.
Let Γ be the graph represented in ﬁgure 11.1; examples of edge sequences
in Γ are:
(i)
e1, e3, e4, e5, e3;
(ii)
e3, e3;
(iii)
e2, e3, e4;
(iv)
e4, e3;
(v)
e4, e5, e2.
Sequence (i) is a closed edge sequence beginning and ending at v1: it
determines the vertex sequence v1, v1, v3, v2, v3, v1. This edge sequence
is not a path because the edge e3 is traversed twice.
Sequence (ii) is also closed, but it is ambiguous whether it begins (and
ends) at v1 or v3.
The vertex sequence could be either v1, v3, v1 or
v3, v1, v3. This ambiguity will always occur in an edge sequence of the
form ei, ei, . . . , ei where e1 is not a loop†. Again, it is not a path.
Sequence (iii) is a cycle: it begins and ends at v2 and no edge or vertex
(except v2 itself) is repeated.
Sequence (iv) is a simple path from v2 to v1.
Sequence (v) is a path with initial and ﬁnal vertices v2, v1 respectively.
It is not a simple path because vertex v2 appears twice in the associated
vertex sequence.
2.
Let Γ be Petersen’s graph illustrated in ﬁgure 11.3. Beginning at any
vertex there is a simple path which passes through every vertex; we leave
† There is another edge sequence whose vertex sequence is ambiguous, namely the empty sequence
which has no edges. We regard this has having vertex sequence vi for any vertex vi. The empty edge
sequence is, in fact, a simple closed path but not a cycle.

564
Graph Theory
it as an easy exercise to ﬁnd such a simple path. However, there are no
cycles which pass through every vertex.
Let Γ be the graph represented in ﬁgure 11.1. Its adjacency matrix is
A =




1
1
1
0
1
0
2
0
1
2
0
0
0
0
0
0



.
The (i, j)-entry of A is the number of edges joining vertices vi and vj. We
can think of this as the number of edge sequences of length 1 joining these two
vertices. Now the square of the adjacency matrix is
A2 =




3
3
3
0
3
5
1
0
3
1
5
0
0
0
0
0



.
In A2 the (i, j)-entry represents the number of edge sequences of length 2 joining
vi and vj. For example, the (2, 2)-entry is 5 and there are the following ﬁve edge
sequences of length 2 joining v2 to itself: e2, e2; e4, e4; e5, e5; e4, e5; e5, e4.
It is not too difﬁcult to see why this occurs. The (i, j)-entry of A2 is obtained by
‘multiplying’ the ith row and the jth column of A. In deﬁnition 6.3 we expressed
this as
n
X
k=1
aikakj.
The rth term in this sum, airarj is the product of the number of edges connecting
vi and vr with the number connecting vr and vj; in other words the number of
edge sequences of length 2 joining vi and vj via vr. Summing over all k gives the
total number of length 2 edge sequences connecting vi and vj.
Similarly the (i, j)-entry of A3 represents the number of edge sequences of length
3 joining vi and vj. For this graph
A3 =




9
9
9
0
9
5
13
0
9
13
5
0
0
0
0
0



.
The nine edge sequences of length 3 joining v1 and v2 are: e1, e1, e2; e2, e2, e2;
e1, e3, e4; e1, e3, e5; e3, e3, e2; e2, e4, e4; e2, e5, e5; e2, e4, e5; e2, e5, e4.

Paths and Cycles
565
The following theorem can be proved by induction. The inductive step is similar
to the argument used above (exercise 11.2.1).
Theorem 11.1
Let Γ be a graph with vertex set {v1, v2, . . . , vr} and adjacency matrix A.
The (i, j)-entry of An is the number of edge sequences of length n joining
vi and vj.
Connectedness
In an intuitively obvious sense, some graphs are ‘all in one piece’ and others are
made up of several pieces. We can use paths to make this idea more precise.
Deﬁnition 11.7
A graph is connected if, given any pair of distinct vertices, there exists a
path connecting them.
An arbitrary graph naturally splits into a number of connected subgraphs, called
its (connected) components.
The components can be deﬁned formally as
maximal connected subgraphs. This means that Γ1 is a component of Γ if it is
a connected subgraph of Γ and it is not itself a proper subgraph of any other
connected subgraph of Γ. This second condition is what we mean by the term
maximal; it says that if Σ is a connected subgraph such that Γ1 ⩽Σ, then Σ = Γ1
so there is no connected subgraph of Γ which is ‘bigger’ than Γ1.
We shall not be too concerned with the formal deﬁnition of components as the
intuitive idea is clear; the components of a graph are just its connected ‘pieces’.
In particular, a connected graph has only one component. Decomposing a graph
into its components can be very useful. It is usually simpler to prove results
about connected graphs and properties of arbitrary graphs can frequently then be
deduced by considering each component in turn.

566
Graph Theory
The following is an outline of an alternative way of deﬁning the components of a
graph Γ. Deﬁne a relation R on VΓ by
v R w
if and only if v and w can be joined by a path in Γ.
Provided we allow the empty path with no edges, it is easily seen that R is an
equivalence relation (exercise 11.2.2). Let {V1, V2, . . . , Vp, } be the partition of
the vertex set by the equivalence classes of R. We can now form subgraphs Γi
with vertex Vi and whose edges are those of Γ which join two vertices of Vi.
These subgraphs Γi are the components of Γ.
Examples 11.6
1.
The graph illustrated in ﬁgure 11.1 has two components, one of which is
the null graph with vertex set {v4}.
All the other graphs which have been considered so far are connected, i.e.
have one component.
2.
Frequently it is clear from a diagram of Γ how many components it has.
Sometimes, however, we need to examine the diagram more closely. For
instance, both graphs illustrated in ﬁgure 11.7 have two components,
although this is not instantly apparent for the graph (b).
Figure 11.7
Eulerian Paths
We have mentioned Euler’s 1736 paper which marked the birth of graph theory.
This paper developed a theory which was able to solve the so-called K¨onigsberg

Paths and Cycles
567
Bridge problem, which is the following. The Pregel River ﬂows through the
town of K¨onigsberg in Russia. There are two islands in the river, connected to
the banks and each other by bridges as shown in ﬁgure 11.8(a). The problem for
the citizens of K¨onigsberg was whether there was a walk, beginning on one of the
banks or islands, which took in every bridge exactly once and ﬁnished back at the
starting position. They were unable to ﬁnd such a walk; the problem was either to
ﬁnd such a walk or to show that none existed.
Euler ﬁrst represented the essential features of K¨onigsberg’s geography by a
graph, as illustrated in ﬁgure 11.8(b). Each of the river banks and islands is
represented by a vertex with the edges corresponding to the connecting bridges.
In graph-theoretic terms the question is whether there exists a closed path which
includes all the edges of the graph.
Figure 11.8
Deﬁnition 11.8
An Eulerian path in a graph Γ is a closed path which includes every edge
of Γ. A graph is said to be Eulerian if it has at least one Eulerian path.
Recall that in a path no edge can be traversed more than once. Thus an Eulerian
path includes every edge exactly once, although, of course, vertices may be visited
more than once.
For a connected graph Γ there is an easily recognized necessary condition for it to
have an Eulerian path; namely, every vertex must have even degree. To see this,
suppose that Γ is connected and has an Eulerian path. Since Γ is connected, the
vertex sequence of the Eulerian path contains every vertex. Whenever the path
passes through a vertex it contributes two to its degree (one from the edge ‘going
in to’ and one from the edge ‘coming out from’ the vertex). Since every edge
occurs exactly once in the path, every vertex must have even degree.

568
Graph Theory
The people of K¨onigsberg had not been able to ﬁnd their Eulerian path for a very
good reason—there isn’t one. The graph representing the problem, ﬁgure 11.8(b),
is connected but fails the required condition. Every vertex, in fact, has odd degree.
Euler also proved the less obvious fact that, for a connected graph Γ, this
necessary condition is also sufﬁcient for it to be Eulerian. A proof of this is
indicated in exercise 11.2.15.
Theorem 11.2 (Euler)
A connected graph Γ is Eulerian if and only if every vertex has even degree.
Examples 11.7
1.
The complete graph Kn is (n−1)-regular—every vertex has degree n−1.
Since it is connected, Kn is Eulerian if and only if n is odd (so that n −1
is even). The graph K3 has an obvious Eulerian path and we leave it as
an exercise to ﬁnd an Eulerian path in K5—see ﬁgure 11.4. (In fact, K5
has 264 Eulerian paths.)
2.
The complete bipartite graph K4,4 is represented in ﬁgure 11.9. The
vertices have been partitioned into the sets {1, 2, 3, 4} and {a, b, c, d}.
The graph is connected and every vertex has degree 4, so K4,4 is Eulerian
by theorem 11.2.
One Eulerian path beginning at the vertex 1 has the following vertex
sequence:
1, a, 2, b, 3, c, 4, d, 1, c, 2, d, 3, a, 4, b, 1.
Hamiltonian Cycles
An Eulerian path seeks to travel along every edge of the graph (once) and return to
the starting position. An analogous problem is whether we can visit every vertex
once, without travelling along any edge more than once, and return to the starting

Paths and Cycles
569
Figure 11.9
position. This problem was considered by Hamilton† (although he was probably
not the ﬁrst to do so in the case of general graphs) and his name is now associated
with these paths.
Deﬁnition 11.9
A Hamiltonian cycle in a graph is a cycle which passes once through every
vertex. A graph is Hamiltonian if it has a Hamiltonian cycle.
Example 11.8
Figure 11.10 illustrates Hamiltonian cycles in two graphs. The graph (a) is the
complete bipartite graph K3,3 deﬁned in example 11.2.2, and the graph (b) is
called the dodecahedral graph‡. In fact, it was cycles in the dodecahedral graph,
† Sir William Rowan Hamilton (1805–65) was Ireland’s most gifted mathematician–scientist. As a
22-year old undergraduate he was elected Professor of Astronomy and Astronomer Royal of Ireland.
In fact he made little contribution to astronomy; his most signiﬁcant work was in mathematics and
physics. In 1843 he discovered the quaternions—a sort of generalized complex numbers—and he
devoted most of the rest of his life to their study. His name is also associated with the Hamiltonian
energy operator used in physics, particularly wave mechanics.
‡ The dodecahedron is one of the ﬁve regular three-dimensional solids; it has 12 faces, each a regular
pentagon, 30 edges and 20 vertices each of degree 3.
The dodecahedral graph is a (necessarily
distorted) two-dimensional representation of the solid. The other four regular solids can also be
represented by graphs—see exercise 11.2.8.

570
Graph Theory
in particular, that interested Hamilton and he invented a board game, The Icosian
Game, which explored the cycles in this graph.
Figure 11.10
Although Eulerian graphs have a simple characterization, the same is not
true of Hamiltonian graphs.
Indeed after more than a century of study, no
characterization of Hamiltonian graphs is known. (By a ‘characterization’ of
Hamiltonian graphs we mean necessary and sufﬁcient conditions for a graph to be
Hamiltonian.) This remains one of the major unsolved problems of graph theory.
An obvious necessary condition is that the graph be connected. Various sufﬁcient
conditions are also known; most require the graph to have ‘enough’ edges in some
sense. One of the simplest such results is the following.
Theorem 11.3
If Γ is a connected simple graph with n (⩾3) vertices and if the degree
deg(v) ⩾1
2n for every vertex v, then Γ is Hamiltonian.
The condition on the degrees, deg(v) ⩾1
2n, is not a necessary condition for Γ to
be Hamiltonian, so a graph can be Hamiltonian without satisfying this condition.
We can see this by considering the dodecahedral graph—ﬁgure 11.10(b). The
graph has 20 vertices, every vertex has degree 3, but it is still Hamiltonian. In
fact the graph of each of the ﬁve regular solids has a Hamiltonian cycle—see
exercise 11.2.8.

Paths and Cycles
571
Exercises 11.2
1.
Using induction on the number of vertices, prove theorem 11.1.
2.
Prove that the relation R on the set V of vertices of graph Γ deﬁned by
v R w
if and only if there exists a path in Γ joining v and w
is an equivalence relation. (You need to allow the empty path with no
edges, which can be viewed as joining any vertex to itself.)
3.
(i)
Is a null graph Eulerian?
(ii)
Is it possible for a non-connected graph to be Eulerian?
4.
(i)
For which values of n is the complete graph Kn Eulerian?
(ii)
For which values of r and s is the complete bipartite graph Kr,s
Eulerian?
5.
(i)
Each of the following matrices is an adjacency matrix of a
graph. In each case, determine whether the corresponding graph is
Eulerian.
(a)








0
1
1
1
1
0
1
0
0
0
1
0
1
0
0
1
0
0
1
0
1
0
1
1
1
1
0
1
0
1
0
0
0
1
1
0








(b)








0
1
0
1
1
0
1
0
0
0
1
0
0
0
0
1
0
1
1
0
1
0
1
0
1
1
0
1
0
1
0
0
1
0
1
0








(c)








0
0
1
0
1
0
0
0
0
1
0
1
1
0
0
0
1
0
0
1
0
0
0
1
1
0
1
0
0
0
0
1
0
1
0
0









572
Graph Theory
(d)








1
1
0
0
0
1
1
0
1
2
1
1
0
1
0
1
0
0
0
2
1
0
1
0
0
1
0
1
0
0
1
1
0
0
0
0








.
(ii)
Let Γ be a connected graph. How can you determine, from its
adjacency matrix, whether or not Γ is Eulerian?
6.
Consider the graph Γ whose diagram is given below.
(i)
Is Γ Eulerian?
(ii)
Is Γ Hamiltonian?
7.
A connected graph is called semi-Eulerian if it has a (not necessarily
closed) path containing all edges. Use theorem 11.2 to prove: Γ is semi-
Eulerian, but not Eulerian, if and only if all vertices except two have even
degree.
8.
Show that each of the graphs of the regular solids shown in ﬁgure 11.11
is Hamiltonian.
Figure 11.11

Paths and Cycles
573
Figure 11.11 (Continued)
9.
(i)
Show that a closed path in a bipartite graph contains an even
number of edges.
(ii)
When is Kr,s Hamiltonian?
10.
A graph is called semi-Hamiltonian if there exists a (not necessarily
closed) simple path which passes through each vertex.
Which of the graphs deﬁned in §11.1 are (i) Eulerian, (ii) semi-
Eulerian but not Eulerian, (iii) Hamiltonian, (iv) semi-Hamiltonian but
not Hamiltonian?
11.
(i)
What is the minimum number of bridges which would have to have
been built in K¨onigsberg so that its graph is (a) semi-Eulerian,
(b) Eulerian?
(ii)
Is the graph of the K¨onigsberg bridges Hamiltonian? If not, what
is the minimum number of bridges which would have to have been
built so that the graph becomes Hamiltonian?
12.
For each of the following, determine whether the graph is:
(a)
Eulerian, semi-Eulerian or neither;
(b)
Hamiltonian, semi-Hamiltonian or neither.

574
Graph Theory
13.
(i)
Prove that a connected graph Γ is Eulerian if and only if it can be
split into cycles, no two of which have any edges in common.
(ii)
Show that the Eulerian graph below can be split into four cycles,
no two of which have any edges in common. How can these cycles
be combined to form an Eulerian path?
14.
Knight’s tour problems
Can a knight visit each square of a chessboard by a sequence of knight’s
moves and ﬁnish on the same square as it began?
A solution to the
problem, if it exists, is called a knight’s tour of the board.
The problem can be modelled by a graph whose vertices represent the
squares of the board where two vertices are joined by an edge if and
only if there is a knight’s move between the corresponding squares on the
chessboard.
For example, a 4×4 ‘chessboard’ and its corresponding graph are shown
below.
(i)
Draw the graph corresponding to a 3 × 3 chessboard. Deduce that
there is no knight’s tour on a 3 × 3 board.
(ii)
In fact there is no knight’s tour on the 4 × 4 board—experiment a
bit to convince yourself of this.
(iii)
Prove that a bipartite graph with an odd number of vertices is not
Hamiltonian. Deduce that there is no knight’s tour on a 5 × 5 or a
7 × 7 board.

Isomorphism of Graphs
575
(iv)
The original knight’s tour problem on an 8 × 8 board does have a
solution. See if you can ﬁnd a knight’s tour.
15.
Prove that if Γ is connected and every vertex has even degree then Γ is
Eulerian. (This completes the proof of theorem 11.2.)
One method of proof is by induction on |E|, the number of edges of Γ.
The inductive step is outlined below.
Firstly, choose any vertex of v of Γ and a closed path P beginning and
ending at v.
If P contains every edge of Γ we are ﬁnished; otherwise remove all
the edges of P to form a new graph Γ′.
This new graph Γ′ may
be disconnected.
Consider each component of Γ′ in turn and use
the inductive hypothesis to obtain an Eulerian path in each of these
components.
Finally use P and the Eulerian paths in each component of Γ′ to piece
together an Eulerian path for Γ.
11.3
Isomorphism of Graphs
Consider the two graphs Γ and Σ deﬁned as follows: Γ has vertex set {1, 2, 3, 4}
and the adjacency matrix A, and Σ has vertex set {a, b, c, d} and the adjacency
matrix B, where
A =




1
2
1
1
2
0
0
1
1
0
0
3
1
1
3
0




B =




0
3
0
1
3
0
1
1
0
1
0
2
1
1
2
1



.
Diagrams representing Γ and Σ are given in ﬁgure 11.12.
With some thought it should be apparent that the graphs represented in
ﬁgure 11.12 are essentially the same. If we re-label the vertices a, b, c, d of
Σ as 3, 4, 2, 1, in that order, and re-label the edges fi as ei for i = 1, . . . , 8, then
the two diagrams in ﬁgure 11.12 could be regarded as different representations of
the same graph. Of course, Γ and Σ are not identical graphs—they have different
vertex sets, for instance. However they do have the ‘same structure’ in some

576
Graph Theory
Figure 11.12
sense. We say that Γ and Σ are ‘isomorphic’ graphs. (The notion of isomorphic
graphs is precisely the graph theory equivalent of isomorphic groups considered
in chapter 8.)
In relabelling the vertices of Σ we have deﬁned a bijection between the vertex
sets of Γ and Σ in such a way that the edge sets also correspond.
In other
words, if there are n edges joining two vertices in Γ, then there are also n edges
joining two vertices in Σ. This correspondence of vertices and edges is called an
‘isomorphism’ from Γ to Σ. The technical deﬁnition is the following.
Deﬁnition 11.10
Let Γ and Σ be two graphs. An isomorphism from Γ to Σ consists of a
pair (θ, φ) of bijections
θ : VΓ →VΣ
and
φ : EΓ →EΣ
such that, for every edge e of Γ, if δΓ(e) = {v, w} then δΣ(φ(e)) =
{θ(v), θ(w)}.
To graphs are said to be isomorphic, denoted Γ ∼= Σ, if there exists an
isomorphism from one graph to another.
The condition that if δΓ(e) = {v, w} then δΣ(φ(e)) = {θ(v), θ(w)} is to ensure
that the two correspondences between vertices and edges of the two graphs ‘match
up’ in the correct way. In other words, if the edge e of Γ corresponds to the edge

Isomorphism of Graphs
577
φ(e) of Σ then their endpoint vertices also correspond (under the vertex bijection
θ). This is best illustrated by ﬁgure 11.13.
Figure 11.13
If we let E(v, w) denote the set of edges joining the vertices v and w in Γ, then
the edge function φ deﬁnes a bijection E(v, w) →E(θ(v), θ(w)). Therefore,
for all pairs of edges v, w of Γ, |E(v, w)| = |E(θ(v), θ(w))|, i.e. the number of
edges in Γ joining v and w is the same as the number of edges in Σ joining their
corresponding vertices θ(v) and θ(w).
For a simple graph Γ, in order to deﬁne an isomorphism from Γ to Σ, we need only
specify the appropriate vertex bijection θ : VΓ →VΣ. This is because there is at
most one edge joining any pair of vertices, so once θ has been (correctly) deﬁned
there is only one function φ : EΓ →EΣ with the required properties. (For such an
isomorphism to be possible Σ must also be simple—see theorem 11.4(vi) below.)
Since isomorphic graphs have essentially the same structure, any graph-theoretic
property which one has, the other must also have. We list, without proof, some of
these properties in the next theorem.
Theorem 11.4
Let (θ, φ) be an isomorphism from Γ to Σ. Then:
(i)
Γ and Σ have the same number of vertices;
(ii)
Γ and Σ have the same number of edges;
(iii)
Γ and Σ have the same number of components;
(iv)
corresponding vertices have the same degree: for every v ∈VΓ,
deg(v) = deg(θ(v));
(v)
Γ and Σ have the same degree sequence (see exercise 11.1.8);
(vi)
if Γ is simple, then so too is Σ;
(vii)
if Γ is Eulerian, then so too is Σ;
(viii)
if Γ is Hamiltonian, then so too is Σ.

578
Graph Theory
Proof
See exercise 11.3.10.
□
Examples 11.9
1.
For the graphs in ﬁgure 11.12, an isomorphism is deﬁned by the functions
θ : {1, 2, 3, 4} →{a, b, c, d};
1 7→d, 2 7→c, 3 7→a, 4 7→b
and
φ : {e1, . . . , e8} 7→{f1, . . . , f8};
ei 7→fi
for i = 1, . . . , 8.
2.
Consider the graphs K6 and K3,3. Both graphs have six vertices so there
certainly exist bijections between their vertex sets. However, none of the
possible bijections is an isomorphism, since all the vertices of K6, for
example, have degree 5, but all those of K3,3 have degree 3.
3.
Determine which of the graphs represented in ﬁgure 11.14 are
isomorphic. (For convenience we have chosen to label the vertices by
upper-case letters rather than v1, v2, . . . .)
Figure 11.14

Isomorphism of Graphs
579
Solution
Note ﬁrst that each graph is simple, connected and has seven vertices and ten
edges. Furthermore each has degree sequence (2, 2, 3, 3, 3, 3, 4). Theorem 11.2
shows that each graph is non-Eulerian.
Thus the ﬁrst seven properties listed
in theorem 11.4 cannot be used to show that any pair of these graphs is non-
isomorphic.
Using the Hamiltonian property, we can show that neither graph (a) nor (c) is
isomorphic to graph (b) or (d). It can be proved (by considering the vertices of
degree 2) that graphs (a) and (c) are not Hamiltonian; however graphs (b) and
(d) are Hamiltonian (with Hamiltonian cycles AGBCDEFA and ABCDGEFA
respectively).
Since it is difﬁcult in general to show that a graph is not
Hamiltonian, we prefer to follow a different approach as follows.
If we look a bit more closely we can see that (a) and (b) are not isomorphic: in
graph (a) the vertex of degree 4 is adjacent to two vertices of degree 3, but in
graph (b) the vertex of degree 4 is adjacent to four vertices of degree 3. A similar
argument also shows that graphs (a) and (d) are not isomorphic.
Graphs (a) and (c) are isomorphic.
Labelling their vertex sets V1 and V2
respectively, an isomorphism is deﬁned by the following vertex bijection
V1 →V2 : A 7→F, B 7→E, C 7→D, D 7→G, E 7→C, F 7→A, G 7→B.
Graphs (b) and (c) are not isomorphic since the latter is isomorphic to graph (a)
but the former is not. Similarly graphs (d) and (c) are not isomorphic.
It only remains to determine whether or not graphs (b) and (d) are isomorphic.
In fact they are not. We can see this, for instance, by noting that in (d) there is a
vertex of degree 3 (vertex E) which is adjacent to two other degree 3 vertices, but
in (b) every vertex of degree 3 is adjacent to only one other degree 3 vertex.
In summary, graphs (a) and (c) are isomorphic but no other pairs of graphs are
isomorphic.
The examples above illustrate the following general principle. (Compare this with
the case of groups.)

580
Graph Theory
Isomorphism principle
To show that two graphs are isomorphic, an isomorphism from one to the
other must be found; to show that two graphs are not isomorphic, a graph-
theoretic property must be found which one graph has but the other does
not.
Exercises 11.3
1.
Let A and B be adjacency matrices of two isomorphic graphs. How are
A and B related?
2.
Are the following two graphs isomorphic? Justify your answer either by
ﬁnding an isomorphism between them or by showing one has a graph-
theoretic property which the other does not have.
3.
Show that all of the following graphs are isomorphic by deﬁning explicit
isomorphisms between them.

Isomorphism of Graphs
581
4.
(i)
Draw the diagrams of two non-isomorphic simple graphs each of
which has degree sequence (2, 2, 3, 3, 4, 4). Explain why your two
graphs are not isomorphic.
(ii)
Draw the diagrams of two non-isomorphic simple graphs each of
which has degree sequence (2, 2, 2, 4, 4, 5, 5). Explain why your
two graphs are not isomorphic.
5.
Write down the degree sequence of the following disconnected graph Γ.
Draw two non-isomorphic connected graphs with the same degree
sequence as Γ and explain why your two graphs are non-isomorphic.
6.
Three graphs Γ1, Γ2 and Γ3 have diagrams given below.
Show that one and only one pair of the graphs is isomorphic.
7.
Which of the following graphs are isomorphic?
For those which are
isomorphic deﬁne isomorphisms and for those which are not isomorphic

582
Graph Theory
give reasons to explain why they are not isomorphic.
8.
(i)
There are 11 non-isomorphic simple graphs with four vertices.
Draw diagrams to represent these 11 graphs. How many of them
are connected?
(ii)
How many non-isomorphic simple graphs are there with ﬁve
vertices? How many of these are connected?
9.
Show that the two graphs illustrated in ﬁgure 11.3 are isomorphic.
10.
Prove theorem 11.4.
11.4
Trees
In §11.2 we considered paths and cycles in graphs. There is a special class of
connected graphs, called ‘trees’, which have no cycles at all. The ﬁrst use of trees
was by physicist Gustav Kirchhoff in 1847; two years earlier (whilst a student at
the University of K¨onigsberg!) Kirchhoff had formulated the laws governing the
ﬂow of electricity in a network of wires. The network of wires can be considered
as a graph in our sense. The equations which follow from Kirchhoff’s laws, as

Trees
583
they are now called, are not all independent and Kirchhoff used trees to obtain
an independent subset of equations. The term ‘tree’ was coined by the British
mathematician Arthur Cayley† ten years later; Cayley was motivated to study
trees by a problem within mathematics itself.
Trees have become important within graph theory for a number of reasons.
They also feature in many of the applications of graph theory. Cayley himself
provided one of the applications—to the study of isomers in chemistry (see
exercises 11.4.13–11.4.15). More recently, computer scientists have found that
trees provide a convenient structure for storage and retrieval of certain types of
data—using so-called hierarchical databases.
Deﬁnition 11.11
A tree is a connected graph which contains no cycles.
It is immediately apparent from the deﬁnition that a tree has no loops or multiple
edges. Any loop is a cycle by itself, and if edges ei and ej join the same pair of
vertices then the sequence ei,ej is also a cycle. Some examples of trees are given
in ﬁgure 11.15.
Figure 11.15
One reason for the importance of trees in graph theory itself is that every
connected graph contains a tree—called a ‘spanning tree’—which connects all
its vertices. Amongst other things, a spanning tree provides a convenient set of
paths connecting any pair of vertices of the graph.
† Cayley’s interest in trees was motivated by some work of his colleague James Joseph Sylvester on
operators in differential calculus. It was Sylvester who, in 1877, ﬁrst used the term ‘graph’ in the
sense we are using here.

584
Graph Theory
Deﬁnition 11.12
Let Γ be a connected graph with vertex set V . A spanning tree in Γ is a
subgraph which is a tree and has vertex set V .
Theorem 11.5
Every connected graph contains a spanning tree.
Proof
Let Γ be a connected graph; if Γ contains no cycle then there is nothing to prove
as Γ is its own spanning tree.
Suppose, then, Γ contains a cycle. Removing any edge from the cycle gives a
graph which is still connected. If the new graph contains a cycle then again
remove one edge of the cycle. Continue this process until the resulting graph
T contains no cycles. We have not removed any vertices so T has the same vertex
set as Γ, and at each stage of the above process we obtain a connected graph.
Therefore T itself is connected; it is a spanning tree for Γ.
□
Note that a given connected graph Γ will generally have many different spanning
trees. Examples of two spanning trees for the complete graph K6 are given in
ﬁgure 11.16.
Figure 11.16
The simple structure of trees enables us easily to deduce some elementary facts
about them which we give in the following theorem.

Trees
585
Theorem 11.6
Let T be a tree with vertex set V and edge set E. Then:
(i)
for every pair of distinct vertices v and w there is a unique path in
T connecting them;
(ii)
deleting any edge from T produces a graph with two components
each of which is a tree†;
(iii)
|E| = |V | −1.
Furthermore, a connected graph satisfying any one of these properties is a
tree.
Proof
(i)
Let v and w be any two disjoint vertices in T; since T is connected, there
exists a path P1 : e1, e2, . . . , en joining v to w. Suppose that there is
another path P2 : f1, f2, . . . , fm also joining v to w. At some point the
two paths must diverge; let v∗be the last vertex the two paths have in
common before they diverge. Since the two paths both end at w, they
must also converge again; let w∗be the ﬁrst vertex at which P1 and P2
converge—see ﬁgure 11.17. (We need to take w∗to be the ﬁrst vertex
at which they converge because two paths may later diverge once more.)
Deﬁne a path as follows: take those edges of P1 joining v∗to w∗followed
by those edges of P2 (in reverse order) joining w∗to v∗. This path joins
v∗to itself and repeats no edge; it is a cycle in T. This is a contradiction
since T is a tree. Therefore there is a unique path connecting v to w.
Figure 11.17
† A graph which is the union of non-intersecting trees is, not surprisingly, called a forest. A forest
could be deﬁned more simply as any graph with no cycles, so that a connected forest is just a tree!
Clearly theorem 11.5 generalizes to show that every graph has a spanning forest.

586
Graph Theory
(ii)
Let e be any edge in T joining vertices v and w, and let Γ be the graph
obtained by removing e from T. Since e is itself the unique path in T
joining v to w, there is no path in Γ connecting v and w; thus Γ is not
connected.
Let V1 be the set of vertices of Γ which can be joined by a path (in Γ) to v,
and let V2 be the set of vertices of Γ which can be joined by a path to w.
Then V1 ∪V2 = V and V1 and V2 deﬁne two connected subgraphs of Γ.
(Exercise: prove this last statement.) Each of these components of Γ must
be a tree because any cycle in one of them would also be a cycle in T.
(iii)
The proof is by induction on the number of vertices of T and uses part
(ii). It is left as an exercise (11.4.8(i)).
For the last part of the theorem, we will again leave property (iii) as an
exercise, and prove only that if Γ is a connected graph satisfying either
property (i) or property (ii) then Γ is a tree.
Firstly, suppose that Γ is connected and satisﬁes (i). If there is a cycle in Γ
containing a pair of distinct vertices v and w then this cycle provides two
distinct paths connecting v and w. Since this contradicts (i), there is no
such cycle. There can also be no loops (cycles connecting only one ver-
tex) in Γ. If e is a loop at vertex v, and w is any other vertex, then there are
two distinct paths connecting v and w: one path which begins with e and
one which does not. Therefore Γ contains no cycles at all and so is a tree.
Finally suppose that Γ is connected and satisﬁes (ii). If Γ contains a cy-
cle, then we could delete an edge of the cycle without disconnecting Γ,
contradicting (ii). Therefore again Γ must contain no cycles at all and so
is a tree.
□
Exercises 11.4
1.
How many non-isomorphic trees are there with:
(i)
three vertices,
(ii)
four vertices,
(iii)
ﬁve vertices,
(iv)
six vertices?
In each case draw diagrams to represent the non-isomorphic trees.

Trees
587
2.
Prove that a tree with at least one edge is a bipartite graph.
3.
Draw spanning trees for each of the graphs illustrated in ﬁgures 11.3,
11.4, 11.5, 11.9, 11.10, 11.11 and 11.12.
4.
(i)
Draw the diagrams of two graphs with degree sequence
(1, 1, 1, 2, 2, 2, 3), one which is a tree and one which is not a tree.
(ii)
Explain why any graph with degree sequence (1, 1, 2, 2, 2, 3, 3) is
not a tree.
5.
When is Kr,s a tree?
6.
A full binary tree is a tree in which exactly one vertex has degree 2 and
all other vertices have degree 1 or 3. The vertex of degree 2 is called
the root of the tree; vertices of degree 3 are called decision vertices and
vertices of degree 1 are called leaf vertices. Binary trees are frequently
used in computer science; we consider some of the applications of binary
trees in chapter 11. Examples of binary trees are given in the following
diagram.
(i)
Show that every full binary tree has an odd number of vertices.
(Hint: see exercise 11.1.7(ii).)
(ii)
Let T be a full binary tree with n ⩾3 vertices. Prove that there
are always two more leaf vertices than decision vertices.
(iii)
How many non-isomorphic full binary trees are there with (a) ﬁve
vertices, (b) seven vertices and (c) nine vertices?

588
Graph Theory
7.
Let T be a full binary tree with root r. Let v be any vertex of T. Deﬁne
the level of the vertex v to be the length of the unique path in T joining r
and v. Also deﬁne the level of the binary tree T to be the greatest of the
levels of its vertices.
For example, the following full level 3 binary tree has one level 0 vertex
(the root r), two level 1 vertices (a and b), two level 2 vertices (c and d)
and two level 3 vertices (e and f).
(i)
Draw all the level 1 and level 2 full binary trees.
(ii)
Let an denote the number of full level n binary trees. Show that,
for n ⩾2,
an = 2an−1(a0 + a1 + · · · + an−2) + a2
n−1.
(iii)
Hence ﬁnd a3 and a4.
8.
(i)
(Theorem 11.6(iii).) Prove, by induction on the number of edges
of T, that if T is a tree then
|E| = |V | −1.
(ii)
Use theorems 11.5 and 11.6(iii) to show that if Γ is any connected
graph then
|E| ⩾|V | −1.
(iii)
Prove the converse to theorem 11.6(iii) by showing that if Γ is a
connected graph which is not a tree then
|E| > |V | −1.
9.
(i)
Let F be a forest with c components (see footnote, page 585. Write
and prove an equation connecting |V |, |E| and c which generalizes
theorem 11.6(iii).
(ii)
Let Γ be any graph. Write and prove an inequality connecting |V |,
|E| and c which generalizes question 8(ii) above.

Trees
589
10.
How many spanning trees are there in K2,n?
Prove your answer is
correct.
11.
Let Γ be a connected graph and let t(Γ) denote the number of spanning
trees in Γ. Let e be an edge of Γ. Then:
Γ −e denotes the graph obtained from Γ by deleting the edge e;
Γ\e denotes the graph obtained from Γ by ‘contracting’ the edge e;
that is, amalgamating the two vertices that are incident with e. This is
illustrated in the following diagram.
The edge e is a bridge if Γ −e is disconnected. This is illustrated in the
following diagram.
(i)
Prove that, if e is an edge of Γ which is not a bridge, then
t(Γ) = t(Γ −e) + t(Γ\e).
(ii)
What is the corresponding result to that in part (i) in the case where
e is a bridge? Explain your answer.
(iii)
Using the result of part (i), ﬁnd the number of spanning trees in the
following graph.

590
Graph Theory
12.
(i)
How many spanning trees does each of the following graphs have?
(ii)
What general observation can be made about the number of
spanning trees in a connected graph which has a bridge or a cut
vertex? (‘Bridge’ is deﬁned in exercise 11.11 above. A ‘cut vertex’
in a connected graph is a vertex which, if removed from the graph,
together with its incident edges, produces a disconnected graph.)
13.
In chemistry, graphs are used as symbolic representations of molecules.
Each atom is represented as a vertex and each chemical bond is
represented as an edge. For example, molecules of ethene (C2H4) and
ethanol (C2H5OH) can be represented by the following graphs.
The alkane series of saturated hydrocarbons have chemical formula
CnH2n+2. Each carbon atom (C) always has valency (degree) 4 and each
hydrogen atom (H) always has valency (degree) 1.
(i)
Draw the graphs to represent methane (CH4) and ethane (C2H6).
(ii)
Prove that each alkane is represented by a tree.
(Hint:
use
theorem 11.6.)
A structural isomer of a chemical compound is an isomorphism class of
its corresponding graph. This means that molecules of different structural
isomers of the same compound have non-isomorphic graphs.
(iii)
Butane (C4H10) has two structural isomers; draw their graphs.
(iv)
Draw graphs of the different structural isomers of pentane (C5H12)
and hexane (C6H14).
14.
The alkenes and mono-cyclo-alkanes are unsaturated hydrocarbons with
chemical formula CnH2n, where n ⩾2.
(The graph of ethene, the
simplest alkene, is given above.)

Planar Graphs
591
(i)
The compound whose chemical formula is C3H6 has two structural
isomers; draw their graphs. (One of these is propene and the other
is cyclo-propane.)
(ii)
Draw the graphs of the different structural isomers of C4H8.
(Some of these are butenes; others are methyl-cyclo-propane and
cyclo-butane.)
15.
The hydrocarbon C5H8 has many structural isomers. We found 27 (we
think!). How many can you ﬁnd?
(These isomers delight in such names as pentyne, methyl-butadiene,
methyl-cyclo-butene, etc. Several of the isomorphism types do not exist
as chemical compounds because the stresses in the molecular structure
are too great. Of course, the mathematics cannot tell us that—it is the
domain of the chemists.)
11.5
Planar Graphs
Examining the diagrams of graphs earlier in this chapter, it is apparent that some
graphs can be represented by diagrams drawn in the plane (without any edges
crossing) and some cannot.
The object of this section is to examine which
graphs can be represented by plane diagrams. Clearly this question is of potential
importance in some of the applications of graph theory, notably in the design of
electronic circuits which can be printed on boards.
The question of whether a graph can be represented in the plane is illustrated by
the so-called three utilities problem. Three houses are each to be supplied with
three utilities—electricity, gas and water. The problem is whether the three houses
can be supplied without any of the utility lines having to cross. The graph which
models this situation is the complete bipartite graph K3,3. The three vertices of
one set represent the three utility outlets and the three of the second set represent
the three houses. The edges of the graph represent the utility lines; each utility is
connected to each house. In graph-theoretic terms the problem is whether K3,3
can be represented in the plane. Our diagram of K3,3 (ﬁgure 11.5(b) on page 555)
is not drawn in the plane, but if we were more ingenious perhaps we could have
drawn the diagram without the edges crossing.
Intuitively, we say a graph is ‘planar’ if it can be represented by a diagram in the
plane with no edges crossing. The formal deﬁnition is the following.

592
Graph Theory
Deﬁnition 11.13
A graph whose vertices are points in the plane and whose edges are lines or
arcs in the plane which only meet at vertices of the graph is called a plane
graph. (Thus a plane graph is a certain subset of R2.)
A graph is a planar if it is isomorphic to a plane graph, i.e. if it can be
represented by a diagram drawn in the plane with no edges crossing.
A little trial and error should be enough to convince you that K3,3 is not planar. In
the three utilities problem, the utilities must cross somewhere. Perhaps we would
regard a formal proof of this unnecessary, but suppose we were confronted with
a signiﬁcantly more complex graph which we had been unable to represent in the
plane; how could we be certain that we hadn’t overlooked some conﬁguration of
edges and vertices which might show the graph to be planar?
Euler’s Formula
Let Γ be a connected planar graph. A diagram of Γ drawn in the plane (technically,
a plane graph isomorphic to Γ) divides the plane into regions, usually called faces.
Referring to ﬁgure 11.4, we see that K3 divides the plane into two regions—one
bounded and one unbounded—and K4 divides the plane into four regions—three
bounded and one unbounded. It would also appear from ﬁgure 11.4 that K5 is not
planar (although, of course, the diagram does not prove this).
It turns out that there is a simple formula connecting the number of vertices, edges
and faces of a connected planar graph. To investigate this, the following table
provides some evidence which may guide us towards a conjecture.
Graph
Number of vertices
Number of edges
Number of faces
K3
3
3
2
K4
4
6
4
Figure 11.2(a)
7
7
2
Figure 11.5(a)
9
14
7
Figure 11.8(b)
4
7
5
Figure 11.12(b)
4
9
7
Any tree
n
n −1
1

Planar Graphs
593
All of these graphs are connected and planar and satisfy the relationship
|F| = |E| −|V | + 2
where |F|, |E| and |V | are the number of faces, edges and vertices respectively.
This relationship holds for all connected planar graphs, and is known as Euler’s
formula†.
Theorem 11.7 (Euler’s formula)
Let Γ be any connected planar graph with |V | vertices, |E| edges and
dividing the plane into |F| faces or regions. Then
|F| = |E| −|V | + 2.
Proof
The proof is by induction on the number of edges of Γ. If |E| = 0 then |V | = 1
(Γ is connected, so there cannot be two or more vertices) and there is a single face
(consisting of the whole plane except the single vertex), so |F| = 1. The theorem
therefore holds in this case.
Suppose, now, that the theorem holds for all graphs with fewer than n edges. Let
Γ be a connected planar graph with n edges; that is |E| = n. If Γ is a tree, then
|V | = n + 1 (theorem 11.6) and |F| = 1, so the theorem holds in this case too. If
Γ is not a tree choose any cycle in Γ and remove one of its edges. The resulting
graph Γ′ is connected, planar and has n −1 edges, |V | vertices and |F| −1 faces.
By the inductive hypothesis, Euler’s formula holds for Γ′:
|F| −1 = (|E| −1) −|V | + 2
so
|F| = |E| −|V | + 2
as required.
□
† The formula has an interesting history, which is related in the book by Imre Lakatos (1976)
Proofs and Refutations. Lakatos uses an imaginary classroom discussion of Euler’s formula to raise
philosophical questions about the nature of mathematical discovery. The discussion itself is very lucid
and follows, to a large extent, the actual historical development of the formula.

594
Graph Theory
Examples 11.10
1.
We can deduce from Euler’s formula that K3,3 is not planar. Suppose
that K3,3 is planar. Then it divides the plane into faces, the boundary of
each face being a cycle‡. Every edge in a cycle of K3,3 forms part of the
boundary of two faces. Thus the sum of the numbers of edges belonging
to the boundaries of all the faces is 2|E|. In K3,3 it is easy to see that
every cycle contains at least four edges, so every face must have at least
four edges in its boundary. Since every edge belongs to some cycle,
2|E| ⩾4|F|.
Substituting for |F| in Euler’s formula gives
2|E| ⩾4(|E| −|V | + 2).
From ﬁgure 11.5(b) we see that |V | = 6 and |E| = 9 so this last
inequality becomes
18 ⩾4 × (9 −6 + 2) = 20
which is a contradiction. Therefore K3,3 is not planar.
2.
A similar argument to the above can be used to show that K5 is not planar.
(See ﬁgure 11.4.)
Kuratowski’s Theorem
The two examples above of non-planar graphs—K3,3 and K5—represent, in a
sense, the only ways in which a graph can fail to be planar. Clearly if a graph Γ
contains a subgraph isomorphic to either K3,3 or K5 then Γ must be non-planar.
The converse is almost true, except that we need to replace ‘isomorphic’ by a
slightly weaker relationship.
To understand why we need a slightly different relationship than isomorphism,
imagine the graph obtained from K5, say, by dividing one of its edges in two
‡ For any (ﬁnite) planar graph, one of these faces is unbounded. It is perhaps less clear what we
mean by the boundary of the unbounded face. An example should clarify this point. The cycle graph
in ﬁgure 11.2(a) divides the plane into a bounded and an unbounded face which share a common
boundary cycle.

Planar Graphs
595
by adding a vertex of degree 2 in the middle of the edge. The resulting graph is
clearly also non-planar, but it is not isomorphic to K5 (it has one more vertex and
one more edge, for example). We say the resulting graph is ‘homeomorphic’ to
K5.
Deﬁnition 11.14
Two graphs are homeomorphic if (an isomorphic copy of) one graph can
be obtained from the other by adding and/or deleting vertices of degree 2
into or from its edges.
Example 11.11
All of the graphs shown in ﬁgure 11.18 are homeomorphic. To obtain the graph
(b) from (a) we delete two vertices, and to obtain (c) from (b) delete one vertex
and add two. From (c) to (d) we add one vertex and delete another and from (d)
to (e) we add one vertex. The graphs (e) and (f) are isomorphic—no vertices need
be added or deleted.
Figure 11.18
There is an alternative, perhaps simpler, way of determining whether two graphs
Γ and Σ are homeomorphic. Successively delete vertices of degree 2 from the

596
Graph Theory
edges of both graphs until no further deletions can be performed. Let Γ′ and Σ′
respectively denote the resulting graphs. Then Γ and Σ are homeomorphic if and
only if Γ′ and Σ′ are isomorphic.
Successively deleting vertices of degree 2 from each of the graphs in ﬁgure 11.18
produces the graph shown in ﬁgure 11.19.
This conﬁrms that the graphs in
ﬁgure 11.18 are all homeomorphic.
Figure 11.19
We can now explain more precisely what we mean by graphs K5 and K3,3
being essentially the only ways a graph can fail to be planar. The following
theorem, ﬁrst proved by the polish mathematician Kuratowski in 1930, provides
an elegant characterization of planar graphs. Unfortunately the proof is too long
and complicated to include here—see, for example, Gould (1988) or Harary
(1969) for a proof.
Theorem 11.8 (Kuratowski’s theorem)
A graph is planar if and only if it contains no subgraph homeomorphic to
K5 or K3,3.
Exercises 11.5
1.
Using an argument similar to that given for K3,3 in example 11.10.1,
prove that the complete graph K5 is not planar.
2.
(i)
Using Kuratowski’s theorem, or otherwise, show that Kn is planar
for n ⩽4 and non-planar for n ⩾5.
(ii)
Show that K1,n and K2,n are planar for all n ⩾1.

Planar Graphs
597
3.
Show that one of the two graphs in the following diagram is planar and
the other is not.
4.
(i)
Show that the following graph is planar.
(ii)
Determine whether or not each of the following graphs is planar.
(a)
(b)
5.
Use Kuratowski’s theorem to show that each of the following graphs is
not planar.
(i)
(ii)

598
Graph Theory
6.
Prove that, for n ⩾1, the cycle graph Cn is homeomorphic to K3. (Cycle
graphs are deﬁned on page 551.) Deduce that any two cycle graphs are
homeomorphic.
7.
Show that, for each of the pairs of graphs illustrated in the following
diagram, the graphs are homeomorphic.
8.
(i)
Use Euler’s formula to show that, for a connected simple planar
graph, 3|F| ⩽2|E|. Deduce that |E| ⩽3|V | −6. (Hint: use an
argument similar to the one in example 11.10.1.)
(ii)
Show that every connected simple planar graph has at least one
vertex of degree less than or equal to ﬁve. (Hint: use proof by
contradiction.)
9.
The dual Γ∗of a plane graph Γ is also a plane graph obtained ﬁrstly by
placing a vertex of Γ∗in the interior of each face of Γ. Every edge in Γ
separating two faces gives rise to an edge in Γ∗joining the corresponding
vertices of Γ∗. This is illustrated in the following diagram.

Planar Graphs
599
(i)
Draw the dual graph of each of the following plane graphs.
(a)
(b)
(c)
(ii)
How do the numbers of vertices, edges and faces of Γ∗relate to
the numbers of vertices, edges and faces of Γ?
10.
A connected, simple graph Γ has vertex degree sequence (2, 2, 2, 3, 3,
4, 5, 5).
(i)
Show that Γ must be planar.
(ii)
Find the numbers of edges and faces of Γ.
(iii)
Write down the numbers of vertices, edges and faces of Γ∗, the
dual of Γ.
11.
Graphs on other surfaces. Intuitively, a planar graph is one which can
be drawn in the plane. We can also consider graphs which can be drawn
(without intersecting edges) on other surfaces such as the sphere or torus
(‘ring doughnut’) as in the following diagram.

600
Graph Theory
Euler’s formula says that |V | −|E| + |F| = 2 for any graph drawn in the
plane. In fact, the value of |V | −|E| + |F| depends only on the surface
on which the graph can be drawn, and not on the graph itself.
(i)
Draw several graphs on the surface of a sphere and calculate
|V | −|E| + |F| for each.
(ii)
Repeat for graphs drawn on the surface of a torus.
12.
Show that each of the graphs K5 and K3,3 can be drawn on the surface
of the torus.
11.6
Directed Graphs
The graphs we have considered so far have all been undirected graphs, which
means that the edges have no preferred direction. Using the analogy of a road
map, all our roads have been two-way streets. In some applications, however, we
require graphs where the edges do have a speciﬁed direction; on a diagram this
is indicated by an arrow on each edge. We have already seen examples of the
directed graph of a relation (on a ﬁnite set) in chapter 4.
There is a simple way of modifying the deﬁnition of an undirected graph to give
each edge a direction. In deﬁnition 11.1 the ‘endpoints’ of an edge e are deﬁned
to be a set of vertices δ(e) = {v, w}. A directed edge can be thought of as having
an ‘initial’ and a ‘ﬁnal’ vertex, rather than two endpoint vertices of equal status.
This can be made precise by deﬁning δ(e) = (v, w), the ordered pair of vertices.
The ordering of the vertices then determines the direction of the edge.

Directed Graphs
601
Deﬁnition 11.15
A directed graph, or digraph, D consists of a ﬁnite non-empty set
V = VD of vertices, a ﬁnite set E = ED of (directed) edges and a
mapping δ : E →V × V . If δ(e) = (v, w) then v is called the initial
vertex and w the ﬁnal vertex of the edge e.
Given a directed graph D, we can forget about the direction of the edges and
obtain an undirected graph Γ, called the underlying graph of D. Formally the
underlying graph of D has the same vertex and edge sets as D, with mapping δΓ
deﬁned by δΓ(e) = {v, w} if δD(e) = (v, w).
A digraph is simple if it has no loops (edges e with δ(e) = (v, v)) and multiple
edges (edges with the same initial and ﬁnal vertices). We should point out that the
underlying graph of a simple digraph need not be a simple graph. Figure 11.20
shows a simple digraph and its underlying graph. The two edges of the digraph
connecting the vertices v4 and v5 are not multiple edges because they have
different directions—that is, different initial and ﬁnal vertices. In the underlying
graph, however, they are multiple edges.
Figure 11.20
Many of the deﬁnitions for undirected graphs carry over to directed graphs either
unchanged or with obvious modiﬁcations to take account of the directions of the
edges. In particular, the deﬁnitions of adjacent, incident and degree remain
unaltered. Thus, for instance, two vertices v and w of a digraph are adjacent if
and only if there exists a directed edge either from v to w or from w to v. However,
we do modify the deﬁnition of the adjacency matrix of a digraph to take account
of the directions of the edges. The (i, j)-entry of the adjacency matrix represents
the number of edges with initial vertex vi and ﬁnal vertex vj, so the matrix need
not be symmetric. The adjacency matrix for the digraph in ﬁgure 11.20 is the

602
Graph Theory
following, where we have given the vertex set the obvious ordering:










0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
1
0
0
0
1
0
1
0
0
1
0
0
1
1
0
0
0
0
0
0
1
0
0
0










.
For a digraph, the degree of a vertex is the sum of the entries in the column
and the row corresponding to it in the adjacency matrix. The entries in the row
corresponding to vertex v represent those edges with v as initial vertex and the
entries in the column corresponding to v represent the edges with v as ﬁnal vertex.
(Note that the diagonal elements are automatically counted twice, once when
summing a row and once when summing a column. Thus, unlike in the undirected
case, we do not need to double the diagonal entries to obtain the correct degree.)
The sum of all the entries in the adjacency matrix is, of course, the total number
of edges in the digraph.
A directed edge sequence from v0 to vn in a digraph is a sequence of edges
e1, e2, . . . , en such that δ(ei) = (vi−1, vi) for i = 1, . . . , n. The deﬁnitions of
directed path and directed cycle are the obvious modiﬁcations of the undirected
deﬁnitions. We can deﬁne connectivity for a digraph in two different (and non-
equivalent) ways.
Deﬁnition 11.16
A digraph is connected (or weakly connected) if its underlying undirected
graph is connected. A digraph is strongly connected if, for every ordered
pair of vertices (v, w), there is a directed path from v to w.
Examples 11.12
1.
Figure 11.21 shows two digraphs, both of which are (weakly) connected;
they have the same underlying connected graphs. The graph (a) is not
strongly connected because, for example, there is no directed path in (a)
from v to w: less formally, we cannot travel from vertex v to vertex w
along edges in the direction of the arrows. The graph (b) is strongly

Directed Graphs
603
Figure 11.21
connected, however, because we can journey from any vertex to any other
by travelling the ‘correct’ way along edges.
2.
A simple situation which can be conveniently modelled using a digraph
is a ‘round-robin’ tournament.
This is a tournament in which every
competitor plays every other and every match has a winner. It can be
modelled by a digraph with vertices representing the players, and with
an edge with initial vertex v and ﬁnal vertex w if and only if player v
beats player w. The underlying graph of this digraph is complete because
every competitor plays every other. Any digraph whose underlying graph
is complete is called a tournament.
There are notions of Eulerian digraph and Hamiltonian digraph deﬁned in the
obvious way; an Eulerian digraph has a closed directed path containing every edge
and a Hamiltonian digraph has a directed cycle passing through every vertex.
The theorem corresponding to Euler’s theorem (theorem 11.2) for digraphs is an
easy modiﬁcation of Euler’s theorem for graphs. We deﬁne the in-degree of a
vertex v to be the number of edges with ﬁnal vertex v, and the out-degree of v
to be the number of edges with initial vertex v. Thus the in- and out-degrees of
a vertex are the number of edges ‘going in to’ and ‘coming out from’ the vertex
respectively. The proof of the following theorem is a directed version of the proof
of Euler’s theorem and is left as an exercise (11.6.10).
Theorem 11.9
A connected digraph is Eulerian if and only if the in-degree equals the out-
degree for every vertex.

604
Graph Theory
As we might expect from the undirected case, the situation for Hamiltonian
digraphs is more complicated.
However, for tournaments we can prove the
following result.
Theorem 11.10
(i)
Every tournament has a directed path containing all the vertices.
(In other words every tournament is semi-Hamiltonian; see
exercise 11.2.10 for the deﬁnition of semi-Hamiltonian for
undirected graphs.)
(ii)
Every strongly connected tournament is Hamiltonian.
Proof
(i)
We show that a directed path which does not pass through every vertex
can be extended to pass through another vertex. The result then follows
because we can begin with any directed path (e.g. containing a single
edge) and continue extending it until it passes through every vertex.
Let P : e1, e2, . . . , en be any directed path, such that δ(ei) = (vi−1, vi)
for i = 1, 2, . . . , n. Suppose P does not pass through every vertex; let v
be any vertex through which P does not pass. Since the underlying graph
is complete, there is either an edge from v to v0 or from v0 to v. In the
former case adding this edge at the beginning of P extends it to a path
passing through v.
In the latter case let r be the largest integer such that there are edges from
v0, v1, . . . , vr to v. If r < n then there are edges from vr to v and from v
to vr+1; we can insert these edges into P between er and er+2. If r = n
we can add the edge from vn to v at the end of P. In either case we have
extended P to pass through v.
(ii)
We might as well also suppose that |V | ⩾2—if D has only a single
vertex, it must be a null graph, so the empty directed cycle passes through
the vertex.
For a strongly connected tournament D, the above argument can be
modiﬁed to show that a directed cycle not passing through every vertex

Directed Graphs
605
can be extended to pass through an additional vertex. The result will then
follow (as for part (i)) if we can ﬁnd a directed cycle of any length at all;
below we show that D must have a directed cycle of length 3.
Choose any vertex v and let V1 be the set of all vertices w such that there
is an edge e with δ(e) = (v, w). Similarly let V2 be the set of all vertices
w such that there is an edge e with δ(e) = (w, v). Since the tournament
is strongly connected it follows that V1 and V2 are both non-empty. For
example, if V1 = ∅then v has out-valency 0, so there is no directed path
from v to any other vertex of D, which contradicts the assumption that D
is strongly connected. A similar argument shows that V2 is non-empty.
Now there must be at least one directed edge with initial vertex belonging
to V1 and ﬁnal vertex belonging to V2, otherwise it would not be possible
to join any vertex of V1 to any vertex of V2, which again contradicts the
strong connectivity of D. Choose an edge e with initial vertex v1 ∈V1
and ﬁnal vertex v2 ∈V2. By the deﬁnitions of V1 and V2 there exist edges
e1 from v to v1 and e2 from v2 to v. The edge sequence e1, e, e2 is a
directed cycle of length 3 in D; the result now follows from the existence
of this cycle, as we explained above.
□
Exercises 11.6
1.
How is the adjacency matrix of a digraph related to the adjacency matrix
of its underlying graph?
2.
Show that, for the edges of any digraph,
sum of in-degrees = sum of out-degrees = |E|.
3.
(i)
Explain why a directed graph whose underlying graph is a tree
cannot be strongly connected.
(ii)
Is it possible for a strongly connected directed graph to have an
underlying graph which is simple? Justify your answer.
4.
For each of the following matrices draw a diagram to represent a digraph
which has the given matrix as its adjacency matrix:

606
Graph Theory
(i)




0
1
1
1
0
1
0
0
0
1
0
1
0
0
1
0




(ii)






0
1
1
0
0
0
1
0
0
0
0
0
2
0
0
1
0
1
0
2
1
0
0
0
0






.
5.
For each of the digraphs represented in the following diagram, determine
whether the digraph:
(a)
is simple;
(b)
has a simple underlying graph;
(c)
is strongly connected;
(d)
is Eulerian;
(e)
is Hamiltonian.
6.
Let R be a relation on a set A and let D be the directed graph of R—see
chapter 4. Explain why the adjacency matrix of D is equal to the binary
matrix of R.
7.
A directed graph is said to be unilaterally connected if, for every pair of
vertices v and w, either there is a directed path from v to w or there is a
directed graph from w to v.

Directed Graphs
607
(i)
Clearly: strongly connected ⇒unilaterally connected ⇒weakly
connected. Show that none of these implications reverse.
(ii)
Which of the digraphs in question 5 above are unilaterally
connected?
8.
A graph is orientable if it is the underlying graph of some strongly
connected digraph. Show that Petersen’s graph (example 11.1.2), K5 and
K3,3 are all orientable.
9.
Let D be a digraph with vertex set V = {v1, v2, . . . , vn} and edge set
E = {e1, e2, . . . , em}.
The incidence matrix of D is the m × n matrix B = (bij) where
bij =





1
if edge i has initial vertex vj
−1
if edge i has ﬁnal vertex vj
0
otherwise.
For example, the digraph with diagram
has incidence matrix










1
−1
0
0
1
−1
0
0
0
−1
1
0
0
0
−1
1
0
0
1
−1
1
0
0
−1
1
0
−1
0










.
Note: the incidence matrix, as deﬁned here, applies only to digraphs
without loops.

608
Graph Theory
(i)
Write down the incidence matrix of the following digraph. (You
will need to label the edges.)
(ii)
What information, if any, about a digraph D is provided by the row
and column sums of its adjacency matrix?
(iii)
What information, if any, about a digraph D is provided by the row
and column sums of its incidence matrix?
(iv)
Draw a diagram of the digraph which has incidence matrix












1
−1
0
0
0
−1
1
0
0
0
1
0
0
0
−1
−1
0
0
1
0
0
−1
0
0
1
0
1
0
−1
0
0
0
0
−1
1
0
0
0
1
−1












.
(v)
How is the adjacency matrix of a digraph related to the adjacency
matrix of its underlying graph?
(vi)
How is the incidence matrix of a digraph related to the incidence
matrix of its underlying graph?
10.
Prove theorem 11.9.
11.
Deﬁne the term isomorphism for directed graphs.
If two digraphs are isomorphic, does it necessarily follow that their
underlying undirected graphs are isomorphic?
Conversely, if the underlying graphs of two digraphs are isomorphic, does
it necessarily follow that the digraphs are themselves isomorphic?

Directed Graphs
609
12.
The diagrams of three digraphs D1, D2 and D3 are given below.
(i)
Show that no pair of the digraphs is isomorphic.
(ii)
Prove that two of the three digraphs have underlying graphs which
are isomorphic.
13.
Which of the following digraphs are isomorphic? Justify your answers.
14.
The converse ˜D of a digraph D is the digraph obtained by reversing the
direction of every edge of D.
(i)
Give an example of a digraph which is isomorphic to its converse
and give an example of a digraph which is not isomorphic to its
converse.
(ii)
What is the connection between the adjacency matrices of D and
˜D?

610
Graph Theory
(iii)
What is the connection between the incidence matrices of D and
˜D?
15.
Let (G, ∗) be a ﬁnite group and S a subset of G. Deﬁne a directed graph
D of the pair (G, S) as follows. The vertex set of D is G, and there exists
a directed edge from g1 to g2 if and only if g2 = g1s for some s ∈S.
Draw the directed graph of (G, S) for each of the following.
(i)
G = {e, x, x2, x3}, the cyclic group of order 4.
S = {x}.
(ii)
G = D3 = {r0, r1, r2, m1, m2, m3}, the dihedral group of de-
gree 3.
S = {r1, m1}.

Chapter 12
Applications of Graph Theory
12.1
Introduction
In chapter 11 we claimed that graph theory has many applications. Our aim
in this chapter is to explain brieﬂy a few of these. Of necessity we can only
outline a handful of the many applications of the theory. The interested reader is
referred to one of the more specialized texts for more comprehensive treatments
and additional uses of graph theory.
Broadly, our applications fall into two
categories—those in computing and those in a branch of applied mathematics
known as combinatorial optimization. However, the distinctions between the two
are not entirely clear cut.
Two particular classes of graphs, each of which has some additional structure,
ﬁgure prominently in this chapter; they are ‘rooted trees’ and ‘weighted graphs’.
We shall need to devote some space to the theoretical aspects of these, so this
chapter should not be seen as wholly concerned with applying graph theory.
Rooted trees, which occupy the ﬁrst part of the chapter, are used extensively
in computing. Our main applications of rooted trees are in the representation
and sorting of data. Weighted graphs feature in the second half of the chapter;
they are widely used in certain kinds of optimization problems. As the name
suggests, optimization is concerned with ﬁnding the optimum or ‘best’ solution to
a problem. The exact meaning of‘best’ depends on the particular situation under
consideration; it may mean longest, shortest, greatest, least, etc. You might be
familiar with the use of calculus for ﬁnding maxima and minima of functions—
this is a branch of optimization. The problems we consider are of a discrete, rather
than continuous, nature, so the ‘tools’ required tend to be different.
611

612
Applications of Graph Theory
Before explaining our applications in more detail, perhaps we should mention
brieﬂy what is probably the most famous, if not necessarily the most signiﬁcant,
application of graph theory: the four-colour conjecture. In about 1852, Francis
Guthrie† pointed out that it appeared that the countries of any map drawn in the
plane (or, equivalently, on the surface of a sphere) could be coloured in such a
way that countries sharing a common border are coloured differently using only
four colours. A map in the plane can be modelled by a (planar) graph whose
vertices represent the countries and whose edges represent the common borders.
At ﬁrst sight this is somewhat counter-intuitive; the graph corresponding to a
particular map looks rather different from the map itself. Some thought should be
sufﬁcient to appreciate that this representation captures the essential properties of
the original map. Assigning colours to the countries corresponds to colouring the
vertices of the graph in such a way that no pair of adjacent vertices are given the
same colour.
Over the years, various ‘proofs’ of the conjecture were given, but each was
subsequently found to be ﬂawed. (‘Flawed’ does not mean ‘worthless’, however,
as many interesting ideas and techniques were developed in several of the
unsuccessful attempts at a proof.)
Eventually in 1976, Kenneth Appel and
Wolfgang Haken completed a proof of the four-colour theorem as it then
became known. However, their announcement of the proof caused considerable
controversy in the mathematical community because, for the ﬁrst time in
mathematics, they had used a computer in an essential way to check the many
hundreds of possible conﬁgurations to which the problem had been reduced.
The sheer size of the number of computations required made the use of many
hours of mainframe computer time essential. Even today, many mathematicians
are reluctant to accept Appel–Haken proof as ‘genuine’ because it can only
be checked by another computer—to work through the details ‘by hand’ is
impossible in practice.
12.2
Rooted Trees
Many of the applications of graph theory, particularly in computing, use a certain
kind of tree, called a ‘rooted tree’. This is simply a tree where a particular vertex
has been distinguished or singled out from the rest. These are the trees used
to show the relationships between a person’s descendants—the familiar ‘family
† At the time Francis Guthrie was a student at the university of London, Francis’s brother Frederick
drew the problem to the attention of De Morgan who subsequently mentioned it in his lectures and
once in print (in a book review). However, the problem did not become so widely known until Cayley
introduced it to a meeting of the London Mathematical Society in 1878.

Rooted Trees
613
tree’.
Figure 12.1 is a typical family tree showing the descendants of great-
grandmother Mary who would be the distinguished vertex in this case.
Figure 12.1
Rooted trees are perhaps most familiar in computing as models for the structure
of ﬁle directories. Figure 12.2 shows part of a typical multi-user ﬁle directory,
organized as a rooted tree. Directories are organized in this way for two main
reasons: so that related ﬁles can be grouped conveniently together and, in multi-
user systems, to protect the security of the users’ ﬁles. Each user would usually
have a password which would be required to gain access to his or her ﬁles.
Figure 12.2
Some of the other important uses of rooted trees in computing include the
representation of data and the representation of algebraic expressions (see
exercises 12.1.10–12.1.14).
The rooted tree data structure is particularly
appropriate for data where there are hierarchical relationships among the data sets.
Data represented as a rooted tree allow various subclasses of data to be accessed
readily, often with less processing than some other hierarchical data structures
such as linked lists. The detailed study of data structures and tree representations
of algebraic expressions is beyond the scope of this book, so the interested reader
should consult one of the more specialized texts for further details.

614
Applications of Graph Theory
Deﬁnitions 12.1
A rooted tree is a pair (T, v∗) where T is a tree and v∗∈VT .
The
distinguished vertex v∗is called the root of the tree.
A leaf in a rooted tree is a vertex which has degree 1 which is not equal to
the root; a decision vertex (or internal vertex) is a vertex which is neither
the root nor a leaf.
The name ‘decision vertex’ comes from so-called ‘decision trees’. These are
rooted trees which are used to model multi-stage decision processes where the
decision made at one stage affects the possible decisions available at the next
stage. The decision vertices represent the points at which decisions need to be
made. Sometimes it is convenient to use slightly less formal terminology and
refer to a ‘rooted tree T with root v∗’ rather than having always to use the ordered
pair notation (T, v∗) for a rooted tree.
Unlike in nature, it is usual to draw the diagram of a rooted tree so that it ‘grows
downwards’ with the root at the top of the diagram. Figure 12.3(a) shows a tree T
with root v∗. (The choice of the root in this example is arbitrary; we could equally
well have chosen a different vertex as the root.) The diagram of T is redrawn in
ﬁgure 12.3(b) growing downwards with the root at the top. The leaf vertices are
a, b, d, e, h, j, m, n, q and s, and the decision vertices are c, f, g, k, p and r.
Figure 12.3(b) suggests that we can partition the vertex set VT into sets of vertices
at different ‘levels’ according to how far they are from the root as follows:
Level 0:
{v∗}
Level 1:
{g, k, p}
Level 2:
{c, f, h, m, n, r}
Level 3:
{a, b, d, e, j, q, s}.
The formal deﬁnition of the level of a vertex is the following which relies on the
fact (theorem 11.6(i)) that there is a unique path in the tree joining the root to any
given vertex.

Rooted Trees
615
Figure 12.3
Deﬁnition 12.2
Let (T, v∗) be a rooted tree. The level of a vertex w of T is the length of
the (unique) path in T from v∗to w. The height of T is the maximum of
the levels of its vertices.
Let T be a rooted tree and let p be a vertex of level k ̸= 0. (This is just another
way of saying that p ̸= v∗.) Since the path in T from v∗to p is unique, it follows
that p is adjacent to a unique vertex of level k −1. The terminology of the next
deﬁnition is motivated by the example of a family tree.

616
Applications of Graph Theory
Deﬁnition 12.3
Let (T, v∗) be a rooted tree and let p be a vertex of level k > 0. The
(unique) vertex q of level k −1 which is adjacent to p is called the parent
of p. Similarly, p is the child of q, and any vertex of level k which is also
adjacent to q is called a sibling of p.
It is clearly possible to deﬁne further terms such as grandparent, grandchild,
ancestor, descendant, etc (see exercise 12.1.4). Also it is sometimes useful to
use the term parent vertex to refer to a vertex which has children, i.e. a vertex
which is either the root or a decision vertex (except in the case where the tree has
no edges at all so that the root is childless).
Examples 12.1
1.
The rooted tree shown in ﬁgure 12.3 has height 3. Also, g is the parent of
c, f and h, r is the parent of q and s, and so on. Similarly b, e and j are
siblings, a, b, d, e and j are all grandchildren of g, etc.
2.
Figure 12.4 represents part of the (line) organizational structure of a large
company as a rooted tree whose root is the Managing Director. (In the
full organizational structure, of course, each of our leaf vertices would
probably have children.)
The ﬁve level 1 directors are siblings; indeed it is clearly always the case
that the level 1 vertices of a rooted tree are siblings since they are all
children of the root. Of the level 1 directors, the Production Director
has the most descendants—three children and three grandchildren. The
Research and Personnel Directors are childless and therefore have no
descendants. Stretching the family relation deﬁnitions still further, we
could say that any pair of the level 2 managers are either siblings or
‘cousins’. Again this is the case for any rooted tree.

Rooted Trees
617
Figure 12.4

618
Applications of Graph Theory
Deﬁnitions 12.4
(i)
An m-ary tree is a rooted tree in which every parent vertex has at
most m children and some parent vertex has exactly m children.
(The terms binary and ternary tree are used when m = 2 and
m = 3 respectively.)
If every parent vertex of an m-ary tree has exactly m children, we
say the tree is full.
(ii)
A rooted tree of height h is complete if all the leaf vertices are at
level h.
There are several points to note about these deﬁnitions. The ﬁrst is that every
(ﬁnite) rooted tree is an m-ary tree for some m—we can simply take m to be
the largest number of children of all the vertices of the tree. Some authors deﬁne
an m-ary tree to be what we called a full m-ary tree; that is, one in which every
parent has exactly m children. However, for the applications which we consider
below, our terminology is more convenient. Note also that a complete rooted tree
is not a complete graph (see deﬁnition 11.3). Indeed any tree with more than two
vertices is not a complete graph. It is a little unfortunate that the word ‘complete’
is given two different meanings in graph theory, but the context should make it
clear which is intended.
Examples 12.2
1.
In ﬁgure 12.5, diagram (a) is a full binary tree and (b) is a full ternary
tree. Neither tree is complete—(a) and (b) have leaf vertices at all levels
greater than one and zero respectively.
2.
Figure 12.6 shows two complete rooted trees—in both cases all the leaf
vertices occur at the bottom of the diagram at the highest level of the tree.
Clearly this will always be the case in the usual diagram of a complete
rooted tree. The tree (a) is not full; the tree (b) is a complete full binary
tree.

Rooted Trees
619
Figure 12.5
Figure 12.6
Binary Trees
Binary trees will be used in the sorting procedures outlined in the next section. We
therefore need to consider these in a little more detail now. It will be convenient
to stretch deﬁnition 12.4(i) somewhat and call the tree with no edges a binary tree.
Strictly, deﬁnition 12.4(i) labels this a 0-ary tree. We could modify the deﬁnition
to include the tree with no edges as a binary tree, but it would then be somewhat
clumsy. We prefer just to adopt the convention that this tree is included in the set
of binary trees.
Let T be full binary tree of height greater than zero and with root v∗. Deleting
v∗and its two incident edges produces two disjoint binary trees (a binary forest?)
whose roots are the two level 1 vertices of T. These are called the left subtree
and right subtree of the root v∗. The roots of these subtrees are called the left
child and right child of v∗and the edges which were deleted are called the left
branch and right branch of v∗respectively. For each of these subtrees, if the
subtree has height 1 at least, its left and right subtrees can be deﬁned, and so on

620
Applications of Graph Theory
throughout the tree. If T is not a full binary tree, a left or right subtree may be
empty.
There is, of course, a choice to be made in which subtree to call the left subtree
and which to call the right subtree at each stage. (In the applications which we
consider, the choices will be made in a systematic manner.) Having made these
choices we shall always draw the diagram of the tree in the obvious way with the
left subtree of each vertex to the left of its corresponding right subtree. The set
of choices as to which subtree is ‘left’ and which is ‘right’ at each stage actually
introduces additional structure to the tree because the choices give the tree a ‘left
half’ and a ‘right half’ rather than just two halves.
These ideas can be put to use giving the following recursive deﬁnition of a binary
tree which is rather different from the previous one, but is essentially equivalent
to it.
Deﬁnitions 12.4′
A binary tree comprises a triple of sets (L, S, R) where L and R are binary
trees (or are empty) and S is a singleton set. The single element of S is the
root, and L and R are called, respectively, the left and right subtrees of
the root.
This deﬁnition is recursive because it deﬁnes a binary tree in terms of the
‘components’ L, S and R, two of which are themselves binary trees. Thus L and
R, if non-empty, are both deﬁned as triples of the form (L′, S′, R′) and so on. This
way of deﬁning binary trees is extremely useful for their computer representation.
The following example illustrates how to unravel the recursive deﬁnition of binary
tree to obtain the usual diagram.
Example 12.3
Consider the recursively deﬁned binary tree (L, {v∗}, R) where
L = (L1, {v1}, R1)
and
R = (L2, {v2}, R2)
L1 = (L3, {v3}, R3)
and
R1 = (∅, {v4}, R4)
L2 = (L5, {v5}, R5)
and
R2 = (∅, {v6}, ∅)
L3 = (∅, {v7}, ∅)
and
R3 = (L8, {v8}, R8)
R4 = (∅, {v9}, ∅)

Rooted Trees
621
L5 = (∅, {v10}, ∅)
and
R5 = (∅, {v11}, ∅)
L8 = (∅, {v12}, ∅)
and
R8 = (∅, {v13}, ∅).
The triples of sets deﬁning the tree have been grouped into the levels of their
roots. Figure 12.7 shows the usual diagram of this binary tree. To draw the
diagram, simply work through the triples above systematically. Begin by drawing
the root v∗.
Its left and right subtrees have roots v1 and v2 respectively, so
these are the next vertices to add to the diagram. Each of these vertices is then
considered in turn.
Figure 12.7
The left subtree of v1 is L1, with root v3, and the right subtree is R1, with root
v4, so these two vertices can then be added to the picture. Continuing in this way
produces the ﬁgure shown. Note that the binary tree is not full because the vertex
v4 has only one child. Neither is it complete since there are leaf vertices at levels
2 and 3.
Exercises 12.1
1.
For each of the following rooted trees, redraw the diagram with the root
at the top and the tree growing downwards. Determine:
(a)
the height of the tree;
(b)
whether the tree is complete;
(c)
whether the tree is a full m-ary tree for some m.

622
Applications of Graph Theory
2.
Label the vertices of each of the rooted trees in question 1 above. For
each tree:
(i)
list the grandchildren of the root;
(ii)
arrange the great-grandchildren of the root into sets of siblings.
3.
Let T be a rooted tree with root v∗. Deﬁne a relation R on VT , the set of
vertices of T, by:
p R q if and only if the lengths of the paths from
v∗to p and from v∗to q are equal.
Show that R is an equivalence relation on VT and that the equivalence
classes are the sets of vertices with equal levels.
4.
Let (T, v∗) be a rooted tree.
Give formal deﬁnitions of the terms
descendant and ancestor applied to vertices. (See deﬁnition 12.3.)

Rooted Trees
623
Deﬁne a relation R on the set VT of vertices T by:
v R w if and only if either v = w or v is an ancestor of w.
Show that R is a partial order relation on VT .
5.
Let (T, v∗) and (S, w∗) be rooted trees. A rooted isomorphism from
(T, v∗) to (S, w∗) is an isomorphism of the trees (θ, φ) : T →S such
that θ(v∗) = w∗. In other words, a rooted isomorphism of rooted trees
is an isomorphism of trees which maps the root of one tree to the root of
the other. (See §11.3 for the deﬁnition of isomorphism for graphs.)
(i)
There are four non-isomorphic rooted trees with four vertices.
Draw a diagram of each of these.
(ii)
There are nine non-isomorphic rooted trees with ﬁve vertices.
Draw a diagram of each of these.
(Compare this with the number of unrooted trees with four and ﬁve
vertices—exercise 11.4.1—and the number of full binary trees with ﬁve
vertices—exercise 11.4.6.)
6.
(i)
Give a recursive deﬁnition of the binary tree which is represented
by the diagram in ﬁgure 12.5(a).
(ii)
Draw a diagram of the recursively deﬁned binary tree
T = (L, {v∗}, R)
where:
L = (L1, {v1}, R1)
R = (∅, {v2}, R2)
L1 = (L3, {v3}, ∅)
R1 = (∅, {v4}, R4)
R2 = (L5, {v5}, ∅)
L3 = (∅, {v6}, ∅)
R4 = (L7, {v7}, ∅)
L5 = (∅, {v8}, R8)
L7 = (L9, {v9}, R9)
R8 = (∅, {v10}, ∅)
L9 = (∅, {v11}, ∅)
R9 = (∅, {v12}, ∅).
7.
Let T = (L, {v∗}, R) be a recursively deﬁned binary tree.
Give a
(recursive) deﬁnition of the level of a vertex of T.

624
Applications of Graph Theory
8.
Let R be a partial order relation on a ﬁnite set A. Under what conditions
is the Hasse diagram of the poset the standard diagram of a rooted tree
(with the root at the top of the tree)?
9.
Let T be a full m-ary tree of height h and with vertex set V . Let B denote
the set of leaf vertices and let Vk denote the set of vertices at level k.
Prove each of the following inequalities:
(i)
m ⩽|Vk| ⩽mk
(ii)
hm + 1 ⩽|V | ⩽(1 + m + m2 + · · · + mh)
(iii)
h(m −1) + 1 ⩽|B| ⩽mh.
Find full m-ary tress for which the extremes of the inequalities hold.
(These examples show that the inequalities cannot be ‘improved’; that
is, made more restrictive.)
(Questions 10–14) Rooted Tree Representation of Algebraic Expressions
Let S be a set on which two binary operations, ⊕and ∗, are deﬁned. Without an
order of precedence convention an expression such as x ⊕y ∗z is ambiguous—it
could refer either to (x ⊕y) ∗z or to x ⊕(y ∗z).
Such expressions can be unambiguously represented using binary trees as follows.
An expression can be deﬁned recursively as XαY , where α is (the symbol for)
a binary operation and X and Y are either elements of S or expressions. Such
an expression can be represented as a binary tree with root α, left child X and
right child Y . (Compare this with the deﬁnition of a Boolean expression: see
deﬁnition 9.3.)
For example, the expressions above can be represented by the following binary
trees.

Rooted Trees
625
10.
(i)
Represent each of the following expressions as binary trees:
(a)
(x ⊕y) ∗(z ⊕t)
(b)
((x ⊕y) ∗z) ⊕t
(c)
(r ∗s) ⊕((x ⊕y) ∗z)
(d)
r ∗(s ⊕((x ⊕y) ∗z))
(e)
(((r ∗s) ⊕x) ⊕y) ∗z.
(ii)
How many different possible interpretations are there of each of
the following expressions?
(a)
x ∗y ∗z
(b)
t ⊕x ∗y ∗z
(c)
t ∗x ⊕y ∗z.
If ∗is associative, how many (essentially) different interpretations
are there?
11.
In the preﬁx (or Polish) form of an expression, the symbol for the binary
operation is written before the two elements or expressions which it
combines. Thus αXY is the preﬁx form of the expression XαY .
Parentheses are not necessary when using preﬁx notation as there is no
inherent ambiguity. For example, the expressions (x⊕y)∗z and x⊕(y∗z)
have the respective preﬁx forms ∗⊕xyz and ⊕x ∗yz.
(i)
Write each of the expressions in questions 10(i) above in preﬁx
notation.
(ii)
Describe how the binary tree of an expression can be obtained from
its preﬁx form.
12.
In the postﬁx (or reverse Polish) form of an expression, the symbol
for the binary operation is written after the two elements or expressions
which it combines. Thus XY α is the postﬁx form of the expression
XαY .
As for preﬁx expressions, parentheses are not necessary when using
postﬁx notation. The expressions (x ⊕y) ∗z and x ⊕(y ∗z) have the
respective postﬁx forms xy ⊕z∗and xyz ∗⊕.
(i)
Write each of the expressions in question 10 (i) above in postﬁx
notation.
(ii)
Describe how the binary tree of an expression can be obtained from
its postﬁx form.

626
Applications of Graph Theory
13.
Write the usual (or inﬁx), the preﬁx and the postﬁx forms of the
expressions represented by the following binary trees.
x
t
y
Å
*
*
*
y
*
x
z
r
Å
x
y
Å
*
x
t
z
Å
*
(i)
(ii)
14.
Recall from chapter 10 that a Boolean expression may involve two binary
operations, ⊕and ∗, as well as the (unary) complement operation¯.
(i)
In what way may the rooted tree representation of a Boolean
expression differ from those considered above?
(ii)
Draw the rooted trees of each of the following Boolean
expressions:
(a)
x1 ⊕(x2 ∗x3)
(b)
(x1 ∗x2) ⊕(¯x3 ∗x4)
(c)
x1 ⊕{x2 ∗[x3 ⊕(x4 ∗¯x5)]}.
12.3
Sorting
A common and important operation in data processing is the sorting of data into
an appropriate order. The nature of the data and the use to which they are to
be put will determine the desired ordering. Commonly this will be numerical or
alphabetical order, although the exact nature of the required ordering need not
concern us here. However, we shall assume that the order relation is a total order.
Recall from §4.5 that a total order R is such that any pair of elements can be
‘compared’: either a R b or b R a (or both). The methods we outline below can be
modiﬁed to apply to an ordering which is just a partial order, but the results are
less meaningful. (See exercise 12.2.10.)
To be more precise, let A be a ﬁnite totally ordered set with order relation denoted
by ⩽. We write a ⩽b and ‘a is less than or equal to b’ rather than a R b and ‘a

Sorting
627
is related to b’, which we used in chapter 4. Suppose that the elements of A
are given as an unsorted list a1, a2, . . . , aN and we wish to obtain a sorted list
s1, s2, . . . , sN of the same elements, i.e. a list such that s1 ⩽s2 ⩽· · · ⩽sN.
(If A were a partially—but not totally—ordered set, it would not be possible to
produce such a list.)
There are many techniques for achieving the desired list. You may be familiar
with some of these, for instance ‘selection sort’, ‘bubble sort’, ‘insertion sort’,
‘quick sort’, etc. The aim of this section is to outline two methods of sorting—
‘tree sort’ and ‘heap sort’—both of which use rooted trees as an essential part of
the process.
Tree Sort
The tree sort procedure uses a special kind of binary tree, appropriately called a
‘sort tree’. This is a binary tree whose vertex set is totally ordered and has the
property that, for every vertex v of the tree, the vertices in its left subtree are less
than or equal to v, and v is less than or equal to the vertices in its right subtree.
The term ‘binary search tree’ is used by some authors, but we use ‘search’ in a
different context in the next section. The formal deﬁnition is the following.
Deﬁnition 12.5
A sort tree is a binary tree T such that:
(i)
the vertex set V is totally ordered (with order relation denoted ⩽),
and
(ii)
for each v ∈V , wL ⩽v for every vertex wL in the left subtree of
v, and v ⩽wR for every vertex wR in the right subtree of v.
The tree sort procedure occurs in two phases. Suppose we are given the elements
of A in an unsorted list a1, a2, . . . , aN. In the ﬁrst phase, the list is used to
construct (or grow!) a sort tree whose vertices are the elements of A and whose
root is a1, the ﬁrst element of the unsorted list. The second phase obtains the
sorted list from the sort tree.
Before describing how the tree sort technique works in general, we illustrate it
through an example.

628
Applications of Graph Theory
Example 12.4
Suppose the initial list is 6, 2, 9, 4, 15, 1, 12, 7, 20, 10, 3, 11 which is to be sorted
(eventually) into increasing order. A sequence of sort trees is produced by adding
each element of the list in turn as a leaf vertex. The vertex set of the last sort tree
in the sequence contains all the elements of the list.
The ﬁrst element 6 is deﬁned to be the root. Since 2 ⩽6 we create a left branch
from the root with 2 at its end. Next, as 9 ⩾6 we create a right branch from the
root and place 9 at its end.
To process the next element 4, ﬁrst compare it with the root 6. Since 4 ⩽6, we
must place 4 in the left subtree of 6. Thus we consider the left subtree whose root
is 2. Now 4 ⩾2, so we create a right branch from 2 with 4 as its child.
Inserting the next element 15 as a leaf vertex involves the following comparisons:
15 ⩾6 so go to the right subtree of 6 whose root is 9;
15 ⩾9 so create a right branch from 9 with 15 as a level 2 vertex.
At this stage, we have grown the following sort tree.
Now continue in the same way adding each element in turn. To insert an element
x we ﬁrst compare it with the root 6. If x ⩽6 go to the left subtree; otherwise go
to the right subtree. Repeat the comparison of x and the root of the new tree. At
some stage there is no left or right subtree to ‘go to’; then add x as a left or right
child as appropriate.
Applying this process to the remaining elements of the unsorted list produces the
sequence of sort trees shown in ﬁgure 12.8, the last of which is the sort tree of the
whole (unsorted) list.
This completes the ﬁrst phase of the tree sort. Next we use the sort tree to obtain
the sorted list. Since every vertex in the left subtree of the root is less than or
equal to the root, these need to be listed before the root itself. Similarly all the
vertices in the right subtree of the root need to be listed after the root itself. It is
important to realize that both the left and right subtrees of the root are themselves

Sorting
629
Figure 12.8
sort trees and so their left and right subtrees are also sort trees, and so on down
through the tree.
The required listing procedure can be described recursively as follows:
(i)
list the elements in the left subtree of the root,
(ii)
list the root itself, and
(ii)
list the elements in the right subtree of the root.

630
Applications of Graph Theory
The reason that this is a recursive description of the listing process is that to
perform each of the steps (i) and (iii) we need to repeat the whole process for
the appropriate subtree. (When we do this ‘the root’ will refer to the root of
whichever subtree is being processed, and not the root of the main tree.)
To illustrate how this works, we consider step (i) in a little more detail. For the
sort tree in ﬁgure 12.8(g), the left subtree of the root is the following.
To process this sort tree we list the elements in its left subtree (1), list the
root (2) and then list the elements in the right subtree. To list the elements of
the right subtree, we apply the same three steps again: list the elements in its left
subtree (3), list the root (4) and then list the elements in its right subtree (none).
This gives the list 1, 2, 3, 4 and completes step (i) for the main tree.
Step (ii) then lists (6), the root of the main sort tree. Finally we need to list the
elements of its right subtree in a similar way. This requires performing our steps
several more times, and is left as an exercise. In this way we obtain the sorted list
1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 15, 20.
We shall describe this listing procedure more fully for a general sort tree later.
We now turn to general descriptions of the two phases of tree sort: growing the
sort tree from the unsorted list and listing the elements from the sort tree.
Step 1: Growing the Sort Tree
Suppose that a1, a2, . . . , aN is an unsorted list (with order relation ⩽). Working
through the list, the sort tree is grown one branch at a time. Each time an element
is processed it is added to the tree as a leaf vertex, although it may subsequently
become a parent. To begin, the ﬁrst element a1 is deﬁned to be the root vertex v∗.
We now have a sort tree T1 whose root has empty left and right subtrees.
To add a branch with element x at its end proceed as follows. Compare x with
the root; if x ⩽v∗proceed to the root of the left subtree and if v∗⩽x proceed to
the root of the right subtree. Then we compare x with the root of the appropriate
subtree and repeat. Eventually one of the following two possibilities occurs when
comparing x with the current root v: either x ⩽v and v has empty left subtree or

Sorting
631
v ⩽x and v has empty right subtree. In the ﬁrst case we add x as a left child of v
and in the second we add x as a right child of v.
Growing the sort tree from the unsorted list a1, a2, . . . , aN can be summarized as
follows.
Algorithm 12.1
1.
Set n = 1, a1 equal to the root and T1 equal to the sort tree with
a1 as its only vertex. Increase n to n + 1.
2.
If n > N then end. Otherwise compare an with the root.
If an ⩽root then proceed to the left subtree.
If the left subtree is empty then add an as a left child of the root
to form the next sort tree Tn, increase n to n + 1 and repeat
step 2;
otherwise repeat step 2 using the left subtree.
Otherwise proceed to the right subtree.
If the right subtree is empty then add an as a right child of the
root to form the next sort tree Tn, increase n to n + 1 and repeat
step 2;
otherwise repeat step 2 using the right subtree.
Note the recursive nature of algorithm 12.1. To process step 2 for a particular
subtree we may need to process step 2 itself for various smaller subtrees.
Example 12.5
Grow a sort tree from the list Hawk, Raven, Wren, Falcon, Dove, Eagle, Pelican,
Robin, Osprey, Egret, Rook under alphabetical ordering.
Solution
Firstly, place Hawk as the root. Since Hawk ⩽Raven a right branch is added
with Raven as its leaf. Next, since Hawk ⩽Wren and Raven ⩽Wren we add a

632
Applications of Graph Theory
right branch from Raven with Wren as its leaf. Falcon ⩽Hawk so it is added as
a left child of the root. Similarly, Dove ⩽Hawk and Dove ⩽Falcon so we add
a left branch from Falcon with Dove as its root. At this stage we have grown the
following sort tree.
The next element to be added is Eagle. Since Eagle ⩽Hawk we proceed to the left
subtree with root Falcon. Now Eagle ⩽Falcon as well so we again proceed to the
left subtree with root Dove. This time Dove ⩽Eagle and Dove has an empty right
subtree so we add Eagle as a right branch from Dove, and then process Pelican,
the next element of the unsorted list.
Continuing in this way we eventually obtain the sort tree shown in ﬁgure 12.9.
Figure 12.9
Step 2: Listing The Elements of a Sort Tree
Let T be the sort tree grown from the unsorted list a1, . . . , aN according to step 1.
To list the vertices of the sort tree in the appropriate order we apply the following
three steps.
1.
Process left subtree.
2.
List the root.
3.
Process right subtree.
To process a subtree we repeat each of the three steps with the proviso that an
empty tree needs no processing!

Sorting
633
Example 12.6
List the elements of the sort tree in ﬁgure 12.9.
Solution
Performing the three steps 1, 2 and 3 recursively gives the following. Each level
of the process has been indented so that the three steps begin in the same vertical
position. Of course, each indentation represents a level of the sort tree, so the
maximum number of indentations (four in this case) equals the height of the tree.
We have used ‘left subtree(X)’ to denote the left subtree which has X as its root
(and similarly for right subtrees). When no processing is required [Empty] is
written after the particular process.
Following through the process, we can see that the elements are listed in the
correct alphabetical order: Dove, Eagle, Egret, Falcon, Hawk, Osprey, Pelican,
Raven, Robin, Rook, Wren.
Step1: Process left subtree(Hawk)
Step 1: Process left subtree(Falcon)
Step 1: Process left subtree(Dove) [Empty]
Step 2: List Dove
Step 3: Process right subtree(Dove)
Step 1: Process left subtree(Eagle) [Empty]
Step 2: List Eagle
Step 3: Process right subtree(Eagle)
Step 1: Process left subtree(Egret) [Empty]
Step 2: List Egret
Step 3: Process right subtree(Egret) [Empty]
Step 2: List Falcon
Step 3: Process right subtree(Falcon) [Empty]
Step 2: List Hawk
Step 3: Process right subtree (Hawk)
Step 1: Process left subtree (Raven)
Step 1: Process left subtree (Pelican)
Step 1: Process left subtree (Osprey) [Empty]
Step 2: List Osprey
Step 3: Process right subtree(Osprey) [Empty]
Step 2: List Pelican
Step 3: Process right subtree(Pelican) [Empty]

634
Applications of Graph Theory
Step 2: List Raven
Step 3: Process right subtree(Raven)
Step 1: Process left subtree(Wren)
Step 1: Process left subtree(Robin) [Empty]
Step 2: List Robin
Step 3: Process right subtree(Robin)
Step 1: Process left subtree(Rook) [Empty]
Step 2: List Rook
Step 3: Process right subtree(Rook) [Empty]
Step 2: List Wren
Step 3: Process right subtree(Wren) [Empty]
Heap Sort
The heap sort algorithm also uses a special kind of binary tree, called a ‘heap’,
whose vertices are again members of the list to be sorted. The ‘shape’ of a heap
is such that it has the smallest possible height for the number of its vertices. This
is achieved by ensuring that, if we ignore its highest level, the remainder of the
tree is both complete and full. In addition, the leaf vertices at the highest level
are situated as far to the left of the diagram as possible. These two conditions
determine the shape of the diagram of a heap (see ﬁgure 12.10 below). The
last condition in the following deﬁnition refers to the relationship between the
vertices.
Deﬁnition 12.6
A (descending) heap is a binary tree of height h with the following
properties.
(i)
All leaf vertices are at levels h −1 or h.
(ii)
The leaf vertices at level h are situated as far to the left of the
diagram as possible. (This implies that any leaf vertices at level
h−1 are situated as far to the right as possible. In other words, the
subtree formed by deleting the level h vertices—and their incident
edges—is a complete full binary tree.)
(iii)
The vertex set has a total order, denoted by ⩽.
(iv)
If q is a parent of p then p ⩽q, i.e. each child is less than or equal
to its parent with respect to the given total order.

Sorting
635
In a descending heap the vertices along any path beginning with the root occur in
descending order with respect to the given total order. An ascending heap can be
deﬁned similarly, the only difference being that each parent is less than or equal
to each of its children. Of course, in an ascending heap the vertices along any
path beginning with the root occur in ascending order.
Example 12.7
Figure
12.10
shows
two
different
(descending)
heaps
with
vertex
set
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} having the usual ordering ⩽. The heaps have the same
‘shape’ and differ only in the positions of the elements of the vertex set. In
general, conditions (i) and (ii) of the deﬁnition ensure that all heaps with a given
number of vertices have the same shape.
Figure 12.10
The heap sort procedure is similar in overall structure to tree sort. Firstly, a heap
is created from the unsorted list and then the sorted list is obtained from the given
heap. We outline each phase in the procedure separately.
Step 1: Creating the Heap
As for tree sort, we suppose an unsorted list a1, a2, . . . , aN is given. Set a1 as the
root, which creates a (rather trivial) heap. Suppose a1, a2, . . . , ak−1 have already
been formed into a heap and we need to add the next element ak. There is a
unique position to add ak to satisfy conditions (i) and (ii) of the deﬁnition of a
heap; this is either immediately to the right of the rightmost vertex at the highest
level or, if the highest level is complete, the leftmost vertex at a new higher level.
However, adding ak in this position does not necessarily produce a heap. For
example, suppose ak = 17 is added to the heap in ﬁgure 12.11(a) giving the
binary tree (b). This new tree is no longer a heap since 17 is greater than its parent
12. In such a case the new vertex is interchanged with its parent, producing the

636
Applications of Graph Theory
tree (c). This has only moved the problem further up the tree as 17 is still greater
than its new parent 15. We therefore need to swap 15 and 17 which ﬁnally restores
the heap—ﬁgure 12.11(d). In this process of restoring the heap, we say that the
newly added vertex is bubbled up through the tree.
Figure 12.11
The creation of the heap from the (unsorted) list a1, a2, . . . , aN can be
summarized by the following algorithm.
Algorithm 12.2
1.
Set a1 as the root.
2.
Add the next element in the (unique) position so that the tree
satisﬁes conditions (i) and (ii) of the deﬁnition of a heap.
3.
If necessary, restore the heap by bubbling the new vertex up
through the tree.
4.
Repeat steps 2 and 3 for each element of the list in turn.
Example 12.8
Create a heap from the list Hawk, Raven, Wren, Falcon, Dove, Eagle, Pelican,
Robin, Osprey, Egret, Rook used in example 12.5.

Sorting
637
Solution
(We strongly recommend that you draw diagrams to follow the construction of
the heap.)
Firstly, place Hawk as the root and then add Raven as a left branch. Since Hawk
⩽Raven we need to swap the two vertices. Next Wren is added as a right child of
the root (which is now Raven) and then interchanged with Raven. Now Falcon,
Dove, Eagle and Pelican can be added in turn at level 2 without any bubbling up
through the tree. At this stage we have the following heap.
The next element, Robin, is added at the leftmost end of the next level. Since
Falcon ⩽Robin they need to be interchanged; then we have Hawk ⩽Robin
so these two vertices must also be swapped.
The result is that Robin has
bubbled up to the position previously occupied by Hawk, producing the heap in
ﬁgure 12.12(a). Next to be added is Osprey which we need to interchange with
its parent, Hawk, giving ﬁgure 12.12(b). When Egret is added as a left branch
from Dove and the two vertices are swapped, ﬁgure 12.12(c) is obtained. Finally,
Rook is added as a right branch to Egret and then is bubbled up to the position
previously occupied by Robin. This produces the ﬁnal heap—ﬁgure 12.12(d).
Figure 12.12

638
Applications of Graph Theory
Step 2: Obtaining the Sorted List from the Heap
To generate the sorted list from a heap, we ﬁrst create a binary tree whose vertices,
when read from top to bottom and left to right, are in increasing order, i.e. the
vertices, read in this way, form the sorted list we are seeking. How this is achieved
is illustrated in the next example.
Example 12.9
Suppose the heap in ﬁgure 12.13(a) has been created from an unsorted list of
numbers with respect to the usual ordering ⩽. We need a process which will
create the tree in ﬁgure 12.13(b) from this heap. The sorted list 1, 2, 3, 4, 5, 6, 7,
8, 9, 10 can then be read off from the tree (b) by going through the tree from top
to bottom and left to right.
Figure 12.13
Clearly the root of a heap is the maximal element and should therefore be placed
at the end of the list.
Interchanging it with the last vertex in the heap (i.e.
the rightmost vertex at the highest level) places it in the desired position. In
ﬁgure 12.13(a), we need to interchange 10 and 4. This destroys the heap, of
course, because the largest vertex is now a leaf. However, since 10 is now in the
desired position of the tree it will be ignored. Even disregarding this vertex, the
remainder of the tree is still not a heap because the root 4 is less than each of its
children.
We therefore need to restore the heap (remembering to ignore the vertex 10 which
is already in its ﬁnal position). To do this the root 4 needs to be sunk down
through the tree, by successively interchanging it with the larger of its children.
In this instance it is interchanged ﬁrstly with its right child 9 and subsequently
with its (new) left child 8. This produces the following tree, where the circled

Sorting
639
vertex is in its ﬁnal position and the subtree with uncircled vertex set is a heap.
The process is then repeated ignoring the circled vertex 10 which is now in its
ﬁnal position. In general, at each stage we consider only the portion of the tree
which is a heap; that is, the portion of the tree whose vertices are not (necessarily)
in their ﬁnal positions. These are the uncircled vertices. Figure 12.14 shows the
remaining stages in this process.
Figure 12.14

640
Applications of Graph Theory
The process of converting the heap into the new tree explained in example 12.9
can be summarized as the following algorithm.
Algorithm 12.3
1.
Interchange the root with the last vertex, u; that is, the rightmost
vertex at the highest level.
Fix the position of the (new) last
vertex.
(By ﬁxing the position of a vertex, we mean that it is
to be ignored in all subsequent manipulations of the tree—in the
previous example we denoted the ﬁxed vertices by circling them.)
2.
Restore the tree (apart from any ﬁxed vertices) to a heap by sinking
down the root u as follows.
(i)
Interchange the root u with the largest of its two children.
(ii)
Go to the subtree with u as its root. If this subtree has height
0 (ignoring, of course, any ﬁxed vertices) then go to step 3;
otherwise repeat step 2(i) for this subtree.
3.
Repeat steps 1 and 2, remembering to ignore any ﬁxed vertices
until all vertices are ﬁxed.
We end this section with a few brief comments on the relative performance of
various sorting procedures.
Unfortunately perhaps, no one sorting method is
clearly superior to all the others. Various factors inﬂuence the choice of sort
process in a given application: size of input data, amount of available memory,
whether or not the input data are likely to be ‘roughly’ sorted, etc. Of the two
algorithms we have outlined, tree sort would probably be preferred for small data
sets and heap sort for large data sets. From a theoretical point of view heap sort is
a ‘fast’ algorithm. In practice, other algorithms such as quick sort perform better
‘on average’ than heap sort although on some sets of input data quick sort may be
signiﬁcantly slower than heap sort.
Exercises 12.2
1.
Construct a sort tree from each of the following unsorted lists.

Sorting
641
(i)
7, 3, 15, 4, 12, 14, 6, 8, 2, 5, 13, 19 ordered by ⩽.
(ii)
when, shall, we, three, meet, again, in, thunder, lightning, or, in,
rain ordered alphabetically. (Note that there is a repeated element
in the list. However, the construction of the sort tree described in
the text takes care of this possibility.)
(iii)
8, 64, 1, 4, 256, 512, 2, 32, 128, 16 ordered by divisibility.
2.
List the elements of each of the following sort trees using the method
described in the text. (The ordering in each case is ‘less than or equals’,
⩽.)
From which of the two sort trees is the sorted list most quickly obtained?
Why?
3.
Work through both steps of tree sort for each of the following unsorted
lists.
(i)
17, 31, 5, 23, 2, 7, 19, 29, 11, 3, 13 with order relation ⩽.
(ii)
bca, cbc, acb, bcb, bac, cac, abc, aca, bab, aba, cba, cab with
alphabetical orderings of strings.
4.
Construct a heap from each of the unsorted lists in question 1 above.
5.
Following the procedure of example 12.9, convert each of the following
heaps into a tree whose vertices are in increasing order reading from top

642
Applications of Graph Theory
to bottom, left to right.
6.
Work through both steps of heap sort for each of the unsorted lists in
question 3 above.
7.
Let A be a set of (English) words. A total order relation ⩽on A is deﬁned
as follows:
w1 ⩽w2 if and only if either length(w1) < length(w2) or length(w1)
= length(w2) and w1 comes before w2 in the usual alphabetical
ordering of words.
(Here length(w) is the number of letters in the word w, of course.)
(i)
Work through both steps of tree sort to sort the list group,
morphism, heap, sort, Boolean, algebra, algorithm, tree, discrete
with respect to this order relation.
(ii)
Repeat (i) using heap sort.
8.
A heap is deﬁned as a binary tree (with additional properties). Explain
why a heap is a fully binary tree if and only if it has an odd number of
vertices. When is a heap a complete binary tree?
9.
(i)
Write a program in any high-level computer language or in
pseudocode to grow a sort tree from an unsorted list of numerical
data, assuming that the order relation is ⩽.
(ii)
Write a program in any high-level computer language or in
pseudocode which inputs the sort tree from your program for
part (i) and outputs the sorted list.
Note: you may wish to use the recursive deﬁnition of a binary tree given
in §12.2.

Searching Strategies
643
10.
(Sorting a partially ordered set which is not totally ordered.)
Consider the list 10, 2, 6, 30, 3, 15, 5, 1, 60, 4, 20, 12 (the divisors of 60)
ordered by divisibility. The order relation is not a total order since, for
example, 5 does not divide 6 and 6 does not divide 5.
Work through both steps of tree sort for this list. (Note that algorithm 12.1
needs to be applied with care: if an doesn’t divide the root then proceed
to the right subtree even if the root doesn’t divide an either.)
Check that your ‘sorted’ list can be divided into ‘blocks’, each of which is
a chain. (Recall from §4.5 that a chain is a subset which is totally ordered
by the given relation.)
12.4
Searching Strategies
A searching of the vertices of a connected graph Γ is a systematic procedure
for ‘visiting’ all the vertices of Γ by ‘travelling along’ its edges. An edge of Γ
may be traversed more than once and a vertex may be visited more than once in
the search. In other words, a searching of a graph is a method of constructing
an edge sequence whose associated vertex sequence includes every vertex of
the graph. (Edge sequences and their associated vertex sequences are deﬁned
in deﬁnition 11.6.)
We can regard a searching of a graph Γ as a systematic procedure for constructing
a subgraph which contains all the vertices of Γ.
We call such a subgraph a
spanning subgraph of Γ. (The term ‘wide subgraph’ is used by some authors.)
In fact both of the procedures we consider actually construct a spanning tree in Γ.
The situation is analogous to a game of hide-and-seek. The ‘seeker’ has to check
various potential hiding places (vertices) until all the ‘hiders’ are found. There
are two principal strategies that the seeker may adopt. He or she can move from
one potential hiding place to the next, always checking new locations until forced
to ‘backtrack’ in order to continue visiting new hiding places. (One can well
imagine an excited child playing the game in this way.) This mode of search is
called a ‘depth-ﬁrst search’.
An alternative technique which the seeker may adopt is ﬁrstly to check all the
potential hiding places in the immediate vicinity of his or her starting position
before widening the search area and again checking all the (new) locations in the
widened area. The search area is gradually extended until the whole region has
been searched. This searching strategy is called a ‘breadth-ﬁrst search’.

644
Applications of Graph Theory
We shall consider each of these two methods more precisely beginning with the
depth-ﬁrst approach. We describe this as an algorithm that constructs a sequence
of trees, the last of which is the desired spanning tree. In general, Γ will have
many spanning trees; the particular tree obtained depends on the choices made in
the execution of the algorithm.
Algorithm 12.4 (Depth-ﬁrst search)
Let Γ be a connected graph and v0 a vertex of Γ. A depth-ﬁrst search
of Γ, with initial vertex v0, is the following procedure for constructing a
spanning tree for Γ.
1.
Set w = v0 and n = 0; w is called the current centre of the
search. Let T0 denote the tree with no edges and vertex v0; T0 is
the ﬁrst tree in our sequence.
2.
If possible, choose an edge en which is incident with w and a new
vertex vn+1. (By a new vertex, we mean a vertex which does not
appear in Tn, the current tree; in other words, a vertex which has
not yet been visited.) Adjoin en to the current tree Tn to form the
next tree in the sequence, Tn+1. Set w = vn+1 and increase n to
n + 1.
3.
Repeat step 2 until the current w is not adjacent to any new vertex.
If Tn is a spanning tree, the search is complete. If not, backtrack
along the last edge to the previous centre.
More precisely, set
w = vn−1 and increase n to n + 1. Repeat step 2 (if possible;
several backtrackings may be required before a new vertex can be
visited).
As with many algorithms, it is only really possible to understand the way that
depth-ﬁrst search constructs the required edge sequence by working through an
example.
Example 12.10
We use the depth-ﬁrst search algorithm to construct a spanning tree for the graph
illustrated in ﬁgure 12.15. The initial vertex of the search is v0.
Suppose we begin by choosing the edge e0 (see ﬁgure 12.16). Thus the ﬁrst tree

Searching Strategies
645
Figure 12.15
is e0 (together with its vertices) and the vertex v1 becomes the new centre of the
search. This is illustrated by ﬁgure 12.16(a), where the new centre of the search
is circled.
Since v1 is not adjacent to any new vertex we need to backtrack to v0. That is, we
set w = v0 (again). Now perform step 2 twice choosing to add the edges e1 and
e2 in turn to the tree. The current centre moves ﬁrstly to v2 and then to v3. Our
status at this stage is reﬂected in ﬁgure 12.16(b); again the current centre of the
search has been circled.
Step 2 cannot be repeated at this point so backtrack and set w = v2. Applying
step 2 four times brings us to the situation represented in ﬁgure 12.16(c). At this
stage the tree has edge set {e0, e1, e2, e3, e4, e5, e6} and the current centre is v7.
To continue, we need to backtrack twice to v5 before further edges can be added.
The rest of the search is illustrated in ﬁgures 12.16(d)–(f). The diagrams show
the search frozen at each stage where backtracking is required, with the current
centre circled in each case.
At the end of the search, the last tree contains all the vertices of Γ and so is a
spanning tree. The edges have been added to the tree in the order e0, e1, . . . , e12,
and the vertices have been ‘visited’ in the order v0, v1, . . . , v13. (Of course, had
we made different choices we might have obtained a different spanning tree and
the vertices would have been visited in a different order.)

646
Applications of Graph Theory
Figure 12.16
We now turn to the breadth-ﬁrst search, which is again described as an algorithm
that constructs a sequence of trees, the last of which is a required spanning tree.
The essential difference between breadth-ﬁrst and depth-ﬁrst searches concerns
the movement of the current centre. In algorithm 12.4 we saw that the current
centre moves each time an edge is added to the tree. By contrast, in the following
algorithm the current centre moves only when it is ‘forced to’, i.e. when all
possible edges incident with it have already been added to the tree.

Searching Strategies
647
Algorithm 12.5 (Breadth-ﬁrst search)
Let Γ be a connected graph and v0 a vertex of Γ. A breadth-ﬁrst search of
Γ, with initial vertex v0, is the following procedure for growing a spanning
tree.
1.
Set w = v0, n = 0 and m = 1. (As for the depth-ﬁrst search
algorithm, w is called the current centre of the search.) Let T0
denote the tree with no edges and vertex v0; T0 is the ﬁrst tree in
the sequence.
2.
If possible, choose an edge en which is incident with w and a new
vertex vn+1. (Recall that a new vertex is one which does not appear
in the current tree Tn.) Adjoin en to Tn to form the next tree Tn+1.
Increase n to n + 1.
3.
Repeat step 2 until there are no further new vertices adjacent to w.
If all vertices have now been visited then Tn is a spanning tree and
we stop. Otherwise set w = vm, the ﬁrst of the vertices which has
not yet acted as the current centre, and increase m to m+1. Repeat
step 2.
Example 12.11
To understand how algorithm 12.5 itself works as well as to contrast it with
algorithm 12.4, we work through the breadth-ﬁrst algorithm for the graph in
ﬁgure 12.15.
Beginning with current centre v0, we perform step 2 ﬁve times, successively
adjoining the edges e0, e1, e2, e3 and e4.
This gives the tree shown in
ﬁgure 12.7(a), where the convention introduced in example 12.10 of circling the
current centre is again employed. (Note that we are using a different labelling of
vertices and edges from that used in example 12.10 and ﬁgure 12.16.) We cannot
adjoin further edges incident to v0, so the current centre moves to v1. Clearly
there are no edges which can be adjoined with v1 as the current centre, so the
centre moves to v2 where again step 2 cannot be performed.
Thus the centre then moves on to v3, where step 2 can be performed once
resulting in adjoining the edge e5. The position at this stage is represented in
ﬁgure 12.17(b).

648
Applications of Graph Theory
Figure 12.17

Searching Strategies
649
The current centre moves to v4 and the edges e6 and e7 are then adjoined to the
tree producing ﬁgure 12.17(c). The remainder of the execution of the algorithm is
represented by the graphs in ﬁgures 12.17(d)–(f). The diagrams show the search
frozen at each stage where edges have been added to the tree.
We can regard the execution of the breadth-ﬁrst search algorithm as having several
phases. In the ﬁrst phase we adjoin all possible edges to the initial vertex v0. This
is the situation reached in ﬁgure 12.17(a). In the second phase each of the vertices
added in the ﬁrst phase becomes the current centre in turn and edges are adjoined
where possible. This phase is illustrated by the graphs in ﬁgures 12.17(b)–(d).
In general, during the (n + 1)th phase of the execution of the algorithm each
vertex added in the nth phase becomes the current centre in turn. If we regard the
resulting spanning tree as a rooted tree with root v0, then the level n vertices are
those added during the nth phase of the execution of the algorithm. Our example
has been completed in three phases.
Comparing the algorithms for depth-ﬁrst and breadth-ﬁrst searches, the signiﬁcant
difference concerns when the current centre moves to a new vertex. In depth-ﬁrst
search, the current centre moves at each step (where possible) to the most recently
visited vertex. By contrast, in breadth-ﬁrst search the centre moves only when all
vertices adjacent to the current centre have been visited.
We now have two systematic methods for growing a spanning tree in a connected
graph Γ.
The obvious question which springs to mind is: ‘which is to be
preferred?’ It can be shown that the depth-ﬁrst and breadth-ﬁrst algorithms have
the same ‘worst-case complexity’. The worst-case complexity of an algorithm
is an approximate measure of the maximum number of operations required to
perform the algorithm; it is of course, a function of the size of the input data,
which would be the graph itself in this case. (The computational complexity of
algorithms is explained in a little more detail in §12.6.)
Since algorithms 12.4 and 12.5 have the same worst-case complexity, neither is to
be preferred over the other as a general algorithm. For particular graphs, however,
one algorithm might produce a spanning tree with much less fuss than the other.
To take a simple example, depth-ﬁrst search would more efﬁciently on a cycle
graph (ﬁgure 12.18(a)) whilst breadth-ﬁrst would work more efﬁciently on the
‘Maltese cross’ graph (ﬁgure 12.18(b)). This becomes apparent as soon as you
apply the algorithms to each of these graphs, regardless of the choice of initial
vertex.

650
Applications of Graph Theory
Figure 12.18
Exercises 12.3
1.
Find a spanning tree for each of the following graphs using (a) depth-ﬁrst
search and (b) breadth-ﬁrst search with the indicated initial vertex.

Searching Strategies
651
2.
Show that, if Γ is a connected graph, then the depth-ﬁrst algorithm 12.4
does indeed produce a spanning tree in Γ.
3.
Show that, if Γ is a connected graph, then the breadth-ﬁrst algorithm 12.5
does indeed produce a spanning tree in Γ.
4.
Execute the depth-ﬁrst and the breadth-ﬁrst algorithms on the complete
graphs K4 and K5.
5.
The ﬂoor plan of a museum is given in the following diagram, where the
entrance to the exhibits is from the foyer, labelled E.
(i)
Draw a graph to represent the geography of the museum by
representing each room as a vertex and each doorway as an edge.
(ii)
Perform both a depth-ﬁrst and a breadth-ﬁrst search of the
museum, with initial vertex E in both cases.
(iii)
A visitor to the museum particularly wishes to see an exhibit in
room X. Which of your two searches would you recommend to
the visitor?

652
Applications of Graph Theory
6.
The following is a plan of a maze with entrance A and exit B. Draw a
graph to represent the maze as follows. Each letter represents a point in
the maze and is represented by a vertex. An edge joins two vertices if
and only if there is a path in the maze from one vertex to the other which
does not pass another vertex. (For instance, there is an edge in the graph
joining J and R, but there is no edge joining J and S since a path in the
maze from J to S passes either R or D.)
You are lost in the maze at the point E. Perform a depth-ﬁrst and a
breadth-ﬁrst partial search of the maze until you ﬁnd your way out. (By
a partial search, we mean execute the appropriate algorithm only until B,
the exit to the maze, is included in the tree.)
Repeat both searches but this time, whenever there is a choice of vertices
to visit, choose the one which comes ﬁrst in the alphabet.
7.
Let Γ be a connected graph. Explain why it is possible, with suitable
choices (including that of initial vertex), to perform the depth-ﬁrst search
algorithm without backtracking if and only if Γ is semi-Hamiltonian.
(Semi-Hamiltonian graphs are deﬁned in exercise 11.2.10.)
12.5
Weighted Graphs
In chapter 11 we used the analogy of a road map to illustrate various graph-
theoretic concepts. There is one aspect of a road map, however, which is not
modelled by a graph—namely, the distances between towns. In many applications

Weighted Graphs
653
of graph theory it is important to be able to attach numbers to the edges of the
graph which represent certain physical quantities. For example, if we wish to
use a graph to represent an electrical network, it may be important to record the
resistances of each of the components represented by the edges of the graph.
Similarly, if our graph is a mathematical model of a network of ﬂuid-carrying
pipes, we might wish to include information about the capacities of the various
different pipes on the graph itself. In probability and decision theory, trees are
used where the edges represent possible outcomes of an experiment or possible
decisions made. It is often useful to assign probabilities to the outcomes which
are represented by the corresponding edges.
The logic networks of §10.5 can be considered as graphs. Boolean expressions,
rather than numbers, were attached to the edges of the corresponding graphs.
Although such possibilities are not considered here, these graphs could be
considered generalizations of those in this section.
To be able to use graphs to represent situations such as these, we need to introduce
the concept of a ‘weighted graph’. Intuitively a weighted graph is a graph where
a non-negative real number w(e), called the ‘weight’ of e, is ‘attached’ to each
edge e. The formal deﬁnition is the following.
Deﬁnition 12.7
A weighted graph is a graph Γ together with a function
w : EΓ →R+ ∪{0}.
If e is an edge of Γ, then the number w(e) is called the weight of e.
Some authors require that the weights of edges should be (non-negative) integers;
we will refer to such a weighted graph as an integer-weighted graph.
There is an obvious way to represent a weighted graph pictorially—simply write
the weight of each edge on the usual diagram of the graph.
For example,
ﬁgure 12.19 is a diagram of an integer-weighted graph.
So far we have only deﬁned the weights of individual edges of a graph. In many,
if not most, situations where weighted graphs are applied, it is the sum of the
weights of the edges of some subgraph which is important.

654
Applications of Graph Theory
Figure 12.19
Deﬁnition 12.8
Let Γ be a weighted graph. For any subgraph Γ′ of Γ we deﬁne the weight
of Γ′, w(Γ′), to be the sum of the weights of its edges. Symbolically,
w(Γ′) =
X
e∈EΓ′
w(e).
In §12.4 two techniques (depth-ﬁrst and breadth-ﬁrst searches) for growing
spanning trees in a connected graph were considered. One of the reasons why
spanning trees are important is that a spanning tree provides a ‘complete’ set of
paths in a connected graph: if T is a spanning tree for Γ, then any two vertices in
Γ can be joined by a unique path in T (see theorem 11.6(i)). If Γ is a connected
weighted graph, it is often desirable to have a spanning tree with smallest weight.
Deﬁnition 12.9
Let Γ be a connected weighted graph. A minimal spanning tree for Γ is a
spanning tree T which has the smallest possible weight in the sense that if
T ′ is any other spanning tree then
w(T) ⩽w(T ′).

Weighted Graphs
655
The ﬁrst thing to observe is that every (ﬁnite) connected weighted graph Γ has a
minimal spanning tree. Since Γ has only ﬁnitely many spanning trees one of them
must have minimal weight. Note, however, that a given weighted graph may have
more than one minimal spanning tree. Figure 12.20 shows an integer-weighted
graph with two minimal spanning trees, both of weight 22.
Figure 12.20
In an attempt to deﬁne an algorithm which produces a minimal spanning tree, we
might try to modify the depth-ﬁrst and breadth-ﬁrst algorithms (12.4 and 12.5)
so that the edge with the smallest weight amongst those under consideration is
added to the tree at each stage. Unfortunately these modiﬁcations do not produce
the required algorithm because the edge set under consideration at any stage is too
restricted. Recall that, in both depth-ﬁrst and breadth-ﬁrst searches, only edges
incident to the current centre are allowed to be attached to the current tree. (It
is an interesting exercise to construct weighted graphs for which these modiﬁed
depth-ﬁrst and breadth-ﬁrst searches do not produce minimal spanning trees. See
exercise 12.4.5.)
Widening the possible choice of edges to be added at each stage produces the
following simple algorithm for constructing a minimal spanning tree. (Another
algorithm is given in exercise 12.4.2.) The algorithm builds up successive subtrees
by attaching one edge at a time so that at each stage an edge with the smallest
weight is chosen, subject only to the restriction that a subtree results; that is, no
cycle is created. The algorithm can be described as follows.

656
Applications of Graph Theory
Algorithm 12.6 (Prim’s algorithm)
1.
First select any vertex. This begins the process with the ﬁrst subtree
T0 (which has no edges).
2.
Consider the set of edges which are incident with one of the
vertices of the subtree Tn. Of those edges which do not produce
a cycle when added to Tn, select one which has smallest weight.
(There may be more than one choice of this edge). Adjoin the
chosen edge to Tn to form a new subtree Tn+1.
3.
Repeat step 2 until it is not possible to adjoin an additional edge
to Tn without creating a cycle. The resulting tree is a minimal
spanning tree.
Note that it is not immediately apparent that this algorithm actually produces a
minimal spanning tree. It is conceivable, for instance, that beginning the whole
process at a different vertex may produce a tree with a smaller weight. However,
working through the algorithm for a few weighted graphs should convince you
that it does indeed give the required minimal spanning tree regardless of the
choices that are made. For a formal proof see Biggs (1990), for example.
Algorithm 12.6 can, in fact, be used to construct a spanning tree in any connected
(unweighted) graph Γ. Firstly, turn Γ into a weighted graph by giving every edge
weight 1 and then apply the algorithm. In this situation there will usually be
several choices of edges to adjoin at each step in the algorithm as all edges have
equal weight.
Example 12.12
Figure 12.21 illustrates the construction of a minimal spanning tree using Prim’s
algorithm 12.6. We begin by choosing the vertex v as the starting subtree T0 and
add an edge at a time until we produce the minimal spanning tree T5. Note that in
this example there is only one choice of edge at each stage, which means that the
minimal spanning tree is unique.

Weighted Graphs
657
Figure 12.21
Exercises 12.4
1.
For each of the following weighted graphs, use Prim’s algorithm 12.6,
beginning with the indicated vertex v, to ﬁnd a minimal spanning tree,
and give its weight.

658
Applications of Graph Theory
2.
An alternative algorithm for constructing a minimal spanning tree is
Kruskal’s algorithm†, which can be described as follows.
Beginning with the empty subgraph, form a sequence of (not
necessarily connected) subgraphs by adding at each stage an edge
with smallest weight which does not form a cycle with the existing
† The essential difference between Prim’s and Kruskal’s algorithms is that Prim’s algorithm
constructs a sequence of connected subgraphs without cycles (i.e. trees) whereas Kruskal’s algorithm
does not require the subgraphs to be connected.
Both Kruskal’s and Prim’s algorithms are examples of greedy algorithms. A greedy algorithm is one
which always chooses the best option available at each stage, without looking further ahead. Greedy
algorithms are ‘short sighted’ and do not always produce the optimum result because a ‘greedy’
choice early on may lead to a reduced set of options later. For Prim’s and Kruskal’s algorithms,
however, choosing the best (i.e. smallest weight) edge available at each stage does produce the best
(i.e. minimal) spanning tree.

Weighted Graphs
659
subgraph. When no further edge can be added, the resulting graph is a
minimal spanning tree.
Perform Kruskal’s algorithm on each of the weighted graphs in question 1
above.
3.
Let T be a minimal spanning tree for a weighted graph Γ. Determine
whether the following statements are necessarily true.
(i)
The weight of every edge belonging to T is less than or equal to
the weight of every edge not belonging to T.
(ii)
If no two edges of Γ have the same weight, then T is unique.
4.
Let T be a minimal spanning tree for a weighted graph Γ and let e be an
edge not belonging to T which joins distinct vertices v and w. Show that
w(e) ⩾w(e′) for every edge e′ belonging to the unique path in T joining
v and w.
(Hint: argue by contradiction. Show that if w(e) < w(e′) for some e′
then a spanning tree T ′ can be obtained with smaller weight than that of
T.)
5.
Construct a connected weighted graph and specify a choice of initial
vertex for which the application of the depth-ﬁrst algorithm (modiﬁed
so that an edge of least weight is added to the tree at each stage) does not
produce a minimal spanning tree.
6.
(i)
Let Γ be a connected weighted graph with v vertices and e
edges. The weights of the edges of Γ are the integers 1, 2, . . . , e.
Determine a lower bound for the weight of a minimal spanning
tree.
(ii)
A connected weighted graph Γ has degree sequence (1, 1, 2, 2, 3,
3, . . . , n, n).
The weights of the edges of Γ are the integers
1, 2, . . . , e, where e is the number of edges of Γ. Determine a
lower bound for the weight of a minimal spanning tree.
Show, by example, that this lower bound cannot be attained for
some weighted graphs Γ of this type.
7.
The weighted graph below represents a ﬁre prevention sprinkler system.
The vertices represent sprinklers and the edges represent the water
pipe connections between them.
The connections between sprinklers
need checking periodically but some are more difﬁcult and take longer
than others due to the location of the sprinklers and the length of the
connecting pipe. The weights denote the maintenance costs of checking
the corresponding connections.

660
Applications of Graph Theory
The sprinkler system will remain effective provided the corresponding
graph is connected. The company operating the system wishes to save
maintenance costs by removing some of the connecting pipes. What is
the maximum saving the company can make?
12.6
The Shortest Path and Travelling Salesman Problems
Let Γ be a connected weighted graph and let v, v′ be two vertices of Γ. The
shortest path problem is to ﬁnd a path joining v and v′ with the smallest weight.
Of course, since a path is a subgraph of Γ, its weight is deﬁned as the sum of the
weights of its edges. As we are dealing with ﬁnite graphs, it should be obvious
that a shortest path exists, although there may be more than one shortest path
joining a given pair of vertices.
There are various methods for ﬁnding a shortest path between two given vertices.
We describe an algorithm which, like the minimal spanning tree algorithm of the
previous section, constructs the path one edge at a time.
The idea is to begin at the vertex v and move through the graph assigning a number
L(u) to each vertex u in turn which represents the length of the shortest path yet
discovered from v to u. These ‘length numbers’ L(u) are initially considered
temporary and may subsequently be changed if we discover a path from v to
u which has length less than the currently assigned value L(u). The algorithm
is detailed more precisely below. It actually constructs a subtree of the graph
containing the vertices v and v′; a shortest path between the two vertices is then
the unique path in this tree joining them. (Note that the subtree constructed by
algorithm 12.7 need not be a spanning subtree of the graph—the algorithm stops
as soon as a shortest path joining v and v′ has been found.)

The Shortest Path and Travelling Salesman Problems
661
Algorithm 12.7 (Dijkstra’s algorithm)
1.
First assign L(v) = 0 to the starting vertex v. We say that the
vertex v has been labelled with the value 0. Furthermore, this
label is permanent as we will not subsequently change its value.
Since we are constructing a sequence of trees, we also begin with
the tree consisting of vertex v only and no edges.
2.
Let u be the vertex which has most recently been given a
permanent label. (Initially v = u as this is the only vertex with
a permanent label.) Consider each vertex u′ adjacent to u in turn,
and give it a temporary label as follows. (Only those vertices u′
without a permanent label are considered.)
If u′ is unlabelled, then set L(u′) equal to L(u) + w(e) where e
is the edge joining u to u′. (If there is more than one such edge
e, choose the one with the smallest weight.)
If u′ is already labelled, then again calculate L(u) + w(e) as
above and if this is less than the current value of L(u′) then
change L(u′) to L(u)+w(e); otherwise leave L(u′) unchanged.
3.
Choose a vertex w with the smallest temporary label and make the
label permanent. (There may be a choice to be made here as several
temporarily labelled vertices may have equal smallest labels. It is
also important to realize that w need not be adjacent to u, the most
recently labelled vertex.) At the same time adjoin to the tree so far
formed the edge which gives rise to the value L(w).
4.
Repeat steps 2 and 3 until the ﬁnal vertex v′ has been given a
permanent label. The path of shortest length from v to v′ is then
the unique path in the tree thus formed joining v and v′. Its length
is the permanent value of L(v′).
Example 12.13
We illustrate this algorithm by constructing a shortest path from A to H in the
weighted graph illustrated in ﬁgure 12.22(a).

662
Applications of Graph Theory
Figure 12.22

The Shortest Path and Travelling Salesman Problems
663
We begin by giving a vertex A permanent label 0. Then we give vertices B, C
and D temporary labels 2, 6 and 7 respectively. Since 2 is the smallest, we make
B’s label permanent, which is indicated by printing the label in bold type. At the
same time we add the edge joining A to B to the tree we began with (which of
course had no edges). This is the stage reached in ﬁgure 12.22(b).
Next we consider the vertices adjacent to B, the most recently permanently
labelled vertex. Since E and F are unlabelled, we label them with the result of
adding L(B) = 2 to the weight of the edge joining B to each vertex; that is,
2 + 7 = 9 and 2 + 15 = 17 respectively. Vertex C is already labelled. However,
the current (temporary) label L(C) is greater than L(B) plus the weight of the
edge joining B to C, so we need to change L(C) to the smaller value 5. Now
C has the smallest temporary label, so we make it permanent and add the edge
joining B and C to the tree. We have now obtained ﬁgure 12.22(c).
Repeating this process for vertex C, we give G the temporary label 8. (Note that
the label of vertex D does not change at this stage since the current label, 7, is
less than L(C) plus 4, the weight of the edge joining C and D.) Now D has the
smallest temporary label, so it is made permanent and the edge joining D to the
tree is added to the tree. At this stage—ﬁgure 12.22(d)—it might appear that, in
adjoining the ﬁrst two edges to the tree, we had made a ‘false start’. It is quite
possible for this to happen although, as we shall see, in this example these edges
will eventually form part of the shortest length path we are seeking.
Repeating the process, we need to consider those vertices (which do not have
permanent labels) adjacent to D, the vertex which has most recently received its
permanent label. The label on vertex F needs to be reduced to 16 which is the
sum of L(D) and the weight of the edge joining D and F. Despite this reduction,
it is G which has the smallest temporary label. This label is made permanent and
the edge joining C to G is added to the tree producing ﬁgure 12.22(e).
A further repetition of the process produces ﬁgure 12.22(f).
This does not
complete the execution of the algorithm as the label on H is still temporary at this
stage. However, one further run through of steps 2 and 3 of the algorithm does
complete the construction of the required tree because H receives a permanent
label this time—see ﬁgure 12.22(g).
Now that the tree is complete we can ﬁnd the shortest path from A to H. It is the
unique path in the tree joining the two vertices, passing through vertices B, C and
G and with total weight 14 (the permanent label for H).
Using Dijkstra’s algorithm 12.7 for a relatively simple graph like the one in the
previous example may seem tedious; you could almost certainly ﬁnd the shortest

664
Applications of Graph Theory
path by trial and error far more quickly. However, it has the advantage of being a
mechanical procedure which will produce the desired path in a more complicated
graph for which trial and error would be a considerable trial of endurance and
extremely prone to error.
The Travelling Salesman Problem
Suppose a travelling salesman needs to visit each of several towns and return to
his starting position. Given the network of roads connecting the various towns on
his itinerary, the travelling salesman’s problem is to ﬁnd a route which minimizes
his total distance travelled. (Such a route may visit some towns more than once.)
The network of roads can be represented by a weighted graph as follows. Each
town is represented by a vertex and each road connecting two towns by an
edge joining the corresponding vertices whose weight is the length of the given
road. A journey which visits each town and ends back at the starting position
is represented by a closed edge sequence in the graph whose associated vertex
sequence contains every vertex (see deﬁnition 11.6). In graph-theoretic terms, the
travelling salesman problem is to ﬁnd such a closed edge sequence of minimum
total weight. We shall assume that the graph is connected so that it is indeed
possible for the salesman to visit every town using the roads of the given network.
It is usual to restate the problem in a slightly different graph-theoretic form. The
graph described above is replaced by a complete graph with one vertex for every
town. (Recall that a complete graph is one in which there is a unique edge joining
every pair of distinct vertices.) An edge is given weight equal to the shortest
distance between the corresponding towns using the roads of the network. These
shortest distances can be found by applying algorithm 12.7 to the original graph.
Figure 12.23 gives an example of the weighted graph representing a network of
roads and the corresponding complete weighted graph. Note that the weight of an
edge in the complete graph can be less than the weight of an edge joining the same
vertices in the original graph. This is because there may be a shorter route than
the single-edge one connecting the two vertices. In ﬁgure 12.23(a) the weight of
the edge joining B to E is 15 which is greater than the weight of the path via C
and D joining these vertices. In the complete graph the weight of the edge joining
B and E is 13, which is the weight of this path via C and D.
From now on we shall work with the complete graph. In order that we can later
recover information about the original, not necessarily complete, graph, we shall
need to keep a note of which paths in the original graph gave rise to the various
edges of the complete graph.

The Shortest Path and Travelling Salesman Problems
665
Figure 12.23
The closed edge sequence visiting every vertex of the graph representing the road
network corresponds to a Hamiltonian cycle in the complete graph. (Hamiltonian
cycles are deﬁned in deﬁnition 11.9.) Thus our formal statement of the problem
is the following.
Travelling Salesman Problem
Given a connected, weighted, complete graph, construct a Hamiltonian
cycle of minimum weight; that is, a minimal Hamiltonian cycle.
Recall from chapter 11 that not every graph has a Hamiltonian cycle; however,
every complete graph with at least three vertices does have one. This follows
from theorem 11.3—it is also easy to see directly. Since our graphs are ﬁnite,
there can be only a ﬁnite number of Hamiltonian cycles which means that there
must be (at least) one which is minimal.
Since the weights of the edges in the complete graph are the shortest distances
between vertices of the original road-network graph, the complete graph must
satisfy the following triangle inequality.
For every triple of distinct vertices (v1, v2, v3),
w(v1, v2) + w(v2, v3) ⩾w(v1, v3)

666
Applications of Graph Theory
where w(vi, vj) denotes the weight of the (unique) edge joining vi and vj†.
The travelling salesman problem has received considerable attention from graph
theorists and others and various algorithms have been given to construct a minimal
Hamiltonian cycle. One reason for the interest in the problem is related to the fact
that all of the known algorithms which solve the problem are ‘computationally
inefﬁcient’. To explain precisely what this means is beyond the scope of this
book; however, we can give an intuitive explanation. To apply any computational
algorithm involves a ‘cost’, which may be measured, for example, in terms of the
number of operations required or the time taken for a computer to execute the
algorithm. The cost will of course depend on the ‘size’ (and nature) of the actual
problem being solved, which is frequently measured by the number n of pieces
of information required to deﬁne the problem.
In 1965 J Edmonds and A Cobham introduced a broad classiﬁcation of algorithms
into those which run in polynomial time and those which run in exponential time.
Roughly speaking, an algorithm is computationally efﬁcient if its ‘cost’ is no
bigger than some power of n, say nk for some integer k, where n is the ‘size’ of
the problem. These are the so-called polynomial time algorithms. Alternatively,
the algorithm is computationally inefﬁcient if its cost is an exponential function
of n; that is, the cost depends on an for some real number a > 1. These, of course,
are the exponential time algorithms. The classiﬁcation of algorithms as efﬁcient
or inefﬁcient corresponds roughly to the practical experience of programmers. A
computationally efﬁcient algorithm is likely to be performed in a reasonable time
even for fairly large values of the size n of the input data. On the other hand
an inefﬁcient algorithm will most likely take too long to be practical for large
problems.
Table 12.1 vividly illustrates how a computationally inefﬁcient algorithm can
become unmanageable, even for problems of relatively small size. Assuming
a processing speed of 10 million operations per second, the approximate
computation times of an efﬁcient algorithm (cost = n2) and an inefﬁcient
algorithm (cost = 2n) are compared for various sizes of problem. (The situation
is not quite as clear cut as the table might suggest—see exercise 12.5.2.)
Returning to the travelling salesman problem, all known algorithms are
computationally inefﬁcient with respect to the number of vertices of the graph;
that is, all known algorithms are exponential time algorithms.
In fact there
† In fact the weight function, considered as a function
w : V × V →R+ ∪{0}
(v1, v2) 7→w(v1, v2)
is a metric on the set V of vertices of the complete graph. (See §8.7 for the deﬁnition of a metric.) Of
course, V is the set of towns on the travelling salesman’s itinerary.

The Shortest Path and Travelling Salesman Problems
667
Table 12.1 Processing times for efﬁcient and inefﬁcient algorithms at a rate of 107
operations per second.
Cost (c) of
Size (n) of problem
algorithm
n = 10
n = 40
n = 70
n2
0.000 01 s
0.000 16 s
0.000 49 s
2n
0.000 1 s
30.5 h
37 436 centuries
are many problems of a similar nature in mathematics, for which all known
algorithms are inefﬁcient. On the theoretical side, much work has been done
on the question of whether the travelling salesman problem is inherently too
complex for an efﬁcient algorithm to exist†.
On the practical side, however,
various efﬁcient algorithms are known which give an approximate solution. In
other words they provide a Hamiltonian cycle whose weight is ‘close to’ the
smallest possible, but may not actually be the best possible.
There is a fairly obvious simple ‘approximate’ algorithm, known as the ‘nearest
neighbour algorithm’. It is a modiﬁcation of the depth-ﬁrst search algorithm given
in §12.4. The algorithm starts any vertex and ‘travels along’ an edge of smallest
weight incident with it to visit a new vertex. At each step in the algorithm we
proceed from the most recently visited vertex to a new vertex by travelling along
an edge of smallest possible weight. (There may be several choices of a ‘minimal
edge’ at each stage.) When all vertices have been visited, we return to the starting
position along the unique edge of the complete graph from the last vertex back
to the ﬁrst. As we noted previously, in order to recover information about the
original (not necessarily complete) road-network graph, we need to have recorded
the paths in the original graph which gave rise to the edges of the complete graph.
This is clearly important if we wish to advise a real-life travelling salesman.
Since it is relatively straightforward, we leave the formal description of this
algorithm as an exercise (12.5.6). The nearest neighbour algorithm is a greedy
algorithm in the sense that it is a nearest vertex which is the one visited at each
stage. (See the footnote to exercise 12.4.2 for an explanation of the term ‘greedy
algorithm’.) It turns out that this algorithm is extremely poor in the following
sense.
Although it will sometimes produce a minimal Hamiltonian cycle, in
† This is still unknown, so we don’t know whether the search for an efﬁcient algorithm is bound
to be fruitless. The problem is one of the famous unsolved problems of mathematics. Brieﬂy, the
travelling salesman problem belongs to a class of problems known as the ‘NP-complete problems’.
The existence of an efﬁcient algorithm for any one of these problems would imply the existence of
efﬁcient algorithms for all other NP problems. Thus, proving that the travelling salesman problem has
an efﬁcient algorithm would be truly signiﬁcant both theoretically and practically. Finding an actual
efﬁcient algorithm would be even better. We ought to point out, however, that most experts believe
that it is very unlikely that any of the NP-complete problems has an efﬁcient algorithm.

668
Applications of Graph Theory
general the cycle produced by the algorithm can have weight considerably greater
than the minimum possible. In fact the performance of the algorithm is about as
bad as one could imagine. Given any positive integer k (no matter how large)
there exist graphs for which the weight of the Hamiltonian cycle produced by the
nearest neighbour algorithm is greater than k times the weight of the minimal
cycle.
We turn instead to another ‘approximate’ algorithm—the ‘nearest insertion
algorithm’. Although more complicated to describe, this algorithm guarantees to
ﬁnd a Hamiltonian cycle with total weight no more than twice the minimum—a
considerable improvement on the nearest neighbour algorithm. Usually, however,
the nearest insertion algorithm will produce a Hamiltonian cycle whose weight is
signiﬁcantly less than twice the minimum.
Algorithm 12.8 (Nearest insertion algorithm)
1.
First choose any vertex. Select an edge e with smallest weight
incident with the initial vertex and let C be the edge sequence: e, e.
We regard C as our starting ‘cycle’, although strictly speaking it is
not a cycle as it repeats an edge.
2.
Select an edge with the smallest weight which joins a vertex in C
to one not in C and let v be the vertex not in C incident with this
edge. (There may be several possible choices for the edge.)
3.
The next step is to enlarge the cycle to include the chosen vertex
v. To decide how to insert v, consider all pairs u1, u2 of adjacent
vertices in C and select a pair for which the expression
I = w(u1, v) + w(v, u2) −w(u1, u2)
is a minimum, where w(u, v) denotes the weight of the edge
joining u and v. This expression I represents the increase in the
total weight of C when it is enlarged to include the vertex v. We
enlarge C to include v by adjoining the edges connecting u1 and v
and connecting v and u2, and deleting the edge joining u1 and u2.
4.
Repeat steps 2 and 3 until the cycle includes all the vertices of the
graph.

The Shortest Path and Travelling Salesman Problems
669
The basic step in the nearest insertion algorithm is to take a cycle in the graph
and enlarge it to include a vertex which is ‘closest’ to the given cycle. This step
is then repeated until all vertices are included in the cycle. The algorithm can be
applied to any complete weighted graph satisfying the triangle inequality.
Example 12.14
We illustrate the nearest insertion algorithm for the weighted graph in ﬁgure 12.24
(which satisﬁes the triangle inequality).
Figure 12.24
We shall adopt the convention that the (unique) edge joining vertices X and Y is
denoted eXY or eY X. To perform step 1, we ﬁrst choose the vertex labelled A in
the diagram. (We could, of course, have chosen any of the vertices.) The edge
eAF is the edge incident with A which has smallest weight, so the ﬁrst ‘cycle’ is
eAF, eFA from A to F and back to A.
The edge with smallest weight which is incident with either A or F is eAB, so B is
the ﬁrst vertex to be inserted. The shortest way in which B can be inserted using
a cycle is to go from A to B and back via F. This gives the cycle eAB, eBF, eFA
shown in ﬁgure 12.25(a).
The vertex which is nearest to a vertex of this cycle is C, so we need to ﬁnd the
best way to insert it. The values of I for the three edges of the current cycle are
I(eAB) = 6 + 9 −4 = 11
I(eBF) = 6 + 12 −6 = 12
I(eFA) = 12 + 9 −3 = 18.
We therefore enlarge the cycle by removing the edge eAB and inserting in its place
the edges eAC, eCB. This gives the cycle shown in ﬁgure 12.25(b).
Repeating this process twice more, we enlarge the cycle ﬁrstly to include D
and then to include E as shown in ﬁgures 12.25(c)–(d).
The ﬁnal cycle,

670
Applications of Graph Theory
Figure 12.25
eAC, eCD, eDE, eEB, eBF, eFA, is the required Hamiltonian cycle with total weight
9 + 5 + 4 + 10 + 6 + 3 = 37.
This example illustrates the fact that the nearest insertion algorithm may not
produce a minimal Hamiltonian cycle. The graph in fact has a unique minimal
Hamiltonian cycle—eAB, eBC, eCD, eDE, eEF, eFA—with total weight 29.
Exercises 12.5
1.
Apply Dijkstra’s algorithm 12.7 to obtain the shortest path from v to w in
each of the following weighted graphs.

The Shortest Path and Travelling Salesman Problems
671
2.
Assuming an operation speed of 10 million operations per second (107
operations s−1), ﬁnd the processing times of two algorithms, one with
cost = 105n6 and the other with cost = 10−92n, for n = 10, 40 and 70.
(These calculations show that calling polynomial time algorithms
‘efﬁcient’ and exponential time algorithms ‘inefﬁcient’ is only a rough
guide to their expected performance.)
3.
(a)
Modify the shortest path algorithm 12.7 to produce the shortest
directed path connecting two given vertices in a weighted digraph.
What will be the result of applying your algorithm if there is no
directed path in the graph connecting the two speciﬁed vertices?
(b)
Apply your algorithm to ﬁnd the shortest path from v to w in each
of the following weighted digraphs.

672
Applications of Graph Theory
4.
A weighted graph has vertex set {v1, v2, . . . , v10}, edge set {e1, e2, . . . ,
e16} and incidence matrix




























1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
1
0
0
1




























.
(The incidence matrix of a graph is deﬁned in exercise 11.1.16.)
The weights of the edges are as follows.
Edge:
e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11 e12 e13 e14 e15 e16
Weight:
4
8
2
3
7
1
2
3
2
6
5
14
9
7
3
15
Draw a diagram of the graph and ﬁnd the shortest path (i) from v1 to v8,
and (ii) from v4 to v10.
5.
Apply the nearest insertion algorithm 12.8 to each of the following
complete weighted graphs, with v0 as the choice of initial vertex. State
the weight of the resulting Hamiltonian cycle. You may assume that the
triangle inequality holds in each case.

Networks and Flows
673
Repeat the algorithm with other choices of initial vertex. Do you always
obtain the same cycle?
6.
Write down in detail the nearest neighbour algorithm (outlined on
page 667) for ﬁnding a ‘near’ minimal Hamiltonian cycle.
Apply the nearest neighbour algorithm to the two graphs in the previous
question and compare the cycles obtained with those obtained using the
nearest insertion algorithm.
7.
Each of the following weighted graphs represents a network of roads.
For each graph, construct the complete weighted graph which shows the
shortest distances between the various vertices and perform the nearest
insertion algorithm on the complete graph to ﬁnd a ‘near’ minimal
Hamiltonian cycle
12.7
Networks and Flows
In this section we outline two applications of weighted digraphs. The ﬁrst of these
is to scheduling problems. Suppose a project (e.g. a construction project of some
kind) involves the completion of several tasks. Each task takes a certain length of
time and some tasks need to be completed before others can begin. The problem
is to schedule the various activities to minimize the total time taken to complete
the project.
The situation can be modelled using a weighed digraph as follows. Each edge
of the digraph represents an activity and its weight represents the time taken to

674
Applications of Graph Theory
complete the activity. The vertices represent phases of the project, each phase
being the completion of one or more activity. (There is an alternative way of
modelling the situation using a weighted digraph in which the vertices represent
activities. An edge is drawn from vertex v to vertex w if activity v must precede
activity w and the edge is given a weight corresponding to the duration of activity
v. Although we shall not consider this type of ‘activity network’, it requires much
the same analysis that we will develop below—earliest times, latest times, critical
path, and so on.)
Example 12.15
Figure 12.26 represents a project which has 10 activities, A1, A2, . . . , A10, whose
completion times (in days, say) are indicated as the weights of the edges. The
direction of each edge is from start to ﬁnish of the relevant task. The vertices S
and F represent the start and ﬁnish respectively of the project.
The digraph indicates that A1 must be completed before A4 or A5 can begin, both
A2 and A5 must be completed before either A7 or A9 can commence, etc.
Figure 12.26
Figure 12.26 is called the scheduling network or activity network for the
project. Notice that there is only one vertex (the start S) which has in-degree
0, and only one vertex (the ﬁnish F) which has out-degree 0.
Deﬁnitions 12.10
(i)
Let D be a weighted digraph. A source is a vertex with in-degree
0 and a sink is a vertex with out-degree 0.
(ii)
A network is a connected, weighted digraph (with no loops) which
has a single source and a single sink.

Networks and Flows
675
To avoid repetition we shall assume that all digraphs in this section are connected
and contain no loops, although multiple edges are allowed. The restriction that
a network should have only one source and one sink is not a substantial one.
Suppose D is a weighted digraph which has several sources and/or sinks. We
can deﬁne a network N by adding two vertices v and w to D, joining v to each
source with a directed edge and joining each sink to w with a directed edge. This
is illustrated in ﬁgure 12.27; N has a single source v and a single sink w. The
weights which are appropriate to assign to these additional edges depend on what
the network represents. In scheduling problems the additional edges would be
given weight 0, so that the completion time for the whole project is not artiﬁcially
increased.
Figure 12.27
Without going into too many generalities, we now consider the scheduling
network shown in ﬁgure 12.26 and indicate the kind of analysis which can be
performed. The ﬁrst task will be to ﬁnd the minimum completion time for the
project.
To do this we work through the network, labelling each vertex v with the number
E(v) which represents the earliest time by which an activity starting at v can
commence. Firstly give the source the label E(S) = 0. Next consider the set
of those vertices v such that every edge ending in v begins at an edge which has
already been labelled. For each such vertex, set E(v) equal to the maximum of
the values E(u) + w(e), where e is an edge from u to v. Repeat this step until
all vertices have been labelled. We need to repeat this step four times in order to
label all the vertices in ﬁgure 12.26. The vertices labelled at each stage are shown
in ﬁgure 12.28.
The value E(F), the label on the sink, is the earliest time at which an activity
beginning at F could commence. Of course, there are no activities which begin at
F, so E(F) is the minimum completion time for the whole project: 22 days in this
example.
The next stage in the analysis looks at how to schedule the various tasks in order
to achieve this minimum completion time. In a similar manner to the assignment

676
Applications of Graph Theory
Figure 12.28
of the labels E(v) to the vertices, we next determine for each vertex the latest time
at which the activities beginning at the vertex must be started in order to achieve
the minimum completion time.
To do this we work ‘backwards’ through the network from the sink assigning
labels L(v) to each vertex in turn. The number L(v) represents the latest time by
which the activities beginning at v must be started. Firstly, label the sink with the
value E(F), the minimum completion time—22 in our example. Next consider
the set of all vertices v such that each edge beginning at v ends at a vertex which
is already labelled. Set L(v) equal to the minimum of the values L(u) −w(e)
for all the edges e from v to u. Repeat this step until all vertices have been
labelled. Again, in our example, four steps are required to label all the vertices:
the complete set of labels is shown in ﬁgure 12.29(a).
From E(v) and L(v) we now calculate the ﬂoat time
ﬂoat(v) = L(v) −E(v)
for each vertex, which represents the maximum possible delay which can occur
at the point without increasing the overall completion time. These ﬂoat times are
given in ﬁgure 12.29(b). Notice that there is a directed path from source to sink
whose weight equals the minimum completion time and which is the longest path
from source to sink—see ﬁgure 12.29(c). This is called a critical path for the
network. All activities represented by edges of a critical path must commence
without any delay if the minimum completion time is to be achieved. Although it
is not entirely obvious, every network will have at least one critical path, and may

Networks and Flows
677
Figure 12.29 (a) Latest start times; (b) ﬂoat times; (c) critical path.
have several. All of the vertices on any critical path must have zero ﬂoat time.
The converse is not true—there may be paths passing only through vertices with
zero ﬂoat time which are not the longest. However, if there is a path from source
to sink which passes through all vertices with zero ﬂoat time, it must be critical.
Flows and Cuts
For our last application we consider networks which represent the ‘ﬂow’ of a
commodity through a series of ‘pipelines’. The commodity could actually be
a ﬂuid ﬂowing through a network of pipes, but it need not be. The edges just
represent parts of a transportation network for the particular commodity and the
weights represent their maximum capacities. These could be rail, road or air
links for example, or even electrical or ﬁbre-optic cables if the ‘commodity’ being
transported were digital signals.
If the digraph is not a network because there is more than one source and/or sink,
we ﬁrst create a network as illustrated in ﬁgure 12.27. In order not to alter the
total rate of ﬂow of the commodity through the system, each edge from the new
source v to a (previous) source would be given a weight equal to the sum of the
weights of the edges beginning at that previous source. Similarly an edge from a
(previous) sink to the new sink w would be given a weight equal to the sum of the
weights of the edges ending at the previous sink.

678
Applications of Graph Theory
The problem is, given a network, to determine the maximum possible ﬂow of the
commodity from the source to the sink. To make some headway with this problem
we ﬁrst need some deﬁnitions.
Deﬁnitions 12.11
Let N be a network.
(i)
A ﬂow in N is a function f : EN →R+ ∪{0}. For each directed
edge e ∈EN, the non-negative real number f(e) is called the ﬂow
in e.
(ii)
A ﬂow is conservative if, for every vertex v except the source and
sink, the ﬂow into v equals the ﬂow out from v. More formally,
X
δ(e)=(−,v)
f(e) =
X
δ(e)=(v,−)
f(e)
where the summation on the left is over all edges with ﬁnal vertex v
and the summation on the right is over all edges with initial vertex
v.
(iii)
A ﬂow is feasible if, for each edge e ∈EN, f(e) ⩽w(e). In other
words, the ﬂow in e is no greater than the weight (capacity) of e.
(iv)
The value of a ﬂow is the total ﬂow into the sink. If w is the sink,
the value of a ﬂow is the sum of the ﬂows in the edges with ﬁnal
vertex w:
X
δ(e)=(−,w)
f(e).
We shall henceforth assume that all ﬂows are both conservative and feasible.
‘Conservative’ means that there is no ‘leakage’ in the network; apart from the
source and the sink, all the material ﬂowing into any vertex must ﬂow out of it.
The feasibility of a ﬂow ensures that the capacities (weights) of the edges are not
exceeded. The value of a (conservative) ﬂow could equally well be deﬁned as
the sum of the ﬂows in the edges with initial vertex equal to the source; whatever
ﬂows out of the source must ﬂow into the sink as nothing is lost in the network.
With the notation of deﬁnition 12.11, the maximum ﬂow problem can be stated
as follows.

Networks and Flows
679
Given a network, ﬁnd a (conservative, feasible) ﬂow which has maximum
possible value. Such a ﬂow is called maximal.
It is easy to see that there may be more than one maximal ﬂow. For example, the
maximum value of any ﬂow through the network in ﬁgure 12.30(a) is 3. There are
various ways in which this can be achieved: the ﬂow in both e1 and e4 must be 3,
but the ﬂows in e2 and e3 can be chosen in a variety of ways so that their sum is 3.
In other words, the maximum value of all possible ﬂows is 3, but there are various
different maximal ﬂows which achieve this maximum. Two different maximal
ﬂows are shown in ﬁgures 12.30(b) and (c). We have adopted a convention in
representing ﬂows, which is to show the ﬂow in each edge as a number printed
in bold type. (This is to avoid confusing the ﬂow in an edge with the capacity of
that edge, which is printed in the usual typeface. When attempting ﬂow problems
you are strongly advised to adopt a convention which distinguishes the weight of
an edge from the ﬂow in that edge. A convenient way of doing this is to circle the
numbers which represent ﬂows in an edge and leave the weights uncircled.)
The maximum value of a ﬂow in a network is closely linked with the idea of a
‘cut’ of a network, which we now describe. Intuitively, a cut can be thought of as
a set of edges which, if ‘blocked’, would completely stop the ﬂow from source to
sink, but if any one edge were unblocked the ﬂow could get through again. The
network in ﬁgure 12.30 has three different cuts: {e1}, {e2, e3} and {e4}.

680
Applications of Graph Theory
Figure 12.30
Deﬁnitions 12.12
Let N be a network.
(i)
A cut of N is a set of edges which, when removed from N,
produces a digraph with two components, one containing the
source and the other containing the sink.
(ii)
The capacity of a cut is the sum of the weights of those of its edges
which are directed from the component of N containing the source
to the component containing the sink. (Those edges directed from
the component containing the sink to that containing the source are
ignored when calculating the capacity.)
(iii)
A cut is minimal if its capacity is less than or equal to the capacity
of any other cut.
Figure 12.31 shows a network with source S and sink T.
The set of edges
{AD, DB, BE, EC, CH} is a cut, because removing them from the network
separates it into two components, one containing the source and one the sink. Of
the edges in the cut, AD, BE, CH are directed from the component containing
the source to that containing the sink; the edges DB and EC are directed from
the component containing the sink to that containing the source. Therefore the
capacity of the cut {AD, DB, BE, EC, CH} is 12 + 10 + 9 = 31.

Networks and Flows
681
Figure 12.31
The capacities of the three cuts of the network in ﬁgure 12.30 are as follows
Cut:
{e1}
{e2, e3}
{e4}
Capacity:
4
14
3
There is a unique minimal cut in this example —{e4} with capacity 3—although
in general there may be several minimal cuts of a given network.
Note that
the capacity of the minimal cut equals the value of a maximal ﬂow through the
network. In fact this is true for any network. This is known as the ‘max-ﬂow
min-cut theorem’; it was proved in 1955 by L R Ford and D R Fulkerson. As our
aim here is to give only a brief outline of the theory of ﬂows, we shall omit the
proof.
Theorem 12.1 (The max-ﬂow min-cut theorem)
In a network, the value of any maximal ﬂow is equal to the capacity of any
minimal cut.
It should be reasonably clear that the value of any ﬂow cannot be greater than
the capacity of any cut. The essential part of the proof of theorem 12.1 involves
showing that there is a ﬂow whose value equals the capacity of a minimal cut.
We complete this section by outlining a method for ﬁnding a maximal ﬂow. We
shall consider the network shown in ﬁgure 12.31. The basic idea is to begin
with some ﬂow and, if it is not already maximal, improve it. It is relatively
straightforward to ﬁnd some ﬂow. The network is shown again in ﬁgure 12.32(a).
Suppose we begin with the ﬂow shown in ﬁgure 12.32(b) which has value 15 and
is fairly clearly not maximal.

682
Applications of Graph Theory
Figure 12.32
To improve the ﬂow, we really mean replace it with a new ﬂow which has greater
value. To do this, ﬁrst look for a directed path from the source to the sink with the
property that the ﬂow in every edge of the path is strictly less than its weight. An
example of such a path is indicated in ﬁgure 12.32(c). (There are other choices
of such a path.) For the given path, calculate the minimum value of w(e) −f(e)
for its edges. It is possible to increase the ﬂow in every edge of the path by this
amount giving a new ﬂow with a larger value than the original. The minimum
value of w(e) −f(e) for the edges of the path in ﬁgure 12.32(c) is 2. When
the ﬂow in each edge of this path is increased by 2, we obtain the ﬂow shown in
ﬁgure 12.32(d), which has value 17.
The result of several repeats of this process produces the ﬂow, with value 24,
shown in ﬁgure 12.33(a). For this ﬂow there is no directed path from source to
sink with the property that the ﬂow in each edge is strictly less than the capacity
of the edge. We may, therefore, be tempted to conclude that this ﬂow is maximal.
Unfortunately it is not, but to improve on the ﬂow we need to be slightly more
devious.
Consider the path shown in ﬁgure 12.33(b). This is not a directed path from
source to sink; strictly speaking we need to regard it as a path in the underlying
undirected graph. The ﬂow in each of the three ‘forward’ edges is strictly less

Networks and Flows
683
Figure 12.33
than their capacity, and the ﬂow in the ‘backward’ edge is positive. If we increase
the ﬂow in each forward edge by 1 (the minimum value of w(e) −f(e) for its
forward edges) and reduce the ﬂow in the backward edge by 1, the resulting ﬂow
is still conservative and the value has been increased by 1. The resulting ﬂow,
with value 25, is shown in ﬁgure 12.33(c).
There are no further (undirected) paths from source to sink of this type where the
ﬂow in each forward edge can be increased and not exceed its capacity, and the
ﬂow in each backward edge can be reduced and not become negative. This means
that the ﬂow shown in ﬁgure 12.33(c) is indeed maximal.
In this example it is fairly easy to see that there are no other paths from source
to sink which allow the ﬂow to be increased. For larger and more complicated
networks, however, it may be difﬁcult to determine with conﬁdence that there are
no other such paths. This is where the max-ﬂow min-cut theorem is useful. Since
the edges of a cut are those which, if blocked, would completely stop the ﬂow, the
value of any ﬂow cannot exceed the capacity of any cut. Therefore the value of a
ﬂow can equal the capacity of a cut only when the ﬂow is maximal and the cut is
minimal.

684
Applications of Graph Theory
If we can ﬁnd a cut whose capacity equals 25, the value of the ﬂow, then we can
deduce that our ﬂow is indeed maximal (and, of course, that the cut is minimal).
Such a cut is shown in ﬁgure 12.33(d). The max-ﬂow min-cut theorem then
guarantees that the ﬂow in ﬁgure 12.33(c) is maximal. It is not unique, however.
We leave it as an exercise to ﬁnd another ﬂow which also has value 25.
Exercises 12.6
1.
For each of the scheduling networks shown below, determine the
minimum completion time, the ﬂoat times of each vertex and a critical
path.
2.
For each of the networks below, ﬁnd a maximal ﬂow, and prove that your
ﬂow is maximal by ﬁnding a (minimal) cut whose capacity equals the
value of your ﬂow.

Networks and Flows
685
3.
For each of the following weighted digraphs, add a unique source and
sink as illustrated in ﬁgure 12.27.
(a)
If the digraphs represent scheduling problems, give the new edges
appropriate weights, and ﬁnd the minimum completion time and a
critical path for the resulting network.
(b)
If the digraphs represent ﬂow problems, give the new edges
appropriate weights, and ﬁnd a maximal ﬂow and a minimal cut
in the resulting network.
4.
Show that, in any network, the set of edges incident with the sink is a cut,
and similarly that the set of edges incident with the source is also a cut.
5.
Explain why a scheduling network cannot have any directed cycle.
6.
A certain project requires the completion of 16 activities: A1, A2, . . . ,
A16.
The following table gives the time (in days) required and the
prerequisites for each activity.

686
Applications of Graph Theory
Activity
Time required
Prerequisite activities
A1
3
None
A2
6
None
A3
5
None
A4
6
None
A5
4
A1, A2
A6
8
A1, A2
A7
2
A3
A8
5
A6, A7
A9
10
A5, A8
A10
7
A6, A7
A11
3
A6, A7
A12
3
A3
A13
4
A4, A12
A14
6
A4, A12
A15
5
A11, A13
A16
8
A14
(i)
Draw a scheduling network for the project.
(ii)
Find the minimum completion time and a critical path for the
network.

References and Further Reading
The following is a list of books and papers referred to in the text as well as sources
of further reading. We have grouped the books into subject areas which relate
approximately to the chapters of the book. Those under the heading ‘General
Discrete Mathematics’ cover material found in several of our chapters at a roughly
comparable level, but frequently with a different emphasis.
General Discrete Mathematics
Albertson M O and Hutchinson J P 1988 Discrete Mathematics with Algorithms
(New York: Wiley)
Gersting J L 2006 Mathematical Structures for Computer Science 5th edn (New
York: Freeman)
Grimaldi R P 2004 Discrete and Combinatorial Mathematics:
an Applied
Introduction 5th edn (Boston: Addison-Wesley)
Grossman P 2002 Discrete Mathematics for Computing 2nd edn (New York:
Macmillan)
Johnsonbugh R 2004 Discrete Mathematics 6th edn (Englewood Cliffs, NJ:
Prentice-Hall)
Kolman B, Busby R C and Ross S C 2004 Discrete Mathematical Structures
5th edn (Englewood Cliffs, NJ: Prentice-Hall)
Penner R C 1999 Discrete Mathematics: Proof Techniques and Mathematical
Structures (Singapore: World Scientiﬁc)
Piff M 1991 Discrete Mathematics: an Introduction for Software Engineers
(Cambridge: Cambridge University Press)
Ross K A and Wright C R B 2003 Discrete Mathematics 5th edn (Englewood
Cliffs, NJ: Prentice-Hall)
687

688
References and Further Reading
Logic and Proof
Ben-Ari M 2001 Mathematical Logic for Computer Science 2nd edn (Berlin:
Springer)
Franklin J and Daoud M 1996 Proof in Mathematics: an Introduction (Quaker
Hill Press)
Hamilton A G 1988 Logic For Mathematicians revised edn (Cambridge:
Cambridge University Press)
Solow D 2004 How to Read and Do Proofs: an Introduction to Mathematical
Thought Processes 4th edn (New York: Wiley)
Stirling D S G 1997 Mathematical Analysis and Proof (Chichester: Horwood
Publishing)
Vellerman D J 2006 How to Prove it: a Structured Approach 2nd edn (Cambridge:
Cambridge University Press)
Wolf R S 1998 Proof, Logic and Conjecture: The Mathematician’s Toolbox (NY:
W H Freeman)
Sets, Relations and Functions
Blyth T S and Robert E F 1984a Algebra through Practice: Book 1: Sets,
Relations and Mappings (Cambridge: Cambridge University Press)
Ciesielski K 1997 Set Theory for the Working Mathematician (Cambridge:
Cambridge University Press)
Devlin K J 2003 Sets, Functions and Logic:
an Introduction to Abstract
Mathematics 3rd edn (London: Chapman and Hall)
Fejer P A and Simonvici D A 1991 Mathematical Foundations of Computer
Science: Volume 1: Sets, Relations and Induction (Berlin: Springer)
Hamilton A G 1982 Numbers, Sets and Axioms: the Apparatus of Mathematics
(Cambridge: Cambridge University Press)
Nissanke N 1999 Introductory Logic and Sets for Computer Scientists (Reading,
MA: Addison-Wesley-Longman)
Stoll R R 1979 Set Theory and Logic (New York: Dover)
Database Theory
Codd E F 1970 A relation model for large shared data banks Commun. ACM 13
377–87
Connolly T and Begg C 2004 Database Systems: a Practical Approach to Design,
Implementation and Management 4th edn (Reading, MA: Addison-Wesley)

Matrices and Linear Equations
689
Date C J 2006 An Introduction to Database Systems 8th edn (Reading, MA:
Addison-Wesley)
Eaglestone B 1991 Relational Databases (Cheltenham: Stanley Thornes)
Elmasri R and Navathe S 2004 Fundamentals of Database Systems 4th edn
(Reading, MA: Addison-Wesley)
Levene M and Loizou G 1999 A Guided Tour of Relational Databases and Beyond
(Berlin: Springer)
Matrices and Linear Equations
Anton H 2006 Elementary Linear Algebra 9th edn abridged (New York: Wiley)
Blyth T S and Robertson E F 1984b Algebra through Practice: Book 2: Matrices
and Vector Spaces (Cambridge: Cambridge University Press)
Johnson L W, Dean Reiss R and Arnold J T 2001 Introduction to Linear Algebra
5th edn (Reading, MA: Addison-Wesley)
Kaye R and Wilson R 1998 Linear Algebra (Oxford: Oxford University Press)
Penney R C 2008 Linear Algebra: Ideas and Applications 3rd edn (New York:
Wiley)
Algebraic Structures
Asche D 1989 Introduction to Groups (Bristol: Institute of Physics Publishing)
Deskins W E 1996 Abstract Algebra (New York: Dover)
Foldes S 1994 Fundamental Structures of Algebra and Discrete Mathematics
(New York: Wiley)
Fraleigh J B 2002 A First Course in Abstract Algebra 7th edn (Reading, MA:
Addison-Wesley)
Gallian J A 2002 Contemporary Abstract Algebra 5th edn (Boston, MA:
Houghton Mifﬂin)
Green J A 1988 Sets and Groups: a First Course in Algebra (Dordrecht: Kluwer
Academic)
Ledermann W and Weir A J 1996 Introduction to Group Theory 2nd edn
(Reading, MA: Addison-Wesley)
Rotman J J 1995 An Introduction to the Theory of Groups 4th edn (Berlin:
Springer)

690
References and Further Reading
Coding Theory
Berlekamp E R 1984 Algebraic Coding Theory revised edn (Laguna Hills, CA:
Aegean Park Press)
Kabatiansky G, Krouk E and Semenov S 2005 Error Correcting Coding
and Security for Data Networks:
Analysis of the Superchannel Concept
(Chichester: Wiley)
Lin S and Costello D J Jr 2004 Error Control Coding : Fundamentals and
Applications 2nd edn (Englewood Cliffs NJ: Prentice Hall)
MacWilliams F J 1993 The Theory of Error Correcting Codes (Amsterdam:
North-Holland)
Pless V 1998 Introduction to the Theory of Error Correcting Codes 3rd edn (New
York: Wiley)
Sweeney P 2002 Error Control Coding: an Introduction (Englewood Cliffs, NJ:
Prentice-Hall)
Number Theory
Burn R P 1997 A Pathway into Number Theory 2nd edn (Cambridge: Cambridge
University Press)
Berton D M 2007 Elementary Number Theory 6th edn (McGraw Hill)
Jones G A and Jones J M 1998 Elementary Number Theory (Berlin: Springer)
Shapiro H N 2008 Introduction to the Theory of Numbers (New York: Dover)
Rosen K H 1992 Elementary Number Theory and its Applications 3rd edn
(Reading, MA : Addison-Wesley)
Boolean Algebra, Logic and Switching Circuits
Gregg J 1998 Ones and Zeros: Understanding Boolean Algebra, Digital Circuits,
and the Logic of Sets (New York: IEEE)
Maxﬁeld C and Waddell P 2003 Bebop to the Boolean Boogie 2nd edn (Burlington
MA: Newnes)
Raﬁquzzaman M 2005 Fundamentals of Digital Logic and Microcomputer
Design 5th edn (Hoboken NJ: Wiley)
Whitesitt J E 1995 Boolean Algebra and its Applications (New York: Dover)

Graph Theory and Applications
691
Graph Theory and Applications
Appel H and Haken W 1986 The four-colour proof sufﬁces Math. Intell. 8 10–20
Biggs N L 2002 Discrete Mathematics 2nd edn (Oxford: Clarendon)
Biggs N L, Lloyd E K and Wilson R J 1986 Graph Theory 1736–1936 revised edn
(Oxford: Clarendon)
Gould R 1988 Graph Theory (Menlo Park, CA: Cummings)
Harary F 1969 Graph Theory (Reading, MA: Addison-Wesley)
Lawler E L, Lenstra J K, Rinnooy Kan A H G and Shmoy D B (eds) 1985 The
Travelling Salesman Problem. A Guided Tour of Combinatorial Optimisation
(New York: Wiley)
Saaty T L and Kainen P G 1986 The Four Colour Problem: Assaults and Conquest
(New York: Dover)
Trudeau R J 1993 Introduction to Graph Theory (New York: Dover)
Wilson R J and Watkins J J 1990 Graphs: an Introductory Approach—a First
Course in Discrete Mathematics (New York: Wiley)
Wilson R J 1996 Introduction to Graph Theory 4th edn (Reading, MA: Addison-
Wesley-Longman)
Miscellaneous
Hallett M 1984 Cantorian Set Theory and the Limitation of Size (Oxford: Oxford
University Press)
Hardy G H 1992 A Mathematician’s Apology New edition (Cambridge:
Cambridge University Press)
Lakatos I 1976 Proof and Refutations: the Logic Of Mathematical Discovery
(Cambridge: Cambridge University Press)

Hints and Solutions to
Selected Exercises
Chapter 1
Exercises 1.1
1.
(i)
Max is not sulking and today is my birthday.
(iii)
If Max is not sulking then today is my birthday.
2.
(ii)
If and only if Jo shouts and Sally cries, then Mary laughs.
(iv)
Mary laughs or Sally doesn’t cry or Jo doesn’t shout.
3.
(i)
p →¯q.
(iii)
¯r ∧(p →¯q).
5.
(i)
(ii)
(iv)
(vi)
p
q
¯p →q
¯q ∧p
(p →q) ⊻¯q
(¯p ∧q) ⊻(p ∨¯q)
T
T
T
F
T
T
T
F
T
T
T
T
F
T
T
F
T
T
F
F
F
F
F
T
692

Hints and Solutions to Selected Exercises
693
7.
(ii)
(iv)
p
q
r
(p ⊻r) ∧¯q
(p →(¯q ∨¯r)
T
T
T
F
F
T
T
F
F
T
T
F
T
F
T
T
F
F
T
T
F
T
T
F
T
F
T
F
F
T
F
F
T
T
T
F
F
F
F
T
Exercises 1.2
1. Tautology
2. Neither
3. Tautology
4. Tautology
5. Contradiction
6. Neither
7. Contradiction
8. Tautology
9. Tautology
10. Neither.
Exercises 1.3
2.
p
q
p ∧q
p →¯q
T
T
T
T
T
F
F
F
F
T
F
F
F
F
F
F
Since the last two columns of the truth table are identical, we can
conclude that (p ∧q) ≡(p →¯q).
5.
The truth table for ¯q →¯p and for p →q is as follows.
p
q
¯q →¯p
p →q
T
T
T
T
T
F
F
F
F
T
T
T
F
F
T
T

694
Hints and Solutions to Selected Exercises
Whenever ¯q →¯p is true (rows 1, 3 and 4), p →q is true so that (¯q →
¯p) ⊢(p →q). (In fact (¯q →¯p) ≡(p →q).)
6.
(iv)
The truth table is as follows.
p
q
r
(p →q) ∧(p ∨r)
q ∨r
T
T
T
T
T
T
T
F
T
T
T
F
T
F
T
T
F
F
F
F
F
T
T
T
T
F
T
F
F
T
F
F
T
T
T
F
F
F
F
F
Whenever (p →q) ∧(p ∨r) is true (rows 1, 2, 5 and 7), q ∨r is
true. Hence (p →q) ∧(p ∨r) ⊢(q ∨r).
(vi)
p
q
¯q
p ∨q
(p ∨q) ∧¯q
T
T
F
T
F
T
F
T
T
T
F
T
F
T
F
F
F
T
F
F
In each of the cases where (p ∨q) ∧¯q is true, p is also true. Hence
[(p ∨q) ∧¯q] ⊢p.
Exercises 1.4
1.
(i)
(p ∧p) ∨(¯p ∨¯p) ≡p ∨¯p
(Idem)
≡t
(Comp).
(iii)
p →q ≡¯p ∨q
(Imp)
≡¯p ∨¯¯q
(Inv)
≡p ∧¯q
(De M).
(v)
This can be proved using (in this order): De Morgan’s law, an
involution law, a commutative law and, ﬁnally, an idempotent law.

Hints and Solutions to Selected Exercises
695
Exercises 1.5
1.
Deﬁne the following:
p : You gamble.
q : You’re stupid.
The premises are then p →q, ¯q and the conclusion is ¯p. A truth table
shows that the compound proposition [(p →q) ∧¯q] →¯p is a tautology.
Hence the argument is valid.
2.
Deﬁne the following:
p : I leave college.
q : I get a job in a bank.
The compound proposition [(p →q) ∧¯p] →¯q is not a tautology and
therefore the argument is not valid.
3.
Valid (regardless of whether ‘either .. . or ... ’
is interpreted as an
inclusive or exclusive disjunction).
4.
Not valid.
5.
Valid.
6.
Not valid.
7.
Valid.
8.
Valid.
9.
Valid.
10.
Not valid.
Exercises 1.6
1.
(i)
1.
(p ∧q) →(r ∧s)
(premise)
2.
p
(premise)
3.
q
(premise)
4.
p ∧q
(3, 4 Conj)
5.
r ∧s
(1, 4. MP)
6.
s ∧r
(5. Comm)
7.
s
(6. Simp)

696
Hints and Solutions to Selected Exercises
(iii)
A
formal
proof
can
be
obtained
using
(in
this
order):
simpliﬁcation, modus ponens and disjunctive syllogism.
(v)
A formal proof can be obtained using (in this order):
a
commutative law, simpliﬁcation twice and modus ponens.
(vii)
1.
(p ∨q) ∧(q ∨r)
(premise)
2.
¯q
(premise)
3.
p ∨q
(1. Simp)
4.
q ∨p
(3. Comm)
5.
p
(2, 4. DS)
6.
(q ∨r) ∧(p ∨q)
(1. Comm)
7.
q ∨r
(6. Simp)
8.
r
(2, 7. DS)
9.
p ∧r
(5, 8 Conj)
2.
(i)
Symbolise the premises as follows.
p:
The murder was committed by A.
q:
The murder was committed by B.
r:
The murder was committed by C.
A formal proof of the validity of the argument is then given as
follows.
1.
p ∨(q ∧r)
(premise)
2.
(p ∨q) ∧(p ∨r)
(1. Dist)
3.
(p ∨r) ∧(p ∨q)
(2. Comm)
4.
p ∨r
(3. Simp)
(iii)
A formal proof of validity uses the involution law, the material
implication law and ﬁnally hypothetical syllogism.
(v)
We symbolise the premises:
p:
People are happy.
q:
People are truthful.
A formal proof of the validity of the argument is then given as
follows.
1.
p ↔q
(premise)
2.
p ∧q
(premise)
3.
(p ∧q) ∨(¯p ∧¯q)
(1. Equiv)
4.
(¯p ∧¯q) ∨(p ∧q)
(3. Comm)
5.
¯p ∧¯q
(4. Simp)

Hints and Solutions to Selected Exercises
697
Exercises 1.7
1.
(ii)
B(m) →C(s)
(iv)
∀x[C(x) →F(x)]
(v)
∀x[{C(x) ∧¬B(x)} →F(x)].
2.
(i)
The proposition is symbolized by ∀x[B(x) →C(x)] where the
predicates are B(x) : x is a baby and C(x) : x cries a lot. If we
deﬁne the universe of discourse to be ‘babies’, the proposition may
be shortened to ∀xC(x).
(iii)
We deﬁne S(x) : x is a student and G(x) : x can write a good
essay. The proposition is then
∃x[S(x) ∧¬G(x)].
Alternatively, if we deﬁne the universe of discourse to be
‘students’, the proposition is
∃x[¬G(x)].
(v)
Universe of discourse: people.
U(x) : x has had a university education.
P(x) : x lives in poverty.
∃x[U(x) ∧P(x)].
(vii)
Universe of discourse: people.
F(x) : x is my friend.
N(x) : x believes in nuclear disarmament.
∀x[F(x) →N(x)].
(ix)
Universe of discourse: people in the building.
F(x) : x set off the ﬁre alarm.
B(x) : x left the building.
∃xF(x) ∧∀xB(x).
3.
(i)
Universe of discourse for each variable: people.
L(x, y) : x loves y.
∀x∃yL(x, y).
(ii)
Universe of discourse and predicate as in (i).
∃x∀yL(x, y).
4.
(i)
Somebody doesn’t like strawberry jam.
(ii)
All birds can ﬂy.
5.
(i)
True
(iii)
True
(iv)
False
(v)
True
(vii)
False

698
Hints and Solutions to Selected Exercises
Exercises 1.8
1.
This argument has the same structure as example 1.15.2.
3.
This argument has the same structure as example 1.15.1.
4.
Universe of discourse: people.
G(x) : x is a gambler.
R(x) : x is bound for ruin.
H(x) : x is happy.
Premises:
∀x[G(x) →R(x)]
∀x[R(x) →¬H(x)].
Conclusion:
∀x[G(x) →¬H(x)].
A summary of the argument is as follows. The propositions G(a) →
R(a) and R(a) →¬H(a) both follow by universal speciﬁcation from
the premises and so are true for every a in the universe of discourse. The
fact that (p →q) ∧(q →r) logically implies p →r allows us to deduce
G(a) →¬H(a) for every a in the universe of discourse. Universal
generalization leads to the conclusion.
7.
Universe of discourse: alligators
Deﬁne:
F(x) : x is friendly
S(x) : x is sociable
Z(x) : x lives in the zoo.
Premises:
∃x[F(x) ∧S(x)]
∀x[F(x) →Z(x)].
Conclusion:
∃x[Z(x) ∧S(x)].
1.
∃x[F(x) ∧S(x)]
(premise)
2.
∀x[F(x) →Z(x)]
(premise)
3.
F(a) ∧S(a)
(existential speciﬁcation)
4.
F(a) →Z(a)
(universal speciﬁcation)
5.
F(a)
(from 3 using simpliﬁcation)
6.
Z(a)
(from 5 and 4 using modus ponens)
7.
F(a) ∧Z(a)
(from 5 and 6)
8.
∃x[F(x) ∧Z(x)]
(existential generalization)
9.
Deﬁne the following on the universe of animals:
S(x) : x has scales

Hints and Solutions to Selected Exercises
699
D(x) : x is a dragon
C(x) : x has sharp claws.
1.
∀x[S(x) →D(x)]
(premise)
2.
∃x[¬D(x) ∧C(x)]
(premise)
3.
¬D(a) ∧C(a)
(existential speciﬁcation)
4.
S(a) →D(a)
(universal speciﬁcation)
5.
¬D(a)
(from 3 using simpliﬁcation)
6.
¬S(a)
(from 5 and 4, modus tollens)
7.
C(a)
(from 3, simpliﬁcation)
8.
¬S(a) ∧C(a)
(from 6 and 7)
9.
∃x[¬S(x) ∧C(x)]
(existential generalization)
Chapter 2
Exercises 2.2
1.
Suppose x and y are consecutive integers with x < y. Then
y = x + 1
x + y = x + x + 1
⇒
= 2x + 1
x + y is odd.
⇒
2.
To prove that, if n2 is odd, then n is odd, we prove the contrapositive, i.e.
if n is even, then n2 is even. This is proved in example 2.2.1.
To prove the converse, suppose that n is odd.
Then
n = 2m + 1
where m is an integer
n2 = (2m + 1)2
⇒
= 4m2 + 4m + 1
= 2(2m2 + 2m) + 1
n2 is odd.
⇒
3.
The proof that the product of two consecutive integers is even rests on the
fact that, if m is an integer, either m or m + 1 is even.

700
Hints and Solutions to Selected Exercises
To prove the second result, suppose that the roots of x2 + ax + b = 0 are
m and m + 1 for some integer m. Then the equation can be written as
(x −m)(x −m −1) = 0
x2 −(2m + 1)x + m2 + m = 0
⇒
a = −(2m + 1)
⇒
a is odd
⇒
b = m2 + m
and
= m(m + 1)
b is even by the ﬁrst result.
⇒
5.
m is a factor of n ⇒m = k1n where k1 is a positive integer.
n is a factor of m ⇒n = k2m where k2 is a positive integer.
Therefore
n = k2m
= k1k2n
k1k2 = 1
(since n ̸= 0)
⇒
k1 = k2 = 1
(since k1 and k2 are positive integers)
⇒
m = n.
⇒
6.
The contrapositive is ‘If n is divisible by 5 then n2 is divisible by 5’. If n
is divisible by 5 then
n = 5k
where k is an integer
n2 = 25k2
⇒
= 5(5k2)
where 5k2 is an integer
n2 is divisible by 5.
⇒
9.
If n −2 is divisible by 4 then
n −2 = 4k
where k is an integer
n + 2 = 4k + 4
⇒
= 4(k + 1).
Then
n2 −4 = (n −2)(n + 2)

Hints and Solutions to Selected Exercises
701
= 4k × 4(k + 1)
= 16k(k + 1)
n2 −4 is divisible by 16.
⇒
10.
Assume that an integer n has a smallest factor greater than 1, which is not
prime, and show that this leads to a contradiction.
Exercises 2.3
2.
If n = 1, 2n = 2 > 1, so that the proposition holds for n = 1.
Assume the proposition holds for n = k ⩾1, i.e. 2k > k.
Then
2k+1 = 2 × 2k
> 2k
(by the induction hypothesis)
= k + k
⩾k + 1
(since k ⩾1)
so the proposition holds for n = k + 1.
Hence, by mathematical induction, the proposition holds for all positive
integers n.
4.
If n = 0,
xn+1 −1
x −1
= x −1
x −1 = 1
so that the proposition holds for n = 0.
Suppose that the proposition holds for n = k ⩾0, i.e.
1 + x + · · · + xk = xk+1 −1
x −1
.
Then
1 + x + · · · + xk + xk+1 = xk+1 −1
x −1
+ xk+1
= xk+1 −1 + xk+1(x −1)
x −1

702
Hints and Solutions to Selected Exercises
= xk+1 −1 + xk+2 −xk+1
x −1
= xk+2 −1
x −1
so that the proposition holds for n = k + 1.
By mathematical induction, the proposition holds for all integers n ⩾0.
5.
If n = 1,
n(n + 1)(2n + 1)
6
= 1 × 2 × 3
6
= 1 = 12
so that the proposition holds for n = 1.
Suppose that the proposition holds for n = k ⩾1, i.e.
12 + 22 + · · · + k2 = k(k + 1)(2k + 1)
6
.
Then
12 + 22 + · · · + k2 + (k + 1)2 = k(k + 1)(2k + 1)
6
+ (k + 1)2
= k(k + 1)(2k + 1) + 6(k + 1)2
6
= (k + 1)(k + 2)(2k + 3)
6
= (k + 1)(k + 2)(2[k + 1] + 1)
6
so that the proposition holds for n = k + 1.
The result follows by mathematical induction.
7.
If n = 1, A1 = 3 × 1 so the proposition holds for n = 1.
Assume that
Ak = 3k for k ⩾1.
Then
Ak+1 = Ak + 3
= 3k + 3
= 3(k + 1)

Hints and Solutions to Selected Exercises
703
so that, if the proposition holds for n = k, then it also holds for n = k+1.
The result follows by mathematical induction.
10.
The proof follows the same lines as example 2.7.2.
11.
The result clearly holds for n = 1 and n = 2.
Assume that it holds for all integers r ⩽k, i.e. Ar = 5 × 2r−1 + 1.
For k ⩾2
Ak+1 = 3Ak −2Ak−1
= 3(5 × 2k−1 + 1) −2(5 × 2k−2 + 1)
= 15
2 × 2k + 3 −5
2 × 2k −2
= 5 × 2k + 1.
This completes the inductive step and the result follows.
Chapter 3
Exercises 3.1
1.
(ii)
{3, 6, 9, 12, . . .}
(iv)
{1/3, −2}
(vi)
{−2}
(viii) {1/2, 1, 3/2, 2, 5/2, 3, . . .}.
2.
(ii)
{0, 1, 2, 3, 4}
(iv)
{−2, −1, 0, 1, 2}
(vi)
{−1, 0, 1}.
3.
(ii)
∞
(iv)
4
(vi)
1
(viii)
3.
4.
(ii)
{x : x is an integer multiple of 3 and 3 ⩽x ⩽30}
(iv)
{x : x is a prime number}
(vi)
{x : x = n2 + m2 for some integers n and m}
(viii) {x : x = 13n for some integer n}
(x)
{x : x is a play by William Shakespeare}.

704
Hints and Solutions to Selected Exercises
Exercises 3.2
1.
(i)
True
(v)
True
(ii)
False
(vi)
True
(iii)
False
(vii)
False
(iv)
True
(viii)
True.
3.
(i)
x ⊆A
(ii)
x ∈A
(iii)
x ⊆A
(iv)
Both
(v)
Neither
(vi)
Neither.
4.
(i)
{{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}}
(iv)
{{1}, {1, 2}, {1, 3}, {1, 4}, {1, 2, 3}, {1, 2, 4}, {1, 3, 4}, {1, 2, 3, 4}}.
5.
(ii)
Both (i.e. A = B)
(iv)
B ⊆A
(vi)
Neither.
7.
(i)
If b ∈B then b ∈A ∧P(b) is true; in particular b ∈A. Hence
B ⊆A, as required.
If B ⊂A then there exists an a ∈A such that P(a) is false.
If A = B then for all a ∈A, P(a) is true.
8.
Suppose that A ⊆B and C = {x : x ∈A ∧x ∈B}.
If x ∈A then x ∈B (since A ⊆B). Therefore x ∈A ∧x ∈B is true
which means that x ∈C. Therefore A ⊆C.
Conversely suppose x ∈C. Then x ∈A ∧x ∈B is true, by deﬁnition,
so x ∈A. Therefore C ⊆A.
Since A ⊆C and C ⊆A we conclude that A = C.
10.
(i)
If A ⊆B and B ⊆C then x ∈A ⇒x ∈B ⇒x ∈C, so A ⊆C.
11.
(i)
6 (see exercise 3.2.4(i))
(ii)
8 (see exercise 3.2.4(iv))
(iii)
4
(iv)
3.
12.
A = {1}. Since {1} /∈A, A ∈R.
B = {∅, {∅}, {∅, {∅}}, {∅, {∅}, {∅, {∅}}},
{∅, {∅}, {∅, {∅}}, {∅, {∅}, {∅, {∅}}}}, . . .}.

Hints and Solutions to Selected Exercises
705
B /∈R.
If R ∈R then by deﬁnition R /∈R; conversely, if R /∈R then again by
deﬁnition R ∈R. R is not a set because we cannot specify either R ∈R
or R /∈R without obtaining a contradiction.
Exercises 3.3
2.
(ii)
{2, 3, 4, 5, 6, 7, 8}
(iv)
∅
(vii)
U = {0, 1, 2, . . . , 8, 9}
(viii) ∅
(x)
{6, 8}.
3.
(ii)
X ∩Y = ∅
(iv)
X ⊆Y
(vi)
X ⊆Y
(viii) X ∩Y = ∅
(x)
Y ⊆X.
4.
(i)
{2, 3, 4, 5, 6, 8, 10}
(vii)
{1, 2}
(iii)
{1, 2}
(ix)
{1, 2, 3, 5, 8, 10}
(v)
{3, 4, 5, 6}
(xi)
{2, 3, 5, 8, 10}.
5.
(i)
(a)
{x : x is a prime divisor of 12}
(c)
{x : x is an even prime number}, i.e. the singleton set {2}.
(ii)
(a)
{1, 2, 3, 4, 5, 6, 7, 11, 12}
(c)
{8, 10}
(e)
{1, 4, 5, 6, 7, 8, 9, 10, 11, 12}.
6.
(ii)
{x : ¬[P(x) ∨Q(x)]}
(iv)
{x : ¬P(x) ∨Q(x)}
(vi)
{x : ¬P(x) ∧Q(x)}.
7.
(ii)
x ∈[A ∩(B −C)] ⇔x ∈A and x ∈B −C
⇔x ∈A and x ∈B and x /∈C
⇔x ∈A ∩B and x /∈C
⇔x ∈(A ∩B) −C.

706
Hints and Solutions to Selected Exercises
(v)
x ∈[(A −B) −C] ⇔x ∈A −B and x /∈C
⇔x ∈A and x /∈B and x /∈C
⇔x ∈A and x /∈B ∪C
⇔x ∈A −(B ∪C).
Exercises 3.4
2.
A −B = B ∪¯A, A −(A ∩B) = A ∩¯B, (A −B) ∪(B −A) =
(A ∪B) −(A ∩B).
3.
(A ∩B) ∪(A ∩C) = A ∩(B ∪C), (A −B) ∩C = (A ∩C) −B.
4.
(ii)
A ∩(B −C) = A ∩(B ∩¯C)
(deﬁnition of difference)
= (A ∩B) ∩¯C
(associativity of ∩)
= (A ∩B) −C
(deﬁnition of difference).
(v)
(A −B) −C = (A −B) ∩¯C
(deﬁnition of difference)
= (A ∩¯B) ∩¯C
(deﬁnition of difference)
= A ∩( ¯B ∩¯C)
(associativity of ∩)
= A ∩(B ∪C)
(De Morgan’s law)
= A −(B ∪C)
(deﬁnition of difference).
5.
(i)
A ∗∅= (A −∅) ∪(∅−A)
= A ∪∅
= A
A ∗A = (A −A) ∪(A −A)
= ∅∪∅
= ∅.
(iii)
There are any number of possible examples of sets with the
required properties. For example, if A = {1, 2, 3}, B = {2, 3, 4},
C = {1, 3, 5} then A ∪(B ∗C) = {1, 2, 3, 4, 5} but (A ∪B)∗
(A ∪C) = {4, 5}.
6.
(i)
A = {1}, B = {1, {1}}
(ii)
A = ∅, B = {∅}, C = {∅, {∅}}.
7.
(i)
¯A ∪¯B = (A ∩B)
(ii)
A ∪B = U .

Hints and Solutions to Selected Exercises
707
The statement A ∩B = ∅is not true for all sets A and B so the duality
principle does not apply to the statement.
9.
(i)
35
(ii)
56
(iii)
7.
If |U | = 150 then |(A ∪B ∪C)| = 37.
10.
(i)
50
(ii)
165
(iii)
145
(iv)
95.
11.
(i)
4
(ii)
35
(iii)
28.
Exercises 3.5
1.
(ii)
{∅, {{1}}, {{1, 2}}, A}
(iv)
{∅, {∅}, {{1}}, {{2}}, {{1, 2}}, {∅, {1}}, {∅, {2}},
{∅, {1, 2}}, {{1}, {2}}, {{1}, {1, 2}}, {{2}, {1, 2}},
{∅, {1}, {2}}, {∅, {1}, {1, 2}}, {∅, {2}, {1, 2}},
{{1}, {2}, {1, 2}}, {∅, {1}, {2}, {1, 2}}}.
2.
(i)
Not a partition since 1 and 2 are not subsets of A.
(iii)
Not a partition since 6 does not belong to any of the subsets (so the
ﬁrst condition of deﬁnition 3.4 fails).
(v)
Not a partition since 8 ∈{2, 8, 10} ∩{7, 8, 9} (so the second
condition of deﬁnition 3.4 fails).
3.
(ii), (iv) and (vi) only are partitions.
4.
(i)
15
(ii)
None.
5.
No; neither condition is satisﬁed.
6.
(i)
Not a partition since {1, 2} and {2, 3} are both sets in the family
but {1, 2} ∩{2, 3} ̸= ∅.
(iv)
This is a partition of Z into the sets of even and odd integers
respectively.
7.
Only (ii) is a partition.
8.
X1 = {∅}, X2 = {∅, {∅}}, X3 = {∅, {∅}, {∅, {∅}}}. |Xn| = n.
X = {x : x = ∅or x = y ∪{y} where y ∈X}.

708
Hints and Solutions to Selected Exercises
9.
If A = {1, 2} and B = {2, 3} then P(A) ∪P(B) ⊂P(A ∪B).
Exercises 3.6
1.
First some notation: if (x, y) = {{x}, {x, y}} let T(x, y) = {x}∩{x, y}
and S(x, y) = {x}∪{x, y}. Thus T(x, y) = {x} and S(x, y) = {x, y}.
Now
(x, y) = (a, b) ⇒
\
(x, y) =
\
(a, b) and
[
(x, y) =
[
(a, b)
⇒{x} = {a} and {x, y} = {a, b}
⇒x = a and {x, y} = {a, b}
⇒x = a and y = b.
(Note: this argument avoids having to consider the cases x = y and x ̸= y
separately.)
The converse is easy.
2.
(iii)
{((1, 2), a), ((1, 2), b), ((1, 2), c), ((1, 2), d), ((1, 2), e)}.
3.
(ii)
(A × X) ∩(B × Y ) = {(3, b), (4, b)}.
(iv)
(A ∩X) × Y = ∅since A ∩X = ∅.
(vi)
(A × X) ∪(B × Y ) = {(1, a), (2, a), (3, a), (4, a), (1, b), (2, b),
(3, b), (4, b), (5, b), (3, c), (4, c), (5, c), (3, d), (4, d), (5, d)}.
4.
(i)
Not every possible quadruple in T × A × R+ × Z corresponds to
a book in the library’s collection. For example, there is no book
corresponding to a quadruple of the form (t, a, x, 3000) since no
book (yet) has a publication date of the year 3000.
(iii)
S represents the books in the library’s collection written by
Shakespeare. (More precisely, S represents all ordered quadruples
corresponding to those books in the library’s collection authored
by Shakespeare.)
(v)
It tells us that the library has no books with class number 514.3.
5.
No. If X is empty then X × Y = ∅= X × Z for all sets Y and Z.
6.
(i)
Square including all edges.
(ii)
Square excluding all edges.
(iii)
Square excluding the bottom and right-hand edges.

Hints and Solutions to Selected Exercises
709
(iv)
Square excluding the top and bottom edges.
8.
(iii)
There are many possibilities. A simple example is {(a, 1), (b, 2)}.
9.
(ii)
(x, y) ∈[A × (X ∪Y )] ⇔x ∈A and y ∈(X ∪Y )
⇔x ∈A and (y ∈X or y ∈Y )
⇔(x ∈A and y ∈X)
or (x ∈A and y ∈Y )
⇔(x, y) ∈(A × X)
or (x, y) ∈(A × Y )
⇔(x, y) ∈[(A × X) ∪(A × Y )].
10.
(i)
(A ∩B) × (X ∩Y ) = [A × (X ∩Y )] ∩[B × (X ∩Y )]
= (A × X) ∩(A × Y )
∩(B × X) ∩(B × Y ).
11.
(ii)
Any sets such that A ̸⊆B, B ̸⊆A, X ̸⊆Y , and Y ̸⊆X will
work. For example, A = {1}, B = {2}, X = {a}, Y = {b}.
13.
First suppose that (A × B) ⊆(X × Y ).
If a ∈A then (since B ̸= ∅) (a, b) ∈(A × B) for some b ∈B. By
hypothesis this implies that (a, b) ∈(X × Y ); in particular a ∈X.
Therefore A ⊆X.
The proof that B ⊆Y is similar.
Conversely suppose A ⊆X and B ⊆Y . Then
(a, b) ∈(A × B) ⇒a ∈A and b ∈B
⇒a ∈X and b ∈Y
(since A ⊆X and B ⊆Y )
⇒(a, b) ∈(X × Y ).
Therefore (A × B) ⊆(X × Y ).
Exercises 3.7
1.
(a)
(i)
Integer
(iii)
Boolean
(v)
Boolean .

710
Hints and Solutions to Selected Exercises
(b)
(i)
Type checks
(iii)
Does not type check
(v)
Type checks.
2.
Height( ) : Person →Real
DateOfBirth( ) : Person →Date
YearOfBirth( ) : Person →Integer
Age( ) : Person →Integer
Mother( ) : Person →Person
IsOlderThan
: Person , Person →Boolean
CitizenOf
: Person →Nation or
Person →Set [Nation ]
if multiple nationality is allowed
Children( ) : Person →Set [Person ]
IsTallerThan
: Person , Person →Boolean
Qualiﬁcations( ) : Person →Set [Qualiﬁcation ]
assuming that Qualiﬁcation
is the type of qualiﬁcations
Siblings( ) : Person →Set [Person ].
3.
(i)
False
(iii)
True
(v)
True
(vii)
True
(ix)
True.
4.
(i)
True
(iii)
False
(v)
True
(vii)
False (no-one is 250 years old, for example)
(ix)
True (e.g. n = −2, m = −1)
(xi)
True provided we allow Integer to be a subtype of Real .
5.
(i)
−
: n : Integer , m : Integer →p : Integer
postcondition
n = p + m.
(ii)
−
: n : Integer →p : Integer
postcondition
p = 0 −n
Alternatively
p + n = 0.
(iii)
>
: n : Integer , m : Integer →Boolean
postcondition
n > m ↔IsPositive(n −m).
(iv)
IsNegative( ) : n : Integer →Boolean
postcondition
IsNegative(n) ↔(0 > n)
Alternatively
IsNegative(n) ↔¬IsPositive(n) ∧¬(n = 0).

Hints and Solutions to Selected Exercises
711
(v)
<
: n : Integer , m : Integer →Boolean
postcondition
n < m ↔IsPositive(m −n)
Alternatively
n < m ↔m > n.
(vi)
1/
: n : Integer →r : Real
precondition
n ̸= 0
postcondition
r × n = 1.
(vii)
⩾
: n : Integer , m : Integer →Boolean
postcondition
n ⩾m ↔(n > m) ∨(n = m)
Alternatively
n ⩾m ↔¬(n < m).
(viii)
⩽
: n : Integer , m : Integer →Boolean
postcondition
n ⩽m ↔(n < m) ∨(n = m)
Alternatively
n ⩽m ↔¬(n > m).
(ix)
IsEven( ) : n : Integer →Boolean
postcondition
IsEven(n) ↔(∃m : Integer , n = 2m).
(x)
IsOdd( ) : n : Integer →Boolean
postcondition
IsOdd(n) ↔¬IsEven(n)
Alternatively
IsOdd(n) ↔(∃m : Integer , n = 2m + 1).
(xi)
mod
: n : Integer , k : Integer →p : Integer
precondition
k ̸= 0
postcondition
(0 ⩽p < k) ∧(∃q : Integer , n = q × k + p).
(xii)
|
: n : Integer , m : Integer →Boolean
precondition
n ̸= 0 ∧m ̸= 0
postcondition
n|m ↔(∃k : Integer , m = kn)
Alternatively
n|m ↔(m mod n = 0).
6.
(i)
IsMarried( ) : Person →Boolean
IsFemale( ) : Person →Boolean
IsChildOf
: Person , Person →Boolean
IsMarriedTo
: Person , Person →Boolean .
(iii)
signature
Sons( ) : p : Person →A : Set [Person ]
Informal
postcondition
A = Sons(p) is the set of all
male children of p
Formal
postcondition
A = {q : Person |¬IsFemale(q)
∧q IsChildOf p}.
(v)
signature
FatherInLaw : p : Person →q : Person
precondition
IsMarried(p)

712
Hints and Solutions to Selected Exercises
postcondition
¬IsFemale(q) ∧(∃r : Person ,
p IsMarriedTo r ∧r IsChildOf q).
Chapter 4
Exercises 4.1
1.
(iii)
(a)
(b)

Hints and Solutions to Selected Exercises
713
(c)












1
1
1
1
1
1
1
1
0
1
1
1
1
1
1
1
0
0
1
1
1
1
1
1
0
0
0
1
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
0
0
1
1
1
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1












(vi)
(a)
c
d
a
e
b
c
d
a
e
b
f
f
(b)
(c)






1
0
0
0
1
0
1
1
1
0
0
1
1
1
0
0
1
1
1
0
1
0
0
0
1






.

714
Hints and Solutions to Selected Exercises
(x)
(a)
(b)
(c)
With the rows and columns referring to the elements of A in
the order ∅, {1}, {2}, {3}, {1, 2}, {2, 3}, {1, 3}, {1, 2, 3},
the binary matrix is the following.












0
1
1
1
1
1
1
1
0
0
0
0
1
0
1
1
0
0
0
0
1
1
0
1
0
0
0
0
0
1
1
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0













Hints and Solutions to Selected Exercises
715
3.
(i)
R = {(a, c), (a, d), (a, e), (b, e), (c, a), (c, b), (d, c), (d, e),
(e, a), (e, b)}
(ii)
M =






0
0
1
1
1
0
0
0
0
1
1
1
0
0
0
0
0
1
0
1
1
1
0
0
0






.
4.
R = {(1, {1}), (1, {1, 2}), (1, {1, 3}), (1, {1, 2, 3}), (2, {2}),
(2, {1, 2}), (2, {2, 3}), (2, {1, 2, 3}), (3, {3}), (3, {2, 3}),
(3, {1, 3}), (3, {1, 2, 3})}.
a R b
if and only if a belongs to b.
5.
R = {(A, B), (A, D), (B, A), (B, D), (C, A), (C, B)}




0
1
0
1
1
0
0
1
1
1
0
0
0
0
0
0



.
7.
(i)
(a)
The set of towns or cities through which the River Thames
ﬂows.
(b)
The set of rivers which ﬂow through London.
(ii)
(a)
No river ﬂows through Toronto.
(b)
The only river which ﬂows through Washington DC is the
Potomac.
(iii)
(a)
{a ∈A : a R (Paris)}.
(b)
{a ∈A : ∃b ∈B, a R b}.
(iv)
(a)
∀a ∈A ∃b ∈B : a R b.
(b)
∀b ∈B ∃a ∈A : a R b.
(c)
∃b1, b2 ∈B : (b1 ̸= b2) ∧(Nile R b1) ∧(Nile R b2).
8.
(i)
(a)
The graph has a loop from each vertex to itself and no other
lines.
(b)
The matrix has ones along the diagonal from top left to
bottom right and zeros elsewhere. (It is the identity matrix
In where n = |A|; see chapter 6.)
(ii)
(a)
The graph has a loop from each vertex to itself and every
vertex is joined to every other by a bidirectional edge.

716
Hints and Solutions to Selected Exercises
(b)
Every entry of the matrix is one.
9.
(i)
212
(ii)
2nm.
10.
(i)
The graph of R−1 is obtained from that for R by reversing the
direction of all the arrows.
(ii)
The binary matrix of R−1 is obtained from that of R by reﬂection
in the top left to bottom right diagonal. (In other words, the matrix
for R−1 is the transpose of the matrix for R; see chapter 6.)
11.
{(1, 2, 2), (1, 2, 4), (1, 2, 6), (1, 3, 3), (1, 3, 6), (1, 4, 4), (1, 5, 5),
(1, 6, 6), (2, 3, 3), (2, 3, 6), (2, 4, 4), (2, 5, 5), (2, 6, 6), (3, 4, 4),
(3, 5, 5), (3, 6, 6), (4, 5, 5), (4, 6, 6), (5, 6, 6)}.
12.
Question 1: (i), (ii), (iii), (iv), (vii), (viii) have type Set [Integer × Integer ];
(ix), (x) have type Set [Set [Integer ] × Set [Integer ]];
(v), (vi) have type Set [Character × Character ]
where a, b, . . . , f : Character .
Question 4: Set [Integer × Set [Integer ]].
Question 5: Set [Team × Team ] where A, B, C, D : Team .
Question 8: If A : Set [T] then IA, UA : Set [T × T].
13.
(i)
R−1 : Set [T × S ].
(ii)
R : Set [River × Town ] where A : Set [River ] and B : Set [Town ].
R−1 can naturally be described as: b R−1 a if and only if b lies on
the banks of a.
R−1 : Set [Town × River ].
Exercises 4.2
2.
(i)
Anti-symmetric.
(iii)
Symmetric.
(v)
Anti-symmetric and transitive.
(vii)
Reﬂexive, symmetric and transitive.
(ix)
Symmetric (whether the society is monogamous or polygamous).
3.
(ii)
Reﬂexive: (x, x) ∈R for all x ∈A.
Not symmetric: (a, b) ∈R but (b, a) /∈R.
Anti-symmetric:
there do not exist x, y ∈A where x ̸= y,
(x, y), (y, x) ∈R.
Not transitive: (a, b), (b, c) ∈R but (a, c) /∈R.

Hints and Solutions to Selected Exercises
717
(iv)
Not reﬂexive: (a, a) /∈R for example.
Not symmetric: (a, b) ∈R but (b, a) /∈R.
Anti-symmetric: there do not exist x, y ∈A where x ̸= y and
(x, y), (y, x) ∈R.
Not transitive: (a, b), (b, c) ∈R but (a, c) /∈R.
4.
(ii)
Reﬂexive, anti-symmetric and transitive.
(iv)
Reﬂexive, anti-symmetric and transitive.
5.
Symmetric, anti-symmetric and transitive.
If A = ∅then R is also
reﬂexive.
6.
Yes; any subset of the identity relation is symmetric and anti-symmetric.
7.
(i)
Two—(c, c) and (d, d).
(ii)
None—R is anti-symmetric.
8.
(i)
R = {(a, a), (a, b), (a, c), (b, b), (b, c), (c, c), (d, d)}.
(ii)
R = {(a, a)}.
(iii)
R = {(a, b), (b, a), (b, c), (c, b)}.
9.
(i)
Reﬂexive, symmetric and transitive.
(iii)
Symmetric.
10.
(i)
Reﬂexive, symmetric and transitive.
(iii)
Symmetric.
11.
(i)
The empty relation on any non-empty set.
A more obvious
example is {(x, x), (x, y), (y, x), (y, y)} on the set {x, y, z}.
(ii)
The problem is essentially in the fourth sentence. Given a ∈A
there may be no element b ∈A such that a R b—the ‘proof’
implicitly assumes the existence of such an element b. To illustrate
this, let a = z in the relation deﬁned in (i).
12.
Reﬂexive: p →p is true for all propositions.
Transitive: (p →q) ∧(q →r) logically implies (p →r), so that
whenever (p →q) and (q →r) are both true, then (p →r) is also
true. Thus, whenever p R q and q R r, it is also the case that p R r.
Not symmetric: if p has truth value F, and q has truth value T, then p R q
(since p →q is true), but q
±
R p (since q →p is false).
Not anti-symmetric (provided the set contains at least three different
propositions): let p and q be two different propositions with the same

718
Hints and Solutions to Selected Exercises
truth value; then p R q and q R p (since p →q and q →p are both true),
but p ̸= q.
13.
R−1 inherits each of the properties from R.
Suppose R is symmetric. Then (a, b) ∈R ⇔(b, a) ∈R ⇔(a, b) ∈
R−1, so R = R−1.
Conversely, suppose R = R−1. Then (a, b) ∈R ⇒(a, b) ∈R−1 ⇒
(b, a) ∈R, so R is symmetric.
Exercises 4.3
2.
(i)
The directed graphs of R ∩S and R ∪S both have the same vertex
sets as the directed graphs of R and S. The graph of R∩S contains
those edges which belong to both the graph of R and the graph of
S. The graph of R ∪S contains those edges which belong either
to the graph of R or to the graph of S (or both).
(ii)
The binary matrices of R ∩S and R ∪S both have the same
dimension as the binary matrices of R and S. The matrix of R ∩S
has 1s in those positions where there are 1s in both the matrices of
R and S (and has 0s elsewhere). The matrix of R ∪S has 1s in
those positions where there are 1s in either the matrix of R or the
matrix of S (and has 0s elsewhere).
3.
R1 ⊆(A1 × B1) and R2 ⊆(A2 × B2), so R1 ∩R2 and R1 ∪R2
are both subsets of (A1 × B1) ∪(A2 × B2), which is a subset of
(A1 ∪A2) × (B1 ∪B2). Therefore R1 ∩R2 and R1 ∪R2 are both
subsets of (A1 ∪A2) × (B1 ∪B2), so both are relations from A1 ∪A2 to
B1 ∪B2.
4.
(i)
Suppose R and S are both symmetric. Then (a, b) ∈(R ∪S)
implies (a, b) ∈R or (a, b) ∈S. If (a, b) ∈R then (b, a) ∈R,
since R is symmetric, and if (a, b) ∈S then (b, a) ∈S, since S is
symmetric. Therefore, in either case (b, a) ∈(R ∪S), so R ∪S is
symmetric.
5.
(i)
S ◦R = {(1, 4), (2, 3), (3, 2), (3, 1), (4, 3)}
R ◦S = {(1, 2), (2, 1), (2, 4), (3, 2), (4, 3)}.
(ii)
R−1 = {(1, 3), (2, 2), (2, 4), (3, 1), (4, 3)}
S−1 = {(1, 4), (2, 1), (3, 2), (4, 3)}

Hints and Solutions to Selected Exercises
719
(S ◦R)−1 = {(1, 3), (2, 3), (3, 2), (3, 4), (4, 1)}
(R ◦S)−1 = {(1, 2), (2, 1), (2, 3), (3, 4), (4, 2)}.
(iii)
R−1 ◦S−1 = {(1, 3), (2, 3), (3, 2), (3, 4), (4, 1)}
S−1 ◦R−1 = {(1, 2), (2, 1), (2, 3), (3, 4), (4, 2)}.
(iv)
R−1 ◦S−1 = (S ◦R)−1 and S−1 ◦R−1 = (R ◦S)−1.
6.
(i)
R = {(a, a), (a, b), (c, g), (d, c), (d, e), (e, d), (e, e), (f, b),
(f, c), (g, c), (g, f), (h, a), (h, g)}.
(ii)
R ◦R = {(a, a), (a, b), (c, c), (c, f), (d, d), (d, e), (d, g),
(e, c), (e, d), (e, e), (f, g), (g, b), (g, c), (g, g),
(h, a), (h, b), (h, c), (h, f)}.
(iii)
7.
(i)
x(S ◦R)y if and only if x is the paternal grandmother of y.
(ii)
x(R ◦S)y if and only if x is the maternal grandfather of y.
8.
n(R2)m ⇔m = n4.
10.
(a, d) ∈[(T ◦S) ◦R] ⇔(a, b) ∈R and (b, d) ∈(T ◦S),
for some b ∈B.
⇔(a, b) ∈R and (b, c) ∈S and (c, d) ∈T,
for some b ∈B, c ∈C.
⇔(a, c) ∈(S ◦R) and (c, d) ∈T,
for some c ∈C.
⇔(a, d) ∈[T ◦(S ◦R)].
Therefore (T ◦S) ◦R = T ◦(S ◦R).
11.
If R : Set [T × U ] and S : Set [U × V ], then S ◦R : Set [T × V ]. (This
supposes that A : Set [T ], B : Set [U ] and C : Set [V ].)

720
Hints and Solutions to Selected Exercises
Exercises 4.4
1.
For all n ∈Z, |n| = |n|; hence n R n so R is reﬂexive.
For all n, m ∈Z, n R m ⇒|n| = |m| ⇒|m| = |n| ⇒m R n so R is
symmetric.
For all n, m, p ∈Z, n R m ∧m R p ⇒|n| = |m| ∧|m| = |p| ⇒|n| =
|p| ⇒n R p so R is transitive.
Therefore R is an equivalence relation on Z.
[n] = {m ∈Z : n R m} = {m ∈Z : |n| = |m|} = {n, −n}. Therefore
[0] = {0}, [1] = {1, −1}, [2] = {2, −2}, . . . .
2.
(i)
(b)
£ 1
4
¤
=
£
0, 1
2
¢
,
£ 1
2
¤
=
£ 1
2, 1
¢
.
(c)
The partition of R is {[k/2, (k + 1)/2) : k ∈Z}.
3.
(i)
The equivalence class of a person P contains all those people in A
who are the same age as P.
(ii)
The equivalence class of a person P contains all those people in A
who were born in the same country as P.
4.
For IA, the equivalence classes are singleton sets, i.e. [a] = {a}.
For UA, the only equivalence class is A itself.
5.
(i)
[(a, b)] = {(x, y) : x = a}, the vertical line through (a, b).
(ii)
[(a, b)] = {(x, y) : x + y = a + b}, the line through (a, b) with
gradient −1.
(iii)
[(a, b)] = {(x, y) : x2 + y2 = a2 + b2}, the circle centred at the
origin with radius
√
a2 + b2. If a = b = 0 then [(0, 0)] = {(0, 0)}.
6.
For all (m, n) ∈Z+ × Z+, m + n = n + m, so (m, n) R (m, n): R is
reﬂexive.
(m, n) R (p, q) ⇒m + q = n + p ⇒p + n = q + m ⇒(p, q) R (m, n):
R is symmetric.
(m, n)R(p, q) and (p, q)R(r, s) ⇒m+q = n+p and p+s = q +r ⇒
m + q + p + s = n + p + q + r ⇒m + s = n + r ⇒(m, n) R (r, s): R
is transitive.
[(1, 1)] = {(m, n) : m = n}
= {(1, 1), (2, 2), (3, 3), . . .}

Hints and Solutions to Selected Exercises
721
[(2, 1)] = {(m, n) : m = n + 1} = {(2, 1), (3, 2), (4, 3), . . .}
[(3, 1)] = {(m, n) : m = n + 2} = {(3, 1), (4, 2), (5, 3), . . .}
[(1, 2)] = {(m, n) : m + 1 = n} = {(1, 2), (2, 3), (3, 4), . . .}.
To every integer z there corresponds a unique equivalence class
consisting of all pairs (m, n) such that m −n = z, and conversely
every integer corresponds to an equivalence class. (In the terminology
of chapter 5, there is a ‘bijection’ between Z and the set of equivalence
classes.) Sometimes the integers are deﬁned to be this set of equivalence
classes.
7.
[2] = Z
£ 1
4
¤
=
©
n + 1
4 : n ∈Z
ª
=
©
. . . , −2 3
4, −1 3
4, −3
4, 1
4, 1 1
4, 2 1
4, 3 1
4, . . .
ª
£
−1
4
¤
=
©
n −1
4 : n ∈Z
ª
=
©
. . . , −2 1
4, −1 1
4, −1
4, 3
4, 1 3
4, 2 3
4, 3 3
4, . . .
ª
.
8.
(i)
5
(ii)
15.
9.
(i)
[2] = {2, 4, 6, 8, 10, 12, . . .} = {2k : k ∈Z+}
[3] = {3, 9, 15, 21, 27, . . .} = {3(2k −1) : k ∈Z+}
[5] = {5, 25, 35, 55, 65, 85, 95, . . .} = {5(6k + 1) : k ∈N} ∪
{5(6k −1) : k ∈Z+}
(ii)
[2] = {2, 4, 8, 16, 32, . . .}
[3] = {3, 6, 9, 12, 18, 24, 27, 36, 45, 48, . . .}
[5] = {5, 10, 15, 20, 25, 30, 40, 45, 50, 60, 75, . . .}.
12.
n = 3
+3
[0]
[1]
[2]
[0]
[0]
[1]
[2]
[1]
[1]
[2]
[0]
[2]
[2]
[0]
[1]
×3
[0]
[1]
[2]
[0]
[0]
[0]
[0]
[1]
[0]
[1]
[2]
[2]
[0]
[2]
[1]
n = 4
+4
[0]
[1]
[2]
[3]
[0]
[0]
[1]
[2]
[3]
[1]
[1]
[2]
[3]
[0]
[2]
[2]
[3]
[0]
[1]
[3]
[3]
[0]
[1]
[2]
×4
[0]
[1]
[2]
[3]
[0]
[0]
[0]
[0]
[0]
[1]
[0]
[1]
[2]
[3]
[2]
[0]
[2]
[0]
[2]
[3]
[0]
[3]
[2]
[1]
For n = 4 and 6 there are non-zero elements whose product is zero.

722
Hints and Solutions to Selected Exercises
There do not exist non-zero [a]n and [b]n such that [a]n ×n [b]n = [0]n if
and only if n is prime.
13.
Note that p ↔q is true if and only if p and q have the same truth value
(i.e. both are true or both are false).
Clearly p ↔p for all p so R is reﬂexive.
If p R q then p and q have the same truth value so that q R p. Hence R is
symmetric.
If p R q and q R s then p, q and s must all have the same truth values so
that p R s. Hence R is transitive.
The equivalence classes are {true propositions in A} and {false
propositions in A}.
14.
Let z = y to show that R is symmetric; then show R is transitive.
15.
First suppose that R ◦S is an equivalence relation.
(a, b) ∈(R ◦S) ⇔(b, a) ∈(R ◦S)
⇔(b, x) ∈S and (x, a) ∈R for some x ∈A
⇔(a, x) ∈R and (x, b) ∈S for some x ∈A
⇔(a, b) ∈(S ◦R).
Therefore R ◦S = S ◦R.
Conversely suppose that R ◦S = S ◦R.
For all a ∈A, (a, a) ∈S and (a, a) ∈R, so (a, a) ∈R ◦S, so R ◦S is
reﬂexive.
(a, b) ∈(R ◦S) ⇒(a, b) ∈(S ◦R)
⇒(a, x) ∈R and (x, b) ∈S for some x ∈A
⇒(b, x) ∈S and (x, a) ∈R for some x ∈A
⇒(b, a) ∈(R ◦S), so R ◦S is symmetric.
(a, b) ∈(R ◦S) and (b, c) ∈(R ◦S)
⇒(a, x) ∈S and (x, b) ∈R for some x ∈A
and (b, y) ∈S and (y, c) ∈R for some y ∈A
⇒(a, x) ∈S and (x, y) ∈(S ◦R) and (y, c) ∈R for some x, y ∈A

Hints and Solutions to Selected Exercises
723
⇒(a, x) ∈S and (x, y) ∈(R ◦S) and (y, c) ∈R for some x, y ∈A
⇒(a, x) ∈S and (x, z) ∈S and (z, y) ∈R and (y, c) ∈R
for some x, y, z ∈A
⇒(a, z) ∈S and (z, c) ∈R for some z ∈A
⇒(a, c) ∈(R ◦S),
so R ◦S is transitive.
Therefore R ◦S is an equivalence relation.
Exercises 4.5
1.
For all n ∈Z+, n|n, so R is reﬂexive.
If n|m and m|n then m = k1n and n = k2m where k1, k2 ∈Z+.
Therefore m = k1k2m so k1k2 = 1 and hence k1 = k2 = 1 (since k1
and k2 are positive integers). Therefore m = n so R is anti-symmetric.
If n|m and m|r then m = k1n and r = k2m where k1, k2 ∈Z+.
Therefore r = k1k2n so n|r. Therefore R is transitive.
The least element is 1 since 1 R n for all n ∈Z+.
2.
(i)
R is not reﬂexive; for example {1} ̸⊂{1}.
(iii)
R is not anti-symmetric; for example 1 R (−1) and (−1) R 1.
(v)
This depends on the properties of the people in A.
If there are people P and Q in A such that P is older and shorter
than Q then P R Q and Q R P so R is not anti-symmetric.
If there exist three people P, Q and S in A such that Q is younger
than S is younger than P and S is shorter than P is shorter than Q
then P R Q and Q R S but P
±
R S; therefore R is not transitive.
3.
(i)
For all (x, y) ∈R2, x ⩽x and y ⩽y, so (x, y) R (x, y), and R is
reﬂexive.
If (x1, y1) R (x2, y2) and (x2, y2) R (x1, y1) then x1 ⩽x2,
y1 ⩽y2, x2 ⩽x1 and y2 ⩽y1. Hence x1 = x2 and y1 = y2 so
(x1, y1) = (x2, y2) and R is anti-symmetric.
If (x1, y1) R (x2, y2) and (x2, y2) R (x3, y3) then x1 ⩽x2,
y1 ⩽y2, x2 ⩽x3 and y2 ⩽y3. Hence x1 ⩽x3 and y1 ⩽y3 so
(x1, y1) R (x3, y3) and R is transitive.

724
Hints and Solutions to Selected Exercises
(ii)
The proof is similar to (i).
5.
The reﬂexive and transitive properties are obvious.
Suppose A R B and B R A; then |A| ⩽|B| and |B| ⩽|A| so |A| = |B|.
Since no two of the sets have the same cardinality, this implies A = B.
Therefore R is anti-symmetric and hence a partial order. Clearly, for any
two sets A and B, either |A| ⩽|B| or |B| ⩽|A| so R is a total order.
The maximal (minimal) elements are the sets with the largest (smallest)
number of elements of all those in F.
6.
R is a partial order relation if no two people in A are of the same age.
(The proof that, in this case, R is a total order is similar to question 4.5.5
above.)
The greatest and least elements are the oldest and youngest people in A
respectively.
7.
(i)
Minimal elements are the sets in F with smallest cardinality;
maximal elements are the sets in F with greatest cardinality.
(ii)
R is not a total order on P({1, 2, 3}); for example, {1, 2}
±
R{2, 3}
and {2, 3}
±
R {1, 2}.
11.
We are given that a1Ra2, a2Ra3, . . . , an−1Ran, anRa1. By transitivity
(applied several times) a1Ran. Therefore a1Ran and anRa1 so a1 = an.
Thus we now have a1 R a2, a2 R a3, . . . , an−1 R a1, and repeating the
above argument shows that a1 = an−1.
Continuing in this way we see that a1 = a2 = · · · = an. (A more formal
proof would proceed by mathematical induction.)
13.
(i)
The proof is by induction on the number of elements of the
subsets of A.
Let A be a ﬁnite totally ordered set.
Trivially,
every one-element subset has a least element.
Suppose that
every k-element subset of A (k < n) has a least element and let
B = {b1, b2, . . . , bk+1} be a (k + 1)-element subset of A. Then
{b1, . . . , bk} is a k-element subset and so has a least element, bi
say. Since R is a total order, either bi R bk+1 or bk+1 R bi.
In the ﬁrst case bi R b for every b ∈B, so bi is a least element
for B. In the second case, for every b ∈B, either b = bk+1 or
bi R b. Since bi R b, for every b ̸= bk+1, and bk+1 R bi we have,
by transitivity, bk+1 R b for every b ∈B. Therefore bk+1 is a least
element.

Hints and Solutions to Selected Exercises
725
In either case B has a least element.
Therefore, by induction, every non-empty subset of A has a least
element so A is well ordered.
(ii)
Z+ is inﬁnite and well ordered by the usual < relation.
(iii)
The set of negative integers is a subset of Z which has no least
element so Z is not well ordered.
(0, 1) = {x ∈R : 0 < x < 1} is a subset of R+ with no least
element, so R+ is not well ordered.
Exercises 4.6
1.
(i)
Longest chains: {1, 2, 4, 12}
{1, 2, 6, 12}
{1, 3, 6, 12}
(ii)
Longest chains: {1, 2, 4, 20}
{1, 5, 10, 20}
{1, 2, 10, 20}
(iii)
Longest chain: {1, 2, 4, 8, 16, 32}
(iv)
Longest chains: {1, 2, 6, 30}
{1, 2, 10, 30}
{1, 3, 6, 30}
{1, 3, 15, 30}
{1, 5, 10, 30}
{1, 5, 15, 30}.

726
Hints and Solutions to Selected Exercises
2.
{(a, a), (b, b), (c, c), (d, d), (e, e), (f, f), (g, g), (h, h), (i, i), (a, d),
(a, e), (a, h), (a, i), (b, d), (b, e), (b, h), (b, i), (c, g), (c, i), (d, e),
(d, h), (d, i), (e, h), (e, i), (g, i)}.
Maximal elements: f, h and i.
Minimal elements: a, b, c and f.
3.
Three element: the following are the different possible kinds of Hasse
diagrams together with the number of different posets with the given
diagram type.
Total number of different order relations is equal to 19.
Four element:
Total number of different order relations is equal to 219.

Hints and Solutions to Selected Exercises
727
4.
Maximal elements: {a, b, c}, {a, b, d}, {a, c, d}, {b, c, d}.
Minimal elements: {a}, {b}, {c}, {d}.
Longest chains:
chains of the form {x}, {x, y}, {x, y, z}. There are
24 of these.
5.
(i)
(ii)
Maximal elements: (2, 8), (0, 8), (1, 5), (1, 8) and (2, 5).
Minimal elements: (0, 5), (0, 2), (1, 2) and (2, 5).
6.
The least element is {a} and the greatest element is {a, b, c}.
7.
(i)

728
Hints and Solutions to Selected Exercises
8.
Exercises 4.7
1.
(i)
F3286
Johnson, D
15/12/69
1989
M3415
Singer, R
03/10/71
1989
F0278
Williams, L
19/03/70
1989
(iii)
M1452
Adams, K
23/06/71
1990
CompSci
F3286
Johnson, D
15/12/69
1989
Psyc
F5419
Kirby, F
29/07/63
1990
Math/Econ
M3415
Singer, R
03/10/71
1989
Hist
F0278
Williams, L
19/03/70
1989
CompSci/Math
(v)
F3286
Johnson, D
F5419
Kirby, F
F0278
Williams, L
2.
F5419
Math100
Math150
Econ110
Econ120.
The selection above does not list students whose B1, B2 or B4 attribute
values are ‘Econ110’.
To obtain the complete list of Econ110 students, make four selections
from CURRENT COURSE, those record instances whose B1, B2, B3
and B4 attribute value is ‘Econ110’ respectively; then take the union of
the four resulting tables.
3.
Both combinations of selections result in the following table.
M1452
Adams, K
23/06/71
1990
CompSci
Comp100
Math150
Bus 105
Econ110
F3286
Johnson, D
15/12/69
1989
Psyc
Psyc250
Psyc280
Psyc281
Soc 200
F5419
Kirby, F
29/07/63
1990
Math/Econ
Math100
Math150
Econ110
Econ120
M3415
Singer, R
03/10/71
1989
Hist
Hist210
Hist220
Lit 200
Stat120
F0278
Williams, L
19 /03/70
1989
CompSci/Math
Comp210
Comp230
Math205
Math215
4.
(i)
First take the natural join of PERSONAL and CURRENT
COURSE (or DISCIPLINE and CURRENT COURSE); then
project onto (A2, B1, B2, B3, B4).

Hints and Solutions to Selected Exercises
729
(iii)
First take the natural join of DISCIPLINE and CURRENT
COURSE; then select those record instances whose A5 attribute
value is ‘CompSci’ or CompSci/∗∗∗∗∗∗∗’ or ‘∗∗∗∗∗∗∗/CompSci’;
ﬁnally project onto (A2, B1, B2, B3, B4).
(Note that the middle ‘selection phase’ can be accomplished by
three separate selections followed by their union in a similar
manner to that described in question 4.7.2 above.)
(v)
First take the natural join of PERSONAL and CURRENT
COURSE; then select those record instances whose A3 attribute
value is ‘∗∗/∗∗/71’; ﬁnally project onto (A1, A4, B1, B2, B3, B4).
Chapter 5
Exercises 5.1
1.
(i)
4
(v)
−4
(iii)
3
(vii)
(a + 1)2 −5 = a2 + 2a −4.
2.
(i)
{3, 4}
(v)
{1, 2}
(iii)
∅
(vii)
A.
3.
(i), (iii), (v), (vi) are (vii) are functions.
4.
Only (iii) is a function.
5.
In general, only (ii) is a function. However if A is a singleton set then (i)
and (iii) are also functions.
6.
(ii)
(a)
f(1) = 5, f(2) = 6, f(3) = 7, f(4) = 8, f(5) = 9,
f(6) = 9, f(7) = 9, f(8) = 9, f(9) = 9.
(b)
{5, 6, 7, 8, 9}.
(c)
{(1, 5), (2, 6), (3, 7), (4, 8), (5, 9), (6, 9), (7, 9), (8, 9),
(9, 9)}.
(v)
(a)
f(1) = 7, f(2) = 5, f(3) = 3, f(4) = 1, f(5) = 1,
f(6) = 3, f(7) = 5, f(8) = 7, f(9) = 9.
(b)
{1, 3, 5, 7, 9}.
(c)
{(1, 7), (2, 5), (3, 3), (4, 1), (5, 1), (6, 3), (7, 5), (8, 7),
(9, 9)}.
(viii) (a)
f(1) = 2, f(2) = 2, f(3) = 3, f(4) = 2, f(5) = 2,
f(6) = 2, f(7) = 2, f(8) = 2, f(9) = 2.

730
Hints and Solutions to Selected Exercises
(b)
{2, 3}.
(c)
{(1, 2), (2, 2), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2),
(9, 2)}.
7.
(i)
{x ∈R : x ⩾2}
(iii)
¡
0, 1
2
¤
=
©
x ∈R : 0 < x ⩽1
2
ª
(v)
£
−1
10, 1
2
¤
=
©
x ∈R : −1
10 ⩽x ⩽1
2
ª
.
8.
(i)
{0, 1, 2, 3, 4}
(iii)
{capital cities of the world}
(v)
{{a}, {a, b}, {a, c}, {a, d}, {a, b, c}, {a, b, d}, {a, c, d},
{a, b, c, d}}.
9.
Question 1
f : Set [Real × Real ]
g : Set [Integer × Real ]
h : Set [Real × Integer ].
Question 2
f, g : Set [Set [Integer ] × Set [Integer ]].
Question 8
(i)
f : Set [Set [Character ] × Integer ]
where a, b, c, d : Character .
(ii)
f : Set [Integer × Integer ].
(iii)
f : Set [Country × City ].
(iv) and (v)
f : Set [Set [Character ] × Set [Character ]]
where a, b, c, d : Character .
10.
(i)
[0, 9] = {x ∈R : 0 ⩽x ⩽9}
(iii)
{ 1
2, 1, 2, 4, 8, 16, 32, 64}
(v)
{1, 2, 7, 11}.
11.
(i)
[−3, −2] ∪[2, 3]
(iii)
[−3, 3]
(v)
{(x, y) ∈R+ : x2 + y2 ⩽1}, the ‘closed’ disc, centred at the
origin with radius 1
(vii)
A.
12.
(i)
The domain of f is {1, 2, 3, 4, 5, 6}.
(iii)
The domain of f is {1, 2, 3}.
(v)
The domain of f is R∗= {x ∈R : x ̸= 0}.
(vii)
The domain of f is {integer multiples of 4} = {4n : n ∈Z}.
(ix)
The domain of f is {(n, m) ∈Z+ × Z+ : m divides n}.
13.
(i)
Partial function: if n is odd then f(n) is not deﬁned.
(iii)
Total function.
(v)
Not a function.

Hints and Solutions to Selected Exercises
731
14.
Let A = {a1, a2, . . . , an}. For each ai there are m possibilities for f(ai),
namely the m elements of B. Since the image of each element of A can
be deﬁned independently, there are m × m × . . . × m (n times) = mn
possible functions A →B.
15.
[x] = f −1(f{x}), the set of all elements of A which have the same image
as x.
Exercises 5.2
1.
(i)
(f ◦f)(2) = f(−1) = −4
(iii)
(f ◦g)(2) = f(5) = 20
(v)
(h ◦g)(3) = h
¡ 15
7
¢
= 2
(vii)
(f ◦h)(1.5) = f(1) = −4.
3.
(i)
f(2) = 7
(iii)
(g ◦f)(2) = g(7) = 50
(v)
(f ◦f)(2) = f(7) = 27
(iv)
(f ◦g)(2) = f(5) = 19
(vii)
(f ◦g ◦f)(3) = f(g(f(3))) = f(g(11)) = f(122) = 487
(ix)
(g ◦f)(x) = g(4x −1) = (4x −1)2 + 1 = 16x2 −8x + 2.
4.
(i)
(g ◦f)(1) = g(3) =
1
10
(iii)
(g ◦h)(2) = g(
√
5) = 1
6
(v)
(f ◦g)(x) =
2
x2 + 1 + 1 = x2 + 3
x2 + 1
(vii)
(g ◦h)(x) =
1
x2 + 2
(ix)
((f ◦g) ◦h)(x) = x2 + 4
x2 + 2.
7.
im(f) ⊆C and im(g) ⊆A.
8.
Both expressions are just h(g(f(x))).
9.
f(a) = f(b) = f(c) = a. (There are other functions with the required
property.)
10.
(i)
If n ∈Z then ⌊n⌋= n; since ⌊x⌋∈Z for all x ∈R, it follows
that ⌊⌊x⌋⌋= ⌊x⌋.
(ii)
Every real number x can be expressed uniquely as x = n + δ
where n ∈Z and 0 ⩽δ < 1, and then f(x) = n. If k ∈Z then

732
Hints and Solutions to Selected Exercises
f(x + k) = f(n + δ + k) = n + k = f(x) + k. Conversely, if
f(x + k) = f(x) + k then k = f(x + k) −f(x) ∈Z, so k ∈Z.
(iii)
S
n∈Z
£
n, n + 1
2
¢
.
11.
The proof is by induction on n, the case n = 1 being trivial. Suppose
f [k](x) = 2kx + (2k −1), for some k ⩾1. Then
f [k+1](x) = f [k](2x + 1) = 2k(2x + 1) + (2k −1)
= 2k+1x + (2k + 2k) −1 = 2k+1x + (2k+1 −1)
which completes the inductive step.
12.
(i)
For x ⩾3, g(x) = x −2; but g(1) and g(2) may be chosen
arbitrarily.
(ii)
Since 1 /∈im(f), f(h(1)) ̸= 1 regardless of the deﬁnition of h.
(iii)
f [n](x) = x + 2n.
13.
(ii)
(g ◦f)(x) =









(x + 2)/3
if x ⩾2
|x −1|
if 1 ⩽x < 2
(x3 + 4)/3
if 0 ⩽x < 1
|x3 + 1|
if x < 0.
14.
(i)
iC = {(c, c) : c ∈C}.
(ii)
(x, y) ∈f|C ⇔x ∈C and y = f(x) ∈B
⇔(x, y) ∈(C × B) and (x, y) ∈f
⇔(x, y) ∈(f ∩(C × B)).
(iii)
f|C and f ◦iC are both functions C →B and, for all x ∈C,
(f ◦iC)(x) = f(x) = (f|C)(x).
15.
(i)
f and g are total, g ◦f is partial; domain of g ◦f is {1, 2}.
(ii)
f, g and g ◦f are total.
(iii)
f and g are total, g ◦f is partial; domain of g ◦f is {0}.
(iv)
f is total, g is partial, g ◦f is total.
(v)
f is partial, g is total, g ◦f is partial; domain of g ◦f is
{2n : n ∈Z}.
(vi)
f is total, g is partial, g ◦f is partial; domain of g ◦f is
{n/2 : n ∈Z}.
(vii)
f is total, g is partial, g ◦f is total.
(viii) f is total, g is partial, g ◦f is partial; domain of g ◦f is the closed
interval [−1, 1] = {x ∈R : −1 ⩽x ⩽1}.

Hints and Solutions to Selected Exercises
733
16.
f
g
g ◦f
(i)
Set [Integer × Integer ] Set [Integer × Integer ] Set [Integer × Integer ]
(ii)
Set [Integer × Integer ] Set [Integer × Integer ] Set [Integer × Integer ]
(iii)
Set [Integer × Real ]
Set [Integer × Integer ] Set [Integer × Integer ]
(iv)
Set [Real × Real ]
Set [Real × Real ]
Set [Real × Real ]
(v)
Set [Integer × Integer ]
Set [Real × Real ]
Set [Integer × Real ]
(vi)
Set [Real × Real ]
Set [Integer × Integer ]
Set [Real × Integer ]
(vii) Set [Integer × Integer ] Set [Integer × Integer ] Set [Integer × Integer ]
(viii)
Set [Real × Real ]
Set [Real × Real ]
Set [Real × Real ].
Exercises 5.3
1.
(i)
Not injective: F(b) = F(e);
not surjective:
for example, a /∈im(F).
(iii)
Injective: all the images are distinct;
not surjective:
for example, d /∈im(F).
(v)
Not injective: F(a) = F(d);
not surjective:
for example, c /∈im(F).
2.
(i)
Injective: f(n) = f(m) ⇒n −6 = m −6 ⇒n = m.
Surjective: for each n ∈Z there exists m = n + 6 ∈Z such that
f(m) = m −6 = (n + 6) −6 = n.
(iii)
Not injective: f(−2) = 4 = f(2).
Not surjective:
for example, 2 /∈im(f).
(v)
Not injective: f(2) = 6 = f(−3).
Not surjective:
for example, 1 /∈im(f).
(vii)
Note that f(n) = n + 1 if n is even and f(n) = n −1 if n is odd.
So n and f(n) always have opposite parity (evenness/oddness).
Injective: f(n) = f(m) ⇒n, m are both even
or n, m are both odd
⇒n + 1 = m + 1 or n −1 = m −1
⇒n = m.
Surjective: let n ∈Z.
If n is even, let m = n + 1. Then m is odd so f(m) = m −1 =
(n + 1) −1 = n.
If n is odd, let m = n −1. Then m is even so f(m) = m + 1 =
(n −1) + 1 = n.

734
Hints and Solutions to Selected Exercises
(ix)
Not injective: f(2) = 2 = f(3).
Surjective: let n ∈Z. Let m = 2n −1; then m is odd so
f(m) = (2n −1) + 1
2
= n.
3.
(i)
Surjective
(iii)
Neither
(v)
Injective.
4.
(i)
Injective: f(x) = f(y) ⇒{x} = {y} ⇒x = y.
Not surjective: for example, {1, 2} /∈im(f).
(iii)
Not injective: f({1, 2}) = {1, 2} = f({1, 2, 3}).
Not surjective: for all X ∈B, f(X) ⊆{1, 2} so {1, 2, 3} /∈
im(f), for example.
(v)
Injective: f(X) = f(Y ) ⇒A −X = A −Y ⇒X = Y .
Surjective. Given Y ∈B = P(A), let X = A−Y . Then X ⊆A,
so X ∈B = P(A) and f(X) = A −X = A −(A −Y ) = Y .
5.
(i)
Not injective: f([2]) = [4] = f([3]).
Not surjective: im(f) = {[0], [1], [4]} ̸= Z5.
(iii)
f : [0] 7→[3], [1] 7→[0], [2] 7→[2], [3] 7→[4], [4] 7→[1].
Injective: there are no repeated images.
Surjective: im(f) = Z5.
(v)
Not injective: f([1]) = [1] = f([5]).
Not surjective: im(f) = {[0], [1], [3], [4]} ̸= Z6.
(vii)
Not injective: f([1]) = [5] = f([4]).
Not surjective: im(f) = {[1], [3], [5]} ̸= Z6.
6.
(i)
Neither
(iii)
Injective
(v)
Surjective
(vii)
Injective
(ix)
Neither.
7.
(i)
f is injective provided no two people in A are the same age.
f is surjective provided there is at least one person in A of each
given age from 0 to 100 inclusive.
(iii)
f is injective with no further restrictions on A or B.
f is surjective provided the set B contains only the largest cities
(in population terms) of the countries in set A.

Hints and Solutions to Selected Exercises
735
(v)
f is injective provided it is a well deﬁned function and for this we
require c ⩽a + 10 and d ⩾b + 10.
f is surjective provided c = a + 10 and d = b + 10.
8.
(i)
y ∈f(C1 ∪C2) ⇔y = f(x) for some x ∈C1 ∪C2
⇔y = f(x) for some x ∈C1
or y = f(x) for some x ∈C2
⇔y ∈f(C1) or y ∈f(C2)
⇔y ∈f(C1) ∪f(C2).
(iii)
Suppose that f is injective, and let C1 and C2 be subsets of A.
By (ii) we only need prove f(C1) ∩f(C2) ⊆f(C1 ∩C2), so
let y ∈f(C1) ∩f(C2). Then y = f(x1) where x1 ∈C1 and
y = f(x2) where x2 ∈C2. Since f is injective, x1 = x2 = x,
say. Therefore y = f(x) where x ∈C1 ∩C2 so y ∈f(C1 ∩C2).
Hence f(C1) ∩f(C2) ⊆f(C1 ∩C2), as required.
Conversely, suppose f(C1)∩f(C2) = f(C1 ∩C2), for all subsets
C1, C2 of A. If f(a) = f(b) then f({a}) = f({b}) so {a} = {b}
which implies a = b, so f is injective.
9.
(i)
Let x ∈C. Then f(x) ∈f(C), so by deﬁnition x ∈f −1(f(C)).
Therefore C ⊆f −1(f(C)).
(ii)
Suppose f is injective and let C
⊆
A.
By (i) we need
only show f −1(f(C)) ⊆C.
Let x ∈f −1(f(C)); then by
deﬁnition f(x) ∈f(C), so f(x) = f(c) for some c ∈C. Since
f is injective, x = c ∈C, so f −1(f(C)) ⊆C as required.
(iii)
Suppose f −1(f(C)) = C for all subsets C of A. If f(x) = f(y)
then f(x) ∈f({y}) so x ∈f −1(f({y})). Since f −1(f({y})) =
{y}, this implies x = y, so f is injective.
(iv)
Suppose f is surjective and let D be a subset of B. Let y ∈
f(f −1(D)). Then y = f(x) for some x ∈f −1(D), so f(x) =
y ∈D. Hence f(f −1(D)) ⊆D. Now let y ∈D. Since f is
surjective y = f(x) for some x ∈A. Since f(x) ∈D, x ∈
f −1(D) so y = f(x) ∈f(f −1(D)). Therefore D ⊆f(f −1(D)),
which completes the proof.
11.
(i)
Suppose f is injective and let X, Y
be elements of P(A)
(i.e. subsets of A) such that Pf(X) = Pf(Y ).
In other
words, f(X) = f(Y ) so that f −1(f(X)) = f −1(f(Y )). By
question 5.3.9(ii) this implies X = Y so Pf is injective.

736
Hints and Solutions to Selected Exercises
(ii)
Suppose f is surjective and let D ∈P(B); that is, D ⊆B. For
each d ∈D choose cd ∈A such that f(cd) = d. Let C = {cd :
d ∈D}. Now Pf(C) = {f(cd) : d ∈D} = D so Pf is
surjective.
The converse statements are both true.
Exercises 5.4
1.
(i)
Bijective; f −1 : Z →Z, f −1(n) = n + 17
(iii)
Not bijective
(iv)
Bijective; f −1 = f.
2.
(i)
g ◦f = f ◦g = idZ5; hence f −1 = g
(iii)
f([0]) = [0], f([1]) = [1], f([2]) = [3], f([3]) = [2] and
f([4]) = [4]; f −1 = f.
3.
(i)
f −1 : R →R, f −1(y) = (8y −3)/5
(iii)
f −1 : [−2, 2] →[1, 3], f −1(y) = (y + 4)/2
(v)
f −1 = f
(vii)
f −1 : R2 →R2, f −1(x, y) = ((2x −y)/3, (x −2y)/3)
(ix)
f −1 : Z →Z+, f −1(n) =
(
2n
if n ⩾1
1 −2n
if n ⩽0.
4.
(i)
(a)
δC(a) = 0, δC(b) = 1, δC(c) = 0, δC(d) = 1, δC(e) = 1.
(b)
δC is injective if |A| = 2 and |C| = 1 or if |A| = 1 (and
|C| = 0 or 1).
(c)
δC is surjective if |A| ⩾2 and C ̸= ∅, C ̸= A.
(ii)
The images of the elements of A are: f(∅) = (0, 0), f({a}) =
(1, 0), f({b}) = (0, 1), f({a, b}) = (1, 1), so f is clearly a
bijection.
(iii)
If X = {x1, x2, . . . , xn} and C ⊆X then deﬁne
f(C) = (δC(x1), δC(x2), . . . , δC(xn)).
5.
(i)
f(x) = 2x + 1
(ii)
f(x) = x/(1 −x)
(iii)
f(x) = x −1/x
(iv)
f(x) = (2x −1)/(x −x2) (the composite of the functions in (ii)
and (iii))

Hints and Solutions to Selected Exercises
737
(v)
f : Z →Z+, f(n) =
(
2n
if n ⩾1
1 −2n
if n ⩽0.
6.
(ii)
f(x) = x, g(x) = −x
7.
(iii)
(a)
If |B| = 1 there is only one function A →B which is
obviously surjective.
(b)
If |A| = n and B = {b1, b2} there are 2n functions A →B.
The only functions which are not surjective are the function
which sends every element to b1 and the function which
sends every element to b2.
Therefore there are 2n −2
surjections A →B.
(c)
If m = n −1 then a surjection A →B is such that two
elements a1, a2 of A have the same image and all other
elements have a unique image. There are 1
2n(n−1) ways of
choosing the pair {a1, a2}. Then the number of surjections
such that this pair has the same image is only the number
of bijections from one m-element set to another m-element
set. By (i), there are m! such bijections, so the total number
of surjections is 1
2n(n −1) × m!.
8.
Suppose |A| = |B| = n and f : A →B is injective. Then |f(A)| = n,
so f(A) is an n-element subset of the n-element set B.
Therefore
f(A) = B, so f is surjective.
To prove the converse we prove its contrapositive: if f is not injective
then f is not surjective.
So suppose that f is not injective.
Then,
for some pair of distinct elements a1, a2 of A, f(a1) = f(a2). Thus
|f(A)| < |A| = n, so f(A) ̸= B and f is not surjective.
11.
(i)
f −1 : {1, 2, 3, 4, 5, 6, 7, 8} →{1, 2, 3, 4, 5}, f −1 : x 7→x −2
f −1 is partial with domain {3, 4, 5, 6, 7}.
(iii)
f −1 : R →R, f −1 : x 7→(x + 8)/4; f −1 is total.
(v)
f −1 : Z+ →Z, f −1 =









−√m −1
if m = n2 + 1
for some integer n
√m
if m = n2
for some integer n.
f −1 is partial with domain {n2 : n ∈Z} ∪{n2 + 1 : n ∈Z}.

738
Hints and Solutions to Selected Exercises
Exercises 5.5
1.
(i)
ℵ0. A bijection Z+ →{n : n ⩾106} is n 7→(n + 106 −1).
(ii)
c. A bijection (0, 1) →R is given in exercise 5.4.5(iv).
(iii)
ℵ0. A bijection Z+ × {0, 1} →Z+ is given by
f(n, δ) =
(
2n −1
if δ = 0
2n
if δ = 1.
(iv)
ℵ0. |Z+ × Z+ × Z+| = |Z+ × Z+| × |Z+| = ℵ0 × ℵ0 = ℵ0.
2.
Suppose f : A →P(A) is a bijection and B = {x ∈A : x /∈Ax}
as in the hint. Since f is surjective, B = f(y) for some y ∈A; that is
B = Ay. Now if y ∈B then y /∈Ay (by deﬁnition of B) so y /∈B.
Conversely, if y /∈B then y ∈Ay so y ∈B (again by deﬁnition of B).
This is a contradiction, so there is no bijection A →P(A). (Note: this
proof actually shows that there is no surjection A →P(A).)
3.
Let X = {functions A →{0, 1}}; then |X| = 2|A|.
Deﬁne F :
P(A) →X by F(B) = fB, where fB is the function A →{0, 1} given
by fB(a) = 0 if a /∈B and fB(a) = 1 if a ∈B. It is not too difﬁcult to
show that F is a bijection; hence |P(A)| = 2|A|.
4.
(i)
Let A = {n ∈Z : n > k}, B = {1, 2, . . . , k}.
Then
|A| + |B| = |A ∪B| = ℵ0, since A ∩B = ∅and A ∪B = Z+.
Clearly |B| = k and it is easy to show that |A| = ℵ0; see
exercise 5.5.1(i).
(ii)
Let X be the set of all functions {0, 1} →Z+; then |X| = (ℵ0)2.
The function F : X →Z+ × Z+ given by F : f 7→(f(0), f(1))
is a bijection, so (ℵ0)2 = |X| = |Z+ × Z+| = ℵ0.
5.
Let A be a set with cardinality α, and X be the set of all functions
{0, 1} →A. Then |X| = α2 and |A × A| = α × α. A bijection
F : X →A × A is given by F : f 7→(f(0), f(1)).
Exercises 5.6
1.
(i)
The only functional dependences between single attributes are:
A1 functionally determines A3;
A2 functionally determines A3.

Hints and Solutions to Selected Exercises
739
(ii)
{A1, A2} and {A2, A4} are the only candidate keys.
(iii)
(a)
The table is not in second normal form.
A3 is non-
prime (since it appears in neither candidate key) and A2
functionally determines A3. Thus the non-prime attribute
A3 is functionally dependent on {A2}, a proper subset of
a candidate key (in fact, a proper subset of both candidate
keys).
(b)
The table is not in third normal form, since it is not in
second normal form.
2.
(i)
Since there are no record instances with the same values for A1,
A2 and A4, the set {A1, A2, A4} functionally determines A3 and
A5.
We need to check that no proper subset of {A1, A2, A4}
functionally determines every attribute.
{A1, A2} does not
functionally determine A4 since the record instances in rows 1
and 2 have the same A1 and A2 values but different A4 values.
Similarly, {A1, A4} does not functionally determine A3 (rows 1
and 3) and {A2, A4} does not functionally determine A1 (rows 6
and 7).
(ii)
R is in (second and) third normal form. It is easy to check that
{A1, A3, A4, A5} is another candidate key for R. Therefore every
attribute appears in some candidate key, so there are no non-prime
attributes. Hence R satisﬁes deﬁnitions 5.10 and 5.11 albeit rather
trivially.
3.
Since the key is a single attribute, all three tables are in second normal
form.
(i)
If no two students have exactly the same name then A2
=
STUDENT
NAME
functionally
determines
A3, A4
and
A5.
Therefore PERSONAL and DISCIPLINE are not in third normal
form. However, CURRENT COURSE is in third normal form
since no Bi attribute (i = 1, 2, 3, 4) functionally determines any
other Bi attribute.
(ii)
If two students do have exactly the same name then A2
=
STUDENT
NAME does not functionally determine any other
attribute and therefore all three tables are in third normal form.
Note: we are assuming here that A3 = DATE
OF
BIRTH does not
functionally determine any other attribute. In other words, we suppose
that two different students at the college have the same date of birth, an
assumption which is highly probable.

740
Hints and Solutions to Selected Exercises
4.
(i)
The table is in second normal form since the key is a single
attribute. It is not in third normal form because DEPARTMENT #
functionally determines
WORK
LOCATION.
Allowing the
possibility that several employees have the same name (and work
in the same department with the same job description), there are
no other functional dependences.
The same information can be stored on two tables of attribute
types {EMPLOYEE #,
EMPLOYEE
NAME,
DEPARTMENT #,
JOB DESCRIPTION} and {DEPARTMENT #, WORK
LOCATION}
which are both in third normal form.
(ii)
This table is not in second normal form (and hence not in third)
because, for example,
AIRLINE is functionally dependent on
{FLIGHT #}, which is a proper subset of the primary key. The two
tables of types {PASSENGER
NAME, FLIGHT #, DATE, CLASS}
and {FLIGHT #,
AIRLINE,
EMBARKATION,
DESTINATION}
respectively are both in third normal form.
(iii)
PATIENT HISTORY is not in second normal form since
PATIENT
NAME is functionally dependent on the proper subset
{PATIENT #} of the key.
PATIENT CURRENT is in second
but not third normal form since both CONSULTANT
NAME and
CONSULTANT
PHONE are functionally dependent on {CONSUL-
TANT #}
which
does
not
contain
the
key
as
a
subset.
TREATMENT CURRENT is similarly in second but not third
normal form as
DAILY
COST is functionally dependent on
{DRUG, QUANTITY}.
The following tables hold the same information and are in third
normal form.
PATIENT:
{PATIENT #, PATIENT
NAME}
CONSULTANT:
{CONSULTANT #,
CONSULTANT
NAME,
CONSULTANT
PHONE}
HISTORY:
{PATIENT #, ADMISSION
DATE,
DISCHARGE
DATE, CONDITION}
CURRENT:
{PATIENT #, CONSULTANT #, CONDITION,
WARD #}
TREATMENT:
{PATIENT #, DRUG, QUANTITY}
COST:
{DRUG, QUANTITY, DAILY
COST}.
5.
Let R be a table with attributes (A1, A2, . . . , An); for each attribute
Ai, let Xi denote the set of data items. Let I = {i1, i2, . . . , ik} ⊆
{1, 2, . . . , n} and J = {j1, j2, . . . , jm} ⊆{1, 2, . . . , n} be disjoint sets
of indices. As in the text, for convenience we suppose that i1 < i2 <

Hints and Solutions to Selected Exercises
741
· · · < ik < j1 < j2 < · · · < jm. Then AJ = {Aj1, Aj2, . . . , Ajm} is
functionally dependent on AI = {Ai1, Ai2, . . . , Aik} if the projection
of R onto I ∪J, pI∪J(R), deﬁnes a partial (or total) function Xi1 ×Xi2 ×
· · · × Xik →Xj1 × Xj2 × · · · × Xjm.
Chapter 6
Exercises 6.1
1.
(i)
True
(ii)
False
(iii)
False
(iv)
False
(v)
True.
2.
A =


0
−1
1
0
2
1


B =


−1
−3
0
−2
1
−1


C =


7
10
11
14
15
18

.
4.
Any 1 × 1 matrix.
6.
(i)
(a)
¡
−1
2
3
¢
(c)




−1
0
6
7
3
−1
4
3



.
Exercises 6.2
1.
(i)
µ 10
−5
4
−3
¶
(iii)
µ
2
1
−2
1
¶
(v)
µ 8
−11
4
−5
¶
(vii)
µ
4
−7
−4
5
¶
(ix)
µ
8
−2
−14
4
¶
(xi)
µ
2
−2
1
1
¶
.
2.
(i)
¡
9
13
¢
(iii)
Does not exist
(v)


20
10
−6
18
12
−6
−2
−3
1


(vii)
¡
70
40
−22
¢
.
3.
B is any matrix of the form
µ
a
b
2b
a
¶
.

742
Hints and Solutions to Selected Exercises
6.
Let A = [aij] and In = [bij] where bii = 1 and bij = 0 for i ̸= j. Then
(i, j)-entry of AIn =
n
X
k=1
aikbkj
= ai1b1j + ai2b2j + · · · + ainbnj
= aij
= (i, j)-entry of A.
Therefore
AIn = A.
A similar argument shows that ImA = A.
8.
(i)
n = p, q = r. The dimension of ABC is m × s.
(ii)
s = p, q = m. The dimension of CBA is r × n.
(iii)
n = q = r, m = p. The dimension of (A + B)C is m × s (or
p × s).
9.
Take, for instance,
A =
µ
1
1
1
1
¶
B =
µ
1
1
−1
−1
¶
.
12.
(A + AT)T = AT + (AT)T
(by 11(ii))
= AT + A
(by 11(i))
= A + AT
(by commutativity of matrix addition).
Therefore A + AT is a symmetric matrix.
Exercises 6.3
1.
(i)
Yes. Add the elements of row 2 to row 1, or add the elements of
column 1 to column 2.
(iii)
Yes. Subtract three times row 3 from row 1 or subtract three times
column 1 from column 3.
(v)
No.
(vii)
Yes. Multiply the elements of any row/column by 1.
2.
(i)
To obtain B from A, interchange the ﬁrst and third rows. So
E1 =


0
0
1
0
1
0
1
0
0

.

Hints and Solutions to Selected Exercises
743
(ii)
To obtain A from B, interchange the ﬁrst and third rows.
So
E2 = E1 above.
(iii)
E1E2 = E2E1 = I3.
4.
(i)
To obtain B from A, multiply the second column by −2. So
F1 =


1
0
0
0
−2
0
0
0
1

.
(ii)
F2 =


1
0
0
0
−1
2
0
0
0
1

.
(iii)
F1F2 = F2F1 = I3.
5.
Q =


0
1
2
0
1
0
0
0
0
1

.
7.
No, because elementary row operations are not commutative. Consider,
for example,
E1 =
µ
3
0
0
1
¶
and
E2 =
µ
0
1
1
0
¶
.
Exercises 6.4
2.
(i)
µ 2
3
3
5
¶
(iii)
No inverse
(v)
No inverse
(vii)



1
15
4
15
−1
30
1
5
−1
5
−1
10
1
3
−2
3
1
3


.
4.
(ii)
A−1B4A =
µ
1
0
0
16
¶
.
(iii)
The proof is by induction and is outlined below.
The result holds trivially for n = 1.

744
Hints and Solutions to Selected Exercises
Suppose
(A−1BA)k = A−1BkA
(k ⩾1)
then
(A−1BA)k+1 = (A−1BA)k(A−1BA)
= A−1BkAA−1BA
= A−1BkBA
= A−1Bk+1A.
6.
If A = [aij] where aij = 0 for i ̸= j, then A−1 = [bij] where bij = 0 for
i ̸= j and bii = 1/aii.
7.
(In −A)(An + A) = On×n
(In −A)In + (In −A)A = On×n
(distributive law)
⇔
In −A + A −A2 = On×n
(distributive law)
⇔
A2 = In
⇔
A is involutary.
⇔
Chapter 7
Exercises 7.1
1.
x = 4, y = 2.
2.
x = 1, y = 2, z = −2.
x = 0, y = 0, z = 0.
3.
x = 0, y = 0, z = 4.
Exercises 7.2
1.
x = 1, y = 3, z = −2.
x = y = z = 0.
2.
Inconsistent.
3.
x1 = t, x2 = 2 + t, x3 = 3t.

Hints and Solutions to Selected Exercises
745
4.
x1 = −t, x2 = x3 = t.
5.
x1 = 13 −s −3t, x2 = t −4, x3 = s, x4 = t.
6.
x = z = t, y = 2t.
7.
x1 = 53
49 −30
49t, x2 = 12
49 + 8
49t, x3 = 8
7 + 3
7t, x4 = t.
Exercises 7.3
1.
(i)
x = 2, y = −3
2, z = 5
2
(ii)
x1 = −5, x2 = 0, x3 = 3
(iii)
x = −2, y = 11, z = 4
(iv)
x = 1, y = 1
2, z = 0
(v)
x = y = z = 0.
2.
(i)
x1 = −6 −3t, x2 = 8 −t, x3 = t
(ii)
x = 14
5 , y = −12
5 , z = 2
5
(iii)
x = 6 −2t, y = 2 + 3t, z = t
(iv)
x = y = z = 0.
3.
(i)
x1 = 2t, x2 = 3 + t, x3 = t
(ii)
Inconsistent
(iii)
x = −25, y = −37, z = −14
(iv)
x1 = x2 = x3 = 1
(v)
x = 0, y = z = t
(vi)
Inconsistent.
Chapter 8
Exercises 8.1
1.
(i)
No, because R+ is not closed under subtraction.
(iii)
Yes.
(v)
Yes.
(vi)
No, because only matrices having the same dimension can be
added.
3.
(i)
The Cayley table is symmetric about the leading diagonal.
(ii)
The row corresponding to an element x is the same as the column
headings and the column corresponding to the same element is the
transpose of that row. Then x is the identity.

746
Hints and Solutions to Selected Exercises
4.
(i)
Yes.
(ii)
A.
(iii)
A is the only element with an inverse. (A is self-inverse.)
6.
(i)
24
(ii)
39
(iii)
416
(iv)
nn2.
7.
There are six possible Cayley tables, only one of which deﬁnes an
associative operation.
8.
(ii)
Yes.
(iii)
Yes.
(iv)
Yes: ∅.
(v)
Each element is self-inverse.
Exercises 8.2
1.
To show that matrix multiplication is a binary operation on the set of 2×2
non-singular matrices, note that, if A and B are non-singular matrices,
then AB is also a non-singular matrix since (AB)−1 = B−1A−1 (see
theorem 6.4). It is a simple matter to show that the group properties hold.
2.
Suppose that there are two idempotent elements, the identity (which is
clearly idempotent) and another element x. Pre-multiplying the equation
x ∗e = x ∗x by x−1 shows that x = e and hence that e is the only
idempotent element.
3.
(Z6, ×6) is not a group because each of [0], [2], [3] and [4] has no inverse.
4.
(Zn −{0}, ×n) is a group if and only if n is prime (or n = 1).
5.
(P, ∗) is not a group since ∗is not associative.
6.
(S, ∗) is a monoid if and only if S is a singleton set.
8.
(N, ∗) is a monoid but (N, ◦) is not. The structure (N, ◦) is a semigroup.
9.
(a) Yes.
(b) Yes.
(c) No.
Exercises 8.3
1.
For all a, b ∈G,
(ab)(b−1a−1) = a(bb−1)a−1
= aa−1

Hints and Solutions to Selected Exercises
747
= e.
Hence
(ab)−1 = b−1a−1.
The proof that (an)−1 = (a−1)n is by induction. Note that the ﬁrst result
allows us to write (aka)−1 = a−1(ak)−1.
2.
Most of the Cayley table can be completed using theorem 8.5. However,
there are a couple of places where some reasoning is required.
For
instance:
q2 = (p2)q = p(pq) = pe = p.
4.
∗
r0
r1
m1
m2
r0
r0
r1
m1
m2
r1
r1
r0
m2
m1
m1
m1
m2
r0
r1
m2
m2
m1
r1
r0
5.
The proof utilizes the fact that {gr : r ∈Z} has at most n distinct
elements. If {gr : r = 0, 1, . . . , n} has fewer than n distinct elements,
then gr = gs for some r and s where r < s ⩽n. This gives gs−r = e and
so m = s −r. If {gr : r = 0, 1, . . . , n} has exactly n distinct elements,
then gn+1 = gs for some s ⩽n and m = n + 1 −s.
7.
For any group, if g is a generator then so is g−1. If (G, ∗) has only one
generator g, then g = g−1. This gives g2 = e and so G = {e, g}. (The
trivial group with the identity as its only element is also cyclic with a
single generator.)
8.
The generators are [1], [5].
9.
The group is cyclic with generators r1 and r2.
10.
Use the fact that all elements of G are self-inverse and the ‘shoes and
socks’ theorem (exercise 8.3.1).
12.
The subsets of Z10 which form groups under ×10 are: {[1]}, {[1], [9]}
and {[1], [3], [7], [9]}. All are cyclic.
13.
(b)
The elements of S are those non-zero members of Zn which share
no common factors with n.

748
Hints and Solutions to Selected Exercises
14.
In C5, each non-identity element generates the group.
In C6, only g and g5 generate the group.
In C9, each of g, g2, g4, g5, g7, g8 generates the group.
In Cn, gr generates the group if and only if r and n share no common
factors (other than 1).
Exercises 8.4
1.
Let M ′ = {x : x2 = x, x ∈M}. Clearly M ′ ⊆M and, since e2 = e,
e ∈M. If x, y ∈M
xy = x2y2
= x(xy)y
= xyxy
(since (M, ∗) is abelian)
= (xy)2.
This shows that M ′ is closed under ∗and therefore (M ′, ∗) is a
submonoid of (M, ∗).
2.
(ii)
{[0], [2], [4], [6]} and {[0], [4]}.
5.
({[0], [3], [6]}, +9) is a subgroup of (Z9, +9).
7.
(i)
Let C be the centre of (G, ∗). C ̸= ∅since e ∈C. Suppose
a, b ∈C. Given g ∈G,
abg = agb
(since b ∈C)
= gab
(since a ∈C)
ab ∈C.
⇒
If a ∈C, then
ag = ga
for all g ∈G
a−1aga−1 = a−1gaa−1
for all g ∈G
⇒
ga−1 = a−1g
for all g ∈G.
⇒
Therefore
a−1 ∈C.
From theorem 8.6 we can conclude that (C, ∗) is a subgroup of
(G, ∗).
(ii)
Centre of D3 = {r0}.

Hints and Solutions to Selected Exercises
749
9.
To apply theorem 8.6, we must prove that ab−1 ∈H for all a, b ∈H ⇔
H is closed under ∗and, for all a ∈H, a−1 ∈H. To prove that the
second proposition implies the ﬁrst is straightforward. To prove that the
ﬁrst implies the second, begin by establishing that e ∈H. It then follows
that ea−1 ∈H for all a ∈H, i.e. that a−1 ∈H for all a ∈H. To
show that H is closed under ∗: if a, b ∈H then b−1 ∈H. Hence
a(b−1)−1 ∈H ⇒ab ∈H.
10.
(H∪K, ∗) is not necessarily a subgroup of (G, ∗). Consider, for example,
G = {r0, r1, m1, m2}, the set of symmetries of a non-square rectangle
(see exercise 8.3.4). If H = {r0, r1} and K = {r0, m1}, then (H, ∗) and
(K, ∗) are both groups but (H ∪K, ∗) is not.
11.
({[1], [6]}, ×7) and ({[1], [2], [4]}, ×7).
14.
Let G be a group of order n and let g ∈G.
Suppose that g has order m. Then, by theorem 8.8, g generates a cyclic
subgroup of G,
H = {e, g, g2, . . . , gm−1} where gm = e.
This subgroup H has order m so we know that m is a factor of n by
Lagrange’s theorem. Hence n = km for some k ∈Z+ which implies
gn = gmk = (gm)k = ek = e
as required.
Exercises 8.5
3.
It is easy to show that f is bijective. Also
f
·µ
1
n
0
1
¶ µ
1
m
0
1
¶¸
= f
·µ
1
n + m
0
1
¶¸
= n + m
= f
·µ
1
n
0
1
¶¸
+ f
·µ
1
m
0
1
¶¸
.
4.
(ii) and (iv) are morphisms. Both are isomorphisms.
5.
(i), (iii) and (iv) are morphisms. None are isomorphisms.

750
Hints and Solutions to Selected Exercises
6.
If e is the identity in (A, ∗) then
a ∗e = e ∗a = a
for all a ∈A
f(a ∗e) = f(e ∗a) = f(a)
for all a ∈A
⇒
f(a) ◦f(e) = f(e) ◦f(a) = f(a)
for all f(a) ∈f(A)
⇒
f(e) is the identity in (f(A), ◦).
⇒
7.
There are two isomorphisms:
f : A 7→[1], C 7→[4], B 7→[2], D 7→[3]
g : A 7→[1], C 7→[4], B 7→[3], D 7→[2].
9.
(ii)
Suppose that (G, ∗) is abelian. For all x, y ∈G,
f(x ∗y) = (xy)2
= x(yx)y
= xxyy
= x2y2
= f(x) ∗f(y)
so that f is a morphism.
Suppose that f is a morphism. For all x, y ∈G,
f(x ∗y) = f(x) ∗f(y)
(xy)2 = x2y2
⇒
xyxy = xxyy
⇒
x−1xyxyy−1 = x−1xxyyy−1
⇒
yx = xy
⇒
so that ∗is commutative.
11.
Since every element of a group has a unique inverse, f is a bijective
function. If (G, ∗) is abelian, then
f(x ∗y) = (xy)−1 = y−1x−1 = x−1y−1 = f(x) ∗f(y).
This shows that if (G, ∗) is abelian then f is a morphism. Now suppose
that f is a morphism. For all x, y ∈G,
f(x ∗y) = f(x) ∗f(y)

Hints and Solutions to Selected Exercises
751
(xy)−1 = x−1y−1
⇒
y−1x−1 = x−1y−1.
⇒
So (G, ∗) is abelian.
13.
(a)
(i)
ker f = {0};
(ii)
ker f = {0};
(iii)
kerf = R.
(b)
(i)
Suppose f
is a monomorphism,
i.e. f
is injective.
If g ∈ker f then f(g) = e2.
But f(e1) = e2 so that
f(g) = f(e1) and therefore g = e1. This shows that the
only member of ker f is e1.
To prove the converse, suppose that ker f = {e1}. We have
f(x) = f(y)
f(x) ◦[f(y)]−1 = e2
⇒
f(x ∗y−1) = e2
⇒
x ∗y−1 ∈ker f
⇒
x ∗y−1 = e1
⇒
x = y.
⇒
Hence f is injective.
(ii)
Since e1 ∈ker f, ker f ̸= ∅. If g1, g2 ∈ker f,
f(g1 ∗g2) = f(g1) ◦f(g2)
= e2 ◦e2
= e2
⇒ker f is closed under ∗.
If g ∈ker f, f(g−1) = [f(g)]−1 = e2−1 = e2, so
g−1 ∈ker f.
Theorem 8.6 allows us to conclude that
(ker f, ∗) is a subgroup of (G, ∗).
(iii)
f(g−1 ∗x ∗g) = f(g−1) ◦f(x) ◦f(g)
= f(g−1) ◦f(g)
(since x ∈ker f)
= f(g−1 ∗g)
= f(e1)
= e2.
Hence g−1 ∗x ∗g ∈ker f.

752
Hints and Solutions to Selected Exercises
Exercises 8.6
2.
G =


1
0
0
1
0
0
1
0
0
0
1
0
0
1
0
0
1
0
0
0
1
0
0
1
0
0
1

= (I3 I3 I3).
(i)
Two
(ii)
One.
3.
(i)
Probably correctly transmitted.
(iii)
Probably correctly transmitted.
(v)
Incorrectly transmitted.
4.
The group properties can be established from the Cayley table for the set
of codewords under ⊕.
(i)
Two
(ii)
One.
5.
m = 4, n = 7.
7.
(i)
Likely error in the fourth bit.
(ii)
Probably correctly transmitted.
(iii)
Error in more than one bit and therefore the correct word cannot
be determined.
(iv)
Likely error in second bit.
8.
For the (1, 3) Hamming code,
H =
µ
1
1
0
1
0
1
¶
.
For the (4, 7) Hamming code,
H =


1
1
1
0
1
0
0
1
0
1
1
0
1
0
1
1
0
1
0
0
1

.
(Note that H is not unique.)

Hints and Solutions to Selected Exercises
753
Chapter 9
Exercises 9.1
1.
(i)
(a)
gcd(21600, 2970)
21600 = 7 × 2970 + 810
2970 = 3 × 810 + 540
810 = 1 × 540 + 270
540 = 1 × 270 + 0
Hence gcd(21600, 2970) = 270.
(c)
gcd(2679, 851)
2679 = 3 × 851 + 126
851 = 6 × 126 + 95
126 = 1 × 95 + 31
95 = 3 × 31 + 2
31 = 15 × 2 + 1
2 = 2 × 1 + 0
Hence gcd(2679, 851) = 1.
(ii)
(a)
gcd(21600, 2970) = 270.
270 = 810 −1 × 540
= 810 −1 × (2970 −3 × 810)
= −2970 + 4 × 810
= −2970 + 4 × (21600 −7 × 2970)
= 4 × 21600 −29 × 2970.
(c)
gcd(2679, 851) = 1.
1 = 31 −15 × 2
= 31 −15 × (95 −3 × 31)
= −15 × 95 + 46 × 31
= −15 × 95 + 46 × (126 −1 × 95)
= 46 × 126 −61 × 95
= 46 × 126 −61 × (851 −6 × 126)

754
Hints and Solutions to Selected Exercises
= −61 × 851 + 412 × 126
= −61 × 851 + 412 × (2679 −3 × 851)
= 412 × 2679 −1297 × 851.
2.
(i)
If a|b and b|c then a|c.
Proof
Suppose a|b and b|c. Then there exist m, n ∈N such that b = ma
and c = nb. Therefore c = (nm)a where nm ∈N, so a|c.
□
(iii)
a|b if and only if ma|mb.
Proof
(⇒) Suppose a|b. Then b = na for some n ∈N. Therefore
mb = (mn)a where nm ∈N, so ma|mb.
(⇐) Conversely, suppose ma|mb. Then mb = n(ma) for some
n ∈N. Therefore, since m ̸= 0, b = na so a|b.
□
3.
Theorem 9.2(a)
Let a, b, c ∈Z+. If c|a and c|b then c|ma + nb for all m, n ∈Z+.
Proof
Let a, b, c ∈Z+ and suppose that c|a and c|b. Then there exist p, q ∈Z+
such that a = pc and b = qc.
Now, for all m, n ∈Z+, ma + nb = mpc + nqc = c(mp + nq) where
mp + nq ∈Z+. Therefore c|(ma + nb).
□
7.
(i)
3|57 and 3|45 so 57 and 45 are not coprime.
In fact gcd(57, 45) = 3 but we don’t need to know this; it is
sufﬁcient to show that 57 and 45 have a common factor greater
than 1.
(iii)
112 and 117 differ by 5 so 5 is the only possible common prime
factor. Since 5 divides neither integer, 112 and 117 are coprime.
8.
(i)
(b)
gcd(1092, 1155, 2002)
gcd(1092, 1155) = 21
⇒gcd(1092, 1155, 2002) = gcd(gcd(1092, 1155), 2002)
= gcd(21, 2002)
= 7.

Hints and Solutions to Selected Exercises
755
9.
(ii)
Euclid’s algorithm to calculate gcd(an, an−1) is as follows.
an = 1 × an−1 + an−2
an−1 = 1 × an−2 + an−3
an−2 = 1 × an−3 + an−4
...
3 = 1 × 2 + 1
2 = 1 × 1 + 1
1 = 1 × 1 + 0.
Hence gcd(an, an−1) = 1.
(iv)
In general, for n ∈Z+,
1 = (−1)n+1anan+2 + (−1)n+2a2
n+1.
11.
Let a and b be coprime positive integers and let c be a positive integer
such that a|c and b|c.
Then there exist positive integers k and l such that
c = ka
and
c = lb.
By theorem 9.7 there exist integers n and m such that 1 = na + mb.
Therefore
c = c × 1
= c(na + mb)
= cna + cmb
= lbna + kamb
since c = lb and c = ka
= ab(ln + km)
where ln + km ∈Z+.
Hence ab|c as required.
□
Exercises 9.2
1.
(i)
(a)
1008 = 4 × 252 = 4 × 4 × 63 = 24 × 9 × 7 = 24 × 32 × 7.
(c)
3276 = 6×546 = 2×3×6×91 = 2×3×2×3×7×13 =
22 × 32 × 7 × 13.
(ii)
(b)
gcd(1008, 3276) = gcd(24 × 32 × 7, 22 × 32 × 7 × 13) =
22 × 32 × 7 = 252.

756
Hints and Solutions to Selected Exercises
2.
(ii)
The ﬁrst sequence of 5 consecutive composite positive integers is:
24, 25, 26, 27, 28.
3.
Let p = 6.
(a)
Let a = 8. Then p does not divide a and p and a are not coprime.
So theorem 9.8 (a) fails for p = 6.
(b)
Let a = 3 and b = 4 so ab = 12 Then p|12 but p does not divide
either a or b. So theorem 9.8 (b) fails for p = 6.
5.
If p = 3 then p2 + 2 = 11 is also prime.
If p ̸= 3 then p = 3q + r where r = 1 or r = 2.
When r = 1, p2 + 2 = (3q + 1)2 + 2 = 9q2 + 6q + 3 = 3(3q2 + 2q + 1)
which is divisible by 3.
When r = 2, p2 +2 = (3q +2)2 +2 = 9q2 +12q +6 = 3(3q2 +4q +2)
which is divisible by 3.
Therefore, the only prime p for which p2 + 2 also prime is p = 3.
7.
(i)
Let p be prime and a and b be positive integers such that
gcd(a, p2) = p and gcd(b, p2) = p.
Then a = k1p where gcd(k1, p) = 1 and, similarly, b = k2p where
gcd(k2, p) = 1. Hence ab = k1k2p2 where gcd(k1k2, p2) = 1.
Therefore gcd(ab, p4) = p2.
(iii)
Let gcd(a, b) = p. Then a = k1p and b = k2p where k1, k2 ∈Z+
are coprime.
Note that a2 = k2
1p2 so p2 divides a2.
First suppose k2 is not a multiple of p. Then p2 does not divide
b = k2p so gcd(a2, b) = p.
Now suppose that k2 is a multiple of p. Then p2 divides b = k2p
so p2 divides both a2 and b. In this case, k1 is not a multiple of
p so p2 is the highest power of p that divides a2 = k2
1p2. Hence
gcd(a2, b) = p2 in this case.
Therefore gcd(a2, b) = p or gcd(a2, b) = p2.
8.
Suppose that n ∈Z+ is such that √n is rational.
Let √n = a
b where a and b are integers.
By the Fundamental Theorem of Arithmetic, a and b each have (unique)
prime factorisations. It will be helpful to write a and b as products of
the primes using the same primes. This can always be done provided

Hints and Solutions to Selected Exercises
757
we allow zero powers. For example, we can write 20 = 22 × 5 and
105 = 3 × 5 × 7 as 20 = 22 × 30 × 5 × 70 and 105 = 20 × 3 × 5 × 7.
So suppose
a = pe1
1 pe2
2 . . . per
r
and
b = pf1
1 pf2
2 . . . pfr
r
where p1, p2, . . . , pr are prime and the ei and fj are natural numbers.
Now
n = a2
b2 = p2e1
1
p2e2
2
. . . p2er
r
p2f1
1
p2f2
2
. . . p2fr
r
= p2e1−2f1
1
p2e2−2f2
2
. . . p2er−2fr
r
=
³
pe1−f1
1
pe2−f2
2
. . . per−fr
r
´2
= m2
where m = pe1−f1
1
pe2−f2
2
. . . per−fr
r
.
Therefore n is a perfect square.
Exercises 9.3
1.
Any integer n is congruent to one of the numbers 0, 1, 2, . . . , 9 modulo
10 since these are the possible remainders after division by 10. Now:
n ≡0
mod 10 ⇒n2 ≡0
mod 10
n ≡1
mod 10 ⇒n2 ≡1
mod 10
n ≡2
mod 10 ⇒n2 ≡4
mod 10
n ≡3
mod 10 ⇒n2 ≡9
mod 10
n ≡4
mod 10 ⇒n2 ≡6
mod 10
n ≡5
mod 10 ⇒n2 ≡5
mod 10
n ≡6
mod 10 ⇒n2 ≡6
mod 10
n ≡7
mod 10 ⇒n2 ≡9
mod 10
n ≡8
mod 10 ⇒n2 ≡4
mod 10
n ≡9
mod 10 ⇒n2 ≡1
mod 10
Therefore n2 can only be congruent to 0, 1, 4, 5, 6 or 9 modulo 10. These
represent the possible ﬁnal decimal digits of n2.
Therefore the last
decimal digit of n2 cannot be 2, 3, 7 or 8.

758
Hints and Solutions to Selected Exercises
Hence 3190493 a not a perfect square since its last decimal digit is 3.
4.
(ii)
33 = 27 ≡4 mod 23 so 39 ≡43 = 16 × 4 ≡−7 × 4 = −28 ≡
−5 ≡18 mod 23.
(iv)
52 ≡2 mod 23 so 512 ≡26 = 2 × 32 ≡2 × 9 = 18 mod 23.
5.
(ii)
29147 ≡7 mod 10, so 291472 ≡72 ≡9 mod 10.
Hence
291475 ≡92 × 7 ≡7 mod 10. Therefore the last decimal digit
of 234595 is 7.
6.
(i)
This is true.
Proof
Note that (a −b)|(ak −bk). This is because
ak −bk = (a−b)(ak−1 +ak−2b+ak−3b2 +. . .+abk−2 +bk−1).
Therefore
a ≡b
mod n
⇒
n|(a −b)
⇒
n|(ak −bk)
⇒
ak ≡bk
mod n.
(ii)
This is false.
For example, 5 ≡1 mod 4
but
25 = 32 ≡0 mod 4
so
25 ̸≡21 mod 4.
7.
(i)
3x ≡4 mod 6 has no solutions.
(ii)
3x ≡4 mod 7 has unique solution x = 6.
(iii)
x2 ≡2 mod 5 has no solutions.
(iv)
x2 + 2 ≡0 mod 6 has solutions x = 2, 4.
(v)
x2 + 2 ≡0 mod 7 has no solutions.
(vi)
x2 ≡x mod 6 has solutions x = 0, 1, 3.
8.
(i)
gcd(12, 22) = 2 and 2 does not divide 15 so 12x ≡15 mod 22
has no solutions.
(iii)
gcd(19, 50) = 1 so 19x ≡42 mod 50 has a unique solution.
Note that 19 × 3 ≡7 mod 50 so 19 × 18 = (19 × 3) × 6 ≡
7 × 6 = 42 mod 50. Therefore the unique solution is x = 18.
(v)
gcd(65, 169) = 13 and 13 does not divide 27 so there are no
solutions.
(vi)
gcd(65, 169) = 13 and 13|39 so there are 13 solutions.
Dividing by 13 gives 5x ≡3 mod 13 which has unique solution
x = 11.

Hints and Solutions to Selected Exercises
759
Therefore the solutions to 65x ≡39 mod 169 are of the form
11 + 13q where q = 0, 1, 2, . . . , 12. These are:
11, 24, 37, 50, 63, 76, 89, 102, 115, 128, 141, 154, 167.
(viii) Since gcd(20, 637) = 1 and gcd(20, 101) = 1 there is a unique
solution but there is no further simpliﬁcation possible.
Clearly we can ‘get close’: 20 × 5 = 100 and we need to ‘ﬁnd’
another +1.
Note that 20 × 32 = 640 ≡3 mod 637 so 20 × 31 ≡3 −20 =
−17 mod 637.
Therefore 20 × (5 × 32 + 31) ≡5 × 3 −17 = −2 mod 637; in
other words, 20 × 191 ≡−2 mod 637.
Hence 20 × (32 + 191) ≡3 −2 = 1 mod 637; ie 20 × 223 ≡1
mod 637 so here is the ‘other’ +1 we were seeking.
Finally, 20 × 228 = 20 × (5 + 223) = 100 + 1 = 101 mod 637,
so the solution is x = 228.
Exercises 9.4
1.
(i)
φ(24) = φ(3) × φ(8) = 2 × 4 = 8.
(iii)
φ(31) = 30 since 31 is prime.
(v)
φ(170) = φ(10) × φ(17) = 4 × 16 = 64.
(vii)
φ(323) = φ(17) × φ(19) = 16 × 18 = 288.
2.
(i)
φ(9) = 6, φ(25) = 20 and φ(49) = 42.
(ii)
Conjecture: if p is prime, then φ(p2) = p(p −1).
3.
(i)
(a)
We evaluate M 7 = 77 mod 209 in stages.
73
≡
134
mod 209
⇒
76
≡
1342 ≡191
mod 209
⇒
77
≡
7 × 191 ≡83
mod 209.
(ii)
Since the encryption is M ′ = M 7 mod 209, we need to solve
7d ≡1 mod φ(209).
Now φ(209) = φ(11) × φ(19) = 10 × 18 = 180, so we need to
solve
7d ≡1
mod 180.
Since 7 × 25 = 175 ≡−5 mod 180 and 7 × 26 = 182 ≡2
mod 180, we have
7 × (3 × 26 + 25) ≡3 × 2 −5 = 1
mod 180.

760
Hints and Solutions to Selected Exercises
Therefore d = 3 × 26 + 25 = 103 so the decryption scheme is
M = (M ′)103
mod 209.
(c)
M ′ = 20 ⇒M = 20103 mod 209.
202
≡
191 ≡−18
mod 209
⇒
2010
≡
−185 ≡−208 ≡1
mod 209
⇒
20100
≡
1
mod 209
⇒
20103
≡
203 ≡58
mod 209.
Therefore M = 58.
4.
(i)
M ′ = M 13 mod 55
Since n = 55 = 5 × 11 we have φ(55) = 4 × 10 = 40.
Therefore we need to solve the equation 13d ≡1 mod 40.
Since 13 × 3 = 39 ≡−1 mod 40 it follows that d ≡−3 ≡37
mod 40.
Therefore the decryption scheme is
M ≡(M ′)37
mod 55.
(iii)
M ′ = M 7 mod 143
Since n = 143 = 11 × 13 we have φ(143) = 10 × 12 = 120.
Therefore we need to solve the equation 7d ≡1 mod 120.
Since 7 × 17 = 119 ≡−1 mod 120 it follows that d ≡−17 ≡
103 mod 120.
Therefore the decryption scheme is
M ≡(M ′)103
mod 143.
Chapter 10
Exercises 10.1
1.
(ii)
0
(iv)
0
(vi)
1.
4.
The identity with respect to ∗is 24 and the identity with respect to ⊕is
1. There are elements of B for which b ⊕¯b ̸= 24 so that axiom B5 is not
satisﬁed. For instance, if b = 4, ¯b = 6 and then b ⊕¯b = 12.

Hints and Solutions to Selected Exercises
761
The set of divisors of 42 together with three operations is a Boolean
algebra but the set of divisors of 45 is not.
5.
1 ⊕b = (b ⊕¯b) ⊕b
(axiom B5)
= ¯b ⊕(b ⊕b)
(axioms B3, B2)
= ¯b ⊕b
(theorem 9.3)
= 1
(axioms B3, B5).
Applying the duality principle gives 0 ∗b = b ∗0 = 0.
8.
(i)
(b1 ⊕b2) ∗¯b1 ∗¯b2 = (b1 ⊕b2) ∗(b1 ⊕b2)
(De Morgan’s law)
= 0
(axiom B5).
Dual: (b1 ∗b2) ⊕¯b1 ⊕¯b2 = 1.
(iii)
(b1 ⊕b2) ∗(¯b1 ⊕¯b2)
= [b1 ∗(¯b1 ⊕¯b2)] ⊕[b2 ∗(¯b1 ⊕¯b2)]
(axiom B4)
= (b1 ∗¯b1) ⊕(b1 ∗¯b2) ⊕(b2 ∗¯b1) ⊕(b2 ∗¯b2)
(axiom B4)
= 0 ⊕(b1 ∗¯b2) ⊕(b2 ∗¯b1) ⊕0
(axiom B5)
= (b1 ∗¯b2) ⊕(¯b1 ∗b2)
(axioms B1, B3).
Dual: (b1 ∗b2) ⊕(¯b1 ∗¯b2) = (b1 ⊕¯b2) ∗(¯b1 ⊕b2).
(v)
(b1 ⊕b2 ⊕b3) ∗(b1 ⊕b2)
= (b1 ⊕b2) ∗(b1 ⊕b2 ⊕b3)
(axiom B3)
= b1 ⊕b2
(absorption law).
Dual: (b1 ∗b2 ∗b3) ⊕(b1 ∗b2) = b1 ∗b2.
10.
Both conditions are necessary because b1 ∗b2 = b1 ∗b3 holds if b1 = 0
and b2 ̸= b3 and ¯b1 ∗b2 = ¯b1 ∗b3 holds if b1 = 1 and b2 ̸= b3.
11.
(ii)
If
b1 ∗b2 = b1
then
b1 ⊕b2 = (b1 ∗b2) ⊕b2
= b2 ⊕(b2 ∗b1)
(axiom B3)
= b2
(absorption law).
Exactly the same line of argument shows that, if b1 ⊕b2 = b2, then
b1 ∗b2 = b1.

762
Hints and Solutions to Selected Exercises
Exercises 10.2
1.
(i)
f(x1, x2) is already in disjunctive normal form.
(iii)
f(x1, x2) = x1x2.
(v)
f(x1, x2) = x1¯x2 ⊕x1x2.
(vi)
¯x1x2¯x3 ⊕¯x1x2x3 ⊕x1x2x3.
(ix)
¯x1¯x2x3 ⊕¯x1x2¯x3 ⊕¯x1x2x3 ⊕x1¯x2x3 ⊕x1x2x3.
(ii) and (v) are equal and so are (iii) and (iv). Also (vi) and (viii) are
equal, so are (ix) and (x).
2.
(i)
Disjunctive normal form:
f(x1, x2, x3) = ¯x1¯x2¯x3 ⊕¯x1x2¯x3 ⊕¯x1x2x3 ⊕x1¯x2¯x3
⊕x1¯x2x3 ⊕x1x2¯x3 ⊕x1x2x3.
Conjunctive normal form:
f(x1, x2, x3) = x1 ⊕x2 ⊕¯x3.
(iii)
Disjunctive normal form:
f(x1, x2, x3) = x1¯x2x3 ⊕x1x2¯x3 ⊕x1x2x3.
Conjunctive normal form:
f(x1, x2, x3) = (x1 ⊕x2 ⊕x3)(x1 ⊕x2 ⊕¯x3)(x1 ⊕¯x2 ⊕x3)
(x1 ⊕¯x2 ⊕¯x3)(¯x1 ⊕x2 ⊕x3).
5.
(i)
The atoms are {j}, {k}, {l}, {m}.
(ii)
Since a1 is an atom and a1a2 ̸= 0, a1a2 = a1.
Since a2 is an atom and a1a2 ̸= 0, a1a2 = a2.
Therefore a1 = a2.
Exercises 10.3
1.
(i)
f(x1, x2, x3) = x1(x2 ⊕x3).
(iii)
f(x1, x2, x3) = (x1 ⊕x2)x3¯x2.
(v)
f(x1, x2, x3) = x1[x2(x1 ⊕x3) ⊕x3¯x2].
2.
(ii)

Hints and Solutions to Selected Exercises
763
(iv)
3.
(i)
f(x1, x2, x3) = x1 ⊕x2(x1 ⊕x3).
In disjunctive normal form:
f(x1, x2, x3) = ¯x1x2x3 ⊕x1¯x2¯x3 ⊕x1¯x2x3 ⊕x1x2¯x3
⊕x1x2x3.
(ii)
f(x1, x2, x3) is the zero function. Current never ﬂows through this
circuit regardless of the state of the switches.
4.
A possible switching system (derived from the disjunctive normal form)
is the following.
6.
A possible switching system (derived from the disjunctive normal form)
is the following. Switch A1 is operated by the master switch and A2 and
A3 by the sensors.

764
Hints and Solutions to Selected Exercises
A simpler switching system which will achieve the same effect is as
follows.
Exercises 10.4
1.
(i)
x1x2 ⊕¯x3
(iii)
¯x1x2 ⊕¯x1x3
(v)
(x1x2 ⊕x1x2x3)x4.
2.
(ii)
(iv)
3.
(i)
x1x2 ⊕x2x3 and x2x3
(iii)
¯x1x2 ⊕¯x1x3, ¯x1x3 and ¯x1x2 ⊕¯x3x1.
4.
(i)
(ii)

Hints and Solutions to Selected Exercises
765
(iii)
6.
A simple circuit which achieves the desired output is the following.
7.
(ii)
A full-adder consists of two half-adders and an OR-gate arranged
as follows.
The reader is left to ﬁll in the details of the circuit.
Exercises 10.5
1.
(i)
x1x2x3 ⊕¯x1¯x2 ⊕¯x2¯x3
(ii)
x1 ⊕x2x3
(iii)
¯x1x2x3x4 ⊕¯x1¯x2¯x3¯x4 ⊕x1¯x2x3

766
Hints and Solutions to Selected Exercises
(iv)
¯x1¯x2x3 ⊕x1x2¯x3x4 ⊕x3¯x4
(v)
x1x3x4 ⊕¯x1x2x3¯x4 ⊕¯x2¯x3¯x4 ⊕x1¯x2¯x4.
2.
(i)
x1x2 ⊕x1¯x3
(ii)
x2x3 ⊕x1¯x2
(iii)
x3 ⊕¯x1x2
(iv)
x2x3 ⊕x1x3 ⊕x1x2.
3.
(i)
x1x3 ⊕¯x1x2
(ii)
x1 ⊕x2
(iv)
Circuit as in exercise 9.4.2(iv).
4.

Hints and Solutions to Selected Exercises
767
Chapter 11
Exercises 11.1
1.
2.
(ii)
The graph is simple.
3.
(i)
With the labelling shown, we can deﬁne δ by δ(eij) = {vi, vj}.

768
Hints and Solutions to Selected Exercises
(ii)
















0
1
0
0
1
1
0
0
0
0
1
0
1
0
0
0
1
0
0
0
0
1
0
1
0
0
0
1
0
0
0
0
1
0
1
0
0
0
1
0
1
0
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
1
1
0
0
1
0
0
0
0
0
0
1
1
0
0
1
0
0
1
0
0
0
1
0
0
0
1
0
1
1
0
0
0
0
0
0
0
1
0
1
1
0
0
















5.
(ii)
If the vertices are written in the order v1, v2, . . . , vp, w1, w2, . . . ,
wq the adjacency matrix is a partitioned matrix with p×p and q×q
zero matrices as the two submatrices on the diagonal:
µ
Op×p
A
B
Oq×q
¶
.
6.
(i)
No—apply the algorithm in (iii) below.
(ii)
Cn is bipartite if and only if n is even.
(iii)
1.
Choose any vertex and colour it red, and colour all adjacent
vertices green.
2.
Consider each green vertex in turn and colour every
uncoloured adjacent vertex red.
3.
Consider each red vertex in turn and colour every
uncoloured adjacent vertex green.
4.
Repeat steps 2 and 3 until all vertices are coloured.
5.
If any pair of adjacent vertices have the same colour then
the graph is not bipartite; otherwise it is bipartite.
7.
(i)
The handshaking lemma follows from the fact that each edge
connects two vertices (or a vertex to itself) and so contributes two
to the sum of the vertex degrees.
8.
(i)
Figure 10.2:
(2, 2, 2, 2, 2, 2, 2)
Figure 10.3:
(3, 3, 3, 3, 3, 3, 3, 3, 3, 3)
Figure 10.4:
K3
(2, 2, 2)
K4
(3, 3, 3, 3)
K5
(4, 4, 4, 4, 4)
Figure 10.5
(a)
(2, 2, 2, 2, 3, 3, 4, 4, 6)
(b)
(3, 3, 3, 3, 3, 3)
Figure 10.6
Γ
(3, 3, 3, 4, 5)
Σ
(1, 1, 2, 4).

Hints and Solutions to Selected Exercises
769
(iii)
(a)
The number of vertices.
(b)
Twice the number of edges (by the handshaking lemma—
exercise 10.1.7(i)).
9.
(i)
(iii)
(v)
There is no graph with this vertex sequence since, for example,
there are three vertices with odd degree (see exercise 10.1.7(ii)).
10.
(i)
(ii)
11.
(i)
(3, 3, . . . , 3, n)
←−n terms−→
(ii)
The adjacency matrix is an (n + 1) × (n + 1) matrix which can be
partitioned as





1
A
...
1
1
. . .
1
0





where A is the n × n adjacency matrix of Cn. The matrix A
has 1s immediately above and below its leading diagonal and 0s
elsewhere:
A =













0
1
0
. . .
0
1
0
1
0
0
1
0
1
0
...
0
...
...
...
...
...
...
1
0
0
1
0
1
0
. . .
0
1
0













13.
Show that, for Kn, |E| = 1
2n(n −1). Any simple graph with n vertices
cannot have more edges than Kn.

770
Hints and Solutions to Selected Exercises
14.
(i)
If Np denotes a null graph with p vertices, then Np + Nq = Kp,q.
(ii)
Kp + Kq = Kp+q.
16.
(ii)
The sum of the entries is always two.
(iii)
It is the degree of the vertex.
17.
(i)
(ii)
Exercises 11.2
2.
The only difﬁculty is in proving transitivity. If P is a path from u to v
and Q is a path from v to w, then the edge sequence ‘P followed by Q’ is
an edge sequence from u to w, but it may not be a path as P and Q may
have edges in common. If this is the case the edge sequence needs to be
modiﬁed by omitting some edges to give the required path from u to w.
3.
(i)
Yes.
(ii)
Yes, if the graph has a single non-null Eulerian component and its
other components are isolated vertices. For example, a null graph
with more than one vertex is (trivially) Eulerian since it has no
edges.
4.
(i)
Kn is Eulerian if and only if n is odd.
(ii)
Kr,s is Eulerian if and only if both r and s are even.
5.
(i)
Only (a) and (d) are Eulerian.
(ii)
Deﬁne a ‘modiﬁed row sum’ to be the sum of the entries in
a row except that the diagonal element is doubled.
Then the
degrees of the vertices are the modiﬁed row sums of the adjacency
matrix. Hence, by Euler’s theorem (10.2), the (connected) graph
is Eulerian if and only if every modiﬁed row sum is even.
7.
The trick is to notice that adding an appropriate edge to a semi-Eulerian
graph produces an Eulerian graph.

Hints and Solutions to Selected Exercises
771
8.
The following are Hamiltonian cycles in each of the graphs.
9.
(ii)
Kr,s is Hamiltonian if and only if r = s ⩾2.
10.
(i)
Figure 10.2 and K3 and K5 (ﬁgure 10.4).
(ii)
Figure 10.1, ﬁgure 10.5(a) and Σ (ﬁgure 10.6).
(iii)
Figures 10.2, 10.4 (all three graphs), 10.5(b) and Γ (ﬁgure 10.6).
(iv)
Every graph except ﬁgure 10.1 and those in (iii).
11.
(i)
(a)
One
(b)
Two.
(ii)
Yes, the graph is Hamiltonian.
12.
(i)
Graph I is semi-Eulerian; graph II is neither Eulerian nor semi-
Eulerian.
(ii)
Graph I is semi-Hamiltonian; graph II is Hamiltonian.
13.
Let Γ be a connected graph.
Suppose that Γ is Eulerian. Then every vertex has even degree. We
know from the proof of Euler’s theorem that we can choose a cycle in
Γ. Delete the edges of the cycle to produce a new graph Γ′ (which may

772
Hints and Solutions to Selected Exercises
be disconnected) in which every vertex has even degree. Consider (a
component of) Γ′ and choose a cycle, which is certainly edge-disjoint
from the ﬁrst chosen cycle. Remove the edges of the cycle, producing a
graph Γ′′ in which every vertex has even degree. Continue in this way
until all the edges of Γ are included in one (and only one) of the chosen
cycles.
Conversely suppose that Γ is such that its edges form disjoint cycles.
Select one cycle, C1 say, choose a vertex and start traversing its edges
until another a cycle is reached. Traverse the edges of this cycle (as
with C1, breaking off if we meet any other cycle, etc) before resuming
traversing the edges of C1. Continuing in this way we obtain a closed
path containing all the edges of Γ.
For the given graph, a possible choice of cycles is: afea, edge, bdcb,
agba.
Beginning with the cycle afea and starting at a the process described
above unfolds as follows.
Traverse af, fe
(break off to go round edge)
traverse ed
(break off to go round bdcb)
traverse dc, cb
(break off to go round agba)
traverse ba, ag, gb
(resume bdcb)
traverse bd
(resume edge)
traverse dg, ge
(resume afea)
traverse ea.
The resulting Eulerian path is: afedcbagbdgea.
Exercises 11.3
1.
One matrix can be converted to the other by a reordering of the rows and
columns, the same reordering being applied to both rows and columns.
2.
No. Each graph has two vertices of degree 2, which are adjacent in (ii)
but not in (i).

Hints and Solutions to Selected Exercises
773
4.
(i)
There are various possibilities. For example:
The two vertices of degree 2 are adjacent in graph A but not in
graph B; hence the two graphs are not isomorphic.
5.
The degree sequence is (2, 2, 2, 3, 3, 3, 3, 4)
The graphs Γ1 and Γ2 are not isomorphic since the vertex of degree 4 in
Γ1 is adjacent to three vertices of degree 2 but the vertex of degree 4 in
Γ2 is adjacent to only one vertex of degree 2.
6.
Γ1 ∼= Γ2.
8.
(i)
Graphs 5, 6, 8, 9, 10 and 11 are connected.
(ii)
There are 34, of which 21 are connected!

774
Hints and Solutions to Selected Exercises
9.
With the vertex labellings given below, the isomorphism has as its vertex
bijection the mapping, vi 7→wi. Since the graphs are both simple, this is
sufﬁcient to deﬁne the isomorphism.
10.
(i) and (ii) follow from theorem 5.7(ii) applied to the vertex and edge
bijections respectively.
(iii)
If e1, e2, . . . , en is a path connecting v1 and v2 in Γ, then
φ(e1), φ(e2), . . . , φ(en) is a path connecting θ(v1) and θ(v2) in
Σ. Conversely, if f1, f2, . . . , fm is a path connecting w1 and w2 in
Σ, then φ−1(f1), φ−1(f2), . . . , φ−1(fm) is a path in Γ connecting
θ−1(w1) and θ−1(w2).
Therefore v1 and v2 lie in the same component of Γ if and only if
θ(v1) and θ(v2) lie in the same component of Σ. Hence Γ and Σ
have the same number of components.
(iv)
deg(v) =
X
w∈VΓ
|E(v, w)| and deg(θ(v)) =
X
w′∈VΣ
|E(θ(v), w′)|.
The result follows since θ is a bijection and the function φ deﬁnes
bijections E(v, w) →E(θ(v), θ(w)).
(v)
Γ is simple if and only if |E(v, w)| ⩽1 and |E(v, v)| = 0 for all
v, w ∈VΓ. As in (iv) the result follows from the existence of the
bijections E(v, w) →E(θ(v), θ(w)).
(vi)
Follows from (iv) using theorem 10.2.
(vii)
If e1, e2, . . . , en is a Hamiltonian cycle in Γ then φ(e1), φ(e2),
. . . , φ(en) is a Hamiltonian cycle in Σ.
Exercises 11.4
1.
(i)
1
(ii)
2
(iii)
3
(iv)
6.
2.
One method of proof is by induction on the number n of vertices,

Hints and Solutions to Selected Exercises
775
beginning with n = 2 for which the result is obvious. Suppose every
tree with n vertices is bipartite and let T be a tree with n + 1 vertices.
Removing a vertex v of degree 1 and its incident edge from T produces a
tree with n vertices which is bipartite, by hypothesis. Let {V1, V2} be the
partition of the vertex set of this tree and suppose that the vertex adjacent
to v in T belongs to V1. Then {V1, V2 ∪{v}} is a partition of the vertex
set of T, so T is bipartite.
4.
(i)
(ii)
Such a graph has seven vertices (since there are seven entries in
the degree sequence) and 1
2(1 + 1 + 2 + 2 + 2 + 3 + 3) = 7 edges.
Therefore, by theorem 10.6, the graph is not a tree.
5.
Kr,s is a tree if and only if r = 1 or s = 1.
6.
(iii)
(a)
1
(b)
2
(c)
3.
7.
(i)
Level 1:
Level 2:
(ii)
A level n full binary tree looks like the following where 0 ⩽i,
j ⩽n −1 and at least one of i and j is equal to n −1.
If j = 0 then i = n −1: there are a0an−1 such trees;
if j = 1 then i = n −1: there are a1an−1 such trees;
. . .

776
Hints and Solutions to Selected Exercises
if j = n −2 then i = n −1: there are an−2an−1 such trees;
if j = n −1 then i can take any value from 0 to n −1 inclusive:
there are an−1(a0 + a1 + · · · + an−1) such trees.
Hence the total number of level n full binary trees is:
an = a0an−1 + a1an−1 + · · · + an−2an−1
+ an−1(a0 + a1 + · · · + an−1)
= 2an−1(a0 + a1 + · · · + an−2) + a2
n−1.
(iii)
We know a0 = 1 and, from part (i), a1 = 1, a2 = 3. Therefore
a3 = 2a2(a0 + a1) + a2
2
= 2 × 3 × (1 + 1) + 32 = 21,
a4 = 2a3(a0 + a1 + a2) + a2
3
= 2 × 21 × (1 + 1 + 3) + 212 = 651.
9.
(i)
|E| = |V | −c.
(ii)
|E| ⩾|V | −c.
10.
K2,n has n × 2n−1 spanning trees.
Proof
Suppose the partition of V is {{x, y}, {1, 2, 3, . . . , n}} so that we may represent
K2,n as shown in the following diagram.
We can deﬁne a spanning tree of K2,n as follows. First choose a vertex, k say,
in the set {1, 2, . . . , n} that is to be joined to both x and y. Then, for each vertex
r ∈{1, 2, . . . , n} −{k}, choose either rx or ry to be an edge of the spanning

Hints and Solutions to Selected Exercises
777
tree. There are n choices for the vertex k and then 2×2×· · ·×2 = 2n−1 choices
for the remaining edges. One set of choices is illustrated below, where k = 2.
Each of the n × 2n−1 choices gives rise to a unique spanning tree and each
spanning tree is obtained in this way; hence there are n × 2n−1 spanning trees.
11.
(iii)
Γ has 21 spanning trees.
There are various ways of using the
equation in part (i) to show this. For example:
t
Ã
!
= t
Ã
!
+ t
Ã
!
= 3 × 3 + t
Ã
!
+ t
Ã
!
= 9 + t
Ã
!
+ t
Ã
!
+ t
Ã
!
+ t
Ã
!
= 9 + 2 + 3 + 3 + 4 = 21.
12.
(i)
Graph A has nine spanning trees; graph B has 8×4 = 32 spanning
trees.
(ii)
If Γ is a connected graph with a bridge vw (like graph A),
then removing vw produces a graph with two components, say
Γ1 and Γ2. The number of spanning trees in Γ is the product
of the numbers of spanning trees in each of these components.
Symbolically, if t(Γ) denotes the number of spanning trees in Γ
(as in exercise 10.4.10), then
t(Γ) = t(Γ1)t(Γ2).
The result is similar for a connected graph Σ with a cut vertex
v (such as graph B). However the situation is a little more

778
Hints and Solutions to Selected Exercises
complicated.
Removing v produces a graph with a number
of components Σ1, Σ2, . . . , Σm, but none of these components
contains the vertex v itself. We need to ‘put v back’ into each of
these graphs, together with the appropriate edges, before counting
the number of spanning trees in each of the resulting subgraphs.
The number of spanning trees in Σ is then the product of the
numbers of spanning trees in these subgraphs.
13.
(i)
(ii)
|V | = n + (2n + 2) = 3n + 2, |E| =
1
2(4n + (2n + 2)) =
3n + 1. Therefore |E| = |V | −1, so the graph is a tree, by
theorem 10.6(iii).
(iii)
(iv)
Pentane has three structural isomers; hexane ﬁve.
Exercises 11.5
2.
(i)
K2 is a tree and ﬁgure 10.4 shows K3 and K4 are planar. If n ⩾5,
then Kn contains a subgraph isomorphic (hence homeomorphic)
to K5 so Kn is not planar.

Hints and Solutions to Selected Exercises
779
(ii)
The following are plane versions of K1,n and K2,n.
3.
The left-hand graph is not planar: deleting the three vertices of degree 2
produces a graph which clearly contains K5 as a subgraph.
The right-hand graph can be redrawn as follows.
4.
(ii)
(a) is planar; (b) is non-planar.
5.
(i)
Since the graph has only three vertices of degree 4 or more, it does
not contain a subgraph which is a subdivision of K5. Therefore
we look for a subgraph which is a subdivision of K3,3.
Firstly delete the thin edges and then remove the resulting isolated
vertices. Removing the square vertex of degree 2 then gives the
graph shown below, which is K3,3.

780
Hints and Solutions to Selected Exercises
Therefore, the given graph has a subgraph which is a subdivision
of K3,3, so it is non-planar.
7.
(i)
Successively deleting vertices of degree 2 produces the following
two isomorphic graphs. (The isomorphism has vertex mapping
vi 7→wi.)
8.
(i)
Every face is bounded by a cycle which contains at least three
edges, and each edge forms the boundary between two faces.
Hence twice the number of edges is at least three times the number
of faces, 2|E| ⩾3|F|.
The inequality |E| ⩽3|V | −6 comes from substituting 2|E| ⩾
3|F| into Euler’s formula.
(ii)
Let Γ be a connected simple planar graph.
Suppose that each
vertex has degree 6 or more; then 2|E| ⩾6|V | so |E| ⩾
3|V |. However, from (i) we have |E| ⩽3|V | −6, which is a
contradiction.
9.
(ii)
Let v, e, f denote the number of vertices, edges and faces of Γ and
similarly let v∗, e∗, f ∗denote the number of vertices, edges and
faces of Γ∗.
Then v∗= f, e∗= e and f ∗= v.
10.
(i)
Since Γ has only three vertices of degree at least 4, it does not
contain a subgraph which is a subdivision of K5.
Since Γ has only ﬁve vertices of degree at least 3, it does not
contain a subgraph which is a subdivision of K3,3.
Therefore, by Kuratowski’s theorem 10.8, Γ is planar.
(ii)
Γ has n = 8 vertices. The sum of vertex degrees is 26, so Γ has
m = 13 edges (by the handshaking lemma—exercise 10.1.7(i)).
By Euler’s formula (theorem 10.7), if Γ has f faces, then 8 −13 +
f = 2 so f = 7.

Hints and Solutions to Selected Exercises
781
(iii)
Using the terminology introduced in solution 9 above, the dual Γ∗
has
n∗= f = 7 vertices,
m∗= m = 13 edges,
f ∗= n = 8 faces.
and
11.
(i)
For all graphs on the sphere, |V | −|E| + |F| = 2. (In fact a graph
can be drawn on the surface of the sphere if and only if it can be
drawn in the plane.)
(ii)
For all graphs on the torus, |V | −|E| + |F| = 0.
12.
The following are diagrams of K5 and K3,3 drawn on the surface of
a torus.
(For these graphs we have (|V |, |E|, |F|) = (5, 10, 5) and
(|V |, |E|, |F|) = (6, 9, 3) respectively.)
Exercises 11.6
1.
Let A and B be the adjacency matrices of a digraph and its underlying
graph respectively. If the digraph has no directed loops, B = A + AT.
More generally, B = A + AT −J where J is the diagonal matrix whose
diagonal entries are the number of directed loops from the corresponding
vertex to itself.
3.
(i)
In a tree there is a unique path joining any pair of distinct vertices
v and w. If, in the digraph, the path is directed from v to w, then
there cannot be any directed path from w to v.
(ii)
Yes: a cycle graph with all edges directed clockwise is an example.

782
Hints and Solutions to Selected Exercises
4.
(i)
5.
(a)
(i), (ii), (iii) and (iv) are simple.
(b)
(ii), (iii) and (iv) have simple underlying graphs.
(c)
(i), (ii) and (iii) are strongly connected.
(d)
Only (iii) is Eulerian.
(e)
(ii) only is Hamiltonian.
7.
(i)
Graph (a) is unilaterally connected but not strongly connected.
Graph (b) is weakly connected but not unilaterally connected.
(ii)
Graphs (i), (ii) and (iii) are unilaterally connected—in fact,
strongly connected.
8.
The following is the diagram of a strongly connected digraph whose
underlying graph is Petersen’s graph.
9.
(i)
Labelling the edges as shown gives the corresponding incidence
matrix.

Hints and Solutions to Selected Exercises
783












1
−1
0
0
0
0
1
−1
0
0
0
0
−1
1
0
0
0
0
−1
1
−1
0
0
0
1
0
1
0
−1
0
0
−1
0
1
0
0
−1
0
1
0












(ii)
Row sum = out-degree of corresponding vertex.
Column sum = in-degree of corresponding vertex.
(iii)
Row sum = 0 (so provides no information about the digraph).
Column sum = out-degree −in-degree of corresponding vertex.
(iv)
(v)
Let A be the adjacency matrix of the digraph and AT be its
transpose (see exercise 6.1.6). Let B be the adjacency matrix of
the underlying graph.
If the digraph has no loops: B = A + AT.
More generally, B = A + AT −J where J is a diagonal matrix
with diagonal entries equal to the number of loops at each of the
vertices.
(vi)
Let C be the incidence matrix of the digraph and D be the
incidence matrix of the underlying graph.
Then dij = |cij|.

784
Hints and Solutions to Selected Exercises
11.
Let D1 and D2 be two digraphs. An isomorphism D1 →D2 is a pair
(θ, φ) of bijections θ : V1 →V2 and φ : E1 →E2 such that, for every
directed edge e ∈E1, if δD1(e) = (v, w) then δD2(φ(e)) = (θ(v), θ(w)).
Isomorphic digraphs have isomorphic underlying graphs,
but not
conversely.
12.
(ii)
The underlying graphs of D1 and D3 are isomorphic.
13.
D2 and D3 (only) are isomorphic.
14.
(i)
Let D be the digraph
. Then D ∼= ˜D.
Let D be the digraph
. Then D ≇˜D.
(ii)
Using A(D) and A( ˜D) to denote the adjacency matrices of D and
˜D respectively, A(D) = A( ˜D)T .
(iii)
Similarly, using B for the incident matrices, B(D) = −B( ˜D).
15.
(i)
(ii)
Chapter 12
Exercises 12.1
1.
(i)
(a) 2
(b) Yes
(c) No.
(iii)
(a) 6
(b) No
(c) No.
(v)
(a) 3
(b) Yes
(c) No.
4.
Let v and w be vertices of a rooted tree. Deﬁne v to be an ancestor of w
(and w be a descendant of v) if there exist vertices v1, v2, . . . , vn such
that v is the parent of v1, v1 is the parent of v2, . . . , vn is the parent of w.
(An alternative deﬁnition is: v is an ancestor of w if the level of v is less
than the level of w and the unique path in the tree joining the root and w
also contains v.)
R is (by deﬁnition) reﬂexive. Suppose v R w and w R v. If v is an
ancestor of w then w is not an ancestor of v, from which it follows

Hints and Solutions to Selected Exercises
785
that v = w, so R is anti-symmetric. Suppose u R v and v R w. Then
level(u) ⩽level(v) ⩽level(w) and the unique path joining the root to
v contains u and the unique path from the root to w contains v. Therefore
the unique path from the root to w contains u so u R w. Hence R is
transitive.
5.
(i)
(ii)
6.
(i)
T = (L, {v∗}, R) where:
L = (L1, {v1}, R1)
and
R = (L2, {v2}, R2)
L1 = (∅, {v3}, ∅)
and
R1 = (L4, {v4}, R4)
L2 = (∅, {v5}, ∅)
and
R2 = (L6, {v6}, R6)
L4 = (L7, {v7}, R7)
and
R4 = (∅, {v8}, ∅)
L6 = (L9, {v9}, R9)
and
R6 = (∅, {v10}, ∅)
L7 = (∅, {v11}, ∅)
and
R7 = (L12, {v12}, R12)
L9 = (∅, {v13}, ∅)
and
R9 = (∅, {v14}, ∅)
L12 = (∅, {v15}, ∅)
and
R12 = (∅, {v16}, ∅).
(ii)
7.
First deﬁne level(v∗) = 0. Given (L, {v}, R) where L = (L′, {v′}, R′)
and R = (L′′, {v′′}, R′′), deﬁne level(v′) = level(v′′) = level(v) + 1.

786
Hints and Solutions to Selected Exercises
8.
A has a greatest element a and for all minimal elements x ∈A there is
exactly one chain containing both x and a.
10.
(i)
(b)
(d)
(ii)
(a) 2
(b) 5
(c) 5.
If ∗is associative: (a) 1
(b) 3
(c) 4.
11.
(i)
(a) ∗⊕xy ⊕zt
(c) ⊕∗rs ∗⊕xyz
(e) ∗⊕⊕∗rsxyz.
12.
(i)
(a) xy ⊕zt ⊕∗
(c) rs ∗xy ⊕z ∗⊕
(e) rs ∗x ⊕y ⊕z∗.
13.
(i)
Inﬁx:
(x ⊕y) ∗(x ⊕(t ∗z))
Preﬁx:
∗⊕xy ⊕x ∗tz
Postﬁx:
xy ⊕xtz ∗⊕∗.
14.
(i)
It is not a full binary tree: any vertex labelled with the complement
operation ¯ has a single child compared with the vertices labelled
⊕or ∗which have two children (a left child and a right child).
(ii)
(b)
Exercises 12.2
1.
(i)
(ii)

Hints and Solutions to Selected Exercises
787
2.
The sorted list is more quickly obtained from (i) because the tree has
smaller height and is more ‘balanced’.
3.
(ii)
Step 1 produces the following sort tree.
The full listing procedure is the following.
Step 1: Process left-subtree(bca)
Step 1: Process left-subtree(acb)
Step 1: Process left-subtree(abc)
Step 1: Process left-subtree(aba) [Empty]
Step 2: List aba
Step 3: Process right-subtree(aba) [Empty]
Step 2: List abc
Step 3: Process right-subtree(abc)
Step 1: Process left-subtree(aca) [Empty]
Step 2: List aca
Step 3: Process left-subtree(aca) [Empty]
Step 2: List acb
Step 3: Process right-subtree(acb)
Step 1: Process left-subtree(bac)
Step 1: Process left-subtree(bab) [Empty]
Step 2: List bab
Step 3: Process right-subtree(bab) [Empty]
Step 2: List bac
Step 3: Process right-subtree(bca) [Empty]
Step 2: List bca
Step 3: Process right-subtree(bca)
Step 1: Process left-subtree(cbc)
Step 1: Process left-subtree(bcb) [Empty]
Step 2: List bcb
Step 3: Process right-subtree(bcb)
Step 1: Process left-subtree(cac)
Step 1: Process left-subtree(cab) [Empty]
Step 2: List cab

788
Hints and Solutions to Selected Exercises
Step 3: Process right-subtree(cab) [Empty]
Step 2: List cac
Step 3: Process right-subtree(cac)
Step 1: Process left-subtree(cba) [Empty]
Step 2: List cba
Step 3: Process right-subtree(cba) [Empty]
Step 2: List cbc
Step 3: Process right-subtree(cbc) [Empty]
The sorted list is:
aba, abc, aca, acb, bab, bac, bca, bcb, cab, cac, cba, cbc.
4.
(i)
(ii)
5.
(i)
The following sequence of rooted trees shows the positions after
each new vertex is ﬁxed and the remaining part of the tree restored
to a heap.

Hints and Solutions to Selected Exercises
789
8.
In any full binary tree there are an even number of vertices at each level
greater than or equal to one. (This can be proved by induction.) Since
there is only one root, a full binary tree has an odd number of vertices.
The construction of a heap ensures that either it is full or it has a single
parent vertex with only one child. The latter cannot occur if there is an
odd number of vertices. Hence a heap is a full binary tree if and only if it
has an odd number of vertices.
A complete full binary tree has 2k vertices at level k. (Again this can be
proved by induction.) Therefore if the height of the tree is n, the total
number of vertices is 1 + 2 + 22 + · · · + 2n = 2n+1 −1. (Once more,
prove by induction.) Therefore a heap is a complete full binary tree if and
only if it has 2n −1 vertices for some n ∈Z+.
Exercises 12.3
1.
(iii)
Graph (a) shows a spanning tree produced by a depth-ﬁrst search
where no backtracking is required. Graph (b) shows a spanning
tree produced by a breadth-ﬁrst search completed in three phases.
In both cases the vertices are labelled in the order in which they
are visited.
(vi)
Graph (a) shows a spanning tree produced by a depth-ﬁrst search
where two backtrackings were required—these are indicated
by the vertices labelled in bold type.
(The second of these
backtrackings could have been avoided. At least one backtracking
appears to be necessary, however.) Graph (b) shows a spanning
tree produced by a breadth-ﬁrst search completed in four phases.
Again the vertices are labelled in the order in which they are
visited.

790
Hints and Solutions to Selected Exercises
2.
It is clear that, if Tn is a tree, then adjoining en to form Tn+1 does not
create a cycle, so Tn+1 is also a tree. Since a single vertex is a tree, the
algorithm produces only trees, by induction. Suppose that Tm is the ﬁnal
tree produced by the algorithm and Tm is not a spanning tree. Then there
exists a vertex, w say, not belonging to Tm. Let W be the set of vertices
in Γ which are adjacent to w. Since Γ is connected, W is non-empty.
Then Tm could not have been the ﬁnal tree because backtracking would
have eventually set an element v of W as the current centre and the edge
joining v to w could then have been adjoined. This contradiction shows
that Tm must be a spanning tree.
3.
The proof is similar to that in question 11.3.2 above.
4.
The graphs labelled (a) show the result of a depth-ﬁrst search and those
labelled (b) show the result of a breadth-ﬁrst search. In all cases, the
vertices are labelled in the order in which they are visited.
(a)
(b)
(a)
(b)

Hints and Solutions to Selected Exercises
791
5.
(i)
(iii)
Which of the two searches will reach X more quickly depends on
the choices made. Most probably, though, the depth-ﬁrst search
will be preferred.
6.
The following is the graph of a maze. The tree resulting from the partial
searches depends on the choices made.
7.
If Γ is semi-Hamiltonian there exists a simple path containing all the
vertices of Γ.
By removing an edge from the path if necessary, we
may assume that it is not closed; it is therefore a spanning tree for
Γ. The depth-ﬁrst search algorithm can begin at one end of this path
and successively add the next edge in the path, without ever needing to
backtrack, until the other end is reached.
Conversely suppose that the depth-ﬁrst search algorithm can be
performed without backtracking to produce a spanning tree T. Since no
backtracking takes place, no vertex T can have degree greater than two.
Therefore the edges of T, in the order in which they are added, are a
simple path which contains every vertex of Γ since T is a spanning tree.

792
Hints and Solutions to Selected Exercises
Exercises 12.4
1.
(i)
(iii)
(ii)
Weight = 54
(iv)
Weight = 20.
2.
For (i), (iii) and (iv) the resulting spanning tree is the same as the one
obtained by applying Prim’s algorithm. There are choices to be made
in (ii) so the spanning tree may be different from the one obtained by
applying Prim’s algorithm.
3.
(i)
The statement is not true. The following is a counter-example.
(ii)
Yes. Using Kruskal’s algorithm (question 2), at each stage there
is only one choice of edge to be added and the result is the
unique minimal cycle-free subgraph containing the given number
of edges.
5.
6.
(i)
A spanning tree T has v vertices and hence v −1 edges (by
theorem 10.6). The smallest possible weight for T is 1 + 2 +
· · · + (v −1) = 1
2v(v −1).
(ii)
In this case v = 2n so, by part (i), if T is a spanning tree then
w(T) ⩾n(2n −1). The following graph with n = 3 is a suitable

Hints and Solutions to Selected Exercises
793
example. A minimum spanning tree (shown) has weight 17 but
n(2n −1) = 3 × 5 = 15.
7.
We use either Prim’s or Kruskal’s algorithm to produce the following
spanning tree.
The company can remove the pipes corresponding to edges not in the
spanning tree, resulting in a total saving of 56 units.
Exercises 12.5
1.
(i)
(iii)
2.
n = 10
n = 40
n = 70
105n6
2.8 h
474 days
37 years
10−92n
10−13 s
10−4 s
32.8 h
3.
(a)
When assigning temporary labels (step 2) the only vertices
considered are those for which there is an edge directed away from
the most recently permanently labelled vertex to the given one.

794
Hints and Solutions to Selected Exercises
If there is no directed path, a stage will be reached where there
are no new edges directed from a permanently labelled vertex to a
vertex without permanent label.
(b)
(i)
4.
(i)
The shortest path has length 15.
(ii)
The shortest path has length 13.
Graph (a) below shows a diagram of the graph (without the weights of
the edges). Graph (b) shows a tree which contains both the shortest path
from v1 to v8 and the shortest path from v4 to v10.
(a)
(b)
5.
(i)
(ii)
In both cases, starting at a different vertex can result in a different cycle.

Hints and Solutions to Selected Exercises
795
7.
The following are the ‘weight matrices’ for the complete graphs: for
i ̸= j the (i, j)-entry is the weight of the edge joining vi and vj.
(i)








0
10
18
24
14
9
10
0
12
18
24
19
18
12
0
6
14
21
24
18
6
0
10
15
14
24
14
10
0
7
9
19
21
15
7
0








(ii)










0
9
19
20
15
7
15
9
0
10
12
21
12
7
19
10
0
10
19
14
6
20
12
10
0
9
13
5
15
21
19
9
0
10
14
7
12
14
13
10
0
8
15
7
6
5
14
8
0










.
Exercises 12.6
1.
(i)
Minimum completion time = 27
(ii)
Minimum completion time = 48
(iii)
Minimum completion time = 40
(iv)
Minimum completion time = 63.

796
Hints and Solutions to Selected Exercises
2.
(i)
(ii)
(iii)
Value of a maximal ﬂow/capacity of a minimal cut = 31
(iv)
Value of a maximal ﬂow/capacity of a minimal cut = 45.
3.
(i)
(a)
Minimum completion time = 46. The critical path shown
is not unique.
(b)
Value of maximal ﬂow/capacity of minimal cut = 29.

Hints and Solutions to Selected Exercises
797
(ii)
(a)
Minimum completion time = 85
(b)
Maximal ﬂow has value 46.
5.
Suppose edges e1, e2, . . . , en are the edges of a directed cycle
representing activities A1, A2, . . . , An.
Then A1 must be completed
before A2 can begin, A2 must be completed before A3 can begin, and
so on.
In particular, A1 must be completed before An can begin.
However, since e1, e2, . . . , en are the edges of a directed cycle, An must
be completed before A1 can begin, which is not possible.
6.
(i)
(ii)
Minimum completion time = 29.

Index
Abel, Niels, 371
Abelian group, 374, 408, 413
Abelian monoid, 373
Abelian semigroup, 371
Absorption laws
in a Boolean algebra, 499
for sets, 105
Absorption Rule, 30
Addition
modulo n, 185, 375–76, 377,
386–7, 395, 400, 402–3,
407–8, 409–11, 414–5,
535–6
of cardinalities, 275
of functions, 268
of integers, 366
of matrices, 297–9, 376
Rule, for propositions, 30
Additive group, 385
Additive inverse of a matrix, 298,
318
Adjacency matrix
of a digraph, 601–2
of a graph, 555–7
Adjacent edges of a graph, 552
Adjacent vertices of a graph, 552
Aleph zero, 271
Algebra, Boolean, see Boolean
algebra
Algebra of propositions, 22–5, 104
laws of, 498
Algebra of sets, 104–8
laws of, 498
Algebraic expression
inﬁx form of, 625
Polish form of, see Algebraic
expression, preﬁx form of
postﬁx form of, 625
preﬁx form of, 625
reverse Polish form of, see
Algebraic expression,
postﬁx form of
rooted tree representation of,
624–6
Algebraic structures, 370
morphisms of, 412–8
substructures of, 396–404
Algorithm
breadth-ﬁrst search, 647–9
computational efﬁciency of,
666–7
decryption, 480-1, 484–90
depth-ﬁrst search, 644–6
Dijkstra’s, 661–3
Division, 437–8
encryption, 480-1, 484–90
Euclid’s greatest common
divisor, 442–3
exponential time, 666
for creating a heap from a list,
798

Index
799
635–7
for growing a sort tree from a
list, 630–2
for obtaining a sorted list from a
heap, 638–40
for row reduction to an identity
matrix, 325–6
greedy, 658, 667
Kruskal’s, 658
Nearest insertion, 668–70
Nearest neighbour, 667–8
polynomial time, 666
Prim’s, 656–7
Quine–McCluskey, 540
worst-case complexity of, 649
Alphabet, 371–2, 418
free group generated by, 379
free monoid generated by, 373,
415
free semigroup generated by,
372, 402
Alphabetical order relation, 188,
195
Ancestor of a vertex of a rooted
tree, 616
AND-gate, 529–36
Antecedent, 7
Anti-symmetric relation, 164–8,
172–3
Appel, Kenneth, 612
Argument(s), 26–35, 45–9
in predicate logic, 45–9
in propositional logic 26–35
invalid, 26
valid, 26, 56
Arithmetic
fundamental theorem of, 75, 452
modulo, (see also Addition
modulo n and
Multiplication modulo n)
183–5, 375–6
Arithmetic, Fundamental Theorem,
75, 452
Arithmetics, ﬁnite, 185
Arrow diagram of a function, 221,
228, 229, 239
Ascending heap, 635
Associative law(s)
for matrix addition, 299
for matrix multiplication, 306
for propositions, 22
for sets, 105
Associativity of a binary operation,
364
Atom in a Boolean algebra, 520
Attribute, 206–19
non-prime, 283, 285
prime, 283
Augmented matrix, 346–60
reduction to reduced row echelon
form, 346–55
reduction to row echelon form,
355–8
Automorphism, 416
inner, 417
Axiom(s), 51–5
Boolean algebra, 493–4, 496
consistency of, 52
of induction, 70
Axiom system, 51–5
‘Blubs and Glugs’, 53–5, 561
model of, 53–4, 55
Axiomatic method, 51–5
Axiomatic set theory, 79
Axiomatic theory, see Axiom
system
B´ezout’s identity, 444
Biconditional connective, 7
relation to logical equivalence, 16
Biconditional proposition, proof of,
64
Bijection(s), (see also
Permutations) 260–6
composition of, 262
horizontal line test for, 262
vertex, 576
Bijective function, see Bijection

800
Index
Binary device, 529
Binary matrix of a relation, 158–9,
166
Binary numeral, 535
Binary operation, 361–70
associativity of, 364
closure property of, 362
commutativity of, 364
deﬁnition by a Cayley table, 363
identity element for, 364–5, 367
inverse of an element with
respect to, 365, 367–8
Binary relation, 155
Binary search tree, see Sort tree
Binary tree, 618, 619–21
full, 587
decision vertex of, 587
leaf vertex of, 587
recursive deﬁnition of, 620
Binary word(s), 418–35
distance between, 420
error pattern of, 432–3
syndrome of, 432–4
Bipartite graph, 553, 554
complete, 553, 554
Bit, 418
information, 421
Block code, 421
triple-repetition, 434
‘Blubs and Glugs’ axiom system,
53–5, 561
Boole, George, 492
Boolean algebra, 492–520
absorption laws for, 499
atom in, 520
axioms, 493–4, 496
cancellation law for, 502
De Morgan’s law for, 500
duality principle for, 497
idempotent laws for, 498–9
identity elements in, 493
identity laws for, 499
involution law for, 500
two-element, 521
Boolean expression(s), 503–20
conjunctive normal form of,
516–9
disjunctive normal form of,
508–19
equal, see Boolean expressions,
equivalent
equivalent, 505
for logic gates, 529, 534, 535
for logic networks, 529–36
for switching circuits, 520–9
Karnaugh maps for, 537–47
minimal form of, 537–47
minimization of, 537–47
rooted tree representation of, 626
Boolean function(s), 505, 509–20
equal, 506, 514–5
for logic networks, 532–6
for switching circuits, 520–9
Boolean variable, 503
complement of, 503
Boyce–Codd normal form, 289
Breadth-ﬁrst search, 647–9
Cancellation law(s)
for a Boolean algebra, 502
in a group, 381
Candidate key, 212, 283–9
Canonical product-of-sums form,
see Conjunctive normal
form
Canonical sum-of-products form,
see Disjunctive normal form
Cantor, Georg, 270
Cantor’s continuum hypothesis,
274–5
Cantor’s diagonal argument, 272–4,
277
Capacity of a cut, 680
Cardinalities
addition of, 275–7
exponentiation of, 275–7
multiplication of, 275–7
Cardinality, 82, 141, 270–7

Index
801
inﬁnite, 82
of a Cartesian product, 127
of a power set, 117–8, 274, 276
of a set, 82, 141
of inﬁnite sets, 270–7
of the set of positive integers, 271
of the set of prime numbers, 271
of the set of rational numbers,
271–2
of the set of real numbers, 232–3
Cartesian product(s), 122–30, 155,
163, 209, 223–4, 250,
256–60
cardinality of, 127
intersection of, 127–30
subset of, 130
union of, 128–30
Cayley, Arthur, 363, 583, 612
Cayley table, 363, 369, 383–4
Cell of a Karnaugh map, 537
Centre of a group, 403
Chain in a partially ordered set,
195, 203
Check digit, 421
Child
left, of a vertex of a binary tree,
619
of a vertex of a rooted tree, 616
right, of a vertex of a binary tree,
619
Ciphertext, 479
Circuit, switching, 520–9
Circuit(s) see cycles
Circular property of relations, 188
Class, equivalence, 176–85
Closed edge sequence in a graph,
562
Closed interval, 191
Closure property of a binary
operation, 362
Codd, E F, 206
Code, 418–35
block, 421
error correcting, 422–4
error detecting, 422–4
even parity check, 422
generator matrix for, 426–34
group, 425–34
Hamming, 435
(m, n) block, see Code, block
minimum distance of, 423–4, 425
minimum weight of, 431
odd parity check, 422
parity check matrix for, 428
systematic, 421
triple-repetition block, 434
Codeword(s), 421
sum of, 424–5
Codomain of a function, 224, 228,
261
Coefﬁcient(s)
of a variable, 331
matrix of, 334
Cohen, P J, 275
Collections of sets, see Families of
sets
Column matrix, 294
Column operation, elementary,
308–13
Column transformation,
elementary, see Column
operation, elementary
Column vector, see Column matrix
Combinational device, 529
Common divisor, 440
Commutative group, see Abelian
group
Commutative law(s)
for matrix addition, 299
for propositions, 22
for sets, 105
Commutative monoid, see Abelian
monoid
Commutative semigroup, see
Abelian semigroup
Commutativity of a binary
operation, 364
Complement

802
Index
in a Boolean algebra, 493, 497–8
of a Boolean variable, 503
of a set, 94
relative, see Difference of sets
Complement laws
for propositions, 23
for sets, 106
Complete bipartite graph, 553, 554
Complete graph, 553–4
Complete product, see Minterm
Complete product-of-sums form,
see Conjunctive normal
form
Complete rooted tree, 618
Complete sum, see Maxterm
Complete sum-of-products form,
see Disjunctive normal form
Complex numbers, 88
Complexity, worst-case of an
algorithm, 649
Component of a graph, 565–6
Composite function, 238–46,
253–5, 262, 264–5
Composite integer, 75, 449
Composite of two relations, 174–5,
188
Composition
of bijections, 262
of functions, 238–46, 253–5,
262, 264–5
of morphisms, 417
of permutations, 391–3
of symmetries, 388–90, 394
Compound proposition, 2
Computational efﬁciency of an
algorithm, 666–7
Concatenation of strings, 371
Conclusion of an argument, 26
Condition
necessary, 7
necessary and sufﬁcient, 21
sufﬁcient, 7
Conditional connective, 6–7, 19–20
relation to logical implication, 18
Conditional proposition
contrapositive of, 19
converse of, 19
direct proof of, 57–60
inverse of, 19
proof of using the contrapositive,
60–2
Conformable matrices for addition,
297
Congruence modulo n, 183–5,
375–6
linear congruences, 464–72
Conjecture, 53, 66, 69
Goldbach’s, 459
Conjunction, 4–5
Rule, for propositions, 30
relation to intersection of sets, 93
Conjunctive normal form, 516–9
Connected digraph, 602–3
Connected graph, 565–6
Connected subgraph, maximal, see
Component of a graph
Connective
biconditional, 7
conditional, 6–7, 19–20
logical, 2–7
Consequent, 7
Conservative ﬂow, 678
Consistent system of linear
equations, 334
Constructive dilemma, 30
Continuum, 274
Continuum hypothesis, Cantor’s,
274–5
Contradiction, 13, 14–5
proof by, 62–4
Contrapositive
of a conditional proposition, 19
use in proofs, 60–2
Converse
of a conditional proposition, 19
of a digraph, 609–10
Coordinate grid diagram of a
relation, 155–6

Index
803
Coprime, 446
Correction of errors in transmitted
words, 418–35
Countably inﬁnite set, 271
Counter-example, use in proofs,
66–7
Counting principle 1, 100
Counting principle 2, 100
Counting techniques, 100–4
Cover, 198, 200
Critical path in a network, 676
Cryptography, 479–90
public key, 480–1
RSA, 484–90
Cube function, 225, 230
Cut, 680–4
capacity of, 680
minimal, 680
Cycle(s)
directed, in a digraph, 602
Hamiltonian, 569–70
Hamiltonian, minimal, 665
in a graph, 562
Cyclic group, 384–7, 404
Cyclic subgroup, 397, 399–400
Database management system
(DBMS), 205
Database system, 205
Database, relational, 205–19
De Morgan, Augustus, 23, 612
De Morgan’s laws
for a Boolean algebra, 500
for propositions, 23
for sets, 105, 108
Decision vertex
of a full binary tree, 587
of a rooted tree, 614
Decoding
function, 421
nearest neighbour, 421
Decryption, see cryptography
Deductive reasoning, 51, 69
Deﬁnition, inductive, 75–6
Degree
of a dihedral group, 390
of a symmetric group, 393
of a vertex, 552
Dependence
functional, 278–90
transitive, 287
Depth-ﬁrst search, 644–5
Descartes, Rene, 122
Descendant of a vertex of a rooted
tree, 616
Descending heap, 634–40
Detection of errors in transmitted
words, 418–35
Device
binary, 529
combinational, 529
sequential, 529
two-state, 520
Diagonal
leading, 295
principal, see Diagonal, leading
Diagonal argument, Cantor’s,
272–4, 277
Diagonal matrix, 294–5
Diagram
arrow, of a function, 221, 228,
229, 239
coordinate grid, of a relation,
155–6
Hasse, 198–205
of a directed graph, 157–8
of a graph, 451
of a weighted graph, 653–4
Venn, 91–5, 122–4
Dichotomy law, 194
Difference
of sets, 94–5, 148
of tables, 216–7
symmetric, 109
Digit, check, 421
Digraph(s)
adjacency matrix of, 601–2
connected, 602–3

804
Index
converse of, 609–10
directed cycle in, 602
directed edge sequence in, 602
directed path in, 602
Eulerian, 602
Hamiltonian, 603
in-degree of a vertex of, 603
isomorphism of, 608
of a relation, 157, 167–8
out-degree of a vertex of, 603
semi-Hamiltonian, 604
simple, 601
strongly connected, 602–3
underlying graph of, 601
unilaterally connected, 606–7
weakly connected, see Digraph,
connected
weighted, 674–86
sink of, 674
source of, 674
Dihedral group(s), 387–90, 397,
406
of degree 3, 387–90
Dijkstra’s algorithm, 661–3
Dimension of a matrix, 291–2
Direct product, external, 378
Direct proof, 57–60, 62
Directed cycle in a digraph, 602
Directed edge sequence in a
digraph, 602
Directed graph of a relation, see
Digraph of a relation
Directed graph, see Digraph
Directed path in a digraph, 602
Disjoint, pairwise, 118–9
Disjoint sets, 94
Disjunction, 5–6
Disjunctive syllogism, 30
exclusive, 6
inclusive, 5
Disjunctive normal form, 508
Distance
between binary words, 420
minimum, of a code, 423–4, 425
Distributive laws
for matrices, 306
for propositions, 22
for sets, 105
Divides, 439
Divisibility relation, 177, 189, 193,
195, 198–9, 203
Division algorithm, 437–8
Divisor 439
common, 440
greatest common, 440
Dodecahedral graph, 569–70
Dodecahedron, 569
Domain of a function, 224, 261
Dual
of a Boolean algebra statement,
496–7
of a proposition, 24
of a set theoretic statement, 106
Duality principle
for a Boolean algebra, 497
for propositions, 24
for sets, 106–8
Edge(s) of a graph, 549
adjacent, 552
Edge sequence
directed, in a digraph, 602
in a graph, 562
Element(s)
greatest, 191
idempotent, 377
identity, 365, 367
in a Boolean algebra, 493
image of, 224, 228, 246
inverse of, 365, 367
least, 191–2
maximal, 192
minimal, 192
of a matrix, 291
of a set, 79
order of, 400–1, 409–10
powers of, 380, 384–5
self-inverse, 366

Index
805
Elementary column operation,
308–13
Elementary column transformation,
see Elementary column
operation
Elementary matrix, 308–17
inverse of, 320–1
post-multiplication by, 312–6
pre-multiplication by, 310–6, 322
Elementary row operation, 308–17,
320–28, 346–60
inverse of, 320–1
Elementary row transformation, see
Elementary row operation
Elimination
Gauss–Jordan, 342–55
Gaussian, 355–8
Empty relation, 169
Empty set, 81, 85, 140, 148
power set of, 116
Empty string, 373
Encoding function, 421
Encryption, see cryptography
Entry of a matrix, see Element of a
matrix
Enumeration theory, see Counting
techniques
Epimorphism, 412
Equal Boolean expressions, see
Equivalent Boolean
expressions
Equality
of Boolean functions, 506, 514–5
of functions, 224
of matrices, 293
of sets, 82, 86, 148
Equation(s)
in a group, 382–3
in modular arithmetic, 464–72
linear, 331
solution of, 331–2
solution set of, 332
matrix, 334
solution set of, 81, 332
systems of linear, see Systems of
linear equations
Equilateral triangle
rotational symmetries of, 395
symmetries of, 387–90
Equivalence, logical, 15–17
Equivalence class(es), 176–88
connection with partition of a set,
179–83
Equivalence relation, 154, 164,
175–88
Equivalent
Boolean expressions, 504–5
systems of linear equations, 345
Eratosthenes, Sieve of, 455–8
Error, transmission, 418–35
Error correcting codes, 422–4
Error correction in transmitted
words, 418–35
Error detecting codes, 422–4
Error detection in transmitted
words, 418–35
Error pattern of a binary word,
432–3
Euclid, 51, 63, 271
Euclid’s greatest common divisor
algorithm, 442–3
Euler, Leonhard, 68, 548, 566, 568
Euler phi function, 481–4
Euler’s formula for planar graphs,
592–4
Euler’s theorem (congruences), 482
Eulerian digraph, 603
Eulerian graph, 566
Eulerian path in a graph, 567–8
Even parity check code, 422
Exclusive disjunction, 6
Existential generalization, 46
Existential quantiﬁer, 37
Existential speciﬁcation, 46
Exponential time algorithm, 666
Exponentiation of cardinalities,
275–7
Exportation law, 24

806
Index
Expression, Boolean, 503–20
External direct product, 378
Face of a plane graph, 592–3
Factor, 439
Families of sets, 111–20
ordered by inclusion, 188, 189,
192, 199
pairwise disjoint, 118–9, 177
Feasible ﬂow, 678
Fermat, Pierre de, 68
Fermat numbers, 453
Fermat’s little theorem, 482
Fibonacci, 75
Fibonacci numbers, 75–77, 78
Fifth normal form, 289
Final vertex of an edge sequence in
a graph, 562
Finite arithmetics, 185
First normal form, 206, 285
Float time for a vertex of a network,
676–7
Flow
conservative, 678
feasible, 678
in a network, 678
maximal, 678–9
value of, 678
Forest, 585, 588
Formal proof, 29–35, 45–9, 50–1
of validity of arguments, 29–35,
45–9
Four colour conjecture/theorem,
612
Fourth normal form, 289
Fraction, see Rational number
Free group, 379
Free monoid, 373, 415
Free semigroup, 372, 402
Full binary tree, 587
decision vertex of, 587
leaf vertex of, 587
Full rooted tree, 618
Full-adder, 535–6
Function(s)
1–1, see Function, injective
1–1, onto, see Function, bijective
addition of, 268
arrow diagram for, 221, 228, 229,
239
bijective, 260–6
Boolean, 505, 509–20
Boolean, equal, 506, 514–5
codomain of, 224, 228, 261
composition of, 238–46, 253–5,
262, 264–5
cube, 225, 230
decoding, 421
domain of, 224, 261
encoding, 421
equality of, 224
Euler phi function, 481–4
graph of, 227, 231, 251–2
identity, 225, 242
image of, 228–32
image of a subset in the domain
of, 235, 258–9, 269
image of an element, 224, 228,
246
inclusion, 245
injective, 246–60
inverse, 264–66
inverse image of a subset of the
codomain of, 236, 258–9
linear, 250
modulus, 242
multiplication of, 268
natural projection, 250, 259–60,
278–83
onto, see Function, surjective
partial, 236
propositional, 36, 40
range of, see Function, image of
restriction of, 245–6
scalar multiplication of, 268
square, 147, 225, 230, 247, 248,
260–1
square root, 146–7, 225–6, 265

Index
807
surjective, 246–60
switching, 522–9
total, 236
type of, 233
vertical line test for, 228
Functional dependence, 278–90
Functional determination, 279–90
Fundamental theorem of arithmetic,
75, 452
Galoise, Evariste, 379
Gate
AND-, 529–36
logic, 529–36
NAND-, 534
NOR-, 535
NOT-, 529–36
OR-, 529–36
Gauss, Carl Friedrich, 342
Gauss–Jordan elimination, 342–55
Gaussian elimination, 355–8
Generator matrix, 426–35
Generator of a cyclic group, 385
Godel, Kurt, 274
Goldbach’s conjecture, 459
Grandchild of a vertex of a rooted
tree, 616
Grandparent of a vertex of a rooted
tree, 616
Graph(s), 55
adjacency matrix of, 555–7
adjacent edges of, 552
adjacent vertices of, 552
bipartite, 553, 554
closed edge sequence in, 562
complete, 553–4
complete bipartite, 553, 554
components of, 565–6
connected, 565–6
cycle, 551, 649
cycle in, 562
degree of a vertex of, 552
diagram of, 451
directed, see Digraph
dodecahedral, 569–70
drawn on surfaces, 599–600
edge of, 549
edge sequence in, 562
Eulerian, 566
Eulerian path in, 567–8
Hamiltonian, 569–70
Hamiltonian cycle in, 569–70
homeomorphic, 595–6
incidence matrix of, 560
inﬁnite, 551
integer-weighted, 653
isolated vertex of, 556
isomorphic, 575–80
isomorphism of, 575–80
isomorphism principle for, 580
Kuratowski’s theorem for, 594–5
loop in, 550
‘Maltese Cross’, 649–50
null, 553
of a function, 227, 231, 251–2
path in, 561–2
Petersen’s, 552–3, 563
planar, 591–4, 596
planar, Euler’s formula for,
592–4
plane, 592
plane, face of, 592
regular, 552
searching of, 643–52
semi-Eulerian, 572
semi-Hamiltonian, 573
simple, 550
simple path in, 562
spanning tree in, 584
subgraph of, 556–7
sum of, 560
totally disconnected, see Graph,
null
underlying, of a digraph, 601
undirected, see Graph
union of, 560
vertex of, 549
weighted, 652–73

808
Index
wheel, 559–60
Greatest common divisor, 440
Euclid’s algorithm, 442-3
Greatest element, 191
Greedy algorithm, 658, 667
Greedy minimal spanning tree
algorithm, see Kruskal’s
algorithm
Group(s), 373–95
abelian, 374, 408, 413
additive, 385
cancellation laws in, 381
centre of, 403
code, 425–34
commutative, see Group, abelian
cyclic, 384–7, 404
generator of, 385
dihedral, 381–4, 397, 406
equations in, 382–3
external direct product of, 378
free, 379
homomorphisms of, see Groups,
morphisms of
in modular arithmetic, 473–9
isomorphic, 404–12
isomorphism principle for, 409
isomorphisms of, 404–412
Klein four-group, 394, 417
morphisms of, 404–12
normal subgroup of, 418
of codewords, 425
of permutations, 390–3, 406
of rotations of a regular n-sided
polygon, 397
of symmetries, 387–90
order of, 381
order of an element of, 400–1,
409–10
powers of an element of, 380,
384–5
‘Shoes and socks’ theorem for,
393
subgroups of, 396–400
symmetric, 393
Guthrie, Francis, 612
Haken, Wolfgang, 612
Half-adder, 535–6
Half-open interval, 181
Hamilton, Sir William Rowan, 569
Hamiltonian cycle in a graph,
569–70
minimal, 665
Hamiltonian digraph, 603
Hamiltonian graph, 569–70
Hamming code, 435
Hamming distance between binary
words, see Distance
between binary words
Hamming, Richard, 420
Handshaking lemma, 558
Hardy, Godfrey, 50
Hasse diagram, 198–205
Heap
ascending, 635
descending, 634–40
Heap sort, 634–40
Height of a rooted tree, 615
Hilbert, David, 270
Homeomorphic graphs, 595–6
Homogeneous system of linear
equations, 333, 340–1,
353–4
Homomorphisms, see Morphisms
Horizontal line test
for bijectivity, 262
for injectivity, 251
for surjectivity, 251
Hypothetical syllogism, 30
(i, j)-element of a matrix, 293
Idempotent element, 377
Idempotent laws
for a Boolean algebra, 498–9
for propositions, 22
for sets, 105
Identity, 365, 367

Index
809
Identity elements, in a Boolean
algebra, 493
Identity function, 225, 242
Identity laws
for a Boolean algebra,499
for propositions, 23
for sets, 106
Identity matrix, 295, 307, 310
row reduction to, 322–8
Identity relation, 162–3, 175, 186
Image
morphic, 414
of a subset, 235, 258–9, 269
of an element, 224, 228–9, 246–7
set, of a function, 228–32
Implication (see also Conditional
connective)
in a proof, 56
logical, 17–19
Improper subgroup, 396
In-degree of a vertex of a digraph,
603
Incidence matrix
of a graph, 560
Inclusion function, 245
Inclusion–exclusion principle, 101
Inclusive disjunction, 5
relation to union of sets, 93
Inconsistent system of linear
equations, 334, 351, 353
Index set, 278
Indexing set, 111–2
Induction
mathematical, 69–78
principle of, 70
second principle of, 74
Inductive deﬁnition, 75–6
Inductive hypothesis, 70
Inductive reasoning, 51
Inductive step, 70
Inﬁnite, countably, 271
Inﬁnite cardinality, 82
Inﬁnite graph, 551
Inﬁnite set
cardinality of, 270–7
characterization of, 270
Inﬁx form of an algebraic
expression, 625
Informal proof(s), 51
methods of, 55–78
Information bit, 421
Inherited relation, 191
Initial vertex of an edge sequence in
a graph, 562
Injection, 246–60
Injective function, see injection
Inner automorphism, 417
Integer part of a real number, 147,
180
Integer(s), 58, 88, 134–5
positive, cardinality of the set of,
271
Integer-weighted graph, 653
Internal vertex of a rooted tree, see
Decision vertex of a rooted
tree
Intersection
of a family of sets, 111–2
of Cartesian products, 127–30
of relations, 171–3
of sets, 93, 148, 369, 403, 411–2
relation to conjunction, 93
Interval
closed, 191
half-open, 181
open, 191
Invalid argument, 26
Inverse
additive, of a matrix, 298, 318
function, 264–66
image of a subset, 236
of a 2 × 2 matrix, 329
of a conditional proposition, 19
of a diagonal matrix, 329
of a matrix, 318–30, 337–8
of a matrix product, 321, 328
of an element, 365, 367
of an elementary matrix, 320–1

810
Index
of an elementary row operation,
320–1
relation, 163, 170, 197
Inverter, see NOT-gate
Invertible matrix, see Non-singular
matrix
Involution law
for a Boolean algebra, 500
for propositions, 22
for sets, 105
Involutory matrix, 329
Isolated vertex of a graph, 556
Isomer, structural, 590–1
Isomorphic graphs, 575–80
Isomorphic groups, 404–12
Isomorphic monoids, 411
Isomorphic semigroups, 411
Isomorphism(s)
of digraphs, 608
of graphs, 575–80
rooted, 623
of groups, 404–12
of monoids, 411
of semigroups, 411
Isomorphism principle
for graphs, 580
for groups, 409
Join, see Sum
Join, natural, 215–6
Jordan, Wilhelm, 342
Karnaugh map, 537–47
Kernel of a morphism, 418
Key, 212
candidate, 212, 283–9
decryption, 480-1, 484–90
encryption, 480-1, 484–90
primary, 212, 284–9
Kirchhoff, Gustaf, 582
Kirchhoff’s laws, 582
Klein four-group, 394, 417
K¨onigsberg Bridge problem, 548,
566–7
Knight’s tour problems, 574
Kronecher, Leopold, 270
Kruskal’s algorithm, 658
Kuratowski, Kazimierz, 131
Kuratowski’s deﬁnition of the
ordered pair, 131
Kuratowski’s theorem, 594–6
Leading diagonal of a matrix, 295
Leading element of a row of a
matrix, 342
Leaf vertex
of a full binary tree, 587
of a rooted tree, 614
Least element, 191–2
Left branch of a vertex of a binary
tree, 619
Left cancellation law in a group,
381
Left child of a vertex of a binary
tree, 619
Left subtree of a binary tree,
619–20
Length of a string, 371
Level of a vertex of a rooted tree,
615
Lexicographic order relation, see
Alphabetical order relation
Linear congruence(s), 464
solution of, 465–472
Linear equation(s), 331
geometrical interpretation of,
332–3
solution of, 331–2
solution set of, 332
standard form of, 331
systems of, see Systems of linear
equations
Linear function, 250
Linear order relation, see Total
order relation
Literal, 503
Logical predicate, 35–49
arguments in, 45–9

Index
811
Logic gate, 529–36
Logic network, 529–36
Logical connective, 2–7
biconditional, 7
conditional, 6-7, 19–20
conjunction, 4–5
exclusive disjunction, 6
inclusive disjunction, 5
Logical equivalence, 15–17
relation to biconditional
proposition, 16
Logical implication, 17–19
relation to conditional
proposition, 18
Logically equivalent propositions,
15–6
Loop in a graph, 550
m-ary tree, 626
(m, n) block code, see Block code
‘Maltese cross’ graph, 649–50
Mapping, see Function
Material equivalence law, 23
Material implication law, 23
Mathematical induction, 69–78
principle of, 70
second principle of, 74
Matrix (matrices)
addition, 297–9, 376
associative law for, 299
commutative law for, 299
additive inverse of, 298, 318
adjacency
of a digraph, 601–2
of a graph, 555–7
augmented, 346–60
binary, of a relation, 158–9, 166
column, 294
diagonal, 294–5
dimension of, 291–2
distributive laws for, 306
element of, 291
elementary, 308–17
inverse of, 320–1
post-multiplication by, 312–16
pre-multiplication by, 310–6,
322
elementary column operations
on, 308–13
elementary row operations on,
308–17, 320–8, 346–60
entry of, see Matrix, element of
equality of, 293
equation, 334
generator, 426–35
(i, j)-element of, 293
identity, 295, 307, 310
incidence, of a graph, 560
inverse method for solving
systems of linear equations,
337–42
inverse of, 318–30, 337–8
invertible, see Matrix,
non-singular
involutary, 329
leading diagonal of, 295
leading element of a row of, 342
multiplication, 300–306
associative law for, 306
multiplication by a scalar, 296–7
multiplicative inverse of, see
Matrix, inverse of
non-singular, 319
row reduction to identity
matrix, 322–8
null, see Matrix, zero
of coefﬁcients, 334
operations on, 296–306
order of, see Matrix, dimension
of
parity check, 428–30
partitioned, 324, 325, 330, 346
post-multiplication by, 306,
312–16
pre-multiplication by, 306,
310–6, 322
principal diagonal of, see Matrix,
leading diagonal of

812
Index
reduced row echelon form of,
343
representation, of a system of
linear equations, 334
row, 294
row echelon form of, 342
row-equivalence of, 316, 317
self-inverse, see Matrix,
involutary
singular, 319
square, 294
subtraction of, 298
symmetric, 295
transpose of, 296
unit, see Matrix, identity
zero, 294
Max-ﬂow min-cut theorem, 681,
684
Maximal connected subgraph, see
Component of a graph
Maximal element, 192
Maximal ﬂow, 678–9
Maximum ﬂow problem, 678–9
Maxterm, 508
Maxterm form, see Conjunctive
normal form
Meet, see Product
Member of a set, see Element of a
set
Mersenne numbers, 453–4
GIMPS: Great Internet Mersenne
Prime Search, 454
Methods of informal proof, 55–78
Metric, 420, 666
Minimal cut, 680
Minimal element, 192
Minimal form of a Boolean
expression, 537–47
Minimal Hamiltonian cycle, 665
Minimal spanning tree, 654
Minimization of Boolean
expressions, 537–47
Minimum completion time, 675
Minimum distance of a code,
423–4, 425
Minimum weight of a code, 431
Minterm, 506–7
Minterm form, see Disjunctive
normal form
Model of an axiom system, 53–4,
55
Modulo arithmetic, 183–5, 375–6,
equations in 464–72
groups in, 473–9
Modulus function, 242
Modus ponens, 30
Modus tollens, 30
Monoid(s), 372–3
abelian, 373
commutative, see Monoid,
abelian
free, 373, 415
isomorphic, 411
isomorphisms of, 411
submonoids of, 402
Monomorphism, 413
Morphic image, 414
Morphism(s), 404–18
bijective, see Isomorphism
composition of, 417
injective, see Monomorphism
kernel of, 417
surjective, see Epimorphism
Multigraph, see Graph
Multiple, 439
Multiplication
modulo n, 184–5, 377, 396–7,
403, 407, 410, 417, 475–9
of cardinalities, 275–7
of functions, 268
of matrices, see Matrix
multiplication
Multiplicative inverse of a matrix,
see Inverse of a matrix
n-ary relation, 163, 209
n-dimensional space, 126

Index
813
n-tuple, ordered, 125, 132, 133,
209, 331, 332, 333
NAND-gate, 534
Natural join, 215–6
Natural numbers, 88
Natural projection, 250, 259, 278–9
Nearest insertion algorithm, 668–70
Nearest neighbour algorithm, 667–8
Nearest neighbour decoding, 421
Necessary and sufﬁcient condition,
21
Necessary condition, 7
Negation
connection with the complement
of a set, 94
of a proposition, 2–4
of a propositional function, 36
of a quantiﬁed propositional
function, 41–2
Network, 673–84
critical path in, 676
cut in, 680–4
ﬂoat time for a vertex of, 676–7
logic, 529–36
scheduling, 674
Non-homogeneous system of linear
equations, 333
Non-prime attribute, 283, 285
Non-singular matrix, 319
NOR-gate, 534
Normal form(s), 277–90
Boyce–Codd, 289
ﬁfth, 289
ﬁrst, 206, 285
fourth, 289
second, 285–6
third, 287–9
Normal subgroup, 418
NOT-gate, 529–36
NP-complete problems, 667
Null graph, 554
Null matrix, see Zero matrix
Null set, see Empty set
Number(s)
complex, 88
composite, 75
Fibonacci, 75–77, 78
integer, 58, 88, 134–5
natural, 88
prime, 63, 67–8, 74–5
cardinality of the set of, 271
rational, 88
cardinality of the set of, 271–2
real, (see also Scalar) 56, 88
cardinality of the set of, 232–3
integer part of, 147, 180
Stirling, 268
Numeral, binary, 535
Odd parity check code, 422
One–one function, see Injection
One–one onto function, see
Bijection
One–one correspondence, see
Bijection
Onto function, see Surjection
Open interval, 191
Operation(s)
elementary row, 308–17, 320–28,
346–60
on sets, 91–5, 109
unary, 493
Operation(s) on typed sets, 134–42
signature, 135–42
preconditions and
postconditions, 144–48
Order
of a group, 381
of a matrix, see Dimension of a
matrix
of an element of a group, 400–1,
409–10
Order, partial, 189–205
Order relation, 154, 188–205
dichotomy law for, 194
Hasse diagram for, 198–205
linear, see Total order
partial, see Partial order

814
Index
strict, 197
total, see Total order
trichotomy law for, 197
Order, total, 194–5, 201
Ordered
n-tuple, 125, 133, 209, 331, 332,
333
pair, 122
Kuratowski’s deﬁnition of, 131
triple, 133
OR-gate, 529–36
Out-degree of a vertex of a digraph,
603
Pairwise disjoint families of sets,
118-9, 177
Paradox, Russell’s, 91, 140
Parallel connection of switches, 521
Parameter, 348–9, 353
Parent of a vertex of a rooted tree,
616
Parity check code
even, 422
odd, 422
Parity check matrix, 428–30
Partial function, 236
Partial order, 189–205
Partially ordered set, 189–95
chain in, 195, 203
Hasse diagram for, 198–205
Partition of a set, 118–20, 175–88
connection with equivalence
classes, 179–83
Partitioned matrix, 324, 325, 330,
346
Path
directed, in a digraph, 602
Eulerian, 567–8
in a graph, 561–2
simple, in a graph, 562
Permutations, groups of, 390–3,
406
Petersen’s graph, 552–3, 563
Phi function, Euler’s 481–4
Plaintext, 479
Planar graph, 591–4, 596
Plane graph, 592
face of, 592
Plane
punctured, 176, 178
real, 124
Polish form of an algebraic
expression, see Preﬁx form
of an algebraic expression
Polygon, regular
rotations of, 397
symmetries of, 387–90
Polynomial time algorithm, 666
Poset, see Partially ordered set
Post-multiplication
by a matrix, 306, 312–6
by an elementary matrix, 312–6
Postcondition of an operation,
144–48
Postﬁx form of an algebraic
expression, 625
Power of an element of a group,
380, 384–5
Power set, 114–8, 141, 411–2
cardinality of, 117–8, 274, 276
Precondition of an operation,
144–48
Predicate, 35–6
two-place, 40
use in deﬁning sets, 81
Predicate logic, 35–43
arguments in, 45–49
Preﬁx form of an algebraic
expression, 625
Premise
in a proof, 50, 56
of an argument, 26
Pre-multiplication
by a matrix, 306, 310-16, 322
by an elementary matrix,
310–16, 322
Prim’s algorithm, 656–7
Primary key, 212, 284–9

Index
815
Prime attribute, 283
Prime number(s), 63, 67–8, 74–5,
449
coprime, 446
primality testing, 454–8
cardinality of the set of, 271
Principal diagonal of a matrix, see
Leading diagonal of a
matrix
Principle(s)
counting, 100
duality
for propositions, 24
for sets, 106–8
in a Boolean algebra, 497
inclusion–exclusion, 101
of mathematical induction, 70
of mathematical induction,
second, 74
Product
Cartesian, 122–30, 155, 163,
209, 223–4, 250, 259–60
external direct, 378
in a Boolean algebra, 493
Projection
natural, 250, 259, 278–9
Proof
by contradiction, 62–4
by exhaustion, 66–7
by mathematical induction,
69–78
direct, 57–60, 62
formal, 50–1
informal, 51, 55–78
of a biconditional proposition, 64
using counter-example, 66–7
using the contrapositive, 60–2
Proper subgroup, 396
Proper subset, 85
Proposition(s), 1
algebra of, 22–5, 104
associative laws for, 22
biconditional, proof of, 64
commutative laws for, 22
complement laws for, 23
compound, 2
De Morgan’s laws for, 23
direct proof of, 57–60, 62
distributive laws for, 22
dual of, 24
duality principle for, 24
idempotent laws for, 22
identity laws for, 23
involution law for, 22
logical equivalence of, 15–17
logical implication for, 17–19
negation of, 2–4
proof using the contrapositive,
60–2
replacement laws for, 22–4
simple, 2
truth value of, 2
Propositional function, 36
doubly quantiﬁed, 40, 41
negation of, 36
quantiﬁed, 36–45, 56, 66
negation of, 38, 41–2
two-variable, 40–1
use of in deﬁning sets, 82
Punctured plane, 176, 178
Quantiﬁcation denial, rule of 42
Quantiﬁed propositional function,
36–45, 56, 66
negation of, 38, 41–2
two-variable, 40–1
Quantiﬁed variable, 36
Quantiﬁer, 36–8
existential, 37
universal, 36–7
Quine–McCluskey algorithm, 540
Quotient, 437
R-equivalence class, see
Equivalence class
r-regular graph, 552
Range of a function, see Image of a
function

816
Index
Rational number(s), 88
cardinality of the set of, 271–2
Real n-dimensional space, 126
Real number, (see also Scalar) 56,
88
cardinality of the set of, 232–3
integer part of, 147, 180
Real plane, 124
Real variable, 220
Reasoning
deductive, 51, 69
inductive, 51
Record instance, 207–19, 278–90
Record type, 207–19
Rectangle, symmetries of, 394, 417
Recursion, 117
Recursive
deﬁnition of a binary tree, 620
deﬁnition of a set, 117
procedures, 117
Reduced row echelon form of a
matrix, 343
Reductio ad absurdum, see Proof
by contradiction
Reﬂexive relation, 164–68, 171–3
Regular graph, 552
Regular polygon
rotations of, 397
symmetries of, 387–90
Relation(s)
alphabetical order, 188, 195
anti-symmetric, 164–8, 172–3
binary, 155
binary matrix of, 158–9, 166
chain in, 195, 203
circular property of, 188
composite of, 174–5, 188
congruence modulo n, 183–5,
375–6
coordinate grid diagram of,
155–6
dichotomy law for, 194
divisibility, 177, 189, 193, 195,
198–9, 203
empty, 169
equivalence, 154, 164, 175–88
graph of, 157–8, 166
identity, 162–3, 175, 186
inherited, 191
intersection of, 171–3
inverse, 163, 170, 197
linear order, see Relation, total
order
n-ary, 163, 209
order, 154, 188–205
partial order, 189–205
reﬂexive, 164–8, 171–3
restriction of, 191
strict order, 197
symmetric, 164–8, 172–3
total order, 194-5, 201
transitive, 164–8, 172–3
trichotomy law for, 197
type of, 159, 206–9
union of, 171–3
universal, 163, 186
well ordering, 198
Relational database, 205–19
Relative complement of sets, see
Difference of sets
Relatively prime, see Coprime
Remainder, 437
Replacement laws for propositions,
22–4
Restriction
of a function, 245–6
of a relation, 191
Reverse Polish form of an algebraic
expression, see Postﬁx form
of an algebraic expression
Right branch of a vertex of a binary
tree, 619
Right cancellation law in a group,
381
Right child of a vertex of a binary
tree, 619
Right subtree of a binary tree,
619–20

Index
817
Ring, 185
Root of a rooted tree, 614
Rooted tree(s), 612–26
complete, 608
decision vertex of, 614
full, 618
height of, 615
leaf vertex of, 614
level of a vertex of, 615
representation of algebraic
expressions, 624–6
root of, 614
rooted isomorphism of, 623
Rooted isomorphism of rooted
trees, 623
Rotational symmetries
of a regular polygon, 387–90
of an equilateral triangle, 395
Row echelon form of a matrix, 342
Row matrix, 294
Row operation, elementary,
308–17, 320–28, 346–60
Row reduction to an identity
matrix, 322–26
Row transformation, elementary,
see Row operation,
elementary
Row vector, see Row matrix
Row-equivalent matrices, 316, 317
RSA encryption, 484–90
Rules of Inference for propositions,
30
Rule of Quantiﬁcation Denial, 42
Russell, Bertrand, 52, 91, 140
Russell’s paradox, 91, 140
Scalar, (see also Real number)
Scheduling network, 674
Scheduling problems, 673–7
Search
breadth-ﬁrst, 647–9
depth-ﬁrst, 644–5
Searching of a graph, 643–52
Searching strategies, 643–50
Second normal form, 285–6
Second principle of induction, 74
Selection, 212–4, 288
Self-inverse
element, 366
matrix, see Involutory matrix
Semi-Eulerian graph, 572
Semi-Hamiltonian
digraph, 604
graph, 573
Semigroup(s), 370–2
abelian, 371
commutative, see Semigroup,
abelian
free, 372, 402
isomorphic, 411
isomorphisms of, 411
subsemigroups of, 401
Sequential device, 529
Series, connection of switches, 521
Set(s)
absorption laws for, 105
algebra of, 104–8
associative laws for, 105
binary operations on, 361–70
cardinality of, 82, 141
Cartesian product of, 122–30,
155, 165, 209, 223–4, 250,
259–60
commutative laws for, 105
complement laws for, 106
complement of, 94
countably inﬁnite, 271
De Morgan’s laws for, 105, 108
deﬁnition of using propositional
functions, 82
difference of, 94–5, 148
disjoint, 94
distributive laws for, 105
duality principle for, 106–8
element of, 79
empty, 81, 85, 140, 148
equality of, 82, 86, 148
families of, 111–20

818
Index
ordered by inclusion, 188, 189,
192, 199
idempotent laws for, 105
identity laws for, 106
index, 278
indexing, 111–2
inﬁnite
cardinality of, 270–7
characterization of, 270
intersection of, 93, 148, 369,
403, 411–2
involution law for, 105
membership of, see Set, element
of
null, see Set, empty
of sets, see Sets, families of
operations on, 91–5, 109
pairwise disjoint, 177
partially ordered, 189–205
partition of, 118–20, 175–88
connection with equivalence
classes, 179–83
power, 114–8, 141, 411–2
cardinality of, 117–8, 274,
2761
recursive deﬁnition of, 117
relative complement of, see Sets,
difference of
singleton, 119
solution set of an equation, 81,
332
subset of, 85–9, 148
superset of, 85
symmetric difference of, 109
theory, axiomatic, 79
type of, 134–8
typed, 134–8
union of, 93–7, 148
universal, 87–8
Venn diagrams for, 91–100
‘Shoes and socks theorem’, 393
Shortest path problem, 660–4
Sibling of a vertex of a rooted tree,
616
Sieve of Eratosthenes, 455–458
Signature of an operation, 135–42
Simple digraph, 601
Simple graph, 550
Simple path in a graph, 562
Simple proposition, 2
Simpliﬁcation Rule, 30
Singleton set, 119
Singular matrix, 319
Sink of a weighted digraph, 674
Solution
of a linear equation, 331–2
of a system of linear equations,
333–4
by Gauss–Jordan elimination,
342–55
by Gaussian elimination,
355–8
by matrix inverse method,
337–42
Solution set
of a linear equation, 332
of an equation, 81
Sort tree, 627–34
Sort
heap, 634–40
tree, 627–34
Source of a weighted digraph, 674
Spanning subgraph, 643
Spanning tree
in a graph, 584, 643
minimal, 654
Sphere, graphs on, 599–600
Square function, 147, 225, 230,
247, 248, 260–1
Square matrix, 294
Square root function, 146–7, 225–6,
265
Standard form of a linear equation,
331
Statement, see Proposition
Stirling number, 268
Strict order relation, 197

Index
819
String(s) over an alphabet, 371–2,
373
concatenation of, 371
empty, 373
length of, 371
Strongly connected
digraph, 602–3
tournament, 603
Structural isomer, 590–1
Structures, algebraic, see Algebraic
structures
Subgraph(s)
maximal connected, see
Components of a graph
of a weighted graph, 654
spanning, 643
weight of, 654
wide, see Subgraph, spanning
Subgroup(s), 396–400
cyclic, 399–400
improper, 396
normal, 418
proper, 396
tests for, 397–9
trivial, 396
Submatrix, 324
Submonoid, 402
Subsemigroup, 401
Subset, 85–9, 148
image of, 235, 258–9, 269
inverse image of, 236
proper, 85
totally ordered, see Chain
Substitution instance, 14
Substitution Rule, 24–5
Substructure, 396–404
Subtraction of matrices, 298
Subtree
left, 619–20
right, 619–20
Subtype, 143
Sufﬁcient condition, 7
Sum, in a Boolean algebra, 493
Sum of codewords, 424–5
Sum of graphs, 560
Superset, 85
Surjection, 246–60
Surjective function, see Surjection
Switch(es)
parallel connection of, 521
series connection of, 521
Switching circuits, 520–9
Boolean expression for, 520–9
Switching function, 522–9
Sylvester, James Joseph, 583
Symmetric difference of sets, 109
Symmetric group, 393
Symmetric matrix, 295
Symmetric relation, 164–8, 172–3
Symmetries
of a rectangle, 394, 417
of a regular polygon, 387–90
of an equilateral triangle, 387–90
rotational
of a regular polygon, 397
of an equilateral triangle, 395
Syndrome of a binary word, 432–4
System, axiomatic, 51–5
System(s) of linear equations
consistent, 330
equivalent, 345
equivalent matrix equation, 334
homogeneous, 333, 340–1,
353–4
inconsistent, 334, 351, 353
matrix representation of, 334
non-homogeneous, 333
Systematic code, 421
Table(s), 205–19, 277–90
difference of, 216–7
natural join of, 215–6
projection of, 214–5
union of, 216
Tautology, 13–15
Term, undeﬁned, 52
Ternary tree, 618
Theorem, 51, 52, 55–7

820
Index
Euler’s theorem (congruences),
482
Fermat’s little theorem, 482
fundamental theorem of
arithmetic, 75, 452
‘Shoes and socks’, 393
Third normal form, 287–9
Three-utilities problem, 591
Torus, graphs on, 599–600
Total function, 236
Total order, 194–5, 201
Hasse diagram for, 198–205
Totally disconnected graph, see
Null graph
Totally ordered subset, see Chain
Tournament, 603–5
strongly connected, 604
Transformations, see Symmetries
Transitive dependence, 287
Transitive relation, 164–8, 172–3
Transmission error, 418–35
Transmitted words
error correction in, 418–35
error detection in, 418–35
Transpose of a matrix, 296
Transposition law, 23
Travelling salesman problem,
664–70
Tree, 582–91
binary, 618, 619–21
recursive deﬁnition of, 620
full binary, 587
decision vertex of, 587
leaf vertex of, 587
m-ary, 618
minimal spanning, 654
rooted, 612–26
complete, 618
decision vertex of, 614
full, 618
height of, 615
leaf vertex of, 614
level of a vertex of, 615
root of, 614
rooted isomorphism of, 623
sort, 627–34
spanning, 584, 643
ternary, 618
Tree sort, 627–34
Triangle inequality, 665
Trichotomy law, 197
Triple, ordered, 133
Triple-repetition block code, 434
Trivial solution of a homogeneous
system of linear equations,
340
Trivial subgroup, 396
Truth table, 2–12
construction of, 8–11
Truth value of a proposition, 2
Two-place predicate, 40–1
Two-state device, 520
Two-variable propositional
function, 40–1
Type
Boolean, 135
checking, 142
of an empty set, 140–1
of a function, 233
Integer, 135
of a power set, 141
of a set, 134–38
String, 135
subtype, 143
Real, 135
of a relation, 159
Typed set(s), 134–8
operations on, 139–53
Unary operation, 493
Undeﬁned term, 52
Underlying graph of a digraph, 601
Undirected graph, see Graph
Unilaterally connected digraph,
606–7
Union
of a family of sets, 111–2
of Cartesian products, 128–30

Index
821
of graphs, 560
of relations, 171–3
of sets, 93–7, 148
relation to inclusive
disjunction, 93
of tables, 206
Unit matrix, see Identity matrix
Universal generalization, 46, 56, 59
Universal quantiﬁer, 36–7
Universal relation, 163, 186
Universal set, 87–8
Universal speciﬁcation, 45, 56, 59
Universe of discourse, 39, 45, 46–8,
81, 88
Valid argument, 26, 56
in predicate logic, 45–9
in propositional logic, 29–35
Value of a ﬂow, 678
Variable, 36, 331
Boolean, 503
Boolean, complement of, 503
coefﬁcient of, 331
quantiﬁed, 36
real, 220
Vector, column, see Matrix, column
Vector, row, see Matrix, row
Venn, John, 91
Venn-Euler diagram, 91–100
Vertex (vertices)
adjacent, of a graph, 552
bijection, 576
decision, of a full binary tree, 587
ﬁnal, of an edge sequence in a
graph, 562
initial, of an edge sequence in a
graph, 562
isolated, of a graph, 556
leaf, of a full binary tree, 587
of a digraph
in-degree of, 603
out-degree of, 603
of a graph, 549
degree of, 552
of a network, ﬂoat time for,
676–7
of a rooted tree
ancestor of, 616
child of, 616
descendant of, 616
grandchild of, 616
grandparent of, 616
level of, 616
parent, 616
sibling of, 616
Vertical line test for functions, 228
Von Neumann, John, 121–2
Weakly connected digraph, see
Connected digraph
Weight of a subgraph, 654
Weight of a word, 425
Weight of an edge of a weighted
graph, 653
Weighted digraph, 674–86
sink of, 674
source of, 674
Weighted graphs, 652–73
Well ordering, 198
Wheel graph, 559–60
Wide subgraph, see Spanning
subgraph
Wildcard, 218
Word(s) (see also String)
binary, 418–35
distance between, 420
error pattern of, 432–3
sum of, 424–5
syndrome of, 432–4
weight of, 425
Worst-case complexity of an
algorithm, 649
Zero matrix, 294

Mathematics
w w w. c rc p r e s s . c o m
an informa business
6000 Broken Sound Parkway, NW
Suite 300, Boca Raton, FL 33487
270 Madison Avenue
New York, NY 10016
2 Park Square, Milton Park
Abingdon, Oxon OX14 4RN, UK
ISBN: 978-1-4398-1280-8
9 781439 812808
90000
K10650
“…The choice of the topics covered in this text is largely suggested by the needs 
of computer science. It contains chapters on set theory, logic, algebra (matrix 
algebra and Boolean algebra), and graph theory with applications. … The style of 
exposition is very clear, step by step and the level is well adapted to undergraduates 
in computer science. The treatment is mathematically rigorous; therefore it is also 
suitable for mathematics students. Besides the theory there are many concrete 
examples and exercises (with solutions!) to develop the routine of the student. 
So I can recommend warmly this book as a textbook for a course. … an excellent 
textbook.”
                                  —H.G.J. Pijls, University of Amsterdam, The Netherlands
Discrete Mathematics: Proofs, Structures, and Applications, Third Edition provides a 
rigorous yet accessible exposition of discrete mathematics, including the core 
mathematical foundation of computer science. In the expanded first chapter, the 
text includes a new section on the formal proof of the validity of arguments in 
propositional logic before moving on to predicate logic. This edition also contains a 
new chapter on elementary number theory and congruences. This chapter explores 
groups that arise in modular arithmetic and RSA encryption, a widely used public 
key encryption scheme that enables practical and secure means of encrypting 
data. 
Exploring the relationship between mathematics and computer science, this 
third edition continues to provide a secure grounding in the theory of discrete 
mathematics and to augment the theoretical foundation with salient applications. 
The approach is comprehensive yet maintains an easy-to-follow progression from 
the basic mathematical ideas to the more sophisticated concepts examined later 
in the book. The text is designed to help readers develop the rigorous logical 
thinking required to adapt to the demands of the ever-evolving discipline of 
computer science.

