Synthesis of Strategies from Interaction Traces
Tsz-Chiu Au
Dept. of Computer Science
University of Maryland
College Park, MD 20742, USA
chiu@cs.umd.edu
Sarit Kraus
Dept. of Computer Science
Bar-Ilan University
Ramat Gan, 52900, Israel
sarit@cs.biu.ac.il
Dana Nau
Dept. of Computer Science
University of Maryland
College Park, MD 20742, USA
nau@cs.umd.edu
ABSTRACT
We describe how to take a set of interaction traces produced by
different pairs of players in a two-player repeated game, and com-
bine them into a composite strategy. We provide an algorithm that,
in polynomial time, can generate the best such composite strategy.
We describe how to incorporate the composite strategy into an ex-
isting agent, as an enhancement of the agent’s original strategy.
We provide experimental results using interaction traces from
126 agents (most of them written by students as class projects) for
the Iterated Prisoner’s Dilemma, Iterated Chicken Game, and It-
erated Battle of the Sexes. We compared each agent with the en-
hanced version of that agent produced by our algorithm. The en-
hancements improved the agents’ scores by about 5% in the IPD,
11% in the ICG, and 26% in the IBS, and improved their rank by
about 12% in the IPD, 38% in the ICG, and 33% in the IBS.
Categories and Subject Descriptors
I.2.11 [Artiﬁcial Intelligence]:
Distributed Artiﬁcial Intelli-
gence—Multiagent systems
General Terms
Algorithms, Performance, Economics, Experimentation, Theory
Keywords
Agents, interaction, learning, multi-agent systems, prisoner’s
dilemma, repeated games
1.
INTRODUCTION
To create new and better agents in multi-agent environments, we
may want to examine the strategies of several existing agents, in or-
der to combine their best skills. One problem is that in general, we
won’t know what those strategies are. Instead, we’ll only have ob-
servations of the agents’ interactions with other agents. The ques-
tion is how to synthesize, from these observations, a new strategy
that performs as well as possible.
In this paper we present techniques for taking interaction traces
(i.e., records of observed interactions) from many different pairs
of agents in a 2-player iterated game, and synthesizing from these
traces a new strategy called a composite strategy (see Figure 1).
We also show how an existing agent can enhance its performance
Cite as: Synthesis of Strategies from Interaction Traces, Tsz-Chiu Au,
Sarit Kraus and Dana Nau, Proc. of 7th Int. Conf. on Autonomous
Agents and Multiagent Systems (AAMAS 2008), Padgham, Parkes,
Müller and Parsons (eds.), May, 12-16., 2008, Estoril, Portugal, pp. XXX-
XXX.
Copyright c⃝2008, International Foundation for Autonomous Agents and
Multiagent Systems (www.ifaamas.org). All rights reserved.
1
5
2
4
3
Figure 1: A composite strategy is synthesized from records of
the interactions among many existing agents. An agent using
this strategy can potentially outperform the agents from whom
the interaction traces were obtained.
by combining its own strategy with the composite strategy. Our
contributions include the following:
• We give a formal deﬁnition of a composite strategy, and present
necessary and sufﬁcient conditions under which a set of inter-
action traces from different agents can be combined together to
form a composite strategy.
• We provide a polynomial-time algorithm for synthesizing, from
a given set of interaction traces T , a composite strategy that is
optimal, in the sense that it performs at least as well as any other
composite strategy that can be formed from T .
• We provide a way to enhance an existing agent’s performance
by augmenting its strategy with a composite strategy.
• We provide experimental results demonstrating our algorithm’s
performance in the Iterated Prisoner’s Dilemma (IPD), Iterated
Chicken Game (ICG), and Iterated Battle of the Sexes (IBS), us-
ing interaction traces from 126 agents (117 written by students
as class projects, and 9 standard agents from the published lit-
erature). For each agent, we compared its performance with the
performance of our enhanced version of that agent. On the aver-
age, the percentage improvements in score were about 5% in the
IPD, 10% in the ICG, and 25% in the IBS; and the percentage
improvements in rank were about 11% in the IPD, 38% in the
ICG, and 33% in the IBS.
2.
BASIC DEFINITIONS
Consider a two-player ﬁnite repeated game, such as the IPD. At
any time point t, two agents φA and φB can choose actions a and
b from ﬁnite sets of actions A and B, respectively. Neither agent
learns what the other’s action is until after choosing its own action.
Throughout this paper, we will look at games from φA’s point of
view; i.e., φA is “our agent” and φB is the opponent.

We call the pair (a, b) an interaction between φA and φB. We
assume interactions occur at a ﬁnite number of discrete time points
t1, t2, . . . , tn, where n is the total number of interactions.1 An
interaction trace between φA and φB is a sequence of interactions
τ = ⟨(a1, b1), (a2, b2), . . . (an, bn)⟩, where ai and bi are the ac-
tions of φA and φB at time ti.2 The histories of φA’s and φB’s
actions are τ A = ⟨a1, a2, . . . , an⟩and τ B = ⟨b1, b2, . . . , bn⟩, re-
spectively. The history up to time tk is the k’th preﬁx of τ, i.e., it
is the subsequence preﬁxk(τ) = ⟨(a1, b1), (a2, b2), . . . (ak, bk)⟩.
τ A
k and τ B
k are ⟨a1, a2, . . . , ak⟩and ⟨b1, b2, . . . , bk⟩, respectively.
A pure strategy is a function φ from histories into actions, that
returns the next action to perform.
A mixed strategy is a pair
¯φ = (Φ, ∆), where Φ is a nonempty set of pure strategies and ∆
is a probability distribution over Φ. To make it easy to distinguish
between pure and mixed strategies (or equivalently, between deter-
ministic and probabilistic agents), we’ll normally write the latter
with a line over it (e.g., ¯φ rather than φ).
An agent uses a strategy to determine what actions it should take
in every time point. A deterministic agent is one whose strategy is
a pure strategy, and a probabilistic agent is one whose strategy is
a mixed strategy. We often will use the same symbol (e.g., φA or
φB) to denote both the agent and its strategy.
When two deterministic agents φA and φB interact, only one
possible interaction trace can be generated, namely the interac-
tion trace trace(φA, φB) = τn, where τi is deﬁned recursively
as follows: τ0 = ⟨⟩, and τj+1 = τj ◦⟨(φA(τj), φB(τj))⟩for
i = 1, . . . , n. We assume φA’s performance is measured by a util-
ity function UA(trace(φA, φB)) ≥0 that gives the reward for φA
when its opponent is φB.
Consider an agent ¯φB that uses a mixed strategy (ΦB, ∆B). We
will call the pure strategies in ΦB the possible strategies for ¯φB. In
each game, ¯φB will choose one strategy φ ∈ΦB according to the
probability distribution ∆B, and φA will not know which strategy
was chosen. We call the chosen strategy ¯φB’s actual strategy in
this game. The overall performance of φA in its interactions with
¯φB is φA’s expected utility, which is
E(φA, ¯φB) =
X
{∆B(φ) × UA(trace(φA, φ)) : φ ∈ΦB}.
3.
SYNTHESIS OF STRATEGIES FROM
INTERACTION TRACES
Section 3.1 presents an algorithm that can reconstruct the strat-
egy of a single agent φA, given a collection of interaction traces
generated by φA. Section 3.2 shows how to take a collection of
traces that were generated by more than one agent, and construct a
new strategy called a composite strategy, that combines parts of the
strategies of all of those agents. Section 3.3 gives an algorithm to
ﬁnd the optimal one of these composite strategies.
3.1
Reconstructing an Agent’s Strategy
Suppose we play a pure strategy φA against a mixed strategy
¯φB = (Φ, ∆), where Φ = {φ1, φ2, φ3}. Then the set of all possi-
ble interaction traces is
T = {trace(φA, φj) : j ∈{1, 2, 3}}.
1It is easy to modify our deﬁnitions and algorithms to handle games
in which the number of interactions can vary. But to simply our
discussion, we’ll assume the number of interactions is constant.
2This formalism can easily model environments where the actions
are interleaved rather than occurring simultaneously, by specifying
that at odd time points φA’s action must always be a “null” action,
and similarly for φB at even time points.
Agent CA(T )
/* a composite agent synthesized from T */
1. i := 1 ; Ti := T
2. Loop until the end of the game
3.
If Ti = ∅, then exit with failure because T is insufﬁcient
4.
If Ti ̸= ∅, then
5.
Ai := {a : (∃τ ∈Ti) a is the i’th action of τ A}
6.
If |Ai| ̸= 1, then exit with failure because T is incompatible
7.
If |Ai| = 1, then let ai be the action in Ai
8.
Output ai and receive the other agent’s action bi
9.
T ′
i := Ti
10.
For each τ ∈Ti,
11.
If the i’th interaction in τ isn’t (ai, bi), remove τ from T ′
i
12.
Ti+1 := T ′
i ; i := i + 1
Figure 2: The pseudocode of a composite strategy.
T contains enough information for us to construct a new strategy
φT whose interaction traces with ¯φB will be exactly the same as
φA’s. Figure 2 gives the pseudocode for an agent CA(T ) that can
generate and play the strategy φT . We will call CA(T ) a composite
agent, and φT a composite strategy. The strategy φT generated by
CA is partial, i.e., it is only deﬁned over some of the possible his-
tories. However, as we will see in Theorem 1, φT is ¯φB-total, i.e.,
it is deﬁned over the set of all histories that can are possible when
playing it against ¯φB. In other words, in any history τk of inter-
actions between φT and ¯φB, φT will always be able to determine
what its next action ak+1 should be.
To see how the CA agent works, suppose that the three interac-
tion traces in T are
τ1
=
⟨(C, C), (C, D), (D, C)⟩,
τ2
=
⟨(C, C), (C, C), (C, C)⟩,
τ3
=
⟨(C, D), (C, D), (D, C)⟩.
Next, suppose we play CA(T ) against ¯φB. Since φA’s ﬁrst action
is C in all three traces, we have Ai = {C} at Line 5 of CA, so
CA(T ) returns a1 = C as its ﬁrst action. Suppose ¯φB’s ﬁrst action
is C. Then ¯φB cannot be using the strategy that produced τ3, so
Line 11 of the agent removes τ3 from T ′
1, leaving τ1 and τ2 in T2.
Hence in the next interaction with ¯φB, CA(T ) will choose a2 = C.
Suppose ¯φB’s second action is b2 = D. Then τ2 is removed from
T ′
2, and CA(T )’s next action is D. Hence that the composite agent
CA(T ) ended up “replaying” the interaction trace τ1.
The following theorem provides a condition under which CA(T )
will always generate the same action that φA would generate
against ¯φB:
THEOREM 1. Let φA and ¯φB = (ΦB, ∆B) be pure and mixed
strategies, respectively. If T = {trace(φA, φ) : φ ∈ΦB}, then
trace(CA(T ), φ) = trace(φA, φ) for every φ ∈ΦB.
Sketch of Proof.
Due to limited space, we only sketch the proof
here. Without loss of generality, let φ be the actual strategy of ¯φB,
and let τ = trace(φA, φ) be the interaction trace between φA and
φ. Suppose there exists i such that (ai, bi) at Line 8 of Figure 2 is
not the i’th interaction (a′
i, b′
i) in τ, but, for 1 ≤j < i, (aj, bj)
is the j’th interaction in τ. First, all interaction traces in Ti have
the same preﬁx up to the (i −1)’th iteration (any traces without
this preﬁx were removed at Line 11 on a previous iteration) and
φA is a deterministic agent. Therefore all traces in Ti (including
τ) have the same i’th action a′
i, and the composite agent will cer-
tainly output a′
i at the iteration i. Thus, ai = a′
i. Second, φ will
certainly output b′
i at the i’th interaction, given the action sequence
a1, a2, . . . , ai−1 as its inputs; that is, bi = b′
i. Hence, a contradic-
tion occurs. Therefore (ai, bi) at Line 8 of Figure 2 is always the

i’th interaction in τ, and trace(CA(T ), φ) = τ.
2
3.2
Constructing New Strategies
The previous section showed how to reconstruct a strategy of a
single agent φA against an agent ¯φB, given φA’s interaction traces
with ¯φB. The same approach can be used to construct new strate-
gies from a collection of traces generated by more than one agent,
provided that two conditions, called compatibility and sufﬁciency,
are satisﬁed.
To illustrate the notion of compatibility, consider two well-
known strategies for the IPD: Tit-For-Tat (TFT) and Tit-For-Two-
Tats (TFTT). In the usual setting, the optimal strategy against TFT
is to cooperate in every iteration, and the optimal strategy against
TFTT is to defect and cooperate alternatively. For an IPD game
of ﬁve iterations, these can be represented by the following two
interaction traces:
τ1
=
⟨(C, C), (C, C), (C, C), (C, C), (C, C)⟩,
τ2
=
⟨(D, C), (C, C), (D, C), (C, C), (D, C)⟩.
However, no pure strategy can generate both of these interaction
traces, because in the ﬁrst interaction, no agent can choose both
Cooperate and Defect simultaneously.
Thus, there is no single
agent that can act optimally against both TFT and TFTT, and we
say that τ1 and τ2 are incompatible with each other. If we run
CA({τ1, τ2}), it will return an error message at Line 6 of Figure 2.
Now
consider
an
agent
that
defects
in
the
ﬁrst
itera-
tion and then cooperates for the rest of a game.
When
this agent plays against TFT, the interaction trace is τ3
=
⟨(D, C), (C, D), (C, C), (C, C), (C, C)⟩. Although this strategy
is not optimal against TFT, it is compatible with τ2: both τ2 and
τ3 produce D for a1 and C for a2; and the opponent’s response at
the end of the 2nd interaction gives us enough information to decide
which of τ2 and τ3 to use thereafter. If the opponent’s second action
b2 is C, then we discard τ3 and continue with τ2, and if b2 is D, we
discard τ2 and continue with τ3. This is exactly what CA({τ2, τ3})
does when it plays against a mixed strategy that includes TFT and
TFTT.
We now formalize the concepts introduced in the above example,
to provide necessary and sufﬁcient condition on T for the synthesis
of a composite strategy.
DEFINITION 1. The longest common preﬁx of two action se-
quences α = ⟨a1, a2, . . . , an⟩and α′ = ⟨a′
1, a′
2, . . . , a′
n⟩is the
longest action sequence lcp(α, α′) that’s a preﬁx of both α and α′.
We now deﬁne a condition called compatibility, that (as we’ll see
in Theorem 2) is necessary for a set of interaction traces T to be
used successfully by CA. The interaction traces do not all need to
be generated by the same agent.
DEFINITION 2. Two interaction traces τ1 and τ2 are compati-
ble if either (1) τ A
1 = τ A
2 , or (2) |lcp(τ A
1 , τ A
2 )| > |lcp(τ B
1 , τ B
2 )|.
Otherwise, τ1 and τ2 are incompatible.
DEFINITION 3. A set T of interaction traces is compatible if
and only if there is no incompatible pair of interaction traces in T .
Even if a set T of interaction traces is compatible, CA(T ) will not
always be able to use them against ¯φB unless there is at least one
interaction trace for each of ¯φB’s possible strategies. The following
deﬁnition formalizes this notion.
DEFINITION 4. Let ¯φB = (ΦB, ∆B) be a mixed strategy.
A set T of interaction traces is ¯φB-sufﬁcient if and only if for
every strategy φ ∈ΦB, T contains an interaction trace τ =
⟨(a1, b1), . . . , (an, bn)⟩such that the action sequence ⟨b1, . . . , bn⟩
can be generated by φ given ⟨a1, . . . , an⟩as its inputs.
The following theorem shows that compatibility and ¯φB-
sufﬁciency are necessary and sufﬁcient to guarantee that CA(T )
will always be able to play an entire game against ¯φB.
THEOREM 2. The composite agent CA(T ) will never exit with
failure when it plays against an opponent ¯φB = (ΦB, ∆B), if and
only if T is compatible and ¯φB-sufﬁcient.
Sketch of Proof. The “only if” part is trivial. For the “if” part, sup-
pose T is compatible and ¯φB-sufﬁcient. Without loss of generality,
let φ be the strategy that ¯φB chooses to use at the start of the game.
By induction on i, we prove that CA(T ) does not exit with failure at
the i’th iteration. More precisely, we prove (1) |Ai| = 1 at Line 6,
and (2) there exist τ ∈Ti such that τ can be generated by φ (i.e.,
Ti ̸= ∅at Line 3), for 1 ≤i ≤n. First, let us consider i = 1. For
any two interaction traces τ1, τ2 ∈T , the ﬁrst actions in τ A
1 and
τ A
2 must be the same since T is compatible. Therefore, |A1| = 1.
Since T is ¯φB-sufﬁcient and φB is nonempty, it follows that there
is at least one interaction trace τ ∈T that can be generated by φ.
Thus, T1 ̸= ∅. Now consider i = k + 1. Suppose |Ak| = 1 and
there exist τ ∈Tk that can be generated by φ. First, since all traces
in Tk have the same preﬁx up to the (k −1)’th iteration and φ is
a pure strategy, φ will return a unique bk at Line 8, which is the
k’th action of τ (otherwise τ is not generated by φ). In addition,
by Deﬁnition 2, for any two interaction traces τ1, τ2 ∈Tk, the ﬁrst
k actions in τ A
1 and τ A
2 must be the same. Thus, the k’th action of
τ A must be ak, the action generated at Line 8. Therefore, (ak, bk)
is the i’th interaction of τ, and τ will not be removed from T ′
k at
Line 11. Hence τ ∈Tk+1 since Tk+1 = T ′
k at Line 12. Second,
|Ak+1| ≥1 because |Tk+1| ≥1. Third, at the start of each iter-
ation k + 1, all interaction traces in Tk+1 have the same preﬁx up
to the k’th iteration (any traces without this preﬁx were removed
at Line 11 on a previous iteration). By Deﬁnition 2, for any two
interaction traces τ1, τ2 ∈Tk+1, the ﬁrst k + 1 actions in τ A
1 and
τ A
2 must be the same; and consequently |Ak+1| ≤1. Therefore,
|Ak+1| = 1. By induction, |Ai| = 1 and Ti ̸= ∅, for 1 ≤i ≤n. 2
3.3
Finding the Best Composite Strategy
We now consider the problem of ﬁnding a strategy against a
mixed strategy. It is not difﬁcult to show by reduction from 3SAT
that the problem of ﬁnding an optimal strategy—a strategy with
the highest expected utility against a mixed strategy—is NP-hard.
Therefore, instead of ﬁnding the optimal strategy, we ﬁnd the best
composite strategy that can be synthesized from a given set of inter-
action traces, and then experimentally show that the best composite
strategy can perform pretty well in practice.
Let ¯φB = (ΦB, ∆B) be a mixed strategy. Suppose we are given
a set TB of interaction traces that were played against ¯φB; and
suppose that for each interaction trace τ ∈TB, we know the utility
UA(τ) for the agent that played against ¯φB. Let
T = {T ⊆TB : T is compatible and ¯φB-sufﬁcient};
and for each T ∈T, let φT be the composite strategy constructed
from T . Then the optimal composite strategy problem is the
problem of ﬁnding a composite agent φT ∗such that T ∗∈T and
E(φT ∗, ¯φB) ≥E(φT , ¯φB) for every T ∈T. We say that φT ∗is
(TB, ¯φB)-optimal.
Here is another formulation of the optimal composite strategy
problem that’s equivalent to the above formulation. Suppose we are
given sets of interaction traces T1, . . . , Tm from games against sev-
eral different agents φ1, . . . , φm, and for each τi ∈Ti we are given

Procedure CIT(k, {Ti}i=1,...,m, {pi}i=1,...,m)
1. If k > n, then
/* n is the maximum number of interactions */
2.
For i = 1 to m, choose any one trace, namely τi, in Ti
3.
Return ({τi}i=1..m, P
i=1..m{pi × U(τi)})
4. Else
5.
Ai := {ak : ⟨(aj, bj)⟩j=1..n ∈Ti}, for i := 1..m
6.
Bi := {bk : ⟨(aj, bj)⟩j=1..n ∈Ti}, for i := 1..m
7.
A′ := A1 ∩A2 ∩. . . ∩Am; B′ := B1 ∪B2 ∪. . . ∪Bm
8.
If A′ = ∅, then return (∅, −1)
/* incompatibility detected */
9.
If |Bi| ̸= 1 for some i, then exit with failure.
10. For i := 1 to m
11.
Partition Ti into T ab
i
for each pair (a, b) ∈A′ × B′ such that
12.
T ab
i
:= {τ ∈Ti : the k’th interaction in τ is (a, b)}
13. For each (a, b) ∈A′ × B′
14.
Tab := {T ab
i
: 1 ≤i ≤m, T ab
i
̸= ∅}
15.
Pab := {pi : 1 ≤i ≤m, T ab
i
∈T ab}
16.
(T ab
∗, Uab
∗) := CIT(k + 1, Tab, Pab)
/* call CIT itself */
17. For each a ∈A′
18.
If Uab
∗
≥0 for all b ∈B′, then
19.
ˆTa := S
b∈B′{T ab
∗}; ˆUa := P
b∈B′ Uab
∗
20.
Else
/* i.e., T ab
∗
is not a solution for some b ∈B′ */
21.
ˆTa := ∅; ˆUa := −1
/* i.e., no solution */
22. amax := arg maxa∈A′{ ˆUa}; Return ( ˆTamax, ˆUamax)
Figure 3: The pseudocode of the CIT algorithm.
the utility UA(τi) for the agent that played against φi. Furthermore,
suppose we are given numbers p1, . . . , pm such that pi is the proba-
bility that we’ll need to play against φi. Now, consider the problem
of ﬁnding a strategy with an optimal expected utility against these
agents. This is equivalent to an optimal composite strategy problem
in which TB = {T1 ∪. . . ∪Tm} and ¯φB = ({φ1, . . . , φm}, ∆),
where ∆(φi) = pi for each i.
As an example, suppose we want to play the IPD against two
agents φ1 = TFT and φ2 = TFTT, who will be our opponents
with probabilities p1 = 0.7 and p2 = 0.3, respectively. Suppose
we are given T1 = {τ1, τ2, τ3}, where
τ1 = ⟨(C, C), (C, C), (D, C)⟩;
UA(τ1) = 11.0;
τ2 = ⟨(D, C), (C, D), (C, C)⟩;
UA(τ2) = 8.0;
τ3 = ⟨(D, C), (D, D), (D, D)⟩;
UA(τ3) = 7.0;
and T2 = {τ ′
1, τ ′
2, τ ′
3}, where
τ ′
1 = ⟨(C, C), (C, C), (C, C)⟩;
UA(τ ′
1) = 9.0;
τ ′
2 = ⟨(D, C), (C, C), (D, C)⟩;
UA(τ ′
2) = 13.0;
τ ′
3 = ⟨(D, C), (D, C), (D, D)⟩;
UA(τ ′
3) = 11.0.
We can map this into the optimal composite agent problem by let-
ting ¯φB = (ΦB, ∆B) and TB = T1 ∪T2, where ΦB = {φ1, φ2},
∆B(φ1) = 0.7, and ∆B(φ2) = 0.3.
There are nine subsets of T that contain one trace for each
of φ1 and φ2 (and hence are ¯φB-sufﬁcient), namely {τj, τ ′
k} for
j = 1, 2, 3 and k = 1, 2, 3.
Only two of these nine sets are
compatible: {τ2, τ ′
2} and {τ3, τ ′
3}. Of the two sets, {τ2, τ ′
2} is
the (TB, ¯φB)-optimal one, because E({τ2, τ ′
2}, ¯φB) = 9.5 >
8.2 = E({τ3, τ ′
3}, ¯φB). Hence, our (TB, ¯φB)-optimal strategy
is to choose D and C in the ﬁrst two iterations, and then choose C
(or D) in the third iteration if the opponent chooses D (or C) in the
second iteration, respectively.
Figure 3 shows an algorithm, CIT, that can be used to solve the
above problem. CIT is a recursive algorithm that works by analyz-
ing interaction traces played against a set of agents {φ1, . . . , φm}.
Its inputs include, for each φi, a set of interaction traces Ti and a
probability pi that we’ll have φi as our opponent; and a number k
that represents how many moves deep we have gone in our analysis
of the interaction traces.
S0 
S1 
S2 
EU=9.5 
S6 
S3 
S4 
Incompatible !
S9
S10 
S12
S13 
S16
S17 
S19
S20 
S8
S11
S15
S18
S7
S14
EU=0.3x13=3.9
EU=0.7x8=5.6
EU=0.3x11=3.3
EU=0.7x7=4.9
EU=3.9
EU=5.6
EU=3.3
EU=4.9
EU=4.9
EU=3.3
EU=5.6
EU=3.9
EU=3.9+5.6=9.5
EU=3.3+4.9=8.2
EU = max(9.5,8.2) = 9.5 
S5 
EU = max(9.5) = 9.5
{τ1, τ2, τ3}0.7 {τ ′
1, τ ′
2, τ ′
3}0.3
{τ1}0.7 {τ ′
1}0.3
{τ1}0.7 {τ ′
1}0.3
{τ1}0.7 {τ ′
1}0.3
{τ1}0.7 {τ ′
1}0.3
{τ2, τ3}0.7 {τ ′
2, τ ′
3}0.3
{τ2, τ3}0.7 {τ ′
2, τ ′
3}0.3
{τ2}0.7 {τ ′
2}0.3
{τ ′
2}0.3
{τ ′
2}0.3
{τ ′
2}0.3
{τ2}0.7
{τ2}0.7
{τ2}0.7
{τ3}0.7 {τ ′
3}0.3
{τ ′
3}0.3
{τ ′
3}0.3
{τ ′
3}0.3
{τ3}0.7
{τ3}0.7
{τ3}0.7
a=C
b=C
a=C
b=C
a=D
b=C
a=C
a=D
b=C
b=D
a=D
b=C
a=C
b=C
b=C
b=D
a=D
a=D
b=D
b=D
k = 1
k = 2
k = 3
Figure 4: The search space of the CIT algorithm.
Example. We’ll now illustrate CIT’s operation on the same exam-
ple that we discussed earlier. In Figure 4, the node S0 represents the
initial call to CIT. The two sets of traces are T1 and T2 described
earlier, and the superscripts on these traces are the probabilities
p1 = 0.7 and p2 = 0.3 described earlier.
Each path from S0 to the bottom of the tree is one of the interac-
tion traces; and the value k = 1 at S0 indicates that we’re currently
looking at the ﬁrst interaction in each trace.
At the k’th interaction for each k, we need to consider both our
possible moves and the opponent’s possible moves. Although these
moves are made simultaneously, we can separate the computation
into two sequential choices: for each value of k, the higher layer
of nodes corresponds to our possible moves, and the lower layer of
nodes corresponds to the opponent’s possible moves. In the higher
layer, the expected utility of each node can be computed by taking
the maximum of the expected utilities of the child nodes; hence we
call these nodes max nodes. In the lower layer, the expected utility
of each node can be computed by adding the expected utilities of
the child nodes; hence we call these nodes sum nodes.3
In our example, the max node at k = 1 is S0, and the sum nodes
at k = 1 are S1 and S5. The edges between the max node and the
sum nodes correspond to our actions that can lead from the max
node to the sum nodes. For instance, if our action is C at S0, the
next sum node is S1; otherwise, the next sum node is S5. The
edges between the sum nodes at k = 1 and the max nodes at k = 2
corresponds to the actions that can be chosen by the opponent. At
k = 1, the opponent can only choose C, but at S7 at k = 2, the
opponent can choose either C or D, thus leading to two different
max nodes S8 and S11. A terminal node corresponds to the set of
interaction traces that are consistent with the actions on the path
3Mathematically, what we’re computing here is a weighted aver-
age; but each number has already been multiplied by the appropri-
ate weight, so we just need to add the numbers together.

from S0 to the terminal node. The expected utility of a terminal
node is the sum of the probability of the set of interaction traces
(denoted by the superscripts in Figure 4) times the utility of any
interaction trace in the set. For instance, at S10, the expected utility
is P
i=1..m{pi × UA(τi)} = 0.3 × 13 = 3.9. Notice that all
interaction traces in a set of interaction trace of a terminal node are
the same; thus the algorithm chooses any τi from Ti at Line 2 since
they all have the same utility.
The CIT algorithm basically does a depth-ﬁrst search on a tree
as shown in Figure 4, and propagates the expected utilities of the
terminal nodes to S0 together with the compatible set of interaction
traces that gives the expected utility. The expected utility of a max
node is the maximum of the expected utilities of its child nodes,
whereas the expected utility of a sum node is the sum of the ex-
pected utilities of its child nodes. Notice that at each max node the
CIT algorithm will check the compatibility of the set of interaction
traces of the node at Line 8. For example, at S4 the algorithm dis-
covers that τ1 and τ ′
1 are incompatible. Then a failure signal (the
expected utility −1) is passed from S4 to the ancestor nodes. The
max nodes and the sum nodes would then ignore the solution with
a failure signal, thus eliminate the incompatibility. This is one of
the key difference between the CIT algorithm and other search al-
gorithms for game trees or MDPs—the CIT algorithm directly op-
erates with interaction traces and checks the incompatibility among
them as the search process proceeds.
Running time. To achieve efﬁcient performance in CIT we have
used some indexing schemes that we will not describe here, due
to lack of space. CIT’s running time is O(nM), where n is the
number of iterations and M = Pm
i=1 |Ti|. In practice, the CIT
algorithm is very efﬁcient—it can ﬁnd the optimal solution from a
database of 500, 000 interaction traces in less than 15 minutes on a
computer with a 3.8GHz Pentium 4 CPU and 2GB RAM.
Discussion. At ﬁrst glance, CIT’s search space (see Figure 4) looks
somewhat like a game tree; but there are several important differ-
ences. First, our purpose is to compute an agent’s entire strategy
ofﬂine, rather than having the agent do an online game-tree search
at every move of a game. Second, we are not searching an entire
game tree, but are just searching through a set of interaction traces;
hence the number of nodes in our tree is no greater than the total
number of interactions in the interaction traces. Third, game-tree
search always assumes, either explicitly or tacitly, a model of the
opponent’s behavior.4 Rather than assuming any particular model
of the opponent, we instead proceed from observations of agents’
actual interactions.
3.4
Using a Base Strategy
Recall that CA(T )’s strategy is partial. In particular, although
CA(T ) will never exit with failure when playing against ¯φB when
T is compatible and ¯φB-sufﬁcient, it might do so if we play it
against another opponent ¯φC = (ΦC, ∆C) for which T is not
¯φC-sufﬁcient. However, in iterated games such as the IPD, there
is enough overlap in the strategies of different agents that even if
¯φC was not used to construct any of the traces in T , T may still be
¯φC-sufﬁcient.
4For example, minimax game-tree search assumes that the oppo-
nent will always use its dominant strategy. In perfect-information
games such as chess, this opponent model has worked so well that it
is taken more-or-less for granted. But in an imperfect-information
variant of chess, it has been shown experimentally [13] that this
model is not the best one—instead, it is better to use a model that
assumes the opponent has very little idea of what its dominant strat-
egy might be.
Agent MCA(TB, ∆, φbase)
/* a modiﬁed composite agent */
1. T ∗:= CIT(0, TB, ∆)
/* T ∗is (TB, ¯φB)-optimal */
2. φA := CA(T ∗)
/* φA is a composite agent */
3. Loop until the end of the game
4.
Get an action a from φA
5.
If φA fails, then get an action a from φbase and φA := φbase
6.
Output a and receive the other agent’s action b
7.
Give b to φA as its input
Figure 5: Algorithm for a composite agent with a base strategy.
Even if T is not ¯φC-sufﬁcient, CA(T ) may still be able to play
against ¯φC most of the time without exiting with failure. To handle
cases where CA(T ) does exit with failure, we can modify the CA
algorithm so that instead of exiting with failure, it instead uses the
output produced by a “base strategy” φbase, which may be TFT or
any other strategy. We call this modiﬁed algorithm the modiﬁed
composite agent (MCA), and its pseudo-code is shown in Figure 5.
A modiﬁed composite agent may be viewed in either of two ways:
• As a composite strategy that can use φbase when the composite
strategy is insufﬁcient.
• As an enhanced version of φbase, in which we modify φbase’s
moves in cases where the set of traces TB might yield a better
move. From this viewpoint, TB is a case base that is processed
by the CIT algorithm so that cases can be selected quickly when
they are appropriate.
4.
EXPERIMENTAL EVALUATION
We evaluated our technique in three well-known games: the Iter-
ated Prisoner’s Dilemma (IPD), the Iterated Chicken Game (ICG),
and the Iterated Battle of the Sexes (IBS). The IPD and ICG are
the iterated versions of the Prisoner’s Dilemma [1] and the Game
of Chicken [5], whose payoff matrices are:
Player B
Prisoner’s Dilemma
Cooperate (C)
Defect (D)
Cooperate (C)
(3, 3)
(0, 5)
Player A
Defect (D)
(5, 0)
(1, 1)
Player B
Chicken Game
Cooperate (C)
Defect (D)
Cooperate (C)
(4, 4)
(3, 5)
Player A
Defect (D)
(5, 3)
(0, 0)
The IBS is the iterated version of the Battle of the Sexes [11],
whose payoff matrix is shown below. To allow arbitrary agents
to play against each other without having to take on different roles
(hence different strategies), we needed to reformulate the IBS to
make the roles of Husband and Wife interchangeable. This was
quite easy to do, as shown in the following matrix: for the Wife
we renamed Football to C and Opera to D; and for the Husband we
renamed Football to D and Opera to C.
Husband
Battle of the Sexes
D (was Football)
C (was Opera)
C (was Football)
(1, 2)
(0, 0)
Wife
D (was Opera)
(0, 0)
(2, 1)
4.1
Experimental Design
To obtain a large collection of agents for the games, we asked the
students in several advanced-level AI classes to contribute agents.
We did not tell the students the exact number of iterations in each

game, but did tell them that it would be at least 50 (in all of our
experiments we used 200 iterations). The students contributed 43
IPD agents, 37 ICG agents, and 37 IBS agents. For each game, we
also contributed 9 more agents that used the following well-known
strategies: ALLC, ALLD, GRIM, NEG, PAVLOV, RAND, STFT,
TFT, and TFTT.5 This gave us a total of 52 IPD agents, 46 ICG
agents, and 46 IBS agents. In the rest of this section, we’ll call
these the original agents, to distinguish them from the composite
agents generated by our algorithms.
For each agent φ, we wanted to ﬁnd out how much improvement
we could get by replacing φ with a modiﬁed composite agent whose
base strategy is φ. We investigated this by doing a 5-fold cross-
validation experiment that worked as follows.
For each of the three games (IPD, ICG, and IBS), we took our
original set of agents for the game, and randomly and evenly par-
titioned it into ﬁve subsets Φ1, Φ2, Φ3, Φ4, Φ5. Then we repeated
the following steps ﬁve times, once for each Φi:
1. For the test set Φtest, we chose Φi.
For the training set
Φtrain, we chose S
j̸=i Φj.
2. We ran a 200-iteration tournament (the details are described
below) among the agents in Φtrain (with one modiﬁcation,
also described below), and let Ttrain be the set of all interac-
tion traces recorded in this tournament.
3. For each agent φ ∈Φtrain, we played 100 tournaments,
each 200 iterations long, involving φ and all of the agents
in Φtest. We calculated the agent’s average score, which is
equal to
1
100×|Φtest|
P
1≤l≤100
P
φk∈Φtest{ the score of φ
when it plays against φk in the l’th tournament}.
4. Likewise, for each agent φ ∈Φtrain, we played 100 tourna-
ments, each 200 iterations long, involving a modiﬁed com-
posite agent φ′ = MCA(Ttrain, ∆, φ) and all of the agents
in Φtest, where ∆is a uniform probability distribution over
Φtrain. Apart from the average score of φ′, we also recorded
the frequency with which φ′ used its base strategy φ.
All of our tournaments were similar to Axelrod’s IPD tourna-
ments [1] and the 2005 IPD tournament [8].
Each participant
played against every participant including itself (thus a tournament
among n agents consists of n2 iterated games).
One problem in Step 2 is that MCA’s input needs to come from
deterministic agents, but many of our students’ agents were prob-
abilistic (see Table 1). We handled this problem as follows. Each
probabilistic agent ¯φ used a random number generator for which
one can set a starting seed. By using ten different starting seeds,
we created ten determinized versions of ¯φ; and we generated inter-
action traces using the determinized agents rather than ¯φ.
Note that we used the determinized agents only during training.
The modiﬁed composite agents generated from the training data
are quite capable of being played against probabilistic agents, so
we played them against the probabilistic agents in our tests.
4.2
Experimental Results
Table 2 tells how many traces, on average, were collected for
each type of game, and how many traces were in the composite
strategies generated by CIT.
In each experiment, we calculated 4 average scores for each
agent (one for each test sets that did not contain the agent) and
4 average scores for each MCA agent. We repeat the above ex-
periment 100 times using different partitions and random seeds.
5These are often used as standard strategies; e.g., they were used
as standard entries in the 2005 IPD tournament [8].
Table 1: Among the original agents, how many of each type.
IPD
ICG
IBS
Deterministic agents
34
22
17
Probabilistic agents
18
24
29
Table 2: Average number of interaction traces collected dur-
ing the training sessions, before and after removing duplicate
traces, and average number of interaction traces in the com-
posite strategies generated by the CIT algorithm.
IPD
ICG
IBS
Before removing duplicates
29466.0
44117.4
60470.5
After removing duplicates
7452.0
25391.5
29700.8
Composite strategies
171.2
209.6
245.6
Figure 6 shows the agents’ overall average scores, each of which
is an average of 400 average scores.
Since each average score
is an average of 100 × |Φtest| scores, each data point in Fig-
ure 6 is computed from the scores of 40000 × n games, where
n is the average number of agents in the test sets.
Hence in
the IPD, each data point is computed from the scores of approx-
imately 415769.2 games, and in both the ICG and the IBS each
data point is computed from the scores of approximately 367826.1
games. The data ﬁle of the experiments can be downloaded at http:
//www.cs.umd.edu/~chiu/papers/Au08synthesis_data.tar.gz.
Figure 6 includes three graphs, for the IPD, ICG, and IBS, re-
spectively. In each graph, the x axis shows the agents and the y axis
shows their scores. The lower line shows the performance of each
original agent φ (the agents are sorted in order of decreasing over-
all performance). The upper line shows the average performance of
the corresponding MCA agents.
From the graphs, we note the following. In all three games, the
average scores of the MCA agents were as good or better than the
corresponding base agent. In the IPD, the differences in perfor-
mance were usually small; and we believe this is because most IPD
agents have similar behaviors and therefore leave little room for
improvement. In the ICG and IBS, the differences were usually
quite large. Finally, in all three games, the MCA agents performed
well even when their base agents performed poorly. For example,
in the IPD and the IBS, when we incorporated our composite strat-
egy into the weakest of the existing strategies, it more than doubled
that strategy’s score.
Figure 7 shows the increase in rank of an agent after incorpo-
rating the composite strategy into it, while all other agents did not
incorporate the composite strategy. We can see that the ranks of
most agents increased after the modiﬁcation. We conducted sign
tests to see whether the overall average scores of MCAagents are
greater than that of the original agents. The p-values of one-sided
sign tests are less than 0.00001 in all games. Therefore, the MCA
agents are signiﬁcantly better at the 99.9% level.
Table 3 shows the average improvement that each MCA agents
provided relative to the corresponding original agent. On the av-
erage, the percentage improvements in score were about 5% in the
IPD, 11% in the ICG, and 26% in the IBS; and the percentage im-
provements in rank were about 12% in the IPD, 38% in the ICG,
and 33% in the IBS.
Table 4 shows how frequently the MCA agents invoked their
base agents. We can make the following observations.
• In more than 84% of the IPD games, the MCA agents did not
need to use their base strategies at all.
In other words, the
MCA agents’ composite strategies worked successfully through-
out those games, even though the games were played with oppo-

400
500
600
700
verage Scores
Iterated Prisoner's Dilemma
200
300
400
1
6
11
16
21
26
31
36
41
46
51
Overall Av
Modified Composite Agent
Base Agent
700
800
900
1000
verage Scores
Iterated Chicken Game
500
600
700
1
6
11
16
21
26
31
36
41
46
Overall Av
Modified Composite Agent
Base Agent
200
250
300
350
verage Scores
Iterated Battle of the Sexes
50
100
150
1
6
11
16
21
26
31
36
41
46
Overall Av
Modified Composite Agent
Base Agent
Figure 6: Overall average scores of the base agents and the
MCA agents. The agents are displayed on the x axis in order of
decreasing score of the base agent. The error bars denote the
95% conﬁdence intervals of the overall average scores.
nents other than the ones used to build the composite strategies.
One possible the reason for this high reusability of interaction
traces is that the IPD is a well known game and thus most of the
original strategies are similar to certain well-known strategies
such as Tit-for-Tat.
• In the ICG and IBS, the MCA agents invoked their base strate-
gies a little more than half of the time. We think the reason for
this is that there was more diversity among the original strate-
gies, perhaps because these games are not as well-known as IPD.
But even though the MCA agents used their composite strate-
gies less frequently than in the IPD, the composite strategies
provided much more improvement, relative to the base strategy,
than in the IPD. In other words, the places where MCA was used
its composite strategy provided a much bigger enhancement to
the base strategy’s performance.
5.
RELATED WORK
Reinforcement learning. One similarity between our approach
and reinforcement learning is that our technique improves an
agent’s performance by incorporating previous experiences. But
reinforcement learning agents usually use on-line learning (e.g., Q-
learning agents learn and improve their policy during acting), while
our composite agent uses off-line learning to produce a composite
strategy that can then be incorporated into an agent.
A more important difference is that our technique does not re-
quire us to know the set of all possible states of the other agents, as
35
40
IPD
ICG
20
25
30
creases
ICG
IBS
5
10
15
Rank Inc
‐5
0
5
1
6
11
16
21
26
31
36
41
46
51
Figure 7: Increase in rank of each enhanced agent, relative to
the corresponding base agent. The x axis is as in Figure 6.
Table 3: Average increases in score and rank of the MCA
agents, relative to the corresponding original agents.
IPD
ICG
IBS
Average MCA agent score
582.69
856.37
262.47
Average original agent score
553.62
774.38
208.73
Average difference in score
29.08
81.99
53.74
Average % difference in score
5.3%
10.6%
25.7%
Average increase in rank
6.0
17.4
15.2
Average % increase in rank
11.5%
37.8%
33.0%
Table 4: Average frequency of invocation of base strategies.
IPD
ICG
IBS
Average percentage of games
15.4%
52.7%
54.4%
opposed to most existing work on POMDPs or learning automata,
in which the set of all possible states must be known beforehand.
In open environments such as IPD tournaments in which the oppo-
nents’ strategies are not known, it would be difﬁcult, if not impossi-
ble, to identify all possible states of the opponent’s strategy. More-
over, the Markovian assumption intrinsic to a policy or automaton
does not always hold because an agent’s decision can depend on the
entire history in a game. Our paper demonstrates that it is possible
to perform well in certain open environments such as the IPD with-
out knowing the set of the opponents’ internal states. To the best
of our knowledge, contemporary reinforcement learning techniques
are, unlike our techniques, not yet efﬁcient enough to compete with
strategies such as TFT and Pavlov in the IPD. In future, we would
like to run a much wider variety of experiments to see whether our
technique can be applicable in other open environments.
Modeling other agents. One approach for playing against a given
opponent is to develop an opponent model that predicts the oppo-
nent’s likely behavior [3, 4, 7]. This model is then used to aid in
developing strategies against that opponent. In contrast, we focus
on generating strategies directly from observations, without con-
structing an explicit opponent model.
Case-based reasoning.
Our technique has some similari-
ties to Derivational Analogy [15] and the reuse of macro ac-
tions/operators [9], in which records of a problem-solver’s deci-
sions or actions are used to solve new problems. Most work on
derivational analogy and macro actions focuses on problems in
which there is no need to interact with the environment during prob-
lem solving. But there are some works on using derivational anal-
ogy or macro actions in interactive environments [2, 12, 14]. In
these domains, it would be beneﬁcial not to discard the observation
sequences generated by the environments, since the observation se-
quences capture important information that can be used to deter-
mine whether an agent can utilize two different action sequences in

the same problem (see Deﬁnition 2). Our work pushes this idea fur-
ther by showing how to construct a strategy from interaction traces.
Case-based reasoning techniques have been used to improve ex-
isting reinforcement learning techniques [6, 16]. But so far case-
based reasoning played a supporting role only in these work. In
contrast, our technique generates fully-functional agents out of pre-
vious problem solving experiences, without the help of existing re-
inforcement learning techniques.
The winner of the 2005 IPD tournament is based on some sort of
case-based reasoning technique [10]. Similarly, the winning strat-
egy of our ICG tournament is a combination of three different ways
to deal with three different type of opponents. The success of these
strategies compels us to believe that one way to dominate mono-
lithic strategies such as Tit-for-Tat is to combine the winning strate-
gies for different type of opponents together.
One problem with the case-based reasoning strategies mentioned
in the above paragraph is that the ways they combine strategies
together are quite ad hoc, and only manage to consider a handful of
possible opponents’ strategies. Our work shows a systematic way
to combine strategies that can be scaled up to handle a large number
of different opponent’s strategies.
6.
SUMMARY AND FUTURE WORK
The idea that an agent can improve its performance by observing
interactions among other agents is not new. What is new in this
paper is how to do it without the knowledge of the set of opponents’
internal states. In open environments such as IPD tournaments, we
know little about the opponents’ strategy, let alone the current state
of the opponents’ strategy. Hence methods that do not require us to
know the opponents’ states can be quite valuable in such domains.
To avoid using the notion of states or belief states as in poli-
cies or automata, our approach directly selects and combines the
records of the other agents’ interactions to form a partial strategy
called a composite strategy, which maximizes the expected utility
with respect to an estimated probability distribution of opponents
that can possibly generate those interactions, without knowing the
states or other details of the strategies of these opponents. The com-
posite strategy can be used to decide how an agent should respond
to typical behaviors of opponents in an environment. Out of a set
of 126 agents in three iterated games (the IPD, ICG, and IBS), our
technique signiﬁcantly improved the agent’s rank (compared to the
other agents) after incorporating the partial strategy.
In future, we would like to address the following issues:
• In iterated games such as the IPD, ICG, and IBS, players fre-
quently encounter the game situations that have been seen be-
fore; hence combining the interaction traces was sufﬁcient to
provide large increases in an agent’s performance. By itself, this
would not be suitable for large-scale non-iterated games such
as chess, where games among experts usually lead to situations
that no player has ever seen before. We would like to develop
a way to combine our technique with game-tree search, to see
how much that extends the scope of the technique.
• A signiﬁcant limitation of our technique is that it requires an es-
timate of the probability with which we’ll encounter each of op-
ponents we have observed. In the future, we would like to study
how to estimate the probability from a database of interaction
traces and modify our techniques to alleviate this requirement.
• We’ve done preliminary tests on some nondeterministic
partially-observable planning domains (e.g., transportation do-
mains with unknown trafﬁc patterns).
Our approach worked
quite well in our tests, but we need to do larger cross-validated
experiments before reporting the results. In future, we intend to
generalize our method for domains that are more complicated
than two-player repeated games.
• Our current formalization cannot handle problems whose hori-
zon is inﬁnite. We would like to see how to extend our algorithm
to deal with interaction trace of potentially inﬁnite length.
7.
ACKNOWLEDGMENTS
This
work
was
supported
in
part
by
AFOSR
grants
FA95500510298,
FA95500610405,
and
FA95500610295,
DARPA’s Transfer Learning and Integrated Learning programs,
and NSF grants IIS0412812 and IIS0705587. Sarit Kraus is also
afﬁliated with UMIACS. The opinions in this paper are those of the
authors and do not necessarily reﬂect the opinions of the funders.
8.
REFERENCES
[1] R. Axelrod. The Evolution of Cooperation. Basic Books,
1984.
[2] S. Bhansali and M. T. Harandi. Synthesis of UNIX programs
using derivational analogy. Machine Learning, 10:7–55,
1993.
[3] D. Billings, N. Burch, A. Davidson, R. Holte, and
J. Schaeffer. Approximating game-theoretic optimal
strategies for full-scale poker. In IJCAI, pages 661–668,
2003.
[4] J. Denzinger and J. Hamdan. Improving modeling of other
agents using tentative stereotypers and compactiﬁcation of
observations. In Proceedings of the International Conference
on Intelligent Agent Technology, pages 106–112, 2004.
[5] M. Deutsch. The Resolution of Conﬂict: Constructive and
Destructive Processes. Yale University Press, 1973.
[6] C. Drummond. Accelerating reinforcement learning by
composing solutions of automatically identiﬁed subtasks.
JAIR, 16:59–104, 2002.
[7] D. Egnor. Iocaine powder explained. ICGA Journal,
23(1):33–35, 2000.
[8] G. Kendall, X. Yao, and S. Y. Chong. The Iterated Prisoner’s
Dilemma: 20 Years On. World Scientiﬁc, 2007.
[9] R. E. Korf. Macro-operators: A weak method for learning.
Artif. Intel., 26(1):35–77, 1985.
[10] J. Li. How to design a strategy to win an IPD tournament. In
G. Kendall, X. Yao, and S. Y. Chong, editors, The Iterated
Prisoner’s Dilemma: 20 Years On, pages 89–104. World
Scientiﬁc, 2007.
[11] R. D. Luce and H. Raiffa. Games and Decisions:
Introduction and Critical Survey. Wiley, 1957.
[12] A. McGovern and R. S. Sutton. Macro-actions in
reinforcement learning: An empirical analysis. Technical
Report 98-70, University of Massachusetts, Amherst, 1998.
[13] A. Parker, D. Nau, and V. Subrahmanian. Overconﬁdence or
paranoia? search in imperfect-information games. In AAAI,
July 2006.
[14] D. Precup, R. S. Sutton, and S. Singh. Theoretical results on
reinforcement learning with temporally abstract options. In
ECML, pages 382–393, 1998.
[15] M. M. Veloso and J. G. Carbonell. Derivational analogy in
prodigy: Automating case acquisition, storage, and
utilization. Machine Learning, 10(3):249–278, 1993.
[16] D. Zeng and K. Sycara. Using case-based reasoning as a
reinforcement learning framework for optimization with
changing criteria. In ICTAI, 1995.

