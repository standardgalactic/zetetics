Towards an Improved Understanding and Utilization
of Maximum Manifold Capacity Representations
Rylan Schaeffer ∗
Stanford CS
Victor Lecomte†
Stanford CS
Dhruv Pai†
Stanford CS
Andres Carranza†
Stanford CS
Berivan Isik†
Stanford EE
Alyssa Unell†
Stanford CS
Mikail Khona
MIT Physics
Thomas Yerxa
NYU Neural Science
Yann LeCun
NYU Data Science & Meta AI FAIR
SueYeon Chung
NYU Neural Science & Flatiron Institute
Andrey Gromov‡
UMD Physics & Meta AI FAIR
Ravid Shwartz-Ziv‡
NYU Data Science
Sanmi Koyejo‡
Stanford CS
Abstract
Maximum Manifold Capacity Representations (MMCR) is a recent multi-view
self-supervised learning (MVSSL) method that matches or surpasses other leading
MVSSL methods. MMCR is intriguing because it does not fit neatly into any of the
commonplace MVSSL lineages, instead originating from a statistical mechanical
perspective on the linear separability of data manifolds. In this paper, we seek to
improve our understanding and our utilization of MMCR. To better understand
MMCR, we leverage tools from high dimensional probability to demonstrate that
MMCR incentivizes alignment and uniformity of learned embeddings. We then
leverage tools from information theory to show that such embeddings maximize a
well-known lower bound on mutual information between views, thereby connecting
the geometric perspective of MMCR to the information-theoretic perspective com-
monly discussed in MVSSL. To better utilize MMCR, we mathematically predict
and experimentally confirm non-monotonic changes in the pretraining loss akin
to double descent but with respect to atypical hyperparameters. We also discover
compute scaling laws that enable predicting the pretraining loss as a function of
gradients steps, batch size, embedding dimension and number of views. We then
show that MMCR, originally applied to image data, is performant on multimodal
image-text data. By more deeply understanding the theoretical and empirical
behavior of MMCR, our work reveals insights on improving MVSSL methods.
1
Introduction
Multi-View Self-Supervised Learning (MVSSL; also known as Joint-Embedding Self-Supervised
Learning) is a powerful approach to unsupervised learning. The idea is to create multiple trans-
formations, or “views”, of unsupervised data, then use these views in a supervised-like manner to
learn generally useful representations. MVSSL methods are diverse but can be loosely grouped into
∗Correspondence to rschaef@cs.stanford.edu
†Equal contribution.
‡Equal advising.
Preprint. Under review.
arXiv:2406.09366v1  [cs.LG]  13 Jun 2024

σ
σ
σ
a
b
c
Figure 1: Schematic of Maximum Manifold Capacity Representations (MMCR). (Left) K ≥2
views are generated of each datum, then embedded through a deep neural network on the surface of
the hypersphere. Center: For each datum, the centroid of the embeddings is computed. (Right) The
MMCR pretraining loss, which is the negative nuclear norm of the centers, is then minimized.
different families [6]: (1) contrastive, e.g., CPC [57], MoCo 1 [38], SimCLR [16], MoCo 2 [18],
CMC [75], RPC [76] and TiCo [85]; (2) clustering e.g., Noise-as-Targets [12], DeepCluster [13],
Self-Labeling [3], Local Aggregation [86], SwAV [14]; (3) distillation/momentum e.g., BYOL [35],
DINO [15], SimSiam [17], TiCo [85]; and (4) redundancy reduction e.g., Barlow Twins [83], VICReg
[7], TiCo [85]. Many MVSSL methods either explicitly originate from information theory [57, 5] or
can be understood from an information-theoretic perspective [80, 81, 29, 73].
Recently, Yerxa et al. [82] proposed a new MVSSL method named Maximum Manifold Capacity
Representations (MMCR) that achieves superior-to-similar performance compared with leading
MVSSL methods. MMCR is interesting for at least two reasons. Firstly, MMCR does not fit
neatly into any of the MVSSL families: it is not (explicitly) contrastive, it performs no clustering,
it leverages no distillation, and it does not (explicitly) reduce redundancy. Secondly, unlike many
MVSSL methods that originate in information theory, MMCR distances itself by pointing out
that estimating mutual information in high dimensions has proven difficult and that more closely
approximating mutual information may not improve representations; instead, MMCR’s foundation
lies in the statistical mechanical characterization of the linear separability of data manifolds. In this
work, we seek to better understand MMCR and utilize this understanding to drive implementation
decisions. Our contributions are as follows:
1. We leverage tools from high dimensional probability to show that embeddings with perfect
invariance and perfect uniformity minimize the MMCR pretraining loss with high proba-
bility. This analysis involves bounding the MMCR pretraining loss, allowing us to define
a “pretraining percent error" for MMCR; this pretraining percent error then reveals two
interesting empirical phenomena (below).
2. We connect this distribution of embeddings to information theory by showing that such a
distribution maximizes a well-known variational lower bound on the mutual information
between embeddings of multiple views.
3. Our analysis of the MMCR pretraining loss predicts a double descent-like behavior in the
pretraining percent error as a function of two parameters: the number of manifolds N
and the embedding dimensionality D. We empirically test and confirm this prediction in
ResNet-18s [37] pretrained on STL-10 [22]. This is notable because (to the best of our
knowledge) double descent has not been observed in MVSSL and because these parameters
differ from the typical double descent parameters (number of data and number of model
parameters).
4. Our pretraining percent error additionally enables comparing different hyperparameters
on the MMCR pretraining loss – an ability not commonly available in MVSSL methods –
which reveals the existence of compute scaling laws.
5. We demonstrate that MMCR, originally proposed for images, can be similarly performant in
the multi-modal image+text setting. We show that MMCR applied to image+text pairs can
match CLIP [60] on DataComp Small containing 128M high quality image+caption pairs
[26] at smaller batch sizes but falls off with larger batch sizes.
2

2
Preliminaries
2.1
Multi-View Self-Supervised Learning (MVSSL)
Let fθ : X →Z denote our neural network with parameters θ. Suppose we have a dataset of P points
{xp}P
p=1 and a set of random transformations (augmentations) T such as color jittering, Gaussian
blur, solarization, etc.. For each datum xp in a batch of inputs, we sample K transformations
t(1), t(2), ..., t(K) ∼T yielding a set of augmented views: t(1)(xp), ..., t(K)(xp). We feed these
transformed data into the network and obtain embeddings Z:
z(k)
p
def
= fθ(t(k)(xp)) ∈Z.
In practice, Z is commonly the D-dimensional hypersphere SD−1 def
= {z ∈RD : zT z = 1} or
RD. Given that we will later touch on information theory, we need notation to refer to the random
variables; we use Z(k)
p
to denote the random variable for the embedding whose realization is z(k)
p .
2.2
Maximum Manifold Capacity Representations
Maximum Manifold Capacity Representations (MMCR) [82] originates from classical results regard-
ing performance of linear binary classifiers [23, 30, 31]. Consider P points (data) in D dimensions,
with arbitrarily assigned binary class labels; what is the probability that a linear binary classifier will
be able to successfully classify the points? Statistical mechanical calculations reveal that in the ther-
modynamic limit (P, D →∞; P/D →α ∈(0, ∞)), a phase transition occurs at capacity αc = 2.
More precisely, if α < αc, the linear binary classifier will succeed with probability 1; but if α > αc,
the classifier will succeed with probability 0. MMCR is based on an extension of this result from
points to manifolds [20]. MMCR proceeds in the following manner: MMCR takes the embeddings
output by the network and normalizes them to lie on the hypersphere: z(1)
p , ..., z(K)
p
∈SD−1. Then,
MMCR computes the center (average) of the embeddings for each datum:
cp
def
=
1
K
X
k
z(k)
p .
(1)
Next, MMCR forms a P × D matrix C where the n-th row of C is the center cp and defines the loss:
LMMCR
def
= −∥C∥∗
def
= −
rank(C)
X
r=1
σr(C),
(2)
where σr(C) is the r-th singular value of C and ∥· ∥∗is the nuclear norm (trace norm, Schatten 1-
norm). Minimizing the MMCR loss means maximizing the nuclear norm of the mean matrix C. Yerxa
et. al (2023) [82] note that no closed form solution exists for singular values of an arbitrary matrix,
but when P = 2, D = 2, a closed form solution exists that offers intuition: ∥C∥∗will be maximized
when (i) the norm of each mean is maximized i.e., ∥cp∥2 = 1 (recalling that 0 ≤∥cp∥< 1 since the
embeddings live on the hypersphere), and (ii) the means c1, c2 are orthogonal to one another. While
we commend the authors for working to offer intuition, it is unclear to what extent the P = 2, D = 2
setting sheds light on MMCR in general, as MMCR was theoretically derived and numerically
implemented in the large data and high dimension regime. We seek to generalize the dimensionality
of this statement and examine the impact of MMCR across arbitrary manifolds and dimensions.
3
A High-Dimensional Probability Analysis of Maximum Manifold Capacity
Representations
In this section, we derive and intuitively explain two properties of Maximum Manifold Capacity
Representations (MMCR). We specifically consider MMCR’s regime of large number of patterns P
and high embedding dimension D. We contribute two results:
1. The MMCR loss LMMCR can be minimized by (a) making each center cp = 1
K
P
k z(k)
p
lie on the surface of the hypersphere, and (b) making the distribution of centers as close to
uniform on the hypersphere as possible.
3

Figure 2: Embeddings with perfect reconstruction and perfect uniformity achieve the lowest
possible MMCR loss. Away from the P = D threshold, uniform random vectors achieve the
theoretically derived upper bound on the nuclear norm of the mean matrix M i.e. the lower bound on
LMMCR. The gap between the network’s loss and the lowest possible LMMCR falls (left) ∝P −1 or
(right) ∝D−1 away from the P = D threshold.
2. This configuration of centers maximizes a well-known variational lower bound on the mutual
information between embeddings [28] that was recently used to study and unify several
MVSSL families [29].
More formally, we begin by adopting two useful definitions from prior works [80, 29]:
Definition 3.1 (Perfect Reconstruction). We say a network fθ achieves perfect reconstruction if
∀x ∈X, ∀t(1), t(2) ∈T , z(1) = fθ(t(1)(x)) = fθ(t(2)(x)) = z(2).
Definition 3.2 (Perfect Uniformity). Let p(Z) be the distribution over the network representations
induced by the data sampling and transformation sampling distributions. We say a network fθ
achieves perfect uniformity if the distribution p(Z) is the uniform distribution on the hypersphere.
Intuitively, perfect reconstruction means that all views of the same datum are mapped by the network
to the same embedding, and perfect uniformity means that the embeddings are distributed uniformly
around the hypersphere. We will show that a network that achieves both perfect reconstruction and
perfect uniformity obtains the lowest possible MMCR loss by first showing that LMMCR has a lower
bound and then showing that such a network achieves this bound.
Proposition 3.3. Suppose that ∀p ∈[P], cT
p cp ≤1. Then, 0 ≤||C||∗≤
p
P min(P, D).
Proof. Let σ1, . . . , σmin(P,D) denote the singular values of C, so that ∥C∥∗= Pmin(P,D)
i=1
σi. The
lower bound follows by the fact that singular values are nonnegative. For the upper bound, we have
min(P,D)
X
i=1
σ2
i = Tr

CCT 
=
P
X
n=1
cT
p cp ≤P.
Then, by Cauchy-Schwarz on the sequences (1, . . . , 1) and
 σ1, . . . , σmin(P,D)

, we get
min(P,D)
X
i=1
σi ≤
v
u
u
u
t


min(P,D)
X
i=1
1




min(P,D)
X
i=1
σ2
i

≤
p
min(P, D) P.
Proposition 3.4. Let fθ achieve perfect reconstruction. Then, ∥cp∥2 = 1 ∀n.
Proof. Because fθ achieves perfect reconstruction, ∀n, ∀t(1), t(2), z(1)
p
= z(2)
p .
Thus cp =
(1/K) P
k z(k)
p
= (1/K) P
k z(1)
p
= z(1)
p , and since ∥z(1)
p ∥2 = 1, we have ∥cp∥2 = 1.
4

Theorem 3.5. Let fθ : X →SD be a network that achieves perfect reconstruction and perfect
uniformity. Then fθ achieves the lower bound of LMMCR with high probability. Specifically:
∥C∥∗=
P(1 −O(P/D))
if P ≤D
√
PD(1 −O(D/P))
if P ≥D ,
with high probability in min(P, D).
We defer the proof to Appendix A but offer intuition here. To show the inequality in Proposition 3.3
is roughly tight, we need to show the singular values σi are all roughly equal to each other. When
P ≪D, since C has few rows cp, they are almost perfectly orthogonal to each other, so all P
singular values will be ≈∥cp∥= 1. When P ≫D, since C has many rows, for any x ∈RD the sum
∥Cx∥2
2 = P
p(cT
p x)2 will be concentrated, so C scales all vectors roughly equally, and therefore its
D singular values are all roughly equal to each other. We confirm this via numerical simulations (Fig.
2); for code, see Appendix B. This demonstrates that the MMCR pretraining loss can be minimized
by minimizing the distances of all embeddings corresponding to the same datum and maximizing the
distances of all data’s centers.
4
An Information Theoretic Understanding of Maximum Manifold Capacity
Representations
Many MVSSL methods originate in information theory or can be understood from an information
theoretic perspective [57, 5, 80, 81, 29, 73]. Based on our newfound understanding of what distri-
butions of embeddings MMCR incentivizes, how can we connect MMCR’s statistical mechanical
geometric viewpoint to an information theoretic viewpoint? The answer is that MMCR incentivizes
representations that maximize a lower bound on the mutual information shared by two embeddings
corresponding to two views of the same datum.
Consider the mutual information between the embeddings of two different views Z(1) and Z(2) of
some input datum. The mutual information between the two views must be at least as great as the
sum of two terms: the ability of one embedding to “reconstruct” the other, plus the entropy of the
embeddings [28]:
I[Z(1); Z(2)] ≥Ep(Z(1),Z(2))[log q(Z(1)|Z(2))]
|
{z
}
Reconstruction
+ H[Z(1)]
| {z }
Entropy
,
(3)
where q(Z(1)|Z(2)) is a variational distribution because the true distribution p(Z(1)|Z(2)) is unknown.
This bound is well-known, e.g., [23, 80, 29], but we repeat them to show how MMCR connects to an
information-theoretic perspective.
Proposition 4.1. For any distribution on a bounded space, the uniform distribution has maximum
entropy.
Theorem 4.2. Let fθ : X →SD be a network, and let the number of views per datum K be a
constant. Let Q be the variational family of distributions on the hypersphere. Then fθ maximizes
the mutual information lower bound (Eqn. 3) if and only if fθ achieves perfect reconstruction and
perfect uniformity.
Proof. Perfect reconstruction maximizes reconstruction term. Perfect uniformity maximizes entropy.
Based on this result, we can now understand that a minimizer of the MMCR pretraining loss
is a maximizer of the lower bound of the mutual information between two embeddings of two
transformations of the same datum.
Theorem 4.3. Let fθ∗be a network that achieves perfect reconstruction and perfect uniformity, let
the number of views per datum K be a constant, and let Q be the variational family of distributions
on the hypersphere. Then fθ∗is both a minimizer of LMMCR and a maximizer of the variational
lower bound of mutual information Eqn. 3.
Proof. The proof follows from Theorem 3.5 and Theorem 4.2.
5

Figure 3: Double-Descent in Maximum Manifold Capacity Representations. As predicted
mathematically, MMCR’s pretraining percent error
def
= (
p
P min(P, D) −||C||∗)/
p
P min(P, D)
exhibits non-monotonic double descent-like behavior, peaking when the number of data points P
equals the number of dimensions D. On either side of the P = D threshold, the pretraining percent
error falls. Networks are ResNet-18s pretrained on STL-10’s “unlabeled" split.
5
Double Descent in Maximum Manifold Capacity Representations
Pretraining Loss
An unexpected and interesting insight from our high-dimensional probability analysis (Theorem
3.5) is a prediction that the Maximum Manifold Capacity Representations (MMCR) pretraining loss
should also exhibit a non-monotonic double descent-like behavior in its pretraining loss. Double
descent is a well-known machine learning phenomenon where the test loss exhibits non-monotonic
changes as a function of the total number of data and the number of model parameters [77, 47, 33,
48, 58, 25, 74, 9, 8, 10, 55, 59, 2, 50, 1, 63, 61, 62, 53, 36, 4, 68, 24, 70, 71]. However, our analysis
suggests that this double descent-like behavior should occur with respect to atypical parameters:
the number of manifolds P and the number of dimensions D, rather than the number of data and
the number of model parameters. Specifically, our theory predicts that the highest pretraining error
should occur exactly at the threshold P = D, with pretraining error falling on either side of the
threshold. Indeed, we discovered that Yerxa et al. (2023) [82] contains preliminary evidence of
such a phenomenon (App Fig. 8); however, the double descent-like effect in the figure was neither
commented on nor explored further, and the swept hyperparameters were suggestive but insufficient
to be conclusive4.
To compare losses across different pairs of hyperparameters number of points P and data dimension-
ality D, we use our MMCR pretraining bound (Prop. 3.3) to define a pretraining percent error:
Pretraining Percent Error(C)
def
=
p
P min(P, D) −||C||∗
p
P min(P, D)
.
(4)
To experimentally test our prediction, we pretrained ResNet-18 convolutional neural networks [37]
on STL-10 [22], a dataset similar to CIFAR-10 but higher resolution (96x96x3) and containing
an additional unlabeled split of 100000 images. We swept P ∈{64, 128, 256, 512, 1024} × D ∈
{64, 128, 256, 512, 1024} × K ∈{2, 4, 8}, where K is the number of views. For all combinations of
number of points P, number of dimensions D and number of views K, we found that the pretraining
percent error peaked when P = D (Fig. 3). On either side of the P = D threshold, the pretraining
percent error declined. Earlier work found that self-supervised learning doesn’t seem to produce
double descent like behavior [51], albeit for autoencoders. To the best of our knowledge, this is the
first observation of double descent-like behavior in Multi-View Self-Supervised Learning, especially
as a function of two unusual quantities (the number of data points P and number of embedding
4Specifically, the purpose of the figure from Yerxa et. al (2023) [82] was to study the effect of number of
manifolds, meaning that the data included only a single embedding dimension (D = 512) and only one number
of manifolds to the left of the threshold (P = 256).
6

Figure 4: Compute Scaling Laws. For all values of number of points P (equivalently, batch
size), embedding dimension D and number of views per datum K, the pretraining percent error
falls predictably as a power law with the amount of compute i.e. total floating point operations.
Consistent with the double descent-like findings in Fig. 3, the on-diagonal subfigures (corresponding
to P = D) exhibit higher initial pretraining percent errors and less steep slopes with compute than
the off-diagonal subfigures (corresponding to P ̸= D).
dimensions D) rather than the classical double descent quantities (total number of data points and
number of model parameters).
6
Compute Scaling Laws in Maximum Manifold Capacity Representations
In many MVSSL methods, changing hyperparameters often renders the pretraining losses incommen-
surate, making comparisons between runs difficult if not impossible. However, because the MMCR
pretraining percent error yields a quantity bounded between 0 and 1, we can compare different training
runs with different hyperparameter values for the number of data points P and data dimensionality
D. Performing such a comparison yields a second interesting empirical phenomenon: compute
neural scaling laws in the MMCR pretraining percent error. Scaling laws are another wide-spread
phenomenon of interest in machine learning where the pretraining loss follows a predictable power
law-like trend with respect to specific quantities such as number of parameters, number of data or
amount of compute [42, 64, 39, 46, 34, 40, 45, 84, 43, 21, 56, 41, 52, 69, 65, 54, 11, 27, 72].
By plotting the ResNet-18 networks pretrained on STL-10, one can clearly see power law scaling
in the pretraining percent error with the amount of compute for all number of points P, embedding
dimensions D, and number of views K (Fig. 4). A key detail is that these neural scaling curves
highlight the double descent-like behavior: the on-diagonal subfigures (corresponding to runs where
P = D) have both higher pretraining percent error and a less steep slope for the pretraining percent
7

Figure 5: Multimodal MMCR on Image-Text Caption Pairs. Left: Multimodal MMCR vs
Contrastive Language-Image Pretraining (CLIP) performance on ImageNet measured in zero-shot
top-1 accuracy. Multimodal MMCR outperforms CLIP for smaller batch sizes but underperforms
CLIP for larger batch sizes. Right: Imagenet top-1 accuracy sweep over batch sizes for MMCR.
Unlike CLIP, MMCR exhibits non-monotonic performance scaling with batch size, and best results
are found at intermediate batch sizes. To generate strong validation performance scaling behavior,
MMCR requires that both batch size and dimension increase simultaneously.
error, meaning that the pretraining percent error starts higher and falls more slowly. The takeway is
that practictioners would be well advised to not pretrain networks where the number of points P (i.e.
the batch size) equals the embedding dimension D.
7
Multi-Modality in Maximum Manifold Capacity Representations
We next demonstrate that MMCR can be high-performing in a decidedly more challenging setting:
multimodal self-supervised learning. Specifically, we consider the setting of OpenAI’s Contrastive
Language-Image Pretraining model (CLIP) [60], in which two different networks are pretrained
on image-text caption pairs. In this multimodal setting, two networks fθ and gθ′ embed data from
two different data domains X and Y . X and Y are paired, such that every example in X has a
corresponding positive pair in Y and vice versa. As such, from an MMCR perspective, X and
Y can be understood as two "views" of the same underlying object. The optimal transformed
embeddings fθ(X) and gθ′(Y ) therefore should map to the same space, and we can use our improved
understanding of MMCR to train these optimal networks. The notable difference between this
setting and the commonplace MVSSL setting is first that X and Y might represent extremely
different distributions in practice, and second fθ and gθ are two separate and different neural network
architectures. CLIP is a prominent example of such a cross-modal feature alignment task between a
text encoder and an image encoder [60]. In this paper, we investigate whether applying the MMCR
objective to the CLIP setting can improve the quality of learned representations.
In image-text alignment, we have access to image-text pairs, which are respectively fed through a
vision encoder (here, a ResNet-50) and a text encoder (a transformer [78]). We apply the MMCR
objective between the embeddings produced by the two modalities. We base our Multimodal MMCR
experiments off of the open-source CLIP implementation OpenCLIP [19]. We apply Multimodal
MMCR to DataComp-Small and compare zero-shot Imagenet performance with the standard CLIP
objective, which is equivalent to SimCLR with τ = 1. DataComp-Small is the smallest version of the
curated DataComp dataset family for training CLIP-style models [26]. This dataset consists of 128
million high-quality image and text pairs that can be used in multimodal training.
We found that convergence of MMCR in the image-text mapping setting is highly dependent on
learning rate, and models will fail to converge for learning rates above ≈1e −4. For all runs, we
set our Multimodal MMCR learning rate as 1e −4 and our normal CLIP learning rate as 1e −3.
With the standard CLIP embedding size of D = 1024, we swept performance of our models over the
critical hyperparameter of batch size (N), finding the optimal batch size to be 128. We compare the
performance of the optimal batch size Multimodal MMCR to normal CLIP (Fig. 5). We find that
while Multimodal MMCR outperforms CLIP at small batch sizes (< 512) and remains competitive
8

with CLIP with a batch size of 512, it underperforms CLIP at higher batch sizes. The CLIP loss is a
batch contrastive method, and thus benefits directly from increasing batch size. MMCR, however, is
simultaneously batch and dimension contrastive, and as a result to achieve similar scaling it is likely
Multimodal MMCR would need to increase the size of its latent embedding space beyond 1024 [32].
8
Relationship of MMCR to the Duality of Sample-Contrastive and
Dimension-Contrastive Self-Supervised Learning
In their ICLR 2023 paper “On the Duality Between Contrastive and Non-Contrastive Self-Supervised
Learning", Garrido et. al (2023) [32] noted that contrastive (also known as sample-contrastive) and
non-contrastive (also known as dimension-contrastive) SSL methods can be seen as two sides of
the same coin. Specifically, letting Z ∈RP K×D denote the matrix of stacked embeddings, then
sample-contrastive methods (e.g., SimCLR) incentivize entropy via:
LSample-Contrastive
def
= ||ZZT −diag(ZZT )||2
F ,
whereas dimension-contrastive methods (e.g., BarlowTwins) incentivize entropy via:
LDimension-Contrastive
def
= ||ZT Z −diag(ZT Z)||2
F .
Both families also include an invariant loss LInvariance as part of the total loss, typically the MSE
between the positive pairs. We observe that both families of loss aim to maximize on-diagonal
elements through their invariance loss and minimize off-diagonal elements through their contrastive
losses, on a batch-wise or dimension-wise correlation matrix respectively. In both cases, the loss
functions aim to maximize the spectra of their matrices (either ZZT or ZT Z), and given that these
matrices have related spectra, one might wonder why not maximize the spectrum of Z directly?
Maximizing Z’s spectrum is qualitatively what MMCR aims to do via its nuclear norm-based loss.
9
Discussion
In this paper, we theoretically and empirically studied Maximum Manifold Capacity Representations
(MMCR) [82], a recent high-performing multi-view self-supervised learning (MVSSL) method that
has demonstrated competitive-to-superior performance when compared to established methods in the
field. Our investigation into MMCR has two primary objectives: enhancing theoretical comprehension
and expanding practical usage.
Our theoretical exploration of MMCR reveals its potential to be understood not only through its
original geometric lens but also from an information-theoretic standpoint. The nuclear norm-based
objective of MMCR correlates with the maximization of a recognized mutual information lower bound
between views. This insight provides a deeper understanding of MMCR’s operational mechanics.
Additionally, we identified a non-monotonic behavior in pretraining loss associated with MMCR,
reminiscent of the double descent phenomenon, but influenced by non-traditional hyperparameters.
Furthermore, the compute scaling curves, derived from MMCR’s loss function, facilitate a direct
comparison of hyperparameters, marking an innovative step forward in hyperparameter evaluation
and selection within the MVSSL framework.
Expanding MMCR’s application beyond its initial success in image data, we demonstrate efficacy in
multimodal image-text scenarios. Leveraging our refined understanding of MMCR’s hyperparameters,
we illustrate its adaptability and robustness across diverse data types. This extension underscores
MMCR’s versatility, attributed to its geometric foundations, and opens up new avenues for its
application in various MVSSL challenges.
In conclusion, our deeper theoretical insights into MMCR not only elucidate its underlying principles
but also enable the development of more versatile and effective implementations. By harnessing these
theoretical advancements, MMCR’s applicability across a broader spectrum of MVSSL problems is
significantly enhanced, promising new directions for future research in the field.
Note: This manuscript appeared earlier at several workshops [44, 49, 66, 67].
9

References
[1] B. Adlam and J. Pennington. Understanding double descent requires a fine-grained bias-variance
decomposition. Advances in neural information processing systems, 33:11022–11032, 2020.
[2] M. S. Advani, A. M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization
error in neural networks. Neural Networks, 132:428–446, 2020.
[3] Y. Asano, C. Rupprecht, and A. Vedaldi.
Self-labelling via simultaneous clustering and
representation learning. In International Conference on Learning Representations, 2019.
[4] F. Bach. High-dimensional analysis of double descent for linear regression with random
projections, 2023.
[5] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual
information across views. Advances in neural information processing systems, 32, 2019.
[6] R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes,
G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez,
A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum. A cookbook of self-supervised learning,
2023.
[7] A. Bardes, J. Ponce, and Y. LeCun. VICReg: Variance-invariance-covariance regularization for
self-supervised learning. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=xm6YD62D1Ub.
[8] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression.
Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.
[9] M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and
the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116
(32):15849–15854, 2019.
[10] M. Belkin, D. Hsu, and J. Xu. Two models of double descent for weak features. SIAM Journal
on Mathematics of Data Science, 2(4):1167–1180, jan 2020. doi: 10.1137/20m1336072. URL
https://doi.org/10.1137%2F20m1336072.
[11] T. Besiroglu, E. Erdil, M. Barnett, and J. You. Chinchilla scaling: A replication attempt, 2024.
[12] P. Bojanowski and A. Joulin. Unsupervised learning by predicting noise. In International
Conference on Machine Learning, pages 517–526. PMLR, 2017.
[13] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning
of visual features. In Proceedings of the European conference on computer vision (ECCV),
pages 132–149, 2018.
[14] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of
visual features by contrasting cluster assignments. Advances in neural information processing
systems, 33:9912–9924, 2020.
[15] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging
properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 9650–9660, 2021.
[16] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning
of visual representations. In International conference on machine learning, pages 1597–1607.
PMLR, 2020.
[17] X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 15750–15758, 2021.
[18] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive
learning. arXiv preprint arXiv:2003.04297, 2020.
[19] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann,
L. Schmidt, and J. Jitsev. Reproducible scaling laws for contrastive language-image learning,
2022.
[20] S. Chung, D. D. Lee, and H. Sompolinsky. Classification and geometry of general perceptual
manifolds. Physical Review X, 8(3):031003, 2018.
10

[21] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. Hecht-
man, T. Cai, S. Borgeaud, et al. Unified scaling laws for routed language models. In International
Conference on Machine Learning, pages 4057–4086. PMLR, 2022.
[22] A. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pages 215–223. JMLR Workshop and Conference Proceedings, 2011.
[23] T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326–334,
1965.
[24] A. Curth, A. Jeffares, and M. van der Schaar. A u-turn on double descent: Rethinking parameter
counting in statistical learning. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023.
[25] R. P. Duin. Classifiers in almost empty spaces. In Proceedings 15th International Conference
on Pattern Recognition. ICPR-2000, volume 2, pages 1–7. IEEE, 2000.
[26] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman,
D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. Pratt, V. Ramanujan, Y. Bitton,
K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. Ratner,
S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y. Carmon,
V. Shankar, and L. Schmidt. Datacomp: In search of the next generation of multimodal datasets,
2023.
[27] S. Y. Gadre, G. Smyrnis, V. Shankar, S. Gururangan, M. Wortsman, R. Shao, J. Mercat, A. Fang,
J. Li, S. Keh, R. Xin, M. Nezhurina, I. Vasiljevic, J. Jitsev, A. G. Dimakis, G. Ilharco, S. Song,
T. Kollar, Y. Carmon, A. Dave, R. Heckel, N. Muennighoff, and L. Schmidt. Language models
scale reliably with over-training and on downstream tasks, 2024.
[28] R. G. Gallager. Information theory and reliable communication, volume 588. Springer, 1968.
[29] B. R. Gálvez, A. Blaas, P. Rodríguez, A. Golinski, X. Suau, J. Ramapuram, D. Busbridge, and
L. Zappella. The role of entropy and reconstruction in multi-view self-supervised learning. In
International Conference on Machine Learning, pages 29143–29160. PMLR, 2023.
[30] E. Gardner. Maximum storage capacity in neural networks. Europhysics letters, 4(4):481, 1987.
[31] E. Gardner.
The space of interactions in neural network models.
Journal of physics A:
Mathematical and general, 21(1):257, 1988.
[32] Q. Garrido, Y. Chen, A. Bardes, L. Najman, and Y. LeCun. On the duality between contrastive
and non-contrastive self-supervised learning. In The Eleventh International Conference on
Learning Representations, 2023.
[33] S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma.
Neural computation, 4(1):1–58, 1992.
[34] M. A. Gordon, K. Duh, and J. Kaplan. Data and parameter scaling laws for neural machine
translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 5915–5922, 2021.
[35] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch,
B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new ap-
proach to self-supervised learning. Advances in neural information processing systems, 33:
21271–21284, 2020.
[36] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless
least squares interpolation. The Annals of Statistics, 50(2):949–986, 2022.
[37] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–
778, 2016.
[38] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual
representation learning. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 9729–9738, 2020.
[39] T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhari-
wal, S. Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint
arXiv:2010.14701, 2020.
11

[40] D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer. arXiv
preprint arXiv:2102.01293, 2021.
[41] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-Showk, N. Elhage,
Z. Hatfield-Dodds, T. Henighan, T. Hume, et al. Scaling laws and interpretability of learning
from repeated data. arXiv preprint arXiv:2205.10487, 2022.
[42] J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali,
Y. Yang, and Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint
arXiv:1712.00409, 2017.
[43] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas,
L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models.
arXiv preprint arXiv:2203.15556, 2022.
[44] B. Isik, V. Lecomte, R. Schaeffer, Y. LeCun, M. Khona, R. Shwartz-Ziv, S. Koyejo, and A. Gro-
mov. An information-theoretic understanding of maximum manifold capacity representations.
In UniReps: the First Workshop on Unifying Representations in Neural Models, 2023.
[45] A. L. Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021.
[46] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-
ford, J. Wu, and D. Amodei.
Scaling laws for neural language models.
arXiv preprint
arXiv:2001.08361, 2020.
[47] A. Krogh and J. Hertz. A simple weight decay can improve generalization. Advances in neural
information processing systems, 4, 1991.
[48] A. Krogh and J. A. Hertz. Generalization in a linear perceptron in the presence of noise. Journal
of Physics A: Mathematical and General, 25(5):1135, 1992.
[49] V. Lecomte, R. Schaeffer, B. Isik, M. Khona, Y. LeCun, S. Koyejo, A. Gromov, and R. Shwartz-
Ziv. An information-theoretic understanding of maximum manifold capacity representations.
In NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations, 2023.
[50] T. Liang and A. Rakhlin. Just interpolate: Kernel “ridgeless” regression can generalize. The
Annals of Statistics, 48(3):1329–1347, 2020.
[51] A. Lupidi, Y. Gideoni, and D. Jayalath. Does double descent occur in self-supervised learning?
arXiv preprint arXiv:2307.07872, 2023.
[52] A. Maloney, D. A. Roberts, and J. Sully. A solvable model of neural scaling laws. arXiv preprint
arXiv:2210.16859, 2022.
[53] S. Mei and A. Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics,
75(4):667–766, 2022.
[54] N. Muennighoff, A. Rush, B. Barak, T. Le Scao, N. Tazi, A. Piktus, S. Pyysalo, T. Wolf, and
C. A. Raffel. Scaling data-constrained language models. Advances in Neural Information
Processing Systems, 36, 2024.
[55] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever. Deep double descent:
Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and
Experiment, 2021(12):124003, 2021.
[56] O. Neumann and C. Gros. Scaling laws for a multi-agent reinforcement learning model. arXiv
preprint arXiv:2210.00849, 2022.
[57] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748, 2018.
[58] M. Opper. Statistical mechanics of learning: Generalization. The handbook of brain theory and
neural networks, pages 922–925, 1995.
[59] T. Poggio, G. Kur, and A. Banburski. Double descent in the condition number. arXiv preprint
arXiv:1912.06190, 2019.
[60] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from
natural language supervision, 2021.
12

[61] J. W. Rocks and P. Mehta. The geometry of over-parameterized regression and adversarial
perturbations. arXiv preprint arXiv:2103.14108, 2021.
[62] J. W. Rocks and P. Mehta. Bias-variance decomposition of overparameterized regression with
random linear features. Physical Review E, 106(2):025304, 2022.
[63] J. W. Rocks and P. Mehta. Memorizing without overfitting: Bias, variance, and interpolation in
overparameterized models. Physical Review Research, 4(1):013201, 2022.
[64] J. S. Rosenfeld, A. Rosenfeld, Y. Belinkov, and N. Shavit. A constructive prediction of the
generalization error across scales. In International Conference on Learning Representations,
2019.
[65] N. Sardana and J. Frankle. Beyond chinchilla-optimal: Accounting for inference in language
model scaling laws, 2023.
[66] R. Schaeffer, B. Isik, V. Lecomte, M. Khona, Y. LeCun, A. Gromov, R. Shwartz-Ziv, and
S. Koyejo. An information-theoretic understanding of maximum manifold capacity represen-
tations. In NeurIPS 2023 Workshop: Information-Theoretic Principles in Cognitive Systems,
2023.
[67] R. Schaeffer, B. Isik, D. B. Pai, A. Carranza, V. Lecomte, A. Unell, M. Khona, T. E. Yerxa,
Y. LeCun, S. Chung, et al. Towards an improved understanding and utilization of maximum
manifold capacity representations. In NeurIPS 2024 Workshop on Mathematical and Empirical
Understanding of Foundation Models, 2023.
[68] R. Schaeffer, M. Khona, Z. Robertson, A. Boopathy, K. Pistunova, J. W. Rocks, I. R. Fiete, and
O. Koyejo. Double descent demystified: Identifying, interpreting and ablating the sources of a
deep learning puzzle, 2023.
[69] R. Schaeffer, B. Miranda, and S. Koyejo. Are emergent abilities of large language models a
mirage? Advances in Neural Information Processing Systems, 36, 2023.
[70] R. Schaeffer, Z. Robertson, A. Boopathy, M. Khona, I. Fiete, A. Gromov, and S. Koyejo.
Divergence at the interpolation threshold: Identifying, interpreting & ablating the sources of a
deep learning puzzle. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning,
2023.
[71] R. Schaeffer, Z. Robertson, A. Boopathy, M. Khona, K. Pistunova, J. W. Rocks,
I. R. Fiete, A. Gromov, and S. Koyejo.
Double descent demystified.
In ICLR
Blogposts 2024,
2024.
URL https://iclr-blogposts.github.io/2024/blog/
double-descent-demystified/. https://iclr-blogposts.github.io/2024/blog/double-descent-
demystified/.
[72] R. Schaeffer, H. Schoelkopf, B. Miranda, G. Mukobi, V. Madan, A. Ibrahim, H. Bradley,
S. Biderman, and S. Koyejo. Why has predicting downstream capabilities of frontier ai models
with scale remained elusive?, 2024.
[73] R. Shwartz-Ziv, R. Balestriero, K. Kawaguchi, T. G. Rudner, and Y. LeCun. An information-
theoretic perspective on variance-invariance-covariance regularization.
arXiv preprint
arXiv:2303.00633, 2023.
[74] S. Spigler, M. Geiger, S. d’Ascoli, L. Sagun, G. Biroli, and M. Wyart. A jamming transition
from under-to over-parametrization affects loss landscape and generalization. arXiv preprint
arXiv:1810.09665, 2018.
[75] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16,
pages 776–794. Springer, 2020.
[76] Y.-H. H. Tsai, M. Q. Ma, M. Yang, H. Zhao, L.-P. Morency, and R. Salakhutdinov. Self-
supervised representation learning with relative predictive coding. In International Conference
on Learning Representations, 2021. URL https://openreview.net/forum?id=068E_
JSq9O.
[77] F. Vallet.
The hebb rule for learning linearly separable boolean functions: learning and
generalization. Europhysics Letters, 8(8):747, 1989.
[78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017.
13

[79] R. Vershynin, Y. Eldar, and G. Kutyniok. Compressed sensing, theory and applications. In
Introduction to the non-asymptotic analysis of random matrices, pages 210–268. Cambridge
Univ. Press, 2012.
[80] T. Wang and P. Isola. Understanding contrastive representation learning through alignment
and uniformity on the hypersphere. In International Conference on Machine Learning, pages
9929–9939. PMLR, 2020.
[81] M. Wu, C. Zhuang, M. Mosse, D. Yamins, and N. Goodman. On mutual information in
contrastive learning for visual representations. arXiv preprint arXiv:2005.13149, 2020.
[82] T. Yerxa, Y. Kuang, E. Simoncelli, and S. Chung. Learning efficient coding of natural images
with maximum manifold capacity representations. arXiv preprint arXiv:2303.03307, 2023.
[83] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny. Barlow twins: Self-supervised learning via
redundancy reduction. In International Conference on Machine Learning, pages 12310–12320.
PMLR, 2021.
[84] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104–12113,
2022.
[85] J. Zhu, R. M. Moraes, S. Karakulak, V. Sobol, A. Canziani, and Y. LeCun. Tico: Transformation
invariance and covariance contrast for self-supervised visual representation learning. arXiv
preprint arXiv:2206.10698, 2022.
[86] C. Zhuang, A. L. Zhai, and D. Yamins. Local aggregation for unsupervised learning of visual
embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 6002–6012, 2019.
14

A
Proof of Theorem 3.5
Recall that LMMCR = −∥C∥∗is minimized when ∥C∥∗is maximized and that ∥C∥∗is upper
bounded by
√
ND if N > D and N if N < D (Proposition 3.3). We want to show a network that
achieves perfect reconstruction and perfect uniformity achieves this upper bound on the nuclear norm
(equivalently, lower bound on the MMCR loss).
Following the proof of Proposition 3.3, let σ1, . . . , σmin(N,D) denote the singular values of C, so that
∥C∥∗= P
i σi. By Proposition 3.4, we have
X
i
σ2
i = Tr

CCT 
=
N
X
n=1
µT
nµn = N.
Now,
by
the
equality
version
of
Cauchy–Schwarz
on
the
sequences
(1, . . . , 1)
and
 σ1, . . . , σmin(N,D)

, we have
X
i
σi =
v
u
u
tmin(N, D)
 X
i
σ2
i −
X
i

σi −
P
j σj
min(N, D)
2!
.
(5)
So if we can bound this “variance” of the singular values P
i

σi −
P
j σj
min(N,D)
2
, we can show that
∥C∥∗closely matches the upper bound obtained in Proposition 3.3.
To do this, let us consider matrix
√
DC. The vectors µn are uniform over the D-dimensional
hypersphere SD, so its rows
√
Dµn have mean zero, are isotropic, and (by Example 5.25 in Vershynin
(2012) [79]) are sub-gaussian with parameter ∥
√
Dµn∥ψ2 = O(1).5 Therefore,
• If N ≤D, then (using the fact that ∥µn∥2 = 1 for all n ∈[N]) we can to apply The-
orem 5.58 in Vershynin (2012)[79] on the transpose of
√
DC, obtaining that for any
t ≥0, the singular values of
√
DC are within
√
D ± O(
√
N) + t with probability at
least 1 −2 exp(−Ω(t2)). Setting t to a large enough multiple of
√
N, they are all within
√
D ± O(
√
N) with probability at least 1 −2 exp(−N). Consequently, with the same
probability, the singular values of C are all within ±O(
p
N/D) of each other, and we get
P
i

σi −
P
j σj
min(N,D)
2
≤N · O
p
N/D
2
= O(N 2/D). Plugging this into Eqn. 5, we
get ∥C∥∗≤
p
N(N −O(N 2/D)) =
√
N(1 −O(N/D)).
• If N ≥D, then we can apply Theorem 5.39 in Vershynin (2012) [79] on
√
DC, obtaining
that for any t ≥0, the singular values of
√
DC are within
√
N ± O(
√
D) + t with
probability at least 1 −2 exp(−Ω(t2)). Setting t to a large enough multiple of
√
D, they
are all within
√
N ± O(
√
D) with probability at least 1 −2 exp(−D). Consequently, with
the same probability, the singular values of C are all within ±O(1) of each other, and
we get P
i

σi −
P
j σj
min(N,D)
2
≤D · O(1)2 = O(D). Plugging this into Eqn. 5, we get
∥C∥∗≤
p
D(N −O(D)) =
√
ND(1 −O(D/N)).
5Here, ∥· ∥ψ2 denotes the sub-gaussian norm (intuitively, the “effective standard deviation” of a sub-gaussian
random variable). For a scalar random variable X, it is defined as ∥X∥ψ2 := supp≥1 p−1/2(E[|X|p])1/p
(Definition 5.7 in Vershynin (2012)[79]), and for a random vector u ∈RD, it is defined as ∥u∥ψ2 :=
supv∈SD ∥uT v∥ψ2 (Definition 5.22 in [79]).
15

Figure 6: Multimodal MMCR exhibits strong batch size dependence in ImageNet zero-shot validation
performance. Intermediate batch sizes, such as 128 and 256, achieved the best validation performance
by a large margin. By contrast, the smallest batch sizes or batch sizes closest to the embedding
dimension size of 1024 fared the poorest.
B
Python Code for Perfect Reconstruction and Perfect Uniformity
Embeddings
To test our claim that networks which achieve perfect reconstruction and perfect uniformity achieve
the nuclear norm upper bound, we sample a uniform distribution of centroids (thereby enforcing
reconstruction by construction) and measure the nuclear norm relative to our claimed upper bound.
Python code for our simulations is included below:
import
pandas
as pd
import numpy as np
N _ l i s t = np . logspace ( s t a r t =1 ,
stop =4 , num=11). astype ( i n t )
D _ l i s t = np . logspace ( s t a r t =1 ,
stop =4 , num=11). astype ( i n t )
r e p e a t s = np . arange ( 5 ) . astype ( i n t )
u n i f o r m _ d i s t r i b u t i o n _ n u c l e a r _ n o r m _ d a t a _ l i s t = [ ]
for N in
N _ l i s t :
for D in
D _ l i s t :
print ( f "N:
{N}\ tD :
{D} " )
for
r e p e a t
in
r e p e a t s :
embeddings = np . random . normal ( loc =0 ,
s c a l e =10.0 ,
s i z e =(N, D) )
embeddings
/= np . l i n a l g . norm ( embeddings ,
a x i s =1 ,
keepdims=True )
row = {
" Spectrum " :
" uniform " ,
"Number of
Data
Manifolds
(N) " : N,
" Embedding
Dimensionality
(D) " : D,
" Repeat " :
repeat ,
" Nuclear Norm" :
np . l i n a l g . norm ( embeddings ,
ord=" nuc " ) ,
}
u n i f o r m _ d i s t r i b u t i o n _ n u c l e a r _ n o r m _ d a t a _ l i s t . append ( row )
u n i f o r m _ d i s t r i b u t i o n _ n u c l e a r _ n o r m _ d f = pd . DataFrame (
u n i f o r m _ d i s t r i b u t i o n _ n u c l e a r _ n o r m _ d a t a _ l i s t
)
C
Multimodal Maximum Manifold Capacity Representations
16

Figure 7: To understand the perplexing batch size dependence, we analyze the complement of the
average centroid norm (1 −∥µ∥2
2) and the pretaining percent error relative to the lower bound as
defined earlier(1 −∥M∥∗
N
). The complement of the average centroid norm is an unbiased estimator
for perfect reconstruction in our network. We find that lower batch sizes converge closer to perfect
reconstruction and to lower percent error.
17

