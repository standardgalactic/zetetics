Undergraduate Lecture Notes in Physics
Albrecht Lindner
Dieter Strauch
A Complete 
Course on 
Theoretical 
Physics
From Classical Mechanics to Advanced 
Quantum Statistics

Undergraduate Lecture Notes in Physics
Series editors
Neil Ashby, University of Colorado, Boulder, CO, USA
William Brantley, Department of Physics, Furman University, Greenville, SC, USA
Matthew Deady, Physics Program, Bard College, Annandale-on-Hudson, NY, USA
Michael Fowler, Department of Physics, University of Virginia, Charlottesville,
VA, USA
Morten Hjorth-Jensen, Department of Physics, University of Oslo, Oslo, Norway

Undergraduate Lecture Notes in Physics (ULNP) publishes authoritative texts covering
topics throughout pure and applied physics. Each title in the series is suitable as a basis for
undergraduate instruction, typically containing practice problems, worked examples, chapter
summaries, and suggestions for further reading.
ULNP titles must provide at least one of the following:
•
An exceptionally clear and concise treatment of a standard undergraduate subject.
•
A solid undergraduate-level introduction to a graduate, advanced, or non-standard subject.
•
A novel perspective or an unusual approach to teaching a subject.
ULNP especially encourages new, original, and idiosyncratic approaches to physics teaching
at the undergraduate level.
The purpose of ULNP is to provide intriguing, absorbing books that will continue to be the
reader’s preferred reference throughout their academic career.
More information about this series at http://www.springer.com/series/8917

Albrecht Lindner
• Dieter Strauch
A Complete Course
on Theoretical Physics
From Classical Mechanics to Advanced
Quantum Statistics
123

Albrecht Lindner
Pinneberg, Germany
Dieter Strauch
Theoretical Physics
University of Regensburg
Regensburg, Germany
ISSN 2192-4791
ISSN 2192-4805
(electronic)
Undergraduate Lecture Notes in Physics
ISBN 978-3-030-04359-9
ISBN 978-3-030-04360-5
(eBook)
https://doi.org/10.1007/978-3-030-04360-5
Library of Congress Control Number: 2018961698
The original, German edition was published in 2011 under the title “Grundkurs Theoretische Physik”.
© Springer Nature Switzerland AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

In memory of Albrecht Lindner (1935–2005),
scientist, teacher, friend

Preface
This textbook is a translation of the third German edition of Grundkurs
Theoretische Physik (A Basic Course on Theoretical Physics), originally published
by Teubner, Stuttgart, Germany. Actually, this edition is much more than a typical
textbook since it offers a mixture of basic and advanced material of all of the
fundamental disciplines of theoretical physics in one volume, whence it may well
serve also as a reference book. The large number of cross-references will guide the
reader from the basic experimental observations to the construction of a “uniﬁed”
theory, and the present compactness should ensure that the reader does not get lost
along the way.
A wide range of problems invite the reader to tackle further applications at
various stages of sophistication, and a list of textbooks offers the way forward to
possible open questions.
The material itself and the way it is presented is due to the late Albrecht Lindner.
My contribution is restricted merely to the translation into the English language; in
fact, my sincerest gratitude goes to Dr. Steven Lyle who corrected the translation in
manly places; whatever remains of insufﬁcient vocabulary or grammar is due to my
limited mastery of the language. The only changes I have made are to adjust to the
publisher’s requirements, made some changes in the numerical tables as to be
expected from May 2019 on, and adapt the list of textbooks to an English-speaking
readership.
I am proud, nevertheless, to present this book to the English-speaking
community.
Regensburg, Germany
Dieter Strauch
vii

Preface to the First German Edition
Like the standard course in theoretical physics, the present book introduces the
physics of particles under the heading Classical Mechanics, the physics of ﬁelds
under Electromagnetism, quantum physics under Quantum Mechanics I, and sta-
tistical physics under Thermodynamics and Statistics. Besides these branches,
which would form a curriculum for all students of physics, there is a complement
entitled Quantum Mechanics II, for those who wish to obtain a deeper under-
standing of the theory, which discusses scattering problems, quantization of ﬁelds,
and Dirac theory (as an example of relativistic quantum mechanics).
The goal here is to stress the interrelations between the individual subjects. In an
introductory chapter, there is a summary of the most important parts mathematical
tools repeatedly needed in the different branches of physics. These constitute the
mathematical foundation for rationalizing our practical experience, since we wish
to describe our observations as precisely as possible.
The selection of material was mainly inspired by our local physics diploma
curriculum. Only in a few places did I go beyond those limits, e.g., in Sect. 4.6
(quantum theory and dissipation), Sect. 5.2 (three-body scattering), and Sect. 5.4
(quasi-particles, quantum optics), since I have the impression that the essentials can
also be worked out rather easily in these areas.
Section 5.5 on the Dirac equation also differs from the standard presentation,
because I prefer the Weyl representation over the standard representation—despite
my intention to avoid any special representation as far as possible. In this respect, I
am grateful to my colleagues Till Anders (Munich), Dietmar Kolb (Kassel), und
Gernot Münster (Münster) for their valuable comments on my drafts.
Thanks go also to numerous students in Hamburg and especially to Dr. Heino
Freese and Dr. Adolf Kitz for many questions and suggestions, and various forms
of support. The general interest in my notes encourages me to present these now to
a larger community.
(Notes on ﬁgure production are left out here—D.S.)
Hamburg, Germany
Albrecht Lindner
Fall 1993
ix

Preface to the Second German Edition
The text has been improved at many places, in particular in Sects. 3.5 and 5.4, and
all ﬁgures have been inserted with pstricks. In addition, three-dimensional objects
now appear in central instead of of parallel perspetive.
Hamburg, Germany
Albrecht Lindner
Summer 1996
xi

Preface to the Third German Edition
The Basic Course (Grundkurs) was discovered in a third, extensively revised
edition, after Albrecht Lindner, a passionate teacher, unexpectedly passed away. As
one of those rare textbooks which presents a complete curriculum of theoretical
physics in a single volume—compact and simultaneously profound—it should be
offered to the teacher and student community. In the present third edition the
material has been revised in many places, and the number of ﬁgures has been
approximately doubled. Also in this edition is an additional chapter containing
numerous problems.
My contribution here is restricted to adjusting the material to the changed
appearance required by the Teubner publishing company.
Regensburg, Germany
Dieter Strauch
Spring 2011
xiii

Contents
1
Basics of Experience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Vector Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Space and Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.2
Vector Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.3
Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.4
Vector Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.1.5
Gradient (Slope Density) . . . . . . . . . . . . . . . . . . . . . . . .
10
1.1.6
Divergence (Source Density) . . . . . . . . . . . . . . . . . . . . .
11
1.1.7
Curl (Vortex Density). . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.1.8
Rewriting Products. Laplace Operator . . . . . . . . . . . . . . .
14
1.1.9
Integral Theorems for Vector Expressions . . . . . . . . . . . .
16
1.1.10
Delta Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.1.11
Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.1.12
Calculation of a Vector Field from Its Sources
and Curls. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.1.13
Vector Fields at Interfaces . . . . . . . . . . . . . . . . . . . . . . .
27
1.2
Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.2.1
Orthogonal Transformations and Euler Angles . . . . . . . . .
28
1.2.2
General Coordinates and Their Base Vectors . . . . . . . . . .
31
1.2.3
Coordinate Transformations . . . . . . . . . . . . . . . . . . . . . .
33
1.2.4
The Concept of a Tensor . . . . . . . . . . . . . . . . . . . . . . . .
35
1.2.5
Gradient, Divergence, and Rotation in General
Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
1.2.6
Tensor Extension, Christoffel Symbols . . . . . . . . . . . . . .
41
1.2.7
Reformulation of Partial Differential Quotients. . . . . . . . .
43
1.3
Measurements and Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
1.3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
1.3.2
Mean Value and Average Error . . . . . . . . . . . . . . . . . . .
46
1.3.3
Error Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
xv

1.3.4
Error Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
1.3.5
Finite Measurement Series and Their Average Error. . . . .
50
1.3.6
Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
1.3.7
Method of Least Squares . . . . . . . . . . . . . . . . . . . . . . . .
51
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Suggestions for Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2
Classical Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1
Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1.1
Force and Counter-Force . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1.2
Work and Potential Energy . . . . . . . . . . . . . . . . . . . . . . .
56
2.1.3
Constraints: Forces of Constraint, Virtual
Displacements, and Principle of Virtual Work . . . . . . . . .
58
2.1.4
General Coordinates and Forces . . . . . . . . . . . . . . . . . . .
59
2.1.5
Lagrangian Multipliers and Lagrange Equations
of the First Kind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.1.6
The Kepler Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.1.7
Summary: Basic Concepts . . . . . . . . . . . . . . . . . . . . . . .
68
2.2
Newtonian Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.2.1
Force-Free Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.2.2
Center-of-Mass Theorem . . . . . . . . . . . . . . . . . . . . . . . .
70
2.2.3
Collision Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
2.2.4
Newton’s Second Law . . . . . . . . . . . . . . . . . . . . . . . . . .
76
2.2.5
Conserved Quantities and Time Averages . . . . . . . . . . . .
77
2.2.6
Planetary Motion as a Two-Body Problem,
and Gravitational Force . . . . . . . . . . . . . . . . . . . . . . . . .
79
2.2.7
Gravitational Acceleration . . . . . . . . . . . . . . . . . . . . . . .
81
2.2.8
Free-Fall, Thrust, and Atmospheric Drag . . . . . . . . . . . . .
83
2.2.9
Rigid Bodies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
2.2.10
Moment of Inertia . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
2.2.11
Principal Axis Transformation . . . . . . . . . . . . . . . . . . . .
87
2.2.12
Accelerated Reference Frames and Fictitious Forces. . . . .
90
2.2.13
Summary of Newtonian Mechanics . . . . . . . . . . . . . . . . .
92
2.3
Lagrangian Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.3.1
D’Alembert’s Principle . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.3.2
Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
2.3.3
Lagrange Equations of the Second Kind . . . . . . . . . . . . .
95
2.3.4
Velocity-Dependent Forces and Friction . . . . . . . . . . . . .
97
2.3.5
Conserved Quantities. Canonical and Mechanical
Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
xvi
Contents

2.3.6
Physical Pendulum . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
2.3.7
Damped Oscillation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
2.3.8
Forced Oscillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
2.3.9
Coupled Oscillations and Normal Coordinates . . . . . . . . .
112
2.3.10
Time-Dependent Oscillator. Parametric Resonance . . . . . .
116
2.3.11
Summary: Lagrangian Mechanics . . . . . . . . . . . . . . . . . .
120
2.4
Hamiltonian Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
2.4.1
Hamilton Function and Hamiltonian Equations . . . . . . . .
121
2.4.2
Poisson Brackets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
2.4.3
Canonical Transformations . . . . . . . . . . . . . . . . . . . . . . .
125
2.4.4
Inﬁnitesimal Canonical Transformations. Liouville
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
2.4.5
Generating Functions . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
2.4.6
Transformations to Moving Reference Frames.
Perturbation Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
2.4.7
Hamilton–Jacobi Theory . . . . . . . . . . . . . . . . . . . . . . . . .
135
2.4.8
Integral Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
2.4.9
Motion in a Central Field . . . . . . . . . . . . . . . . . . . . . . . .
142
2.4.10
Heavy Symmetrical Top and Spherical Pendulum . . . . . .
144
2.4.11
Canonical Transformation of Time-Dependent
Oscillators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
2.4.12
Summary: Hamiltonian Mechanics . . . . . . . . . . . . . . . . .
152
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
Suggestions for Textbooks and Further Reading . . . . . . . . . . . . . . . . . .
162
3
Electromagnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
3.1
Electrostatics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
3.1.1
Overview of Electromagnetism . . . . . . . . . . . . . . . . . . . .
163
3.1.2
Coulomb’s Law—Far or Near Action? . . . . . . . . . . . . . .
165
3.1.3
Electrostatic Potential . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
3.1.4
Dipoles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
3.1.5
Polarization and Displacement Field . . . . . . . . . . . . . . . .
174
3.1.6
Field Equations in Electrostatics . . . . . . . . . . . . . . . . . . .
176
3.1.7
Problems in Electrostatics . . . . . . . . . . . . . . . . . . . . . . . .
178
3.1.8
Energy of the Electrostatic Field . . . . . . . . . . . . . . . . . . .
181
3.1.9
Maxwell Stress Tensor in Electrostatics . . . . . . . . . . . . . .
182
3.1.10
Summary: Electrostatics . . . . . . . . . . . . . . . . . . . . . . . . .
185
3.2
Stationary Currents and Magnetostatics . . . . . . . . . . . . . . . . . . . .
186
3.2.1
Electric Current . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
3.2.2
Ohm’s Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
3.2.3
Lorentz Force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
Contents
xvii

3.2.4
Magnetic Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
3.2.5
Magnetization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
3.2.6
Magnetic Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
3.2.7
Basic Equations of Macroscopic Magnetostatics
with Stationary Currents . . . . . . . . . . . . . . . . . . . . . . . . .
195
3.2.8
Vector Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
3.2.9
Magnetic Interaction. . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
3.2.10
Inductance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
3.2.11
Summary: Stationary Currents and Magnetostatics . . . . . .
203
3.3
Electromagnetic Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
3.3.1
Charge Conservation and Maxwell’s Displacement
Current . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
3.3.2
Faraday Induction Law and Lenz’s Rule . . . . . . . . . . . . .
205
3.3.3
Maxwell’s Equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
3.3.4
Time-Dependent Potentials . . . . . . . . . . . . . . . . . . . . . . .
208
3.3.5
Poynting’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
3.3.6
Oscillating Circuits. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
3.3.7
Momentum of the Radiation Field. . . . . . . . . . . . . . . . . .
214
3.3.8
Propagation of Waves in Insulators . . . . . . . . . . . . . . . . .
215
3.3.9
Reﬂection and Diffraction at a Plane . . . . . . . . . . . . . . . .
220
3.3.10
Propagation of Waves in Conductors. . . . . . . . . . . . . . . .
224
3.3.11
Summary: Maxwell’s Equations . . . . . . . . . . . . . . . . . . .
226
3.4
Lorentz Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
3.4.1
Velocity of Light in Vacuum . . . . . . . . . . . . . . . . . . . . .
227
3.4.2
Lorentz Transformation . . . . . . . . . . . . . . . . . . . . . . . . .
228
3.4.3
Four-Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
3.4.4
Examples of Four-Vectors . . . . . . . . . . . . . . . . . . . . . . .
234
3.4.5
Conservation Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
3.4.6
Covariance of the Microscopic Maxwell Equations . . . . .
239
3.4.7
Covariance of the Macroscopic Maxwell Equations . . . . .
241
3.4.8
Transformation Behavior of Electromagnetic Fields . . . . .
243
3.4.9
Relativistic Dynamics of Free Particles . . . . . . . . . . . . . .
244
3.4.10
Relativistic Dynamics with External Forces . . . . . . . . . . .
247
3.4.11
Energy–Momentum Stress Tensor . . . . . . . . . . . . . . . . . .
248
3.4.12
Summary: Lorentz Invariance . . . . . . . . . . . . . . . . . . . . .
249
3.4.13
Supplement: Hamiltonian Formalism for Fields . . . . . . . .
250
3.5
Radiation Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
3.5.1
Solutions of the Inhomogeneous Wave Equations . . . . . .
253
3.5.2
Radiation Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
3.5.3
Radiation Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
3.5.4
Radiation Fields of Point Charges . . . . . . . . . . . . . . . . . .
260
3.5.5
Radiation Fields of Oscillating Dipoles . . . . . . . . . . . . . .
261
3.5.6
Radiation Power for Dipole, Braking, and Synchrotron
Radiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
xviii
Contents

3.5.7
Summary: Radiation Fields . . . . . . . . . . . . . . . . . . . . . . .
266
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Suggestions for Textbooks and Further Reading . . . . . . . . . . . . . . . . . .
274
4
Quantum Mechanics I. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
4.1
Wave–Particle Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
4.1.1
Heisenberg’s Uncertainty Relations . . . . . . . . . . . . . . . . .
275
4.1.2
Wave–Particle Dualism . . . . . . . . . . . . . . . . . . . . . . . . .
276
4.1.3
Probability Waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
4.1.4
Pure States and Their Superposition (Superposition
Principle) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
4.1.5
Hilbert Space (Four Axioms) . . . . . . . . . . . . . . . . . . . . .
282
4.1.6
Representation of Hilbert Space Vectors . . . . . . . . . . . . .
284
4.1.7
Improper Hilbert Vectors . . . . . . . . . . . . . . . . . . . . . . . .
287
4.1.8
Summary: Wave–Particle Dualism . . . . . . . . . . . . . . . . .
287
4.2
Operators and Observables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
4.2.1
Linear and Anti-linear Operators . . . . . . . . . . . . . . . . . . .
288
4.2.2
Matrix Elements and Representation of Linear
Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
4.2.3
Associated Operators . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
4.2.4
Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . .
294
4.2.5
Expansion in Terms of a Basis of Orthogonal
Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
4.2.6
Observables. Basic Assumptions . . . . . . . . . . . . . . . . . . .
298
4.2.7
Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
4.2.8
Field Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
301
4.2.9
Phase Operators and Wave–Particle Dualism . . . . . . . . . .
304
4.2.10
Doublets and Pauli Operators . . . . . . . . . . . . . . . . . . . . .
308
4.2.11
Density Operator. Pure States and Mixtures . . . . . . . . . . .
311
4.2.12
Space Inversion and Time Reversal . . . . . . . . . . . . . . . . .
313
4.2.13
Summary: Operators and Observables . . . . . . . . . . . . . . .
315
4.3
Correspondence Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
4.3.1
Commutation Relations . . . . . . . . . . . . . . . . . . . . . . . . .
315
4.3.2
Position and Momentum Representations. . . . . . . . . . . . .
317
4.3.3
The Probability Amplitude hr j Pi . . . . . . . . . . . . . . . . . .
318
4.3.4
Wave Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
320
4.3.5
Wigner Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321
4.3.6
Spin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
4.3.7
Correspondence Principle . . . . . . . . . . . . . . . . . . . . . . . .
325
Contents
xix

4.3.8
Angular Momentum Operator . . . . . . . . . . . . . . . . . . . . .
328
4.3.9
Spherical Harmonics . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
4.3.10
Coupling of Angular Momenta . . . . . . . . . . . . . . . . . . . .
335
4.3.11
Summary: Correspondence Principle . . . . . . . . . . . . . . . .
337
4.4
Time Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
4.4.1
Heisenberg Equation and the Ehrenfest Theorem . . . . . . .
338
4.4.2
Time Dependence: Heisenberg and Schrödinger
Pictures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
4.4.3
Time Dependence of the Density Operator . . . . . . . . . . .
342
4.4.4
Time-Dependent Interaction and Dirac Picture . . . . . . . . .
345
4.4.5
Current Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
4.4.6
Summary: Time Dependence . . . . . . . . . . . . . . . . . . . . .
350
4.5
Time-Independent Schrödinger Equation . . . . . . . . . . . . . . . . . . .
351
4.5.1
Eigenvalue Equation for the Energy . . . . . . . . . . . . . . . .
351
4.5.2
Reduction to Ordinary Differential Equations . . . . . . . . . .
352
4.5.3
Free Particles and the Box Potential . . . . . . . . . . . . . . . .
353
4.5.4
Harmonic Oscillations . . . . . . . . . . . . . . . . . . . . . . . . . .
358
4.5.5
Hydrogen Atom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
4.5.6
Time-Independent Perturbation Theory . . . . . . . . . . . . . .
368
4.5.7
Variational Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
370
4.5.8
Level Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
4.5.9
Summary: Time-Independent Schrödinger Equation . . . . .
373
4.6
Dissipation and Quantum Theory . . . . . . . . . . . . . . . . . . . . . . . .
374
4.6.1
Perturbation Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . .
374
4.6.2
Coupling to the Environment . . . . . . . . . . . . . . . . . . . . .
377
4.6.3
Markov Approximation . . . . . . . . . . . . . . . . . . . . . . . . .
379
4.6.4
Deriving the Rate Equation and Fermi’s Golden Rule . . .
382
4.6.5
Rate Equation for Degeneracy. Transitions Between
Multiplets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
4.6.6
Damped Linear Harmonic Oscillations . . . . . . . . . . . . . .
386
4.6.7
Summary: Dissipation and Quantum Theory . . . . . . . . . .
389
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
389
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
396
Suggestions for Textbooks and Further Reading . . . . . . . . . . . . . . . . . .
397
5
Quantum Mechanics II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
5.1
Scattering Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
5.1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
5.1.2
Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
5.1.3
Time Shift Operators in Perturbation Theory . . . . . . . . . .
403
5.1.4
Time-Dependent Green Functions (Propagators). . . . . . . .
405
5.1.5
Energy-Dependent Green Functions (Propagators)
and Resolvents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
xx
Contents

5.1.6
Representations of the Resolvents
and the Interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408
5.1.7
Lippmann–Schwinger Equations . . . . . . . . . . . . . . . . . . .
411
5.1.8
Möller’s Wave Operators . . . . . . . . . . . . . . . . . . . . . . . .
413
5.1.9
Scattering and Transition Operators . . . . . . . . . . . . . . . . .
414
5.1.10
The Wave Function hr j k i þ for Large Distances r . . . . .
416
5.1.11
Scattering Cross-Section . . . . . . . . . . . . . . . . . . . . . . . . .
417
5.1.12
Summary: Scattering Theory . . . . . . . . . . . . . . . . . . . . . .
418
5.2
Two- and Three-Body Scattering Problems . . . . . . . . . . . . . . . . .
419
5.2.1
Two-Potential Formula of Gell-Mann
and Goldberger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
419
5.2.2
Scattering Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
421
5.2.3
Scattering of Charged Particles . . . . . . . . . . . . . . . . . . . .
422
5.2.4
Effective Hamilton Operator in the Feshbach Theory . . . .
423
5.2.5
Separable Interactions and Resonances . . . . . . . . . . . . . .
425
5.2.6
Breit–Wigner Formula . . . . . . . . . . . . . . . . . . . . . . . . . .
426
5.2.7
Averaging over the Energy . . . . . . . . . . . . . . . . . . . . . . .
427
5.2.8
Special Features of Three-Body Problems . . . . . . . . . . . .
429
5.2.9
The Method of Kazaks and Greider . . . . . . . . . . . . . . . .
430
5.2.10
Faddeev Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
432
5.2.11
Summary: Two- and Three-Body Scattering
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433
5.3
Many-Body Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433
5.3.1
One- and Many-Body States . . . . . . . . . . . . . . . . . . . . . .
433
5.3.2
Exchange Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . .
434
5.3.3
Symmetric and Antisymmetric States . . . . . . . . . . . . . . .
436
5.3.4
Creation and Annihilation Operators for Fermions . . . . . .
438
5.3.5
Creation and Annihilation Operators for Bosons . . . . . . .
440
5.3.6
General Properties of Creation and Annihilation
Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
442
5.3.7
The Two-Body System as an Example . . . . . . . . . . . . . .
443
5.3.8
Representation of One-Particle Operators . . . . . . . . . . . . .
445
5.3.9
Representation of Two-Body Operators . . . . . . . . . . . . . .
446
5.3.10
Time Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
448
5.3.11
Wave–Particle Dualism . . . . . . . . . . . . . . . . . . . . . . . . .
450
5.3.12
Summary: Many-Body Systems . . . . . . . . . . . . . . . . . . .
451
5.4
Fermions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
451
5.4.1
Fermi Gas in the Ground State . . . . . . . . . . . . . . . . . . . .
451
5.4.2
Hartree–Fock Equations . . . . . . . . . . . . . . . . . . . . . . . . .
454
5.4.3
Rest Interaction and Pair Force . . . . . . . . . . . . . . . . . . . .
456
5.4.4
Quasi-Particles in the BCS Formalism . . . . . . . . . . . . . . .
457
Contents
xxi

5.4.5
Hartree–Fock–Bogoliubov Equations . . . . . . . . . . . . . . . .
458
5.4.6
Hole States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
462
5.4.7
Summary: Fermions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
462
5.5
Photons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
463
5.5.1
Preparation for the Quantization of Electromagnetic
Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
463
5.5.2
Quantization of Photons . . . . . . . . . . . . . . . . . . . . . . . . .
466
5.5.3
Glauber States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
470
5.5.4
Quenched States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
473
5.5.5
Expansion in Terms of Glauber States . . . . . . . . . . . . . . .
476
5.5.6
Density Operator in the Glauber Basis . . . . . . . . . . . . . .
479
5.5.7
Atom in a Light Field . . . . . . . . . . . . . . . . . . . . . . . . . .
482
5.5.8
Summary: Photons . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
487
5.6
Dirac Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
487
5.6.1
Relativistic Invariance . . . . . . . . . . . . . . . . . . . . . . . . . .
487
5.6.2
Quantum Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
488
5.6.3
Dirac Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
490
5.6.4
Representations of the Dirac Matrices . . . . . . . . . . . . . . .
492
5.6.5
Behavior of the Dirac Equation Under Lorentz
Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
494
5.6.6
Adjoint Spinors and Bilinear Covariants . . . . . . . . . . . . .
497
5.6.7
Space Inversion, Time Reversal, and Charge
Conjugation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
498
5.6.8
Dirac Equation and Klein–Gordon Equation . . . . . . . . . .
501
5.6.9
Energy Determination for Special Potentials . . . . . . . . . .
504
5.6.10
Difﬁculties with the Dirac Theory . . . . . . . . . . . . . . . . . .
509
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
509
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
510
Suggestions for Textbooks and Further Reading . . . . . . . . . . . . . . . . . .
511
6
Thermodynamics and Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
6.1
Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
6.1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
6.1.2
Statistical Ensembles and the Notion of Probability . . . . .
515
6.1.3
Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . .
516
6.1.4
Gauss and Poisson Distributions . . . . . . . . . . . . . . . . . . .
518
6.1.5
Correlations and Partial Systems . . . . . . . . . . . . . . . . . . .
520
6.1.6
Information Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . .
521
6.1.7
Classical Statistics and Phase Space Cells . . . . . . . . . . . .
523
6.1.8
Summary: Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
6.2
Entropy Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
6.2.1
Entropy Law and Rate Equation . . . . . . . . . . . . . . . . . . .
525
6.2.2
Irreversible Changes of State and Relaxation-Time
Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
527
xxii
Contents

6.2.3
Liouville and Collision-Free Boltzmann Equation . . . . . .
529
6.2.4
Boltzmann Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
6.2.5
Proof of the Entropy Law Using the Boltzmann
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
533
6.2.6
Molecular Motion and Diffusion . . . . . . . . . . . . . . . . . . .
534
6.2.7
Langevin Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
537
6.2.8
Generalized Langevin Equation and the
Fluctuation–Dissipation Theorem . . . . . . . . . . . . . . . . . .
539
6.2.9
Fokker–Planck Equation . . . . . . . . . . . . . . . . . . . . . . . . .
542
6.2.10
Summary: Entropy Law . . . . . . . . . . . . . . . . . . . . . . . . .
546
6.3
Equilibrium Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
546
6.3.1
Maxwell Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . .
546
6.3.2
Thermal Equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . .
548
6.3.3
Micro-canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . .
549
6.3.4
Density of States in the Single-Particle Model . . . . . . . . .
550
6.3.5
Mean Values and Entropy Maximum . . . . . . . . . . . . . . .
552
6.3.6
Canonical and Grand Canonical Ensembles . . . . . . . . . . .
554
6.3.7
Exchange Equilibria . . . . . . . . . . . . . . . . . . . . . . . . . . . .
557
6.3.8
Temperature, Pressure, and Chemical Potential . . . . . . . .
558
6.3.9
Summary: Equilibrium Distributions . . . . . . . . . . . . . . . .
561
6.4
General Theorems of Thermodynamics . . . . . . . . . . . . . . . . . . . .
561
6.4.1
The Basic Relation of Thermodynamics . . . . . . . . . . . . .
561
6.4.2
Mechanical Work and Heat . . . . . . . . . . . . . . . . . . . . . .
563
6.4.3
State Variables and Complete Differentials . . . . . . . . . . .
565
6.4.4
Thermodynamical Potentials and Legendre
Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
566
6.4.5
Maxwell’s Integrability Conditions and Thermal
Coefﬁcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
6.4.6
Homogeneous Systems and the Gibbs–Duhem
Relation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
571
6.4.7
Phase Transitions and the Clausius–Clapeyron
Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
572
6.4.8
Enthalpy and Free Energy as State Variables . . . . . . . . . .
574
6.4.9
Irreversible Alterations . . . . . . . . . . . . . . . . . . . . . . . . . .
576
6.4.10
Summary: General Theorems of Thermodynamics . . . . . .
576
6.5
Results for the Single-Particle Model . . . . . . . . . . . . . . . . . . . . . .
577
6.5.1
Identical Particles and Symmetry Conditions . . . . . . . . . .
577
6.5.2
Partition Functions in Quantum Statistics. . . . . . . . . . . . .
578
6.5.3
Occupation of One-Particle States . . . . . . . . . . . . . . . . . .
580
6.5.4
Ideal Gases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
582
6.5.5
Mixing Entropy and the Law of Mass Action . . . . . . . . .
586
6.5.6
Degenerate Fermi Gas and Conduction Electrons
in Metals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
588
Contents
xxiii

6.5.7
Electromagnetic Radiation in a Cavity . . . . . . . . . . . . . . .
594
6.5.8
Lattice Vibrations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
596
6.5.9
Summary: Results for the Single-Particle Model . . . . . . .
599
6.6
Phase Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
599
6.6.1
Van der Waals Equation . . . . . . . . . . . . . . . . . . . . . . . . .
599
6.6.2
Conclusions Regarding the van der Waals Equation . . . . .
601
6.6.3
Critical Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
603
6.6.4
Paramagnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
605
6.6.5
Ferromagnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
607
6.6.6
Bose–Einstein Condensation . . . . . . . . . . . . . . . . . . . . . .
608
6.6.7
Summary: Phase Transitions . . . . . . . . . . . . . . . . . . . . . .
611
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
612
List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
618
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
620
Suggestions for Textbooks and Further Reading . . . . . . . . . . . . . . . . . .
620
Appendix A: Important Constants. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
623
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
625
xxiv
Contents

Chapter 1
Basics of Experience
1.1
Vector Analysis
1.1.1
Space and Time
Space and time are two basic concepts which, according to Kant, inherently or
innately determine the form of all experience in an a priori manner, thereby making
possible experience as such: only in space and time can we arrange our sensations.
[According to the doctrines of evolutionary cognition, what is innate to us has devel-
oped phylogenetically by adaption to our environment. This is why we only notice
the insufﬁciency of these “self-evident” concepts under extraordinary circumstances,
e.g., for velocities close to that of light (c0) or actions of the order of Planck’s quan-
tum h. We shall tackle such “weird” cases later—in electromagnetism and quantum
mechanics. For the time being, we want to make sure we can handle our familiar
environment.]
To do this, we introduce a continuous parameter t. Like every other physical
quantity it is composed of number and unit (for example, a second 1 s = 1 min/60
= 1 h/3600). The larger the unit, the smaller the number. Physical quantities do not
depend on the unit—likewise equations between physical quantities. Nevertheless,
the opposite is sometimes seen, as in: “We choose units such that the velocity of light
c assumes the value 1”. In fact, the concept of velocity is thereby changed, so that
instead of the velocity v, the ratio v/c is taken here as the velocity, and ct as time or
x/c as length.
The zero time (t = 0) can be chosen arbitrarily, since basically only the time
difference,i.e.,thedurationofaprocess,isimportant.Adifferentiationwithrespectto
time (d/dt) is often marked by a dot over the differentiated quantity, i.e., dx/dt ≡˙x.
In empty space every direction is equivalent. Here, too, we may choose the zero
point freely and, starting from this point, determine the position of other points in
a coordinate-free notation by the position vector r, which ﬁxes the distance and
direction of the point under consideration. This coordinate-free type of notation is
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5_1
1

2
1
Basics of Experience
particularly advantageous when we want to exploit the assumed homogeneity of
space. However, conditions often arise (i.e., when there is axial or spherical sym-
metry) which are best taken care of in special coordinates. We are free to choose a
coordinate system. We only require that it determine all positions uniquely. This we
shall treat in the next section.
Besides the position vector r, there are other quantities in physics with both
value and direction, e.g., the velocity v = ˙r, the acceleration a = ˙v, the momentum
p = mv, and the force F = ˙p. The appropriate means to handle such quantities is
vector algebra, with which we shall be extensively concerned in this section. This
method allows us to encompass both the value and the direction of the quantities
under consideration much better than using components, which, moreover, depend
on the coordinate system.
For the time being—namely for plane and three-dimensional problems—we
understand a vector as a quantity with value and direction, which can be repre-
sented as an arrow of corresponding length. (Generally, vectors are mathematical
entities, which can be added together or multiplied by a number, with the usual rules
of calculation being valid.) Sometimes they are denoted by a letter with an arrow
atop. The value (the length) of a is denoted by a or |a |.
1.1.2
Vector Algebra
From two vectors a and b, their sum a + b may be formed according to the con-
struction of parallelograms (as the diagonal), as shown in Fig. 1.1. From this follows
the commutative and associative law of vector addition:
a + b = b + a ,
(a + b) + c = a + (b + c) .
The product of the vectors a with a scalar (i.e., directionless) factor α is understood
as the vector α a = a α with the same (for α < 0 opposite) direction and with value
|α| a. In particular, a and −a have the same value, but opposite directions. For α = 0
the zero vector 0 results, with length 0 and undetermined direction.
Fig. 1.1 Sum and difference of vectors a and b. The vectors may be shifted in parallel, e.g., a−b
can also lie on the dashed straight line

1.1 Vector Analysis
3
Fig. 1.2 Scalar and vector products: e · a is the component of a in the direction of the unit vector
e, and | a × b | is the area shown
The scalar product (inner product) a · b of the two vectors a and b is the product
of their values times the cosine of the enclosed angle φab (see Fig. 1.2 left):
a · b ≡a b cos φab .
The dot between the two factors is important for the scalar product—if it is missing,
thenitisthetensorproduct ofthetwovectors,whichwillbeexplainedinSect.1.2.4—
with a · b c ̸= a b · c, if a and c have different directions, i.e., if a is not a multiple
of c . Consequently, one has
a · b = b · a
and
a · b = 0
⇐⇒
a ⊥b or a = 0 or b = 0 .
If the two vectors are oriented perpendicularly to each other (a ⊥b), then they are
also said to be orthogonal. Obviously, a · a = a2 holds. Vectors with value 1 are
called unit vectors. Here they are denoted by e. Given three Cartesian, i.e., pairwise
perpendicular unit vectors ex, ey, ez, all vectors can be decomposed in terms of these:
a = ex ax + ey ay + ez az ,
with the Cartesian components
ax ≡ex · a ,
ay ≡ey · a ,
az ≡ez · a .
Here the components will usually be written after the unit vectors. This is particularly
useful in quantum mechanics, but also meaningful otherwise, since the coefﬁcients
depend on the expansion basis. Since for a given basis a is ﬁxed by its three compo-
nents (ax, ay, az), a is thus often given as this row vector, or as a column vector, with
the components written one below the other. However, the coordinate-free notation
a is in most cases more appropriate to formal calculations, e.g., a + b combines the
three expressions ax + bx, ay + by, and az + bz. Because ex · ex = 1, ex · ey = 0
(and cyclic permutations ey · ey = 1, ey · ez = 0 and so on), one clearly has
a · b = ax bx + ay by + az bz .
Hence it also follows that a · (b + c) = a · b + a · c.

4
1
Basics of Experience
The vector product (outer product) a × b of the two vectors a and b is another
vector which is oriented perpendicularly to both and which forms with them a right-
hand screw, like the thumb, foreﬁnger, and middle ﬁnger of the right hand. Its value
is equal to the area of the parallelogram spanned by a and b (see Fig. 1.2 right):
|a × b | = a b sin φab .
Hence it also follows that
a × b = −b × a ,
a × (b + c) = a × b + a × c ,
and
a × b = 0
⇐⇒
a ∥b or a = 0 or b = 0 .
Using a right-handed Cartesian coordinate system, we have
ex × ey = ez
(and cyclic permutations ey × ez = ex, . . .) ,
and also ex × ex = 0, etc., whence
a × b = ex (ay bz −az by) + ey (az bx −ax bz) + ez (ax by −ay bx) .
This implies
a × (b × c) = (c × b) × a = b c · a −c a · b .
(This decomposition also follows without calculation because the product depends
linearly upon its three factors, lies in the plane spanned by b and c, vanishes for
b ∝c, and points in the direction of b for c = a ⊥b.) According to the last equation,
every vector a can be decomposed into its component along a unit vector e and its
component perpendicular to it:
a = e e · a −e × (e × a) .
In addition, it satisﬁes the Jacobi identity (note the cyclic permutation)
a × (b × c) + b × (c × a) + c × (a × b) = 0 .
The scalar product of a vector with a vector product, viz.,
a · (b × c) = b · (c × a) = c · (a × b) ,
is called the (scalar) triple product of the three vectors. It is positive or negative, if
a, b, and c form a right- or left-handed triad, respectively. Its value gives the volume
of the parallelepiped with edges a, b, and c. In particular, ex · (ey × ez) = 1.

1.1 Vector Analysis
5
In this context, the concept of a matrix is useful. An M × N matrix A is understood
as an entity made of M × N “matrix elements”, arranged in M rows and N columns:
Aik (i ∈{1, . . . , M}, k ∈{1, . . . , N}), e.g.,
A =
⎛
⎝
A11 A12 A13
A21 A22 A23
A31 A32 A33
⎞
⎠
⇐⇒
A =
⎛
⎝
A11 A21 A31
A12 A22 A32
A13 A23 A33
⎞
⎠.
The transposed matrix A just introduced has elements Aik = Aki, hence N rows and
M columns. We shall mainly be concerned with square matrices, which have equal
numbers of rows and columns, i.e., M = N. The matrix product of A and B is
C = AB
with Cik =
N

j=1
Ai j B jk ,
which is, of course, deﬁned only if the number of columns of A is the same as the
number of rows of B. We have 
AB = B A.
If we now combine the 3×3 Cartesian components of the vectors a, b, and c in
the form of a matrix, its determinant
						
ax ay az
bx by bz
cx cy cz
						
≡ax (bycz −bzcy) + ay (bzcx −bxcz) + az (bxcy −bycx)
= ax (bycz −bzcy) + bx (cyaz −czay) + cx (aybz −azby)
is equal to the triple product a · (b × c). For determinants, we have
det A = det A
and
det (AB) = det A × det B .
Therefore, also
a · (b × c) f · (g × h) =
						
a · f a · g a · h
b · f b · g b · h
c · f c · g c · h
						
.
Moreover, from (a × b) · c = a · (b × c) and replacing c by c × d, it follows that
(a × b) · (c × d) = (a · c)(b · d) −(a · d)(b · c) ≡
				
a · c a · d
b · c b · d
				 ,
the determinant of a 2×2 matrix, and in particular,
(a × b) · (a × b) = a2b2 −(a · b)2 ,
which, of course, follows from sin2φab = 1 −cos2φab.

6
1
Basics of Experience
Table 1.1 Space-inversion
behavior
Type
Original image
Mirror image
Polar vector
↑
↓
Axial vector
−↑−
−↑−
It is not allowed to divide by vectors—neither scalar products nor vector products
can be decomposed uniquely in terms of their factors, as can be seen from the
examples a · b = 0 and a × b = 0.
In the context of the vector product, we have to consider the fact that only in
three-dimensional space can a third vector be assigned uniquely as a vector normal
to two vectors. Otherwise a perpendicular direction cannot be ﬁxed uniquely, and
no direction can be given in the sense of the right-hand rule. In fact, in Sect. 3.4.3,
in order to extend the three-dimensional space to the four-dimensional space-time
continuum of the theory of special relativity, we change from the vector product to
a skew-symmetric matrix (or a tensor of second rank) which, in three-dimensional
space, has three independent elements, just like every vector.
Actually, we also have to distinguish between polar vectors (like the position
vector r and the velocity v = ˙r) and axial vectors (e.g., the vector product of two
polar vectors), because they behave differently under a space inversion (with respect
to the origin): the direction of a polar vector is reversed, while the direction of an
axial vector is preserved. Correspondingly the triple product of three polar vectors is
a pseudo-scalar, because it changes its sign under space inversion. Axial vectors can
actually be viewed as rotation axes with sense of rotation and not as arrows—they
are pseudo-vectors (Table 1.1).
Inversion involves a special change of coordinates: it cannot be composed of
inﬁnitesimal transformations, like rotations and translations. General properties of
coordinate transformations will be treated in the next section. Until then we will
thus assume only right-handed Cartesian coordinate systems with ex × ey = ez (and
cyclic permutations).
1.1.3
Trajectories
If a vector depends upon a parameter, then we speak of a vector function. The vector
function a (t) is continuous at t0, if it tends to a (t0) for t →t0. With the same limit
t →t0, the vector differential da and the ﬁrst derivative da/dt is introduced. These
quantities may be formed for every Cartesian component, and we have
d(a + b) = da + db ,
d(αa) = α da + a dα ,
d(a · b) = a · db + b · da ,
d(a × b) = a × db −b × da .
Obviously, a · da/dt = 1
2d(a · a)/dt = 1
2da2/dt = a da/dt holds. In particular the
derivative of a unit vector is always perpendicular to the original vector—if it does
not vanish.

1.1 Vector Analysis
7
As an example of a vector function, we investigate r (t), the path of a point as
a function of the time t. Thus we want to consider also the velocity v = ˙r and the
acceleration a = ¨r rather generally. The time is not important for the trajectories as
geometrical lines. Therefore, instead of the time t we introduce the path length s as
a parameter and exploit ds = |dr | = v dt.
We now take three mutually perpendicular unit vectors eT, eN, and eB, which are
attached to every point on the trajectory. Here eT has the direction of v:
tangent vector
eT ≡dr
ds = v
v .
For a straight path, this vector is already sufﬁcient for the description. But in general
the
path curvature
κ ≡
				
deT
ds
				 =
				
d2r
ds2
				
is different from zero. In order to get more insight into this parameter we consider a
plane curve of constant curvature, namely, the circle with s = R ϕ. For r (ϕ) = r0 +
R (cos ϕ ex + sin ϕ ey), we have κ = |d2r/d(Rϕ)2| = R−1. Instead of the curvature
κ, its reciprocal, the
curvature radius
R ≡1
κ ,
can also be used to determine the curve. Hence as a further unit vector we have the
normal vector
eN ≡R deT
ds = R d2r
ds2 .
SinceithasthedirectionofthederivativeoftheunitvectoreT,itisperpendiculartoeT.
Now we may express the velocity and the accelerations because ˙eT = (deT/ds) v =
(v/R) eN as follows:
v ≡˙r = v eT ,
a ≡¨r = ˙v eT + v2
R eN .
Thus there is a tangential acceleration a · eT ≡aT = ˙v, if the value of the veloc-
ity changes, and a normal acceleration a · eN ≡aN = v2/R, if the direction of the
velocity changes. From this decomposition we can also see why motions are often
investigated either along a straight line or along a uniformly traveled circle—then
only aT or only aN appears.
If the curve leaves the plane spanned by eT and eN, then the
binormal vector
eB ≡eT × eN
also changes with s. Because deT/ds = κeN, its derivative with respect to s is equal
to eT × deN/ds. This expression (perpendicular to eT) must be proportional to eN,

8
1
Basics of Experience
because derivatives of unit vectors do not have components in their direction. Since
eN = eB × eT, besides
deT
ds = κ eN ,
the derivatives
deB
ds = −τ eN
and
deN
ds = τ eB −κ eT
appear with the torsion τ, also called the winding or second curvature. For a right-
hand thread, one has τ > 0, and for a left-hand thread, τ < 0. The relation
τ = R2 
dr
ds × d2r
ds2

· d3r
ds3
also holds, because of τ = eB · (deN/ds) and eB = eT × eN. (Here it is unimportant
for the winding whether the curvature depends upon s.)
With the Darboux vector
δ = κ eB + τ eT ,
the expressions just obtained for the derivatives of the three unit vectors with respect
to the curve length s (Frenet–Serret formulas) can be combined to yield
de•
ds = δ × e•
with e• ∈{eT, eN, eB} .
As long as neither the ﬁrst nor the second curvature changes along the curve, the Dar-
boux vector is constant: dκ/ds = 0 = dτ/ds =⇒dδ/ds = 0, because κ deB/ds =
−τ deT/ds. The curve winds around it. An example will follow in Sect. 2.2.5,
namely the spiral curve of a charged particle in a homogeneous magnetic ﬁeld:
in this case the Darboux vector is δ = −qB/(mv). The curves with constant δ thus
depend upon the initial velocity v0. Among these are also circular orbits (perpen-
dicular to δ) and straight lines (along ±δ), where admittedly a straight line has
vanishing curvature (κ = 0 ), and the concept of the second curvature (winding)
thus has no meaning. The quantities δ and v0 yield the winding τ = δ · v0/v0 and
curvature κ (≥0) because of δ2 = κ2 + τ 2. The radius h and the helix angle α
(with |α| ≤1
2π) of the associated thread follow from h = κ/δ2 and α = arctan τ/κ.
[With r = r0 + h (cos ϕ ex + sin ϕ ey + tan α ϕ ez) and s cos α = h ϕ and because
of tan α = τ/κ, the scalar triple product expression for τ yields the equation
cos2 α = h/R.] The geometrical meaning of the curvature radius R and radius h
is thus the reciprocal of the length of the Darboux vector (see Fig. 1.3).
If the curve traveled is given by the functions y(x) and z(x) in Cartesian coordi-
nates, then we have
d2r
ds2 = d
ds

dr
ds

= d
dx

 dr
dx
dx
ds
 dx
ds ,
and because ds2 = dx2 + dy2 + dz2, we also have dx/ds = 1/

1 + y′ 2 + z′ 2 with
y′ ≡dy/dx and z′ ≡dz/dx. Hence, the square of the path curvature is given by

1.1 Vector Analysis
9
Fig. 1.3 Spiral curve around the constant Darboux vector δ oriented to the right (constant curvature
and winding, here with κ = τ). Shown are also the tangent and binormal vectors of the moving
frame and the tangential circle. Not shown is the normal vector eN = eB × eT, which points toward
the symmetry axis
κ2 = (y′z′′ −y′′z′)2 + y′′ 2 + z′′ 2
(1 + y′ 2 + z′ 2)3
and the torsion by
τ =
y′′z′′′ −y′′′z′′
(1 + y′ 2 + z′ 2)3
1
κ2 .
For the curvature, we have κ ≥0, while τ is negative for a left-hand thread.
1.1.4
Vector Fields
If a vector is associated with each position, we speak of a vector ﬁeld. With scalar
ﬁelds, a scalar is associated with each position. The vector ﬁeld a (r) is only contin-
uous at r0 if all paths approaching r0 have the same limit. For scalar ﬁelds, this is
already an essentially stronger requirement than in one dimension.
Instead of drawing a vector ﬁeld with arrows at many positions, it is often visu-
alized by a set of ﬁeld lines: at every point of a ﬁeld line the tangent points in the
direction of the vector ﬁeld. Thus a ∥dr and a × dr = 0.
For a given vector ﬁeld many integrals can be formed. In particular, we often
have to evaluate integrals over surfaces or volumes. In order to avoid double or triple
integral symbols, the corresponding differential is often written immediately after
the integral symbol: dV for the volume, df for the surface integral, e.g.,

df × a
instead of −

a × df (in this way the unnecessary minus sign is avoided for the
introduction of the curl density or rotation on p. 13). Here df is perpendicular to the
related surface element. However, the sign of df still has to be ﬁxed. In general, we
consider the surface of a volume V , which will be denoted here by (V ). Then df
points outwards. Corresponding to (V ), the edge of an area A is denoted by (A).
An important example of a scalar integral is the line integral

dr · a (r) along
a given curve r (t). If the parameter t determines the points on the curve uniquely,
then the line integral

10
1
Basics of Experience

dr · a (r) =

dt dr
dt · a (r (t))
is an ordinary integral over the scalar product a · dr/dt. Another example of a scalar
integral is the surface integral

df · a (r) taken over a given area A or over the surface
(V ) of the volume V .
Besidesthescalarintegrals,vectorialintegralslike

dV a,

df × a,and

dr × a
can arise, e.g., the x-component of

dV a is the simple integral

dV ax.
Different forms are also reasonable through differentiation: vector ﬁelds can be
deduced from scalar ﬁelds, and scalar ﬁelds (but also vector ﬁelds and tensor ﬁelds)
from vector ﬁelds. These will now be considered one by one. Then the operator ∇
will always turn up. The symbol ∇, an upside-down 	, resembles an Ancient Greek
harp and hence is called nabla, after W. R. Hamilton (see 122).
1.1.5
Gradient (Slope Density)
The gradient of a scalar function ψ(r) is the vector ﬁeld
grad ψ ≡∇ψ ,
with ∇ψ · dr ≡dψ ≡ψ(r + dr) −ψ(r) .
This is clearly perpendicular to the area ψ = const. at every point and points in
the direction of dψ > 0 (see Fig. 1.4). The value of the vector ∇ψ is equal to the
derivative of the scalar function ψ(r) with respect to the line element in this direction.
In Cartesian coordinates, we thus have
∇ψ = ex
∂ψ
∂x + ey
∂ψ
∂y + ez
∂ψ
∂z =

ex
∂
∂x + ey
∂
∂y + ez
∂
∂z

ψ .
Fig. 1.4 Gradient ∇ψ of a scalar ﬁeld ψ(r) represented by arrows. Contour lines with constant ψ
are drawn as continuous red and ﬁeld lines (slope lines) of the gradient ﬁeld as dashed blue. In the
example considered here, both families of curves contain only hyperbolas (and their asymptotes)

1.1 Vector Analysis
11
Here ∂ψ/∂x is the partial derivative of ψ(x, y, z) with respect to x for constant
y and z. (If other quantities are kept ﬁxed instead, then special rules have to be
considered, something we shall deal with in Sect. 1.2.7.)
The gradient is also obtained as a limit of a vectorial integral:
∇ψ = lim
V →0
1
V

(V )
df ψ(r) .
If we take a cube with inﬁnitesimal edges dx, dy, and dz, we have on the right-hand
side as x-component (dx dy dz)−1{dy dz ψ(x + dx, y, z) −dy dz ψ(x, y, z)} =
∂ψ/∂x, and similarly for the remaining components. Hence, also

V
dV ∇ψ =

(V )
df ψ ,
because a ﬁnite volume can be divided into inﬁnitesimal volume elements, and for
continuous ψ, contributions from adjacent planes cancel in pairs. With this surface
integral the gradient can be determined even if ψ is not differentiable (singular) at
individual points—the surface integral depends only upon points in the neighbour-
hood of the singular point, where everything is continuous. (In Sect. 1.1.12, we shall
consider the example ψ = 1/r.)
Corresponding to dψ = (dr · ∇) ψ, we shall also write in the following
da = (dr · ∇) a = dx ∂a
∂x + dy ∂a
∂y + dz ∂a
∂z .
We also attribute a meaning to the operation ∇a, but notice that there is no scalar
product between ∇and a (rather it is the dyadic or tensor product, as shown in the
next section), but there is a scalar product between dr and ∇. Then for a Taylor
series, we may write
ψ(r + dr) = ψ(r) + (dr · ∇)ψ + 1
2 (dr · ∇)2ψ + · · · ,
where all derivatives are to be taken at the position r.
1.1.6
Divergence (Source Density)
While a vector ﬁeld has been derived from a scalar ﬁeld with the help of the gradient,
the divergence associates a scalar ﬁeld with a vector ﬁeld:
div a ≡∇· a ≡lim
V →0
1
V

(V )
df · a .

12
1
Basics of Experience
For the same cube as in the last section, the right-hand expression yields
1
dx dy dz [dy dz {ax(x+dx, y, z) −ax(x, y, z)}
+dz dx {ay(x, y+dy, z) −ay(x, y, z)}
+dx dy {az(x, y, z+dz) −az(x, y, z)}] = ∂ax
∂x + ∂ay
∂y + ∂az
∂z ,
as suggested by the notation ∇· a, i.e., a scalar product between the vector operator
∇and the vector a. With this we have also proven Gauss’s theorem

V
dV ∇· a =

(V )
df · a ,
since for any partition of the ﬁnite volume V into inﬁnitesimal ones and for a contin-
uous vector ﬁeld a, the contributions of adjacent planes cancel in pairs. The integrals
here may even enclose points at which a (r) is singular (see Fig. 1.5 left). We shall
discuss this in more detail in Sect. 1.1.12.
The integral

df · a over an area is called the ﬂux of the vector ﬁeld a (r) through
this area (even if a is not a current density). In this picture, the integral over the closed
area (V ) describes the source strength of the vector ﬁeld, i.e., how much more ﬂows
into V than out. The divergence is therefore to be understood as a source density.
A vector ﬁeld is said to be source-free if its divergence vanishes everywhere. (If the
source density is negative, then “drains” predominate.)
The concept of a ﬁeld-line tube is also useful (we discussed ﬁeld lines in
Sect. 1.1.4). Its walls are everywhere parallel to a (r). Therefore, there is no ﬂux
through the walls, and the ﬂux through the end faces is equal to the volume integral
of ∇· a. For a source-free vector ﬁeld (∇· a = 0), the ﬂux ﬂowing into the ﬁeld-line
tube through one end face emerges again from the other.
Fig. 1.5 Fields between coaxial walls. On the left and in the center, the walls are drawn as con-
tinuous lines and the ﬁeld lines as dashed lines. On the left, the ﬁeld is curl-free and has sources
on the walls, while in the center it is source-free and has curls on the wall, if in both cases the ﬁeld
strength |a| = a decays with increasing distance R from the axis as shown in the right-hand graph,
i.e., in such a way that aR is constant

1.1 Vector Analysis
13
1.1.7
Curl (Vortex Density)
The curl (rotation) of the vector ﬁeld a (r) is the vector ﬁeld
rot a ≡∇× a ≡lim
V →0
1
V

(V )
df × a .
For the above-mentioned cube with the edges dx, dy, dz, the x-component of the
right-hand expression is equal to
1
dx dy dz [+dz dx {az(x, y + dy, z) −az(x, y, z)}
−dx dy {ay(x, y, z + dz) −ay(x, y, z)}] = ∂az
∂y −∂ay
∂z .
With ∂i ≡1/∂xi, we thus have
∇× a = ex

∂az
∂y −∂ay
∂z

+ ey

∂ax
∂z −∂az
∂x

+ ez

∂ay
∂x −∂ax
∂y

≡
						
ex ey ez
∂x ∂y ∂z
ax ay az
						
,
which is the vector product of the operators ∇and a. This explains the notation
∇× a. Moreover, we have

V
dV ∇× a =

(V )
df × a
for all continuous vector ﬁelds, although they may become singular point-wise, and
even along lines, as will become apparent shortly.
An important result is Stokes’s theorem

A
df · (∇× a) =

(A)
dr · a ,
where df is taken in the rotational sense on the edge (A) and forms a right-hand screw.
The right-hand side is the rotation (curl) of a, that is, the line integral of a along
the edge of A. In order to get an insight into the theorem, consider an inﬁnitesimal
rectangle in the yz-plane. On the left, we have

A
df · (∇× a) =

dy dz

∂az
∂y −∂ay
∂z

,
and on the right

14
1
Basics of Experience

(A)
dr · a =

dy ay(x, y, z) −

dy ay(x, y, z + dz)
+

dz az(x, y + dy, z) −

dz az(x, y, z) .
The ﬁrst two integrals on the right-hand side together result in −

dy (∂ay/∂z) dz,
the last two in

dz (∂az/∂y) dy. This implies

(A)
dr · a =

dy dz

∂az
∂y −∂ay
∂z

.
The theorem holds thus for an inﬁnitesimal area. A ﬁnite area can be divided into
sufﬁciently small ones, where adjacent lines do not contribute, since the integration
paths from adjacent areas are opposite to each other.
According to Stokes’s theorem we may also set
eA · (∇× a) = lim
A→0
1
A

(A)
dr · a ,
where the unit vector eA is perpendicular to the area A and dr forms a right-hand
screw with eA. The curl density ∇× a can be introduced more pictorially with this
equation than with the one mentioned ﬁrst, and even for vector ﬁelds which are
singular along a line (perpendicular to the area). Therefore, the inner “conductor” in
Fig. 1.5 may even be an arbitrarily thin “wire”.
For ∇× a ̸= 0, the vector ﬁeld has a non-vanishing rotation, or vortex. If ∇× a
vanishes everywhere, then the ﬁeld is said to be curl-free (vortex-free).
1.1.8
Rewriting Products. Laplace Operator
Given various ﬁelds, the linear differential operators gradient, divergence, and rota-
tion assign other ﬁelds to them. They have the following properties:
∇(φ ψ) = φ ∇ψ + ψ ∇φ ,
∇· (ψ a) = ψ ∇· a + a · ∇ψ ,
∇× (ψ a) = ψ ∇× a −a × ∇ψ ,
∇· (a × b) = b · (∇× a) −a · (∇× b) ,
∇× (a × b) = (b · ∇) a −b (∇· a) −(a · ∇) b + a (∇· b) ,
∇(a · b) = (b · ∇) a + b × (∇× a) + (a · ∇) b + a × (∇× b) .
All these equations can be proven by decomposing into Cartesian coordinates and
using the product rule for derivatives. For the last three, however, it is better to refer

1.1 Vector Analysis
15
to Sect. 1.1.2 (and the product rule) and place ∇between the other two vectors, so
that this operator then acts only on the last factor (see Problem 3.1). Since
∇· r = 3 ,
∇× r = 0 ,
(a · ∇) r = a
(Problem 3.2), we ﬁnd in particular
∇· (ψ r) = 3ψ + r · ∇ψ ,
∇× (ψ r) = −r × ∇ψ ,
(a · ∇) ψr = a ψ + r (a · ∇ψ) ,
and
∇· (a × r) = r · (∇× a) ,
∇× (a × r) = 2a + (r · ∇) a −r (∇· a) ,
∇(a · r) = a + (r · ∇) a + r × (∇× a) .
These equations are generally applicable and save us lengthy calculations—we shall
use them often. Besides these, we also have
∇rn = n rn−2 r ,
not only for integer numbers n, but also for fractions. Furthermore, if ψ and a
have continuous derivatives with respect to their coordinates, then the order of the
derivatives may be interchanged, viz.,
∇× ∇ψ = 0
and
∇· (∇× a) = 0 .
Hence, gradient ﬁelds are curl-free (vortex-free), and curl ﬁelds are source-free.
Point-like singularities do not alter these results.
The operator 	 in the expression
	ψ ≡∇· ∇ψ
is called the Laplace operator. For a ﬁnal reformulation, we make use once again of
a result in Sect. 1.1.2, namely b · c a = c (b · a) −b × (c × a), whence
	 a ≡∇· ∇a = ∇(∇· a) −∇× (∇× a).
Therefore, this operator can act on scalars ψ(r) and vectors a (r). In Cartesian coor-
dinates it reads in both cases
	 = ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 .

16
1
Basics of Experience
According to Gauss’s theorem,

(V )
df · ∇ψ =

V
dV 	ψ ,
thus
	ψ = lim
V →0
1
V

(V )
df · ∇ψ .
The Laplace operator is thus to be understood as the limit of a surface integral. It
is apparently only different from zero if ∇ψ changes on the surface (V ). A further
important relation is
∇· (ψ∇φ −φ∇ψ) = ψ 	φ −φ 	ψ ,
which can be derived from the above equations.
According to Gauss’s theorem a source- and curl-free ﬁeld has to vanish every-
where, if it vanishes on the surface (“at inﬁnity”). Every curl-free vector ﬁeld can
be represented as a gradient ﬁeld ∇ψ, where ψ obeys the Laplace equation 	ψ =
0 everywhere, because the ﬁeld is also taken to be source-free. Hence, we have
∇· ψ∇ψ = ∇ψ · ∇ψ, according to Gauss’s theorem

(V ) df · ψ∇ψ =

V dV ∇ψ ·
∇ψ. The left-hand side has to be zero, and on the right the integrand is nowhere
negative, whence it has to vanish everywhere.
1.1.9
Integral Theorems for Vector Expressions
The concepts gradient, divergence, and rotation follow from the equations

V
dV ∇ψ =

(V )
df ψ ,

V
dV ∇· a =

(V )
df · a
(Gauss’s theorem),

V
dV ∇× a =

(V )
df × a .
Dividing a ﬁnite volume into inﬁnitesimal parts, the contributions of adjacent planes
cancel in pairs. Corresponding to these, we found in Sect. 1.1.7 [the ﬁrst expression
is, of course, also equal to

A(df × ∇) · a]

A
df · (∇× a) =

(A)
dr · a
(Stokes’s theorem),

A
df × ∇ψ =

(A)
dr ψ .

1.1 Vector Analysis
17
The last equation can be proven like Stokes’s theorem. Likewise, we may also derive
the following equation:

A
(df × ∇) × a =

(A)
dr × a .
If we take the area element df = ex dy dz once again, then using the vector product
expansion on p. 4, the integrand on the left-hand side is equal to ∇(ex · a) −ex ∇· a.
On the right, one has the same, namely, dz ez × (∂a/∂y) dy −dy ey × (∂a/∂z) dz.
In addition, since ∇· (ψa) = ψ ∇· a + a · ∇ψ Gauss’s theorem implies

(V )
df · ψa =

V
dV (ψ ∇· a + a · ∇ψ) .
(Here the left- and right-hand sides should be interchanged, i.e., the triple integral
should be simpliﬁed to a double integral.) Hence, we deduce the ﬁrst and second
Green theorems

(V )
df · ψ ∇φ =

V
dV (ψ 	φ + ∇φ · ∇ψ) ,

(V )
df · (ψ ∇φ −φ ∇ψ) =

V
dV (ψ 	φ −φ 	ψ) .
Taking ψ as the Cartesian component of a vector b, we may also infer

(V )
(df · a) b =

V
dV { b (∇· a) + (a · ∇) b } .
Since b = r and (a · ∇) r = a, it also follows that

V
dV a =

(V )
(df · a) r −

V
dV r (∇· a) .
The volume integral over a source-free vector ﬁeld a is thus always zero if a vanishes
on the surface (V ) .
Finally, we should mention the equation

(V )
df × ψ a =

V
dV (ψ ∇× a −a × ∇ψ) ,
where we have used ∇× (ψa) = ψ ∇× a −a × ∇ψ.

18
1
Basics of Experience
1.1.10
Delta Function
In the following, we shall often use the Dirac delta function. Therefore, its properties
are compiled here, even though it does not actually belong to vector analysis, but to
general analysis (and in particular to integral calculus).
We start with the Kronecker symbol
δik =
 0
for i ̸= k ,
1
for i = k .
It is useful for many purposes. In particular we may use it to ﬁlter out the k th element
of a sequence { fi}:
fk =

i
fi δik .
Here, of course, within the sum, one of the i has to take the value k. Now, if we
make the transition from the countable (discrete) variables i to a continuous quantity
x, then we must also generalize the Kronecker symbol. This yields Dirac’s delta
function δ(x −x′). It is deﬁned by the equation
f (x′) =
 b
a
f (x) δ(x −x′) dx
for a < x′ < b , zero otherwise ,
where f (x) is an arbitrary continuous test function. If the variable x (and hence also
dx) is a physical quantity with unit [x], the delta function has the unit [x]−1.
Obviously, the delta function δ(x −x′) is not an ordinary function, because it has
to vanish for x ̸= x′ and it has to be singular for x = x′, so that the integral becomes

δ(x −x′) dx = 1. Consequently, we have to extend the concept of a function:
δ(x −x′) is a distribution, or generalized function, which makes sense only as a
weight factor in an integrand, while an ordinary function y = f (x) is a map x →y.
Every equation in which the delta function appears without an integral symbol is an
equation between integrands: on both sides of the equation, the integral symbol and
the test function have been left out.
The delta function is the derivative of the Heaviside step function:
ε(x −x′) =
0
for x < x′
1
for x > x′
=⇒
δ(x) = ε′(x) .
At the discontinuity, the value of the step function is not usually ﬁxed, although the
mean value 1/2 is sometimes taken, whence it becomes point symmetric. The step
function is often called the theta function and noted by θ (or ) instead of ε (con-
trary to the IUPAP recommendation). The derivative of the step function vanishes for
x ̸= x′, while
 b
a ε′(x −x′) dx ≡ε(b −x′) −ε(a −x′) is equal to one for
a < x′ < b and zero for other values of x′.

1.1 Vector Analysis
19
Hence, using
ε(x) = 1
2 + 1
π lim
ε→+0 arctan x
ε ,
we ﬁnd the important equations
δ(x) = 1
π
lim
ε→+0
ε
x2 + ε2 =
1
2πi
lim
ε→+0

1
x −iε −
1
x + iε

≡
1
2πi

1
x −io −
1
x + io

.
We may thus represent the generalized function δ(x) as a limit of ordinary functions
which are concentrated ever more sharply at only one position. According to the last
equation it is practical here to decompose the delta function in the complex plane
into two functions with the same pole for ±io with opposite residues, then to take
the limit o →+0.
Clearly, we also have
i
x + io = 2π δ(x) +
i
x −io = π δ(x) + i
2

1
x + io +
1
x −io

,
if we make use of π δ(x) = 1
2i {(x + io)−1 −(x −io)−1} for the second reformu-
lation. Here, the expression in the last bracket vanishes for x2 ≪o2, while it turns
into 2x/(x2 + o2) ≈2/x for x2 ≫o2. This can be exploited for the principal-value
integral (the principal value) P . . . , a kind of opposite to the delta function, because
it leaves out the singular position x′ in the integration, with equally small paths on
either side of it:
P
 b
a
f (x) dx
x −x′
≡lim
ε→+0

 x′−ε
a
+
 b
x′+ε
 f (x) dx
x −x′
.
Like the delta function, the symbol P also makes sense only in the context of an
integral. Hence we may also write the equation above as
1
x ± io = P
x ∓iπ δ(x) .
This result is obtained rather crudely here, because the inﬁnitesimal quantity o is
supposed to be arbitrarily small, but nevertheless different from zero. It can be proven
using the residue theorem from the theory of complex functions. To this end, we
consider
 ∞
−∞
f (x) dx
x −(x′ −io) ±
 ∞
−∞
f (x) dx
x −(x′ + io) =

 
C1
±

C2
 f (z) dz
z −z′
,
with the two integrations running from left to right because of C1 (above) and C2
(below the symmetry axis) in Fig. 1.6 for regular test functions f (z). In the complex

20
1
Basics of Experience
Fig. 1.6 Integration paths C1 and C2 (continuous lines) to determine the principal value and the
residues. The (real) symmetry axis is shown by the dashed line
z-plane the integrand only has the pole at z′ = x′ −io in the lower half-plane and at
x′ + io in the upper half-plane, whence the indicated integrations can be performed.
The difference between the two integrals is equal to −

f (x) (x −x′)−1 dx,
according to the residue theorem, thus equal to −2πi f (x′). In the sum of the two
integrals the contributions from the half circles cancel, since for z = z′ + ε exp(iφ),
we have dz = iε exp(iφ) dφ = i(z −z′) dφ, and what remains is twice the princi-
pal value, which is what was to be shown. Hence, we have proven our claim that
(x ± io)−1 = P x−1 ∓iπ δ(x).
Since x δ(x) = 0, the integrand may even be divided by functions which have
zeros:
A = B
⇐⇒
A
x = B
x + C δ(x) .
The constant C in the integrals can be ﬁxed, provided that we also ﬁx the integration
path across the singularity (e.g., as for the principal value integral).
An important property of the delta function is
δ(a x) = 1
|a| δ(x) ,
because both sides are equal to dε(y)/dy for y = ax. In particular, the delta function
is even, i.e., δ(−x) = δ(x). Hence we can even infer
 ∞
0 δ(x) dx = 1
2. If instead of
ax we take a function a(x) as argument, and if a(x) has only one-fold zeros xn, then
it follows that
δ(a(x)) =

n
δ(x −xn)
|a ′(xn)| ,
and
in
particular
also
that
δ(x2 −x02) = {δ(x −x0) + δ(x + x0)}/(2|x0|).
In addition,

f (x) δ(x −y) δ(y −x′) dx dy =

f (y) δ(y −x′) dy = f (x′) =

f (x) δ(x −x′) dx delivers the equation

δ(x −y) δ(y −x′) dy = δ(x −x′) .
This is similar to the deﬁning equation of the delta function, in which we allowed
only for ordinary, continuous functions as test functions.
For the n th derivative of the delta function, n partial integrations (for a < x′ < b,
zero otherwise) result in

1.1 Vector Analysis
21
 b
a
f (x) δ(n)(x −x′) dx = (−)n f (n)(x′) ,
because the limits do not contribute. It thus follows that x δ′(x) = −δ(x), which we
shall need in quantum theory (Sect. 4.3.2) for the real-space representation of the
momentum operator, viz., P = (ℏ/i) ∇.
If, in the interval a ≤x ≤b, we have a complete orthonormal set of functions
{gn(x)}, i.e., a series of functions with the properties
 b
a
gn
∗(x) gn′(x) dx = δnn′
as well as f (x) = 
n gn(x) fn for all (square-integrable) functions f (x), then after
interchange of summation and integration, we have fn =
 b
a gn∗(x) f (x) dx for
the expansion coefﬁcients, and hence 
n
 b
a gn(x′) gn∗(x) f (x) dx = f (x′), which
leads to
δ(x −x′) =

n
gn
∗(x) gn(x′) .
Each complete set of functions delivers a representation of the delta function, i.e., it
can be expanded in terms of ordinary functions.
In particular, we can expand the delta function in the interval −a ≤x ≤a
in terms of a Fourier series: we have gn(x) = 1/
√
2a exp(inxπ/a) with n ∈
{0, ±1, ±2, . . .} and (the result is even in x −x′)
δ(x −x′) = 1
2a

n
exp inπ(x −x′)
a
for −a ≤x ≤a .
For a →∞, we can even go over to a Fourier integral. For very large a, the sequence
kn = nπ/a becomes nearly continuous. Therefore, we replace the sum 
n f (kn) 	k
with 	k = π/a by its associated integral
δ(x −x′) = 1
2π
 ∞
−∞
exp{ik(x −x′)} dk
for −∞< x < ∞.
For the Fourier expansion, we therefore take g(k, x) = 1/
√
2π exp(ikx). We now
have the basics for the Fourier transform, which we shall discuss in the next section.
The integral from −∞to +∞can be decomposed into the one from −∞to
0 plus the one from 0 to +∞. But with k →−k, we have
 0
−∞exp (ikx) dk =
 ∞
0 exp (−ikx) dk, so this part delivers the complex-conjugate of the other part.
Therefore, we infer Re
 ∞
0 exp (ikx) dk = π δ(x) or
δ(x) = 1
π
 ∞
0
cos kx dk
and
ε(x) = 1
2 + 1
π
 ∞
0
sin kx
k
dk .

22
1
Basics of Experience
On the other hand, the usual integration rules for
 ∞
0 exp (ikx) dk deliver the expres-
sion (ix)−1 exp (ikx)|k=∞
k=0 . For real x, this is undetermined for k →∞. But if x
contains an (even very small) positive imaginary part, then it vanishes for k →∞.
We include this small positive imaginary part of x as before through x + io (with
real x):
 ∞
0
exp(ikx) dk =
i
x + io = π δ(x) + i P
x .
We have already proven this for the real part of the integral, because the real part
of the right-hand side has turned out to be equal to π δ(x). But then the equation
holds also for the imaginary part, because the proof used only general properties of
integrals.
1.1.11
Fourier Transform
If the region of deﬁnition is inﬁnite on both sides, we use
f (x) =
 ∞
−∞
g(k, x) f (k) dk ,
f (k) =
 ∞
−∞
g∗(k, x) f (x) dx ,
with g(k, x) = 1/
√
2π exp(ikx):
f (x) =
1
√
2π
 ∞
−∞
exp(+ikx) f (k) dk ,
f (k) =
1
√
2π
 ∞
−∞
exp(−ikx) f (x) dx .
Generally, f (x) and f (k) are different functions of their arguments, but we would
like to distinguish them only through their argument. [The less symmetric notation
f (x) =

exp(ikx) F(k) dk with F(k) = f (k)/
√
2π is often used. This avoids the
square root factor with the agreement that (2π)−1 always appears with dx.] Instead
of the pair of variables x ↔k, the pair t ↔ω is also often used.
Important properties of the Fourier transform are
f (x) = f ∗(x)
⇐⇒
f (k) = f ∗(−k) ,
f (x) = g(x) h(x)
⇐⇒
f (k) =
1
√
2π
 ∞
−∞
g(k−k′) h(k′) dk′ ,
f (x) = g(x−x′)
⇐⇒
f (k) = exp(−ikx′) g(k) .
For a periodic function f (x) = f (x −l) the last relation leads to the condition kn =
2π n/l with n ∈{0, ±1, ±2, . . . }, thus to a Fourier series instead of the integral.
In addition, by Fourier transform, all convolution integrals

g(x −x′) h(x′) dx′ can

1.1 Vector Analysis
23
clearly be turned into products
√
2πg(k) h(k) (Problem 3.9), which are much easier
to handle.
If f (x) vanishes for all x < 0, then f (x) = ε(x) f (x) holds with the step function
mentioned in the last section, e.g., for “causal functions” f (t), which depend upon
the time t. Then the Fourier transform yields the relation
f (x) = ε(x) f (x)
⇐⇒
f (k) = i
π P
 ∞
−∞
dk′
f (k′)
k −k′ .
Here, due to the factors i in the Fourier transformed f (k), the real and imaginary
parts are related to each other in such a way that only the one or the other (for all
k) needs to be measured. This relation is sometimes called the Kramers–Kronig or
dispersion relation, even though it also actually exploits the fact that f (x) is real,
whence the integration has to be performed over just half the region, viz., 0 to ∞.
Another result that is often useful is Parseval’s equation
 ∞
−∞
dx g∗(x) h(x) =
 ∞
−∞
dk g∗(k) h(k) .
In order to prove it, we expand the left-hand side according to Fourier and obtain
the integral (2π)−1 
dx dk dk′ exp{i(k −k′)x} g∗(k′) h(k). After integration over
x, we encounter the delta function 2π δ(k −k′) and can then also integrate easily
over k′, which yields the right-hand side. In particular,

dx | f (x)|2 =

dk | f (k)|2.
Table 1.2 shows some of the Fourier transforms commonly encountered. To prove
the last relation in the table, we have to use a square addition in the exponent and the
integral
 ∞
−∞exp (−x2) dx = √π, the latter following from
 ∞
−∞
exp (−x2 −y2) dx dy = 2π
 ∞
0
e−s 1
2ds = π ,
with s = r2 = x2 + y2 .
Table 1.2 Some functions
and their Fourier transforms
f (x)
f (k)
δ(x −x′)
exp (−ikx′)
√
2π
1
2a ε(a2 −x2)
1
√
2π
sin(ak)
ak
ε(x) exp (−λx)
1
√
2π
1
λ + ik
if Reλ > 0
exp −(x −x′)2
2	2
	 exp −	2k2
2
exp(−ikx′)

24
1
Basics of Experience
Fig. 1.7 Fourier transform (left, red) of the box function (right, blue). This is useful, e.g., for the
refraction from a slit
Fig. 1.8 Fourier transform (right) of the truncated exponential function f (x) = ε(x) exp(−λx)
(left). This is useful for decay processes, if x stands for the time and k for the angular frequency.
Here the dashed blue curve shows the real part and the continuous red curve the imaginary part of
λf (k). The Kramers–Kronig relation relates these real and imaginary parts
From the ﬁrst example with x′ = 0, the Fourier transform of a constant is a delta
function, and from the fourth example with x′ = 0, the Fourier transform of a Gaus-
sian function is a Gaussian function again. The second relation is represented in
Fig. 1.7 and the third in Fig. 1.8.
Correspondingly, in three dimensions with k as wave vector (more on p. 137), we
have
δ(k −k ′) =
1
(2π)3
 ∞
−∞
d3r exp{i (k −k ′) · r } ,
f (r) =
1
√
2π 3
 ∞
−∞
d3k exp(+i k · r) f (k) ,
f (k) =
1
√
2π 3
 ∞
−∞
d3r exp(−i k · r) f (r) .
Here, d3r is used for the volume element dV in real space and correspondingly d3k
for the volume element in reciprocal space. In Cartesian coordinates, we then have
δ(r −r ′) = δ(x −x′) δ(y −y′) δ(z −z′).

1.1 Vector Analysis
25
From the expansion
a (r) =
1
√
2π 3

d3k exp(ik · r) a(k)
of a vector ﬁeld a (r), since Fourier expansions are unique, it follows that
∇× a (r) = b (r)
⇐⇒
ik × a (k) = b (k)
and
∇· a (r) = b (r)
⇐⇒
ik · a (k) = b (k) .
If, for example, the curly bracket in

d3k exp(ik · r) {ik × a (k) −b(k)} vanishes
for all k, then of course the integral also does for all r. Rotation-free ﬁelds thus
have Fourier component a (k) in the direction of the wave vector (longitudinal ﬁeld
along). In contrast, source-free ﬁelds have Fourier component a (k) perpendicular to
the wave vector (transverse ﬁeld atrans). According to p. 4, the decomposition
a(k) = ek (ek · a(k)) −ek × (ek × a(k)) ,
with ek ≡k
k ,
therefore splits up into a longitudinal and a transverse part, i.e., into the vortex-free
and the source-free part.
Some important examples of Fourier transforms in the three-dimensional space
are listed on p. 410.
1.1.12
Calculation of a Vector Field from Its Sources
and Curls
Every vector ﬁeld that is continuous everywhere and vanishes at inﬁnity can be
uniquely determined from its sources and curls (rotations, vortices):
a (r) = −∇

dV ′ ∇′ · a (r ′)
4π|r −r ′| + ∇×

dV ′ ∇′ × a (r ′)
4π|r −r ′ | .
The ﬁrst term here becomes ﬁxed by the sources of a and, like every pure gradient
ﬁeld, is vortex-free, while the second, like every pure vortex ﬁeld, is source-free and
becomes ﬁxed by the vortex of a. The operator ∇′ acts on the coordinate r ′, while
∇acts on the coordinate r and therefore may be interchanged with the integration.
The decomposition is unique. If there were two different vector ﬁelds a1 and a2
with the same sources and curls, then a1 −a2 would have neither sources nor curls,
and in addition would vanish at inﬁnity. But according to p. 16, a1 = a2 has to hold.

26
1
Basics of Experience
To prove the claim, we evaluate ∇· a and ∇× a :
∇· a = −1
4π

dV ′ ∇′ · a (r ′) 	
1
|r −r ′| ,
∇×a = −1
4π

dV ′ 
∇′×a (r ′) 	
1
|r −r ′| −∇

∇· ∇′×a (r ′)
|r −r ′|

.
Still, a (r) could contain a constant term, which would affect neither ∇· a nor ∇× a,
but a = 0 has to hold at inﬁnity and this ﬁxes this term uniquely. Now we show—and
this is sufﬁcient for the proof—that
	
1
|r −r ′| = −4π δ(r −r ′) ,
and that the last term in ∇× a does not contribute. With r ′ = 0 and recalling from
Sect. 1.1.8 that ∇rn = n rn−2 r, we have
	1
r = ∇· ∇1
r = −∇· r
r3 = −

∇· r
r3
+ r · ∇1
r3

= −

 3
r3 + r · −3r
r5

.
This expression vanishes for r ̸= 0. On the other hand, if we evaluate the source
strength at the origin using Gauss’s theorem with a sphere of radius r > 0 around it,
we have

dV ∇· ∇1
r =

df · ∇1
r = −1
r2

df · er = −4π .
This shows the ﬁrst part of the proof, since δ(r −r ′) vanishes for r ̸= r ′ and

dV δ(r −r ′) is equal to 1. In addition, with b = ∇′ × a(r ′), which depends only
upon r ′, but not upon r, we have
∇

∇·
b
|r −r ′|

= ∇

b · ∇
1
|r −r ′|

= (b · ∇) ∇
1
|r −r ′| .
Since ∇|r −r ′|−1 = −∇′|r −r ′|−1, this is equal to (b · ∇′) ∇′|r −r ′|−1, and using

(V ) df · b a =

V dV {a ∇· b + (b · ∇) a} (see p. 17), it therefore delivers

V
dV ′ ∇

∇·
b
|r −r ′|

=

V
dV ′ (b · ∇′) ∇′
1
|r −r ′|
=

(V )
df ′ · b ∇′
1
|r −r ′| −

V
dV ′ ∇′
1
|r −r ′| ∇′ · b .
Since ∇′ · b = ∇′ · (∇′ × a (r ′)) = 0, the last integral does not contribute. For the
surface integral, we take a sphere with sufﬁciently large radius r′. Its surface area is
4πr′2, while ∇|r −r ′|−1 is equal to r′−2 there. Thus we only have to require that
∇× a vanishes at the surface with r′ →∞and everything is proven.

1.1 Vector Analysis
27
According to the relation 	|r −r ′|−1 = −4π δ(r −r ′) just proven, the solution
of the inhomogeneous differential equation 	 = φ(r) (Poisson equation) can be
represented as an integral over the inhomogeneity φ(r) with suitable weight factor.
This is called the Green function G(r, r ′) of the Laplace operator:
	G(r, r ′) = δ(r −r ′)
⇐⇒
G(r, r ′) = −1
4π
1
|r −r ′| .
In particular, it yields the solutions of the differential equations
	 = φ(r)
and
	A = a(r) ,
i.e., of ∇· ∇ = φ and of ∇(∇· A) −∇× (∇× A) = a with  ∼0 and A ∼0
for r →∞. In electromagnetism, we shall meet them in the context of the scalar
potential (Sect. 3.1.3) and the vector potential (Sect. 3.2.8). These solutions are
(r) =

dV ′ G(r, r ′) φ(r ′)
and
A(r) =

dV ′ G(r, r ′) a(r ′) .
By partial integration, they have the properties
∇ =

dV ′ G(r, r ′) ∇′φ(r ′) ,
∇· A =

dV ′ G(r, r ′) ∇′ · a(r ′) ,
∇× A =

dV ′ G(r, r ′) ∇′ × a(r ′) .
Here, we used the fact that  and A vanish at inﬁnity, whence the inhomogeneities
φ and a vanish faster by two orders. Thus, if a is source- or curl-free, the solution A
of the Poisson equation 	A = a is likewise.
The theorem proven in this section is called the principal theorem of vector anal-
ysis. It assumes that the source and curl densities are known everywhere—these ﬁx
the vector ﬁelds.
1.1.13
Vector Fields at Interfaces
If ∇· a or ∇× a are different from zero only on a sheet, the volume integrals just
mentioned simplify to surface integrals. Correspondingly, instead of ∇· a and ∇× a,
we now introduce the surface divergence and surface rotation. They have different
units from ∇· a and ∇× a, related to the area instead of the volume:

28
1
Basics of Experience
Fig. 1.9 View of a sheet of discontinuity of a vector ﬁeld. Dashed red lines show the envelope
Div a ≡∇A · a ≡lim
V →0
1
A

(V )
df · a ,
Rot a ≡∇A × a ≡lim
V →0
1
A

(V )
df × a .
Here, V is the volume of a thin layer, covering the latter surface A (see Fig. 1.9).
Even though A is inﬁnitesimally small, it nevertheless has dimensions that are large
compared with the layer thickness, so only the faces contribute to the surface integrals
of the layer. With n as unit normal vector to the face, pointing “from minus to plus”,
we may then write
∇A · a = n · (a+ −a−) ,
∇A × a = n × (a+ −a−) .
Thus, if the vector ﬁeld a changes in a step-like manner at a sheet (from a−toa+), then
forδa ∥n,ithasanareadivergence(discontinuousnormalcomponentlike,e.g.,atthe
interface on the left in Fig. 1.5) and for δa ⊥n, it has an area rotation (discontinuous
tangential component like, e.g., at the interface on the right in Fig. 1.5).
1.2
Coordinates
1.2.1
Orthogonal Transformations and Euler Angles
In order to perform sums, we now prefer to write e1, e2, e3 instead of ex, ey, ez.
In addition, the coordinate origin will be assumed ﬁxed here for every coordinate
transformation. Displacements would be easy to include.
For the transition from a Cartesian frame {e1, e2, e3} to one rotated about the
origin {e1′, e2′, e3′}, we have

1.2 Coordinates
29
ei
′ =

k
(ei
′ · ek) ek ≡

k
Dik ek
and
ek =

i
(ek · ei
′) ei
′ =

i
Dik ei
′ .
Since ek · el = δkl = ek′ · el′,

i
Dik Dil = δkl =

i
Dki Dli ,
and in addition Dik = Dik
∗.
These equations may be written as matrix equations, if we understand Dik as the
element of the matrix D in row i and column k. Then, if D is the transpose of D
(with Dik = Dki), we have
D D = 1 = D D
(so D−1 = D) ,
and in addition D = D∗.
This is called an orthogonal transformation. If D−1 = D ∗≡D†, the transforma-
tion is unitary. Real unitary transformations are thus orthogonal transformations.
Because det (D2D1) = det D2 · det D1 and det D = det D (see p. 5), orthogonal
transformations have det D = ±1. Depending on the sign, we distinguish between
proper orthogonal transformations with
det D = +1
and improper orthogonal transformations with det D = −1. Only the proper ones
are connected continuously to the identity and therefore correspond to rotations.
But if we go over from a right- to a left-handed frame, then this is an improper
transformation, in particular, Dik = −δik, i.e., D = −1, corresponds to a space
reﬂection (inversion or parity operation).
Carrying out two rotations D1 and D2 one after the other amounts to doing a single
rotation D = D2D1, because D D = 
D2D1 D2D1 = D1 D2D2D1 = 1 and D D =
D2D1 D1 D2 = 1. However, the resulting rotation depends on the order, that is, in
general D1D2 ̸= D2D1, e.g., for ﬁnite rotations about different axes.
For the Cartesian components of a vector a, we have
ak ≡ek · a ,
ai
′ ≡ei
′ · a =

k
Dik ak .
Instead of going over to a rotated coordinate system, we may also stick with the refer-
ence frame and rotate all objects. In both cases we change the Cartesian components
of every vector a. However, the rotation of an object through an angle α corre-
sponds to the opposite rotation of the coordinate systems, through the angle −α, and

30
1
Basics of Experience
Fig. 1.10 The Euler angles α, β, γ , used to describe the transition from unprimed to primed
coordinates. The dashed line is the line of nodes ez × ez′. The sequence is black →blue →green
→red. The initial equator is black and the last one red
vice versa. Therefore, with column matrices A′ and A and with the rotation matrix
D, we write
A′ = D A ,
or a ′ = D a .
Here, the second equation refers to a rotation of the vectors, because a and a ′ should
be ﬁxed independently of the coordinate system. Correspondingly, we may also write
the scalar product a · b as a matrix product AB of a row and of a column vector, for
which their Cartesian components are necessary. Then we ﬁnd
DADB = ADDB =
AB, implying that a ′ · b ′ = a · b, as it should be for a scalar product. (In the next
section, we will obtain the scalar product for other coordinate systems.)
Because of 1 = 1 the requirement DD = 1 constitutes six conditions in three
dimensions, and 1
2 N(N + 1) conditions in N dimensions. Consequently, orthogonal
transformations in three dimensions depend upon three real parameters. A rotation
can be ﬁxed uniquely by specifying these, e.g., by specifying the (axial) rotation
vector in the direction of the rotation axis, with value equal to the rotation angle,
or by specifying the three Euler angles α, β, γ , with which one goes over from the
original frame {ex, ey, ez} to the rotated one {ex′, ey′, ez′} (see Fig. 1.10):
• The ﬁrst Euler angle α ﬁxes the azimuth, i.e., {ex, ey, ez} →{e ˜x, e ˜y, e˜z} with
e˜z = ez, while the other axes move in a horizontal plane P1.
• The second Euler angle β describes the polar distance (motion of the z-direction),
i.e., {e ˜x, e ˜y, e˜z} →{e ˜x′, e ˜y′, e˜z′}, with e ˜y′ = e ˜y. The new e ˜x′ and e ˜y′ axes span
a plane P2 inclined at an angle β to the horizontal. The two planes P1 and P2
intersect along e ˜y′ = e ˜y.
• The third Euler angle γ describes the rotation about the new ˜z′ direction, that is,
{e ˜x′, e ˜y′, e˜z′} →{ex′, ey′, ez′}, with e˜z′ = ez′, and the other axes moving on the
plane P2. The common axis is along e˜z′ = ez′, the so-called line of nodes.

1.2 Coordinates
31
The ﬁrst two Euler angles are called the azimuth and polar distance of the new z-axis
in the old system, while the third Euler angle gives the angle between the new y-axis
and the line of nodes. This line of nodes forms a right-handed system with the old
and the new z-axes.
In some cases the Euler angles are deﬁned differently, namely with a left-handed
frame or the angles between the line of nodes and the x-axes instead of the y-axes,
but the simple assignment of α to the azimuth of the new z-axis is then lost.
We now have
D = Dα Dβ Dγ
with
Dα =
⎛
⎝
cos α −sin α 0
sin α
cos α 0
0
0
1
⎞
⎠,
Dβ =
⎛
⎝
cos β 0 sin β
0
1
0
−sin β 0 cos β
⎞
⎠,
and Dγ like Dα, but γ instead of α, because Dα and Dγ describe rotations about
the (old) z-axis, Dβ a rotation about the y-axis. If it were the coordinate system that
were rotated, then every sine would have the opposite sign, because of the opposite
rotation. Of course, starting from the Euler angles, we can evaluate the rotation vector,
and vice versa, but we shall not discuss that here. Further properties are derived in
Problems 2.1–2.3.
1.2.2
General Coordinates and Their Base Vectors
So far all quantities have been written in a coordinate-free manner as far as possible—
Cartesian coordinates and unit vectors have occasionally been useful only for con-
versions. Sometimes curvilinear coordinates are more appropriate, e.g., spherical
coordinates (r, θ, ϕ) or cylindrical coordinates (r, ϕ, z), where circles also appear as
coordinate lines. Still, for these two examples the coordinates are orthogonal to each
other everywhere. We are thus dealing here with curvilinear rectangular coordinates.
But we would like to allow also for oblique coordinates. These are convenient, e.g.,
for crystallography, and they also provide with a suitable framework for relativity
theory. Curvilinear oblique coordinates are what restrict us the least.
Even though a three-dimensional space is assumed throughout the following, most
of the discussion can be transferred easily to higher dimensions. We shall hint at the
special features of three-dimensional space in the appropriate place, namely, for axial
vectors.
As usual, from now on we will write (x1, x2, x3) = {xi} for the coordinate triple of
coordinates, despite the risk here of confusing i with a power. In addition, instead of
the Cartesian unit vectors, we introduce two sorts of base vectors. In crystal physics,
gi is called a lattice vector and gi (except for a factor of 2π) a reciprocal lattice
vector, but restricted to linear coordinates with constant base vectors:

32
1
Basics of Experience
Fig. 1.11 Oblique coordinates are indicated here by lines with δxi = 1. Shown are their covariant
base vectors gi and also their contravariant base vectors gi. If g1 and g2 form an angle γ and if
these vectors have lengths g1 and g2, respectively, then the lengths of the contravariant base vectors
are gi = 1/(gi sin γ ) (from gi · gk = δi
k). Oblique coordinates appear, e.g., if for unequal masses
two-body coordinates are transformed to center-of-mass and relative coordinates (see Fig. 2.7)
covariant base vectors (g i down) gi ≡∂r
∂xi ,
contravariant base vectors (g i up)
gi ≡∇xi .
In these equations the index i on the right-hand side is really a lower or upper index.
The covariant base vector gi is tangent to the coordinate line xi (all other coor-
dinates remain ﬁxed), and the contravariant base vector gi is perpendicular to the
surface xi = const. (all other coordinates may change) (see Fig. 1.11). For rectangu-
lar coordinates, gi and gi have the same direction, but for oblique ones, they do not.
For rectangular coordinates the two base vectors generally have different lengths.
Only for Cartesian coordinates are covariant and contravariant base vectors equal,
viz., to the corresponding unit vectors (see Problems 3.10 to 3.12).
The two scalar products
gik ≡gi · gk = ∂r
∂xi ·
∂r
∂xk = gki ,
gik ≡gi · gk = ∇xi · ∇xk = gki ,
depend on the chosen coordinates (because all base vectors depend on them), but not
the scalar products of covariant and contravariant base vectors,

1.2 Coordinates
33
gi · gk = gk · gi = ∇xk · ∂r
∂xi = ∂xk
∂xi = δk
i =
0
for i ̸= k ,
1
for i = k .
Covariant and contravariant base vectors each form an expansion basis. Therefore,
also
a =

i
gi (gi · a) =

i
gi (gi · a) ,
in particular, gk = 
i gi gik, gk = 
i gi gik, and

i
gik gil = gk · gl = δl
k .
This very decisively generalizes the decomposition into Cartesian unit vectors, not
only to curvilinear, but also to oblique coordinates. With the useful concepts
covariant component of a :
ai ≡gi · a
and contravariant component of a :
ai ≡gi · a
and with a = 
i gi ai = 
i gi ai, we thus obtain
ai =

k
gik ak ,
ai =

k
gik ak ,
and
a · b =

i
ai bi .
Covariant and contravariant components can be converted into each other, referred to
as raising and lowering indices. With the scalar product, covariant and contravariant
components always appear. We shall always meet sums of products where the index
in the factors appears one up and one down. Therefore, we generally use Einstein’s
summation convention, according to which, for these index positions, the summation
symbol is left out. This is indeed what we shall do below (from Sect. 3.4.3 on).
1.2.3
Coordinate Transformations
New and old quantities are usually denoted with and without a prime, respectively.
In view of various indices being added, a bar will be used instead of the prime in this
book.
With a change of coordinates, the behavior depends decisively on the position
of the indices. Since ∂/∂¯xi = 
k(∂xk/∂¯xi) (∂/∂xk), on the one hand, and since
we also have ¯gi · dr = d ¯xi = 
k(∂¯xi/∂xk) dxk, with dxk = gk · dr, on the other,
the transition xi →¯xi is connected to the following equations, the order of factors
being irrelevant. Here the coefﬁcients form a matrix, the row index being given by
the numerator and the column index by the denominator:

34
1
Basics of Experience
¯gi =

k
gk
∂xk
∂¯xi ,
¯gi =

k
∂¯xi
∂xk gk ,
¯ai =

k
ak
∂xk
∂¯xi ,
¯ai =

k
∂¯xi
∂xk ak .
Here, ¯ai ≡a · ¯gi and ¯ai ≡a · ¯gi . With the change of coordinates, the base vectors
change, but not the other vectors a. Covariant and contravariant quantities have
transformation matrices inverse to each other:

k
∂¯xi
∂xk
∂xk
∂¯x j = ∂¯xi
∂¯x j = δi
j .
Thesystemofequationsd ¯xi = 
k(∂¯xi/∂xk) dxk canbewrittenasamatrixequation:
⎛
⎝
d ¯x1
d ¯x2
d ¯x3
⎞
⎠=
⎛
⎜⎜⎜⎜⎜⎝
∂¯x1
∂x1
∂¯x1
∂x2
∂¯x1
∂x3
∂¯x2
∂x1
∂¯x2
∂x2
∂¯x2
∂x3
∂¯x3
∂x1
∂¯x3
∂x2
∂¯x3
∂x3
⎞
⎟⎟⎟⎟⎟⎠
⎛
⎝
dx1
dx2
dx3
⎞
⎠.
The transformation matrix is called the Jacobi matrix or functional matrix. Naturally,
it also exists for space dimensions other than three.
For two successive transformations, the two associated Jacobi matrices can be
combined in a single product matrix. If the second transformation is the transforma-
tion back to the original coordinates, then the result is the unit matrix: the inverse
transformation is described by the inverse matrix. This exists only if the Jacobi
determinant (functional determinant), viz.,
∂(¯x1, ¯x2, ¯x3)
∂(x1, x2, x3) ≡
											
∂¯x1
∂x1
∂¯x1
∂x2
∂¯x1
∂x3
∂¯x2
∂x1
∂¯x2
∂x2
∂¯x2
∂x3
∂¯x3
∂x1
∂¯x3
∂x2
∂¯x3
∂x3
											
,
does not vanish, and likewise the determinant of the inverse Jacobi matrix, because
the two coordinate systems should be treated on an equal footing.

1.2 Coordinates
35
1.2.4
The Concept of a Tensor
We generalize the expressions derived so far for a vector ﬁeld and denote as a tensor
of rank n + m (with n covariant and m contravariant indices) a quantity whose
components transform under a change of coordinates according to
¯T i1...im
k1...kn =

j1...ln
∂¯xi1
∂x j1 · · · ∂¯xim
∂x jm
∂xl1
∂¯xk1 · · · ∂xln
∂¯xkn T j1... jm
l1...ln
.
Scalars are tensors of zeroth rank and vectors are tensors of ﬁrst rank. If T (x) is a
scalarﬁeld,thenthenewfunction ¯T (¯x)shouldhavethesamevalueforthecoordinates
¯x as the old function T (x) for the old coordinates x = f (¯x), whence we should have
¯T (¯x) = T ( f (¯x)) without further transformation matrices. In contrast, for a gradient
ﬁeld with ∇Ti ≡∇T · gi, because gi = ∂r/∂xi and ∇Ti = ∂T/∂xi, we have
∇T k ≡∂¯T (¯x)
∂¯xk
=

i
∂T (x)
∂xi
∂xi
∂¯xk ≡

i
∇Ti
∂xi
∂¯xk ,
showing that this is a vector ﬁeld.
Tensors of the same type can be added, and the (tensor) product of a tensor of nth
rank with a tensor of mth rank is a tensor of rank n + m:
T i1...in T k1...km = T i1...ink1...km .
Of course, some covariant components may occur on the left- and right-hand sides.
But one can also lower the tensorial rank by contracting the tensor:

i
T i i1...im
i k1...kn = T i1...im
k1...kn ,
because covariant and contravariant components transform inversely to each other.
(Here, too, the summation symbol is often left out, using the Einstein summation
convention.) A special case of this is the scalar product of two vectors,

i
ai bi = a · b =

i
ai bi =

i
¯ai ¯bi .
Generally, a tensor of nth rank can be contracted with n vectors to produce a scalar.
This ﬁxes tensors in a coordinate-free way. In Sect. 2.2.10, for example, we shall
introduce the moment of inertia I, which is a tensor of second rank. The tensor
product Iω delivers the vector L (angular momentum) and 1
2 ω · L a scalar (kinetic
energy), where I is contracted twice with the vector ω.

36
1
Basics of Experience
The trace of a square matrix is the sum of its diagonal elements: 
i I i i = tr I,
which is the contraction of a tensor of second rank to a scalar. In fact, tr I remains
unchanged under a change of coordinates.
The change of coordinates under a rotation on p. 30 led to the matrix equation
A′ = DA for a column vector A. Correspondingly, L = Iω reads L = I as a
matrix equation where L and  are column matrices and I is a square matrix. For a
rotation we have L′ = DL, ′ = D, and  = D−1′, respectively, and hence L′ =
DI D−1′, so L′ = I ′′ with I ′ = DI D−1. Here we now write Li = 
k I i k ωk and
¯I i
k =

jl
∂¯xi
∂x j
∂xl
∂¯xk I j
l ,
with

j
∂¯xi
∂x j
∂x j
∂¯xk = δi
k .
The last equation corresponds to DD−1 = 1.
The quantities gik and gik introduced above are tensors of second rank. Since
dr · dr =

ik
∂r
∂xi · ∂r
∂xk dxi dxk =

ik
gik dxi dxk ,
we call (gik) the metric tensor. The matrices (gik) and (gik) are diagonal for rectan-
gular coordinates, but not for oblique coordinates. With Cartesian coordinates, they
are unit matrices.
The indices of all tensors can be raised or lowered using the tensors gik and gik,
as we have seen already in Sect. 1.2.2 for vectors. Similarly,
T ik =

j
gi j T k
j =

jl
gi j gkl Tjl ,
and similarly, Tik = 
jl gi jgklT jl.
If an equation holds in Cartesian coordinates and if it holds as a tensor equation,
then it holds also in general coordinates. If a tensor of second rank is symmetric or
antisymmetric, T ik = ±T ki, then it has this property in every coordinate system.
The (scalar) triple product of the three base vectors g1, g2, g3 is denoted by ε123.
Generally, we have
εi jk ≡gi · (g j × gk) = ∂r
∂xi ·

 ∂r
∂x j × ∂r
∂xk

=
∂(x, y, z)
∂(xi, x j, xk) .
This is the totally anti-symmetric (Levi-Civita) tensor of third rank. Under a change
of coordinates, εi jk transforms like a tensor with three lower indices and changes
sign for the interchange of two indices. Therefore, we only need to evaluate ε123.
This component can be traced back to the determinant of (gik) because, according
to p. 5, we have

1.2 Coordinates
37
{gi · (g j × gk)}2 =
						
gi · gi gi · g j gi · gk
g j · gi g j · g j g j · gk
gk · gi gk · g j gk · gk
						
.
The (scalar) triple product of three real vectors is always real, and only zero if they are
coplanar (in which case the coordinates would be useless). Therefore, the determinant
is positive. We thus have
ε123 = ±√g ,
with g ≡det (gik) > 0 ,
where the plus sign corresponds to a right-handed coordinate system and the minus
sign to a left-handed one. (In particular, for a “reﬂection at the origin”, i.e., for
xi →−xi for all i, the sign of ε123 switches.) In addition,
εi jk = gi · (g j × gk) = ∂(xi, x j, xk)
∂(x, y, z)
,
and hence, according to p. 5,
εi jk εlmn =
						
δl
i δm
i δn
i
δl
j δm
j δn
j
δl
k δm
k δn
k
						
.
We deduce that ε123 ε123 = 1, but also

i
εi jk εimn =
				
δm
j δn
j
δm
k δn
k
				
and

i j
εi jk εi jn = 2 δn
k .
This equation is often useful.
The last paragraph is true only in three-dimensional space. Only there is the
vector product determined uniquely—otherwise the direction perpendicular to two
given directions is not determined. (But a totally antisymmetric tensor can also be
introduced for spaces of different dimensions via the functional determinant.)
Hence, in three dimensions we have
gk × gl =

i
gi εikl
and
a × b =

ikl
gi ak bl εikl .

38
1
Basics of Experience
The volume element is the parallelepiped spanned by the line elements (∂r/∂xi) dxi,
dV = |g1 · (g2 × g3) dx1 dx2 dx3| = |ε123 dx1 dx2 dx3| = √g |dx1 dx2 dx3|
=
			 ∂(x, y, z)
∂(x1, x2, x3) dx1 dx2 dx3 			 .
In addition to |dx1 dx2 dx3|, the functional determinant of the associated coordinates
appears.
The area element df(1) is related to the vector g1 which is perpendicular to the
area x1 = const. of the parallelepiped. Its scalar product with the vector g1 dx1 results
in ε123 dx1 dx2 dx3. Hence, we infer that
df(1) = g2 × g3 dx2 dx3 = ε123 g1 dx2 dx3 ,
with the value d f (1) =

g g11 |dx2 dx3|. As we shall soon see, these expressions
are useful for vector analysis—and, by the way, also for relativity theory. (Of course,
cyclic permutation of the three numbers 1, 2, 3 is allowed in this paragraph.)
1.2.5
Gradient, Divergence, and Rotation in General
Coordinates
For general coordinates, we ﬁnd the expressions
∇ψ =

i
gi (gi · ∇ψ) =

i
gi ∂ψ
∂xi ,
∇· a = lim
V →0
1
V

(V )
df · a =
1
√g

i
∂
∂xi (√g ai) ,
	ψ =
1
√g

ik
∂
∂xi

√g gik ∂ψ
∂xk

.
However, the corresponding surface integrals for gradient and rotation are not
useful here, because df(i) can change its direction. Therefore, for the still miss-
ing curl density, we start from Stokes’s theorem, viz.,

df · (∇× a) =

dr · a =
 
i ai dxi, and hence infer the equation √g g1 · (∇× a) = ∂a3/∂x2 −∂a2/∂x3.
Since √g ε123 = −√g ε132 = 1, we may also write √g 
kl ε1kl ∂al/∂xk for the
right-hand side:
∇× a =

ikl
εikl gi
∂al
∂xk .
Now we have all the quantities mentioned in the title (and the Laplace operator) in
general coordinates.

1.2 Coordinates
39
Fig. 1.12 Spherical coordinates r, θ, ϕ and their unit vectors, with g1 = er = r/r (red), g2 = r eθ
(blue), and g3 = r sin θ eϕ (green). Here, the angles ϕ and θ correspond to the “meridian” and
“latitude”, respectively, in geodesy. However, the polar distance θ is measured from the north pole
(always positive), and the “latitude” from the equator
For rectangular coordinates, much is simpliﬁed here. In particular, (gik) and (gik)
are diagonal, and gi and gi have the same direction ei. Only their lengths are different:
ei = gi
gi
= gi gi ,
with gi
2 = gii = 1
gii
and
gi > 0 .
Hence, dr = 
i ei gi dxi and √g = g1 g2 g3, together with ai = (a · ei) gi and
ai = (a · ei) / gi. We thus obtain
∇ψ =

i
ei
1
gi
∂ψ
∂xi ,
∇· a =
1
g1g2g3

i
∂g1g2g3 ai
∂xi
,
	ψ =
1
g1g2g3

i
∂
∂xi
g1g2g3
gi 2
∂ψ
∂xi ,
e1 · (∇× a) =
1
g2g3

 ∂a3
∂x2 −∂a2
∂x3

(and cyclic permutations).
The most important examples are, on the one hand, spherical coordinates, for which
dr = er dr + eθ r dθ + eϕ r sin θ dϕ (see Fig. 1.12):
∇ψ = er
∂ψ
∂r + eθ
1
r
∂ψ
∂θ + eϕ
1
r sin θ
∂ψ
∂ϕ ,
∇· a = 1
r2
∂r2ar
∂r
+
1
r sin θ
∂sin θ aθ
∂θ
+
1
r sin θ
∂aϕ
∂ϕ ,

40
1
Basics of Experience
Fig. 1.13 Cylindrical coordinates R, ϕ, z and their unit vectors. Instead of the Cartesian coordinates
x, y, the polar coordinates R and ϕ appear, so g3 = ez (green), g1 = eR = R/R (red), and g2 =
R eϕ = ez × R (blue)
	ψ = 1
r2
∂
∂r r2 ∂ψ
∂r +
1
r2 sin θ
∂
∂θ sin θ ∂ψ
∂θ +
1
r2 sin2 θ
∂2ψ
∂ϕ2 ,
∇× a = er
1
r sin θ

∂sin θ aϕ
∂θ
−∂aθ
∂ϕ

+ eθ
1
r

1
sin θ
∂ar
∂ϕ −∂raϕ
∂r

+ eϕ
1
r

∂raθ
∂r
−∂ar
∂θ

,
and on the other hand, cylindrical coordinates, for which dr = eR dR + eϕ R dϕ +
ez dz (see Fig. 1.13):
∇ψ = eR
∂ψ
∂R + eϕ
1
R
∂ψ
∂ϕ + ez
∂ψ
∂z ,
∇· a = 1
R
∂R aR
∂R
+ 1
R
∂aϕ
∂ϕ + ∂az
∂z ,
	ψ = 1
R
∂
∂R R ∂ψ
∂R + 1
R2
∂2ψ
∂ϕ2 + ∂2ψ
∂z2 ,
∇× a = eR

 1
R
∂az
∂ϕ −∂aϕ
∂z

+ eϕ

∂aR
∂z −∂az
∂R

+ ez
1
R

∂R aϕ
∂R
−∂aR
∂ϕ

.
In many cases, the ﬁelds ψ or a depend only on r (isotropy) or R (cylindrical
symmetry), respectively—then we need only ordinary derivatives in spherical or
cylindrical coordinates, instead of partial derivatives.
For rectilinear coordinates there are also simpliﬁcations with constant base vec-
tors, because then g remains the same everywhere:

1.2 Coordinates
41
(∇ψ)i = ∂ψ
∂xi ,
∇· a =

i
∂ai
∂xi ,
	ψ =

ik
gik
∂2ψ
∂xi ∂xk ,
(∇×a)i =

kl
εikl ∂al
∂xk ,
e.g., (∇×a)1 =
1
√g

 ∂a3
∂x2 −∂a2
∂x3

.
The next section should only be read by those who want to enter into more detail—
it is not needed to understand the following. Section 1.2.7 will be important only for
thermodynamics.
1.2.6
Tensor Extension, Christoffel Symbols
In deriving a gradient ﬁeld from a scalar ﬁeld, the rank of a tensor increases by one.
This tensor extension through differentiation also arises for tensors of higher rank,
but in this case variable base vectors require additional terms. In particular, we have
∂gk
∂xl =

i
gi 
gi · ∂gk
∂xl

=

i
gi {kl, i}
=

i
gi

gi · ∂gk
∂xl

=

i
gi
 i
kl

,
with the Christoffel symbols of the ﬁrst kind
{kl, i} ≡∂gk
∂xl · gi =
∂2r
∂xk ∂xl · gi = ∂gik
∂xl −gk · ∂gi
∂xl = {lk, i}
= ∂gik
∂xl −{il, k} = 1
2

∂gik
∂xl + ∂gil
∂xk −∂gkl
∂xi

and the Christoffel symbols of the second kind
 i
kl

≡∂gk
∂xl · gi =
∂2r
∂xk ∂xl · gi =
 i
lk

=

j
gi j {kl, j} .
Despite the last equation, the new symbols are generally not tensors of third rank,
because they contain second derivatives. Therefore, we shall avoid the notations Γikl
for {kl, i} and Γ i
kl for {i
kl}.
From these equations, it follows immediately that

42
1
Basics of Experience
1
2
∂gkk
∂xl = {lk, k} = {kl, k} .
For rectangular coordinates, all gik with i ̸= k vanish. If 1
2 ∂gkk/∂xl = gkk { k
kl} holds,
andinadditionfor k ̸= l, this is equal to−{kk, l} = −gll { l
kk}, because gkl = 0. Since
gi · gk is constant, we have ﬁnally
∂gk
∂xl =

i
gi
gi · ∂gk
∂xl

= −

i
gi
∂gi
∂xl · gk
= −

i
gi k
il

.
For the derivatives of the vector ﬁeld, we have
∂a
∂xl =

k
gk
gk · ∂a
∂xl

=

k
gk

gk · ∂a
∂xl

.
These coefﬁcients are referred to as covariant derivatives:
ak;l ≡gk · ∂a
∂xl = ∂ak
∂xl −a · ∂gk
∂xl = ∂ak
∂xl −

i
ai
 i
kl

,
ak
;l ≡gk · ∂a
∂xl = ∂ak
∂xl −a · ∂gk
∂xl = ∂ak
∂xl +

i
ai k
il

.
They are clearly tensors of second rank, obtained by differentiation from tensors of
ﬁrst rank.
These observations can be applied to the velocity and acceleration. Since
dg j
dt =

i
∂g j
∂xi
dxi
dt =

ik
gk
 k
i j
 dxi
dt ,
we obtain
v ≡dr
dt =

k
∂r
∂xk
dxk
dt =

k
gk
dxk
dt
=⇒
vk = dxk
dt ,
and since a = ˙v = 
k(gk ¨xk + ˙gk ˙xk), we ﬁnd
ak = d2xk
dt2 +

i j
 k
i j
 dxi
dt
dx j
dt .
For motion along the coordinate line xk, the ﬁrst term ¨xk here describes the tangential
accelerations and the rest the normal accelerations. This decomposition was already
explained on p. 7.

1.2 Coordinates
43
1.2.7
Reformulation of Partial Differential Quotients
In the analysis of functions of multiple variables, partial derivatives appear. Here we
restrict to two variables for reasons of simplicity, but generalization is straightfor-
ward. The main interest here is in the transformation to new variables (coordinates).
For a function f of the two variables x and y, we have
d f (x, y) = ∂f (x, y)
∂x
dx + ∂f (x, y)
∂y
dy .
It is common to leave out the arguments of f and instead attach the ﬁxed parameter
to the differential quotient as a lower index. Hence the equation appears in the form
d f =

∂f
∂x

ydx +

∂f
∂y

xdy ,
with

∂f
∂x

f = 0 and

∂f
∂f

x = 1 .
From here various relations can be derived.
If we divide by d f and form the limit d f →0 with constant y or x, respectively,
i.e., for dy = 0 or dx = 0, respectively, then
1 =

∂f
∂x

y

∂x
∂f

y =

∂f
∂y

x

∂y
∂f

x .
The derivative of a function is thus equal to the reciprocal of the derivative of
the inverse function, as suggested by the notation (due to Leibniz). On the other
hand, if we divide by dy and form the limit dy →0 for ﬁxed f , we have 0 =
(∂f/∂x)y (∂x/∂y) f + (∂f/∂y)x, whence the noteworthy equation

∂f
∂y

x = −

∂f
∂x

y

∂x
∂y

f .
We thus see that the ﬁxed and the changed variable can be exchanged. This equation
may also be written in the form (∂f/∂x)y (∂x/∂y) f (∂y/∂f )x = −1 if we consider
the reciprocal of (∂f/∂y)x.
If we replace a variable with a new one, e.g., y with g(x, y), then from
d f = (∂f/∂x)g dx + (∂f/∂g)x {(∂g/∂x)ydx + (∂g/∂y)xdy}, we may deduce the
two important equations

∂f
∂x

y =

∂f
∂x

g +

∂f
∂g

x

∂g
∂x

y
and

∂f
∂y

x =

∂f
∂g

x

∂g
∂y

x .

44
1
Basics of Experience
According to the ﬁrst equation, the ﬁxed variable can be changed. The second cor-
responds to the chain rule for ordinary derivatives.
In the last product, if we swap the ﬁxed and adjustable pair of variables and then
apply the chain rule twice, it follows that

∂f
∂g

x

∂g
∂f

y =

∂x
∂y

f

∂y
∂x

g .
Here, the pair ( f, g) is exchanged with the pair (x, y). By the way,

∂f
∂g

x

∂g
∂f

y +

∂y
∂g

x

∂g
∂y

f = 1 .
For the proof, we can trace (∂f/∂g)x back to (∂f/∂g)y and then exploit the equations
above.
If in (∂f/∂g)x we use the chain rule with the variable y and then in (∂y/∂g)x
exchange the ﬁxed and the adjustable variable, we also have

∂f
∂g

x

∂g
∂x

y = −

∂f
∂y

x

∂y
∂x

g .
This corresponds to the replacement y ↔g. In addition,

∂f
∂g

x

∂g
∂y

f = −

∂f
∂x

g

∂x
∂y

f .
This can be understood by replacing x ↔g in the ﬁrst factor.
1.3
Measurements and Errors
1.3.1
Introduction
The search for laws prepares the ground on which the principles of nature are built.
We generalize by relating comparable things. Of course, this has its limitations. When
are two things equal to each other, and when are they only similar? The following is
important for all measurements, but also for quantum theory and for thermodynamics
and statistics.
We consider an arbitrary physical quantity which we assume does not change with
time and can be measured repeatedly, e.g., the length of a rod or the oscillation period
of a pendulum. Each measurement is carried out in terms of a “multiple of a scale
unit”. It may be that a tenth of the unit can be estimated, but certainly not essentially
ﬁner divisions. An uncertainty is therefore attached to each of our measured values,
and this uncertainty can be estimated rather simply.

1.3 Measurements and Errors
45
Fig. 1.14 Frequency distribution of the measurement series {xn} mentioned in the text. The more
often the same value is measured, the higher the associated column (blue). The adjusted red bell-
shaped curve is symmetrical with respect to the mean value (x = 10.183), and the half-width
(	x = 0.058) corresponds to the “measurement error”
It is more difﬁcult to ﬁnd a statement about how well an instrument is adjusted
and whether there are further systematic errors. We will not deal with these questions
here, but we do want to be able to estimate the bounds on the error from the statistical
ﬂuctuations of our measured data.
In particular, if we repeat our measurement in order to ensure against erroneous
readings, then the values xn (n ∈{1, . . . , N}) may not all be equal, e.g., we may
ﬁnd three times 10.1 scale units (that is, three values with 10.05<xn <10.15), eight
times 10.2 (eight values with 10.15 < xn < 10.25), and one 10.3 (with 10.25 < xn <
10.35) in an arbitrary order. Apparently, there are always “measurement errors”, the
origin of which we do not know. (Systematic errors can be estimated separately.)
Therefore, we have to assign a greater uncertainty than the assumed scale ﬁneness
to the results of our measurements.
Hence, from the N readings {xn} of our measurement, we would like to determine
a measurement result with error estimate in the form x ± 	x. For the example
mentioned, the result is 10.183 ± 0.058, as will be shown shortly, often abbreviated
to 10.183(58). This example is shown in Fig. 1.14. The error estimate here presents
only a frame for the actual error: improved measurement readings may also lie
outside the error limits given previously. If we compare, e.g., the error analysis for
the fundamental constants of the year 1999 (see p. 623) with the ones from 1986, we
obtain Table 1.3. Only the value for the Boltzmann constant k has remained within
the old error limits. The Avogadro constant (NA) and Planck constant (h) came to lie
outside the old limits, as did the value for the elementary charge e. The error limits
for the gravitational constant G even went up by more than two orders.
It is pointless to give the error to more than the two leading digits, and the mean
value more exactly than the error. This is forgotten by many laypeople, if they com-
municate their computational result “exactly”, with far too many digits.

46
1
Basics of Experience
Table 1.3 Improvement of precision with time
Quantity
Relative uncertainty
1986 in 10−7
Relative change
1999/1986 in 10−7
Relative uncertainty
1999 in 10−7
e
±3
−5.6
±0.4
h
±6
−10.3
±0.8
NA
±6
+8.6
±0.8
k
±87
−56
±17
1.3.2
Mean Value and Average Error
After N measurements of x, we have a sequence of measured values {x1, . . . , xN}.
These values are generally not all equal, but we want to assume that their ﬂuctuations
are purely random, and we shall only deal with such errors in the following.
Since none of the measurement readings should be preferred, the true value x0 is
assumed to be near the mean value
x ≡
1
N
N

n=1
xn ,
because deviations may occur equally often to higher or lower values: x0 ≈x. Our
best estimate for the true value x0 is the mean value x.
Here, the less the values xn deviate from x, the more we trust the approximation
x0 ≈x. From the ﬂuctuations, we deduce a measure 	x for the uncertainty in our
estimate. To do this, we take the squares (xn −x)2 of the deviations rather than their
absolute values |xn −x |, because the squares are differentiable, while the absolute
values are not, something we shall exploit in Sect. 1.3.7. However, we may take their
mean value
(x −x)2 = x2 −2x x + x 2 = x2 −2x x + x 2 = x2 −x 2
as a measure for the uncertainty only in the limit of many measurements, not just a
small number of measurements. So, for a single measurement nothing whatsoever
can be said about the ﬂuctuations. For a second measurement, we would have only
a ﬁrst clue about the ﬂuctuations. In fact, we shall set
(	x)2 =
1
N −1
N

n=1
(xn −x)2 =
N
N −1 (x −x)2 ,
as will be justiﬁed in the following sections. Here we shall rely on a simple special
case of the law of error propagation. But this law can also be proven rather easily in its
general form and will be needed for other purposes. Therefore, we prove it generally
now, whereupon the last equation can be derived easily. To this end, however, we
have to consider general properties of error distributions.

1.3 Measurements and Errors
47
1.3.3
Error Distribution
We presume that the errors are distributed in a purely random manner. Then the
error probability can be derived from sufﬁciently many readings of the measure-
ment (N ≫1). From the relative occurrences of the single values, we can determine
the probability ρ(ε) dε that the error lies between ε and ε + dε. The probability
density ρ(ε) is characterized essentially by the average error σ, as the following
considerations show.
Each probability distribution ρ has to be normalized to unity and may not take neg-
ative values:

ρ(ε) dε = 1 and ρ(ε) ≥0 for all ε (∈R). In addition, we expect ρ(ε)
to be essentially different from zero only for ε ≈0 and to tend to zero monotonically
with increasing |ε|. The distribution is also assumed to be an even function, at least in
the important region around the zero point: ρ(ε) = ρ(−ε). Hence,

ε ρ(ε) dε = 0.
The next important feature is the width of the distribution. It can be measured with
the second moment, the average of the squared errors σ (≥0), also called the mean
square ﬂuctuation or variance,
σ 2 ≡

ε2 ρ(ε) dε .
Note, however, that the mean square error is not ﬁnite for all allowable error distri-
butions, e.g., for the Lorentz distribution ρ(ε) = γ/{π(ε2 + γ 2)}, which is instead
characterized by half the Lorentz half-width γ —more on that in the discussion around
Fig. 5.6.
From the probability distribution ρ(ε), we can evaluate the expectation value
⟨f ⟩of any function f (ε). Each value of the function is summed with its associated
weight:
⟨f ⟩≡
 ∞
−∞
f (ε) ρ(ε) dε .
In particular, ⟨εn⟩=

εn ρ(ε) dε.
For the error distribution in the following we use only on the properties ⟨ε0⟩= 1,
⟨ε1⟩= 0, and ⟨ε2⟩= σ 2, among which only the middle one might be disputed—the
ﬁrst is obvious by normalization, the last ﬁxes the average error σ.
If, however, we want to write down the probability W(λ) for an error |ε| ≤λσ,
i.e.,
W(λ) =
 λσ
−λσ
ρ(ε) dε ,
we have to know ρ(ε) in more detail. Detailed statistical investigations suggest the
normal or Gaussian distribution—this will become apparent in Sect. 6.1.4:
ρ(ε) = exp (−1
2ε2/σ 2)
√
2π σ
.

48
1
Basics of Experience
Fig. 1.15 Normal distribution of the error. Gauss function (bell-shaped curve). In order for all
average errors σ (>0) to result in the same curve, the probability ρ for the error ε times the average
error as a function of the ratio ε/σ is shown here. The area is unity for all σ
Fig. 1.16 Error integral W(λ) (blue): the probability of errors with |ε| ≤λσ (σ is the average
error). The dashed red curve is the function tanh(√2/πλ) for comparison
Figure 1.15 shows this function and Fig. 1.16 the associated error integral W(λ).
The error integral is related to the error function
erfx ≡
2
√π
 x
0
exp (−y2) dy = W(
√
2 x) ,
for which the following expansions are useful:
erfx =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
2
√π
∞

n=0
(−)n
n!
x2n+1
2n + 1 ,
1 −exp (−x2)
√π x

1 −

n=0
(−)n (2n + 1)!!
(2x2)n+1

for x ≫1.

1.3 Measurements and Errors
49
The second series is semi-convergent, i.e., it does not converge for n →∞, but
approximates the function sufﬁciently well for ﬁnite n (< x).
From Fig. 1.16, we see that, for the normal distribution, slightly more than 2
3 of
all values have an error |ε| ≤σ and barely 5% an error |ε| > 2σ.
1.3.4
Error Propagation
We now start from K physical quantities xk with average errors σk and consider the
derived quantity y = f (x1, . . . , xK). Here all the quantities xk will be independent
of each other. What is then the average error in y?
To begin with, the error ε in f (x1, . . . , xK) is to ﬁrst order
ε =
K

k=1
∂f
∂xk
εk ,
and hence
σ 2 = ⟨ε2⟩
=

· · ·
 ∞
−∞

 K

k=1
∂f
∂xk
εk
2
ρ(ε1, . . . , εK) dε1 · · · dεK
=
 K

k=1
∂f
∂xk
εk ·
K

l=1
∂f
∂xl
εl

=
K

k,l=1
∂f
∂xk
∂f
∂xl
⟨εk εl⟩.
Since the quantities xk and xl should not depend upon each other, they are not
correlated to each other (the property xl does not care how large xk is—correlations
will be investigated in more detail in Sect. 6.1.5). With ⟨εk2⟩= σk2 this leads to
⟨εkεl⟩=
 ⟨εk⟩⟨εl⟩for k ̸= l ,
σk2
for k = l .
Here, ⟨εk⟩= 0 holds for all k (andl). Therefore, the law of error propagation follows:
σ 2 =
K

k=1
 ∂f
∂xk
2
σk
2 .
In the proof, no normally distributed errors were necessary—thus other distributions
with the properties ⟨εk0⟩= 1, ⟨εk1⟩= 0, and ⟨εk2⟩= σk2 deliver the error propa-
gation law and with it the basis for all further proofs in this section. In particular,
we may invoke this law for repeated measurements of the same quantity, as we shall
now do.

50
1
Basics of Experience
1.3.5
Finite Measurement Series and Their Average Error
If we consider the expression
⟨x⟩≈x ≡
1
N
N

n=1
xn
as x = f (x1, . . . , xN), then we can use it in the law of error propagation and deduce
that ∂f/∂xn = N −1. Hence, all single measurements enter into the error estimate
with the same weight—as already for the estimated value x0.
Inordertodeterminetheerrorσn,wethinkofanaverageoverseveralmeasurement
series, each with N measurements. In this way, we can introduce the average error of
the single measurement and ﬁnd that all single measurements have the same average
error 	x. Therefore, the law of error propagation for N equal terms N −2 (	x)2
delivers
(	x)2 = N · N −2 (	x)2 = (	x)2
N
.
The average error 	x in the mean value of the measurement series is thus the
√
N th
part of the average error in a single measurement: the more often measurements are
made, the more accurate is the determination of the mean value. However, because
of the square root factors, the accuracy can be increased only rather slowly.
Since we do not know the true value x0 itself, but only its approximation x, we
still have to account for its uncertainty 	x in order to determine the average error of
the single measurement:
(	x)2 = (x −x0)2 = (x −x + x −x0)2 = (x −x)2 + 2 (x −x) (x −x0) + (x −x0)2 .
Here, x −x = x −x = 0 and thus (x −x)2 = x2 −x2 is rather easy to evaluate. For
(x −x0)2,wetake(	x)2 = (	x)2/N.Hence,because1 −N −1 = N −1(N −1),the
average error of the single measurement is
(	x)2 =
N
N −1 (x −x)2 ,
as claimed previously (see p. 46). And so we have the announced proof. For sufﬁ-
ciently large N, we may write (	x)2 = x2 −x2. The expression 	x is referred to
as the uncertainty of x in quantum theory (see p. 275).
1.3.6
Error Analysis
How should we modify the result obtained so far if the same quantity is measured
in different ways: ﬁrst as x1 ± 	x1, then as x2 ± 	x2, and so on? What is then the
most probable value for x0, and what average error does it have?

1.3 Measurements and Errors
51
If the readings of the measurement were taken with the same instrument and
equally carefully, the difference in the average errors stems from values xn from
measurement series of different lengths. According to the last section, the average
error of every single measurement in such a measurement series should be equal to
	xn
√Nn, and this independently of n in each of the measurement series. Therefore,
the mentioned values xn should contribute with the weight
ρn =
Nn

k Nk
=
1
(	xn)2

k
1
(	xk)2 ,
whence x = 
n ρn xn is the properly weighted mean value. The error propagation
law delivers
σ 2 =

n
ρn
2 σn
2 =
1
(
k(	xk)−2)2

n
1
(	xn)4 (	xn)2 =

n(	xn)−2
(
k(	xk)−2)2
=
1

n(	xn)−2
=⇒
1
(	x)2 =

n
1
(	xn)2 .
The more detailed the readings of the measurement, the more important they are for
the mean value and for the (un)certainty of the results. These considerations are only
then valid without restriction, if the values are compatible with each other within
their error limits. If they lie further apart from each other, then we have to take
(	x)2 =
1
N −1
1

n(	xn)−2

n
(xn −x)2
(	xn)2
.
Note that, if the values xn do not lie within the error limits, then systematic errors
may be involved.
Thus, these two equations answer the questions raised in the general case, where
measurements are taken with different instruments and different levels of care: to
each value xn, we must attach the relative weight 1/(	xn)2.
1.3.7
Method of Least Squares
A further generalization is necessary if the readings of measurement happen to be
along a straight line, but scatter about it due to random errors. What are then the
best values a and b for {yn = a xn + b}? More generally, we can ﬁt a power series,
a Fourier series, or some series of known functions.
We always want to determine the readings of measurements as precisely as pos-
sible, in order to make the average error as small as possible. This requirement is
effective under general conditions. Thus the values a and b of the ﬁtting line are to
be determined from the conditions

52
1
Basics of Experience
Fig. 1.17 Example of a ﬁtting line through 12 pairs of measurement values (•). The continuous line
shows y = ax + b,andthedashedlines showtheupperandlowererrorlimits(a+	a) x + (b+	b)
and (a−	a) x + (b−	b), respectively. (In a beginners’ lab course, we can thus establish Hooke’s
law by showing how the length y of a copper wire depends linearly upon the load x)
N

n=1
(yn −axn −b)2 = min(a, b) ,
i.e., ∂N
n=1(yn −axn −b)2/∂a = 0 = ∂N
n=1(yn −axn −b)2/∂b. From the last,
we have
b =

n(yn −axn)
N
= y −ax ,
and from the condition above,
a = xy −x y
x2 −x2 = (x −x)(y −y)
(x −x)2
.
Here, the ﬁrst fraction is easy to evaluate, the last less easy to interpret—hence the
reformulation. We have thus answered our question as to which values for a and b
are the best. For an example, see Fig. 1.17.
To calculate the average errors 	a and 	b, we have to consider the fact that
pairs of values (xn, yn) are always associated and only the error in each pair counts,
not the error in xn and yn separately. Therefore, for reasons of simplicity we take
the error in xn as an additional error in yn and then take a and b in the law of error
propagation as functions of yn. From b = y −ax, it then follows that
(	b)2 = (	y)2 + x2 (	a)2 .
From a = {
n(xn −x) yn}/{N(x2 −x2)}, we obtain ∂a/∂yn = (xn −x)/{N(x2 −
x2)}, and thus, ﬁnally,

1.3 Measurements and Errors
53
(	a)2 =
N

n=1
(xn −x)2
N 2 (x2 −x2)2 (	yn)2 .
If all errors 	yn in the pairs of values are equally large, then (	y)2 = (	y)2/N and
hence
(	a)2 =
(	y)2
N (x2 −x2)
and
(	b)2 = x2 (	a)2 .
We still lack a prescription for calculating the average error 	y in a single measure-
ment. From the original equation y = ax + b, where two pairs of values are now
necessary in order to determine a and b, we have
(	y)2 =
1
N −2
N

n=1
(yn −a xn −b)2 .
More generally, with K parameters we would have the denominator N −K, because
the equation (	x)2 = (	x)2/N in one dimension becomes (	y)2 = K (	y)2/N in
K dimensions, and this can then be used in (	y)2 = (y −y)2 + (	y)2.
List of Symbols
We stick closely to the recommendations of the International Union of Pure and
Applied Physics (IUPAP) and the Deutsches Institut für Normung (DIN). These
are listed in Symbole, Einheiten und Nomenklatur in der Physik (Physik-Verlag,
Weinheim 1980) and are marked here with an asterisk. However, one and the same
symbol may represent different quantities in different branches of physics. Therefore,
we have to divide the list of symbols into different parts (Table 1.4).
Table 1.4 Standard notation and symbols
Symbol
Name
Page reference
∗
t
Time
1
∗
r
Position vector
1
∗
V
Volume
9
(V )
Surface of a volume V
9
∗
A
Area
9
(A)
Boundary of an area A
9
1
dr
Path element vector
7
1
df
Surface element vector
9
1
dV , d3r
Volume element
9, 24
∗
a · b
Scalar product of a and b
3
∗
a × b
Vector product of a and b
4
(continued)

54
1
Basics of Experience
Table 1.4 (continued)
Symbol
Name
Page reference
∗
a b
Dyadic product of a and b
11
∗
ex
Unit vector x/x
3
∗2
∇
Nabla operator
10
∗2
∇φ
Gradient of a scalar ﬁeld φ
10
∗2
∇· a
Divergence of a vector ﬁeld a
11
∗2
∇× a
Rotation (curl) of a vector ﬁeld a
13
∇A · a
Area divergence of a vector ﬁeld a
27
∇A × a
Area rotation of a vector ﬁeld a
27
∗
	
Laplace operator
15
∗
δik
Kronecker symbol
18
∗
δ(x)
Dirac delta function
18
∗
δx
Variation of x
58, 139
∗
ε(x)
Discontinuity function (theta function, step function)
18
P I
Principal value of I
19
∗
k
Wave vector
24
∗
D
Transpose of the matrix D
5
∗
D−1
Inverse of the matrix D
29
∗
D∗
Conjugate of the matrix D
29
∗
D†
Adjoint of the matrix D
29
∗
det D
Determinant of the matrix D
5
∗
tr D
Trace of the matrix D
36
gi
Covariant base vector ∂r/∂xi
31
g i
Contravariant base vector ∇xi
31
ai
Covariant component of a
33
ai
Contravariant component of a
33
∗
x
Mean value of x
46
	x
Uncertainty in x
46
1Total differentials are written with an upright d rather than an italic d. We stick to this throughout.
2In the recommended notation there is no vector arrow above ∇, even though it is a vector operator.
Suggestions for Further Reading
1. J. Arfken, H.J. Weber, Mathematical Methods for Physicists, 6th edn. (Elsevier Academic,
Burlington MA, 2005)
2. E. Ph Blanchard, Brüning: Mathematical Methods in Physics: Distributions, Hilbert Space
Operators, and Variational Methods (Springer Science + Business, Media, 2003)
3. S. Hassani, Mathematical Physics-A Modern Introduction to Its Foundations (Springer, Berlin,
2013)
4. A. Sommerfeld: Lectures on Theoretical Physics 6-Partial Differential Equations in Physics
(Academic, London, 1949/1953)
5. H. Triebel, Analysis and Mathematical Physics (Springer, Berlin, 1986)

Chapter 2
Classical Mechanics
2.1
Basic Concepts
2.1.1
Force and Counter-Force
The best known example of a force is the gravitational force. If we let go of our book,
it falls downwards. The Earth attracts it. Only with a counter-force can we prevent
it from falling, as we clearly sense when we are holding it. Instead of our hand, we
can use something to ﬁx it in place. We can even measure the counter-force with a
spring balance, e.g., in the unit of force called the newton, denoted N=kg m/s2.
Each force has a strength and a direction and can be represented by a vector—if
several forces act on the same point mass, then the total force is found using the
addition law for vectors. As long as our book is at rest, the gravitational and counter-
force cancel each other and the total force vanishes. Therefore, the book remains in
equilibrium.
Forces act between bodies. In the simplest case, we consider only two bodies. It
is this case to which Newton’s third law refers: Two bodies act on each other with
forces of equal strength, but with opposite direction. This law is often phrased also as
the equation “force = counter-force” or “action = reaction”, even though they refer
only to their moduli. If body j acts on body i with the force Fi j, then
Fi j = −F ji .
According to this, no body is preferred over any another. They are all on an equal
footing.
We often have to deal with central forces. Then,
Fi j = ∓F(ri j) ei j ,
with ei j ≡ri j
ri j
and ri j ≡ri −r j = −r ji ,
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5_2
55

56
2
Classical Mechanics
Fig. 2.1 The force can only be derived from a potential energy if the work needed to move against
the force from the point r0 to point r1 does not depend upon the path between these points, i.e.,
only for

F · dr = 0 for all closed paths
where we have a minus sign for an attractive force and a plus sign for a repulsive
force (see Fig. 1.1). Clearly, they have the required symmetry.
The force between two magnetic dipoles mi and m j is not a central force, but a
tensor force:
Fi j = 3μ0
4π
mi ·ei j
ri j 4
m j + m j ·ei j
ri j 4
mi + mi ·m j −5 mi ·ei j m j ·ei j
ri j 4
ei j

.
This expression is derived in Sect. 3.2.9 and presented in Fig. 3.12. It depends on the
directions of the three vectors mi, m j, and ri j, but also has the required symmetry.
Newton’s third law also holds for changing positions ri j(t), but we shall deal with
this in the next section. For the time being, we restrict ourselves to statics. The total
force of the bodies j on a test body i is thus
Fi =

j
Fi j .
This force will generally change with the position ri of the test body, if the other
positions r j are kept ﬁxed. We now want to investigate this in more detail and write
r instead of ri.
2.1.2
Work and Potential Energy
It may be easier to work with a scalar ﬁeld than with a vector ﬁeld. Therefore, we
derive the force ﬁeld F(r) from a scalar ﬁeld, viz., the potential energy V (r) :
F = −∇V .
But for this to work, since ∇× ∇V = 0, F has to be curl-free, i.e., the integral

F · dr has to vanish along each closed path. We conclude that a potential energy
can only be introduced if the work
A ≡
 r1
r0
F · dr

2.1 Basic Concepts
57
depends solely upon the initial and ﬁnal points r0 and r1 of the path, but not on
the actual path taken in-between (see Fig. 2.1). (Instead of the abbreviation A, the
symbol W is often used, but we shall use W in Sect. 2.4.7 for the action function.)
Generally, on a very short path dr, an amount of work δA = F · dr is done. Here we
write δA instead of dA, because δA is a very small (inﬁnitesimally small) quantity,
but not necessarily a differential one. It is only a differential quantity if there is a
potential energy, hence if F is curl-free and can be obtained by differentiation:
dV ≡∇V · dr = −F · dr ≡−δA .
For the example of the central and tensor forces mentioned in the last section, a
potential energy can be given, but it cannot for velocity-dependent forces, i.e., neither
for the frictional nor for the Lorentz force (acting on a moving charge in a magnetic
ﬁeld). We shall investigate these counter-examples in Sect. 2.3.4.
If there is a potential energy, then according to the equations above it is determined
only up to an additive constant. The zero of V can still be chosen at will and the
constant “adjusted” in some suitable way. The zero of V is set at the point of vanishing
force. If it vanishes for r →∞, then it follows that
V (r) = −
 r
∞
F(r ′) · dr ′ .
But it should be noted once again that this is unique only for ∇× F = 0, that is, only
then is there a potential energy.
For a homogeneous force ﬁeld, the force F does not depend on the position.
Then the expression V = −F · r ﬁts. Likewise, for a central force with F ∝rn, the
potential energy is easily found:
F = c rn r
r
⇐⇒
V = −c
n+1 rn+1 ,
if n ̸= −1, otherwise V = −c ln (r/r0), with an arbitrary gauge constant r0. Note
that V (∞) = 0 holds only for n < −1.
If we can derive the two-body force Fi j from a potential energy Vi j, we have (with
r j kept ﬁxed)
Fi j = −∇iVi j ,
with ∇iVi j · dri = dVi j .
Newton’s third law now delivers −∇iVi j = ∇jVji (on the right-hand side, ri is kept
ﬁxed and r j variable) with ∇i = −∇j, since ri j = −r ji here, so we have dri =
−dr j. Therefore, with a convenient gauge, we can obtain the symmetry
Vi j = Vji .
Hence a many-body problem has the total potential energy

58
2
Classical Mechanics
V =

i< j
Vi j = 1
2

i̸= j
Vi j ,
because each pair (i j) is to be counted only once. (It is often taken for granted that Vii
vanishes and the summation is then simply over i and j, without indicating i ̸= j.)
2.1.3
Constraints: Forces of Constraint, Virtual
Displacements, and Principle of Virtual Work
We can often replace forces by geometric constraints. If the test body has to remain
on a plane, we should decompose the force acting on it into its tangential and normal
components—because it is only the tangential component that is decisive for the
equilibrium (as long as there is no static friction, since this depends upon the normal
component). The normal component describes only how strongly the body presses
on the support, e.g., a sphere on a tabletop.
Geometrically conditioned forces are called forces of constraint Z. In equilibrium,
the external forces cancel, whence 
i Fi = 
i Zi. We now consider virtual changes
in the conﬁguration of an experimental setup. In our minds, we alter the positions
slightly, while respecting the constraints, in order to ﬁnd out how much of it is
rigid and how much is ﬂexible. These alterations (variations) will be denoted by
δr . If there is no perturbation due to static friction, then the forces of constraint are
perpendicular to the permitted alterations of position, and therefore the displacement
δr does not contribute to the work. Since Zi · δri = 0, we ﬁnd the extremely useful
principle of virtual work:

i
Fi · δri = 0 .
In equilibrium, the virtual work of the externally applied forces vanishes. We do
not need to calculate the forces of constraint here—instead, only the geometrical
constraints must be obeyed. If only curl-free forces are involved, then the associated
potential energy of the total system also sufﬁces. Equilibrium prevails if it does not
change under a virtual displacement: ∇V · δr ≡δV = 0.
For a lever, the virtual work can be evaluated in a particularly easy way with a
virtual rotation, because the length R of the lever arm does not change, and therefore
we may set δr = δϕ × R, if δϕ points in the direction of the axis of rotation (right-
hand rule) and if R points from the axis of rotation to the point where the force acts
(see Fig. 2.2). For the virtual work we obtain therefore
δA = F · δr = F · ( δϕ × R ) = ( R × F ) · δϕ = M · δϕ ,
with the torque M of the force F on the lever arm R deﬁned by

2.1 Basic Concepts
59
Fig. 2.2 Lever law. The rigid lever transmits all those forces to the axle bearing (open circle)
which do not have an angular momentum with respect to the axis—they are canceled by forces of
constraint, here by F∥. Equilibrium prevails, if the torque due to the force F and the counter-force
G cancel each other, as indicated by the two hyperbola branches
M ≡R × F .
Since δϕi for a rigid lever is the same everywhere, equilibrium prevails here if
the sum of the torques vanishes. The principle of virtual work then implies the lever
law

i
Mi = 0 ,
with
Mi = Ri × Fi .
The equilibrium of the lever depends upon the torques, i.e., the vector products of
lever arm and force.
2.1.4
General Coordinates and Forces
This section and the next actually touch upon the subject of Lagrangian mechanics,
wherein, by a clever choice of coordinates, problems can be made soluble which
otherwise would be intractable. In the static case, many things are much simpler
than for time-dependent phenomena. It is this that we want to exploit here, and then
begin by solving several examples (Problems 2.7–2.10), in order to get used to these
notions. The lever law introduces us to this way of thinking.
Very often the solubility of a problem depends on a choice of coordinates which
can lead to mathematical as well as physical simpliﬁcations. For example, for the two-
body problem we employ center-of-mass and relative coordinates, because the forces
only depend upon the relative coordinate. At best, we choose the coordinates such
that each constraint removes a variable and hence only the remaining ones survive as
variables, e.g., in the case of the lever, we use cylindrical coordinates, because then
the forces of constraint determine R and z, and only the angular coordinate ϕ can
vary. Then for an N-body problem, we do not need the 3N coordinates of the real
space, but only f ≤3N coordinates in the conﬁguration space. Here f is called the
number of degrees of freedom of the mechanical problem.

60
2
Classical Mechanics
In most textbooks, generalized coordinates are denoted by qk. But here we shall
adopt the notation xk used in relativity theory and lattice dynamics. This is explained
in detail in Sects. 1.2.2–1.2.5.
The variables
xk = xk(t, r1, . . . , rN) ,
with k ∈{1, . . . , f } and
f ≤3N ,
can be Cartesian coordinates, but also curvilinear (e.g., spherical or cylindrical)
coordinates, or even oblique ones—for which ∇xi is not perpendicular to ∇xk and
therefore we have to distinguish between xk and xk.
The f generalized coordinates xk (in addition to parameters like t) will describe
the given problem completely:
ri = ri(t, x1, . . . , x f ) ,
for all i ∈{1, . . . , N} .
Correspondingly, we have for the virtual displacements, keeping time ﬁxed,
δri =
f

k=1
∂ri
∂xk δxk ,
at δt = 0 ,
and for the principle of virtual work,
0 =
N

i=1
Fi · δri =
f

k=1
Fk δxk ,
with the generalized forces
Fk ≡
N

i=1
Fi · ∂ri
∂xk ,
or
Fk = −∂V
∂xk ,
if the external forces can be associated with a potential energy. The notation Fk with
lower index k corresponds to the convention of Sect. 1.2.2, while the indices i are
used here to count the particles. The xk do not need to be lengths and the Fk are not
necessarily forces in the usual sense, but Fk δxk has to be an energy. Thus, according
to the last section, the generalized force “torque” corresponds to the generalized
coordinate “angle”.
In static equilibrium, all the Fk are equal to zero if none of the xk depends upon the
others. However, the constraints do not always have such simple properties: not every
constraint ﬁxes a coordinate and leaves the remaining ones undetermined. Therefore,
we now want to treat a more general case.

2.1 Basic Concepts
61
2.1.5
Lagrangian Multipliers and Lagrange Equations
of the First Kind
If, for the moment, we do not consider the 3N −f constraints for N point masses,
then in addition to the f generalized coordinates xk introduced so far, 3N −f further
coordinates xκ (with κ ∈{ f + 1, . . . , 3N}) are still required, and these are in fact
determined by the constraints. We assume that these constraints are given in the form
of equations:
n(t, x1, . . . , x3N) = 0 ,
for all n ∈{1, . . . , 3N −f } .
Here, equations for differential forms sufﬁce, because only the following 3N −f
equations have to be valid for arbitrary parameter variations at ﬁxed time:
f

k=1
∂n
∂xk δxk +
3N

κ= f +1
∂n
∂xκ δxκ = 0 ,
with δt = 0 ,
where the coefﬁcients do not need to be differential quotients—this becomes nec-
essary, when we trace the forces back to a potential energy. Now we want to make
use of the fact that only the f variations δxk are free, but the remaining δxκ depend
upon the former and require the 3N −f Lagrangian (undetermined) multipliers λn
(one Lagrangian multiplier for each constraint) to satisfy
3N−f

n=1
λn
∂n
∂xκ = −Fκ ,
with
Fκ ≡
N

i=1
Fi · ∂ri
∂xκ .
This is an inhomogeneous linear system of 3N −f equations with the same number
κ of dependent variables. Once we have determined all Lagrangian multipliers λn
from this, then the relation

κ
Fκ δxκ = −

κn
λn
∂n
∂xκ δxκ =

nk
λn
∂n
∂xk δxk ,
at δt = 0 ,
implies the following expression for the principle of virtual work (with δt = 0):
N

i=1
Fi · δri =
f

k=1
Fk δxk +
3N

κ= f +1
Fκ δxκ =
f

k=1

Fk +
3N−f

n=1
λn
∂n
∂xk

δxk .
This has to vanish for arbitrary δxk. The bracket has to be zero for all f values k.
Here, the Lagrangian multipliers have to be chosen such that the bracket vanishes also
for the remaining 3N −f values κ. We thus have generally for all l ∈{1, . . . , 3N},

62
2
Classical Mechanics
Fl +
3N−f

n=1
λn
∂n
∂xl = 0 ,
with
Fl ≡
N

i=1
Fi · ∂ri
∂xl
at δt = 0 .
These are essentially the Lagrange equations of the ﬁrst kind. For time-dependent
problems, only the inertial forces are missing, and this will be treated later, in the
context of d’Alembert’s principle (Sect. 2.3.1).
We consider a plane problem as an example. Let z = 0 be given. Then we
can leave out the z-coordinate right away or, using the position vector r in three-
dimensional space, calculate with the constraint  = z = 0. With the coordinates
(x1, x2, x3) = (x, y, z), we have 0 = F1 = F2 = F3 + λ in equilibrium. Here, the
Lagrangian multiplier λ is equal to the force of constraint −F3, while the forces in
the plane have to vanish. (Further examples can be found in Problems 2.7–2.10.)
Since we could also have required λn n = 0 instead of the constraint n = 0,
only the product λn n has a physical meaning, but not the Lagrangian multiplier λn
itself.
If the external forces can be derived from a potential energy, then for all l ∈
1, . . . , 3N in equilibrium, we also have
Fl = −∂V
∂xl
and
∂
∂xl

V −
3N−f

n=1
λn n

= 0 .
Consequently, the forces of constraint are obsolete, if we subtract the constraints
with suitable Lagrangian multipliers from the potential energy.
2.1.6
The Kepler Problem
The three laws due to Kepler1 lead uniquely to the acceleration
¨r = −C r
r3 ,
with C = 1.33 × 1020 m3
s2 ,
as we will prove immediately. It is more usual to start with the gravitational law
and deduce Kepler’s laws, something we shall do only afterwards. It is customary
to infer the possible types of motion from a given coupling. But to begin with, we
solve here the so-called inverse problem, that is, we infer the coupling from the
observed motion, just as one derives the interaction from scattering experiments. In
this context, Lenz’s vector results as a conserved quantity, something that is not easy
to explain with the usual procedure.
1Johannes Kepler (1571–1630), among other things, imperial mathematician and astronomer in
Prague and then Professor in Linz (Austria).

2.1 Basic Concepts
63
Fig. 2.3 An ellipse with eccentricity ε = 2/3, its left focus (ﬁlled circle), the distance η of the
ellipse from the focus, perpendicular to the main axis, a ray r, and the vector aε to the center,
where a is the semi-major axis. The straight dotted lines have length a and b, and a2 = b2 + a2ε2,
according to Pythagoras. The apex P is called the perihelion and A the aphelion (from the Greek
helios for the Sun)
According to Kepler’s ﬁrst law, each planet moves along an ellipse with the Sun
at the focus.
Both celestial objects will be treated as point-like.
Consider an ellipse with semi-major axis a. For each point on the ellipse speciﬁed
by the vector r with origin at one of the foci, the sum of the distances from the foci,
viz., 2a = r + √(r −2aε ) · (r −2aε ) is ﬁxed. Here, aε is the vector from one
of the foci to the center of the ellipse, as shown in Fig. 2.3. Hence it follows that
(2a −r)2 = r2 −4a ε · r + 4 a2ε2, and we have
r −ε · r = a (1 −ε2) ≡η ,
where η is the distance of the ellipse from the focus, measured perpendicular to the
symmetry axis, i.e., at r ⊥ε . This is the starting equation for what follows. The
number ε is the eccentricity of the ellipse. The vector ε is the Lenz vector which will
be important later on because it is a characteristic of the orbit, hence a constant of
the motion. (The vector A = −m2C ε is often taken as the Lenz vector.) The area of
the ellipse is A = πab = πa2√
1 −ε2, something we shall need for Kepler’s third
law.
Note that our starting equation has not yet ﬁxed a plane orbit, if we take r as a
vector in three dimensions. The plane orbit is required by Kepler’s second law (in
vector form). In addition, the equation for ﬁxed η > 0 comprises further plane orbits:
ε = 0 : circle ,
0 < ε < 1 : ellipse ,
ε = 1 : parabola ,
ε > 1 : hyperbola branch .
If η is negative, then the branch is described from the other focus, but for ε ≤1 there
is no longer a real solution. Still, we would like to allow for the generalization to
ε ≥1. In this way, we include orbits of meteorites, but also the motion of electrical
point charges in the Coulomb ﬁeld of other point charges.

64
2
Classical Mechanics
Fig. 2.4 The triangle spanned by r and dr has area dA = 1
2 |r × dr | (see Fig. 1.2). For the area–
velocity law, we use dr = ˙r dt
Differentiating r2 = r · r with respect to time, the starting equation yields
˙r = r · ˙r
r
= ε · ˙r
=⇒
r
r −ε

· ˙r = 0 .
(As an aside, note that ˙r2 is not equal to ˙r · ˙r, as we can see immediately from a
circular orbit with ˙r = 0, but ˙r ̸= 0.) Thus, ˙r is perpendicular to r/r −ε. Here we
have
d
dt
r
r = ˙r
r −r
r2 ˙r = ˙r (r · r) −r (˙r · r)
r3
= (r × ˙r ) × r
r3
,
and therefore a further differentiation with respect to time yields
r
r −ε

· ¨r = −d
dt
r
r −ε

· ˙r = −(r × ˙r ) · (r × ˙r )
r3
as a further consequence of Kepler’s ﬁrst law. This equation for ¨r makes a statement
about the normal acceleration, since (r/r −ε ) is perpendicular to ˙r.
According to Kepler’s second law, the ray r traces equal areas dA = 1
2|r × ˙r dt|
in equal times dt.
This is also called the area–velocity law (see Fig. 2.4). Here, r and ˙r always span the
same plane. Consequently, the product r × ˙r is constant:
r × ˙r = c
=⇒
r × ¨r = 0
=⇒
¨r = f (r) r .
(Later on, we shall introduce the momentum m˙r and the orbital angular momentum
L = r × m˙r, where m is the reduced mass, explained in more detail in Sect. 2.2.2.
The angular momentum is a constant of the motion in the non-relativistic context:
according to the area–velocity law, the orbital angular momentum is conserved.)
Using the above-mentioned relation here, we obtain
−c2
r3 = f (r) (r −ε · r) = η f (r)
=⇒
f (r) = −c2
η r3 .
The acceleration is always oriented towards the focus for η > 0 (away from the focus
for η < 0) and decreases as r−2 with distance r.

2.1 Basic Concepts
65
Fig. 2.5 For the Kepler problem, the velocity ˙r traces a circle about the center −c × ε/η with radius
c/η if η > 0 (for ε > 1, it is only a section of a circle, because then r traces only one hyperbola
branch). At perihelion (P), the speed is greatest, and at aphelion it is smallest, with the ratio equal
to (1+ε) : (1−ε)
The orbit runs perpendicular to c = r × ˙r. Therefore, the velocity ˙r, which has
to be perpendicular to both c and r/r −ε, also satisﬁes ˙r ∝c × (r/r −ε ). The
missing factor follows from c = r × ˙r, because (r × ˙r ) × (r/r −ε ) is equal to
˙r r · (r/r −ε ) −r ˙r · (r/r −ε ) = ˙r (r −r · ε ) = ˙r η ,
so
η ˙r = c ×
r
r −ε

.
Since c is perpendicular to r, all vectors c × r/r have the ﬁxed length c. Therefore,
˙r describes a circle about the center −c × ε/η with radius c/η (see Fig. 2.5).
Since ε and r are perpendicular to c , the last equation delivers c × ˙r η/c2 =
ε −r/r or
r
r + η
c2 c × ˙r = ε .
Thus, the left vector is a constant of the motion (namely, Lenz’s vector), as is r × ˙r =
c.
The two Kepler laws discussed so far can be derived only for pure two-body
problems. However, other planets (and moons) perturb, so those laws are valid only
approximately, as we shall see in Sect. 2.2.6. There we shall also see that, for Kepler’s
third law, the mass of the planet has to be negligible compared to the mass of the
Sun. With Kepler’s third law the properties of different planets can be compared with
each other:
The cubes of the semi-major axes a of all planets behave like the squares of the
periods T.
Indeed,
a3 =
C
(2π)2 T 2 ,
with C = 1.33 × 1020 m3
s2 .

66
2
Classical Mechanics
According to the second law, the area A = πa2√
1 −ε2 of the ellipse (see p. 63) is
equal to 1
2cT and thus T = 2πa2√
1 −ε2/c, so we have
C = a3/(T/2π)2 = c2/{a (1 −ε2)} = c2/η .
The abbreviation η introduced above may therefore be replaced by c2/C:
r −ε · r = c2
C ,
˙r = C
c2 c ×
r
r −ε

,
¨r = −C r
r3 ,
c = r × ˙r ,
ε = r
r + c × ˙r
C
,
with c · ε = 0 .
Thus, the Kepler problem is uniquely characterized by the two ﬁxed vectors c and ε
and the constant C (or a), where c and ε are perpendicular to each other. This gives
6 independent parameters: three Euler angles ﬁx the orbital plane and the direction
of the major axis, while two further parameters determine the lengths of the axes and
the sixth the period.
If conversely we would like to infer the orbit from the acceleration ¨r = −C r/r3,
then Kepler’s second law follows immediately from ¨r ∥−r. Therefore, we may
introduce the vector c = r × ˙r as a constant of the motion. It is perpendicular to the
orbit, i.e., to r and ˙r. A further constant of the motion follows from
d
dt
r
r + c × ˙r
C

= (r × ˙r ) × r
r3
+ c × ¨r
C
= c × r
r3
−c × r
r3
= 0 ,
namely Lenz’s vector
ε = r
r + c × ˙r
C
.
This can be solved for ˙r, because c · ˙r = 0, and we can also take the scalar product
with r:
˙r = C
c2 c ×
r
r −ε

and
r · ε = r −c2
C .
From this, we obtain (for C > 0) the elliptical orbit with the focus as the origin
(Kepler’s ﬁrst law). We can thus derive all Kepler’s laws from the single equation
¨r = −C r/r3, because the third follows from the other two if C is the same for all
planets. Instead of c2/C, we have used the geometric quantity η above.
Since ε = r/r + c × ˙r η/c2 and c · ˙r = 0, we obtain the relation ε2 = ε · ε =
1 −2η/r + ˙r · ˙r η2/c2 for the square of the Lenz vector ε, and since c2/η = C, the
square of the velocity is given by
v2 ≡˙r · ˙r = 2C/r −(1 −ε2) C/η .

2.1 Basic Concepts
67
Fig. 2.6 The hyperbola branch r −ε · r = η for eccentricity ε = 3/2 (red) with the two foci (full
circle, η > 0, attractive force and open circle, η < 0, repulsive force) and the asymptotes (dashed
blue lines) in the directions to the initial and ﬁnal points. In addition to the ray r , the vector aε to
the center and the length η can be seen as in Fig. 2.3. The turning point is at a distance a from the
center. In addition, the scattering angle θ and the collision parameter s are shown
This relation can also be derived from the conservation of energy (p. 78), because
1
2mv2 is the kinetic energy and −mC/r the potential.
For a circular orbit about the zero point, the two foci coincide and r · ˙r = 0.
For constant orbital angular momentum, r × ˙r = c is also conserved. Then, with
ω ≡c/r2,
˙r = ω × r .
We shall encounter this differential equation repeatedly. With r (0) ⊥ω, it is solved
by r (t) = r (0) cos(ωt) + ω−1ω × r (0) sin(ωt). Note that, if r (0) also has a com-
ponent in the direction of ω, then this is conserved.
Let us thus look at the hyperbolic orbit (with ε > 1) (see Fig. 2.6). The directions
of their asymptotes are determined by r −ε · r = 0 and ε cos ϕ = 1, where ϕ is half
the opening angle. It is convenient here to deﬁne the scattering angle θ = π −2ϕ
and obtain sin 1
2θ = ε−1 and cot 1
2θ =
√
ε2 −1. This can be expressed in terms of
v∞, because v∞2 = (ε2 −1)(C/c)2 and thus cot 1
2θ = v∞c/C. If we then introduce
the collision parameter s (distance of the asymptotes from the foci) with c = s v∞,
we obtain cot 1
2θ = s v∞2/C.
This result is useful for the Rutherford cross-section, which describes the angu-
lar distribution for the elastic scattering of point charges q by a point charge q′—
whatever enters the circular ring 2πs ds is scattered into the cone opening 2π sin θ dθ
(shown on the left of Fig. 5.5):
dσ
d =

2π s ds
2π sin θ dθ
 =

1
2 ds2
d cos θ
 =
C2
2 v∞4

d cot2 1
2θ
d cos θ
 .
Since cos θ = cos2 1
2θ −sin2 1
2θ = 2 cos2 1
2θ −1 and

68
2
Classical Mechanics
d cot2 1
2θ
d cos θ
= 1
2
d
d cos2 1
2θ
cos2 1
2θ
1 −cos2 1
2θ =
1
2 sin4 1
2θ ,
we obtain
dσ
d = C2
4
1
(v∞sin 1
2θ)4 =
4 C2
|v −v ′|4 ,
where v is the initial velocity and v ′ the ﬁnal velocity. Here, C = qq′/(4πε0 m) with
the reduced mass m, explained in more detail in Sect. 2.2.2, and the electric ﬁeld
constant ε0, if the charges q and q′ are given in coulomb (see p. 165 ff.). This is also
obtained in (non-relativistic) quantum mechanics, as will be shown in Sect. 5.2.3.
The scattering cross-section integrated over all directions  diverges, because the
Coulomb force extends too far out. In reality, it will be screened by further charges.
2.1.7
Summary: Basic Concepts
We set the notion of force F as a basic ingredient of mechanics. The next section will
be concerned with a different possibility, and we shall thus derive further quantities.
In particular, a force can do work
	
F · dr along a path. If this work depends only
upon the initial and ﬁnal point of the path and not upon the path in-between, then we
may set F = −∇V and work with the simpler, scalar potential energy V .
According to Newton’s third law, two bodies act on each other via equal but
oppositely directed forces, which are not necessarily central forces.
A special kind of forces are the forces of constraint. They originate from geo-
metric constraints and do no work. Therefore, they do not need to be accounted for
as forces due to virtual displacements—instead, the geometric constraint has to be
obeyed for all displacements δr. If we can write the constraint as an equation  = 0,
then it can be accounted for by a Lagrangian parameter for the potential energy:
δ (V −λ ) = 0 in equilibrium.
In addition to these notions, decisive for statics, we have also treated the Kepler
problem as an example for kinematics. From Kepler’s ﬁrst and second laws (dating to
1609), we could infer ¨r ∝−r/r3. The missing factor here is the same for all planets,
according to Kepler’s third law (dating to 1619). With these laws, their motions can
be described by a single differential equation—the different orbits follow from the
corresponding initial conditions.

2.2 Newtonian Mechanics
69
2.2
Newtonian Mechanics
2.2.1
Force-Free Motion
Newton2 took the inertial law due to Galileo (1564–1642) as his ﬁrst axiom in 1687:
If no force acts on it (also no frictional force), a body remains in its state of rest
or of uniform rectilinear motion—it is inertial.
Here uniform rectilinear motion and the state of rest are equivalent. Different points
of view are permitted, at rest and moving, as long as they are not accelerated relative
to one other. Such allowable reference frames will be called inertial frames. In these
frames, force-free bodies obey the inertial law. In contrast, bodies on curved orbits are
always accelerated, according to Sect. 1.1.3. As a measure for uniform rectilinear
motion, it is natural to think of the velocity. But Newton introduced instead the
momentum
p ≡m v = m ˙r
as motional quantity. This is the velocity weighted by the inertial mass m. We shall
encounterthenotionofinertialmassinthecontextofthescatteringlawsinSect.2.2.3.
For the moment it is sufﬁcient for our purposes to note that each invariable body has
a ﬁxed mass, which depends neither upon time nor upon the position or the velocity,
and is therefore a conserved quantity: a quantity is said to be conserved if it does not
change with time. (Burning rockets and growing avalanches are “variable bodies”,
whose mass does not remain constant in time.) Therefore, the inertial law may also
be called the momentum conservation law (often called momentum conservation for
short):
dp
dt ≡˙p = 0 ,
for force-free motion.
If no force acts, the momentum is conserved (inertial law, law of persistence).
According to the theory of special relativity (Sect. 3.4), a body cannot move faster
than the speed of light (c = 299 792 458 m/s), and therefore one actually has to set
v = γ dr
dt ,
with γ ≡
1

1 −(dr/dt)2/c2 .
We then have p = m γ dr/dt. The factor γ is notably different from 1 only for
v ≈c, as is clear from Fig. 3.23. Therefore, the simple non-relativistic calculation
2Isaac Newton (1643–1727) was professor in Cambridge from 1669–1701, Master of the Royal
Mint in London in 1699, and President of the Royal Society of London in 1703.

70
2
Classical Mechanics
fully sufﬁces for many applications. To a similarly good approximation, “ﬁxed” stars
always remain at the same position and deliver a generalized reference frame.
As long as m does not depend on time and we consider force-free motion, then in
addition to the polar vector p, a scalar and an axial vector remain conserved, namely,
the kinetic energy T and the angular momentum L:
T ≡1
2m p · p = m
2 v · v
and
L ≡r × p ,
since for ﬁxed p and m, the quantity T is also conserved—and for ˙p = 0, we also
have ˙L = ˙r × p = p × p/m = 0. Altogether then, we have
˙T = 0 ,
˙p = 0 ,
and
˙L = 0 ,
if no force acts.
In what follows, it will be useful to view the kinetic energy T as a scalar ﬁeld of
the variables v. Hence, in the velocity space, we may also take the momentum p as
the gradient of T (and use lower indices according to p. 35):
p = ∇vT
and
pk = ∂T
∂vk .
This will help us later in Lagrangian and Hamiltonian mechanics, but also for the
separation into center-of-mass and relative motion.
2.2.2
Center-of-Mass Theorem
We have just introduced the mass m as a constant factor in p = m v. It has not yet
been explained why we need the momentum at all in addition to the velocity. This
changes only when there are several masses m1, m2, . . ., for the above-mentioned
laws are valid not only for a single body, but also for several bodies, which normally
act on each other and thus exert forces—as long as there are no external forces acting
on the bodies. According to Newton’s third law (force equal to counter-force), the
forces between the bodies cancel each other. Therefore, without external forces there
is also no force on the system as a whole. This system we can treat as a single body.
Its momentum is composed of the individual momenta and is conserved:
P ≡

i
pi ,
˙P = 0 ,
if no external forces act.
The masses thus weight the individual velocities. Hence, for two bodies without
external forces, we have ˙p1 + ˙p2 = 0, but ˙p1 = −˙p2 ̸= 0 if they act on each other.

2.2 Newtonian Mechanics
71
If we introduce the total mass M and the position R of the center of mass,
M ≡

i
mi ,
R ≡1
M

i
mi ri ,
then for F = 0, it moves with the constant velocity
V ≡˙R = 1
M

i
mi ˙ri = 1
M

i
pi = 1
M P .
The total momentum is thus equal to the momentum of the center of mass. It is
conserved if there are no external forces (center-of-mass law)—and hence according
to the last section, the kinetic energy and the angular momentum of the center of
mass remain conserved.
For many-body problems it is helpful to introduce center-of-mass and relative
vectors instead of the position vectors ri. We shall show this for the case of two point
masses with M = m1 + m2:
R ≡m1 r1 + m2 r2
M
and
r ≡r2 −r1 .
(For more point masses, we must proceed stepwise. After the two-body problem, the
third is to be treated with respect to the center of mass of the ﬁrst two, and so on.
This leads to the Jacobi coordinates. In view of this, we thus take r2 −r1 and not
r1 −r2 as the relative vector.) For this, it is convenient to write
R
r

=
m1/M m2/M
−1
1
 r1
r2

.
The determinant of the matrix here is equal to 1 and thus the map is area-preserving
(see Fig. 2.7). (In more than two dimensions, the corresponding volume should
remain conserved, whence the functional determinant has also to be equal to 1, as
discussed further in Sect. 1.2.4.) Therefore, conversely, we have
 r1
r2

=
 1 −m2/M
1
m1/M
 R
r

,
since the inverse of a 2 × 2 matrix is
a b
c d
−1
=
1
ad −bc
 d −b
−c a

,
something we shall use repeatedly.

72
2
Classical Mechanics
Fig. 2.7 With the change from two-body to center-of-mass and relative coordinates (left) or vice
versa (right), we make a transformation from rectangular to oblique coordinates which is not angle-
preserving, shown here for the x-components and m1 = 2m2. The unit square (dashed lines) turns
into a rhomboid of equal area (see Fig. 1.11). For m1 = m2, we have a rectangle on the left and a
rhombus on the right
The same matrices appear for the transition (v1, v2) ↔(V, v ), because they
remain conserved for the derivative with respect to time. Because v1 = V −v m2/M
and v2 = V + v m1/M, the kinetic energy is
T = m1v12 + m2v22
2
= MV 2 + μv2
2
,
with μ ≡m1m2
M
.
Only this reduced mass μ is important for relative motions. Hence, in T the mixed
term V · v vanishes if we introduce a relative vector r ∝r2 −r1 in addition to
the center-of-mass vector R. The center-of-mass and relative motion then already
decouple. With r = r2 −r1, we even obtain an area-preserving map (hence also
r1 × r2 = R × r ), but not an angle-preserving map, because the matrices are not
orthogonal. Since we have already made a transition from T (v1, v2) to T (V, v ), we
can also easily derive the momenta as gradients in velocity space:
P = M V ,
p = μ v
=⇒
T = p12
2m1
+ p22
2m2
= P2
2M + p2
2μ .
We already know the expression for P. Clearly, the two momenta P and p can be
expressed as linear combinations of p1 and p2, viz.:
P
p

=

1
1
−m2/M m1/M
 p1
p2

,
or
p1
p2

=
rrm1/M −1
m2/M
1
 cP
p

,
noting that the momentum transformations are also area-preserving. In addition, we
ﬁnd for the angular momentum
L = r1 × p1 + r2 × p2 = R × P + r × p .

2.2 Newtonian Mechanics
73
If no external forces act, the forces depend only upon r (and possibly upon v ), and
we only need to deal with the relative motion. With this, the two-body problem is
reduced to a single-body problem and has become essentially easier.
The center-of-mass frame stands out here: if we choose the center of mass as
origin, then P = 0 and consequently, p2 = −p1 = p .
2.2.3
Collision Laws
If two bodies collide without external forces acting, then the relative motion changes,
but not the motion of the center of mass: P ′ = P. (Primed quantities will be used to
describe the ﬁnal state.) As far as the relative motion is concerned we need further
information. In the following we consider only the motion before and after the colli-
sion, not during the collision—therefore we do not care about the forces between the
collision partners. These are necessary, however, if we need to determine the scatter-
ing angle. In genuine scattering theory (see, e.g., Sects. 5.1 and 5.2), the interaction
between the partners is indispensable.
In addition to elastic scattering, we need also to deal with inelastic processes, but
without exchange of mass, i.e., the collision partners keep their masses, but during
the collision, their relative motion could possibly lose energy which is converted
into work of deformation, rotational energy, or heat. (With exchange of mass the
equations become less clear, but in principle, the situation is no more difﬁcult to
treat.) Here, we introduce the heat tone Q = (p′ 2 −p2)/2μ—for elastic scattering
p′ = p and hence Q = 0. In contrast, for completely inelastic scattering, we have
p′ = 0, and thus Q = −p2/2μ . The ratio p′/p is abbreviated to
ξ ≡p′
p ,
with
p′ =

p2 + 2μ Q .
For elastic scattering ξ = 1 and for completely inelastic scattering ξ = 0.
The relative momenta p′ and p may have different moduli and also different direc-
tions. Therefore, we set p′ = ξ D p with the rotation operator D given in Sect. 1.2.1.
Then with P′ = P, according to the last section, we obtain
p′
1
p′
2

= 1
M
m1 + ξ m2 D m1 −ξ m1 D
m2 −ξ m2 D m2 + ξ m1 D
 p1
p2

.
For a completely inelastic collision (ξ = 0), we thus have v′
1 = v′
2 = V.
Rather simple situations also occur for collisions between two mass points,
because they take place only for r = 0. In this case the conservation of angular
momentum leads to D = −1. If we consider here an elastic collision, then ξ D = −1
and hence,

74
2
Classical Mechanics
p′
1
p′
2

= 1
M
m1 −m2
2m1
2m2
m2 −m1
 p1
p2

.
In the special case where m2 = m1, it follows that p′
1 = p2 and p′
2 = p1: for equal
masses the momenta (velocities) are exchanged. In contrast, for m2 ≪m1, it fol-
lows that p′
1 ≈p1 + 2μ v2 and p′
2 ≈2μv1 −p2 with μ ≈m2: only the small mass
signiﬁcantly changes its velocity.
Let us return to the collision of extended particles, but choose p1 = 0 and in this
“laboratory frame” derive p′
1L and p′
2L—here and in the following we indicate clearly
whether the quantity refers to the laboratory frame (L) or the center-of-mass frame
(S). However, this is unnecessary for P and p: since PS = 0, the total momentum
P should always refer to the laboratory frame, and the relative momentum p does
not depend on the reference frame. According to the last section, p = p2S = −p1S,
and for p1L = 0, we have P = p2L and also p = (m1/M) p2L, as well as p2/2μ =
(m1/M) T2L. This can be used to determine the parameter ξ:
ξ =

1 + 2μ Q
p2
=

1 + M
m1
Q
T2L
.
Since p1L = 0 and p2L = (M/m1) p, we now have
p1L
′ = (1 −ξ D) p
and
p2L
′ = (m2
m1
+ ξ D) p .
The scattering normal n = p × p′/|p × p ′| in Fig. 2.8 points into the viewing
direction, so the vector n × p points to the right. Therefore, in the center-of-mass
frame, as a function of the scattering angle (θS), the rotated vector Dp can be
Fig. 2.8 A mass (open circle) collides with a mass twice as heavy (closed circle) at rest. The
momenta before (left) and after (right) the collision are indicated in the laboratory frame (top) and
the centre-of-mass frame (bottom). In the latter, only the relative momenta p and p′ before and
after are shown, as the total momentum P is conserved. Here an elastic collision was assumed,
whence the dashed lines around the full circle form a rhombus and are in the ratio m2 : m1. The
large rhombus angle is clearly equal to π −θS and twice as large as θ•L. For equal masses the two
objects ﬂy off from each other at right angles

2.2 Newtonian Mechanics
75
expanded in terms of p and n × p: Dp = cos θS p + sin θS n × p. We then obtain
for the recoil momentum p′
1L and for the momentum of the colliding particle p′
2L,
p1L′ = ( 1 −ξ cos θS) p −ξ sin θS n × p ,
p2L′ = ( m2
m1
+ ξ cos θS) p + ξ sin θS n × p .
Hence, noting that we should always have 0 ≤θ ≤π, and in addition that tan θiL =
|piL′ × p |/(p′
iL · r) and (n × r) × p = −n, the scattering angle in the laboratory
frame of the scattering particle (the one that is impinged upon) is given by
tan θ1L =
sin θS
ξ −1 −cos θS
,
while for the scattered (impinging) particle,
tan θ2L =
sin θS
ζ + cos θS
,
with
ζ ≡
m2
ξ m1
.
From this we can conclude that, for elastic collision (ξ = 1), as in Fig. 2.8, θ1L =
1
2 (π −θS) and for ζ = 1, θ2L = 1
2θS, so that for equal masses (and elastic collision)
θ1L + θ2L = 1
2π.
For a given θS, there is a value θ1L and a value of θ2L, and for ξ ≤1, θ1L ≤1
2π.
In most cases, we do not consider the recoil, and instead of θ2L, we simply write θL.
Since sin θS cos θL = (ζ + cos θS) sin θL, the relation between θL and θS can thus be
written as
ζ sin θL = sin (θS −θL)
⇐⇒
θS = θL + arcsin (ζ sin θL) .
This relation is shown in Fig. 2.9 for different values of ζ. For ζ > 1, only values
θL ≤arcsin ζ −1 occur, and for each θL below this bound, there are two values of θS.
For the moduli of the momenta, we ﬁnd
p′
1L = p

1 −2ξ cos θS + ξ 2
and
p′
2L = p ξ

1 + 2ζ cos θS + ζ 2 .
The recoil momentum p′
1L is equal to the momentum transfer in the laboratory or
center-of-mass frame (|p2L′ −p2L| = |p′
2S −p2S|). For an elastic collision, it is equal
to 2p sin 1
2θS. Since cos θL = p′
2L · p/(p′
2L p), we have

76
2
Classical Mechanics
Fig. 2.9 Relation between
the scattering angles in
laboratory and
center-of-mass coordinates
for ζ = 1/2 and 3/4 (dashed
red), for ζ = 1 (full
magenta), and for ζ = 4/3
and 2 (dotted blue)
cos θL =
ζ + cos θS

1 + 2ζ cos θS + ζ 2 .
Hence one obtains the ratio dL/dS = |d cos θL/d cos θS|, namely
dL
dS
=
|1 + ζ cos θS|

1 + 2ζ cos θS + ζ 2 3 ,
whence the scattering cross-sections can be converted from the laboratory to the
center-of-mass frame (or vice versa).
In principle the mass ratio can be determined from the velocities before and
after the collision, even if it is inelastic, whence ξ = |v′
2 −v′
1|/v2 ̸= 1: for a central
collision (θS = π) and since v′
2 : v2 = (m2 −ξm1) : (m2 + m1), we have
m2
m1
= ξv2 + v′
2
v2 −v′
2
.
For all other collisions, the momenta perpendicular to the original one cancel each
other (momentum conservation): m2 : m1 = v′
1⊥: v′
2⊥. Further supplements can be
found in Problems 2.15–2.18.
2.2.4
Newton’s Second Law
Newton’s law of motion is understood as his second axiom:
Each force F on a freely mobile body changes its momentum according to
F = ˙p .

2.2 Newtonian Mechanics
77
The inertial law, referring to the case F = 0, seems to be a special case. But this was
taken as deﬁning an inertial system, because only then can the mass and momentum
be introduced as observables. Since dp = F dt, the force often appears in an integral
over F dt, which is referred to as the impulse (impulsive force). In p = m˙r, we can
often take the mass as constant, whereupon
F = m ¨r .
In relativistic dynamics, the factor γ = 1/

1 −v2/c2 also enters our considerations,
because we must refer to the proper time, as will be shown in Sect. 3.4.10.
The equation F = ˙p can be applied to rotational motion. Since ˙r ∥p, it is clear
that d (r × p)/dt = r × ˙p = r × F = M, and since r × p = L, we conclude that
M = ˙L .
A torque on a mobile body changes its angular momentum.
For an invariable mass, the law of motion delivers a differential equation of second
order:
¨r = F(t, r, ˙r )
m
.
This differential equation has to be integrated, because we are interested in the orbit,
and from r (t) we can derive the velocity. Then, for each integration an integration
constant occurs (here, actually an integration vector). The law of motion leaves
us with the choice of initial position and velocity, so the general solution r of the
differential equation depends upon t, r0, and ˙r0. These values have to determine the
solution uniquely, otherwise the force is unphysical.
If the force does not depend upon the velocity, but only on position r and possibly
time t, we speak of a given force ﬁeld. Since for a given force the acceleration ¨r is
inversely proportional to the mass, we consider the ﬁeld F/m, and for curl-free force
ﬁelds, the potential  ≡V/m instead of the potential energy V . Only then is the
force ﬁeld independent of the test body, and we have
¨r = −∇ ,
with  ≡V
m ,
if ∇× F = 0 and ˙m = 0.
2.2.5
Conserved Quantities and Time Averages
If a force acts, F ̸= 0, then the momentum is no longer a conserved quantity because it
changes with time. But let us consider also the two conserved quantities encountered

78
2
Classical Mechanics
so far, the kinetic energy and the angular momentum: what are their derivatives with
respect to time when a force acts? If we assume a constant mass, we obtain
dT
dt = 1
2m
d
dt p · p = p
m · dp
dt = v · F = F · dr
dt .
For a time-independent force, we thus ﬁnd dT = F · dr. If moreover the force ﬁeld
is curl-free, then it can be derived from a potential energy V , and because dV =
∇V · dr = −F · dr, we clearly have dT = −dV . (If the force depends upon time,
then neither dT = F · dr nor

F · dr = 0 can be inferred, and it then depends on
the time span over which the work is done.) Thus, there is a conservation law for the
energy, viz.,
E ≡T + V ,
if V (or the associated force F ) does not depend on time.
In the following sections, we shall discuss several examples with curl-free forces,
to which a potential can therefore be assigned. An important counter-example is
provided by the Lorentz force
F = q (E + v × B ) ,
which acts on an electric charge q in an electromagnetic ﬁeld speciﬁed by E and B.
The Maxwell equations ∇× E = −∂B/∂t and ∇· B = 0 imply that F has the curl
density
∇× F = −q
∂B
∂t + (v · ∇) B

= −q ˙B ,
since r and v have to be treated as mutually independent variables, whereupon
∇× (v × B ) = v ∇· B −(v · ∇) B. Even if the magnetic ﬁeld does not depend on
position (only on time), the force ﬁeld has curls. Then there is no potential energy,
unless we introduce a generalized potential energy as in Sect. 2.3.4. In any case, here
(with E = 0 and constant mass) the equation of motion is
˙v = ω × v ,
with ω = −qB
m
.
The value of ω is the cyclotron frequency. (Note that we encountered a similar
differential equation for the circular orbit, but for r rather than v, on p. 67.) Even
though a force acts here, the kinetic energy is still conserved because the Lorentz
force is always perpendicular to v and thus does not change v. Therefore, if we
set v = v eT with ﬁxed v, it follows that ˙eT = ω × eT, or deT/ds = v−1 ω × eT. We
have already met this differential equation on p. 8. Quite generally, the charge moves
on a helical orbit in the homogeneous magnetic ﬁeld, with ﬁxed Darboux vector
τeT + κeB = ω/v.

2.2 Newtonian Mechanics
79
Theotherconservedquantityintroducedsofar,theangularmomentumL = r × p,
is only a conserved quantity if the torque M vanishes, i.e., if F is a central force,
since dL/dt = M according to the last section. This is the case, e.g., if the potential
has spherical symmetry:
(r) = (r)
=⇒
∇ = d
dr
r
r
=⇒
dL
dt = 0 .
Note that here only the angular momentum with respect to the symmetry center is
conserved. It is only if no force acts at all that it is conserved with respect to any
point. For cylindrical symmetry, thus if  does not depend upon the angle coordinate
ϕ, at least the angular momentum component along the symmetry axis is conserved.
Of course, mean values taken over time are also conserved. This is important for
the virial theorem, which says that, if r and p always stay ﬁnite (and the mass always
the same), then for the time-averaged value of the virial r · F,
r · F = −2 T .
Hence, if r and p always stay ﬁnite, then so does the auxiliary quantity G(t) = r · p.
For sufﬁciently long times τ, the quantity τ −1 {G(τ) −G(0)} thus vanishes, and this
is the mean value of ˙G = v · p + r · F = 2T + r · F between 0 and τ. For example,
for a central force F = crn r/r, the virial is equal to crn+1, and hence, according
to p. 57, it is equal to −(n+1) V . This theorem leads here to T = 1
2(n+1) V . In
particular, for a harmonic oscillation, we have n = 1 and thus T = V , while for the
gravitational and Coulomb forces n = −2, and thus T = −1
2 V . The virial theorem
must not be applied to the hyperbolic orbit, because r · p does not remain ﬁnite.
2.2.6
Planetary Motion as a Two-Body Problem,
and Gravitational Force
If there are no external forces, the total momentum is conserved. Then we are con-
cerned only with the relative motion. An important example is application to the
Sun–Earth system, which may be viewed approximately as a two-body problem,
although the Moon and the other planets should be accounted for in a more accurate
solution.
Here gravity acts, that is, the force between massive bodies. So far, the term “mass”
has always been understood as inertial mass. But in fact, the active gravitational mass
m1 exerts a force
F21 = −G
m2m1
|r2 −r1|2
r2 −r1
|r2 −r1|
on the passive gravitational mass m2, where G is the gravitational constant (see
p. 623). But from experience, we may assume the active and passive gravitational

80
2
Classical Mechanics
masses and the inertial mass to be the same, at least to an accuracy of one part in
1011)—this is the basis of general relativity theory.
Exactly as the Sun (S) attracts the Earth (E) and hence exerts the force FES, the
Earth attracts the Sun with the opposite force FSE according to Newton’s third law.
Hence we infer
˙pE = FES = −FSE = −˙pS ,
and ˙P ≡˙pS + ˙pE = 0 for the center of mass. Then, according to p. 72,
˙p ≡mS ˙pE −mE ˙pS
mS + mE
= ˙pE = FES = −G mE mS
r2
r
r ,
for the relative momentum.
Once again, only the relative coordinate is of interest—no external force acts
on the center of mass, as long as the inﬂuence of other celestial objects remains
negligible. Since p = μ˙r with μ = mS mE/(mS + mE),
¨r = −G mS + mE
r2
r
r .
Therefore, the ﬁrst two Kepler laws are valid not only with the Sun at the coordinate
origin, but also for the relative motion. With the third law, however, we have
a3
T 2 = G mS + mE
4π2
,
i.e., for every planet there is another “constant”, since a3/T 2 = C/4π2 holds with
¨r = −Cr/r3, according to p. 65. However, the mass ratio of planet to Sun is less than
0.001 even for Jupiter. In addition, we have neglected the mutual attractions of the
other planets and moons. This perturbation can be accounted for approximately. This
is how, from the perturbed orbit of Uranus, Leverrier deduced the presence of the as
yet unknown planet Neptune, a jewel of celestial mechanics. Incidentally, Kepler had
already noticed that Jupiter and Saturn did not travel on purely elliptical orbits—these
two neighboring planets are the heaviest in the Solar System and therefore perturb
each other with a particularly strong force. Likewise, returning comets move on
elliptical orbits about the Sun which are sensitive to perturbations (see Problem 2.11).
The gravitational force acts “not only in heaven, but also on Earth”. All objects
are pulled toward the Earth—they have weight. However, this notion is used with
different meanings. In the international system (SI), the (gravitational) mass is under-
stood, but in any everyday context, the associated gravitational force. If we buy 1 kg
of ﬂour, we actually want to have the associated mass, but when we weigh it, we
use the force with which the Earth attracts this mass. Physicists should stick to the
international system and also take “weight” as mass.

2.2 Newtonian Mechanics
81
2.2.7
Gravitational Acceleration
According to the gravitational law, at its surface, the Earth exerts the gravitational
force
F = m g
with g = −G mE
R2
R
R ,
on a body of mass m, if we assume a spherically symmetric Earth. Here the gravita-
tional acceleration g is assumed constant, as long as the distance R from the center
of the Earth changes only negligibly (since the Earth rotates about its axis, we should
also take into account the position-dependent centrifugal force). The vector −R/R
is a unit vector which, at the surface of the Earth, points “vertically downwards”.
The gravitational acceleration g thus follows from the mass mE and radius R of the
Earth and the gravitational constant G.
According to this equation, the total mass mE can be considered as concentrated
at the center of the Earth when evaluating the gravitational force on a test body near
the surface of the Earth. For the proof, since a scalar ﬁeld is easier to work with than
the associated force ﬁeld, we consider the gravitational potential
(r) = −G mE
r
⇐⇒
F(r) = −G m mE
r2
r
r ,
which we derive for r ≥R from
(r) = −G
 ρ(r ′) d3r ′
|r −r ′|
.
Here we assume the density distribution to be spherically symmetric, i.e., ρ(r ′) =
ρ(r′), although it does not need to be homogeneous (actually, the Earth’s mantle has
a lower density than the core). Thus let
mE =

ρ(r ′) d3r ′ = 4π

ρ(r′)r′ 2dr′ .
In order to evaluate the potential, we expand |r −r ′|−1 in powers of s = r′/r < 1
and introduce the angle θ between r ′ and r:
1
|r −r ′| =
1
r
√
1 −2 s cos θ + s2 = 1
r
∞

n=0
Pn(cos θ) sn .
The expansion coefﬁcients Pn(cos θ) are called Legendre polynomials. We shall meet
them occasionally, e.g., in electrostatics (see p. 181) and in quantum theory with the
spherical functions (see p. 334 ff). The ﬁrst of these are (see Fig. 2.10)

82
2
Classical Mechanics
Fig. 2.10 Legendre
polynomials Pn(z) with n
from 0 to 5. Continuous
curves: Even n. Dashed
curves: Odd n. It can be
proven recursively that
Pn(1) = 1 and
Pn(−z) = (−)n Pn(z) for all
n ∈{0, 1, 2, . . .} and that n
gives the number of zeros of
the given function
P0(z) = 1 ,
P1(z) = z ,
P2(z) = 1
2 (3z2 −1) ,
. . . .
The remaining ones can be obtained via the recursion relation
(n+1) Pn+1(z) −(2n+1) z Pn(z) + n Pn−1(z) = 0 ,
which follows from the generating function (see the power series above)
1
√
1 −2sz + s2 =
∞

n=0
Pn(z) sn ,
for |s| < 1 .
This means that, if we differentiate this equation with respect to s and then multiply it
by 1 −2sz + s2, we obtain (z −s) 
n Pn(z) sn = (1 −2sz + s2) 
n n Pn(z) sn−1,
and hence by comparing coefﬁcients, the recursion relation is proven.
In addition, the Legendre polynomials have the property (important for us)
 1
−1
Pn(z) Pn′(z) dz =
 π
0
Pn(cos θ) Pn′(cos θ) sin θ dθ =
2
2n + 1 δnn′ ,
whence they form a (complete) orthogonal system for −1 ≤z ≤1. This can be
proven using the generating function of the Legendre polynomials. For |s| < 1 and
|t| < 1, this delivers
1
√
1 −2sz + s2 √
1 −2tz + t2 =

mn
Pm(z) Pn(z) sm tn .
But now, if we cancel a factor of
√
2t +
√
2s,

2.2 Newtonian Mechanics
83
 1
−1
dz
√
1 −2sz + s2√
1 −2tz + t2
= −
1
√st ln

2t (1 −2sz + s2) +

2s (1 −2tz + t2

+1
−1
=
1
√st ln
√
2t (1 + s) +
√
2s (1 + t)
√
2t (1 −s) +
√
2s (1 −t)
=
1
√st ln 1 + √st
1 −√st .
Hence, since ln(1 + x) = ∞
n=0 xn/n for |x| < 1, it follows that
 1
−1
dz
√
1 −2sz + s2 √
1 −2tz + t2 =
∞

n=0
2
2n + 1 (st)n ,
for |st| < 1 .
Comparing coefﬁcients proves the claim.
Further properties of the Legendre polynomials are given on p. 334, and more can
be found in, e.g., [1].
Since we started from a spherically symmetric density distribution ρ(r ′) =
ρ(r′) P0, after integrating over all directions, only the term with n = 0 remains:
 ρ(r ′) d3r ′
|r −r ′|
= 2 × 2π
r

ρ(r′) r′ 2 dr′ = mE
r
.
This means that we can perform calculations as though the mass of the Earth were
concentrated at its center. (Problem 2.20 is also instructive here.)
2.2.8
Free-Fall, Thrust, and Atmospheric Drag
If we calculate with the same gravitational acceleration g everywhere on the surface
of the Earth, then, according to Newton’s law of motion, we obtain
¨r = g ,
˙r = v0 + g t ,
r = r0 + v0 t + 1
2g t2 .
According to pp. 57 and 77, a gravitational potential
(r) = −g · r
is associated with the constant acceleration. The gauge here is such that the potential
vanishes at the surface of the Earth, where the coordinate origin is taken. If a body
loses height h, its potential energy decreases by mgh. For free fall, the kinetic energy
increases by this amount, so its velocity goes from zero to v = √2gh.
If the body is thrown through air instead of empty space, then it loses momentum
to the air molecules it collides with. The number of collisions per unit time increases

84
2
Classical Mechanics
linearly with its velocity, and in each collision, it loses on average a fraction of
its momentum determined by the mass ratio. Hence we have to set the frictional
force proportional to −v v (Newtonian friction, not Stokes friction, which would be
proportional to v, as, e.g., later on p. 99) and write with β > 0,
˙v = g −β2 g v v .
For objects surrounded by ﬂuids one normally writes cw 1
2ρ A v2 for the frictional
force, where cw is the drag coefﬁcient, ρ the density of the medium (here the air),
and A the cross-section of the body perpendicular to the air stream. Streamlined
bodies have the smallest drag coefﬁcient, namely 0.055. As far as the author is
aware, the above non-linear differential equation can be solved in closed form only
in one dimension. Therefore we assume that v0 is parallel to the vertical and measure
v in the direction of g. Then we have
dv
dt = g (1 −β2v2)
=⇒
dv
1 −β2v2 = g dt .
After separation of variables, we can integrate and obtain
v = 1
β
βv0 + tanh(βgt)
1 + βv0 tanh(βgt) .
Consequently, the velocity changes at ﬁrst linearly with time, v ≈v0 + (1 −β2v02)
gt, and ﬁnally becomes constant (incidentally faster than the horizontal component
of v which tends to zero):
v ≈1
β

1 −2 1 −βv0
1 + βv0
exp (−2βgt) + · · ·

.
For x ≫1, i.e., tanh x ≈−2 exp (−2x), and with b = βv0 and e = 2 exp(−2x), we
have
b + 1 −e
1 + b (1 −e) = 1 −e/(1 + b)
1 −be/(1 + b) ,
and because |e| ≪1, we may replace {1 −be/(1 + b)}−1 approximately by 1 +
be/(1 + b) and likewise {1 −e/(1 + b)}{1 + be/(1 + b)} by 1 −e (1 −b)/(1 + b).
The body is accelerated until its gravitational force and frictional force cancel
each other. It then permanently loses potential energy without increasing its kinetic
energy—the energy has now completely turned into frictional energy (heat).
Note that a solution initially changing linearly in time and ending up exponentially
approaching a constant velocity also occurs for free fall with Stokes’s friction, viz.,
˙v = g −αv. Then we have v = α−1g + (v0 −α−1g ) exp (−αt), where v0 and g
may span a plane. As can be seen from Fig. 2.11, it is better in any case to calculate
with this approximation than to neglect friction completely.

2.2 Newtonian Mechanics
85
Fig. 2.11 Free fall from rest with friction with the air. Continuous red curves: Newtonian friction.
Dotted blue curves: Stokes’s friction (here, α = βg). Dashed green curves: Without friction (with
appropriate scaling v = gt and s = 1
2 gt2)
2.2.9
Rigid Bodies
The parts of a rigid body keep always their relative distances. Therefore, we shall
refer the position vectors of the mass elements dm to a ﬁxed point in the body, usually
the center of mass R =
	
r dm/M:
r ′ = r −R
=⇒

r ′ dm = 0 .
The vectors r ′ have constant lengths, so we can infer the equations
d
dt (r ′ · r ′) = 2 r ′ · dr ′
dt = 0
=⇒
dr ′
dt = ω × r ′ .
Here ω is an axial vector in the direction of the axis of rotation (right-hand rule)
and with the value of the angular velocity, as already introduced on p. 67 for circular
motion. It describes the rotation of the rigid body and does not depend on the position
r ′. For all i and k, r′
i · r′
k will not depend on time, whence ˙r′
i · r′
k + r′
i · ˙r′
k = (ωi −
ωk) · (r′
i × r′
k) must always vanish, and ωi = ωk must hold.
From these considerations, we may generally decompose the motion of each point
of the body into that of the reference point and a rotational motion:
˙r = V + ω × r ′ .
For the total momentum,
P =

˙r dm = M V + ω ×

r ′ dm ,

86
2
Classical Mechanics
where we may write ρ(r ′) d3r ′ instead of dm. The last term vanishes because
	
r ′ dm = 0. The expressions for the angular momentum and the kinetic energy
(see p. 72) then simplify to (otherwise there would be further terms):
L ≡

r × ˙r dm = R × P +

r ′ × (ω × r ′) dm ,
T ≡1
2

˙r · ˙r dm = 1
2 M V 2 + 1
2 Iω ω2 .
Here Iω is the moment of inertia of the body with respect to the axis eω = ω/ω,
which must go through the center of mass:
Iω ≡

(eω × r ′)2 dm =

{r′ 2 −(eω · r ′)2} ρ(r ′) d3r ′ .
More precisely, we should write IωCM, because the axis of rotation goes through
the center of mass. For a rotation about the origin, eω × r ′ is to be replaced by
eω × (R + r ′), and therefore both moments of inertia differ by the non-negative
quantity
Iω −IωCM = M (eω × R)2 ,
i.e., by the mass multiplied by the square of the distances of the center of mass from
the axis of rotation. This is Steiner’s theorem. It is very helpful, because we may then
choose the origin of our coordinate systems in a more convenient place.
2.2.10
Moment of Inertia
In general, the moment of inertia Iω also depends upon the rotational orientation
eω. This is what we shall investigate now. Here we let the center of mass remain
at rest and thus take it as the reference point of the ﬁxed body system. We shall
write r instead of r ′ as we have done so far. Then, because ˙r = ω × r, we obtain the
expression
L =

r × (ω × r) dm
for the angular momentum of the rigid body, which is also important for the kinetic
energy of the rotation (the rotational energy), since (ω × r)2 = (ω × r) · (ω × r) =
ω · {r × (ω × r)} delivers
T = 1
2 Iω ω2 = 1
2

(ω × r)2 dm = 1
2 ω · L ,

2.2 Newtonian Mechanics
87
corresponding to T = 1
2v · p for rectilinear motion. Clearly, L and ω depend on each
other linearly, but may have different directions. If we write
L = I ω ,
then I is a linear operator—more precisely a tensor of second rank, because it assigns
a vector linearly to another vector. If we decompose
L =

r × (ω × r) dm =

{ω r2 −r (r · ω)} dm
in terms of Cartesian components, e.g., Lx =
	
ωxr2 −x (xωx + yωy + zωz) dm,
we arrive at the system of linear equations
⎛
⎝
Lx
L y
Lz
⎞
⎠=
⎛
⎝
Ixx Ixy Ixz
Iyx Iyy Iyz
Izx Izy Izz
⎞
⎠
⎛
⎝
ωx
ωy
ωz
⎞
⎠
with
Ixx =
	
(r2 −x2) dm =
	
(y2 + z2) ρ(r) d3r ,
Ixy =
	
(−xy)
dm =
	
(−xy)
ρ(r) d3r
= Iyx ,
and cyclic permutations thereof. The 3 × 3 matrix is symmetric and has thus only
six (real) independent elements. The three on the diagonal are called the moments
of inertia, the remaining ones (without minus signs) the deviation moments, i.e.,
deviation of the direction of L from the direction of ω.
In the next section it will turn out that, for a suitable choice of axes, all the
deviation moments vanish. In addition to the three principal moments of inertia on
the diagonal, three further parameters are then required to ﬁx the orientation of the
axes, e.g., the Euler angles. This transition to diagonal form is called the principal
axis transformation.
2.2.11
Principal Axis Transformation
If I is diagonal, there are three eigenvectors ui, for which I ui is in the direction of
ui, namely the three column vectors with two components equal to zero. Since I is a
linear operator, the value of ui is of no interest here. We take unit vectors. The factors
Ii in the equation I ui = Ii ui are called eigenvalues. If I is not diagonal, then we
still have to rotate. Only DI D−1 can then be diagonal and correspondingly Dui is
an eigenvector with two vanishing components. We therefore consider
(I −Ii 1) ui = 0 ,

88
2
Classical Mechanics
and determine Ii and ui from this homogeneous linear system of equations. As is
well known, it is only soluble if its determinant vanishes:
det(I −Ii 1) = 0 .
This characteristic equation, involving 3 × 3-matrices, leads to an equation of third
order with three solutions I1, I2, I3, actually, to three such equations, viz., 0 =
det(I −I1 1) = det(I −I2 1) = det(I −I3 1). These solutions are all real, because
I is real and symmetric. They would still be real if I were only Hermitian, i.e., if I =
I † ⇔I = I ∗. As for the orthogonal transformations, we may write the eigenvectors
ui as a column matrix Ui, and for Ii Ui its three elements multiplied by the number
Ii. Therefore,
Ii U j
† Ui = U j
† (I Ui) = U j
† I † Ui = (I U j)† Ui = I j
∗U j
† Ui .
It follows that the eigenvalues are real, because we may set j = i and haveUi †Ui = 1,
and since (Ii −I j ∗) U j †Ui = 0, it also follows that the eigenvectors corresponding
to different eigenvalues Ii ̸= I j are orthogonal to each other, since in the given case,
we have U j = U j ∗⇔U j † = 
U j. If, however, two eigenvalues are equal, the two
eigenvectors need not be perpendicular to each other, but then any vector from the
subspace spanned by the two eigenvectors is an eigenvector, so any pair of mutually
orthogonal unit vectors may be chosen from this set: then all eigenvectors are pairwise
orthogonal to each other. Since the diagonal elements of I are sums of squares, the
eigenvalues here are not only real, but positive-deﬁnite, i.e., non-negative (positive
or zero).
When determining the principal moments of inertia, symmetry considerations are
often helpful—then we can avoid the diagonalization of I. Here, axial symmetry is
not necessary. Reﬂection symmetry with respect to a plane sufﬁces. Then from the
distribution with ρ (x, y, z) = ρ (−x, y, z), symmetric in the yz-plane, it follows
that Ixy = −
	
xy ρ d3r = Iyx as well as Ixz = Izx vanish, whence Ixx is a principal
moment of inertia. The normal to the mirror plane is a principal axis of the moment
of inertia.
For a plane mass distribution, the moment of inertia with respect to the normal to
the plane is composed of the moments of inertia of the two mutually perpendicular
axes in the plane. Hence, if we choose the x and y axes in the plane (z = 0), we ﬁnd
Ixx =
	
y2 dm, Iyy =
	
x2 dm, and Izz = Ixx + Iyy. See also Problems 2.24–2.26.
Of course, we may order the eigenvectors so that they form a right-handed frame.
Thenwitharotation D wearriveatthesenewunitvectorsand,asonp.36,wemayalso
set I ′ = DI D−1.Thesumandproductoftheeigenvaluescanthusbedeterminedeven
without a principal axis transformation. Because tr(AB) = tr(B A) and det(AB) =
det(B A) (and D−1D = 1), the trace and the determinant are conserved under the
principal axis transformation.

2.2 Newtonian Mechanics
89
Fig. 2.12 Poinsot’s construction. The inertial ellipsoid rolls on the invariant plane, i.e., the plane
tangential to the inertial ellipsoid at the contact point of the angular velocity ω. The angular momen-
tum L is perpendicular to this plane. As an example, of a body with the inertial ellipsoid shown here
(continuous curve) we take an appropriate cylinder (dotted line). The principal moment of inertia
about the symmetry axis (dashed curve) is 1
2mR2, while the one perpendicular to it is 1
4m (R2 + 1
3l2)
For an arbitrary axis of rotation ω, since ω = 
i ui (ui · ω), the moment of
inertia is
Iω = eω ·

r × (eω × r ) dm = eω · I eω =

i
Ii (ui · eω)2 .
It can thus be evaluated rather easily from the principal moments of inertia. Hence,
they only have to be weighted with the squares of the directional cosines of ω along
the principal axes of inertia ﬁxed in the body.
When the principal moments of inertia are known, the equation
T (ω) = 1
2 Iω ω2 = 1
2 (I1 ω1
2 + I2 ω2
2 + I3 ω3
2)
represents an ellipsoid in the variables ω, with semi-axes √2T/Ii. This is the iner-
tial ellipsoid. Clearly, ∂T/∂ωi = Ii ωi = Li, or in vector notation, just as we had
∇v T = p ,
∇ω T = L .
For a given ω, the angular momentum L is perpendicular to the tangential plane of the
inertial ellipsoid at the point of contact of ω (Poinsot’s construction) (see Fig. 2.12).
Conversely, for a given angular momentum, the rotation vector can be found at each
time using the inertial ellipsoid.
If no torque acts, then T = 1
2 ω · L and L are constant, and so also is the projection
of ω onto the the spatially ﬁxed angular momentum. The point of contact of ω then
moves on an invariant plane perpendicular to the angular momentum. The inertial
ellipsoid rolls on this plane and the center of mass is at a constant distance from
this plane. This motion is also called nutation (see Fig. 2.13). Instead of “nutation”,

90
2
Classical Mechanics
Fig. 2.13 Nutation of the ﬁgure axis (dashed line) for an axially symmetric moment of inertia. Here
the polhode cone (continuous curve) rolls on the herpolhode cone (dotted curve). As in Fig. 2.12,
an elongated top is assumed here—otherwise the polhode cone does not roll outside the herpolhode
cone, but rather inside it. Quantitatively, this rolling is described by the Euler equations (without
torque) in Sect. 2.2.12
regular precession is occasionally used, since for a precession the angular momentum
changes because a torque acts.
For an axially symmetric moment of inertia, ω(t) generates the spatially ﬁxed
herpolhode cone on which the the body-ﬁxed polhode cone rolls about the ﬁgure
axis. For an axially symmetric moment of inertia, the rotation of the ﬁgure axis
about the angular momentum axis degenerates to a nutation cone.
2.2.12
Accelerated Reference Frames and Fictitious Forces
So far the laws have been valid in arbitrary inertial systems. But in accelerated
reference frames, “ﬁctitious forces” also appear. We shall deal with those here.
In a rectilinear accelerated (body-ﬁxed) system with rK = rT −rN, the accelera-
tion ¨rK differs from that in the inertial system (¨rT) by the acceleration of the origin,
¨rN. In particular, from m ¨r = F, we have
m ¨rK = F −m ¨rN .
The last term is the additional inertial force in the accelerated system.
But in a rotating reference frame, e.g., ﬁxed in the Earth, according to our con-
siderations about rigid bodies and for arbitrary vectors x (the origin of all position
vectors being ﬁxed), we have
dx
dt

T
=
dx
dt

K
+ ω × xK
⇐⇒
dx
dt

K
=
dx
dt

T
−ω × xK .
In particular, vK = vT −ω × rK. Taking this as an operator equation

2.2 Newtonian Mechanics
91
Fig. 2.14 Coriolis force on the Earth. Our “laboratory” rotates eastwards (indicated by the arrow
at the equator and the rotation vector at the north pole). Rectilinear motions are thus deﬂected:
motions restricted to the horizontal are thus deﬂected to the right in the northern hemisphere and
to the left in the southern hemisphere (see also Problem 2.29)
 d
dt •

T
=
 d
dt • +ω × •

K
,
we can easily obtain the second derivative with respect to time:
 d2
dt2 •

T
=
 d
dt • +ω × •
2
K
=
 d2
dt2 • + ˙ω × • + 2 ω × d
dt • +ω × (ω × •)

K
.
Hence, aT = aK + ˙ω × rK + 2 ω × vK + ω × (ω × rK), and the force equation is
m aK = F −m ˙ω × rK −2 m ω × vK −m ω × (ω × rK) .
The last term is the well-known centrifugal force. It points away from the axis of
rotation. If r⊥is the part of rK perpendicular to ω (measured from the axis of rotation),
the centrifugal force is equal to mω2r⊥.
The term −2m ω × vK is the Coriolis force, named after G.-G. Coriolis,3 which
occurs only for moving bodies and is formally similar to the Lorentz force −qB × v.
On the Earth, it is weak compared to the attraction of the Earth. Therefore, we express
the rotational vector ω in terms of the local unit vectors of the spherical coordinates
(θ, ϕ) (see Fig. 1.12): ω = ω (cos θ er −sin θ eθ). The part 2ω cos θ v × er deﬂects
horizontal motions in the northern hemisphere (0 ≤θ < 1
2π) to the right, and in the
southern hemisphere ( 1
2π < θ ≤π) to the left (see Fig. 2.14). Among other things, it
rotates the oscillation plane of Foucault’s pendulum. The remainder 2ω sin θ eθ × v
is strongest at the equator and deﬂects uprising masses to the west, i.e., against the
rotational orientation the Earth.
3Gustave-Gaspard Coriolis (1792–1843).

92
2
Classical Mechanics
The equation ˙L = M, which is valid in the inertial system, is more complicated
in the rotating system because dLT/dt = dLK/dt + ω × LK, i.e.,
˙L = M −ω × L ,
where we now leave out the index K for L (the torque refers further on to the inertial
system). On the other hand, the angular momentum and the rotational vector are
related to each other in a simpler way, because in the inertial system the moment
of inertia (of a rigid body) does not change with time. In particular, if we introduce
Cartesian coordinates along the principal axes of the moment of inertia, such that
Li = Ii ωi, then it follows that
I1 ˙ω1 = M1 + ω2ω3 (I2 −I3) ,
and cyclic permutations.
These are the Euler equations for the rigid body. We shall investigate these now for
M = 0, namely for the free top, and deal with the heavy top (M ̸= 0) in Sect. 2.4.10.
Since ˙ω = 0, the spherically symmetric top (with I1 = I2 = I3) always rotates
about a ﬁxed axis. With the axially symmetric top (I1 = I2 ̸= I3), only the component
along the symmetry axis is conserved ( ˙ω3 = 0 ⇒ω3 and L3 constant). With the
ﬁxed vector
 ≡I3 −I1
I1
ω3 e3 ,
and because ˙ω1 = − ω2 ˙ω2 =  ω1 ˙ω3 = 0, the Euler equations (for I1 = I2) can
be taken together as
˙ω =  × ω .
Thus the vector ω moves with angular frequency  on a polhode cone about the
body-ﬁxed ﬁgure axis. The opening angle of the cone is determined by the integration
constants, e.g., energy and value of the angular momentum.
For a three-axis inertial ellipsoid (I1 ̸= I2 ̸= I3), all three components of ω change
in the course of time. Then Poinsot’s construction can lead us to the result. In any case,
T = 1
2 ω · L is a constant of the motion (if no torque acts) and therefore L = ∇ωT .
Problem 2.28 will also be instructive here.
2.2.13
Summary of Newtonian Mechanics
Newton identiﬁed three basic laws for non-relativistic mechanics: the inertial law
which says that force-free bodies move in a uniform rectilinear way or are at rest
(this allows us to draw conclusions about mass ratios in collision processes), the
equation ˙p = F (where p is an abbreviation for mv), and the law of “action and
reaction”. Without the action of a force, the momentum p is conserved—we only

2.2 Newtonian Mechanics
93
need to investigate those motions that are affected by forces. We have explained this
in some detail for collisions and the motion of planets. Here the bodies were treated
as point masses. We then also treated extended rigid bodies, describing their motion
about the center of mass with the Euler equations. In accelerating reference frames,
ﬁctitious forces must also be accounted for, e.g., the centrifugal and Coriolis forces.
2.3
Lagrangian Mechanics
2.3.1
D’Alembert’s Principle
We could have considered many more applications of Newtonian mechanics. Basi-
cally, there will be no new physical effects in the next few sections. These will only
appear in electrodynamics (relativity theory), quantum mechanics, and statistical
mechanics. But with new notions and better mathematical methods, we can often
simplify the workload and even obtain a complete mastery of it. In particular, we
shall deal more easily with “geometric constraints” (forces of constraint)—this is
accomplished by Lagrangian mechanics.4
Here we generalize the notion of momentum and, in addition to the mechani-
cal momentum mv considered so far, introduce also the canonical momentum p.
Therefore, instead of the usual letter p, we shall always write mv for the mechanical
momentum from now on.
To begin with, we generalize the principle of virtual work (p. 58) of statics to
time-dependent processes, i.e., to d’Alembert’s principle. Here the inertial force
−d(mv )/dt appears as a new force:

i

Fi −d(mivi)
dt

· δri = 0 ,
for δt = 0 .
As long as we neglect frictional forces, forces of constraint do not contribute, i.e., Zi ·
δri = 0.Thenweonlyneedtoaccountfortheremainingforces.Forthedetermination
of the force of constraint for accelerated bodies, we have to use the expression
Z = m ˙v −F, and the body presses against the geometrically formulated boundaries
with the opposite force.
If, for example, we enforce a curved orbit with the curvature radius R for a given
velocity v, then according to p. 7, the normal acceleration is v2/R eN. A force of
constraint equal to m (v2/R) = m ω2 R will thus be necessary, if no further force
acts—only then will the centrifugal force be canceled. Since inertial forces occur
only for accelerations, they can be taken as ﬁctitious forces, and can be “transformed
away” in an accelerated reference frame. To do this we generally require curvilinear
4Joseph Louis de Lagrange (1736–1813) became professor in Turin in 1755, was Euler’s successor
in Berlin in 1766, and became professor in Paris in 1787.

94
2
Classical Mechanics
coordinates—this idea leads to general relativity theory, where we use the fact that
the gravitational and inertial masses are always equal.
As long as no forces of constraint occur, we do not need d’Alembert’s principle,
as we have seen in the last section. But otherwise this principle is very useful—in
statics the principle of virtual work may be employed repeatedly. And now we even
know the generalization to changes in time.
Correspondingly, we can generalize the Lagrangian equations of the ﬁrst kind
from statics (see p. 61) to time-dependent processes:
F +

n
λn ∇n = d(mv )
dt
.
This equation refers to one particle—as in statics it can be generalized to more
particles. Then further coordinates and masses are involved.
2.3.2
Constraints
We already know an example of constraints from the case of the rigid body: instead
of introducing 3N independent coordinates (degrees of freedom) for N point masses,
sixaresufﬁcient, becausefor arigidbodytheremainingones canbechosenas ﬁxed—
clearly an example of “geometrical” constraints. Something like this has already been
encountered in statics: for a displacement along a line, there is only one degree of
freedom, for the displacement on a plane there are only two. A constraint is said to
be holonomic or integrable if it can be brought into the form  (t, r1, . . . , rN) = 0.
(The Greek holos means whole or perfect, implying that it can be integrated.) If
the constraint refers to velocities or if it can be expressed only differentially or as
an inequality, e.g., conﬁnement within a volume, then we are dealing with a “non-
holonomic” (non-integrable) condition. (Sometimes constraints given as inequalities
are referred to as unilateral or bilateral, because the forces of constraint act only
in one direction or two.) If a constraint does not depend explicitly upon time then
it is said to be scleronomous (skleros means ﬁxed or rigid), otherwise rheonomous
(rheos means ﬂowing). In statics, we always assumed holonomic and scleronomous
constraints. They are barely simpler than the differentials—they occur, e.g., when a
wheel rolls on a plane. Then its rotation is related to the motion of the contact point
(Problem 2.7).
Instead of constraints, we can also introduce forces of constraint which ensure
that the constraints are respected: constraints and forces of constraint are two pic-
tures for the same situation, because both allow us to deal with the motion of the
body. However, geometrical constraints are intuitively descriptive, while forces of
constraint often have to be computed, something that is necessary, however, when
designing machines in order to determine forces and loads.

2.3 Lagrangian Mechanics
95
In general, constraints couple the equations of motion. But for holonomic con-
straints, the number of independent variables can often be reduced by a clever choice
of coordinates. Then positions can no longer be described by three-vectors, and
the coordinates are often different physical quantities, appearing, e.g., as angles or
amplitudes of a Fourier decomposition. In Hamiltonian mechanics, we may also take
(angular) momentum components and energy as new variables.
In the following, we shall neglect kinetic friction. Then the forces of constraint
do not lead to tangential acceleration, but just a normal acceleration, whence they
cannot perform work, being perpendicular to the allowed displacements—as long
as no kinetic friction perturbs the system, we do not need to account for forces of
constraint in the energy conservation law.
If the constraints lead to a single degree of freedom and are scleronomous, then the
energy (conservation) law helps—so instead of one differential equation of second
order, only one of ﬁrst order remains to be solved (with the energy as integration
constant):
E = m
2 v2 + V (x)
=⇒
dx
dt =

E −V (x)
m/2
.
Of course, there can only be curl-free forces here, otherwise there is no potential
energy.
2.3.3
Lagrange Equations of the Second Kind
For time-dependent problems, we start from d’Alembert’s principle, i.e., from the
equation 
i{Fi −d(mivi)/dt} · δri = 0 for δt = 0. Since
δri =
f

k=1
∂ri
∂xk δxk ,
where the δxk do not depend upon each other (otherwise Lagrangian parameters are
still necessary)—or in particular if there is only one δxk ̸= 0—and since
Fk ≡
N

i=1
Fi · ∂ri
∂xk ,
we ﬁnd the equations
Fk =

i
d(mivi)
dt
· ∂ri
∂xk
for k ∈{1, . . . , f } .

96
2
Classical Mechanics
The right-hand side can be simpliﬁed:
d(mv )
dt
· ∂r
∂xk = d
dt

mv · ∂r
∂xk

−mv · ∂v
∂xk ,
and since v = ˙r, we also have ∂r/∂xk = ∂v/∂˙xk, because t is treated as the orbital
parameter, whence
d(mv )
dt
· ∂r
∂xk = d
dt

mv · ∂v
∂˙xk

−mv · ∂v
∂xk .
But now we have v · dv = 1
2 d(v · v) = dT/m and therefore, with T = 
i Ti,

i
d(mivi)
dt
· ∂ri
∂xk = d
dt
 ∂T
∂˙xk

−∂T
∂xk .
Finally, the f equilibrium conditions Fk = 0 can be generalized to
Fk ≡

i
Fi · ∂ri
∂xk = d
dt
 ∂T
∂˙xk

−∂T
∂xk ,
for k ∈{1, . . . , f } .
These are the generalized Lagrange equations of the second kind. In general, how-
ever, we also assume that the external forces can be derived from a potential energy:
Fi = −∇iV (r1, . . . , rN)
=⇒
Fk = −

i
∇iV · ∂ri
∂xk ≡−∂V
∂xk .
Since ∂V/∂˙xk = 0, we introduce the Lagrange function
L = T −V ,
and we obtain the Lagrange equations of second kind (Euler–Lagrange equations)
d
dt
 ∂L
∂˙xk

−∂L
∂xk = 0 ,
for k ∈{1, . . . , f } .
Many problems of mechanics can be solved with these. We need only the scalar
Lagrange function L and a convenient choice of coordinates.
Let us consider as an example the plane motion of a particle of mass m under
arbitrary (but not necessarily curl-free) forces. In Cartesian coordinates x, y, we have
T = 1
2m (˙x2 + ˙y2) ,
and consequently,

2.3 Lagrangian Mechanics
97
∂T
∂˙x = m ˙x ,
∂T
∂˙y = m ˙y ,
∂T
∂x = ∂T
∂y = 0 .
Therefore, for constant mass, the Lagrange equations lead to Newton’s relation F =
m ¨r. In contrast, in (curvilinear) polar coordinates r, ϕ, we have
T = 1
2m (˙r2 + r2 ˙ϕ2) ,
and consequently,
∂T
∂˙r = m ˙r ,
∂T
∂˙ϕ = m r2 ˙ϕ ,
∂T
∂r = m r ˙ϕ2 ,
∂T
∂ϕ = 0 .
With Fr ≡F · ∂r/∂r = F · r/r and Fϕ ≡F · ∂r/∂ϕ = F · r eϕ (according to p. 40),
whence Fϕ = F · (n × r) = (r × F) · n = M · n, we have
Fr = m ¨r −m r ˙ϕ2
and
Fϕ = d
dt (m r2 ˙ϕ) .
According to the ﬁrst equation for the radial motion, in addition to Fr, the centrifugal
force mr ˙ϕ2 is accounted for, and for ˙ϕ we have so far set ω, e.g., on p. 91. The second
equation has been written so far as M = dL/dt, because L · n = mr2 ˙ϕ. From our
new viewpoint, it is the same equation as F = d(mv )/dt, only expressed in other
coordinates.
2.3.4
Velocity-Dependent Forces and Friction
For time- and velocity-dependent forces, there is no potential energy and thus also
no Lagrange function as yet. But in fact, a generalized potential energy U with the
property
Fk = −∂U
∂xk + d
dt
 ∂U
∂˙xk

,
for k ∈{1, . . . , f } ,
also sufﬁces, because then the generalized Lagrange function
L = T −U
obeys the Lagrange equations of the second kind.
The most important example is the Lorentz force on a charge in an electromagnetic
ﬁeld:
F = q (E + v × B ) .

98
2
Classical Mechanics
In order to derive this from a generalized potential energy U, we employ the two
Maxwell equations
∇× E = −∂B
∂t ,
∇· B = 0 .
According to this the two vector ﬁelds E and B are related to each other and can be
derived from a scalar potential  and a vector potential A, taken at the coordinates
of the test body:
E = −∇ −∂A
∂t ,
B = ∇× A .
The two potentials  and A are functions of t and r (but not v ). Hence the position
of the test body depends upon the time, and therefore total and partial derivatives are
to be distinguished from each other: dA/dt −∂A/∂t = (v · ∇) A. But since r and v
are to be treated as independent variables, we may set v × (∇× A ) = ∇(v · A ) −
(v · ∇) A, because the terms to be expected formally −A × (∇× v) −(A · ∇) v do
not contribute. This leads to
F = q

−∇( −v · A ) −dA
dt

.
Therefore, the generalized potential energy for the Lorentz force on a charge q in
the electromagnetic ﬁeld is
U = q ( −v · A ) .
However, the potentials are not yet uniquely determined. In particular, we may still
have gauge transformations: ′ =  + ∂/∂t and A ′ = A −∇ deliver the same
ﬁelds E and B as  and A. This gauge invariance of the ﬁelds leads to the fact
that U ′ = q (′ −v · A′) = U + q d/dt can be taken as the generalized potential
energy, corresponding to the undetermined Lagrange function (an example is given
in Problem 2.31)
L′ = L −dG
dt .
We will come back to the gauge dependence of the Lagrange function in Sect. 2.4.5.
There it will also be understood why we write G here instead of q, because there G
is a generating function (generator) of a canonical transformation. (However, here
G depends only upon t and xk, while there it may also depend on further variables.)
For friction there is no generalized potential energy U. Then we have to take
d
dt
 ∂L
∂˙xk

−∂L
∂xk = fk ,

2.3 Lagrangian Mechanics
99
where fk contains all forces which cannot be derived from a generalized potential
energy U.
There are many examples where the frictional force is proportional to the velocity,
i.e., f = −αv, which is called Stokes friction, contrasted with Newtonian friction,
where f ∝−vv (e.g., for the case of free fall), e.g., laminar ﬂow (only turbulent ﬂow
leads to a squared term) or electrical loop currents with Ohm resistance. Stokes-type
friction also occurs in the Langevin equation (Sect. 6.2.7). Then we may set
f = −∇vF ,
with F = α
2 v · v and α > 0 ,
where F is Rayleigh’s dissipation function. It supplies half the power which the
system has to give off because of the friction: dA = −f · dr = −f · v dt = α v2 dt =
2 F dt. Inthis casewealsoneedtwoscalar functions, L andF, toderivetheequation
of motion (and to describe the thermal expansion).
But for this friction and ∂L/∂˙x = m ˙x, we can also take the Lagrange function
L exp(αt/m) (now time-dependent). The unknown term α ˙x then appears in addition
to d(∂L/∂˙x)/dt −∂L/∂x.
2.3.5
Conserved Quantities. Canonical and Mechanical
Momentum
TheLagrangefunction L(t, x, ˙x)containsthevelocityinanon-linearway.Therefore,
the Lagrange equation (without friction!),
d
dt
 ∂L
∂˙xk

= ∂L
∂xk ,
is a differential equation of second order, because ¨x occurs. We search for “solutions”
C(t, x, ˙x) = 0, which are differential equations of only ﬁrst order.
This is straightforward if L does not depend on xk, but on ˙xk :
∂L
∂xk = 0
=⇒
d
dt
∂L
∂˙xk = 0
=⇒
∂L
∂˙xk = const.
The assumption ∂L/∂xk = 0 is justiﬁed if we can move the origin of xk with
impunity, i.e., if we can add an arbitrary constant to xk. For example, the dynamics of
a rotating wheel does not depend on the angle coordinate ϕ, but only on the angular
velocity ˙ϕ. Therefore, all coordinates which do not appear in L are said to be cyclic
—a further important example of cyclic coordinates is given in Problem 2.32.
Generally, ∂L/∂˙xk is called the canonical momentum conjugate to xk :
pk ≡∂L
∂˙xk
=⇒
˙pk = ∂L
∂xk .

100
2
Classical Mechanics
(Here we have the decisive quantity for Hamiltonian mechanics, as we shall see in the
next section.) A free point mass has L = m
2 v · v and p = mv, whence p = ∇vL. For
rotations, we have L = 1
2 I ˙ϕ2 and we obtain pϕ = I ˙ϕ as the canonical momentum,
i.e., the angular momentum, or more precisely, the angular momentum component
along the corresponding axis of rotation. This holds even if a potential energy V also
appears. If, however, a point mass with charge q moves in an electromagnetic ﬁeld,
then L = m
2 v · v −q ( −v · A ), and hence the canonical momentum is
p = mv + qA .
It differs from the mechanical momentum mv by the additional term qA and depends
on the gauge, whence p ′ = p −∇G is a canonical momentum.
In the following, p will always denote the canonical momentum and mv the
mechanical one. Therefore, we may no longer call ˙p a force F, because we have
d(mv )
dt
= ˙p −q ˙A ,
and according to the last section,
F = −∇U −q ˙A .
Consequently,
d(mv )
dt
= F
=⇒
˙p = −∇U
delivers a noteworthy result.
A homogeneous magnetic ﬁeld B can be obtained from the vector potential A =
1
2 B × r (among others), and since  ≡0, this leads to −∇U = 1
2q v × B. Here ˙p
is thus equal to half the Lorentz force.
In a constant and homogeneous magnetic ﬁeld, which thus does not depend upon
either t or r, neither the mechanical nor the canonical momentum is conserved.
However, since m ˙v = q˙r × B, only the pseudo-momentum
K ≡mv + q B × r
is conserved. In fact, on the helical orbit, only the mechanical momentum in the ﬁeld
direction is conserved (K∥= mv∥). Perpendicular to it, there is a circular orbit, and
we use ω = −qB/m from p. 78. Using
mv⊥= mω × r + K⊥= mω × (r −rA) ,
which implies
K⊥= mrA × ω ,

2.3 Lagrangian Mechanics
101
we infer on the helical axis
rA = ω
ω × K
mω ,
and the radius v⊥/ω.
The canonical momentum conjugate to a cyclic variable is conserved according to
whatwassaidabove,i.e., ˙p = ∂L/∂x = 0.Therefore,for(inﬁnitesimal)translational
invariance, the momentum is conserved, and for isotropy (rotational invariance), the
angular momentum is conserved.
If L does not depend explicitly on time, then, according to the Lagrange equation,
we have
dL
dt =

k
 ∂L
∂xk ˙xk + ∂L
∂˙xk
d ˙xk
dt

=

k

˙pk ˙xk + pk ¨xk
= d
dt

k
pk ˙xk .
Thus, for ∂L/∂t = 0, the sum 
k pk ˙xk −L is also a conserved quantity (constant
of the motion). Here 
k pk ˙xk is equal to 2T if the kinetic energy T is a homogeneous
function of second order in the velocity, thus if T (kv ) = k2 T (v ) holds for all real k,
which according to Euler’s identity for continuously differentiable T is equivalent to
v · ∇vT (v ) = 2 T (v ). (For time-independent constraints, but not for time-dependent
ones, T is homogeneous of second order.) Thus for ∂L/∂t = 0 and 2T = v · ∇vT ,
the quantity 2T −L is conserved. If there is in addition a potential energy V , then
L = T −V and the energy T + V is conserved.
2.3.6
Physical Pendulum
Here we discuss a rigid body of mass m with moment of inertia I with respect to a
(horizontal) axis of rotation: a plane pendulum. (A rotational pendulum would move
freely about a point, as discussed in Sect. 2.4.10. A mathematical pendulum is a
point mass which moves. It has I = m s2, but is otherwise not easy to treat. On the
other hand, friction is neglected for the time being. It will be accounted for in the
next section.) The angle θ gives the displacement from the equilibrium position (see
Fig. 2.15).
In this situation, the kinetic and potential energies are
T = 1
2 I ˙θ2
and
V = 2 I ω2 sin2 1
2θ ,
with ω2 ≡m g s
I
.
As in the case of free fall (Sect. 2.2.8), we have assumed here that the gravitational
ﬁeld of the Earth is homogeneous. As an aside, formally the same expression holds
for an electric dipole moment p in a homogeneous electric ﬁeld E, because there
the potential energy is V = −p · E (see Sect. 3.1.4), and for a magnetic moment m

102
2
Classical Mechanics
Fig. 2.15 Plane pendulum. The center of mass (full circle) is a distance s from the axis of rotation
(open circle) and a height h = s (1−cos θ) = 2s sin2 1
2θ above the equilibrium position. Right:
Potential energy V (relative to 2mgs) as a function of θ. Dashed curve: Approximation for harmonic
oscillation
in a homogeneous magnetic ﬁeld B, since V = −m · B, according to Sect. 3.2.9.
The following considerations can also be transferred to the pendulum motion of an
undamped compass needle, because for V , the origin is not important and Iω2 is
then the product of the dipole moment and the ﬁeld strength.
As stressed in Sect. 2.3.2, for such problems with a single unknown and time-
independent energy T + V , conservation of energy is useful:
E = 2 I {( 1
2 ˙θ)2 + ω2 sin2 1
2θ } .
According to this, ( 1
2 ˙θ)2 = E/2I −ω2 sin2 1
2θ, which is a differential equation of
ﬁrst order for the unknown function θ(t).
Small pendulum amplitudes are generally considered, and we may set sin 1
2θ ≈
1
2θ. This leads to the differential equation E = 1
2 I ( ˙θ2 + ω2θ2) for harmonic oscil-
lation, viz.,
θ(t) = θ0 cos(ωt) + ( ˙θ0/ω) sin(ωt) = θ cos (ωt −φ) ,
with the initial values θ(0) ≡θ0 = θ cos φ and ˙θ(0) ≡˙θ0 = ωθ sin φ. The amplitude
θ then follows from θ 2 = 2E/Iω2 = θ02 + ( ˙θ0/ω)2 and the phase shift (at zero
time) φ from tan 1
2φ = (θ −θ0) ω/ ˙θ0. Note that we use the equation tan 1
2φ = (1 −
cos φ)/ sin φ, not the more suggestive tan φ = sin φ/ cos φ, because this gives φ
uniquely only up to an even multiple of π. As the integration constant we thus have
either the energy E (or, respectively, θ ) and the phase shift φ or the initial values θ0
and ˙θ0.
However, we would like also to allow for larger pendulum amplitudes, and for
that we use the abbreviation (with k ≥0)
k2 ≡
E
2Iω2 =
E
2mgs
and
x ≡1
2θ .

2.3 Lagrangian Mechanics
103
Fig. 2.16 Pendulum trajectories in phase space (y ∝p). These are solutions of the equation y2+
sin2 x = k2, here for k2 from 0.2 to 1.8 in steps of 0.2 and a periodicity interval −1
2π ≤x ≤1
2π.
Thus x = 1
2θ and y = ˙x/ω. The dashed red curve (k2 = 1) is the separatrix—it separates the rotating
solutions (green) from the librations (blue). The curves are always plotted clockwise ⟳: for x > 0,
the velocity decreases (¨x < 0), for x < 0, it increases. This happens also for the damped oscillation
(see Fig. 2.21)
We then have the non-linear differential equation k2 = ω−2 ˙x2 + sin2 x. So far we
have restricted ourselves to k ≪1 and so have been able to use sin x ≈x. In this
way, in the (x, ˙x)-plane, we obtained an ellipse with semi-axes k and kω. With
increasingk (<1),theellipseincreasesinsizeandchangesshape—infact,itnolonger
remains an ellipse. For k = 1, we have ˙x = ±ω cos x. The requirement | sin x | ≤k
limits the x values for k < 1 (then k = sin 1
2θ), but no longer for k > 1. Hence,
the pendulum rotates (see Fig. 2.16). In all cases, the highest angular velocity is
˙θmax = 2ωk = √2E/I. For k ≫1, the term sin2 x is negligible compared to ω−2 ˙x2
and the pendulum then rotates with constant angular velocity ˙θ.
In the differential equation ˙x2 = ω2 (k2 −sin2 x), the variables can be separated:
ω dt =
dx

k2 −sin2 x
.
We ﬁrst consider the oscillations (the case k < 1) and then the rotating solutions
(k > 1). In both cases, we choose the zero time (i.e., the second ﬁtting parameter in
addition to k or E) at θ(0) = 0 (with ˙θ > 0).
For k < 1, we transform sin x = k sin z, thus cos x dx = k cos z dz: the denom-
inator becomes k cos z and dx/

k2 −sin2 x becomes dz/

1 −k2 sin2 z. Then we
arrive at the incomplete elliptic integral of the ﬁrst kind (in the Legendre normal
form)
F(ϕ | k2) ≡
 ϕ
0
dz

1 −k2 sin2 z
,
and hence,

104
2
Classical Mechanics
Fig. 2.17 Dependence of the oscillation period T on the pendulum amplitude, here in relation
to the oscillation period T0 = 2π/ω for small amplitude. Dashed blue curve: Limiting curve
(2/π) ln(4/cos 1
2θ) for large amplitude. Continuous red curve: Complete elliptic integral of the
ﬁrst kind K(sin2 1
2θ) up to a factor 1
2π (see Fig. 2.33)
ω t = F(arcsin sin( 1
2θ)
k
| k2) .
This equation yields the oscillation period T (see Fig. 2.17), because for 1
4T , we
have sin 1
2θ = k or ϕ = 1
2π:
1
4ωT = F( 1
2π | k2) ≡K(k2) .
Here K(k2) is a complete elliptic integral of the ﬁrst kind. (More details on the special
functions mentioned here can be found, e.g., in [1], or in particular [2].)
The Legendre normal form of the the elliptic integrals mentioned here depends
on a circular function. If we take sin z as integration variable t, then the incomplete
elliptic integral reads
F(ϕ | k2) =
 sin ϕ
0
dt

(1 −t2)(1 −k2t2)
,
and the complete elliptic integral
K(k2) =
 1
0
dt

(1 −t2)(1 −k2t2)
.
Thus we only need a purely algebraic integrand.
If the pendulum oscillates with small angle amplitudes, then k2 ≈0. If we expand
the integrand for k2 < 1 in terms of powers of k2 and integrate term by term, this
yields

2.3 Lagrangian Mechanics
105
Fig. 2.18 The amplitude of
the elliptic functions, ϕ =
am F, during a quarter period
for k2 = 0 (black), 0.5 (red),
0.9 (blue), and 0.99 (green).
This is needed for the Jacobi
functions, e.g., sine
amplitudes (see Fig. 2.31).
The dependence of the
inverse functions F(ϕ) can
also be read off
K(k2) = π
2
∞

n=0
(2n)!2
24n n!4 k2n ,
for k2 < 1 ,
and thus T = 2πω−1 (1 + 1
4k2 + 9
64k4 + · · · ) . Only for amplitudes larger than 23◦
does the bracket deviate by more than 1% from unity. In the special case k2 = 1, the
oscillation period T increases beyond all limits, because for k′ =
√
1 −k2 ≪1 it is
K(k2) =
∞

n=0

−1
2
n

ln 4
k′ −2
2n

j=1
(−) j−1
j

k′ 2n = ln
4
√
1 −k2 + · · · ,
as proven in Fig. 3.14. We shall use these relations in electrodynamics.
In order to obtain the amplitudes as a function of time, however, we also need
the inverse functions of the incomplete elliptic integrals of the ﬁrst kind, namely the
(angle) amplitude of F (see Fig. 2.18):
τ = F(ϕ | k2)
⇐⇒
ϕ = am(τ|k2) ≡am τ .
Then our result with τ = ωt can be brought into the form (see Fig. 2.19)
sin( 1
2θ) = k sin(am τ) ≡k sn(τ | k2) ≡k sn τ .
The Jacobi elliptic function sinus amplitudinis sn τ arises. It is odd in τ and, like all
elliptic functions, it is doubly periodic, if we allow for a complex arguments:
sn τ = sn{τ + 4 K(k2)} = sn{τ + 2i K(1−k2)} .
For k2 = 0, it is sin τ, and for k2 = 1 (with K →∞), it is tanh τ.
For the rotating solutions (with k > 1), the calculation is easy, because here,
even without the above-mentioned transformation x →z, the differential equation

106
2
Classical Mechanics
Fig. 2.19 Pendulum
amplitude θ for one period
when k2 = 0.5 (red), 0.9
(blue), and 0.99 (green)
ω dt = dx/

k2 −sin2 x = k−1 dx/

1 −k−2 sin2 x leads to an incomplete elliptic
integral of the ﬁrst kind:
ω t = F( 1
2θ | k−2)
k
and
1
2θ = am(kωt | k−2) .
For θ = π, we have half a rotation and the time K(k−2)/kω.
2.3.7
Damped Oscillation
If we had restricted ourselves to small displacements above, then we would still have
had the simple differential equation for the harmonic oscillation:
¨x + ω0
2 x = 0 .
Multiplying by ˙x and integrating over t, we deduce the “conservation of energy”,
˙x2 + ω0
2 x2 = const.
But the harmonic oscillation can also be perturbed by other additional terms—in
particular, it normally decays, i.e., it is damped. We now write ω0 for the ω used so
far, because the angular frequency of the oscillation depends upon the damping, as
we shall see shortly.
We assume Stokes’s friction because only comparably small velocities occur and
therefore a term linear in ˙x will contribute more than a squared one. We thus consider
the differential equation
¨x + 2 γ ˙x + ω0
2 x = 0 ,
with γ > 0 .
In the solutions, γ can be viewed as the decay coefﬁcient and γ −1 as the decay or
relaxationtime.Becauseofthedamping,theconservationofenergydoesnothelp,but
becausethelineardifferentialequationishomogeneous,theansatz x = c exp(−i ωt)
leads to (see Fig. 2.20)

2.3 Lagrangian Mechanics
107
Fig.2.20 Dependenceofthepairω± inthecomplexω-planewithincreasingdamping.Forγ ≪ω0,
they start from ±ω0 (∗) and move symmetrically towards each other on a semi-circle (from ∗to ◦
to +) until, for γ = ω0, they coincide at −iω0 (•). Because |ω+ω−| = ω02 for γ > ω0, they move
apart again as mirror points (×) of the circle on the imaginary axis. Damped oscillations occur only
for negative imaginary part
Fig. 2.21 Damped
oscillations for γ = ω0/10.
As in Fig. 2.16, ˙x/ω0 is
represented as a function of
x, with equal time intervals
between neighboring points
(•). For other initial values,
the ﬁgure is rotated about the
origin (◦), where all orbits
end. This is the attractor of
all orbits
ω2 + 2i γ ω = ω0
2
=⇒
ω± = ±

ω02 −γ 2 −iγ .
In the following, the angular frequency
 ≡

|ω02 −γ 2|
will be useful, because ω± = ± −iγ for γ < ω0 and ω± = −i (γ ∓) with γ >
 for γ > ω0.
Hence we have two linearly independent solutions exp(−i ω±t). Note that, for
γ = ω0, the two solutions x± coincide, but their difference at the transition γ →ω0
is to a ﬁrst approximation proportional to t exp(−γ t), which then delivers a linearly
independent solution. Therefore, we can adjust x(t) to the initial values x0 and ˙x0
(see Fig. 2.21):

108
2
Classical Mechanics
Fig. 2.22 Left: Critically damped oscillation (γ = ω0). Right: Supercritically damped oscillation
(with γ = 2ω0). The representation is the same as in the last ﬁgure, except that here the trajectories
depend upon the initial conditions, but all ﬁnish at the origin
γ < ω0 :
x = exp(−γ t)

x0 cos t + ˙x0 + γ x0

sin t

,
γ = ω0 :
x = exp(−γ t)

x0 + (˙x0 + γ x0) t

,
γ > ω0 :
x = exp(−γ t)

x0 cosh t + ˙x0 + γ x0

sinh t

.
Except for the exponential factor in front of the brackets, the last two brackets no
longer describe periodic motion. What we have here is in fact aperiodic damping:
for γ = ω0, critical damping, for γ > ω0, supercritical damping (see Fig. 2.22).
2.3.8
Forced Oscillation
For the time being, we assume a force acting periodically with angular frequency ω
and consider the inhomogeneous linear differential equation
¨x + 2 γ ˙x + ω0
2x = c cos(ωt) .
On the right-hand side, we could could have assumed a Fourier integral, and then
we would have to superpose the corresponding solutions. The general solution is
composed of the general solution of the homogeneous equation treated above and
a special solution of the inhomogeneous equation. The special solution describes
here the long-time behavior (with γ > 0), because the solutions of the homogeneous
equation decay exponentially in time—they are important only for the initial process
and are needed to satisfy the initial conditions.

2.3 Lagrangian Mechanics
109
For a special solution, we make the ansatz
x = C cos(ωt −φ) = C [cos φ cos(ωt) + sin φ sin(ωt)] .
Hence, for φ ̸= 0, the solution is delayed with respect to the exciting oscillation.
Therefore, we set ωt −φ and expect φ ≥0. In order to ensure that φ is unique (mod
2π), we require that C should have the same sign as c. With this ansatz and after
comparing coefﬁcients of cos(ωt) and sin(ωt), the differential equation leads to the
conditions
(ω0
2 −ω2) cos φ + 2γ ω sin φ = c/C > 0 ,
(ω0
2 −ω2) sin φ −2γ ω cos φ = 0 ,
which we can solve for the unknown C and φ. For unique determination of φ, we
ﬁrst consider ω = ω0 and ﬁnd here φ = 1
2π mod 2π. Hence, 0 ≤φ ≤π has to
hold. Therefore, we derive φ from tan ( 1
2π −φ) = cot φ = (ω02 −ω2)/2γ ω and
use sin φ = 1/

1 + cot2 φ for c/C (see Fig. 2.23):
C =
c

(ω02 −ω2)2 + 4 γ 2ω2
and
φ = π
2 −arctan ω02 −ω2
2γ ω
.
For ω ≈ω0, the ratio C/c is very large. For γ ̸= 0, the maximum lies at somewhat
lower frequencies than ω0. However, for larger amplitudes, the starting equation is
no longer valid, because then the free oscillation becomes anharmonic. Note also
that the phase shift φ increases with ω. For ω ≪ω0, it is negligible, for ω = ω0, it
takes the value 1
2π, and for ω ≫ω0, it tends to π. The higher the driving frequency,
the more the forced oscillation is delayed, until it ﬁnally oscillates in opposite phase.
This transition from in-phase to opposite-phase becomes ever more sudden with
decreasing damping γ .
Fig. 2.23 Forced oscillation. Left: Amplitude of the ratio ω02C/c as a function of ω/ω0. Right:
Phase shift φ for γ = 0.1 ω0 (continuous red curve) and for γ = 0.5 ω0 (dashed blue curve)

110
2
Classical Mechanics
Somewhat more concise is the treatment using complex variables. The ansatz
x = Re {C exp(−i ωt)} ,
with C = C exp(i φ)
leads via the differential equation to (ω02 −ω2 −2i γ ω) C = c. With
ω± = ±

ω02 −γ 2 −i γ
from the last section, or ω2 + 2i γ ω −ω02 = (ω −ω+)(ω −ω−), we then arrive at
C =
c
(ω −ω−)(ω+ −ω) =
c
ω+ −ω−

1
ω −ω−
−
1
ω −ω+

.
For γ ̸= ω0, the amplitude C thus has two simple poles below the real ω-axis (see
Fig. 2.20). This representation is particularly suitable when the driving force is not
purely harmonic and therefore has to be integrated (according to Fourier)—this is
then straightforward using the theorem of residues.
In addition, in many cases it is not only the long-time behavior that is of interest.
Therefore, we still wish to generalize the previous considerations. To this end, we
shall transform the inhomogeneous linear differential equation
¨x(t) + 2γ ˙x(t) + ω0
2x(t) = f (t) ,
with a Laplace transform, viz.,
x −→L {x} ≡
 ∞
0
exp (−st) x(t) dt ,
into an algebraic equation [3], where Res > 0 has to hold. Naturally, the solution
here still has to undergo the inverse Laplace transform. Note that the great advantage
of the Laplace transform over the similar Fourier transform is the fact that only one
integration limit is unrestricted. The Laplace-transformed derivative ˙x is equal to
L {˙x} = s L {x} −x(+0) ,
since partial integration delivers
	 ∞
0 e−st ˙x dt = e−stx
t=∞
t=0 + s
	 ∞
0 e−stx dt. The
region t < 0 is of no interest. Hence for t = 0, x may even jump from x(−0) to
ﬁnite x(+0). Since L {¨x} = s (sL {x}−x(0))−˙x(0) and s2 + 2γ s + ω02 = (s +
i ω+)(s + i ω−), the original differential equation leads to
L {x} = L { f } + (s+2γ ) x(0) + ˙x(0)
(s + i ω+)(s + i ω−)
.
The result may also be written as

2.3 Lagrangian Mechanics
111
L {x} = L {x0} + L {g} · L { f } ,
with
L {g} ≡
1
(s + i ω+)(s + i ω−) =
i
2

ω02 −γ 2

1
s + i ω+
−
1
s + i ω−

,
if x0(t) solves the associated homogeneous differential equation under the given
initial conditions: ¨x0 + 2γ ˙x0 + ω02x0 = 0, along with x0(0) = x(0) and ˙x0(0) =
˙x(0). According to the last section, we can determine this auxiliary quantity.
The product of the Laplace-transformed L {g} · L { f } comes from a convolution
integral:
x(t) = x0(t) +
 t
0
g(t −t′) f (t′) dt′ .
Since we are only interested here in 0 ≤t′ ≤t, we may then amend both functions g
and f so that they vanish for negative arguments. Then we may also integrate from
−∞to +∞. This leads to the convolution theorem, as for the Fourier transform on
p. 22, because the Laplace transform
L {F} =
 ∞
−∞
exp(−st) g(t −t′) f (t′) dt dt′
arises for the function F(t) =
	 ∞
−∞g(t −t′) f (t′) dt′. And because we have
exp(−st) = exp{−s(t −t′)} exp(−st′)withthenewintegrationvariablesτ = t −t′
(and equal integration limits for τ and t), this double integral can be split into the
product of the Laplace-transformed functions of g and f , as required.
In order to determine g, we compare the expression {(s + i ω+)(s + i ω−)}−1 for
L {g} with that for L {x}. The two Laplace-transformed functions are apparently
equal, if x(0) = 0, ˙x(0) = 1 holds and f vanishes—the oscillation is not forced. If
we set τ ≡t −t′, then for g(τ), the constraints are
g(0) = 0 ,
˙g(0) = 1 ,
and
¨g + 2γ ˙g + ω0
2g = 0 .
Consequently, according to the last section, we already know g(τ). In particular,
we have g(τ) = exp(−γ τ) −1 sin(τ) with  =

ω02 −γ 2 for γ < ω0 (see
Fig. 2.24).
Note that the integral often extends to ∞, where g(τ) then has to vanish for τ < 0.
This function remains continuous, but its ﬁrst derivative at τ = 0 has to jump from
zero to one. This leads to the differential equation ¨g(τ) + 2γ ˙g(τ) + ω02g(τ) = δ(τ),
thusthestartingequationwith f (τ) = δ(τ)asinhomogeneity.Generally,solutionsof
lineardifferentialequationswiththedeltafunctionasinhomogeneityarecalledGreen
functions. Using these, the solutions for other inhomogeneities can be represented

112
2
Classical Mechanics
Fig. 2.24 Green function g(τ) (for the damped oscillator). For τ ̸= 0, it satisﬁes the homogeneous
differential equation ¨g + 2γ ˙g + ω02g = 0. For τ = 0, its ﬁrst derivative jumps by one. Hence, the
second derivative is given there by the delta function δ(τ)
as convolution integrals (Problem 2.38). We encountered the Green function for the
Laplace operator on p. 26, and the one here will be generalized in Sect. 2.3.10.
If for ﬁnite damping only the long-time behavior is of interest, then we may leave
out x0(t) and take −∞as the lower integration limit. Then we arrive at a convolution
integral from −∞to +∞.
2.3.9
Coupled Oscillations and Normal Coordinates
So far we have restricted ourselves to oscillations of just one coordinate. Now we
consider several coordinates ( f > 1), e.g., a double pendulum (one hanging from the
other) or several point masses coupled to each other by springs (atoms in a molecule
or in a crystal). Here we start from a conservative system with the potential energy
V (x1, . . . , x f ) and choose the origin of all f coordinates xk in their equilibrium
position. Then all forces vanish:
Fk = −∂V
∂xk

0 = 0 ,
for k ∈{1, . . . , f } .
We assume a stable equilibrium, i.e., small displacements from the equilibrium cost
energy. Then the extremum of V has to be a local minimum, and for the corresponding
gauge, according to Taylor, we have
V = 1
2

kl
∂2V
∂xk ∂xl

0 xk xl ≡
1
2

kl
Akl xk xl ,
with
Akl = Alk = Akl
∗,
if we neglect higher-order terms—the pendulum is just barely displaced, and no
anharmonic forces act between the atoms. Here the coefﬁcients do not depend upon
the time t.
In addition, we need the kinetic energy, for which we make an ansatz of the form
T = 1
2

kl
Bkl ˙xk ˙xl ,
with
Bkl = Blk = Bkl
∗,

2.3 Lagrangian Mechanics
113
where it is assumed that these coefﬁcients do not depend on time (which is approx-
imately true only occasionally). In any case, no linear terms in ˙xk should appear,
because otherwise they would change sign for t →−t.
For k ∈{1, . . . , f } and since Akl = Alk and Bkl = Blk, the Lagrange equations
now deliver
0 = d
dt
∂L
∂˙xk −∂L
∂xk =

l
Bkl ¨xl +

l
Aklxl .
If we take A and B as square matrices and (x1, . . . , x f ) as a row vector x, we then
have
V = 1
2 x A x
and
T = 1
2 ˙x B ˙x ,
and also
B ¨x + A x = 0 ,
or ¨x = −B−1 A x. For one degree of freedom ( f = 1), we could have written simply
ω2 instead of the matrix product B−1A.
Now we would like to make a transition to new coordinates, called normal coor-
dinates x′, relative to which the matrices A and B become diagonal, the oscillations
thus become decoupled, and the solutions are already known. The total energy is
then the sum of the energies of the individual decoupled oscillators.
If we set
x = C x′
=⇒
V = 1
2 x′
C ACx′
and
T = 1
2 ˙x′
C BC ˙x′ ,
then we search for a matrix C, which diagonalizes 
C AC as well as 
C BC. Here we
choose the free factor—only the product Cx′ is ﬁxed—such that 
C BC = 1 holds.
Then the diagonal elements λ of 
C AC are the squares of the angular frequencies.
These are the frequencies with which the normal coordinates oscillate. The ampli-
tudes and phases are adjusted to the initial values.
In this case, Ax becomes ACx′, and with ¨x′ = −λx′, B ¨x becomes −λBCx′.
The vector Cx′ will be denoted by c and we shall seek f such column vectors and
combine them to form the matrix C. Finally, from B ¨x + Ax = 0, we have
(A −λ B) c = 0 ,
with
A = A∗= A ,
B = B∗= B ,
c = c∗.
The homogeneous linear system of equations (A −λB) c = 0 is an eigenvalue prob-
lem, because it is soluble only for suitable eigenvalues λk. With these, we determine
the eigenvectors ck. Despite the fact that in general the number of degrees of freedom
is f ̸= 3, this eigenvalue problem differs from that of the principal axis transforma-
tion for the moment of inertia in Sect. 2.2.11 in that B was a unit matrix there.

114
2
Classical Mechanics
The eigenvalues can be determined from the characteristic equation
det (A −λB) = 0 .
Since we are working with Hermitian matrices with f rows, there are f real eigen-
values λk and associated eigenvectors ck, which then follow from
(A −λk B) ck = 0 .
These eigenvectors are determined only up to a factor, which we shall soon choose
in a convenient way. So if we combine the total set of f column vectors {ck}, each
with f components, to form an eigenvector matrix C = (c1, . . . , c f ), we arrive at

C BC = 1 .
With the help of the kth diagonal element of this matrix and an appropriate “nor-
malization factor” (a scale transformation), we can choose the kth (row and) column
vector and make all non-diagonal elements—in different row and column vectors—
equal to zero. This can be seen immediately for different eigenvalues λk ̸= λk′,
because (λk −λk′) ck B ck′ is the same as ck (A −A) ck′. But for equal eigenvalues
λk = λk′ (degeneracy) all linear combinations of these eigenvectors are still eigen-
vectors, and this freedom can be exploited for 
C BC = 1. The matrix 
C AC ≡Λ is
then also diagonal. Thus we have
2 T = ˙x B ˙x = ˙x′ 
C BC ˙x′ = ˙x′ 1 ˙x′ ,
2 V = x A x = x′ 
C AC x′ = x′ Λ x′ .
In the new coordinates, the kinetic and potential energy no longer contain mixed
terms. The f harmonic oscillations are decoupled in the normal coordinates.
The eigenvalues λ of Λ are the squares of the desired angular frequencies because
they represent the harmonic oscillation in the expression 1
2m (˙x2 + ω2x2) for the
energy.
For example, for two coupled oscillations ( f = 2), we thus arrive at the eigen-
frequencies
ω±
2 = K ±
√
K 2 −4 det A det B
2 det B
,
with
K = A11B22 + A22B11 −2A12B12 .
To these belong the two eigenvectors c±, each with two components, whose ratio is
c2±
c1±
= −A11 −ω±2B11
A12 −ω±2B12
,

2.3 Lagrangian Mechanics
115
and which are normalized via c1±−2 = B11+2B12 (c2±/c1±)+B22 (c2±/c1±)2. With
this,
C =
 c1+ c1−
c2+ c2−

,
and its inverse matrix (see p. 71) can be calculated. In normal coordinates, the solu-
tions read
x±
′ = x0±
′ cos(ω±t) + ˙x0±′
ω±
sin(ω±t) ,
where the coefﬁcients x0±′ and ˙x0±′ follow from the initial conditions:
x0
′ = C−1x0
and
˙x0
′ = C−1 ˙x0 .
Note that, according to p. 102, we may thus write also x±′ = x±′ cos(ω±t −φ±).
Since all unknown quantities have then been determined from the matrix elements
of A and B and from the initial values, the solution x = Cx′ can ﬁnally be calculated
(Problems 2.39–2.42).
If the two eigenfrequencies are nearly equal (ω+ ≈ω−), then beats are formed,
i.e., the oscillation amplitudes change periodically, and this all the more clearly as
the amplitudes are close to one another. From
xi = ci+x+
′ cos(ω+t −φ+) + ci−x−
′ cos(ω−t −φ−) ,
with ω+ > ω−and positive amplitude ci±x±′ abbreviated to Ci±, together with the
notation ω± =  ± ω and φ± = φ ± ϕ, it follows that
xi = +(Ci+ + Ci−) cos(t −φ) cos(ωt −ϕ)
−(Ci+ −Ci−) sin(t −φ) sin(ωt −ϕ) .
Since ω+ ≈ω−, we have ω≪, whence the amplitude of the oscillation changes
periodically with the angular frequency  according to

(Ci+ −Ci−)2 + 4Ci+Ci−cos2(ωt −ϕ) .
Examples are shown in Fig. 2.25.
If one of the eigenvalues is zero, then this is not an oscillation, but free motion. If
no external forces act, in a ﬁrst step we separate out the center-of-mass motion and
also the rotation of a rigid body. The following considerations are necessary only for
the relative motion.

116
2
Classical Mechanics
Fig. 2.25 Examples of the displacements of two coupled oscillations as a function of time t during a
period T . Left: Equal amplitudes of cx ′. Right: For the amplitude ratio 1:2. In both cases, φ± = 1
2π
and 8ω+ = 9ω−. The oscillation amplitudes are shown by dashed lines
2.3.10
Time-Dependent Oscillator. Parametric Resonance
If the parameters kept ﬁxed so far are assumed now to change rhythmically with
time, then this affects the stability of the system. This is observed for a child’s swing,
where the moment of inertia ﬂuctuates in the course of time, and for a pendulum
if its support oscillates vertically up and down. In both cases we encounter Hill’s
differential equation
¨x + f (t) x = 0 ,
with
f (t + T ) = f (t) = f ∗(t) ,
which we shall discuss now. We shall often write ω2 instead of f , even though f
may also become negative. In the end, generalizations of the functions cos(ωt) and
sin(ωt) are obtained, which belong to constant f > 0. Incidentally, Hill’s differential
equation also arises in the quantum theory of crystals and in the theory of charged
particles in a synchrotron with alternating gradients, although t is then a position
coordinate. The Bloch function is encountered in the context of a periodic potential.
We take the two (presently unknown) fundamental solutions x1 and x2 with
the properties x1(0) = 1 = ˙x2(0) and ˙x1(0) = 0 = x2(0). Their Wronski determi-
nant x1 ˙x2 −˙x1x2 has the value 1 for all t, this being the value for t = 0 and
a constant, because its derivative vanishes. All remaining solutions of the differ-
ential equation can be expanded in terms of this basis. We clearly have x(t) =
x(0) x1(t) + ˙x(0) x2(t) since this expression satisﬁes the differential equation and
the initial conditions. We may thus write
cx(t)
˙x(t)

=
ccx1(t) x2(t)
˙x1(t)
˙x2(t)
 cx(0)
˙x(0)

≡U(t)
cx(0)
˙x(0)

,

2.3 Lagrangian Mechanics
117
and with this obtain an area-preserving time-shift matrix U(t) (since det U = 1).
We would now like to exploit the periodicity—so far we have not used it for the
time shift and, of course, we could also have introduced the matrix for other factors f .
The Floquet operator U(T ) will be important for us: for given initial conditions
it delivers x(T ) and ˙x(T ), and we have
x(t + T ) = x(T ) x1(t) + ˙x(T ) x2(t) ,
because this expression satisﬁes the initial conditions and, since f (t) = f (t + T ),
it also satisﬁes the differential equation.
Therefore, we look for the eigenvalues σ± of U(T ). For a 2 × 2 matrix U, they
follow from σ 2 −σtrU + det U = 0 and, because of det U = 1, satisfy the equa-
tions σ+σ−= 1 and σ+ + σ−= trU. We thus set σ± = exp(±i φ) and determine φ
from trU = 2 cos φ, which is uniquely possible only up to an integer multiple of π.
However, we require in addition that φ should depend continuously on f , and set
φ = 0 for f ≡0. (For f ≡0, we have trU = 2 because x1 = 1 and x2 = t.) Since
x1 and x2 are real initially and remain so for all times, trU will also be real. Since
cos(α + iβ) = cos α cosh β −i sin α sinh β, this means that either φ has to be real
(β = 0 for |trU| ≤2) or its real part has to be an integer multiple of π (α = nπ for
|trU| ≥2). For |trU| < 2, we thus have |σ±| = 1, and for |trU| > 2, it is clear that
|σ±| ̸= 1. (We will return to the degeneracy for |trU| = 2.)
For the two eigensolutions (Floquet solutions), we have x±(t + T ) = σ±x±(t).
Then for |trU| > 2, their moduli change by the factor |σ±| ̸= 1 for each additional T .
Fort →∞,oneofthemexceedsalllimits,whilefor t →−∞,itistheotherthatdoes
so. Therefore, they are said to be (Lyapunov) unstable. For |trU| > 2 all solutions of
the differential equation are unstable, because they are linear compositions of both of
these eigensolutions. In contrast, for |trU| < 2, the eigensolutions change only by a
complex factor of absolute value one with the time increment T —here all solutions
are stable, and we may choose x−= x+∗.
Except for the factor σ±t/T = exp(±i φt/T ), the Floquet solutions have period
T = 2π/ and can therefore be represented by a Fourier series or a Laurent series.
These solutions are linearly independent if there is no degeneracy. For degeneracy
(|trU| = 2 or σ±2 = 1), there are stable as well as unstable solutions: x(t) = Q(t) +
t P(t) with periodic P and Q (for σ± = +1 with period T , for σ± = −1 with period
2T ). Here the differential equation for x can be satisﬁed if ¨P + f P = 0 and ¨Q +
f Q = −2 ˙P. The expansion coefﬁcients in the Fourier series depend on the function
f (t).
The special case f (t) = 1
42(a −2q cos t), Mathieu’s differential equation,
has been thoroughly investigated (see, e.g., [1, 3]). It also arises in the separa-
tion of the wave equation (△+k2) u = 0 in elliptic coordinates, where only peri-
odic solutions make sense—and they then acquire the special eigenvalues a(q). The
curves a(q) (see Fig. 2.26) separate the regions of stable and unstable Mathieu
functions—thus also allowed and non-allowed energy bands in crystal ﬁelds with
the potential energy V (x) = V0 cos(kx), because there we have a = 8mE/(ℏk)2 and

118
2
Classical Mechanics
Fig. 2.26 Stability chart of
the functions solving the
special Hill differential
equation ¨x + ( 1
2)2(a −
2q cos t) x = 0 , here for
0 ≤q ≤8 and
−5 ≤a ≤15. Curves
indicate the stability limits.
For q = 0, we must have
a ≥0, while for q > 0, the
region splits into bands
which become ever narrower,
but also allow for a < 0
q = 4mV0/(ℏk)2 (see Fig. 2.27). The computation of the Mathieu functions and their
stability chart is explained in more detail in Sect. 2.4.11.
Simpliﬁcations are generally available if f is an even function, thus if f (−t) =
f (t)holds.Inparticular, x1 isthenevenand x2 odd,whence x(T −t) = x(T ) x1(t) −
˙x(T ) x2(t). If this is used for t = T for the two fundamental solutions x1 and x2,
we obtain ˙x2(T ) = x1(T ). For even f , we thus have cos φ = x1(T ). Therefore, the
solutions for |x1(T )| < 1 are then stable and otherwise unstable. In addition, not
only x(t) but also x(−t) now solves the given differential equation. Therefore, we
may now also set x−(t) = x+(−t) and P−(t) = P+(−t).
Fig. 2.27 Real part of the Mathieu functions x± for 0 ≤t ≤8T and a = 0, for q = 1/4 (dotted
curve), q = 2/4 (continuous curve), and q = 3/4 (dashed curve)

2.3 Lagrangian Mechanics
119
Finally, we consider the weakly time-dependent oscillator:
f (t) = ω0
2 {1 + ε a(t)} ,
with ε ≪1 and a(t + T ) = a(t) .
Here trU ≈2 cos(ω0T ) holds, whence φ ≈ω0T . Therefore, the stability is only at
risk if ω0T is an integer multiple of π, thus if the period T of f or a is a half or
integer multiple of the period T0 = 2π/ω0 of the basic oscillation. Even for very small
ﬂuctuations in the moments of inertia, an (undamped) swinging effect comes about.
This instability is called parametric resonance. It is particularly pronounced for T =
1
2T0 because, according to Fig. 2.26, the ﬁrst unstable band for a ≈1 is particularly
close to the axis q = 0 and ever smaller for the higher ones (a ≈22, 32, . . .): when
swinging on a child’s swing, we must move on the way back and forth, and anyone
who does that too rarely will not get into motion, whatever the effort.
Our starting equation also holds for a linear frictional force. Hence, if we start from
¨y + 2γ ˙y + h(t) y = 0 and set y = exp(−γ t) x, then with f = h −γ 2, we arrive at
the starting equation. Naturally, the factor exp(−γ t) strengthens the stability, because
γ is positive and only t > 0 is of interest. Now the solutions with |Imφ | ≤γ T are
still stable.
For a forced oscillation ¨y + 2γ ˙y + h(t) y = f (t), we may make the ansatz
y(t) = y0(t) +
 t
0
g(t, t′) f (t′) dt′
for the solution. If h did not depend on t, we might simplify the Green function g(t, t′)
to g(t −t′), as was shown in Sect. 2.3.8. Correspondingly, we now have to require
g(t, t) = 0, ˙g(t, t) = 1, and ¨g + 2γ ˙g + h g = 0, for 0 ≤t′ ≤t. If we replace (as
there) the upper integration limit by ∞, then g(t, t′) = 0 has to hold for t′ > t,
and therefore ¨g + 2γ ˙g + h g = δ(t −t′) must be valid. If x1 and x2 are linearly
independent solutions of the homogeneous differential equation ¨x + (h −γ 2) x = 0,
then all these requirements can be satisﬁed with
g(t, t′) = exp{−γ (t−t′)} x1(t′) x2(t ) −x1(t ) x2(t′)
x1(t′) ˙x2(t′) −˙x1(t′) x2(t′) ,
for t ≥t′ (zero otherwise) .
In particular, for t ̸= t′, this expression satisﬁes the differential equation, it vanishes
for t = t′, and its ﬁrst derivative with respect to t makes a jump there from 0 to 1. The
above-mentioned Wronski determinant x1 ˙x2 −˙x1x2 appears in the denominator.
Incidentally, g(t, t′) does not need to vanish for t < t′, if we account for the
contribution to the initial values y(0) and ˙y(0) (thus modify y0). The Green function
only has to satisfy the differential equation ¨g + 2γ ˙g + hg = δ(t−t′). This can be
done with
g(t, t′) = exp{−γ (t−t′)} x1(t<) x2(t>)
x1 ˙x2 −˙x1x2
,

120
2
Classical Mechanics
where t< is the smaller and t> the larger of the two values t and t′, and again the
Wronski determinant appears in the denominator. Here, ˙g jumps by 1 for t = t′, but
is not zero at lower t.
2.3.11
Summary: Lagrangian Mechanics
In Sect. 2.1, we already anticipated some important aspects of Lagrangian mechanics,
although we restricted ourselves there to time-independent phenomena. Geometric
constraints can often be incorporated through the use of appropriate coordinates in a
simpler way than by the associated forces of constraint. In particular, it is often the
case that fewer variables (generalized coordinates) depend on the time—otherwise
the constraints have to be accounted for by Lagrangian parameters in the Lagrange
equations of the ﬁrst kind. To this end, we generalized the principle of virtual work
to d’Alembert’s principle by taking into account inertial forces.
With a convenient choice of coordinates, we have an N-body problem in the “con-
ﬁguration space” with f (≤3N) dimensions and Lagrange equations of the second
kind
Fk = d
dt
∂T
∂˙xk −∂T
∂xk .
Here, the generalized forces Fk = 
i Fi · ∂ri/∂xk are often derived from a potential
energy. The forces may even depend upon the velocity, since there may also be a
generalized potential energy U with the property
Fk = d
dt
∂U
∂˙xk −∂U
∂xk .
Then we can use the Lagrange function
L = T −U
for calculations with the equations
d
dt
∂L
∂˙xk −∂L
∂xk = 0 .
Several applications have been discussed and exempliﬁed for these methods. With
the canonical momentum
pk = ∂L
∂˙xk

2.3 Lagrangian Mechanics
121
whichisconjugateto xk,wemayalsowrite ˙pk = ∂L/∂xk,or ˙p = ∇L withp = ∇vL.
This canonical momentum is to be distinguished from the mechanical momentum
mv, e.g., p = mv + qA holds if the vector potential A acts on the electric charge q
(the curl of A is the magnetic ﬁeld B). If L does not explicitly depend on time, then

k pk ˙xk −L is a constant of the motion. Furthermore, the conjugate momenta are
conserved for all cyclic variables, i.e., for those xk that do not appear in L, pk does
not depend on time.
We have investigated examples of various oscillations (harmonic, anharmonic,
damped, forced, and coupled). Note that, while the solutions of linear differential
equations change continuously with the initial conditions, this is different for non-
linear ones, as illustrated by the example of the (anharmonic) pendulum near the
separatrix.
2.4
Hamiltonian Mechanics
2.4.1
Hamilton Function and Hamiltonian Equations
According to the last section, when a (generalized) potential is given, we may always
start from the Euler–Lagrange equation
d
dt
∂L
∂˙x = ∂L
∂x .
Here x stands for an arbitrary generalized position coordinate xk and ˙x for its velocity
˙xk. But in Sect. 2.3.5, it already turned out that, instead of the velocity ˙x, it is often
better to consider the canonical momentum
p ≡∂L
∂˙x
=⇒
˙p = ∂L
∂x
conjugate to x. From now on, instead of the velocity ˙x, we shall always take this
momentum p as an independent variable and investigate everything in the phase
space (x, p), as we have already done for the pendulum orbits in Fig. 2.16 (see
Fig. 2.28). Here we may still gauge arbitrarily—only then does the canonical momen-
tum depend uniquely on the velocity. This greater freedom is occasionally of use and
often also provides a deeper understanding of the interrelations.
The new variable p is the derivative of L with respect to the variable ˙x (hereafter
˙x will be replaced by p). Therefore, a Legendre transformation is necessary. Instead
of the Lagrange function L(t, x, ˙x) with
dL = ∂L
∂t dt + ∂L
∂x dx + ∂L
∂˙x d ˙x
and
∂L
∂˙x = p ,

122
2
Classical Mechanics
Fig. 2.28 Representation of a harmonic oscillation in the (two-dimensional) phase space (with
convenient scales for the x- and the p-coordinate—otherwise we obtain an ellipse). The points (•)
are traversed clockwise. The phase-space units may not be arbitrarily small, according to quantum
physics—otherwise there would be a contradiction with Heisenberg’s uncertainty relation
we have to take the Hamilton function5 H(t, x, p) with
dH = ∂H
∂t dt + ∂H
∂x dx + ∂H
∂p dp
and
H = p ˙x −L .
In particular, the last equation implies dH = ˙x dp + p d ˙x −dL or dH = ˙x dp −
(∂L/∂t) dt −(∂L/∂x) dx. Comparing this expression with the one before, we then
ﬁnd
∂H
∂t = −∂L
∂t ,
∂H
∂x = −∂L
∂x ,
∂H
∂p = ˙x .
We reformulate the middle relation with the Lagrange equation and ﬁnd that, for the
conjugate variables xk and pk, and with the Hamilton function
H ≡

k
pk ˙xk −L ,
we obtain the Hamilton equations
˙xk = ∂H
∂pk
,
˙pk = −∂H
∂xk .
These are very general and we shall thus refer to them as the canonical equations.
In Lagrangian mechanics, there is one differential equation of second order for each
degree of freedom, whereas in Hamiltonian mechanics, there are always two differ-
ential equations of ﬁrst order. In addition, one has
dH
dt = ∂H
∂t +

k
∂H
∂xk
∂H
∂pk
−∂H
∂pk
∂H
∂xk

= ∂H
∂t .
5William Rowan Hamilton (1805–1865).

2.4 Hamiltonian Mechanics
123
If further the Hamilton function H does not depend explicitly on time, then it remains
a conserved quantity along all orbits.
If there is a potential energy V (and hence also L = T −V ), and if T is a homo-
geneous function of second order in the velocities (so that p ˙x = 2T , according to
p. 101), we have H = T + V , so H is an energy. But we shall ﬁnd shortly that H
and E may also be different.
For a non-relativistic particle of mass m and charge q in an electromagnetic ﬁeld,
we infer the Hamilton function from the Lagrange function
L = m
2 v · v −q ( −v · A)
in Sect. 2.3.5 (p. 100) as H = p · v −L. To this end, we only have to express the
velocity v in terms of the canonical momentum p = m v + q A (see p. 100). Since
(mv + qA) · v −L = m
2 v · v + q  and v = (p −q A)/m this leads to
H(t, r, r) = (p −q A) · (p −q A)
2m
+ q  .
If the magnetic ﬁeld B depends neither on time nor on position, then according to
p. 100, we may use the vector potential A = 1
2 B × r with qB = −mω (see p. 78),
where ω is the associated cyclotron frequency. It then follows from m˙r = p −qA =
p + 1
2m ω × r or from ˙r = ∇p H that
˙r = p
m + ω × r
2
.
In addition, for  = 0, ˙p = −∇H (in agreement with ˙p = 1
2F = 1
2q v × B on p. 100)
delivers
˙p = 1
2 ω × (p + 1
2m ω × r) ,
and thus,
˙p = 1
2m ω × ˙r .
We have already integrated these differential equations on p. 100.
According to p. 98, for a gauge transformation ′ =  + ∂/∂t, A ′ = A −∇,
the Lagrange function is transformed into L′ = L −dG/dt with G = q , and the
canonical momentum into p ′ = p −∇G (see p. 100). Since dG/dt = ∂G/∂t +
∇G · v, the Hamilton function is
H ′ =

k
pk
′ ˙xk′ −L′ = H + ∂G
∂t .
The term ∂G/∂t may depend on position and time—this is more than the arbitrariness
in the choice of the zero energy. Therefore, the Hamilton function agrees with the

124
2
Classical Mechanics
energy only for an appropriate gauge. A more detailed investigation is available in
[4]. The scalar potential  may not depend upon time! So only then can q(r) be
a potential energy V (r) and H −V a homogeneous function of second order in the
velocity—consequently, H is an energy. If the electric ﬁeld E depends on time, then
this has to be included in the vector potential A, or more precisely, in its sources,
because its curl determines the magnetic ﬁeld B. For a time-dependent force, its path
integral depends upon the amount of time needed to traverse this path. The force ﬁeld
is then not always curl-free and therefore cannot be derived from a potential energy.
In the Lagrangian formalism, we ﬁnd that pk is a constant of the motion if L does
not depend on xk, i.e., if xk is a cyclic coordinate. This leads to 0 = ∂L/∂xk = ˙pk =
−∂H/∂xk in the Hamiltonian formalism: then xk does not appear in H. Hence,
the conservation of momentum and angular momentum follows immediately for
each system with only internal forces, for which H does not involve center-of-mass
coordinates.
2.4.2
Poisson Brackets
The Poisson brackets for functions u(t, x, p) and v(t, x, p) are deﬁned by
[u, v] ≡

k
 ∂u
∂xk
∂v
∂pk
−∂u
∂pk
∂v
∂xk

and have the properties (with constant α and β)
[u, v] = −[v, u] ,
[u v, w] = u [v, w] + [u, w] v ,
[αu + βv, w] = α [u, w] + β [v, w] .
In addition, the Jacobi identity holds:
[u, [v, w]] + [v, [w, u]] + [w, [u, v]] = 0 ,
as for the vector product on p. 4. This is proved using ∂u/∂x = ux, ∂u/∂p = up
and similarly for v and w instead of u in Problem 2.43. The Hamilton equations
lead to
[u, H] =

k
 ∂u
∂xk ˙xk + ∂u
∂pk
˙pk

= du
dt −∂u
∂t ,
and for arbitrary u, we deduce
du
dt = ∂u
∂t + [u, H] .

2.4 Hamiltonian Mechanics
125
If u does not depend explicitly on time, then ˙u is equal to the Poisson bracket of u
with the Hamilton function H. In particular, we obtain
˙xk = [xk, H] ,
˙pk = [pk, H] ,
instead of the Hamilton equations.
Since position and momentum coordinates do not depend on each other, we also
have
[xi, x j] = 0 = [pi, p j] ,
[xi, p j] = δi
j =

1 for i = j ,
0 for i ̸= j .
These equations will play an important role for the transition to quantum mechan-
ics, where the quantities will be replaced by (Hermitian) operators and the Poisson
brackets by commutators (divided by iℏ). Connections can also be found with these
results in thermodynamics (statistical mechanics), namely, with the Liouville equa-
tion. The latter gives the time dependence of the probability density ρ in phase space
and states that dρ/dt = 0:
dρ
dt = 0
=⇒
∂ρ
∂t + [ρ, H] = 0 .
Whatever is altered in a volume element of the phase space happens because of the
equations of motion. This equation is proven in Sect. 2.4.4. With the probability
density ρ, the mean values A of functions A(t, x, p) can be evaluated from A =
	
ρ A dx dp.
2.4.3
Canonical Transformations
We would now like to choose new coordinates in phase space (still for ﬁxed time),
and possibly also a new Hamilton function, such that the canonical equations are
still valid. In the Lagrangian formalism, we only considered transformations in the
conﬁguration space, which has only half as many coordinates. For the moment we
restrict ourselves to just one degree of freedom and leave out the index k. Then
the Poisson bracket [u, v] is the same as the functional determinant ∂(u, v)/∂(x, p).
Since
∂(u, v)
∂(x, p) = ∂(u, v)
∂(x′, p′)
∂(x′, p′)
∂(x, p) ,
it only remains the same for transformations of the phase-space coordinates when
the functional determinant of the new phase-space coordinates is equal to 1, viz.,

126
2
Classical Mechanics
∂(x′, p′)
∂(x, p)
≡[x′, p′] = 1 ,
i.e., if the map is area-preserving. (If we no longer require the restriction f = 1, then
this constraint is necessary, but not sufﬁcient for canonical transformations. We shall
deal with this later.) If we write
dx′
dp′

= K
dx
dp

,
with
K =
⎛
⎜⎜⎝
∂x′
∂x
∂x′
∂p
∂p′
∂x
∂p′
∂p
⎞
⎟⎟⎠,
then, because [x′, p′] = 1, for the inverse K −1 of this 2 × 2 matrix given by the
formula on p. 71, we have
K −1 =
⎛
⎜⎝
∂x
∂x′
∂x
∂p′
∂p
∂x′
∂p
∂p′
⎞
⎟⎠=
⎛
⎜⎝
∂p′
∂p
−∂x′
∂p
−∂p′
∂x
∂x′
∂x
⎞
⎟⎠.
The two matrices must have equal elements. This results in the four equations
∂x′
∂x = ∂p
∂p′ ,
∂x′
∂p = −∂x
∂p′ ,
∂p′
∂x = −∂p
∂x′ ,
∂p′
∂p = ∂x
∂x′ .
Here one alone actually sufﬁces (e.g., the ﬁrst), because the remaining ones follow
from this one according to p. 44, in particular the second from
∂x′
∂p

x
 ∂p
∂p′

x′ = −
∂x′
∂x

p
 ∂x
∂p′

x′ ,
the third from
 ∂p
∂p′

x′
∂p′
∂x

p = −
 ∂p
∂x′

p′
∂x′
∂x

p ,
and the fourth from
∂x′
∂x

p
 ∂x
∂x′

p′ =
 ∂p
∂p′

x′
∂p′
∂p

x .
This we generalize now to f > 1 for time-independent canonical transformations.
With i, k ∈{1, . . . , f }, we obtain the following constraints:
∂xi′
∂xk = ∂pk
∂pi ′ ,
∂xi′
∂pk
= −∂xk
∂pi ′ ,
and
∂pi ′
∂xk = −∂pk
∂xi′ ,
∂pi ′
∂pk
= ∂xk
∂xi′ .

2.4 Hamiltonian Mechanics
127
Here for the ﬁrst (and last) equation, the notation with upper and lower indices
from Sect. 1.2.2 turns out to be quite successful, and the remaining equations follow
therefrom. In fact, these equations ensure that
˙xk′ =

l
∂xk′
∂xl ˙xl + ∂xk′
∂pl
˙pl

= +

l
 ∂pl
∂pk′
∂H
∂pl
+ ∂xl
∂pk′
∂H
∂xl

= + ∂H
∂pk′ ,
˙p′
k =

l
∂pk′
∂xl
˙xl + ∂pk′
∂pl
˙pl

= −

l
 ∂pl
∂xk′
∂H
∂pl
+ ∂xl
∂xk′
∂H
∂xl

= −∂H
∂xk′ .
If, for a time-independent transformation, we have H ′ = H, then the canonical equa-
tions remain untouched. Therefore, the name canonical transformation makes sense.
The linear transformation (with functional determinant 1)
x′
p′

=
axx axp
apx app
 x
p

,
with
det a = 1 ,
is clearly also canonical. In particular, we may choose axx = app = cos α and
apx = −axp = sin α, i.e., rotate in phase space. Therefore, the identity (with α = 0)
is canonical, but so also is the transformation x′ = p, p′ = −x (with α = −1
2π).
This shows clearly that the meaning of position and momentum coordinates becomes
blurred for the canonical equations—therefore q is often written preferentially for
the generalized position coordinates rather than x. Moreover, the canonical transfor-
mations are essentially more general than the point transformations which are the
only ones allowed in the Lagrangian formalism, i.e., in the latter, only the coordinates
could be chosen, but not the velocities.
Let us consider the example of a linear harmonic oscillation with
H(x, p) = p2 + m2ω2x2
2m
.
Here only the squares of x and p appear. Therefore, by a non-linear canonical trans-
formation, a cyclic coordinate x′ can be introduced. We make a transition to polar
coordinates, which are suggested according to Fig. 2.28:
x = f (p′) sin x′
mω
,
p = f (p′) cos x′
=⇒
H ′ = f 2(p′)
2m
.
The transformation is only canonical if f (p′) obeys the constraint f d f/dp′ = mω
(since mω det K −1 = f d f/dp′). The associated differential form f d f = mω dp′
is easily integrated:
1
2 f 2(p′) = mωp′
=⇒
H ′ = ωp′ .

128
2
Classical Mechanics
No integration constant is added here because it would only move the zero energy.
Now the Hamilton equations are very simple and are easily integrated:
˙x′ = +∂H ′
∂p′ = ω
=⇒
x′ = ω(t−t0) ,
˙p′ = −∂H ′
∂x′ = 0
=⇒
p′ = const = H ′
ω .
If we write E0 instead of H ′ for the total energy, then because f 2(p′) = 2mE0 and
with the abbreviations p = √2mE0 and x = p/(mω) for the original variables, we
obtain
x = x sin(ω(t−t0)) ,
p = p cos(ω(t−t0)) .
As expected, we have had to integrate two differential equations of ﬁrst order instead
of one of second order. The integration constants E0 and t0 can be adjusted to the
initial values.
For a charged point mass in a homogeneous magnetic ﬁeld, we only search for the
motion perpendicular to this ﬁeld and, according to p. 123, the Hamilton function is
H(x, y, px, py) = (px −1
2mωy)2 + (py + 1
2mωx)2
2m
.
We carry out the canonical transformation
x′ = x
2 + py
mω ,
px
′ = px −mω
2
y ,
y′ = x
2 −py
mω ,
py
′ = px + mω
2
y .
The proof that it is truly canonical is rather cumbersome at the present stage, because
here there are four derivatives of the primed quantities with respect to the unprimed
ones to be determined, and likewise many derivatives of the inverse functions, but
at the end of Sect. 2.4.5, there is a generating function of this transformation, which
simpliﬁes the proof (see Problems 2.47–2.48). The Hamilton function now reads
(px ′2 + m2ω2x′2)/2m. The coordinates y′ and py′ are cyclic, and we recognize the
Hamilton function of a linear harmonic oscillation with the cyclotron frequency as
angular frequency. The two cyclic coordinates are related to the pseudo-momentum
K (treated on p. 100):
K = p + 1
2q B × r = p −1
2m ω × r ,
whence Kx = py′ and Ky = −mωy′.Itwasintroducedearlierasaconservedquantity
and delivered the center of the circular orbit. Here it is also clear that K · K/2m
belongs to a linear oscillation with the cyclotron frequency as the angular frequency.

2.4 Hamiltonian Mechanics
129
The angular momentum is given by
Lz = xpy −ypx = 1
ω

H ′ −K · K
2m

.
We can thus split H ′ into ωLz and K · K/2m.
2.4.4
Inﬁnitesimal Canonical Transformations. Liouville
Equation
An inﬁnitesimal canonical transformation is deﬁned by
x′ = x + ∂g(x, p)
∂p
ε ,
p′ = p −∂g(x, p)
∂x
ε ,
if ε is small enough to be able to neglect terms of the order of ε2 compared to 1 in
the functional determinant, and thus use the fact that ∂2g/∂p ∂x = ∂2g/∂x ∂p (for
which g has to be twice continuously differentiable). In particular, also
x′ = x + ˙x dt = x + ∂H
∂p dt ,
p′ = p + ˙p dt = p −∂H
∂x dt ,
is a canonical transformation:
We can interpret the time evolution of the system as a canonical transformation.
This yields Liouville’s theorem, regarding the time dependence of the probability
density in phase space, thus of the weight with which each volume element of the
phase space contributes to a statistical ensemble (e.g., for the molecules of an ideal
gas—more on that in Sect. 6.2.3). In particular, the density has to have the prop-
erty ρ′(t, x′, p′) dx′ dp′ = ρ(t, x, p) dx dp because, despite its motion, each phase-
space element keeps its probability content. Since each canonical transformation is
area-preserving, it follows that
ρ′(t, x′, p′) = ρ(t, x, p)
=⇒
dρ
dt = 0 ,
and hence the Liouville (continuity) equation
∂ρ
∂t + [ρ, H] = 0 .
In equilibrium, ρ does not depend explicitly on time. Then [ρ, H] = 0.

130
2
Classical Mechanics
Table 2.1 Generators and inﬁnitesimal transformations
Generating function g
Change
Inﬁnitesimal transformation
H
dt
x′ = x + ˙x dt ,
p′ = p + ˙p dt
p
dx
x′ = x + dx ,
p′ = p
pϕ
dϕ
ϕ′ = ϕ + dϕ ,
p′
ϕ = pϕ
The above function g(x, p) is usually called the generating function (generator)
of the inﬁnitesimal canonical transformation. In particular, the Hamilton function
H generates a time shift, the momentum p a change in position, and the angular
momentum pϕ a rotation, as listed in Table 2.1.
For Cartesian coordinates in the last row, the generating function Lz = x py −
y px is to be taken. This delivers
x′ = x −y dϕ ,
p′
x = px −py dϕ ,
y′ = y + x dϕ ,
p′
y = py + px dϕ ,
as required for a rotation through the angle dϕ about the z-axis. Generally, we require
as generating function the quantity canonically conjugate to the differential variable,
so we also view the time t and the Hamilton function (energy) H as canonically
conjugate to each other.
2.4.5
Generating Functions
Finite and time-dependent canonical transformations can also be derived from gen-
erating functions. To this end, we start preferably from the gauge dependence of the
Lagrange function (see p. 98), and L = p ˙x −H. Since L′ = L −˙G, we have
dG = (L −L′) dt = (H ′ −H) dt + p dx −p′ dx′ .
If we now make the ansatz that G and x′ are functions of t, x, and p, we obtain
dG = ∂G
∂t dt + ∂G
∂x dx + ∂G
∂p dp ,
dx′ = ∂x′
∂t dt + ∂x′
∂x dx + ∂x′
∂p dp .
Therefore, we infer
∂G
∂t = H ′ −H −p′ ∂x′
∂t ,

2.4 Hamiltonian Mechanics
131
∂G
∂x = p −p′ ∂x′
∂x ,
∂G
∂p = −p′ ∂x′
∂p .
The transformation is canonical if the two mixed derivatives
∂2G
∂p ∂x = 1 −∂p′
∂p
∂x′
∂x −p′ ∂2x′
∂p ∂x ,
∂2G
∂x ∂p =
−∂p′
∂x
∂x′
∂p −p′ ∂2x′
∂x ∂p ,
agree with each other, and likewise those of x′(t, x, p). Then, in particular, we have
∂x′
∂x
∂p′
∂p −∂x′
∂p
∂p′
∂x
≡∂(x′, p′)
∂(x, p)
≡[x′, p′] = 1 .
Thus x′, p′, and H ′ have to obey the partial differential equations for G above
(derivatives of G with respect to t, x, and p). In particular H ′ = H holds if G and
x′ do not depend explicitly on time.
In the last section, we introduced generating functions g(x, p) for the inﬁnitesimal
transformations. We now ask how they are connected with G(x, p). Since
x′ = x + ε ∂g
∂p
and
p′ = p −ε ∂g
∂x ,
then up to terms of order ε2, we have
∂G
∂x = p −

p −ε ∂g
∂x

1 + ε
∂2g
∂x ∂p

≈ε ∂
∂x

g −p ∂g
∂p

,
∂G
∂p = −

p −ε ∂g
∂x

ε ∂2g
∂p2 ≈−εp ∂2g
∂p2 = ε ∂
∂p

g −p ∂g
∂p

.
Therefore, we may take G(x, p) ≈ε (g −p ∂g/∂p) and obtain a unique connection
between G and g, whereupon both shall be referred to as generating functions.
Likewise, we may also take x and p as functions of x′ and p′ or any other pair of
old and new phase-space coordinates as functions of the other pair. However, different
generating functions appear then. Later we will denote them by G and include the
associated variables. So, with G(t, x′, p′), x(t, x′, p′), and p(t, x′, p′), for example,
we have
∂G
∂t = H ′ −H + p ∂x
∂t ,
∂G
∂x′ = −p′ + p ∂x
∂x′ ,
and
∂G
∂p′ = p ∂x
∂p′ .
Here, too, x and p result from partial differential equations.

132
2
Classical Mechanics
But if the generating function depends upon a primed and an unprimed variable
(exceptforthetime,whichisnottransformed,i.e.,t = t′),thenevensimpleralgebraic
equations follow instead of the (partial) differential equations. So we require
dG(t, x, x′) = ∂G
∂t dt + ∂G
∂x dx + ∂G
∂x′ dx′ ,
because of the starting equation
H ′ = H + ∂G
∂t ,
p = ∂G
∂x ,
and
p′ = −∂G
∂x′ .
If the mixed derivatives ∂2G/∂x∂x′ and ∂2G/∂x′∂x are equal, then it follows
that ∂p/∂x′ = −∂p′/∂x, whence the transformation is canonical if in addition
p = ∂G/∂x can be solved for x′. Further generating functions follow from the Leg-
endre transformations:
G(t, x, x′) = G(t, x, p′) −p′ x′
= G(t, p, x′) + p x
= G(t, p, p′) + p x −p′ x′ .
Actually, here we should use four different notations instead of just G, and these are
often written G1, G2, G3, and G4, i.e., generating functions of type 1, type 2, type
3, and type 4. However, only their variables are important. Each of these generating
functions depends on one primed and one unprimed variable, except for the time.
Thus we obtain the list in Table 2.2—in all these cases we also have
H ′ = H + ∂G
∂t ,
with the other variables held ﬁxed in each case. The remaining constraints for the
canonical transformation are then also fulﬁlled, because one constraint already takes
care of det K = 1. However, there are not always all four. Thus, the identity can be
generated by G(x, p′) = xp′, for example, while this is not satisﬁed by the trans-
formed function G(x, x′) = (x −x′) p′.
Table 2.2 Different generating functions
Generating function
Fixed variables
Reason
G(t, x, x′)
p = +∂G
∂x ,
p′ = −∂G
∂x′
∂p
∂x′ = −∂p′
∂x
G(t, x, p′)
p = +∂G
∂x ,
x′ = +∂G
∂p′
∂p
∂p′ = +∂x′
∂x
G(t, p, x′)
x = −∂G
∂p ,
p′ = −∂G
∂x′
∂x
∂x′ = +∂p′
∂p
G(t, p, p′)
x = −∂G
∂p ,
x′ = +∂G
∂p′
∂x
∂p′ = −∂x′
∂p

2.4 Hamiltonian Mechanics
133
For functions with several pairs of parameters, mixing is also allowed. Thus the
generating function x1 p1′ + x2x2′ leads to x1′ = x1, p1′ = p1 and x2′ = p2, p2′ =
−x2. With the ﬁrst pair, nothing is changed here, while for the second pair, position
and momentum swap names.
The canonical transformation x = √2p′/(mω) sin x′, p = √2mωp′ cos x′ (see
p. 127) with a harmonic oscillation can be generated by the function
G(x, x′) = mω
2
x2 cot x′ ,
because it leads to p = mωx cot x′ and p′ = 1
2mωx2 sin−2 x′.
The following canonical transformation for a point charge (with mass m) in the
homogeneous magnetic ﬁeld can be derived from the generating function (Prob-
lem 2.47)
G(x, px
′, py, py
′) = x px ′ + py′
2
+ py
px ′ −py′
mω
,
whence it can be proven easily that the transformation mentioned on p. 128 is truly
canonical.
2.4.6
Transformations to Moving Reference Frames.
Perturbation Theory
An important application is transformations to moving reference frames. We inves-
tigate in particular
H = H0(p) + H1(x, p) ,
in which x is cyclic with respect to H0, but not with respect to the total Hamilton
function. For H1 = 0, the condition ∂H0/∂x = 0 leads to constant p = p0 and
˙x = ∂H0
∂p

p=p0 ≡v0
=⇒
x = v0 t + x0 .
With the generalized case H1 ̸= 0, we now take the canonical transformation
x′ = x −v0 t −x0 ,
p′ = p −p0 ,
which can be derived from the generating function
G (t, x, p′) = (x −v0 t −x0) (p0 + p′) ,
with p = ∂G/∂x and x′ = ∂G/∂p′. Since H ′ = H + ∂G/∂t, we have

134
2
Classical Mechanics
H ′ = H0(p0+ p′) + H1(v0 t+x0+x′, p0+ p′) −v0 (p0+ p′) .
These equations have been derived without approximations.
But these are often useful also for perturbation theory, if one has the solution for
H0, but not for H. If we have |H1| ≪|H0|, then for not too long times, x′ and p′
will also be small compared to x and p, because they even vanish for H1 = 0. Here
we may still choose x0 such that, for t = 0, |H1| is as small as possible compared to
|H0|. The perturbation theory then works as follows. In
˙x′(t, x′, p′) = +∂H ′
∂p′
= ∂H0
∂p′ + ∂H1
∂p′ −v0 ,
˙p′(t, x′, p′) = −∂H ′
∂x′
= −∂H1
∂x′ ,
we ﬁrst set x′ and p′ equal to 0 on the right, and thus ﬁnd solutions to ˙x′(t, 0, 0) and
˙p′(t, 0, 0). Here the integration constant has to be ﬁxed in such a way that x′ and p′
vanish for t = 0. With these approximations we can improve the expressions on the
right of the differential equations and evaluate the next approximation, i.e., the next
order in the Taylor expansion. Where possible, we may even be able to identify the
complete solutions.
If we consider as an example a harmonic oscillation and the free motion as unper-
turbed (a coarse approximation, where here actually V = T holds),
H0 = p2
2m ,
H1 = mω2x2
2
,
delivers
H ′ = (p0 + p′)2
2m
+ mω2
2
(v0 t + x′)2 −v0 (p0 + p′) .
With this and because of v0 = ∂H0/∂p|p0 = p0/m, we have
˙x′ = p0 + p′
m
−v0 = p′
m ,
˙p′ = −mω2 (v0 t + x′) ,
and consequently p′ ≈−1
2! p0ω2t2 and x′ ≈−1
3! v0ω2t3. The next order delivers the
additional terms 1
4! p0ω4t4 for p′ and 1
5! v0ω4t5 for x′. In fact, the correct solution is
p = p0 cos(ωt) = p0 + p′ ,
x = (v0/ω) sin(ωt) = v0t + x′ ,
with x(0) ≡x0 = 0.

2.4 Hamiltonian Mechanics
135
2.4.7
Hamilton–Jacobi Theory
The Hamilton–Jacobi theory is a further application of time-dependent canonical
transformations and will be explained brieﬂy here. Note that, in his book (see the
suggestions for textbooks on p. 162), H. Goldstein devotes a whole chapter to this
subject. Unfortunately, he, along with many others, does not comply with the IUPAP
recommendations: the quantities W (action function) and S (characteristic function)
are used by him in the opposite notation S and W, respectively.
In this theory the Hamilton function is transformed canonically to zero. Then all
new variables x′ and p′ are conserved quantities, ﬁxed by the initial values. Here, the
generating function is the associated Hamilton action function W(t, x, p′). Because
H ′(t, x′, p′) = H(t, x, p) + ∂W(t, x, p′)/∂t for H ′ = 0 and because p = ∂W/∂x,
W has to satisfy the Hamilton–Jacobi differential equation
∂W
∂t + H

t, x, ∂W
∂x

= 0 .
Since here p′ does not depend on time, we have
dW
dt = ∂W
∂t + ∂W
∂x
˙x = p ˙x −H = L
=⇒
W =

L dt .
The integration constant is left out here, because we may still ﬁnd a suitable gauge.
The single partial differential equation of Hamilton and Jacobi replaces all f pairs
of ordinary differential equations in the Hamilton theory! However, it is difﬁcult
to solve, because the momenta in the Hamilton function and hence the required
functions mostly appear squared. But the theory is useful for formal considerations.
Using this we shall be able to discover in particular a connection with geometrical
optics(rayoptics).Notethatwehavesofarexpressedalllawsasdifferentialequations
and taken, e.g., the Lagrange function L as the quantity to start from. Now L is the
derivative of the “anti-derivative” W, so the action has to be viewed as the original
quantity.
The choice of the new momenta p′ is not unique. Functions of it are also allow-
able, and we shall choose their structure to be as simple as possible. Of course, the
associated coordinates x′ = ∂W/∂p′ depend upon this choice. In any case, x′ and
p′ are constants of the motion, which have to be adjusted to the initial values. After
that, x(t, x′, p′) and p(t, x′, p′) can be obtained.
If the Hamilton function does not depend on time, the ansatz
W(t, x, p′) = S(x, p′) −E t
sufﬁces, since it leads from p = ∂W/∂x to p = ∂S/∂x and from the Hamilton–
Jacobi equation to

136
2
Classical Mechanics
H

x, ∂S
∂x

= E ,
and H should also be taken as energy. Since S depends only on x and p′, it is
sometimes called the reduced action, but usually the characteristic function. This
can be concluded from ∂S/∂x = p and leads to a sheet in phase space:
S =

p dx ,
or
S =

f

k=1
pk dxk ,
with f > 1 .
Again, the integration constant vanishes here for a suitable gauge. For periodic
motions (oscillations or rotations), we also introduce the phase integral (sometimes
called the action variable), taken along the closed path in phase space, viz.,
J =

p dx ,
or several action variables Jk for more periodic degrees of freedom. According to
quantum theory (Bohr–Sommerfeld quantization rule), this quantity cannot change
continuously, but only in steps of the action quantum h (see also p. 367).
We may take one of the new momenta p′
k as energy. Then the associated coordinate
xk′ is connected to the choice of the zero time, as we show now for a simple example.
If a coordinate oscillates harmonically, then H = (P2 + m2ω2x2)/2m leads to
1
2m
∂S
∂x
2
+ m
2 ω2 x2 = E .
From this we could immediately conclude S =
	
(∂S/∂x) dx by integration, with the
result S = 1
2mω x

2E/mω2 −x2 + E/ω arcsin(

mω2/2E x). But this is unnec-
essary, since with x′ = ∂W/∂p′ and p′ = E, we can also immediately obtain
x′ = ∂W
∂E = ∂S
∂E −t = ∂
∂E
 ∂S
∂x dx −t ,
and hence then, with x′ = −t0,
t −t0 = 1
ω

dx

2E/mω2 −x2 = 1
ω arcsin

mω2
2E x .
This is the solution x = x sin[ω(t−t0)] with amplitudex =

2p′/mω2 and the sec-
ond adjusted parameter t0 = −x′ mentioned on p. 128. Note that, inserting the solu-
tion x(t) into the expression for S, we can also obtain W(t) = E/(2ω) sin[2ω(t −
t0)] and dW/dt = L, implying J = ET = 2π E/ω for the phase integral.

2.4 Hamiltonian Mechanics
137
In order to understand the properties of W and S, we have to start from a time-
independent Hamilton function. At time zero, W and S agree. If, in conﬁguration
space (i.e., the space of coordinates x ), we investigate the areas of constant W values
or S values as functions of time, then the sheets of the S values stay constant, while the
sheets of constant W values move like a wave front. The latter follows in particular
from dW/dt = 0, thus ∇W · ˙x −E = 0 or p · ˙x = E. The larger the momentum p,
the smaller the velocity of the wave for given energy.
In order to understand what kind of wave this is, we consider the wave equation
△ψ −1
c2
∂2ψ
∂t2 = 0 ,
where c is the phase velocity of the wave, as can be seen from the ansatz for the
solution ψ ∝exp{i(k · r −ωt)}, which contains the wave vector k with k ≡2π/λ
andtheangularfrequency ω ≡2π/T ,whereλisthewavelengthand T theoscillation
period of the wave. For the differential equation to be satisﬁed, ck = ω or c = λ/T
has to hold. In an inhomogeneous medium, the wavelength depends on the position,
and so also does the phase velocity. For this notion of a wave to make sense at all,
we would like to assume that both vary only slowly on their paths. We thus restrict
ourselves to waves of very short wavelength or very high wave number k and call
the smallest of the occurring wave numbers k0. Then we can make an ansatz
ψ = exp{A(r) + ik0 (S(r) −c0t)}
for the solution of the wave equation in the inhomogeneous medium with c0 =
ω/k0, real amplitude exp[A(r)], and real path eikonal S(r). (The word eikonal is
reminiscent of the Greek εικων, meaning picture or icon. With the mapping of an
object point r0 on the image point r1, both points are singular points of the wave
areas, and the optical paths for all connecting rays are equal to S(r1) −S(r0). The
eikonal is related to the characteristic function, as we shall see soon.) In particular,
this ansatz leads to ∇ψ = ψ ∇(A + ik0S) and
△ψ = ψ {(A + ik0S) + ∇(A + ik0S) · ∇(A + ik0S)} ,
which, according to the wave equation, should agree with −(c0k0/c)2 ψ. Then, after
separation into real and imaginary parts, we infer
△A + ∇A · ∇A + k0
2 (n2 −∇S · ∇S) = 0 ,
with the position-dependent refractive index n ≡c0/c and
△S + 2∇A · ∇S = 0 .
The refractive index should barely vary, according to the assumption about the wave-
length: k0 should be sufﬁciently large. With this we obtain the eikonal equation of
geometrical optics, viz.,

138
2
Classical Mechanics
Fig. 2.29 Geometrical optics and classical mechanics (beam path and particle path) have much in
common, here shown for a lens with refractive index n = 2. But note that the refractive index for
a wave corresponds to the ratio c0/c, in contrast to the ratio v/v0 for a particles. Actually, we have
to distinguish between phase and particle velocity. Dashed lines are the wave fronts. Those of W
move in the course of time, but not those of S. The wave fronts are singular at the object and image
points (•)
∇S · ∇S = n2 ,
an inhomogeneous differential equation of ﬁrst order and second degree. (It holds
only in the limit of short wavelengths, because otherwise we would also have to take
into account △A + ∇A · ∇A = 0: ∇A would have to have only drains, because its
source density would be ∇· ∇A = △A = −∇A · ∇A ≤0.) If we integrate to ﬁnd
the eikonal S(r), then from the second differential equation △S + 2∇A · ∇S = 0,
we obtain the gradient of the amplitude function A in the direction of the gradient
of S. Perpendicular to it, the gradient of A remains undetermined. In this plane it
may even vary in steps, whence, according to geometrical optics, rays are possible.
The wave propagates along ∇S, (see Fig. 1.4) perpendicular to the wave fronts S =
const. (see Fig. 2.29).
With the Hamilton–Jacobi equation for H =
1
2m p · p + V (r), we arrive at
∇S · ∇S = 2m {E −V (r)} ,
and hence also at the eikonal equation with n2 = 2m {E −V (r)}, which however
is not a pure number, and where the “characteristic function” appears instead of the
eikonal. Classical mechanics can describe the motion of particles of mass m with the
same differential equation as geometrical optics. This holds for waves of negligible
wavelength. Conversely, the propagation of light can be viewed as the motion of
particles (photons), as long as the wavelength is sufﬁciently small.

2.4 Hamiltonian Mechanics
139
2.4.8
Integral Principles
So far we have derived the basic laws from differential equations, e.g., from the
Lagrange equations of the second kind
d
dt
 ∂L
∂˙xk

= ∂L
∂xk .
However, for the problem under consideration, there has to be a potential energy,
or at least a generalized U. But these differential equations can also be related to
integral expressions via the variational calculus. Then there is no need for a potential
energy, and the basic laws can be interpreted a different way. This is also important
for our general understanding.
In the variational calculus, we seek functions x(t) that make an integral
I =
 t1
t0
f (t, x, ˙x) dt
extremal under constraints. Here the boundaries t0 and t1 are given as ﬁxed, or at
least connected to constraints that deliver ﬁxed boundaries after a transformation
t →t′. The values of the function are also given at those boundaries, viz., δx(t0) =
0 = δx(t1), but not their derivatives ˙x(t0) and ˙x(t1).
If we search for the “extremal” x(t) for the regime between t0 and t1, then initially,
in addition to x, we also have to allow for x + δx and hence, in addition to ˙x, also
for ˙x + δ˙x. Here, to begin with, the variations always refer to the same time: δt = 0
(see Fig. 2.30). Consequently, we have δ ˙x = δ dx/dt = d δx/dt, and therefore (with
partial integration for the second equation)
δI =
 t1
t0
∂f
∂x δx + ∂f
∂˙x
d δx
dt

dt = ∂f
∂˙x δx

t1
t0
+
 t1
t0
∂f
∂x −d
dt
∂f
∂˙x

δx dt .
Now, for δI to vanish for arbitrary δx,
Fig. 2.30 Path variation with δt = 0 but δx ̸= 0 along dashed lines. Since δx(t0) = 0 = δx(t1),
each permitted orbit ends at the points shown by the dots (•). Since t1 may follow arbitrarily quickly
after t0, ˙x(t0) and ˙x(t1) effectively vary

140
2
Classical Mechanics
δI = 0 at δt = 0
⇐⇒
 t1
t0
δ f dt = 0 at δt = 0 ,
whence (uniquely) we must satisfy Euler’s differential equation
d
dt
∂f
∂˙x

−∂f
∂x = 0 .
Correspondingly, for f (t, x1, . . . , x f , ˙x1, . . . , ˙x f ), one of the extremal conditions
delivers a total of f such differential equations of second order.
From the Lagrange equations of the second kind, it follows that the action function
W introduced on p. 135 takes an extremum, yielding Hamilton’s principle:
δW ≡δ
 t1
t0
L dt = 0 ,
at δt = 0 .
Among all possible paths the one with extremal W is realized. We usually replace
L by T −V . But Hamilton’s principle holds even if there is no potential energy at
all. This can be understood with d’Alembert’s principle (m ˙v −F) · δr = 0, implying
that F · δr = δA and ˙v · δr = d (v · δr)/dt −v · δ˙r hold with mv · δ˙r = δT . Since
v · δr vanishes at the integration limits, we thus obtain
	
(δT + δA) dt = 0. This we
may also write as (general Hamilton principle)
δ
 t1
t0
T dt +
 t1
t0
δA dt = 0 ,
at δt = 0 .
Note that the virtual work δA makes sense, but the work A does not generally as
such. Only if a potential energy V produces the (external) forces do we have
δT + δA = δ (T −V ) = δL ,
and then the variation can be moved in front of the integral.
Hamilton’s principle does not depend on the choice of coordinates. Arbitrary
(unique) transformations of t and of the generalized coordinates xk are permitted.
We only need to be able to give T and V or, respectively, δA. With this, we have
a general basis for the problems of mechanics, and even for friction. If there is a
potential energy and hence also a Lagrange function, then from the same principle
we can immediately conclude that
L′ = L −dG
dt
is also an allowable Lagrange function (gauge invariance, see p. 98).
Another integral principle is the action principle (due to Maupertuis, Leibniz,
Euler, Lagrange), for which, however, it is not the action W that is varied, but the

2.4 Hamiltonian Mechanics
141
characteristic function (reduced action) S, and where the energy is held ﬁxed instead
of the time (and likewise the integration limits r0 and r1). In addition, the Hamilton
function need not depend on time for S to be formed. In particular, S = Et + W
with E = −∂W/∂t, and therefore δS = t δE + E δt + δW. Then from δW = 0 for
δt = 0, we have δS = 0 for δE = 0:
δS = δ
 r1
r0
p · dr = 0 ,
for δE = 0 .
The action principle is often written in the form
δ
 t1
t0
2 T dt = 0 ,
for δE = 0 .
In fact, dS is not only equal to p dx, but also to 2T dt, because for dE = 0, we
can derive dS = 2T dt from dS = dW + E dt and dW = L dt with L = T −V =
2T −E. However, we must remember here that the integration limits will now also
be varied, because times of different lengths are necessary for the different paths
between r0 and r1, if the kinetic energy is determined by a potential energy.
For a force-free motion neither T nor V is altered, and thus
δ
 t1
t0
dt = δ {t1 −t0} = 0 ,
with constant T and V .
This principle of least time due to Fermat had already been applied by Hero of
Alexandria to the refraction of light. (It could also be a principle of latest arrival,
because we only search for an extremum with the variational calculus. Therefore, I
have also avoided the name principle of least action for the action principle.) Here
the position coordinates are missing in the Hamilton function, e.g., S = p · x. With
the characteristic function and for the action function, each cyclic coordinate x leads
to a term px, which comprises the whole x-dependence!
So far, for all transformations, the time t has not been altered, but treated as
an invariant parameter. If we had altered it in addition to the position and momen-
tum coordinates, then we would have had to keep ﬁxed another parameter τ in the
variation—some parameter has to mark the progress along the path. Then since
L =
f

k=1
pk
dxk
dt −H ,
a generalized Hamilton principle has the form
δ
 τ1
τ0

f

k=1
pk
dxk
dτ −H dt
dτ

dτ = 0 ,
with δτ = 0 .

142
2
Classical Mechanics
This suggests taking t as a further coordinate x0 and −H as its conjugate momentum:
δ
 τ1
τ0
f

k=0
pk
dxk
dτ dτ = 0 ,
with δτ = 0 .
After a canonical transformation here, p′
k and x′k would appear, with −p′
0 as the new
Hamilton function and x′0 the new time. With a generating function G(xk, p′
k), we
obtain f +1 pairs of equations
pk = ∂G
∂xk ,
x′k = ∂G
∂p′
k
,
for k ∈{0, . . . , f } .
These more general equations are only necessary for time-dependent Hamilton
functions. As an example of this, we consider the time-dependent oscillator, in
Sect. 2.4.11.
2.4.9
Motion in a Central Field
For a central ﬁeld the angular momentum is conserved. We may restrict ourselves to a
plane orbit with polar coordinates r and ϕ. According to p. 97, since pr = ∂L/∂˙r =
m˙r and pϕ = ∂L/∂˙ϕ = mr2 ˙ϕ, we obtain for the kinetic energy
T = m
2 (˙r2 + r2 ˙ϕ2) = 1
2m

pr
2 + pϕ2
r2

.
Since ϕ does not appear in L = T −V (r), the component of the angular momentum
perpendicular to the plane of motion, pϕ, is a constant of the motion. Since the energy
E is also conserved, conservation of energy can be used:
˙r2 = 2
m

E −V (r) −pϕ2
2mr2

,
˙ϕ = pϕ
mr2 .
The last term inside the curly brackets comes from the centrifugal force. Part of
the energy appears because of the centrifugal potential as rotational energy. In the
ordinary differential equation ˙r = f (r), the variables can be separated and then
integrated:
t −t0 =

m dr

2m {E −V (r)} −(pϕ/r)2 .
Hence t(r) or r(t) can be obtained. Then the last expression for ˙ϕ no longer contains
any unknown term. This equation supplies the area–velocity law: r2 ˙ϕ = (r × v ) ·
n = pϕ/m. The integration constants are E, pϕ, r0, and ϕ0.

2.4 Hamiltonian Mechanics
143
In many cases, we desire only the equation r(ϕ) of the orbit. Then we use
dr
dϕ = ˙r
˙ϕ =

2m {E −V (r)} −(pϕ/r)2 r2
pϕ
,
and separate again in terms of variables. If the radicand vanishes, we have to expect a
circular orbit, since then ˙r = 0 and thus r = r0 and ϕ = pϕ t/(mr02) + ϕ0 (if pϕ ̸= 0
and r0 > 0).
The Hamilton–Jacobi equation for this problem reads
∂W
∂t + 1
2m
∂W
∂r
2
+ 1
r2
∂W
∂ϕ
2
+ V (r) = 0 .
Since t and ϕ do not occur in H, we may set W = S(r) + pϕϕ −Et, and from the
last differential equation, we obtain
S =
 
2m {E −V (r)} −(pϕ/r)2 dr .
This expression also delivers the orbit equation, because it yields ϕ′ = ∂W/∂pϕ =
∂S/∂pϕ + ϕ. According to this, r and ϕ are then related, as we have found before
from dr/dϕ:
ϕ −ϕ′ =

pϕ dr
r2 
2m {E −V (r)} −(pϕ/r)2 .
Likewise, we could also have arrived immediately at −t0 = ∂W/∂E = ∂S/∂E −t.
From the beginning we have only considered plane orbits. If this plane is still
unknown, then spherical coordinates are suggested. Then we have
T = m
2 (˙r2 + r2 ˙θ2 + r2 sin2 θ ˙ϕ2) = 1
2m

pr
2 + pθ 2
r2 +
pϕ2
r2 sin2 θ

,
with pθ = mr2 ˙θ and (the new) pϕ = mr2 sin2 θ ˙ϕ. With W = S −Et, this leads to
the the Hamilton–Jacobi equation
1
2m
∂S
∂r
2
+ 1
r2
∂S
∂θ
2
+
1
r2 sin2 θ
∂S
∂ϕ
2
+ V (r) = E .
Since ϕ does not appear here, we have a conserved quantity
∂S
∂ϕ = pϕ ,

144
2
Classical Mechanics
in addition to the energy E. For a central force, each component of the angular
momentum is conserved, thus also the square of the angular momentum, which we
denote here as
pθ,ϕ
2 =
∂S
∂θ
2
+
1
sin2 θ
∂S
∂ϕ
2
.
From this we conclude
1
2m
∂S
∂r
2
+ pθ,ϕ2
r2

+ V (r) = E .
Here, pϕ is no longer of interest, but only the conserved quantities pθ,ϕ and E. For
central forces there is a degeneracy, because different pϕ lead to the same pθ,ϕ2. The
last equation once again delivers the above-mentioned expression for ˙r, since
pr = ∂W
∂r = ∂S
∂r =

2m

E −V (r)

−pϕ2
r2
is equal to m˙r.
2.4.10
Heavy Symmetrical Top and Spherical Pendulum
If the center of mass of a pendulum moves on a spherical surface, we have a spherical
pendulum—or even a heavy top, if the body rotates about the axis connecting the
hinge and the center of mass. The spherical pendulum is not much simpler to treat
than the heavy top, and clearly a special case of the top, which we would like to deal
with anyway.
If the center of mass does not lie on the vertical through the rotational point,
the gravitational force exerts a torque and changes the angular momentum along
the horizontal direction. Hence, consideration of the “free” top in Sect. 2.2.11 is
no longer adequate. The kinetic energy of the top reads most simply in Cartesian
coordinates along the principal axes of the moment of inertia ﬁxed in the body:
T = 1
2 (I1 ω1
2 + I2 ω2
2 + I3 ω3
2) .
On the other hand, the Euler angles are suitable coordinates to describe the motion
in space. Therefore we express ω using the Euler angles and their derivatives with
respect to time.
In the body-ﬁxed system, the space-ﬁxed z-axis has polar angles β and π −γ
(see Fig. 1.10). Therefore, for a rotational vector proportional to ˙α, it follows that
(Problem 2.4)

2.4 Hamiltonian Mechanics
145
ωα = ˙α {sin β (−cos γ e1 + sin γ e2) + cos β e3} .
Correspondingly, ωβ = ˙β (sin γ e1 + cos γ e2) and ωγ = ˙γ e3, whence
ω1 = −˙α sin β cos γ + ˙β sin γ ,
ω2 =
˙α sin β sin γ + ˙β cos γ ,
ω3 =
˙α cos β
+ ˙γ .
Hence we have ω12 + ω22 = ˙α2 sin2 β + ˙β2. Since with s as the distance of the
center of mass from the rotational point, the potential energy is
V = mgs cos β ,
we shall restrict in the following to a symmetrical top (I1 = I2) or a symmetric
pendulum. Then, since
T = 1
2 I1 (˙α2 sin2 β + ˙β2) + 1
2 I3 (˙α cos β + ˙γ )2 ,
α and γ are cyclic coordinates, ∂H/∂α = 0 = ∂H/∂γ , and thus the associated gen-
eralized momenta—the angular-momentum components along the lab-ﬁxed and the
body-ﬁxed z-axes—are constants of the motion:
pγ = ∂L
∂˙γ = I3 (˙α cos β + ˙γ ) = const.,
pα = ∂L
∂˙α = I1 ˙α sin2 β + pγ cos β = const.
(If pγ = 0, then we have a spherical pendulum instead of the top—for the plane
pendulum, pα = 0 also holds.) Only pβ = ∂L/∂˙β = I1 ˙β still depends on time.
But this is therefore a one-dimensional problem, which we simply solve using the
conservation of energy—then we avoid a differential equation of second order:
H =
1
2I1

pβ
2 +
 pα −pγ cos β
sin β
2
+ pγ 2
2I3
+ mgs cos β
is a constant of the motion. Hence we now have to determine β(t). The expression
for pα leads to a linear differential equation of ﬁrst order for α(t), and the expression
for pγ to a similar equation for γ (t).
In order to avoid the transcendent circular functions, we set
cos β = z
=⇒
˙β =
−˙z
√
1 −z2 ,
and then obtain

146
2
Classical Mechanics
Fig. 2.31 The three Jacobi elliptic functions sn(τ|k2) (continuous red), cn(τ|k2) (dashed blue),
and dn(τ|k2) (dotted black) for the parameter k2 = 1
2. Compare also with Fig. 2.18
I1
2 ˙z2 = (1 −z2)

H −pγ 2
2 I3
−mgs z

−(pα −pγ z)2
2 I1
≡mgs f (z) .
Here, f (z) is a polynomial of third order in z, which is important for us only in
the regime −1 ≤z ≤1, and there also only for f (z) ≥0. Now f (z) is positive for
z ≫1 and negative for z = ±1 (or zero in the special case of a top with perpendic-
ular axis of rotation and therefore without torque). Thus only the two lower zeros
of f (z) are relevant here. The differential equation can be solved with the Jacobi
function sn(τ|k2) mentioned on p. 105. For this as for the other elliptic functions,
it is customary (see, e.g., [1]) to number the zeros zi of the polynomials in order of
decreasing value, viz., z1 > z2 > z3. The zero time can be chosen as the integration
constant:
z(t) = z3 + (z2−z3) sn2mgs
2I1
(z1−z3) (t−t0)
 z2−z3
z1−z3

.
The derivative of sn τ is equal to the product of the Jacobi elliptic functions cosinus
amplitudinis cn τ and delta amplitudinis dn τ (see Fig. 2.31):
cn(τ|k2) ≡cos(am(τ|k2)) ,
dn(τ|k2) ≡

1 −k2 sn2(τ|k2) .
Consequently,inadditiontosn(τ|k2) = sin(am(τ|k2))andsn′(τ|k2) = cn(τ|k2) ·
dn(τ|k2), we have
sn2(τ|k2) = 1 −cn2(τ|k2) = 1 −dn2(τ|k2)
k2
.
The above-mentioned expression z(t) therefore satisﬁes the original differential
equation ˙z2 = (z −z1)(z −z2)(z −z3) 2mgs/I1 for z3 ≤z ≤z2 < z2z1. The ﬁgure
axis of the heavy top thus tumbles back and forth between two circles of latitude

2.4 Hamiltonian Mechanics
147
Fig. 2.32 Orbits of the body axis of a heavy symmetric top (red line). Left: With loops. Centre:
With peaks. Right: With simple passes. Dashed blue lines are the limiting circles of latitude of the
intersections of the ﬁgure axis on the sphere
β2,3 = arccos z2,3 (with β2 ≤β3). For the ﬁrst return to the old circle of latitude, half
an “oscillation” is performed. Thus the oscillation period is
T = 4

I1
2mgs
 z2
z3
dz
√f (z) = 2

2I1
mgs
2
√z1−z3
K
z2−z3
z1−z3

.
As with the plane pendulum (see p. 104), we thus arrive at a complete elliptic integral
K, however, we still have to determine the three solutions zi (see Fig. 2.32).
For the tumbling motion, there are simple passes, but also loops or peaks. This can
be read off from the zeros of I1 ˙α = (pα −pγ cos β)/ sin2 β, which are determined
by pα −pγ z: for z3 < pα/pγ < z2, there are loops, for pα/pγ equal to z3 or z2, there
are peaks, and otherwise (with pα/pγ < z3 or pα/pγ > z2), neither loops nor peaks.
This clearly holds also for the force-free top (with mgs = 0), which was already dealt
with in Sect. 2.2.12.
Peaks occur, e.g., for the frequent initial condition ˙α(0) = ˙β(0) = 0, for motions
with an energy as small as possible, because ˙α(0) = 0 delivers z(0) = pα/pγ , and
since ˙β(0) = 0, ˙z also vanishes initially and hence so does f (z). We thus start from
one of the limiting circles of latitude with a peak. In fact, the nutation starts from the
upper circle of latitude (z2), because there the potential energy is highest, whence
the kinetic energy is lowest. For these initial conditions, we already know the zero
z2 of f (z), viz.,
z2 = pα
pγ
=
1
mgs

H −pγ 2
2I3

,
and can determine the other zero z3 more easily from a second-order equation,
because
mgs f (z) = (z2 −z)

mgs (1 −z2) −pγ 2
2I1
(z2 −z)


148
2
Classical Mechanics
delivers mgs (1−z32) = [pγ 2/(2I1)] (z2−z3). For a fast top, pγ 2/(2I3) ≫mgs
holds. If now I1 is not very much greater than I3, then because 0 ≤z32 ≤1, it
follows that z3 ≈z2. Therefore, we obtain
z2 −z3 ≈
mgs
pγ 2/(2I1) sin2 β(0) ,
i.e., the faster the top rotates, the less its nutation. It can also happen that the two
circles of latitude coincide—then z and hence β are constant, as are ˙α and ˙γ , and
we have regular precession. For very small nutation compared to the precession, we
speak of pseudo-regular precession.
Thedifferentialequation I1 ˙α = (pα −pγ z)/(1 −z2)fortheEuleranglesα canbe
reformulatedinthefollowingwayusing ˙α = (dα/dz) ˙z = √2mgs/I1
√f (z) dα/dz:
dα
dz =

pγ 2/(2I1)
mgs
pα/pγ −z
2 √f (z)

1
1 + z +
1
1 −z

,
with f (z) = (z−z1)(z−z2)(z−z3) and z1 > z2 ≥z > z3. The solution of this dif-
ferential equation can be given with the help of the incomplete elliptic integral of the
third kind
Π(n; ϕ |k2) ≡
 ϕ
0
dψ
(1 −n sin2 ψ)

1 −k2 sin2 ψ
=
 sin ϕ
0
dt
(1 −n t2)

(1 −t2)(1 −k2 t2)
,
and with the integral of the ﬁrst kind F(ϕ |k2) from Sect. 2.3.6. With the abbreviations
g(z) ≡
 z −z3
z2 −z3
and
k2 = z2 −z3
z1 −z3
,
both with values between 0 and 1, we have in particular
 z
z3
q −t
p −t
dt
√f (t) =
2
√z1 −z3
 q −p
p −z3
Π
z2 −z3
p −z3
arcsin g(z)
 k2

+ F

arcsin g(z)
 k2

.
Therefore, after an oscillation period T , the body axis does not return to the initial
point, in contrast to what happens with the plane pendulum, but precesses about the
angle

2.4 Hamiltonian Mechanics
149
Fig. 2.33 Complete elliptic
integrals of the ﬁrst kind
K(k2) (continuous green), of
the second kind E(k2)
(continuous red), and of the
third kind Π(n | k2) (dashed
black), where n changes in
steps of 1/4 (top 3/4, bottom
−1). We also have
Π(0 | k2) =K(k2)
α = 4

p2
γ /(2I1)
mgs (z1 −z3)
 1 + pα/pγ
1 + z3
Π

−z2 −z3
1 + z3
; π
2
 k2

−1 −pα/pγ
1 −z3
Π

+z2 −z3
1 −z3
; π
2
 k2

.
Due to the argument 1
2π here, complete elliptic integrals of the third kind occur,
written for short Π(n | k2) (see Fig. 2.33).
2.4.11
Canonical Transformation of Time-Dependent
Oscillators
The time-dependent oscillator investigated in Sect. 2.3.10 offers an instructive exam-
ple of how a canonical transformation can transform a time-dependent Hamilton
function into a time-independent one.
According
to
Floquet,
Hill’s
differential
equation
¨x + f (t) x = 0
with
f (t + T ) = f (t) also has quasi-periodic solutions xF(t) = y(t) exp(iφt/T ) with
y(t + T ) = y(t). Here, φ is real for stable solutions, to which we would like to restrict
ourselves here, even if then not all periodic functions f (t) are allowed. We now take
xF and xF∗as the fundamental system and set w = (˙xF xF∗−xF ˙x∗
F)/(2i) = w∗> 0.
(It will turn out that w corresponds to an angular frequency. The similarity with ω is
intended. For w < 0, we have to swap xF ↔xF∗.) The value w does not depend on
t, because it is the Wronski determinant of the two solutions, except for the factor 2i
in the denominator. Two real fundamental solutions are often taken, which behave
for t ≈0 like the circular functions cos(wt) and sin(wt). Here we prefer exp(±iwt)
for t ≈0.

150
2
Classical Mechanics
Fig. 2.34 Solutions of the Mathieu differential equation ¨x = 1
2q2 cos t x for q = 1/4 (dotted
black), q = 2/4 (continuous red), and q = 3/4 (dashed blue). Amplitude A (left) and phase ϕ
(right) of the Floquet solutions as a function of t/T . The amplitude has period T , while the phase
increases by φ during this time
In the following, it will be useful to set x = A exp(iϕ) with real functions A(t)
and ϕ(t). From Hill’s differential equation, we then have the two equations
¨A + f A = w2
A3
and
˙ϕ = w
A2 .
Here the quasi-periodicity of the Floquet solution xF also delivers
AF(t + T ) = AF(t)
and
ϕF(t + T ) = ϕF(t) + φ .
The amplitude AF is thus strictly periodic, while the phase ϕF increases by φ with
each period T . Note that φ > 0 holds because ˙ϕF = w/A2 > 0.
In the following, we leave out the index F and choose as initial conditions A(0) =
1, ˙A(0) = 0, and ϕ(0) = 0. Then w is also uniquely determined.
As an important example we consider the Mathieu differential equation. As in
Sect. 2.3.10, ¨x + f (t) x = 0 with f (t) = 1
42 (a −2q cos t). Figure 2.34 shows
the amplitude and phase of the Floquet solutions, and Fig. 2.27 its real part. Since
the amplitude is periodic, it can be expanded in a Fourier series. We consider now
A2(t) =
∞

n=0
bn cos(nt)
=⇒
ϕ(t) =
 t
0
w dt′
∞
n=0 bn cos nt′ ,
since its Fourier coefﬁcients converge quickly to 0 as qn/(n!)2 and can be deter-
mined from a recursion relation. (This is shown in [5].) The Wronski determinant w
becomes imaginary at the stability limits. Note that, in the unstable region, the same
recursion relation holds for an expansion A2 = 
n bn cosh(t). The phase ϕ fol-
lows numerically from the above-mentioned integral expression using the Simpson
method.

2.4 Hamiltonian Mechanics
151
Ifwenowtakethegeneratingfunction G(t, p, x′) = −A p x′ + 1
2m A ˙A x′ 2 (thus
with x = −∂G/∂p = A x′ and p′ = −∂G/∂x′ = A p −m A ˙A x′), then from H =
1
2m p2 + m
2 f x2, we have
H ′ = H + ∂G
∂t =
p′ 2
2m A2 + m
2

¨A + f A

A x′ 2 ,
since ∂G/∂t = −˙Apx′ + 1
2m( ˙A2+ A ˙A)x‘2. For t = 0, we should have x′ = x and
p′ = p, thus A(0) = 1 and ˙A(0) = 0. Because ¨A + f A = w2/A3, we arrive at
H ′ = I w
A2 ,
with
Iw ≡p′ 2
2m + m w2
2
x′ 2 ,
and because ˙I = [I, H ′] = [I, I] w/A2 = 0, I does not depend upon t and is thus
an invariant. Since w/A2 = dϕ/dt, it is clearly appropriate here to use the phase
instead of the time. For each observable B not explicitly depending on time, we then
have
dB
dt = [B, I] dϕ
dt
=⇒
dB
dϕ = [B, I] .
In order to determine the function B(ϕ), we therefore only need to know the invariant
I. In particular, the position and momentum can then be determined. (Neither ϕ nor
I nor H ′ depend on the choice of scale for w: for A →cA′, we have in particular
w →c2w, x′ →c−1x′, and p′ →cp′.)
The invariant I does indeed help for the computation of the time dependence (of,
e.g., position and momentum), because H ′ = Iw/A2 is a Hamilton function, but H ′
is not an energy. For this, the gauge is chosen such that the Hamilton function is
composed of a potential and a kinetic energy according to p. 124. This works with
E = (p −mFx)2
2m
+ m
2 f x2 ,
if ˙F = f −f (and F = 0 ) .
Once again, the bar indicates the time average (F need not be zero, but this choice
makes F2 as small as possible, which has advantages), and thus m
2 f x2 is a potential
energy. The given expression for E via the generating function
G(t, p, x′) = A { 1
2m ( ˙A + A F) x′ 2 −p x′}
leads to the above-mentioned form H ′ = Iw/A2, thus also allowed as a Hamilton
function. Because ˙x = ∂E/∂p = (p −mFx)/m, the part (p −mFx)2/(2m) can be
viewed as a kinetic energy m
2 ˙x2. Since ˙p = −∂E/∂x = (p −mFx) F −m f x =
m (˙x F −f x), it turns out that ¨x = −f x.

152
2
Classical Mechanics
2.4.12
Summary: Hamiltonian Mechanics
When searching for the time dependence, we tend to rely on conserved quantities.
Therefore, momenta are often better to use than velocities. In the Hamiltonian for-
malism, canonical transformations between position and momentum coordinates are
permitted. Here, the difference between the two kinds of variables is blurred: we only
talk about canonical variables in phase space. Because of the greater freedom in the
choice of the phase space coordinates, even more suitable coordinates for a problem
can be found than in Lagrangian mechanics.
Moreover, formally, Hamiltonian mechanics is to be preferred because the Hamil-
ton function H is the generating function of inﬁnitesimal variations in time. The
Liouville equation can be derived from this (important for statistical mechanics),
and the Poisson brackets are also useful in quantum mechanics.
According to the Hamilton–Jacobi theory, the Hamilton equations
˙xk = ∂H
∂pk
,
˙pk = −∂H
∂xk ,
can be combined into a single partial differential equation which is useful also in
light-ray optics, viz.,
∂W
∂t + H

t, x, ∂W
∂x

= 0 ,
where W is the action
W =

L dt .
Conversely, dW/dt delivers the Lagrange function and everything that follows like-
wise from derivatives.
The goal, namely to treat problems with many degrees of freedom with a single
equation, is therefore achieved by Hamilton’s principle
δW = 0 ,
at δt = 0 .
Since δW =
	 t1
t0 δ(T + A) dt, it may even be applied to cases for which no potential
energy exists, and hence there is neither a Lagrange function nor a Hamilton function.

Problems
153
Problems
Problem 2.1 Determine the 3 × 3 matrix of the rotation operator D for a body as a
function of the Euler angles α, β, γ , which are introduced in Fig. 1.10 on p. 30. (2 P)
Problem 2.2 Verify the result for the following 7 special cases: no rotation, 180◦
rotation about the x-, y-, z-axis, and 90◦rotation about the x-, y-, z-axis. Which
Euler angles belong to these 7 cases?
Hint: Here, occasionally only α + γ or α −γ are determined.
(7 P)
Problem 2.3 Which Euler angles {α, β, γ } belong to the inverse rotations? (Note
that 0 ≤α < 2π, 0 ≤β ≤π, and 0 ≤γ < 2π.)
(3 P)
Problem 2.4 The rotation operator and Euler angles are needed to describe a top.
For this application, the original coordinate system is the laboratory system, the new
system is the body-ﬁxed system. Let the unit vectors be lx, ly, lz or kx, ky, kz. Let
ωγ = ˙γ kz and determine the corresponding decomposition into ωβ = ˙βeline of nodes
and ωα = ˙αlz in the body-ﬁxed and lab-ﬁxed systems.
Let a rotation be D = Dα Dβ. How do the vector A and the matrix M given by
A =
⎛
⎝
ax
ay
az
⎞
⎠
and
M =
⎛
⎝
0
az
−ay
−az
0
ax
ay −ax
⎞
⎠
transform under the rotation D? Note that the fact that M′ = DM D−1 = DM D
shows the same behavior under a rotation as A′ = DA is connected to the notion of
axial vector.
(8 P)
Problem 2.5 Is the tensor force F = 3μ0
4π T with (see p. 56)
T(r) = (m · r) m′ + (m′ · r) m + (m · m′) r
r5
−5 (m · r)(m′ · r) r
r7
curl-free?
Hint: To investigate the singularity for r = 0, we may encircle the origin and apply
Stokes’s theorem ∇× F = limA→0 1
A
	
(A) dr · F.)
(8 P)
Problem 2.6 Determine the potential energy V for Problem 2.5, and check that
F = −∇V .
(4 P)
Problem 2.7 A circular disk of radius R rolls, without sliding, on the x, y-plane. In
addition to the two coordinates (x, y) of the point of contact, the three Euler angles α,
β,γ arise,becausethenormaltothecirculardischassphericalcoordinates(β, α),and
γ describes the rotation of the disc. The problem requires ﬁve coordinates with ﬁnite
ranges, having ﬁve degrees of freedom (see Fig. 2.35). However, the static friction
also delivers two differential conditions between the coordinates on the inﬁnitely
small scale:

154
2
Classical Mechanics
Fig. 2.35 Rolling circular
disc with normal n. The
Euler angles α and β are
shown, but not the rolling
angle γ . The orbit arises
from the rotation about the
normal in the positive sense
Fig. 2.36 Crank motion.
The pinion runs on a circle
of radius R and moves a
connecting rod of length L
• How do the constraints for the virtual displacements read?
• How many degrees of freedom does the disc have on the inﬁnitely small scale?
• Whydotheequations(α, β, γ, x, y) = 0hereleadtoinnercontradictions?(Why
are the constraints non-holonomous)?
(8 P)
Problem 2.8 How do the Lagrange equations of the ﬁrst kind read in statics if the
constraints are given only in differential form (as in the last problem), namely through
3N
m=1 φnm δxm = 0? Here n counts the 3N −f constraints.
Use this for Problem 2.7 to determine the Lagrangian parameters, and interpret
the connection found between the generalized forces. Show in particular that, in the
contact plane, a tangential force acts on the disc, that Fγ cancels its torque, and that
both Fα and Fβ are equal to zero.
(8 P)
Problem 2.9 How strong does the force F2(F1, ϕ) at the crank in Fig. 2.36 have to
be for equilibrium? Determine this using the principle of virtual work.
(4 P)
Problem 2.10 What does one obtain for this crank from the Lagrange equa-
tions (Cartesian coordinates with origin at the center of rotation)? Do the results
agree?
(8 P)
Problem 2.11 How much does the eccentricity ε differ from 1 for a given axis ratio
b : a ≤1 of an ellipse? Relate the difference between the distances at aphelion and

Problems
155
perihelion for this ellipse to the mean value of these distances, and compare this with
the axis ratio b/a. What follows then for small ε if we account only for linear, but no
squared terms in ε? (For the orbits of planets around the Sun, ε < 0.1.) For Comet
Halley, ε = 0.967 276 0. How are the two axes related to each other, and what is the
ratio of the lowest to the highest velocity?
(2 P)
Problem 2.12 Let the polar angle ϕ = 0 be associated with the aphelion of the orbit
of the Earth (astronomers associate ϕ = 0 with the perihelion) and the polar angle ϕF
with the beginning of spring. Then ϕ increases by π/2 at the beginning of the summer,
autumn,andwinter,respectively.InHamburg,Germany,thelengthsoftheseasonsare
Tsp = 92 d 20.5 h, Tsu = 93 d 14.5 h, Tfa = 89 d 18.5 h, Twi = 89 d 0.5 h. Determine
ϕsp and ε, neglecting squared terms in ε compared to the linear ones.
(6 P)
Problem 2.13 By how much is the sidereal day shorter than the solar day (the time
between two highest altitudes of the Sun)? By how much does the length of the
solar day change in a year? (The result should be determined at least to a linear
approximation as a function of ε. Then for ε = 1/60, the difference between the
longest and shortest solar day follows absolutely.)
(4 P)
Problem 2.14 Why are the following three theorems valid for the acceleration ¨r =
−kr (and constant k > 0)?
• The orbit is an ellipse with the center r = 0.
• The ray r moves over equal areas in equal time spans.
• The period T does not depend on the form of the orbital ellipse, but only on k.
Hint: Show that at certain times r and ˙r are perpendicular to each other. With such
a time as the zero time, the problem simpliﬁes enormously.
(8 P)
Problem 2.15 What is the kinetic energy T ′
2L of a mass m2 in the laboratory sys-
tem after the collision with another mass m1 initially at rest, taken relative to
its kinetic energy T2L before the collision as a function of the scattering angle
θS (in the center-of-mass system) and of the heat tone Q or the parameter ξ =
√1 + (m1+m2)/m1 · Q/T2L? How does this ratio read for equal masses and elastic
scattering as a function of the scattering angle in the laboratory system?
(4 P)
Problem 2.16 What is the angle between the directions of motion of two particles
in the laboratory system after the collision? Consider the special case of elastic
scattering and in particular of equal masses.
(4 P)
Problem 2.17 Two smooth spheres with radii R1 and R2 collide with each other
with the collision parameter s (see Fig. 2.37). How large is the scattering angle
θS?
(2 P)
Problem 2.18 How high is the mass m1 of a body initially at rest, which has col-
lided elastically with another body of mass m2 and momentum p2, if it is scattered
by θ2L = 90◦and keeps only the fraction q of its kinetic energy in the laboratory
system?
(3 P)

156
2
Classical Mechanics
Fig. 2.37 For the collision
of two smooth spheres in the
center-of-mass system, the
component of p in the
direction of the line
connecting the sphere
centers becomes reversed,
the component perpendicular
to it is conserved (see
Problem 2.17)
Problem 2.19 A spherical rain drop falls in a homogeneous gravitational ﬁeld with-
out friction through a saturated cloud. Its mass increases in proportion to its sur-
face area with time. Which inhomogeneous linear differential equation follows for
the velocity v, if instead of the time, we take the radius as independent variable?
What is the solution of this differential equation? (In practice, we consider only the
momentum ∝r3v as an unknown function.) Compare with the free fall of a constant
mass.
(7 P)
Problem 2.20 How can we show using Legendre polynomials that the gravitational
potential is constant within an inhomogeneous, but spherically symmetric hollow
sphere, and therefore that it does not exert there a gravitational force on a test body.
How does the potential read if a sphere with radius r1 and homogeneous density ρ1
is covered by a hollow sphere of homogeneous density ρ2 and external radius r2?
(The Earth has a core, mainly of iron, and a mantle of SiO2, MgO, FeO, and others,
approximately 2900 km thick.)
(6 P)
Problem 2.21 What height is reached by a ball thrown vertically upwards with
velocity v0? Consider the friction with the air (Newtonian friction) and determine
the frictional work done, by integration as well as by comparing heights with and
without friction.
(8 P)
Problem 2.22 A horizontal plate oscillates harmonically up and down with ampli-
tude A and oscillation period T . What inequality is obeyed by A and T if a loosely
attached body on the plate does not lift off?
(2 P)
Problem 2.23 A car at a speed of 20 km/h runs into a wall and is then evenly
decelerated, until it stops, at which point it has been deformed by 30 cm. What is the
deceleration during the collision? Can a weightlifter who can lift twice the weight
of his body protect himself from hitting the steering wheel? If two such cars with
relative velocity 40 km/h hit each other head-on, are the same processes valid for the
single drivers as above, or do double or fourfold forces arise?
(4 P)
Problem 2.24 Prove the following theorem: For each plane mass distribution, the
moment of inertia with respect to the normal of the plane is equal to the sum
of the moments of inertia with respect to two mutually perpendicular axes in the
plane.
(1 P)

Problems
157
Problem 2.25 Derive from that the main moments of inertia of a homogeneous
cuboid with edge lengths a, b, and c.
(1 P)
Problem 2.26 Determine the moment of inertia of the cuboid with respect to the
edge c using three methods:
• As in Problem 2.25.
• Using Problem 2.25 but dividing up a correspondingly larger cuboid.
• Using Steiner’s theorem.
(2 P)
Problem 2.27 Decide whether the following claim is correct: The moment of inertia
of a rod of mass M and length l perpendicular to the axis does not depend on the
cross-section A, and with respect to an axis of rotation on the face is four times as
large as with respect to an axis of rotation through the center of mass.
(2 P)
Problem 2.28 Prove the following: Rotations about the axes of the highest and
lowest moments of inertia are stable motions, while rotations about the axis of the
middle moment of inertia are unstable.
Hint: Use the Euler equations for the rigid body, and make an ansatz for the angular
velocity ω = ω1 + δω with constant ω1 along a principal axis of the moment of
inertia under small perturbations δω = δ exp(λt) perpendicular to it. This implies a
constraint for λ(I1, I2, I3, ω1).
(4 P)
Problem 2.29 How high is the Coriolis acceleration of a sphere shot horizontally
with velocity v0 at the north pole? Through which angle ϕ is it deﬂected during the
time t? Through which angle does the Earth rotate during the same time?
(2 P)
Problem 2.30 A uniform heavy rope of length l and mass μl hangs on a pulley of
radius R and moment of inertia I, with the two rope ends initially at the same height.
Then the pulley gets pushed with ˙θ(0) = ω0. Neglect the friction of the pulley about
its horizontal axis. As long as the rope presses on the pulley with the total force
F ≥F0, the static friction leads to the same (angular) velocity of rope and pulley—
after that the rope slides down faster. How does the (angular) velocity depend on the
time, up until the rope starts sliding? What is the difference in height of the ends of
the rope at this time?
(8 P)
Problem 2.31 Show that the homogeneous magnetic ﬁeld B = Bez may be asso-
ciated with the two vector potentials A1 = 1
2(B × r) and A2 = Bxey (gauge invari-
ance). What scalar ﬁeld ψ leads to ∇ψ = A1 −A2? What is the difference between
the associated Lagrange functions L1 and L2? Why is it that this difference does not
affect the motion of a particle of charge q and mass m in the magnetic ﬁeld B? (6 P)
Problem 2.32 Two point masses interact with V (|r1 −r2|) and are not subject to
any external forces. How do the Lagrange equations (of the second kind) read in the
center-of-mass and relative coordinates?
(4 P)

158
2
Classical Mechanics
Fig. 2.38 Double pendulum made from rods of mass m1 and m2 and with moments of inertia I1
and I2 with respect to the hinges (•), which are separated by a distance l. The distances of the
centers of mass of the rods from the hinges are s1 and s2, respectively
Problem 2.33 For the double pendulum in Fig. 2.38, determine T as well as V
as a function of θ1, θ2, ˙θ1, and ˙θ2. How are these expressions simpliﬁed for small
amplitudes?
(6 P)
Problem 2.34 In the last problem, let θ1 = θ2 = 0 for t < 0. At time t = 0, the
upper pendulum obtains an impulse, in fact with angular momentum L with respect
to its hinge. What initial values follow for ˙θ1 and ˙θ2, in particular for the mathematical
pendulum?
(4 P)
Problem 2.35 A homogeneous sphere of mass M and radius r rolls on the inclined
plane shown in Fig. 2.39 (with g · ex = 0). Its moment of inertia is I = 2
5 Mr2.
Determine its Lagrange function and the equations of motion for the coordinates
(x, z) at the point of contact. (Here we use z instead of y, in anticipation of the next
problem.)
(3 P)
Problem 2.36 Treat the corresponding problem if the plane is deformed into a cylin-
drical groove with radius R and axis parallel to ez (see Fig. 2.40). (Instead of x, it is
better to adopt the cylindrical coordinate ϕ with ϕ = 0 at the lowest position.) How
large may ˙ϕ(0) be at most, if we always have |ϕ| ≤1
2 π and ϕ(0) = 0?
(4 P)
Fig. 2.39 Oblique plane
with inclination angle α (the
angle between the
downwards oriented normal
and the vertical), whence
g · ez = g sin α

Problems
159
Fig. 2.40 Sphere in a
groove. A sphere with radius
r rolls on a circle of radius
R, then with the angles ψ
and ϕ shown, the relation
r (ψ + ϕ) = R ϕ holds
Problem 2.37 Determine the resonance angular frequency ωR of a forced damped
oscillation and show that the frequency ω0 of the undamped oscillation is higher than
ωR. What is the ratio of the oscillation amplitude for ωR to that for ω0? What is the
approximate result for γ ≪ω0 ?
(4 P)
Problem 2.38 What differential equation and initial values are valid for the Green
function G(τ) for the differential equation ¨x + 2γ ˙x + ω2
0x = f (t) of the forced
damped oscillation with solution written as x(t) =
	 t
−∞f (t′) G(t −t′) dt′? Which
G(τ) is the most general solution, independent of f (t)? With this, the solution of
the differential equation may be traced back to a simple integration—check this for
the example f (t) = c cos(ωt) in the special case γ = ω0 (> 0).
(9 P)
Problem 2.39 What equation of motion is supplied by the Lagrange formalism for
the double pendulum investigated in Problem 2.33 in the angle coordinates θ1 and
θ2, exactly on the one hand, and for restriction to small oscillations on the other (i.e.,
taking θ1 and θ2 and their derivatives to be small quantities)?
(4 P)
Problem 2.40 Which normal frequencies ω± result for this double pendulum?
Determine the matrices A and B. Investigate also the special case of the mathe-
matical pendulum with s1 = l, and use the abbreviation σ = s2/s1 and μ = m2/m1,
where the normal frequencies are given here at best as multiples of the eigenfrequency
ω1 of the upper pendulum.
(6 P)
Problem 2.41 Determine the normal frequencies and the matrices A, B, and C
for the mathematical double pendulum with s2 = s1 = l and m2 ≪m1. (Here one
should use the fact that μ ≪1 holds in C—why does one have to calculate ω± “more
precisely by one order”?)
(6 P)
Problem 2.42 What functions θ1(t) and θ2(t) belong to the just investigated math-
ematical double pendulum (with μ ≪1) for the following initial values: θ1(0) =
θ2(0) = 0, ˙θ1(0) = −˙θ2(0) = , which according to Problem 2.34 correspond to a
collision against the upper pendulum for the double pendulum initially at rest? (Why
do we only have to consider here the behavior of the normal coordinates?) Which
angular frequencies do the beats have, and how does the amplitude of θ1 behave in
comparison to that of θ2?
(4 P)

160
2
Classical Mechanics
Problem 2.43 Prove the Jacobi identity [u, [v, w]] + [v, [w, u]] + [w, [u, v]] = 0
for the Poisson brackets, with [u, v] = uxvp −upvx and ux = ∂u/∂x, etc.
(3 P)
Problem 2.44 Determine the Poisson brackets of the angular momentum compo-
nent Lx with x, y, z, px, py, pz, and L y. Note that, by cyclic commutation,

L, r

,

L, p

, and

Li, Lk

are then also proven.
(5 P)
Problem 2.45 Under which constraints is the transformation x′ = arctan(αx/p),
p ′ = βx2+γ p2 a canonical one?
(3 P)
Problem 2.46 Is the transformation x′ = xα cos βp, p ′ = xα sin βp canonical?
(2 P)
Problem 2.47 Using the generating function
G(x, px
′, py, py
′) = x 1
2 {px
′ + py
′} −py {px
′ −py
′}/(q B) ,
show that the Hamilton function
H = 1
2m {(px + 1
2 q By)2 + (py −1
2 q Bx)2}
for a charged point mass in the plane perpendicular to a homogeneous magnetic ﬁeld
Bez can be written as the Hamilton function of a linear harmonic oscillation. (3 P)
Problem 2.48 From this derive the transformation on p. 128. Show also, without
using the generating function, that this transformation is canonical. Why does it not
sufﬁce here to compare the four derivatives ∂x′/∂x, ∂x′/∂y, ∂y′/∂x, and ∂y′/∂y
with ∂px/∂px ′, ∂px/∂py′, ∂py/∂px ′, and ∂py/∂py′, as seems to sufﬁce according to
p. 126? (Whence an additional comment is missing here.)
(4 P)
List of Symbols
We stick closely to the recommendations of the International Union of Pure and
Applied Physics (IUPAP) and the Deutsches Institut für Normung (DIN). These
are listed in Symbole, Einheiten und Nomenklatur in der Physik (Physik-Verlag,
Weinheim 1980) and are marked here with an asterisk. However, one and the same
symbol may represent different quantities in different branches of physics. Therefore,
we have to divide the list of symbols into different parts (Table 2.3).

References
161
Table 2.3 Symbols used in mechanics
Symbol
Name
Page reference
∗
v, ˙r
Velocity
2
∗
a, ˙v, ¨r
Acceleration
2
∗
F
Force
55
∗
M
Torque
70
M
Total mass
71
∗
m
Mass
69
∗
μ
Reduced mass
72
∗
A
Work
56
∗
E
Energy
78
∗
V
Potential energy
56
∗
T
Kinetic energy
70
∗
T
Oscillation period
104
∗
ρ
Density (massdensity)
81
ρ
Probability density
125
∗
p
Motional quantity, momentum
69, 93, 99
∗
L
Angular momentum
70
∗
G
Gravitational constant
623, 79
G
Generating function
130
∗
g
Free-fall acceleration
81
∗
I
Moment of inertia
87
∗
ω
Angular frequency
67
ω
Angular velocity
67
xk
Generalized coordinate
60
∗
pk
Momentum canonical conjugate to xk
93, 99
Fk
Generalized force
60
∗
L
Lagrange function
96
∗
H
Hamilton function
122
∗
W
Action function
135
∗
S
Characteristic function
136
[u, v]
Poisson bracket
124
References
1. M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions (Dover, New York, 1970)
2. P.F. Byrd, M.D. Friedman, Handbook of Elliptic Integrals for Engineers and Physicists (Springer,
Berlin, 1954)
3. J. Meixner, F.W. Schäfke, G. Wolf, Mathieu Functions and Spheroidal Functions and Their
Mathematical Foundations (Springer, Berlin, 1980)
4. D.H. Kobe, K.H. Yang, Eur. J. Phys. 80, 236 (1987)
5. A. Lindner, H. Freese, J. Phys. A 27, 5565 (1994)

162
2
Classical Mechanics
Suggestions for Textbooks and Further Reading
6. W. Greiner, Classical Mechanics—System of Particles and Hamiltonian Dynamics (Springer,
New York, 2010)
7. L.D. Landau, E.M. Lifshitz, Course of Theoretical Physics. Volume 1—Mechanics, 3rd edn.
(Butterworth-Heinemann, Oxford, 1976)
8. W. Nolting, Theoretical Physics 1—Classical Mechanics (Springer, Berlin, 2016)
9. W. Nolting, Theoretical Physics 2—Analytical Mechanics (Springer, Berlin, 2016)
10. F. Scheck, Mechanics—From Newton’s Laws to Deterministic Chaos (Springer, Berlin, 2010)
11. A. Sommerfeld, Lectures on Theoretical Physics 1—Mechanics (Academic, London, 1964)
12. D. Strauch, Classical Mechanics (Springer, Berlin, 2009)
13. W. Thirring, Classical Mathematical Physics: Dynamical Systems and Field Theories, 3rd edn.
(Springer, New York, 2013)
14. G. Ludwig, Einführung in die Grundlagen der Theoretischen Physik 1–4 (Vieweg, Braun-
schweig, 1974) (in German)
15. M. Mizushima, Theoretical Physics: From Classical Mechanics to Group Theory of Micropar-
ticles (Wiley, New York, 1972)

Chapter 3
Electromagnetism
3.1
Electrostatics
3.1.1
Overview of Electromagnetism
The basic equations of electromagnetism were found by Maxwell in 1862. They
comprise not only electricity and magnetism, but also (wave) optics (as electromag-
netic radiation)—and thus a very diverse range of phenomena. Actually, most of
this was known before Maxwell, but he discovered the displacement current and
thus also correctly connected the time-dependent electric and magnetic ﬁelds for
non-conductors. Since then the concept of ﬁelds has been accepted.
We start from Coulomb’s law giving the force between two charges, and from
this derive the electric ﬁeld. Then we consider its action on polarizable media and
discriminate between microscopic and macroscopically averaged quantities. The
essential basic concepts are electric charge and polarization.
We then consider moving charges and the Lorentz force. This will lead us to the
concept of the magnetic ﬁeld (the Biot–Savart law). Ampère’s molecular currents in
microscopic conductor loops produce magnetic moments, but otherwise cannot be
veriﬁed (as currents). The magnetic moments of elementary particles with spin 1/2
(e.g., electrons) cannot even be attributed to currents in such microscopic conductor
loops: like charges we have to accept them as non-derivable properties of these
particles. Thus the coupling between two magnetic moments is likewise discarded as
“basic”, in contrast to the force between electric charges and Coulomb’s law as the
sole basis of electromagnetism—even if the scalar interaction between charges can
be described in a simpler way than the tensor coupling between dipole moments.
The conservation law of charges and Faraday’s induction law then result from
Maxwell’s equations:
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5_3
163

164
3
Electromagnetism
∇× E = −∂B
∂t ,
∇· B = 0 ,
∇· D = ρ ,
∇× H = j + ∂D
∂t .
The various quantities have the following names:
E electric ﬁeld strength,
B magnetic displacement ﬁeld (induction),
D electric displacement ﬁeld,
H magnetic ﬁeld strength,
ρ charge density,
j current density.
The term ∂D/∂t is the density of the above-mentioned displacement current.
Maxwell’s equations connect on the one hand E with B and on the other hand D
with H. Therefore, E and B are also sometimes called ﬁeld strengths, while D and
H are referred to as excitations. The last two equations in particular contain further
ﬁelds, viz., the charge and current densities. However, the two quantities E and B
supply the force on a test charge. Here we have to know how D and E as well as H
and B are connected—only then are the source and curl densities of the ﬁelds given,
whereupon the basic theorem of vector analysis on p. 25 becomes applicable.
The wave equations for the ﬁelds result from Maxwell’s equations with D ∝E
and H ∝B. Then waves can propagate in empty space with the velocity of light
c0 = 299 792 458 m/s .
This is the same in all inertial frames, which leads to Lorentz invariance, something
we shall discuss after dealing with Maxwell’s equations. Then the four equations
for the three-vectors appearing above will be derived from two equations for four-
vectors.
After that we shall consider the electromagnetic radiation ﬁeld, which is produced
by an accelerated charge, similar to the electric ﬁeld of a charge at rest and the
magnetic ﬁeld of a uniformly moving charge.
Here we shall comply with the international system of units (SI). In addition to
length, time, and mass with the units m, s, and kg, a basic electromagnetic quantity
is introduced, namely the current strength with the unit A (ampere). Then further
units are related to these, e.g.,
volt
V ≡W/A ,
ohm
Ω ≡V/A ≡S−1 (siemens),
coulomb C ≡A s ,
farad
F ≡C/V = S s ,
weber
Wb ≡V s ,
henry H ≡Wb/A = Ω s ,
tesla
T ≡Wb/m2 .
In the international system of units, a magnetic ﬁeld constant is necessary, viz.,
μ0 ≡4π × 10−7 H/m = 4π × 10−7 N/A2 ,

3.1 Electrostatics
165
and an electric ﬁeld constant, viz.,
ε0 ≡
1
c02 μ0
= 8.854187817622 . . . × 10−12 F/m .
Here, μ0/4π appears in many equations for point charges and dipole moments, as
does 1/4πε0 = c02 μ0/4π, and c0μ0 = (c0ε0)−1 = 376.7303134618 . . .. Ω is the
so-called wave resistance in empty space, mentioned on p. 222.
However, in theoretical and atomic physics, the Gauss system of units is also often
used. There, the electromagnetic quantities are introduced differently (despite the
warning above: Coulomb’s law is taken as the starting point from which Maxwell’s
equations have to be derived, while the international system starts from Maxwell’s
equations and deduces Coulomb’s law), but irritatingly the same names and letters
are used. If we denote the quantities in the Gauss system with an asterisk, we have
E∗= √4π ε0 E ,
B∗= √4π/μ0 B ,
D∗= √4π/ε0 D ,
H∗= √4π μ0 H ,
ρ∗= ρ/√4π ε0 ,
j ∗= j/√4π ε0 .
Then Maxwell’s equations appear in the form
∇× E∗= −1
c0
∂B∗
∂t
,
∇· B∗= 0 ,
∇· D∗= 4π ρ∗,
∇× H∗= 4π
c0
j ∗+ 1
c0
∂D∗
∂t
.
Here, further factors occur in Maxwell’s equations. Particularly bothersome are the
factors 4π. They occur in the Gauss system in plane problems and are missing in
spherically symmetric ones. The difference between the two systems is dismissed as
a problem of units, even though the equations deal with quantities that do not depend
at all upon the chosen units (see Sect. 1.1.1). However, different notions generally
have different units. Thus, in the Gauss system for B∗, the gauss (G) is used and for
H ∗, the oersted (Oe). They are both equal to

g/cm s2, whence B∗and H ∗are also
easily confused. For the transition between the two unit systems, we have 10 kG =
1 T and 4π mOe = 1 A/m.
Particularly elaborate are the textbooks by Jackson and by Panofsky and Phillips
(see the recommended textbooks on p. 274). The ﬁrst employs the Gauss system in
earlier editions, but since then both have used the international system.
3.1.2
Coulomb’s Law—Far or Near Action?
In classical mechanics, mass is associated with all bodies. Some of them also carry
electric charge Q, as becomes apparent from new forces—for point charges we
usually write q. An electron, for example, has the charge

166
3
Electromagnetism
qe = −e = −1.602176462(63) × 10−19 C,
and the proton the opposite charge. There are charges of both signs (in contrast to the
mass, which is always positive) and the excess of positive or negative charge results
in the charge Q of the body. We thus introduce the charge density ρ(r), whereupon
Q =

dV ρ(r).
According to Coulomb (1785), there is a force acting between two point charges
q and q′ (at rest) at the positions r and r ′ in empty space, which depends upon the
distance as |r −r ′|−2 and which is proportional to the product qq′ of the charges.
Here the force is repulsive or attractive, depending on whether the charges have equal
or opposite sign:
F =
1
4πε0
qq′
|r −r′|2
r −r′
|r −r′| .
This is the force on the charge q. The one on q′ (at r′) is oriented oppositely, as
required by Newton’s third law (action = reaction, see p. 55). The factor (4πε0)−1
is connected with the concept of charge in the international system—it is missing
in the Gauss system. Here ε0 is the electric ﬁeld constant, and according to the last
section,
1
4πε0
≡c02
107
H
m = 8.987551787368 . . . × 109 N m2
C2
.
Hence for electron and proton pairs, we have
e2
4πε0
= 2.307 077 06(19) × 10−28 J m = 1.439 964 392(57) eV nm,
where the last expression is suitable for atomic scales, and because eV nm = MeV fm,
for nuclear physics.
Coulomb’s law describes an action at a distance. But we may also introduce a
ﬁeld E(r) which surrounds the charge q′ and acts on the test charge q(r):
F = q(r) E(r) ,
with
E(r) =
1
4πε0
q′
|r −r′|2
r −r′
|r −r′| .
This electric ﬁeld strength E is conveniently given in N/C = V/m.
The concept of a ﬁeld will be proven to be correct in the context of time-dependent
phenomena, because actions propagate only with ﬁnite velocity, which contradicts
the law of action at a distance. Therefore, we shall already use the ﬁeld concept in
electrostatics.
A point-like charge q′ is thus associated with the electric ﬁeld

3.1 Electrostatics
167
E(r) =
q′
4πε0
r −r′
|r −r′|3 = −q′
4πε0
∇
1
|r −r′| ,
the source of which is the charge q′ at the position r′, according to p. 25, and which
is irrotational (curl-free):
∇· E = q′
ε0
δ(r −r′)
and
∇× E = 0 .
From the point-like charge, we extend the notion to an extended charge with charge
density ρ′. So far we have been dealing with the special case of ρ′ = q′ δ(r −r′)
and now generalize this to
∇· E = ρ
ε0
and
∇× E = 0 .
(Here, and in the next few equations, we should have ρ′ instead of ρ and Q′ instead
of Q, but temporarily there will only be ﬁeld-creating charges and no test charges,
so we prefer to simplify the notation.) However, this is allowed only if the ﬁelds
of the various point charges superimpose linearly—and if these charges remain at
their positions when we move the test charge around as a ﬁeld sensor. (Because of
induction, this is not justiﬁed for conductors, as will become apparent on p. 181.)
For charges distributed over a sheet, the normal component of the ﬁeld strength
thus has a discontinuity (see p. 28), while the tangential component is continuous:
n · (E+ −E−) = ρA
ε0
and
n × (E+ −E−) = 0 .
The two basic differential equations for the electrostatic ﬁeld can be converted into
integral equations using the theorems of Gauss and Stokes. Instead of the charge
density ρ, only the charge Q =

dV ρ(r) enclosed in V is important:

(V )
df · E = Q
ε0
and

(A)
dr · E = 0 .
According to the last equation, we also have

dr · F = q

dr · E = 0: no work is
needed to move a test charge on a closed path in an electrostatic ﬁeld, since the
ﬁeld is irrotational. The charge-free space is also source-free. Therefore, the ﬁeld
lines, with tangents in the direction of the ﬁeld, can be taken as the lines forming
the walls of the ﬂux tubes (see Fig. 3.1 and also p. 12). Figures 3.2 and 3.3 present
examples. For two source points, we take a series of cones around the symmetry axis
with increasing units of ﬂux and then connect appropriate intersections (Maxwell’s
construction).

168
3
Electromagnetism
Fig. 3.1 Construction of ﬁeld lines around point charges q. The same displacement ﬁeld passes
through the surface of spheres with radii r ∝√|q| around q (here q = q′ is assumed, and thus equal
spheres). Disks of equal thickness are shown with dashed lines and hence with walls of equal area
dA = 2π R sin α R|dα| = 2π R |dz|, and also equal ﬂux. In the next two pictures, the intersections
of the straight lines with equal parameter sum or difference are to be connected, because what ﬂows
into a quadrangle ♦(solenoidal) (e.g., from below as ↗and ↖), must also emerge again (in the
example, diffracted at the wall of the ﬁeld-line tubes |). See also Problem 3.13
Fig. 3.2 Field lines of two like charges—the ratio of the charges on the left is 1:1 and on the right
3:1—with their saddle point between the two charges
3.1.3
Electrostatic Potential
The electrostatic force ﬁeld is irrotational. Therefore, according to p. 25, we would
like to attribute it to a scalar ﬁeld 	, which will be much easier to calculate with than
the vector ﬁeld:
E = −∇	 ,
with 	(r) ≡
1
4πε0

dV ′
ρ(r′)
|r −r′| .

3.1 Electrostatics
169
Fig. 3.3 Field lines of two unlike charges—ratio of the charges again 1:1 and 3:1 on the left and
right, respectively
	iscalledtheelectrostaticpotential,becauseitisconnectedwiththepotentialenergy
Epot. (Note that here, and in thermodynamics, we use V to denote the volume, so we
cannot use this letter for the potential energy, as is possible in classical mechanics
and quantum mechanics.) As is well known (see p. 56), we have F = −∇Epot, so
here F = q E = −q∇	. Therefore,
Epot = q 	 ,
and in classical mechanics (see p. 77), Epot = m	 with the mechanical potential 	.
Between two points r1 and r0 of different potential, there is a voltage:
U ≡	(r1) −	(r0) =
 r1
r0
dr · ∇	 = −
 r1
r0
dr · E =
 r0
r1
dr · E .
It can be positive or negative, but we are often concerned only with its absolute value.
Since ρ/ε0 = ∇· (−∇	), the potential follows from a linear differential equation
with the charge density as inhomogeneous term, viz., the Poisson equation

	 = −ρ
ε0
.
To obtain unique solutions, we have to set boundary conditions (to gauge) the solu-
tion. The potential and its ﬁrst derivatives must vanish at inﬁnity, like the charge
density.
This boundary condition can also be introduced via Green’s second theorem
(p. 17). Then one obtains the equation

170
3
Electromagnetism

(V ′)
df′ ·

	(r′) ∇′
1
|r −r′| −
1
|r −r′|∇′	(r′)

=

V ′ dV ′
	(r′) 
′
1
|r −r ′| −
1
|r −r′| 
′	(r′)

.
Here the Poisson equation and 
′|r −r′|−1 = −4π δ(r −r′) holds, according to
p. 26. Hence we obtain the “Green function solution” (see, e.g., Fig. 1.5 for the
cylindrical capacitor, with ﬁeld lines on the left ﬁeld and equipotential lines on the
right):’
4π 	(r) = 1
ε0

V ′ dV ′ ρ(r′)
|r −r′|
+

(V ′)
df′ · ∇′	(r′)
|r −r′|
−

(V ′)
df′ · 	(r′) ∇′
1
|r −r′| .
The ﬁrst integral is no longer taken over the whole space. The two surface integrals
account for all charges outside of it and occur as new boundary conditions. In partic-
ular, V ′ can also be a charge-free space, such that the ﬁrst integral vanishes. Then the
potential and ﬁeld strength are uniquely ﬁxed by 	 and ∇	 on the surface. In charge-
free space, these two vary monotonically, as follows from the Poisson equation, so
the ﬁeld has no extremum there.
Incidentally, for a charge-free space, it is sufﬁcient that either only 	 or only (the
normal component of) ∇	 is given on its surface. In particular, according to Gauss’s
theorem, for 
	 = 0, we have

df · 	∇	 =

dV ∇· 	∇	 =

dV ∇	 · ∇	 .
If two solutions 	1 and 	2 of 
	 = 0 now satisfy the boundary conditions, then
the surface integral of 	 ≡	1 −	2 vanishes because of 	1 = 	2 or n · ∇	1 =
n · ∇	2. On the right, the integrand is nowhere negative. Consequently, everywhere
in the considered volume, we have ∇	1 = ∇	2, so 	1 and 	2 differ at most by a
constant, and this can eventually be ﬁxed by the gauge.
In a ﬁnite regime, the same electric ﬁeld can be generated by different charge
distributions. The continuation across the boundaries is not unique. This should be
considered if models for the charge distribution in inaccessible regions are presented.
3.1.4
Dipoles
So far we have allowed charges of both signs, but the test body should carry only
charge of one sign, and as small as possible.
Totally new phenomena arise if the test body carries two point charges ±q of
opposite sign. For simplicity, we assume that its total charge Q =

dVρ(r) vanishes,

3.1 Electrostatics
171
otherwise we would also have to consider the properties of a monopole, which have
already been treated. An ideal dipole consists of two point charges ±q at the positions
r± = ± 1
2 a, where a is as small as possible, but the product qa is nevertheless ﬁnite.
We thus introduce the dipole moment
p ≡

dV r ρ(r) .
In the example considered, we would have p = qa. For ﬁnite a, higher multipole
moments appear, i.e., integrals over ρ with weight factors other than r or 1, which
we shall only discuss at the end of Sect. 3.1.7. If the total charge vanishes, the dipole
moment does not depend upon the choice of the origin of r.
However, in the following it will be advantageous, as in Sect. 2.2.2, to introduce
center-of-charge and relative coordinates. Here we restrict ourselves to Q = 0 and
choose R =

dV r |ρ |/

dV |ρ | as “center of charge”. We derive the potential
energy of the dipole p in the electric ﬁeld E from a series expansion of the potential
around the center of the dipole:
	(R + r) = 	(R) −r · E(R) + · · ·
because ∇	 = −E .
For Q = 0 and with Epot =

dVρ(r) 	(r), this supplies the potential energy
Epot = −p · E .
Here the ﬁeld strength is to be taken at the position of the dipole. For a homogeneous
ﬁeld, it does not depend on the position. Then there is no force F = −∇Epot acting on
the dipole—the forces on the different charges cancel each other in the homogeneous
ﬁeld.
In an inhomogeneous ﬁeld, the forces acting on the two poles have different
strengths. Then there remains an excess ﬁeld
F = −∇Epot = ∇(p · E) = (p · ∇)E
acting on the dipole—its “center-of-charge coordinate”. For the last equation, we
have used ∇× E = 0 and constant p.
In addition, there is a torque
N = p × E ,
if p and E do not have the same direction—then the potential energy is minimal
(stable equilibrium)—or opposite directions (unstable equilibrium). (Note that the
letter M common in classical mechanics is reserved for the magnetization in elec-
tromagnetism, and r × ρE = ρ r × E.) The expression p × E supplies only the part
expressible in relative coordinates. In addition, there is a part connected to r, the
“center-of-charge coordinate” (the position of the dipole, so far called R), namely
r × (p · ∇)E. Because p = (p · ∇) r, the sum can also be combined to

172
3
Electromagnetism
N = (p · ∇) (r × E) .
However, in many cases, only the torque p × E with respect to the center of the
dipole is of interest.
What ﬁeld is generated by a dipole? To answer this question, we consider to
begin with the potential of two point charges ±q′ at the positions r ′
± = r′ ± 1
2a and
investigate the limit a ≪|r −r ′
±|. Since
|r −r′ −1
2a |−1 −|r −r′ + 1
2a |−1 ≈−a · ∇|r −r′|−1 ,
we end up with
4πε0 	(r) = −p′ · ∇
1
|r −r′| = p′ · (r −r′)
|r −r′|3
= −∇·
p′
|r −r′| .
Thus the scalar product of p′ with the unit vector e ≡(r −r′)/|r −r′| from the
source r′ to the point r is important. The potential decays in inverse proportion to the
square of the distance. The ﬁeld strength E = −∇	 decays more strongly by one
power of the distance:
4πε0 E(r) = ∇

p′·∇
1
|r −r′|

= 3 p′·e e −p′
|r −r′|3
−4π
3 p′ δ(r −r′) .
An example is shown in Fig. 3.4. The last term appears because |r −r′|−1 is discon-
tinuous at r = r′. Thus, the volume integral around this point must still be considered
(see Problem 3.8). For a point charge, a delta function appears at ∇· E, thus ulti-
mately with the derivative of the ﬁeld strength—for the dipole this derivative is
already included by taking the limit a →0. We usually only require the ﬁeld outside
the source, so this addition is not needed, but it does contribute to the average ﬁeld,
in particular, for N dipoles p′ in a volume V with 
E = −1
3ε0−1 Np′/V . We will
take advantage of this in the next section, in the context of polarization.
But ﬁrst we consider also dipole moments, which will be distributed evenly over
a sheet d f and lead to a dipole density PA. We then set d f PA = df PA. Note that
Fig. 3.4 Field lines of a dipole pointing along the dashed symmetry axis. Right: The ﬁeld in the
middle is magniﬁed eight times. All other ﬁeld lines are similar to the ones shown here, because
point-like sources do not provide a length scale (see Problem 3.14)

3.1 Electrostatics
173
PA can also be negative, because we have already selected the direction of df, if
the surface of a ﬁnite body is intended (see p. 9). In particular, df should then point
outwards. We obtain the associated potential
	(r) =
1
4πε0
 df′ · (r −r′)
|r −r′|3
PA(r′) .
The fraction in the integrand gives the solid angle d′ subtended by the surface
element df′ at the point r, where the sign changes on crossing the surface. Therefore,
upon crossing the dipole layer, the potential jumps by PA/ε0:
	+ −	−= PA
ε0
,
while, according to Sect. 3.1.2, upon penetrating a monopole layer, it is the ﬁeld E
that jumps, and hence the ﬁrst derivative of 	:
n · (E+ −E−) = −n · (∇	+ −∇	−) = ρA
ε0
.
We may therefore replace the boundary values on p. 170 by suitable mono and dipole
densities on the surface of the considered volume. Then we have
df · ∇	 = −d f n · E = d f ρA
ε0
and
−df 	 = df PA
ε0
,
if the potential (	+) and the ﬁeld (E+) vanish outside (see Fig. 3.5).
Fig. 3.5 Potential (upper) and ﬁeld strength (lower) along the axes of circular disks of radius R.
The disk is loaded with monopole charge density ρA (left) or with dipole charge density PA (right),
where the potential discontinuity at the disk also leads to a delta function. The curve lower right
diverges at z = 0 (see Problems 3.18–3.19)

174
3
Electromagnetism
3.1.5
Polarization and Displacement Field
So far it has been presumed that the charge distribution is also known in the atomic
interior. But in most cases such microscopic quantities are irrelevant. In macroscopic
physics, knowledge of the average charge density is sufﬁcient, if in addition the
average density P of the dipole moments is also used. The average is taken over many
atoms, as long as the volume 
V of the averaging process is sufﬁciently small. For
N molecules (ions) in 
V with charges qi and dipole moments pi, we have
ρ ≡
1

V
N
	
i=1
qi
and
P ≡
1

V
N
	
i=1
pi .
P is called the (electric) polarization. According to the last section, it is associated
with the potential
	(r) = −
1
4πε0
∇·

dV ′
P(r′)
|r −r′| =
1
4πε0

dV ′ P(r′) · ∇′
1
|r −r′| .
The last expression can be rewritten. Since
P(r′) · ∇′
1
|r −r′| = ∇′ · P(r′)
|r −r′| −∇′ · P
|r −r′| ,
Gauss’s theorem yields
	(r) =
1
4πε0

df′ · P(r′)
|r −r′| −
1
4πε0

dV ′ ∇′ · P
|r −r′| .
A polarized medium then has the same ﬁeld as the one due to the surface charge
density ρA′ = n · P and the space charge density ρ′ = −∇· P. The minus sign is
easy to understand. If we assume a rod of homogeneous polarization, then a positive
charge results just there on its surface where the polarization has a sink. For ρ′,
we sometimes speak of apparent charges, because they actually belong to dipole
moments and are not freely mobile. This concept is somewhat misleading, however,
since the apparent charges do exist microscopically.
If we integrate over the total space, then the surface integral does not contribute,
since there is no matter at inﬁnity. Clearly, we may replace the microscopic (“true”)
charge density ρ by the average ρ and the charge density ρ′ = −∇· P of the polar-
ization:
ρ = ρ −∇· P .
From the basic equation ρ = ∇· ε0E of microscopic electrostatics, we can in this
way infer ρ = ∇· (ε0E + P). Therefore, we introduce the electric displacement
ﬁeld (displacement) D, deﬁned by

3.1 Electrostatics
175
D ≡ε0E + P ,
and obtain as basic equations of macroscopic electrostatics
∇· D = ρ ,
∇× E = 0 .
The electric ﬁeld remains irrotational because, according to our derivation, we may
later calculate with a scalar potential. These basic equations also yield
n · (D+ −D−) = ρ A ,
n × (E+ −E−) = 0 ,
and

(V )
df · D = Q ,

(A)
dr · E = 0 .
Like the polarization P, the electric displacement ﬁeld D has the unit C/m2.
So far we have viewed the dipole moments as being given. There are indeed
molecules with permanent electric dipole moments; by allusion to paramagnetism,
they are said to be paraelectric. But if no external ﬁeld is applied and if the temper-
ature is high, then the polarization averages to zero because of the disorder in the
directions. The orientation increases with increasing ﬁeld strength—and decreas-
ing temperature. In addition, an electric ﬁeld also shifts the charges in originally
non-polar atoms and induces an electric dipole moment. In both cases, to a ﬁrst
approximation, P depends linearly upon E:
P = χe ε0E .
The electric susceptibility χe is a mere number. It is related to the polarizability α of
the various molecules. To this end, we assume N equal molecules in the volume V ,
whence P = np with n ≡N/V . Each individual molecule becomes polarized by the
electric ﬁeld E0 at its location, p = αε0E0. In doing this, according to the last section,
we assume that E0 differs from the average ﬁeld strength E by 1
3ε0−1np = 1
3ε0−1P:
P = nα (ε0E + 1
3P)
=⇒
P =
nα
1 −nα/3 ε0E .
We deduce the formula due to Clausius and Mosotti, viz.,
χe =
nα
1 −nα/3
⇐⇒
α = 3
n
χe
χe + 3 ,
which has been derived here following [1].
However, in crystals there may be preferential directions, such that P is not then
parallel to E. In this case, χe is a symmetric tensor of second rank (an anti-symmetric

176
3
Electromagnetism
part would supply an additional term Pa = χe × ε0E with a suitable vector χe, which
contradicts the above-mentioned explanation for the polarization: even if there were
microscopic screw axes, the polarization would nevertheless average out) with three
principal dielectric axes, along which P is parallel to E, but P/E is still different.
There are also ferroelectric materials—in these a permanent polarization appears
even when the ﬁeld is switched off, and the dipole moments do not average out.
In addition, χe does not remain constant at high ﬁelds because there are non-linear
saturation effects. We will not go into all these special cases.
For the electric displacement ﬁeld, it thus follows that
D = (1 + χe) ε0E ≡εE ,
with the permittivity (dielectric constant) ε and the relative dielectric constant εr ≡
ε/ε0 = 1 + χe. This depends upon the temperature and for water is unusually high,
namely equal to 80 at 20 ◦C and 55 at 100 ◦C. In crystals, ε is generally a (symmetric)
tensor of second rank.
We will now always consider the two ﬁelds simultaneously, i.e., the electric ﬁeld
strength determined by the force F = q E acting on a test charge q, and the electric
ﬁeld D given by the average charge density. When we do this, the relation between D
and E is important (D = εE), and we will restrict ourselves to scalar permittivities.
3.1.6
Field Equations in Electrostatics
In the following we will restrict ourselves to macroscopically measurable quantities
and, following the usual practice, omit the bar over the charge density. Thus we start
from the basic equations
∇· D = ρ ,
∇× E = 0 ,
and
D = εE ,
and consider now different cases.
Insulators do not contain mobile charges, and therefore in their interior we have
∇· D = 0 ,
∇× E = 0 .
Since they can be polarized, we have to distinguish carefully between D and E.
According to the second equation, we may replace the ﬁeld strength E by −∇	 and,
using D = εE from the ﬁrst equation, we obtain
∇· ε∇	 = 0 .
In particular, for constant permittivity, we obtain the Laplace equation

3.1 Electrostatics
177
Fig. 3.6 Diffraction of the electric ﬁeld entering into an insulator of higher permittivity (here
ε+ = 2ε−). The force lines become diffracted away from the normal. In contrast, according to the
optical diffraction law (see p. 220) the rays become diffracted towards the normal for n+ > n−and,
instead of tan α, we have in that case sin α

	 = 0 .
The boundary values then become physically decisive.
For two-dimensional problems, analytical functions in the complex plane are
useful. A function f (z) = 	(x, y) + i(x, y) is only differentiable if, regardless of
the direction of approach,
∂f
∂x = ∂f
i∂y
=⇒
∂	
∂x + i∂
∂x = ∂	
i∂y + i∂
i∂y ,
i.e., if the Cauchy–Riemann equations are satisﬁed:
∂	
∂x = ∂
∂y
and
∂	
∂y = −∂
∂x .
These lead to the Laplace equations 
	 = 0 and 
 = 0, and thus to ∇	 · ∇ =
0. If the entity {	 = const.} represents equipotential lines, then the other entity
{ = const.} represents ﬁeld lines. For example, Fig. 1.4 corresponds to f = z2.
At the interface between insulators, the normal components of D and the tangen-
tial components of E are continuous because
n · (D+ −D−) = 0 ,
n × (E+ −E−) = 0 .
Hence, for scalar permittivity, it follows from |n × E+|/|n · D+| = |n × E−|/|n ·
D−| that sin α+/(ε+ cos α+) = sin α−/(ε−cos α−), where α is the angle between
the ﬁeld vector and the normal vector. Hence (see Fig. 3.6),
tan α+
tan α−
= ε+
ε−
.

178
3
Electromagnetism
In homogeneous conductors, charges can move freely. Therefore, for static equi-
librium the ﬁeld strength in the interior of the conductor must vanish, and with it also
the polarization:
E = D = 0 ,
and thus 	 = const.
in the interior of homogeneous conductors.
At the interface between insulator and conductor, there may be surface charges,
but no ﬁelds within the conductor. Therefore, the electric ﬁeld lines in the insulator
end perpendicularly at the interface:
n · DI = ρA ,
n × EI = 0 .
The subscript on EI reminds us that we are considering the insulator, but it is not
actually needed, since the ﬁelds vanish within the conductor.
At the interface between two conductors, the potential has a discontinuity, since
their conduction electrons generally have different work functions. Upon contact
between the two metals, charges move into the more strongly binding regime, until
a corresponding counter-ﬁeld has built up. Only then is there a static situation. Thus
we ﬁnd a contact voltage. The situation for the immersion of a metal in an electrolyte
is similar, e.g., immersion of a copper rod in sulfuric acid, where some Cu++ ions
become dissolved and hence a current ﬂows until the negative loading of the rod has
built up an electric counter-ﬁeld.
All these ﬁelds caused by inhomogeneities are said to be induced, since they
do not originate from an external charge, but from the structure of the material. We
denote the induced ﬁeld strength (as do Panofsky & Phillips) by E′; another common
notation is E (e). In electrostatics, we have
E + E′ = 0
in inhomogeneous conductors—in static equilibrium the induced ﬁeld strength is
canceled by the counter-ﬁeld.
3.1.7
Problems in Electrostatics
In most cases, the ﬁeld E(r) in the insulator is to be determined for a given form and
position of the conductors and with a further requirement: we are given either the
voltage
U = 	1 −	0 = −
 r1
r0
dr · E

3.1 Electrostatics
179
between the conductors 0 and 1—where arbitrary initial and ﬁnal points on the
conductors may be taken (because the potential on each conductor is constant) and
any path in-between, because the ﬁeld is irrotational—or the charges
Qi =

Ai
d f ρA =

Ai
df · εE = −

Ai
df · ε∇	
on the conductor surfaces Ai. For two conductors with charges Q > 0 and −Q and
the voltage U > 0, Q and U are related to each other via a geometrical quantity,
namely the capacity
C ≡Q
U =





df · εE

dr · E




 .
The best approach here is to solve the problem using Gauss’s theorem or using the
Laplace equation, and to adapt the coordinates to the boundary geometry. In the
following, we consider some examples whose solution can be easily anticipated.
The spherical capacitor is a conducting sphere with charge Q and radius rK in a
comparably large (non-conducting) dielectric with scalar permittivity ε. This has a
spherically symmetric ﬁeld, which jumps from 0 to its maximum value at the charged
surface—viewed from outside the sphere, it could also originate from a point charge
at the center of the sphere:
	(r) =
U ,
U rK/r ,
E =
0
for r < rK ,
U rK r/r3
for r > rK ,
withU = Q/C and Q =

df · D = 4πrK2 εE(rK+) = 4πε U rK,whencethecapac-
ity is C = 4πε rK. (The potential has a kink at the charged surface.)
As a cylindrical capacitor, we take two coaxial conducting cylinders of length l,
separated by a dielectric with scalar permittivity ε. If the inner cylinder (with radius
Ri) carries the charge Q and the outer cylinder (with radius Ra) the charge −Q,
then for l ≫Ra, the contribution of the cylinder ends may be neglected. Then in the
dielectric there is a ﬁeld strength decaying as R−1 which is the solution of Gauss’s
theorem Q =

df · εE, since the area of the inner cylinder walls is A = 2π Ri l, and
Q = 2π R l εE(R) and 	 ∝ln(R/Ra) in the capacitor:
	(R) =
⎧
⎪⎪⎨
⎪⎪⎩
U ,
U ln(R/Ra)
ln(Ri/Ra) ,
0 ,
E =
⎧
⎪⎪⎨
⎪⎪⎩
0
for
R < Ri,
U
ln(Ra/Ri)
R
R2
for Ri < R < Ra,
0
for Ra < R,
notingthat−ln(Ri/Ra) = ln(Ra/Ri).Henceweﬁnd Q = 2π Ri l ε U/{Ri ln(Ra/Ri)}
and then C = 2πε l/ ln(Ra/Ri). For conductors (with the very small distance d ≡
Ra −Ri ≪Ri and area A = 2π Ril for the inner conductor), and since ln(Ra/Ri) =
ln(1 + d/Ri) ≈d/Ri, we may replace this by

180
3
Electromagnetism
C ≈ε A
d ,
E ≈|U|
d
≈|Q|
ε A .
These equations are also valid for the plate capacitor, if boundary effects may be
neglected.
When capacitors with capacities Ck are connected in parallel, the total capacity
C = Q/U =  Ck, because U = Uk and Q = 
k Qk. For capacitors connected
in series, we have 1/C = 
k 1/Ck, because now Q = Qk and U = 
k Uk =

k Q/Ck.
For a point charge q at a distance a in front of a conducting plane the ﬁeld lines
must end perpendicularly on the plane and must be irrotational in front of it. In order
to ﬁnd the ﬁeld distribution, we imagine an image charge −q at the same distance a
behind the conductor surface—the ﬁeld of the two point charges is shown on the left
in Fig. 3.3. The total ﬁeld of the two point charges satisﬁes the conditions in front of
the plane. Hence, if we choose the center of this conﬁguration as the origin, so that
q is at a and −q at −a, we have been
E =
q
4πε
 r −a
|r −a |3 −
r + a
|r + a |3

,
for r · a > 0 ,
otherwise 0 .
This ﬁeld is irrotational and has a source in front of the interface only at the position
of the point charge. On the interface, r · a = 0 holds, so |r ± a |3 = (r2 + a2)3/2,
and hence,
E = −q
2πε
a
(r2 + a2)3/2 .
Therefore, E is perpendicular to the plane as required. Behind the mirror there is
no ﬁeld. Therefore, we replace the imagined image charge now by a surface charge
ρA = n · D on the plane, precisely in the sense of the last paragraph of Sect. 3.1.3.
The image charge is replaced by an induced charge on the conductor surface. The
total induced charge is, according to the last two equations, equal to the image charge:

d f ρA =

df · εE = −qa
2π
 ∞
0
2π R dR
(R2 + a2)3/2 =
qa
√
R2 + a2




∞
0
= −q .
Of course, the total charge of the conductor must be conserved. We have to imagine
a charge +q at inﬁnity. For conductors of ﬁnite extension, it is important to know
whether they are isolated or grounded—if necessary the image charge has to be
neutralized by a further charge, e.g., for an ungrounded sphere, the additional charge
has to be spread evenly over the surface.
With the help of image charges, the ﬁelds of other charged interfaces can be rep-
resented, e.g., for a conducting sphere (Problem 3.20) or for a separating plane to a
non-conductor with a different permittivity. But then we have to calculate with dif-
ferent charges q ̸= q′—the ﬁeld-line pictures in the half-space inside the conductor

3.1 Electrostatics
181
are similar to those on the right in Figs. 3.2 or 3.3—and in the half-space of the other
non-conductor, the ﬁeld of a new source appears at the original position.
Each test charge leads to induced charges on the surrounding conductors and thus
changes the ﬁeld to be determined. Since this induction should remain negligible,
the test charge must therefore be very small in comparison with the other charges.
However, this is not possible for small distances because the induced charge is then
very highly concentrated. Therefore, we may apply our concepts only to macroscopic
objects.
If the microscopic charge density ρ(r) is given, the potential and ﬁeld strength
follow from the Poisson equation 
	 = −ρ/ε0 or the integral

dV ′ ρ(r′)/|r −r′|.
Here, we would like to separate the variables r and r′. This is managed by expanding
in terms of Legendre polynomials (see p. 81):
1
|r −r′| = 1
r
∞
	
n=0
Pn(cos θ)
r′
r
n
,
for r′ < r and cos θ ≡r · r′
r r′ .
According to this, and in particular, for positions outside the ﬁeld-creating charges,
we may set
	(r) =
1
4πε0
∞
	
n=0
1
rn+1

dV ′ ρ(r′)r′ n Pn(cos θ′) .
Upon integration, the angle between r and r′ changes, so we have written here cos θ′.
For n = 0, the integral supplies the charge Q′ because P0 = 1. (In Sect. 2.2.7 we
integrated over the mass density and hence obtained the mass.) The next integral leads
to p′ · r/r, because r′ P1(cos θ′) = r′ · r/r, and the dipole moment is thus important.
Generally, the integrals appearing here are called multipole moments. (According to
Sect. 2.2.7, we have (n + 1) Pn+1(z) −(2n + 1) z Pn(z) + n Pn−1(z) = 0. However,
numerical factors are often added to the multipole moments.) For a dipole of ﬁnite
extension (a ̸= 0), there is, e.g., an additional octupole moment, but its inﬂuence
decreases faster with the distance than that of the dipole moment (Problem 3.15).
Apart from the spherical multipoles of order 2n just mentioned, there are (e.g.,
in ion optics) axial multipoles of order 2n. In suitable cylindrical coordinates, their
potentials are proportional to Rn cos(nϕ).
3.1.8
Energy of the Electrostatic Field
The electric ﬁeld carries energy, because according to p. 169, work is required to
load a capacitor, i.e., the work dW = U dQ to move the charge dQ > 0 from the
cathode to the anode. Because Q = C U, if we let the voltage—or charge—increase
from zero to its ﬁnal value, we obtain

182
3
Electromagnetism
W = 1
2 C U 2 = Q2
2 C
for the energy stored in the capacitor.
Since ρ = ∇· D and therefore ρ 	 = ∇· (	D) −D · ∇	, the expression W =
1
2 Q U = 1
2 Q 
	 = 1
2

dV ρ 	 can be rewritten. According to Gauss, the ﬁrst term
supplies a surface integral which vanishes at inﬁnity, since 	D approaches zero as
r−3, and we obtain
W = 1
2

dV ρ 	 = 1
2

dV D · E .
The contribution to the last integral comes from all space containing ﬁelds, but the
contribution to

dV ρ 	, only from space containing charge. However, the energy
density is only
w = 1
2 D · E ,
because, subdividing the space, the interfaces should not contribute—and ρ 	
depends upon the gauge. In the sense of thermodynamics (see p. 575), we are dealing
with the density of the free (fully usable) energy F. Temperature and volume here
are the natural variables. The permittivity ε generally depends upon the temperature
and the distances between the molecules. However, we follow the general custom
and write w and not f . (The symbol u is often also used, but this is misleading, since
U means the inner energy and not the free—fully exploitable—energy.)
Since D = ε0E + P, the energy density is composed of two parts. Firstly, the
ﬁeld energy 1
2ε0E · E “in vacuum”, and secondly, the contribution 1
2P · E from the
dielectric—because according to p. 175 the dipole moment of polarizable molecules
increases linearly with the applied ﬁeld strength and requires the work
 p
0 E · dp =
1
2p · E. (This derivation succeeds only for P ∝E.)
3.1.9
Maxwell Stress Tensor in Electrostatics
Forces are transmitted from one space element to the next, in which case we can speak
of near-action forces. Here this must also be true for empty space, because electric
forces permeate even empty space. We expect space ﬁlled with ﬁelds to behave as
an elastic medium, and this property is described by the Maxwell stress tensor.
In a continuous medium, the force F can be derived from a force density f, which
will be denoted in this section by f. For surface elements, we write n d f or g k d fk.
Then,
F =

dV f(r) .

3.1 Electrostatics
183
Fig. 3.7 Visualization of the Maxwell stress tensor for a homogeneous electric ﬁeld (the ﬁeld
strength points from the ⊕to the ⊖charges). Shown are the surface tensions on four cubes—
depending on their charge, the forces on opposite faces either cancel or supply the expected force
(see Problems 3.21–3.22)
We decompose the force f dV acting on an inﬁnitesimal cube dV = dx dy dz into
surface element times surface tension σ. For tensile and compressive forces, there
are normal stresses perpendicular to the surface, while for shear forces, there are
shear stresses on the surface (see Fig. 3.7). The mechanical stress (Latin tensio) is
described by a tensor σ, viz.,
dV fx =
⎛
⎝
dy dz {σxx(x + dx, y, z) −σxx(x, y, z)}
+ dz dx {σxy(x, y + dy, z) −σxy(x, y, z)}
+ dx dy {σxz(x, y, z + dz) −σxz(x, y, z)}
⎞
⎠
= dV
∂σxx
∂x
+ ∂σxy
∂y
+ ∂σxz
∂z

.
The force density f is thus equal to the source density of the stress tensor σ—so far
we have considered only divergences of vectors and have obtained scalars. According
to Gauss’s theorem, the volume integral of f can be converted into a surface integral.
Adjacent interfaces do not contribute, provided that σ is continuous.
The stress tensor is useful in continuum mechanics, where near-action forces are
assumed. Therefore, we use it now in electromagnetism. However, here we shall
restrict ourselves to homogeneous matter with constant permittivity, since otherwise
the problem is much more involved (see [2]). We thus start from
F =

dV ρ E =

dV E ∇· D .
In order to convert the integrand into the source density of a tensor, we use Sect. 1.1.8,
adding −D × (∇× E) to the integrand. It does not contribute in electrostatics, since
the ﬁeld is irrotational—and the same procedure holds in magnetostatics, but where
the ﬁeld is solenoidal (source-free) and curls (vortices) appear instead.
For rectilinear (possibly oblique) coordinates, according to pp. 33–40 with ﬁxed
vectors g i, we ﬁnd

184
3
Electromagnetism
E ∇· D −D × (∇× E) =
	
ik
g i ∂Ei Dk
∂xk
−Dk ∂Ek
∂xi

.
using a × b = 
ikl g iakblεikl and the two equations ∇· a = 
k
∂ak
∂xk and (∇×
a)i = 
kl εikl
∂al
∂xk , as well as the identity 
l εiklεlmn = δm
i δn
k −δm
k δn
i . In addition,
for homogeneous matter, i.e., an invariant permittivity tensor, we have
	
k
Dk ∂Ek
∂xi =
	
kl
εkl El
∂Ek
∂xi =
	
l
El
∂Dl
∂xi =
	
k
∂1
2 Ek Dk
∂xi
= ∂w
∂xi ,
and thus 
i g i ∂w
∂xi = 
k g k ∂w
∂xk = 
ik gi gik ∂w
∂xk . Together these imply
E ∇· D −D × (∇× E) =
	
ik
gi
∂
∂xk

Ei Dk −1
2gik E · D

.
Therefore, we introduce the Maxwell stress tensor (in rectilinear coordinates):
T ik ≡w gik −Ei Dk .
(In Cartesian coordinates, it has the trace trT = 3w −E · D = w ≥0, and the trace
does not depend upon the choice of coordinates. Some authors use it with opposite
sign,butweshallseeinSect.3.4.11thatthishasdisadvantages.Intheformmentioned
here, it is symmetric only for scalar permittivity. See also other forms discussed by
Brevik.) With this, we ﬁnd
f +
	
ik
gi
∂T ik
∂xk = 0 .
Consequently, the force density vector f is related to the divergence of the tensor T .
Since we work with position-independent basic vectors gi and may employ
Gauss’s theorem (dV = d fk dxk according to p. 38), we also ﬁnd
F +
	
ik
gi

(V )
d fk T ik = 0 .
We can thus form Maxwell’s stress tensor T ik from the ﬁeld strength, which expresses
the force on a volume due to surface forces. Its diagonal elements supply the com-
pressive or tensile stress on the surface pair with equal index, and its off-diagonal
elements supply the shear stress on the remaining surface pairs.

3.1 Electrostatics
185
3.1.10
Summary: Electrostatics
In electrostatics, we investigate the effects of charges Q and charge densities ρ at rest.
All phenomena can be derived from Coulomb’s law. It supplies the force between
two point charges q and q′ in vacuum:
F = qq′
4πε0
r −r′
|r −r′|3 .
From this action-at-a-distance law, we derived a ﬁeld theory. We conceived of a test
charge q and introduced a ﬁeld strength E:
F = q(r) E(r) ,
with ∇× E = 0 and ∇· E = ρ
ε0
.
However, the last equation is true only with the microscopic measurable charge
density ρ, not with the macroscopic charge density ρ, which accounts for freely
moving charges.
We get round this difﬁculty by introducing dipole moments p and their density.
This leads to the macroscopic concept of polarization P. Its action on a test charge
can be described by a charge density −∇· P. With the electric displacement ﬁeld
D ≡ε0E + P ,
we thereby obtain the source equation
∇· D = ρ .
If the connection between the ﬁeld strength E and displacement ﬁeld D is known,
the ﬁeld can be determined. Maxwell’s equations of electrostatics read
∇× E = 0 ,
∇· D = ρ ,
and D = ε E .
It is common to denote the measurable charge density by ρ. We will therefore omit
the bar in the following. The ﬁrst row of these equations yields

(A)
dr · E = 0
and

(V )
df · D = Q ,
and also
n × (E+ −E−) = 0 and n · (D+ −D−) = ρA .
Since E is irrotational, this vector ﬁeld can be attributed to a scalar potential 	,
with which calculations are greatly simpliﬁed:

186
3
Electromagnetism
E = −∇	 ,
∇· (ε∇	) = −ρ .
Then for constant and scalar permittivity, the Poisson equation follows:

	 = −ρ
ε .
Here, a boundary condition is appended, namely that the potential should vanish at
inﬁnity—and that there should be no charge there. Instead of that, conditions may
be introduced at the surface of the considered volumes.
In electrostatics, there are no ﬁelds in homogeneous conductors. Only at their
interface with insulators are charges possible, and this supplies the boundary condi-
tions:
ρA = n · DI = −n · εI∇	 ,
0 = n × EI = −n × ∇	 .
Here the index I refers to the adjacent insulator.
3.2
Stationary Currents and Magnetostatics
3.2.1
Electric Current
So far we have restricted ourselves to charges and dipole moments at rest. We shall
now discard this restriction. We let the charges move and use the concept of current
density:
j ≡ρ v .
We call the current ﬂux through the cross-section A of a conductor the current
strength:
I ≡

A
df · j .
For a cross-section that is small compared to the other dimensions of the conductor,
we often replace
j dV →I dr ,
where dr is in the direction of j , since dV j →dr · df j = dr df · j.
There is a conservation law for electric charges: if the charge Q in a time-
independent volume V changes, then it must ﬂow through the surface of V . (We
can also state that the volume associated with Q has changed—but this serves

3.2 Stationary Currents and Magnetostatics
187
no purpose here.) Therefore, dQ/dt ≡

V dV ∂ρ/∂t = −

(V ) df · ρv. Note that the
vector normal to the cross-section points to the outside. If this is true for ρv,
then positive charge ﬂows out, hence the minus sign. With Gauss’s theorem, viz.,

(V ) df · j =

V dV ∇· j, we have the continuity equation
∂ρ
∂t + ∇· j = 0 ,
and for surface charges ρA,
∂ρA
∂t + n · (j+ −j−) = 0 ,
where n is again the unit vector perpendicular to the element of the cross-section,
from front to back (from j−to j+). The continuity equation thus follows from charge
conservation, and conversely, charge conservation from the continuity equation.
In this section, we shall deal with stationary currents—then the charge density
does not change anywhere as time goes by (∂ρ/∂t = 0), and the current density is
solenoidal (∇· j = 0 and n · j = 0 at conductor surfaces). Only in the next section
will we relax this restriction.
3.2.2
Ohm’s Law
Electric currents are generated in conductors by electric ﬁelds. The ﬁelds exert a
force on the charged particles and accelerate them. If we apply a voltage U(> 0) at
the ends of a conductor, then a current of strength I (> 0) will ﬂow. The ratio U/I
is the resistance R of the conductor:
U = R I .
According to Ohm’s law, the resistance depends on the properties of the conductor,
but not on the applied voltage or the current. For a homogeneous conductor of length
l and cross-section A, apart from its dimensions, it thus depends on the conductivity
σ:
R =
l
A σ .
Since U = E l and I = j A, for a homogeneous conductor, we ﬁnd the differential
form of Ohm’s law, viz.,
j = σ E .

188
3
Electromagnetism
In fact, the current density often depends linearly on the ﬁeld strength. (The conduc-
tivity σ in some crystals is a tensor, because there are preferential directions—but
we do not wish to deal with that here.) However, there are also counterexamples, as
is to be expected, if we try to explain Ohm’s law.
Actually, the ﬁeld strength should accelerate the charges, since the ﬁeld supplies
a force, while the current density is proportional only to the velocity of the charged
particles. This apparent contradiction in Ohm’s law is resolved as for free fall by
invoking friction (see p. 85, and in particular Fig. 2.11). In a metallic conductor,
the electrons always lose the energy they acquire by collisions with the lattice, and
hence move with a constant drift velocity. The associated power appears as Joule
heat,
F · v =

dV ρE · v =

dV j · E = I

dr · E = I U ,
which heats the conductor. Furthermore, the conductivity often depends on the tem-
perature, which limits Ohm’s law.
Ohm’s law cannot be applied as such to superconductors, which conduct currents
loss-free, and then only at the surface of the conductor or in special tubes (supercon-
ductor of ﬁrst or second kind, respectively).
In the given differential form, Ohm’s law also holds only for homogeneous con-
ductors (and insulators, which have σ = 0). For inhomogeneous conductors, we must
also consider the induced ﬁeld strength:
j = σ (E + E′) = σE + j′ ,
where the conductivity σ now also depends on position. The term j′ refers to the
additional current density at the sources.
Electric currents are immersed in magnetic ﬁelds, which in turn act on the
currents—we shall now consider this. If we neglect this back-action, then stationary
currents can be calculated easily, because for ∇· j = 0, ∇× E = 0, and j = σE + j′,
we have
∇· σE = −∇· j′
and
∇× E = 0 ,
n · (σ+E+ −σ−E−) = −n · j ′
A
and
n × (E+ −E−) = 0 ,
where the potential may be introduced everywhere instead of the ﬁeld strength (E =
−∇	). The current density j′ is thereby to be viewed as given. Hence for stationary
currents, we have the same mathematical problem as for E or 	 in electrostatics, but
with the conductivity σ instead of the permittivity ε and with −∇· j′ instead of the
charge density ρ. If we can determine the capacity between two electrodes with a
given form, then according to Ohm, the resistance between the same electrodes for
a conductor satisﬁes

3.2 Stationary Currents and Magnetostatics
189
Fig. 3.8 For the proof Kirchhoff’s laws. Left: Parallel connection. Right: Series connection
R C = ε
σ ,
because from I =

A df · σE = (σ/ε)

A df · D with Q = CU and U = RI. In par-
ticular, Kirchhoff’s laws are obtained. The total resistance R = U/I of the various
individual resistors Rn depends on the type of connection:
parallel connection (with I = 
n In)
1
R = 
n
1
Rn
,
series connection (with U = 
n Un)
R = 
n Rn .
This is illustrated in Fig. 3.8.
3.2.3
Lorentz Force
Moving charges (currents) are deﬂected by magnetic ﬁelds. There is a force acting
on a point charge q moving with velocity v in a magnetic ﬁeld B, namely, the Lorentz
force
F = q v × B
⇐⇒
F =

dV j × B .
Note that, since F and v are polar vectors, B must be an axial vector. This velocity-
dependent force was already mentioned on p. 78 and was generalized to the concept
of potential energy (p. 98) and momentum (p. 100). Here then the acceleration is
perpendicular to the velocity and the kinetic energy is therefore conserved. If we
write ˙v = ω × v, we have ω = −(q/m) B for the cyclotron frequency. For ﬁxed B,
we ﬁnd a helical orbit with the Darboux vector ω/v, and in particular with v ⊥B, a
circular orbit of radius R = v/ω, because only then do the Lorentz force mωv and
the centrifugal force mv2/R cancel each other.
However, there is no force acting on stationary currents in the homogeneous
magnetic ﬁeld, because for ∇· j = 0, according to p. 17 or Problem 3.4, we also

190
3
Electromagnetism
have

dV j = 0. Nevertheless we can measure this magnetic ﬁeld, if we use the
torque on a current loop. This will now be shown for very small conductor loops,
since then a homogeneous magnetic ﬁeld may be assumed.
For the torque, we require the volume integral of r × (j × B) = B · r j −r · j B.
Here we consider a little box around the conductor loop without current at its surface.
Then, since 2 r · j = j · ∇r2 = ∇· (r2j) −r2 ∇· j for a solenoidal current density,
the volume integral of r · j vanishes according to Gauss’s theorem, and for
2 B · r j = {B · r j + B · j r } + (r × j) × B ,
the volume integral of the curly bracket vanishes as well, because we have
ri jk + jirk = j · ∇(rkri) = ∇· (rkrij) −rkri ∇· j ,
and therefore the same procedure as above is applicable, with rkri instead of r2.
Therefore, the homogeneous magnetic ﬁeld B exerts a torque
N = 1
2

dV (r × j) × B
on the conductor loop. Hence the magnetic ﬁeld can be determined and the concept
of magnetic moment introduced.
3.2.4
Magnetic Moments
The last equation suggests introducing the magnetic moment of the conductor loop
(or more precisely, its dipole moment)
m ≡
1
2

dV r × j .
This is an axial vector. For a current of strength I around a plane sheet A, it has
magnitude (see Fig. 2.4)
m = 1
2 I




r × dr



 = I A ,
and points in the direction of the normal to the loop, in such a way that the current
direction forms a right-hand screw around this axis.
Such a magnetic moment in a homogeneous magnetic ﬁeld B experiences a torque
N = m × B ,

3.2 Stationary Currents and Magnetostatics
191
as was shown before. (Instead of this, N = μ0m × B/μ0 is often used and μ0m is
called the magnetic moment, whence for m, the factor μ0 is included—but this idea
goes contrary to the IUPAP recommendation.)
If the current originates from a charge Q of mass M (both evenly distributed,
so that ρ/Q = ρM/M) distributed along the closed orbit, the magnetic moment is
related to the orbital angular momentum L by m = 1
2

dV r × ρv = 1
2(Q/M) L.
In atomic physics, the action quantum ℏis taken as a unit for the orbital angular
momentum, along with the charge and mass of an electron (Q = −e and M = me).
Therefore, in that context, the magnetic moment is related to the Bohr magneton (see
p. 623)
μB = eℏ
2me
.
In atomic physics, magnetic moments are usually denoted by μ, but in macroscopic
electromagnetism, this is already reserved for the permeability.
While electric dipole moments can be formed from monopoles, magnetic mono-
poles have not yet been observed. Such a thing would have to be a pseudo-scalar,
because m is an axial vector. Since Dirac, it has not been excluded that there may be
magnetic monopoles in elementary particle physics—it may just be that they have
not yet been separated. In any case, all our macroscopic considerations work without
magnetic monopoles.
3.2.5
Magnetization
As Fig. 3.9 shows, we can replace macroscopic current loops by many microscopic
ones, and these then by magnetic moments, if we deal with the action of a magnetic
ﬁeld. It is therefore useful to introduce the density of magnetic moments on the
surface or in the volume. If there are N magnetic moments mi in the volume 
V ,
then
M =
1

V
N
	
i=1
mi
is the associated magnetization, an axial vector like m or r × v. (Here, many solid-
state physicists include the factor μ0 for the magnetic moment. This goes against the
IUPAP recommendation.) It is sometimes also called the (magnetic) polarization.
As shown in the right-hand part of Fig. 3.9, M has curls, where the current density
does not vanish. In fact, we ﬁnd ∇× M = j, because if d is the distance between
different current-loop planes, then in addition to m = I · A, we also have m = M ·
d · A. The magnetization clearly has a discontinuity of I/d, i.e., by the surface current
density jA, at the current-carrying surface, whence we arrive at ∇× M = j.

192
3
Electromagnetism
Fig. 3.9 Left: A macroscopic current loop (in the y, z-plane) is divided into 4×5 “microscopic”
ones, each of which represents a magnetic moment. Right: A cuboid with three such planes with
magnetic moments (arrows) is cut open (in the x, z-plane) and a current loop is associated with
each one. The intersection points of the currents are indicated by the black dots
In atoms there are magnetic moments but no electric conduction currents, as can
be veriﬁed in a magnetic ﬁeld. Therefore, for the behavior in magnetic ﬁelds, we
have to account for the magnetization in addition to macroscopic electric currents,
and in microscopic electromagnetism, we have to introduce a “microscopic current
density”
j = j + ∇× M .
(For the magnetic moments of elementary particles this is not justiﬁed, however,
because their moment is connected to the spin and cannot be derived from a molecular
current.) Since j differs from j only by a rotational ﬁeld, j is solenoidal like j.
Later we shall stick to macroscopic electromagnetism and always take only the
macroscopic current density j (leaving out the bar), but for the time being, j will be
the microscopic current density.
3.2.6
Magnetic Fields
Even if Sect. 3.2.3 has already shown that a ﬁeld B can be measured (by forces acting
on magnetic moments or moving charges), we still have to deal with its generation:
magnetic ﬁelds occur for magnetic moments as well as for electric currents.
Since there are no magnetic monopoles, the magnetic ﬁeld B is solenoidal. In
addition, we ﬁnd from experiment that each microscopic current density is related
to the circulation density of a magnetic ﬁeld:
∇· B = 0 and ∇× B = μ0 j = μ0 (j + ∇× M) ,

3.2 Stationary Currents and Magnetostatics
193
since we have the Biot–Savart law
B(r) = μ0
4π ∇×

dV ′
j (r′)
|r −r′| = −μ0
4π

dV ′ j (r′) × ∇
1
|r −r′| .
∇acts only on r and not on r′, and hence we have ∇× Gj′ = −j′ × ∇G. For
sufﬁciently thin conductors, it follows that
B(r) = μ0 I ′
4π
∇×

dr′
|r −r′| = −μ0 I ′
4π

dr′ × ∇
1
|r −r′| .
For a given magnetization, ∇× M may of course appear instead of j.
Since we have ∇× B = μ0 j = μ0 (j + ∇× M) in the macroscopic theory, we
set
B = μ0 (H + M) ,
∇× H = j ,
where H is called the excitation or magnetic ﬁeld strength and B is referred to as
the displacement ﬁeld of the magnetic ﬁeld or magnetic induction. Since B is a
measure of the force on moving charges, it should actually be called the magnetic
ﬁeld strength, but if we compare electrostatics and magnetostatics, the choice of
names is understandable, as we shall now show.
In magnetostatics, we deal with magnetized matter without electric currents,
whence j = 0. Because ∇· B = 0 and B = μ0 (H + M), we clearly then have
∇× H = 0 and ∇· H = −∇· M .
This is reminiscent of ∇× E = 0 and ∇· E = −ε0−1∇· P for uncharged polarized
matter in electrostatics. Since the excitation H is irrotational here, we may likewise
introduce a scalar magnetic potential 	m by
H = −∇	m ,
where (see p. 174)
	m(r) = −1
4π

dV ′ ∇′ · M(r′)
|r −r′|
= −1
4π ∇·

dV ′ M(r′)
|r −r′| .
The magnetic potential cannot be connected to a potential energy though (there are
no magnetic monopoles), and it is a pseudo-scalar.
A tiny rod magnet with moment m′ at the position r′ thus produces the magnetic
ﬁeld (see p. 172 and Problem 3.8)

194
3
Electromagnetism
Fig. 3.10 Field lines of a permanent homogeneous magnetized cylinder. Left: H ﬁeld. Right: B
ﬁeld. Except for the edges, the ﬂux through the surface increases by one unit from line to line. The
right-hand ﬁgure applies also to the H and B ﬁeld of a current-carrying coil
H(r) = −∇	m = 1
4π ∇

∇·
m′
|r −r′|

= 1
4π
3 m′· e e −m′
|r −r′|3
−m′
3 δ(r −r′) ,
with e = r −r′
|r −r′| .
This is related to the magnetic induction ﬁeld
B(r) = μ0H + μ0 m′ δ(r −r′) .
Incidentally, this can also be written as μ0/(4π) ∇× (∇× m′/|r −r′|), because
∇× (∇× a) = ∇(∇· a) −
a, and according to p. 26, 
|r −r′|−1 = −4πδ
(r −r′).
For a homogeneous magnetized cylinder (in air) with the curved surface along M,
the magnetization has sources only on the faces and curls only on the curved surface.
They jump there from M to zero. Therefore, on the faces ∇A · M = −n · M, and
on the curved surface ∇A × M = −n × M. The potential 	m of a circular face
can be expressed with the help of a complete elliptic integral of the ﬁrst kind (see
Problem 3.17), and that of a homogeneous circular disk as an integral of it. If we
have calculated 	m on the edge and on the faces, then the remaining values follow
faster numerically via the Laplace equation (see [3]). Outside the cylinder, the two
ﬁelds μ0H and B are equal, because M = 0, while on the axis inside they are directed
oppositely (see Fig. 3.10).

3.2 Stationary Currents and Magnetostatics
195
3.2.7
Basic Equations of Macroscopic Magnetostatics
with Stationary Currents
Once again we allow for electric currents j and consider the basic equations of
macroscopic magnetostatics with stationary currents derived at the beginning of the
last section:
∇· B = 0 ,
∇× H = j ,
and B = μH .
These differential equations supply the boundary conditions
n · (B+ −B−) = 0 and n × (H+ −H−) = jA ,
and read in integral form

(V )
df · B = 0 and

(A)
dr · H = I .
Here jA denotes the macroscopic current density at the surface. It vanishes normally.
It occurs only for superconductors of the ﬁrst kind: then there is no magnetic ﬁeld in
the interior (Meissner–Ochsenfeld effect), and only the surface carries a current. The
last equation is called Ampère’s circuital law, and also in earlier years the Ørsted
law. It relates the magnetic ﬁeld and current strength in a particularly simple way
and also contains the right-hand rule: the magnetic ﬁeld encircles the current I = Aj
anticlockwise.
For example, for a straight normal conductor wire of circular cross-section with
radius R0 and constant current density j = I/(π R02), the magnetic ﬁeld in cylindrical
coordinates R, ϕ, z about the wire axis is given by
H = 1
2π
⎧
⎪⎨
⎪⎩
I × R
R02
for R ≤R0 ,
I × R
R2
for R ≥R0 .
The right-hand rule requires H to be proportional to I × R (up to a positive factor),
and Ampère’s law ﬁxes the absolute value. We have 2π R H(R) equal to I (R/R0)2
for R ≤R0 and equal to I for R ≥R0. Of course, there is no arbitrarily long straight
wire—therefore the realistic magnetic ﬁelds of stationary currents also decay at large
distances more quickly than R−1, in fact, like a dipole ﬁeld as (R2 + z2)−3/2.
In general, an applied magnetic ﬁeld magnetizes a (magnetic) medium because it
polarizes irregularly oriented moments. We therefore write
M = χm H and B = μ0(1 + χm) H ≡μH ,

196
3
Electromagnetism
Fig. 3.11 Magnetic spheres. Upper: Paramagnetic. Lower: Diamagnetic. The sphere is brought
into a homogeneous magnetic ﬁeld. Left: H ﬁeld. Right: B ﬁeld. Both are axially symmetric, and
we ﬁnd ∇· B = 0 and B = μH in addition to ∇× H = 0. The lower ﬁgure is always useful if the
permeability inside is lower than outside, even if there is no diamagnet (in air). The ﬁgures are also
valid for electric ﬁeld lines for different permittivities and for current lines of stationary currents for
different conductivities, as explained in the text: H →E and either μ →ε and B →D or μ →σ
and B →j
with permeability μ (sometimes expressed as relative permeability μr = μ/μ0) and
magneticsusceptibility χm = μr −1.Thesearetensors,iftherearepreferentialdirec-
tions: B and H may have different directions. For a ferromagnet, B and H are not
related to each other linearly. This is often represented in a hysteresis curve M(H). (In
a weak ﬁeld typical values for these are μr ≈500.) For materials with smaller scalar
permeability, we also distinguish between paramagnets with χm > 0 or μr > 1 and
diamagnets with χm < 0 or 0 < μr < 1. The dielectric susceptibility χe is always
positive: paramagnetism can be explained as orientation of dipole moments, dia-
magnetism as a consequence of Lenz’s law, which will be dealt with only in the next
section.
If we consider a magnetic sphere (radius r0) in a homogeneous magnetic ﬁeld
H0 (at long range), in addition to ∇× H = 0 and because ∇· B = 0 = ∇· μH, we
have μin · Hi = μan · Ha. The magnetic ﬁeld is irrotational and only has sources
on the surface. The associated discontinuity is related to the ﬁeld of a dipole m =
(μi −μa)/(μi + 2μa) r03 H0 (except for a factor of 4π), because with
Hi = H0 −m
r03
and Ha = H0 + 3 m · e e −m
r3
,
for e = r
r ,
all the above-mentioned conditions are satisﬁed. This result is illustrated in Fig. 3.11,
where the pictures are also valid for electric ﬁeld lines for different permittivities
(because ∇· D = 0, D = εE, ∇× E = 0) and for current lines of stationary currents
for different conductivities (because ∇· j = 0, j = σE, ∇× E = 0). To this end,
we replace H →E and either μ →ε and B →D or μ →σ and B →j (see also
Problem 3.23).

3.2 Stationary Currents and Magnetostatics
197
3.2.8
Vector Potential
The displacement ﬁeld B is always solenoidal and therefore a rotational ﬁeld:
B = ∇× A .
A is called the vector potential and is a polar vector ﬁeld because the induction is
an axial ﬁeld. Here the induction ﬁeld B can be measured via the Lorentz force or
by its action on magnetic moments, while the vector potential A represents only
a computational tool and is not unique—only its curl is physically ﬁxed, not its
sources (and an additive constant). Therefore, a gradient ﬁeld may also be added:
A′ = A −∇ would supply the same magnetic ﬁeld as A. The vector potential must
therefore be gauged, and in this case ∇· A is ﬁxed along with a constant additive
term (in most cases we require it to vanish for r →∞). For the Coulomb gauge, the
vector potential is chosen solenoidal.
The equation ∇× B = μ0j does not depend on the gauge, but

A = −μ0 j
does, since we only have 
A = −∇× B for a solenoidal vector potential given that

A = ∇(∇· A) −∇× (∇× A). On the other hand, 
A = −μ0 j (if A →0 for
r →∞holds), and according to p. 27,
A(r) = μ0
4π

dV ′
j (r′)
|r −r′| = μ0 I ′
4π

dr′
|r −r′| ,
which yields the Biot–Savart law (see p. 193). For stationary currents, this vector
potential is solenoidal, because for its source density we require j′ · ∇G, which can be
rephrased as G ∇′ · j′ −∇′ · Gj′ since ∇G = −∇′G. According to Gauss’s theorem,
we then only require a surface where there is no current to prove the statement.
Herej isstillthemicroscopiccurrentdensity,andcanalsoappearasthecirculation
density of a magnetization. In this case, we have G j′ = G ∇′ × M′ = ∇′ × GM′ +
M′ × ∇′G. Therefore, the vector potential of a magnetic moment m′ results in
A(r) = −μ0
4π m′ × ∇
1
|r −r′| = μ0
4π ∇×
m′
|r −r′| ,
because the surface integral of GM′ does not contribute.
For a homogeneous displacement ﬁeld B, we may set A(r) = 1
2B × r, because
then ∇× A = B, and the Coulomb gauge holds everywhere. Then the origin of r
may be chosen arbitrarily—a constant is irrelevant. For other ﬁelds it is ﬁxed by the
condition A = 0 for r →∞, which is not suitable for a homogeneous ﬁeld.
The integral mentioned at the beginning does not need to be taken over the whole
space, if we also take into account surface integrals (as for the scalar potential on

198
3
Electromagnetism
p. 169). We use the equation 
A = −μ0j and Green’s second theorem, i.e., in

V dV (ψ
φ −φ
ψ) =

(V ) df · (ψ∇φ −φ∇ψ), we replace the function ψ by
|r −r′|−1 and the function φ by the three components of the vector potential. It then
follows that
4π A(r) =

V ′ dV ′ μ0 j (r′)
|r −r′|
+

(V ′)
df′ · ∇′ A(r′)
|r −r′|
−

(V ′)

df′ · ∇′
1
|r −r′|

A(r′) .
In particular, we may choose V ′ such that there is no current: then the vector potential
is ﬁxed by its values and its ﬁrst derivatives on the surface (V ′). As in electrostatics
(see the end of Sect. 3.1.3), then also in magnetostatics in a ﬁnite region, the same
physical ﬁeld can be generated in various ways (by distributions in space or on
sheets). The continuation across the boundaries is not unique and allows various
models. This has also been clearly demonstrated in the context of Fig. 3.9.
3.2.9
Magnetic Interaction
An inhomogeneous magnetic ﬁeld exerts a force on a magnetic moment. If we use
the equation

(A) dr × B =

A(df × ∇) × B of p. 17 and m = I A of p. 190, then for
a sufﬁciently small conductor loop, it follows that
F = I

dr × B = (m × ∇) × B .
As for the electric dipole moment, we require likewise small extensions for the
magnetic moment in order for the higher moments to become negligible. Here we
may also write ∇(m · B) −m ∇· B, since the differential operator changes only B.
Given that B is always solenoidal, we ﬁnd
F = ∇(m · B) .
Therefore, we may also introduce a potential energy
Epot = −m · B ,
and again, F = −∇Epot holds. This corresponds to the expression Epot = −p · E
in electrostatics (see p. 171). There, because of ∇× E = 0, we could also write
(p · ∇)E instead of ∇(p · E). In contrast, we have ∇· B = 0 here, and therefore
∇(m · B) is also equal to (m × ∇) × B. Furthermore, p and E are polar vectors,
while m and B are axial.

3.2 Stationary Currents and Magnetostatics
199
Fig. 3.12 Tensor force of a moment ↑at the position ◦on a moment at the position •. Equal
moments (↑), opposite moments (↓), in-between the perpendicular moment →
Hence the potential energy of two dipole moments m and m′ at positions r ̸= r′
is obtained as
Epot = μ0
4π m · ∇m′ · ∇′
1
|r −r′| = μ0
4π
m · m′ −3 m · e m′ · e
|r −r′|3
.
Here e ≡(r −r′)/|r −r′|. (For the last equation, compare p. 172.) With r ̸= r′, this
yields
F = −∇Epot = −μ0
4π m · ∇m′ · ∇′ ∇
1
|r −r′|
= 3μ0
4π
m · e m′ + m′ · e m + (m · m′ −5 m · e m′ · e) e
|r −r′|4
for the force acting on m. This force depends upon the directions of the three vectors
m, m′, and e, and does not always lie in the direction of (±) e: it is not a central, but
a tensor force (see Fig. 3.12).
We generalize the expression for Epot to an extended magnetization:
Epot = −

dV M · B .
The integrand can be rewritten: M · (∇× A) = ∇· (A × M) + A · (∇× M). With
Gauss’s theorem (and no magnetization on the surface of V ), and because ∇× M =
j, it follows that

200
3
Electromagnetism
Epot = −

dV j (r) · A(r) .
(In Sect. 2.3.4, and in particular p. 98, we mentioned that the generalized potential
energy −q v · A belongs to the velocity-dependent Lorentz force acting on point
charges. This is in accord with Epot = −

dV j · A.) Even though the vector potential
can be re-gauged, the difference

dV j · ∇ =

dV {∇· (j) − ∇· j } does not
contribute in the case of stationary currents because of Gauss’s theorem (in ﬁnite
current loops).
For the interaction energy of two conductors, we thus have
Epot = −μ0
4π

dV dV ′ j (r) · j (r′)
|r −r′|
.
In order to derive the associated force, we have to consider the position dependence
of this potential energy. The two current loops change only their relative positions,
but neither their current densities nor their form. The potential energy originates from
the fact that two current loops are brought together from a very great distance and
that forces then appear. We should therefore introduce the average separation R of
the two conductors and consider the double integral

dr′′ · dr′ |R + r′′ −r′|−1 .
The force between the two current loops then follows from F = −∇R Epot as
(Ampère’s force law)
F = μ0
4π

dV dV ′ j (r) · j (r′) ∇
1
|r −r ′| .
According to this, parallel wires attract each other if electric currents ﬂow in the same
direction, and repel each other for currents that ﬂow in opposite directions. In other
words, currents of like sign are attracted, while like charges are repelled, because
Coulomb’s law contains −c02 qq′ instead of

I I ′ dr · dr ′, something we shall be
concerned with in the next section. Here F is the total force which the conductor
with primed quantities exerts on the other (unprimed) one. Since ∇′G(r −r ′) =
−∇G(r −r ′), it follows that F′ = −F, as is required also by Newton’s third law.
(Current-carrying conductors do not exert a force on themselves. In this case primed
and unprimed quantities must be interchangeable.) The factor μ0/4π is connected
with the chosen concept of current strength:
If two parallel (straight) conductors of negligible cross-section a distance 1 m apart in vacuum
each carry a current of 1 A, then they exert a force of 2 × 10−7 N per meter length on each
other.
The double integral of dr · dr ′ = dz dz′ is important. We may restrict ourselves to a
conductor element dz around z = 0. If the two conductors are separated by a distance

3.2 Stationary Currents and Magnetostatics
201
R, then since ∇|r −r ′|−1 = |∂(R2 + z′2)−1/2/∂R| = R (R2 + z′2)−3/2, the integral

dz′ R (R2 + z′2)−3/2 = z′R−1 (R2 + z′2)−1/2 is to be taken from −∞to +∞. We
thus deduce the force per unit length to be
F
l = μ0I I ′
2π R .
Given the magnetic ﬁeld constant μ0 = 4π × 10−7 N/A2, we do indeed ﬁnd the
above-mentioned force from Ampère’s law.
3.2.10
Inductance
For the interaction energy of two thin conductor loops, we ﬁnd
Epot = −I

dr · A = −I I ′ L
with the mutual inductance
L ≡μ0
4π
 dr · dr ′
|r −r ′| .
According to this, known as the Neumann formula, L is positive for currents in the
same direction in coaxial loops. Figure 3.13 shows an example whose inductance we
shall now calculate for radii R and R′, and distance a.
Because |r −r ′|2 = a2 + (R −R′) · (R −R′), we can ﬁnd L from
L = μ0
4π RR′
 2π
0
dϕ
 2π
0
dϕ′
cos (ϕ −ϕ′)

a2 + R2 + R′2 −2RR′ cos (ϕ −ϕ′)
.
Fig. 3.13 Top and side view of two coaxial current loops (continuous lines) at the distance a. The
line |r −r ′| connecting two points is shown by a dashed line. It can be calculated with the help of
the dotted lines (radii R and R′)

202
3
Electromagnetism
Fig. 3.14 Mutual inductance
L(k2) of two coaxial current
loops (radii R and R′ at
distance a) with k2 =
4RR′/{a2 + (R + R′)2}
The double integral is equal to 2π
 2π
0 {a2 + R2 + R′2 −2RR′ cos ψ}−1/2 cos ψ dψ.
If we integrate only from 0 to π, we obtain half the value. With z = 1
2 (π −ψ), it
follows that cos ψ = −cos(2z) = 2 sin2 z −1 and dψ = −2dz:
L = μ0
√
RR′ k
 π/2
0
2 sin2 z −1

1 −k2 sin2 z
dz ,
with k2 ≡
4RR′
a2 + (R + R′)2 .
Note that here k2 < 1, because we consider only separate conductor loops and we
have 4RR′ = (R + R′)2 −(R −R′)2. We thus encounter the complete elliptic inte-
grals of ﬁrst and second kind (see p. 104 and Fig. 2.33):
K(k2) ≡
 π/2
0
dz

1 −k2 sin2 z
and
E(k2) ≡
 π/2
0

1 −k2 sin2 z dz ,
Since sin2 z = {1 −(1 −k2 sin2 z)}/k2, this implies that
 π/2
0
2 sin2 z −1

1 −k2 sin2 z
dz = 2 K(k2) −E(k2)
k2
−K(k2) .
Finally,
L = μ0
√
RR′ 2 (K −E) −k2 K
k
.
The mutual inductance of two coaxial circles can thus be reduced to elliptic integrals
(see Fig. 3.14).
Particularly important is the special case R ≈R′ ≫a, i.e., k ≈1, of two close
current loops. Then the integrand of E is approximately equal to cos z, so E ≈1
and L ≈μ0
√
RR′ (K −2). To calculate K for k ≈1, a series expansion cannot be

3.2 Stationary Currents and Magnetostatics
203
employed, since the indeﬁnite integral for k = 1 diverges as ln cot( 1
4π −1
2 x) at the
upper boundary. But for the incomplete elliptic integral of the ﬁrst kind (see p. 103)
F(ϕ | k2) ≡
 ϕ
0
dz

1 −k2 sin2 z
,
thus
F( 1
2π | k2) = K(k2) ,
there exists the ascending Landen transformation (in k2) 2z1 = z + arcsin(k sin z)
(see Problem 3.29), viz.,
F(ϕ | k2) =
2
1 + k F(ϕ1 | k1
2) ,
with
k1
2 =
4k
(1 + k)2
and
ϕ1 = ϕ + arcsin(k sin ϕ)
2
.
For k2 = 1 −ε and ϕ = 1
2π, we have k12 ≈1 −1
16ε2 and ϕ1 = 1
2π −δϕ with δϕ =
1
2 arccos √1 −ε ≈1
2
√ε. Consequently, for the ascending transformation (in k2), the
upper boundary ϕ decreases, and now we may set
k1
2 ≈1 :
F(1
2π −δϕ | 1) = ln(cot 1
2δϕ) ≈ln(4/√ε) .
Hence for k ≈1, we arrive at K ≈ln(4/
√
1 −k2) and obtain
L = μ0
√
RR′

ln
4 (R + R′)

a2 + (R −R′)2 −2

,
for
R ≈R′ ≫a ,
i.e., for two nearby loops with like axis.
3.2.11
Summary: Stationary Currents and Magnetostatics
For electric currents, we use the current density j = ρv and the current strength I =

df · j. Stationary currents are solenoidal. In the following, according to common
practice, we write the averaged current density without the bar, since we would like
to use only macroscopically measurable quantities anyway.
In many cases, we have Ohm’s law in differential form
j = σ E + j′ .
Here σ is the conductivity and j′ the current density at the current sources.

204
3
Electromagnetism
All electric currents are accompanied by a magnetic ﬁeld. Hence we can also
identify currents in atoms, which do not contribute to macroscopic electric currents.
They can be understood via the magnetization M or via the magnetic moment m =
1
2

dV r × j . Hence, macroscopically,
∇· B = 0 and ∇× H = j ,
with B = μ0 (H + M) = μ H .
Since the induction ﬁeld is solenoidal, it derives from a vector potential A with
the property B = ∇× A. For the Coulomb gauge (∇· A = 0) and using ∇× B =
μ0 (j + ∇× M), we have
A(r) = μ0
4π

dV ′ j (r′) + ∇′ × M
|r −r′|
.
The magnetic ﬁeld acts on a moving charge via the Lorentz force F =

dV j × B.
The force between two conductors with stationary currents is then given by the
Ampère law
F = μ0
4π

dV dV ′ j (r) · j (r′) ∇
1
|r −r′| .
Currents with like orientation in parallel conductors attract each other.
3.3
Electromagnetic Field
3.3.1
Charge Conservation and Maxwell’s Displacement
Current
The charge conservation law was expressed on p. 187 in the form of a continuity
equation, viz.,
∂ρ/∂t + ∇· j = 0 .
Conversely, the continuity equation ensures charge conservation. Since ρ = ∇· D,
we thus also have
∇·

j + ∂D
∂t

= 0 ,
or according to Gauss’s theorem,
0 =

(V )
df ·

j + ∂D
∂t

= I + dQ
dt .

3.3 Electromagnetic Field
205
As long as, e.g., the charge on the anode of a capacitor increases, a current will also
ﬂow, with a sink for the current density j . If we connect the current loop with the
capacitor in a Gedanken experiment, an electric current will ﬂow in the conductor,
while Maxwell’s displacement current will ﬂow in a non-conductor, with current
density ∂D/∂t. If an electric ﬁeld changes with time, then this is the corresponding
current.
The sum of the conduction and displacement current densities is solenoidal, and
hence is a rotational ﬁeld. For stationary currents, it is the curl of the magnetic ﬁeld
H—but this is in fact generally true:
∇× H = j + ∂D
∂t
⇐⇒

(A)
dr · H = I + d
dt

A
df · D .
While a capacitor is being charged, there is thus a magnetic ﬁeld around it, not only
around the connecting wires. For the path integral

dr · H, only the boundary of the
area A is of interest. If we choose two different sheets with the same boundary (A)
for

df · D, then the values of the surface integrals differ by the charge Q enclosed
by these two sheets. In fact, I + ˙Q then no longer depends on the chosen area.
In insulators there is no conduction current, but at most a displacement current,
while in conductors the displacement current is in most cases negligible compared
to the electric current. If we have a periodic process with angular frequency ω,
then for j/ ˙D this clearly depends on the ratio σ/εω. Here most conductors have
σ/ε > 100 THz. Therefore, the order of magnitude of the ratio σ/εω is only unity
for frequencies common in optics.
As long as the displacement is negligible compared to the electric current, the
currentsaresaidtobequasi-static—forstationarycurrentsallderivativeswithrespect
to time vanish.
3.3.2
Faraday Induction Law and Lenz’s Rule
As was just shown, the two equations ∇· D = ρ and ∇× H = j + ∂D/∂t ensure
charge conservation. If there were no free charges but only electric dipoles, we would
have instead ∇· D = 0 and ∂D/∂t = ∇× H. This is noteworthy insofar as we would
not ﬁnd magnetic charges, but only magnetic dipoles—whence we already set up the
equation ∇· B = 0 in magnetostatics. Hence we can ask the question whether ∂B/∂t
is equal to the circulation density of a (polar) vector ﬁeld, in particular, a vector ﬁeld
which is irrotational for time-independent phenomena.
In fact, we have the Faraday induction law,
∇× E = −∂B
∂t .

206
3
Electromagnetism
Fig. 3.15 Lenz’s rule. The time-dependence of the magnetic ﬁeld ∂B/∂t = −∇× E induces a
current density j = σ E in the conductor loop. This current is accompanied by a magnetic ﬁeld
curl density ∇× H = j such that, on the plane of the loop, H and ∂B/∂t are oriented in opposite
directions
A time-dependent magnetic ﬁeld and the curl of the electric ﬁeld are related: the
magnetic ﬁeld induces an electric current in a conductor loop. Every dynamo makes
use of this. The sign in the induction law supplies the important Lenz rule (see
Fig. 3.15): the induced current works against its cause.
In integral form, the induction law reads

(A)
dr · E = −d
dt

A
df · B .
Since ∇· B = 0, the last expression depends only on the boundary of the area A.
The left contour integral is called the circulation voltage or induction voltage. We
note that the concept of electric voltage between two points introduced previously
(p. 169) can now yield different values depending on the path in-between.
3.3.3
Maxwell’s Equations
Now we have prepared sufﬁciently for the famous Maxwell equations, with which
we can describe many phenomena of electricity and optics—including also D = εE
and B = μH:
∇× E = −∂B
∂t ,
∇· B = 0 ,
∇· D = ρ ,
∇× H = j + ∂D
∂t .
These couple the electric and magnetic ﬁelds. It is thus better to speak of the total
electromagnetic ﬁeld. As integral equations, they read

3.3 Electromagnetic Field
207

(A)
dr · E = −d
dt

A
df · B ,

(V )
df · B = 0 ,

(V )
df · D = Q ,

(A)
dr · H = I + d
dt

A
df · D .
because

V dV ρ = Q and

A df · j = I. The boundary conditions for the transition
at an interface are similar to those in the static case:
n × (E+ −E−) = 0 ,
n · (B+ −B−) = 0 ,
n · (D+ −D−) = ρA ,
n × (H+ −H−) = jA .
In particular, there is no ﬁeld B or D whose derivative with respect to time on the
interface is singular like a delta function. There is at most a discontinuity like a
step function. Its source density or circulation density may be singular like a delta
function, but because δ(x) = ε′(x), there is only a ﬁnite discontinuity in the ﬁeld,
not an inﬁnite one as for the delta function. Therefore, the derivatives of B and D
with respect to time do not contribute to the surface curl density.
Clearly, the curl of the electric and the magnetic ﬁeld are connected with time-
dependent changes, while their sources are already known from statics. Therefore,
in statics E and H, or D and B, are similar. But for time-dependent phenomena on
the one hand E and B are connected, and on the other hand D and H are connected.
All Maxwell’s equations were already known prior to Maxwell, except for the one
involving the displacement current, but it is only by virtue of the latter that certain
key phenomena such as charge conservation and electromagnetic waves can exist.
According to the Fourier transform r →k (see p. 22),
E(t, r) =
1
√
2π 3

d3k exp(+ik · r) E(t, k) ,
E(t, k) =
1
√
2π 3

d3r exp(−ik · r) E(t, r) ,
and correspondingly for D, B, H, j, and ρ, Maxwell’s equations read
ik × E(t, k) = −∂B(t, k)
∂t
,
ik · B(t, k) = 0 ,
ik · D(t, k) = ρ(t, k) ,
ik × H(t, k) = j (t, k) + ∂D(t, k)
∂t
,
and the continuity equation
∂ρ(t, k)
∂t
+ ik · j (t, k) = 0 .
The real differential expressions in real space thus become complex in k-space, but
local expressions for the transverse and longitudinal parts of the ﬁeld. In particular,
the induction ﬁeld is purely transverse:

208
3
Electromagnetism
∇× Etrans = −∂Btrans
∂t
,
Blong = 0 .
In addition, ∇· Dlong = ρ holds, and we can split up the fourth of Maxwell’s equa-
tions:
∇× Htrans = jtrans + ∂Dtrans
∂t
and
0 = jlong + ∂Dlong
∂t
.
With ∇· Dlong = ρ, the last equation leads to the continuity equation.
The ﬁelds are real in real space and, according to p. 22, have the symme-
try E(t, k) = E∗(t, −k), and likewise for D, B, H, j, and ρ. In particular, for
a point charge ρ(t, r) = q δ(r −r′) has (complex) Fourier transform ρ(t, k) =
(2π)−3/2q exp(−ik · r ′).
We derived the microscopic Maxwell equations from the “facts of observation”.
There are electric, but no magnetic charges; charges remain conserved; we ﬁnd the
force law due to Coulomb, the one due to Ampère (Lorentz), and also Faraday’s
induction law. The “macroscopic” Maxwell equations start from
D = ε0E + P = εE and B = μ0 (H + M) = μH ,
with averaged charge and current densities, the polarization P, and the magnetiza-
tion M. Actually, we should have written H = B/μ0 −M = B/μ for the magnetic
excitation, since E and B are related, and likewise D and H.
In the following we shall always assume linear relations between D and E and/or
H and B, even though there are also “nonlinear effects”, e.g., for hysteresis and for
strong ﬁelds of the kind occurring in laser light. In addition, we calculate only with
scalar relations—this is generally not allowed in crystal physics, where ε and μ are
tensors. But even there, many phenomena can already be treated, and the calculations
are then simple.
In addition, we have to observe Ohm’s law:
j = σE or U = R I .
To a ﬁrst approximation, the conductivity σ and the resistance R do not depend on
the applied ﬁeld. (Here σ is actually a tensor.)
3.3.4
Time-Dependent Potentials
As long as the ﬁelds do not depend on time, they can be derived from the scalar
potential 	 and the vector potential A, as was shown in Sects. 3.1.3 and 3.2.8.
This works even for time-dependent ﬁelds. The induction ﬁeld in particular remains
solenoidal, and therefore can still be derived from the curl of a vector potential:

3.3 Electromagnetic Field
209
∇· B = 0
⇐⇒
B = ∇× A .
However, for time-dependent magnetic ﬁelds the electric ﬁeld E has curls, and a
gradient ﬁeld (−∇	) is no longer sufﬁcient, but since according to the last equation
we have ∂B/∂t = ∇× ∂A/∂t, the induction law ∇× E = −∂B/∂t now implies
E = −∇	 −∂A
∂t .
With the two quantities 	 and A (which have four components in total), we can thus
determine the two vector ﬁelds E and B (with six components in total). It remains
only to comply only with the two remaining Maxwell equations (where we assume
D = εE and B = μH with constant factors ε and μ, i.e., homogeneous matter). Since

	 = ∇· ∇	, it follows that

	 = −ρ
ε −∂
∂t ∇· A ,
and since 
A = ∇(∇· A) −∇× (∇× A),


 −εμ ∂2
∂t2

A = −μ j + ∇

∇· A + εμ ∂	
∂t

.
We do not use j = σE, since here ρ and j are viewed as given. The potentials are
not unique though, since the source of the vector potential has not yet been given.
The magnetic ﬁeld does not depend on it, and its inﬂuence on the electric ﬁeld can
be counteracted by a change in the scalar potential. Therefore, despite the gauge
transformation
	′ = 	 + ∂
∂t
and
A ′ = A −∇ ,
with continuously differentiable , the same ﬁelds E and B result. Physical quantities
do not depend on the gauge. The curl of the vector potential determines the magnetic
ﬁeld, and the sources determine 
. In the static case we were allowed to choose
these sources arbitrarily, but now their time dependence shows up for the scalar
potential. Every gauge transformation changes the longitudinal component of the
vector potential and the scalar potential. Then it is clear that
Elong = −∇	 −∂Along
∂t
,
Blong = 0 ,
Etrans = −∂Atrans
∂t
,
Btrans = ∇× Atrans .
Longitudinal ﬁelds are irrotational, transverse ones solenoidal.

210
3
Electromagnetism
There are two possibilities for the gauge such that the equations for the scalar and
the vector potential decouple. This can be seen immediately for the Lorentz gauge
∇· A + εμ ∂	
∂t = 0 ,
and in particular,

εμ ∂2
∂t2 −

	 = ρ
ε ,

εμ ∂2
∂t2 −

A = μj .
These formally similar equations will be preferred in the next section on Lorentz
invariance. There is a retardation effect here: ρ and j are important at time t′ = t −
|r −r ′|/c, showing that actions propagate with ﬁnite velocity. This will be explained
in more detail in Sect. 3.5.1.
But for the moment we prefer to take the Coulomb gauge (radiation gauge, trans-
verse gauge)
∇· A = 0 .
Even though initially this yields

	 = −ρ
ε ,


 −εμ ∂2
∂t2

A = −μ

j −ε∇∂	
∂t

,
according to p. 27, the Poisson equation 
	 = −ρ/ε is solved by
	(t, r) =
1
4πε

dV ′ ρ(t, r ′)
|r −r ′| ,
and with the continuity equation ∂ρ/∂t = −∇· j therefore leads to
∂	
∂t = −1
4πε

dV ′ ∇′ · j (t, r ′)
|r −r ′|
.
Thus according to p. 25, ε∇∂	/∂t comprises the part of the current density that
originates in the sources, and therefore j −ε∇∂	/∂t is the solenoidal (transverse)
current density
jtrans(t, r) ≡∇×

dV ′ ∇′ × j (t, r ′)
4π |r −r ′|
.
Consequently, the system of equations is also decoupled in the Coulomb gauge:

εμ ∂2
∂t2 −

A = μ jtrans .

3.3 Electromagnetic Field
211
For this gauge, only a solenoidal current density is therefore of interest. This occurs,
in particular, if there are no macroscopic charges (then even 	 ≡0 holds), e.g., for
the radiation ﬁeld of single atoms. Therefore, it is sometimes called the radiation
gauge. However, it does have a disadvantage: for each Lorentz transformation, a new
gauge must be derived, because it is not Lorentz invariant.
3.3.5
Poynting’s Theorem
The Maxwell equations imply in particular
E · ∂D
∂t + H · ∂B
∂t = E · (∇× H −j) −H · (∇× E) = −j · E −∇· (E × H) .
We recognize the expression j · E from p. 188 as the power density for the Joule heat,
which does not arise in insulators. The power densities of the electric and magnetic
ﬁelds are given on the left. If D and E are related to each other linearly, then the
ﬁrst term is the time-derivative of the known energy density 1
2E · D of the electric
ﬁeld. If we also assume a linear relation between H and B (which is not allowed for
ferromagnets because of hysteresis), then we may take the expression 1
2H · B as the
energy density of the magnetic ﬁeld. It is positive-deﬁnite and is suggested in view
of the similarity between the electric and magnetic ﬁeld quantities. Thus we take
w = 1
2 (E · D + H · B)
as the energy density of a electromagnetic ﬁeld and obtain Poynting’s theorem:
∂w
∂t + ∇· (E × H) = −j · E .
If the Joule heat is missing, then this equation is similar to the continuity equation:
E × H is the energy ﬂux density, which is also called the Poynting vector:
S ≡E × H .
In order to understand what it means for the stationary situation (with ∂w/∂t = 0),
we consider a ﬁnite piece of a conductor in Fig. 3.16. Here we have ∇· S = −j · E =
−σ E2.
Because B = ∇× A and H · (∇× A) = ∇· (A × H) + A · (∇× H) for quasi-
stationary currents (i.e., for ∇× H = j and no contribution from the surface integrals
of A × H), the now-justiﬁed ansatz 1
2 H · B for the energy density of the magnetic
ﬁeld leads to

212
3
Electromagnetism
Fig. 3.16 Interpretation of the Poynting vector S for a stationary current along a wire of length l
with radius R. Here S ﬂows from the outside through the curved surface A and, because E = U/l
and H = I/(2π R), it has the absolute value S = U I/A there. The heat power U I generated inside
then ﬂows out through the curved surface, while the current ﬂows through the faces
1
2

dV H · B = 1
2

dV j · A = 1
2 I

dr · A = 1
2 L I 2 ,
where L is now the self-inductance of the conductor. According to the Neumann
formula
L = μ0
4π
 dr · dr ′
|r −r ′| ,
it can be determined, but no arbitrarily thin conductors can be taken, otherwise
L diverges according to p. 203. We would then have to integrate over the mutual
inductances of the various current lines (Problem 3.30).
For the energy of two stationary currents, we derived the expression Epot =
−

dV j · A on p. 199. Despite the other sign, this does not contradict the value
just found for the self-energy. In the previous case, the current distributions were
given and the mutual position and orientation of the loops were changed for ﬁxed
current density, while now it is the geometrical situation that is kept ﬁxed and the
current strength increases from zero to the ﬁnal value.
The energy of the electromagnetic ﬁeld in thermodynamics is a “free energy”.
It can be fully used for work—more on that in Sect. 6.4.8. There, too, all energies
will be split into products of intensive and extensive quantities, which disproves
the microscopically suggested expression 1
2(ε0E2 + B2/μ0). Thermodynamically,
D and H must appear in addition to E and B.
For static problems we left out integrals of the form

V
dV ∇· S =

(V )
df · S ,

3.3 Electromagnetic Field
213
if integrations with boundaries at inﬁnity were to be performed, since we assumed
that the integrand would decrease more strongly at inﬁnity than r−2: in fact, E at
least as r−2 and H at least as r−3 (monopole or dipole ﬁeld). But for time-dependent
situations, E and H then decrease rather slowly with the distance from the radiation
source, whence the surface integral

df · S does not vanish even for very large
volumes—we must still account for the radiation power, which we will only consider
in Sect. 3.3.7.
3.3.6
Oscillating Circuits
If we connect a resistance R, an inductance L, and a capacity C in series to an
AC voltage U, then the energy appears in three forms: in the resistance according to
p. 188 as Joule heat

R I 2 dt, in the inductance as magnetic energy 1
2 L I 2, and in the
capacity as electric energy 1
2 Q2/C. All three together must be supplied to the setup.
We neglect the radiation power, which increases according to p. 264 as the fourth
power of the frequency and barely contributes for quasi-stationary situations. Since
˙Q = −I, the total power is then I (RI + L ˙I −Q/C). The expression in brackets
must be equal to the applied voltage. The derivative with respect to time yields
L d2I
dt2 + R dI
dt + 1
C I = dU
dt ,
which is the differential equation of a forced damped oscillation, as in Sect. 2.3.8.
There the decay coefﬁcient γ = 1
2 R/L and the angular frequency ω0 = 1/
√
LC were
introduced, and it was shown that the initial eigenoscillation decays with time and
that the solution then oscillates with the angular frequency ω of the source of the
voltage. Therefore, we calculate in the ﬁnal state, with
U = Re {U exp(−iωt)}
and
I = Re {I exp(−iωt)} .
The ansatz exp(+iωt) is often made, and this leads to the opposite sign of i in the
following equations. For our choice, which is also common in quantum theory, its
value moves clockwise in the complex plane. U and I do not depend on time.
In the course of time, their products with exp(−iωt) become purely real as well
as also purely imaginary. Hence the differential equation leads to (−ω2L −iωR +
C−1) I = −iω U and then Ohm’s law for AC currents, viz.,
U = Z I ,
with impedance Z ≡R + i
 1
ωC −ωL

= R + i X .
It is composed of the active resistance R and the reactance X. The imaginary part
shifts the phase between the voltage and current by φ = arctan X/R. The build-up
of the electromagnetic ﬁeld takes time—in the capacitor the voltage follows the
current, while it precedes the current in the coil (see Fig. 3.17). Therefore, |φ| ≤1
2π

214
3
Electromagnetism
Fig. 3.17 Absorption circuit. Resonance for ω0 = 1/
√
LC. Here, ω0L = 5R
Fig. 3.18 Trap circuit. Resonance occurs for

ω02 −(R/L)2. Note that here ω0L = 5R
holds here, in contrast to the forced oscillation in Sect. 2.3.8 (see Fig. 2.23). For
low frequencies (ω < 1/
√
LC), it is determined mainly by the capacity, and for
high frequencies by the inductance. (R does not depend on the frequency, as long
as the conductivity does not depend on it, and it determines the power loss.) For
ω = ω0 ≡1/
√
LC, the reactance vanishes, and therefore the absolute value of the
impedance, the ﬁctitious resistance Z = |Z |, is particularly small.
Corresponding to Kirchhoff’s laws, we have added here the individual contri-
butions of the three parts of the conductor. For parallel connection of a capaci-
tor (capacity C) and a coil (inductance L and resistance R), we have in contrast
Z −1 = (R −iωL)−1 −iωC (see Fig. 3.18):
Z = ω0L (R/ω0L) + i (ω/ω0) {(ω/ω0)2 −1 + (R/ω0L)2}
(R/ω0L)2(ω/ω0)2 + {(ω/ω0)2 −1}2
.
The ﬁctitious resistance is now highest for ω =

ω02 −(R/L)2, where it is equal to
(ω0L)2/R = L/(RC). Therefore, we also refer to such connections as trap circuits
(and, if connected in series, as absorption circuits).
3.3.7
Momentum of the Radiation Field
With the force density ρE + j × B, Maxwell’s equations read
ρ E + j × B = ∇· D E +

∇× H −∂D
∂t

× B .

3.3 Electromagnetic Field
215
Here, the last vector product can be rewritten
B × ∂D
∂t = −∂(D × B)
∂t
+ D × ∂B
∂t = −∂(D × B)
∂t
−D × (∇× E) .
Because ∇· B = 0, we therefore have
F + d
dt

dV D × B =

dV {E ∇· D −D × (∇× E)
+H ∇· B −B × (∇× H)} .
We restrict ourselves to homogeneous matter, but allow also for anisotropic, prefer-
ential directions—then the permittivity and the permeability are tensors, and oblique
coordinates can be useful, although at least rectilinear ones. According to p. 184, for
homogeneous matter we have
E ∇· D −D × (∇× E) =
	
ik
gi ∂(Ei Dk −1
2gikE · D)/∂xk ,
and likewise with H, B instead of E, D. Therefore, we now generalize Maxwell’s
stress tensor from p. 184 to include magnetic ﬁeld contributions (it is symmetric only
for isotropic media):
T ik ≡w gik −Ei Dk −Hi Bk ,
and according to Gauss’s theorem and Sects. 1.2.4 and 1.2.5, obtain for time-
dependent ﬁelds
F + d
dt

dV D × B +
	
ik
gi

(V )
d fk T ik = 0 .
According to this, we have to view D × B as a momentum density. For isotropic
media, it is equal to εμS and then has the same direction as the energy ﬂux density
S, but a different one for anisotropic media.
3.3.8
Propagation of Waves in Insulators
In insulators, i.e., if ρ and j vanish, and for constant ε and μ, we have

εμ ∂2
∂t2 −

A(t, r) = 0 ,

216
3
Electromagnetism
according to Sect. 3.3.4 (see in particular p. 210), and this for both the Lorentz and
the Coulomb gauge. This (homogeneous) wave equation for a vector ﬁeld is also
encountered for the electric and magnetic ﬁelds. In particular, in the insulator,
∇× E = −∂B
∂t ,
∇· B = 0 ,
∇· D = 0 ,
and ∇× H = ∂D
∂t .
Hence, since 
a = ∇(∇· a) −∇× (∇× a) for D = εE and B = μH, we have

E =
∇× ∂B
∂t =
μ ∂
∂t ∇× H = εμ ∂2E
∂t2 ,

B = −μ∇× ∂D
∂t = −μ ∂
∂t ∇× D = εμ ∂2B
∂t2 .
According to these wave equations, we ﬁnd the phase velocity c from the permittivity
ε and permeability μ :
εμ = c−2 ,
in particular in vacuum ε0μ0 = c0
−2 .
This is Weber’s equation. In electromagnetism, in contrast to (non-relativistic)
mechanics where all velocities are on an equal footing, a particular velocity is sin-
gled out. This is connected with the question of Lorentz invariance, discussed in the
next section. If it is taken as an observational fact (Michelson experiment), charge
conservation and Coulomb’s law from the microscopic Maxwell equations can be
derived from it, even without knowing anything about the magnetic ﬁeld. However,
the charge and magnetic moment of elementary particles are not properties on an
equal footing.
The wave equation is a homogeneous partial differential equation of second order.
In order to solve it, we take the Fourier transform (see Sect. 1.1.11) A(t, r) →
A(t, k). Hence with ω ≡ck, the partial differential equation can be simpliﬁed to
 ∂2
∂t2 −c2

A(t, r) = 0
=⇒
 ∂2
∂t2 + ω2
A(t, k) = 0 .
Since A(t, r) must be real, A∗(t, k) = A(t, −k). Therefore, the solution of the dif-
ferential equation reads
A(t, k) = A(k) exp(−iωt) + A∗(−k) exp(+iωt)
2
.
Here, the factor 1/2 is arbitrary (it only has to be real), but it is nevertheless useful
for what follows, because we then have A(k) from the initial values A(0, k) =
1
2 {A(k) + A∗(−k)} and ∂A(t, k)/∂t|t=0 = −1
2iω {A(k) −A∗(−k)} as

3.3 Electromagnetic Field
217
Fig. 3.19 Linearly polarized
electromagnetic wave. The
polarization plane (thus E)
(red curve) lies in the plane
of the page and B (blue
curve) is perpendicular to it
A(k) = A(0, k) + i
ω
∂A(t, k)
∂t



t=0 .
Finally, because exp{i(k · r + ωt)} = (exp{i(−k · r −ωt)})∗(and rewriting k →
−k), it follows that
A(t, r) =
1
√
2π 3

d3k Re

A(k) exp{i(k · r −ωt)}

,
with ω = ck and
A(k) =
1
√
2π 3

d3r exp(−ik · r)

A(0, r) + i
ω
∂A(t, r)
∂t



t=0

.
If we restrict ourselves to one value k, then this gives the propagation direction of
the wave in which it travels through the homogeneous (and isotropic) medium with
velocity c = 1/√εμ and wavelength λ = 2π/k.
In a non-conductor, the ﬁelds E and B are solenoidal, thus transverse:
k · E(t, k) = 0
and
k · B(t, k) = 0 .
The vector potential is only solenoidal for the “transverse gauge” (Coulomb gauge)
∇· A = 0
=⇒
k · A(t, k) = 0 .
For the position and time dependence exp{i(k · r −ωt)} of the ﬁelds, the equation
−iω B(k) = −ik × E(k) follows from the induction law ∂B/∂t = −∇× E:
cB(k) = ek × E(k) ,
with ek = k
k .
For ω ̸= 0, the three vectors k, E(k), and B(k) thus form a right-handed rectangular
frame, and in homogeneous insulators we need only E(k) or B(k) (see Fig. 3.19).
However, this is not yet useful for the energy density 1
2 (E · D + H · B) and the
energy ﬂux density E × H, since for a bilinear expression, a double integral over
k and k ′ would have to be performed. If we average over time, then we arrive at

218
3
Electromagnetism
least at δ(ω + ω′) or at δ(k + k′), respectively, and if we average over space, also
at δ(k + k ′). Here the Fourier components corresponding to k and −k are related,
because the ﬁelds are real. We consider therefore the special case with ﬁxed k :
E(t, r) = Re

E(k) exp{i(k · r −ωt)}

.
The Maxwell equations require ω = ck, k · E(k) = 0, and
cB(t, r) = Re

ek × E(k) exp{i(k · r −ωt)}

.
Because Rez = 1
2 (z + z∗), the expression
1
2 E∗(k) · D(k) follows for the time-
averaged value of E · D. For the mean value of H · B, we ﬁnd the same, because
the ﬁelds are transverse. The average energy density is
w(t, r) = 1
2 E∗(k) · D(k) = 1
2 H∗(k) · B(k) .
Therefore, from the average energy density w, we can also determine the amplitude
E of the ﬁeld strength:
w = ε E2 = 1
2 ε E2
=⇒
E =

2 w
ε
.
This expression is needed, e.g., for the energy of interaction between a wave with
energy ℏω = w V and the dipole moment p of an atom, yielding
W = p

2ℏω/εV cos(ωt) .
For the mean value of the Poynting vector, we obtain
S(t, r) = c w(t, r) ek .
Notethatthebarsareoftenleftout,buttheequationsarevalidonlyfortheaverage.For
the velocity (S/w) of the energy ﬂux, we thus obtain c ek, a vector of absolute value c
in the propagation direction k of the wave. The momentum density εμS has the same
direction, and its absolute value is equal to w/c, from Weber’s equation εμ = c−2.
In Sect. 3.4.9, we shall also arrive at this ratio between energy and momentum for
massless free particles.
A further feature of electromagnetic radiation is its polarization direction. Here
we mean the oscillation direction of the electric ﬁeld—the magnetic ﬁeld oscillates
perpendicular to it, since ωB(k) = k × E(k). Therefore, one of the two unit vectors
e∥and e⊥with e∥· e⊥= 0 and e∥× e⊥= ek sufﬁces for expansion of the ﬁeld
vectors. Then we have, e.g.,

3.3 Electromagnetic Field
219
E(k) = e∥E∥+ e⊥E⊥.
The direction of the two unit vectors is thus not yet uniquely ﬁxed. We are free to
choose a preferred direction. For the example of diffraction, we take the plane of
incidence as the preferred direction: e∥lies in the plane, e⊥is perpendicular to it.
The amplitudes E(k) are Fourier components of the real quantities E(t, r) and so
have complex components E∥and E⊥. Therefore, if we set E = |E| exp(iβ), then
in the plane k · r = 0, it follows that
E(t, r) = Re{E(k) exp(−iωt)} = e∥|E∥| cos(ωt−β∥) + e⊥|E⊥| cos(ωt−β⊥) .
Instead of the the two phases β∥and β⊥, we use their difference δβ ≡β⊥−β∥and
their mean value β ≡1
2 (β∥+ β⊥) :
E(t, r) =
{e∥|E∥| + e⊥|E⊥|} cos( 1
2 δβ) cos(ωt −β)
−{e∥|E∥| −e⊥|E⊥|} sin( 1
2 δβ) sin(ωt −β) .
In general, this is an elliptically polarized wave, because a cos(ωt −β) +
b sin(ωt −β) traces out an ellipse. For a ∝b, we obtain a piece of a straight line (lin-
early polarized wave) and for a = b with a ⊥b, a circle. Therefore, for |E∥| = |E⊥|
with δβ = 1
2π (mod π), the wave is circularly polarized. For δβ = ± 1
2π, the ﬁeld
rotateswithinaquarterperiodfromthedirectione∥to±e⊥.Inoptics,wespeakofleft-
or right-circularly polarized light, depending on how the ﬁeld vector rotates when we
view against the ray direction—anticlockwise or clockwise: δβ = + 1
2π corresponds
to left-circular polarization. In contrast, in particle physics, we view along the ray
direction and for δβ = + 1
2π, we speak of positive helicity (right-handedness) and
for δβ = −1
2π, we speak of negative helicity (left-handedness).
Instead of linear polarization, we may of course expand in terms of circularly
polarized light:
E(k) = e+ E+ + e−E−.
Because Re{E(k) exp(−iωt)} = ReE(k) cos(ωt) + ImE(k) sin(ωt), for circularly
polarized light, ReE(k) must be perpendicular to ImE(k). This property must be
satisﬁed by the vectors e±. We take complex unit vectors and set
e± ≡e∥± ie⊥
√
2
exp(iϕ±) ,
where e+ is appropriate for positive helicity and e−for negative. The phases ϕ±
may be chosen arbitrarily, e.g., such that the coefﬁcients E± are real. (Note that, in
Sect. 5.5.1, we shall take the factor ∓instead of exp(iϕ±).) In any case, we always
have

220
3
Electromagnetism
e±
∗· e± = 1
and
e±
∗· e∓= 0 ,
and hence E± = e±∗· E(k). In addition,
e±
∗× e± = ± i ek
is independent of the phase factor.
3.3.9
Reﬂection and Diffraction at a Plane
We consider the boundary plane between two insulators and let a plane wave with
wave vector ke fall onto the interface. Then there is a diffracted (transmitted) wave
with wave vector kd, and a reﬂected wave with wave vector kr (Problem 3.40) (see
Fig. 3.20).
According to Maxwell’s equations, we have the boundary conditions (see p. 207)
n × (Ee + Er −Ed) = 0 ,
n · (Be + Br −Bd) = 0 ,
n · (De + Dr −Dd) = 0 ,
n × (He + Hr −Hd) = 0 .
Sincethesealwayshavetohold,allthreewavesmusthavethesameangularfrequency
ω, because only then will their exponential functions exp(−iωt) always agree with
each other. Likewise, for all positions r on the interface, we must require
ke · r = kr · r = kd · r ,
Fig. 3.20 Wave vectors ke, kr, and kd at a beam splitter, an interface with the normal vector n, the
unit vector t in the plane of incidence, and the angles θe, θr, and θd. The three wave vectors have—
as proven in the text—equal tangential components and ke and kr opposite normal components.
In addition, kd/ke = ce/cd holds, and the ratio of the indicated circular radii is thus equal to the
refractive index n

3.3 Electromagnetic Field
221
since only then can the exponential functions exp(ik · r) be the same everywhere at
the interface. If r is perpendicular to ke, then it is clearly also perpendicular to kr and
kd: all three vectors ke, kr, and kd lie in the plane spanned by ke and n, the plane of
incidence. If on the other hand we take a vector r along the intersecting line of the
interface and the plane of incidence, namely the vector t, then the three wave vectors
must have equal tangential components:
ke sin θe = kr sin θr = kd sin θd .
Now because ω = ck, we also have kr = ke and cd kd = ce ke, and therefore,
sin θe = sin θr
and
sin θe
sin θd
= ce
cd
=
εd μd
εe μe
≡n ,
which is the Snellius diffraction law (see Fig. 3.20). The ratio ce/cd of the velocities
is the refractive index n. One should not take the static values—the material constants
depend upon the frequency (dispersion).
After the relations between the wave vectors, we now investigate those between
the ﬁeld amplitudes. To this end, it is useful to express all ﬁelds in terms of E(k)
because, for linearly polarized light, the oscillation direction of the electric ﬁeld is
deﬁned as the polarization direction:
D = εE ,
B = ek × E/c ,
H = ek × E/μc .
The set of boundary conditions provides a system of equations. In order to solve
these, we introduce the two unit vectors t and b = t × n in addition to the normal
vector n (b in Fig. 3.20 points toward the observer). With k = t t · k + n n · k and
using the Snellius diffraction law, we ﬁnd
t · ke = +ke sin θe = + t · kr ,
t · kd = +kd sin θd ,
n · ke = −ke cos θe = −n · kr ,
n · kd = −kd cos θd .
If we decompose these three E vectors into their perpendicularly polarized com-
ponents E⊥≡E · b (perpendicular to the plane of incidence) and their parallel
polarized components E∥≡E · (b × ek) (in the plane of incidence),
E = b E⊥+ b × ek E∥,
then, because k × E = k × b E⊥+ b k E∥and k × b = −n t · k + t n · k, we
have
n ·
E =
t · ek E∥
,
n · (k × E) =
−t · k E⊥,
n × (k × E) =
t k E∥−b n · k E⊥,
n ×
E = b n · ek E∥
+t E⊥.

222
3
Electromagnetism
Hence the boundary conditions for the normal components yield
εe sin θe (Ee∥+ Er∥) = εd sin θd Ed∥,
sin θe
ce
(Ee⊥+ Er⊥) = sin θd
cd
Ed⊥,
which are already contained in the requirements for the tangential components, if
we take into account the Snellius diffraction law sin θe : sin θd = ce : cd and Weber’s
equation:
cos θe (Ee∥−Er∥) = cos θd Ed∥,
Ee⊥+Er⊥
=
Ed⊥,
1
μe ce
(Ee∥+Er∥) =
1
μd cd
Ed∥,
cos θe
μe ce
(Ee⊥−Er⊥) = cos θd
μd cd
Ed⊥.
Therefore, with
n′ ≡n μe
μd
= ce μe
cd μd
—in insulators, in particular, μ ≈μ0 and hence n′ ≈n (thus n ≈√εd/εe)—we
obtain
Er∥
Ee∥
= n′ cos θe −cos θd
n′ cos θe + cos θd
,
Er⊥
Ee⊥
= cos θe −n′ cos θd
cos θe + n′ cos θd
,
Ed∥
Ee∥
= 1
n′

1 + Er∥
Ee∥

,
Ed⊥
Ee⊥
= 1 + Er⊥
Ee⊥
.
For the corresponding equations for the magnetic ﬁeld strength B, the factor n is
included in the lower row, because E and B differ by the velocity c. Note in addition
that B oscillates inadirectionperpendicular toE. Clearly, for perpendicular incidence
and n′ = 1, nothing is reﬂected, hence if the wave resistance cμ = √μ/ε remains
the same (the value for the vacuum is approximately 377Ω, according to p. 165).
If, after the approximation n′ ≈n, we use the diffraction law sin θe = n sin θd,
Fresnel’s equations follow (see Fig. 3.21):
Er∥
Ee∥
= tan(θe −θd)
tan(θe + θd) ,
Er⊥
Ee⊥
= −sin(θe −θd)
sin(θe + θd) ,
Ed∥
Ee∥
=
1
cos(θe −θd)
Ed⊥
Ee⊥
,
Ed⊥
Ee⊥
= 1 + Er⊥
Ee⊥
.
Because
tan(α ± β) = sin α cos β ± cos α sin β
cos α cos β ∓sin α sin β
and cos2 α + sin2 α = 1, we have

3.3 Electromagnetic Field
223
Fig. 3.21 Fresnel’s equations for the transition from air to glass (n = 3/2) and back. Brewster
angle ◦. Limiting angle for total reﬂection • (for larger angles, only ReEr/Ee is shown)
tan(α−β)
tan(α+β) = sin α cos α −sin β cos β
sin α cos α + sin β cos β .
Part of the result could have been obtained without the calculation above. If the
transmittedﬁeldstrengthoscillatesinthedirectionofkr,thenthereﬂectedcomponent
Er∥is missing, i.e., Er∥= 0 for kr ⊥kd or θd = 90◦−θr = 90◦−θe. Since n =
sin θe/ sin θd, the Brewster angle is found to be
θe = arctan n ,
the reﬂected wave is linearly polarized, so E oscillates only perpendicularly to the
plane of incidence. Note that, without the approximation n′ ≈n, the Brewster angle
is found to be arctan (n

(n′ 2 −1)/(n2 −1)).
As a function of the angle of incidence and the refractive index in the approxima-
tion n′ ≈n, it follows that
Er∥
Ee∥
= n2 cos θe −

n2 −sin2 θe
n2 cos θe +

n2 −sin2 θe
,
Er⊥
Ee⊥
= cos θe −

n2 −sin2 θe
cos θe +

n2 −sin2 θe
,
Ed∥
Ee∥
= 1
n

1 + Er∥
Ee∥

,
Ed⊥
Ee⊥
= 1 + Er⊥
Ee⊥
,
where we have used cos θd =

1 −sin2 θd =

1 −n−2 sin2 θe. For n < 1, there is a
limiting angle for total reﬂection, viz., θe = arcsin n. For higher angles of incidence,
the amplitude ratio Er/Ee is complex (of absolute value 1) and the refractive index
likewise. Linearly polarized radiation then becomes elliptically polarized, and the
transmitted solution is damped. We shall not discuss this here, because we shall deal
with damped solutions (in space) in the next section anyway. We sometimes speak
of evanescent waves.

224
3
Electromagnetism
3.3.10
Propagation of Waves in Conductors
In contrast to the last two sections, we shall no longer restrict ourselves to σ = 0.
Then,
∇× E = −∂B
∂t ,
∇· B = 0 ,
∇· D = 0 ,
∇× H = σE + ∂D
∂t .
Here, electromagnetic energy is converted into heat and hence, for a homogeneous
medium, the wave equations gain a damping term


 −μ

σ + ε ∂
∂t
 ∂
∂t

E = 0 ,
∇· E = 0 ,
and likewise with B instead of E. These are the telegraph equations.
If an external wave impinges on a conductor surface, then the ﬁelds depend peri-
odically on time. We have to investigate the position dependence in the conductor.
According to the telegraph equation, the ansatz
E(t, r) = Re

E(k ′) exp{i(k ′ · r −ωt)}

for all positions in the conductor leads to the condition
k′ 2 = εμ ω2 
1 + i σ
εω

.
This can be satisﬁed for real ω only with a complex wave vector. A complex permit-
tivity ε (1 + iσ/εω) is also often introduced. Here, for a scalar material with constant
σ, ε, and μ, the real and imaginary parts of the wave vector have the same direc-
tion. The new feature in comparison with non-conductors is longitudinal damping.
Therefore, we set
k ′ = (α + iβ) k ,
where as before ck = ω with c = 1/√εμ. Then we have
exp{i(k ′ · r −ωt)} = exp(−β k · r) exp{i(α k · r −ωt)}
and (α + iβ)2 = 1 + iσ/εω, whence
α =

1
2

1 + (σ/εω)2 + 1
2
and
β =

1
2

1 + (σ/εω)2 −1
2 .
Now, with increasing k · r , the amplitude decreases. The wave is damped spatially.
Since conductors usually have σ/εω ≫1, whereupon the electric current is large
compared to the displacement current, we obtain the decay length

3.3 Electromagnetic Field
225
Fig. 3.22 Repulsion of the current. Decay of the alternating ﬁelds in the interior of a conductor—
dashed lines show their amplitude—here for σ ≫εω and hence α ≈β. (When σ/εω < ∞, there
is also a normal component of the magnetic ﬁeld and a tangential component of the electric ﬁeld)
d ≡
1
β k ≈1
k

2εω
σ
=

2
σμω ,
where the amplitude for perpendicular incidence is smaller than the factor 1/e at the
surface. High-frequency alternating currents are thus repelled from the interior of the
conductor, ﬂowing only at the surface. This is referred to as repulsion of the current
or the skin effect (see Fig. 3.22). The higher the conductivity, the shorter the decay
length. For the phase velocity, we have c′ = ω/αk = c/α, and for σ/εω ≫1, we
thus have α ≈β ≫1, whence also c′ ≈c/β = ωd and therefore c′ ≪c.
Since
k ′ · E(k ′) = 0 ,
ωB(k ′) = k ′ × E(k ′) ,
and k ′ · B(k ′) = 0 ,
the three (complex) vectors k ′ = (α + iβ) k, E(k ′) and B(k ′) are once again per-
pendicular to each other and still form a right-handed frame, but E and B differ in
phase and therefore no longer have the same nodes. If, as in Sect. 3.3.8, we average
over the time, we obtain
H(t, r) · B(t, r) = 1
2 H∗(k ′) · B(k ′) exp(−2β k · r)
=

1 + (σ/εω)2 E(t, r) · D(t, r) ,
where the square-root factor originates from k ′∗· k ′/k2. For most conductors, there
is much more energy in the magnetic ﬁeld than in the electric ﬁeld. Here now the
energy density decreases with increasing distance from the surface, in proportion to
exp(−2β k · r) (Problem 3.41).
If a conductor is adjacent to an insulator, and if n points from the conductor to
the insulator, then we have the boundary conditions
n × (EI −EC) = 0 ,
n · (BI −BC) = 0 ,
n · (DI −DC) = ρA ,
n × (HI −HC) = jA .

226
3
Electromagnetism
The ﬁelds do not enter an ideal conductor at all—it is fully screened by charges
and currents on the surface (EC = . . . = 0). Therefore, the electric ﬁeld lines end up
perpendicular to the surface of an ideal conductor (without tangential component,
i.e., ET = 0), and the magnetic ﬁelds adapt to the surface (without normal compo-
nent, i.e., HN = 0). But if the conductivity is ﬁnite (normal conductor), a current is
accompanied by a ﬁnite ﬁeld in the current direction (ET ̸= 0), and there is no surface
current density. Therefore the tangential component of HC turns continuously into
that of HI and decays exponentially in the conductor (for ω ̸= 0) with increasing
distance from the surface.
3.3.11
Summary: Maxwell’s Equations
Two new quantities lead from statics to time-dependent phenomena: charge conser-
vation (continuity equation) supplies Maxwell’s displacement current ∂D/∂t, and
Faraday’s induction law connects ∂B/∂t with ∇× E, where the sign results in Lenz’s
rule. The induction ﬁeld counteracts the change in the magnetic ﬁeld. Hence we have
the basic Maxwell equations:
∇× E = −∂B
∂t ,
∇· B = 0 ,
∇· D = ρ ,
∇× H = j + ∂D
∂t .
These differential equations correspond to integral equations,

(A)
dr · E = −d
dt

A
df · B ,

(V )
df · B = 0 ,

(V )
df · D = Q ,

(A)
dr · H = I + d
dt

A
df · D ,
and boundary conditions,
n × (E+−E−) = 0 ,
n · (B+−B−) = 0 ,
n · (D+−D−) = ρA ,
n × (H+−H−) = jA .
Taking Fourier transforms with exp{i(k · r −ωt)}, the four Maxwell equations read
k × E(ω, k) =
ωB(ω, k) ,
k · B(ω, k) = 0 ,
k · D(ω, k) = −i ρ(ω, k) ,
k × H(ω, k) = −i j (ω, k) −ωD(ω, k) .
In charge-free, homogeneous space, they lead to transverse waves, and they obey the
telegraph equation, which is the same for E and B. Here the three vectors k, E, and
B are pairwise perpendicular to each other.

3.3 Electromagnetic Field
227
The time-dependent potentials 	(t, r) and A(t, r) are useful:
E = −∇	 −∂A
∂t
and
B = ∇× A .
Then the ﬁrst two Maxwell equations are automatically satisﬁed. However, the scalar
potential 	 is determined only up to an additive term ∂/∂t, and the vector potential
A only up to its sources—it would have to be changed by −∇. The potentials may
still be gauged to our advantage. Here  or ∇· A is ﬁxed. For the Coulomb gauge, we
choose ∇· A = 0, and for the Lorentz gauge, ∇· A = −εμ ∂	/∂t. In both cases,
the resulting system of equations is decoupled.
3.4
Lorentz Invariance
3.4.1
Velocity of Light in Vacuum
In contrast to the situation in mechanics, in electromagnetism a speciﬁc velocity is
picked out, even if there is no matter in space which could supply a reference frame.
This velocity is the velocity of light in vacuum, viz.,
c0 = 299 792 458 m
s .
But in electromagnetism, no inertial system is special, because the four Maxwell
equations are valid in all uniformly moving reference frames. In particular, the veloc-
ity of light in vacuum is the same in all inertial frames.
Due to this astonishing fact, we have to completely rethink the notion of velocity,
and thus also the measurement of lengths and times. In particular, we need a signal
velocity c0 in order to ﬁx equal times everywhere in space (coordinate system). In
order to synchronize clocks at two points with constant separation |r −r ′|, we send
a signal from one point and expect it to arrive at the other point at the time 
t =
|r −r ′|/c0. Without a signal velocity, we cannot synchronize clocks at different
positions, and without clocks we cannot measure a velocity. The fastest velocity is
that of light, a million times faster than sound in air. Therefore, we synchronize our
clocks with light signals. (If there were some kind of action at a distance, with inﬁnite
propagation velocity, then of course we would use that to synchronize our clocks.)
Since c0 is the same in all inertial frames, we may not start from a generally ﬁxed
(absolute) time, as we would in classical mechanics. There it is assumed that, for
two inertial frames moving relative to one another, only the position coordinates
transform, but not the time. That implies the validity of the
Galilean transformation:
t′ = t ,
r ′ = r −v t .

228
3
Electromagnetism
But this can be valid only for v ≪c0, because it does not contain the velocity of
light in empty space.
3.4.2
Lorentz Transformation
We consider an inertial frame with unprimed coordinates (t, r) and one with primed
coordinates (t′, r′), moving uniformly with velocity v relative to the ﬁrst, where the
position vectors are given in Cartesian coordinates. We restrict ourselves to homoge-
neous Lorentz transformations: the origins (0, 0) of the two systems agree with each
other. (Inhomogeneous Lorentz transformations contain four further parameters,
since for them the zero point is also moved, and they form the Poincaré group.) Since
otherwise no event is preferred, the two coordinate systems depend linearly on each
other (via a real transformation matrix). The transition is reversible, and therefore
their determinant must be either positive (a proper Lorentz transformation, continu-
ously connected to the identity) or negative (improper Lorentz transformation, e.g.,
space reﬂection, also called the parity operation, t′ = t, r ′ = −r, or time reversal,
t′ = −t, r ′ = r). If we include these two improper Lorentz transformations with the
proper ones, then we obtain the extended Lorentz group. If the past remains behind
and the future ahead, then the Lorentz transformation is orthochronous (∂t′/∂t > 0).
For inﬁnitesimal Lorentz transformations, the matrix is barely different from the
unit matrix, so no squared terms in this difference for (c0t)2 −r2 = (c0t′)2 −r′2
need be accounted for. The additional terms form a skew-symmetric matrix with six
(real) independent elements and lead for ﬁnite Lorentz transformations to six free
parameters: three Euler angles and three parameters for the boost.
For the time being, we choose the axes such that v has only an x-component
(> 0). Then y = y′ and z = z′, and only (t, x) and (t′, x′) depend on each other in a
more involved way. At least in the two coordinate systems, the relative velocity will
be denoted by v = −v′. Therefore, we require
x′ = γ (x −vt)
and
x = γ (x′ + vt′) ,
because the point x′ = 0 moves away with velocity v = x/t and the point x = 0
with the opposite velocity −v = x′/t′. The factor γ must be the same in the two
equations, otherwise the two reference frames would differ fundamentally from one
another. We determine γ from the requirement that, in the two systems, the same
velocity of light c0 must result. Then we have
c0
t = 
x = γ (
x′ + v
t′) ,
as well as c0
t′ = 
x′ = γ (
x −v
t) .
We must therefore have c0
t = γ (c0 + v) 
t′ and c0
t′ = γ (c0 −v) 
t, and
hence,

3.4 Lorentz Invariance
229
Fig. 3.23 Relations between the parameters β, γ , and γ −1. The dashed line is the relation for the
Galilean transformation

t

t′ = γ (c0 + v)
c0
=
c0
γ (c0 −v)
=⇒
γ 2 =
1
1 −(v/c0)2 .
We therefore use the abbreviation
β ≡
v
c0
=⇒
γ =
1

1 −β2 .
Since the coordinates remain real, β ≤1 must hold, so v ≤c0 and γ ≥1 (see
Fig. 3.23).
From t′ = (x/γ −x′)/v and x′/v = γ (x/v −t), it follows that t′ = (γ −1 −
γ ) x/v + γ t. Here, 1 −γ −2 = β2, so t′ = γ (t −β x/c0). If we combine x′ =
γ (x −vt) y′ = y and z′ = z as a vector equation, we obtain ﬁnally the Lorentz
transformation
t′ = γ

t −β · r
c0

and
r ′ = r + γ −1
β2
β β · r −γ β c0t .
Conversely, because β′ = −β, we have
t = γ

t′ + β · r ′
c0

and
r = r ′ + γ −1
β2
β β · r ′ + γ β c0t′ .
In the limit of small velocities v ≪c0, whence β ≪1 and γ ≈1, we arrive at the
above-mentioned Galilean transformation
t′ = t ,
r ′ = r −v t ,
or t = t′ ,
r = r ′ + v t .
But this holds only approximately because of the ﬁnite signal velocity c0. Therefore,
from now on, we shall only deal with the Lorentz transformation. In particular, for
v ∝ex, we have

230
3
Electromagnetism
c0t′
x′

=
 γ
−βγ
−βγ
γ
c0t
x

,
or
c0t
x

=
 γ
βγ
βγ
γ
c0t′
x′

,
along with y′ = y and z′ = z. With the consequences

t′ = γ


t −β 
x
c0

,

x′ = γ (
x −v 
t) ,
or

t = γ


t′ + β 
x′
c0

,

x = γ (
x′ + v 
t′) ,
we can compare rulers and clocks in reference frames moving relative to each other
and derive two noteworthy phenomena.
The ﬁrst is Lorentz contraction: the ends of a ruler of length 
x in its rest system
must be measured simultaneously in the moving system and are found to be closer
together:

t′ = 0 :

x′ = 
x
γ
< 
x .
Conversely, thanks to the requirement 
t = 0, the length 
x′ in the oppositely mov-
ing (unprimed) system is also shorter, i.e., 
x = 
x′/γ . Moving lengths are shorter
than the proper length in the rest system by the factor 1/γ =

1 −β2 < 1. In addi-
tion to Lorentz contraction, owing to the ﬁnite light propagation time, a dilation by
the factor 1/(1 −β) also occurs when frames approach one another and a compres-
sion by 1/(1 + β) when they move apart. The total factor √1 ± β/√1 ∓β is also
shown in the middle of Fig. 3.26 (and see also Table 3.1, although reversed there,
since frequencies are inversely proportional to wavelengths).
The second striking phenomenon is relativistic time dilation: times must be com-
pared at the position of the clock in the rest system, and result in times in the moving
system being dilated by the factor γ > 1 compared with the proper time (in the rest
system):

x = 0 :

t′ = γ 
t > 
t ,
or 
x′ = 0 :

t = γ 
t′ .
This effect must be included when determining the lifetimes of fast-moving particles:
for v ≈c0, the factor γ is signiﬁcantly greater than 1.
The two phenomena can also be read off from the Minkowski diagram (Fig. 3.24).
But it is worth making a few comments. The quantity (c0t)2 −x2 is a Lorentz
invariant: (c0t′)2 = γ 2 (c0t −βx)2 and x′2 = γ 2 (x −βc0t)2 imply (c0t′)2 −x′2 =
γ 2 {(c0t)2 −x2} (1 −β2) with 1 −β2 = γ −2. Therefore, for a Lorentz transforma-
tion the world points (c0t, x) in the Minkowski diagram move on a hyperbola, and
for c0t = x, on the associated asymptote. We distinguish here the time-like region
with |c0t| > |x| and the space-like region with |c0t| < |x| (actually, we should write
|r | instead of |x|). The surface |c0t| = |r | is called the light cone. Time-like world
points on the hyperbola (c0t)2 −x2 = C2 > 0 then obey the parameter representa-
tion c0t = C cosh φ, x = C sinh φ. (For a space-like world point, c0t is exchanged

3.4 Lorentz Invariance
231
Fig. 3.24 Minkowski diagram. It has a spatial coordinate and the (reduced) time c0t as axes and
the light cone |c0t| = |r | as diagonal. A moving coordinate system is also shown. Its axes have
slopes of β or β−1. The scale transformation is indicated by the hyperbolic curves—they connect
world points at equal positions (blue curves) or times (red curves). The two arrows at bottom right
indicate the length contraction, those top left the time dilation: they point from the unit coordinate
value in the rest systems to the axes of the moving systems, each parallel to the axes
with x.) With α = arctanhβ and φ′ = φ −α, the above-mentioned Lorentz transfor-
mation, i.e., the transition to oblique space-time coordinates, is then simply
c0t = C cosh φ ,
x = C sinh φ ,

=⇒
c0t′ = C cosh φ′ ,
x′ = C sinh φ′ ,
if we employ the addition theorems for hyperbolic functions, namely, the relation
cosh (φ−α) = cosh φ cosh α −sinh φ sinh α (with the special case 1 = cosh2 α −
sinh2 α) and sinh (φ−α) = sinh φ cosh α −cosh φ sinh α.
3.4.3
Four-Vectors
The Lorentz transformation connects space and time and mixes their coordinates.
Therefore, instead of the usual three-vectors in the normal space, we now take four-
vectors in space and time. In order to have all four components as lengths, we use
the path length c0t of the light instead of the time, and take it as zeroth component:
(xμ) = (x0, xk) = (x0, x1, x2, x3) ≡(c0t, x, y, z) = (c0t, r) .
We let Greek indices (e.g., μ) run from 0 to 3, Latin indices (e.g., k) from 1 to 3.

232
3
Electromagnetism
As in Sect. 1.2.2, we also distinguish in four dimensions between covariant and
contravariant vector components with different transformation behavior. Then for a
Lorentz transformation, we have
dx′μ =
3
	
ν=0
∂x′μ
∂xν dxν .
In the following we would like always to sum over doubly appearing indices from
0 to 3 if in an expression each occurs once as a subscript and once as a superscript
(Einstein summation convention). In this way we avoid the bothersome notation of
the summation sign—we often have to contract tensors and we have already used
this idea to abbreviate the scalar product in vector algebra. With this and according
to p. 33, we ﬁnd
A′μ = ∂x′μ
∂xν Aν
for contravariant vector components,
A′
μ = Aν
∂xν
∂x′μ
for
covariant vector components.
We may also read these two equations as matrix equations—they are linear trans-
formations with symmetric transformation matrices which are inverse to each other.
For the example considered in the last section, they read
∂x′μ
∂xν

=
⎛
⎜⎜⎝
γ −βγ
0
0
−βγ
γ
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠
and
 ∂xν
∂x′ μ

=
⎛
⎜⎜⎝
γ βγ
0
0
βγ
γ
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠.
If the coordinate axes are not as well adjusted to the relative velocity as here, then of
course not so many matrix elements aμν will vanish. Generally, with x′μx′μ = xνxν,
we always have
aμ
νaμ
λ = gν
λ
and
aμ
ν
∗= aμ
ν .
The special case here sufﬁces for the general principle.
The transition between covariant and contravariant components is, however, not
as simple as for Cartesian coordinates. In particular, the velocity of light must have
the same value in all coordinate systems, so the Lorentz invariant (c0dt)2 −dr · dr
must be a scalar:
dxμ dxμ = c0
2 dt2 −dr · dr .
This is achieved by introducing the Minkowski metric

3.4 Lorentz Invariance
233

gμν

=
⎛
⎜⎜⎝
1
0
0
0
0 −1
0
0
0
0 −1
0
0
0
0 −1
⎞
⎟⎟⎠=

gμν

.
This matrix is sometimes written also as diag (1, −1, −1, −1). On p. 32, we have
gii = gi · gi > 0 and also gii > 0. This suggests introducing imaginary base vectors,
but we shall not consider these further here, since we only need the metric to inter-
change upper and lower indices. To this end, we always take the above-mentioned
fundamental tensor. Thus, since xμ = gμνxν, we have
(xμ) = (x0, xk) = (x0, x1, x2, x3) = (c0t, −x, −y, −z) .
This suggests choosing the fundamental tensor with opposite sign, since then the
space components remain unchanged for the transition from three to four dimensions.
(It is also common to set x4 = ic0t and drop x0.) But then physically sensible scalar
products like pμ pμ become negative. For this reason, we prefer, like many other
authors, to stick to the choice just made.
For the transition from three to four dimensions, however, we encounter some
difﬁculties with the concept of the vector product. In particular, a × b should be
perpendicular to a and b, but this is unique to three dimensions. In four dimensions,
we may no longer refer to “axial vectors” as vectors. But if we take over the usual
components of a vector product, then it transforms according to (with Latin indices
running from 1 to 3)
a′i b′ j −a′ j b′i = ∂x′i
∂xk
∂x′ j
∂xl

ak bl −al bk
,
thus as a tensor of second rank (see p. 35 and Problem 2.4). It is skew-symmetric:
T i j = −T ji = −Ti
j = +T j
i = +Tj
i = −T i
j = +Ti j = −Tji .
In three dimensions, such a tensor has three independent components T 12 = −T 21,
T 23 = −T 32 and T 31 = −T 13, while T 11 = T 22 = T 33 = 0. In the following, we
shall also consider four-dimensional skew-symmetric tensors of second rank, with
six independent components. They have the properties
T i0 = −T 0i = −Ti
0 = +T 0
i = −T0
i = +T i
0 = −Ti0 = +T0i ,
as follows immediately from the metric.
In addition, the circulation density of a vector ﬁeld is a skew-symmetric tensor of
second rank:
∂a j
∂xi −∂ai
∂x j = ∂a
∂xi · g j −∂a
∂x j · gi .

234
3
Electromagnetism
Note that, because of the derivatives, all indices are taken to be covariant. For
variable base vectors, the derivative of a vector component a j with respect to xi
arises, and then also a · ∂g j/∂xi. We thus have to introduce the Christoffel sym-
bols of Sect. 1.2.6, although for rotations, these contributions cancel: ∂g j/∂xi =
∂2r/(∂xi∂x j) = ∂gi/∂x j. However, for space-time considerations, we now restrict
ourselves to ﬁxed base vectors anyway.
3.4.4
Examples of Four-Vectors
As a ﬁrst example, we have already met the four-vector
(xμ) = (c0t, r)
⇐⇒
(xμ) = (c0t, −r) .
If we want to build the velocity vector (vμ), we cannot simply differentiate with
respect to time, since that would not be Lorentz invariant—we must differentiate with
respect to the proper time τ (see p. 230). We have dt = γ dτ, or d/dτ = γ d/dt, and
hence
(vμ) = γ (c0, v)
⇐⇒
(vμ) = γ (c0, −v)
and
vμvμ = c0
2 .
Thus only in the non-relativistic limit v ≪c0 do we arrive at the usual notion of
velocity, for then γ ≈1. We can also derive this in a different way. Corresponding
to velocity 0, we have the four-vector (vμ) = (c0, 0, 0, 0). If it undergoes a Lorentz
transformation with the velocity −v in the x-direction, or
−1 =
⎛
⎜⎜⎝
γ βγ
0
0
βγ
γ
0
0
0
0
1
0
0
0
0
1
⎞
⎟⎟⎠,
then since the matrix is symmetric, we may multiply it by a row vector from the left
or a column vector from the right to obtain the four-vector (v′μ) = γ (c0, v, 0, 0),
and thus the same result as before.
This second idea allows us to derive the addition law for velocities.
If the
above-mentioned matrix acts on the four-vector with parallel velocity vectors,
γ0 (c0, v0, 0, 0), it follows that
(v∥
′μ) = γ γ0 (1 + ββ0)

c0, v + v0
1 + ββ0
, 0, 0

,
and if it acts on the perpendicular velocity vector γ0 (c0, 0, v0, 0), we ﬁnd

3.4 Lorentz Invariance
235
Fig. 3.25 With the velocity
parameter w = arctanhβ,
known as the rapidity, the
addition law for parallel
velocities reads
β = β0 + β1
1 + β0β1
then simply
w = w0 + w1. For |β| ≪1,
we have w ≈β
(v⊥
′μ) = γ γ0

c0, v, v0
γ , 0

.
Here, v and v0 are thus not equivalent: Lorentz transformations do not in general
commute. The factors γ γ0 (1 + ββ0) or γ γ0 are indeed the same as the quantity
γ ′ = (1 −β′2)−1/2, as we shall now prove by showing that β′2 = 1 −γ ′−2:
 β + β0
1 + ββ0
2
= 1 −(1 −β2)(1 −β02)
(1 + ββ0)2
= 1 −
1
γ 2γ02 (1 + ββ0)2 ,
and
β2 + β02
γ 2 = 1 −(1 −β2)(1 −β0
2) .
Incidentally, this is also equal to β02 + β2γ0−2. The addition law can be summarized
by
v ′ =
1
1 + v · v0/c02

v + v0
γ + v γ −1
γ
v · v0
v2

.
This equation also follows from dr ′/dt′ = dr ′/dt · dt/dt′ with the formulae for the
Lorentz transformation on p. 229, if dr/dt = v0 is used (see Fig. 3.25).
Only if all velocities are small compared to c0 do we have v ′ = v + v0. Otherwise
the velocity of light in vacuum could also be exceeded, but in fact, v′ = c0 if v or v0
is equal to c0. For parallel velocities, this follows immediately from
v′ = (v + v0)/(1 + ββ0) ,
and for perpendicular velocities,
v′ 2 = v2 + v0
2/γ 2 = c0
2 (β2 + β0
2 −β0
2β2) .

236
3
Electromagnetism
Table 3.1 Longitudinal and transverse Doppler effect
θ
0
1
2π
π
ω′/ω
√1 −β/√1 + β
1/

1 −β2
√1 + β/√1 −β
θ′
0
1
2π + arcsin β
π
When β = 1 or β0 = 1, the bracket is equal to 1.
If a medium with refractive index n = c0/c moves with velocity v and there is
light travelling in it in the same direction, the velocity of this light will depend on
this reference system (Fizeau experiment on the drag of light in moving bodies):
c′ =
v + c
1 + β/n = c0
β + 1/n
1 + β/n = c +

1 −1
n2

v + · · · .
The expression in brackets is called the (Fresnel) drag coefﬁcient. However, for
dispersion, in addition to −n−2, it also contains the term (ω/n) dn/dω.
The zero of a wave is determined by the phase ωt −k · r and must not depend
upon the choice of coordinates. The expression must be a Lorentz invariant and must
therefore be written in the form of a scalar product kμxμ. Consequently, we have
(kμ) =
 ω
c0
, k

,
with kμkμ = 0 (because ω = c0k) .
With t = γ (t′ + v · r ′/c02) and r = r ′ + {(γ −1)v−2 v · r ′ + γ t′} v (see p. 229),
and comparing coefﬁcients in ωt −k · r = ω′t′ −k ′ · r ′, we deduce that
ω′ = γ (ω −v · k)
and
k ′ = k +
γ −1
v2
v · k −γ k
c0

v .
With ω = c0k, this implies the Doppler effect for the frequency, viz.,
ω′ = ωγ (1 −β cos θ) ,
where θ is the angle between v and k. Thus the Doppler effect with νλ = c0 yields
the wavelength λ′ = λ/{γ (1−β cos θ)}. Some example applications are given in
Table 3.1 and Figs. 3.26 and 3.27.
With the factor γ , a transverse and a quadratic Doppler effect occur (this does
not of course hold for the propagation of sound, as the velocity of sound is so much
smaller than the velocity of light). In addition, the propagation direction is described
differently (aberration). With the vectors
e ≡k
k
and
e ′ ≡k ′
k′ ,

3.4 Lorentz Invariance
237
Fig. 3.26 Angular dependence of the frequency, wavelength, and deviation. In the left and right
ﬁgures, straight lines refer to β = 0 (black), the curves to β = 1
4 (red), 1
2 (blue), and 3
4 (green).
The middle picture shows the ratio of the wavelengths λ′/λ in a polar diagram, namely the focal
representation of an ellipse with semi-axes γ and 1, and hence eccentricity β (here 1/2). See also
Fig. 3.31
Fig.3.27 Dopplereffect.Afrequencydependsonhowfastthedetectormovesrelativetotheemitter.
Left: Decreasing distance. Right: Increasing distance. The linear Doppler effect is indicated by the
dashed line
and using k′ = ω′/c0 = γ (k −β · k) = γ k (1 −β · e), we deduce that
e ′ =
1
γ (1 −β · e)

e +
γ −1
β2
β · e −γ

β

,
and thus also β · e ′ = (β · e −β2)/(1 −β · e). Here β · e ′ = β cos θ′, so
cos θ′ = cos θ −β
1 −β cos θ
=⇒
tan θ′ =
sin θ
γ (cos θ −β) .
With increasing |β|, the difference between θ and θ′ increases, although not for θ = 0
and π (see Table 3.1 and Fig. 3.26). The motion of the Earth about the Sun produces
an aberration of starlight ≤20.5′′.
For the concept of a density, it is important to note that the three-dimensional
volume element dV = dx dy dz is not invariant, because of the Lorentz contraction,

238
3
Electromagnetism
while the rest volume dV0 = γ dV is. The charge does not change. With this, we
then have ρ dV = ρ0 dV0 and
ρ = γ ρ0 .
From the charge and current density, we build a four-vector
( jμ) ≡ρ0 (vμ) = ρ0 γ (c0, v) = (c0ρ, j) ,
with
jμ jμ = (c0ρ0)2 .
In particular, j 0 = c0ρ and j = ρv as before, but ρ depends on the velocity through
γ , i.e., through the Lorentz contraction.
3.4.5
Conservation Laws
In the following, we use the usual abbreviation
∂μ ≡
∂
∂xμ
and
∂μ ≡
∂
∂xμ
.
Clearly, the components ∂μψ transform covariantly and ∂μψ contravariantly. We
now prove the following theorem: If the four-dimensional source density ∂μ jμ = 0
of a four-vector vanishes everywhere and if this vector differs from zero only in a
ﬁnite region of the three-dimensional space, then

dV j0 is constant for all times.
For the proof we extend Gauss’s theorem to four dimensions:

d4x ∂μ jμ =

dSμ jμ ,
where d4x = c0dt dx dy dz = c0dt dV and dSμ denotes a three-dimensional surface
element for constant xμ. Its sign (direction) is ﬁxed in such a way that it is positive if
its xμ value is greater than in the considered volume (negative otherwise). Now we
choose the surfaces S1(′), S2(′), and S3(′) for large |x1|, |x2|, and |x3| such that j = 0
holds there. Figure 3.28 supplies the rest of the proof.
An important application is the continuity equation:
∂ρ
∂t + ∇· j = 0
⇐⇒
∂μ jμ = 0 .
The theorem supplies

dV j 0 =

dV c0ρ = c0 Q

3.4 Lorentz Invariance
239
Fig. 3.28
j is restricted in ﬁnite space (the cylinder). ∂μ jμ = 0 then yields

dS0 j0 +

dS0′ j0′ =
0 (the circular face S0 is covered here and therefore not indicated). Due to the directional sense of
the surface elements, the conservation law

dV j0 =

dV ′ j0′ follows
as a conserved quantity. The law of charge conservation follows from the continuity
equation, and conversely, the continuity equation follows from charge conservation,
something we already obtained in Sect. 3.2.1.
3.4.6
Covariance of the Microscopic Maxwell Equations
On p. 210, we found the microscopic Maxwell equations, viz.,
1
c02
∂	
∂t + ∇· A=0 ,
 1
c02
∂2
∂t2 −

	= ρ
ε0
,
and
 1
c02
∂2
∂t2 −

A = μ0j ,
with the help of the potentials 	 and A in the Lorentz gauge (the Coulomb gauge
∇· A = 0 is not Lorentz invariant). With the ﬁrst equation, we combine the scalar
and vector potentials to yield the following four-potential:
 
Aμ!
=
	
c0
, A

=⇒
∂Aμ
∂xμ
≡∂μAμ = 0 .
Note that the equation ∂μAμ = 0 does not result in a conservation law, since Aμ
does not vanish sufﬁciently fast for large distances. In addition, using the other two
equations, we generalize the Laplace operator to the d’Alembert operator (quabla)
□≡
1
c2
0
∂2
∂t2 −
 =
∂2
∂xμ ∂xμ
= ∂μ∂μ .
This is a Lorentz invariant, often taken with the opposite sign, in particular, if the other
metric is used, with gik = +δik. If in addition to 	 = c0 A0, we also take into account

240
3
Electromagnetism
ρ = c0−1 j0 and (by Weber’s equation) c0−2ε0−1 = μ0, then the above-mentioned
inhomogeneous wave equations can be brought into the covariant form
□Aμ = μ0 jμ ,
with ∂μAμ = 0 .
In four-notation, the gauge transformation 	′ = 	 + ∂/∂t, A ′ = A −∇
reads
A′μ = Aμ + ∂μ ,
because ∂0 = ∂0 and ∂k = −∂k, in addition to A0 = 	/c0.
With B = ∇× A and E = −∇	 −∂A/∂t, noting that Ak = −Ak and A0 = A0,
we clearly have
Bx = ∂Az
∂y −∂Ay
∂z
=
−∂2 A3 + ∂3A2 = −∂2 A3 + ∂3A2 ,
Ex = −∂	
∂x −∂Ax
∂t
= c0 (∂1A0 −∂0 A1) = c0 (∂0 A1 −∂1A0) ,
and correspondingly for the other two components of B and E. According to the
last two columns, E/c0 and B can be combined in the form of a four-dimensional
skew-symmetric tensor of second rank, the electromagnetic ﬁeld tensor
Fμν ≡∂μAν −∂ν Aμ = −Fνμ ,
or equivalently, Fμν = ∂μAν −∂ν Aμ = −Fνμ:
 
Fμν!
=
⎛
⎜⎜⎝
0
−Ex/c0 −Ey/c0 −Ez/c0
+Ex/c0
0
−Bz
By
+Ey/c0
Bz
0
−Bx
+Ez/c0 −By
Bx
0
⎞
⎟⎟⎠
and
 
Fμν
!
=
⎛
⎜⎜⎝
0
+Ex/c0 +Ey/c0 +Ez/c0
−Ex/c0
0
−Bz
By
−Ey/c0
Bz
0
−Bx
−Ez/c0 −By
Bx
0
⎞
⎟⎟⎠.
Unfortunately, the ﬁeld tensor is not commonly denoted by B, rather than F, even
though B is extended into four dimensions. For the extension of j to jμ and A to
Aμ, we are led by the space-like components, and likewise in the next section for the
extension of M to Mμν. However, the ﬁeld tensor is usually also amended with the
factor c0. Then it has the components of E and c0B as elements.
Mixed derivatives commute with each other, if they are continuous. The Jacobi
identity ∂λ(∂μAν −∂ν Aμ) + ∂μ(∂ν Aλ −∂λAν) + ∂ν(∂λAμ −∂μAλ) = 0 yields

3.4 Lorentz Invariance
241
∂λFμν + ∂μFνλ + ∂ν Fλμ = 0 .
So far we have used two Maxwell equations, namely ∇· B = 0 and ∂B/∂t + ∇×
E = 0, that is, precisely the two for which we have been able to introduce potentials.
The other two microscopic Maxwell equations ∇· E = ρ/ε0 and ∇× B = μ0 (j +
ε0 ∂E/∂t) can be combined if □Aν = μ0 jν and ∂μAμ = 0 hold, to give
∂μFμν = ∂μ (∂μAν −∂ν Aμ) = □Aν −∂ν∂μAμ = μ0 jν .
Hence, we have μ0∂ν jν = ∂ν∂μFμν. The continuity equation ∂ν jν = 0 now follows
immediately from the antisymmetry of the ﬁeld tensor, because Fμν = −Fνμ, but
∂ν∂μ = +∂μ∂ν, thus ∂ν∂μFμν = −∂μ∂ν Fνμ = −∂ν∂μFμν.
According to p. 100, the interaction density is equal to ρ 	 −j · A, which is the
Lorentz invariant jμAμ in four-dimensional notation. Hence, we may also write
∂μFμν = μ0 jν as a generalized Euler–Lagrange equation,
if we introduce the
Lagrange density
L = −Fμν Fμν
4μ0
−jμ Aμ ,
as a function of the Aμ and their derivatives ∂μAν. Using
∂L
∂(∂μAν) = ∂L
∂Fκλ
∂Fκλ
∂(∂μAν) = −Fκλ
2μ0
(δμ
κ δν
λ −δμ
λ δν
κ) = −Fμν
μ0
and ∂μFμν = μ0 jν = −μ0 ∂L /∂Aν, we obtain the differential equation
∂
∂xμ
∂L
∂(∂μAν) = ∂L
∂Aν
for the Lagrange density L . This equation apparently generalizes the Euler–
Lagrange equation in Sect. 2.3.3, viz.,
d
dt
 ∂L
∂˙xk

= ∂L
∂xk ,
where the time is no longer preferred over the space coordinates. Note that 1
2 Fμν Fμν
= B · B −E · E/c02.
3.4.7
Covariance of the Macroscopic Maxwell Equations
If we wish to use only macroscopically measurable notions, then instead of ∇· E =
ρ/ε0 and∇× B = μ0 (j + ε0 ∂E/∂t),orindeed∂μFμν = μ0 j ν wenowhavetotake
the Maxwell equations ∇· D = ρ and ∇× H = j + ∂D/∂t, i.e., in four-notation

242
3
Electromagnetism
∂μGμν = j ν ,
with the skew-symmetric tensor
(Gμν) =
⎛
⎜⎜⎝
0
−c0Dx −c0Dy −c0Dz
c0Dx
0
−Hz
Hy
c0Dy
Hz
0
−Hx
c0Dz −Hy
Hx
0
⎞
⎟⎟⎠,
which is the four-dimensional extension of the vectors H, just as Fμν is that of B,
and the four-vector of the average current density
( j μ) = (c0ρ, j) .
In doing this, we also generalize D = ε0E+P, thus E/c0 = μ0c0 (D−P) and B =
μ0 (H+M) to
Fμν = μ0 (Gμν + Mμν) ,
with the (skew-symmetric) magnetization tensor
(Mμν) =
⎛
⎜⎜⎝
0
c0Px
c0Py
c0Pz
−c0Px
0
−Mz
My
−c0Py
Mz
0
−Mx
−c0Pz −My
Mx
0
⎞
⎟⎟⎠,
which extends the magnetization M to four dimensions. From this, we can easily
establish the magnetization current density jm. The decomposition
jν = j ν + jm
ν ,
with jmν = jν −j ν = μ0−1 ∂μFμν −∂μGμν leads to
jm
ν = ∂μMμν .
Note that there is therefore a continuity equation for the magnetization current, viz.,
∂ν jmν = 0. Then,
( jm
μ) =

−c0∇· P, ∂P
∂t + ∇× M

.
In electrostatics we have already encountered ρ = ρ −∇· P and in magnetostatics
j = j + ∇× M. But the displacement current also contributes and results in the
additional term ∂P/∂t.

3.4 Lorentz Invariance
243
The matrices Gμν and Mμν can be derived easily from Gμν and Mμν according
to p. 233: G0k = −G0k = −Gk0 and Gik = Gik = −Gki, and likewise for Mμν.
For given j ν and Mμν, the skew-symmetric tensors F and G are thus determined.
Out of two Maxwell equations, just one equation has emerged in four-dimensional
space.
3.4.8
Transformation Behavior of Electromagnetic Fields
Under a Lorentz transformation, the ﬁelds E and B (D and H) do not behave like
vector ﬁelds Aμ, but the electromagnetic ﬁeld tensors F and G are indeed tensors of
second rank:
F′ μν = ∂κx′μ ∂λx′ ν Fκλ .
This system of equations corresponds to a matrix equation F′ = "F. The antisym-
metry of "F = −F is transferred to "
F′ = ""F = −F′, so only the six components
with μ < ν have to be determined. Since F is uniquely related to E and B according
to p. 240, and likewise F′ to E′ and B′, this means that the transformation properties
of the ﬁelds can be derived using the matrices ∂νx′ μ mentioned on p. 232. Then for
a system moving with velocity v, we have the ﬁelds
E∥′ = E∥,
E⊥′ = γ

E⊥+ v × B

,
B∥′ = B∥,
B⊥′ = γ

B⊥−v × E
c02

.
These can be combined to give
E′ = γ

E −γ −1
γ
v · E v
v2
+ v × B

,
B′ = γ

B −γ −1
γ
v · B v
v2
−v × E
c02

.
Thus, the components of the electromagnetic ﬁeld parallel to the velocity v remain
unmodiﬁed, but not the perpendicular ones. In particular, in the non-relativistic limit
γ ≈1, it follows that
E′ ≈E + v × B ,
B′ ≈B −v × E
c02
.
(Note that the term v × B is well known, but not v × E/c02 due to technical limi-
tations: we can produce strong magnetic ﬁelds, but strong electric ﬁelds are found
only in the interior of atoms—because it is actually c0B that should be compared

244
3
Electromagnetism
with E in order to have equal units, i.e., 1 T = 3 MV/cm, we should have considered
E + β × c0B rather than c0B −β × E.) Therefore, on a slowly moving electric point
charge q an electromagnetic ﬁeld acts with the Lorentz force
F = q (E + v × B) ,
and on a moving magnetic moment m, an electric ﬁeld acts, because F = ∇m · B
leads to
F = ∇m ·

B −v × E
c02

.
In particular for a radially symmetric central ﬁeld, we have
E = −∇	 = −d	
dr
r
r ,
and hence v × E = r−1 (d	/dr) (r × v). We thus arrive at the spin–orbit coupling,
because there is a magnetic moment associated with a spin and r × v with an orbital
angular momentum. According to this derivation, this is not a relativistic effect,
despite what is often claimed.
Correspondingly, we can now establish the transformation properties of D and H
from the behavior of the tensor G, which is the same as that of the tensor F. We only
need to replace E by c02 D and B by H, which yields
D′ = γ

D −γ −1
γ
v · D v
v2
+ v × H
c02

,
H′ = γ

H −γ −1
γ
v · H v
v2
−v × D

.
For the reverse transformation from the primed to the unprimed system, v is simply
replaced by −v, giving
E = γ

E′ −γ −1
γ
v · E′ v
v2
−v × B′ 
.
Here the components of E, B, D, and H along v remain unchanged.
3.4.9
Relativistic Dynamics of Free Particles
From the velocity we derive the (mechanical) momentum:
(pμ) ≡m (vμ) = mγ (c0, v) ,
with
pμ pμ = (mc0)2 .

3.4 Lorentz Invariance
245
Here, m stands for the mass (a relativistic invariant), often called the rest mass, while
mγ = m/

1 −β2 is called the relativistic mass, even though the factor γ belongs
solely to the velocity—without it the velocity of light would not be the same in all
inertial frames. It is thus a kinematic factor and has nothing to do with the mass. The
zeroth component p0 is connected to the energy:
p0 = E
c0
=⇒
E = mγ c0
2 .
Note that the concept of the position–momentum pair corresponds to the time–energy
pair (we neglect the potential energy and consider only free particles). The total
energy E is composed of the rest energy mc02 and the kinetic energy
T = E −mc0
2 = m (γ −1) c0
2 = 1
2 mv2 + · · · .
By specifying the rest energy, we set the zero-point of the energy, so that is no longer
arbitrary. We have thus set:
(pμ) =
 E
c0
, p

,
with
E = mγ c0
2
and p = mγ v .
With pμ pμ = (mc0)2, we conclude that (E/c0)2 −p · p = (mc0)2, or again, restrict-
ing ourselves to the positive square root,
E = c0

(mc0)2 + p · p .
From the previous pair of equations we conclude
p = E
c02 v .
(This holds for all m ̸= 0, and hence we assume it also for m = 0.) For m ̸= 0 and
with v →c0 so that γ →∞, E and p also increase beyond all limits. In contrast,
for m = 0, the relation pμ pμ = (mc0)2 yields E = c0 p. This leads us to the unit
vector p/p = v/c0: in every inertial frame, massless particles move with the velocity
of light.
In order to derive the Lagrange function for free particles, we use the integral
principles (see Sect. 2.4.8) and take into account the fact that the proper time τ
(but not the coordinate time t) is Lorentz invariant. According to p. 230, we have
dt = γ dτ. Now Hamilton’s principle states that the action function
W =
 t1
t0
L dt =
 τ1
τ0
γ L dτ

246
3
Electromagnetism
takes an extreme value. This must be valid for all reference frames. Consequently,
γ L must be Lorentz invariant. (However, we shall not introduce an abbreviation for
γ L). As explained on p. 250, L is connected to the Lagrange density L as used on
p. 241. For free particles this function depends on the four-velocity, but not on the
space-time coordinates. Then we only have to ﬁnd out how γ L depends on vνvν.
Hence we investigate the ansatz γ L = m f (vνvν), bearing in mind the requirement
mvμ = pμ = −∂γ L
∂vμ
.
We already had the ﬁrst equation at the beginning of this section. The second connects
two contravariant quantities and generalizes p = ∇vL (see p. 99) with vk = −vk to
four dimensions. With ∂vνvν/∂vμ = 2vμ, this requirement can be satisﬁed by any
function f with d f/d(vνvν) = −1/2. However, because vνvν = c02, this does not
seem to be unique. Hence the Lagrange function is often derived from Fermat’s
principle, valid for free particles according to p. 141, or the geodesic principle,
δ
 t1
t0
dt = 0 ,
or δ
 s1
s0
ds = 0 .
(For free particles, the velocity is constant, so the two expressions yield the same
orbit.) If now σ increases monotonically with the proper time τ, but otherwise is an
arbitrary parameter, we have
ds =

gμν
dxμ
dσ
dxν
dσ dσ .
Here the coordinates xμ and their derivatives can be varied. Since the parameter σ
does not need to be equal to the proper time, the inconvenient condition vνvν = c02
does not apply for the variation. On the other hand, it may be equal to the proper time,
and then the expression under the square root is equal to vνvν. Consequently, γ L is
equal to the square root of vνvν, up to a ﬁxed factor, and this factor we derive from
the requirement that, in the non-relativistic limit, we should have L ≈T +const. with
T = 1
2mv · v:
L = −mc0
γ
√vνvν = −mc0

c02 −v · v ≈−mc0
2 + 1
2mv2 .
Since here (for free particles) the Lagrange function does not depend on the
space-time coordinates, the Euler–Lagrange differential equation (p. 96) yields
dpμ
dτ
= 0 ,

3.4 Lorentz Invariance
247
Fig. 3.29 Lagrange function and momentum of free particles as a function of β = v/c0: non-
relativistic (dashed blue) and relativistic (continuous red)
and hence also the energy and momentum conservation law for free particles (see
Fig. 3.29).
3.4.10
Relativistic Dynamics with External Forces
In classical mechanics (see Sect. 2.3.4), we have already derived the generalized
potential U for the interaction of a particle of charge q with an electromagnetic
ﬁeld, namely, U = q (	 −v · A). After multiplying by γ , this expression is Lorentz
invariant:
γ q (	 −v · A) = q vμAμ .
Here Aμ depends only on the space-time coordinates xμ, but not on vμ. Hence we
obtain the Lagrange function
L = −mc0
√vμvμ + qvμ Aμ
γ
.
This yields the canonical conjugate momentum
pμ = −∂γ L
∂vμ
= mvμ + q Aμ .
We have already considered its three-space components on p. 99, though not yet rela-
tivistically, and distinguished between the mechanical momentum and the canonical
conjugate momentum. Its time component p0 is related to the energy E = c0 p0,
which now (with suitable gauge, see p. 124) also contains the potential energy q	,
with A0 = 	/c0, according to p. 239.

248
3
Electromagnetism
Important for the Lagrange equations is
dpμ
dτ
= m dvμ
dτ + q dAμ
dτ
,
with
dAμ
dτ
= vν ∂ν Aμ ,
because with ˙p = ∇L, this must be equal to −∂μ γ L. With the above expression for
the Lagrange function, it follows that −∂μ γ L = q vν ∂μAν. Then we arrive at the
electromagnetic ﬁeld tensor (see p. 240)
dpμ
dτ
= −∂γ L
∂xμ
=⇒
m dvμ
dτ = qvν Fμν .
Here, for μ = 0, vν Fμν is equal to (−γ v) · (−E/c0) = γ v · E/c0, and the space
components can be combined into the three-vector γ (E + v × B). Fμ = qvν Fμν is
referred to as the Minkowski force:
Fμ ≡m dvμ
dτ .
Its space components are greater by the factor γ than those of the Newtonian force.
Its time component is related to the power γ j · E.
The last equation also holds for forces other than electromagnetic ones.
3.4.11
Energy–Momentum Stress Tensor
We would like now to extend Maxwell’s stress tensor to four dimensions. To this
end, we go from the Minkowski force Fμ = qvν Fμν over to a force density:
f μ = jν Fμν .
With μ0 jν = ∂κ Fκν, we have μ0 f μ = (∂κ Fκν) Fμν = ∂κ (Fκν Fμν) −Fκν ∂κ Fμν.
The last term can be rewritten, because F is antisymmetric and the summation
indices κ and ν may be renamed:
Fκν ∂κ Fμν = −1
2 Fκν (∂ν Fμκ + ∂κ Fνμ) .
It is then simpliﬁed using the Maxwell equations:
Fκν ∂κ Fμν = 1
2 Fκν ∂μFκν = 1
4 ∂μFλν Fλν .
Consequently, with ∂μ = gμ
κ∂κ, we ﬁnd μ0 f μ = ∂κ (Fκν Fμν −1
4 gμ
κ Fλν Fλν).
Therefore, the force density f μ is the (four-dimensional) source density of a sym-
metric tensor:

3.4 Lorentz Invariance
249
f μ = −∂κT κμ ,
with
T κμ ≡
1
4 gκμ Fλν Fλν −Fκ ν Fμν
μ0
= T μκ .
If we restrict ourselves to D = ε0E and B = μ0H, then we can extend Maxwell’s
stress tensor, introduced on p. 215, with elements Txx = w −ε0Ex Ex −μ0Hx Hx
and Txy = −ε0Ex Ey −μ0Hx Hy (and cyclic permutations), with the energy density
w = 1
2 (E · D + H · B) and the Poynting vector S = E × H, into four dimensions:
 
T μν!
=
⎛
⎜⎜⎝
w
Sx/c0 Sy/c0 Sz/c0
Sx/c0
Txx
Txy
Txz
Sy/c0
Tyx
Tyy
Tyz
Sz/c0
Tzx
Tzy
Tzz
⎞
⎟⎟⎠,
with trT =
	
μ
Tμ
μ = 0 .
The stress tensor known from the static case is now completed with the Poynting
vector and the energy density. According to p. 215, S/c02 is a momentum density,
whence T is referred to as the energy–momentum stress tensor. Its space components
are
f i + 1
c02
∂Si
∂t + ∂T ik
∂xk = 0 .
Inaddition, f 0 = jν F0ν = j · E/c0,so−j · E = c0 ∂κT κ0 = ∂tw + ∇· S.Wealready
know this equation (p. 211) as Poynting’s theorem.
3.4.12
Summary: Lorentz Invariance
Maxwell’s equations ensure the same vacuum velocity of light in all inertial frames:
the laws of electromagnetism are Lorentz invariant. The space-time description must
be adjusted to this fact, something that leads to unusual consequences for high veloc-
ities. Just as time and space have to be combined to give xμ = (c0t, r), so also do
charge and current density to give jμ = (c0ρ, j), energy and momentum to give
pμ = (E/c0, p), scalar and vector potential to give Aμ = (	/c0, A), and angular
frequency and wave vector to give kμ = (ω/c0, k). By building skew-symmetric ten-
sors Fμν and Gμν from E/c0 and B and from c0D and H, respectively, the pairs of
Maxwell equations for microscopic electromagnetism can each be combined into
one equation, viz.,
∂λFμν + ∂μFνλ + ∂ν Fλμ = 0
and
∂μFμν = μ0 jν ,
and those for macroscopic electromagnetism into
∂λFμν + ∂μFνλ + ∂ν Fλμ = 0
and
∂μGμν = j
ν .

250
3
Electromagnetism
In addition, the equation f ν = −∂μT μν with the (symmetric) energy–momentum
stress-tensor
T μν =
1
4 gμν Fκλ Fκλ −Fμκ Fνκ
μ0
combines Poynting’s theorem and the relation between force density and Maxwell’s
stress tensor.
Lorentz invariance leads to the fact that, in classical mechanics, derivatives with
respect to time must be replaced by derivatives with respect to the proper time,
thereby introducing the factor γ . In particular, for free particles of mass m, we have
the momentum (pμ) ≡m(vμ) = mγ (c0, v) with p0 = E/c0, or E = mγ c02 and
p = c0−2 E v, and otherwise for particles with the charge q,
pμ = −∂γ L
∂vμ
= m vμ + q Aμ .
In the expression (pμ) = mγ (c0, v), the factor γ belongs to the velocity, not to the
mass—this is a Lorentz invariant, as is vμvμ, but only because of the factor γ . There
is no “velocity-dependent mass” (see L.B. Okun: Phys. Today 42, 6 (1989) 31–36.)
3.4.13
Supplement: Hamiltonian Formalism for Fields
On p. 241 the Lagrange function known from the mechanics of particles was extended
to the Lagrange density L for the electromagnetic ﬁeld. Here we present the tran-
sition to the Hamiltonian formulation, which is often applied to ﬁeld quantization,
even though there are other ways to derive the latter, as we shall see in Sect. 5.5.2.
After introducing the Lagrange density L , Hamilton’s principle reads
δ

d4xμ L (xμ, η, ∂η
∂xμ ) = 0 ,
with

d3xk L = L ,
where the coordinates xμ are given and the parameter (or parameters) η of the system
are to be varied. For the electromagnetic ﬁeld, η is equal to the four-potential A.
Therefore, with the Einstein summation convention, we may set
δL = ∂L
∂η δη +
∂L
∂(∂μη) δ(∂μη) ,
using the abbreviation ∂μη ≡∂η/∂xμ. We may change the order of the derivative
with respect to xμ and the variation, i.e., δ(∂μη) = ∂δη/∂xμ, and integrate by parts.
However, here η depends not only on the single coordinate xμ, but also on the three
remaining ones, and therefore the implicit dependence of the ﬁeld quantity η on the

3.4 Lorentz Invariance
251
xμ must also be accounted for, although in many textbooks there is only the partial
derivative instead of the total derivative in the next equation:

dxμ
∂L
∂(∂μη)
∂δη
∂xμ =
∂L
∂(∂μη) δη −

dxμ
d
dxμ
 ∂L
∂(∂μη)

δη ,
where there is of course no summation over μ. Since η is to be kept ﬁxed at the
integration limits during the variation, the ﬁrst term on the right-hand side vanishes.
Hence Hamilton’s principle appears in the form

d4xμ ∂L
∂η −
3
	
μ=0
d
dxμ
∂L
∂(∂μη)

δη = 0 ,
and we obtain the Euler–Lagrange equations
∂L
∂η =
d
dxμ
∂L
∂(∂μη) ,
with Einstein’s summation convention. This we may also write as
d
dt
∂L
∂˙η = ∂L
∂η −
d
dxk
∂L
∂(∂kη) .
The similarity with the usual equation appears more clearly if we use the Lagrange
function L instead of the Lagrange density L , but now take it as a functional of the
functions η and ˙η, introducing the functional derivatives
δL
δη ≡∂L
∂η −
d
dxk
∂L
∂(∂kη)
and
δL
δ˙η ≡∂L
∂˙η .
If we divide space into N cells and discretize to give L = N
i=1 Li 
Vi, it follows
that
δL =
N
	
i=1
∂L
∂η −
d
dxk
∂L
∂(∂kη)

i δηi + ∂L
∂˙η δ˙ηi


Vi .
Since the variations δηi and δ˙ηi with i ∈{1, . . . N} may be performed independently
of each other, the limit 
Vi →0 can be considered separately for each cell. The
functional derivative δL /δη still contains a factor (
V )−1. Therefore the Lagrange
density L appears on the right. Hence the result reads simply
d
dt
δL
δ˙η = δL
δη ,

252
3
Electromagnetism
which is similar to the normal Lagrange equation. However, because of the functional
derivatives, we are now dealing with a differential equation, from which we must
now determine η(t, r) rather than x(t)—instead of the coordinates x (possibly very
many, but nevertheless a ﬁnite number), a whole ﬁeld must now be determined.
The quantity canonically conjugate to the ﬁeld quantity ηi in the volume 
Vi is
pi ≡∂L
∂˙ηi
= 
Vi
∂L
∂˙ηi
= 
Vi πi ,
with the momentum density
π ≡∂L
∂˙η = δL
δ˙η ,
where πi is its mean value in the volume 
Vi. If we go over from the Lagrangian to
the Hamiltonian mechanics then, with H (xμ, η, π, ∂kη), we also have
dH =

d3xk ∂H
∂t
dt + ∂H
∂η dη + ∂H
∂π dπ + ∂H
∂(∂lη) d(∂lη)

.
We integrate the last term by parts (without the summation convention):

dxl
∂H
∂(∂lη) d
 ∂η
∂xl

= ∂H
∂(∂lη) dη −

dxl
d
dxl
∂H
∂(∂lη) dη .
The integrated term vanishes if the considered system exists only in a ﬁnite volume,
as we have assumed. Hence,
dH =

d3xk ∂H
∂t
dt +
∂H
∂η −d
dxl
∂H
∂(∂lη)

dη + ∂H
∂π dπ

,
with the summation convention. Instead of the round bracket, we may also write the
functional derivative δH/δη. On the other hand, the relation
dH =

d3xk 
π d ˙η + ˙η dπ −δL
δη dη −δL
δ˙η d ˙η −∂L
∂t
dt

follows from H = π ˙η −L with π = ∂L/∂˙η. Here, since π = δL/δ˙η, the ﬁrst
term cancels the fourth. If we also use δL/δη = ˙π, then we may set
dH =

d3xk 
−∂L
∂t
dt −˙π dη + ˙η dπ

,
Comparing with the expression found above, we obtain the Hamilton equations for
a ﬁeld, viz.,

3.4 Lorentz Invariance
253
∂H
∂t
= −∂L
∂t
,
δH
δη = −˙π ,
and
δH
δπ = ˙η ,
because H does not depend on the spatial derivatives of π, and therefore δH/δπ =
∂H /∂π.
The Hamilton function H is a conserved quantity if dH/dt vanishes. Clearly,
dH
dt =

d3xk ∂H
∂t
,
because dη/dt = ˙η and dπ/dt = ˙π cancels the remaining terms of the integrand.
The time dependence of an arbitrary quantity O can be obtained from
dO
dt =

d3xk δO
δη ˙η + δO
δπ ˙π

+ ∂O
∂t
=

d3xk δO
δη
δH
δπ −δO
δπ
δH
δη

+ ∂O
∂t = [O, H] + ∂O
∂t .
For the last equation we have extended the concept of the Poisson bracket to ﬁelds,
as an abbreviation for the preceding integral.
The Poisson bracket [ηi, pi] = 1 of particle physics has become [ηi, πi] = 1/
Vi
in ﬁeld theory. For the limit 
Vi →0,
[η(t, r), π(t, r ′)] = δ(r −r ′) ,
and after a Fourier transform [η(t, k), π(t, k′)] = δ(k −k′).
3.5
Radiation Fields
3.5.1
Solutions of the Inhomogeneous Wave Equations
Now we turn to the potential equations of microscopic electromagnetism from
Sect. 3.4.6 (with the Lorentz and Coulomb gauges):
□Aμ = μ0 jμ ,
or □Aμ = μ0 jμ
trans .
Here the inhomogeneities may also depend on time, because otherwise we just obtain
the already known static solutions. We solve both equations with the same Green
function, since they involve the same differential operator □and differ only in the
inhomogeneity. This Green function generalizes the expression (for the Laplace
operator 
) known from statics. In particular, it takes into account the fact that space
and time are connected with each other via the velocity of light c0:

254
3
Electromagnetism
□δ(t′ −t ± |r −r ′|/c0)
4π|r −r ′|
= δ(t −t′) δ(r −r ′) .
So far we have considered the limit c0 →∞(□→−
) and we were therefore
allowed to omit the delta function δ(t −t′) on the left- and right-hand sides. We
shall use only the Green function with the plus sign: the source at the position r ′ acts
at the chosen point r after the lapse of time t −t′ = |r −r ′|/c0. This is called the
retarded solution. The Green function with the minus sign is known as the advanced
solution. It is mathematically but not physically allowable, because effects then occur
before their cause.
Before proving the validity of these Green functions, we ﬁrst show their Lorentz
invariance. If we use δ{(xμ −xμ′)(xμ −x′μ)} = δ{(c0t −c0t′)2 −|r −r ′|2} and
take into account the equation on p. 20, viz.,
δ{(c0
t)2 −|
r |2} = δ(
t −|
r |/c0) + δ(
t + |
r |/c0)
2 c0 |
r |
,
it follows that
δ(t′−t ± |r−r ′|/c0)
|r−r ′|
= 2c0 ε{±(t−t′)} δ{(xμ−xμ
′)(xμ−x′μ)} .
Here, the step function ε{±(t−t′)} seems to violate Lorentz invariance, but we wish
to distinguish uniquely between past and future, and therefore restrict ourselves
to the retarded solutions, that is, to proper Lorentz transformations. Forwards and
backwards light cones then remain separated.
For the actual proof, we use the Fourier representation of the delta function (see
p. 21) with R ≡r −r ′ and k = ω/c0, i.e.,
□δ(t′ −t ± R/c0)
R
= □1
2π
 ∞
−∞
dω exp{iω(t′ −t ± R/c0)}
R
= 1
2π
 ∞
−∞
dω exp{iω(t′ −t)}

−ω2
c02 −
exp(± ikR)
R
.
The d’Alembert operator in the “time representation” then becomes −(
 + k2) in
the “frequency representation”. Now in the general case (the special case k = 0 was
already considered on p. 26)
(
 + k2) exp(± ikR)
R
= −4π δ(R) .
According to p. 39, for R ̸= 0, the left-hand side is equal to R−1(∂2/∂R2 +
k2) exp(± ikR), hence zero. However, for R = 0, it is singular, and its volume inte-
gral, according to p. 27, is equal to −4π.

3.5 Radiation Fields
255
Thus, we have for the Lorentz gauge,
Aμ(t, r) = μ0
4π

dt′ dV ′ jμ(t′, r ′) δ(t′ −t + |r −r ′|/c0)
|r −r ′|
= μ0
4π

dV ′ jμ(t −|r −r ′|/c0, r ′)
|r −r ′|
as the retarded solution. The continuity equation already ensures the gauge condition
∂μAμ = 0. If we use
 ∞
−∞dt exp(iωt) δ(t′ −t + R/c0) = exp(iωt′) exp(ikR) for
the Fourier transform, we obtain the expression
Aμ(ω, r) =
1
√
2π
 ∞
−∞
dt Aμ(t, r) exp(iωt) = μ0
4π

dV ′ jμ(ω, r ′) exp(ik|r−r ′|)
|r−r ′|
.
Note that we take exp(iωt) and not exp(−iωt), since that leads us to ωt −k · r =
kμxμ—of course, jμ(ω, r) is related to jμ(t, r) via the same Fourier transform.
Hence the source density is easy to determine, since ∇f (|r −r ′|) = −∇′ f (|r −
r ′|):
∇· A(ω, r) = −μ0
4π

dV ′ j (ω, r ′) · ∇′ exp(ik|r −r ′|)
|r −r ′|
.
With j · ∇′G = ∇′ · Gj −G ∇′ · j, we can split the integral into two terms. The
ﬁrst can be converted according to Gauss into a surface integral and does not con-
tribute, since j vanishes on the surface, while the second can be rewritten with
the continuity equation, because using ρ(t, r) ∝ρ(ω, r) exp(−iωt) and j (t, r) ∝
j (ω, r) exp(−iωt), it reads
∇· j (ω, r) = iω ρ(ω, r) = iω
c0
j0(ω, r) .
Consequently,
∇· A(ω, r) = iω
c0
A0(ω, r) ,
and hence also ∂μAμ = 0. In the given expression for Aμ, the continuity equation
∂μ jμ = 0 already ensures the Lorentz gauge.
For the derivation of ∂μAμ = 0, the current density must vanish on the surface of
the integration volume. For the Coulomb gauge, only the transverse current density
is of interest (transverse gauge): then it is already sufﬁcient that the current density
should not have a normal component there. Then the source freedom for the Fourier
transformed A(ω, r) is easily checked. As in Sect. 3.2.8, we use Gauss’s theorem,
assuming no current density at inﬁnity and the source freedom of jtrans. As A(ω, r) is

256
3
Electromagnetism
solenoidal, this is true also for A(t, r). In the Coulomb gauge, because 
	 = −ρ/ε0,
we have
	(t, r) =
1
4πε0

dV ′ ρ(t, r ′)
|r −r ′| ,
A(t, r) = μ0
4π

dt′ dV ′ jtrans(t′, r ′) δ(t′ −t + |r −r ′|/c0)
|r −r ′|
,
and after a Fourier transform
	(ω, r) =
1
4πε0

dV ′ ρ(ω, r ′)
1
|r −r ′| ,
A(ω, r) = μ0
4π

dV ′ jtrans(ω, r ′) exp(ik|r −r ′|)
|r −r ′|
.
We would like to use these expressions for the radiation ﬁelds, which is why the
Coulomb gauge is often also called radiation gauge. The fact that the radiation is
transverse is more important for us than Lorentz invariance. For this reason, the
radiation gauge is also used in quantum electrodynamics (see Sect. 5.5.1).
3.5.2
Radiation Fields
For the magnetic ﬁeld, with B(ω, r) = ∇× A(ω, r), we obtain
B(ω, r) = −μ0
4π

dV ′ jtrans(ω, r ′) × ∇exp(ik|r −r ′|)
|r −r ′|
,
with
∇exp(ik|r −r ′|)
|r −r ′|
=

ik −
1
|r −r ′|
 exp(ik|r −r ′|)
|r −r ′|
r −r ′
|r −r ′| .
We thus have two terms with different position dependence. For time-dependent
problems (thus k ̸= 0), the ﬁeld decays more weakly with the distance from the
source than in the static case. This is also shown by the representation
B(t, r) =
1
√
2π
 ∞
−∞
dω B(ω, r) exp(−iωt) ,
because we have

3.5 Radiation Fields
257
Fig. 3.30 The approximation |r −r ′| ≈r −er · r ′ valid for r ≫r′ follows by calculation (using
a series expansion) as well as geometrically. The circular radius |r −r ′| and the double-headed
arrow are nearly equally long
1
√
2π
 ∞
−∞
dω jtrans(ω, r ′) e−iωt eikR = jtrans(t −R/c0, r ′) ,
−i
√
2π
 ∞
−∞
dω ω jtrans(ω, r ′) e−iωt eikR = ∂jtrans(t −R/c0, r ′)
∂t
,
and hence, with t′ = t −|r −r ′|/c0,
B(t, r) = μ0
4π

dV ′∂jtrans(t′, r ′)
c0 ∂t
+ jtrans(t′, r ′)
|r −r ′|

×
r −r ′
|r −r ′|2 ,
for the magnetic ﬁeld. Previously, we took the derivative with respect to the position
instead of the time and thereby could not account explicitly for the ﬁnite propagation
velocity.
Since the current density is connected to the velocity, the part with the derivative of
j with respect to time is called the acceleration ﬁeld and the second the velocity ﬁeld.
With increasing distance from the source, the acceleration ﬁeld clearly contributes
most to B.
For the electric ﬁeld, we conclude from E = −∇	 −∂A/∂t that
E(ω, r)=−∇	(ω, r) + iωA(ω, r)
=−
1
4πε0

dV ′ 
ρ(ω, r ′) ∇
1
|r −r ′| −iω
c02 jtrans(ω, r ′) exp(ik|r −r ′|)
|r −r ′|

,
and thus after the Fourier transform ω →t
E(t, r) = −
1
4πε0

dV ′
ρ(t, r ′) ∇
1
|r −r ′| + ∂jtrans(t′, r ′)/∂t
c02 |r −r ′|

,
noting that we have t in the argument of the charge density, but t′ = t −|r −r ′|/c0
for the current density. Here, we write ﬁrst the longitudinal and then the acceleration
ﬁeld, even though the acceleration ﬁeld is more important for greater distances.
For large distances of the chosen point from the source (r ≫r′), we may set (see
Fig. 3.30)

258
3
Electromagnetism
k|r −r ′| ≈k (r −er · r ′) = kr −k ′ · r ′ ,
with k ′ = k er .
Then, using er = k ′/k,
4πε0 E(ω, r) ≈ik
c0
exp(ikr)
r

dV ′ jtrans(ω, r ′) exp(−ik ′ · r ′) ,
c0 B(ω, r) ≈er × E(ω, r) .
In agreement with Fig. 3.19, the vectors er, E, and B are mutually perpendicular to
each other for r ≫r′, because with er = k ′/k, we have
E(ω, r) · er ∝

dV ′ jtrans(ω, r ′) · k ′ exp(−ik ′ · r ′) ,
and also jtrans · k ′ exp(−ik ′ · r ′) ∝jtrans · ∇′ exp(−ik ′ · r ′), whence generally,
jtrans · ∇′G = ∇′ · Gjtrans −G ∇′ · jtrans .
Thus the volume integral vanishes, because there is no current at the surface and jtrans
is solenoidal.
In the following we shall often use the Fourier transform (see p. 25 and p. 255)
jtrans(ω, k) =
1
√
2π
3

dV exp(−ik · r) jtrans(ω, r)
=
1
(2π)2

dt dV exp{i(ωt −k · r)} jtrans(t, r) .
In particular, we have just obtained the electric ﬁeld strength at large distances, viz.,
E(ω, r) ≈
π
2
ik
ε0c0
exp(ikr)
r
jtrans(ω, k) ,
with k = k er ,
and also the magnetic ﬁeld with c0B(ω, r) ≈er × E(ω, r).
3.5.3
Radiation Energy
Now the Poynting vector can be related to the properties of the radiation source. To
this end, we make a Fourier expansion
E(t, r) =
1
√
2π
 ∞
−∞
dω exp(−iωt) E(ω, r) ,

3.5 Radiation Fields
259
(likewise for H) and obtain for the Poynting vector S = E × H integrated over the
time (according to the Parseval equation on p. 23)
 ∞
−∞
dt S(t, r) =
 ∞
−∞
dω E∗(ω, r) × H(ω, r) ,
because E(t, r) and H(t, r) are real functions. Hence, with E(ω, r) = E∗(−ω, r)
and H(ω, r) = H∗(−ω, r), we ﬁnd
 ∞
−∞
dt S(t, r) = 2Re
 ∞
0
dω E∗(ω, r) × H(ω, r) .
Far from the radiation source, i.e., beyond any magnetization, so H = B/μ0, the last
section now yields
E∗(ω, r) × H(ω, r) ≈
1
μ0c0
E∗(ω, r) · E(ω, r) er ≈π
2
k2
ε0c0


jtrans(ω, k)


2 r
r3 .
With k2/ε0 = μ0ω2, the Poynting vector integrated over all times is therefore asymp-
toticallyequaltoπμ0 r/(c0r3)
 ∞
0 dω ω2 |jtrans(ω, k)|2.Theenergy(injoule)ﬂowing
into the solid angle element d = r · df/r3 is therefore
dW = df ·
 ∞
−∞
dt S(t, r) = πμ0
c0
d
 ∞
0
dω ω2 

jtrans(ω, k)


2 ,
with k = k er. Here jtrans is the solenoidal part of the current density, for which,
according to Sect. 1.1.11, we may also write jtrans(ω, k) = ek × {j(ω, k) × ek} with
ek = k/k. Hence |jtrans(ω, k)|2 = |j (ω, k) × k/k|2.
If the frequency range is very sharp, then it is best to work with a single angular
frequency ω. However, the time integrals then diverge. For a continuous radiation
source, we should consider the radiation power averaged over a period: E(t, r) =
Re {E(ω, r) exp(−iω t)} and the corresponding expression for H(t, r) lead to
S = ω
2π
 2π/ω
0
dt E(t, r) × H(t, r) = Re E∗(ω, r) × H(ω, r)
2
≈π
4
μ0ω |jtrans(ω, k)|2
r2
k .
Therefore, for the average radiation power, we obtain
d ˙W = S · df ≈πμ0
4c0
ω 2 

jtrans(ω, k)


2 d .
This generally depends on the direction of k—some examples are given in Sect. 3.5.5.

260
3
Electromagnetism
3.5.4
Radiation Fields of Point Charges
For point charges q, the Fourier transform with respect to frequencies does not make
sense. Here, it is better to use

dV ′ jμ(t′, r ′) = qvμ(t′, r ′)
γ
for the four-potential Aμ(t, r). The factor γ −1 is necessary because of the Lorentz
contraction. According to p. 255 and with r ′ as a unique function of t′, the Lorentz
gauge is
Aμ(t, r) = μ0 q
4π

dt′ vμ(t′, r ′)
γ
δ(t′ −t + |r −r ′|/c0)
|r −r ′|
.
For the delta function, we use the abbreviation
R ≡r −r ′ ,
e ≡R/R ,
β ≡v/c0 ,
and set u ≡t′ −t + R/c0. Then ∂R/∂t′ = −v and ∂R/∂t′ = −v · e, so we have
du/dt′ = 1 −β · e, implying dt′ = du/(1 −β · e). Then, because dt′ δ(t′ −t +
R/c0) = du δ(u)/(1 −β · e) for the Lorentz gauge, we ﬁnd the Liénard–Wiechert
potential
Aμ(t, r) = μ0 q
4π
vμ(t −R/c0, r −R)
γ (R −β · R)
.
For the corresponding equations 	 ∝c0 and A ∝v, we have (Aμ) = (c0−1	, A)
and (vμ) = γ (c0, v). The factor γ then cancels out. The (retarded) ﬁelds spread
with ﬁnite velocity, and therefore depend on Aμ and vμ at different times, depending
upon the distance R (see Fig. 3.31). Here, with (βν) = γ (1, β) and (Rν) = (c0t −
c0t′, r −r ′), γ (R −β · R) can also be written as the scalar product βν Rν. This does
not depend on the reference frame—since emitter and receiver move against each
other, R alone would not make sense. In fact, information is not radiated evenly in
all space directions, but preferentially in the direction of the motion.
For the ﬁelds E = −∇	 −∂A/∂t and B = ∇× A , we also have to take into
account the retardation effect. Instead of the derivative ∂/∂t, we should take the
derivative ∂/∂t′, and for ∇, keep t′ (but not t) ﬁxed. If, as in Sect. 1.2.7, we indicate
the ﬁxed quantity by a subscript on the differential or operator in brackets, we have
(∇)t = (∇)t′ + (∇t′)t ∂/∂t′. In order to ﬁnd (∇t′)t, we determine the action on
R = c0(t −t′). This is (∇R)t = −c0(∇t′)t and (∇R)t′ = e, whence
(∇)t = (∇)t′ −
e
(1 −β · e) c0
∂
∂t′ ,

3.5 Radiation Fields
261
Fig. 3.31 The existence of a point charge (•) in the spherical shell becomes noticeable only after
the time 
t = R/c0. During this time the point charge will already have displaced by v
t, but that
becomes observable only later on the spherical shell. The associated Lorentz invariant {γ (R −β ·
R)}−1 = 1/βν Rν is sketched here as a continuous line for β = 1
2, which is the weight factor in the
Liénard–Wiechert potential
and
∂R
∂t = c0

1 −∂t′
∂t

= ∂R
∂t′
∂t′
∂t = −v · e ∂t′
∂t
=⇒
∂t
∂t′ = 1 −β · e .
From the above expression for 	 and A (independent of the gauge) and with ∇(β ·
R) = β, ∂R/(c0∂t′) = −β · e, and ∂(β · R)/(c0∂t′) = ˙β · R/c0 −β2, we obtain
E(t, r) =
q
4πε0
1
(R −β · R)3
R −Rβ
γ 2
+ R × {(R −Rβ) × ˙β
c0

,
c0B(t, r) = e × E(t, r) .
The second term here decays more weakly by one power of R than the ﬁrst, but
occurs only for accelerated charges: it describes the acceleration ﬁeld, and the ﬁrst
the velocity ﬁeld. On the right here, all quantities are to be evaluated at the retarded
position of the charge. The magnetic ﬁeld is always perpendicular to the electric
ﬁeld.
3.5.5
Radiation Fields of Oscillating Dipoles
Let us now investigate a dipole oscillating with the angular frequency ω, with the
maximum dipole momentp. In the coordinates t and r, we may then replace j = ρv
by ˙p. In the equation for B(ω, r) from p. 256, we therefore use the expression −iωp
as the Fourier component of j (ω, r):
B(ω, r) = iμ0ω
4π

ik −1
r

p × er
exp(ikr)
r
.

262
3
Electromagnetism
The magnetic ﬁeld is thus perpendicular to r and p. For p = p ez, B(ω, r) has only
a ϕ component (proportional to p sin θ). From ε0 ∂E/∂t = ∇× B/μ0, we then also
have the associated electric ﬁeld (outside the origin):
E(ω, r) = i c02
ω
∇× B(ω, r)
=
1
4πε0

k2 + ik
r −1
r2

p −

k2 + 3ik
r
−3
r2

p · er er
 exp(ikr)
r
.
With p = p ez, this vector has an r and a θ component.
We derive the picture of the ﬁeld lines from dr × E = 0, because dr must have
the direction of E and may be written er dr + eθ r dθ, then express E ∝∇× B in
spherical coordinates (see p. 39). Since for our choice of the dipole direction, B has
only a ϕ component, we ﬁnd (independently of time)
∂
∂r (r sin θ Bϕ) dr + ∂
∂θ (r sin θ Bϕ) dθ = 0 .
This differential equation has the solution r sin θ Bϕ = const., where according to
the ﬁrst equation of this section, Bϕ is complex, and so also is the constant. This
result is exact and does not rely on approximations—in particular this is not par-
titioned into near-, middle-, and far-zone, which would be useless in our search
for zeros. If we now split into real and imaginary parts and set ρ = kr, then,
because r Bϕ ∝(i −ρ−1) sin θ eiρ, we ﬁnd sin−2 θ ∝cos ρ −sin ρ/ρ and sin−2 θ ∝
sin ρ + cos ρ/ρ. The spherical shells with ρ = tan ρ or ρ = −cot ρ = tan(ρ + π
2 )
thus belongtotheset of solutions: therethecurl densities reverse⟳↔⟲and, accord-
ing to the induction law, so also does ∂B/∂t, which means that B is extremal there.
It vanishes everywhere on the axis θ = 0 in the direction of the dipoles. Figure 3.32
shows the electric ﬁeld lines at two times, and Fig. 3.33 the magnetic ﬁeld lines at
the same times.
The distance between the spheres decreases continuously and is only equal to
λ/2 in the far-zone, because the factor i −ρ−1 can also be written in the form
i

1 + ρ−2 exp(i arctan ρ−1), leading to the spherical harmonics of ρ + arctan ρ−1.
3.5.6
Radiation Power for Dipole, Braking, and Synchrotron
Radiation
For the radiation power at sufﬁciently large distances from the source, only the
acceleration ﬁeld is of interest. According to p. 261 for point-like sources, it is given
by
E ≈μ0 q
4π
{˙v × (e −β)} × e
(1 −β · e)3 R
,
and B ≈e × E
c0
,

3.5 Radiation Fields
263
Fig. 3.32 Electric ﬁeld lines of the Hertz dipole at two times. At the circles (dotted lines), the curl
densities reverse ⟳↔⟲, and so also does ∂B/∂t, implying that B is extremal there
Fig. 3.33 Magnetic ﬁeld lines of the Hertz dipole at the same times as in Fig. 3.32, here in an
inclined view of the central plane. At the dotted lines, B is extremal again, while at the dashed
lines, the ﬁeld direction is reversed
with R = |r −r ′|. Then also
S ≈
E2
μ0c0
e ≈μ0
c0
 q
4π
2 {(˙v × (e −β)) × e }2
(1 −β · e)6 R2
e .
We shall use this expression (or d ˙W/d = R2 S · e ) for various examples.
In particular, for low velocities (v ≪c0, i.e., β ≪1), it follows that
E ≈μ0 q
4π
(˙v × e) × e
R
=⇒
S ≈μ0
c0
 q
4π
2 (˙v × e)2
R2
e ,
and thus for the radiation power into the solid angle element d,
d ˙W
d ≈μ0
c0
 q
4π
2
(˙v × e)2 .
The radiation thus depends on the angle between ˙v and R through sin2 θ. There-
fore, with 2π
 π
0 sin θ dθ sin2 θ = 2π
 1
−1 d cos θ (1 −cos2 θ) = 2
3 4π, the integra-
tion over all directions yields the Larmor formula

264
3
Electromagnetism
˙W = μ0 q2
6π c0
˙v · ˙v
for the total radiation power. Here ˙v is to be taken at the retarded time t′ = t −R/c0
(as are all the remaining quantities).
Due to this radiation power, the oscillation is damped. This is referred to as the
radiative reaction. In order to calculate it for a (nearly harmonic) oscillating charge
q with mass m, we use the results on p. 99 to relate the decay constant γ = α/(2m)
to the radiation power ˙W = α v2, obtaining γ = ˙W/(2mv2). Since the ratio of the
accelerationandvelocityamplitudes for aharmonicoscillationis givenbytheangular
frequency ω, we conclude that the decay constant is
γ =
μ0
6π c0
q2
2m ω2 .
This derivation assumes weak damping, viz., γ ≪ω. For electrons, ω ≪3 × 108
PHz must hold, and this is true even for visible light, where ω is a few PHz. Fourier
analysis supplies a not quite sharp frequency: the decay constant leads to a natural
line width. For heat motion, this is also modiﬁed by the Doppler effect.
The last equations are valid only for v ≪c0. This condition is always satisﬁed
for the oscillating dipole (with sufﬁciently small displacements). If p is its dipole
moment (at the retarded time), we have to set q ˙v = ¨p. For a harmonic oscillation
with angular frequency ω, ¨p = −ω2p, and with the maximum dipole momentp, we
ﬁnd, for the radiation power averaged over a period,
˙W =
μ0
12π c0
(ω2 p)2 ,
because the square of the spherical harmonics is on average equal to 1/2. The radi-
ation power (and thus the scattering power) increases as the fourth power of the
frequency. Applied to the scattering of sunlight in the air, since ωblue ≈2ωred, this
explains the blue sky and the red dawn and dusk.
Dipole radiation is linearly polarized. It oscillates in the plane spanned by ˙v and
e, and perpendicularly to e (transverse).
If we now give up the restriction to small velocities, then for the calculation of the
radiation power, we have to account for the retardation. In a time unit, energy is lost
at the rate −dW/dt′, while S is the energy current density at the position r and at
time t. Therefore, it still depends on dt/dt′ (see p. 261). Hence, with ˙W = dW/dt′,
we ﬁnd
d ˙W
d = μ0
c0
 q
4π
2 {(˙v × (e −β)) × e }2
(1 −β · e)5
.
We shall now consider this expression for two special cases.

3.5 Radiation Fields
265
Fig. 3.34 Polar diagram for braking radiation. Left: For β0 ≈0. Right: For β0 = 1
2. The arrow
speciﬁes the direction of the original velocity. The difference in size of the two pictures is to indicate
the intensity difference, even though it is not at all to scale, because in total the energy 1
2mv02 is
radiated off on the left, and (γ0 −1) mc02 on the right, with v02 ≪c02. The plane perpendicular
to v appears as a cone due to aberration, and is indicated by dashed lines. cos θ′ = 0 corresponds
to cos θ = β
To begin with we assume a longitudinal acceleration (deceleration) ˙v ∥v. Then
˙v × β vanishes, and we obtain
E ≈μ0 q
4π
(˙v × e) × e
(1 −β · e)3 R
and
S = μ0
c0
 q
4π
2
(˙v × e)2
(1 −β · e)6 R2 e .
The electromagnetic ﬁeld thus differs only from the one for v ≪c0 by the factor
(1 −β · e)−3. Consequently, the radiation into the forwards direction is even stronger
than expected non-relativistically.
Let us consider braking radiation (also known as deceleration radiation or
bremsstrahlung) as an example. Here, ˙v is constant: the velocity decreases at a con-
stant rate from v0 to zero. From dv/dt′ = ˙v, and hence ˙v2 dt′ = ˙v dv, the energy
radiated into the solid angle element is (see Fig. 3.34)
dW
d = μ0
c0
 q
4π
2
˙v sin2 θ
 v0
0
dv
(1 −v c0−1 cos θ)5
= μ0
4
 q
4π
2 ˙v sin2 θ
cos θ

1
(1 −v0 c0−1 cos θ)4 −1

.
Of course, this relation holds only for truly constant deceleration.
The linear polarization of braking radiation is given by E ∝(˙v × e) × e, as for
dipole radiation, thus in the plane spanned by v and e and perpendicular to e .
For synchrotron radiation, the acceleration is perpendicular to the velocity, i.e.,
˙v · β = 0. This leads to a radiation power
d ˙W
d = μ0
c0
 q
4π
2
1
(1 −β · e)3

˙v · ˙v −
(˙v · e)2
γ 2 (1 −β · e)2

,

266
3
Electromagnetism
Fig. 3.35 Polar diagrams of the synchrotron radiation. Left: For β0 ≈0. Right: For β0 = 1/2.
Continuous line: In the plane of the trajectory. Dotted line: Perpendicular to the trajectory. Dashed
lines: In-between in 15◦steps. The arrow speciﬁes the direction of v, and the plane perpendicular
to v is indicated. Here only the line intersecting the plane of the trajectory is important (compare
with Fig. 3.32, where the direction of the acceleration is likewise shown dashed)
once again more into the forward direction in comparison with the non-relativistic
limit.
The linear polarization of synchrotron radiation lies in the plane spanned by ˙v and
e −β, in particular, perpendicular to e, because
E ∝{˙v × (e −β)} × e = ˙v · e (e −β) −(1 −β · e) ˙v .
The particularly intense radiation in the tangential direction v (i.e., for e ⊥˙v ) has
ﬁeld strength (see Fig. 3.35)
E ≈−μ0 q
4π
˙v
(1 −β · e)2 R .
Here, the electric ﬁeld thus oscillates in the plane of the trajectory.
3.5.7
Summary: Radiation Fields
In this section we have investigated the coupling of the electromagnetic ﬁeld with
its generating sources, and to this end we have appropriately extended the solutions
known from the static cases. Here, retardation becomes important. The result has
been that the ﬁeld due to an accelerated charge decreases more weakly by one power
of the distance than for a uniformly moving (or resting) charge. At large distances,
only the acceleration ﬁeld is important for the radiation ﬁeld. Its properties have been
considered in the last section for various special cases.

Problems
267
Problems
Problem 3.1 Reformulate ∇(a · b) and ∇× (a × b) such that the operator ∇has
only one vector to the right of it (on which it acts). Here the intermediate steps should
be taken without components and the differential operator should treat both ac and
bc as constant, so that the product rule reads ∇(a · b) = ∇(a · bc) + ∇(ac · b), or
again ∇× (a × b) = ∇× (a × bc) + · · · . The equations a × (b × c) = b (c · a) −
c (a · b) = (c · a) b −(a · b) c need not be proven.
(4 P)
Problem 3.2 Using Cartesian components, determine ∇· r, ∇× r, and (a · ∇) r.
These results will be useful for the following problems.
(3 P)
Problem 3.3 Consider an arbitrary (three-times differentiable) scalar function ψ(r)
and the three vector ﬁelds ∇ψ, r × ∇ψ, and ∇× (r × ∇ψ). Which of them are
source-free and which curl-free? Determine the source and curl strengths as functions
of ψ. What is their inversion behavior (parity) if ψ(−r) = ψ(r)?
(9 P)
Problem 3.4 Prove

(V )(df · a) b =

V dV {b (∇· a) + (a · ∇) b} for arbitrary
ﬁelds a(r) and b(r) and show that the volume integral of a source-free vector ﬁeld
a is always zero, if a vanishes on the surface (V ).
(4 P)
Problem 3.5 For which function ψ(r) does the (spatial) central ﬁeld a(r) = ψ(r) r
have sources only at the origin? Does it have curls? Investigate this also for a
plane central ﬁeld. Represent the solutions as gradient ﬁelds (gradients of scalar
ﬁelds).
(3 P)
Problem 3.6 Let (
 + k2) ψ(r) = 0. How can we prove that the three vector ﬁelds
from Problem 3.3 satisfy the equation (
 + k2) a(r) = 0? Note the sources and curls
of the vector ﬁelds.
(4 P)
Problem 3.7 Determine the vector ﬁelds ∇(p · r/r3) and ∇× (r × p/r3) for con-
stant p (dipole moment) when r ̸= 0, and compare them.
(5 P)
Problem 3.8 Derive the singular behavior of the two vector ﬁelds for r = 0 from
the volume integral of a sphere around the origin. Express the results in terms of the
delta function.
(8 P)
Problem 3.9 Prove the representation of the Fourier transform of f (x) = g(x) h(x)
as a convolution integral given on p. 22.
(4 P)
Problem 3.10 For ﬁxed α, β, γ (with α > 0, β > 0, and 0 < γ < π), a rectilinear
oblique coordinate system x1, x2 is given by the two equations x1 = α (x −y cot γ )
and x2 = β y. Which functions y(x) describe the coordinate lines {x1, x2}? At what
angle do the coordinate lines cross? How do the basic vectors gi = and g i read as
linear combinations of the Cartesian unit vectors? How do the fundamental tensors
gik and gik read?
(7 P)

268
3
Electromagnetism
Problem 3.11 For spherical coordinates (r, θ, ϕ), we have to introduce position-
dependent unit vectors er, eθ, and eϕ in the direction of increasing coordinates.
Decompose these three vectors in terms of ex, ey, and ez. Determine their partial
derivatives with respect to r, θ, ϕ and express them as multiples of the unit vectors
er, eθ, eϕ.
(7 P)
Problem 3.12 Determine the covariant and contravariant base vectors {gi} and {g i}
as multiples of the unit vectors ei for spherical coordinates x1 = r, x2 = θ, and
x3 = ϕ.
(2 P)
Problem 3.13 With the help the Maxwell construction, draw the force lines of two
equally charged parallel lines with charges densities q/l and separated by a distance
a. This uses the theorem that, for a source-free ﬁeld, there is the same ﬂux through
any cross-section of a force tube. What changes with this construction for oppositely
charged parallel lines, i.e., charge densities ±q/l, separated by a distance a? Why
is the construction more precise than the method of drawing trajectories orthogonal
to the equipotential lines?
(8 P)
Problem 3.14 Determine the equation f (x, z) = 0 of the ﬁeld line of an ideal dipole
p = pez which lies at the origin (r ′ = 0). Note that, due to the cylindrical symmetry,
we may set y = 0.
(4 P)
Problem 3.15 On the z-axis there are several point charges qi at the positions zi.
Determine their common potential 	 by Taylor series expansion up to order (zi/r)3.
Examine the result for the potential when r ≫a (write 	 as a multiple of q1 = q)
for:
• a dipole (q1 = −q2, z1 = −z2 = 1
2a),
• a linear quadrupole (q1 = −1
2q2 = q3, z1 = −z3 = a, z2 = 0), and
• an octupole (q1 = −1
3q2 = + 1
3q3 = −q4, z1 = 3z2 = −3z3 = −z4 = 3
2a)?
Show that the ﬁeld of a ﬁnite dipole may be written approximately as a superposition
of a dipole ﬁeld and an octupole ﬁeld. How strong is the octupole ﬁeld compared
with the ﬁeld of a pure quadrupole? Justify with the examples above that an ideal
2n-pole can be viewed as a superposition of two 2n−1-poles.
(8 P)
Problem 3.16 Determine the potential and ﬁeld strength of a hollow sphere with
outer and inner radii R and ηR and a charge Q distributed evenly over its volume.
Here 0 ≤η ≤1, so a solid sphere has η = 0 and a surface charge η = 1. Sketch the
results 	(r) and E(r) in the limiting cases η = 0 and η = 1. In these limiting cases,
how much ﬁeld energy is in the space with r ≤R? How much is in the external
space?
(7 P)
Problem 3.17 Express the potential of a metal ring of radius R and charge Q in
terms of the complete elliptic integral of the ﬁrst kind K(m) (with 0 ≤m ≤1) of
p. 202. Here it will be convenient to replace the spherical coordinate ϕ by π −2x.
Determine the potential and the ﬁeld strength on the axis of the ring.
(6 P)

Problems
269
Problem 3.18 Determine the potential and ﬁeld strength on the axis of a thin metal
disc of radius R and charge Q for constant charge density. What is the jump in the
ﬁeld strength at the disc?
(3 P)
Problem 3.19 What is obtained for the potential on the axis if the disc has a constant
dipole density pA? What is the jump in the potential?
(4 P)
Problem 3.20 On a straight line at distance a from the origin, let there be a point
charge q > 0, and at distance a′ on the same side of the origin, a charge −q′ < 0.
For suitable q′(q, a, a′), the potential vanishes on the surface of a sphere about the
origin. What is its radius? Use this to determine the charge density ρA on a grounded
metal sphere of radius R induced by a point charge q at distance a from the center
of the sphere. What changes for an ungrounded metal sphere?
(6 P)
Problem 3.21 How does the Maxwell stress tensor read for a homogeneous ﬁeld
of strength E = E ez in vacuum? How strong is the force on a volume element
dx dy dz? Using the stress tensor, determine the force on an area A if its normal is
n = ex sin θ + ez cos θ.
Hint: Decompose the force into components along n, t = ex cos θ −ez sin θ, and
b = t × n. Draw the vectors E, n, t, and F for θ = 300. Interpret the result for
opposite sides of a cube.
(7 P)
Problem 3.22 How does the stress tensor change at the x, y-plane if it carries the
charge density ρA and is placed in an external (homogeneous) ﬁeld in the z direction?
Can the force on an enclosing layer be related to the mean value of the ﬁeld strength
above and below the plane? Determine the Cartesian components of the Maxwell
stress tensor on the plane midway between two equal charges q (each at distance a
from this plane)? What force is thus exerted from one of the sides on the plane?
Hint: Express the strength of the ﬁeld in cylindrical coordinates.
(7 P)
Problem 3.23 Determine the electric ﬁeld around a metal sphere in a homogeneous
electric ﬁeld. Superpose the ﬁeld of a dipole p on a suitable homogeneous ﬁeld E0
in such a way that the tangential component of the total ﬁeld vanishes on the surface
of the sphere of radius r around the dipole. How large is the normal component (in
particular in the direction of E0, opposite and perpendicular to it)?
(4 P)
Problem 3.24 Determine the current density and resistance for half a metal ring
with circular cross-section (area πa2), whose axis forms a semi-circle of radius A
(conductivity σ), if there is a voltage U between the faces.1 Note the special case
a ≪A.
(5 P)
1Using the substitution t = tan 1
2 x with t′ = 1
2(1 + t2) and cos x = (1 −t2)/(1 + t2), the integral
of (1 + k cos x)−1 for |k| < 1 can be transformed into the integral of 2/(1 −k)(K 2 + t2)−1 with
K 2 = (1 + k)/(1 −k). This yields 2 (1 −k2)−1/2 arctan(t/K).

270
3
Electromagnetism
Problem 3.25 In an otherwise homogeneous conductor, there is a spherical void
of radius r0 containing air. Determine the current density j if it is equal to j0 for
large r.
(3 P)
Problem 3.26 Equal currents I ﬂow through two equal coaxial circles (radii R) a
distance a apart. For which ratio a/R is the magnetic ﬁeld strength at the center
of the setup as homogeneous as possible? What does that mean? Where would we
have to place a further pair of loop currents with radius 1
2 R in order to amplify
the homogeneous ﬁeld? Can the homogeneity be improved by a suitable choice of
current strengths in two pairs of loops?
(8 P)
Problem 3.27 A closed iron ring with permeability μ and dimensions a and A, as in
Problem 3.24, is wrapped around N times with a thin wire. How large is the induction
ﬂux 	 =

df · B in the ring? How large is the relative error δ	 = |	 −	|/	, if
we assume a constant magnetic ﬁeld H equal to the value 	 at the center of the
cross-section? Determine 	 and δ	 for N = 600, μ = 500μ0, A = 20 cm, πa2 =
10 cm2, and I = 1 A. The iron ring may have a narrow discontinuity (air gap) of
width d. It can be so narrow that no ﬁeld lines escape from the slit. How does the
induction ﬂux depend on the width d if we use a constant magnetic ﬁeld H in the
cross-section?
(7 P)
Problem 3.28 The mutual inductance of two coaxial circular rings of radii R and
R′ a distance a apart is determined as L = μ0
√
RR′ {2 (K −E)−k2K}/k, with
the parameter k2 = 4RR′/{a2 + (R+R′)2}, involving the complete elliptic func-
tions of the ﬁrst kind, viz., K(k2) as in Problem 3.17, and the second kind, viz.,
E(k2) =
 π/2
0

1−k2 sin2 z dz. What is obtained to leading order for L at very large
distances (R ≪a, R′ ≪a)?
Hint: Expand K and E in powers of k.
(3 P)
Problem 3.29 In the limit of small distances (R ≈R′ ≫a), we use the Landen
transformation F(x|k2) = 2/(1+k) F(x′|k′2) for the incomplete elliptic integral of
the ﬁrst kind F(x|k2) =
 x
0 dz/

1−k2 sin2z, with x′ = 1
2{x+arcsin(k sin x)} and
k′2 = 4k/(1+k)2.
With sin(2z1 −z) = k sin z, we have cos(2z1 −z) (2 dz1 −dz) = k cos z dz,
hence
also
dz {k cos z + cos (2z1 −z)} = 2 dz1 cos(2z1 −z) = 2 dz1 (1 −k2
sin2 z)1/2. The square of the curly brackets is equal to k2 cos2 z + 2k cos z cos (2z1 −
z) + 1 −k2 sin2 z, or again 1 + k2 + 2k {cos z cos (2z1 −z) −sin z sin (2z1 −z)}.
The curly bracket may be reformulated as cos 2z1 = 1 −2 sin2 z1. Then
dz/(1 −k2 sin2 z)1/2 = 2dz1/{(1 + k)2 −4k sin2 z1}1/2 ,
which is important for the proof of Landen’s transformation.
Prove that K(1 −ε) ≈ln(4/√ε). What follows for the inductance L(R,
R′, a)?
(5 P)

Problems
271
Fig. 3.36 Between two
points of a circuit, a
voltmeter is connected with
thin (loss-free) wires
(resistance R0), such that the
area A spanned by the circuit
is divided in the ratio A1:A2
Problem 3.30 Derive from this the self-inductance of a thin ring of wire with cir-
cular cross-section (abbreviation as in Problem 3.24), which is composed of the
mutual inductances L = (πa2)−2 
d f1 d f2 L12 of its ﬁlaments. Here
 π
0 ln(A +
B cos ϕ) dϕ = π ln{ 1
2(A +
√
A2 −B2)} for A ≥|B|. (For ferromagnetic materials,
there is an additional term, not required here.)
(5 P)
Problem 3.31 For a current strength I, determine the vector potential of a circular
ring of radius R0 at an arbitrary point r. The circular ring suggests using cylindrical
coordinates (R, ϕ, z) with r = ReR + zez.
(6 P)
Problem 3.32 A very long hollow cylinder with inner radius Ri, outer radius Ra,
and permeability μ is brought into a homogeneous magnetic ﬁeld H0 perpendicular
to its axis. Determine B and H for all r. How large is the ﬁeld H0 compared to its
value on the axis for μ ≫μ0?
(9 P)
Problem 3.33 Perpendicular to the circuit shown in Fig. 3.36, made of a thin wire
with resistance R = R1 + R2, a homogeneous magnetic ﬁeld changes by equal
amounts in equal time intervals. What voltage does the voltmeter show, and in par-
ticular, if the circuit forms a circle and the voltmeter sits at the center of the circle
and is connected with straight wires?
(5 P)
Problem 3.34 An insulating cuboid (0 ≤x ≤Lx, 0 ≤y ≤L y, 0 ≤z ≤Lz) of
homogeneous material with scalar permittivity and permeability is enclosed by ide-
ally conducting walls. Investigate the following ansatz for the vector potential:
Ax = ax cos(ωt) cos(kxx + ϕxx) cos(kyy + ϕxy) cos (kzz + ϕxz) ,
Ay = ay cos(ωt) cos(kxx + ϕyx) cos(kyy + ϕyy) cos (kzz + ϕyz) ,
Az = az cos(ωt) cos(kxx + ϕzx) cos(kyy + ϕzy) cos (kzz + ϕzz) ,
with the radiation gauge. Can we restrict ourselves here to 0 ≤ϕik < π? What is the
relation between ω and k if all the Maxwell equations are valid? What requirements
follow from the boundary conditions n × E = 0 and n · B = 0?
(7 P)
Problem 3.35 What requirement does the gauge condition ∇· A = 0 lead to for the
ansatz above? What do we then obtain for the three ﬁelds A, E, and B?
(5 P)

272
3
Electromagnetism
Problem 3.36 What do we obtain if k is parallel to one of the edges of the cuboid?
What is the general ansatz for A in Problem 3.34?
(3 P)
Problem 3.37 Express the energy density w(t, r) of an electromagnetic wave in
terms of its vector potential in the radiation gauge, i.e., with 	 = 0 and ∇· A = 0.
How can Parseval’s equation help to re-express the total energy of the wave (inte-
grated over the whole space) as an integral of the square of the absolute value of
A(t, k) and ∂A/∂t as weight factors? What is the unknown expression?
(5 P)
Problem 3.38 How does the electric ﬁeld amplitude of the reﬂected and trans-
mitted waves depend on the incoming amplitude in the limiting cases θ = 00 and
900 (expressed in terms of the refractive index n)? To what extent are the paral-
lel and perpendicular components to be distinguished for perpendicular incidence
(θ = 00)?
(4 P)
Problem 3.39 How large is the energy ﬂux ˙W, averaged over time, for an electro-
magnetic wave with wave vector k passing through an area A perpendicular to k?
What do we obtain for the reﬂected and the transmitted waves in the limiting cases
investigated above?
(4 P)
Problem 3.40 Does the energy conservation law hold true for an electromagnetic
wave, incident with the wave vector k on the interface between two homogeneous
insulators (with an arbitrary angle of incidence)? Investigate this question for arbi-
trary scalar material constants ε and μ, i.e., also with μ ̸= μ0.
(5 P)
Problem 3.41 For a homogeneous conductor (with scalar σ, ε, and μ), derive the
relation between w, E∗· D, and H∗· B from the Maxwell equations, if only one
wave vector is given. How is the time average of the Poynting vector connected to
the averaged energy density w?
Hint: Use the approximation α2 ≈β2 ≈σ/(2εω) ≫1.
(7 P)
List of Symbols
We stick closely to the recommendations of the International Union of Pure and
Applied Physics (IUPAP) and the Deutsches Institut für Normung (DIN). These
are listed in Symbole, Einheiten und Nomenklatur in der Physik (Physik-Verlag,
Weinheim 1980) and are marked here with an asterisk. However, one and the same
symbol may represent different quantities in different branches of physics. Therefore,
we have to divide the list of symbols into different parts (Table3.2).

List of Symbols
273
Table 3.2 Symbols used in electromagnetism
Symbol
Name
Page number
*
Q
Charge
165
q
Point charge
165
*
ρ
(Space) Charge density
166
a
ρA
Surface charge density
167
*
I
Current strength
186
*
j
Current density
186
jA
Current density in a surface
195
*
E
Electric ﬁeld strength
166
*
D
Electric current density (displacement ﬁeld)
174
*
B
Magnetic current density (magnetic induction ﬁeld)
181
*
H
Magnetic ﬁeld strength
193
*
ε
Permittivity (dielectric constant)
176
*
ε0
Electric ﬁeld constant (vacuum permittivity)
164, 623
*
μ
Permeability
196
*
μ0
Magnetic ﬁeld constant (vacuum permeability)
164, 623
*
c (c0)
Light velocity (in vacuum)
164, 216, 623
*
χe
Electric susceptibility
175
*
χm
Magnetic susceptibility
196
*
P
Electric polarization
174
*
M
Magnetization
191
*
p
Electric dipole moment
171
*
m
Magnetic dipole moment
190
*
U
(Electric) Voltage
169
b
	
Electric potential
56
*
A
Vector potential
197
c
Epot
Potential energy
169
*d
W
Work
181
*
w
Energy density
211
e
N
Torque
171
*
C
Capacitance
179
*
R
Electric resistance
187
*
σ
Electric conductivity
187
*f
L
Inductance
201
(continued)

274
3
Electromagnetism
Table 3.2 (continued)
Symbol
Name
Page number
g
Z
Impedance
213
S
Poynting vector
211
T
Stress tensor
184, 215
Fμν
Electromagnetic ﬁeld tensor
240
aThe abbreviation σ is actually recommended for this, but it is also used also for the conductivity.
The index A reminds us of an area. We also use it for the area divergence and area rotation.
bϕ is actually recommended, but we use it for the azimuth.
c V is needed for the volume.
dThe abbreviation A, common in mechanics, is needed here for the area.
e M is recommended, but used here for the magnetization.
f L is recommended for the self-inductance. We also use this abbreviation for the mutual inductance.
g Z should be taken for the impedance, but Z stresses the fact that it is a complex quantity: (Z =
R + iX, with resistance R and reactance) X
References
1. J.H. Hannay, Eur. J. Phys. 4, 141 (1983)
2. I. Brevik, Phys. Rep. 52, 133 (1979)
3. E.W. Schmid, G. Spitz, W. Lösch, Theoretical physics with the PC (Springer, Berlin, 1987)
Suggestions for Textbooks and Further Reading
4. H. Goldstein, J.L. ChP Poole, Safko, Classical Mechanics, 3rd edn. (Pearson, 2014)
5. W. Greiner, Classical Electrodynamics (Springer, New York, 1998)
6. J.D. Jackson, Classical Electromagnetism, 3rd edn. (Wiley, New York, 1998)
7. L.D. Landau, E.M. Lifshitz, Course of Theoretical Physics Vol. 2 – The Classical Theory of
Fields, 4th edn. (Butterworth–Heinemann, Oxford, 1975)
8. L.D. Landau, E.M. Lifshitz, Course of Theoretical Physics, Vol. 8–Electrodynamics of Con-
tinuous Media, 2nd edn. (Butterworth-Heinemann, Oxford, 1984)
9. P. Lorrain, D. Corson, F. Lorrain, Electromagnetic Fields and Waves, 3rd edn. (W.H. Freeman,
New York, 1988)
10. W. Nolting, Theoretical Physics 3-Electrodynamics (Springer, Berlin, 2016)
11. W. Nolting, Theoretical Physics 4-Special Theory of Relativity (Springer, Berlin, 2017)
12. W.K.H. Panofsky, M. Phillips, Classical Electricity and Magnetism, 2nd edn. (Addison-Wesley,
Reading, 1962)
13. W. Rindler, Essential Relativity-Special, General, and Cosmological, revised, 2nd edn.
(Springer, New York, 1977)
14. F. Scheck, Classical Field Theory (Springer, Berlin, 2018)
15. A.Sommerfeld,LecturesonTheoreticalPhysics3-Electrodynamics(Academic,London,1964)
16. A. Sommerfeld, Lectures on Theoretical Physics 4-Optics (Academic, London, 1964)
17. W. Thirring, Classical Mathematical Physics: Dynamical Systems and Field Theories, 3rd edn.
(Springer, New York, 2013)
18. A. Zangwill, Modern Electromagnetics (Cambridge University Press, 2013)

Chapter 4
Quantum Mechanics I
4.1
Wave–Particle Duality
4.1.1
Heisenberg’s Uncertainty Relations
A natural law is required to be true without exception: for all observers under equal
conditions the same result should be obtained. However, “equal conditions” have to
be reproducible and “identical results” can only be ensured within certain error limits.
With N measurements, the experimental values xi in the statistical ensemble scatter
around the average value x ≡
1
N
N
i=1 xi with an average error (for the individual
measurement)
x ≡

(x −x)2 =

x2 −x2 .
We assume N ≫1 and hence may leave out the factor √N/(N −1) from p. 46.
Here, x2 is the average value of the squares of the experimental values. These notions
have been explained in detail in Sect. 1.3.
A basic feature of quantum physics is that canonically conjugate quantities cannot
simultaneously have arbitrarily small error widths: the smaller the one, the larger
the other. For example, the momentum pk = ∂L/∂˙xk is canonically conjugate to the
coordinate xk (see p. 99). Since Niels Bohr, such pairs of quantities have been referred
to as complementary.
In classical physics, this situation does not have the same relevance, even though
there are complementary quantities, e.g., for the position x and wavenumber k =
2π/λ of a wave group, we have x · k ≥1/2. The inequality holds in particular for
all pairs of quantities connected by Fourier transform. For Gaussian distributions, we
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5_4
275

276
4
Quantum Mechanics I
ﬁnd x · k = 1/2 (Problem 4.1), and these have the smallest uncertainty product
possible for complementary quantities, as will be shown later (p. 321).
However, in classical physics it is often overlooked that canonically conjugate
quantities are always complementary to each other, because there the basic error
limits may be neglected in comparison to the average values. The situation is different
in quantum theory: here the uncertainty relations are indispensable. Hence it must
be a statistical theory, as only then do error widths make sense.
According to Heisenberg, for canonically conjugate quantities like position and
momentum, we have quantitatively
xk · pk′ ≥1
2ℏδk
k′ ,
with ℏ≡h/2π. We use h to denote Planck’s action quantum, but nowadays in
quantum theory it is more common to use ℏ. This does not occur in the classical
relation x · k ≥1/2. According to de Broglie, p = ℏk (more on that on p. 319),
so the two uncertainty relations are connected to each other. Note, however, that the
uncertainty is sometimes deﬁned differently, and then there is a different numerical
factor in the uncertainty relations. Note also that Heisenberg [1] calls uncertain
quantities “undetermined”, but that can be misunderstood.
Thus we cannot, for example, produce an ensemble which is sharp (certain) in the
position as well as in the momentum. If we force a ray with sharp direction through
a narrow slit, in order to minimize the position uncertainty (perpendicular to the ray
direction), then it spreads out because of the diffraction—and this all the more, the
narrower the slit. The momentum orthogonal to the old direction of the ray can no
longer be neglected and is unsharp (uncertain). (Its average value does not need to
change, only the uncertainty.) By eliminating inappropriate parts of the position, we
have changed the original ray.
The uncertainty relations are thus already satisﬁed in the production of a sta-
tistical ensemble. The uncertainties can often be attributed to the (then following)
measurement, but after the measurement the observation is already ﬁnished.
We start from the uncertainty relations as observational facts. As Heisenberg
shows for many examples in the above-mentioned book, quantum phenomena only
contradict our everyday experience if the uncertainty relations are not considered.
4.1.2
Wave–Particle Dualism
In order to solve Hamilton’s equations, unique initial values of position and momen-
tum are necessary, but this requirement can only be satisﬁed within error limits,
because of the uncertainty relations. Hence in quantum theory, we cannot apply the
usual notion of determinism to processes—we can only predict how all possible

4.1 Wave–Particle Duality
277
states will develop. Classically, we could assign probabilities to the possible orbits,
and given what was said above, we should only actually try to ﬁnd such probabilistic
statements.
However, with the probability distributions, interference now occurs, which is the
classical proof that waves are involved. On the other hand, other experimental results
involve shot noise (granularity) and hence support the idea that it is particles, and
not waves, that are involved.
This contradiction shows up clearly in the scattering of monochromatic electrons
of sufﬁcient energy from a crystal lattice. Here interference ﬁgures result on on the
detector screen. This fact is taken as the classical proof of a wave-like nature. With
decreasing radiation intensity, the strength of the detection on the screen is reduced,
but not continuously—detections appear now here, now there, like shot noise: this
is the classical proof of a particle-like nature.
If electrons were classical particles, then they would hit the screen like grains of
shot, and the intensity distribution ρ(r ) would result—without interference—as a
sum of the n intensity distributions ρn(r ) of the single scattering centers:
ρ = 
n
ρn
(particle picture) .
The function ρ(r ) describes the probability density of the strikes. If we exhaust all
possibilities, then we should obtain 1, i.e.,

d3r ρ(r ) = 1. Of course, for discrete
possibilities there is a sum instead of an integral.
If electrons were classical waves, then the intensity should decrease continuously,
and we should not observe any granularity of the radiation. The intensity distribution
ρ(r ) would not simply be the sum of the intensity distributions ρn(r ) of the scat-
tering centers, but would show interference—we would have to work with complex
amplitudes ψn(r ), superpose them, and form the square of the absolute value of the
sum:
ρ =


n
ψn

2
= 
n
|ψn|2 + 
n<m
(ψn∗ψm + ψm∗ψn)
(wave picture) .
The mixed terms (2 Re 
n<m ψn∗ψm) describe the interference (see Fig.4.1).
4.1.3
Probability Waves
Classically, the two pictures (or models), “wave” or “particle”, are mutually exclu-
sive. The quantum theory can remove this contradiction: it contains both pictures as
limiting cases, but restricts their range of application via the uncertainty relations in
such a way that they agree with each other, as we shall soon show.
In the above example, in particular, the point of impact of an individual electron
cannot be predicted with certainty. Only the statistical ensemble exhibits equal results

278
4
Quantum Mechanics I
Fig. 4.1 Intensity distribution behind a single slit (dotted line) and a double slit (continuous line)
according to the wave picture. All three slits have width b, and likewise the separation of the double
slit. The interference can be explained only with the wave picture—in particular the zeros cannot
be explained with the particle picture. For the single slit of width 2a, we have x = a/
√
6 and k
is inﬁnite! For this reason, only the part between the main minima is often considered. Then we
obtain k = 1/a and hence x · k = 1/
√
6
under equal conditions, and in particular, always the same interference ﬁgure. Only
this impact probability—or more precisely probability density—actually obeys a
law, and the theory should only make statements about that. With the observed
interference, we need a wave theory.
But it is important that a wave theory only applies for the impact probability, while
the classical wave picture is based on “real” waves: it is extrinsic to a wave theory
that the ﬁeld quantity in the statistical ensemble ﬂuctuates, whence deviations from
the corresponding classical value may occur. But this statistical error is important
for quantum theory: it allows the “granularity” of the radiation, which ﬁts into the
particle picture. This granularity remains unnoticed for large particle numbers [2,
p. 4]: The all leveling law of large numbers masks completely the true nature of the
single processes. On the other hand, for large numbers, uncertainties in the particle
number barely show up.
In order to capture the granularity for small particle numbers, we have to quantize
the wave theory, that is, to take the intensities as natural multiples of a basic intensity.
We treat ﬁeld quantization in Sect.4.2.8, and in more detail in Sect. 5.3 on many-
particle problems. (Incidentally, the so-called second quantization is nothing else
than ﬁeld quantization—a misleading name that can only be understood from the

4.1 Wave–Particle Duality
279
historical perspective. There is in fact only one prescription for quantization, even
though it may look different for ﬁelds and particles, because it does indeed produce
the same result.)
Here we must also consider the fact that the relative phases are important for
the superposition of waves. Uncertainty in the phase suppresses the possibility of
interference and allows only an incoherent superposition. In fact, as will be shown in
Sect.4.2.9, there is also an uncertainty relation relating particle number and phase,
although it cannot generally be written N · 	 ≥1/2, because the phase makes
sense only up to a multiple of 2π, and the particle number can only be positive-
deﬁnite.
We shall speak of particles, as is common practice, and assign probability waves
to them. Occasionally, we shall speak of quanta, which like particles are natural
multiples of an element and can interfere like waves.
Our way to describe the interference of probability waves has already been indi-
cated in the last section: we introduce probability amplitudes! These amplitudes are
usually called wave functions. Several of them can interfere with each other—the
amplitudes are added, and the square of the absolute value of the sum yields the prob-
ability (or the probability density). Here, the interference phenomenon is expressed
in the mixed terms.
These rules are valid, however, only if the individual parts can interfere with each
other. (It is well known that, in addition to coherent light, there is also incoherent
light, which cannot interfere.) If the phase relations are destroyed by an external
manipulation, for example, using light to observe the different paths of the partial
waves, then it is no longer the probability amplitudes that are added, but only the
probabilities. An “incoherent” superposition arises. For the moment, we restrict our-
selves to (interfering) coherent superpositions, which are particularly important for
the development of the usual quantum theory. We shall only deal with the general
case at the end of the next section (Sect.4.2.11). This extends the applicability of
quantum theory and is important, e.g., in thermodynamics.
Only the probabilities (or probability densities) can be measured (observed), not
the associated amplitudes. Hence also only their values and relative phases. In prin-
ciple, a general phase factor exp(iφ) remains arbitrary.
4.1.4
Pure States and Their Superposition (Superposition
Principle)
By probability we understand the ratio of actual events to the total number of the
considered events taken all together. An ensemble with speciﬁc attributes (signatures)
is investigated for its properties. A statistical ensemble with attributes is called a

280
4
Quantum Mechanics I
state in quantum theory. As already indicated, for the moment we shall not consider
incoherent mixtures, but only the so-called pure states:
Objects in the same state have the same properties and cannot be distinguished from each
other; if they could be distinguished, then the state would not be characterized with sufﬁcient
precision.
The objects here are intended to be representative of a class of objects. The notion
of state serves in a statistical theory. To this end we need a ﬁlter which decides
whether or not the property exists. Only experience can teach us which attributes
are necessary for the complete characterization of a state. For a long time it was
believed, for example, that there was only one species of neutrino, whereas three are
now distinguished. On the other hand, the subdivision may also be too ﬁne: for several
years, the various decay channels of the kaon were assigned to different particles.
As an example, a state is fully speciﬁed by the following statement: “There are
electrons with polarization direction s and momentum p.” For the sake of simplicity,
we shall momentarily take only p as a variable. Then the state is considered to be
determined by the momentum alone. This is represented by the Dirac symbol |p⟩,
although actually we should write |electron, s, p⟩. Instead of the momentum, we
could also give the position r. The corresponding state is then characterized by |r⟩.
But according to the uncertainty relation we may not take r and p together, since
they cannot be determined simultaneously with such accuracy.
But we may wish to know the probability density of the state |p⟩at position r.
For the corresponding probability amplitude, we write ⟨r |p⟩in the Dirac notation.
This complex number depends on r and p.
As a further example, let us consider the double slit experiment. Here from Fig.4.2
we can introduce four different states | i⟩, |1⟩, |2⟩, and | f⟩. Each refers to another
position. Here the states |1⟩and |2⟩are constructed from | i⟩and | f⟩from |1⟩and |2⟩.
Fig. 4.2 Double slit experiment. The source generates the initial state | i⟩and the double slit selects
the states |1⟩and |2⟩. The ﬁnal state | f⟩is detected. The path between | i⟩and | f⟩remains unknown

4.1 Wave–Particle Duality
281
The corresponding probability amplitudes are clearly ⟨1| i⟩and ⟨2| i⟩, or ⟨f |1⟩
and ⟨f |2⟩. The probability amplitude for the formation of | f⟩from | i⟩is composed
as follows:
⟨f | i⟩= ⟨f |1⟩⟨1| i⟩+ ⟨f |2⟩⟨2| i⟩,
and the corresponding probability density is
|⟨f | i⟩|2 = |⟨f |1⟩⟨1| i⟩|2 + |⟨f |2⟩⟨2| i⟩|2 + 2 Re(⟨f |1⟩∗⟨1| i⟩∗⟨f |2⟩⟨2| i⟩) .
According to p. 277, we should add the amplitudes for the different paths and then
take the square of the absolute value, whence each amplitude will factorize into the
product of those amplitudes, to arrive at the slit from the initial state, or at the detector
from the slit.
The equation before last suggests that we imagine the initial state | i⟩at the double
slit as decomposed into two states |1⟩and |2⟩:
| i⟩= |1⟩⟨1| i⟩+ |2⟩⟨2| i⟩.
This superposition of states with corresponding probability amplitudes makes good
sense (superposition principle of states). Such superpositions can be understood
classically with polarized light: we can decompose into either linearly or circularly
polarized light (linearly polarized {| ∥⟩, |⊥⟩}, circularly polarized {|+⟩, |−⟩}):
|+⟩= | ∥⟩−i
√
2 + |⊥⟩
1
√
2 ,
|−⟩= | ∥⟩
i
√
2 + |⊥⟩
1
√
2 ,
and conversely,
| ∥⟩= |+⟩
i
√
2 + |−⟩−i
√
2 ,
|⊥⟩= |+⟩
1
√
2 + |−⟩
1
√
2 .
Instead of the four states |+⟩, |−⟩, | ∥⟩, and |⊥⟩, we had the four unit vectors e+, e−,
e∥, and e⊥on p. 219.
We may take the states | . . .⟩as vectors and, according to the superposition prin-
ciple, combine them linearly with complex coefﬁcients. As already stressed in the
discussion of vector algebra (see p. 3), it is an advantage to state the expansion basis
ﬁrst and then the coefﬁcients. However, this is not true for the dual bra vectors in the
next section. We would now like to set up the rules that will be applicable here.

282
4
Quantum Mechanics I
4.1.5
Hilbert Space (Four Axioms)
So far we have denoted states using the Dirac symbol | . . .⟩and have written the
attribute of the state, e.g., p, between these “brackets” as |p ⟩. If the attributes are
countable, then these symbols can be assigned to vectors in a Hilbert space. Here, we
list the rules for these (proper) Hilbert vectors. However, for continuous attributes like
p , improper Hilbert vectors are necessary, and we shall discuss these in Sect.4.1.7.
The algebraic and topological structure of the Hilbert space are determined by
four axioms. However, in contrast to the usual vector algebra, we shall not restrict
ourselves to three dimensions (or even ﬁnite dimensions, as is clearly expressed by
the fourth axiom):
1. The Hilbert space is a vector space over the ﬁeld of complex numbers, i.e., its
elements |ψ⟩, |ϕ⟩, . . . can be added and multiplied by complex numbers a, b, . . .,
where the usual rules of vector algebra apply:
|ψ⟩+ |ϕ⟩= |ϕ⟩+ |ψ⟩,
(|ψ⟩+ |ϕ⟩) + |χ⟩= |ψ⟩+ (|ϕ⟩+ |χ⟩) ,
|ψ⟩+ |o⟩= |ψ⟩,
|ψ⟩(a + b) = |ψ⟩a + |ψ⟩b ,
(|ψ⟩+ |ϕ⟩) a = |ψ⟩a + |ϕ⟩a .
Occasionally, we will also write |ψ a + ϕ b⟩for |ψ⟩a + |ϕ⟩b. In particular, for each
|ψ⟩, there is a vector | −ψ⟩such that |ψ⟩+ | −ψ⟩= |o⟩is a “zero vector”.
A ﬁnite set of vectors |ψ1⟩, . . . , |ψN⟩are said to be linearly independent if
none of them can be expressed in terms of the remaining ones, so for example,
|ψn′⟩̸= 
n̸=n′ |ψn⟩cn. An inﬁnite set of vectors are said to be linearly independent
of each other if this is true for all ﬁnite subsets. N linearly independent vectors span
a vector space of dimension N. The set of vectors in a one-dimensional Hilbert space
forms a ray—they differ from each other only by a (complex) number.
2. The Hilbert space has a Hermitian metric.1 This means that a complex number
⟨ψ|ϕ⟩≡⟨ψ∥ϕ⟩is assigned to each pair of vectors |ψ⟩, |ϕ⟩. This is called the scalar
product, the inner product, or the Dirac bracket. It has the following properties:
⟨ψ|ϕ + χ⟩= ⟨ψ|ϕ⟩+ ⟨ψ|χ⟩,
⟨ψ|ϕ a⟩= ⟨ψ|ϕ⟩a ,
⟨ψ|ϕ⟩= ⟨ϕ|ψ⟩∗,
⟨ψ|ψ⟩> 0 if |ψ⟩̸= |o⟩.
It is linear, Hermitian-symmetric and positive-deﬁnite. In addition to ⟨ψ|o⟩= 0 =
⟨o|ψ⟩, we thus have
1Charles Hermite (1822–1901) was a French mathematician. Hence the “e” at the end is not pro-
nounced.

4.1 Wave–Particle Duality
283
⟨ψ + ϕ|χ⟩= ⟨ψ|χ⟩+ ⟨ϕ|χ⟩,
⟨ψ a|ϕ⟩= a∗⟨ψ|ϕ⟩.
Therefore the scalar product ⟨ψ|ϕ⟩depends linearly on the ket-vectors |ϕ⟩, but anti-
linearly on the bra-vectors ⟨ψ|. If we multiply the ket-vector by a number a, then
likewise the inner product, but if we multiply the bra-vector by a, then the inner
product gets multiplied by the conjugate complex a∗. The two types of vectors are
in principle different. They cannot be added, because they belong to “dual” spaces,
but they can be assigned to each other. A bra-vector is known if its scalar products
with all ket-vectors are known. Note that we also distinguish between covariant and
contravariant components, and both are needed for a scalar product (see p. 33). Bra-
and ket-vectors may also be considered as row and column vectors, respectively, if
we allow complex components and do not restrict the dimension.
The quantity ∥ψ∥≡√⟨ψ|ψ⟩≥0 is called the norm (length) of the vector |ψ⟩.
It vanishes only if |ψ⟩is the zero vector. We will usually restrict ourselves to vectors
of unit norm, whence we always have |⟨ψ|ϕ⟩|2 ≤1 , as we shall soon see, and that
is indispensable for the probability interpretation. For this purpose, there are two
further notions we need to consider. Two Hilbert vectors are said to be orthogonal to
each other if their scalar product vanishes, and they are said to be parallel if the two
vectors differ only by a numerical factor. From this we can obtain the components
of the vector |ψ⟩parallel and orthogonal to the vector |ϕ⟩from |ψ⟩= |ψ∥⟩+ |ψ⊥⟩,
with |ψ∥⟩= |ϕ⟩⟨ϕ|ψ⟩/⟨ϕ|ϕ⟩and |ψ⊥⟩= |ψ⟩−|ψ∥⟩. Note that we do not assume
here that |ϕ⟩is normalized to 1, and |ψ∥⟩and |ψ⊥⟩are not generally, even if |ψ⟩is.
Clearly, ⟨ϕ|ψ∥⟩= ⟨ϕ|ψ⟩and hence ⟨ϕ|ψ⊥⟩= ⟨ϕ|ψ⟩−⟨ϕ|ψ∥⟩= 0. It then follows
that ∥ψ∥2 = ∥ψ∥∥2 + ∥ψ⊥∥2 with ∥ψ∥∥2 = |⟨ϕ|ψ⟩|2 /∥ϕ∥2, and because ∥ψ⊥∥≥0,
we thus have ∥ψ∥2 ≥∥ψ∥∥2 and
Schwarz inequality
∥ψ∥· ∥ϕ∥≥|⟨ψ|ϕ⟩| .
In particular, |⟨ψ|ϕ⟩|2 ≤1 for vectors normalized to 1. Equality holds only if |ψ⟩and
|ϕ⟩are parallel to each other and hence ∥ψ⊥∥vanishes. With ∥ψ + ϕ∥2 = ∥ψ∥2 +
2Re⟨ψ|ϕ⟩+ ∥ϕ∥2 and Re⟨ψ|ϕ⟩≤|⟨ψ|ϕ⟩|, Schwarz’s inequality also delivers ∥ψ +
ϕ∥2 ≤(∥ψ∥+ ∥ϕ∥)2. This upper limit is true also for ∥ψ −ϕ∥2 = ∥(ψ −χ) −
(ϕ −χ)∥2, where |χ⟩can be an arbitrary vector. Hence,
Triangle inequality
∥ψ −ϕ∥≤∥ψ −χ∥+ ∥ϕ −χ∥.
For this reason, the norm ∥ψ∥is also referred to as the length of the Hilbert vector
|ψ⟩.
Incidentally, from the Schwarz and the triangle inequalities together, it follows
that ∥ψ∥∥ϕ∥≥|⟨ψ|ϕ⟩| ≥|Re ⟨ψ|ϕ⟩| = 1
2 |⟨ψ|ϕ⟩+ ⟨ϕ|ψ⟩|.
We can now investigate convergence in Hilbert space. However, we have to dis-
tinguish between two notions of convergence:

284
4
Quantum Mechanics I
strong convergence |ψn⟩→|ψ⟩if lim
n→∞∥ψn −ψ∥= 0 ,
weak convergence |ψn⟩⇀|ψ⟩if lim
n→∞⟨ψn|ϕ⟩= ⟨ψ|ϕ⟩for all |ϕ⟩.
With the help of the Schwarz inequality, strong convergence implies weak conver-
gence. For all |ϕ⟩(with ∥ϕ∥< ∞), we ﬁnd
|⟨ψn|ϕ⟩−⟨ψ|ϕ⟩| = |⟨ψn −ψ|ϕ⟩| ≤∥ψn −ψ∥· ∥ϕ∥→0 .
But weak convergence does not imply strong convergence, unless we also have
∥ψn∥→∥ψ∥:
∥ψn −ψ∥2 = ∥ψn∥2 + ∥ψ∥2 −2 Re⟨ψn|ψ⟩
⇀∥ψn∥2 + ∥ψ∥2 −2 Re⟨ψ|ψ⟩= ∥ψn∥2 −∥ψ∥2 .
But there are sequences {|ψn⟩} which converge weakly towards the zero vector
without their norm tending towards zero, for example, if each |ψn⟩is normalized to
1 and orthogonal to all the others. These issues are investigated in Problem 4.9.
A Cauchy sequence of vectors |ψn⟩is understood as a sequence for which
∥ψn −ψm∥becomes smaller than any ε > 0 with increasing n and m. Every strongly
convergent sequence is a Cauchy sequence. Conversely, each Cauchy sequence con-
verges strongly if the limit vector also belongs to the Hilbert space. This is taken care
of by the third axiom.
3. The Hilbert space is complete, in the sense that it contains all its accumulation
points. For a ﬁnite-dimensional space, this is not in fact an additional requirement.
The fourth axiom is then obsolete.
4. The Hilbert space is of countably inﬁnite dimension (separable), meaning that it
contains only a countable inﬁnity of mutually orthogonal unit vectors, {|εn⟩} with
⟨εn|εn′⟩= δnn′ for all natural numbers n and n′. A system of such vectors is referred
to as an orthonormal system. It consists of vectors which are orthogonal to each other
and normalized to 1. We will write |n⟩for short, instead of |εn⟩.
The Hilbert space vectors are thus abbreviations for states and the scalar products
(of vectors normalized to 1) for probability amplitudes. Then, e.g., ⟨ψ|ϕ⟩is the
probability amplitude for ﬁnding |ϕ⟩if the system is in the state |ψ⟩. We shall
determine such probability amplitudes later, e.g., ⟨r |p⟩= h−3/2 exp(ir · p/ℏ).
4.1.6
Representation of Hilbert Space Vectors
Every arbitrary (normalizable) Hilbert vector |ψ⟩can be expanded in terms of a
complete basis {|n⟩}, where we assume an orthonormal system so that ⟨n|n′⟩= δnn′
should hold:

4.1 Wave–Particle Duality
285
|ψ⟩=

n
|n⟩⟨n|ψ⟩,
⟨ψ| =

n
⟨ψ|n⟩⟨n| .
The order of the factors, that is, expansion coefﬁcients after ket-vectors and before
bra-vectors, is actually arbitrary, but will turn out later to be particularly practical.
For example, we shall treat 
n |n⟩⟨n| as a unit operator and write equations like
|ψ⟩= 1|ψ⟩and ⟨ψ| = ⟨ψ|1. Then, according to p. 282, ⟨ψ|n⟩= ⟨n|ψ⟩∗holds. The
expansion coefﬁcients
ψn ≡⟨n|ψ⟩
are the (complex) vector components of |ψ⟩in this basis. The sequence {ψn} gives
the representation of the vector |ψ⟩in the basis {|n⟩}. For the scalar product of two
vectors, it then follows that
⟨ψ|ϕ⟩=

n
⟨ψ|n⟩⟨n|ϕ⟩=

n
ψn
∗ϕn ,
described as insertion of intermediate states or insertion of unity (in Fig.4.2, we used
only two states |1⟩and |2⟩). The special case |ϕ⟩= |ψ⟩, viz.,
∥ψ∥2 =
∞

n=1
|ψn|2 ,
is called the completeness relation. It holds only if no basis vector is missing. Finally,
∥ψ∥2 ≥
N

n=1
|ψn|2
is Bessel’s inequality.
The Hilbert vectors which were initially introduced only formally thus become
rather simple constructs as soon as a discrete basis is introduced in Hilbert space.
Then each vector is given by its (possibly inﬁnitely many) complex components
with respect to this basis, i.e., by a sequence of complex numbers. We then speak of
vectors in sequence space.
If we take the sequence {⟨n|ψ⟩= ψn} as a column vector and {⟨ψ|n⟩= ψn∗} as
a row vector, i.e.,
|ψ⟩=
 ψ1
ψ2
...
	
and
⟨ψ| = (ψ1
∗, ψ2
∗, . . .) ,
then the scalar products ⟨ψ|ϕ⟩obey matrix multiplication rules.

286
4
Quantum Mechanics I
Of course, we may introduce a new basis, e.g., {|m⟩} with ⟨m|m′⟩= δmm′. For the
change of representation {|n⟩} →{|m⟩}, we clearly have
ψm ≡⟨m|ψ⟩=

n
⟨m|n⟩⟨n|ψ⟩=

n
mn
∗ψn ,
i.e., the components in one basis follow from the components in the other basis,
where the components of the basis vectors occur as expansion coefﬁcients. This is
similar to our procedure for the orthogonal transformation on p. 29, and will be
important in Sect.4.2 as a “unitary transformation”.
We now take an obvious step which leads to a new representation of the Hilbert
space. We plot the complex numbers ψn ≡⟨n|ψ⟩versus the natural numbers {n} on
the number axis, and then, not only to the natural numbers, but to all real numbers
x, assign values ψ(x) ≡⟨x|ψ⟩. This delivers a different representation of the Hilbert
space, namely the Hilbert function space {ψ(x)}. It combines all complex functions
deﬁnedontherealaxisforwhichthesquareoftheirabsolutevaluecanbeintegratedin
the Lebesgue sense, i.e., they are integrable almost everywhere, in the sense that only
a set of arguments of measure zero need be excluded: ⟨ψ|ψ⟩=

|ψ(x)|2 dx < ∞.
Such functions are said to be normalizable. With a ﬁnite numerical factor, they can
be normalized to 1. The range of integration corresponds to the domain of deﬁnition,
which can be inﬁnite on both sides. This function space is a linear space. It is complete
and has a countable inﬁnity of dimensions. The inner product is now given by
⟨ψ|ϕ⟩=

ψ∗(x) ϕ(x) dx ,
i.e., the sum in sequence space becomes an integral in the function space.
With this we can then express a complete orthonormal system of functions {gn(x)}
(see p. 21) with

gn∗(x) gn′(x) dx = δnn′ in the useful form
gn(x) ≡⟨x|n⟩.
An arbitrary (normalizable) function can be expanded in terms of this orthonormal
system (represented in this basis):
ψ(x) = ⟨x|ψ⟩=

n
⟨x|n⟩⟨n|ψ⟩=

n
gn(x) ψn ,
with the expansion coefﬁcients (“Fourier coefﬁcients”)
ψn = ⟨n|ψ⟩=

⟨n|x⟩⟨x|ψ⟩dx =

gn
∗(x) ψ(x) dx .
The best-known example is the Fourier expansion, and another the expansion in
terms of Legendre polynomials, with domain of deﬁnition −1 ≤x ≤1.

4.1 Wave–Particle Duality
287
The function ψ(x) in Hilbert function space is then represented as a vector {ψn}
in Hilbert sequence space, or the vector as a function. Depending on the basis,
the same Hilbert vector |ψ⟩appears in different forms—in the sequence space, we
obtain Heisenberg’s matrix mechanics, and in function space, Schrödinger’s wave
mechanics.
4.1.7
Improper Hilbert Vectors
However, |x⟩and ⟨x| are not genuine (proper) Hilbert vectors. If we compare
the scalar product ⟨ψ|ϕ⟩= 
nn′ ⟨ψ|n⟩⟨n|n′⟩⟨n′|ϕ⟩with the expected expression

⟨ψ|x⟩⟨x|x′⟩⟨x′|ϕ⟩dx dx′, then

⟨x|x′⟩ϕ(x′) dx′ must be equal to ϕ(x). The scalar
product ⟨x|x′⟩is clearly equal to the Dirac delta function (see Sect. 1.1.10), i.e.,
⟨x|x′⟩= δ(x −x′) ,
and hence is no longer a typical number—Dirac symbols with continuous variables
are not proper Hilbert space vectors, but improper Hilbert vectors. The normalization
to the delta function is called normalization in the continuum, and often also delta-
normalization.
Since x is a continuous variable, |⟨x|ψ⟩|2 should not be called a probability: it is
a probability density. Only |⟨x|ψ⟩|2 dx is a probability, in particular, for the interval
dx, and only probabilities can be compared with observed values. For example, there
is no particle at the position r, but only in a region d3r around r. The more certain its
position, the more uncertain its momentum! While we may often speak of a particle
with the momentum p, e.g., in Sect.4.1.4, our main interest is not the “small error
interval” p, otherwise its position would have to be totally uncertain.
Continuous variables are often convenient for calculation, and we shall use them
repeatedly, even if they are only ever observed in a certain interval. For the same
reason we will not be disturbed by the fact that ⟨x|x′⟩= δ(x −x′) is not a standard
number (function of x and x′). It is quite sufﬁcient that the delta-function has a deﬁnite
meaning in an integral.
4.1.8
Summary: Wave–Particle Dualism
Quantum mechanics is more general than its classical limiting case, since quantum
theory includes the fact that canonically conjugate quantities cannot simultaneously
be sharp (or certain)—they are complementary, in the sense that the more precise one
quantity is, the less precise the other will be, a fact overlooked in classical mechanics.
We take Heisenberg’s uncertainty relation xk · pk′ ≥1
2ℏδk
k′ as the basic exper-
imental fact. The consequences are far-reaching. In particular, the particle and wave
pictures in quantum theory are no longer in contradiction, because all measurable

288
4
Quantum Mechanics I
quantities then have “uncertain” values in precisely such a way that the two pictures
remain compatible with each other—neither the particle number nor the phase in the
statistical ensemble has to have a sharp value.
Uncertainty is a statistical notion and quantum theory a statistical theory for the
determination of probabilities and average values. Since interference occurs for these
probabilities, we work with probability amplitudes and take these as scalar products
of Hilbert vectors. The ket-vector | . . .⟩speciﬁes the attributes of the considered
ensemble and the bra-vector ⟨. . . | the attributes for the probability. Then the square
of the absolute value of the scalar product ⟨ψ|ϕ⟩gives the probability for the attribute
ψ in the ensemble ϕ. The rules for these state vectors have been presented here. We
assign proper or improper Hilbert space vectors to them, depending on whether they
are valid for countable or continuous variables, respectively.
Concerning the scalar product ⟨ψ|ϕ⟩, initially only the square of the absolute value
can be measured, i.e., as the associated probability. Only if two amplitudes interfere
with each other can the relative phase be determined, and even then, a global phase
factor remains free.
Incidentally, as early as 1781, I. Kant wrote in his Kritik der reinen Vernunft:
“[…] consequently we cannot have knowledge of a matter as a thing as such, but
only as much as it is an object of the sensuous perception”, something Heisenberg
also stressed. Only then can such knowledge be proven as a law, if the experiment
is repeated. This leads to statistics. Then the uncertainty relations are valid from the
moment the statistical ensemble has been produced, not at the time of the individual
measurements. Anyone who does not take this fact into account will very likely ﬁnd
quantum theory incomplete.
4.2
Operators and Observables
4.2.1
Linear and Anti-linear Operators
The state vectors | . . .⟩and ⟨. . . | are mathematical tools to describe pure states in
quantum theory. In addition, we need quantities which act on these state vectors,
which we call operators. We always write them with upper-case letters:
|ψ′⟩= A |ψ⟩.
Operators assign an image vector |ψ′⟩to each object vector |ψ⟩. (We can also
consider operators which are only deﬁned on a part of space, but we do not wish to
deal with those here.) If we know the image vector |ψ′⟩for each vector |ψ⟩, then
we also know the operator A, just as a bra-vector is determined if its scalar products
with all ket-vectors are known. If A |ψ⟩= A′ |ψ⟩for all |ψ⟩, then the two operators
are equal, i.e., A = A′. The zero operator assigns the zero vector to all vectors, i.e.,

4.2 Operators and Observables
289
0 |ψ⟩= |o⟩, while the unit operator assigns the original vectors to all vectors, i.e.,
1|ψ⟩= |ψ⟩.
Inquantummechanics,onlylinearandanti-linearoperatorsoccur.Theyarelinear,
if
A |ψ + ϕ⟩= A |ψ⟩+ A |ϕ⟩
and
A |ψ a⟩= A |ψ⟩a ,
while for an anti-linear operator, A |ψ a⟩= (A |ψ⟩) a∗. In quantum theory, there is
only one important anti-linear operator, namely the time reversal operator T , which
we shall discuss in Sect.4.2.12 (and also the charge-inversion operator C for the
Dirac equation). Until then, we shall deal only with linear operators. They can be
added and multiplied by complex numbers:
(a A + b B) |ψ⟩= A |ψ⟩a + B |ψ⟩b .
The product of two operators depends on the order of the factors: AB may differ from
BA. We deﬁne the commutator and anti-commutator of two operators A and B by
A B −B A ≡[A, B]
commutator of A and B ,
A B + B A ≡[A, B]+ ≡{A, B}
anti-commutator of A and B .
If A B = B A holds, the two operators commute with each other. Then also aA and bB
commute with each other. The unit operator and the zero operator commute with all
operators. In quantum theory, it is important to know whether or not two operators
commute with each other, so here are several properties of commutators:
[A, B] = −[B, A] ,
[A, B + C] = [A, B] + [A, C] ,
[A, BC] = [A, B] C + B [A, C] .
Hence, with [A, Bn] = [A, B]Bn−1 + B[A, Bn−1] for n ∈{1, 2, . . .}, it follows that
[A, Bn] =
n−1

k=0
Bk [A, B] Bn−1−k .
In particular, for [[A, B], B] = 0, we have [A, Bn] = n [A, B] Bn−1. The last expres-
sion is sometimes written as [A, B] dBn/dB, because we can also differentiate with
respect to operators, as we shall see on p. 316. In addition, we ﬁnd Jacobi’s identity
[A, [B, C]] + [B, [C, A]] + [C, [A, B]] = 0 ,

290
4
Quantum Mechanics I
as for the vector product (see p. 4) and the Poisson brackets (p. 124). Note that,
together with skew-symmetry [A, B] = −[B, A] and the bilinearity property
[aA + bB, C] = a[A, C] + b[B, C] ,
in the abstract vector space of the quantities A, B, C, . . ., the Jacobi identity shows
that the commutator makes the set of operators into a Lie algebra.
There are functions of operators, e.g., polynomials in A like
f (A) = c0 1 + c1 A + c2 A2 + · · · + cn An ,
and the “exponential function” exp A ≡eA = ∞
n=0 An/(n!). Incidentally,
eA B e−A = B + [A, B]
1!
+ [A, [A, B]]
2!
+ [A, [A, [A, B]]]
3!
+ · · ·
iscalledtheHausdorffseries.Thisequationcanbeprovenbyconsideringthefunction
f (t) = eAtBe−At, for which ˙f = [A, f ], ¨f = [A, ˙f ], and similar. With f (0) = B, its
Taylor series about t = 0 for t = 1 delivers the Hausdorff series. From [A, 1] = 0, it
follows in particular that eAe−A = 1, which can be generalized:
[A, [A, B]] = 0 = [B, [A, B]]
=⇒
eA eB = e
1
2 [A,B] eA+B .
In particular, if we set f (t) = eAt eBt, then ˙f = Af + fB = (A + eAt B e−At) f , and
with [A, [A, B]] = 0, according to Hausdorff, ˙f = (A + B + [A, B] t) f . With f (0) =
1 this implies f (t) = exp{(A + B) t + 1
2[A, B] t2}. The claim follows for t = 1, since
exp( 1
2[A, B]) may be factored out, because [A, B] is assumed to commute with A
and B.
Numerical factors multiplying Hilbert vectors can be considered as very simple
operators. They are multiples of the unit operator and hence commute with every
linear operator.
4.2.2
Matrix Elements and Representation of Linear
Operators
So far the operators have been acting only on ket-vectors | . . .⟩. We consider now the
scalar product of an arbitrary bra-vector ⟨ψ| with the ket-vectors A |ϕ⟩, where A is a
linear operator. Each scalar product depends linearly on its ket-vector, but now the
ket-vector A |ϕ⟩also depends linearly on |ϕ⟩. Thus the scalar product depends linearly
on |ϕ⟩. Consequently, a bra-vector can be constructed from the other quantities ⟨ψ|
and A. Hence, for linear operators, we have

4.2 Operators and Observables
291
⟨ψ| (A |ϕ⟩) = (⟨ψ| A) |ϕ⟩= ⟨ψ| A |ϕ⟩.
This complex number is called the matrix element of the operator A between the
states ⟨ψ| and |ϕ⟩.
In order to understand the connection with matrices, we take any discrete com-
plete orthonormal system {|n⟩} and consider A |ψ⟩= 
n′ A |n′⟩⟨n′|ψ⟩and A |n′⟩=

n |n⟩⟨n| A |n′⟩. If we compare this with the original expression, we have
A =

nn′
|n⟩⟨n| A |n′⟩⟨n′| .
The complex numbers ⟨n| A |n′⟩form the matrix of the operator A in the n represen-
tation (possibly with inﬁnitely many matrix elements):
⎛
⎜⎝
⟨n1| A |n1⟩⟨n1| A |n2⟩. . .
⟨n2| A |n1⟩⟨n2| A |n2⟩. . .
...
...
...
⎞
⎟⎠.
If the matrix is known, then so is the operator “in the n representation”. Its “diagonal
elements” are ⟨n| A |n⟩and its “off-diagonal elements” ⟨n| A |n′⟩(with n ̸= n′).
In the Dirac notation, ⟨. . . | . . .⟩is thus a number and | . . .⟩⟨. . . | an operator. A
particularly important example is the unit operator
1 =

n
|n⟩⟨n| ,
with ⟨n| 1 |n′⟩= ⟨n|n′⟩= δnn′ .
In this sense, the above-mentioned representations |ψ⟩= 
n |n⟩⟨n|ψ⟩for states
and A = 
nn′ |n⟩⟨n|A|n′⟩⟨n′| for operators are to be interpreted as |ψ⟩= 1|ψ⟩and
A = 1A1. This also shows why the notation ⟨n|A|n′⟩is preferred over the abbreviation
Ann′, even though the symbol takes up more room.
We can take the objects |n⟩⟨n| as operators projecting onto the states |n⟩. In
particular, if |ψ⟩is normalized to 1, the projection operator onto the state |ψ⟩is
Pψ ≡|ψ⟩⟨ψ| ,
with ⟨n| Pψ |n′⟩= ⟨n|ψ⟩⟨ψ|n′⟩= ψn ψn′∗,
because this operator projects an arbitrary vector |ϕ⟩onto the vector |ψ⟩:
Pψ |ϕ⟩= |ψ⟩⟨ψ|ϕ⟩.
For ∥ψ∥= 1, we always have Pψ 2 = Pψ, even though Pψ ̸= 1 (and ̸= 0) holds:
projection operators are idempotent.
For the operator product A B, we have

292
4
Quantum Mechanics I
⟨n| A B |n′⟩=

n′′
⟨n| A |n′′⟩⟨n′′| B |n′⟩,
and hence the usual law of matrix multiplication.
4.2.3
Associated Operators
If the product of two operators is equal to the unit operator, as exempliﬁed by eA and
e−A above, each operator is said to be the inverse of the other:
A A−1 = 1 = A−1 A .
But note that not every operator has an inverse. For singular operators, there is a
|ψ⟩̸= |o⟩with A |ψ⟩= |o⟩, and from |o⟩there is no operator leading back to |ψ⟩.
For operator products, we have
(A B)−1 = B−1 A−1 ,
because their product with AB gives 1 in both cases.
What the operator A produces for ket-vectors, its (Hermitian) adjoint operator A†
does for bra-vectors:
A |ψ⟩= |ψ′⟩
⇐⇒
⟨ψ| A† = ⟨ψ′| .
Hence we always have ⟨ψ| A† |ϕ⟩= ⟨ψ′|ϕ⟩= ⟨ϕ|ψ′⟩∗= ⟨ϕ| A |ψ⟩∗, together with
A†† = A
and
(A B)† = B† A† ,
since ⟨ψ|(AB)†|ϕ⟩= ⟨ϕ|AB|ψ⟩∗= 
n⟨ϕ|A|n⟩∗⟨n|B|ψ⟩∗= ⟨ψ|B†A†|ϕ⟩for all ⟨ψ|
and |ϕ⟩. For real matrices, the adjoint is the same as the transpose (reﬂected in the
main diagonal). Incidentally, (A†)−1 = (A−1)† holds, since A†(A−1)† = (A−1A)† =
1† = 1. Instead of “Hermitian adjoint”, we usually speak of the Hermitian conjugate
and abbreviate this to h.c., e.g., A + A† is the same as A+ h.c.
An operator would be called self-adjoint if
A† = A ,
i.e.,
⟨ψ| A |ϕ⟩= ⟨ϕ| A |ψ⟩∗,
for all |ψ⟩and |ϕ⟩,
which is true, e.g., for any projection operator. Such operators are also said to be
Hermitian, even though the domains of deﬁnition of A and A† for a Hermitian operator
do not have to coincide. Hence, all self-adjoint operators are Hermitian, but not all
Hermitian operators are self-adjoint. Note also that Hermitian operators always have
real diagonal elements. If all the matrix elements are real, as for the tensor of inertia,
then we speak of a symmetric matrix rather than a Hermitian matrix. The product

4.2 Operators and Observables
293
of two Hermitian operators is only Hermitian if they commute with each other. But
{A, B} and [A, B]/i are Hermitian, so we can use
A B = A B + B A
2
+ i A B −B A
2i
= {A, B}
2
+ i [A, B]
2i
,
if AB is not Hermitian.
An operator is said to be unitary, if
U † = U −1
⇐⇒
U U † = 1 = U † U ,
whence 
n⟨n′| U |n⟩⟨n′′| U |n⟩∗= ⟨n′|n′′⟩= 
n⟨n| U |n′⟩∗⟨n| U |n′′⟩. If the matrix
isunitaryandreal,liketherotationmatrixonp.29,thenitisalsosaidtobeorthogonal.
Note that any unitary 2 × 2 matrix can be obtained from 3 real parameters α, β, γ ,
in particular from
U =
 cos α exp(iβ) sin α exp(−iγ )
−sin α exp(iγ ) cos α exp(−iβ)
	
,
if we disregard a common phase factor, hence a fourth parameter. The inverse of a 2
× 2 matrix is given on p. 71.
Unitary operators U can be derived from self-adjoint operators A via
U = exp(iA) ,
since, according to the last section, for A = A†, we ﬁnd
UU † = exp(iA −iA) exp(1
2[iA, −iA]) = 1 .
For
inﬁnitesimal
transformations
(with
A = A† ≪1),
the
approximation
exp(± iA) ≈1 ± iA is often used. A different relation between a Hermitian and
a unitary operator is produced by U = (1 −iA)(1 + iA)−1. For the proof, we use the
fact that the factors commute.
If all vectors are subjected to a unitary transformationU, then their scalar products
remain the same:
⟨ψ′|ϕ′⟩= ⟨ψ| U † U |ϕ⟩= ⟨ψ|ϕ⟩,
in particular all vectors keep the same norm. Unitary transformations are thus isomet-
ric. Here, only U †U = 1 is necessary, but not UU † = 1. If U is isometric, then UU †
is a projection operator, since (UU †)2 is then equal to UU †. For a ﬁnite dimension,
unitarity follows from isometry.
With a unitary operator U, a complete orthonormal system {|n⟩} can be
transformed into a different basis {|m⟩= U |n⟩}. A change of representation always

294
4
Quantum Mechanics I
corresponds to a unitary transformation. If the vectors are transformed with |ψ′⟩=
U |ψ⟩, then the operators are likewise transformed with
A′ = UAU −1 = UAU † ,
since |ϕ⟩= A |ψ⟩and U|ϕ⟩= |ϕ′⟩= A′ |ψ′⟩= A′U |ψ⟩implies UA = A′U. Corre-
spondingly, the operator function f (A) turns into U f (A) U −1 = f (UAU −1), since
all f (A) are to be taken as power series of A, and the unit operator U −1U may be
inserted between the individual factors.
The trace of an operator, i.e., the sum of its diagonal elements, always remains
constant under unitary transformations in ﬁnite dimensions, since then tr(AB) =
tr(BA) and hence tr(UAU −1) = trA. For inﬁnite dimensions, this is not always true,
as we shall see for a counterexample on p. 303.
If A and B commute, then the operator B remains the same for the transformation
U = exp(iA), as follows immediately from the Hausdorff series (p. 290). Here, U
does not need to be unitary. This is only necessary if, for self-adjoint A, we also
require A′ to be self-adjoint, because A′ = UAU −1 implies A′† = U −1†A†U †, and
this is equal to A′ if U −1 = U †.
4.2.4
Eigenvalues and Eigenvectors
These notions are deﬁned as follows (see p. 87). If
A |α⟩= |α⟩a ,
then a is an eigenvalue and |α⟩(̸= |o⟩) an eigenvector of the operator A. Only the
ray speciﬁed by |α⟩is important. For linear operators, |α⟩is an eigenvector with
eigenvalue a if and only if |α⟩c is an eigenvector with eigenvalue a. Then also
⟨α| A† = a∗⟨α| holds, since ⟨α| A† |β⟩= ⟨β| A |α⟩∗= ⟨β|α⟩∗a∗= a∗⟨α|β⟩, for all
|β⟩. While the eigenvalues have physical relevance, the state vectors | . . .⟩are only
mathematical tools. For discrete eigenvalues an, the number n is called the quantum
number, e.g., we speak of the oscillation, angular momentum, direction, and principal
quantum numbers in various contexts.
The transformed operator UAU −1 has the same eigenvalue a and the eigenvector
U|α⟩. From the equation above, it follows in particular thatUAU −1 U|α⟩= UA|α⟩=
U|α⟩a.
An important claim is that Hermitian operators have real eigenvalues. In partic-
ular, if A = A†, then the left-hand side of ⟨α| A |α⟩= ⟨α|α⟩a is real. On the right,
the factor ⟨α|α⟩is real, and so therefore is the eigenvalue a.
Unitary operators have only eigenvalues of absolute value 1. If in particular
A†A = 1, then we have ⟨α|α⟩= ⟨α| A†A |α⟩= a∗⟨α|α⟩a = |a|2⟨α|α⟩, so |α|2 = 1.
If two eigenvectors of a Hermitian operator A = A† belong to different eigen-
values, then those eigenvectors are orthogonal to each other. This is because

4.2 Operators and Observables
295
A |αn⟩= |αn⟩an implies 0 = ⟨α1| A† −A |α2⟩= (a1 −a2) ⟨α1|α2⟩, with a1 ̸= a2, so
⟨α1|α2 ⟩must vanish, as required. In fact, we have already shown this for the principal
axes of the inertia tensor on p. 88. If all eigenvalues an are different, then we may
take the normalized eigenvectors |αn⟩as an expansion basis in Hilbert space, since
they then form a complete orthonormal system, and A will then be diagonal:
⟨αn|αn′⟩= δnn′ ,
1 =

n
|αn⟩⟨αn| ,
and A =

n
|αn⟩an ⟨αn| .
For this reason, the determination of the eigenvalues and eigenvectors of an operator
is referred to as determining the eigen-representation (or diagonalization) of the
operator—it corresponds to a unitary transformation to a more convenient expansion
basis for the operator (which gets along without a double sum). For the sum and the
product of the eigenvalues, no transformation is necessary, since that changes neither
trace nor determinant.
However, an operator can have several linearly independent eigenvectors corre-
sponding to the same eigenvalue, e.g., the unit operator has only the eigenvalue 1. We
then speak of degeneracy: if there are in total N linearly independent eigenvectors
with the same eigenvalue, it is said to be N-fold degenerate. Then N orthonormalized
eigenvectors |αn⟩can be chosen as basis vectors with this eigenvalue. This is what
happens in mechanics when we seek the principal moments of inertia.
The eigenvectors |αn⟩of an operator A also diagonalize the powers Ak of the
operator A and the operator functions f (A):
f (A) =

n
|αn⟩f (an) ⟨αn| .
The special case A−1 = 
n |αn⟩an−1 ⟨αn| shows that none of the eigenvalues can
be zero if the inverse exists. If an = 0 for some n, A is singular.
All functions of the same operator thus have the same eigenvectors, while their
eigenvalue spectra can differ. They also commute with each other. Generally, the
following claim is true: two operators A and B commute with each other if and
only if they share a complete orthonormal system of eigenvectors. If they have
only common eigenvectors, then they commute, because the order of any prod-
uct of their eigenvalues is of no importance: AB = 
n |αn⟩anbn⟨αn| is equal to
BA = 
n |αn⟩bnan⟨αn|. On the other hand, if initially only A = 
n |αn⟩an ⟨αn| is
given with 1 = 
n |αn⟩⟨αn|, then we have
A B =

nn′
|αn⟩an ⟨αn| B |αn′⟩⟨αn′| ,
B A =

nn′
|αn⟩⟨αn| B |αn′⟩an′ ⟨αn′| .

296
4
Quantum Mechanics I
From A B −B A = 0, we deduce that ⟨αn| B |αn′⟩(an −an′) = 0, because the zero
operator in each basis has only zeros as matrix elements. If there is no degeneracy,
then an ̸= an′ holds for all n ̸= n′ and hence ⟨αn| B |αn′⟩is diagonal. Then each |αn ⟩
is also an eigenvector of B. But if eigenvalues an are degenerate, then one can make
use of the freedom in the choice of the basis vectors to diagonalize the matrix B.
When [A, B] = 0, there is thus always a complete system of eigenvectors for both
operators.
If an operator has degenerate eigenvalues, we must search for further operators
which commute with it and lift the degeneracy. Then we can denote the eigenvectors
by the associated eigenvalues, e.g., |αn⟩= |an, bn, . . .⟩. Here we may leave out the
index n on the right-hand side and write for short |a, b, . . .⟩. Each eigenvector differs
from the others by the order of the values. If there is no degeneracy for A, then the
notation |a⟩sufﬁces. Hence we write in the following
A |a⟩= |a⟩a ,
with ⟨a|a′⟩= δaa′
and

a
|a⟩⟨a| = 1 .
Here a is assumed to be discrete. But the operator may also have a continuous
eigenvalue spectrum, or even some discrete and some continuous eigenvalues, as
happens for the Hamilton operator of the hydrogen atom. For continuous eigenvalues,
we have A =

|a⟩a ⟨a| da with ⟨a|a′⟩= δ(a −a′) and

|a⟩⟨a| da = 1. Then sums
have to be replaced by integrals and Kronecker symbols by delta functions.
If a Hermitian operator depends on a parameter λ, e.g., A(λ) = 
n cn(λ) X n,
then so do its eigenstates and eigenvalues. For the eigenvalues, we then have the
Hellmann–Feynman theorem2:
A |a⟩= |a⟩a
=⇒
⟨a| ∂A
∂λ |a⟩= ∂a
∂λ .
For the proof we differentiate ⟨a| A −a |a⟩= 0 with respect to λ and make use of A
being Hermitian:
⟨a| ∂A
∂λ −∂a
∂λ |a⟩+ 2 Re ⟨∂a
∂λ | A −a |a⟩= 0 .
This sufﬁces for the proof because (A −a) |a⟩= 0. The theorem is mainly applied to
the Hamilton operator, but we may use it also for other observables. This is connected
to the adiabatic theorem. If the Hamilton operator H(t) varies sufﬁciently slowly
with time, then a system initially in an eigenstate of H(t0) remains in the eigenstate
developing from it, provided that it always remains non-degenerate. This will be
demonstrated in Fig.4.11 on p. 348 for the time-dependent oscillator.
2Before these two authors, it was already formulated by Güttinger [3] in his diploma thesis.

4.2 Operators and Observables
297
4.2.5
Expansion in Terms of a Basis of Orthogonal Operators
Two operators A and B are said to be orthogonal to each other if the trace of A†B
vanishes. For matrices of ﬁnite dimension, the order of the factors is not important
and nor is it important which factor is chosen as adjoint, since tr(A†B) is then equal
to tr(BA†) = {tr(A†B)†}∗and (A†B)† = B†A. In particular, tr(A†A) is real, and it is
also non-negative, since tr(A†A) = 
nn′ |⟨n|A|n′⟩|2.
We can introduce an orthogonal system of operators Cn as a common expansion
basis for all operators. If we take Hermitian operators (Cn† = Cn), then that simpliﬁes
the considerations even further, but we shall not do so yet. Thus we only require, for
all n, n′,
tr(Cn
† Cn′) = c δnn′ .
Here, c = c∗> 0 is a normalization factor, which we can choose at our convenience.
c = 2 is often chosen, e.g., for the Pauli matrices in Sect.4.2.10 and their generaliza-
tions to more than two dimensions, the Gell-Mann matrix. But occasionally, c = 1
is also chosen. In fact, it can also depend on n = n′, but we shall not pursue this any
further here.
If the basis {Cn} is complete, then for arbitrary operators A, we have
A =

n
Cn
tr(Cn†A)
c
,
with tr(Cn
†A) = {tr(CnA†)}∗.
For a Hermitian basis {Cn† = Cn}, all Hermitian operators have real expansion coef-
ﬁcients.
In an N-dimensional vector space, we need N 2 basis operators, one for each matrix
element. But they would all also commute with each other. This is no longer true for
our general basis. Nevertheless, their commutators can be expanded:
i [Cn′, Cn′′] =

n
Cn
tr(i Cn† [Cn′, Cn′′])
c
.
If the basis consists of Hermitian operators, then on the right there are only real expan-
sion coefﬁcients, the so-called structure constants of the associated Lie algebra (see
[8]), which are antisymmetric in the three indices: symmetric for cyclic permutations
since tr(Cn[Cn′, Cn′′]) = tr(Cn′[Cn′′, Cn]), and antisymmetric for anti-cyclic permu-
tations since tr(Cn[Cn′, Cn′′]) = −tr(Cn′[Cn, Cn′′]). Unitary transformations do not
change that.
It is advantageous to start the basis with the unit operator, C0 ∝1, because this
operator commutes with all other operators and only its trace is non-zero, since the
other operators should be orthogonal to it: trCn ∝δn0.

298
4
Quantum Mechanics I
A ﬁrst example will be presented in Sect.4.2.10. In a two-dimensional vector
space, the Pauli operators are useful as an expansion basis. With the Wigner function
(Sect.4.3.5), we also employ an operator basis.
4.2.6
Observables. Basic Assumptions
In the above, we have provided the mathematical tools, and now we turn to physics
again. We start with basics. So far we have assumed only that (pure) states can be
represented by proper or improper Hilbert vectors and that the scalar product ⟨ψ|ϕ⟩
yields the probability amplitude for the state |ψ⟩to be contained in the state |ϕ⟩.
Now we add the following:
To every measurable quantity (an observable, e.g., position, momentum, energy,
angular momentum) is assigned a Hermitian operator. Its real eigenvalues are
equal to all possible measurable results of this observable.
If the statistical ensemble is in an eigenstate, then the associated eigenvalue is always
measured: the measured result is sharp. And conversely, if the same value is always
measured, then it is in this state. In contrast, if the ensemble is not in an eigenstate
of the measurable operator, then the measured results scatter about the average value
with a non-zero uncertainty.
For dynamical variables, we may take only Hermitian operators, because only
they have real eigenvalues; and measured results are real quantities. For a complex
quantity, we would have to measure two numbers. As possible measured results for
the dynamical variable A, only the eigenvalues {a} of the assigned operator A occur.
This is the physical meaning of the eigenvalues. Furthermore, the orthogonality of
two eigenstates can be interpreted physically: the two states always deliver different
experimental values. But note that, for degenerate states, we have to consider further
properties.
If the system ensemble is in the state |a⟩, the measured results for all variables
f (A) are ﬁxed, namely, f (a). In contrast, for all other quantities B with [A, B] ̸= 0
in the statistical ensemble, generally different values b will be measured. If B does
not commute with A, then in most cases |a⟩cannot be represented by a single eigen-
vector of B. Then the state |a⟩= 
b |b⟩⟨b|a⟩can only be decomposed into several
eigenstates |b⟩of B. This is the physical relevance of the superposition principle.
If the system is prepared in the state |ψ⟩, then generally different values for the
variable A are measured—except for the case where |ψ⟩is an eigenstate of A. We
consider now the average value, and in the next section the uncertainty. For the
average value, we have to weight the possible measured results a with the associated
probabilities |⟨a|ψ⟩|2. Since we measure the value a with the probability |⟨a|ψ⟩|2
and the value a′ with the probability |⟨a′|ψ⟩|2,

4.2 Operators and Observables
299
A =

a
|⟨a|ψ⟩|2 a =

a
⟨ψ|a⟩a ⟨a|ψ⟩= ⟨ψ| A |ψ⟩≡⟨A⟩.
Instead of the average value A, we also call it the expectation value ⟨A⟩. The matrix
element ⟨ψ| A |ψ⟩delivers the expectation value for the observable A in the state |ψ⟩.
The expectation value is determined from the set of possible experimental values. For
discrete eigenvalues, it may deﬁnitely differ from all possible experimental values,
e.g., it may lie between the n th and (n + 1) th level.
In the real-space representation {|r⟩}, we have correspondingly
⟨ψ|A|ψ⟩=

d3r d3r′ ⟨ψ|r ⟩⟨r |A|r ′⟩⟨r ′|ψ⟩,
with ⟨r ′|ψ⟩= ψ(r ′) and ⟨ψ|r⟩= ψ∗(r ), according to Sect.4.1.6. In most cases,
we have to deal with local operators. These are diagonal in the real-space represen-
tation, so the double integral becomes a single one. For local operators, we thus have
⟨ψ|A|ψ⟩=

d3r ψ∗(r ) A(r ) ψ(r ) =

d3r |ψ(r )|2 A(r ).
Thegeneral matrixelement⟨ψ| A |ϕ⟩withψ ̸= ϕ cannot beinterpretedclassically
for three reasons: it depends on two states, it is a complex number, and it involves
(like ⟨ψ| and |ϕ⟩) an arbitrary phase factor. In quantum theory, we deal with the
transition amplitude from |ϕ⟩with A to ⟨ψ|. Note that it is important to get used to
reading the expressions in quantum theory from right to left: the operator A acts on
the ket-vector and only then is the probability amplitude of this new ket-vector with
the bra-vector of importance. These difﬁculties with the classical meaning do not
occur for the diagonal elements ⟨ψ| A |ψ⟩: because A is Hermitian, it is real, and if
|ψ⟩is multiplied by exp(iφ), then likewise ⟨ψ| is multiplied by exp(−iφ).
4.2.7
Uncertainty
If the system in an eigenstate of the considered measurable quantity A, then the
measured result is known sharply (with certainty) and A = 0. Otherwise, different
experimental values occur with their corresponding probabilities. Nevertheless, the
average value A of the experimental values is equal to the expectation value ⟨ψ| A |ψ⟩,
and also A2 = ⟨ψ| A2 |ψ⟩is known. Hence, according to p. 275, the uncertainty is
also known:
A =

⟨ψ| A2 |ψ⟩−⟨ψ| A |ψ⟩2 .
It only vanishes if |ψ⟩is an eigenstate of A. Otherwise we have A2 > A 2. In particular,
if we take a basis with |ψ⟩as the ﬁrst vector, then for Hermitian operators A, we
have

300
4
Quantum Mechanics I
⟨ψ| A2 |ψ⟩= ⟨ψ| A |ψ⟩2 + ⟨ψ| A |ψ′⟩⟨ψ′| A |ψ⟩+ · · ·
= ⟨ψ| A |ψ⟩2 + |⟨ψ| A |ψ′⟩|2 + · · · .
If |ψ⟩is not an eigenstate of A, the ﬁrst term is not the only one to contribute. We
then have A > 0.
For the uncertainty relation, we consider two Hermitian operators A and B. Then
with |α⟩≡(A −A) |ψ⟩and |β⟩≡(B −B) |ψ⟩, and because (A)2 = ⟨ψ| (A −
A)2 |ψ⟩= ∥α∥2 and (B)2 = ∥β∥2, we obtain for ∥ψ∥= 1,
(A)2 · (B)2 = ∥α∥2∥β∥2 ≥|⟨α|β⟩|2 = |⟨ψ| (A −A)(B −B) |ψ⟩|2 ,
where we have used Schwarz’s inequality (see p. 283). For Hermitian operators
C and D, according to p. 293, we now have | CD |2 = 1
2{C, D}
2
+ 1
2i[C, D]
2
. With
{A −A, B −B} = {A, B} −2 A B and [A −A, B −B] = [A, B], it thus follows that
(A)2 · (B)2 ≥⟨ψ| 1
2 {A, B} −A B |ψ⟩2 + ⟨ψ| 1
2i [A, B] |ψ⟩2 .
If the operators A and B do not commute with each other, but if their commutator
is equal to the unit operator up to an imaginary constant, the last term contributes
positively and the two quantities A and B cannot both be sharp.
Heisenberg’s uncertainty relation for canonically conjugate quantities A and B,
viz.,
A · B ≥1
2 ℏ,
can thus be guaranteed with non-commuting operators. They only have to obey the
requirement
[A, B] = iℏ1 .
According to Born and Jordan, we can require this of all canonically conjugate
quantities and this is the very reason why we actually deal with operators and Hilbert
vectors. In the next section, we shall point out connections with these commutation
relations.
There are two conditions under which the product of the uncertainties A · B
is as small as possible. Firstly, we must have 1
2 AB + BA = A B, or AB −1
2 [A, B] =
A B, so that (A −A) B = 1
2 [A, B]. Secondly, according to p. 283, only if the consid-
ered vectors |α⟩and |β⟩are parallel to each other does Schwarz’s inequality become
an equation, i.e., if
(A −A) |ψ⟩= λ (B −B) |ψ⟩.

4.2 Operators and Observables
301
But then also
1
2 [A, B] = ⟨ψ|(A −A) B|ψ⟩= λ∗(B −B) B = λ∗(B)2. Here,
according to the initial equation in the considered extreme case, we have A B =
±( −i
2 [A, B]), where the left-hand expression is ≥0 and the one on the right ﬁxes the
sign. In short then, A B = ∓iλ∗(B)2, or
λ = ∓i A
B .
For canonically conjugate quantities A and B with [A, B] = iℏ1, we have to take the
upper sign, and for i [A, B] > 0, the lower sign.
4.2.8
Field Operators
Once again, we turn to the wave–particle duality and here restrict ourselves to (many)
“quanta in the same state”, e.g., with equal momentum. So the following considera-
tions apply only to bosons, but not fermions, e.g., not electrons, because according
to the Pauli principle only one fermion may occupy a given state. The discussion
here will be useful later for the harmonic oscillator (Sect.4.5.4), where the transition
to neighboring states is always connected with an oscillation quantum of the same
energy. Note that sound quanta are also called phonons, and light quanta photons.
The Dirac symbol |n⟩will now be used to indicate that there are n particles. The
numbers n ∈{0, 1, 2, . . .} are the eigenvalues of the number operator N and |n⟩its
eigenstates, which we shall investigate now in some detail. To this end, we introduce
(non-Hermitian) creation and annihilation operators:
†|n⟩∝|n+1⟩⇐⇒|n⟩∝|n−1⟩.
Note that, in many textbooks on quantum mechanics, and also according to the
IUPAP recommendations, a or b is used instead of , which is common practice in
ﬁeld theory though, and indeed this operator has something to do with the state |ψ⟩.
|ψ⟩results in particular in the vacuum, as we shall soon show. Instead of the state
|ψ⟩, we may also speak of the ﬁeld |ψ⟩, if we think of its real-space representation
⟨r |ψ⟩≡ψ(r ). Since negative eigenvalues n may not occur, |0⟩has to deliver the
zero vector |o⟩. Note, however, that |0⟩is not the zero vector |o⟩, but the state with
n = 0. If n gives the number of “particles”, then |0⟩is the state without particles, the
“vacuum”, for which ⟨0|0⟩= 1, in contrast to ⟨o|o⟩= 0.
Both † and †  therefore have the eigenvectors |n⟩. We now require
N = †  .
Hence, due to the normalization, it follows from
n = ⟨n|N|n⟩= ⟨n|† |n⟩∝⟨n −1|n −1⟩,
for
n > 0 ,

302
4
Quantum Mechanics I
that
 |n⟩= |n −1⟩√n
⇐⇒
† |n⟩= |n + 1⟩
√
n + 1 ,
if we choose the phase factor (arbitrarily) equal to 1. The operator  thus reduces the
particle number by one, and is therefore called the annihilation operator, while the
adjoint operator † increases it by one and is therefore called the creation operator.
This leads to
|n⟩=
1
√
n!
(†)n |0⟩,
i.e., all states can be created with this from the “vacuum state” |0⟩. It is special
insofar as the annihilation operator  maps only this to the zero vector |o⟩. We
have † |0⟩= |o⟩, but † |0⟩= |0⟩, and generally, †|n⟩= |n⟩n as well as
†|n⟩= |n⟩(n + 1), for all natural numbers n. Hence, we arrive at the basic com-
mutation relation
[, †] ≡† −† = 1 .
Thus † = 1 + N holds, andweobtainfrom †† = † (1 + N), or theadjoint
† = (1 + N) ,
[N, †] = † ,
[N, ] = − .
Conversely, from [, †] = 1, we can derive the real eigenvalue spectrum of
† and the matrix elements of  and † in the eigenbasis of this Hermi-
tian operator, for an appropriate phase convention. In particular, from † =
(† −[, †]) = († −1), we conclude that the operator  creates more
eigenvectors of † from eigenvectors of †, but with an eigenvalue that is
reduced by one. On the other hand, this decrease ﬁnally has to lead to the zero vec-
tor, and therefore to an end, since ⟨. . . |†| . . .⟩, being the square of the norm of the
Hilbert vector | . . .⟩, may not become negative and yet is still equal to one of the pos-
sible eigenvalues of †. Hence † has the natural numbers as eigenvalues, and
we choose the phases such that †|n⟩= |n + 1⟩√n + 1 ⇐⇒|n⟩= |n −1⟩√n
holds.
Using the ﬁeld operators, we can expand the projection operator |0⟩⟨0|:
|0⟩⟨0| =

n
(−)n
n!
†n n ,
since, for all natural numbers m, ⟨m|0⟩⟨0|m⟩= δm0 = (1 −1)m = 
n(−)nm
n

and
⟨m|†n n|m⟩= n!
m
n

, and the operator is diagonal in the basis {|m⟩}.

4.2 Operators and Observables
303
Even though the operators † and † have discrete eigenvalues, there are
inﬁnitely many of them. Hence the associated basis is of inﬁnite dimension and the
traces of both operators diverge. Only then can tr(†) = tr(†) hold on the one
hand and † −† = 1 on the other, whence tr[, †] ̸= 0 is valid.
Even if we reject very large eigenvalues n ≫1 as unphysical, a ﬁnite basis does
of course exist. To investigate this possibility more closely (we shall use it in the next
section, but only there), we introduce an upper limit s and require n ∈{0, . . . , s}.
Here, s is then assumed large, so that the ﬁnite basis comprises all physically neces-
sary states. However, we can then no longer require [, †] = 1, since for a ﬁnite
basis, the trace of the commutators must vanish. But according to Pegg and Barnett
[4], there are operators (we shall call them ), which act on physical states like 
and nevertheless need only a ﬁnite basis:
 =
s

n=1
|n −1⟩√n ⟨n|
⇐⇒
† =
s

n=1
|n⟩√n ⟨n −1| .
With the ﬁnite sum and with 1 = s
n=0 |n⟩⟨n|, we now obtain
[, †] = 1 −|s⟩(s + 1) ⟨s| .
The new term ensures that tr[, †] = 0, as is appropriate for a ﬁnite basis.
Before we make any further use of the ﬁeld operators for bosons, let us make here
a brief mention of the ﬁeld operators for fermions, even though we shall treat these
in more detail in Sect.4.2.10. We use once again N = †, but N will only have the
eigenvalues 0 and 1, as required by thePauli principle, and 2 and hence also (†)2
will always be zero. We write the two states as column vectors, with |0⟩as
0
1

and
|1⟩as
1
0

, for the number to increase upwards (conversely, for bosons, the state |n⟩is
a column vector with just zeros and a 1 in the nth row). Then all these requirements
can be satisﬁed with
N =
 1 0
0 0
	
,
 =
 0 0
1 0
	
,
and consequently
† =
0
1
0
0
	
,
† =
 0
0
0
1
	
.
For fermions, it is thus the anti-commutator of  and † which is equal to 1:
† + † = 1 ,
and † |0⟩= |1⟩, † |1⟩= |o⟩=  |0⟩,  |1⟩= |0⟩. We often ﬁnd 0 written here
instead of |o⟩, even though A|ψ⟩is a Hilbert vector.

304
4
Quantum Mechanics I
4.2.9
Phase Operators and Wave–Particle Dualism
The natural numbers as eigenvalues ﬁt into the particle picture. But because of the
necessary interference, we need an uncertain particle number for wave–particle dual-
ity, thus a superposition of different states |n⟩. Then the initial equations [, †] = 1
and N = † are still valid further on, but now the phase factors are also important
in the wave picture for the superposition of different states |n⟩.
The appropriate determination of the phase operators was long a subject of
research. Dirac was himself occupied with this in 1927. Only Pegg and Barnett
(see the last section) succeeded in solving the problem: the basis must not be inﬁnite,
but only of unmeasurably high dimension. Let us discuss this now, but simply set
the phase of the vacuum equal to zero, not leaving it open.
The phases φ are unique only between 0 and 2π. In order not to introduce a
continuous basis (with improper Hilbert vectors), we set
φm =
2π
s + 1 m ,
with m ∈{0, . . . s} .
Here (as in the last section), we take a very large limit s, but nevertheless ﬁnite, since
the phase (like any continuous quantity) cannot be measured with arbitrary accuracy.
We also introduce a Hermitian operator 	 with eigenvalues φm, such that 	|φm⟩=
|φm⟩φm. It is important to show that the states m = 0 and m = s are neighboring
states. Hence, initially, we search for the unitary operator E = exp(i	) with the
property E |φm⟩= |φm⟩exp(iφm) for m ∈{0, . . . s} (see Fig.4.3).
The basis {|φm⟩} is assumed orthonormal and complete. Then we have
E =
s

m=0
|φm⟩exp(iφm) ⟨φm|
with
⟨φm|φm′⟩= δmm′ .
It should be stressed here that s is assumed very large, even though we leave out
lims→∞in front of the sum. In contrast to the last section, however, we may anticipate
Fig. 4.3 Eigenvalues of the operator E = exp(i	) with 	 = 	†. These are evenly distributed over
the unit circle in the complex plane. Here s = 44 has been chosen. It could be much larger. The
only requirement is that it should be ﬁnite

4.2 Operators and Observables
305
that all states |φm⟩can be important physically, those with m ≈s as much as those
with m ≈0, while we shall not take the particle number to be arbitrarily large.
We now relate phase and particle number states. The wave–particle duality allows
a sharp phase only for an uncertain particle number, and conversely a sharp phase
only for an uncertain particle number. Here we simultaneously require the expansion
bases {|n⟩} and {|φm⟩}, and use
|φm⟩=
s

n=0
|n⟩⟨n|φm⟩
and
|n⟩=
s

m=0
|φm⟩⟨φm|n⟩,
with ⟨φm|n⟩= ⟨n|φm⟩∗. Here the same limit s was deliberately chosen for both expan-
sions, since the last equations are then fully valid—approximations were made pre-
viously, in particular, with discrete phases instead of continuous ones and with a
ﬁnite number of particles. If s is sufﬁciently large, these assumptions are probably
justiﬁed.
As the eigenvalues show, E is unitary (EE† = E†E = 1). With † = N and the
known decomposition into amplitude and phase factor, we set
 = E
√
N
⇐⇒
† =
√
N E† .
Hence,  = s
n=1 |n −1⟩√n ⟨n| implies that
E =
s

n=1
|n −1⟩⟨n| + |s⟩⟨0| .
The last term results from the unitarity of E (where we have chosen the phase
factor equal to unity). Consequently, we have ⟨n|E = ⟨n + 1|, for 0 ≤n < s, and
⟨s|E = ⟨0|. Hence the eigenvalue equation of E delivers the recursion formula
⟨n + 1|φm⟩= ⟨n|E|φm⟩= ⟨n|φm⟩exp(iφm). If we choose the phase of the vac-
uum state |0⟩(arbitrarily) equal to zero, we ﬁnd ⟨n|φm⟩= exp(inφm)/√s + 1 as
a solution of the recursion formula, where the normalization factor results from
1 = ⟨φm|φm⟩= s
n=0⟨φm|n⟩⟨n|φm⟩.
Hence in the basis {|n⟩}, the three matrices N,  (or ), and E read
N =
⎛
⎜⎜⎜⎜⎜⎝
0 0 0 · · ·
0 1 0 · · ·
0 0 2 · · ·
... ... ... ...
⎞
⎟⎟⎟⎟⎟⎠
,  =
⎛
⎜⎜⎜⎜⎜⎝
0
√
1 0 · · ·
0 0
√
2 · · ·
0 0
0 · · ·
...
...
...
...
⎞
⎟⎟⎟⎟⎟⎠
, E =
⎛
⎜⎜⎜⎜⎜⎝
0 1 0 · · ·
0 0 1 · · ·
0 0 0 · · ·
... ... ... ...
1
⎞
⎟⎟⎟⎟⎟⎠
.
The element 1 in the matrix E stands at the end of the ﬁrst column—then E = exp(i	)
is unitary and 	 cyclic.
From the expression for E in the particle number representation, we have the
commutation relation

306
4
Quantum Mechanics I
[E, N] = E −|s⟩(s + 1) ⟨0| ,
and also [E†, N] = −[E, N]† = −E† + |0⟩(s + 1) ⟨s|, along with [, †] = 1 −
|s⟩(s + 1)⟨s|. We now decompose the unitary operator E like exp(iφ), using Euler’s
formula to obtain
E ≡C + iS ,
with C = E + E†
2
= C† ,
and S = E −E†
2i
= S† ,
and ﬁnd
[C, N] = +iS + s + 1
2
(|0⟩⟨s| −|s⟩⟨0|) ,
[S, N] = −iC + is + 1
2
(|0⟩⟨s| + |s⟩⟨0|) ,
for these Hermitian operators, along with C2 + S2 = 1
4

(E + E†)2 −(E −E†)2
=
1
2(E E† + E†E) = 1. Note that, we have [C, S] = 0, because [E, E†] = 0.
According to Sect.4.2.7, we may now derive an uncertainty relation between
particle number and phase. In particular, we make use of A · B ≥1
2 |[A, B]| for
A = A† and B = B†. Since not all physical states overlap with the state |s⟩, we obtain
initially
C · N ≥1
2 | S |
and
S · N ≥1
2 | C | ,
along with C · S ≥0.
If we now associate the phase operator 	 with the unitary operator E, according
to p. 293, viz.,
E = exp(i	) ,
with 	 = 	† ,
then, for small phase uncertainty 	 ≪π,
C ≈cos 	 ,
C ≈| sin 	 | 	 ,
S ≈sin 	 ,
S ≈| cos 	 | 	 .
Hence the above-mentioned uncertainty relations deliver the inequality N · 	 ≥
1
2 already announced in Sect.4.1.3. However, this is not generally valid. If, for exam-
ple, all phases between 0 and 2π are equally probable (the phase uncertainty thus
being as large as possible), then C and S are both zero. Then N = 0 may hold,
even though we have 	 = π/
√
3. Note that the phase uncertainty depends on the
reference phase—this is connected inextricably with the periodicity. In particular, if
two neighbouring phase states φm and φm+1 are occupied with equal probability, then
for m < s, the uncertainty for s →∞is negligible, while for m = s, it is equal to π.
The reference phase is then chosen so that 	 becomes as small as possible, or so

4.2 Operators and Observables
307
Fig. 4.4 Uncertainty relation between particle number and phase. If all phases are equally probable,
then the phase uncertainty is 	 = π/
√
3 and never greater. The continuous curve shows N ·
	 = 1/2, thus the approximation for 	 ≈0. The dashed curve shows the approximation for
N ≈0. The two approximations complement each other quite well—only for N ≈1/2, or
	 ≈1, do they differ somewhat from the true curve, and this can be seen only if the image is
enlarged. See Phys.Lett. A 218 (1996) 1
that 	 ≈π. For N ≈0, it is better to take three neighboring states in the particle
number representation with the amplitudes
⟨n |ψ⟩=

1 −(N)2
and
⟨n ± 1|ψ⟩= exp(± iφ )
√
2
N ,
since, after a Fourier transform, we obtain a phase uncertainty that is as small as
possible:
(	)2 ≥1
3π2 −4
√
2 N

1 −(N)2 + 1
2 (N)2 .
Here, when calculating ⟨	⟩and ⟨	2⟩, we replace the sums over φm by integrals and
⟨φ|ψ⟩by (2π)−1/2 
n exp(−inφ) ⟨n|ψ⟩. The exact limit is shown in Fig.4.4. It is
rather well described by either approximation.
Hence now we understand that the particle and wave pictures—granularity and the
capacity to interfere—are not in contradiction if we take into account the uncertainties
in particle number and phase.
In the rest of this chapter (Quantum Mechanics I), we will consider only one-
particle states (as representative of a statistical ensemble of bosons or fermions), but
now these particles will no longer be restricted to a single state.

308
4
Quantum Mechanics I
4.2.10
Doublets and Pauli Operators
The two-dimensional vector space is highly instructive and full of possibilities for
applications. It is needed for the spin states of fermions with spin 1/2 (e.g., for
electrons), for isospin (neutron and proton states as the two states of the nucleon),
and also for the Pauli principle and model calculations of the excitation of atoms
(Sect. 5.5.7).
If we call the two states |↑⟩and |↓⟩(up and down), then, according to Sect.4.2.8,
the umklapp operator  can be introduced for this system with the property
† + † = 1 .
We now write |↓⟩instead of |0⟩and |↑⟩instead of |1⟩, because up and down are
easier to remember than the position of 0 and 1. If we consider these states as column
vectors, then with † |↓⟩= |↑⟩, † |↑⟩= |o⟩=  |↓⟩, and  |↑⟩= |↓⟩, we have
 =
0
0
1
0
	
,
† =
0
1
0
0
	
,
† =
0
0
0
1
	
,
† =
 1
0
0
0
	
.
All other 2 × 2 matrices can be expressed as linear combinations of these. However,
we prefer to have Hermitian matrices as a basis, including among them the unit
matrix:
C0 = † + † = 1 =
 1
0
0
1
	
,
C1 =
 + †
= σx =
0
1
1
0
	
,
C2 = i ( −†) = σy =
0 −i
i 0
	
,
C3 = † −† = σz =
 1
0
0 −1
	
.
The notation Cn is taken from Sect.4.2.5, but the notation with 1 and the Pauli
operator σ is more often used, where σ± = 1
2 (σx ± iσy) is introduced. Clearly, we
also have
 = σx −i σy
2
= σ−,
† = σx + i σy
2
= σ+ ,
† = 1 −σz
2
,
† = 1 + σz
2
.
The operators of the new basis {Cn} are not only Hermitian, but also unitary, this
resulting from the necessary normalization in tr(Cn†Cn′) = δnn′ tr1 = 2 δnn′, since

4.2 Operators and Observables
309
their squares are equal to the unit operator:
Cn = Cn
† = Cn
−1 ,
trCn = 2 δn0 .
Hence, their eigenvalues are real and of absolute value 1. They result from the last
equation: C0 has the two-fold eigenvalue 1, while the other three operators each
have the eigenvalues +1 and −1. In addition, these three do not commute with each
other, but anti-commute. Matrix multiplication delivers C1C2 = i C3, and because
Cn† = Cn, we thus have C2C1 = −i C3 = −C1C2. Cyclic permutation of the indices
1, 2, 3 is allowed, since C2C3 = C2(iC2C1) = i C1 = i C1C2C2 = −C3C2, and so
on:
C1C2 = i C3 = −C2C1 ,
or [C1, C2] = 2i C3
and cyclic permutations.
Here the notation with the Pauli operator σ proves useful because the commutator
can then be written as a vector product:
σ × σ = 2i σ .
The vector product of the Pauli operator σ with itself does not vanish, in contrast to
what happens with classical vectors, because its components do not commute with
each other. Hence apart from 1, only one further component can be diagonalized—in
our example, this is σz = C3. But it could also be σx or σy. Here only a rotation
(unitary transformation) would be necessary, but note that σz would then no longer
be diagonal.
The four operators are orthogonal to each other:
tr(CnCn′) = 2 δnn′ .
Here we recognize why in Sect.4.2.5 the normalization of the basis operators was
left open. With this orthonormalization, according to p. 297, all 2 × 2-matrices A
can be written in the form
A = 1 trA + σ · tr(σ A)
2
.
Theireigenvaluesfollowfromdet (A −a1) = 0,hencea2 −atrA + det A = 0,which
implies
a± = trA ±

(trA)2 −4 det A
2
.
If we expand the eigenvectors |±⟩in terms of the other basis {|↑⟩, |↓⟩}, then from
A|±⟩= |±⟩a±, we obtain the homogeneous system of equations

310
4
Quantum Mechanics I
Fig. 4.5 Level repulsion. For ﬁxed interaction V = 2⟨↓| A | ↑⟩, the splitting of the eigenvalues a±
(i.e., a+ −a−) is shown here (red) as a function of the unperturbed level distance δ = ⟨↑| A |↑⟩−⟨↓
| A |↓⟩. Here the state |+⟩goes from |↓⟩to |↑⟩, the state |−⟩from −|↑⟩to |↓⟩. Without anti-crossing,
the dashed blue lines would be valid
⟨↑| A −a± |↑⟩⟨↑|±⟩+ ⟨↑|
A
|↓⟩⟨↓|±⟩= 0 ,
⟨↓|
A
|↑⟩⟨↑|±⟩+ ⟨↓| A −a± |↓⟩⟨↓|±⟩= 0 .
This ﬁxes the expansion coefﬁcients only up to a common factor, but because of
the normalization condition |⟨↑|±⟩|2 + |⟨↓|±⟩|2 = 1, only a common phase factor
remains open. If (for A = A†), we set
|+⟩
|−⟩
	
=

cos α
eiβ sin α
−e−iβ sin α
cos α
	 |↑⟩
|↓⟩
	
,
with real parameters 0 ≤α ≤1
2π and 0 ≤β < 2π, according to p. 293, and using
the abbreviations (for a Hermitian operator A they are real)
δ = ⟨↑| A |↑⟩−⟨↓| A |↓⟩
and
 = a+ −a−≥|δ| ,
since  =

δ2 + 4|⟨↓| A |↑⟩|2, we obtain the equation
exp(iβ) tan α = 2 ⟨↓| A |↑⟩
 + δ
.
The phase of ⟨↓| A |↑⟩is thus equal to β and that of ⟨↑| A |↓⟩is equal to −β, while
δ and  determine the parameter α:
cos α =

 + δ
2
and
sin α =

 −δ
2
.
If A is the Hamilton operator H, then, with  ≥|δ|, we speak of level repulsion or
anti-crossing. Once the off-diagonal element ⟨↓| A |↑⟩contributes, the separation
between the eigenvalues increases (see Fig.4.5).

4.2 Operators and Observables
311
4.2.11
Density Operator. Pure States and Mixtures
The properties of a given statistical ensemble can be determined by appropriate
measurements. They deliver the expectation values of the corresponding Hermitian
operators A, and hence we arrive at conclusions relating to the state of the ensemble.
So far we have dealt only with pure states |ψ⟩. Then we have
⟨A⟩= ⟨ψ| A |ψ⟩=

nn′
⟨ψ|n′⟩⟨n′| A |n⟩⟨n|ψ⟩,
if we assume a countable basis, otherwise there is a double integral instead of the
double sum. The expression on the right can be simpliﬁed to 
n⟨n|ψ⟩⟨ψ|A|n⟩. Hence
we may also write ⟨A⟩= tr(Pψ A), where Pψ = |ψ⟩⟨ψ| was introduced on p. 291 as
the projection operator acting on the (pure) state |ψ⟩.
A ﬁnite number of measurable quantities sufﬁces to determine the given statistical
ensemble uniquely. An ensemble of experimental values {⟨Ak⟩} will describe our
object.Forexample,inSect.4.1.4,wetooktheensembleofelectronswithmomentum
⟨P⟩and spin polarization ⟨S⟩. But then the statistical ensemble does not need to form
a pure state |ψ⟩. It may also be a mixture thereof, thus an incoherent superposition
of pure states |n⟩(or projectors |n⟩⟨n|) with probabilities ρn. Hence, instead of Pψ,
we now take the general density operator
ρ =

n
|n⟩ρn ⟨n|
⇐⇒
⟨A⟩=

n
ρn ⟨n| A |n⟩= tr(ρ A) .
The ensemble of experimental values {⟨Ak⟩} ﬁxes the density operator ρ, since the
matrix elements of A follow from the relevance of this operator (position, momentum,
energy, etc.) and hence {⟨Ak⟩} = {tr(ρ Ak)} is an inhomogeneous linear system of
equations for the matrix elements of ρ. Here the density operator describes the given
system and the Hermitian operators Ak the observables.
The properties of ρ compiled in the following are valid for pure as well as for
mixed states (as shown below, the two kinds of states can be distinguished by the
easily veriﬁable attributes of the density operator). We want to ﬁx ρ only by {⟨Ak⟩} =
{tr(ρ Ak)} and make use of known properties of the observables.
The density operator is a matrix of ﬁnite dimension, determined by a ﬁnite number
of experimental values. Hence the operators commute in tr(ρA).
All Hermitian operators A have real expectation values. Hence also the density
operator is Hermitian: ⟨A⟩−⟨A⟩∗= tr{(ρ −ρ†) A} must always vanish. In addi-
tion, all observables with only positive eigenvalues (so-called positive-deﬁnite oper-
ators) have positive expectation values, so the density matrix has to be positive-
semideﬁnite—none of the diagonal elements of ρ can be negative in any represen-
tation. Since the unit operator always has unit expectation value, the trace of ρ must
be equal to 1. Thus we can list a total of three requirements, viz.,

312
4
Quantum Mechanics I
ρ = ρ† ,
⟨n| ρ |n⟩≥0 ,
trρ = 1 ,
and this actually results in the fact that ρ is positive-semideﬁnite, with ρ = ρ†. Here
the diagonal element ⟨n|ρ|n⟩gives the probability (or probability density) for the
state |n⟩, while the last equation corresponds to our normalization condition for the
probabilities. The off-diagonal elements lead to interference and are occasionally
referred to as the coherences of the system.
Under unitary transformations all operators change, including the density opera-
tor, according to the prescription A′ = UAU †. But here the expectation values remain
the same, because with the ﬁnite dimension of ρ, the trace of ρ′A′ = UρAU † remains
constant, according to p. 294.
For a pure state, we have ρ = ρ2, but not for a mixture. In the eigen-representation
of ρ, ρ2 is also diagonal, and for a pure state only one of these diagonal elements
is different from zero (namely 1), but for a mixture at least two are different from
zero—and these are then smaller for ρ2 than for ρ. With trρ = 1, it thus follows that
trρ2 = 1for all pure states,
trρ2 < 1for all mixtures.
With the trace of ρ2, we thus have a very simply test of whether we are dealing
with a pure state or a mixture, since for the trace, we do not need to search for the
eigen-representation, because the diagonal elements sufﬁce.
In particular, for a two-level system with trρ = 1 and because tr(σρ) = ⟨σ⟩,
according to the last section, we have
ρ = 1 + σ · ⟨σ⟩
2
and
trρ2 = 1 + ⟨σ⟩· ⟨σ⟩
2
.
The quantity ⟨σ⟩is called the polarization. Since the eigenvalues of the components
of σ are equal to ±1, we have |⟨σ⟩| ≤1. If the equality sign holds here, then we have
a pure state, otherwise a mixture, e.g., for an unpolarized state ⟨σ⟩= 0: unpolarized
electrons form a mixture, their two spin states being incoherently superposed.
For an N-state system, the density matrix has N 2 elements, which are determined
by equally many real numbers because ρ† = ρ. One of them is known already due
to the normalization. Thus N 2 −1 experimental values sufﬁce for this system. In
contrast, we could ﬁx a pure state with just 2N −2 real numbers, or N complex
numbers, but where two real numbers are omitted because of the normalization and
the arbitrary common phase. For the density operator, there is no arbitrariness in the
phase—its bearing on the bra- and ket-vector cancels.
The smaller trρ2, the less pure the N-state system appears, and trρ2 is smallest
when all eigenvalues are equal, in which case it is a complete mixture, and then ρ is a
multiple of the unit operator, in particular, with the eigenvalues N −1, since trρ = 1.
Hence we have upper and lower bounds for trρ2:

4.2 Operators and Observables
313
1
N ≤trρ2 ≤1 .
These only depend on the dimension N of the Hilbert space.
Let us consider these properties for the operator basis {Cn} from Sect.4.2.5. For
Hermitian basis operators, the expansion coefﬁcients are real, and we have
ρ = 1
c
N 2−1

n=0
Cn ⟨Cn⟩
and
trρ2 = 1
c
N 2−1

n=0
⟨Cn⟩2 .
If C0 is a multiple of the unit operator, then the old requirement trC02 = c in the
N-dimensional Hilbert space leads us to C0 = √c/N 1, and hence with trρ = 1 to
⟨C0⟩= trρ C0 = √c/N. Then only the remaining N 2 −1 expectation values ⟨Cn⟩
are important, and these can be taken as components of a vector, usually called the
Bloch vector (more on that in Sect.4.4.3). The square of its length is
N 2−1

n=1
⟨Cn⟩2 = c

trρ2 −1
N

,
thus zero for complete mixtures and greatest for pure states, when it is equal to
c (1 −N −1).
4.2.12
Space Inversion and Time Reversal
With a space inversion P, the space directions are reversed, and with a time reversal
T , only motions are reversed:
P R P−1 = −R ,
T R T −1 = +R ,
P P P−1 = −P ,
T P T −1 = −P .
The space inversion is a unitary transformation, but not the time reversal, since
unitary transformations do not change algebraic relations between operators—
however, T [X , P] T −1 = [T X T −1, T PT −1] is equal to −[X , P]. This can
only be inserted into the previous context without contradiction if T is an anti-
linear operator, thus changing all numbers into their complex conjugates, and hence
T (iℏ1) T −1 into −iℏ1.
For anti-linear operators, according to p. 289, we have T |ψ a⟩= (T |ψ⟩) a∗. If
we set
|ψ⟩≡T |ψ⟩,

314
4
Quantum Mechanics I
then |ψ⟩can be expanded with |ψ⟩= 
n |n⟩ψn, |ψ⟩= 
n |n⟩ψn∗, and correspond-
ingly ⟨ϕ| = 
n ϕn⟨n|. We obtain generally
⟨ϕ|ψ⟩= ⟨ϕ|ψ⟩∗.
Notethat|ψ⟩= T |ψ⟩dependsanti-linearlyon|ψ⟩,asdoesthescalarproduct⟨ϕ|ψ⟩.
Consequently,itscomplexconjugatevaluedependslinearlyon|ψ⟩.Correspondingly,
from |χ⟩= A |ψ⟩, we infer |χ⟩= TAT −1 |ψ⟩, and then also
⟨ϕ | TAT −1 |ψ⟩= ⟨ϕ | A |ψ⟩∗.
For A = A†, this is equal to ⟨ψ|A|ϕ⟩. In particular, we have ⟨ϕ | R |ψ⟩= ⟨ψ| R |ϕ⟩
and ⟨ϕ | P |ψ⟩= −⟨ψ| P |ϕ⟩.
If |ψ⟩is an eigenstate of T , then its phase inﬂuences the eigenvalue. In particular,
if T |ψ⟩= |ψ⟩holds, then so does T (|ψ⟩eiφ) = |ψ⟩e−iφ = (|ψ⟩eiφ) e−2iφ. Thus,
the two eigenvalues differ by the factor e−2iφ. Hence we can ﬁx each state via the
time reversal behavior of the phase, but we cannot assign a quantum number to the
time reversal.
For particles without spin, after the choice of a basis with unique phases, the
complex conjugation operator K can be used as the time reversal operator T . Then
we have T 2 = 1, independently of the choice of phases.
For particles with half-integer spin, we also have to consider T σ T −1 = −σ.
For motion reversal, in particular, the spin becomes inverted along with the angular
momentum, since the spin is to be understood as an eigen angular momentum S,
as we shall see on p. 329. Now, according to Sect.4.2.10, K (σx, σy, σz) K −1 =
(σx, −σy, σz) holds. Hence only T = iσy K leads to the ﬁnal behavior, where the
phase factor i is arbitrary, but then the factor in front of K corresponds to a rotation
through the angle π about the y-axis. Independently of this choice of phase, we now
have T 2 = −1 (for spin-1/2 particles), a truly astonishing result, since classically
the two-fold reversal of the motion leads back to the original state. But note that a
360◦rotation of a spin-1/2 particle leads to the state with the opposite sign.
For T 2 = ±1, we have the equations
⟨φ|ψ⟩= ⟨φ|ψ⟩∗= ±⟨φ|ψ⟩∗= ±⟨ψ|φ⟩.
FromthisitfollowsforT 2 = −1(half-integerspin)that⟨ψ|ψ⟩= 0 .Forhalf-integer
spin, the states |ψ⟩and |ψ⟩are orthogonal to each other and hence different. Since
the Hamilton operator is generally invariant under time reversal, i.e., H = THT −1,
fermions always have pairs of states (|ψ⟩, |ψ⟩) with equal energy. This is known as
Kramers theorem. For bound states, |ψ⟩and |ψ⟩differ by the spin orientation.
The eigenvalue of the space inversion operator P is the parity. Because P2 = 1,
it takes the values ±1.

4.2 Operators and Observables
315
4.2.13
Summary: Operators and Observables
In every physical theory, there are observables (measurable quantities). In quantum
theory they are described by Hermitian operators, the eigenvalues of which corre-
spond to the possible experimental values. Then the associated eigenvalue always
results as the experimental value, and the observable is sharp (certain). Otherwise,
the eigenvalue a (of possible experimental values) results with a statistical weight
(or probability) given by ⟨a| ρ |a⟩, so that, on the average, the expectation value is
⟨A⟩=

a
⟨a| ρ |a⟩a .
For a pure state |ψ⟩, it is ⟨a| ρ |a⟩= |⟨a|ψ⟩|2, so ρ = |ψ⟩⟨ψ|. For the uncertainty (the
average error), we have A =

⟨A2⟩−⟨A⟩2 with ⟨A2⟩= 
a⟨a| ρ |a⟩a2. Hence
A = 0 for ρ = |a⟩⟨a|.
Non-commuting operators have no common set of eigenstates. Hence, not all the
correspondingobservablescanbesharpatthesametime.Inparticular,theuncertainty
relation X · P ≥1
2ℏfollows from the commutation law
[X , P] = i ℏ1 ,
with which we shall deal later. Here X and P have continuous eigenvalue spectra
which differ from the operators so far considered, and which require improper Hilbert
vectors.
4.3
Correspondence Principle
4.3.1
Commutation Relations
According to p. 300, we can ensure Heisenberg’s uncertainty relation
X k · Pk′ ≥1
2ℏδk
k′
by assigning Hermitian operators X k and Pk to the complementary variables position
and momentum which obey the commutation relations (of Born and Jordan)
[X k, Pk′] = iℏδk
k′ 1 .
Since the commutator is proportional to the unit operator, the product X k · Pk of
the uncertainties cannot be smaller than ℏ/2 for any state |ψ⟩.
Here once again we shall always deal with pairs of canonically conjugate quanti-
ties and hence rely on Hamiltonian mechanics. The commutators correspond to the

316
4
Quantum Mechanics I
Poisson brackets, as we shall now show, since we shall use this key idea repeatedly
to translate between classical and quantum dynamics.
According to p. 124, all pairs of dynamical quantities u, v have a Poisson bracket
deﬁned by
[u, v] ≡

k
 ∂u
∂xk
∂v
∂pk
−∂u
∂pk
∂v
∂xk

= −[v, u] ,
which does not depend on the choice of canonical coordinates xk and momenta
pk = ∂L/∂xk (otherwise it would not be canonical). In particular, classically, we
have
[xk, xk′] = 0 = [pk, pk′] ,
[xk, pk′] = δk
k′ .
If we now require the classical Poisson bracket [u, v] to become the expression
[U, V ]/iℏin quantum theory,
[u, v]
!
=⇒
[U, V ]
iℏ
,
where U and V are the Hermitian operators in quantum theory corresponding to the
classical u and v, then we do indeed have
[X k, X k′] = 0 = [Pk, Pk′] ,
[X k, Pk′] = iℏδk
k′ 1 .
If we replace the classical observables by Hermitian operators and the Poisson
brackets by commutators divided by iℏ, then the uncertainty relations are satisﬁed.
Since position and momentum operators do not commute with each other, in
quantum physics no state can be given which contains position and momentum
simultaneously as characterizing items. We have to choose: either the position alone
or the momentum alone. But for each additional Cartesian component a new choice
can be made. With each new degree of freedom, the state is amended by a new
quantum number.
With [X , P] = iℏ1, according to p. 289, we have [X , Pn] = n iℏPn−1. This is
also true for negative integers: [X , P−n] = P−n (PnX −XPn) P−n = −niℏP−n−1.
Since the operators X and P have continuous eigenvalue spectra, in their eigen-
representation, the derivative with respect to X , or indeed P, makes sense—it is
simply the derivative with respect to the eigenvalue x, or the eigenvalue p. Hence,
we write generally,
[X , f (P)] = iℏdf (P)
dP
,
[f (X ), P] = iℏdf (X )
dX
.
It follows in particular (see the Hausdorff series on p. 290) that

4.3 Correspondence Principle
317
exp(i a · P) R exp(−i a · P) = R + ℏa .
According to this, the unitary operator exp(i a · P) shifts all positions by ℏa, so
it is a displacement operator. Furthermore, in classical mechanics, the (canonical)
momentum is the generating function for inﬁnitesimal displacements (see p. 130).
Correspondingly, we have exp(i a · R) P exp(−i a · R) = P −ℏa.
4.3.2
Position and Momentum Representations
In the real-space representation, the position operator X is diagonal. We restrict
ourselves initially to one dimension:
⟨x| X |x′⟩= x δ(x −x′) .
In this representation the momentum operator P follows from the commutation rela-
tion
[X , P] = iℏ1,
since
from
iℏδ(x −x′) = ⟨x| XP −PX |x′⟩= (x −x′)
⟨x| P |x′⟩with δ(x) = −x δ′(x) (p. 21), we obtain
⟨x| P |x′⟩= −iℏ∂
∂x δ(x −x′) = iℏ∂
∂x′ δ(x −x′) .
Hence we have ⟨x| P |ψ⟩=

dx′ ⟨x| P |x′⟩ψ(x′) = −iℏdψ(x)/dx. This can also be
used for higher powers of P in the real-space representation, since for
⟨x| Pn |ψ⟩=

dx′ ⟨x| P |x′⟩⟨x′| Pn−1 |ψ⟩,
the integral can be simpliﬁed with the delta function to −iℏ∂⟨x| Pn−1 |ψ⟩/∂x =
(−iℏ)n ∂nψ/∂xn.
In the real-space representation, we may thus replace P |ψ⟩by −iℏdψ/dx. This
is usually abbreviated as
P = ℏ
i
d
dx ,
which is of course true only in the real-space representation, if P acts on ⟨x|ψ⟩≡
ψ(x). Correspondingly, in the displacement operator U = exp(iaP), all powers of
the derivatives with respect to x occur:
exp(iaP) =
∞

n=0
(ℏa)n
n!
dn
dxn ,
as we also expect for the Taylor series. Note that, with

318
4
Quantum Mechanics I
UXU −1 = X + ℏa
and (X + ℏa)|x −ℏa⟩= |x−ℏa⟩x ,
UX |x⟩= U|x⟩x leads to U|x⟩= |x−ℏa⟩, or to |x + ℏa⟩= U †|x⟩, and this in turn to
ψ(x + ℏa) = ⟨x + ℏa|ψ⟩= ⟨x|U|ψ⟩.
In the momentum representation and since [X , P] = −[P, X ], we also have
⟨p| P |p′⟩= p δ(p −p′) ,
⟨p| X |p′⟩= iℏ∂
∂p δ(p −p′) = ℏ
i
∂
∂p′ δ(p −p′) ,
thus ⟨p| P |ψ⟩= p ψ(p) and ⟨p| X |ψ⟩= iℏdψ/dp.
The results are easily extended to three dimensions. With dψ = ∇r ψ · dr =
∇p ψ · dp, we ﬁnd in particular,
⟨r | R |r ′⟩=
r δ(r −r ′)
=⇒
⟨r | R |ψ⟩=
r ψ(r ) ,
⟨r | P |r ′⟩= ℏ
i ∇r δ(r −r ′)
=⇒
⟨r | P |ψ⟩= ℏ
i ∇r ψ(r ) ,
⟨p | R |p ′⟩= iℏ∇p δ(p −p ′)
=⇒
⟨p | R |ψ⟩= iℏ∇p ψ(p ) ,
⟨p | P |p ′⟩=
p δ(p −p ′)
=⇒
⟨p | P |ψ⟩=
p ψ(p ) .
This can also be used for the matrix elements of this operator between the states ⟨ϕ|
and |ψ⟩, if ϕ(r ) = ⟨r |ϕ⟩or ϕ(p ) = ⟨p |ϕ⟩are known, since
⟨ϕ| A |ψ⟩=

d3r ϕ∗(r ) ⟨r | A |ψ⟩=

d3p ϕ∗(p ) ⟨p | A |ψ⟩.
4.3.3
The Probability Amplitude ⟨r | P⟩
We can now determine the Dirac bracket ⟨r |p⟩, i.e., the density of the prob-
ability amplitude of the state |p⟩at the position r, and then change from the
position to the momentum representation. The reverse transformation is possible
with ⟨p |r⟩= ⟨r |p⟩∗. We have in particular p ⟨x|p⟩= ⟨x| P |p⟩= −iℏ∂/∂x ⟨x|p⟩and
hence ⟨x|p⟩∝exp(ip x/ℏ) as a function of x. On the other hand, we also have
x ⟨x|p⟩= ⟨p| X |x⟩∗= −iℏ∂/∂p ⟨x|p⟩, and hence ⟨x|p⟩∝exp(ixp/ℏ) as a function
of p. The unknown proportionality factor thus depends neither on x nor on p. We
call it temporarily c and determine it from the normalization condition δ(p −p′) =

dx ⟨p|x⟩⟨x|p′⟩= |c|2 
dx exp{i (p′ −p) x/ℏ} = |c|2 2π ℏδ(p′ −p). Hence, it fol-
lows that 2πℏ|c|2 = 1, where 2πℏ= h (so we could just write h here, but ℏoccurs
much more often than h, and we shall use it here too). We choose the arbitrary phase
factor in the simplest possible way, viz., equal to unity. Then,

4.3 Correspondence Principle
319
⟨x|p⟩= exp(ipx/ℏ)
√
2πℏ
⇐⇒
⟨r |p⟩= exp(i p · r/ℏ)
√
2πℏ3
.
For time reversal (motion reversal), according to p. 314, we then also have ⟨x|p⟩=
⟨x|p⟩∗= (2πℏ)−1/2 exp(−ip x/ℏ) = ⟨x| −p⟩.
The probability of the state |p⟩in the space element d3r about r is now given by
|⟨r |p⟩|2 d3r =
d3r
(2πℏ)3 .
It does not depend on the position, so is equally large everywhere. (For a state with
sharp momentum, whence P = 0, X must be inﬁnite!) Note that the integral over
the inﬁnite space does not result in 1, as we should require. The improper Hilbert
space vector |p⟩is not normalizable, so we need an error P > 0.
For the superposition of several states, interference shows up. If, for instance, the
state |ψ⟩contains the momenta p1 and p2 with probability amplitudes ⟨p1|ψ⟩and
⟨p2|ψ⟩, respectively, then the associated probability density is
|⟨r |ψ⟩|2 = | exp(ip1 · r/ℏ) ⟨p1|ψ⟩+ exp(ip2 · r/ℏ) ⟨p2|ψ⟩|2
(2πℏ)3
= |⟨p1|ψ⟩|2
(2πℏ)3
 1 + ⟨p2|ψ⟩
⟨p1|ψ⟩exp i (p2 −p1) · r
ℏ

2
.
It now depends on position, in particular in the direction of p2 −p1, and periodically,
with the wave vector
k ≡p2 −p1
ℏ
.
This we interpret as the interference of probability waves with wave vectors k1 and
k2. Hence we arrive at the de Broglie relation
p = ℏk .
It follows therefore from our assumptions.
It is clearly more convenient for the exponential function to use the wave vector
k instead of the momentum p, since then the denominator ℏdrops out, and they are
related to each other simply via the de Broglie relation. Hence, |p⟩is often replaced
by |k⟩—both states belong to the same ray in (improper) Hilbert space, but are
differently normalized. With ⟨p |p ′⟩= δ(p −p ′) = δ{ℏ(k −k ′)} = ℏ−3⟨k|k ′⟩, we
have
|p⟩=
|k⟩
√
ℏ3 ,
and hence ⟨r |k⟩= exp(ik · r )
√
2π 3
.

320
4
Quantum Mechanics I
The transition from the momentum space to the real space representation (or vice
versa) is a standard Fourier transform (see p. 22), since we have
⟨r |ψ⟩=

d3k ⟨r |k ⟩⟨k |ψ⟩=
1
√
2π 3

d3k exp(ik · r ) ⟨k |ψ⟩.
Actually, we should have used the term wave vector representation instead of momen-
tum representation—other authors do not distinguish between these notions and
simply state that they could have set ℏequal to 1.
4.3.4
Wave Functions
The wave function of a state is usually understood to be its real-space representation:
ψ(r ) = ⟨r |ψ⟩,
but generally the representation can be in any basis. The real-space representation
is often stressed too strongly, since the momentum representation is more suitable
for scattering problems and the angular momentum representation for problems with
rotation invariance. We shall thus proceed here in a way that is as independent of
the representation (as coordinate-free) as possible. The real-space representation is
preferred by many, and even if sometimes obvious, it is often rather inconvenient and
in principle not superior to the other representations (as emphasized by H. S. Green
in the introduction to his textbook, mentioned on p. 396).
If |ψ⟩is a proper Hilbert space vector, then the function ψ(r ) must be normaliz-
able and inﬁnitely differentiable. With the requirement ⟨ψ|ψ⟩= 1, we must have

d3r ψ∗(r ) ψ(r ) = 1 ,
thus in particular ψ(r ) →0 for r →∞, and ψ(r ) must be differentiable so that
the momentum expectation value ⟨ψ| P |ψ⟩can be calculated. Higher powers of P
require higher derivatives, as we have seen in Sect.4.3.2.
We already had an example of a wave function in the last section, namely the
wave function for a given momentum p (such that P vanishes):
⟨r |p⟩= (2πℏ)−3/2 exp(ip · r/ℏ) .
However, |p ⟩and |r⟩are improper Hilbert vectors. Such states are idealizations.
An ensemble can only then be characterized by continuous variables when error
widths (uncertainties) are included. To each continuous measurable quantity belongs
a distribution function (density).

4.3 Correspondence Principle
321
Occasionally, improper Hilbert vectors are required [2]. They are very convenient,
and with appropriate distribution functions, a fuzziness can still be introduced. For
example, a wave packet can be formed from ⟨r |k⟩:
⟨r |ψ⟩=

d3k ⟨r |k ⟩⟨k |ψ⟩=
1
√
2π 3

d3k exp(ik · r ) ψ(k ) .
For ψ(k ) ̸= δ(k −k0), this has a non-vanishing momentum uncertainty, and for
ψ(k ) ̸= c, a position uncertainty.
We may ask which wave function has the smallest possible X · P, i.e., equal
to ℏ/2? According to p. 301, we must then have (X −X ) |ψ⟩= −i X /P (P −
P)|ψ⟩with 1/P = 2X /ℏ. With ⟨x| X |ψ⟩= x ψ(x) and ⟨x| P |ψ⟩= −iℏψ′(x),
we arrive at the differential equation

−d
dx + i
ℏP

ψ(x) =
x −X
2 (X )2 ψ(x) .
For an appropriate choice of phase, so that no integration constant remains free, its
normalized solution reads (for the normalization of the Gauss function, see p. 23)
ψ(x) =
1
4√
2π
√
X
exp

−
x −X
2 X
2
+ i P (x −1
2X )
ℏ

.
It contains three free parameters, namely X , X , and P, but the last drops out
for the probability density |ψ(x)|2. This density is a normal distribution (Gauss
function) with maximum at X . For the canonically conjugate variable, using a Fourier
transform, we ﬁnd another Gauss function:
ψ(p) =
1
4√
2π
√
P
exp

−
p −P
2 P
2
−i X (p −1
2P)
ℏ

.
We shall return to this result in the context of harmonic oscillations (Sect.4.5.4). The
phase factors exp(∓1
2 i P X /ℏ) have been added for the sake of the symmetry—then
ψ(x) and ψ(p) are really mutually Fourier-transformed quantities.
4.3.5
Wigner Function
In statistical mechanics (p. 523), we introduce the classical density function ρcl(r, p )
in phase space and use it to determine the average values
A =

d3r d3p ρcl(r, p ) Acl(r, p ) .

322
4
Quantum Mechanics I
In quantum theory this density corresponds to the Wigner function. It follows via
Fourier transforms from the density operator ρ in the position or momentum repre-
sentation.
To show this, we adopt a basis {C(r, p )} of Hermitian, unitary, and orthogonal
(in r and p) operators. For example, the Pauli operators are Hermitian, unitary, and
orthogonal in the space of 2 × 2 matrices, according to p. 309. In the real-space
representation,
⟨r1| C(r, p ) |r2⟩≡δ(2 r −r1 −r2) exp +i p · (r1 −r2)
ℏ
,
and in the momentum representation
⟨p1| C(r, p ) |p2⟩= δ(2 p −p1 −p2) exp −i r · (p1 −p2)
ℏ
.
These are practical as an operator basis, according to Sect.4.2.5, because
C(r, p ) = C†(r, p )
=
C−1(r, p ) ,
tr{C(r, p ) C(r ′, p ′)} = ( 1
2πℏ)3 δ(r −r ′) δ(p −p ′) .
As in Sect.4.2.11, the expectation values of the basis operators are also important.
They deliver the Wigner function
ρ(r, p ) ≡⟨C(r, p )⟩
(πℏ)3
,
which is in fact the Fourier transform of the density operator:
⟨C(r, p )⟩=

d3r ′ ⟨r −r ′| ρ |r + r ′⟩exp +2i p · r ′
ℏ
=

d3p ′ ⟨p −p ′| ρ |p + p ′⟩exp −2i r · p ′
ℏ
.
Conversely, we obtain the density operator from the Wigner function (see Figs.4.6
and 4.7):
⟨r | ρ |r ′⟩=

d3p ρ
r + r ′
2
, p

exp +i (r −r ′) · p
ℏ
,
⟨p | ρ |p ′⟩=

d3r ρ

r, p + p ′
2

exp −i (p −p ′) · r
ℏ
.
If we integrate the Wigner function ρ(r, p ) over all momenta or all positions, we
obtain the probability densities in position and momentum space, respectively:

4.3 Correspondence Principle
323
Fig. 4.6 Superposition of the two states ψ±(x) ∝exp{−(x ∓2)2}: ψ+ −ψ−(left) and ψ+ + ψ−
(right). Below the wave functions, the density operators ρ = |ψ⟩⟨ψ| are shown with equal-value
lines for ρ > 0 (continuous line) and for ρ ≤0 (dotted lines), in the real-space representation
(ρ(x, x′) = ⟨x|ρ|x′⟩) and the momentum representation (ρ(p, p′) = ⟨p|ρ|p′⟩). The axes can be rec-
ognized as symmetry axes, and ρ is always real here. Along the diagonal x′ = x, we have ρ ≥0,
which corresponds to the “classically expected” density
Fig. 4.7 Wigner functions of the superpositions of states from Fig.4.6. Equal-value lines are shown
once again, for ρ > 0 (continuous lines) and for ρ ≤0 (dotted lines). Here, ρ is symmetric with
respect to the x- and p-axes. The Wigner function can be negative, which depends sensitively on the
phase difference of the superposed states, while in the “classically preferred” phase-space regions
(here x ≈±2, p ≈0), there is almost no dependence

324
4
Quantum Mechanics I

d3p ρ(r, p ) = ⟨r | ρ |r⟩
and

d3r ρ(r, p ) = ⟨p | ρ |p⟩.
Hence we have the usual normalization

d3r d3p ρ(r, p ) = 1. Incidentally, a
Fourier transform yields

d3r d3p ρ2(r, p ) = (2πℏ)−3 trρ2. We can also test
whether we have a pure state or a mixture using the Wigner function. In addition, the
Wigner function, being the expectation value of a Hermitian operator, is real. How-
ever, it can also be negative, and this distinguishes it from classical density functions:
it is only a quasi-probability, but this difference is also necessary for the description
of interference.
With the Wigner function, the expectation value of every observable A(R, P )
can be determined. In particular, if we expand the operator A in terms of the basis
{C(r, p )}, viz.,
A =

d3r d3p C(r, p ) (2/πℏ)3 tr{C(r, p ) A} ,
according to p. 297, then using ⟨C(r, p )⟩= (πℏ)3 ρ(r, p ), we can determine ⟨A⟩,
We set
A(r, p ) ≡23 tr{C(r, p ) A} = 23

d3r ′ ⟨r −r ′| A |r + r ′⟩exp +2i p · r ′
ℏ
= 23

d3p ′ ⟨p −p ′| A |p + p ′⟩exp −2i r · p ′
ℏ
,
because then formally—only formally, since the Wigner function can also be
negative—we have the same as in statistical mechanics, that is
⟨A⟩=

d3r d3p ρ(r, p ) A(r, p ) ,
and A(r, p ) is real for a Hermitian operator A.
4.3.6
Spin
So far we have taken the position or momentum representation and then proceeded
as if a (pure) state were already deﬁned by r or p . But for electrons (and nucleons),
we must also take into account their eigen angular momentum (spin). This degree
of freedom must also be determined if the statistical ensemble is to be described
uniquely. For this “inner degree of freedom”, we only require a Hilbert space of
ﬁnite dimension. For electrons and nucleons, two dimensions sufﬁce, so here we
shall restrict ourselves to that situation and use Sect.4.2.10. Hence, |r, ↑⟩and |r, ↓⟩
ﬁx the state, or indeed |p, ↑⟩and |p, ↓⟩.

4.3 Correspondence Principle
325
Correspondingly, we have to distinguish the operators by the space in which they
act. For example, neither R nor P affects the inner degrees of freedom—they act in
the spin space as the unit operator. Conversely, σ does not act on |r⟩and |p⟩. Hence R
and P commute with σ. Of course, there are also operators, which act in two spaces,
e.g., the helicity (P · P )−1/2 P · σ, for which the orientation of the spin relative to
the momentum is important.
IfAandBdonotactinthespinspace,thenwithσx2 = 1andσxσy = −σyσx = iσz
(and cyclic permutations), we have
A · σ B · σ = A · B + i (A × B) · σ .
Since here A and B may be arbitrary vector operators, we have as special cases of
this equation
A · σ σ = A −i A × σ
and
[σ, A · σ ] = 2i A × σ .
The unit operator in the spin space is not written explicitly, as previously for R and
P. Moreover, on the left of the last equations, we should write 1 ⊗σ instead of just
σ.
If we write σ as a 2 × 2 matrix, then the Hilbert vectors in the sequence space
must also be written as 2-spinors—for ψ the two elements atop each other, for ψ†
side-by-side and complex-conjugate to those of ψ.
4.3.7
Correspondence Principle
In quantum theory, we describe all observables using Hermitian operators whose
eigenvalues correspond to the possible experimental values. So far we have presented
only two observables, namely position and momentum, but according to Hamilto-
nian mechanics, further quantities can be derived. The corresponding observables
in quantum theory are in general easy to ﬁnd—we simply have to take the classical
equations as operator equations: If in classical physics y = f (x, p), where y, x, and
p have real values, then usually in quantum theory Y = f (X , P), where Y, X , and
P are Hermitian operators. Hence we have given a mathematical form to Bohr’s
correspondence principle. Classical and quantum mechanical quantities correspond
to one another to a large extent, but are distinguished in their mathematical rele-
vance, since instead of classical quantities (number times unit), we now have linear
operators.
However, the operators are canonically conjugate quantities and do not commute
with each other—for products, the order of the factors is important. This difﬁculty
rarely arises though. Let us take, e.g., the orbital angular momentum
L = R × P .

326
4
Quantum Mechanics I
In the vector product here, all components commute without posing a problem.
Although L does not generally commute with R and P, at least equal components
do: L · R = R · L and L · P = P · L.
If necessary, we can invoke the Weyl correspondence. If the Wigner function is
used, then a Fourier transform is allowed. In particular, if the classical function f (x, p)
is given, its Fourier transform reads
f (α, β) = 1
2π

dx dp exp{−i (αx + βp)} f (x, p) ,
and its operator function (where α and β remain real variables)
f (X , P) = 1
2π

dα dβ exp{+i (αX + βP)} f (α, β) .
On p. 290, we already derived the relations (note that [iαX , iβP] = −iℏαβ)
exp{i (αX + βP)} = exp(iαX ) exp(iβP) exp(+ 1
2iℏαβ)
= exp(iβP) exp(iαX ) exp(−1
2iℏαβ) ,
so we can determine f (X , P) from a double Fourier integral, after which we have
found f (α, β). In this way, f (x) p has the Fourier transformed form f (α, β) =
√
2πi f (α) δ′(β),andhence,accordingtoWeyl,wehavetotakef (X , P) = f (X ) P −
1
2iℏf ′(X ). According to p. 316, in particular, with iℏf ′(X ) = [f (X ), P], this leads to
the symmetrized product 1
2 {f (X ), P}. Generally, the power series of the exponential
function of i (αX + βP) leads to completely symmetrized products of X and P.
If we use quasi-probabilities instead of the Wigner function, we have to order
differently, as will be discussed in Sect. 5.5.6.
Let us consider, e.g., the Hamilton operator for a particle of mass m and charge
q in an electromagnetic ﬁeld. According to p. 123, the classical Hamilton function
is
1
2m (p −qA ) · (p −qA ) + q	. The quantities m and q do not become operators,
and in the usual quantum theory neither does the electromagnetic ﬁeld—this happens
only in quantum electrodynamics (see Sect. 5.5). Since P does not commute with A,
we arrive at
H = P2 −q (P · A + A · P ) + q2 A2
2m
+ q 	 ,
thus at the symmetrical product {Pk, Ak}. Now in the real-space representation, P
corresponds to the operator −iℏ∇, and we ﬁnd ∇· Aψ = ψ ∇· A + A · ∇ψ. For
the Coulomb gauge, ∇· A vanishes, whence P · A = A · P holds, even though P and
R do not commute with each other. For a homogeneous magnetic ﬁeld B, we have in
particular for the vector potential (in the Coulomb gauge) A = 1
2 B × R, and hence
P · A + A · P = (B × R) · P = B · (R × P) = B · L. Here, according to p. 191, a

4.3 Correspondence Principle
327
point charge q of mass m with orbital angular momentum L has magnetic moment
μ =
1
2m qL, giving a potential energy −μ · B in addition to q	.
However, this ansatz does not sufﬁce for electrons in a magnetic ﬁeld because
they have one more inner moment, which is connected to their spin and which has
not been accounted for so far. Here it has been shown that the Pauli equation, viz.,
H = P2 −q B · (L + ℏσ ) + q2 A2
2m
+ q 	
= (P −q A) · (P −q A)
2m
+ q 	 −qℏ
2m σ · B ,
is appropriate. The new feature is the last term, where the factor
μB ≡eℏ
2m
is known as the Bohr magneton. Due to the factor σ in the Pauli equation, H acts
on a wave function with two components, a 2-spinor, which we shall discuss in
Sect.4.5.8. For a homogeneous magnetic ﬁeld B (we restrict ourselves to this case),
the Pauli equation can be brought into the form
H = {(P −q A ) · σ}2
2m
+ q 	 ,
since according to p. 325, we have
{(P −q A ) · σ}2 = (P −q A) · (P −q A ) + i {(P −q A) × (P −q A)} · σ .
If P were to commute with A, then the vector product would vanish, but now for
A = 1
2 B × R, the term P × A + A × P = −iℏB remains, since B commutes with
R and P and hence P × A + A × P = 1
2[R, B · P] −1
2B(R · P −P · R).
In the form H =
1
2m {(P −q A ) · σ}2 + q 	, the Pauli equation is the non-
relativistic limiting case of the Dirac equation (as will be shown in Sect. 5.6.8).
Hence the results here do not describe relativistic effects, even though it is some-
times claimed otherwise. Incidentally, ℏσ will appear as the origin of the doublets
of the spin momentum S in the next section. In the Pauli equation, it thus occurs as
the scalar product (L + 2 S ) · B. The spin momentum enters with twice the weight
(magneto-mechanical anomaly). So this factor of 2 is not a relativistic effect.
If the classical equations are valid for operators in quantum theory, this will also
apply for the expectation values. However, the expectation value of a product is not
generally equal to the product of the expectation values—that would only be true for
eigenstates. Hence, generally, we also have A 2 ̸= A2 and then A ≥0.

328
4
Quantum Mechanics I
4.3.8
Angular Momentum Operator
The orbital angular momentum operator is deﬁned by
L ≡R × P ,
where the fact that R and P do not commute does not create problems, because in
the vector product only factors commuting with each other occur together. Hence L
is also Hermitian like R and P.
From the commutation relations for R and P, we ﬁnd
[Lx, X ] = 0 ,
[Lx, Y] = iℏZ ,
[Lx, Z] = −iℏY ,
[Lx, Px] = 0 ,
[Lx, Py] = iℏPz ,
[Lx, Pz] = −iℏPy ,
[Lx, Ly] = iℏLz ,
since we have, e.g., [Lx, X ] = [YPz −ZPy, X ] = 0, but
[Lx, Y] = −[ZPy, Y] = Z [Y, Py] = iℏZ .
The above are valid for Ly and Lz, with suitable cyclic permutations. Hence we
ﬁndthecommutator[Lx, Ly] = [Lx, ZPx −XPz] = −iℏYPx + iℏXPy = iℏLz. Gen-
erally, for a vector operator A, we can derive the commutation relation
[L · e1 , A · e2 ] = iℏA · (e1 × e2 ) ,
because, according to Hamiltonian mechanics (see p. 130), the angular momentum
is the generating function of inﬁnitesimal rotations. In addition, the corresponding
equations for the Poisson brackets are valid with R or P instead of A (see Prob-
lems 2.44 and 4.30).
The commutation relations [Lx, Ly] = iℏLz (and cyclic) mean that there are gen-
erally no common eigenvectors for all three components of the angular momentum
operator. We can make only one component diagonal. As for the spherical coordi-
nates, we prefer the z-component and choose ez as the quantization direction. Then
the y- and z-components do also have unique expectation values, but with uncertain-
ties. In general, the angular momenta in a state have no sharp direction. They are
unsharp (uncertain), as in the time average for each precession, for which only the
component along the precession vector is ﬁxed. This is shown in Fig.4.8.
We have already encountered commutation relations similar to those for the com-
ponents of the orbital angular momentum L, viz., for the Pauli operators on p. 309.
These read [σx, σy] = 2i σz and cyclic permutations. Hence with
S = 1
2ℏσ ,

4.3 Correspondence Principle
329
Fig. 4.8 Angular momentum eigenstates. For sharp L2, all allowable vectors L have the same
absolute value. They span a sphere (dashed circle). Here l = 1 is chosen. Then there are three
eigenstates |l, m⟩with sharp Lz, and hence uncertain Lx and Ly. Their angular momentum vectors
thus form three cones about the quantization axis. The one for m = 0 degenerates to a circle
we conclude
[Sx, Sy] = iℏSz
and cyclic permutations.
In fact, we need S ≡1
2ℏσ for the spin (eigen angular momentum) of electrons and
nucleons. But this is easier to treat than the orbital angular momentum, because only
two eigenvectors occur. For the three Cartesian components, we have σi2 = 1, and
hence S2 ≡Sx2 + Sy2 + Sz2 = 3
4 ℏ2 1.
The square of the orbital angular momentum, viz.,
L2 = Lx
2 + Ly
2 + Lz
2 ,
is Hermitian and commutes with all components:
[L2, Lz] = 0 = [L2, Lx] = [L2, Ly] ,
since
[Lx
2, Lz] = Lx[Lx, Lz] + [Lx, Lz]Lx
is equal to
−[Ly, Lz]Ly −Ly[Ly, Lz] = −[Ly
2, Lz] .
Hence, there is a complete orthonormal system of eigenvectors of L2 and Lz.
Since the operators L2 and Lz are Hermitian, they have real eigenvalues, and we
shall now seek these, along with a set of common eigenvectors. From the commu-
tation relations, we will determine the eigenvalues l (l+1) ℏ2 with l ∈{0, 1, . . .} of
L2 and the eigenvalues mℏwith m ∈{0, ±1, . . . , ±l} of Lz, where l and m could
also take half integer values (1/2, 3/2, etc.). But half-integer values do not lead to a
unique real-space representation (see the next section) and are therefore to be dis-
carded. This is different for the inner degree of freedom, where the values s = 1/2
and m = ±1/2 are allowed.

330
4
Quantum Mechanics I
The proof is similar to that for the ﬁeld operators (see Sect.4.2.8). We use the
non-Hermitian operators
L± ≡Lx ± iLy = L∓
†
⇐⇒
Lx = L+ + L−
2
,
Ly = L+ −L−
2i
,
with the properties
[Lz, L±] = ±ℏL± ,
[L+, L−] = 2ℏLz ,
[L2, L±] = 0 = [L2, L±L∓] .
Nowlet|a, b⟩beacommoneigenvectorofL2 andLz,sothatL2 |a, b⟩= |a, b⟩aℏ2 and
Lz |a, b⟩= |a, b⟩bℏ. Then with the commutation relations, we obtain the following
results for L± |a, b⟩:
L2 L± |a, b⟩= L± |a, b⟩aℏ2 ,
Lz L± |a, b⟩= L± |a, b⟩(b ± 1) ℏ.
The ladder operators L± thus connect eigenstates of L2 with equal eigenvalue, but
with a different eigenvalue of Lz, i.e., L± |a, b⟩∝|a, b ± 1⟩. Hence, we call L+ a
creation operator and L−an annihilation operator.
However, the construction method with the ladder operators has to lead to the
zero vector after a ﬁnite number of steps, and then stop. Otherwise, the norm of the
vectors L± |a, b⟩might become imaginary. From
L2 = Lz
2 + 1
2 (L+L−+ L−L+)
and the commutation relation [L+, L−] = 2ℏLz, it follows that
L∓L± = L2 −Lz (Lz ± ℏ) ,
and hence for the squared norm of L± |a, b⟩, which is just the expectation value
⟨a, b| L±†L± |a, b⟩, we obtain the value {a −b (b ± 1)} ℏ2. Hence, the expression
must vanish for bmax and bmin:
a = bmax (bmax + 1) = bmin (bmin −1) .
We deduce that bmin = −bmax (or bmin = bmax + 1, but this contradicts bmin ≤bmax).
Starting from |a, bmin⟩, we must arrive at |a, bmax⟩with the creation operator L+.
Hence, bmax −bmin = 2 bmax is an integer and the claim is proven. We denote bmax
by l and usually write m for b. Following the usual practice, we write for short |l, m⟩
instead of | l (l+1), m⟩. Incidentally, the orbital angular momentum eigenstates are
often not speciﬁed by the value of l, but by letters. The ﬁrst four have historical
origin, the rest follow in alphabetical order, without j (see Table4.1).
With the eigenvalue equations

4.3 Correspondence Principle
331
Table 4.1 Coulomb-state “quantum numbers”
l
0
1
2
3
4
5
6
7
. . .
Name
s
p
d
f
g
h
i
k
. . .
L2 |l, m⟩= |l, m⟩l (l+1) ℏ2 ,
with l ∈{0, 1, 2, . . .} ,
Lz |l, m⟩= |l, m⟩m ℏ,
with m ∈{0, ±1, . . . , ±l} ,
the phase factors are not yet determined. But since Condon and Shortley [5], the
phase factor for L± is chosen positive real and the relative phases of the states with
equal l are then determined by
L± |l, m⟩= |l, m ± 1⟩√l (l + 1) −m (m ± 1) ℏ
= |l, m ± 1⟩√(l ∓m) (l ± m + 1) ℏ,
using L∓L± = L2 −Lz(Lz ± ℏ). The relative phases of states with unequal l are
still free. Hence we can still arrange things so that the matrix elements of all those
operators that are invariant under rotations and time-reversal are real. This is possible,
e.g., by satisfying the requirement
T |l, m⟩= (−)l+m |l, −m⟩.
But we shall not deal with this here, because we would then have to investigate the
behavior of the states under rotations.
In the states |l, m⟩, the expectation values of L± vanish and so therefore do those
of Lx, Ly and L+2 + L−2 = 2 (Lx2 −Ly2). Consequently, we have (Lx)2 = ⟨Lx2⟩=
⟨Ly2⟩= (Ly)2:
(Lx)2 = (Ly)2 = 1
2 ⟨L2 −Lz
2⟩= 1
2 {l(l + 1) −m2} ℏ2 ≥1
2 lℏ2 .
For ﬁxed l, these uncertainties are smallest for m = ±l and greatest for m = 0. Only
the s-state is such that all three components of the angular momentum are sharp.
4.3.9
Spherical Harmonics
The spherical harmonics are the real-space representation of the orbital angular mo-
mentum eigenstates |l, m⟩. However, it is not the length of the position vector that
is important, but only its direction. Hence it is practical to calculate with spherical
coordinates (r, θ, ϕ). With ⟨r | R |r ′⟩= r δ(r −r ′) and ⟨r | P |ψ⟩= −iℏ∇ψ(r ),
we have

332
4
Quantum Mechanics I
⟨r | L |ψ⟩= ℏ
i r × ∇ψ(r ) ,
with r × ∇= −eθ
1
sin θ
∂
∂ϕ + eϕ
∂
∂θ ,
where (see Fig. 1.12)
eθ = cos θ (cos ϕ ex + sin ϕ ey) −sin θ ez ,
eϕ =
−sin ϕ ex + cos ϕ ey .
The angular momentum operators thus act only on the angular coordinates  ≡
(θ, ϕ), not on the length of r. Hence, in the following, we consider
⟨ | lm⟩≡i l Y (l)
m () .
The factor i l is a practical phase factor which turns out to be useful for time reversal.
In particular, with T |⟩= |⟩and T |l, m⟩= (−)l+m|l, −m⟩, we ﬁnd ⟨|l, m⟩∗=
(−)l+m⟨|l, −m⟩and hence (with the factor i l),
Y (l) ∗
m () = (−)m Y (l)
−m() .
Consequently, all spherical harmonics with m = 0 are real—we can even arrange for
them all to be positive for  in the z-direction, i.e., for (θ, ϕ) = (0, 0). Without the
factor il, this would not be possible.
Since Lz in the real-space representation of the operator corresponds to −iℏ∂/∂ϕ,
and since we also have Lz | lm⟩= | lm⟩mℏ, the function ⟨ | lm⟩must be connected
to ϕ via the factor exp(imϕ). It is only unique (mod 2π), if m is an even number—thus
also l must be an integer, i.e., l ∈{0, 1, . . .}. The commutation relations also allow
half-integer values, which would be connected with an ambiguity, and this is without
contradiction only for unobservable internal coordinates (spin).
We set Y (l)
m () = flm(θ) exp(imϕ) and determine the unknown function flm using
the ladder operators. With
eθ · (ex ± iey) = cos θ exp(± iϕ) ,
eϕ · (ex ± iey) = ± i exp(± iϕ) ,
and consequently also
(r × ∇)± = exp(± iϕ) (−cot θ ∂/∂ϕ ± i ∂/∂θ) ,
we have
⟨ | L± | lm⟩= ⟨ | l, m ± 1⟩√(l ∓m) (l ± m + 1) ℏ
= ℏexp(± iϕ)

± ∂
∂θ + i cot θ
∂
∂ϕ
	
⟨ | lm⟩.
Hence, we obtain the differential equation

4.3 Correspondence Principle
333

± d
dθ −m cot θ

flm(θ) = fl,m±1(θ) √(l ∓m) (l ± m + 1) .
In particular, ⟨| L± | l, ±l⟩vanishes. Then (d/dθ −l cot θ) fl,±l(θ) = 0, and con-
sequently, fl,±l(θ) ∝sinl θ. The value of the still missing factors is determined by
the normalization condition

d |⟨|lm⟩|2 = 1. From

 π
0
sin2l+1 θ dθ = 2 (2l l!)2/(2l+1)! ,
we deduce an appropriate choice of the phase:
Y (l)
±l () = (∓)l
2l l !

(2l + 1) !
4π
sinl θ exp(± i lϕ) .
Theremainingsphericalharmonicsarenowobtainedbyapplyingtheladderoperators
L±. However, the operator ±d/dθ −m cot θ is not quite appropriate here, because it
contains two terms. But let us consider the function sin∓m θ flm and take cos θ instead
ofθ asthevariable.Weonlyneed0 ≤θ ≤π anyway.Thend/dθ = −sin θ d/d cos θ
leads to
d sin∓m θ flm
d cos θ
= ∓sin∓m−1 θ

± d
dθ −m cot θ

flm
= ∓sin∓m−1 θ fl,m±1
√
l ∓m
√
l ± m + 1 .
After differentiating n times, we have on the right-hand side
(∓)n sin∓m−n θ fl,m±n

(l ∓m) !
(l ∓m −n) !
(l ± m + n) !
(l ± m) !
.
Hence,
fl,m±n = (∓)n sinn±m θ dn sin∓m θ flm
d cosn θ

(l ± m) ! (l ∓m −n) !
(l ∓m) ! (l ± m + n) ! .
This recursion formula connects all spherical harmonics with equal l to each other.
This is achieved by the ladder operators, according to the last section. In particular,
with L−and for n = m = l, it leads to fl0 = dl sinl θ fll/d cosl θ (2l)!−1/2, or (see
Fig.4.9)
Y (l)
0 () = (−)l
2l l !

2l + 1
4π
dl sin2l θ
d cosl θ
≡

2l + 1
4π
Pl(cos θ) .

334
4
Quantum Mechanics I
Fig. 4.9 Spherical
harmonics. Their positive
real part is shown in white,
the negative part hatched.
l = 0(1)2 increases upwards
from sphere to sphere, m to
the right. In addition, there
are two frames
Here Pl(cos θ) is a Legendre polynomial. We already met them in Sect. 2.2.7,
when we considered their generating function
1
√
1 −2sz + s2 =
∞

n=0
Pn(z) sn ,
for |s| < 1 .
They lead to P0(z) = 1, P1(z) = z, and the recursion formula
(n + 1) Pn+1(z) −(2n + 1) z Pn(z) + n Pn−1(z) = 0 .
We also proved the orthonormalization condition on p. 82:

 1
−1
dz Pn(z) Pn′(z) =
2
2n + 1 δnn′ .
Hence we can also show the Rodrigues formula, viz.,
Pn(z) =
1
2n n!
dn (z2 −1)n
dzn
.
Without this, we would not have met the Legendre polynomials previously at all. If
we integrate by parts, where we may assume n ≤n′, then we obtain, for n′ > 0,

 1
−1
dz dn(z2 −1)n
dzn
dn′(z2 −1)n′
dzn′
= (−)n

 1
−1
dz d2n(z2 −1)n
dz2n
dn′−n(z2 −1)n′
dzn′−n
,
with the factor d2n(z2 −1)n/dz2n = (2n)!. For n′ > n, this is zero and otherwise
equal to (2n)!
 π
0 dθ sin2n+1 θ = (2nn!)2 2/(2n + 1). The polynomials deﬁned by
Rodrigues’ formula are thus also orthonormalized like the Legendre polynomials
and are real polynomials of the same degree. Hence, they can differ from each other
by at most a sign. But the coefﬁcients for the highest power are positive according
to the recursion formula and also according to the Rodrigues formula. This leads to

4.3 Correspondence Principle
335
Pn(z) = 1
2n

k
(−)k
(2n −2k)!
k! (n −k)! (n −2k)! zn−2k .
From here, we have Pn(−z) = (−)nPn(z).
Clearly, the spherical harmonics with m = 0 are real and positive in the z-
direction, so the choice of phase for m = ±l corresponds to our above-mentioned
wishes in connection with the factor il. Generally, with m ≥0, fl0 = Y (l)
0
and
fl,±m = (∓)m sinm θ (dmfl0/d cosm θ) √(l −m)!/(l + m)!, we obtain the expression
Y (l)
±m() = (∓)m

2l+1
4π
(l−m) !
(l+m) ! sinm θ dm Pl(cos θ)
d cosm θ
exp(± i mϕ) .
For spherically symmetric problems, we will often expand the wave functions in
terms of these spherical harmonics, beginning in Sect.4.5.2.
Since Pl(−cos θ) = (−)l Pl(cos θ), the spherical harmonics with orbital angu-
lar momentum l have parity (−)l, using the standard results sin (π −θ) = sin θ,
cos (π −θ) = −cos θ, and exp(± i m(ϕ + π)) = (−)m exp(± i mϕ).
With the spherical harmonics, we know the eigenfunctions of the operator L2 in
the real-space representation (directional representation):
⟨ | L2 | lm⟩= ⟨ | lm⟩l(l+1) ℏ2
=

−
1
sin θ
∂
∂θ

sin θ ∂
∂θ

−
1
sin2 θ
∂2
∂ϕ 2
	
⟨ | lm⟩ℏ2 .
We will need this operator in Sect.4.5.2 for central ﬁelds, because, according to
p. 142, the centrifugal potential is proportional to L2.
4.3.10
Coupling of Angular Momenta
In addition to the orbital angular momentum of electrons and nucleons, we also have
to account for their eigen angular momentum (spin). Their total angular momentum
involves both. Hence we now consider
J = L + S .
Since L acts in real space and S in spin space, the two operators commute. J is
Hermitian like L and S, so
[Jx, Jy] = iℏJz ,
and cyclic permutations.
Hence the considerations in Sect.4.3.8 deliver

336
4
Quantum Mechanics I
J 2 |j, m⟩= |j, m⟩j(j+1) ℏ2 ,
for j ∈{0, 1
2, 1, . . .} ,
Jz |j, m⟩= |j, m⟩m ℏ,
for m ∈{j, j −1, . . . , −j} ,
J± |j, m⟩= |j, m±1⟩

j(j+1) −m(m±1) ℏ
= |j, m±1⟩

(j∓m) (j±m+1) ℏ,
T |j, m⟩= (−)j+m |j, −m⟩.
We would now like to apply these general equations to the spin 1/2 case.
Here we could take the uncoupled representation |l, ml; 1
2, ms⟩which diagonal-
izes L2, Lz, S2, and Sz. But if there is a spin–orbit coupling, which we derive from
the operator product
L · S = LzSz + 1
2(L+S−+ L−S+) ,
then neither Lz nor Sz will be sharp, only their sum Jz. Then the coupled representation
|(l, 1
2) j, m⟩is more useful, because it simultaneously diagonalizes L2, S2, J 2, and
Jz, and hence also 2L · S = J 2 −L2 −S2. With Jz = Lz + Sz, we then have m =
ml + ms, and for a given l, m ≤l + 1
2 = j. In fact, |l, l; 1
2, 1
2⟩is also an eigenstate of
J 2 = L2 + 2 L · S + S2, since with 2 L · S = 2 LzSz + L+S−+ L−S+, L+|ll⟩= |o⟩,
and S+| 1
2
1
2⟩= |o⟩, we ﬁnd that {l (l + 1) + 2 l 1
2 + 3
4} ℏ2 is an eigenvalue, and with
j = l+ 1
2, this can also be written as j(j+1) ℏ2. Hence for j = l+ 1
2, we may set the
two states |l, l; 1
2, 1
2⟩and |(l, 1
2) l+ 1
2, l+ 1
2⟩equal to each other. Here we ﬁnally ﬁx
the phase of the coupled state. The remaining states with j = l + 1
2 are obtained from
therewiththecreationoperatorJ−= L−+ S−.Sincewerestrictourselvesheretos =
1
2, the operator S 2
−turns out to be zero, and then we have J n
−= L n
−+ n L n−1
−
S−. For an
appropriate choice of the phase and with J−n|jj⟩= |j, j −n⟩

(2j)! n!/(2j −n)! ℏn,
it follows that
|(l, 1
2) l+ 1
2, m⟩= |l, m+ 1
2; 1
2, −1
2⟩

l+ 1
2 −m
2l+1
+ |l, m−1
2; 1
2, 1
2⟩

l+ 1
2 +m
2l+1
.
We then have all 2j + 1 = 2l + 2 states with j = l + 1
2 in the coupled basis expanded
in terms of the uncoupled states. But in the uncoupled basis, there are (2l + 1) · 2
states with equal l, thus 2l more states. In fact, we can also couple with j = l−1
2.
These states have to be orthogonal to those with equal l and m, so the expansion
coefﬁcients are
|(l, 1
2) l−1
2, m⟩= |l, m+ 1
2; 1
2, −1
2⟩

l+ 1
2 +m
2l+1
−|l, m−1
2; 1
2, 1
2⟩

l+ 1
2 −m
2l+1
.
We may also include a phase factor. The phase of the coupled state remains free to
choose—only the relative phases of the states with different m are already ﬁxed by
the choice of matrix elements of J±. The last equation obeys a second requirement

4.3 Correspondence Principle
337
due to Condon and Shortley, namely, for j1 + j2 ≥j ≥|j1 −j2|,
⟨(j1, j2) j, j|j1, j1; j2, j−j1⟩= ⟨j1, j1; j2, j−j1|(j1, j2) j, j⟩> 0 ,
i.e., all coefﬁcients with m = j and m1 = j1 are to be positive.
Hence all expansion coefﬁcients of the angular momentum coupling, i.e., all
Clebsch–Gordan coefﬁcients, are now real. Here we adopt the abbreviation
 j1
j2
m1 m2

j
m
	
≡⟨j1, m1; j2, m2|(j1, j2) j, m⟩= ⟨(j1, j2) j, m|j1, m1; j2, m2⟩,
but other notations do occur. We have now derived, e.g.,

l
1
2
m+ 1
2 −1
2

l± 1
2
m
	
=

l + 1
2 ∓m
2l + 1
= ∓

l
1
2
m−1
2
1
2

l∓1
2
m
	
.
Likewise we can now couple two spin- 1
2 states to triplet and singlet states. If
instead of | 1
2, 1
2⟩we write for short |↑⟩(spin up), and instead of | 1
2, −1
2⟩the abbre-
viation |↓⟩(spin down), it follows that
|( 1
2, 1
2) 1, +1⟩= |↑↑⟩,
|( 1
2, 1
2) 1, 0⟩= |↑↓⟩+ |↓↑⟩
√
2
,
|( 1
2, 1
2) 1, −1⟩= |↓↓⟩,
|( 1
2, 1
2) 0, 0⟩= |↑↓⟩−|↓↑⟩
√
2
.
The triplet states are thus symmetric under exchange of the two uncoupled states,
while the singlet state is antisymmetric.
4.3.11
Summary: Correspondence Principle
In the last three sections, we have worked out the basic features of quantum theory.
The observables of classical mechanics become Hermitian operators, and relations
between measurable quantities become operator equations. Important here is the
commutation behavior. The commutator corresponds to the classical Poisson bracket,
except for the factor iℏ. The factor i has to occur for a quantity to be Hermitian, while
here ℏintroduces Planck’s action quantum as a scale factor.
The comparison of the position and momentum representations {|r⟩} and {|p⟩}
is instructive. These diagonalize the position and momentum operators, respec-
tively. In particular, from the basic commutation relation [X k, Pk′] = iℏδk
k′ 1, we
have derived the representation of each operator in the other basis, and also ⟨r |p⟩=
⟨p |r⟩∗= (2πℏ)−3/2 exp(ip · r/ℏ). This probability amplitude is usually called the
wave function of the state with momentum p. For the derivation we used the equation

338
4
Quantum Mechanics I
Fig. 4.10 Eigenvalues of the angular momentum operator for m ∈{−j, . . . , j} and j ∈{0, 1
2, 1, . . .}.
Half-integer eigenvalues (open circles) occur only for spin momenta, because the real-space repre-
sentations are then ambiguous
x δ′(x) = −δ(x) and thus found
Pk = ℏ
i
∂
∂xk
and
X k = iℏ
∂
∂pk
for Pk in the position representation and X k in the momentum representation. If we do
notuseCartesiancoordinates,thencovariantandcontravariantcomponentsarediffer-
ent. Note that the metric fundamental tensor generally depends upon the position. For
the kinetic energy, which is a scalar, we need, e.g., the quantity 
k PkPk= −ℏ2.
We have already derived the Laplace operator for general coordinates on p. 38:
ψ =
1
√g

ik
∂
∂xi
√g gik ∂ψ
∂xk

,
with
g ≡det(gik) .
We have also investigated the way the non-commutability of operators affects phys-
ical laws for the case of the angular momentum. For l ̸= 0, only one directional
component can be sharp, along with the square of the angular momenta, which has
eigenvalues l (l + 1) ℏ2 with l ∈{0, 1, 2, . . .}. The directional quantum number m for
a given l can only be an integer between −l and +l. Using L = R × P, we derived
these properties from those of R and P (see Fig.4.10).
4.4
Time Dependence
4.4.1
Heisenberg Equation and the Ehrenfest Theorem
We now consider time dependence. We shall be guided once again by classical
physics.
If a is a function of the canonical position and momentum coordinates, and also
of the time, we have
da
dt =

k
 ∂a
∂xk
dxk
dt + ∂a
∂pk
dpk
dt

+ ∂a
∂t .

4.4 Time Dependence
339
As already shown on p. 124, using the Hamilton equations
dxk
dt = ∂H
∂pk
,
dpk
dt = −∂H
∂xk ,
we ﬁnd classically
da
dt =

k
 ∂a
∂xk
∂H
∂pk
−∂a
∂pk
∂H
∂xk

+ ∂a
∂t
≡[a, H] + ∂a
∂t .
The derivative da/dt is thus equal to the Poisson bracket [a, H], if we disregard any
explicit time dependence.
Now, in quantum theory, on p. 316 we already assigned the commutator of the
corresponding operators (divided by iℏ) to the classical Poisson bracket. This idea
for translating between the classical and quantum cases leads us to
dA
dt = [A, H]
iℏ
+ ∂A
∂t ,
known as the Heisenberg equation. Here we have to take any time-independent rep-
resentation and then differentiate each matrix element of A with respect to time in
order to form dA/dt (in this representation). We shall usually restrict ourselves to
operators A, which do not depend on time explicitly. Then all operators commuting
with H (their eigenvalues are called good quantum numbers) are constants of the
motion, in particular the Hamilton operator H itself. Hence the energy representa-
tion, which diagonalizes H, is particularly important, and we shall consider many
examples in the next section. Note that friction effects are beyond the scope of this
section and will be treated only in Sect.4.4.3.
With the Heisenberg equation, we can now determine the derivatives of expecta-
tion values with respect to the time, taking time-independent states as the basis:
d ⟨A⟩
dt
= i
ℏ⟨[H, A] ⟩+ ∂⟨A⟩
∂t
.
If we use here H = P2/2m + V (R) and determine the derivatives of ⟨R⟩and ⟨P⟩
with respect to time, then ⟨[P2, R ] ⟩is important in the ﬁrst case and ⟨[V (R), P ] ⟩
in the second. Now [P2, X ] = [Px2, X ] = −2iℏPx, and in addition (according to
p. 316), [f (X ), P] = iℏf ′(X ) holds. Consequently, the following equations are valid:
d ⟨R⟩
dt
= ⟨P⟩
m
and
d ⟨P⟩
dt
= ⟨−∇V ⟩≡⟨F⟩.
Thus the expectation values satisfy the equations of classical physics, which is known
as Ehrenfest’s theorem, although ⟨F(R )⟩does not need to be equal to F(⟨R⟩).

340
4
Quantum Mechanics I
In order to see how the uncertainties in R and P change with time, we determine
d

⟨R · R⟩−⟨R⟩· ⟨R⟩

dt
= ⟨P · R + R · P⟩−2 ⟨R ⟩· ⟨P⟩
m
,
d

⟨P · P⟩−⟨P⟩· ⟨P⟩

dt
= ⟨P · F + F · P⟩−2 ⟨P ⟩· ⟨F⟩.
For a constant force (e.g., in the free case), we have ⟨P · F ⟩= ⟨P⟩· F = ⟨F · P⟩.
Thus then the momentum uncertainty remains constant, and for sharp momentum,
so does the position uncertainty.
4.4.2
Time Dependence: Heisenberg and Schrödinger
Pictures
Inthelastsection,westartedfromtheso-calledHeisenbergpicture.IntheHeisenberg
picture the observables depend on the time, but the states do not:
d
dt AH = i
ℏ[HH, AH] + ∂
∂t AH ,
d
dt |ψH⟩= |o⟩.
To solve the Heisenberg equation, we search for a time-dependent unitary transfor-
mation U which connects the operator AH (A in the Heisenberg picture) with an
operator AS (A in the Schrödinger picture), which does not depend upon time:
AS = UAH U † ,
with
dAS
dt
= 0 .
Hence the Heisenberg equation delivers
0 = dU
dt AH U † + U
 i
ℏ[HH, AH] + ∂AH
∂t

U † + UAH
dU †
dt
.
If we restrict ourselves to observables which depend on time only implicitly, whence
∂AH/∂t = 0, then this condition can be satisﬁed for all operators AH if the unitary
operator U satisﬁes
dU
dt + i
ℏUHH = 0
⇐⇒
dU †
dt
−i
ℏHH U † = 0 .
Here the zero times coincide in the two pictures: U(0) = 1 or AH(0) = AS. Both
requirements are satisﬁed by the time-shift operator
U(t) = exp −i HH t
ℏ
,

4.4 Time Dependence
341
if HH does not depend on time (otherwise we still have to integrate, as we shall see
in Sect.4.4.4). Note that, since p. 317, we already know of similar position- and
momentum-shift operators. The Hamilton operator in this situation also commutes
with U, and hence HH = HS = H. We shall now restrict ourselves to this case.
In addition, from |ψS⟩= U |ψH⟩, we can say that: In the Schrödinger picture the
states do not depend on time, but the observables (Schrödinger equation) do:
d
dt AS = 0 ,
d
dt |ψS⟩= −i
ℏH |ψS⟩.
In general, differential equations for Hilbert space vectors are easier to integrate than
those for operators (the Heisenberg equation). Hence, we shall work mainly in the
Schrödinger picture and leave out the subscript S. In particular, we then have, in the
real-space representation,
ℏ
i
∂ψ(t, r )
∂t
+ Hψ(t, r ) = 0 ,
where H(R, P ) = H(r, −iℏ∇) is to be taken. This equation is similar to the
Hamilton–Jacobi differential equation of p. 135, viz.,
∂W
∂t + H(r, ∇W) = 0 ,
if Hamilton’s action function W =

L dt is replaced by −iℏψ, with Planck’s action
quantum h = 2πℏ. However, instead of ∇W · ∇W, we have not −ℏ2∇ψ · ∇ψ, but
rather −ℏ2∇· ∇ψ = −ℏ2ψ.
If we restrict ourselves to particles of mass m and charge q in an electric potential
	, then the time-dependent Schrödinger equation (in the real-space representation)
reads
iℏ∂
∂t ψ(t, r ) =

−ℏ2
2m  + V (r )

ψ(t, r ) ,
with V (r ) = q 	(r ). If we consider the wave function associated with an eigenstate
of H with the sharp energy En, where the zero energy may be chosen arbitrarily (a
different zero leads only to a new time-dependent phase factor in the wave function,
which will not affect the experimental value), we have
ψn(t, r ) = exp −i En t
ℏ
ψn(r ) ,
with ψn(r ) ≡ψn(0, r ) ,
and it only remains to solve the time-independent Schrödinger equation (in the real-
space representation)

342
4
Quantum Mechanics I
En ψn(r ) =

−ℏ2
2m  + V (r )

ψn(r ) .
For a magnetic ﬁeld, instead of V and in addition to q 	, further terms are still to be
considered, as was shown in Sect.4.3.7. Since for all states with sharp energy, the
time appears only in the phase factor, which does not affect the expectation values,
they are called stationary states.
In the Schrödinger picture, if we transform with any time-dependent unitary oper-
ator U, we obtain
|ψ′⟩= U |ψ⟩,
with iℏd
dt |ψ⟩= H |ψ⟩and iℏd
dt |ψ′⟩= H ′ |ψ′⟩,
and clearly also iℏ( ˙U|ψ⟩+ U| ˙ψ⟩) = H ′U|ψ⟩, or
H ′ = iℏdU
dt U † + UH U † .
An example of an application is the unitary transformation to H ′ = 0, which clearly
results in iℏ˙U = −UH, or U = exp(iHt/ℏ) (if H does not depend on time). This
corresponds to the transition from the Schrödinger to the Heisenberg picture, the
states of which do not depend on time.
4.4.3
Time Dependence of the Density Operator
The density operator turns out to be useful also for the time dependence. In particular,
we may also use time-independent expansion bases in the Schrödinger picture if the
density operator takes care of the time dependence. In the Heisenberg picture, it does
not depend on time.
According to p. 312, unitary transformations do not change expectation values.
Hence for the time dependence, the notation
⟨A⟩= tr{U(t) ρH U †(t) AS}
is to be preferred, since ρH and AS do not depend on time, and in addition to
U †(t) AS U(t) = AH(t), we have
ρS(t) = U(t) ρH U †(t) .
We can read off from this that the density operator ρS(t) and the observables AH(t)
depend oppositely (contravariantly) on time. With ρ = UρHU † (leaving out the sub-
script S), we have the von Neumann equation

4.4 Time Dependence
343
dρ
dt = [H, ρ]
iℏ
.
The equation dρ/dt = 0 in the Heisenberg picture corresponds classically to the
Liouville equation (see p. 129) dρ/dt = 0, which is then reformulated as ∂ρ/∂t +
[ρ, H] = 0, because the classical probability density ρ (in phase space) depends
upon further variables in addition to t. The density operator depends only on time,
the other variables being selected only with their representation. Hence, it does not
make sense to write the von Neumann equation (as an operator equation) with the
partial derivative ∂ρ/∂t = [H, ρ]/iℏ.
In the energy representation, that is, with H |n⟩= |n⟩En, ⟨n|n′⟩= δnn′, and

n |n⟩⟨n| = 1, the von Neumann equation implies
⟨n| ρ (t) |n′⟩= ⟨n| ρ (0) |n′ ⟩exp −i (En −En′) t
ℏ
.
Only the energy differences are important here—the zero of the energy does not
affect the density matrix.
According to the von Neumann equation, none of the expectation values of powers
of ρ depend on time, since d⟨ρn⟩/dt ∝tr(ρn[H, ρ]) always vanishes. This does not
lead to arbitrarily many invariants, but to exactly N constants of the motion in an
N-dimensional Hilbert space (the normalization condition ⟨ρ0⟩= ⟨1⟩= 1 counts
here). In particular, the purity of a state remains (trρ2), something that is changed
only by dissipation (see Sect.4.6), and this cannot be described with Hamiltonian
mechanics.
The von Neumann equation becomes rather simple for doublets. For these, accord-
ing to pp. 309 and 312, we have
H = 1 trH + σ · tr(σ H)
2
and
ρ = 1 + σ · ⟨σ⟩
2
.
We thus search for d⟨σ⟩/dt = tr(σ dρ/dt). Now tr(σ [H, ρ]) = ⟨[σ, H]⟩. The com-
mutator of σ with 1
2σ · tr(σ H) is thus important, and according to p. 325, this can
be derived from the expression i tr(σ H) × σ. Hence we obtain in total
d ⟨σ⟩
dt
=  × ⟨σ⟩,
with  ≡tr(σ H)
ℏ
,
as for the motion (see p. 92). A well known example is the Larmor precession of a
magnetic moment in a magnetic ﬁeld, where H = −μB σ · B appears as the Hamilton
operator in the Pauli equation (p. 327), whence tr(σ H) = −2μB B.
For the Larmor precession, ⟨σ⟩denotes the spin polarization. But in general we
may also understand |↑⟩and |↓⟩as states other than those with ms = ± 1
2. We then
speak generally of a Bloch vector ⟨σ⟩. According to p. 308, with † + † = 1,
we then have

344
4
Quantum Mechanics I
H =† H↑↑+ † H↑↓+  H↓↑+ † H↓↓
=1 H↑↑+ H↓↓
2
+ σx
H↓↑+ H↑↓
2
+ σy
H↓↑−H↑↓
2i
+ σz
H↑↑−H↓↓
2
.
With trσ = 0, trσσi = 2ei, and H↑↓= H↓↑∗, we obtain
tr(σ H) = 2

ex Re H↓↑+ ey Im H↓↑+ ez
H↑↑−H↓↓
2
	
= ℏ ,
and this vector determines the precession of the Bloch vectors ⟨σ⟩in a space whose
z-component contains information about the occupation of the states |↑⟩and |↓⟩.
Here ℏ tells us how much the two energy eigenvalues differ from each other,
which follows from det(H −E) = 0. According to p. 309, we have in particular,
E± = 1
2 (trH ± ℏ), where ℏ is the square-root of (trH)2 −4 det H = (H↑↑−
H↓↓)2 + 4|H↓↑|2.
The considerations can be transferred from 2 to N dimensions of the Hilbert space,
if, according to Sect.4.2.5, we start from a basis {Cn} of time-independent Hermitian
operators. In particular, according to the von Neumann equation (see p. 313),
trρ2 = 1
c

n
⟨Cn⟩2
is conserved, and for C0 = √c/N 1, so is ⟨C0⟩. The Bloch vector with real compo-
nents ⟨C1⟩, . . . has the same length at all times. Here, according to the von Neumann
equation, we have
d⟨Cn⟩
dt
=

n′
nn′ ⟨Cn′⟩,
with ℏnn′ ≡tr(iH [Cn, Cn′])
c
,
and for C0 ∝1, we may restrict ourselves to n ̸= 0 ̸= n′. If H does not depend on
time, then neither do any of the coefﬁcients nn′ of the system of linear differential
equations. Since they are all real and form a skew-symmetric matrix,
nn′ = nn′∗= −n′n ,
their eigenvalues are purely imaginary and pairwise complex-conjugate to each other.
The von Neumann equation also yields the time dependence of the Wigner func-
tion from Sect.4.3.5:
ρ(t, r, p ) =
1
(πℏ)3

d3r ′ ⟨r −r ′| ρ(t) |r + r ′⟩exp +2i p · r ′
ℏ
=
1
(πℏ)3

d3p ′ ⟨p −p ′| ρ(t) |p + p ′⟩exp −2i r · p ′
ℏ
.

4.4 Time Dependence
345
With ⟨p −p ′| [P2, ρ] | p + p ′⟩= −4 p · p ′ ⟨p −p ′| ρ | p + p ′⟩, we have in par-
ticular [P2, ρ]/iℏ= −2p · ∇ρ, while on the other hand, if V depends upon the
position only locally, i.e., if we have ⟨r |V |r ′⟩= V (r ) δ(r −r ′), then
∂ρ(t, r, p )
∂t
+ p
m · ∇ρ(t, r, p )
= −i
ℏ
1
(πℏ)3

d3r ′ {V (r−r ′) −V (r+r ′)} ⟨r−r ′| ρ |r+r ′⟩exp 2i p · r ′
ℏ
.
For a harmonic oscillation, the right-hand side can be traced back to the expression
∇V · ∇p ρ(t, r, p ), i.e., to the gradient of ρ in momentum space. With p/m = v, we
thus have in the harmonic approximation (and naturally also for the free motion with
F = 0),
 ∂
∂t + v · ∇r + F · ∇p

ρ(t, r, p ) = 0 .
This is the collision-free Boltzmann equation, which holds quite generally in classical
mechanics (and also for other potentials, see Sect. 6.2.3), where ρ(t, r, p ) is then
the probability density in phase space.
4.4.4
Time-Dependent Interaction and Dirac Picture
In addition to the Heisenberg and Schrödinger pictures, there is also the Dirac pic-
ture, often called the interaction representation, used in particular in time-dependent
perturbation theory and scattering theory. There the Hamilton operator is split into a
free part H0 and an interaction V , viz.,
H = H0 + V ,
where H0 does not depend on time—otherwise the following equation would have
to be generalized, as will be shown later. If we set
U0(t) = exp −i H0 t
ℏ
,
then for H ≈H0, U ≈U0 is also valid, at least for time spans that are not too long.
Under the interaction representation, we now understand
|ψD(t)⟩= U0
†(t) |ψS(t)⟩= U0
†(t) U(t) |ψH⟩,
and correspondingly

346
4
Quantum Mechanics I
AD = U0
†AS U0 = U0
† U AH U † U0 .
Hence it follows that d|ψD⟩/dt = iℏ−1 (H0 −U0†H U0) |ψD⟩, so with H0 −HD =
−VD, we ﬁnd
d
dt |ψD⟩= −i
ℏVD |ψD⟩
and
d
dt AD = i
ℏ[H0, AD] .
In the Dirac picture the time dependence of the observables becomes ﬁxed by H0 and
that of the states by VD.
If we set |ψD(t)⟩= UD(t) |ψD(0)⟩with |ψD(0)⟩= |ψS(0)⟩= |ψH⟩, then we
obtain
UD(t) = U0
†(t) U(t)
⇐⇒
U(t) = U0(t) UD(t) .
Clearly, with iℏ−1U0†(H0 −H)U = −iℏ−1VD, we have the differential equation
dUD
dt
= −i
ℏVD(t) UD(t) .
To integrate this, we have to respect the order of the operators—an operator at a later
time should only act later and thus should stand to the left of operators at earlier
times. This requirement is indicated by the special time-ordering operator T:
UD(t) = T exp

−i
ℏ

 t
0
dt′ VD(t′)

.
The derivative of T exp
 t
0 A(t′) dt′ with respect to t is equal to A(t) times the expres-
sion to be differentiated. In addition, we have T exp(0) = 1. We thus obtain the
integral equation
T exp

 t
0
dt′ A(t′) = 1 +

 t
0
dt′ A(t′) T exp

 t′
0
dt′′ A(t′′) ,
which can be solved step by step:
T exp

 t
0
dt′ A(t′) = 1 +

 t
0
dt′ A(t′) +

 t
0
dt′

 t′
0
dt′′ A(t′) A(t′′) + · · · .
In the term of nth order, there are n time-ordered operators A. This expansion is used
in time-dependent perturbation theory. Terms higher than the ﬁrst contribution are
usually neglected.
For density operators, we have the equation ⟨A⟩= tr(ρA) in each picture. With
AS = U0ADU0† and AH = U †ASU = UD†ADUD, we thus ﬁnd

4.4 Time Dependence
347
ρD = U0
† ρS U0 = UD ρH UD
† ,
which leads to the differential equation
dρD
dt
= −i
ℏ[VD, ρD] .
With the series expansion for UD(t), we thus obtain
ρD(t) = ρD(0) −i
ℏ

 t
0
dt′ [VD(t′), ρD(0)]
−1
ℏ2

 t
0
dt′

 t′
0
dt′′ [VD(t′), [VD(t′′), ρD(0)]] + · · · .
Instead of [V ′, [V ′′, ρ]], we may also write [V ′, V ′′ρ]+h.c., where h.c. stands for
the Hermitian conjugate, because the operators are Hermitian.
Time-dependent perturbation theory leads to Fermi’s golden rule for the transition
rates. However, the procedure is often superﬁcial. We shall go into more detail when
we derive the golden rule in Sect.4.6.
An exact treatment without approximations can be found for the time-dependent
oscillator. This was already done for the classical case in Sect. 2.3.10 and especially
in Sect. 2.4.11. In particular, the Hamilton operator
H(t) =
1
α2(t)
 P2
2m + m
2 w2X 2
	
leads to the eigenvalue problem of the usual (time-independent) oscillator of mass
m and angular frequency w. The time dependence is contained here in the classical
functionα(t) andthus involves notime-orderingproblem—andthetime-independent
oscillator has eigenvalues ℏw (n + 1
2), as will be shown on p. 359. But, as already in
classical mechanics (see Sect. 2.4.11), these values for the −m f (t) X with the force
f are not the energy
E = (P −m F X )2
2m
+ m
2 f X 2 ,
with
˙F = f −f
and F = 0 .
In addition to the eigenvalues of H(t), Fig.4.11 shows the expectation values of
the energy with respect to the eigenstates of H. However, the energy uncertainties
are very large, and the values actually overlap in the right-hand picture. At least it
becomes clear that the eigenvalues do depend on time, although the energy barely
does so. In many cases, these properties of a time-dependent interaction are derived
only in the adiabatic approximation (for sufﬁciently slow changes).

348
4
Quantum Mechanics I
Fig. 4.11 Eigenvalues of H
(left) and expectation values
E = ⟨n|E|n⟩(right) for a
time-dependent harmonic
oscillator (both in the same
arbitrary unit). Here a = 1/2
and q = 1/4 was chosen in
the Mathieu equation. For
t = 0 it is force-free
4.4.5
Current Density
For stationary problems, an expression for the probability current follows from the
time-dependent Schrödinger equation. Since the total probability is conserved (it is
equal to 1), according to p. 187, we have the continuity equation
∂ρ
∂t + ∇· j = 0 ,
where ρ is the probability density |ψ(t, r )|2. Hence, from the Schrödinger equation,
we obtain
∂ρ
∂t = ψ∗∂ψ
∂t + ψ ∂ψ∗
∂t
= ψ∗Hψ −ψ Hψ∗
iℏ
,
and with
H = (P −qA) · (P −qA)
2m
+ q 	 ,
for the Coulomb gauge (i.e., with P · A = A · P) with P · P = (−iℏ)2 and −A ·
P = iℏA · ∇, we conclude
∂ρ
∂t = iℏψ∗ψ −ψ ψ∗
2m
+ qA · ψ∗∇ψ + ψ ∇ψ∗
m
.
Here, according to p. 16, the ﬁrst numerator is equal to ∇· (ψ∗∇ψ −ψ ∇ψ∗) and
the second is equal to ∇(ψ∗ψ). Hence, assuming the Coulomb gauge, the probability
current density is given by
j = ℏ
i
ψ∗∇ψ −ψ ∇ψ∗
2m
−qA ψ∗ψ
m
.

4.4 Time Dependence
349
Note that, for real wave functions, only the last term contributes. With ℏ∇ψ = iPψ
andℏ∇ψ∗= −i (Pψ)∗,togetherwithA = A ∗,andinthereal-spacerepresentation,
this is equivalent to
j = Re

ψ∗P −qA
m
ψ
	
.
Here, classically, (p −qA)/m is the velocity for a point-like particle of mass m and
charge q, and ψ∗ψ is the probability density. For the electric current density, we
obtain qj.
For spherically symmetric problems, we prefer to take the wave function
ψnlm(r ) = unl(r)
r
i l Y (l)
m () ,
with the spherical harmonic Y (l)
m () of p. 335, which is real up to the factor exp(imϕ).
Note that the radial functions unl are real for bound states, but complex for scattering
states, as we shall see in Sect.4.6. If we call the mass m0, in order not to confuse
with the directional quantum number m, and refer to p. 39, then using
∇= er
∂
∂r + eθ
1
r
∂
∂θ + eϕ
1
r sin θ
∂
∂ϕ ,
it follows for bound states that
j = eϕ
mℏ
m0
|ψnlm(r )|2
r sin θ
.
The term in A is missing here, because we have restricted ourselves to spherically
symmetric potentials. For bound states and eigenstates of the orbital angular momen-
tum L, there is only a probability current along the L -axis, if m ̸= 0.
For electrons, however, the spin (and magnetic moment) have to be considered.
We should take the Pauli equation from p. 327 as the Hamilton operator. Hence,
noting that electrons have negative charge q = −e, we start with
H = H0 + μB B · σ .
Since σ appears here, we use the spinors
ψ =
ψ↑
ψ↓
	
⇐⇒
ψ† = (ψ∗
↑, ψ∗
↓) ,
and ﬁnd the equations

350
4
Quantum Mechanics I
iℏ∂ψ
∂t = H0 ψ + μB B · σ ψ ,
−iℏ∂ψ†
∂t
= H0 ψ† + μB ψ† B · σ .
Note that H0 acts like the unit operator in the spin space, but generally changes ψ in
the position space, whence we have H0 ψ† and not ψ† H0 in the last row.
If we multiply the ﬁrst equation on the left by ψ† and the second on the right by
ψ, then subtract one from the other, it follows that
iℏ∂
∂t (ψ†ψ) = ψ†H0 ψ −(H0 ψ†) ψ .
Hence for the probability current density j , we obtain nearly the same expression as
previously. Instead of ψ∗(P −qA ) ψ, it now reads
ψ∗
↑(P −q A ) ψ↑+ ψ∗
↓(P −q A ) ψ↓.
For the electric current density, we should now not only take qj (with q = −e for
electrons), but also consider the magnetic moments, and according to p. 192, amend
∇× M, using M = −μB ψ† σ ψ for electrons.
4.4.6
Summary: Time Dependence
The time dependence is determined by the Hamilton operator. Then we distinguish
between the Heisenberg and Schrödinger pictures, depending on whether only the
observables or only the state vectors depend on time, respectively. In the Schrödinger
picture, we have the time-dependent Schrödinger equation
iℏd|ψ⟩
dt
= H|ψ⟩,
and in the Heisenberg picture, the Heisenberg equation
dA
dt = i
ℏ[H, A] + ∂A
∂t ,
which can be looked at as the quantum generalization of the classical equation
da/dt = [a, H] + ∂a/∂t (p. 124). We may also take observables and basis vectors as
constant and describe the time dependence by the density operator. This then obeys
the von Neumann equation (in the Schrödinger picture), viz.,
dρ
dt = −i
ℏ[H, ρ] ,

4.4 Time Dependence
351
which is the generalization of the Liouville equation to quantum theory.
Stationary states have a well-deﬁned energy. Hence, if H does not depend on time,
they are eigenstates of the Hamilton operator:
H|ψn⟩= |ψn⟩En ,
and in the Schrödinger picture they contain the time factor exp(−iEnt/ℏ). This leads
from the time-dependent to the time-independent Schrödinger equation (the last
equation), which in the real-space representation has the form

−ℏ2
2m  + V (r )

ψn(r ) = En ψn(r ) ,
since with P = (ℏ/i)∇, T =
1
2mP · P turns into T= −
1
2mℏ2. For particles with spin
in a magnetic ﬁeld, special terms also appear for the potential energy V .
If the problem cannot be solved for the full Hamilton operator H, but for the time-
independent approximation H0 = H −V , a perturbation theory is possible, using
the Dirac picture. Then H0 determines the time dependence of the observables and
VD = U0†VU0 that of the states.
4.5
Time-Independent Schrödinger Equation
4.5.1
Eigenvalue Equation for the Energy
In this section we search for the eigenvalues En and eigenvectors |n⟩of the Hamilton
operator H for a given interaction. We deal with the equation H |n⟩= |n⟩En and
assume that H has the form T + V with the (local) potential energy V (r ). (We shall
treat special cases, and in particular a magnetic ﬁeld and also particles with spin 1
2,
at the end. The exchange interaction in the Hartree–Fock potential is nonlocal, as
we shall see in Sect. 5.4.2.) Actually, V is an operator which is ﬁxed in the real-
space representation by ⟨r | V |r ′⟩. But for the local interaction here, we can write
V (r ) δ(r −r ′) and V (r ) ψ(r ) instead of ⟨r | V |ψ⟩=

d3r ′ ⟨r | V |r ′⟩⟨r ′|ψ⟩.
We shall usually take the real-space representation in order to make use of this
locality of the interaction. From (H −En) |n⟩= 0 and ⟨r |n⟩≡ψn(r ) with P = −
iℏ∇, we obtain the differential equation

−ℏ2
2m  + V (r ) −En

ψn(r ) = 0 .
This is not yet an eigenvalue equation though, but only a partial, linear, and homo-
geneous differential equation of second order (for which the value and the gradient

352
4
Quantum Mechanics I
of the solution at a boundary can still be given arbitrarily in order to ﬁx a special
solution).
But ψn(r ) will now be a probability amplitude, which means that the expression

d3r |ψn(r )|2 will be normalized to 1. However, we have also allowed improper
Hilbert vectors, for which we have

d3r ψn
∗(r ) ψn′(r ) = δ(n −n′) ,
with continuous n and n′. But for discrete values we require

d3r ψn
∗(r ) ψn′(r ) = δnn′ ,
which can only be satisﬁed for special energies, as will be shown soon.
In order to make that clear, we restrict ourselves to the one-dimensional problem,
i.e., to a standard differential equation, and consider
ψ′′(x) + 2m
ℏ2 {E −V (x)} ψ(x) = 0 .
If V (x) decreases faster than |x|−1 for large |x|, so that E −V →ℏ2k2/2m, the
asymptotic solutions exp(± ikx) for k ̸= 0 can be superposed linearly. For k2 > 0,
theyoscillateandwecannormalizeinthecontinuum.Butfork2 < 0,wecanonlytake
exp(−|kx|), since exp(+|kx|) is not normalizable. For E < V (x), all wave functions
have to vanish exponentially for x →±∞, with speciﬁc dependence according to the
differential equation. This is possible only for appropriate (countable) eigenvalues.
These considerations are also valid for the case in which V (x) behaves asymptot-
ically as |x|−1 (which requires an amendment ∝i ln |kx| to the exponent). The sign
of E −V is decisive, also in three dimensions.
4.5.2
Reduction to Ordinary Differential Equations
We shall only consider potentials whose variables can be separated, i.e., potentials
which can be written as a sum of terms, each of which depends on only one variable.
Then the partial differential equation can be separated into three ordinary ones and
solved much more easily.
Suppose for example that V (r ) = V (x) + V (y) + V (z). Then the product ansatz,
with each term involving just one Cartesian coordinate, i.e., ⟨x|nx⟩⟨y|ny⟩⟨z|nz⟩(and
energy En separating into three terms) provides a way forward. In this way, the given
partial differential equation can be reduced to three ordinary ones of the form
 d2
dx2 + 2m
ℏ2 {Enx −V (x)}

⟨x|nx⟩= 0 .

4.5 Time-Independent Schrödinger Equation
353
If we multiply this equation by ⟨y|ny⟩⟨z|nz⟩and add the corresponding equations in
the variables y and z, then with En = Enx + Eny + Enz, we have the original partial
differential equation. If at least two of these potentials are the same, then degeneracy
arises and the different equations result in the same eigenvalues.
For a central potential V (r ) = V (r) spherical coordinates are usually more appro-
priate than Cartesian ones. As is well known, the Laplace operator in spherical coor-
dinates reads (see p. 39)
ψ = 1
r
∂2
∂r2 rψ + 1
r2

1
sin θ
∂
∂θ

sin θ ∂
∂θ

+
1
sin2 θ
∂2
∂ϕ 2

ψ .
According to p. 335, the eigenfunctions of the operator in the curly bracket are the
spherical harmonics, with the eigenvalue −l (l+1). In classical mechanics, for a
central ﬁeld, we also made use of the angular momentum as a conserved quantity
(p. 142). We thus set
ψnlm(r, θ, ϕ) = unl(r)
r
i l Y (l)
m () ,
where m is the directional quantum number, and obtain the radial equation
 d2
dr2 −l (l+1)
r2
+ 2m
ℏ2 {Enl −V (r)}

unl(r) = 0 ,
with unl(0) = 0 ,
withmthemassonceagain.Thisboundaryconditionrequiresψnlm tobedifferentiable
at the coordinate origin, since we have divided by r. The further boundary condition
unl →0 for r →∞is still required for the normalizability of the bound states. It
leads to an eigenvalue equation for the energy. Note that these eigenvalues no longer
depend on the directional quantum number m. The spherical symmetry leads to a
2l-fold degeneracy, i.e., there are 2l+1 different eigensolutions with equal energy.
Near the origin, for l ̸= 0, the second term usually outweighs the other ones and we
have u′′ −l (l+1) r−2 u ≈0. This differential equation has the linearly independent
solutions r−l and rl+1. Only the second vanishes at the origin (also for l = 0). Hence,
we usually set unl in the form unl(r) = rl+1 fnl(r).
4.5.3
Free Particles and the Box Potential
For free particles, the Hamilton operator consists of only the kinetic energy P2/(2m),
so we use the eigenfunctions of the momentum, or indeed of k = p/ℏ, from
Sect.4.3.3:
H = P2
2m
=⇒
Ek = ℏ2k2
2m
and
ψk(r ) = exp(ik · r )
√
2π 3
.

354
4
Quantum Mechanics I
There we also saw that

d3r ψk∗(r ) ψk′(r ) = δ(k −k ′) in that case.
The sharp wave vector k (and the sharp energy Ek) are idealizations. Actually,
for these continuous variables, we should consider their uncertainty and hence take
a superposition of terms with different wave vectors, a so-called wave packet. The
energy uncertainty means that we cannot simply split off a factor exp(−iωt), but we
only have
ψ(t, r ) =
1
√
2π 3

d3k ψ(k ) exp{i (k · r −ω(k ) t)} ,
because ω = ℏk2/(2m) depends upon k. If only wave numbers from the near
neighborhood of k contribute, then the group velocity of this wave packet, viz.,
(dω/dk)k = ℏk/m = p/m = v, is twice the phase velocity ω/k. Hence, in the course
of time, the wave packet changes shape. If we take, e.g., a Gauss function for ψ(k ),
as on p. 321 (the smallest possible uncertainty product x(0) · k = 1/2), then the
position uncertainty increases with time:
x(t) = x(0)

1 + {2ℏ(k)2 t/m}2 =

{x(0)}2 + {v t}2 ,
since x(0) · k = 1/2, while x moves with the velocity v = ℏk/m.
A further example is that of a box with impermeable walls. Here the probability
density may differ from zero only inside the box. Outside the container, the wave
function must vanish, since the time-independent Schrödinger equation makes sense
only if V (r ) ψ(r ) is ﬁnite everywhere. In addition, the wave function must also be
differentiable, thus continuous everywhere. This allows only a countable sequence
of energies.
In the one-dimensional case, with V (x) = 0 for 0 < x < a, otherwise inﬁnite, the
boundary conditions ψ(0) = 0 = ψ(a) and the normalization to 1 ﬁx the eigenso-
lutions up to a phase factor. For n ∈{1, 2, . . .} and the abbreviation
kn = nπ
a ,
and with ψn′′ + kn
2ψn = 0, we have
ψn(x) =

2
a sin knx ,
for 0 ≤x ≤a, otherwise zero, En = ℏ2kn
2
2m
.
There is no normalizable solution for n = 0, and negative integers n deliver no further
linearly independent solutions (see also Fig.4.12).
Correspondingly, for a cuboid in three dimensions with side lengths ax, ay, az, if
we have ki = niπ/ai with ni ∈{1, 2, . . . },

4.5 Time-Independent Schrödinger Equation
355
Fig. 4.12 Energy eigenvalues and eigenfunctions of a box potential with inﬁnitely high walls. The
ﬁgure shows the potential and also the eigenvalues as horizontal lines. Each of these lines serves as
an axis for the associated eigenfunction, where functions with even n are plotted with continuous
lines, and those with odd n as dashed lines
ψn(r ) =

8
V sin kxx sin kyy sin kzz ,
for 0 ≤x ≤ax , etc. ,
En = ℏ2 (kx
2 + ky
2 + kz
2)
2m
.
For a cube (ax = ay = az), there is degeneracy due to the symmetry, since we can
permute nx, ny, nz with each other and obtain the same energy value En ∝n2 =
nx2 + ny2 + nz2. In addition, there are also accidental degeneracies. For example,
the state (nx, ny, nz) = (3, 3, 3) and the three states (5,1,1), (1,5,1), and (1,1,5) have
the same energy, because here n2 is equal to 27 for each.
The potential discussed here is used for the Fermi gas model. In this many-body
model,weneglecttheinteractionbetweentheparticlesandconsideronlythequantum
conditions, which stem from the inclusion of the particles in the cube volume. In
contrast to the classical behavior, only discrete energy values (and wave numbers)
are allowed. For such a gas we also need the number of states whose energy is less
than an energy bound called the Fermi energy:
EF = ℏ2
2m kF
2 .
Then clearly, n2 ≤(a kF/π)2. Hence contributions come from all points with positive
integer Cartesian coordinates inside the sphere of radius akF/π. For sufﬁciently large
akF, the number of states is
N ≈1
8
4π
3
akF
π
3
=
V
6π2 kF
3 =
V
6π2
2mEF
ℏ2
3/2
.
According to the Pauli principle, for spin-1/2 particles, each of these states can be
occupied by two fermions.

356
4
Quantum Mechanics I
If we search for the bound states, with negative energy eigenvalues En < 0, in
a box of ﬁnite depth V0 and width a = 2 l, i.e., with V (x) = −|V0| for −l < x < l,
otherwise zero, then with the real abbreviations
κn ≡

2m
ℏ2 |En|
and
kn ≡

2m
ℏ2 (|V0| −|En|) ,
the differential equations ψ′′ −κ 2
n ψ = 0 for |x| > l and ψ′′ + k 2
n ψ = 0 for |x| < l
imply a set of even states {ψ+(x) = ψ+(−x)}:
ψ+(x) ∝
⎧
⎨
⎩
exp{κn(l + x)}
for
x ≤−l ,
α cos(knx)
for −l ≤x ≤+l ,
exp{κn(l −x)}
for
l ≤x ,
and a set of odd states {ψ−(x) = −ψ−(−x)}:
ψ−(x) ∝
⎧
⎨
⎩
+ exp{κn(l + x)}
for
x ≤−l ,
β sin(knx)
for −l ≤x ≤+l ,
−exp{κn(l −x)}
for
l ≤x .
The wave functions and their ﬁrst derivatives have to be continuous everywhere, oth-
erwise a differential equation of second order does not make sense. (In the present
case, the second derivative jumps twice by a ﬁnite value. For the previously consid-
ered inﬁnite potential step, however, the second derivative changes so considerably
stepwise that even the ﬁrst derivative jumps there.) At the limits x = ± l, these prop-
erties ﬁx α and β and also require as eigenvalue condition that κn/kn be equal to
tan(knl) for the even states and −cot(knl) for the odd states. These requirements
with z ≡knl = 1
2 kna, ζ ≡(2mℏ−2 |V0|)1/2 l, and κn2/kn
2 = ζ 2/z2 −1 are easier to
solve, if we satisfy (starting with n = 0)
even eigensolution
| cos z | = z
ζ
for
n π ≤z ≤(n+ 1
2) π ,
odd eigensolution
| sin z | = z
ζ
for (n + 1
2) π ≤z ≤(n+1) π .
From z = knl, it follows that En = −|V0| (1 −z2/ζ 2). For ﬁnite V0a2, there are also
only ﬁnitely many bound-state eigensolutions, namely at most 2ζ/π (see Fig.4.13).
For the unbound solutions (“continuum states” with arbitrary E > 0), the potential
can be attractive or repulsive:
V (x) = V0 ,
for
−a < x < 0 , otherwise zero.
Here we use the real abbreviations
K ≡
√
2mℏ−2 E
and
k ≡

2mℏ−2 |E −V0| ,

4.5 Time-Independent Schrödinger Equation
357
Fig. 4.13 Eigenvalues for
the box potential of ﬁnite
depth. Solutions are the
intersections (full circles) of
the straight line z/ζ with the
curves | cos z | (continuous
lines) and | sin z | (dashed
lines). Here,
ζ = |2mV0|1/2 a/2ℏ
and let a wave come in from the left (x < −a). At the potential steps, it is partially
reﬂected and partially refracted. For E > V0, we then have
ψ(x) ∝
A exp{iK(x+a)}+ B exp{−iK(x+a)} ,
for
x ≤−a ,
cos kx + iκ sin kx ,
for −a ≤x ≤0 ,
exp(iKx) ,
for 0 ≤x ,
with κ = K/k. Here use has already been made of the continuity of the wave function
and of its ﬁrst derivative at x = 0, and the factor for x > 0 was set arbitrarily equal
to 1, while a common factor is still missing. The continuity conditions for x = −a
require
A = cos ka −i κ + κ−1
2
sin ka
and
B = −i κ −κ−1
2
sin ka .
With the parameter ζ = |2mV0|1/2 a/2ℏ, we have ka = 2ζ |E/V0 −1|1/2. For E <
V0, k is to be replaced by ik (κ by −iκ) and we note that cos iz = cosh z and sin iz =
i sinh z.
If the probability current density jd is refracted (transmitted), then the probability
current density je = jd |A|2 comes in and jr = jd |B|2 is reﬂected. The transmittance
D ≡jd/je and reﬂectivity R ≡jr/je together sum to 1: D + R = (1 + |B|2)/|A|2 = 1.
We obtain (see Fig.4.14)
Fig. 4.14 Transmittance D
at steps of height V0 and
width a as a function of the
energy E for three values of
the parameter
ζ = |2mV0|1/2 a/2ℏ, namely
1/2 (green), 1 (blue), and 2
(red). The classical case is
shown with a /it dashed line

358
4
Quantum Mechanics I
D =
⎧
⎪⎪⎨
⎪⎪⎩

1 + V02 sin2 ka
4 E |E −V0|
−1
for E > V0 ,

1 + V02 sinh2 ka
4 E |E −V0|
−1
for E < V0 ,
While for E < V0 nothing is refracted classically, according to quantum theory, the
tunnel effect occurs because the uncertainty relations have to be observed. Due to
the position uncertainty, the ﬁnite length a does not “really” act, and because of the
momentum uncertainty, neither does the ﬁnite potential step height. In particular, for
ka ≫1 (and E < V0), we have
D ≈16 E |E −V0|
V02
exp(−2ka) .
On the other hand, for E > V0, D = 1 classically, but according to quantum theory
all is refracted only if E ≫|V0| or ka is an integer multiple of π. This is also shown
in Fig.4.14.
4.5.4
Harmonic Oscillations
We shall not determine the eigenvalues for linear oscillations here using their dif-
ferential equation and boundary conditions, but algebraically, using some extremely
useful operators. We have H =
1
2m P2 + m
2 ω2 X 2. With an energy unit ℏω, a momen-
tum unit p0 ≡
√
2ℏmω, and a length unit x0 ≡2ℏ/p0 = √2ℏ/mω, this leads to the
equation
H
ℏω = X 2
x02 + P2
p02 .
If we now set
X = x0
 + †
2
and
P = p0
 −†
2i
,
whence  = X /x0 + iP/p0 and † = X /x0 −iP/p0, then the commutation relation
[X , P] = iℏ1 together with x0 p0 = 2 ℏimply the equation
[, †] = 1 ,
and in addition
H = 1
2 ℏω {, †} = ℏω († + 1
2) .

4.5 Time-Independent Schrödinger Equation
359
The commutation relation [, †] = 1 is known already from p. 302, in particular,
for the creation and annihilation operators of bosons. From this commutation relation,
we obtained there the eigenvalues of †. Hence we already know the energy
eigenvalues of the linear oscillator:
En = ℏω (n + 1
2) ,
with n ∈{0, 1, 2, . . . } .
The energies of neighboring states all differ by ℏω (see Fig.4.15). This use of Bose
operators makes it possible to treat oscillations as particles. The sound quantum is
called a phonon, and the quantum of the electromagnetic ﬁeld (the light quantum) a
photon.
The energy ℏω/2 of the ground state, with n = 0, is called the zero-point energy.
It is not zero, because otherwise position and momentum would both be sharp. But
then the product of the uncertainties could be as small as possible. The expectation
values of  and † vanish in the ground state and so also do X and P. In contrast,
for X 2 and P2, it is important to note that († ± )2 = ± † = ±1. We thus have
X = 1
2x0 and P = 1
2p0, so their product is equal to 1
2ℏ, and hence as small as
possible.
According to p. 128 the Hamilton function of a point charge in a magnetic ﬁeld
can be transformed canonically to that of a linear oscillation with the cyclotron
frequency ω = qB/m. Quantum mechanically, we then ﬁnd the energy eigenvalues
(Landau levels) with equal distances. However, degeneracy should be noted, as for
two-dimensional isotropic oscillations.
According to p. 321, we already know the wave functions of all states with the
smallest possible product of the uncertainties X · P: these are the Gauss functions
normalized to 1. Consequently, for the ground state we have
ψ0(x) =
4√2/π
√x0
exp −x2
x02 .
Let us now turn to its remaining stationary states, i.e., those with sharp energy.
According to p. 302, their eigenfunctions can be can built up with the creation
operators † from the ground state: |n⟩= (n!)−1/2 (†)n |0⟩. From there, we have
† = x/x0 −1
2x0 d/dx.
With
s =
√
2 x/x0 = x√mω/ℏ,
this
becomes
† =
2−1/2 (s −d/ds). But we may also replace the operator s −d/ds by −exp( 1
2s2)
d/ds exp(−1
2s2) and apply n times to ψ0. Now we have Rodrigues’ formula for
Hermite polynomials:
Hn(s) ≡(−)n exp(s2) dn
dsn exp(−s2) .
With δ(s −s′) = δ(x −x′) x0/
√
2 and x0/
√
2 = √ℏ/mω, which implies |s⟩=
|x⟩4√ℏ/mω, the result is (see Fig.4.15)

360
4
Quantum Mechanics I
Fig. 4.15 Energy eigenvalues and eigenfunctions for linear oscillations. As in Fig.4.12, we show
the potential and eigenvalues (horizontal lines). These lines also serve as axes for the associated
eigenfunctions, both even (continuous lines) and odd (dashed lines). States with sharp energy are
stationary. Only for uncertain energy do oscillations occur. This will be discussed in Sect. 5.5.3
(see also Figs.4.20 and 4.21). As a function of the displacement, the eigenfunctions oscillate in the
classicallyallowedregion,whileintheclassicallyforbiddenregions,theytendtozeromonotonically
(tunnel effect)
ψn(s) = exp(−1
2 s2)

2n n!√π
Hn(s) .
So we only need to know the Hermite polynomials.
Clearly, H0(s) = 1 and H1(s) = 2s. The other polynomials can be obtained faster
than by differentiation, if we use the recursion formula
Hn+1(s) = 2s Hn(s) −2n Hn−1(s) .
Before the proof, we derive the generating function of the Hermite polynomials:
exp(2st −t2) =
∞

n=0
Hn(s) tn
n! .
We have exp{−(t −s)2} = 
n dn exp{−(t −s)2}/dtn|t=0 tn/n! according to Taylor.
Here the derivative up to the factor (−1)n is equal to the n th derivative with respect
to s for t = 0, thus equal to (−1)n dn exp(−s2)/dn. Consequently, using the above-
mentioned generating function, we may derive further properties of the Hermite
polynomials. In particular, we only need to differentiate with respect to t and then
compare coefﬁcients in order to prove the formula. For |s| ≫1, we also ﬁnd Hn(s) ≈
(2s)n. If we differentiate the generating function with respect to s, then Hn′ = 2n Hn−1

4.5 Time-Independent Schrödinger Equation
361
and hence Hn′′ = 2nHn−1′. If we use the recursion formula in the ﬁrst derivative, we
obtain the differential equation
Hn
′′(s) −2s Hn
′(s) + 2n Hn(s) = 0 .
Written as a polynomial, we have
Hn(s) =
[n/2]

k=0
ak (2s)n−2k ,
with
ak+1
ak
= −(n −2k)(n −2k −1)
k + 1
,
and a0 = 1. Clearly, Hn(−s) = (−)n Hn(s), so we also know the parities of the states.
According to classical mechanics, there are oscillations only for T = E −V > 0.
Hence, we would have to require 1
2ℏω (2n + 1) > 1
2 mω2x2, or put another way,
s2 = x2 mω/ℏ< 2n + 1. In fact, the Schrödinger equation for linear oscillations can
be written in the form ψ′′(s) + (2n + 1 −s2) ψ(s) = 0. For s2 = 2n + 1, the sign of
ψn′′ therefore changes, without |ψn|2 vanishing for larger values of |s|. Moreover, in
the classically forbidden region (with T < 0), there is still a ﬁnite probability density.
We already met this tunnel effect in the last section.
In three dimensions, for the isotropic oscillator, we have
En = (n + 3
2) ℏω ,
with n = nx + ny + nz ∈{0, 1, 2, . . . }.
Except for the ground state, all states are degenerate: nx and ny can be chosen arbi-
trarily, as long their sum is ≤n, while nz is ﬁxed. There are therefore 1
2(n + 2)(n + 1)
different states in the same “oscillator shell”. They all have parity (−1)n.
Since a central ﬁeld is given, we can also express the oscillation quantum number
n in terms of the angular momentum quantum number l and the radial quantum
number nr. There are always 2l+1 degenerate states of equal parity for each value
of l. However, the isotropic oscillator is more strongly degenerate. Here n and l are
either both even or both odd because of the parity. Their difference is an even number.
In fact, we have
n = 2 (nr−1) + l .
Here the radial quantum number nr starts with the value 1, as is usual in nuclear
physics. We then have the following shells: 1s, 1p, 1d-2s, 1f-2p, 1g-2d-3s, and so on.
4.5.5
Hydrogen Atom
In the following we shall investigate only the bound states of a particle with the
reduced mass m in an attractive Coulomb potential

362
4
Quantum Mechanics I
V (r) = −
e2
4πε0 r ,
and restrict ourselves therefore to negative energies—we have to consider the scat-
tering off a Coulomb potential (E > 0) separately, and we shall do this in Sect. 5.2.3.
The standard example of this potential is the hydrogen atom, but where the mag-
netic moment μB is neglected. If we introduce the charge number Z, we also have
the theory for hydrogen-like ions (He+, Li++, etc.). To some approximation, even
atoms with one outer electron can be treated. If the remaining core electrons can be
replaced by a point charge at the position of the nucleus, then the considered outer
electron is relatively far away from the core (it is said to be in a Rydberg state).
Then, according to Rydberg, a quantum defect δl can be introduced, and instead of
the principal quantum number n, we have the effective principal quantum number
n∗= n −δl.
The problem is centrally-symmetric. Hence, according to p. 353, the radial Schrö-
dinger equation
 d2
dr2 −l(l + 1)
r2
+ 2m
ℏ2

E +
e2
4πε0 r

unl(r) = 0 ,
with unl(0) = 0 ,
remains to be solved. We take the Bohr radius a0 and the Rydberg energy ER, which,
via the ﬁne structure constant (see p. 623)
α ≡
e2
4πε0
1
ℏc0
=
1
137.0 . . . ,
can be derived from the length unit ℏ/mc0 or the energy unit mc02, as becomes
understandable in the context of the (relativistic) Dirac equation (Sect. 5.6.9):
a0 ≡1
α
ℏ
mc0
,
ER ≡α2
2 mc0
2 =
ℏ2
2ma02 .
(We shall encounter the ﬁne structure constant in Sect.4.5.8 for the spin–orbit ﬁne-
splitting, which is where it gets its name. For hydrogen-like ions, it is Z times greater.)
We set
E = −ER/n2
and
r = n a0 ρ ,
where n will turn out to be the principal quantum number, and obtain the simpler
differential equation
 d2
dρ2 −l(l + 1)
ρ2
−1 + 2 n
ρ

unl(ρ) = 0 ,
with unl(0) = 0 .

4.5 Time-Independent Schrödinger Equation
363
We could already have used the following solution method for the one- and
three-dimensional oscillations. It is more cumbersome, but more generally appli-
cable than the methods mentioned so far. Hence I will introduce it here, even though
the Coulomb problem can also be solved with operators, which are related to Lenz’s
vector (see, e.g., [6]).
For large ρ, the differential equation takes the form u′′ −u = 0, with the two
linearly independent solutions exp(±ρ). Only the exponentially decreasing one is
normalizable. In contrast, for small ρ, according to p. 353, we have u ≈ρl+1. With
these boundary conditions for small and large ρ, we set
u(ρ) = ρl+1 exp(−ρ) F(ρ) ,
with F(ρ) =
nr

k=0
ck ρk .
For the still unknown function F, the differential equation for u implies
1
2
d2F
dρ2 + l + 1 −ρ
ρ
dF
dρ + n −l −1
ρ
F = 0 ,
and hence for the expansion coefﬁcients ck, the recursion formula
ck = −2
k
n −l −k
2l + 1 + k ck−1 .
The coefﬁcient c0 is not yet ﬁxed by the homogeneous differential equation. Its value
is determined from the normalization. But the solution is normalizable only if we
are dealing with a polynomial (with nr < ∞), hence if the recursion terminates,
otherwise we have in particular ck/ck−1 ≈2/k, which corresponds to the function
exp(2ρ), and despite the remaining factors, it is not normalizable. Hence not only
must the radial quantum number nr be a natural number, but so must the principal
quantum number
n = nr + l + 1
∈{1, 2, . . . } .
F is thus a polynomial of order nr, and the energy eigenvalues are (see Fig.4.16)
En = −ER
n2
with n ∈{1, 2, . . . } .
Except for the ground state, all states are degenerate—and not only like for the
centrally symmetric ﬁelds (where 2l+1 states have equal energy), but even more so.
A total of
n−1

l=0
(2l + 1) = n2

364
4
Quantum Mechanics I
Fig. 4.16 Energy eigenvalues and radial functions of the hydrogen atom. The ﬁgure shows the
potential, the ﬁrst (degenerate) eigenvalues, and the associated radial functions, for l = 0 (contin-
uous red lines), l = 1 (dashed blue lines), and l = 2 (continuous green lines)
Table 4.2 Multiplicity of Coulomb states. Note that all these states are to be counted twice because
of the spin
States nl
1s
2s-2p
3s-3p-3d
4s-4p-4d-4f
. . .
−En/ER
1
1/4
1/9
1/16
. . .
Multiplicity
1
4
9
16
. . .
different states belong to the energy En. In atomic physics, it is usual to give the
principal quantum number n and the orbital angular momentum, using the letters
indicated in Table4.2.
To determine the polynomials F, we use the variable s ≡2ρ = 2r/na0. Then the
differential equation reads
s d2F
ds2 + (2l + 2 −s) dF
ds + (n −l −1) F = 0 ,
the solution of which is the generalized Laguerre polynomial L(2l+1)
n−l−1(s) (see, e.g.,
[7]). Other functions also carry this name:
L(m)
n (s) ≡
1
n! s−m es dn(sn+m e−s)
dsn
=
n

k=0
n + m
n −k
	 (−s)k
k!
,
with the resulting eigenfunctions also shown in Fig.4.16. As for the Legendre poly-
nomials (p. 334) and the Hermite polynomials (p. 359), the ﬁrst equation is called
Rodrigues’ formula. It ﬁxes the polynomial by a correspondingly high derivative of
a given function. With the Leibniz formula

4.5 Time-Independent Schrödinger Equation
365
dn(f g)
dxn
=
n

k=0
n
k
	 dkf
dxk
dn−kg
dxn−k ,
the second expression follows from Rodrigues’ formula. It shows that it is indeed a
polynomial of n th order. Before we prove that the differential equation is satisﬁed, let
us also deal with the generating function of the generalized Laguerre polynomials:
1
(1 −t)m+1 exp −st
1 −t =
∞

n=0
L(m)
n (s) tn ,
for |t| < 1 .
It is easy to prove this. If we differentiate it with respect to s, then the left-hand side
leads to −t ∞
n=0 L(m+1)
n
(s) tn. Hence, comparing coefﬁcients, we ﬁnd
L(m+1)
n
= −dL(m)
n+1
ds
⇐⇒
L(m)
n (s) = (−)m dmLm+n
dsm
.
The generalized Laguerre polynomial L(m)
n (s) is thus equal to the m th derivative of
the Laguerre polynomials Ln+m(s) ≡L(0)
n+m(s), up to the factor (−1)m. In addition, the
equation for the generating function holds for s = 0, since it is L(m)
n (0) =
n+m
n

, and
this binomial coefﬁcient also occurs for the Taylor series expansion of (1 −t)−m−1
in powers of t (for |t| < 1), because for arbitrary p and natural number n, we have
p
n
	
≡p (p−1) · · · (p−n+1)
n!
= (−)n
n−p+1
n
	
.
Hence,
n+m
n

= (−)n−m−1
n

for p = n+m and the generating function is correct.
If we differentiate it with respect to t and compare the coefﬁcients, we obtain the
recursion formula
(n+1) L(m)
n+1(s) = (2n+m+1−s) L(m)
n (s) −(n+m) L(m)
n−1(s) .
Its derivative with respect to s delivers, along with the recursion formula,
L(m)
n (s)
=
L(m−1)
n
(s) +
L(m)
n−1(s) ,
(n+1)L(m)
n+1(s)
=
(n+1−s)L(m)
n (s)
+ (n+m)L(m−1)
n
(s) ,
sL(m+1)
n
(s) = (n+m+1)L(m)
n (s)
−
(n+1)L(m)
n+1(s) ,
sL(m+1)
n
(s) =
(n+m)L(m)
n−1(s)
+
(s−n)L(m)
n (s) ,
and the further recursion formula
s L(m+1)
n
(s) −(m+s) L(m)
n (s) + (n+m) L(m−1)
n
(s) = 0 ,

366
4
Quantum Mechanics I
as well as s L(m+1)
n−1 + (s−m) L(m)
n + (n+1) L(m−1)
n+1 = 0, which leads to the original
differential equation

s d2
ds2 + (m+1−s) d
ds + n

L(m)
n (s) = 0 .
For the normalization and the matrix elements of Rk, the following equation is impor-
tant:

 ∞
0
ds e−s sk L(m)
n (s) L(m′)
n′ (s) = (−)n+n′ 
l
k−m
n−l
	k−m′
n′−l
	 (k+l)!
l!
.
It can be derived from the generating function using
 ∞
0
ds e−s sk = k! and
−x
n

=
(−)nx+n−1
n

, which is necessary also for k < m or k < m′. In particular, the gener-
alized Laguerre polynomials with equal index m = m′ in the range 0 ≤x ≤∞form
an orthogonal system for the weight function exp(−s) sm:

 ∞
0
ds exp(−s) sm L(m)
n (s) L(m)
n′ (s) = (m + n)!
n!
δnn′ .
Correspondingly, in the range −∞≤x ≤∞, the Hermite polynomials form an
orthogonal system for the weight function exp(−s2).
Thus we may set
unl(r) = c ( 1
2s)l+1 exp(−1
2s) L(2l+1)
n−l−1(s) ,
with s ≡
2 r
n a0
,
with the still unknown normalization factor c, obtaining
⟨Rk⟩=

r2dr d |ψ|2 rk =

 ∞
0
dr |u|2 rk ,
according to p. 353 and p. 333 (or Problem 4.35). Hence,
|c|2 = 4l+1 (n−l−1)!/{n2a0 (n+l)!} ,
and for the ground state
u10(r) =
2
√a0
r
a0
exp −r
a0
,
and generally

4.5 Time-Independent Schrödinger Equation
367
Fig. 4.17 ⟨R⟩± R depends not only on the principal quantum number n, but also on the orbital
angular momentum l. Hence the error bars for the lowest l (=0) (red) and the highest l (= n−1)
(black) are shown, and the associated ⟨R⟩as a dot
⟨n, l | Rk |n, l⟩=
n a0
2
	k (n−l−1)!
2n (n+l)!

m
1+k
m
	2 (n+l+1+k−m)!
(n−l−1−m)!
.
In particular, we have a0⟨R−1⟩= n−2, and hence,
⟨V ⟩= −e2/(4πε0) ⟨R−1⟩= −2 ER n−2 = 2 En .
With En = ⟨T + V ⟩, we have ⟨T⟩= −1
2 ⟨V ⟩, which also delivers the virial theorem
(see p. 79) with a Coulomb ﬁeld for the time average. For the average distance
⟨R⟩, we ﬁnd 1
2 {3n2 −l (l + 1)} a0, and in particular, in the ground state, 3a0/2. The
most probable distance is given by the maximum of |u(r)|2. The states with radial
quantum number nr = 0 (and the highest angular momentum in the multiplet of equal
principal quantum number) each have only one—at n2 a0, in the ground state thus
for the Bohr radius—while the probability densities |u(r)|2 of the remaining states
have nr secondary maxima (see p. 367) (Fig. 4.17).
In Bohr’s atomic model, the centrifugal force cancels the Coulomb force between
the electron and nucleus, i.e., m v2/r = e2/(4πε0r2). Hence, T = −1
2 V and E =
1
2 V = −ER a0/r. Here, according to Bohr, not all distances r are allowed, because
the orbital angular momentum lz has to be a multiple of ℏ, i.e., mvr = nℏwith
n ∈{1, 2, . . . }. Consequently, according to Bohr’s atomic model, we have r = n2 a0
and En = −ER/n2. It delivers the same energy values as the Schrödinger equation.
However,inBohr’smodel,allstateshaveanorbitalangularmomentum nℏthatdiffers
from zero: s-states are not allowed, and n is not the principal, but the orbital angular
momentum quantum number. In addition, Bohr’s atomic model assumes a unique
orbital curve, and does not incorporate the position and momentum uncertainty.

368
4
Quantum Mechanics I
4.5.6
Time-Independent Perturbation Theory
If, for given H, we cannot solve the eigenvalue equation (H −En) |n⟩= 0, thus
cannot determine the eigenvalues En and the eigenvectors |n⟩, then an approximation
methodoftenhelps.Inparticular,ifH = H + V andtheeigenvaluesandeigenvectors
of H are known,
(H −En) |n⟩= 0 ,
with

n
|n⟩⟨n| = 1 and ⟨n|n′⟩= δnn′ ,
then we can expand the unknown eigenvector | . . .⟩of H for the eigenvalue E with
respect to this basis and also determine the matrix elements ⟨n| H −E |n′⟩. Using
⟨n| H −E | . . .⟩= ⟨n| H + V −E | . . .⟩= ⟨n|En + V −E | . . .⟩= 0, together with
| . . .⟩= 
n |n⟩⟨n| . . .⟩, we obtain the system of equations
⟨0|E0 + V −E |0⟩⟨0| . . .⟩+ ⟨0|
V
|1⟩⟨1| . . .⟩+ . . . = 0
⟨1|
V
|0⟩⟨0| . . .⟩+ ⟨1|E1 + V −E |1⟩⟨1| . . .⟩+ . . . = 0
...
...
...
...
Numerical calculations can be performed only for ﬁnite basis states |n⟩, thus only
approximately. If we take only two (thus a doublet), then we have already determined
the eigenvalues on p. 309:
E± = 1
2trH ± 1
2 ℏ ,
where now the average value is half of
trH = E0 + ⟨0| V |0⟩+ E1 + ⟨1| V |1⟩,
and the square of the splitting is
(ℏ)2 = (E0 + ⟨0| V |0⟩−E1 −⟨1| V |1⟩)2 + 4 |⟨0| V |1⟩|2 .
The two eigenvalues E± are always different for ⟨0| V |1⟩̸= 0. With coupling, there
is no degeneracy, but the effect of level repulsion (see p. 310). Note that, without this
coupling, the original eigenvalues En change by the expectation values ⟨n | V |n⟩to
En + ⟨n| V |n⟩. The expansion coefﬁcients ⟨n|±⟩have already been determined in
Sect.4.2.10.
For more than two basis states, the eigenvalue problem can be solved in pertur-
bation theory (or numerically, using the variational method explained in the next
section). Here we try to solve for (En −H) |n⟩= 0 with (En −H) |n⟩= 0. To deal
with degeneracy, we take a new basis: if the eigenvalue of H is, e.g., g-fold, then

4.5 Time-Independent Schrödinger Equation
369
only the g-dimensional problem (H −E) | . . .⟩= 0 has to be solved, as was just
discussed for g = 2.
To derive |n⟩, we avoid cumbersome normalization factors if we now require
⟨n|n⟩= 1. The normalization can be changed again right at the end. Then we have
⟨n| V |n⟩= ⟨n| H −H |n⟩= En −En, or
En = En + ⟨n| V |n⟩.
The matrix element follows from |n⟩⟨n| V |n⟩= |n⟩(En −En) = (En −H) |n⟩. Here
we use the mutually orthogonal projection operators
P ≡|n⟩⟨n|
and
Q ≡1 −P .
Hence, P V |n⟩= (En −H |n⟩and also P V = V −Q V = H −H −QV , and con-
sequently(En −H −Q V ) |n⟩= (En −H) |n⟩. If thereis nodegeneracy, thesingular
operator (En −H)−1 can act from the left, since with the projection operator Q, no
singular operator appears on the left. The state with H|n⟩= |n⟩En is missing and
hence the operator 1 −(En −H)−1Q V is regular, while the unit operator appears
on the right. Thus with the propagator

G(E) ≡
Q
E −H ,
we ﬁnd {1 −
G(En) V }|n⟩= |n⟩, and hence the representation
|n⟩= {1 −
G(En) V }−1 |n ⟩,
and the eigenvalue equation
En = En + ⟨n | V {1 −
G(En) V }−1 |n⟩.
This is the perturbation theory of Wigner and Brillouin. Unfortunately, in this result,
the unknown quantity En also occurs on the right and is not easy to determine. But
if we may expand in a geometrical series and the method converges fast enough, we
may replace 
G(En) by 
G(En) and can immediately give En:
En = En + ⟨n| V |n⟩+ ⟨n| V 
G(En) V |n⟩+ · · · .
The expansion is clearly good if the absolute values of the matrix elements of V are
small compared with the energy-level separations |En −En′|.
By the way, 
G(En) is encountered instead of 
G(En) in the perturbation theory of
SchrödingerandRayleigh.Withtheabbreviationn ≡En −En = ⟨n| V |n⟩,wehave

G(En) = {1 + 
G(En) n} 
G(En),
since
A−1(A −B)B−1 = B−1 −A−1
delivers
A−1 = {1 + A−1(B −A)}B−1 .

370
4
Quantum Mechanics I
Hence, 1 −
G(En) (V −n) factorizes in the form
{1 + 
G(En) n} {1 −
G(En) V } .
For the inverse of {1 −
G(En) V }, we may also write
{1 −
G(En) (V −n)}−1 {1 + 
G(En) n} ,
and so avoid 
G(En). Since Q |n ⟩vanishes, and hence also 
G(En) n |n⟩, we therefore
have
|n⟩= {1 −
G(En) (V −n)}−1 |n⟩.
With {1 −
G(En) (V −n)}−1 = 1 + {1 −
G(En) (V −n)}−1
G(En) (V −n),
this can be reformulated as
|n⟩=

1 + {1 −
G(En) (V −n)}−1 
G(En) V

|n⟩.
The propagator is now taken for the known energy, although n still contains the
unknown energy, so once again there is no explicit expression for it. But at least this
equation is easier to solve than the one from the perturbation theory of Wigner and
Brillouin. Then we obtain, to third order,
(1 + ⟨n|V 
G2 V |n⟩) n = ⟨n|V + V 
GV + V 
GV 
GV |n⟩,
and only encounter nonlinear equations for still higher orders. To second order, we
have the same result via both methods.
For 
GV ≪1, the quantum numbers mentioned in |n⟩are thus also approximately
valid for |n⟩. To next order, however, other states become mixed in. The eigenvalues
of operators which commute with H but not with H are no longer good quantum
numbers.
4.5.7
Variational Method
If the perturbation theory does not converge fast enough because no good approxi-
mation H is known, then a variational method sometimes helps. It delivers ﬁrst the
ground state and after that also the higher states, if there is no degeneracy.
Eacharbitraryapproximation|ψ⟩tothegroundstatewiththeenergyE0 deliversan
expectation value ⟨ψ|H|ψ⟩≥E0, since with the eigen representation {|n⟩} of H, we
have
⟨ψ|H|ψ⟩= 
n En |⟨n|ψ⟩|2 ≥E0,
with
E0 ≤E1 ≤. . .
and

n |⟨n|ψ⟩|2 = 1. Consequently, we can take any other basis {|n⟩}, and with |ψ⟩=
N
n |n⟩⟨n|ψ⟩satisfy

4.5 Time-Independent Schrödinger Equation
371
δ {⟨ψ| H |ψ⟩−E (⟨ψ|ψ⟩−1)} = 0 ,
where E is the Lagrange parameter introduced to deal with the normalization condi-
tion. In the framework of the ﬁnite basis {|n⟩}, it turns out to be the best approx-
imation to the ground state energy. The expansion coefﬁcients ⟨n|ψ⟩are to be
varied here. Since H is Hermitian, we can trace the variational method back to
⟨δψ| H −E |ψ⟩= 0. Note that this requirement for the matrix elements means that
(H −E) |ψ⟩must vanish, since ⟨δψ| is arbitrary. Naturally, the method leads more
quickly to a useful result the better the basis {|n⟩} already describes the actual ground
state with few states, but it should also be easy to determine ⟨n|H|n′⟩.
If, in the ﬁnite basis {|n⟩}, we ﬁnd the linear combination which minimizes
⟨ψ| H |ψ⟩with the additional condition ⟨ψ|ψ⟩= 1, then within this framework the
ground state |ψ0⟩and its energy are determined as well as possible. The proper ground
state may still have components orthogonal to the real one. The ﬁrst excited state
then follows with the same variational method and the further additional condition
⟨ψ|ψ0⟩= 0.
4.5.8
Level Splitting
For the coupling of a magnetic moment m of velocity v with a centrally symmetric
electric ﬁeld E = −∇	, the following expression was derived on p. 372:
V = 1
r
d	
dr
m · (r × v )
c02
.
If we use here the potential 	 = e/(4πε0r), Weber’s equation c−2
0
= ε0μ0, the mag-
netic moment m = −eS/m0 for the reduced mass m0 (see p. 327), and
r × v = l/m0 ,
thenaccordingtothecorrespondenceprinciple—thetransitiontoquantummechanics
with [L, R] = 0 is easy—we ﬁnd
V = μ0
4π
e2
m02
L · S
R3
= α2 ER
a03
R3
2 L · S
ℏ2
.
With the factor L · S, we speak of spin–orbit coupling. (This is stronger in nuclear
physics, and leads there, with a box potential, to the “magic nucleon numbers” of
the shell model.) The observables Lz and Sz are no longer sharp, but the total angular
momentum J = L + S is, as indeed are J 2 and Jz:
2L · S = 2LzSz + L+S−+ L−S+ = J 2 −L2 −S2 .

372
4
Quantum Mechanics I
Fig. 4.18 Fine splitting of
the ﬁrst excited state
multiplet of the hydrogen
atom. Left: Inclusion of the
spin–orbit coupling. Right:
The result of the Dirac
theory, with splitting due to a
magnetic ﬁeld of increasing
strength. The Landé factor is
2 for s 1
2 states, 2
3 for p 1
2
states, and 4
3 for p 3
2 states
We thus use the coupled basis {|(ls)jm⟩} from Sect.4.3.10 and ﬁnd
2L · S |(l 1
2)jm⟩= |(l 1
2)jm⟩ℏ2

l
for j = l + 1
2 ,
−l −1
for j = l −1
2 .
The degeneracies of the hydrogen levels are thus lifted (for l > 0) by the spin–orbit
coupling: ⟨V ⟩= α2 ER ⟨a 3
0 /R3⟩l for the 2l+2 states with j = l+ 1
2 and
⟨V ⟩= −α2 ER ⟨a 3
0 /R3⟩(l + 1) ,
for the 2l states with j = l −1/2. The average value of the scalar product L · S is
thus zero. This is a general sum rule.
According to this, the ﬁrst excited state of the hydrogen atom should split into
three. The 2s 1
2 state remains unaltered (as do all s-states), the 2p 3
2 state increases
by 1
24 α2ER, and the 2p 1
2 state is lowered by twice that value—the energies given in
Sect.4.5.5 are no longer valid to order α2ER. In fact, another ﬁne splitting is found
which follows only from the (relativistic) Dirac equation (Sect. 5.6.9). It leads to the
result
E = −ER
n2

1 + α2
n
 1
j+ 1
2
−
3
4n

+ · · ·
	
,
and shows that the previously found degeneracy is only partially lifted. It depends on
n and j, but not on l and m. The energy of 2p 3
2 is lower than −1
4ER by 1
64 α2ER and
that of 2p 1
2 and 2s 1
2 even by 5
64 α2ER. According to the Dirac equation, the average
value is also lowered, and the splitting amounts to 1
16 α2ER (see Fig.4.18).
Incidentally, according to p. 366, we ﬁnd for the hydrogen atom and n > l > 0,
!a03
R3
"
= 1
n3
2
l (l+1) (2l+1) .

4.5 Time-Independent Schrödinger Equation
373
According to this, the classical spin–orbit splitting differs by a factor of 2 from the
corresponding splitting due to Dirac.
Even though the spin–orbit coupling in atomic physics is clearly of the same
order of magnitude as other “intricacies”, it is suitable as an example application,
since in nuclear physics it is the spin–orbit coupling which leads to the magic nucleon
numbers, as mentioned above. In addition, these considerations support the following
chain of thoughts.
Thedirectionaldegeneracyisliftedbyamagneticﬁeldwhichwewouldnowliketo
consider in perturbation theory. According to p. 327, we should use the Pauli equation
for electrons. We neglect the term proportional to A2, which leads to diamagnetism,
a generally very small effect:
V = −q
2m0
B · (L + 2 S) .
If we quantize along the magnetic ﬁeld, then according to perturbation theory, Lz +
2 Sz = Jz + Sz is important for the state |(l 1
2)jm⟩. The ﬁrst term on the right has the
eigenvalue m ℏ, so only the expectation value of Sz in the state |(l 1
2)jm⟩is missing.
According to Sect.4.3.10, for this purpose, it follows that
⟨V ⟩= m g μB B ,
with the Landé factor
g = 2j + 1
2l + 1 ,
because in the uncoupled basis, we have Sz|l, ml; 1
2, ms⟩= msℏand the Clebsch–
Gordan coefﬁcients of p. 337 then deliver
⟨(l 1
2) l ± 1
2, m|Sz|(l 1
2) l ± 1
2, m⟩= ±m ℏ/(2l + 1) .
This result of perturbation theory is true only for small external magnetic ﬁelds, such
that higher-order terms can be neglected.
4.5.9
Summary: Time-Independent Schrödinger Equation
This Schrödinger equation is a second order differential equation for the unknown
wave function. For bound states, the solution must vanish at inﬁnity in order to
be normalizable, and only then can it deliver a probability amplitude. On account
of these boundary conditions, the time-independent Schrödinger equation becomes
an eigenvalue equation for the energy. For unbound states, there is no eigenvalue
condition: the energy can change continuously, and the improper Hilbert vectors
serve only as an expansion basis for wave packets.
Since, according to the uncertainty relation, position and momentum, and hence
also potential and kinetic energy, cannot be sharp simultaneously, there is a tunnel

374
4
Quantum Mechanics I
effect in quantum mechanics: for a given energy, there is a ﬁnite probability of ﬁnding
a particle in classically forbidden regions.
Particularly important examples of the application of the time-independent Schrö-
dinger equation are harmonic oscillations (their energy spectrum is equally spaced
above the zero-point energy) and the hydrogen atom, or more precisely, the Kepler
problem V (r) ∝r−1 (with countable energy eigenvalues En = −ER/n2 for bound
states and continuous eigenvalues E > 0 for scattering states). Free motion and piece-
wise constant potentials are even simpler to treat.
4.6
Dissipation and Quantum Theory
This section goes beyond the usual scope of a course entitled Quantum Mechanics
I and, apart from Sect.4.6.4 on Fermi’s golden rule, can be skipped or studied only
after the Chaps. 5 and 6.
4.6.1
Perturbation Theory
The Dirac picture is applied in particular to the coupling of atomic structures to their
macroscopic surroundings. Without this inﬂuence we would not be able to observe
atomic objects at all, since all detectors and measuring instruments belong to the
macroscopic environment. (Hence this section is indispensable for the theory of the
measurement process, although we shall not pursue this any further here.) We observe
only a few degrees of freedom, but we have to consider their coupling to the many
degrees of freedom of the environment. The difference between these two numbers
is essential for the following. Hence we shall use the abbreviations “m” and “f” (for
many and few) to indicate the two parts. Of course, it would be impossible to follow
the many “inner degrees of freedom” of a solid separately. They have to be treated
like those of the environment. At any given time, we observe only a few degrees of
freedom of the system.
Let us consider, e.g., an excited atom, which emits light. In the simplest case we
may consider the atom as a two-level system and the environment as the surrounding
electromagnetic ﬁeld. Even if it was initially particularly simple (without photons),
the light quantum (photon) can still be emitted from many different states, these
being distinguished, e.g., by the propagation direction, but also by the time of arrival
at the detector.
For these considerations, pure states alone are not enough. In particular, averaging
effects will enhance the degree of “impurity”, so we describe everything with den-
sity operators. For their time dependence, in the interaction picture (Dirac picture),
according to p. 346, we have

4.6 Dissipation and Quantum Theory
375
iℏdρD
dt
= [VD, ρD] ,
where the operators ρ and V act on both parts. But only the few degrees of freedom
of the open system are of interest, and hence also only the equation of motion for a
simpler reduced density operator will concern us, viz.,
ρf ≡trm ρD ,
since we consider only measurable quantities Of which do not depend on the many
degrees of freedom and hence are unit operators with respect to these degrees of
freedom:
⟨Of⟩= trvw ρDOf = trf ρfOf .
In particular, we shall derive an equation of motion for ρf from the expression for
˙ρD. The result will not be a von Neumann equation: open systems differ in principle
from closed systems.
Concerning the experimental conditions, we require that initially the “object” and
“environment” should be independent of each other (“uncorrelated”), so that initially
ρD factorizes into ρf and ρm ≡trfρD (more on the notion of correlation in Sect. 6.1.5.)
This initial condition is suggestive, because for each repetition of the experiment, we
produce the object as identically as possible, but the environment has far too many
possibilities of adjustment. Often we simply require that the coupling necessary for
the correlation should be turned on only at the beginning of the experiment—for the
discussion below, both requirements deliver the same result.
Using the product form, the number of independent density matrix parameters is
much reduced. If we pay attention only to ρ = ρ†, but not to trρ = 1, then an N ×N
matrix requires N 2 real parameters, but for the product form, instead of the (NmNf)2
parameters, only Nm2 + Nf2 are needed. Generally, for uncorrelated systems, we
have trρ2 = trρ2
1 · trρ2
2, otherwise this is not true—correlated systems form entangled
states. For example, the singlet state of two electrons in the spin space has trρ2 = 1,
but trρ2
i = 1
2.
If the parts are not coupled, then for all times, ρD could be split into the product
ρm ⊗ρf. But the interaction leads to a correlation. Hence we write
ρD = ρm ⊗ρf + ρk ,
with ρk(0) = 0 (and trρk(t) = 0, not 1) .
Then we obtain iℏ˙ρf = trm[VD, ρm ⊗ρf + ρk] and a corresponding expression for
iℏ˙ρm. The term trm[VD, ρm ⊗ρf] is equal to the commutator of trmVDρm ⊗1f with
ρf, where trmVDρm ⊗1f describes the average interaction of the environment with
the experimental object. It can be taken as a part of the free Hamilton operator Hf, and
correspondingly trfVD1m ⊗ρf for Hm. Then these terms for the interaction vanish,
and we ﬁnd

376
4
Quantum Mechanics I
iℏdρf
dt = trm[VD, ρk]
and
iℏdρm
dt
= trf[VD, ρk] .
Since ρk is of at least ﬁrst order in the interaction, the changes in ρf and ρm with time
in the Dirac picture are at least of second order in VD, and this can be exploited in
perturbation theory.
The correlation ρk changes by one order less:
iℏdρk
dt = [VD, ρm ⊗ρf] .
Here on the right, the expression [VD, ρk] −trf[VD, ρk] ⊗ρf −ρm ⊗trm[VD, ρk] is
left out, because it depends on a higher order of the coupling. Hence, with regard to
the initial value,
ρk(t) = −i
ℏ

 t
0
dt′ [VD(t′), ρm(t′) ⊗ρf(t′)] .
The ﬁnal result is thus a coupled system of integro-differential equations: ρk follows
from an integral of ρm and ρf and these quantities from differential equations which
depend on ρk. In particular, for the unknown ρf, we now have the equation
dρf
dt = −1
ℏ2 trm[VD(t),

 t
0
dt′ [VD(t′), ρm(t′) ⊗ρf(t′)]]
= −1
ℏ2

 t
0
dt′ trm[VD(t), VD(t′) ρm(t′) ⊗ρf(t′)] + h.c.
Here use was made of the fact that the operators are Hermitian. The double commu-
tator can then be reformulated into two simple commutators.
In order to further simplify the equation, we decompose the coupling VD into
factors which each act only on one of the two parts, although there are several such
products, and only their sum delivers the full coupling:
VD =

k
Ck
m ⊗V k
f .
Then, e.g., for a two-level system, a Vf may occur (even though VD is Hermitian,
the factors on the right-hand side may not be—there are further terms which ensure
VD = VD†) and both are interconnected with appropriate factors Cm. However, this
does not mean that each has only one creation and annihilation operator. In fact, each
factor Ck
m embraces a huge set of basis operators (modes) for the environment. But
since we are interested only in a few degrees of freedom and, when we form the
trace, we average over many degrees of freedom, the notation is rather useful. Here
for the time being we shall not ﬁx the normalization of the basis operators Ck
m, so
the V k
f will remain undetermined.

4.6 Dissipation and Quantum Theory
377
Hence, the integrand splits up into factors for the individual parts:
trm[VD(t), VD(t′)ρm(t′) ⊗ρf(t′)] =

kk′
trm{Ck
m(t)Ck′
m(t′)ρm(t′)} [V k
f (t), V k′
f (t′)ρf(t′)] .
Here the inﬂuence of the part with many degrees of freedom is contained in the
factors
gkk′(t, t′) = trm{Ck
m(t) Ck′
m(t′) ρm(t′)} .
If they are determined, then a decoupled integro-differential equation remains for
the unknown density operator ρf :
dρf
dt = 1
ℏ2

kk′
#
 t
0
gkk′(t, t′) V k′
f (t′) ρf(t′) dt′, V k
f (t)
$
+ h.c.
4.6.2
Coupling to the Environment
So far we have respected the two parts as equivalent terms in a weak coupling and
have not yet made use of the fact that they differ essentially by the number of degrees
of freedom. This difference allows us to estimate the weight functions gkk′ and to
simplify the integro-differential equation.
As discussed in Sect.4.4.4, ρ(t) = ρ(0) + (iℏ)−1  t
0[V (t′), ρ(t′)] dt′ solves the
initial equation iℏ˙ρ = [V, ρ], but since the unknown ρ(t) appears on the right, the
solution is not found yet. In a perturbation theory, we replace ρ(t′) in the integrand
by the initial value ρ(0) and then obtain at least an approximate solution. In the
given case, we do not need this approximation for ρf(t′); only for ρm(t′) will it be
necessary. In particular, it will turn out that gkk′(t, t′) puts the main weight on t′ ≈t
and hence the main weight of ρf is only for the time t.
Here we start from the fact that the environment is initially in equilibrium. Oth-
erwise we would also like to obtain the response of the considered object to new
environmental conditions, which is in fact also an important question, but will only
be investigated afterwards.
Without coupling of the two parts, the environment would remain in its initial state.
We now assume that there is no feedback: so the object perturbs its environment
(otherwise we could not investigate it at all), but not so strongly that it would be
noticed, otherwise we would have to ﬁx the boundary between the two differently.
Hence,
gkk′(t, t′) ≈trmCk
m(t) Ck′
m(t′) ρm(0) .

378
4
Quantum Mechanics I
The “recurrence time” expected for a given closed system depends on the feedback.
But with environmental conditions, we shall introduce a damping of the open system
which prohibits this feedback.
With Cm(t) = Um†(t) CmUm(t) and Um(t) Um†(t′) = Um(t −t′), and because
ρm(0) is stationary and hence commutes with Um(t′), it follows that
gkk′(t, t′) = trmCk
m(t −t′) Ck′
m(0) ρm(0) = gkk′(t −t′) .
Thus only the time difference is important for gkk′, and the energy representation is
therefore particularly useful:
gkk′(t −t′) =

nmnm′
⟨nm|Ck
m|nm′⟩⟨nm′|Ck′
m|nm⟩⟨nm|ρm|nm⟩exp i(Enm −Enm′)(t −t′)
ℏ
.
Here the many degrees of freedom are reduced to a nearly continuous eigenvalue
spectrum of the environmental energy E with the state density gm(E). We replace
the double sum by a double integral,
gkk′(t′′) =


dE dE′ gm(E) gm(E′) ⟨E′|Ck
m|E⟩⟨E|Ck′
m|E′⟩ρm(E′) exp i(E′ −E)t′′
ℏ
,
and now make the ansatz ρm(E′) = gm−1(E0) δ(E′ −E0). The factor gm−1(E0)
follows from the normalization condition

dE′ gm(E′) ρm(E′) = 1. (Actually, we
should start from a thermal distribution with a temperature T, but this is not impor-
tant here.) Hence, we obtain
gkk′(t′′) =

dE gm(E) ⟨E0|Ck
m|E⟩⟨E|Ck′
m|E0⟩exp i(E0 −E)t′′
ℏ
.
When forming the trace, we clearly require that an annihilation operator Ck
m always be
followed by its adjoint creation operator. Hence the product in front of the exponential
function is real (and non-negative). In the last equation of Sect.4.6.1, in the Hermitian
conjugate expression, where gkk′(t, t′) is actually to be replaced by gk′k(t′, t), we may
now also have gk′k∗(t −t′). If we rephrase k ↔k′ there, then we arrive at
dρf
dt = 1
ℏ2

kk′
#
 t
0
gkk′(t −t′) V k′
f (t′) ρf(t′) dt′, V k
f (t)
$
.
+
#
V k′
f (t),

 t
0
gkk′∗(t −t′) ρf(t′) V k
f (t′) dt′$
.
This integro-differential equation can still be simpliﬁed quite decisively using the
Markov approximation.

4.6 Dissipation and Quantum Theory
379
4.6.3
Markov Approximation
Since the environment has many different eigenfrequencies, gkk′ changes fast in
comparison to ρf. The “memory” of the environment is much shorter than that of
the atomic object. We therefore expect gkk′ to decrease rather fast towards zero
with increasing |t −t′|. Hence we take ρf(t′) in the integrand for t′ = t (Markov
approximation) and may then extract it from the integral, whereupon the integro-
differential equation becomes a simpler differential equation. The change in ρf at
time t then depends only on the simultaneous value of ρf and no longer on the earlier
values. Hence we introduce two dimensionless auxiliary quantities and if gkk′(t′′)
tends sufﬁciently fast towards zero, we may also integrate to inﬁnity:
Akk′
f
≡1
ℏ

 ∞
0
gkk′(t′′) Uf(t′′) V k′
f U †
f (t′′) dt′′ ,
¯Ak′k
f
≡1
ℏ

 ∞
0
gkk′∗(t′′) Uf(t′′) V k
f U †
f (t′′) dt′′ .
With Akk′
f (t) = U †
f (t) Akk′
f
Uf(t), we then obtain the differential equation
dρf
dt = 1
ℏ

kk′
{[Akk′
f (t) ρf(t), V k
f (t)] + [V k′
f (t), ρf(t) ¯Ak′k
f (t)]} ,
where the operators Akk′
f
and ¯Ak′k
f
still have to be investigated in more detail.
Hence, we assume that V k
f changes the energy of the state by δEk
f , and likewise
for ¯Ak′k
f , while Akk′
f
changes it by δEk′
f . If we average now over the fast processes and
pay attention only to the contributions of the slower parts, we have δEk′
f = −δEk
f .
For the excitation of an atom by a transverse electromagnetic wave, this procedure
is called the rotating-wave approximation, because these terms seem to be slowly
variable to an observer rotating along with the wave. In each of the two commutators,
there is a creation and an annihilation operator Vf. Hence, we have Akk′
f
= π akk′ V k′
f
and ¯Ak′k
f
= π akk′∗V k
f , using the common abbreviation
akk′ ≡1
π

dE gm(E) ⟨E0|Ck
f |E⟩⟨E|Ck′
m|E0⟩1
ℏ

 ∞
0
dt′′ exp i(E0 −E + δEk
f ) t′′
ℏ
.
The differential equation under consideration then simpliﬁes to
dρf
dt = π
ℏ

kk′
Reakk′ %
V k′
f (t) ρf(t), V k
f (t)
&
+
%
V k′
f (t), ρf(t) V k
f (t)
&
,
since Im akk′ is multiplied by [V k
f (t) V k′
f (t), ρf]. This commutator is not important in
the present discussion, because we shall not occupy ourselves with the determination
of Hf here; we would only obtain an amendment to the Hamilton operator, e.g., for

380
4
Quantum Mechanics I
the electromagnetic coupling of an atom to the surrounding vacuum, the famous
Lamb shift. According to Sect. 1.1.10, the real part of the integral over t′′ is equal to
ℏπ δ(E0 −E + δEk
f ):
Reakk′ = gm(E0 + δEk
f ) ⟨E0|Ck
m|E0 + δEk
f ⟩⟨E0 + δEk
f |Ck′
m|E0⟩.
Here, for appropriate normalization of the operators V k
f , we may take the factors
Ck
m and Ck′
m as Bose operators k and k′†. This is true for δEk
f > 0; for δEk
f < 0,
conversely Ck
m is to be replaced by k† and Ck′
m by k′. In the following we shall
write ±δE instead of δEk
f and assume δE ≥0.
If no degeneracy occurs, then k and k′ are uniquely related to each other, and
instead of the double sum, a single sum sufﬁces. Note that, for an isotropic environ-
ment, we have in fact the usual directional degeneracy, but trCk
m(t −t′) Ck′
m(0) ρm(0)
then also contributes only as a scalar, and this again relates k and k′ to each other
uniquely. For gm(E0 + δE) stands the factor kk†, for gm(E0 −δE) the factor
k†k = Nk. With [, †] = 1, the factor for gm(E0 + δE) is therefore greater by
one than that for gm(E0 −δE). In Sect. 6.5.7, it will be shown that, for thermal
radiation, we have nk = {exp(ℏωk/kBT) −1}−1, where the factor kB in front of the
temperature T is the Boltzmann constant, and the normalization volume V has the
state density gm(E) = V E2/(π2ℏ3c3). For the coupling to the vacuum (for sponta-
neous emission), we naturally work with nk = 0 (or T = 0) so that only the term with
gm(E0 + δE) appears, and not the term with gm(E0 −δE). Then there is only forced
absorption, described by Hf, but both forced and spontaneous emission. (For T > 0,
thereis alsospontaneous absorption.) Spontaneous processes arenot describedbyHf,
but by the dissipation discussed here. Taking all this together, if Hf is not degenerate,
we then obtain
dρ
dt = π
ℏ

k
{gk
+ [V k
−(t) ρ(t), V k
+(t)] + gk
−[V k
+(t) ρ(t), V k
−(t)]} + h.c.
Here we have left out the subscript f, because all operators now refer to the few
relevant degrees of freedom anyway. In addition, with δE appearing implicitly in k,
we have
gk
−= nk gm(E0 −δE)
and
gk
+ = (nk + 1) gm(E0 + δE) .
Note that, if there is no spontaneous absorption, then nk = 0 and hence also gk
−= 0.
The Hermitian conjugate of g±[V∓ρ, V±] is equal to g±[V∓, ρV±]. With
g±[V∓ρ, V±] + h.c. = g± (2V∓ρV± −{V±V∓, ρ}) ,
the equation of motion is often reformulated accordingly.
If we return to the Schrödinger picture (without including the subscript S), then
with time-independent operators Hf and V k
±, it follows that

4.6 Dissipation and Quantum Theory
381
dρ
dt = [Hf, ρ(t)]
iℏ
+ π
ℏ

k
'
gk
+ [V k
−ρ(t), V k
+] + gk
−[V k
+ ρ(t), V k
−] + h.c.
(
.
We shall apply this Liouville equation to different examples. It conserves the trace
of ρ, because dρ/dt can be expressed purely in terms of commutators, but not the
purity of the state, since we generally have
d
dt trρ2 = 2π
ℏ

k
{gk
+tr([V k
−ρ, V k
+] ρ) + gk
−tr([V k
+ρ, V k
−] ρ)} ,
which differs from zero. Hence for dissipation, there is also no unitary operator U(t)
with the property ρ(t) = U(t)ρ(0)U †(t). Incidentally, for real g± and V± = V∓†,
ρ ≥0 is also conserved, as was proven by Lindblad in 1976.
There may still be amendments g0[Vρ, V †] without energy exchange. These
destroy the phases of the density operator. For example, for a doublet, we thus have
ρ(t) = 1
2 (1 + σ · ⟨σ(t)⟩) and H = 1
2ℏσz, whence
dρ
dt = −i [σz, ρ]
2
+

γ+[σ−ρ, σ+] + γ−[σ+ρ, σ−] + γ0
[σzρ, σz]
4
+ h.c.

.
Here γ0 captures the coherence-destroying processes without energy exchange with
the environment, γ+ those with energy delivery to it, and γ−those with energy intake
from it, which is only possible for T > 0. Hence with
⟨σz⟩∞= −(γ+ −γ−)/(γ+ + γ−) ,
we ﬁnd
⟨σx + iσy⟩t = ⟨σx + iσy⟩0 exp(−it) exp{−(γ++γ−+γ0) t} ,
⟨σz⟩t = ⟨σz⟩∞+ {⟨σz⟩0 −⟨σz⟩∞} exp{−2(γ++γ−) t} .
For γ+/γ−≈(n+1)/n, we have ⟨σz⟩∞= −(2n + 1)−1, and thermal radiation
n = {exp(ℏ/kBT)−1}−1 ,
whence the Bloch vector for kBT ≪ℏ tends towards −ez and for kBT ≫ℏ to
zero (see Fig.4.19).
Incidentally, we often see the claim that the dissipation might be describable by
a non-Hermitian Hamilton operator H = R −iI, with R = R† and I = I†. Then,
Hnn′∗= Rn′n + iIn′n, and from iℏ˙ψn = 
n′ Hnn′ψn′, for ρnn′ = ψnψn′∗, the equation
˙ρ = −(i[R, ρ] + {I, ρ})/ℏwould follow. Here, in contrast to the previously derived
equation of motion, the trace of ρ would not be conserved. Thus the ansatz H =
R −iI could not be valid generally—at most for special states, e.g., in scattering

382
4
Quantum Mechanics I
Fig. 4.19 Spiral orbit of the Bloch vector with damping by an environment at temperature T = 0.
Without damping, according to p. 343, it proceeds on a circle with axis tr(σH). The damping leads
to a spiral orbit. Here γ++γ−= γ0 is assumed, so the orbit lies on a cone, unless it already starts
on the axis. Larger γ0 narrows the orbit towards the axis and perturbs the coherence even faster. For
T > 0, the attractor (open circle) lies higher, for kT ≫ℏ in the center
theory, we consider “decaying states” (see Sect. 5.2.5), the probabilities of which
decrease in the course of time.
For degeneracy of Hf, the situations are not quite as simple, since the index k
actually belongs to V k
f , while Ck
m embraces many modes, and now for k ̸= k′ in Ck
m
and Ck′
m, the same modes may occur, so we may no longer separate the factor δkk′
from gkk′
± . Of these, only the mutually degenerate states are captured—instead of the
term with k, many terms now occur, corresponding to the degree of degeneracy. We
shall discuss this problem in more detail in Sect.4.6.5.
4.6.4
Deriving the Rate Equation and Fermi’s Golden Rule
The Schrödinger and von Neumann equations lead to the time-development operator
U(t) = exp(−iHt/ℏ), and hence immediately after the beginning of the experiment
to U ≈1 −iHt/ℏ. If initially only the energy eigenstate |n0⟩is occupied, then the
occupation probabilities immediately after the beginning of the experiment do not
change linearly, but quadratically with the time, i.e., ⟨n|ρ|n⟩≈|⟨n|H|n0⟩t /ℏ|2 for
n ̸= n0. Actually, the occupation probability is expected initially (for small t) to
increase linearly—the quadratic dependence is so surprising that it is even referred
to as the quantum Zeno paradox.
But linear behavior follows immediately from the Liouville equation just derived,
since for the diagonal elements of the density operator in the energy representation
(Hf|n⟩= |n⟩En), it delivers the rate equation (occasionally also called the Pauli
equation, but which must not be confused with the non-relativistic approximation of
the Dirac equation mentioned on p. 327):

4.6 Dissipation and Quantum Theory
383
d⟨n|ρ|n⟩
dt
=

n′

Wnn′ ⟨n′|ρ|n′⟩−Wn′n ⟨n|ρ|n⟩

,
with the transition rate
Wnn′ ≡2π
ℏg± |⟨n|V∓|n′⟩|2 ,
for En ≶En′ ,
where the index k becomes ﬁxed by n and n′. Note that the transition rate is often
referred to as the transition probability, but is not normalized to 1. It gives the
average number of transitions in the time dt. As for operators, we shall write the
initial state after the ﬁnal state here, even though we are not strictly speaking dealing
with matrix elements in the usual sense. If we swap n′ and n, we obtain Wn′n =
2π g± |⟨n′|V∓|n⟩|2/ℏfor En′ ≶En, as is indeed required. In particular, we often also
use the abbreviation (without terms n′ = n)
n ≡ℏ

n′
Wn′n ,
which is already useful in the above-mentioned rate equation. We shall discuss such
rate equations in Sect. 6.2 and use them to prove in particular the entropy law for
“closed systems”, i.e., systems separated from their environment, but which also have
many internal degrees of freedom in addition to a few observable ones, and hence
according to Sect.4.6.1 ﬁt into the framework considered here. Energy is conserved
in such closed systems, so we may set gk
+ = gk
−and obtain Wnn′ = Wn′n.
Since the change in the atomic system is only relatively slow, this suggests using
the initial values on the right-hand side of the rate equation and then determining
the derivatives with respect to time, without ﬁrst integrating the coupled system
of equations. If initially we have the pure state |n0⟩, Fermi’s golden rule for the
determination of the transition rates follows for all states |n⟩̸= |n0⟩:
d⟨n|ρ|n⟩
dt
= Wnn0 = 2π
ℏg±|⟨n|V |n0⟩|2 ,
for En ≶En0 .
Since the rate equation conserves the trace of ρ, we now also infer
d⟨n0|ρ|n0⟩/dt = −

n
Wnn0 = −n0/ℏ,
initially, i.e., for t ≪ℏ/n0.
For the off-diagonal elements of ρ, the so-called coherences, as long as we leave
out the terms g0 (2VρV −V 2ρ −ρV 2) with energy-conserving V = V †, from the
general result of the last section, we obtain
d⟨n|ρ|n′⟩
dt
=
En −En′
iℏ
−n + n′
2ℏ

⟨n|ρ|n′⟩,
for n ̸= n′ .

384
4
Quantum Mechanics I
In particular, for En ≶En′, from
g+(2V−ρV+ −{V+V−, ρ}) + g−(2V+ρV−−{V−V+, ρ}) ,
only the part −g±ρV±V∓−g∓V∓V±ρ contributes, because the creation and anni-
hilation operators each connect only two states to each other—only the sum over k
comprises all different states. Addition of the term with the factor g0, viz.,
⟨n|2VρV −V 2ρ −ρV 2|n′⟩= −(⟨n|V |n⟩−⟨n′|V |n′⟩)2⟨n|ρ|n′⟩,
increases the damping in comparison with the expression we have kept here. In this
way, the differential equations decouple and lead to
⟨n|ρ(t)|n′⟩= ⟨n|ρ(0)|n′⟩exp

−
1
2 (n + n′) + i (En −En′)
ℏ
t

,
or even more strongly damped. The coherences thus decrease with time. The density
operator in the energy representation ﬁnally becomes diagonal, and occupation prob-
abilities also become classically understandable. This was discussed for doublets in
the last section, and shown in Fig.4.19. There we had W↓↑= 2γ+ and W↑↓= 2γ−.
4.6.5
Rate Equation for Degeneracy. Transitions Between
Multiplets
When we have degeneracy, we have to consider still further states. We shall denote
them with a bar, viz., |¯n⟩and |¯¯n⟩will be degenerate with |n⟩, and |¯n′⟩with |n′⟩. Instead
of the rate equation for the occupation probabilities, we have
d⟨n|ρ|¯n⟩
dt
=

n′ ¯n′
Wn¯nn′ ¯n′ ⟨n′|ρ|¯n′⟩−

¯¯n
¯¯nn ⟨¯¯n|ρ|¯n⟩+ ¯n¯¯n ⟨n|ρ|¯¯n⟩
2ℏ
,
with
Wn¯nn′ ¯n′ ≡2π
ℏg± ⟨n|V∓|n′⟩⟨¯n′|V±|¯n⟩
for En ≶En′ ,
and nn′ ≡ℏ
n′′ Wn′′n′′nn′. (When there was no degeneracy, we introduced Wnn′ =
Wnnn′n′ and n = nn.) In contrast, for the matrix elements of ρ between states of
different energy, it follows that
d⟨n|ρ|n′⟩
dt
= En −En′
iℏ
⟨n|ρ|n′⟩−

¯n ¯nn ⟨¯n|ρ|n′⟩+ 
¯n′ n′ ¯n′ ⟨n|ρ|¯n′⟩
2ℏ
.

4.6 Dissipation and Quantum Theory
385
Here the sum over ¯n also takes the value n, the sum over ¯n′ takes the value n′, and
above, the sum over ¯¯n also takes the values n and ¯n.
The directional degeneracy of the angular momentum multiplets delivers an
important example. Instead of |n⟩, here it is better to write |jm⟩. In the following, Ej is
the energy of the ground state and Ej′ is the energy of the excited state. If we restrict
ourselves to the coupling to the vacuum, with g−= 0 and g+ = gm(E0 + δE), then
we have
Wjmm′,j′m′′m′′′ = 2π
ℏg+ ⟨jm|V |j′m′′⟩⟨j′m′′′|V |jm′⟩,
in addition to Wj′m′′m′′′,jmm′ = 0 with g−= 0. The vacuum does not prefer any direc-
tion, and hence leads to a special selection between k and k′. The two interactions
couple only to a scalar. We restrict ourselves to radiation of multi-polarity n (usually
dipole radiation, i.e., n = 1, but in nuclear physics, higher multipole radiation also
occurs) and use the Wigner–Eckart theorem:
⟨jm| V (n)
ν
|j′m′⟩=
 j′ n
m′ ν

j
m
	 ⟨j∥V (n) ∥j′⟩
√2j + 1
.
This means that the directional dependence of the matrix elements is included via the
Clebsch–Gordan coefﬁcients. Then only one reduced matrix element ⟨j∥V (n) ∥j′⟩and
the factor (2j + 1)−1/2 remains, split-off in such a way that, for a Hermitian operator,
the symmetry |⟨j∥V (n) ∥j′⟩| = |⟨j′∥V (n) ∥j⟩| remains. The above-mentioned isotropy
delivers
ℏWjmm′,j′m′′m′′′
2π
= g+

ν
 j n
m ν

j′
m′′
	 j n
m′ ν

j′
m′′
	 |⟨j∥V (n) ∥j′⟩|2
2j′ + 1
,
and hence, using the orthogonality of the Clebsch–Gordan coefﬁcients,
j′m′′m′′′ = ℏ

m
Wjmm,j′m′′m′′′ = 2π g+
|⟨j∥V (n) ∥j′⟩|2
2j′ + 1
δm′′m′′′ .
We note that m′′ = m′′′ has to hold, whence j′m′′m′′′ here does not depend on the
directional quantum numbers. Hence we set
j′ ≡2π g+
|⟨j∥V (n) ∥j′⟩|2
2j′ + 1
,
and obtain, for the matrix elements of the density operator in the upper multiplet,
d⟨j′m|ρ|j′m′⟩
dt
= −j′
ℏ⟨j′m|ρ|j′m′⟩,

386
4
Quantum Mechanics I
for those in the lower multiplet,
d⟨jm|ρ|jm′⟩
dt
=

m′′m′′′
Wjmm′,j′m′′m′′′ , ⟨j′m′′|ρ|j′m′′′⟩,
and for the matrix elements between the two multiplets,
d⟨jm|ρ|j′m′⟩
dt
=
Ej −Ej′
iℏ
−j′
2ℏ

⟨jm|ρ|j′m′⟩.
Here the properties of the Clebsch–Gordan coefﬁcients lead to
Wjmm′,j′m′′m′′′ =
 j
n
m m′′−m

j′
m′′
	 j
n
m′ m′′′−m′

j′
m′′′
	 j′
ℏδm−m′,m′′−m′′′ ,
since all other terms vanish. Consequently, all sub-states of the excited levels decay
with the same time constants—and the amplitudes of the coherences ⟨jm|ρ|j′m′⟩
decrease exponentially with time, but only with half the time constants. If all sub-
states of the excited levels were initially occupied with the same probability and
those of the ground state were unoccupied, so that initially ⟨j′m′′|varrho|j′m′′′⟩=
δm′′m′′′/(2j′ + 1), it then follows that
d⟨jm|ρ|jm′⟩
dt
= j′
ℏ
δmm′
2j + 1 ,
if we make use of the properties of the Clebsch–Gordan coefﬁcients.
4.6.6
Damped Linear Harmonic Oscillations
An important example is provided by the oscillator coupled to its environment. It is
without degeneracy, but has only one creation and one annihilation operator between
its states—as long as we neglect multi-quantum processes for the damping (like 2,
but also † for V k
−). Hence the index k is superﬂuous, and we set V+ = v † and
V−= v  with [, †] = 1. The result of Sect.4.6.3 then takes the form
dρ
dt = −iω [†, ρ] + πv2 g+ [ρ, varPsi†] + g−[†ρ, ] + h. c.
ℏ
.
Note that expressions like [†ρ, †] + h.c. lead to pure phase damping, which
we shall not pursue here, and multi-quantum processes are still possible. Using the
abbreviations

4.6 Dissipation and Quantum Theory
387
γ = πv2 g+ −g−
ℏ
and
⟨†⟩∞=
g−
g+ −g−
,
we obtain the differential equations
d⟨†⟩
dt
= −2γ {⟨†⟩−⟨†⟩∞}
and
d⟨l⟩
dt
= −(lγ + ilω) ⟨l⟩,
which can be integrated easily:
⟨†⟩t = ⟨†⟩∞+ {⟨†⟩0 −⟨†⟩∞} exp(−2γ t) ,
⟨l⟩t = ⟨l⟩0 exp(−ilωt) exp(−lγ t) .
This result is similar to what we found for the two-level system (see Sect.4.6.3).
However, now ⟨†⟩∞= g−/(g+ −g−). With g+/g−≈(n + 1)/ n, the average
excitation energy approaches the value ℏω n, hence the average excitation energy
of the environment—for thermal radiation, we have n = {exp(ℏω/kBT) −1}−1, and
for the vacuum it is equal to zero.
Since X and P are linear combinations of  and †, for the damped harmonic
oscillation, ⟨X ⟩and ⟨P⟩decrease at the rate γ independently of the initial state, while
for stationary states the ﬁnal state is already reached at the outset (see p. 388):
⟨X ⟩
x0
+ i ⟨P⟩
p0
≡⟨⟩t = ⟨⟩0 exp(−iωt) exp(−γ t) .
Classically (in Sect. 2.3.7), for γ ≪ω, i.e., weak coupling to the environment, we
have the same result, as Ehrenfest’s theorem conﬁrms (Sect.4.4.1). But classically,
we do not have the uncertainties:
X
x0
2
+
P
p0
2
= ⟨†⟩∞+ 1
2 + {⟨†⟩0−⟨†⟩0⟨⟩0−⟨†⟩∞} exp(−2γ t) ,
X
x0
2
−
P
p0
2
= Re{(⟨2⟩0−⟨⟩02) exp(−2iωt)} exp(−2γ t) .
In the course of time, X /x0 and P/p0 take the same value, which is determined
solely by the environmental temperature and respects the limit X · P = 1
4x0p0 =
1
2ℏset by Heisenberg’s uncertainty relation.
In addition, the initial values of ⟨X ⟩, ⟨P⟩, X , and P clearly do not yet ﬁx
the uncertainties, since ⟨2⟩0 −⟨⟩02 is a complex number and therefore requires
further input (namely its rate of change, or the direction of the ellipse in Fig.4.20).
The example shown in Fig.4.20 comes from an initial “quench state”. This will
be discussed in Sect. 5.5.4. These are pure states and have X /x0 ̸= P/p0 (hence
the name), but the smallest possible uncertainty product X · P, i.e., 1
4x0p0 = 1
2ℏ.
There are of course states for which the product of these uncertainties is greater. For

388
4
Quantum Mechanics I
Fig. 4.20 Phase space representation of damped linear oscillations according to quantum theory—
with equal damping as in the classical case (see Fig. 2.21). Except for the values (indicated already
there) for ⟨X /x0⟩and ⟨P/p0⟩, the uncertainties X /x0 and P/p0 can still be read off here. They
remain ﬁnite, but always become more similar with time. The circle in the middle shows the ﬁnal
state. Of course, for the uncertainties, other initial conditions could be valid, as drawn here
Fig. 4.21 Time-dependence
of the excitation energy E∗
(dashed green curve) and its
uncertainty for the same
damped oscillations as in
Fig.4.20, here relative to the
initial energy E∗0.
Continuous blue curves
show (E∗± E∗)/E∗0 for
the initial state there, and
dotted red curves the same
for initially sharp energies
⟨l⟩, the above-mentioned phase damping leads to a factor exp(−l2γ0t) which also
affects the uncertainties X and P, but not the energy.
Figure4.21showsthetimedependenceoftheexcitationenergy⟨E∗⟩= ⟨†⟩ℏω
and its uncertainty. With (X /x0)2 + (P/p0)2 = ⟨† + 1
2⟩−⟨†⟩⟨⟩, the
energy is already ﬁxed with the initial values introduced so far, and its uncertainty
only by the further initial value ⟨(†)2⟩0, which for quench states can be deter-
mined using the normal-ordered characteristic function introduced in Sect. 5.5.6 (see
p. 481):

4.6 Dissipation and Quantum Theory
389
⟨(†)2⟩t = 1
2⟨†⟩t + {⟨(†)2⟩0 −1
2⟨†⟩0} exp(−4γ t) .
Thus it can also be zero initially, for ⟨(†)2⟩0 = (⟨†⟩0)2, but this dependence
of the initial uncertainty is rather quickly damped, as Fig.4.21 shows.
4.6.7
Summary: Dissipation and Quantum Theory
The coupling of an object to unobservable degrees of freedom induces dissipation.
The energy does not remain conserved. Classically, this is assigned to friction, which
is inaccessible to Hamiltonian mechanics. In quantum theory we also require exten-
sions which go beyond the von Neumann equation (and the Schrödinger equation).
Dirac’s perturbation theory helps quite a bit here, but further approximations (in
particular the Markov approximation) are necessary, until the expressions can be
evaluated.
These lead to Fermi’s golden rule among other things. The derivative of the occu-
pation probability of an energy state with respect to time, thus the transition rate
from the initial to the ﬁnal state, is equal to the square of the absolute value of its
coupling to the initial state times the state density of the relevant reservoir (for ﬁnite
temperatures, there is one further factor), except for a factor of 2π/ℏ.
But we have also found out how the coherences (the non-diagonal elements of the
density operators) depend on time. Their damping ensures decoherence: quantum-
physical phase effects vanish in favor of classically understandable occupation prob-
abilities. Decoherence leads to a collapse of the wave function. It is often overlooked
that we always deal with a statistical ensemble, and by selecting a special state, we
prepare the old state anew. Decoherence thus leads from quantum physics to classi-
cal physics, which is essential for each measurable process, since only then can we
arrive at classically realizable situations.
As important as these results are, there remain essential example applications for
further chapters (Quantum Mechanics II). We have not yet dealt with many-body
problems (where in particular the fact that the particles are indistinguishable has
noteworthy consequences), nor with scattering problems and relativistic effects.
Problems
Problem 4.1 Which probability amplitude ψ(x) ﬁts a Gauss distribution |ψ(x)|2
with x=0 and x̸=0? What does its Fourier transform
ψ(k) =
1
√
2π

 ∞
−∞
exp(−ikx) ψ(x) dx

390
4
Quantum Mechanics I
look like? Show that the factor 1/
√
2π here ensures
 ∞
−∞|ψ(k)|2 dk = 1. Determine
x · k for this example.
(6 P)
Problem 4.2 Given a slit of width of 2a, assume that the probability amplitude
ψ(x) = 1/
√
2a for |x| ≤a, otherwise zero. How large is x? Determine the Fourier
transform. Where are the maximum and the neighboring minima of |ψ(k)|2, and how
large are they? Show that the “interference pattern” |ψ(k)|2 becomes more extended
with decreasing slit width, but that the product x · k is problematic.
(6 P)
Problem 4.3 Consider the Lorentz distribution
|ψ(ω)|2 ∝1/{(ω −ω0)2 + (1
2γ )2}−1 .
How large is the uncertainty ω, and how large is its half-width, i.e., the distance
at which |ψ(ω)|2 has decayed to half the maximum value? Show that ψ(ω) is the
Fourier transform of ψ(t) ∝exp{−i(ω0 −i 1
2γ ) t} for t ≥0, zero for t < 0. Can we
describe decays with it? How large is the time uncertainty t?
(8 P)
Problem 4.4 The transition from the initial state |i⟩to the ﬁnal state |f ⟩should be
possible via any of the states |a⟩, |b⟩, and |c⟩. How large is the transition proba-
bility |⟨f |i⟩|2 if the states |a⟩and |b⟩may interfere, but |c⟩has to be superposed
incoherently?
(2 P)
Problem 4.5 Prove
 ∞
−∞f (x) δ(n)(x −x′) dx = (−)n f (n)(x′)
for
square-
integrable functions using integration by parts. Deduce from this that the equation
x δ′(x) = −δ(x) is true for the integrand. Prove δ(ax) =
1
|a| δ(x).
(6 P)
Problem 4.6 A series of functions {gn(x)} forms a complete orthonormal set in
the interval from a to b, if
 b
a gn∗(x)gn′(x) dx = δnn′ and f (x) = 
n gn(x)fn for all
(square-integrable) functions f (x). How can the expansion coefﬁcients fn be deter-
mined? Expand the delta-function δ(x −x′) (with x′ ∈[a, b]) with respect to this
basis. Does the sequence gn(x) = (2a)−1/2 exp(iπnx/a) form a complete orthonor-
mal system in the interval −a ≤x ≤a?
(6 P)
Problem 4.7 The system of Legendre polynomials Pn(x) is complete in the interval
|x| ≤1. The generating function is 1/
√
1 −2sx + s2 = ∞
n=0 Pn(x) sn for |s| < 1.
How does the associated orthonormal system read? Show that the Legendre polyno-
mials may also be represented by
Pn(x) = 1/{2n n!} dn(x2 −1)n/dxn
(Rodrigues’ formula).
(6 P)
Problem 4.8 The normalized state |ψ⟩= |α⟩a + |β⟩b is constructed from the
orthonormalized states |α⟩and |β⟩. What constraint do the coefﬁcients a ̸= 0 and

Problems
391
b ̸= 0 satisfy? How do they depend on |ψ⟩? Determine which of the following
normalized states |ψi⟩are physically equivalent to |ψ⟩(disregarding the phase
factor): |ψ1⟩= −|α⟩a −|β⟩b, |ψ2⟩= |α⟩a −|β⟩b, |ψ3⟩= |α⟩aeiϕ + |β⟩be−iϕ,
|ψ4⟩= |α⟩cos ϕ ± |β⟩sin ϕ.
(6 P)
Problem 4.9 Does the sequence of Hilbert space vectors
⎛
⎜⎜⎜⎝
1
0
0
...
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0
1
0
...
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0
0
1
...
⎞
⎟⎟⎟⎠, . . .
converge strongly, weakly, or not at all? If so, give the vector to which the sequence
converges.
(4 P)
Problem 4.10 Consider the function ψ(x) = x for −π ≤x ≤π. How does it read
as a Hilbert vector in the sequence space if we take the basis {gn(x)} of Problem 4.6
(with a = π)? How does the Hilbert vector in the function space read if it has the
components ψn = δn,1 + δn,−1 in this basis of the sequence space?
(4 P)
Problem 4.11 Are the functions f0(x) ∝1 and f1(x) ∝x orthogonal to each other
for−π ≤x ≤π?Determinetheirnormalizationfactors.Extendtheorthonormalized
basis {f0, f1} so that it is complete for all second-order functions f (x) = a0 + a1 x +
a2 x2 in −π ≤x ≤π.
(6 P)
Problem 4.12 Determine
[A, [B, C]±] + [B, [C, A]±] + [C, [A, B]±]
and
simplify the expression [C, [A, B]±]+ −[B, [C, A]±]+. Is
(A[B, C]± −[C, A]±B)D + C(A[B, D]± −[D, A]±B)
a simple commutator?
(6 P)
Problem 4.13 Let the unit operator 1 be decomposed into a projection operator P
and its complement Q, viz., 1 = P + Q. Is Q also idempotent? Are P and Q orthogonal
to each other, i.e., is tr(PQ) = 0 true? What are the eigenvalues of P and Q?
(4 P)
Problem 4.14 Is the inverse of a unitary operator also unitary? Is the product of two
unitary operators unitary? Is (1 −iA)(1 + iA)−1 unitary if A is Hermitian? Justify
all answers!
(4 P)
Problem 4.15 Suppose (A −a11)(A −a21) = 0 and let |ψ⟩be arbitrary, but not an
eigenvector of A. Show that (A −a1)|ψ⟩and (A −a2)|ψ⟩are eigenvectors of A, and
determine the eigenvalues. Determine the eigenvalues of the 2 × 2 matrix A with
elements Aik. If the matrix is Hermitian, show that no degeneracy can occur if the
matrix is not diagonal.
(6 P)

392
4
Quantum Mechanics I
Problem 4.16 Do orthogonal operators remain orthogonal under a unitary transfor-
mation?
(2 P)
Problem 4.17 Why is the determinant of the matrix elements of the operator A equal
to the product of its eigenvalues?
(4 P)
Problem 4.18 Let the vectors a and b commute with the Pauli operator σ. How
can (a · σ)(b · σ) then be expressed in the basis {1, σ}? What follows for (a · σ)2
and what for the anti-commutator {a · σ, b · σ}? Expand the unitary operator U =
exp(i a · σ) in terms of the basis {1, σ }.
(6 P)
Problem 4.19 The boson annihilation operator  is in fact not Hermitian and
therefore does not necessarily have real eigenvalues, but any complex number ψ
may be an eigenvalue of . Determine (up to the normalization factor) the associ-
ated eigenvector in the particle-number basis, and hence the coefﬁcients ⟨n|ψ⟩in
|ψ⟩= ∞
n=0 |n⟩⟨n|ψ⟩. Why is this not possible for the creation operator †? For
arbitrary complex numbers α and β, consider the scalar product ⟨α|β⟩and determine
the unknown normalization factor.
(8 P)
Problem 4.20 Show using the method of induction that
m † n
† n m

=

l
(±)lm! n!
l! (m −l)! (n −l)!
† n−l m−l ,
m−l † n−l .
(7 P)
Problem 4.21 Which 2 × 2 matrices correspond to the Pegg–Barnett operators
, †, and †± †, if the basis has only two eigenvalues (s = 1)? Do these
operators behave like ﬁeld operators for fermions?
(4 P)
Problem 4.22 From σxσy = iσz = −σyσx and σx2 = 1 (and cyclic permutations),
and also σ± = 1
2 (σx ± iσy), determine σzσ±, σ±σz, σ±σ∓and σ±2. What do we
obtain therefore for Uσ±U † with U = exp(iασz), according to the Hausdorff series?
Simplify the Hermitian operators σzσσz, σ±σσ∓, and σσ±σ∓+ σ±σ∓σ.
(9 P)
Problem 4.23 As is well known, the position and momentum coordinates of a par-
ticle span its phase space. Show that a classical linear oscillation with angular fre-
quency ω traces an ellipse in phase space, and determine its area as a function of
the energy. How large is the probability density for ﬁnding the oscillator at the dis-
placement x for oscillations with amplitudex, if all phase angles are initially equally
probable? (Here we thus consider a statistical ensemble.)
(6 P)
Problem 4.24 Since X · P ≥1
2ℏ, the phase-space cells may not be chosen arbi-
trarily small (more ﬁnely divided cells would be meaningless). How large is the area
if the energy increases by ℏω from cell to cell? Is it possible to associate particles at
rest with the cell of lowest energy, which would start oscillating only after gaining
energy? What is the mean value of the energy in this cell?
(4 P)

Problems
393
Problem 4.25 Show that the matrix ⟨ψ1| P |ψ2⟩=
 ∞
−∞ψ1∗(x) ℏ
i
d
dx ψ2(x) dx is
Hermitian. What can be concluded from this for the expectation values ⟨P⟩and
⟨P2⟩for a real wave function?
(6 P)
Problem 4.26 Derive the 2 × 2 density matrix of the spin states of unpolarized
electrons. Why is it not possible to represent it by a Hilbert vector?
(4 P)
Problem 4.27 Why does the quantum-mechanical expression 1
2 {f (X ) P+P f (X )}
correspond to the classical f (x) p according to the Weyl correspondence?
Hint: Use iℏf ′(X ) = [f (X ), P].
(6 P)
Problem 4.28 Justify the validity of the following quantum-mechanical expres-
sions—independent of the representation—with a homogeneous magnetic ﬁeld B
and Coulomb gauge: A = 1
2B × R, P · A + A · P = B · L, and P × A + A × P =
−iℏB.
(4 P)
Problem 4.29 In approximate calculations for motions with high orbital angular
momentum, we often replace ⟨L2⟩/ℏ2 by the square of a number (as if it were the
expectation value of L/ℏ). Which number is better than l? How large is the relative
error for l = 3 and l = 5?
(4 P)
Problem 4.30 Is it possible to express the Poisson bracket [l · e1, a · e2] in terms of
the triple product a · (e1 × e2) if a is the position or momentum vector?
(4 P)
Problem 4.31 Derive the uncertainties Lx and Ly for the state |l, m⟩. Hence,
determine also (Lx)2 + (Ly)2 + (Lz)2.
(2 P)
Problem 4.32 Does L commute with R2 and P2?
(2 P)
Problem 4.33 For classical vectors r and p, the equations
(r × p )2 = r2 p2 −(r · p )2 ,
p × (r × p ) = r p2 −p r · p ,
are valid. How do they read for the associated operators?
(4 P)
Problem 4.34 Derive all spherical harmonics for l = 0, 1, and 2.
(4 P)
Problem 4.35 Determine the integrals over all directions  of Y (l) ∗
m
(), Y (l′)
m′ (),
and Y (l)
m ().
Hint: Express the integrals initially with scalar products ⟨|lm⟩.
(2 P)
Problem 4.36 For spherically symmetric problems, the ansatz
ψnlm(r ) = r−1 unl(r) i l Y (l)
m ()

394
4
Quantum Mechanics I
turns out to be useful. Using this, reduce ⟨nlm| r cos θ |n′00⟩to a simple integral,
given that the integral over the directions is known.
Hint: r cos θ corresponds to R · ez in the position representation.
(4 P)
Problem 4.37 What do we obtain for ⟨nlm| (r cos θ)2 |n′00⟩and ⟨nlm| P · ez |n′00⟩
with the ansatz just mentioned?
(4 P)
Problem 4.38 The scalar product of two angular momentum operators J1 and J2
may be expressed in terms of J1z, J1± and J2z, J2±, viz.,
J1 · J2 = 1
2(J1+J2−+ J1−J2+) + J1zJ2z .
This helps for the uncoupled basis, but for the coupled basis, the total angular momen-
tum J = J1 + J2 should be used. Determine the matrix elements of the operator
σ1 · σ2 in the uncoupled basis {| 1
2m1, 1
2m2⟩} and in the coupled one {|( 1
2
1
2)sm⟩}.
How can we express the projection operators PS on the singlet and triplet states (with
S = 0 and S = 1, respectively) using σ1 · σ2?
(6 P)
Problem 4.39 Represent all d3/2 states |(2 1
2) 3
2m⟩in the uncoupled basis.
(4 P)
Problem 4.40 How many p states are there for a spin- 1
2 particle? Expand in terms
of the basis of the total angular momentum.
(4 P)
Problem 4.41 Which Ehrenfest equations are valid for the orbital angular momen-
tum? In particular, is the angular momentum a constant on average for a central
force?
(6 P)
Problem 4.42 Let ψ(r ) ≈f (θ) r−1 exp(ikr) hold for large r. How large is the
associated current density for large r?
(2 P)
Problem 4.43 How does the position uncertainty for the Gauss wave packet
ψ(k)= exp{−1
4(k)−2(k−k)2}/
4√
2π
√
k
depend upon time? In the ﬁnal result, use x(0) and v instead of k. Determine
x(t) for the case x(0) = 0.
(6 P)
Problem 4.44 Write down the Schrödinger equation for the two-body hydrogen
atom problem in center-of-mass and relative coordinates. Which (normalized) solu-
tion do we have in center-of-mass coordinates?
(4 P)
Problem 4.45 For the generalized Laguerre polynomials L(m)
n (s) and for |t| < 1,
there is a generating function (1 −t)−m−1 exp{−st/(1 −t)} = ∞
n=0 L(m)
n (s) tn. For
 ∞
0 e−ssk L(m)
n (s) L(m′)
n′ (s) ds, use this to derive the expansion

Problems
395
(−)n+n′ 
l
k −m
n −l
	k −m′
n′ −l
	
(k + l)!/l! .
It is needed for the expectation value ⟨Rk⟩of the hydrogen atom, viz., ⟨Rk⟩=
 ∞
0 |u|2 rk dr, with
unl(r) =

(n −l −1)!
a0 (n + l)!
sl+1
n
exp

−s
2

L(2l+1)
n−l−1(s)
and s ≡2r/(na0). How large is ⟨R⟩as a function of n, l, and a0?
(8 P)
List of Symbols
We stick closely to the recommendations of the International Union of Pure and
Applied Physics (IUPAP) and the Deutsches Institut für Normung (DIN). These
are listed in Symbole, Einheiten und Nomenklatur in der Physik (Physik-Verlag,
Weinheim 1980) and are marked here with an asterisk. However, one and the same
symbol may represent different quantities in different branches of physics. Therefore,
we have to divide the list of symbols into different parts (Table 4.3).
Table 4.3 Symbols used in Quantum Mechanics I
Symbol
Name
Page number
*
|ψ⟩
Ket-vector (state vector)
282
*
⟨ψ|
Bra-vector
283
*
⟨ϕ|ψ⟩
Scalar product,
Probability amplitude
282
*
⟨r |ψ⟩≡ψ(r )
Wave function (position
representation)
286, 320
*
⟨p |ψ⟩≡ψ(p )
Wave function (momentum
representation)
286
⟨n| A |n′⟩≡Ann′
Matrix element of the
operator A
290
*
⟨A⟩≡A
Expectation value of the
operator A
298
*
[A, B] ≡[A, B]−
Commutator of A and B
289
(continued)

396
4
Quantum Mechanics I
Table 4.3 (continued)
Symbol
Name
Page number
{A, B} ≡[A, B]+
Anti-commutator of A and B
289
*
A†
Hermitian adjoint of operator
A
292
*
A−1
Inverse of operator A
292
U
Unitary operator
(U† = U−1)
293

Annihilation operator
302
†
Creation operator
302
R
Position operator
318
P
Momentum operator
318
L
Orbital angular momentum
operator
328
S
Spin (angular momentum)
operator
335
*
σ
Pauli operator
308
H
Hamilton operator
339
P
Parity operator
313
T
Time-reversal operator
313
T
Time-ordering operator
346
ρ
Density operator
323
ρ(r, p )
Wigner function
322
Y (l)
m ()
Spherical harmonic
332
 l
s
ml ms

j
m
	
Clebsch–Gordan coefﬁcient
337
*
α
Fine structure constant
623
*
a0
Bohr radius
362
μB
Bohr magneton
327
References
1. W. Heisenberg, The Physical Principles of the Quantum Theory (Dover, 1930)
2. J. von Neumann, Mathematische Grundlagen der Quantentheorie (Springer, Berlin, 1968), p.
4
3. P. Güttinger, Z. Phys. 73, 169 (1931)
4. D.T. Pegg, S.M. Barnett, Europhys. Lett. 6(483) (1988). Phys. Rev. A 39(1665) (1989)
5. E.U. Condon, G.H. Shortley, The Theory of Atomic Spectra (Cambridge University Press, 1935)
6. O.L. deLange, R.E. Raab, Phys. Rev. A 34(1650) (1986)
7. M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions (Dover, New York, 1964)
8. E. Stiefel, A. Fässler, Group Theoretical Methods and Their Applications (Birkhäuser–Springer
(, Heidelberg, 1992)

References
397
Suggestions for Textbooks and Further Reading
9. C. Cohen-Tannoudji, B. Diu, F. Laloè, Quantum Mechanics 1–2 (Wiley, New York, 1977)
10. R. Dick, Advanced Quantum Mechanics: Materials and Photons (Springer, New York, 2012)
11. P.A.M. Dirac: The Principles of Quantum Mechanics (Clarendon, Oxford)
12. A.S. Green: Quantum Mechanics in Algebraic Representation (Springer, Berlin)
13. W. Greiner, Quantum Mechanics—An Introduction (Springer, New York, 2001)
14. G. Ludwig, Foundations of Quantum Mechanics (Springer, New York, 1985)
15. L.D. Landau, E.M. Lifshitz: Course of Theoretical Physics Vol. 3—Quantum Mechanics, Non-
Relativistic Theory 3rd edn. (Pergamon, Oxford, London, 1977)
16. A. Messiah: Quantum Mechanics I–II (North-Holland, Amsterdam, 1961–1962)
17. C. Itzykson, J. Zuber, Quantum Field Theory (McGraw-Hill, New York, 1980)
18. D. Jackson, Mathematics for Quantum Mechanics (Benjamin, New York, 1962)
19. J.M. Jauch, F. Rohrlich, The Theory of Photons and Electrons. The Relativistic Quantum Field
Theory of Charged Particles with Spin One-half (Springer, Berlin, 1976)
20. W. Nolting, Theoretical Physics 6—Quantum Mechanics—Basics (Springer, Berlin, 2017)
21. W. Nolting, Theoretical Physics 7—Quantum Mechanics—Methods and Approximations
(Springer, Berlin, 2017)
22. P. Roman: Advanced Quantum Theory (Addison-Wesley, Reading)
23. J.J. Sakurai, Advanced Quantum Mechanics (Addison-Wesley, Reading MA, 1967)
24. J.J. Sakurai, J. Napolitano, Modern Quantum Mechanics, 2nd edn. (Addison-Wesley, Boston,
2011)
25. F. Scheck, Quantum Physics, 2nd edn. (Springer, Berlin, 2013)
26. F. Schwabl: Quantum Mechanics (Springer, Berlin)

Chapter 5
Quantum Mechanics II
5.1
Scattering Theory
5.1.1
Introduction
In simple descriptions of the scattering process, where a sharp energy is assumed and
the time factor exp(−iωt) subsequently left out, the obvious result of this chapter can
be stated immediately: if a plane wave exp(ik · r ) falls on a scattering center, then the
original wave and the outgoing spherical wave f (θ) exp(ikr)/r become superposed,
and then the scattering amplitude f (θ) is of decisive importance. Here the center-
of-mass system is assumed, and the reduced mass m0 and kinetic energy E = ℏω =
(ℏk)2/2m0 are given. As will be shown in the following, for large distances r from
the scattering center, we have (see Fig. 5.1)
⟨r |k ⟩+ ≈
1
√
2π
3

exp(ik · r ) + f (θ)exp(ikr)
r

.
Here the scattering amplitude f (θ) is connected to the scattering operator S and the
transition operator T, these being the important quantities in scattering theory. From
the scattering amplitude, we can obtain, e.g., the differential scattering cross-section
for the scattering angle θ (as derived on p. 418)
dσ
d = |f (θ)|2 .
With these expressions we can already solve the simplest scattering problems.
To this end, we decompose the plane wave exp(ik · r ) in terms of spherical waves:
exp(ik · r ) = 4π
kr

lm
Fl(kr) Y (l) ∗
m (k) i l Y (l)
m (r) .
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5_5
399

400
5
Quantum Mechanics II
Fig. 5.1 Scattering with scattering angle θ (angle of deﬂection) and collision parameter s (see Fig.
2.6). If s is too large, there is no scattering force
In order to prove this equation, with ρ ≡kr, we start from exp(iρ cos θ). We expand
this in terms of Legendre polynomials. According to p. 82 (or Problem 4.7), they
form an orthogonal system, normalized to (l+ 1
2)−1/2, in the variables cos θ:
exp(iρ cos θ) =

l
2l + 1
ρ
Fl(ρ) i lPl(cos θ) ,
with the regular spherical Bessel function (see Fig. 5.2 top)
Fl(ρ) = ρ
2 il
 1
−1
d cos θ Pl(cos θ) exp(iρ cos θ) .
Note that this name usually refers to jl(ρ) ≡ρ−1Fl(ρ) = √π/(2ρ)Jl+1/2(ρ), but for
the expansion in terms of spherical harmonics in Sect. 4.5.2, we always wanted to take
out a factor 1/r from the radial function, and Fl(ρ) actually has more comfortable
properties. In particular, F0(ρ) = sin ρ and F1(ρ) = ρ−1 sin ρ −cos ρ (since P0 = 1
and P1 = cos θ), and the higher Bessel functions result from the recursion relation
Fl+1(ρ) = 2l + 1
ρ
Fl(ρ) −Fl−1(ρ) ,
which can themselves be derived from the recursion relations for Legendre poly-
nomials (see p. 82). For the rest of the proof, we still have to expand the Legendre
polynomials in terms of spherical harmonics:
Pl(cos θ) =
4π
2l + 1
l
m=−l
Y (l) ∗
m (k) Y (l)
m (r) .
For the proof this addition theorem for spherical harmonics, we rotate by a rotational
vector ω. We thus have Y (l)
m (′) = 
m′ Y (l)
m′ () D(l) ∗
m′m (ω) and the rotation operator D
is unitary: 
m D(l)
m′m(ω) D(l) ∗
m′′m(ω) = δm′m′′. If we now choose one of the two direc-
tions k or r as the new z-direction and use Sect. 4.3.9, and in particular, the
equations Y (l)
m (0, 0) = √(2l + 1)/4π δm0 and Y (l)
0 () = √(2l + 1)/4π Pl(cos θ),
then the addition theorem is proven.

5.1 Scattering Theory
401
Fig. 5.2 Spherical Bessel
functions with l from 0
(black) to 3 (blue)
(continuous for l even, dotted
for l odd). Top: regular Fl.
Bottom: irregular Gl. In
addition to these spherical
functions, there are also the
normal (cylindrical) Bessel
functions (see Fig. 5.17)
For the regular spherical Bessel functions Fl(ρ), we have asymptotically
Fl(ρ) ≈
 ρl+1/(2l+1)!!
forρ ≈0 ,
sin(ρ −1
2lπ)
for ρ ≫l(l+1) ,
where the double factorial (2l + 1)!! is the product of all odd integers up to 2l +
1, viz., (2l + 1)!! = l
n=0(2n + 1) = (2l + 1)!/2l l! Then, with ⟨k′|k ⟩= (2π)−3
	
d3r exp{i(k −k′) · r }, we have
	 ∞
0 dr Fl(kr) Fl(k′r) = 1
2π δ(k −k′). In addi-
tion, it solves the differential equation
 d2
dρ2 + 1 −l(l+1)
ρ2

Fl(ρ) = 0 ,
as do the remaining spherical Bessel functions, i.e., the irregular Bessel functions
(Neumann functions) (see Fig. 5.2 bottom)
Gl(ρ) ≈
(2l−1)!! ρ−l ,
for ρ ≈0 and l > 0 (cos ρforl = 0) ,
cos(ρ −1
2lπ) ,
for ρ ≫l(l+1) ,
the outgoing Bessel function (Hankel function)
Ol(ρ) ≡Gl(ρ) + iFl(ρ) ≈exp{i(ρ −1
2lπ)} ,
for ρ ≫l(l+1) ,
and the incoming Bessel function (Hankel function)
Il(ρ) ≡Gl(ρ) −iFl(ρ) = O∗
l (ρ) .

402
5
Quantum Mechanics II
These functions are solutions of the radial Schrödinger equation
 ∂2
∂r2 + k2 −l(l+1)
r2
−2m0
ℏ2
V (r)

ul(k, r) = 0 ,
for large r, because there V (r) will be negligibly small compared to E > 0:
ul(k, r) ≈Nl {Fl(kr) −π Tl Ol(kr)} .
Here, we shall actually superpose a plane wave and an outgoing spherical wave.
Starting from the boundary condition ul(k, 0) = 0, which is necessary according to
p. 353, so that the wave function is differentiable at the origin, and with a convenient
slope at the origin which just ﬁxes an inessential factor, we can integrate the differen-
tial equation up to the point where the above-mentioned splitting in terms of Bessel
functions occurs. Since this is also possible for the ﬁrst derivative with respect to r,
noting that the normalization factor Nl cancels, the unknown transition amplitude is
given by
Tl = 1
π
W(ul, Fl)
W(ul, Ol) ,
with the Wronski determinant
W(ul, Fl) = ul
∂Fl
∂r
−∂ul
∂r Fl
and
W(ul, Ol) = ul
∂Ol
∂r
−∂ul
∂r Ol .
With the normalization factor Nl = √2/π/k of ul in
⟨r |k ⟩+ =

lm
ul(k, r)
r
Y (l) ∗
m (k) il Y (l)
m (r) ,
the asymptotic expression for ⟨r |k ⟩+ with
Ol(kr) ≈i−l exp(ikr)
yields the scattering amplitude
f (θ) = −π
k

l
(2l + 1) Tl Pl(cos θ) ,
and we can derive the scattering cross-section from this. Note that, for low energies,
only a few terms contribute to this series. With increasing l the centrifugal potential
always dominates the remaining V (r), whence ul →Fl, and along with it Tl →0.
Having made this introduction with its prescriptions for proper calculations, we
shall now proceed to investigate the scattering process in more detail.

5.1 Scattering Theory
403
5.1.2
Basics
In order to clarify the basic notions of scattering theory, we restrict ourselves initially
to elastic two-body scattering and investigate only the change in the motion due to
the forces between the two scattering partners. Since the interaction V depends only
on the relative distance (and possibly also on the spin) of the scattering partners, it
is thus translation invariant, and we can disregard the center-of-mass motion. The
centre of mass moves unperturbed, with ﬁxed momentum. Therefore, we consider
only the relative motion and use the reduced mass m0—keeping m for the directional
(magnetic) quantum number.
As already for classical collisions (Sect. 2.2.3), we assume that the partners before
and after the scattering move unperturbed. The coupling V is assumed to have a
ﬁnite range, i.e., it should decrease more rapidly than r−1. The Coulomb force is an
exception, which we consider separately in Sect. 5.2.3. The ray is usually directed
toward an uncharged probe, and then there is no Coulomb ﬁeld, but it is nevertheless
important in nuclear physics, because the screening action of the atomic shell may be
neglected there, and only the interaction between the nuclei counts. But in the present
discussion, we shall assume that the scattering partners act on each other only for a
comparably short while—before and after, they are outside the range of the forces
and move unperturbed. Each scattering is a time-dependent situation. Therefore the
unperturbed motion must not be described by a plane wave, since this would be
equally probable in the whole space, and there would be no “before” and no “after”.
Instead we have to take wave packets. This we shall do rather superﬁcially, in the
sense that we shall not provide the exact form of the wave packet. We shall then be
able to work out basic notions of time-dependent scattering theory. The next step
will be to go over to time-independent scattering theory (with sharp energy) using a
Fourier transform, whereupon the calculations become rather simple.
The Schrödinger equation is normally taken as the most important starting equa-
tion in any introduction to quantum theory. This is suitable for bound states, because
their wave functions are essentially already determined by this differential equation.
The boundary conditions are still missing, of course, but these are self-evident for
bound states with the required normalizability and lead to the well-known eigenvalue
problem. In contrast to the situation for unbound states (scattering states), where the
boundary conditions still play an important role in determining the solution, only
the asymptotic behavior is signiﬁcant for many applications. Therefore, we shall
struggle with an integral equation which contains the Hamilton operator as well as
the boundary conditions, and then of course use the Lippmann–Schwinger equation
to solve that.
5.1.3
Time Shift Operators in Perturbation Theory
In the Schrödinger picture the development of a state with time t can be given by the
unitary time shift operator U(t, t0):

404
5
Quantum Mechanics II
|ψ(t)⟩= U(t, t0) |ψ(t0)⟩,
with U(t, t) = 1 ,
and thus U(t, t0) = U −1(t0, t) = U †(t0, t). Here, according to the Schrödinger equa-
tion,
iℏ∂
∂t U(t, t0) = H U(t, t0)
=⇒
U(t, t0) = exp −iH (t −t0)
ℏ
,
provided that the Hamilton operator H does not depend on time, which we assume.
The time shift operator by itself is not enough for the description of the scattering
problem. Initial conditions have to be added. But these refer to states in which there
are no forces acting between the scattering partners, so not all of the Hamilton
operator H is important, only the free (unperturbed) Hamilton operator H0:
H = H0 + V .
We indicate, e.g., the initial state by the relative momentum p with a suitable distri-
bution function for a wave packet. It remains unaltered only until the interaction V
between the scattering partners becomes notable:
[H, P] ̸= 0 ,
but [H0, P] = 0 .
The above-mentioned Hamilton operators do not depend on time, only their effects
on the states do.
In addition to the full Hamilton operator H and time shift operator U(t, t0), it is
therefore appropriate to consider also the free operator H0 or again U0(t, t0), and to
employ the Dirac picture. According to p. 346, we have U(t, t0) = U0(t, t0) UD(t, t0)
and
UD(t, t0) = 1 +
 t
t0
dt′ VD(t′, t0) UD(t′, t0)
iℏ
,
with VD(t′, t0) = U0†(t′, t0) V U0(t′, t0). Here U0(t, t0) can be decomposed into
U0(t, t′)U0(t′, t0), and U0 is unitary, with U0(t, t0) U0†(t′, t0) = U0(t, t′). From this
follows the important equation
U(t, t0) = U0(t, t0) +
 t
t0
dt′ U0(t, t′) V U(t′, t0)
iℏ
,
which can be derived from
iℏ∂
∂t′ {U0(t, t′) U(t′, t0)} = U0(t, t′) {−H0 + H} U(t′, t0)
by integrating over t′ from t0 to t. With

5.1 Scattering Theory
405
iℏ∂
∂t′ {U(t, t′) U0(t′, t0)} = U(t, t′) {−H + H0} U0(t′, t0) ,
we clearly have the equally important result
U(t, t0) = U0(t, t0) +
 t
t0
dt′ U(t, t′) V U0(t′, t0)
iℏ
.
These two “important” equations form the basis for all that follows. Since |ψ(t0)⟩has
to be given by the initial conditions, everything worth knowing about the scattering
power of the interaction is contained in U(t, t0). Note that U0 is known here, but the
question remains as to how V affects U.
For stepwise integration, the two forms deliver the same Neumann series
U(t, t0) = U0(t, t0) +
 t
t0
dt′ U0(t, t′) V U0(t′, t0)
iℏ
+
 t
t0
dt′
 t′
t0
dt′′ U0(t, t′) V U0(t′, t′′) V U0(t′′, t0)
(iℏ)2
+ · · · .
It represents the time shift operator U(t, t0) of the full problem as a sum of time
shift operators which feel the potential only at the times t′, t′′, etc., between t0 and
t and are otherwise determined by H0, i.e., they are “free” (unperturbed). With the
nth term, n interactions occur. If V changes the motion only a little, then this series
converges fast. In the Born approximation, we terminate after the ﬁrst term (with one
V ). This is often a good approximation, but certainly not for resonances.
5.1.4
Time-Dependent Green Functions (Propagators)
We search for the time shift operators for long time spans, because we want to connect
the initial and ﬁnal states. We shall not be concerned with intermediate states that
cannot be measured. Therefore, we now set t0 = 0 and investigate the behavior for
t →±∞. For these convergence investigations it is better to consider the distant
past (t →−∞) and the far future (t →+∞) separately.
Using the step function ε(x) from p. 18 (see Fig. 5.3), whose derivative is the delta
function, the following quantities are introduced:
Fig. 5.3 The discontinuity
functions ε(t) (left) and
ε(−t) (right). Since we have
ε(−t) = 1 −ε(t), −ε(−t)
has the same derivative as
ε(t), namely δ(t)

406
5
Quantum Mechanics II
G±(t) ≡ε(±t) U(t, 0)
± iℏ
and
G±
0 (t) ≡ε(±t) U0(t, 0)
± iℏ
.
They satisfy the differential equations

iℏd
dt −H

G±(t) = δ(t) ,
or

iℏd
dt −H0

G±
0 (t) = δ(t) ,
and are therefore called Green functions, since Green functions always solve linear
differential equations which have a delta function as the inhomogeneous term. We
have encountered other examples of Green functions on pp. 27, 112, and 119. In
fact, we are actually dealing with operators, often also called propagators. Clearly,
the functions carrying a “+” are unequal to zero only for t > 0 and those carrying
a “−” only for t < 0. Hence we speak of the retarded (+) and advanced (−) Green
functions (propagators). We have
for t ≷0 ,
U(t, 0) = ± iℏG±(t) ,
U0(t, 0) = ± iℏG±
0 (t) ,
and use the integral equations of the last sections to derive similar ones for the Green
functions:
G±(t) = G±
0 (t) +
 ∞
−∞
dt′ G±
0 (t −t′) V G±(t′)
= G±
0 (t) +
 ∞
−∞
dt′ G±(t −t′) V G±
0 (t′) .
For G+, the integrand vanishes outside 0 ≤t′ ≤t, and for G−, outside t ≤t′ ≤0.
With the higher integration limits, we may combine the equations for the retarded
and advanced Green functions and obtain integral equations of the Volterra type.
Here we ﬁnd convolution integrals. According to p. 22, we can transform them into
products using a Fourier transform and then evaluate the unknown G± from G±
0 and
V algebraically.
5.1.5
Energy-Dependent Green Functions (Propagators) and
Resolvents
Fourier transforming the integral equations of the time-dependent propagators
G±(E) ≡
 ∞
−∞
dt exp iEt
ℏ
G±(t) ,
and keeping the factor
√
2π, we obtain theLippmann–Schwinger equations

5.1 Scattering Theory
407
G±(E) = G±
0 (E) + G±
0 (E) V G±(E)
= G±
0 (E) + G±(E) V G±
0 (E) ,
since, with τ = t −t′,
G±(E) = G±
0 (E) +
 ∞
−∞
dt
 ∞
−∞
dt′ exp iEt
ℏ
G±
0 (t −t′) V G±(t′)
= G±
0 (E) +
 ∞
−∞
dτ exp iEτ
ℏ
G±
0 (τ) V G±(E) .
These equations can be solved formally:
G±(E) =
1
1 −G±
0 (E)V G±
0 (E) = G±
0 (E)
1
1 −V G±
0 (E) .
We often write the right-hand side as a Neumann series, viz.,
G±(E) = G±
0 (E) + G±
0 (E) V G±
0 (E) + · · · ,
and here possibly neglect the higher order terms (Born approximation).
However, before evaluating G±(E), we must ﬁrst determine the simpler prop-
agator G±
0 (E) of the free motion and here determine the Fourier integral. With
G±
0 (t) = (± iℏ)−1 ε(±t) U0(t, 0) and U0(t, 0) = exp(−iH0t/ℏ), we also have
G±
0 (E) = 1
iℏ
 ±∞
0
dt exp i (E−H0) t
ℏ
=
1
± iℏ
 ∞
0
dt exp ± i (E−H0) t
ℏ
,
where we may use an eigenvalue E0 of H0 in the energy representation. We have
already investigated these integrals on p. 22 in the context of distributions, and found
there
 ∞
0
dk exp(± ikx) =
± i
x ± io = ± i

P
x ∓iπ δ(x)

,
where(x ± io)−1 indicatesthelimitingvalueε →+0of(x ± iε)−1 andP(Cauchy’s)
principal value:
∞

−∞
dx P
x f (x) ≡P
∞

−∞
dx f (x)
x
≡lim
ε→+0

 −ε

−∞
+
∞

+ε

dx f (x)
x
.
This cuts out apiecearoundthesingular point, withboundaries that convergesymmet-
rically towards this position—the cut-out region is investigated by the delta function
δ(x) (as in Fig. 1.6):

408
5
Quantum Mechanics II
G±
0 (E) =
P
E −H0
∓iπ δ(E −H0) .
In the following, however, we shall often use
G±
0 (E) =
1
E ± io −H0
,
and correspondingly for G±, or even just G0 ≡G0(E ) ≡(E −H0)−1, although this
is only unique for Im E ̸= 0. The Lippmann–Schwinger equations follow simply
from the operator identity
1
A = 1
B + 1
B (B −A) 1
A = 1
B + 1
A (B −A) 1
B ,
if we set A = E ± io −H and B = E ± io −H0, then as a consequence we have
B −A = V , and we replace the limiting value of the product by the product of the
limiting values. In addition, we clearly have
G±† = G∓,
G±†
0
= G∓
0 .
Retarded and advanced propagators are thus adjoints of one another.
At ﬁrst glance, it may seem astonishing that we have found an expression for
G±
0 (E) which makes sense only as a weight function in an integrand. But we describe
a time-dependent situation (in particular for each scattering process, we distinguish
between before and after) and the Fourier transform t →E obscures this situation.
This procedure is only comprehensible if we calculate with unsharp energies (using
wave packets, i.e., integral expressions).
5.1.6
Representations of the Resolvents and the Interactions
The resolvent G±
0 (E) = (E ± io −H0)−1 is diagonal in the energy representation
{|E⟩} and also in the momentum representation {|k ⟩} (with E = ℏ2k2/2m0), and it
is interesting to use both representations for scattering problems:
⟨E′′| G±
0 (E) |E′′′′⟩= ⟨E′′|E′′′′⟩
E ± io −E′ ,
⟨k′| G±
0 (E) |k′′⟩=
⟨k′|k′′⟩
E ± io −ℏ2k′2/2m0
= 2m0
ℏ2
⟨k′|k′′⟩
k2 ± io −k′2 .
However, the coupling V is usually given as a function of r. Therefore, we now
search for the resolvent in the real-space representation. Using the fact that ⟨r |k ⟩=
(2π)−3/2 exp(ik · r ), we ﬁnd

5.1 Scattering Theory
409
⟨r | G±
0 (E) |r ′⟩=
1
(2π)3
2m0
ℏ2

d3k′ exp{ik′ · (r −r ′)}
k2 ± io −k′2
.
The integration over the directions of k′ is easy. In particular, if we express the
plane wave in terms of spherical harmonics, then introducing Y (l)
m () = i−l⟨|lm⟩
and Y (0)
0 () = 1/
√
4π, the contribution for the integration over all directions comes
only from l = 0, since
	
d ⟨lm|⟩⟨|00⟩= ⟨lm|00⟩:

dk exp(ik · a ) = 4π F0(ka)
ka
= 4π sin ka
ka
.
Hence the triple integral is reduced to a single one:

d3k′
exp(ik′ · a)
k2 ± io −k′2 = 4π
2i
 ∞
0
k′ dk′
a
exp(ik′a) −exp(−ik′a)
k2 ± io −k′2
= 2π
ia
 ∞
−∞
k′ dk′
exp(ik′a)
k2 ± io −k′2 .
These integrals can be evaluated using complex analysis. The integrands each have
two simple poles in the complex k′-plane, with k′
1 =
√
k2 ± io and k′
2 = −
√
k2 ± io.
Here, according to the residue theorem

f (z) (z −z0)−1 dz = 2πi f (z0), the
residues in the upper half plane are important because there the integrals over the
semi-circle with radius |k′| vanish in the limit |k′| →∞. Then,
 ∞
−∞
k′ dk′
exp(ik′a)
k2 ± io −k′2 = −2πi (±

k2 ± io) exp(± i
√
k2 ± io a)
±2
√
k2 ± io
= −πi exp(± ika) ,
and therefore in the real-space representation, the resolvent becomes
⟨r | G±
0 (E) |r ′⟩= −1
4π
2m0
ℏ2
exp(± ik|r −r ′|)
|r −r ′|
.
It is no accident that we encountered the functions exp(± ik|r −r ′|)/|r −r ′| in
our discussion of electrodynamics (see p. 255), since we were discussing there the
scattering of waves.
Since the momentum representation for scattering problems is actually better
than the real-space representation (given that the momenta mark the initial and ﬁnal
states, and the free propagators are diagonal in the momentum representation), we
now derive the matrix elements of some popular interactions in the momentum rep-
resentation. Here we restrict ourselves to couplings, which do not act on the spin, and
hence only involve Wigner forces, and we shall in fact focus on local and isotropic
couplings. Then with ℏq as the momentum transfer, we have

410
5
Quantum Mechanics II
Table 5.1 Scattering potentials and their Fourier transforms
Potential
V (r)/V0
V (q) · (
√
2π/a)3/V0
Yukawa
a/r exp(−r/a)
4π/(1 + a2q2)
Coulomb
a/r
4π/(a2q2)
Box
ε(a −r)
4π/(a2q2) · F1(aq)
Gauss
exp(−r2/a2)
√π 3 exp(−1
4a2q2)
Fig. 5.4 Fourier transforms of several isotropic potentials. Top: Yukawa and Coulomb potentials.
Bottom: Gauss and box potentials. With 1
2q = k sin 1
2θ, V (q) can be used in the Born approximation
T(q) ≈V (q) for the differential scattering cross-section, as will be shown
⟨k + q | V |k ⟩=

d3r ⟨k + q |r ⟩V (r) ⟨r |k ⟩=
1
(2π)3

d3r V (r) exp(−iq · r ) .
This is the Fourier transformed V (q ) of the coupling, disregarding the factor
(2π)−3/2. As long as V (r ) depends only upon r, as in the present case, we can
easily integrate over the directions:
⟨k + q | V |k ⟩=
V (q )
(2π)3/2 =
4π
(2π)3
 ∞
0
dr r2 V (r) sin qr
qr
.
Consequently, this matrix element only depends on the modulus of the momentum
transfer: V (q ) = V (q) for each (isotropic) Wigner force. Here q = kf −ki, and
consequently q2 = kf
2 + ki
2 −2kf · ki, so for elastic scattering q = 2k sin 1
2θ, where
θ is the scattering angle in the center-of-mass system.
Important examples with two parameters V0 and a for strength and distance are
shown in Table 5.1 and Fig. 5.4, where the spherical Bessel function is

5.1 Scattering Theory
411
F1(ρ) = ρ−1 sin ρ −cos ρ .
Note that the Coulomb potential turns up as the limit a →∞of the Yukawa potential,
but with aV0 held ﬁxed. We can thus take

d3k k−2 exp(−ik · r ) = 4π
 ∞
0
dk (kr)−1 sin(kr) ,
because according to Sect. 1.1.10 this is equal to 4π r−1 π {ε(r) −1
2}, i.e., with
r > 0, it is equal to 2π2r−1. Then k−2 is the Fourier transform of √π/2 r−1. For the
Gauss potential, we can use p. 23.
5.1.7
Lippmann–Schwinger Equations
On p. 407, we derived
the Lippmann–Schwinger equations for the propagators
G± = G±
0 + G±
0 V G± = G±
0 + G± V G±
0 . In the following, we shall generally skip
the reference to E. Then,
G± = G±
0 (1 + V G±) = (1 + G± V ) G±
0 ,
and also G±
0 = G± (1 −V G±
0 ) = (1 −G±
0 V ) G±. This leads to
G±
0 = G±
0 (1 + V G±) (1 −V G±
0 ) = (1 −G±
0 V ) (1 + G± V ) G±
0 ,
G± = G± (1 −V G±
0 ) (1 + V G±) = (1 + G± V ) (1 −G±
0 V ) G± .
Here G±
0 acts in the Hilbert space of all states of the unperturbed problem, but G±
only in the space of the scattering states: the bound states are missing. Therefore, the
projection operator onto the scattering states of H is now useful. Following Feshbach
[1], we shall denote this by P. Then it follows that
(1 + V G±) (1 −V G±
0 ) = (1 −G±
0 V ) (1 + G± V ) = 1 ,
(1 −V G±
0 ) (1 + V G±) = (1 + G± V ) (1 −G±
0 V ) = P .
We shall return to the fact that the bound states are missing in the next section.
Before that, however, we shall also derive the Lippmann–Schwinger equations
for the states. They are superior to the Schrödinger equation for scattering prob-
lems, since for a differential equation, we still need boundary conditions in order to
determine the solution uniquely.
We denote the free states in the following by |ψ⟩, but the scattering states by
|ψ⟩+ or |ψ⟩−(see Fig. 5.5). We take two different ones. In particular, we shall mark
the “retarded” solution |ψ⟩+ of H with the initial momentum—this is not a good
quantum number because it is not conserved—and the “advanced” solution |ψ⟩−with

412
5
Quantum Mechanics II
Fig. 5.5 The scattering states |p ⟩± (momentum upwards) with an attractive Coulomb potential,
represented by the classical orbital curves (calculated according to Sect. 2.1.6). From orbit to orbit,
the collision parameter changes each by one unit. Quantum-mechanically, sharp orbits are not
allowed—this is to be noted particularly for the straight orbit through the center
the ﬁnal momentum. Now t0 should mean the beginning of the scattering process for
|ψ(t)⟩+ and the end for |ψ(t)⟩−. This leads to
|ψ(t)⟩± = U(t, t0)|ψ(t0)⟩± = ± iℏG±(t −t0)|ψ(t0)⟩± ,
and in both cases |ψ(t0)⟩± = |ψ(t0)⟩. In addition, instead of ± iℏG±
0 (t −t0)|ψ(t0)⟩,
we may also use |ψ(t)⟩. With
G±(t −t0) = G±
0 (t −t0) +
 ∞
−∞
dt′ G±(t −t′) V G±
0 (t′ −t0) ,
according to p. 406, this leads to the equation
|ψ(t)⟩± = |ψ(t)⟩+
 ∞
−∞
dt′ G±(t −t′) V |ψ(t′)⟩.
Once again, the convolution integral can be transformed into a product via a Fourier
transform (in the following, we shall again skip the reference to the energy represen-
tation):
|ψ⟩± = (1 + G±V ) |ψ⟩.
With this the Lippmann–Schwinger equation holds, so (1 −G±
0 V )|ψ⟩± = |ψ⟩, and
hence,
|ψ⟩± = |ψ⟩+ G±
0 V |ψ⟩± .
If we use the Born approximation for G± or for |ψ⟩±,

5.1 Scattering Theory
413
|ψ⟩± ≈|ψ⟩+ G±
0 V |ψ⟩,
then there are only known quantities on the right.
5.1.8
Möller’s Wave Operators
According to the last section, the scattering states |ψ⟩± are related to the free states
|ψ⟩via operators:
|ψ⟩± = (1 + G±V ) |ψ⟩.
These are Möller’s wave operators ±, with the property
± |ψ⟩= |ψ⟩±
⇐⇒
⟨ψ|±† = ±⟨ψ| .
Here, in fact, the set {|ψ⟩} forms a complete basis, but the set {|ψ⟩+} or {|ψ⟩−}
comprises only the scattering states for H. The bound states are missing. If, following
Feshbach as before, we introduce the projection operator P onto the scattering states
and the projection operator Q = 1 −P onto the bound states, then
±† ± = 1 ,
but ± ±† = 1 −Q = P .
The wave operators are not unitary, but only isometric, i.e., they conserve the norm.
The wave operators ± do not map onto the whole space, and the adjoints ±† from
a part of the space onto the whole space. Therefore, in
± = P (1 + G± V ) ,
we should not forget the projection operator P—in any case, in
± (1 −G±
0 V ) = P ,
we must not put 1 on the right, because ± does not lead to bound states. On the
other hand, with (1 −G±
0 V ) G± = G±
0 and G±† = G∓, we have
± G±
0 = P G±
⇐⇒
G∓
0 ±† = G∓P ,
and with ± = P (1 + G±V ), the Lippmann–Schwinger equation
± = P + ± G±
0 V
for the wave operators. For the adjoint operators, we then obtain the equations

414
5
Quantum Mechanics II
±† = (1 + V G∓) P = P + V G∓
0 ±† ,
or (1 −V G∓
0 ) ±† = P. While ± maps the free states to the scattering states of
the full system, conversely, ±† maps the scattering states to the unperturbed system,
and the bound states |ψ⟩B to zero vectors, ±† |ψ⟩B = |o⟩.
Incidentally, we also have
H ± = ± H0 ,
since for all eigenstates of the energy, we have H±|ψ⟩= H|ψ⟩± = E|ψ⟩±, and
the quantum number E commutes with the wave operators ±, so
E±|ψ⟩= ±E|ψ⟩= ± H0|ψ⟩.
5.1.9
Scattering and Transition Operators
We shall now look for the transition probability from the initial state |ψi⟩+ to the
ﬁnal state |ψf⟩−, or more precisely, the amplitude −⟨ψf|ψi⟩+ = ⟨ψf| −†+ |ψi⟩.
Note that this does not depend upon time, because |ψi⟩+ and |ψf⟩−relate to the same
Hamilton operator H. The free states form a complete basis. Therefore, we follow
Heisenberg and introduce the scattering operator
S ≡−†+ ,
which relates the initial state directly with the ﬁnal state:
⟨ψf| S |ψi⟩= −⟨ψf|ψi⟩+ .
If we know its matrix elements, then the scattering problem is essentially solved.
It remains to show that the scattering operator is unitary, even though the
wave operators ± are only isometric. With S†S = +†−−†+ and SS† =
−†++†−, we therefore investigate ±†∓∓†± = ±† P ±. Since ±
maps only onto the space of scattering states, we have P± = ±, and thus
±†± = 1 is left over. The scattering operator is thus unitary:
S†S = SS† = 1 .
Unitarity guarantees, among other things, that nothing is lost in the scattering process,
whence the norm of the original wave remains conserved.
In order to show the inﬂuence of the interaction V as clearly as possible, we
reformulate the transition amplitude. With

5.1 Scattering Theory
415
|ψi⟩+ −|ψi⟩−= (G+−G−) V |ψi⟩= −2πi δ(E−H) V |ψi⟩,
+⟨ψf| −−⟨ψf| = ⟨ψf| V (G−−G+) = +2πi ⟨ψf| V δ(E−H) ,
we have in particular,
⟨ψf| S |ψi⟩= −⟨ψf|ψi⟩+= −⟨ψf|ψi⟩−−2πi δ(Ei−Ef) −⟨ψf| V |ψi⟩
= +⟨ψf|ψi⟩+ −2πi δ(Ef−Ei) ⟨ψf| V |ψi⟩+ .
Given the isometry of the wave operators, we have
−⟨ψf|ψi⟩−= ⟨ψf|ψi⟩=
+⟨ψf|ψi⟩+. Furthermore, the delta function δ(Ef −Ei) can be extracted and this
ensures conservation of the energy:
⟨ψf| S |ψi⟩= δ(Ef −Ei) {⟨f|i⟩−2πi ⟨ψf| T |ψi⟩} ,
where the transition operator is deﬁned by
T ≡−† V = V + .
Here the expressions are only to be evaluated “on the energy shell”, i.e., for Ef = Ei.
Then we have G+
0 T = G+
0 −† V = G+ P V . Since G+ acts only in the P-space,
we write for short
G+
0 T = G+ V ,
or T G+
0 = V G+ .
Then for the retarded propagators,
G+ = G+
0 + G+
0 T G+
0
from the Lippmann–Schwinger equations. Correspondingly, from T = V + =
V P (1 + G+V ), we deduce the Low equation
T = V + V G+ V .
According to the above equations, the Lippmann–Schwinger equations are valid for
the transition operator T:
T = V + V G+
0 T = V + T G+
0 V .
These equations are particularly useful, because the transition operator T is directly
connected to the scattering cross-section and indeed other experimental quantities
(observables), as we shall see in the next section.
In the Born approximation, we replace T by V and thereby avoid having to com-
putetheresolvents. Then, however, G+V must not betoolarge, whichis whytheBorn
approximation fails for resonances. Note ﬁnally that, in the Lippmann–Schwinger

416
5
Quantum Mechanics II
equation for T, different energies can occur in bra and ket, whereas for two-body
scattering, they do not contribute.
5.1.10
The Wave Function ⟨r | k ⟩+ for Large Distances r
We now consider the real-space representation of the scattering states |k ⟩+ in the rel-
ative coordinate r of the two scattering partners. The limit r →∞will be important
for the scattering cross-section, with which we shall occupy ourselves subsequently.
Particularly convenient is the starting equation
|k ⟩+ = (1 + G+
0 T) |k ⟩,
because we have already found the real-space representation of G+
0 on p. 409:
⟨r | G±
0

ℏ2k2
2m0

|r ′⟩= −1
4π
2m0
ℏ2
exp(± ik|r −r ′|)
|r −r ′|
.
For r ≫r′ and |r −r ′| ≈r

1 −2 r · r ′/r2 ≈r −r · r ′/r (see Fig. 3.30), and with
the abbreviation
k ′ ≡k r
r ,
the last expression goes over into
⟨r | G±
0

ℏ2k2
2m0

|r ′⟩≈−1
4π
2m0
ℏ2
exp(± ikr)
r
exp(∓ik ′ · r ′) .
Here, exp(−ik ′ · r ′) = (2π)3/2 ⟨k ′|r ′⟩. Therefore, we have (see p. 399)
⟨r |k ⟩+ ≈⟨r |k ⟩−
√
2π m0
ℏ2
⟨k′| T |k ⟩exp(ikr)
r
=
1
√
2π 3

exp(ik · r ) + f (θ) exp(ikr)
r

,
with scattering amplitude
f (θ) ≡−

2π
ℏ
2
m0 ⟨k′| T |k⟩= −(2π)2
k
⟨Ef| T |Ei⟩.
For the second formulation here, note that |k ⟩= |E⟩ℏ/√m0k, which follows from
⟨k|k ′⟩= k−2δ(k−k′) δ(−′) and δ(E−E′) = 2m0ℏ−2 δ(k2−k′2) with δ(k2−
k′2) = (2k)−1 δ(k−k′) (see p. 20). Here we recognize the difference between the

5.1 Scattering Theory
417
wave vector and the energy representations. We have already discussed the differ-
ence between the wave vector and the momentum representations on p. 319. Here
i gives the direction before scattering and f the direction afterwards. If there is
a Wigner force—no spin dependence—only the scattering angle θ between the two
directions is important, because for rotational invariance the transition operator in
the angular momentum representation is diagonal and does not depend upon the
directional (magnetic) quantum number:
⟨f| T |i⟩=

lm
⟨f|lm⟩Tl ⟨lm|i⟩=

lm
Y (l)
m (f) Tl Y (l) ∗
m (i)
=

l
2l + 1
4π
Tl Pl(cos θ) .
It follows that f (θ) = −(π/k) 
l (2l + 1) Tl Pl(cos θ), as claimed on p. 402.
5.1.11
Scattering Cross-Section
Scattering cross-sections are not the only observables in scattering processes. For
particles with spin, polarizations (i.e., spin distributions) can be measured. But in
that case, only the angular momentum algebra need be applied. The basic notions can
be explained with the example of spinless particles, and we shall restrict ourselves
here to this essentially simple case.
The differential scattering cross-section dσ/d is given by the number of particles
scattered into the solid angle element d relative to the number of incoming particles
per area unit and the number of scattering centers. (For stationary currents, we have
to refer to equal time spans in the numerator and denominator. In addition, the
expression does not hold if the incoming or outgoing particles interact with each
other, or if the individual centers scatter coherently, as for the refraction of slow
neutrons in crystals.) We can also express the scattering cross-section in terms of the
current densities of the scattering wave and the incoming wave:
dσ
d = jscat() r2
j i
.
Here it is well known that, in the real-space representation, we have (see p. 348)
j (r ) = ℏ
i
ψ∗∇ψ −ψ ∇ψ∗
2m0
,
and from ψscat(r ) ≈(2π)−3/2 exp(ikr) f (θ)/r and ψi(r ) = (2π)−3/2 exp(ik · r ),
we obtain the current densities

418
5
Quantum Mechanics II
j i =
1
(2π)3
ℏk
m0
,
jscat ≈
1
(2π)3
ℏk
m0
|f (θ)|2
r2
.
Therefore, the differential scattering cross-section can be evaluated from the scatter-
ing amplitude f and the transition matrix T as follows:
dσ
d = |f (θ)|2 = (2π)4
k2
|⟨Ef| T |Ei⟩|2 ,
if we also use the last section for the relation between f and T.
Using ⟨E| S |E′′⟩= ⟨E|E′′⟩−2πi⟨E|E′⟩⟨E|T|E′⟩and the unitarity of
the scattering operators, viz., S†S = 1, which expresses current conservation, we
obtain
i⟨E|T|E′⟩−i⟨E′|T|E⟩∗= 2π

d′′ ⟨E′′|T|E⟩∗⟨E′′|T|E′⟩,
after splitting off the factor 2π δ(E −E′). With ′ = , this implies
−2Im⟨E|T|E⟩= 2π

d′ |⟨E′|T|E⟩|2 =
k2
(2π)3

d′ dσ
d′ ,
and what is known as the optical theorem relating the integrated scattering cross-
section and the forward scattering amplitude:
σ = (2π)3
k2
(−2Im⟨E|T|E⟩) = 4π
k Imf (0) .
To ﬁrst order in the Born approximation, the forward scattering amplitude is real,
which contradicts unitarity. In fact, for the forward scattering amplitude, at least the
second order is necessary.
If there are other processes in addition to elastic scattering, such as inelastic or
even disorder reactions, then σ in the last equation stands for the sum of all integrated
scattering cross-sections, the total scattering cross-section, because we have to insert
a complete basis in order to arrive at |T|2 when computing T †T.
5.1.12
Summary: Scattering Theory
In the scattering theory, we investigate how an original state is transformed into a new
state as a consequence of a perturbation V . In addition to the quantities associated
with the unperturbed system, i.e., the Hamilton operator H0, the time shift operator
U0, the propagators (Green functions) G±
0 , and the states |ψ⟩, there are quantities
associated with the (full) perturbed problem: the Hamilton operator H = H0 + V , the
time shift operator U, the propagators G±, and the states |ψ⟩±. These quantities are

5.1 Scattering Theory
419
related to each other, in the time-dependent case via integral equations, in the energy-
dependent case via the Lippmann–Schwinger equations. The scattering operator S,
or again the transition operator T, describe the transition from the unperturbed initial
state to the unperturbed ﬁnal state.
5.2
Two- and Three-Body Scattering Problems
5.2.1
Two-Potential Formula of Gell-Mann and Goldberger
This formula is important for many applications of the generalized scattering theory
and starts from
V = V + δV ,
where the approximate scattering problem for V is considered already solved, so that
the propagator for H0 + V is known, viz.,

G = G0 (1 + V 
G) = (1 + 
G V ) G0 ,
along with the transition operator T:
T = V (1 + G0 T) = (1 + T G0) V .
Note that, from now on, we shall usually skip the indices ± and the argument E.
According to p. 415, we also have G0T = 
GV and TG0 = V 
G. In addition, using
G = G0 + G0 (V + δV ) G, which implies (1 −G0V ) G = G0 (1 + δV G), then
multiplying by 1 + 
GV and using the relation (1 + 
GV )(1 −G0V ) = P = (1 −
V G0)(1 + V 
G) found on p. 411 (with δV = 0), we deduce that
G = 
G (1 + δV G) = (1 + G δV ) 
G ,
where we just write G instead of PG or GP once again, since we restrict ourselves to
scattering states anyway. Another proof this equation follows using G−1 = E −H,

G−1 = E −H, and δV = V −V = 
G−1 −G−1:

G δV G = G −
G = G δV 
G .
We thus have a Lippmann–Schwinger equation in which, instead of the full coupling,
only the “perturbation” δV appears, but with 
G instead of the free propagators G0.
According to the last equation, we have
(1 + G δV )(1 + 
GV ) = 1 + G δV + 
GV + (G −
G) V = 1 + GV .

420
5
Quantum Mechanics II
This factorization of 1 + GV is useful because then, from |ψ⟩± = (1 + G±V )|ψ⟩,
with the states deformed by V |ψ⟩± = (1 + 
G±V )|ψ⟩, we have the helpful relation
|ψ⟩± = (1 + G± δV ) |ψ⟩± .
Note that 1 + V G factorizes into (1 + V 
G)(1 + δV G).
For the Low equation T = V (1 + GV ), we can also use this kind of factorization.
With
V (1 + G δV ) = V + (1 + V G) δV ,
(1 + V G) δV = (1 + V 
G)(1 + δV G) δV ,
and the modiﬁed Low equation
δT = (1 + δV G) δV ,
along with T = V (1 + 
GV ), we obtain the formula of Gell-Mann and Goldberger
T = T + (1 + V 
G) δT (1 + 
G V ) = T + (1 + T G0) δT (1 + G0 T) ,
which is extremely useful here.
For the matrix elements of the transition operators, we thus have
⟨ψf| T |ψi⟩= ⟨ψf| T |ψi⟩+ −⟨ψf| δT |ψi⟩+ .
If we take the Born approximation δT ≈δV for δT here, we obtain a better Born
approximation known as the distorted-wave Born approximation (DWBA). Whereas
all higher order terms in V are left out in the Born approximation, now only those in
δV are missing. However, the states |ψ⟩± (distorted by V ) still have to be calculated,
as does T.
Note that we also have
(1 + G δV )(1 −
G δV ) = 1 = (1 −
G δV )(1 + G δV ) ,
since the product is equal to 1 + (G −
G −G δV 
G) δV , and we have already
proven G −
G = G δV 
G. Consequently, multiplying |ψ⟩± = (1 + G δV )|ψ⟩± by
1 −
G δV , we ﬁnd |ψ⟩± = (1 −
G δV )|ψ⟩±, or the Lippmann–Schwinger equation
|ψ⟩± = |ψ⟩± + 
G± δV |ψ⟩± .
We shall refer to this in Sect. 5.2.4.

5.2 Two- and Three-Body Scattering Problems
421
5.2.2
Scattering Phases
This result will now be explained using the methods mentioned in Sect. 5.1.1. There
we introduced the spherical Bessel functions
Fl ≈sin(ρ −1
2l π) ,
Ol ≈exp{+i(ρ −1
2l π)} ,
Gl ≈cos(ρ −1
2l π) ,
Il ≈exp{−i(ρ −1
2l π)} ,
and expanded the radial function of the Schrödinger equation with respect to two of
them in the region with V = 0. If V vanishes everywhere (and hence the transition
operator along with it), then the function Fl alone sufﬁces, because only this is
differentiable at the origin. Generally, ul ≈Nl (Fl −π Tl Ol), where Nl ensures the
correct normalization. Given the unitarity of the scattering operators, we set
Sl = exp(2iδl) ,
and make use of Sl = 1 −2πi Tl. Then,
−π Tl = exp(2iδl) −1
2i
= exp(iδl) sin δl ,
and with Fl = (Ol −Il)/(2i), it follows that
2i ul/Nl ≈Ol −Il + {exp(2iδl) −1} Ol = exp(2iδl) Ol −Il ,
so
ul ≈Nl exp(iδl) sin(ρ −1
2lπ + δl) .
In order to ﬁx the scattering phase δl, not only mod π, we also require it to depend
continuously on k and vanish for k →∞, because for E →∞, the coupling V
should be negligible—to the (repulsive) centrifugal force there clearly corresponds
the (negative) scattering phase −1
2lπ, independent of the energy. Note that, on the
other hand, according to the Levinson theorem, the phase shift for k = 0 is equal to
π times the number of bound states.
After these preliminaries, we introduce the scattering phaseδl associated with V ,
and in addition to Ol = exp(iδl) Ol, we use
Fl ≈cosδl Fl + sinδl Gl = cosδl Fl + sinδl (Ol −iFl) = exp(−iδl) Fl + sinδl Ol .
Withthisweobtainforul asymptoticallytheexpressionNl {Fl −πTl Ol},andinstead
of the curly brackets, we may also write
exp(iδl) {Fl −sinδl exp (−iδl) Ol −πTl exp(−2iδl) Ol}
= exp(iδl) {Fl −exp(−2iδl) Ol [exp(iδl) sinδl + πTl]} .

422
5
Quantum Mechanics II
Since we now have to set exp(iδl) sinδl = −π Tl, we obtain
ul ≈Nl exp(iδl) {Fl −π(Tl −Tl) exp(−2iδl) Ol} .
From this we can conclude that we should take
Tl = Tl + exp(2iδl) δTl ,
which corresponds to the two-potential formula. Here, the factor exp(2iδl) originates
from the distortion of the states due to the coupling V , because we have used the
functions Fl and Ol.
5.2.3
Scattering of Charged Particles
An important application is to scattering by the Coulomb potential, since it decreases
so slowly with increasing r that the previous results cannot simply be carried over.
Here we use the Sommerfeld parameter (Coulomb parameter)
η ≡zZe2
4πε0
m0
ℏ2k ,
together with the Coulomb scattering phase
σl(η) ≡arg (l + 1 + iη)
=⇒
exp(2iσl) = (l + 1 + iη)
(l + 1 −iη) .
The spherical Bessel functions are now replaced by the Coulomb wave functions
Fl(η, ρ) ≈sin(ρ −η ln 2ρ −1
2lπ + σl) ,
Ol(η, ρ) ≈exp{+i(ρ −η ln 2ρ −1
2lπ + σl)} ,
where the logarithm originates from the long range of the potential in the radial
Schrödinger equation

 d2
dρ2 −l(l + 1)
ρ2
+ 1 −2η
ρ

ul(ρ) = 0 ,
with ρ = kr .
Note that with the bound states stands −1 instead of +1 for the energy and the
principal quantum number n instead of −η (see p. 362). Despite the long range, we
can introduce a Coulomb scattering amplitude
fC(θ) = −η
2k
exp{2i (σ0 −η ln sin 1
2θ)}
sin2 1
2θ
,

5.2 Two- and Three-Body Scattering Problems
423
and hence determine the Rutherford cross-section
dσ
d = |fC(θ)|2 =
η2
4k2 sin4( 1
2θ) .
With fC(θ) = −(2π)2k−1⟨f|TC|i⟩, the matrix element of the transition operators
for the Coulomb problem follows from the scattering amplitude:
TC(θ) = η
2
exp{2i (σ0 −η ln sin 1
2θ)}
(2π sin 1
2θ)2
.
Incidentally, its modulus 1
2η (2π sin 1
2θ)−2 is equal to ⟨Ef| VC |Ei⟩, because with
VC(r) =
zZe2
4πε0 r = η ℏ2k
m0 r ,
we have
⟨kf|VC|ki⟩= 1
2ηℏ2k m0
−1 (2π k sin 1
2θ)−2 ,
according to p. 410, and in addition,
⟨Ef| VC |Ei⟩= m0kℏ−2 ⟨kf| VC |ki⟩,
according to p. 417. Only the phase is missing from the Born approximation!
We thus have Fl(ρ) = Fl(η, ρ) and Ol(ρ) = Ol(η, ρ) for the scattering of charged
particles, along with T = TC(θ) and δl = σl(η). Further forces (e.g., nuclear forces)
then contribute in the term δTl.
5.2.4
Effective Hamilton Operator in the Feshbach Theory
A further important application of the two-potential formula is the uniﬁed theory of
nuclear reactions due to Feshbach (see p. 411). This leads us to a deeper understand-
ing of all resonances and direct reactions (not only in nuclear physics) and embraces
several other resonance models.
The decisive point of the Feshbach formalism is the separation of the Hilbert
space into two parts, on which we project with the operators P and Q:
P = P† = P2 ,
Q = Q† = Q2 ,
P Q = 0 = Q P ,
P + Q = 1 .
P maps onto those states which do not vanish for large r, viz., the scattering states
describing open channels, and Q onto the “bound” states, which vanish for large

424
5
Quantum Mechanics II
r and describe closed channels. This division considers only large distances of the
scattering partners (asymptotic boundary conditions) and allows several cases for
short distances. Therefore, different resonance theories are still possible. If we intro-
duce, e.g., a channel radius R with the property that the interaction vanishes for
larger distances, we may let Q project onto the space 0 ≤r ≤R and P onto the space
r > R: this leads to the scattering matrix of Wigner and Eisenbud [2] (see also [3]).
(It differs from the transition matrix of Kapur and Peierls [4] in that the boundary
conditions for r = R depend upon the energy.) In the Feshbach formalism, there is
no need for the channel radius R.
Along with the division of the Hilbert space into open and closed channels, we
also have to decompose the Hamilton operator correspondingly:
H = (P + Q) H (P + Q) ≡HPP + HPQ + HQP + HQQ .
For the scattering cross-section, only P|ψ⟩± is important. We now search for the
“effective” Hamilton operator acting on these scattering states, and after that derive
the associated Lippmann–Schwinger equation.
To begin with, from (E −H) |ψ⟩± = 0, after projection with P and Q such that
1 = P2 + Q2, we have the general result
(E −HPP) P|ψ⟩± = HPQ Q|ψ⟩±
and (E −HQQ) Q|ψ⟩± = HQP P|ψ⟩± .
Since Q projects onto the closed channels, the inhomogeneous term is missing in its
Lippmann–Schwinger equation:
Q|ψ⟩± = GQ HQP P|ψ⟩± ,
with GQ ≡
1
E −HQQ
.
If we insert this into the other relation, we obtain the homogeneous equation
(E −HPP −HPQ GQ HQP) P|ψ⟩± = 0 .
We thus ﬁnd the effective Hamilton operator HPP + HPQ GQ HQP. Clearly, it can be
used for the two-potential formula: HPP plays the role of H0 + V and HPQGQHQP
that of δV . However, from now on, we write G±P ≡(E −HPP)−1 with complex E ,
instead of 
G±, and according to p. 420, we now have
P |ψ⟩± = |ψ⟩± + G±
P HPQ GQ HQP P|ψ⟩± ,
with G±
P ≡
1
E −HPP
,
as the Lippmann–Schwinger equation for the unknown scattering state.

5.2 Two- and Three-Body Scattering Problems
425
5.2.5
Separable Interactions and Resonances
The key feature of the new residual interaction δV = HPQ GQ HQP is the product
form. Such couplings are said to be separable. They can be diagonalized in the
space of scattering states and therefore not in real space, and are thus non-local:
⟨r |V |r ′⟩̸= V0(r ) δ(r −r ′). The transition operator δT now also factorizes, because
the relations δT = δV (1 + G δV ) and 1 + G δV = (1 −
G δV )−1 mentioned on
p. 420 deliver δT = δV/(1 −
G δV ):
δT = HPQ GQ HQP
1
1 −G+P HPQ GQ HQP
.
Here, A (1 −BA)−1 = (1 −AB)−1A with (1 −AB) A = A (1 −BA), and thus
HQP
1
1 −G+P HPQ GQ HQP
=
1
1 −HQP G+P HPQ GQ
HQP .
With {GQ (1 −HQPG+PHPQGQ)−1}−1 = (1 −HQPG+PHPQGQ)GQ−1, the operator
between HPQ and HQP can also be simpliﬁed:
δT = HPQ
1
E −HQQ −HQP G+P HPQ
HQP .
Here, since
HQP G+
P HPQ = HQP

P
E −HPP
−iπ δ(E −HPP)

HPQ =  −1
2 i ,
it is clear that the poles do not occur at the eigenvalues of HQQ, but are displaced by
the level shift , and have a level width  (see Fig. 5.6):
|δT|2 ∼
1
(E −HQQ −)2 + 1
42 ,
We will discuss these resonance parameters in the next section. When considering
δT, the coupling HQP which leads from the P- to the Q-space is initially important,
then the resonance level in Q-space, and ﬁnally again the coupling HPQ which leads
back from the Q- to the P-space.
Near the resonance,
ψ(+)(t) ∼exp −i (HQQ +  −1
2 i) t
ℏ
,

426
5
Quantum Mechanics II
Fig. 5.6 Lorentz curve 1
4/(x2+ 1
4) (continuous red), line shape of a scattering resonance about the
resonance energy ER with half-width , where x = (E −ER)/. The curve has half the maximum
value at two points which have the distance of the half-width (dashed blue). For this distribution,
E = ∞and the associated average lifetime is ℏ/
and consequently,
|ψ(+)(t)|2 ∼exp −t
ℏ
= exp −t
τ ,
with τ ≡ℏ
 ,
where τ is the average lifetime of the resonance state. We can also view it as the time
uncertainty of the state, because it is now t2 −¯t 2 = τ 2. The associated distribution
function |ψ+(E)|2 in the energy representation is given by a Lorentz curve (with
inﬁnite energy uncertainty according to Problem 4.3). Therefore, the equation τ  =
ℏ, which is a lifetime–half-width relation, is not a time–energy uncertainty relation,
even though this is often claimed—there is no Hermitian time operator in quantum
theory and hence there is also no such inequality, even though each ﬁnite wave train
has a ﬁnite time and frequency uncertainty, even classically.
5.2.6
Breit–Wigner Formula
There are various methods for computing
δT = HPQ
1
E −HQQ −HQP G+P HPQ
HQP .
In order to proceed without approximations, we have to diagonalize the denominator,
which means searching for the eigen representation of
H ′ ≡HQQ + HQP G+
P HPQ ,
with G+
P =
P
E −HPP
−iπδ(E −HPP) ,
where the last term is not Hermitian. Therefore, we now need two sets of solutions
(a bi-orthogonal system) in the Q-space,

5.2 Two- and Three-Body Scattering Problems
427
{Eν −H ′ (E )} |ν (E )⟩= 0
{E ∗
ν −H ′ †(E )} |A
ν(E )⟩= 0
⇐⇒
⟨A
ν | {Eν −H ′(E )} = 0
with ⟨A
ν(E ) | ν′(E )⟩= δνν′ and 
ν |ν(E )⟩⟨A
ν(E )| = Q. The eigenvalues Eν of
H ′ are complex, and
Q
E −H ′ =

ν
|ν(E )⟩⟨A
ν(E )|
E −Eν
holds. Here G+P still depends on the energy, and therefore also on H ′ and the whole
bi-orthogonal system. This seriously complicates the computation.
These difﬁculties can be avoided with an approximation. We take the eigenstates
of HQQ,
(En −HQQ) |n⟩= 0 ,
e.g., those of the box or quadratic potential (see Sects. 4.5.3 and 4.5.4), and obtain
the shift and width according to perturbation theory from
⟨n|HQP G+
P(E)HPQ|n⟩= P

dE′ |⟨n|HQP|ψ(E′)⟩+|2
E −E′
−iπ|⟨n|HQP|ψ(E)⟩+|2
≈n(E) −1
2 in(E) .
For elastic scattering, this leads to the Breit–Wigner formula
−⟨ψ| δT |ψ⟩+ ≈1
π

n
1
2 n
E −En −n + 1
2in
.
With the level width n, the terms for all real energy values remain ﬁnite. This is
similar to the result that only ﬁnite amplitudes are permitted, as for the damping of
a forced oscillation (Sect. 2.3.8) .
5.2.7
Averaging over the Energy
So far we have assumed that the energy can be arbitrarily sharp. Actually we should
not do this, but rather calculate with mean values. Even disregarding this aspect, it
is instructive to given an overview of the average behavior.
We denote the mean values as usual with angular brackets or bars and use suitable
weight factors ρ(E, E′) to compute them, as in

428
5
Quantum Mechanics II
⟨f (E)⟩≡f (E) ≡

dE′ ρ(E, E′) f (E′) ,
where
ρ(E, E′) = 0 ,
for |E −E′| ≫I
and

dE′ ρ(E, E′) = 1 .
The Lorentz distribution is analytically convenient:
ρ(E, E′) = I
2π
1
(E −E′)2 + I2/4 .
It is symmetric in E and E′ and has a maximum for E′ = E, while the half-width I
does not lead to cumbersome boundary effects, as the box distribution does. However,
the Lorentz distribution does not have a ﬁnite energy uncertainty E—only the
half-width is ﬁnite. For a test function f (E) which is regular in the upper complex
half plane and vanishes sufﬁciently fast for large |E|, we have by the Breit–Wigner
formula,
f (E ) =

n
an
E −En
,
with En = En −1
2 in ,
n > 0 .
The residue theorem then implies
⟨f (E)⟩=
I
2π

dE′
(E′ −E −1
2iI)(E′ −E + 1
2iI)

n
an
E′ −En
=
I
2π 2πi

n
an
iI(E + 1
2iI −En) = f (E + 1
2iI) .
While the limit E + io has been necessary so far, the average now already leads
to a complex energy: the imaginary part is equal to half of the half-width of the
distribution function. In then averaged scattering amplitude, the level widths are thus
broadened:
⟨ψf| T |ψi⟩= ⟨ψf| T |ψi⟩+

n
−⟨ψf| VPQ |n⟩⟨A
n| VQP |ψi⟩+
E −{En −1
2i(n + I)}
.
Here we have assumed that T does not depend strongly on the energy. The interval I
of the averaging procedure may be large compared with the resonance widths n, but
it must nevertheless be so small that the average T is not altered. (T comprises only
the broad “potential resonances”, and δT the narrow “compound nucleus (Feshbach)
resonances”.)

5.2 Two- and Three-Body Scattering Problems
429
5.2.8
Special Features of Three-Body Problems
In the rest of this section, we shall treat a special aspect of the scattering theory which
in fact does not belong to a standard course on Quantum Theory II, although it is
nevertheless important and instructive. If three partners 1, 2, and 3 are involved in a
reaction, then there are many more reaction possibilities than for only two of them.
If initially, e.g., 2 and 3 are bound to each other and form the collision partner for 1,
then the following transitions are possible:
1 + (2 + 3) →1 + (2 + 3) elastic (and inelastic) scattering,
→2 + (3 + 1) disorder reaction,
→3 + (1 + 2) disorder reaction,
→1 + 2 + 3
ﬁssion reaction.
For ﬁssion, one partner can initially also leave the interaction regime, while the
others stay together for a while. We then speak of stepwise decay, and of a ﬁnal-state
interaction between ﬁrst and second decay, even though this “ﬁnal state” also decays.
If we trace the reaction back to two-body forces (not including many-body forces),
then we must nevertheless be careful to distinguish between genuine three-body
operators and those for which the unit operator for one particle may be split-off, then
multiplied by a two-body operator for the two remaining particles. For example, for
the interaction between particles 2 and 3, we write
V 23 ≡V1 ≡v1 11 .
If the particle is involved, then its number appears up, if it is not involved, then it
appears down. Lower-case letters now indicate two-body operators. For two-body
forces, we then have V = V 23 + V 31 + V 12 = V1 + V2 + V3.
Since for the disorder reaction 1 + (2 + 3) →2 + (3 + 1), initially V1 and then
V2 leads to a bound state of the corresponding pair, instead of the free Hamilton
operators H0, we clearly now need the channel Hamilton operators
Hα ≡H0 + Vα ,
and the “residual interaction” is
V α ≡V −Vα = H −Hα .
V α thus contains all two-body interactions involving α, e.g., then V 1 = V 12 + V 13 =
V3 + V2.
In order to capture the ﬁssion, let us also allow α = 0, i.e., α ∈{0, 1, 2, 3}, and
require V0 ≡0 or V 0 ≡V . In addition to the full resolvent G, we also introduce
channel resolvents Gα:

430
5
Quantum Mechanics II
Fig. 5.7 Unconnected graphs for three-body scattering. Here partner 1 is not involved and delivers
useless factors. Left: V1 Center: T1. Right: V1G0T1. Partners 2 and 3 participate in the two-body
scattering
G(E ) ≡
1
E −H ,
Gα(E ) ≡
1
E −Hα
.
Then according to Sect. 5.2.1, the Lippmann–Schwinger equations are valid:
Gα= (1 + GαVα ) G0= G0 (1 + Vα Gα) ,
G = (1 + G V α) Gα= Gα (1 + V α G ) .
These equations are in fact correct, but the last row does not ﬁx the unknown resolvent
G uniquely. Here we would have to invert the operator
1 −Gα V α = 1 −G0 (1 + Vα Gα) V α = 1 −G0 V α −G0 Vα Gα V α .
But with V 1 = V2 + V3 (with α = 1), it contains the parts G0V2 and G0V3, and
hence different unit operators (the “non-involved part”, unconnected graphs) (see
Fig. 5.7). In the energy and momentum representations, this leads to delta functions,
and in the real-space representation to divergent integrals, which requires another
approach. Note that such problems do not occur for VαGαV α, because all parts are
involved.
5.2.9
The Method of Kazaks and Greider
One possibility for solution is a method due to Kazaks and Greider [5]. As for the two-
potential formula, we deal initially only with parts of the interaction. In particular,
we take the transition operators for the two-body scattering to vα (with α ̸= 0),
tα = vα (1 + g0 tα) = (1 + tα g0) vα ,
and use the energy E −Eα in g0. We leave the particles α untouched and begin by
solving the scattering problem for the two remaining partners. Then we may also use
Tα = tα 1α = Vα (1 + G0 Tα) = (1 + Tα G0) Vα ,

5.2 Two- and Three-Body Scattering Problems
431
with
(1 −G0 Vα) (1 + G0 Tα) = 1
and
Tα G0 = Vα Gα ,
and we need T1, T2, and T3. Then with α ̸= β ̸= γ ̸= α, and thus V α = Vβ + Vγ ,
we obtain
1 −G0 V α = 1 −G0 Vβ −G0 Vγ = (1 −G0 Vβ) {1 −(1 + G0 Tβ) G0 Vγ } .
The last factor is equal to 1 −G0 Vγ −G0 Tβ G0 Vγ , and with Tγ = (1 + Tγ G0) Vγ ,
or Vγ = Tγ (1 −G0Vγ ), it may also be factorized:
1 −G0 Vγ −G0 Tβ G0 Vγ = (1 −G0 Tβ G0 Tγ ) (1 −G0 Vγ ) .
Consequently, (1 −G0V α)−1 can be decomposed into three factors:
(1 −G0 V α)−1 = (1 + G0 Tγ ) (1 −G0 Tβ G0 Tγ )−1 (1 + G0 Tβ) .
Here β and γ may be exchanged. Therefore, for the transition operator T α associated
with V α = Vβ + Vγ (with α ̸= 0), we obtain
T α = (1 + T α G0) V α = V α (1 + G0 T α) = V α (1 −G0 V α)−1 ,
along with Vβ (1 + G0Tβ) = Tβ and Vγ (1 + G0Tγ ) = Tγ , and hence the expression
(see Fig. 5.8)
T α = Tβ (1 −G0 Tγ G0 Tβ)−1 (1 + G0 Tγ )
+Tγ (1 −G0 Tβ G0 Tγ )−1 (1 + G0 Tβ) .
The initially non-invertible operator 1 −GαV α with V α = T α (1 −G0V α) may now
be split-up into a product:
1 −Gα V α = 1 −G0 V α −G0 Tα G0 V α = (1 −G0 Tα G0 T α) (1 −G0 V α) .
Fig. 5.8 Connected graphs for three-body scattering. Here we consider the example T3G0T1. These
arise for the method of Kazaks and Greithe and also for the iterated Faddeev equation. The scattering
problem is therefore soluble

432
5
Quantum Mechanics II
Both factors are invertible. In particular, (1 −G0 V α) (1 + G0 T α) = 1. Therefore,
for the unknown resolvent G from (1 −GαV α) G = Gα, we have the unique result
G = (1 + G0 T α) (1 −G0 Tα G0 T α)−1 Gα .
The operators Tα, Tβ, and Tγ are extremely useful for solving the problem. Only by
controlling the two-body scattering can we treat the three-body scattering.
5.2.10
Faddeev Equations
In the last equation, G may be decomposed into three parts:
G = G1 + G2 + G3 ,
where (with α = 1)
G1 = G1 + G0 T1 (G2 + G3)
G2 =
G0 T2 (G1 + G3)
G3 =
G0 T3 (G1 + G2) .
Hence,
G2 = G0 T2 G1 + G0 T2 G0 T3 (G1 + G2)
= (1 −G0 T2 G0 T3)−1 G0 T2 (1 + G0 T3) G1 .
Using (1 −A B)−1 A = A (1 −B A)−1, this is equivalent to
G2 = G0 T2 (1 −G0 T3 G0 T2)−1 (1 + G0 T3) G1 ,
G3 = G0 T3 (1 −G0 T2 G0 T3)−1 (1 + G0 T2) G1 .
We then also have G2 + G3 = G0 T 1 G1 and thus G1 = G1 + G0 T1 G0 T 1 G1. If we
solve with respect to G1, then we ﬁnd G1 = (1 −G0 T1 G0 T 1)−1 G1. Consequently,
the initial equation is equivalent to
G = (1 + G0T 1) G1 = (1 + G0 T 1) (1 −G0 T1 G0 T 1)−1 G1 .
This expression for the resolvent G was also derived in the last section. Hence, if the
initial state has α = 1, we have proven the Faddeev equations
⎛
⎝
G1
G2
G3
⎞
⎠=
⎛
⎝
G1
0
0
⎞
⎠+ G0
⎛
⎝
0 T1 T1
T2 0 T2
T3 T3 0
⎞
⎠
⎛
⎝
G1
G2
G3
⎞
⎠,

5.2 Two- and Three-Body Scattering Problems
433
which deliver G = G1 + G2 + G3. After an iteration, they have a unique solution,
because then only connected graphs occur:
⎛
⎝
G1
G2
G3
⎞
⎠=
⎛
⎝
G1
G0 T2 G1
G0 T3 G1
⎞
⎠+ G0
⎛
⎝
T1G0(T2+T3)
T1G0T3
T1G0T2
T2G0T3
T2G0(T1+T3)
T2G0T1
T3G0T2
T3G0T1
T3G0(T1+T2)
⎞
⎠
⎛
⎝
G1
G2
G3
⎞
⎠.
More details can be found in the book by Schmid and Ziegelmann [6].
5.2.11
Summary: Two- and Three-Body Scattering Problems
Here we presented the generalized framework for scattering theory, followed by
several important applications. These made use of the two-potential formula (V =
V + δV ) due to Gell-Mann and Goldberger: T = T + (1 + TG0) δV (1 + G0V ).
This helps, e.g., with the scattering of charged particles, because the Coulomb poten-
tial has too long a range for a simple scattering theory, but also for resonances, where
the coupling of the scattering states to bound states becomes important.
5.3
Many-Body Systems
5.3.1
One- and Many-Body States
Since generally n is taken as the occupation number for many-particle problems, we
shall now write |ν⟩to indicate a one-particle basis, instead of |n⟩as used so far. We
start from a complete orthonormal set of one-particle states |ν⟩, whence

ν
|ν⟩⟨ν| = 1
and
⟨ν|ν′⟩= δνν′ .
For continuous quantum numbers ν, there will be an integral here instead of the sum,
and the delta function instead of the Kronecker symbol. Here we order the states |ν⟩
with respect to their energy. This is not actually important for the time being, but
the notation ν < ν′ should always make sense, and later it is mainly states with low
energy that will be occupied.
N particles have N times as many degrees of freedom as a single particle, and the
Hilbert space has correspondingly more quantum numbers and dimensions. As long
as they do not interact with each other, for each individual particle, we can identify
the one-particle state it is in—if there are pure states, the case to which we restrict
ourselves here. Let the ﬁrst particle be in the state |ν1⟩, the second in |ν2⟩, and so on.
Then we may consider a product of one-particle states

434
5
Quantum Mechanics II
|ν1ν2 . . . νN⟩≡|ν1⟩⊗|ν2⟩⊗· · · ⊗|νN⟩
for the corresponding N-particle state.
One basic assumption in the following is now that these N-particle states always
form a complete and orthonormalized basis, even if the particles interact with each
other. Then any possible N-particle state |N . . .⟩may be built from these states:
|N . . .⟩=

ν1...νN
|ν1 . . . νN⟩⟨ν1 . . . νN|N . . .⟩,
since the states |ν1 . . . νN⟩form a complete basis, i.e.,

ν1...νN
|ν1 . . . νN⟩⟨ν1 . . . νN| = 1 ,
and are orthonormalized, i.e.,
⟨ν1 . . . νN|ν1
′ . . . νN
′⟩= ⟨ν1|ν1
′⟩· · · ⟨νN|νN
′⟩.
Here we shall also allow for improper Hilbert vectors, where integrals occur instead
of sums.
This framework is generally unnecessary, however, for identical particles. For
indistinguishable particles, we cannot state which is the ﬁrst or which is the last,
because the interchange of two particles does not change the expectation value of
an arbitrary observable—otherwise the particles were not identical. Since we shall
now occupy ourselves with such indistinguishable particles, it is clear that we should
only have superpositions of states with an exchange symmetry: if the order of the
particles changes, at most the phase factor of the states can change.
5.3.2
Exchange Symmetry
Let the transposition operator Pkl = Plk exchange the particles labelled k and l:
Pkl | . . . νk . . . νl . . .⟩= | . . . νl . . . νk . . .⟩.
Since Pkl2 leads back to the old state, the operator Pkl has the eigenvalues ±1.
Its eigenstates for the particles k and l are said to be symmetric (pkl = +1) or anti-
symmetric (pkl = −1). Let us now consider all N! different permutations P of an
N-particle state |ν1 . . . νN⟩. They can be built from products of pair-exchange opera-
tors Pkl, although not uniquely. The only thing that is ﬁxed is whether an even or an
odd number of pair exchanges is necessary. We speak of even and odd permutations
(see Fig. 5.9).

5.3 Many-Body Systems
435
Fig. 5.9 The 3! = 6 different permutations of three objects. The even permutations are the identity,
the cyclic, and the anti-cyclic permutations, the odd ones are the three transpositions. The last shows
three transpositions, even though it can also be understood as a single transposition with particle 2
remaining unchanged
Fig. 5.10 Representation of
PkmPlmPkmPkl = 1. As
pkm2 = 1, it is clear that
plm = plk for all k ̸= l ̸= m
For identical particles, the eigenvalues pkl have to be either all +1 or all −1,
because the exchange symmetry is a characteristic of the considered particles: they
form either symmetric or antisymmetric states. The state cannot have one exchange
symmetry in the pairs (k, l) and (k, m), but the other in the pair (l, m), as Fig.
5.10 shows. Therefore we may restrict ourselves to either completely symmetric or
completely antisymmetric states.
In the following, we label symmetric states with an s on the Dirac symbol and
anti-symmetric ones with an a:
| . . . νk . . . νl . . .⟩s = +| . . . νl . . . νk . . .⟩s ,
for all k and l ,
| . . . νk . . . νl . . .⟩a = −| . . . νl . . . νk . . .⟩a ,
for all k and l ,
or, with δP = +1 for even permutations and δP = −1 for odd,
P |ν1 . . . νN⟩s =
|ν1 . . . νN⟩s ,
P |ν1 . . . νN⟩a = δP |ν1 . . . νN⟩a .
Symmetric states describe bosons, and antisymmetric ones fermions.
Hence, two fermions cannot occupy the same one-particle state, because upon
transposition of the two particles, the many-body state has to change sign. We now
have the basic ingredient for the famous Pauli exclusion principle. For symmetric
states (bosons), this restriction does not exist. If nν gives the particle number in the
state |ν⟩, then for bosons, nν ∈{0, 1, 2, . . .} holds, while for fermions nν ∈{0, 1}.
The sum of all occupation numbers nν yields the total number N of particles, viz.,
N = 
ν nν.
The permutation operators P all have an inverse:
P P−1 = 1 = P−1P .
In addition, nothing changes if all bra- and all ket-vectors undergo the same permu-
tation,

436
5
Quantum Mechanics II
P†P = 1
=⇒
P† = P−1 .
Permutation operators are thus unitary.
All observables O of an N-particle system have to commute with permutations,
as long as we are dealing with identical particles:
O = P†OP
=⇒
[O, P] = 0 .
Therefore, no perturbation can alter the symmetry: O = P†
klPklO = P†
klOPkl
delivers s⟨ν1 . . . νN| O |ν′
1 . . . ν′
N⟩a = −s⟨ν1 . . . νN| O |ν′
1 . . . ν′
N⟩a = 0. In particular,
symmetric and antisymmetric states are orthogonal to each other, which follows by
inserting O = 1, and the symmetry does not change with time, because the Hamilton
operator is invariant under permutations.
5.3.3
Symmetric and Antisymmetric States
In order to form arbitrary many-body states |ν1 . . . νN⟩from symmetric and antisym-
metric states, we take the symmetrizing and anti-symmetrizing operators
S = 1
N!

P
P
and
A = 1
N!

P
δP P .
Here the sums run over all N! different permutations. The two expressions can be
proven together. For this we set
 = 1
N!

P
λ(P) P ,
with
 = S , λ(P) = 1
for bosons,
 = A , λ(P) = δP for fermions.
In particular, with
PklS |ν1 . . . νN⟩= S |ν1 . . . νN⟩,
PklA |ν1 . . . νN⟩= −A |ν1 . . . νN⟩,
we ﬁnd Pkl  = λ(Pkl)  and therefore also
P = λ(P)  = P .
It remains to show that  is idempotent, to be sure that it is a projection operator.
But now N! 2 = 
P λ(P) P = 
P λ2(P)  holds and 
P 1 = N!, so we
do indeed have 2 = . In addition,  is Hermitian, because P is unitary, λ(P) =
λ(P−1), and the sum over all P is equal to the sum over all P−1:
 = 2 = † ,
for  = S and  = A .

5.3 Many-Body Systems
437
TheoperatorisalinearcombinationoftheunitaryoperatorsP,andhenceitselfnot
unitary. Furthermore, although we have already found the projection operators S and
A , we must nevertheless also normalize the unknown symmetric and antisymmetric
states correctly. If nν gives the number of bosons in the one-particle state |ν⟩, then
we have
|ν1 . . . νN⟩s =

N!
n1! n2! . . . S |ν1 . . . νN⟩,
|ν1 . . . νN⟩a =
√
N!
A |ν1 . . . νN⟩.
For fermions, the last equation with † =  delivers
a⟨ν1 . . . νN|ν1 . . . νN⟩a = N! ⟨ν1 . . . νN| A |ν1 . . . νN⟩
=

P
δP ⟨ν1 . . . νN| P |ν1 . . . νN⟩.
ButhereonlyP = 1contributes—with⟨ν1 . . . νN|ν1′ . . . νN ′⟩= ⟨ν1|ν1′⟩. . . ⟨νN|νN ′⟩
and because for fermions all νi have to be different. Thus |ν1 . . . νN⟩a is normal-
ized correctly. In contrast, in the expression ⟨ν1 . . . νN| P |ν1 . . . νN⟩for bosons, the
n1! n2! . . . terms contribute a 1, for which P|ν1 . . . νN⟩is equal to |ν1 . . . νN⟩. This
implies
|ν1 . . . νN⟩s =
1
√
N!
1
√n1! n2! . . .

P
P |ν1 . . . νN⟩,
|ν1 . . . νN⟩a =
1
√
N!

P
δP P |ν1 . . . νN⟩,
where both sums run over all N! permutations. The ﬁrst sum has n1! n2! . . . equal
terms and can be summed up correspondingly:
|ν1 . . . νN⟩s =

n1! n2! . . .
N!

P ′
P ′|ν1 . . . νN⟩,
if we take only the permutations P ′ which lead to different states.
To compute matrix elements with †O = O = O, it is sufﬁcient to sym-
metrize only in the bra- or ket-vectors. But we then have to normalize correctly:
s⟨ν1 . . . νN| O |ν1
′ . . . νN
′⟩s =

N!
n1! n2! . . . ⟨ν1 . . . νN| O |ν1
′ . . . νN
′⟩s ,
a⟨ν1 . . . νN| O |ν1
′ . . . νN
′⟩a =
√
N! ⟨ν1 . . . νN| O |ν1
′ . . . νN
′⟩a .
Note that the completeness relation for the N-fermion system (hence considering
only antisymmetric states) with

438
5
Quantum Mechanics II
|ν1
′ . . . νN
′⟩a = 1
N!

ν1...νN
|ν1 . . . νN⟩aa⟨ν1 . . . νN|ν1
′ . . . νN
′⟩a
=

ν1<···<νN
|ν1 . . . νN⟩aa⟨ν1 . . . νN|ν1
′ . . . νN
′⟩a
may be written in two ways, viz.,

ν1...νN
|ν1 . . . νN⟩aa⟨ν1 . . . νN| = N! ,
or with far fewer terms

ν1<···<νN
|ν1 . . . νN⟩aa⟨ν1 . . . νN| = 1 .
In the real-space representation, the N-fermion state |ν1 . . . νN⟩a reads
⟨r1 . . . rN|ν1 . . . νN⟩a =
1
√
N!

P
δP ⟨r1 . . . rN|P|ν1 . . . νN⟩
=
1
√
N!

⟨r1|ν1⟩. . . ⟨rN|ν1⟩
...
...
...
⟨r1|νN⟩. . . ⟨rN|νN⟩

.
The last expression is called the (normalized) Slater determinant,
Calculations with symmetric or antisymmetric states can be greatly simpliﬁed
with creation and annihilation operators: the former increase the particle number by
one, while the latter lower them by one. Therefore, in the following we have the
particle vacuum |0⟩, one-particle states |ν⟩, and N-particle states |ν1 . . . νN⟩s and
|ν1 . . . νN⟩a for N ≥2. The set of their Hilbert spaces forms the Fock space. Here
states with different particle number N are orthogonal to each other. The Hilbert
vector |0⟩of the vacuum state should not be confused with the zero vector |o⟩, where
⟨0|0⟩= 1, but ⟨o|o⟩= 0.
We begin with fermions, because the states are easier to normalize than those of
bosons.
5.3.4
Creation and Annihilation Operators for Fermions
Let the operator A†
ν create a fermion in the one-particle state |ν⟩from the vacuum
|0⟩:
A†
ν|0⟩= |ν⟩.

5.3 Many-Body Systems
439
Let generally A†
ν make the state |ν1 . . . νNν⟩a from the N-fermion state |ν1 . . . νN⟩a,
if this is possible at all—the state |ν⟩has to be unoccupied previously, so that an
antisymmetric (N +1)-particle state can be constructed:
A†
ν|ν1 . . . νN⟩a =
|o⟩
if ν ∈{ν1 . . . νN} ,
|ν1 . . . νNν⟩a if ν /∈{ν1 . . . νN} .
Note the phase convention employed here: if ν is arranged differently, the state may
differ in sign. For example, |νν1 . . . νN⟩a requires another creation operator. This will
be discussed separately on p. 442. It follows that
|ν1 . . . νN⟩a = A†
νN · · · A†
ν1|0⟩,
and the antisymmetry requires
A†
ν A†
ν′ = −A†
ν′ A†
ν ,
in particular (A†
ν)2 = 0 (Pauli principle) .
States with different particle number should be orthogonal to each other. Therefore,
⟨0|A†
ν = ⟨o|
and
⟨ν′|A†
ν = ⟨ν′|ν⟩⟨0| .
For the operator Aν Hermitian conjugate to A†
ν, it follows that
AνAν′ = −Aν′Aν ,
a⟨ν1 . . . νN| = ⟨0|Aν1 · · · AνN ,
together with
Aν|0⟩= |o⟩,
and Aν|ν′⟩= |0⟩⟨ν|ν′⟩.
We thus have
Aν|ν1 . . . νN⟩a =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
|ν1 . . . νN−1⟩a
ifν = νN ,
−|ν1 . . . νN−2νN⟩a ifν = νN−1 ,
...
...
(−)N−1|ν2 . . . νN⟩a
ifν = ν1 ,
|o⟩
ifν /∈{ν1 . . . νN} .
The operator Aν thus removes a fermion from the state |ν⟩. Therefore, Aν is an
annihilation operator of the fermion in the state |ν⟩.
With these creation and annihilation operators, many-fermion states can be treated
very conveniently—also if the particle number does not change at all, e.g., if equally
many creation and annihilation operators occur in operator products. In particular,
there is no longer any need to pay attention to the antisymmetry. We merely follow
the calculation rules for the new operators:

440
5
Quantum Mechanics II
A†
νA†
ν′ + A†
ν′A†
ν = 0 = AνAν′ + Aν′Aν ,
AνA†
ν′ + A†
ν′Aν = ⟨ν|ν′⟩.
The ﬁrst two commutation laws have already been proven. In addition, we have
⟨0|AνA†
ν′|0⟩= ⟨ν|ν′⟩and ⟨0|A†
ν′ Aν|0⟩= 0. For more particles, we ﬁrst consider the
case ν ̸= ν′: AνA†
ν′ and A†
ν′Aν create one fermion in the state |ν′⟩and destroy one in the
state |ν⟩, but the new states have opposite sign, e.g., A†
νN AνN−1 changes from the state
|ν1 . . . νN−1⟩a to the state |ν1 . . . νN−2νN⟩a, but AνN−1A†
νN results in AνN−1|ν1 . . . νN⟩a =
−|ν1 . . . νN−2νN⟩a. Therefore, it only remains to show that AνA†
ν + A†
νAν = 1:
AνA†
ν|ν1 . . . νN⟩a =
|o⟩
ifν ∈{ν1 . . . νN} ,
|ν1 . . . νN⟩a
ifν /∈{ν1 . . . νN} ,
A†
νAν|ν1 . . . νN⟩a =
|ν1 . . . νN⟩a
ifν ∈{ν1 . . . νN} ,
|o⟩
ifν /∈{ν1 . . . νN} .
With this our claim is proven: simple anti-commutation relations are valid for fermion
ﬁeld operators.
An additional result is that the eigenvalue of
Nν = A†
νAν
gives the occupation number of the state |ν⟩, and hence that Nν is the occupation
number operator. For the particle number operator, we must therefore take
N =

ν
A†
νAν .
Its eigenvalue gives the total number of fermions.
5.3.5
Creation and Annihilation Operators for Bosons
So far we have associated each particle with a one-particle state. For bosons, such
a state may be repeated very often, because many of them can be in the same one-
particle state. As already shown on p. 437, the occupation numbers nν are important
forthenormalization.Aparticularlysuitablerepresentationforbosonsistheso-called
occupation-number representation. We ﬁx an order for the one-particle states |ν⟩
and then give only the occupation numbers nν, thus writing |n1 . . . nν . . .⟩with N =

ν nν. If unoccupied states also occur, we may leave those out of the representation.
Therefore, we order the one-particle states with respect to their energy (see p. 433).
We consider here only the (symmetric) boson states | . . . nν . . .⟩s.

5.3 Many-Body Systems
441
The boson creation operators B†
ν with the property
B†
ν| . . . nν . . .⟩s = | . . . nν+1 . . .⟩s c(nν+1) ,
whose normalization factors c(nν+1) remain unknown for the moment, have to
commute with each other so that a symmetric state is produced:
B†
νB†
ν′ −B†
ν′B†
ν = 0 .
For the Hermitian adjoint annihilation operator, we have
BνBν′ −Bν′Bν = 0 .
With s⟨. . . nν . . . |Bν| . . . nν′ . . .⟩∗
s = s⟨. . . nν′ . . . |B†
ν| . . . nν . . .⟩s = δnν ′,nν+1 c(nν+1),
we may infer
Bν| . . . nν
′ . . .⟩s = | . . . nν
′−1 . . .⟩s c∗(nν
′) .
Hence c(0) = 0 holds, because no particles can be destroyed in the vacuum:
Bν|0⟩= |o⟩.
For ν ̸= ν′, it follows that BνB†
ν′| . . . nν . . . nν′ . . .⟩s = B†
ν′Bν| . . . nν . . . nν′ . . .⟩s, so
both are equal to | . . . nν−1 . . . nν′ +1 . . .⟩s c∗(nν) c(nν′ +1), in contrast to the case
for ν = ν′, where
BνB†
ν| . . . nν . . .⟩s = | . . . nν . . .⟩s |c(nν+1)|2 ,
B†
νBν| . . . nν . . .⟩s = | . . . nν . . .⟩s |c(nν)|2 .
If we choose therefore c(nν) = √nν, so that |c(nν+1)|2 −|cν|2 = 1, then for boson
ﬁeld operators, the following simple commutator relations are valid:
BνB†
ν′ −B†
ν′Bν = ⟨ν|ν′⟩,
B†
νB†
ν′ −B†
ν′B†
ν = 0 = BνBν′ −Bν′Bν .
In addition then,
Nν = B†
νBν
is the occupation-number operator for the one-particle state |ν⟩and
N =

ν
B†
νBν

442
5
Quantum Mechanics II
is the operator for the total number of bosons. We also have the equation
|n1n2 . . .⟩s = (B1†)n1
√n1!
(B2†)n2
√n2! · · · |0⟩.
Theﬁeldoperators inSect. 4.2.8havethesameproperties—therewesought operators
N whose eigenvalues were the natural numbers n. And the ladder operators for
harmonic oscillations in Sect. 4.5.4 also had those properties.
5.3.6
General Properties of Creation and Annihilation
Operators
We now summarize the previous considerations. Except for the all important sign
in the commutation relations, the creation and annihilation operators for bosons and
fermions are very similar. Therefore, we now write, with the upper sign for bosons
and the lower sign for fermions,
[ν, †
ν′]∓= ⟨ν|ν′⟩
and
[ν, ν′]∓= 0 = [†
ν, †
ν′]∓.
With these ﬁeld operators, the many-body states |n1 . . .⟩s and |n1 . . .⟩a can be created
from the vacuum state |0⟩:
|n1 . . .⟩s,a =
∞

ν=1
(†
ν)nν
√nν! |0⟩.
For fermions, the occupation numbers nν are only equal to zero or one, because
for them (ν)2 = −(ν)2 vanishes, and hence nν! = 1. In addition, with (i) ≡
∞
k=i+1 nk, we have
†
νi | . . . ni . . .⟩s,a = (±)(i) | . . . ni+1 . . .⟩s,a
√1 ± ni ,
νi | . . . ni . . .⟩s,a = (±)(i) | . . . ni−1 . . .⟩s,a
√ni .
The other phase convention (i) = i
k=1nk, already mentioned in Sect. 5.3.4, is
often used. It seems simple, because then k only runs over a ﬁnite number of values,
but it is less convenient because the states of higher energy are all unoccupied and
usually only the states near the Fermi energy are important.
For a change of representation, viz.,
|μ⟩=

ν
|ν⟩⟨ν|μ⟩,

5.3 Many-Body Systems
443
new creation and annihilation operators are necessary. From
|μ⟩= †
μ|0⟩=

ν
†
ν|0⟩⟨ν|μ⟩,
it follows generally that
†
μ =

ν
⟨μ|ν⟩∗†
ν
⇐⇒
μ =

ν
⟨μ|ν⟩ν .
For example, we can go over to the real-space representation. Let †(r ) create a
particle at the position r and (r ) destroy one there. Then we have
†(r ) =

ν
⟨r |ν⟩∗†
ν ⇐⇒(r ) =

ν
⟨r |ν⟩ν ,
with ⟨r |ν⟩= ψν(r ). Conversely,
†
ν =

d3r ⟨ν|r ⟩∗†(r ) ⇐⇒ν =

d3r ⟨ν|r ⟩(r ) .
The commutation laws are transferred:
[μ, †
μ′]∓=

νν′
⟨μ|ν⟩⟨μ′|ν′⟩∗[ν, †
ν′]∓=

ν
⟨μ|ν⟩⟨ν|μ′⟩= ⟨μ|μ′⟩,
and correspondingly [μ, μ′]∓= 0. The particle number operator reads
N =

ν
†
νν =

ν

d3r d3r ′ ⟨ν|r ⟩∗⟨ν|r ′⟩†(r ) (r ′)
=

d3r d3r ′ ⟨r |r ′⟩†(r ) (r ′) =

d3r †(r ) (r ) .
Thus †(r ) (r ) is the particle density operator. Note that the expectation value of
the particle-number operator †
νν does not need to be an integer: if it is so in the
basis {|ν⟩}, then it will not generally be so in the basis {|μ⟩}.
5.3.7
The Two-Body System as an Example
Here there are only the permutations |ν1ν2⟩and |ν2ν1⟩:
|ν1ν2⟩s = |ν1ν2⟩+ |ν2ν1⟩
√2 (1 + ⟨ν1|ν2⟩)
and
|ν1ν2⟩a = |ν1ν2⟩−|ν2ν1⟩
√
2
.

444
5
Quantum Mechanics II
For the matrix elements of an arbitrary operator O and with
⟨ν1ν2|O|ν1
′ν2
′⟩= ⟨ν2ν1|O|ν2
′ν1
′⟩,
it follows generally that
s,a⟨ν1ν2|O|ν1
′ν2
′⟩s,a = ⟨ν1ν2|O|ν1′ν2′⟩± ⟨ν1ν2|O|ν2′ν1′⟩
√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′|ν2′⟩
.
We shall only be concerned with one-particle operators T and two-body operators
V . Here, for a one-particle operator,
⟨ν1ν2|T|ν1
′ν2
′⟩= ⟨ν1|T|ν1
′⟩⟨ν2|ν2
′⟩+ ⟨ν1|ν1
′⟩⟨ν2|T|ν2
′⟩.
Its bra- and ket-states are thus distinguished at most in one particle, otherwise the
matrix element vanishes. (For a two-body operator, two states are distinguished in at
most two particles.) For fermions, we must also have ν1 ̸= ν2 and ν1′ ̸= ν2′, otherwise
the parts cancel each other.
The last equation also follows if we build T up from bilinear products of cre-
ation and annihilation operators and take the matrix elements of T in the chosen
representation as expansion coefﬁcients:
T =

νν′
⟨ν|T|ν′⟩†
νν′ .
We have in particular,
s,a⟨ν1ν2|†
νν′|ν′
1ν′
2⟩s,a =
⟨0|ν1ν2†
νν′†
ν′
2†
ν′
1|0⟩
√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′|ν2′⟩,
and with this, the factor
ν1ν2†
ν = ν1 (⟨ν2|ν⟩± †
νν2) = ⟨ν2|ν⟩ν1 ± (⟨ν1|ν⟩± †
νν1) ν2 ,
and also the other factor ν′†
ν′
2†
ν′
1 in the expectation value of the adjoint of
ν1′ν2′†
ν′. With ⟨0|†
ν = ⟨o| and ν|0⟩= |o⟩, together with
⟨0|μ†
μ′|0⟩= ⟨μ|μ′⟩,
we thus ﬁnd
s,a⟨ν1ν2|†
νν′|ν1
′ν2
′⟩s,a = ⟨ν2|ν⟩⟨ν′|ν2′⟩⟨ν1|ν1′⟩± ⟨ν′|ν1′⟩⟨ν1|ν2′⟩
√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′|ν2′⟩
+ ⟨ν1|ν⟩⟨ν′|ν1′⟩⟨ν2|ν2′⟩± ⟨ν′|ν2′⟩⟨ν2|ν1′⟩
√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′|ν2′⟩
,

5.3 Many-Body Systems
445
which is what was to be shown.
Ifwenowconsideratwo-bodyoperator,e.g.,theinteraction V (r1, r2) = V (r2, r1),
then
s,a⟨ν1ν2|V |ν1
′ν2
′⟩s,a = ⟨ν1ν2|V |ν1′ν2′⟩± ⟨ν1ν2|V |ν2′ν1′⟩
√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′|ν2′⟩
.
With μ and μ′ from the same basis as ν and ν′, we may also write
V = 1
2

νμ ν′μ′
⟨νμ|V |ν′μ′⟩†
μ†
νν′μ′ ,
because if we use
ν1ν2†
μ†
ν = ν1 (⟨ν2|μ⟩± †
μν2) †
ν
= ⟨ν2|μ⟩(⟨ν1|ν⟩± †
νν1) ± (⟨ν1|μ⟩± †
μν1) (⟨ν2|ν⟩± †
νν2)
in the previous equation and its adjoint for νμ†
ν2†
ν1, along with ⟨0|† = ⟨o|
and |0⟩= |o⟩, then it follows that
s,a⟨ν1ν2|V |ν1
′ν2
′⟩s,a = ⟨ν1ν2|V |ν1′ν2′⟩+ ⟨ν2ν1|V |ν2′ν1′⟩
2√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′ν2′⟩
± ⟨ν1ν2|V |ν2′ν1′⟩+ ⟨ν2ν1|V |ν1′ν2′⟩
2√1 + ⟨ν1|ν2⟩√1 + ⟨ν1′ν2′⟩
,
and hence, with ⟨νμ|V |ν′μ′⟩= ⟨μν|V |μ′ν′⟩, the original equation.
The expectation value of the symmetry-independent ﬁrst terms ⟨ν1ν2|V |ν1ν2⟩is
called the direct term while that of the symmetry-dependent term ⟨ν1ν2|V |ν2ν1⟩is
known as the exchange term. The expansion of one- and two-body operators with
respect to products of creation and annihilation operators turns out to be useful for
all N-particle states, as we shall now show.
5.3.8
Representation of One-Particle Operators
One-particle operators such as the kinetic energy and the one-particle potential act
on the degrees of freedom of only one particle. Clearly,
T =

νν′
⟨ν|T|ν′⟩†
νν′ ,
where †
νν′ is the one-particle density operator. In the occupation-number repre-
sentation, it has the matrix elements

446
5
Quantum Mechanics II
Fig. 5.11 The two Feynman diagrams are to be read from bottom to top, i.e., from initial to ﬁnal
state(s). One-particle operators (T) act on one particle, two-body operators (V ) on two. Uninvolved
partners do not change their state and would appear here as simple straight arrows. Such diagrams
are useful for compositions of operations, similar to those in Figs. 5.7 (right) and 5.8
s,a⟨. . . ninj . . . |i
†j| . . . ni
′nj
′ . . .⟩s,a
= (±)(i)−(j)
ninj′ s,a⟨. . . ni −1nj . . . | . . . ni
′nj
′ −1 . . .⟩s,a .
We have in particular,
s,a⟨ν1ν2 . . . νN|T|ν1 ν2 . . . νN⟩s,a =
N

n=1
⟨νn|T|νn⟩,
s,a⟨ν1ν2 . . . νN|T|ν1
′ν2 . . . νN⟩s,a = ⟨ν1|T|ν1
′⟩

n1n1′ ,
for ν1 ̸= ν1
′ ,
s,a⟨ν1ν2 . . . νN|T|ν1
′ν2
′ . . . νN⟩s,a = 0 ,
forν1and ν2 /∈{ν1
′ν2
′} .
A one-particle operator can change the quantum numbers of at most one particle.
Therefore, its matrix elements do not depend on the symmetry of the many-body
state. Its expectation value is the sum of the expectation values of all occupied one-
particle states (see Fig. 5.11 left).
5.3.9
Representation of Two-Body Operators
Two-body operators like the interaction V between two particles can alter the quan-
tum numbers of two particles. Here we have
V = 1
2

νμ ν′μ′
⟨νμ|V |ν′μ′⟩†
μ†
νν′μ′ .
The expression
V =

ν≤μ,ν′≤μ′
1
√1 + ⟨ν|μ⟩√1 + ⟨ν′|μ′⟩
s,a⟨νμ|V |ν′μ′⟩s,a †
μ†
νν′μ′

5.3 Many-Body Systems
447
is equivalent, where, as in Sect. 5.3.7,
s,a⟨νμ|V |ν′μ′⟩s,a = ⟨νμ|V |ν′μ′⟩± ⟨νμ|V |μ′ν′⟩
√1 + ⟨ν|μ⟩√1 + ⟨ν′|μ′⟩
.
With the commutation behavior of the annihilation operators, we obtain
(⟨νμ|V |ν′μ′⟩±⟨νμ|V |μ′ν′⟩) †
μ†
νν′μ′
= (⟨νμ|V |μ′ν′⟩±⟨νμ|V |ν′μ′⟩) †
μ†
νμ′ν′ ,
and in the sum ν′ ≤μ′, we may swap these two indices (with μ′ ≤ν′) without any
consequences. Instead of the claimed equation, we then have
V =
1
2

ν≤μ,ν′μ′
⟨νμ|V |ν′μ′⟩± ⟨νμ|V |μ′ν′⟩
1 + ⟨ν|μ⟩
†
μ†
νν′μ′
=
1
4

νμ,ν′μ′
(⟨νμ|V |ν′μ′⟩± ⟨νμ|V |μ′ν′⟩) †
μ†
νν′μ′
=
1
4

νμ,ν′μ′
⟨νμ|V |ν′μ′⟩†
μ†
ν(ν′μ′ ± μ′ν′) ,
where the summation indices have been renamed at the end. As above, the upper
sign holds for bosons, the lower one for fermions. Therefore, ±μ′ν′ = ν′μ′.
Thus the two expressions are indeed equal.
The expectation value of a two-body operator consists of the direct and exchange
terms. It depends on the symmetry of the many-body state and, according to
Sect. 5.3.6, this yields
s,a⟨n1 . . . |†
μ†
νν′μ′|n1 . . .⟩s,a
=

i̸=j
ninj (⟨νi|ν⟩⟨νj|μ⟩± ⟨νj|ν⟩⟨νi|μ⟩) ⟨ν′|νi⟩⟨μ′|νj⟩
+

i
ni(ni−1) ⟨νi|ν⟩⟨νi|μ⟩⟨ν′|νi⟩⟨μ′|νi⟩.
Then quite generally,
s,a⟨n1 . . . |V |n1 . . .⟩s,a = 1
2

i̸=j
ninj (⟨νiνj|V |νiνj⟩± ⟨νjνi|V |νiνj⟩)
+ 1
2

i
ni(ni−1) ⟨νiνi|V |νiνi⟩.
For fermions, the second sum does not contribute—none of the states is doubly
occupied. Therefore, the result may also be reformulated as

448
5
Quantum Mechanics II
s,a⟨ν1 . . . νN|V |ν1 . . . νN⟩s,a = 1
2
N

n̸=m
⟨νnνm|V |νnνm⟩± ⟨νnνm|V |νmνn⟩
1 + ⟨νn|νm⟩
= 1
2
N

n̸=m
s,a⟨νnνm|V |νnνm⟩s,a ,
where we also use p. 444. For the expectation value, it thus follows that
s,a⟨ν1 . . . νN|V |ν1 . . . νN⟩s,a =
N

n<m
s,a⟨νnνm|V |νnνm⟩s,a .
Apart from this, we must consider the off-diagonal matrix elements of V . A two-body
operator can alter the quantum numbers of at most two particles (see Fig. 5.11 right).
For ν1 ̸= ν1′, it follows that (compare with the result for one-particle operators in the
last section)
s,a⟨ν1ν2 . . . νN|V |ν1
′ν2 . . . νN⟩s,a
=
N

n=2
s,a⟨ν1νn|V |ν1
′νn⟩s,a

n1
1 + ⟨ν1|νn⟩
n1′
1 + ⟨νn|ν1′⟩,
and for ν1 and ν2 /∈{ν1′ν2′},
s,a⟨ν1ν2ν3 . . . νN|V |ν1
′ν2
′ν3 . . . νN⟩s,a
= s,a⟨ν1ν2|V |ν1
′ν2
′⟩s,a

n1 (n2−⟨ν1|ν2⟩)
1 + ⟨ν1|ν2⟩
n1′ (n2′−⟨ν1′|ν2′⟩)
1 + ⟨ν1′|ν2′⟩
.
As before the particle numbers ni refer to the bra-vector and the particle numbers
ni′ to the ket-vector. If the two vectors differ in more than two particles, the matrix
element is zero.
5.3.10
Time Dependence
So far all our considerations of many-body systems are valid for a ﬁxed time. Now we
ask how the creation and annihilation operators behave under exchange at different
times. We assume that the time-dependence is determined by a Hermitian Hamilton
operator consisting only of one- and two-body operators, which neither depends on
time explicitly nor changes the particle number:
H =

νν′
⟨ν|T|ν′⟩†
νν′ + 1
2

νμ ν′μ′
⟨νμ|V |ν′μ′⟩†
μ†
νν′μ′ .

5.3 Many-Body Systems
449
Here, in addition to the kinetic energy, T may also include other one-particle opera-
tors. We shall return to this shortly.
The Schrödinger picture is less suitable for ﬁeld operators than the Heisenberg
picture (and the Dirac picture), because in ﬁeld theory, we also trace the states back
to operators, and acting on the vacuum, which does not depend on time. Therefore,
we now transfer the relation AH(t) = U †(t) AH(0) U(t) with U(t) = exp(−iHt/ℏ)
from Sect. 4.4.2 to ﬁeld theory (without reference to the Heisenberg picture):
(t) = U †(t) (0) U(t)
⇐⇒
†(t) = U †(t) †(0) U(t) .
We thus take over the equations valid for observables and apply them to ﬁeld opera-
tors. Using ˙U = −iHU/ℏ= −iUH/ℏ, this yields
d
dt = i
ℏ[H, ]
⇐⇒
d†
dt
= i
ℏ[H, †] .
With [AB, C] = A[B, C]∓−[C, A]∓B, we obtain [†
νν′, κ] = −⟨κ|ν⟩ν′, and
also
[†
μ†
νν′μ′, κ] = [†
μ†
ν, κ] ν′μ′ ,
with
[†
μ†
ν, κ] = ∓⟨κ|ν⟩†
μ −⟨κ|μ⟩†
ν .
For bosons and also for fermions, this leads to
[†
νν′, κ] = −⟨κ|ν⟩ν′ ,
[†
μ†
νν′μ′, κ] = −⟨κ|μ⟩†
νν′μ′ −⟨κ|ν⟩†
μμ′ν′ .
Therefore, for both sorts of particles, the Heisenberg equation with the chosen Hamil-
ton operator can be reformulated as
iℏdκ
dt
=

ν
⟨κ|T|ν⟩ν +

νν′μ′
⟨νκ|V |ν′μ′⟩†
νν′μ′ ,
if we use ⟨νμ|V |ν′μ′⟩= ⟨μν|V |μ′ν′⟩.
Later on we shall introduce the average two-body interaction V , a one-particle
operator which can be combined with T to give H0 = T +V . Subtracting from V , this
yields the residual interaction V −V as a two-body operator, which is often small
and unimportant. If we neglect it and take the one-particle basis which diagonalizes
H0, we obtain
iℏdν
dt
≈⟨ν|H0|ν⟩ν ,
e.g., iℏ∂(t, r )
∂t
≈⟨r |H0|r ⟩(t, r ) .

450
5
Quantum Mechanics II
This is similar to the usual Schrödinger equation. However, ν is not a state, but
an operator, and a matrix element of the Hamilton operator H0 is to be taken. We
discuss this further in the next section.
Before that we shall set ⟨ν|H0|ν⟩= ℏων, whence
ν(t) = ν(0) exp(−iωνt)
and
†
ν(t) = †
ν(0) exp(+iωνt) .
In the last paragraph the two-body interaction was neglected. There the equations are
valid (without neglecting this) in the Dirac picture, which includes the time depen-
dence of the states due to V . So far H0 should be sufﬁciently simple for mathematical
treatment, but now we distinguish between H0 and V through physical properties,
namely, whether one- or two-body operators are involved.
5.3.11
Wave–Particle Dualism
In this chapter we have always started from (several) particles, but we would also
have arrived at creation and annihilation operators if we had quantized the wave
picture, i.e., if we had taken each ﬁeld strength as a Hermitian operator (e.g., the
electromagnetic ﬁeld, as will be shown in Sect. 5.5). In fact, such a ﬁeld quantization
would have taken us beyond the usual scope of a course on quantum mechanics,
but otherwise would also have had many advantages. Note that the term second
quantization instead of ﬁeld quantization is misleading: we quantize either once in
the ﬁeld picture or once in the particle picture, with several particles.
So far we have investigated the laws governing the behavior of particles and looked
atthemasrepresentativesofaclassofidenticalparticles.Forsingleparticles,thereare
only statements about probabilities. Therefore, we always take a very large number
N of equal particles and use them to repeat the same experiment. The more often the
same particle attribute appears, the higher its probability. But this probability now
shows interference effects and therefore requires a wave theory. We assume for these
considerations that the particles do not act on each other, which we also had to do
when deriving the generalized Schrödinger equation in the last section.
For N ≫1, it is of no importance whether N is a natural number: even a (small)
uncertainty in the particle number might occur. It is just then that a sharp probability
statement (appropriate for the wave picture) holds for single particles! On the other
hand, if an uncertainty in the wave quantities (phase) is not important, then sharp
statements in the particle picture are possible. This has already been pointed out for
the uncertainty relation between particle number and phase (Sect. 4.2.9).
Considering the relative frequency, it is easily overlooked how the particle picture
is contained in such a seemingly unimportant constraint: N had to be a natural
number—other values were meaningless. This granularity is foreign to the wave
picture. Field intensities and wave functions are appropriate for there, but classically
these distributions can be arbitrarily normalized. Clearly, the wave picture then has to
be modiﬁed in such a way that the arbitrary values for N become restricted to natural

5.3 Many-Body Systems
451
numbers by quantum conditions—and the observables of the wave picture (ﬁeld
strengths and intensities) have to become operators. We have become acquainted
with the commutation laws for the ﬁeld operators in this chapter.
As long as wave functions are taken as classical ﬁeld quantities, not as probability
amplitudes normalized to 1, the Schrödinger equation is not an equation of quantum
mechanics, but of classical physics.
5.3.12
Summary: Many-Body Systems
In the quantum mechanics of many-particle problems, bosons and fermions behave
differently: bosons form symmetric states and fermions antisymmetric states. Such
states with special exchange symmetry are easily treated by introducing creation and
annihilation operators †
ν and ν which satisfy the commutation laws (upper sign
for bosons, lower for fermions):
ν†
ν′ ∓†
ν′ν = ⟨ν|ν′⟩
and
νν′ ∓ν′ν = 0 .
Both are called ﬁeld operators. They also arise when quantizing the wave pic-
ture. Here it is best to work in the Heisenberg or Dirac picture, where ν(t) =
ν(0) exp(−iωνt).
It is also important to distinguish between one- and two-body operators:
T =

νν′
⟨ν|T|ν′⟩†
νν′
and
V = 1
2

νμ ν′μ′
⟨νμ|V |ν′μ′⟩†
μ†
νν′μ′ .
Here T can also contain “average one-particle potentials”.
5.4
Fermions
5.4.1
Fermi Gas in the Ground State
As a ﬁrst application, we shall evaluate the one- and two-body densities
ρ(r )
= a⟨ν1 . . . νN|
†(r ) (r )
|ν1 . . . νN⟩a ,
ρ(r, r ′) = a⟨ν1 . . . νN| †(r ′) †(r ) (r ) (r ′) |ν1 . . . νN⟩a ,
of a Fermi gas in the ground state, i.e., the probability density for a particle at the
position r or for two particles at the positions r and r ′. Actually, in a Fermi gas, the
individual fermions do not interact with each other, but the antisymmetry nevertheless
correlatesthem.Sinceonlyonefermionmaybeinanyone-particlestate,intheground

452
5
Quantum Mechanics II
state of the Fermi gas, the particles are distributed over the various states with as low
a total energy as possible.
The following calculation would be more complicated if we enclosed the fermions
inacubeofvolumea3,asinSect.4.5.3.Itiseasierwithperiodicboundaryconditions,
i.e., with ψ(x, y, z) = ψ(x+a, y, z) = ψ(x, y+a, z) = ψ(x, y, z+a). They lead to
the eigenfunctions
ψν(r) = exp(ikν · r )
√
V
,
if we leave out the spin functions χν(s ). The wave vector kν and the energy Eν are
then determined by the constraints
kν = 2π
a nν
and
Eν = (ℏkν)2
2m
,
where each Cartesian component of nν has to be an integer (0, ±1, . . .). With the
cube and impenetrable walls (see p. 355), each component of nν takes only the values
1, 2, 3, . . .. However, then for kν ∝nν, there is a factor π/a instead of 2π/a. More-
over, for periodic boundary conditions, all states apart from those with a vanishing
nν component are more strongly degenerate by the factor 23 = 8, but lie further apart
from each other by the same factor than for the cube. The density of states is the
same in both cases. But the boundary conditions are not important for our problem.
In particular, as in Sect. 4.5.3, for the number of states with E ≤EF, a factor of 2
accounts for the two spin states of spin-1/2 particles and we have
N ≈2
V
6π2 kF
3 .
With the above-mentioned wave functions, we shall now calculate the one- and
two-particle densities ρ(r ) and ρ(r, r ′). Generally, we have
⟨a⟨ν1 . . . νN| †(r ) (r ) |ν1 . . . νN⟩a =
N

n=1
|ψn(r )|2 ,
and using |χn(s )|2 = 1 = |χm(s )|2,
a⟨ν1 . . . νN| †(r ′) †(r ) (r ) (r ′) |ν1 . . . νN⟩a
= 1
2
N

n,m=1

|ψn(r )|2 |ψm(r ′)|2 −ψn
∗(r ) ψm
∗(r ′) ψn(r ′) ψm(r )|⟨sn|sm⟩|2
+|ψm(r )|2 |ψn(r ′)|2 −ψm
∗(r ) ψn
∗(r ′) ψm(r ′) ψn(r )|⟨sn|sm⟩|2
.

5.4 Fermions
453
Here we see immediately the advantage of periodic boundary conditions: the one-
particle density is then constant and given by
ρ(r ) = N
V = kF
3
3π2 = ρ0 ,
and the two-particle density simpliﬁes to
ρ(r, r ′) = 1
V 2
N

n,m=1

1 −1
2 exp{i (kn −km) · (r ′ −r )} |⟨sn|sm⟩|2
−1
2 exp{i (kn −km) · (r −r ′)} |⟨sn|sm⟩|2
.
The double sum can be approximated by a double integral, integrating over d3n =
2V/(2π)3 d3k and including a factor of 2 for spin-1/2 particles. For the latter, we
have on average |⟨sn|sm⟩|2 = 1/2. Using this, the double integral factorizes:
ρ(r, r ′) ≈N 2
V 2 −
1
2V 2

 2V
(2π)3

d3k exp{ik · (r −r ′)}
2
.
Here we have to integrate over all directions of k and the modulus from 0 to kF =
(3π2ρ0)1/3. According to p. 409, we obtain
	
dk exp(ik · a ) = 4π sin ka/(ka), and
consequently, using F1(x) = x−1 sin x −cos x from p. 400,
ρ(r, r ′) = ρ0
2 
1 −1
2

3F1(x)
x2
2
with x = kF|r −r ′| .
Of course, the factor of 1/2 comes from the two spin states.
The antisymmetry thus correlates fermions of equal spin. They tend to avoid each
other, each fermion being surrounded by an exchange hole (see Fig. 5.12). This
anti-correlation shows up only for short distances.
Therefore, the boundary conditions are not important for this consideration, and
the function (3F1/x2)2 may be approximated by a Gauss function.
Fig. 5.12 Exchange hole
around a fermion. Two-body
density as a function of
x = kF|r −r ′| (continuous
red) and the approximation
1 −1
2 exp(−x2/5) (dashed
blue)

454
5
Quantum Mechanics II
5.4.2
Hartree–Fock Equations
Each N-fermion state can be expanded in the basis {|ν1 . . . νN⟩a}. We shall use this
freedom in the choice of one-particle states {|ν⟩} to diagonalize the Hamilton operator
H =

νν′
⟨ν|H0|ν′⟩†
ν ν′ + 1
2

νμν′μ′
⟨νμ|V |ν′μ′⟩†
μ†
νν′μ′
as well as possible with a single state |ν1 . . . νN⟩a. Here, in addition to the kinetic
energy, H0 also contains the potential energy which originates from external forces,
while V describes the coupling of the single fermions among each other.
The diagonal elements of the Hamilton operator H = H0 + V just mentioned,
viz.,
a⟨ν1 . . . νN|H|ν1 . . . νN⟩a =
N

n=1
⟨νn|H0|νn⟩+
N

n<m
a⟨νnνm|V |νnνm⟩a ,
supply the energy eigenvalues in zeroth order perturbation theory. Of the remaining
matrix elements a⟨ν1 . . . νN|H|ν1′ . . . νN ′⟩a, all of those whose bra- and ket-states
differ in more than two particles will in fact vanish, but generally neither
a⟨ν1ν2ν3 . . . νN|H| ν1′ν2′ν3 . . . νN⟩a = a⟨ν1ν2|V |ν1′ν2′⟩a , with {ν1, ν2} /∈{ν1′, ν2′} ,
nor the matrix elements which are not diagonal with respect to just one particle will
vanish. For example,
a⟨ν1ν2 . . . νN|H|ν1
′ν2 . . . νN⟩a = ⟨ν1|H0|ν1
′⟩+
N

n=1
a⟨ν1νn|V |ν1
′νn⟩a ,
if ν1 ̸= ν1′. At least this second kind of non-diagonal element vanishes if we deter-
mine the basis {|ν⟩} (one-particle states) from the Hartree–Fock equations:
⟨ν|H0|ν′⟩+
N

n=1
a⟨ννn|V |ν′νn⟩a = eν ⟨ν|ν′⟩.
We shall thus derive the one-particle states |ν⟩and the one-particle energies eν from
the Hartree–Fock equations and take approximately |ν1 . . . νN⟩a for the N-fermion
states. With this in fact generally the non-diagonal elements of H will not all vanish
yet, but a better approximation can be obtained using just a superposition of several
states—we shall come back to this in the next section.
In the Hartree–Fock equations, the test particle is coupled to the remaining
fermions. The sum of the one-particle energies eν of the occupied states counts this
coupling twice, so this is not equal to the ground-state energy E0 of the N fermions:

5.4 Fermions
455
E0 = a⟨ν1 . . . νN|H|ν1 . . . νN⟩a =
N

n=1
⟨νn|H0|νn⟩+ 1
2
N

n,m=1
a⟨νnνm|V |νnνm⟩a
=
N

n=1
en −1
2
N

n,m=1
a⟨νnνm|V |νnνm⟩a .
But we have Koopman’s theorem: the last particle has the energy
eN = E0(N) −E0(N −1) ,
since
eN = ⟨νN|H0|νN⟩+
N−1

n=1
a⟨νNνn|V |νNνn⟩a .
We now consider the Hartree–Fock equations in the real-space representation, where
we restrict ourselves to local Wigner forces. Then the following abbreviations are
useful:
VH(r) ≡
N

n=1

d3r ′ ψn
∗(r ′) V (r, r′) ψn(r′)
(Hartree term) ,
VF(r, r ′) ≡
N ′

n=1
ψn
∗(r′) V (r, r ′) ψn(r )
(Fock term) .
Note that only the states |νn⟩with the same spin orientation as the unknown solution
(indicated by N ′) contribute to the Fock term. For spin-independent operators H0 and
V , we have
 
H0 + VH(r )
!
ψν(r ) −

d3r ′ VF(r, r ′) ψν(r ′) = ψν(r ) eν .
Here the direct term (Hartree term) and the exchange term (Fock term) contain the
wave functions to be determined. The Hartree–Fock equations can be solved only
iteratively. We ﬁrst use a suitable ansatz for VH and VF, solve the eigenvalue equation,
and then use the eigenfunctions found in this way to get a better approximation for VH
and VF, and so on. This method has to be repeated until the solutions of the Hartree–
Fock equations do not change within given limits (until they are self-consistent).
The exchange term is non-local and impedes the calculations. If we neglect it,
we have the simple Hartree equations, but their solutions are not orthogonal to each
other, because they belong to a wrong Hamilton operator [7]. The exchange hole is
less effective for repulsive (Coulomb) forces than for attractive (nuclear) forces. The
Hartree equations are therefore essentially more appropriate for atomic physics than
for nucleus physics.

456
5
Quantum Mechanics II
5.4.3
Rest Interaction and Pair Force
The Hartree–Fock method thus delivers the best one-particle states—it diagonalizes
the Hamilton operator as well as possible with a single antisymmetric product of such
states. However, there are still off-diagonal elements originating from the two-body
coupling. In fact, only
HHF =

νν′

⟨ν| H0 |ν′⟩+
N

n=1
a⟨ννn| V |ν′νn⟩a

†
ν ν′
becomes diagonalized—there remains a residual interaction
VR = H −HHF .
In order to include this term, we have to superpose several product states whose
components differ from each other by the quantum numbers of at last two particles.
This conﬁguration mixture delivers a further correlation, in addition to the symmetry
condition, which is related to the exchange hole.
We thus ask which parts of the coupling V are already well approximated by a
one-particle operator and which remain as the residual interaction. Clearly, the parts
with longer range change only weakly with the distance from the remaining partners.
These can be well described by an average one-particle potential. Consequently, the
residual interaction describes the parts of short range.
In order to study those effects, we could investigate the limit of a delta force
∝δ(r −r ′). But since the matrix elements
⟨νμ| δ |ν′μ′⟩=

d3r ψν
∗(r ) ψμ
∗(r ) ψν′(r ) ψμ′(r )
very often differ from zero, the corresponding problem is still too involved. We take
the so-called pair force, which, to exaggerate somewhat, has even shorter range: it
acts only between fermions in mutually time-reversed states (and which are therefore
equally probable everywhere). For a Hamilton operator with time-reversal symmetry,
they have the same energy according to Kramers’ theorem (see p. 314).
Thus as residual interaction we take
Vpair =

νν′
a⟨νν | V |ν′ν′⟩a ν
† †
ν ν′ ν′ ,
withoutsummationoverν andν′.Thetwostates|ν⟩and|ν ⟩haveoppositemomentum
and angular momentum. Therefore, it is often assumed that ν and ν differ only in
the sign (ν = −ν > 0) and we then require ν, ν′ > 0 for the sum. Since the matrix
elements of the delta function are always positive, we shall also assume that the

5.4 Fermions
457
matrix elements of the pair force all have the same sign, which, for an attractive pair
force, will be negative.
For such a pair force, close to the Hartree–Fock ground state, it is particularly
convenient for the energy if the fermion level is pairwise occupied or empty. If |ν⟩
is occupied, then so is |ν⟩. If the ground state according to the Hartree–Fock method
(with even particle number) is of the form |ν1ν1 . . . νN/2νN/2⟩a, it now also contains
(superposed) states which differ by pairs νν. These have neither momentum nor
angular momentum. In excited states, these pairs can also break up.
5.4.4
Quasi-Particles in the BCS Formalism
Despite all the simpliﬁcations which result from the pair force (compared to the
actually expected residual interaction), the eigenvalue problem is still too difﬁcult.
Bardeen, Cooper, and Schrieffer proposed an approximating ansatz for the ground
state which allows the pair force to be diagonalized rather easily:
|BCS⟩=

ν>0
(uν + vν †
ν†
ν)|0⟩,
where instead of uν and vν we could also take cos ϕν and sin ϕν (see p. 310):
uν
2 + vν
2 = 1 ,
uν = uν
∗≥0 ,
vν = vν
∗.
The occupation probabilities of the states |ν⟩and |ν⟩are thus equal and easy to
remember. With probability u2
ν, they are unoccupied (empty), and with probability
v2
ν, they are occupied (ﬁlled). However, the ansatz has the disadvantage that the
particle number is not sharp. In fact, we require the expectation value to deliver the
correct particle number n, i.e.,
⟨BCS| N |BCS⟩=

ν>0
2vν
2 = n ,
but the particle number is not sharp, as will be shown later:
(N)2 = ⟨BCS| N 2 |BCS⟩−⟨BCS| N |BCS⟩2 = 4

ν>0
uν
2vν
2 .
In fact, for most terms, we ﬁnd either uν2 = 0 or vν2 = 0, and hence (N)2 ≪
4 
ν vν2 = 2n, but this uncertainty is nevertheless irritating for the smaller particle
numbers—hence particularly in atomic and nuclear physics, but less in solid-state
physics. Clearly, in this approximation, we cannot describe any properties varying
quickly with the particle number, only the slowly varying ones.

458
5
Quantum Mechanics II
The state |BCS⟩may be taken as a quasi-vacuum. It has neither momentum nor
angular momentum, but energy. Moreover, its particle number is not zero. Acting
on this quasi-vacuum are quasi-particle operators (†)
ν , which again obey the Fermi
exchange rule. Carrying out the Bogoliubov transformation,
ν ≡uν ν −vν †
ν
⇐⇒
†
ν = uν †
ν −vν ν ,
the commutation rule for the operators  implies
ν †
ν′ + †
ν′ ν = ⟨ν|ν′⟩
and
ν ν′ + ν′ ν = 0 .
Whether a particle is annihilated in the state |ν⟩or created in the state |ν ⟩makes no
difference to the momentum and angular momentum—only for the particle number
and the energy. For this reason, the Bogoliubov transformation is not as peculiar as
it may appear at ﬁrst sight.
According to p. 314, for fermions, we have |ν⟩= −|ν⟩. This yields
ν = uν ν + vν†
ν
⇐⇒
†
ν = uν †
ν + vνν ,
ν = uν ν + vν †
ν
and
ν = uν ν −vν †
ν .
Now we may deduce that
ν |BCS⟩= |o⟩
and
ν
†|BCS⟩= †
ν

ν′(̸=ν)>0
(uν′ + vν′ †
ν′ ν ′†)|0⟩,
as well as ν †
ν′|BCS⟩= |BCS⟩⟨ν|ν′⟩. We can see that the particle number operator

ν>0(†
ν ν + †
ν ν) is generally no longer diagonal by considering
N =

ν>0
2vν
2 + (uν
2 −vν
2)(†
ν ν + †
ν ν) + 2uνvν (†
ν †
ν + ν ν) .
With this result, we can also prove the above-mentioned expression for (N)2.
5.4.5
Hartree–Fock–Bogoliubov Equations
Using the Bogoliubov transformation, we can go over from particle to quasi-particle
operators and generalize the Hartree–Fock equations in such a way that, within the
framework of the BCS ansatz, pair correlations are also included. Then the quasi-
particle energies eν and the occupation probabilities vν2 = 1 −uν2 of the ground
state are conserved.
AsintheHartree–Fockmethod,wealsodiagonalizetheone-particlepartshere,but
now in the quasi-particle formalism. Since the particle number is no longer sharp, we

5.4 Fermions
459
want to obtain at least its mean value correctly, and therefore introduce the chemical
potential μ as a Lagrangian parameter (see p. 560). TheHartree–Fock–Bogoliubov
equations read
⟨ν| H0 + V −μ N |ν′⟩= ⟨ν|ν′⟩eν .
Here ν and ν′ are either both positive or both negative, time-reversed states being
orthogonal to each other in any case. The Hamilton operator should be Hermitian
and invariant under time reversal. Then according to p. 314, we have ⟨ν ′| H0 |ν ⟩=
⟨ν| H0 |ν ′⟩∗= ⟨ν| H0 |ν′⟩, so
H0 =

νν′>0
⟨ν| H0 |ν′⟩(†
ν ν′ + †
ν ′ ν) + ⟨ν | H0 |ν′⟩†
ν ν′ + ⟨ν| H0 |ν ′⟩†
ν ν ′ .
In order to determine eν, we need only the part with the factors
†
ν ν′ + †
ν ′ ν = 2vν
2 ⟨ν|ν′⟩+
 
uνuν′−vνvν′! 
†
ν ν ′+†
ν ′ ν
!
+
 
uνvν′+vνuν′! 
†
ν †
ν ′+ν ν′!
.
The remaining terms of H0 do not contribute to the matrix element above, because
they have opposite signs of ν and ν′.
Only the terms with pairwise positive or negative νμν′μ′ are important for V =

ν<μ,ν′<μ′ a⟨νμ| V |ν′μ′⟩a †
μ †
ν ν′ μ′, viz.,
+ 1
4

νμν′μ′>0
a⟨νμ| V |ν′μ′⟩a (†
μ †
ν ν′ μ′ + †
μ ′ †
ν ′ ν μ)
+ 1
2

νμν′μ′>0
a⟨νμ| V |ν′μ′⟩a (†
μ †
ν ν′ μ′ + †
μ ′ †
ν ′ ν μ)
= +

νμ>0
(a⟨νμ| V |νμ⟩a + a⟨νμ| V |νμ⟩a) vν
2 vμ
2
+

νμ>0
a⟨νν| V |μμ⟩a uν vν uμ vμ
+

νμν′>0
(a⟨νμ| V |ν′μ⟩a + a⟨νμ| V |ν′μ⟩a) vμ
2
{+(uν uν′ −vν vν′)(†
ν ν′ + †
ν ′ ν)
+(uν vν′ + vν uν′)(†
ν †
ν ′ + ν ν′)}
+

νμν′>0
a⟨νν′| V |μμ⟩a vμ uμ
{−(uν vν′ + vν uν′)(†
ν ν′ + †
ν ′ ν)
+(uν uν′ −vν vν′)(†
ν †
ν ′ + ν ν′)} + · · · ,
where we have left out terms with 4 quasi-particle operators, because we do not
need them in the following. The quasi-particle representation of the particle-number

460
5
Quantum Mechanics II
operator N was already given at the end of the last section. With this we have all the
terms necessary for the Hartree–Fock–Bogoliubov equations. In particular, with the
abbreviations
⟨ν|  |ν′⟩=

μ>0

a⟨νμ| V |ν′μ⟩a + a⟨νμ | V |ν′μ⟩a

vμ
2 ,
νν′ = −

μ>0
a⟨νν′| V |μμ⟩a uμ vμ ,
the expectation value of the energy in the ground state is
⟨BCS| H |BCS⟩=

ν>0
2vν
2 ⟨ν| H0 +  |ν⟩−uν vν νν .
New compared with the Hartree–Fock expression are the terms νν′, i.e., they are no
longer neglected in the Hartree–Fock–Bogoliubov method. However, in addition to
the one-particle energies, the occupation probabilities vν2 = 1 −uν2 must now also
be determined. They follow from the Hartree–Fock–Bogoliubov equations
eν ⟨ν|ν′⟩= (uν uν′−vν vν′) ⟨ν| H0+−μ |ν′⟩+ (uν vν′+vν uν′) νν′ ,
0 = (uν vν′+vν uν′) ⟨ν| H0+−μ |ν′⟩−(uν uν′−vν vν′) νν′ .
The states |ν⟩are required to diagonalize the operator H0 + , whose eigenvalues
εν + μ are the Hartree–Fock one-particle energies:
⟨ν| H0 +  |ν′⟩= ⟨ν|ν′⟩(εν + μ) .
In addition, we restrict ourselves to the pair force as the residual interaction and
assume an attractive pair force (see p. 456):
νν′ = ⟨ν|ν′⟩ν ,
with ν ≥0 .
Then the Hartree–Fock–Bogoliubov equations read
eν =+(uν2 −vν2) εν + 2uνvν ν ,
0 = −(uν2 −vν2) ν + 2uνvν εν .
With uν2 + vν2 = 1, we set uν →cos ϕν, vν →sin ϕν and make use of the prop-
erties of the trignonometric functions: uν2 −vν2 →cos(2ϕν), 2uνvν →sin(2ϕν).
The second Hartree–Fock–Bogoliubov equation then delivers cot(2ϕν) = εν/ν:
ϕν decreases from π/2 to 0 between εν ≪−ν and εν ≫ν. According to the
ﬁrst Hartree–Fock–Bogoliubov equation, the quasi-particle energies eν are never
negative, and with sin α = (1 + cot2 α)−1/2 and cos α = cot α · sin α, it follows that
(see Fig. 5.13)

5.4 Fermions
461
Fig. 5.13 Effects of the pair
force. Quasi-particle
energies eμ for equidistant
one-particle energies as a
function of the gap
parameter  (left) and
occupation probability of the
BCS ground state as a
function of εμ/ (right)
eν = +

εν2 + ν2 ,
uν =
eν + εν
2 eν
,
vν =
eν −εν
2 eν
.
For ν = 0, we do not ﬁnd pair effects, but the usual Hartree–Fock result: either
uν = 0, vν = 1, and eν = −εν or uν = 1, vν = 0, and eν = +εν. While the Hartree–
Fock one-particle energies εν, evaluated at the Fermi energy μ, can be positive or
negative, the Hartree–Fock–Bogoliubov eigenvalues eν are always positive.
Generally, the pair potential satisﬁes ν ̸= 0. Then the Fermi edge is not sharp,
and that alters the states close to it. Thus there, the quasi-particle energies eν =
(εν2 + ν2)1/2 are different from the Hartree–Fock energies εν. An energy gap ν
appears, and only above this gap are there quasi-particle levels. Note that the energy
gap corresponds to the rest energy mc2 in the expression E/c =

p2 + (mc)2 for the
energy of free particles according to special relativity theory (see p. 245).
The gap parameter ν with ν = −
μ>0 a⟨νν|V |μμ⟩a uμvμ has to satisfy the
so-called gap condition (or gap equation)
ν = −

μ>0
a⟨νν| V |μμ⟩a
μ
2eμ
.
It is mainly the terms with μ ≈νF that contribute to the sum, because uμvμ is
only different from zero close to the Fermi edge. In addition, the matrix elements
a⟨νν| V |μμ⟩a are particularly large for ν ≈μ, hence so is the gap parameter ν for
ν ≈νF. The pair interaction can thus only be felt close to the Fermi edge. As long as
we are only interested in states close to Fermi edge, we may use an average matrix
element
G ≡−a⟨νν| V |μμ⟩a ,
for ν ≈νF , otherwise zero .
Then the gap parameter ν no longer depends on the state |ν⟩, and the gap condition
simpliﬁes to
 = G
2

μ>0
1

εμ2 + 2 .

462
5
Quantum Mechanics II
In addition to the trivial solution  = 0, there is another if
G
2

μ>0
1
|εμ| > 1 .
The pair correlations grow stepwise with increasing pair force G, and hence every
perturbation theory fails.
5.4.6
Hole States
So far we have described the transition of a fermion from |ν⟩to |ν′⟩using the operator
†
ν′ν. Such “particle scattering” occurs for small excitation energies only close to
the Fermi edge, and in fact preferably with eν < eF and eν′ > eF. Here we assume
a unique Fermi edge—in atomic and nuclear physics (for non-deformed nuclei), we
take closed shells, otherwise at least an even fermion number so that the ground state
is not degenerate. We denote this “normal state” by |0 ⟩.
Removing a particle from the state |ν ⟩turns this normal state into a hole state
|ν−1⟩. It behaves with respect to momentum and angular momentum like the state |ν⟩.
Instead of particle scattering, we may thus also speak of particle–hole pair generation
|0⟩→|ν−1ν′⟩. Below the Fermi edge, we also use hole operators , and above the
Fermi edge, the particle operators  as before, with
†
ν |0⟩= |ν−1⟩,
ν†
ν′ + †
ν′ν = ⟨ν|ν′⟩,
νν′ = −ν′ν .
With †
ν = ν, whence ν = †
ν, †
ν = −ν, and ν = −†
ν, they barely differ
from the BCS quasi-particle operators. Here, for states below the Fermi edge, we
carry out a Bogoliubov transformation of all ﬁeld operators with uν = 0, vν = −1
(see Fig. 5.14).
5.4.7
Summary: Fermions
The treatment of many-body systems with fermion creation and annihilation opera-
tors was explained using the example of the Fermi gas. The best one-particle basis
derives from the Hartree–Fock equations. For pair forces, it is better to use quasi-
particles and the Hartree–Fock–Bogoliubov equations to introduce pair correlations.

5.5 Photons
463
Fig. 5.14 Feynman graphs
with hole states. Hole arrows
point downwards (time
reversal of the particle arrow
in Fig. 5.11). Upper row:
The four diagrams for a
one-particle operator (T),
viz., pair creation, pair
annihilation, hole scattering,
and the vacuum expectation
value ⟨0|T|0⟩. Lower row: A
selection of two-particle
operators, viz., particle–hole
and hole–hole scattering,
particle scattering with pair
creation, and the one-particle
potential
5.5
Photons
5.5.1
Preparation for the Quantization of Electromagnetic
Fields
The electromagnetic ﬁeld is described classically by the Maxwell equations. Accord-
ing to p. 215, for homogeneous non-conductors, they deliver wave equations for the
electric ﬁeld strength E and the magnetic ﬂux density B, and likewise for the scalar
potential  and the vector potential A. In the following, we restrict ourselves to
homogeneous and isotropic media, hence constant scalar ε and μ.
According to quantum theory, we have to alter our notion of waves to permit a
particle interpretation—radiation may exhibit interference effects, but it may also
be granular. This can be obtained only via uncertainties: the experimental quantities
have to be replaced by Hermitian operators with suitable commutation behavior.
For the wave function we prefer the four-potential instead of the ﬁeld strengths
E and B, because, from the relations ∂B/∂t = −∇× E and ∇· B = 0, we see that
their components are not independent of each other. These two equations are already
automatically satisﬁed with the ansatz E = −∂A/∂t −∇ and B = ∇× A. How-
ever, the potentials cannot be measured and also depend on the gauge, but then the
wave functions for electrons are not measurable and contain an arbitrary phase.
It is better to characterize free particles by their momentum (wave vector) than by
their position. Therefore, we now consider the Fourier transform of the ﬁelds and take
the Coulomb or radiation gauge k · A(t, k) = 0. Then the transverse parts of the ﬁeld
strengths E = −∂A/∂t −ik and B = ik × A are −∂A/∂t and ik × A and their
longitudinal parts are −ik and 0. For any other gauge the vector potential also has
a longitudinal part. Note, however, that the Coulomb gauge is not Lorentz invariant.

464
5
Quantum Mechanics II
If we do adopt the Lorentz gauge, we encounter other difﬁculties in quantum theory,
because the Lorentz condition cannot be transferred to operators. Then we have to
introduce longitudinal and scalar photons, which are not easily normalized (see, e.g.,
[8]). Here Elong = −ikρ/(εk2) holds, according to the third Maxwell equation.
We now consider the energy W = 1
2
	
d3r (E · D + H · B) (see p. 211) and the
momentum P =
	
d3r D × B (see p. 215) in a non-conductor, i.e., with ρ = 0 and
j = 0, as well as D = εE and H = B/μ. According to Parseval’s equation (p. 23),
for the energy
W(t) = ε
2

d3k (E∗· E + c2B∗· B ) = ε
2

d3k

∂A∗
∂t
· ∂A
∂t
+ ω2A∗· A

,
with transverse gauge, and for the momentum
P(t) = ε

d3k (E∗× B) = −iε

d3k k ∂A∗
∂t
· A .
According to p. 216, we have
A(t, k) = A(k) exp(−iωt) + A∗(−k) exp(+iωt)
2
= A∗(t, −k) ,
and thus ∂A(t, k)/∂t = −i
2ω {A(k) exp(−iωt) −A∗(−k) exp(+iωt)}. For the
energy, we may replace the integrand A∗(−k) · A(−k) by A∗(k) · A(k) and for
the momentum, k A∗(−k) ·A(−k) by −k A∗(k) ·A(k) (a variable transformation),
to deduce the time-independent expressions
W = ε
2

d3k ω2 A∗(k) · A(k) ,
P = ε
2

d3k ωk A∗(k) · A(k) ,
since the oscillating factors cancel for the energy and the momentum—in the latter
case, for the symmetry under k ↔−k. This distinguishes the results calculated with
potentials from those calculated with ﬁeld strengths.
Because of the spins, we also have to consider the angular momentum:
J = ε

d3r r × (E × B) .
Here we replace only B by ∇× A, but not E by −∂A/∂t for the time being. If Ec
and rc are now treated as constant, then
E × (∇× A) = ∇Ec · A −E · ∇A ,
according to Sect. 1.1.8,

5.5 Photons
465
r × {E × (∇× A)} = −∇× (Ec · A r ) −E · ∇rc × A
= −∇× (Ec · A r ) −E · ∇r × A + E × A .
The volume integrals of ∇× (Ec · A r ) and E · ∇r × A can be changed into surface
integrals. Then there is initially only one more volume integral, of r × A ∇· E, but
the electric ﬁeld is source-free here. These surface integrals
	
df × r E · A and
	
df ·
E r × A pick up the orbital angular momentum of the ﬁelds. They depend on where
the origin of the position vectors lies and do not have a component in the direction
of propagation. This is different with the volume integral of E × A = A × ∂A/∂t.
Here, using Parseval’s equation, we arrive at the eigen angular momentum
S = ε

d3k A∗(t, k) × ∂A(t, k)
∂t
.
Since only terms even in k contribute to the integral, the parts oscillating at 2ω cancel
again, and we ﬁnd
S = −iε
2

d3k ω A∗(k) × A(k) .
The result S(k) = −i
2ε ω A∗(k) × A(k) is useful for the helicity S(k) · ek.
Because of the transversality, on p. 218, we already introduced two mutually
orthogonal unit vectors e ∥and e⊥with e ∥× e⊥= ek, and shortly after that also
complexunitvectorse± ∝(e ∥± ie⊥)/
√
2.There,however,wedidnotdeterminethe
phase factor, which we now adjust to the spherical harmonics Y (l)
m (). Let e () = er
be the unit vector in the direction of  = (θ, ϕ). Then, with e0 = i ek, we require
em · e () =

4π
3 iY (1)
m () =

i cos θ
for m = 0 ,
∓i
√
2 sin θ exp(± iϕ) for m = ±1 .
According to p. 332, we always took the factor i l for the expansions of functions
f (r ) in terms of spherical harmonics. If, for k in the z-direction, we choose e ∥= ex
and e⊥= ey, then we have e± · e ∥= ∓i/
√
2 and e± · e⊥= 1/
√
2. Therefore, for
the expansion in terms of circularly polarized light, we take
e± ≡∓i e ∥± ie⊥
√
2
= e∗
∓,
with the properties
e∗
± · e± = 1 ,
e∗
± × e± = ± iek ,
e∗
± · e∓= 0 ,
e∗
± × e∓= 0 .
The amplitudes for the two helicities are then

466
5
Quantum Mechanics II
A±(k) = e±
∗· A(k)
⇐⇒
A(k) = e+ A+(k) + e−A−(k) ,
and hence we deduce the two equations
A∗(k) · A(k) = |A+(k)|2 + |A−(k)|2 ,
A∗(k) × A(k) = (|A+(k)|2 −|A−(k)|2) i ek .
We can also give the contribution of the respective helicities to the energy and the
momentum, as soon as we know the amplitude of A±.
Actually, for e±, we should also include the argument k, because we need to note
that e±∗(−k) × e±(−k) = ± i (−ek), and e±(−k) = e∗
±(k) = e∓(k). With this we
deduce
A(t, k) =

λ=±
eλ
Aλ(k) exp(−iωt) + A∗
λ(−k) exp(+iωt)
2
,
or Aλ(t, k) = 1
2{Aλ(k) exp(−iωt) + Aλ∗(−k) exp(+iωt)}. Here we also have
Aλ(−k) = eλ(k) · A(−k) .
5.5.2
Quantization of Photons
Clearly, the two quantities |A±(k)|2 depend on the intensity of the radiation ﬁeld.
Classically, in the wave picture, they may take arbitrary values ≥0, but in quantum
physics, only natural numbers. There are only integer light quanta, no fractions of
them. We usually speak of photons rather than light quanta.
The properties of these photons can be read off from the previous expressions for
energy, momentum, and helicity densities in k-space:
W(k) = 1
2ε ω2 {|A+(k)|2 + |A−(k)|2} ,
P(k) = 1
2ε ωk {|A+(k)|2 + |A−(k)|2} ,
H (k) = 1
2ε ω {|A+(k)|2 −|A−(k)|2} .
The ratio of their energy to their momentum is thus ω/k = c. According to relativity
theory (see p. 245), for all massless particles we can state that photons do not have
mass and therefore move with the velocity of light.

5.5 Photons
467
If we now assume the known Planck–de Broglie relations for single photons, viz.,
E = ℏω
and
p = ℏk ,
then the density of the quanta with helicity λ = ±1 is obtained as
ρλ(k) = εω
2ℏ|Aλ(k)|2 .
The angular momentum in the motional direction thus yields ±ℏ. We distinguish
between two helicities or two sorts of photons. In fact, they all have spin one, but it
is oriented only in or opposite to the direction of motion, not orthogonal to it—this
is a relativistic effect, which relates to the Lorentz contraction. With integer spins,
they are therefore bosons. (Electrons also have only two spin states, but they are
fermions.)
The integral
	
d3k ρλ(k) in the classical calculation does not need to be an even
number. But in the particle picture, we have to enforce this by a special quantum con-
dition, viz., for photons we have to take creation and annihilation operators satisfying
the Bose commutation law:
[λ(k), †
λ′(k′)] = ⟨k, λ|k′, λ′⟩
and
[λ(k), λ′(k′)] = 0 .
According to p. 450, in the Heisenberg picture, the time dependence is given by
λ(t, k) = λ(k) exp(−iωt) .
Since we are dealing with bosons, several photons can be in the same state |k, λ⟩.
From the expression for the particle density, which we understand as the expectation
value of † , we deduce the assignment
λ(k) =
εω
2ℏAλ(k) and †
λ(k) =
εω
2ℏAλ
∗(k) .
The Hamilton, momentum operator, and helicity operator then follow with ω = ck:
H =

d3k ℏω {†
+(k) +(k) + †
−(k) −(k)} ,
P =

d3k ℏk {†
+(k) +(k) + †
−(k) −(k)} ,
H =

d3k
{†
+(k) +(k) −†
−(k) −(k)} .
The vector potential has now become an operator:
A(t, k) =

2ℏ
εω

λ=±
eλ
λ(t, k) + †
λ(t, −k)
2
.

468
5
Quantum Mechanics II
Hence it follows that A †(t, k) = A(t, −k), with eλ∗(k) = eλ(−k). The transverse
electric and magnetic ﬁeld operators are then obtained from E = −∂A/∂t and B =
ik × A (as well as from Weber’s equation):
E(t, k) = i√2ℏω/ε

λ=±
eλ
λ(t, k) −†
λ(t, −k)
2
,
B(t, k) = i

2ℏωμ

λ=±
ek × eλ
λ(t, k) + †
λ(t, −k)
2
,
where we can also use iek × eλ = λ (eλ∗× eλ) × eλ = λeλ, although this does not
always help.
In order to make the transition from k to r, we consider arbitrary Cartesian com-
ponents n, unrelated to k, instead of the helicities, and investigate [n(k), †
n′(k′)].
This is equal to 
λλ′ en · eλ e∗
λ′ · en′ ⟨k, λ|k′, λ′⟩. Because of the last factor, we may
restrict ourselves to λ = λ′. Here 
λ eλ e∗
λ · en′ is the part of en′ perpendicular to
k/k ≡ek, which, according to p. 4, we may thus write as en′ −ek ek · en′. Therefore
we deduce
[n(k), †
n′(k′)] = (δnn′ −en · ek ek · en′) ⟨k|k ′⟩,
as a generalization of [λ(k), †
λ′(k′)] = ⟨k, λ|k′, λ′⟩.
For the ﬁelds A and B, there is the sum of  and †, and for E, their difference.
Therefore, the commutation laws are different. In fact,
0 = [An(k), An′(k′)] = [En(k), En′(k′)] = [Bn(k), Bn′(k′)]
and [An(k), Bn′(k′)] = 0, but also, using ⟨kλ| −k′λ′⟩= ⟨k| −k′⟩⟨λ| −λ′⟩and
e−λ = e∗
λ as well as Weber’s equation,
[An(k), En′(k′)] = ℏ
iε (δnn′ −en · ek ek · en′) ⟨k| −k′⟩,
[En(k), Bn′(k′)] = ℏ
ε (en × en′) · k ⟨k| −k′⟩.
Here we have at last made use of 
λ eλ e∗
λ · (en′ × k) = en′ × k.
After a Fourier transform k →r, the corresponding operator functions of r, rather
than k, are
A(t, r ) =
1
√
2π 3

d3k exp(ik · r ) A(t, k) = A†(t, r ) ,
where the last equation corresponds to the classical relation A(t, r ) = A∗(t, r ) ⇐⇒
A(t, k) = A∗(t, −k). With (t) = (0) exp(−iωt) and †(t) = †(0) exp(iωt),

5.5 Photons
469
it is often useful to decompose the ﬁelds into the so-called positive-frequency part
A+(t, r ) and negative-frequency part A−(t, r ) = A+†(t, r ):
A(t, r ) = A+(t, r ) + A−(t, r ) ,
where
A +(t, r ) =
1
√
2π 3

d3k

ℏ
2εω

λ=±
eλ λ (k) exp{i (k · r −ωt)} ,
and likewise for the electric and magnetic ﬁelds.
In real space, with the transverse delta function
δtrans
nn′ (r ) =
1
(2π)3

d3k (δnn′ −en · ek ek · en′) exp(ik · r ) ,
we have the not so simple commutation laws
[An(r ), En′(r ′)] = ℏ
iε δtrans
nn′ (r −r ′) ,
[En(r ), Bn′(r ′)] = ℏ
iε (en × en′) · ∇δ(r −r ′) .
Integration of [En(r ), Bn′(r ′)] over a space element around r ′ yields zero. Electric
and magnetic ﬁeld strengths at the same position commute, and equal components
(n = n′) of E and B also commute everywhere. Note that A(r ) and −εEtrans(r ) may
be taken as canonical conjugates, provided that we have ensured that the ﬁelds are
transverse.
The transverse delta function clearly has the following symmetries:
δtrans
nn′ (r ) = δtrans
n′n (r ) = δtrans
nn′ (−r ) .
In addition, it is source-free, i.e., 
n ∂δtrans
nn′ /∂xn = 0, because

n
kn (δnn′ −knkn′/k2) = 0 ,
with k2 =

n
kn
2 .
To relate this to the usual delta function, we consider
−

d3k
(2π)3
knkn′
k2
exp(ik · r ) =
∂2
∂xn∂xn′

d3k
(2π)3
exp(ik · r )
k2
.
According to p. 410, the right-hand integral is equal to (4πr)−1. We thus have
δtrans
nn′ (r ) = δnn′ δ(r ) +
∂2
∂xn∂xn′
1
4πr ,

470
5
Quantum Mechanics II
where, according to p. 172,
∂2
∂xn∂xn′
1
4πr = 3xnxn′/r2 −δnn′
4πr3
−δnn′ δ(r )
3
.
Thus it has to be accounted for even when n ̸= n′, otherwise we would also have to
split off the factor δnn′.
All commutation laws have been derived here for equal times—and in the
Schrödinger picture, the ﬁeld operators do not depend upon time. To avoid inte-
grals and improper Hilbert vectors, we must consider a ﬁnite volume V and periodic
boundary conditions, as in Sect. 5.4.1.
5.5.3
Glauber States
According to Sect. 4.2.8, the commutation law [, †] = 1 leads to the eigenvalues
n ∈{0, 1, 2, . . .} of the operators †, and for a suitable phase convention to
|n⟩= |n−1⟩√n
⇐⇒
†|n⟩= |n+1⟩
√
n + 1 .
If n is the particle number, |0⟩corresponds to the vacuum state,  is an annihilation
operator, and † is a creation operator.
In Sect. 4.5.4, we used these operators for linear oscillations and set X =
x0 ( + †)/2, P = p0 ( −†)/(2i). Since we are dealing here with canonically
conjugate quantities, for which the scale factors are not essential, we now consider
the components
A1 ≡ + †
2
= A1
†
and
A2 ≡ −†
2i
= A2
† .
If in particular the mean value (expectation value) A1 oscillates harmonically, then
so does A2, but with the phase shifted by π/2. The commutation law [, †] = 1
delivers
[A1, A2] = 1
2i ,
and thus, according to p. 300, the uncertainty relation A1 · A2 ≥1/4.
In this and the next section, we shall consider in detail those states whose uncer-
tainty product A1 · A2 is as small as possible, thus “as classical as possible”.
Then, according to p. 300, we must have
(A1 −A1) |ψ⟩= −iA1
A2
(A2 −A2) |ψ⟩.

5.5 Photons
471
In this section, we restrict ourselves to A1 = A2 = 1/2 and hence to Glauber
states (which were in fact introduced by Schrödinger much earlier [9]), also called
coherent states, although this is somewhat misleading, because all pure states can be
superposed coherently. They are particularly important for the electromagnetic ﬁeld
(the “photon states”) of lasers. In the next section we shall consider the more general
case A1 ̸= A2, and in particular, quenched states.
With the ﬁeld operators  |ψ⟩= |ψ⟩ from above, the constraint (A1 −A1) |ψ⟩
= −i (A2 −A2) |ψ⟩reads:Glauberstatesareeigenstatesoftheannihilationoperator
. This operator is not Hermitian. Therefore, we need a complex number in order
to label the eigenvalue. α is normally used, and we shall follow that here:
 |α⟩= |α⟩α ,
with ⟨α|α⟩= 1 .
Then ⟨α|† = α∗⟨α|, and consequently,
⟨α| A1 |α⟩= Reα
and
⟨α| A2 |α⟩= Imα ,
or α = A1 + i A2. Note that, when X = x0A1 and P = p0A2, we also have α =
X /x0 + iP/p0,sowetakethetworealphase-spacecomponentsoftheone-dimensional
oscillation as a complex number.
We can create the Glauber state |α⟩with a unitary operator D(α) (the exponent is
anti-Hermitian) from the ground state |0⟩:
D(α) ≡exp(α† −α∗) ,
with D†(α) = D(−α) = D−1(α) .
Using the property D†(α) D(α) =  + α 1 (the Hausdorff series, see p. 290, only
contains two terms here), D(α) is called the displacement operator. It leads to
D(α)|0⟩= D(α) ( + α)|0⟩= D(α)|0⟩α ,
so
|α⟩= D(α)|0⟩.
Here, according to p. 290, we may factorize, hence,
D(α) = exp(α†) exp(−α∗) exp(−1
2|α|2) ,
and use exp(−α∗)|0⟩= |0⟩along with † n|0⟩= |n⟩
√
n!:
|α⟩= exp(−1
2|α|2)
∞

n=0
αn
√
n!
|n⟩.

472
5
Quantum Mechanics II
Incidentally, D(α + β) does not simply factorize into D(α) D(β), because a phase
factor also occurs: D(α + β) = exp{iIm(α∗β)} D(α) D(β). This yields
D(α) D(β) = exp(αβ∗−α∗β) D(β) D(α) .
Consequently, we also have
⟨α|α′⟩= exp{−1
2 |α −α′|2 + iIm(α∗α′)} .
The eigenstates of the non-Hermitian operators  are thus neither countable nor
orthogonal to each other. They nevertheless form a complete basis. We only have to
integrate over the whole complex plane. Instead of dReα dImα, however, we write
for short d2α and take α and α∗to be independent of each other. Then
 d2α
π
|α⟩⟨α| = 1 .
If we expand the left-hand side in terms of the complete basis {|n⟩}, then we obtain
⟨n|α⟩⟨α|n′⟩= exp(−|α|2) αn α∗n′/
√
n! n′!, with α = a exp(iϕ), or d2α = a da dϕ
and
 d2α
π
⟨n|α⟩⟨α|n′⟩=
 ∞
0
an+n′+1e−a2
√
n! n′!
da 1
π
 2π
0
ei (n−n′) ϕ dϕ .
The last integral is equal to 2π δnn′ and, for n = n′, the one to the left of it is equal to
1/2 (if set we x = a2, so that dx = 2a da, then
	 ∞
0 xn exp(−x) dx = n! leads to the
result). The double integral is equal to ⟨n|n′⟩.
So far we have always taken orthogonal bases and, for continuous variables, have
arrived at simple integrals. But now the states are no longer orthogonal to each other
and we require double integrals. The basis {|α⟩} is said to be over-complete. An
arbitrary state can be decomposed in terms of these, but no longer uniquely, because
the basis states now depend linearly on each other. Hence, e.g., for all n ∈{1, 2, . . .},
|o⟩= n|0⟩=
 d2α
π
n|α⟩⟨α|0⟩=
 d2α
π
|α⟩αn exp(−1
2|α|2) .
Consequently, there are even inﬁnitely many linear combinations of states |α⟩which
may result in the zero vector |o⟩.
In the Glauber state |α⟩, the operator N = † has the expectation value
⟨α| N |α⟩= |α|2 ,
and, with N 2 = †(† + 1), the uncertainty N = |α|. Note that this increases
with |α|, but the relative uncertainty N/N = |α|−1 decreases, as expected for the

5.5 Photons
473
transition to classical mechanics. For a harmonic oscillation, we obtain the result
H = ℏω (|α|2 + 1/2) and H = ℏω |α|.
Furthermore, the probability for the Fock state |n⟩, with sharp particle number
and unsharp phase, depends only on the modulus of α:
|⟨n|α⟩|2 = exp(−|α|2) |α|2n
n!
.
ThisisaPoissondistributionρn withmeanvalue⟨n⟩= |α|2 anduncertaintyn = |α|
(see p. 519).
For the time dependence, using H|n⟩= |n⟩ℏω (n+ 1
2) and |α(0)⟩= |α0⟩, we
obtain
|α(t)⟩= exp −iHt
ℏ
|α0⟩= exp −|α0|2 −iωt
2
∞

n=0
(α0 e−iωt)n
√
n!
|n⟩,
whence ⟨α(t)|  |α(t)⟩= α0 exp(−iωt), and its complex conjugate for the expecta-
tion value of †. Consequently, we have
X (t) = x0 Re(α0 e−iωt)
and
P(t) = p0 Im(α0 e−iωt) .
The Glauber states oscillate harmonically with angular frequency ω and with ﬁxed
position, momentum, and energy uncertainties. Ehrenfest’s equations are also valid
here.
5.5.4
Quenched States
We now allow for A1 ̸= A2, but keep searching for further states with an uncer-
tainty product A1A2 as small as possible. The necessary equation mentioned in
the last section can be reformulated as the eigenvalue equation
(A1 + i A1
A2
A2) |ψ⟩= |ψ⟩(A1 + i A1
A2
A2) ,
with An = A†
n .
But the non-Hermitian operator is now composed linearly of the annihilation operator
 and the creation operator †. Therefore, we consider the Bogoliubov transforma-
tion, but now for boson operators, with u = u∗> 0 and v = v∗:
 = u + v†
⇐⇒
† = u† + v .
Note that a common phase factor is unimportant, so we may choose u = u∗> 0, and
v ̸= v∗would then lead to A1 · A2 > 1/4.
With [, †] = (u2 −v2) [, †], we also require

474
5
Quantum Mechanics II
Fig. 5.15 Uncertainties of the squared components (A1 horizontal and A2 vertical) for the Glauber
state (z = 0) and quenched states (z = ±1/2). As in Fig. 4.20, instead of the A values, we now plot
ellipses of the same area with corresponding principal axes, where the product is A1 · A2 = 1/4
u2 −v2 = 1
⇐⇒
[ , †] = 1 .
For u = cosh z and v = sinh z, this is possible with a single real parameter z (see
Fig. 5.15). Recall that, for fermions, we had u2 + v2 = 1 (see p. 457), and therefore
circular instead of hyperbolic functions. Note also that, with u ≥1, we are no longer
allowed to choose u = 0 and then replace  by †. Conversely, then  = u −v†
and † = u† −v.
The Bogoliubov transformation can be carried out by a unitary operator S:
 = S S†. In particular, if we set S = exp A with A† = −A with S† = S−1, then
according to Hausdorff (see p. 290), A follows from
 = u + v† = cosh z  + sinh z †
= S S† =  + 1
1! [A, ] + 1
2! [A, [A, ]] + · · · ,
since here only [A, ] = z† has to hold, and thus [A, †] = z. Consequently,
A = 1
2z (2 −† 2) up to an arbitrary phase factor in S. The quench operator (or
“squeeze operator”)
S(z) = exp z (2 −† 2)
2
affects the ratio A1/A2, as will now be shown.
Corresponding to the Glauber state |α⟩(eigenstate of ) is a quenched state S|α⟩,
an eigenstate of . With S = S, we have in particular,
S|α⟩= S|α⟩α .
In order to be able to employ |α⟩= D(α)|0⟩, we investigate the product SD(α) with
D(α) = exp(α† −α∗) = f (, †). Since
Sf (, †)S† = f (SS†, S†S†) = f (, †) ,

5.5 Photons
475
we also have Sf (, †) = f (, †)S. Here,
α† −α∗ = (αu −α∗v) † −(αu −α∗v)∗ .
If we therefore set
β ≡uα −vα∗
⇐⇒
α = uβ + vβ∗,
we ﬁnd that SD(α) = D(β) S, so for the eigenstate of  with eigenvalue α,
S |α⟩= S D(α) |0⟩= D(β) S |0⟩.
For the quasi-vacuum (“quenched vacuum”), we have S|0⟩, hence S|0⟩= |o⟩.
The expectation value of the operator F(, †) in the quenched state S|α⟩is thus
the vacuum expectation value of
D†(α) S† F(, †) S D(α) = S† D†(β) F(, †) D(β) S .
In the term D†(β) F(, †) D(β) = F( + β, † + β∗), it is now useful to change
the representation  →u −v† and † →u† −v, with ⟨0|†S† = ⟨o| and
S|0⟩= |o⟩. Thus, it follows in particular that
⟨⟩= β ,
⟨2⟩= β2 −uv ,
⟨†⟩= |β|2 + v2 ,
and for the Hermitian operator F = f  + f ∗†,
⟨F⟩= f β + f ∗β∗
and F = |fu −f ∗v| = |f |

cosh 2z −cos 2ϕ sinh 2z ,
where f = eiϕ|f |. For the two components in the quenched state S|α⟩with f = 1/2
or f = −1/2i and u ≥v, we ﬁnd
A1 = 1
2(u −v)
and
A2 = 1
2(u + v) .
Herewehaveu ∓v = exp(∓z)andhenceA1 · A2 = 1
4 andA1/A2 = exp(−2z).
Quenched states are appropriate, e.g., when comparing two oscillations of dif-
ferent frequency, because their ground states have X = 1
2x0 and P = 1
2p0, with
x0p0 = 2ℏ, but x0 and p0 =
√
2ℏmω depend upon the given frequency. In an “inap-
propriate basis”, the oscillation appears compressed (or expanded) (see Fig. 5.16).
The quenched states are formed under parametric ampliﬁcation (in this context,
see the discussion of parametric resonance in Sects. 2.3.10 and 2.4.11), with the
Hamilton operator

476
5
Quantum Mechanics II
Fig. 5.16 Inﬂuence of the quench parameter z on the particle number (continuous red) and its
uncertainty N =

|uβ −vβ∗|2 + u2v2 (dashed blue), here shown for the quenched vacuum.
The average particle number is in fact then as small as possible for a given z, but greater than zero
for z ̸= 0
H = ℏω † −iℏκ exp(iωpt) 2 −exp(−iωpt) † 2
2
,
or with H = ℏω † + 1
2ℏκ {exp(iωpt) 2 + exp(−iωpt) † 2}, which can lead
back to the former under the phase transformation  →exp(−iπ/4) . Here ω
is the angular frequency of the considered light and ωp = 2ω that of the pump light,
while κ gives the (real) coupling constant. The pump light is described classically
here, with ﬁxed intensity, and in this sense, the above Hamilton operator is “semi-
classical”. This will be discussed in more detail in Sect. 5.5.6. We thus have the
Heisenberg equation
d
dt = i[H, ]
ℏ
= −iω  + κ exp(−2iωt) † .
It can be solved by carrying out the time-dependent Bogoliubov transformation
(t) = exp(−iωt) {cosh(κt) (0) + sinh(κt) †(0)} .
The phase factor is unimportant here. This therefore leads to quenched states. For
the photon number operator N(t) = †(t) (t), we have
N(t) = sinh2(κt) 1 + cosh(2κt) N(0) + sinh(2κt) † 2(0) + 2(0)
2
.
IfthereisnolightinitiallysothatN(0) = 0,thentheaveragephotonnumberincreases
as sinh2(κt), although the result for long times is certainly not correct, because the
pump light cannot supply energy inexhaustibly.
5.5.5
Expansion in Terms of Glauber States
In Sect. 5.5.2, we gave different observables (e.g., E, B, H, P) as functions of the
ﬁeld operators , †. If we now expand the operator f (, †) in terms of Glauber
states,

5.5 Photons
477
f (, †) =
 d2α
π
d2α′
π
|α⟩⟨α| f (, †) |α′⟩⟨α′| ,
then we may evaluate the coefﬁcients, if f (, †) is normal ordered, i.e., in all
products, the creation operators occur to the left of the annihilation operators:
f (, †) =

rs
f (n)
rs
†rs .
Then ⟨α| f (, †) |α′⟩= 
rs f (n)
rs α∗rα′s ⟨α|α′⟩. With the abbreviation
f (n)(α′, α∗) =

rs
f (n)
rs
α∗rα′s ,
it follows that ⟨α| f (, †) |α′⟩= f (n)(α′, α∗) ⟨α|α′⟩with
⟨α|α′⟩= exp{−1
2|α−α′|2 + iIm(α∗α′)} ,
according to p. 472.
The operator f (, †) may also be anti-normal-ordered, with the creation oper-
ators to the right of the annihilation operators:
f (, †) =

rs
f (a)
rs r†s .
Then for the function f (, †), just one double integral (over d2α) sufﬁces. If we
insert the unit operator between r and †s, then we obtain the important relation
f (, †) =
 d2α
π
f (a)(α, α∗) |α⟩⟨α| ,
with
f (a)(α, α∗) =

rs
f (a)
rs αrα∗s .
Here we have f (a)(α, α∗) ̸= f (n)(α, α∗), as can be recognized, e.g., from f (, †) =
† = † + 1, because then f (a)(α, α∗) = |α|2, f (n)(α, α∗) = |α|2 + 1, and so
f (n)(α, α∗) = f (a)(α, α∗) + 1. More general than † n = † n + n † n−1 is
m † n
† n m
"
=

l
(±)lm! n!
l! (m −l)! (n −l)!
† n−l m−l ,
m−l † n−l ,
as can be shown by induction (see Problem 4.20).

478
5
Quantum Mechanics II
Note that, from f (a)(α, α∗), we can also determine f (n)(α, α∗), but we cannot
determine f (n)(α′, α∗) for α′ ̸= α:
f (n)(α′, α′∗) =
 d2α
π
exp(−|α−α′|2) f (a)(α, α∗) ,
with f (n)(α′, α′∗) = ⟨α′| f (, †) |α′⟩and |⟨α|α′⟩|2 = exp(−|α−α′|2).
Generally, we may set
f (, †) =
 d2ξ
π
exp(ξ†) exp(−ξ ∗) F(n)(ξ, ξ ∗)
=
 d2ξ
π
exp(−ξ ∗) exp(ξ†) F(a)(ξ, ξ ∗) ,
with the expansion coefﬁcients
F(n)(ξ, ξ ∗) = tr{exp(ξ ∗) exp(−ξ†) f(, †)} ,
F(a)(ξ, ξ ∗) = tr{exp(−ξ†) exp(ξ ∗) f(, †)} .
If we replace f (, †) in tr{exp(ξ ∗) exp(−ξ†) f(, †)} by the normal-
ordered double integral
	
d2ξ ′ π−1 exp(ξ ′†) exp(−ξ ′ ∗) F(n)(ξ ′, ξ ′ ∗), then we
arrive at
	
d2ξ ′ π−1F(n)(ξ ′, ξ ′ ∗) tr{exp[(ξ −ξ ′)∗] exp[−(ξ −ξ ′)†]}. If we insert
the unit operator
	
d2α π−1|α⟩⟨α| between the two exponential functions in the
trace, then we obtain
	
d2α π−1 exp{(ξ −ξ ′)∗α −(ξ −ξ ′)α∗}, and the exponent is
2i Im{(ξ −ξ ′)∗α}, thus equal to 2i Re(ξ −ξ ′)Imα −2i Im(ξ −ξ ′)Reα. In this way,
we arrive at the Fourier expansions of delta functions of the real and imaginary parts
of 2(ξ −ξ ′). This is easily integrated over ξ ′, and we arrive at F(n)(ξ, ξ ∗). The proof
for F(a)(ξ, ξ ∗) is very similar. We thus obtain the Fourier transforms
F(n)(ξ, ξ ∗) =
 d2α
π
exp(ξ ∗α −ξα∗) f (n)(α, α∗) ,
f (n)(α, α∗) =
 d2ξ
π
exp(ξα∗−ξ ∗α) F(n)(ξ, ξ ∗) .
Note that we usually require the normalization factor 2π for the Fourier trans-
form. Here π sufﬁces, because the factor of 2 is already contained in the expression
2 Im(ξ ∗α) = Im(2ξ ∗α). Of course, the relation between F(a)(ξ, ξ ∗) and f (a)(α, α∗)
is also a Fourier transform.
We have the trace of the anti-normal-ordered products exp(ξ ∗) exp(−ξ†) for
F(n)(ξ, ξ ∗), and that of the normal-ordered products for F(a)(ξ, ξ ∗). In both cases,
the product of the exponential functions and of f (, †) can be reformulated as
a normal-ordered product of powers of  and †, and the unit operator inserted
between the two factors.

5.5 Photons
479
5.5.6
Density Operator in the Glauber Basis
If we set likewise
ρ(, †) =

rs
ρ(n)
rs †rs =

rs
ρ(a)
rs r†s ,
for the density operator ρ(, †), then
tr (r†s†uv) =

d2α π−1α∗s+uαr+v
implies the equations
⟨f (, †)⟩=
 d2α
π
ρ(a)(α, α∗) f (n)(α, α∗) =
 d2α
π
ρ(n)(α, α∗) f (a)(α, α∗) ,
where one normal and one anti-normal-ordered operator always occur, like covari-
ant and contravariant components for the scalar product. Since f (, †) = 1, we
therefore also have
	
d2α ρ(n)(α, α∗) =
	
d2α ρ(a)(α, α∗) = π.
As for the Wigner function (see Fig. 4.7), the different representations of ⟨f ⟩
suggest introducing quasi-probability densities, and in particular, the P-function
P(α) ≡ρ(a)(α, α∗)
π
,
with

d2α P(α) = 1 ,
and the Q-function (or Husimi function)
Q(α) ≡ρ(n)(α, α∗)
π
,
with

d2α Q(α) = 1 .
It then follows that
⟨f (, †)⟩=

d2α P(α) f (n)(α, α∗) =

d2α Q(α) f (a)(α, α∗) .
Since ρ =
	
d2α P(α) |α⟩⟨α|, but |α⟩⟨α| does not project on orthogonal states, the
P-function is only a quasi-probability density. The Q-function does in fact have the
properties of a probability density, i.e., it is real and never negative, with ρ(n)(α, α∗) =
⟨α|ρ|α⟩, but does not lead to the full density operator.
Very useful here are also the normal-ordered characteristic function
C(n)(ξ, ξ ∗) ≡⟨exp(ξ†) exp(−ξ ∗)⟩

480
5
Quantum Mechanics II
and the anti-normal-ordered characteristic function
C(a)(ξ, ξ ∗) ≡⟨exp(−ξ ∗) exp(ξ†)⟩.
These can be used to derive the moments at the position ξ = ξ ∗= 0:
⟨†rs⟩= (−)s ∂r+sC(n)
∂ξ r ∂ξ ∗s
and
⟨r†s⟩= (−)r ∂r+sC(a)
∂ξ ∗r ∂ξ s .
The two functions are related, because, according to p. 290,
exp(ξ† −ξ ∗) = exp(ξ†) exp(−ξ ∗) exp(−1
2|ξ|2)
= exp(−ξ ∗) exp(ξ†) exp(+ 1
2|ξ|2) ,
so C(n)(ξ, ξ ∗) = C(a)(ξ, ξ ∗) exp(|ξ|2). The characteristic functions are the Fourier
transforms of ρ(α, α∗), so
C(a)(ξ, ξ ∗) =
 d2α
π
exp(ξα∗−ξ ∗α) ρ(n)(α, α∗) ,
ρ(n)(α, α∗) =
 d2ξ
π
exp(ξ ∗α −ξα∗) C(a)(ξ, ξ ∗) ,
and likewise C(n)(ξ, ξ ∗) and ρ(a)(α, α∗) are Fourier transforms of one another.
Let us consider some useful examples:
(1) Clearly, for the Glauber state |α⟩, we have
C(n)(ξ, ξ ∗) = exp(ξα∗−ξ ∗α) ,
with ρ = |α⟩⟨α| .
(2) For the laser, a superposition of these states with equal amplitude and unknown
phase arg α = ϕ is important. We have to average over ϕ to obtain ρ = |α⟩⟨α|.
Then we arrive at C(n)(ξ, ξ ∗) =
1
2π
	 2π
0
dϕ exp{|ξα|(e−iϕ −eiϕ)}. With z = |2ξα|
and t = exp(−iϕ), the integrand can be expanded in terms of regular Bessel functions
Jn(z), because they have the generating function
exp

z t −t−1
2

=
∞

n=−∞
Jn(z) tn ,
fort ̸= 0 ,
with the symmetry J−n(z) = (−)n Jn(z). If we expand exp( 1
2zt) and exp(−1
2z/t) in
series, we obtain the regular Bessel functions
Jn(z) =
∞

k=0
(−)k (z/2)n+2k
k! (n + k)!
,

5.5 Photons
481
Fig. 5.17 Regular Bessel
functions and irregular
Bessel functions, also called
Neumann functions, for n
from 0 (black) to 3 (blue)
(continuous for n even,
dotted for n odd).
Asymptotically, Jn(x) ≈

2
πx cos{x −(n + 1
2) 1
2π}
and Nn(x) ≈

2
πx sin{x −(n + 1
2) 1
2π}
2π
4π x
0
1
Bessel functions Jn
2π
4π x
-1
0
Neumann functions Nn
as shown in Fig. 5.17. Note that the spherical Bessel functions Fl(z) mentioned
on p. 401 are Bessel functions of half-integer index, viz., Fl(z) = √πz/2 Jl+1/2(z).
From the last equation,
exp(iz sin ϕ) =
∞

n=−∞
Jn(z) exp(inϕ) .
With this we obtain
C(n)(ξ, ξ ∗) = J0(2|ξα|) ,
with ρ = |α⟩⟨α| .
The anti-normal-ordered function C(a) also contains the factor exp(−|ξ|2).
(3) The quenched state S|α⟩has the normal-ordered characteristic function
⟨α|S† exp(ξ†) exp(−ξ ∗)S|α⟩= exp( 1
2|ξ|2) ⟨0|S†D†(β)D(ξ)D(β)S|0⟩,
with β = uα −vα∗. Here, according to p. 472, we have
D†(β) D(ξ) D(β) = exp(ξβ∗−ξ ∗β) D(ξ) ,
whence
C(n)(ξ, ξ ∗) = exp( 1
2|ξ|2 + ξβ∗−ξ ∗β) ⟨0| S† exp(ξ† −ξ ∗) S |0⟩.
As on p. 475, we replace ξ† −ξ ∗ →(uξ + vξ ∗) † −(uξ + vξ ∗)∗, and the
vacuum expectation value is found to be exp(−1
2|uξ + vξ ∗|2). So in total, for the
quenched state,

482
5
Quantum Mechanics II
C(n)(ξ, ξ ∗) = exp

β∗ξ −βξ ∗−v2ξξ ∗−uv ξ 2 + ξ ∗2
2

.
This leads, e.g., to the expressions mentioned in connection with Fig. 4.21.
(4) According to p. 580, the canonical density operator
ρ =

n
⟨N⟩n
⟨N + 1⟩n+1 |n⟩⟨n|
with⟨N⟩= {exp(ℏω/kT) −1}−1 andthus⟨N + 1⟩= {1 −exp(−ℏω/kT)}−1 isasso-
ciated with the temperature T. Hence, |⟨α|n⟩|2 = exp(−|α|2) |α|2n/n! implies
ρ(n)(α, α∗) = ⟨α| ρ |α⟩=
1
⟨N + 1⟩exp −|α|2
⟨N + 1⟩.
This means that C(a)(ξ, ξ ∗) is the Fourier component of a Gauss function, thus also
a Gauss function, according to p. 23:
C(a)(ξ, ξ ∗) = exp{−⟨N + 1⟩|ξ|2} .
The normal-ordered function C(n)(ξ, ξ ∗) also requires the factor exp(|ξ|2):
C(n)(ξ, ξ ∗) = exp{−⟨N⟩|ξ|2}
⇐⇒
ρ(a)(α, α∗) =
1
⟨N⟩exp −|α|2
⟨N⟩
for the canonical distribution.
5.5.7
Atom in a Light Field
We consider an atom with two eigenstates {|↑⟩, |↓⟩} at the energies ± 1
2ℏωA and
a light ﬁeld with the energy quantum ℏωL. The atom can be described using Pauli
operators σ and the ﬁeld using Bose operators , †, and for the coupling −p · E,
the dipole moment with σx = σ+ + σ−and the ﬁeld strength with i( −†), if we
combine all remaining factors into the real factor 1
2ℏg. The phase transformation
 →i changes −i( −†) into  + †. In comparison to σ+ + σ−†, the
parts σ+† + σ− couple to states of much higher frequency, viz., ωL+ωA instead
of ωL−ωA, and therefore do not contribute to the time average. Note that σ+
describes induced or forced absorption, and σ−† induced or forced emission. With
this we arrive at the Hamilton operator of the Jaynes–Cummings model:
H = 1
2ℏωA σz + ℏωL † + 1
2ℏg (σ+ + σ−†) .

5.5 Photons
483
Fig. 5.18 Eigenfrequencies
ω± in the Jaynes–Cummings
model as a function of the
detuning  ≡ωL −ωA,
each relative to ωA, and here
for g/ωA = 0.1
Above the ground state |↓, 0⟩, with energy −1
2ℏωA, it couples the state pair |↑, n⟩
and |↓, n+1⟩, where n is the photon number:
H|↑, n⟩
= ℏ{(n + 1
2) ωL −1
2} |↑, n⟩+ 1
2ℏg
√
n+1 |↓, n+1⟩,
H|↓, n+1⟩= 1
2ℏg
√
n+1 |↑, n⟩+ ℏ{(n + 1
2) ωL + 1
2} |↓, n+1⟩,
with detuning  = ωL −ωA between the light ﬁeld and the atom. According to
p. 309 ( 1
2trH ± 1
2

(trH)2 −4 det H), the eigenvalues of H are (see Fig. 5.18)
ω± = ωL (n+ 1
2) ± 1
2n ,
with the generalized (to  ̸= 0) Rabi frequency
n =

(n+1) g2 + 2 .
According to p. 310, the eigenstates associated with this doublet are
|+, n⟩=
|↑, n⟩cos θn + |↓, n+1⟩sin θn ,
|−, n⟩= −|↑, n⟩sin θn + |↓, n+1⟩cos θn ,
where cos θn = √1−/n/
√
2 and sin θn = √1+/n/
√
2. They are thus eigen-
states of † + σ+σ−with eigenvalue n+1. For the remaining expectation values,
we can use
cos(2θn) = −
n
and
sin(2θn) =
√n + 1 g
n
.
For example, the matrix elements of † and σz = 2σ+σ−−1 between the basis
states with
(† −σ+σ−)|±, n⟩= |±, n⟩(n ∓cos(2θn)) + |∓, n⟩sin(2θn)
are easy to evaluate, and their time dependence is known to be exp(−iω±t). If initially
either the state |↑, n⟩was occupied (upper sign) or the state |↓, n+1⟩(lower sign),
it follows that

484
5
Quantum Mechanics II
⟨σz⟩= ± (cos2(2θn) + sin2(2θn) cos nt) .
In particular, only for a resonance ( = 0) do all atoms end up in the other state.
This is of course true for other initial light ﬁelds, e.g., for the Glauber state |α⟩. If
initially the state |↑, α⟩was occupied (upper sign) or the state |↓, α⟩(lower sign), we
arrive at the weight factors exp(−|α|2) |α|2n+1∓1{(n + 1
2 ∓1
2)!}−1. We shall restrict
ourselves to the case |α| ≫1. Then the weight factors for n + 1
2 ∓1
2 ≈|α|2 −1
2 are
particularly large (Stirling’s formula on p. 518 is used in the proof), and therefore
for an approximate calculation we use the generalized Rabi frequency
α =
#
(|α|2 ± 1
2) g2 + 2
in cos2(2θα) = (/α)2 = 1 −sin2(2θα). But for the sum over cos(nt), we have
to calculate more precisely by one order. Here the abbreviations
ω =
g2
2α
≪α
and
κ = |α|2 + 22
g2
are useful, because then for the important terms we have n ≈(κ + n) ω, and this
leads to the approximation
⟨σz⟩= ±[cos2(2θα)
+ sin2(2θα) exp{−|α|2 (1−cos(ωt))} cos{κωt) + |α|2 sin(ωt)}] ,
with the upper sign for the initial state |↑, α⟩and the lower sign for |↓, α⟩. Here, in
the time 1
2π/ω, the factor exp{−|α|2 (1 −cos ωt)} decreases from one to a negligibly
small value. The oscillations observed for the Fock state stop after this time, and set
in again at the time 2π/ω (see Fig. 5.19).
Fig. 5.19 Absence and return of the excitation of an atom in a light ﬁeld described by a Glauber
state. Initially, |α|2 = 10 and the atom was in the ground state. Continuous curve: resonance. Dashed
curve: Detuning  = |αg|. Here ⟨σz⟩= 0 indicates that on average there are equally many atoms
in excited states as in the ground state

5.5 Photons
485
This absence and return (“collapse” and “revival”) occurs only with the unsharp
Rabi frequency {n}, as can be seen by comparing with the semi-classical ansatz:
only the atom is treated according tos quantum physics, but the ﬁeld classically.
Its Hamilton operator describes an illuminated atom (quasi-atom or dressed atom),
which is the expectation value of H with respect to a Glauber state |α⟩:
H ≡⟨α|H −ℏωL†|α⟩= 1
2ℏωAσz + 1
2ℏg (ασ+ + α∗σ−) .
Note that we have taken ℏωL|α|2 as the zero energy and subtracted, as usual. Here,
according to p. 473, the quantity α = |α| exp(−iωLt), and consequently also H,
depends on time. But this can be eliminated by a unitary transformation U(t) =
exp( 1
2iωLt σz). Here we go over to a reference frame rotating with the light wave.
The the rotating-wave approximation (RWA) neglects the terms σ+† + σ−, and
with the new axes, we arrive likewise at the time-independent Hamilton operator H.
Since U depends on time, using Problem 4.22, we ﬁnd
H = U HU † + iℏ˙UU † = 1
2ℏμ σx −1
2ℏ σz ,
with μ = |α| g .
Its eigenvalues are E± = ± 1
2ℏα with α =

μ2+2. Using
α = tr(σH)
ℏ
= μex − ez ,
in the equation for the Bloch vector ⟨σ ⟩≡tr(ρσ ) deduced from the von Neumann
equation iℏ˙ρ = [H, ρ] on p. 343, we ﬁnd
d⟨σ⟩
dt
= α × ⟨σ ⟩.
According to the semi-classical ansatz, the Bloch vector thus rotates about the vector
α in the reference frame rotating with the light frequency, and with this a complete
change from ⟨σz⟩= ±1 to ⟨σz⟩= ∓1 is only possible for resonance. But since the
Bloch vector rotates about α for arbitrarily long times, there is no absence and
return semi-classically.
So far we have not considered spontaneous emission (the coupling to the remain-
ing modes)—and this is often more apparent than the absence or the neglected terms
σ+† + σ−. For a two-level system, it is easy to write down the differential equa-
tion, according to p. 381 for T = 0:
dρ
dt = [H, ρ]
iℏ
+ [σ−ρ, σ+] + [σ−, ρσ+]
2τ
+ [σzρ, σz] + h.c.
2τ0
.

486
5
Quantum Mechanics II
This implies the Bloch equation (see also Problem 4.22)
d⟨σ⟩
dt
= α × ⟨σ⟩−(1+4τ/τ0)⟨σx ex + σy ey⟩+ 2⟨σz+1⟩ez
2τ
,
or again, writing γ −1 instead of 2τ and setting β = 1+4τ/τ0,
d⟨σ⟩
dt
= O ⟨σ⟩−2γ ez ,
with O =
⎛
⎝
−βγ

0
− −βγ −μ
0
μ
−2γ
⎞
⎠.
The previously skew-symmetric operator O thus obtains some diagonal elements: its
eigenvalues are not purely imaginary, and its real part leads to damping. The inverse
of O is
O−1 =
−1
γ (2β2γ 2 + 22 + βμ2)
⎛
⎝
μ2 + 2βγ 2 2γ 
−μ
−2γ 
2βγ 2
−βμγ
−μ
βμγ β2γ 2 + 2
⎞
⎠.
Using ⟨σ ⟩≈2γ O−1ez, the z-component of the stationary ﬁnal state is
⟨σz⟩≈−
β2γ 2 + 2
β2γ 2 + 2 + 1
2βμ2 =
−1
1 + 1
2βμ2/(β2γ 2 + 2) =
−1
1 + I/IS
,
since μ2 = |gα|2 is proportional to the light intensity I. The saturation intensity IS is
clearly proportional to β2γ 2 + 2, so at resonance ( = 0), it is particularly small
and increases quadratically with the detuning . For I ≪IS, ⟨σz⟩approaches −1,
which corresponds to the lower energy state, but for I ≫IS, it tends towards 0, the
two states then being equally probable. For the rotating-wave transformation, the
z-component is conserved, while
⟨σx + iσy⟩≈μ
 + iβγ
β2γ 2 + 2 + 1
2βμ2
becomes constant due to this transformation (see Fig. 5.20).
We have considered spontaneous emission only semi-classically. In a full quantum
mechanical treatment, we would also have to describe the electromagnetic ﬁeld using
operators (, †), and hence assume the Jaynes–Cummings model. In addition to
the considered damping, we would also have to include terms [ρ, †] + [, ρ †].
This damping couples the Jaynes–Cummings doublets and can be solved analytically
only with further approximations.

5.5 Photons
487
Fig. 5.20 Motion of the Bloch vector for the illuminated two-level atom (from out of the ground
state)usingtherotating-waveapproximationinthe(y, z)plane(topview)and(x, z)plane(sideview):
left for resonance ( = 0) and right with detuning (here  = μ). Dashed curves indicate without
spontaneous emission (dissipation, γ = 0), and continuous curves with spontaneous emission (here
γ = μ/10 and β = 1). Without dissipation, a circle is obtained, otherwise a spiral with the attractor
indicated by the open circle. For resonance, the quantization axis lies in the plane of the circle,
otherwise not (so the right-hand circle for detuning is inclined, and smaller)
5.5.8
Summary: Photons
As an example of a many-boson system, we have considered the light ﬁeld and quan-
tized the classical Maxwell equations, thereby investigating the quantum properties
of a classical ﬁeld. Instead of the occupation-number representation, we prefer to
take Glauber states, which are “as classical as possible”. Then as polar coordinate
we have the amplitude and phase of the ﬁeld and we do indeed ﬁnd oscillations, in
contrast to states with sharp energy.
5.6
Dirac Equation
5.6.1
Relativistic Invariance
The Dirac equation is a relativistic equation. Therefore, we use the notation with
four-vectors known from electrodynamics (Sect. 3.4). The position vector with its
Cartesian components
xk :
(x1, x2, x3) = (x, y, z) ,
with k ∈{1, 2, 3} ,
is amended with a further component x0 = ct (the “light path”), to yield the four-
vector x with contravariant components
xμ :
(x0, x1, x2, x3) = (ct, xk) ,
with μ ∈{0, 1, 2, 3} .
Correspondingly, the components of the mechanical momentum p are (see p. 245)

488
5
Quantum Mechanics II
pμ :
(p0, p1, p2, p3) =

E
c , pk
,
and those of the vector potential A are (see p. 239)
Aμ :
(A0, A1, A2, A3) =


c , Ak
.
If we consider a particle with charge q in the electromagnetic ﬁeld, its mechanical
momentum differs from the canonical momentum P, which has components (see
p. 247)
Pμ = pμ + q Aμ .
Apart from the contravariant components (upper index), we also need the covariant
components (lower index). These can be derived for the pseudo-Euclidean metric of
special relativity theory using the metric tensor
(gμν) =
⎛
⎜⎜⎝
1 0
0
0
0 −1 0
0
0 0 −1 0
0 0
0 −1
⎞
⎟⎟⎠= (gμν) .
We shall always use Einstein’s summation convention from now on, and thus leave
out the summation sign whenever the summation index in a product occurs once up,
once down. For the present case, xμ = gμν xν and hence x0 = x0, xk = −xk.
The Lorentz invariant scalar products are sums over products of covariant and
contravariant components. In particular, for free particles, we have
vμ vμ = c2 ,
and with pμ = m vμ ,
also pμ pμ = m2c2 .
Here m is the mass of the particle under consideration. With pμ pμ = (p0)2 −p · p,
we thus have for free particles
(E/c)2 = (m c)2 + p · p .
However, we shall generally use the equation pμ pμ = m2c2.
5.6.2
Quantum Theory
In the following we have to replace the observables by Hermitian operators, but we
shall use the same letters. In particular, p should mean the mechanical momentum
and P the canonical momentum. Here we have to account for the fact that P does

5.6 Dirac Equation
489
not commute with A. Therefore, for all bilinear equations, we shall restrict ourselves
initially to the case q A = 0 and treat the generalized case only in Sect. 5.6.8.
The Dirac equation is a relativistic equation for a wave ﬁeld ψ which we shall
interpret as a probability amplitude. For the superposition principle to remain valid,
the equation has to be linear in ψ. In addition, if ψ(t0) is given, everything at later
times should be ﬁxed. Consequently, it must also be a ﬁrst order differential equation
in time, and relativistic covariance then allows only ﬁrst derivatives with respect to
the position. We note that the Schrödinger equation also contains only ﬁrst derivatives
with respect to time, but second derivatives with respect to the position.
According to the correspondence principle, we have to obtain classical mechanics
in the classical limit of special relativity theory. However, we cannot use the equation
pμpμ = m2c2, because taking into account
Pμ = iℏ
∂
∂xμ = iℏ∂μ ,
or pμ = iℏ∂μ −q Aμ ,
it leads to a differential equation of second order, i.e., the Klein–Gordon equation
[10, 11], derived also by [12] and [13]. According to Dirac [14], we should make an
ansatz with a linear expression in pμ:
(γ μ pμ −m c) ψ = 0 ,
or (i γ μ ∂μ −q
ℏγ μ Aμ −κ) ψ(x) = 0 ,
where κ ≡m c/ℏ. (It is common practice to set ℏ= c = 1 and put the mass m instead
of κ, even though the Compton wavelength 2π/κ is a well known quantity.) Note
that, setting
γ μ = (γ 0, γ k) ,
together with pμ = (E/c, −pk) and Aμ = (/c, −Ak), we have on the one hand,
γ μ pμ = γ 0 E
c −γ · p ,
γ μ Aμ = γ 0 
c
−γ · A ,
but on the other,
γ μ ∂μ = γ 0 1
c
∂
∂t + γ · ∇,
where ∂μ = (∂/(c∂t), ∇k).
We could also have written the Dirac equation in the form (γ μ pμ + m c) ψ = 0,
because the only restriction is pμ pμ = (m c)2. In this bilinear equation, we would
have to restrict ourselves to qA = 0—the generalization to qA ̸= 0 will follow in
Sect. 5.6.8.) We must now deal with this ambiguity.

490
5
Quantum Mechanics II
5.6.3
Dirac Matrices
The novel feature in Dirac’s ansatz is to take the square root of pμ pμ, i.e., to require
pμ pμ = (γ μ pμ)2. This equation requires γ μγ ν + γ νγ μ = 0 for μ ̸= ν and γ μγ μ =
gμμ, if we assume that all the γ μ commute with the operators considered so far.
The four quantities γ μ must therefore anti-commute, i.e., they cannot be normal
numbers. If we make an ansatz with matrices, then ψ must have correspondingly
many components. We combine the last equations to give
γ μ γ ν + γ ν γ μ = 2 gμν ,
which is the basic relation deﬁning a Clifford algebra. On the right, we should write
the unit operator, but we shall leave it out for many of the following equations.
If only three such operators were necessary, then we could take the Pauli matrices
discussed on p. 308, viz.,
σ 1 =
0 1
1 0

,
σ 2 =
0 −i
i 0

,
σ 3 =
1 0
0 −1

.
Note that, for μ ∈{1, 2, 3}, we should also have a factor ± i for σ μ 2 = −1 to hold.
Together with the unit matrix, these form a complete basis for 2×2 matrices. Con-
sequently, the Dirac matrices must have a higher dimension.
Since the squares of the γ μ are equal to +1 or −1, we can form a total of 16
different products. These include unity and the four operators γ μ, plus six 2-products
γ μγ ν with μ < ν, four 3-products γ λγ μγ ν with λ < μ < ν, and ﬁnally, the 4-
product
γ 5 = i γ 0γ 1γ 2γ 3 .
The index 5 is commonly used, since μ is sometimes allowed to run from 1 to
4 instead of 0–3. In contrast, authors vary in the use of the factor i. In any case,
the abbreviation for the four-product is suggested because γ μ γ 5 + γ 5γ μ = 0 and
(γ 5)2 = 1. Therefore we shall also set gμ5 = g5μ = 0 for μ ̸= 5 and g55 = 1, which
is not common practice, and then generalize the starting equation [γ μ, γ ν]+ = 2gμν.
As basis operators, we prefer to use
σ μν = i
2 [γ μ, γ ν]
in the following, instead of the six 2-products and the four 3-products,
σ μ5 = i
2 [γ μ, γ 5] ,
and this is also not standard practice. Given that σ μν = −σ νμ, this introduces 10
new quantities for which we have included a factor of i. For μ ̸= ν (including 5), we
then have σ μν = iγ μγ ν. We also have (again including 5)

5.6 Dirac Equation
491
γ μγ ν = gμν −i σ μν ,
γ μγ νγ κ = gμν γ κ + gνκ γ μ −gκμ γ ν + 
λ<ρ
εμνκ
λρ σ λρ ,
where εμνκ λρ = εμνκλ′ρ′ gλ′λgρ′ρ, and εμνκλρ is the totally antisymmetric Levi-Civita
symbol. In particular, ε01235 = 1. For the commutators, we now have (also with 5)
[γ μ, γ ν] = −2i σ μν ,
[γ κ, σ μν] = −2i (gκν γ μ −gκμ γ ν) ,
[σ κλ, σ μν] = −2i (gκμ σ λν + gλν σ κμ −gκν σ λμ −gλμ σ κν) ,
and for the anti-commutators
[γ μ, γ ν]+ = 2 gμν ,
[γ κ, σ μν]+ = 2i 
λ<ρ
εκμν
λρ σ λρ ,
[σ κλ, σ μν]+ = 2 (i εκλμν
ρ γ ρ + gκμ gλν −gκν gλμ) .
In the last equation, the Einstein convention implies a sum over ρ. For κ ̸= λ ̸= μ ̸=
κ, we now obtain
σ κλ σ λμ = −σ λμ σ κλ = i gλλ σ κμ
and
σ κλσ κλ = gκκgλλ .
For the space-like components, we have σ 12 σ 23 = −σ 23 σ 12 = −iσ 13 = iσ 31 (and
cyclic permutations in 1, 2, 3), as is usual for the three Pauli operators, whence the
letter σ has been adopted.
We have thus introduced 16 operators γ A. Of these, only the unit operator com-
mutes with all the others, while each of the others commutes with eight and anti-
commutes with the remaining eight. Only the unit operator commutes with all four
operators γ μ.
The traces of the 15 operators γ A without the unit operator all vanish. This is
immediately clear for the ten products σ μν, because tr[A, B] always vanishes. For
the ﬁve remaining ones, using 2i gκκ γ μ = [γ κ, σ κμ], we arrive likewise at a com-
mutator and can therefore infer vanishing traces here.
Each product of two operators γ A and γ B is (except for the sign and possibly a fac-
tor of i) equal to one of the 16 operators. These 16 operators are linearly independent,
because if 
A aA γ A = 0 were to hold, then any one of them could be multiplied by
some γ B, and by forming the trace, we could conclude that aB = 0. Clearly, the linear
combination gives zero only if all coefﬁcients vanish. Therefore, the 16 operators
are indeed linearly independent.
All 16 matrices γ A are unitary for a Hermitian Hamilton operator. To show
this, we multiply the starting equation (γ μpμ −mc)ψ = 0 from the left by c γ 0
and use (γ 0)2 = 1. We have γ 0 (γ μpμ −mc) = p0 −γ 0 (γ · p + mc), and with
cp0 = iℏ∂/∂t −q, it then follows that

492
5
Quantum Mechanics II
iℏ∂ψ
∂t = H ψ ,
with H = q  + γ 0 (γ · cp + mc2) .
The Hamilton operator H is only Hermitian for γ 0 = γ 0 † and γ 0 γ k = (γ 0 γ k)† =
γ k † γ 0, so γ k † = −γ k and γ 5 † = γ 5. In what follows, we shall instead use the
equation
γ μ † = γ 0γ μγ 0 ,
for μ ∈{0, 1, 2, 3} .
Consequently, we have γ μ †γ μ = 1 for μ ∈{0, 1, 2, 3, 5}. With this, the remaining
operators σ μν are also unitary:
γ A −1 = γ A † = γA .
This is what we set out to prove, and with this we have also derived the Hamilton
operator of the Dirac theory.
5.6.4
Representations of the Dirac Matrices
Since we have arrived at 16 linearly independent operators γ A, we are dealing with at
least 4×4 matrices, which may all be written as (super-)matrices of the Pauli matri-
ces, the 2×2 zero matrix, and the 2×2 unit matrix. In the standard representation
γ 0 is diagonal, in the Weyl representation γ 5 is. For each, we set
σ kl =
σ m 0
0 σ m

,
thus in particular
σ 12 =
σ 3 0
0 σ 3

,
with (k, l, m) = (1, 2, 3) or a cyclic permutation thereof, and σ m the 2×2 Pauli
matrix. Except for these three matrices (and the unit matrix), the two representations
are different (see Table 5.2).
These representations can be unitarily transformed into each other using the oper-
ator U = (γ 0 + γ 5)/
√
2 = U † = U −1. It relates the ﬁrst and third components, or
again the second and fourth components:
(ψ1, ψ2, ψ3, ψ4) ↔(ψ1 + ψ3, ψ2 + ψ4, ψ1 −ψ3, ψ2 −ψ4)/
√
2 .
With γ 0γ k = −iσ 0k, the Hamilton operator reads H = q + γ 0 (γ · cp + mc2) in
the standard representation
HD = q  + α · cp + β mc2 =
q  + mc2
σ · cp
σ · cp
q  −mc2

,
and in the Weyl representation

5.6 Dirac Equation
493
Table 5.2 Standard and
Weyl representations of the γ
matrices
4×4 matrix
Standard
representation
Weyl
representation
&
1 0
0 −1
'
= β
γ 0
γ 5
&
0 1
1 0
'
γ 5
γ 0
&
0 −1
1 0
'
+iσ 05
−iσ 05
&
σ k
0
0 −σ k
'
−iσ k5
−iσ 0k
&
0 σ k
σ k 0
'
= αk
−iσ 0k
−iσ k5
&
0 −σ k
σ k
0
'
=
αkβ = −βαk
−γ k
γ k
HW =
q  + σ · cp
mc2
mc2
q  −σ · cp

.
The standard representation is in fact convenient for low energies, i.e., for |σ · cp | ≪
mc2, but otherwise the Weyl representation is to be preferred, not only for neutrinos
and quarks which may have very small masses, but because HW is easier to diago-
nalize. The helicity σ · p/p is a good quantum number for massless Dirac particles
(even for q ̸= 0)—neutrinos are left-handed and anti-neutrinos right-handed.
Later, we shall also need the complex-conjugate Dirac matrices for the anti-linear
operators used to describe time reversal and charge conjugation. Therefore, for μ = 0
to 3, we now consider
γ μ ∗= B γ μ B−1
=⇒
γ 5 ∗= −B γ 5 B−1 .
This ﬁxes B only up to a numerical factor. We may choose B unitary:
B† = B−1 .
This ﬁxes the modulus of the numerical factor, while its phase remains free to be
chosen.
The operator B depends on the representation of the operators γ μ. B has to com-
mute with the real γ μ (for μ ∈{0, 1, 2, 3}) and to anti-commute with the imaginary
ones. Then conversely for γ 5, e.g., γ 5B = −Bγ 5 for realγ 5. In both the represen-
tations considered above, γ 0, γ 1, γ 3, and γ 5 are real and γ 2 is imaginary, thus in
both cases B ∝σ 25, and only the phase factor remains open. We choose B = σ 25
and B real: B = B∗and thus B−1 = B† = 
B.

494
5
Quantum Mechanics II
In any case, B is antisymmetric in both representations:

B = −B .
This actually holds in all representations, because each unitary transformation leaves
B unitary and antisymmetric. If γ ′ = U γ U † holds with γ ∗= Bγ B−1 and γ ′∗=

U −1γ ∗
U = B′γ ′B′ −1, then also γ ′ = U B−1 
U B′γ ′(U B−1 
U B′)−1. There-
fore, U B−1 
U B′ commutes with the four γ ′ μ, and consequently, according to
p. 491, it is a multiple of the unit. Except for a phase factor, B′ = U ∗BU †, so

B′ = −B′ for 
B = −B.
The complex conjugation operator K also depends on the representation. In
contrast to B, it acts on all degrees of freedom, and according to Sect. 4.2.12, it is
anti-linear and anti-unitary:
K c K −1 = c∗
and
K † = K −1 = K .
But the product K B = BK does not depend on the representation. Here,
(K B)2 = K BK −1B = B∗B ,
and since B∗= −B−1, we have generally
(K B)2 = −1
and
(K B)† = (K B)−1 .
For μ ∈{0, 1, 2, 3}, we have in addition K B γ μ = K γ μ∗B = γ μK B, while
K B γ 5 = −K γ 5∗B = −γ 5K B .
5.6.5
Behavior of the Dirac Equation Under Lorentz
Transformations
The equation
(γ μpμ −mc) ψ = 0 ,
with γ μγ ν + γ νγ μ = 2 gμν ,
is written in a relativistically covariant way. For a change of coordinates xμ →x′ μ,
we have
(γ ′ μp′
μ −mc) ψ = 0 .

5.6 Dirac Equation
495
The notation γ μpμ indicates a relativistic invariant (a scalar). Hence for a homoge-
neous Lorentz transformation (see p. 232),
x′ μ = aμ
ν xν ,
with aμ
ν aμ
λ = gν
λ = aν
μ aλ
μ
and aμ
ν
∗= aμ
ν ,
the Dirac matrices have to transform as a 4-vector, viz.,
γ ′ μ = aμ
ν γ ν ,
and with γ κγ λ + γ λγ κ = 2 gκλ, it follows that
γ ′ μ γ ′ ν + γ ′ ν γ ′ μ = aμ
κ aν
λ (γ κγ λ + γ λγ κ) = 2 aμλ aν
λ = 2 gμν .
We deduce that σ μν transforms as a tensor of second rank, and the unit as a scalar.
We now prove that γ 5 = i γ 0γ 1γ 2γ 3 transforms as a pseudo-scalar if γ ′ μ =
aμνγ ν. It sufﬁces to show that
γ ′ 5 = γ 5 det a ,
because, according to p. 228, all proper Lorentz transformations have det a = +1,
while for a space inversion, we have det a = −1. The properties of the determinant
can be described with the totally antisymmetric tensor εκλμν:
εκ′λ′μ′ν′ det a = εκλμν aκ
κ′ aλ
λ′ aμ
μ′ aν
ν′ .
The matrix γ 5 can be taken as i
4! εκλμν γ κ γ λ γ μ γ ν and γ ′ 5 as i
4! εκλμν γ ′ κ γ ′ λ γ ′ μ
γ ′ ν, whence
γ ′ 5 =
i
4! εκλμν aκ
κ′ aλ
λ′ aμ
μ′ aν
ν′ γ κ′γ λ′γ μ′γ ν′ = γ 5 det a .
The claim is therefore proven, and σ μ5 ∝γ μγ 5 is an axial vector, or pseudo-vector.
With [γ μ, γ ν]+ = 2gμν = [γ ′ μ, γ ′ ν]+, it is usual to take the same Gamma matri-
ces and transfer the transformation to the states ψ. After multiplication from the left
by L , the transformed Dirac equation (γ ′ μ p′μ −mc) ψ = 0, with
γ ′ μ = L −1 γ μ L ,
becomes
(γ μ p′
μ −mc) ψ′ = 0 ,
with ψ′ = L ψ .
We may thus always calculate with the same Gamma matrices, if we transform the
states suitably.

496
5
Quantum Mechanics II
In order to determine the form of L , we start with L −1γ μL = aμν γ ν. If we
take the Hermitian conjugate and multiply on the left and right by γ 0, then it fol-
lows that γ 0L †γ 0γ μγ 0L −1 †γ 0 = aμνγ ν = L −1γ μL , and with γ 0 = γ 0 −1 and
L −1 † = L † −1, we also have L γ 0L †γ 0γ μ = γ μL γ 0L †γ 0, i.e., L γ 0L †γ 0
commutes with all four γ μ and therefore, according to p. 491, is a multiple of the
unit: L γ 0L † = b γ 0. Here b has to be real, because γ 0 and hence also the left-
hand side are Hermitian. Its sign, with L † = γ 0L −1b γ 0 or L †L = b γ 0 a0νγ ν,
is determined by a00, because γ 0 a0νγ ν = a00 −i a0kσ 0k and trσ μν = 0 leads to
4b a00 = trL †L > 0. For orthochronous Lorentz transformations, the time direc-
tion remains unchanged, so a00 > 0 (see p. 228) and also b > 0, while for time
reversal, b < 0. Here, |b| = 1, if we impose the group property that the product of
two Lorentz transformations is another Lorentz transformation, and hence (as for the
canonical transformations in Sect. 2.4.3) that det L = 1 has to be valid. Taking this
together then, only b = ±1 remains possible, so
L † = ±γ 0 L −1 γ 0 ,
with the plus sign for orthochronous Lorentz transformations and the minus sign for
time reversal. This means that L is not always unitary, and in fact ψ†ψ transforms
as the time-like component of a four-vector, as will be shown in the next section.
Let us now consider an inﬁnitesimal Lorentz transformation
aμν ≈gμν + ωμν ,
with ωμν = −ωνμ ,
and |ωμν| ≪1 ,
and make the ansatz L ≈1 −i
2 ωμν Sμν, whence L −1 ≈1 + i
2 ωμν Sμν. Then
Sμν = −Sνμ remains to be determined. Since on the one hand,
aμ
ν γ ν = L −1 γ μ L ≈γ μ −i
2 ωκλ (γ μ Sκλ −Sκλ γ μ) ,
and on the other,
aμνγν = (gμν + ωμν) γν ≈γ μ + 1
2 ωκλ (gμ
κ γλ −gμ
λ γκ) ,
we infer that −i[γ μ, Sκλ] = gμκ γλ −gμλ γκ. Here, according to p. 491, the quantity
gμκ γλ −gμλ γκ is equal to −i
2[γ μ, σκλ]. This suggests
Sμν = 1
2 σμν .
However, a term can be added which commutes with the Dirac matrices, hence a
multiple of the unit. But that contradicts the constraint det L = 1. Consequently, for
inﬁnitesimal transformations,
L ≈1 −i
4 ωμν σμν

5.6 Dirac Equation
497
holds uniquely, and, e.g., for a rotation through the small angle ε about the z-axis,
i.e., with ω21 = −ω12 = ε, all others being zero, L (ε) = 1 + i
2 ε σ12. With σ122 = 1,
this can be generalized for a ﬁnite rotation L (φ) = L φ/ε(ε) to
L = cos φ
2 + i σ12 sin φ
2 .
Here we recognize that these particles have spin 1/2. In particular, for a rotation
through 2π, the sign switches, and only after two full rotations does the system
return to its original state.
From inﬁnitesimal Lorentz transformations, we can obtain all proper Lorentz
transformations. For the improper ones, we may restrict ourselves to time reversal
and space inversion, possibly combined with a proper Lorentz transformation, and
we shall discuss these in detail in Sect. 5.6.7. There we shall also consider charge
inversion (charge conjugation). We may then understand why the solutions ψ have
four rather than two components.
5.6.6
Adjoint Spinors and Bilinear Covariants
So far we have been considering the Dirac equation (γ μpμ −mc) ψ = 0. Then, with
γ μ † = γ 0γ μγ 0, for μ ∈{0, 1, 2, 3}, and (γ 0)2 = 1, the Hermitian adjoint Dirac
equation is
¯ψ (γ μpμ −mc) = 0 ,
with
¯ψ ≡ψ† γ 0 .
Instead of the Hermitian conjugate spinors ψ†, it is better then to consider the adjoint
¯ψ of ψ, because the same operator acts on ψ and ¯ψ, once on the right, once on the
left. Here, in the standard representation, we have ¯ψ = (ψ1∗, ψ2∗, −ψ3∗, −ψ4∗),
but in the Weyl representation, ¯ψ = (ψ3∗, ψ4∗, ψ1∗, ψ2∗), where we have set ψ† =
(ψ1∗, ψ2∗, ψ3∗, ψ4∗) in both cases.
In the real-space representation of pμ = Pμ −qAμ, according to p. 489, Pμ cor-
responds to the operator iℏ∂μ. With ⟨ψ|P†
μ|xμ⟩= ⟨xμ|Pμ|ψ⟩∗= −iℏ∂μψ∗, p†
μ acts
like the operator −iℏ∂μ −q Aμ acting on the left. Consequently, we may write the
adjoint Dirac equation in the real-space representation in the form
(iℏ∂μ + q Aμ) ¯ψ γ μ + mc ¯ψ = 0 ,
or free of any representation, (Pμ + qAμ) ¯ψ γ μ + mc ¯ψ = 0.
For an orthochronous transformation ψ →ψ′ = L ψ with L † = +γ 0L −1γ 0,
we have
¯ψ′ = ψ′ † γ 0 = ψ† L † γ 0 = ψ† γ 0 L −1 = ¯ψ L −1

498
5
Quantum Mechanics II
and ¯ψ′γ μψ′ = ¯ψL −1γ μL ψ = ¯ψγ ′μψ. Thus,
¯ψ 1
ψ scalar ,
¯ψ γ μ ψ vector ,
¯ψ σ μν ψ tensor ,
¯ψ σ μ5 ψ axial vector ,
¯ψ γ 5
ψ pseudo-scalar ,
as was to be expected for the operators γ A according to the last section.
From the differential equations for ψ(x) and ¯ψ(x), viz.,
γ μ (iℏ∂μ −q Aμ) ψ = +mc ψ,
i.e.,
γ μ∂μ ψ = −i
ℏ(q Aμ γ μ ψ + mc ψ) ,
(iℏ∂μ + q Aμ) ¯ψ γ μ = −mc ¯ψ,
i.e.,
∂μ ¯ψ γ μ = + i
ℏ(q Aμ ¯ψ γ μ + mc ¯ψ) ,
we deduce the “continuity equation”
∂μ ( ¯ψ γ μ ψ) = 0 ,
and according to p. 239, a conservation law for
	
d3r ¯ψ γ 0 ψ =
	
d3r ψ†ψ ≥0.
Therefore, we relate the time-like component ¯ψ γ 0ψ to a “density”, in fact the charge
density, as will be shown in the next section. However, the different components of
γ μ do not commute with each other, and therefore the probability current is not sharp.
This is worth noting for a plane wave, which solves the Dirac equation in the ﬁeld-free
space (with Aμ = 0). Therefore, we often speak here of Zitterbewegung (trembling
motion), but we should nevertheless explain the fact that ψ has four components,
not just two, as would have been expected for spin-1/2 particles. Hence we consider
improper Lorentz transformations and then treat the phenomenon of Zitterbewegung
on p. 505.
5.6.7
Space Inversion, Time Reversal, and Charge
Conjugation
For these three improper Lorentz transformations, the Dirac equation keeps the same
form. However, for time reversal and charge conjugation, we also need here the anti-
linear complex conjugation operator K , which already appeared for time reversal in
non-relativistic quantum mechanics (see p. 313). Since the operator K does not act
only on the Dirac matrices, but also on the remaining quantities, we shall now give
the full transformation operator, differently from the proper Lorentz transformations
considered so far.
Under a the space inversion, all polar three-vectors change their sign, while the
axial vectors do not—so all time-like components remain conserved. Consequently,
(P′0, P′k) = (P0, −Pk) and also (′(t′, r ′), A′(t′, r ′)) = ((t, −r ), −A(t, −r )).

5.6 Dirac Equation
499
The Dirac equation thus keeps the same form if (γ 0, γ k) are transformed into
(γ 0, −γ k). This can be done with
P = γ 0 P0 ,
whereP0 istheinversionintheusualspace,whichwealreadyneedinnon-relativistic
quantum mechanics. The sign remains undetermined, because a rotation by 2π
changes the sign of ψ without changing any measurement values. The phase factor
has been chosen such that
P2 = 1 ,
as in the non-relativistic case. We then also have P = P† = P−1 and
(γ μ p′
μ −mc) Pψ = 0 ,
as claimed.
Under time reversal, (t, r ) has to change into (t′, r ′) = (−t, r ) and (, A)
changesinto(′(t′, r ′), A′(t′, r ′)) = ((−t, r ), −A(−t, r )),becausethemagnetic
ﬁeld switches sign for motion reversal. The position vectors remain the same for time
reversal, but not the momentum vectors. We thus need an anti-linear transformation,
as was shown already on p. 313.
In fact, the time reversal operator T in real space has the same properties as the
anti-linear complex conjugation operator K , but the latter also changes the Dirac
matrices, as we have seen in Sect. 5.6.4. Only the operator K B commutes with
them. B acts like a unit operator in real space.
For the invariance of the Dirac equation under time reversal (motion reversal),
we need an anti-linear operator which changes the sign of the three space-like Dirac
matrices. This we can do with
T = γ 0K B ,
where the sign is arbitrary. Since (γ 0K B)2 = (γ 0)2(K B)2 with (γ 0)2 = 1 and
(K B)2 = −1, we thus have
T 2 = −1
and
T † = T −1 .
These two properties do not depend on the representation. In both the standard and
the Weyl representation (with B = σ 25), we have to take T = iσ 31K .
Starting with the adjoint Dirac equation (Pμ + qAμ) ¯ψ γ μ + mc ¯ψ = 0 of the last
sections, we can construct the charge-conjugate solution. In particular, if we take the
space-inverted matrices of this equation and set
(
γ μ = −U −1 γ μ U ,
for μ ∈{0, 1, 2, 3}
(=⇒

γ 5 = + U −1 γ 5 U ) ,

500
5
Quantum Mechanics II
and if we multiply (
γ μ (Pμ + qAμ) ¯ψ + mc¯ψ = 0 by −U , we obtain
{γ μ (Pμ + qAμ) −mc} U ¯ψ = 0 .
The sign of the charge q has been reversed here, relative to that in the original Dirac
equation, and this is the required charge conjugation. Hence we infer the charge
conjugation operator
C = γ 0U K ,
since with ¯ψ = ψ†γ 0, we have ¯ψ = 
γ 0ψ∗and therefore U ¯ψ = −γ 0U K ψ. Note
that the phase factor remains arbitrary.
The properties of the operators U follow from (
γ μ = −U −1γ μU , but only
up to a factor, which allows us to choose U unitary, i.e., U −1 = U †. Since
(
γ μ = (γ μ †)∗= (γ 0γ μγ 0)∗= Bγ 0γ μγ 0B−1, we must still require γ 0γ μγ 0 =
−B−1U −1γ μU B. Thus the three operators γ k commute with U B, while γ 0
and γ 5 anti-commute with it. Therefore, U B is proportional to σ 05, independently
of the representation, and consequently U with B−1 = −B is proportional to σ 05B.
The still missing factor has to have the absolute value one, because U , σ 05, and B
should be unitary. We can thus write U = uσ 05B with |u| = 1.
For the charge conjugation operator C , we thus have uγ 0σ 05K B = iuγ 5K B.
In the following, we choose u = −i, whence the charge conjugation operator is
C = γ 5K B .
Independently of the representation, we thus ﬁnd
C † C = (γ 5K B)† γ 5K B = (K B)−1γ 5 † γ 5K B = 1 ,
along with C 2 = (γ 5K B)2 = −(γ 5)2(K B)2 = 1, and hence,
C † = C −1 = C .
The charge conjugation operator is thus unitary and anti-commutes with all Gamma
matrices except for the unit: C γ A = −γ A C , for γ A ̸= 1. Due to the factor K , it
is anti-linear and therefore C Pμ = −Pμ C , but C Aμ = Aμ C . In both the standard
and the Weyl representation, we have C = −iγ 2K .
The common transition T P (= PT ) is thus described by K BP0, and the
transitionT PC (= −C PT )byγ 5P0.Inthenextsection,wewillseehowimpor-
tant the operator γ 5 = γ 5 † is. But let us already recognize a noteworthy property
of the CPT transformation: with (γ 5)2 = 1 and γ μγ 5 = −γ 5γ μ, it leaves scalars,
pseudo-scalars, and tensors of second rank unaltered, while for vectors and pseudo-
vectors, the sign changes—such statements form the object of the CPT theorem.
If we denote the charge-conjugate state of |ψ⟩by |ψ⟩c, then by p. 314,

5.6 Dirac Equation
501
|c⟨ϕ|ψ⟩c = ⟨ϕ|ψ⟩∗
and
|c⟨ϕ| C OC −1 |ψ⟩c = ⟨ψ| O† |ϕ⟩.
With C γ AC −1 = −γ A, for γ A ̸= 1, and γ A † = γA, this leads to the expectation
values
⟨γ A⟩c = −⟨γA⟩,
thus
⟨γ 0⟩c = −⟨γ 0⟩,
⟨γ k⟩c = +⟨γ k⟩,
⟨σ 0k⟩c = +⟨σ 0k⟩,
⟨σ ik⟩c = −⟨σ ik⟩,
and
⟨1⟩c = +⟨1⟩,
⟨X μ⟩c = +⟨X μ⟩,
⟨Pμ⟩c = −⟨Pμ⟩,
⟨Aμ⟩c = +⟨Aμ⟩.
Moreover, H = q + γ 0{γ · c (P −qA) + mc2} yields C H = −H C , or
⟨H(q)⟩c = −⟨H(−q)⟩.
Thus the eigenvalues of the Hamilton operator change their sign along with the
charge. If we take them as energy eigenvalues, then we necessarily arrive at negative
energy values and ﬁnd no ground state. Thus an arbitrary amount of energy could be
emitted. (However, for time-dependent forces the Hamilton operator and the energy
operator agree only for a suitable gauge, so we can also require E = |H| here.)
We can repair this difﬁculty if we quantize the ﬁeld and attach zero energy to the
vacuum. Every particle creation should cost energy, independently of the charge.
Due to charge conservation, particles can only be created from the vacuum in pairs
of opposite charge, and with a supply of energy. Then twice the energy is necessary
compared with what would be required for one particle (disregarding the binding
energy between the two).
Here we recall the non-relativistic Fermi gas. In its ground state, all one-particle
states below the Fermi edge are occupied, while all those above are empty. Adding
energy raises a fermion from an occupied state to an unoccupied one. The excited
state differs from the ground state by a particle–hole pair. This picture is also suitable
for the Dirac theory. We only have to choose the Fermi edge as the zero energy, i.e.,
as a quasi-vacuum. If a particle is missing from this quasi-vacuum, then we have a
hole, i.e., an anti-particle, which is a particle of opposite charge and energy (see Fig.
5.21).
As the quantity adjoint to ψc = C ψ, we take ψc = ψ†γ 0C . This implies ψcγ 0ψc
= ψ†γ 0C γ 0C ψ = −ψ†(γ 0)2C 2ψ = −ψ †γ 0ψ,asexpected,with⟨γ 0⟩c = −⟨γ 0⟩.
5.6.8
Dirac Equation and Klein–Gordon Equation
We now turn to the problem mentioned in Sect. 5.6.2 that P does not commute with
A, and therefore additional terms occur for qA ̸= 0 compared to the Klein–Gordon
equation

502
5
Quantum Mechanics II
Fig. 5.21 Charge symmetry. For charge inversion, the signs of ⟨H⟩, ⟨P ⟩, ⟨p ⟩, and ⟨σ ⟩are all
reversed. The continuum of H eigenvalues of free particles is indicated by dotted lines (left). The
eigenvalues of p are shown next to it: top for ⊕and bottom for anti-particles ⊖, and right next to it
the same after time reversal (right)
(pμpμ −m2c2) ψ = 0 .
To this end, it can be advantageous to use the projection operators
P± ≡1
2 (1 ± γ 5) = P±
† = P±
2 ,
P± P∓= 0 ,
P+ + P−= 1 .
They commute with pμ, but not with γ μ, for μ ∈{0, 1, 2, 3}:
P± γ μ = γ μ P∓,
but P± γ 5 = γ 5 P± = ±P± .
Therefore, P±γ μpμ ψ = γ μpμP∓ψ also holds. On the other hand, the Dirac equa-
tion implies γ μ pμψ = mc ψ, and mc commutes with P±, so from P∓mcψ =
P∓γ μpμψ = γ μpμP±ψ, we may infer
(P∓+ P±) mc ψ = (γ μ pμ + mc) P±ψ ,
where P∓+ P± = 1 and division by mc is allowed for m ̸= 0. From a component
P+ψ or P−ψ, we thus obtain the total solution ψ which has to satisfy the Dirac
equation. Consequently,
(γ μ pμ −mc) (γ ν pν + mc) P±ψ = (γ μγ ν pμpν −m2c2) P±ψ = 0 ,
i.e., each component P± ψ satisﬁes the same equation. With γ μγ ν = gμν −iσ μν
and σ μνpμpν = −σ νμpμpν = −σ μνpνpμ = 1
2σ μν[pμ, pν], together with
[pμ, pν] = [Pμ −q Aμ, Pν −q Aν] = q ([Pν, Aμ] + [Aν, Pμ])
= iqℏ(∂νAμ −∂μAν) = −iqℏFμν ,
the equation for the components can be reformulated as
(pμpμ −m2c2 −1
2 qℏσ μν Fμν) P±ψ = 0 .

5.6 Dirac Equation
503
The operator pμpμ −m2c2 of the Klein–Gordon equation must therefore be amended
by the term −1
2qℏσ μν Fμν. This couples the different components of P±ψ via the
operators σ μν, and in the standard representation, disregarding a factor qℏ, it reads
−1
2 (σ μν Fμν)D = σ · B −iα · E/c =

σ · B
−iσ · E/c
−iσ · E/c
σ · B

,
while in the Weyl representation, it reads
−1
2 (σ μν Fμν)W =
σ · (B −iE/c)
0
0
σ · (B + iE/c)

.
Since the projection operators P± are also diagonal in the Weyl representation,
P+ =
1 0
0 0

,
P−=
0 0
0 1

,
this leads us to 2-spinors ψ± ≡(P± ψ)W, an advantage over the standard represen-
tation:
{pμpμ −m2c2 + qℏσ · (B ∓iE/c)} ψ± = 0 .
Generally, we have
pμpμ = (Pμ −q Aμ) (Pμ −q Aμ) = PμPμ −q (PμAμ + AμPμ) + q2 AμAμ .
Now Pμ commutes with Aμ for the Lorentz gauge ∂μAμ = 0, so it follows that
PμAμ + AμPμ = 2 AμPμ. In the scalar product, the order of the operators P and A
is thus irrelevant for the Lorentz gauge and we obtain
pμpμ = (E −q )2/c2 −(P −q A)2 ,
whereupon
{(E −q)2 −c2 (P −qA)2 −(mc2)2 + qℏc σ · (cB ∓iE)} ψ± = 0 .
In this way we have reformulated the Dirac equation as two similar equations for
2-spinors, each being an equation for spin-1/2 particles. (In the standard represen-
tation, the same goal is pursued with the Foldy–Wouthuysen transformation, but this
proceeds only stepwise and approximations have to be made.)
How are the components ψ+ and ψ−to be interpreted? To ﬁnd out, we consider the
equation K Bγ 5 = −γ 5K B. It yields C P± = P∓C . If P±ψ describes a particle,
then C P±ψ describes its anti-particle, which we ﬁnd as P∓C ψ in the complementary
space of the particle. We may thus interpret ψ+ as a particle and ψ−as its anti-particle.
In the non-relativistic limit, we have E −q ≈mc2 and consequently,

504
5
Quantum Mechanics II
(E −q)2 −(mc2)2 ≈2 mc2 (E −q −mc2) .
In addition, we may then neglect ℏσ · E compared to 2mc , since for E = −∇
with x · P ≥1
2ℏ, we have

ℏσ · E
2mc
 ≈
ℏ
2mc

x
≤P 
mc
≪|| .
Therefore, in the non-relativistic limit, we ﬁnd the Pauli equation (see p. 327)

E −

mc2 + (P −q A)2
2m
+ q  −qℏ
2m σ · B

ψ± = 0 .
Hence there is a real magnetic dipole moment qℏσ/2m, and according to the pre-
ceding equation, there is also an electric dipole moment, although this is imaginary
and therefore not observable, as Dirac himself stressed [14].
5.6.9
Energy Determination for Special Potentials
For free motion (qAμ = 0), we arrive at the equation
E2 −c2P2 −(mc2)2 = 0
=⇒
E = c

(mc)2 + P2 .
Here, the energy does not depend on the spin (degeneracy). In addition to the momen-
tum, the helicity σ · p/p also commutes with the free Hamilton operator (in both the
standard and the Weyl representation). Therefore the free 2-spinors can be decom-
posed in terms of their helicity (η = ±1). If p has the direction (θ, ϕ), then the
helicity states, i.e., the eigenstates of (σx cos ϕ + σy sin ϕ) sin θ + σz cos θ, can be
represented by
|+⟩=
c
s

and
|−⟩=
−s∗
c

,
where we use the abbreviations c ≡cos( 1
2θ) and s ≡sin( 1
2θ) exp(iϕ), along with
⟨+| = (c, s∗) and ⟨−| = (−s, c). The directions of p and σ are reversed under charge
conjugation, so the helicity is conserved.
So far we have had to write the Hamilton operator for the free motion as a 4×4
matrix
HD = γ 0 mc2 + c γ 0γ · P ,
but now we can decompose it into two 2×2 matrices, viz.,

5.6 Dirac Equation
505
H± = ±c

(mc)2 + P2 ,
where we choose H+ for particles and H−for anti-particles.
The advantage of this separation can be illustrated by considering, e.g., the veloc-
ity. We determine the derivative of the position operator R with respect to time via
the Heisenberg equation. In the standard representation, this yields
dRD
dt
≡[R, HD]
iℏ
= c γ 0γ .
Hence not all three Cartesian velocity components—each with modulus c—can be
sharp simultaneously, because they do not commute with each other. This is often
interpreted as Zitterbewegung. But with [R, f (P2)] = 2iℏ(∂f /∂P2) P, we also have
the equation
dR±
dt
≡[R, H±]
iℏ
= c cP
H±
,
which does indeed make sense, because according to p. 245, for free particles, we
have p = c−2Ev. The split into particles and anti-particles clariﬁes this matter. The
anti-particles move against their momenta here.
For free motion, the associated 4×4 matrix HW (see p. 492) can also be decom-
posed into two 2×2 matrices, one for each of the two helicities η = ±1. If we now
use the parameter τ to distinguish particles (τ = 1) and anti-particles (τ = −1), then
we obtain the eigenvalue equation
η cp −τE
mc2
mc2
−η cp −τE
 ψτη
ϕτη

= 0 .
This leads to the above-mentioned energy eigenvalue (with E > 0) and to
ψτη
ϕτη
= τE + η cp
mc2
=
mc2
τE −η cp .
For the normalization, we invoke the invariant ¯ψψ = 2Re ψτη∗ϕτη, noting that
|ψτη|2 + |ϕτη|2 is not suitable here. Then the expressions just found deliver
¯ψψ = 2|ψτη|2 Re ϕτη
ψτη
= 2|ψτη|2
mc2
τE + η cp
= 2|ϕτη|2 Reψτη
ϕτη
= 2|ϕτη|2
mc2
τE −η cp .
With E > cp, ¯ψψ is positive for particles (τ = 1) and negative for anti-particles
(τ = −1). We therefore require ¯ψψ = τ and infer

506
5
Quantum Mechanics II
Fig. 5.22 Large amplitude (red) and small amplitude (magenta). These differ by the product τη=
±1 and depend on p/mc. In the case of free motion considered here, we have p = mγ v, with v/c = β
and hence p/mc = γβ and the approximation √p/mc (dashed blue)
|ψτη|2 = E + τη cp
2mc2
and
|ϕτη|2 = E −τη cp
2mc2
.
We choose ψτη and ϕτη real, and ψτη ≥0. With this and with 2Re ψτη∗ϕτη = τ, the
sign of ϕτη is the same as that of τ:
ψτη =

E + τη cp
2mc2
and
ϕτη = τ

E −τη cp
2mc2
,
again with E > 0. For high energies, E ≈cp and therefore one or the other amplitude
is negligible—but |Reψτη∗ϕτη| = 1
2. We speak here of large and small amplitudes
(see Fig. 5.22).
For the Weyl representation, these expressions are then also to be multiplied by
the helicity amplitudes mentioned above:
⎛
⎜⎜⎝
ψ++ c
ψ++ s
ϕ++ c
ϕ++ s
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
−ψ+−s∗
ψ+−c
−ϕ+−s∗
ϕ+−c
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
ψ−+ c
ψ−+ s
ϕ−+ c
ϕ−+ s
⎞
⎟⎟⎠,
⎛
⎜⎜⎝
−ψ−−s∗
ψ−−c
−ϕ−−s∗
ϕ−−c
⎞
⎟⎟⎠.
The momentum eigenfunction must be included with all these “internal” wave func-
tions.
In a homogeneous magnetic ﬁeld B = B0, the Coulomb gauge is A = 1
2 B0 ×
R, with  and E equal to zero, and we have P · A = A · P = 1
2ℏB0 · L, where we
introduce the dimensionless quantity L = R × P/ℏ. This yields
(P −q A) · (P −q A) = P2 −qℏB0 · L + q2A2
and
E2
c2 = (mc)2 + P2 −qℏB0 · (L + σ) + q2
4 (B0 × R)2 .

5.6 Dirac Equation
507
For charge conjugation, q is to be replaced by −q and ⟨L + σ⟩by −⟨L + σ⟩, and
we thus arrive at the same value of |E|, despite the charge (a)symmetry.
For the hydrogen problem with q = −e2/(4πε0 r) and B ≡0, it is advantageous
to take the ﬁne-structure constant
α ≡
e2
4πε0
1
ℏc =
1
137. . . . ,
and use the further abbreviation r ′ = r/r. With P2 = −ℏ2(d2/dr2 −L2/r2), we
arrive at the differential equation
 d2
dr2 −m2c4 −E2
ℏ2c2
+ α E
ℏc
2
r −L2 ∓iα r ′ · σ −α2
r2

ψ(r) = 0 .
It is similar to the non-relativistic radial equation of the hydrogen problem, investi-
gated in more detail in [15] (see also p. 422):
 d2
dρ2 −1 + 2η
ρ −l(l + 1)
ρ2

ul(ρ) = 0 ,
with the Coulomb parameter η (not to be confused with the helicity, which we shall
no longer consider). Normalizability requires η −l to be a natural number (1, 2, 3,
. . .). We shall denote it by nr+1, whence nr gives the number the nodes of the radial
function. (The zeros at the boundaries 0 and ∞do not constitute nodes.)
To exploit this well known result, we now have to express the eigenvalues of
L2 ∓i α r ′ · σ −α2 in terms of λ(λ + 1). In fact, λ is somewhat smaller than l, as
will now be shown.
The dipole–ﬁeld coupling ∝σ · r ′/r2 does not commute with the orbital angular
momentum, but like any scalar, it does commute with the total angular momentum
(L + S) ℏ, so for the spin angular momentum, we split off the factor ℏand write S =
1
2σ. It is therefore appropriate to take the coupled representation |(l 1
2)jm⟩of p. 336. In
particular, the operator L · σ is diagonal, and with L × L = iL, according to p. 325,
we have (L · σ)2 = L2 + i(L × L) · σ = L2 −L · σ, so L2 = L · σ (L · σ + 1).
The term ∓i α r ′ · σ −α2 with (r ′ · σ)2 = 1 may also be written
∓i α r ′ · σ (1 ∓i α r ′ · σ) .
Therefore, it follows that
L2 ∓i α r ′ · σ −α2 =  ( + 1) ,
with  = −L · σ ∓i α r ′ · σ −1 ,
ifwecanprovethatL · σ r ′ · σ + r ′ · σ L · σ = −2r ′ · σ.Here,accordingtop.325,
the left-hand side is equal to (L · r ′ + r ′ · L) + i (L × r ′ + r ′ × L) · σ, and with
this the ﬁrst bracket vanishes, because L and 1/R commute and we have R · (R ×
P) = −R · (P × R) = −(R × P) · R. For the second, we may use R·P = 3iℏ+ P·

508
5
Quantum Mechanics II
R along with [R, P·R] = iℏR and [P, R2] = −2iℏR. This leads to L × R + R ×
L = 2iR, whence L · σ r ′ · σ + r ′ · σ L · σ is indeed equal to −2r ′ · σ. We thus
obtain
2 = (L · σ + 1)2 −α2 .
The eigenvalues of this Hermitian operator depend only on α2, not on the sign ∓iα.
But in our further calculations, we have to distinguish between j = l ± 1
2 and we also
need the different signs now for another purpose.
In particular, by p. 372,
(L · σ + 1) |(l 1
2)jm⟩= ± |(l 1
2)jm⟩(j + 1
2) ,
for j = l ± 1
2 (∈{ 1
2, . . .}) ,
and consequently,
2 |(l 1
2)jm⟩= |(l 1
2)jm⟩{(j + 1
2)2 −α2} ,
as well as
 |(l 1
2)jm⟩= ∓|(l 1
2)jm⟩{(j + 1
2)2 −α2}1/2 ,
for j = l ± 1
2 .
Note that the sign follows from the limit α →0, whence  tends towards −L · σ −1.
If we now denote the eigenvalue of  ( + 1) by λ(λ + 1), we have
λ = {(j + 1
2)2 −α2}1/2 −(j + 1
2 −l) ,
and hence,
λ = l −εj ,
with εj ≡j + 1
2 −
#
(j + 1
2)2 −α2 ≈
α2
2j + 1 ≪1 .
With this we may now return to the known result of the non-relativistic calculation.
Comparing the two radial equations with (m2c4 −E2)/(ℏc)2 = k2 and αE/(ℏc) =
ηk leads to
η =
α E
√
m2c4 −E2
=⇒
E =
mc2

1 + (α/η)2 .
Normalizability now requires
η = nr + 1 + λ = n −εj ,
with the principal quantum number n ≡nr + l + 1 (see p. 363). Finally, we obtain

5.6 Dirac Equation
509
E =
mc2

1 + α2/(n −εj)2 = mc2 −ER
n2

1 + α2
n

1
j + 1
2
−3
4n

+ · · ·
"
,
where j ∈{ 1
2, . . . , n−1
2}, so that 1/(j + 1
2) > 3/4n, and the Rydberg energy (see
p. 362)
ER ≡1
2 α2 mc2 .
As can already be seen from Fig. 4.18, there is now no degeneracy with respect to
the angular momentum j (only with respect to the orbital angular momentum l), in
contrast to the non-relativistic calculation. The terms indicated by dots in the above
may be left out, being smaller than the effects neglected in the Dirac theory (like the
Lamb shift mentioned on p. 380).
5.6.10
Difﬁculties with the Dirac Theory
In fact, the Dirac equation describes electrons (and neutrinos) better than the
Schrödinger equation, because it accounts for relativistic effects and spin (although it
is still not the end of the story). In particular, it also holds for anti-particles (positrons),
and their energy spectrum is reﬂected at E = 0. There are thus inﬁnitely many states
of negative energy, with no lower bound. In particular, the free Dirac equation allows
any energy above mc2 and below −mc2, but none in-between.
Dirac suggested viewing the vacuum as a many-body state, where all states of
negative energy are occupied and all states of positive energy empty. If this vacuum
is excited by more than 2mc2 (through photon absorption), then a particle switches
from a state of negative energy into a state of positive energy. This creates a particle–
hole pair, which may be interpreted as electron–positron pair creation. Conversely,
there may also be pair annihilation, where a particle makes a transition to a hole state
and emits electromagnetic radiation.
Even though pair generation and annihilation may be described with the hole
theory, the Dirac equation leaves some questions open. In particular, it cannot be
a one-particle theory. The many particles of negative energy should interact with
each other. In addition it remains to be clariﬁed whether electrons or positrons have
negative energy. These problems can only be tackled by ﬁeld quantization.
List of Symbols
We stick closely to the recommendations of the International Union of Pure and
Applied Physics (IUPAP) and the Deutsches Institut für Normung (DIN). These
are listed in Symbole, Einheiten und Nomenklatur in der Physik (Physik-Verlag,

510
5
Quantum Mechanics II
Table 5.3 Symbols used in quantum mechanics II
Symbol
Name
Page number
H
Full Hamilton operator 404
H0
Free Hamilton
operator
404
V
Interaction operator
404
G
Propagator for H
405
G0
Propagator for H0
405
S
Scattering operator
414
T
Transition operator
415
One-particle operator
444

Solid angle
417
±
Möller’s wave
operators
413
P, Q
Projection operators
413
| ⟩±
Scattering states
412
*
σ
Scattering
cross-section
418
δ
Scattering phase
421
*

Level width
425

Annihilation operator
442
†
Creation operator
442
N
Particle number
operator
443
γ μ
Dirac matrix
489
σ μν
Dirac matrix
490
Weinheim 1980) and are marked here with an asterisk. However, one and the same
symbol may represent different quantities in different branches of physics. Therefore,
we have to divide the list of symbols into different parts (Table 5.3).
References
1. H. Feshbach, Ann. Phys. 19, 287 (1962)
2. E.P. Wigner, L. Eisenbud, Phys. Rev. 72, 29 (1947)
3. A.M. Lane, R.G. Thomas, Rev. Mod. Phys. 30, 257 (1958)
4. P.I. Kapur, R. Peierls, Proc. Roy. Soc. A 166, 277 (1937)
5. P.A. Kazaks, K.R. Greider, Phys. Rev. C 1, 856 (1970)
6. E.W. Schmid, H. Ziegelmann, The Quantum Mechanical Three-Body Problem (Vieweg, Braun-
schweig, 1974)
7. G.E. Brown, Many-Body Problems (North-Holland, Amsterdam, 1972), p. 22
8. C. Cohen Tannoudji, J. Dupont-Roc, G. Grynber, Photons and Atoms (Wiley, New York, 1989),
Chap. 5

References
511
9. E. Schrödinger, Naturwissenschaften 14, 664 (1926)
10. O. Klein, Z. Phys. 37, 895 (1926)
11. W. Gordon, Z. Phys. 40, 117 (1926)
12. E. Schrödinger, Ann. Physics 79, 489 (1926)
13. V. Fock, Z. Phys. 38, 242; 39, 226 (1926)
14. P.A.M. Dirac, Proc. Roy. Soc. A 117, 610 (1928)
15. R.A. Swainson, G.W.F. Drake: J. Phys. A 24, 79, 95 (1991)
Suggestions for Textbooks and Further Reading
16. W. Greiner, J. Reinhardt: Field Quantization (Springer, New York 1996)
17. W. Greiner: Relativistic Quantum Mechanics: Wave Equations (Springer, New York 2000)
18. V.B. Berestetskii, E.M. Lifshitz, L.P. Pitaevskii, Course of Theoretical Physics—Quantum
Electrodynamics, vol. 4 ,2nd edn. (Butterworth–Heinemann, Oxford 1982)

Chapter 6
Thermodynamics and Statistics
6.1
Statistics
6.1.1
Introduction
Although this chapter is announced in the usual way as being about thermodynamics
and statistics, we shall nevertheless begin with statistics. Then we shall be able to
justify thermodynamics with quantum theory, and present the entropy S in a more
logical way.1 The entropy is a key basic notion in the theory of heat and must oth-
erwise be introduced axiomatically. In such a representation, thermodynamics starts
with the following main theorems, where the notion of state variable appears three
times and, as an observable, is associated with the instantaneous state of the consid-
ered system, e.g., position, momentum, and energy in particle mechanics:
Zeroth main theorem (R. H. Fowler): There is a state variable called temperature T
(in kelvin K). Two systems (or two parts of a systems) are only in thermal equilibrium
if they have equal temperature.
First main theorem (R. Mayer, H. v. Helmholtz): There is a state variable called
the internal energy U of the system. It increases by the (reversible or irreversible)
addition of an amount of heat δQ and addition of work δA:
1It is interesting to quote Carathéodory [1] in his inaugural address to the Prussian Academy as cited
in [2]: “It is possible to ask the question as to how to construct the phenomenological science of
thermodynamics when it is desired to include only directly measurable quantities, that is volumes,
pressures, and the chemical composition of systems. The resulting theory is logically unassailable
and satisfactory for the mathematician because, starting solely with observed facts, it succeeds with
a minimum of hypotheses. And yet, precisely these merits impede its usefulness to the student of
nature, because, on the one hand, temperature appears as a derived quantity, and on the other, and
above all, it is impossible to establish a connection between the world of visible and tangible matter
and the world of atoms through the smooth walls of the all too artiﬁcial structure.”
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5_6
513

514
6
Thermodynamics and Statistics
dU ≡δQ + δA .
Note that dU is a complete differential, while the terms on the right-hand side are
not necessarily so. For a cycle,

dU = 0 holds, while not all closed integrals of the
individual quantities on the right would vanish. Therefore, U is a state variable, but
heat and work are not, as already stressed in Fig. 2.1 (more on that in Sect. 6.4.2).
Symbols containing δ are common in variational calculus (see Sects. 2.1.2 and 2.1.3).
For a closed system the energy conservation law holds, i.e., dU = 0. Generally, there
are no conservation laws for heat or work alone.
Second main theorem (R. Clausius, W. Thomson/Lord Kelvin): There is a state vari-
able called entropy S. This increases by the reversibly added quantity δQrev/T ,
dS ≡δQrev
T
,
and for a closed system it can only increase with time:
dS
dt ≥0 ,
for a closed system.
This inequality is called the entropy law.
Third main theorem (W. Nernst): At the absolute zero of the temperature T = 0, the
entropy depends only on the degree of degeneracy of the ground state. There we can
set S = 0.
The entropy seems to many people like a mysterious auxiliary quantity. What is
important for its measurement is the amount of heat added reversibly, and this
depends only on the entropy, whether or not a process can be reversed in a closed
system. It may possibly break time-reversal invariance.
On the other hand, if we begin with statistics and derive the phenomena associated
with heat from the disordered motion of particles like molecules, atoms, or photons,
as described in [3, 4], for example, then we can begin by introducing the information
entropy (the many different possibilities of expression). This can be used to justify
the main theorems of thermodynamics. On the other hand, in statistics we rely on
“sensible” assumptions.
Therefore, we can already clarify the notion of entropy in this section. In Sect. 6.2,
we introduce the time dependence and justify the entropy law. After that we will
consider equilibrium distributions and use this to understand what entropy can do
for us. In Sect. 6.4, we can then deal with the main theorems of thermodynamics and
subsequently turn to applications.
In the following, we consider systems with very many degrees of freedom (very
many “particles”), whose individual characteristics neither can nor shall be pursued
in detail. If we take a mole of some gas (i.e., nearly a septillion molecules), then we
can neither solve the coupled equations of motion, nor set the initial conditions for all

6.1 Statistics
515
the individual particles correctly and actually follow their time evolution. In fact, we
do not want to observe the single molecules, but only a few properties (parameters)
of the system as a whole. We can ﬁx the macro state through a handful of collective
(macroscopic) parameters and follow its evolution, but not the basic micro state,
which contains far too many microscopic parameters. (Even if only a few particles
appear to be important, their coupling to the environment with its many degrees
of freedom cannot be switched off completely, and this environment is continually
changing.)
A truly enormous number of different micro states belong to any given macro
state, speciﬁed by its particle number and type, its energy and volume, etc. We shall
treat these many states using statistical methods. All the micro states belonging to
the same macro state form a statistical ensemble.
6.1.2
Statistical Ensembles and the Notion of Probability
A statistical ensemble is described by a small number of parameters, while many
other parameters vary from member to member within the ensemble. As an example,
we have already mentioned a gas of molecules, whose energy, volume, and particle
number have been given. Another statistical ensemble need not even assume a ﬁxed
number of particles. So the local particle densities in the considered gases may differ
signiﬁcantly from the mean value N/V. The different values occur with different
sub-ensembles of particles in the ensembles.
From the occurrence of an attribute (signature) in a sequence (of micro states), we
may infer its probability, i.e., its relative occurrence in the limit of large sequences.
If we consider, e.g., the results of tossing a dice, then the “6” will not always occur
exactly once for very six throws (sometimes not at all, sometimes repeatedly), but for
a fair dice every number z ∈{1, . . . , 6} will occur on the average equally often. The
probabilities ρz for a fair dice do not depend on z. Summed over all possibilities, we
must therefore obtain unity, i.e., 
z ρz = 1, because the probability that the event
z1 or z2 occurs, is generally equal to ρ1 + ρ2 (to be contrasted with the probability
that only z1, then z2 appears, which is equal to ρ1 · ρ2, and that once z1 and once z2
appears, equal to ρ1 · ρ2 + ρ2 · ρ1 = 2ρ1 · ρ2). With ρ1 = · · · = ρ6 and 6
z=1 ρz =
1, we conclude that the probability ρz for each number of spots is equal to 1/6, for a
fair dice.
If z is generally a natural number which may assume Z values, and ρz the asso-
ciated probability (relative occurrence in the statistical ensemble), then ρz is real,
non-negative, and normalized:
ρz = ρz
∗≥0 ,
Z

z=1
ρz = 1 .
If z is a continuous variable, then ρ(z) will be a probability density, and instead of
the sum, there will be an integral. Only ρ(z) dz is then a probability, namely that the
variable takes a value between z and z + dz.

516
6
Thermodynamics and Statistics
The mean value of a quantity Az in an ensemble given by {ρz} is clearly
A ≡⟨A⟩=
Z

z=1
ρz Az ,
since each value Az is weighted here with its associated probability. In quantum
mechanics (see Sect. 4.2.11), ρ and A are Hermitian operators, which we may rep-
resent in a basis {|n⟩} as matrices. Then,
⟨A⟩=

nn′
⟨n| ρ |n′⟩⟨n′| A |n⟩=

n
⟨n| ρ A |n⟩= tr(ρ A) .
In the eigen representation of ρ or A, only the diagonal elements of ρ and A are
necessary, and thus only a sum over ρn An. Therefore, in the following we shall
often write ⟨A⟩= tr(ρ A), even though we think mostly of ρ and A as the classical
quantities.
The mean value is a linear functional, i.e., for arbitrary constant α and β, we have
⟨α A + β B⟩= α ⟨A⟩+ β ⟨B⟩,
since tr {ρ (αA + βB)} = αtr(ρ A) + βtr(ρB). With ⟨1⟩= 1, the mean value of the
deviations from the mean value vanishes:
⟨A −⟨A⟩⟩= ⟨A⟩−⟨A⟩⟨1⟩= 0 ,
but generally the square of the ﬂuctuation (the variance or dispersion) will not be
zero:
(A)2 ≡⟨(A −⟨A⟩)2⟩= ⟨A2 ⟩−⟨A⟩2 ,
where A ≥0. We call A the standard deviation (error width or average square
deviation, and in quantum theory, the uncertainty) and A/⟨A⟩the relative ﬂuctu-
ation: the smaller it is, the less frequently the members of the ensemble are in states
with a value Az which deviates essentially from ⟨A⟩. Results of measurements will
be given in the form ⟨A⟩± A.
6.1.3
Binomial Distribution
For the probability distribution {ρz} of a statistical ensemble of Z mutually indepen-
dent experiments, we are often led to ask whether z of them have turned out to be con-
venient (positive), with the remaining ones being inconvenient (negative), i.e., there
are essentially two outcomes. This is similar to the problem of a one-dimensional
random walk with ﬁxed step length. Here, a body can move forward or backward,

6.1 Statistics
517
and in fact with the probability p for each step ahead and probability q = 1 −p for
each step back: then we ask where it will be after Z steps? If there are z steps ahead
and Z −z steps back, then ﬁnally it will have made z −(Z −z) = 2z −Z steps
ahead—for z < 1
2 Z, it will thus have moved back.
The probability that out of the Z steps the ﬁrst z were ahead and the remaining
ones back is clearly equal to pz q Z−z. Here the order is not important at all, since we
ask only for the probability that in total there were z steps ahead. As is well known,
there is a total of Z! different ways to order Z distinguishable objects—the last can
be placed at Z sites and therefore increases the number of ways by a factor of Z,
while for Z = 1, there is only one site. But here only two outcomes are distinguished
(ahead or back), and therefore Z! is to be divided by z! (Z −z)! Thus
Z
z

different
combinations deliver the same result. The unknown probability is therefore equal
to the probability pz q Z−z for the ﬁrst-mentioned possibility times this number of
equivalent series. We ﬁnd the binomial distribution (Bernoulli distribution)
ρz =
Z
z

pz q Z−z .
With Z
z=0
Z
z

pzq Z−z = (p + q)Z and p + q = 1, we do indeed obtain 
z ρz = 1.
The mean value of “convenient” occurrences is ⟨z⟩= 
z ρz z. Such mean values
can often be evaluated as derivatives with respect to suitable parameters. For the
binomial distribution, for instance,
⟨z⟩=

z
Z
z

pzq Z−z z = p ∂
∂p

z
Z
z

pzq Z−z ,
so we can say ⟨z⟩= p ∂(p + q)Z/∂p at the site p = 1 −q. This yields
⟨z⟩= p Z ,
as expected, because the probability p is the ratio ⟨z⟩/Z. We can also ﬁnd the mean
value of z2 for this distribution in a similar way, by noting that ⟨z2⟩is equal to
(p ∂/∂p)2 (p + q)Z|p=1−q = p ∂/∂p {pZ (p + q)Z−1}|p=1−q ,
which is equal to
pZ + p2Z (Z −1) = p2Z2 + p (1 −p) Z .
Hence the binomial distribution yields the standard deviation and relative ﬂuctuation
(see Fig. 6.1)
z =
	
pqZ
and
z
⟨z⟩=

 q
p
1
√
Z
.

518
6
Thermodynamics and Statistics
Fig. 6.1 Binomial distributions (Bernoulli distributions), represented by bars, with 20 possibilities
and the mean values ⟨z⟩∈{1, 3, 10}. For comparison, we also show the values for the associated
Poisson and Gauss distributions
With increasing Z, we ﬁnd that z/⟨z⟩becomes ever smaller, the maximum of {ρz}
becoming sharper. For example, with Z = 1020 and p ≈q, the measurement value
is uncertain only in the tenth digit.
6.1.4
Gauss and Poisson Distributions
For very large Z the binomial coefﬁcients are difﬁcult to evaluate. Then it is better
to use approximation formulas for the factorials of Z and Z −z, and in particular,
Stirling’s formula
Z! ≈(Z/e)Z √
2π Z .
This can be proven using n! =
 ∞
0
xn e−x dx, if the exponent n ln x −x can be
expanded in a power series about the maximum x = n (Problem 6.2). For very large
Z, we may even leave out the square-root factor, because ln
√
2π Z ≪Z (ln Z −
1) = ln (Z/e)Z, as also represented in Fig. 6.2. The logarithmic scale is very appro-
priate here.
Let us start by considering the case p ≪1 (or equivalently q ≪1, because then
we need to interchange only p and q). We have ⟨z⟩≪Z, implying that only z ≪Z
is important. Therefore, we may now approximate the binomial coefﬁcients
Z
z

with
(1 −z/Z)z ≈1, but (1 −z/Z)Z ≈e−z as follows:
Z
z

≈1
z!
(Z/e)Z
{(Z −z)/e}Z−z = 1
z!
 Z
e
z (1 −z/Z)z
(1 −z/Z)Z ≈Z z
z! .

6.1 Statistics
519
Fig. 6.2 Quality of the Stirling formula. The ratio (Z/e)Z√
2π Z/Z! (left) and the ratio of the
logarithms of
√
2π Z and (Z/e)Z (right) versus Z
In addition, with ln(1 −p) ≈−p, we may set q Z−z ≈e−p(Z−z). For z ≪Z, the
factor epz can be neglected in comparison to e−pZ. Consequently, for Z ≫1 and p ≪
1, with ⟨z⟩= pZ, the binomial distribution goes over into the Poisson distribution
ρz = exp(−⟨z⟩) ⟨z⟩z
z!
.
Since Z
z=0⟨z⟩z/z! tends to e⟨z⟩for Z ≫⟨z⟩, the normalization is conserved, despite
the approximations. In addition, for q ≈1, from the standard deviation of the bino-
mial distribution, we now obtain (z)2 = pZ = ⟨z⟩and likewise from the Poisson
distribution.
The Poisson distribution always occurs if there are a great many possibilities, but
only a few are actually realized, e.g., for the probability of weakly coupled quanta
striking the atoms of a multi-layered lattice, or for the clump probability in a beam
of mutually independent particles, where we may ask how soon one quantum is
followed by the next and we refer to the average distance. (The sequence is no longer
independent, if the quanta occur preferably in pairs or single.)
So far, with p ≪1, only z ≪Z was important, or equivalently, for q ≪1, only
z ≈Z ≫1 was important. If neither p nor q are very small, then these boundary
values are no longer relevant. We may then take z as a continuous variable and expand
ln ρ(z) in a Taylor series about the maximum ⟨z⟩. If we use the Stirling formula for
the factorials in
Z
z

, then we obtain (Problem 6.4) the Gauss distribution, also called
the normal distribution (see Figs. 1.15 and 6.1):
ρ(z) =
1
√
2πz
exp −(z −⟨z⟩)2
2(z)2
.
Here we always have ⟨z⟩= pZ and z = √pqZ. Instead of the error width z, the
Lorentz distribution is sometimes taken, i.e., the interval, in which ρ(z) is greater

520
6
Thermodynamics and Statistics
Table 6.1 Correlation
between people’s size and
weight in terms of thin (◦)
and thick (•)
Size
Light
Average
weight
Heavy
Short
◦
•
Tall
◦
•
than or equal to half the maximum value. For the Gauss distribution, it is 2
√
ln 4z ≈
2.35z.
What is important in all these examples of probability distributions is the result
that the relative deviation from the mean value with increasing Z becomes ever
smaller, and in the limit Z ≫1, the uncertainty z becomes negligible, because we
can only give the mean value ⟨z⟩to a few signiﬁcant ﬁgures.
6.1.5
Correlations and Partial Systems
We usually consider several observables and investigate how they are connected to
each other. We restrict ourselves here to two quantities A and B. Their deviations
from the average value may be correlated to each other, e.g., people’s height and
weight (see Table 6.1).
A measure for such correlations is clearly
K AB ≡⟨(A −⟨A⟩)(B −⟨B⟩)⟩= ⟨AB⟩−⟨A⟩⟨B⟩,
which can be usefully related to the ﬂuctuations A and B. A better measure is
the normalized correlation or correlation coefﬁcient
κAB ≡
K AB
A · B = ⟨AB⟩−⟨A⟩⟨B⟩
A · B
.
The ﬂuctuation (squared) (A)2 is thus equal to the auto-correlation K AA, and κAA
is 1. While K AB may be negative, in which case we speak of an anti-correlation, this
is not possible for the auto-correlation. Note that, in quantum mechanics, we have
⟨AB⟩̸= ⟨B A⟩, if the operators A and B do not commute. Then, according to p. 326,
for the correlation, we often use the symmetrized product 1
2(AB+B A) and takes
K AB = 1
2⟨AB+B A⟩−⟨A⟩⟨B⟩as the correlation coefﬁcient.
If several independent variables z(1), . . . , z(n) occur, we combine them into a
vector z and consider ρ(z). We shall soon see that ρ(z) may be written exactly as
a product ρ(1)(z(1)) · · · ρ(n)(z(n)) if there are no correlations between observables,
which are only related to different variables.
In particular, if we take a property A(i), for which only the ith variable z(i)
is important, then we may immediately sum over all other variables z(k̸=i) in
⟨A(i)⟩= 
z ρ(z)A(i)(z), because A(i)(z) does not depend on them. With this sum,

6.1 Statistics
521
ρ(z) becomes a function of z(i) alone, and in fact ρ(i)(z(i)), if ρ(z) factorizes and use
is made of  ρ(k)(z(k)) = 1.
For i ̸= k, we then have
⟨A(i)A(k)⟩=

z(i),z(k)
ρ(ik)(z(i), z(k))A(i)(z(i))A(k)(z(k)) .
If ρ(ik)(z(i), z(k)) factorizes, this is equal to ⟨A(i)⟩⟨A(k)⟩, so the mean value of the
products is the product of the mean values. Conversely, if all correlations vanish,
then the probability factorizes.
If a system can be decomposed into mutually independent parts, then there are no
correlations between them, and its probability can be broken down into the products
of the individual probabilities, one for each part.
6.1.6
Information Entropy
To each probability distribution {ρz}, we assign an “information measure” I ≥0. It
vanishes if the same thing always happens, i.e., if only a “boring ” case z′ is always
realized, thus if ρz = δzz′. The more there are other possibilities that can be realized,
the more information can be transmitted, the sooner there will be a rare message,
and the greater will be the uncertainty concerning the present event. As information
measure, we take the number of yes–no decisions with which, for a given distribution
{ρz}, on the average, one of the possibilities can be determined. This information
measure is
I ≡−

z
ρzlbρz ,
where lb denotes the binary logarithm, i.e., to the base 2, deﬁned by
2lbx ≡x ,
whence lbx ≡log2 x = ln x
ln 2 .
Occasionally, ldx is used instead of lbx, referred to as the logarithmus dualis. The
unit of information is the bit (binary number). For example, a set of 32 = 25 playing
cards contains 5 bit of information, as we shall see soon.
However, the information measure I only evaluates how rarely an event occurs,
but does not account for its worthiness, in the sense of how much it is worth to us.
The playing cards have different values for the different team members, but each
contributes an information content of 5 bit, and a row with 100 arbitrarily chosen
letters (and punctuation marks) has the same information content as an equally long
piece of prose or verse. Since there may be overwhelmingly many “misprints”, I is
often called a measure of disorder. (It is interesting to note that, in written texts, the
letters do not not all occur with equal probability. In German, the information content

522
6
Thermodynamics and Statistics
of one of the 26 letters of the alphabet, together with the space, is not lb27 = 4.76
bit, but only approximately 4 bit.)
In order to understand that the given sum achieves what is required of it, we
proceed stepwise. First, we restrict ourselves to Z equally probable possibilities,
whence ρz = 1/Z. For Z = 2m possibilities, clever questioners after each response
drop half the remaining possibilities: after m responses, they know which of the 2m
possibilities actually exist. Here we thus have I = lbZ. If Z is not a power of 2,
we do not always need the same number of questions. For Z = 3, in a third of all
cases, one question sufﬁces. Then with the second question, we could already check
the next attribute. Here the information measure for the questions for two attributes
has to be additive: if the ﬁrst attribute Z1 has equally probable possibilities and the
second Z2 likewise, then in total there are Z = Z1Z2 equally probable possibilities,
and we must have I (Z1Z2) = I (Z1) + I (Z2). This requirement is fulﬁlled only by
the function I (Z) = c ln Z, where the factor c cannot depend on Z, and clearly has
to be equal to 1/ ln 2, so that everything is correct for Z = 2m. For ρz = 1/Z, we do
indeed ﬁnd the above-mentioned expression, because −Z(1/Z)lb(1/Z) = lbZ.
The additivity of the information measure for independent attributes must also
be valid for other distributions {ρz ̸= 1/Z}. For these, we take the largest common
divisor 1/Z′ of all fractions ρz and start from a total of Z′ equally probable events
which we combine into Z groups, each with Nz = ρz Z′ members (see Figs. 6.3 and
6.4). The information measure lbZ′ may then be composed of two terms: one is the
unknown auxiliary quantity Iz and measures the information which is related to the
characterization of the group z, while the other rates the information in this group
and clearly has the value lbNz. Then lbZ′ = Iz + lbNz, and with Nz/Z′ = ρz, this
delivers the expression Iz = −lbρz, while its mean value gives the unknown variable.
Thusontheaverage I = −
z ρzlbρz questionsareindeednecessarybeforereaching
the ﬁnal decision.
As ρz →0, the quantity lbρz increases beyond all bounds, but nevertheless so
slowly that ρzlbρz →0. We do not need to question completely improbable possi-
bilities as they do not contribute to the uncertainty—physicists often speak of frozen
degrees of freedom.
Fig. 6.3 Information measure for {ρ1 = 1
3, ρ2 = 1
2, ρ3 = 1
6}. This is indicated here by the upper
probability distribution. The problem can be mapped onto Z′ = 6 equally probable cases, whence
the steps turn into a single bar of equal area. With the additivity of the information measure, it then
follows that lb6 = I1 + lb2 = I2 + lb3 = I3 + lb1 and hence Iz = −lbρz

6.1 Statistics
523
Fig. 6.4 Information
entropy I for the binary
system. It has only two
states, and therefore
ρ1 + ρ2 = 1. Hence, I may
be represented here as a
function of ρ1. Note the
steep slope for ρ1 ≈0 and
ρ2 ≈0. The uncertainty is
greatest when the two states
are occupied with equal
probability (ρ1 and ρ2 both
equal to 1/2)
In thermodynamics, instead of the information measure I, we use the information
entropy
S ≡(k ln 2) I = −k

z
ρz ln ρz ,
where k is the Boltzmann constant, already mentioned in the list of fundamental
constants on p. 623. Note that we prefer the natural logarithm ln x, because it can be
differentiated with respect to x more easily than lbx. With 0 ≤ρz ≤1, the entropy
is never negative. It vanishes if only one state is occupied, and takes its largest value
if all possible states are equally probable (Problem 6.6).
6.1.7
Classical Statistics and Phase Space Cells
The notion of entropy just introduced is useful only for countable attributes z. This
is because ρz has to be dimensionless, given that we cannot take a logarithm of a
probability density. This means that continuous variables have to be discretized. We
shall investigate this more precisely for the probability density ρ(x, p).
According to Hamiltonian mechanics, a system of N point masses is completely
determined if their positions and momenta are given. This therefore means specifying
6N quantities. Classical N-particle systems will be represented by a point (x, p) in
the 6N-dimensional phase space. (This is also called the larger phase space or 
-
space, the generalization of the 6-dimensional phase space of a single particle, which
is also called μ-space. In μ-space, N points are occupied.) The vectors x and p each
have 3N components.
Weareconcernedherewithstatisticalensemblesandthereforeassignaprobability
density ρ(x, p) with the following properties to each phase space point:
ρ(x, p) = ρ∗(x, p) ≥0 ,

ρ(x, p) d3Nx d3Np = 1 ,
i.e., ρ(x, p) is real, non-negative, and normalized.

524
6
Thermodynamics and Statistics
Using this, the mean values of the quantities A(x, p) may be evaluated from
⟨A⟩≡

ρ(x, p) A(x, p) d3Nx d3Np .
Here arbitrary canonical transformations (x, p) ↔(x′, p′) are allowed, i.e., those
ensuring dx dp = dx′ dp′, because we require ρ(x, p) = ρ′(x′, p′) and A(x, p) =
A′(x′, p′), according to Sect. 2.4.4.
In quantum theory, this is true if we take the Wigner function as the density (see
p. 324). However, it is sometimes negative. This disadvantage can be avoided with
the density operator (see Sect. 4.2.11), hence with ⟨A⟩= tr(ρ A). (In the position
representation, this is equal to

⟨x|ρ|x′⟩⟨x′|A|x⟩d3N x d3N x′, and in the momentum
representation, to

⟨p|ρ|p′⟩⟨p′|A|p⟩d3N p d3N p′. In contrast, the Wigner function
uses x and p, even though they cannot be sharp simultaneously.) The density oper-
ator is Hermitian, non-negative, and normalized. Here, unitary transformations U
are also permitted, so instead of the position representation, the momentum or any
other representation may be used. If we have ρ′ = Uρ U −1 and A′ = U A U −1, then
tr(AB) = tr(B A) implies tr(ρ′A′) = tr(ρ A).
We shall now divide each continuous variable x, p into equal sections δx and δp,
so that the phase space is divided into cells of size (δx δp)3N. The smaller these cells,
the more precisely the states are determined. Here, in the classical description, the
cell size may be arbitrarily small, while in quantum physics, according to Heisen-
berg’s uncertainty relation, position and momentum cannot both be arbitrarily sharp,
because x · p ≥1
2ℏ. In fact, only for
δx · δp = h ≡2π ℏ
do classical and quantum mechanics yield the same number of states. We shall now
show this for free particles in a cube. Another example is given in Fig. 6.5 (or
Problem 6.7), namely for harmonic oscillators.
Fig. 6.5 Phase space cells partition the action variable (with the phase integrals J =

p dx, as
discussed on p. 136). They lead us to the action quantum. In addition to linear cell boundaries (as in
Fig. 2.28), curved ones are also possible. Thus polar coordinates are appropriate for an oscillation.
If the phase angle is completely unsharp (as on the time average), then for suitable scale factors,
the phase space cells are concentric circular rings of equal area

6.1 Statistics
525
In a cube of side L, according to quantum theory (p. 355), the Cartesian compo-
nents of the wave vector have eigenvalues kn = n π/L with n ∈{1, 2, . . .}. Only the
wave function √2/L sin(knx) vanishes at the container walls x = 0 and x = L. The
number of one-particle states with momenta p ≤pF = ℏkF = ℏnF π/L = 1
2h nF/L
is thus equal to the number of unit cubes in the octant with radius nF = 2h−1L pF:
 = 1
8
4π
3 nF
3 = 4π
3
V
h3 pF
3 .
If we divide the phase space volume 4
3π pF3 V into cells of size h3, then we have
as many cells as states according to quantum theory, and we shall exploit this in the
following.
We recognize here the meaning of the Planck constant h for thermodynamics.
While the classical description for discretization remains completely undetermined,
the quantum-mechanical uncertainty relation supplies a unique cell size in phase
space. So we are not dealing with an uncertainty relation, a name that can be con-
sidered less appropriate.
6.1.8
Summary: Statistics
In statistics, we consider ensembles in which the Z possibilities occur with prob-
abilities ρz. The probability distribution {ρz} satisﬁes the constraints ρz = ρz∗≥0
and 
z ρz = 1. (For continuous z, integrals occur instead of sums. Nevertheless,
according to quantum theory, the phase space cells have size δx δp = h and we
may discretize.) The observable A in the statistical ensemble has the average value
⟨A⟩= tr(ρ A) and the uncertainty (error width) A =
	
⟨A2⟩−⟨A⟩2. Two quanti-
ties A and B have the correlation K AB = ⟨AB⟩−⟨A⟩⟨B⟩. With such correlations,
we can determine whether mutually independent variables occur in the statistical
ensemble. If this is the case, then the probability distribution may be factorized into
a product whose factors each depend only on one of the variables. Important for the
following is also the information entropy S = −ktr (ρ ln ρ). Disregarding the factor
k ln 2, this gives the average number of yes–no decisions with which one of the pos-
sibilities for the given probability distribution {ρz} may be determined. This entropy
is one of the most important parameters characterizing the statistical ensemble.
6.2
Entropy Theorem
6.2.1
Entropy Law and Rate Equation
The information entropy must satisfy the extremely important entropy law:
dS
dt ≥0 ,
for all closed systems.

526
6
Thermodynamics and Statistics
This is also called Boltzmann’s H-theorem, because instead of the entropy, Boltzmann
used the upper-case Greek letter Eta, which resembles the Latin letter H, and he
deﬁned H ≡tr(ρ ln ρ) = −S/k. We shall avoid this quantity here, because it could
be confused with the enthalpy, which, according to international recommendations,
should be abbreviated with the Latin H. Here a system is called closed, if it is not in
contact with the environment, whence it exchanges neither energy nor particles, nor
anything else. Therefore, in addition to invariable macro parameters, only its entropy
depends on the time (or the probability distribution, which for its part does depend
on the external parameters, and the time). We shall only allow for changes in other
macro parameters at the end of the next section.
As will be shown in Sect. 6.2.3, this inequality follows from the rate equation for
the probability (also called the balance or master equation), demonstrated in Sect.
4.6.4:
dρz
dt =

z′̸=z
(Wzz′ ρz′ −Wz′z ρz) ,
where Wz′z (≥0) gives the transition rate from the state z into the state z′. Note that,
as in quantum theory, the ﬁnal state is also on the left of the initial state here. On
p. 383, Wz′z ∝|⟨z′| H |z⟩|2 for z′ ̸= z was already determined. Such rate equations
are often set as an ansatz (further examples in the second part of this section), which
should not be confused with the (entropy conserving) Liouville or von Neumann
equation, which we shall discuss in Sect. 6.2.3. The term 
z′ Wzz′ρz′ is the yield rate
and 
z′ Wz′zρz the loss rate for the state z, and the balance depends on both.
As a rate equation, we may also take the diffusion equation
∂ρ
∂t = D ρ ,
as we shall now show in one dimension, in particular, with ∂2ρ/∂z2 instead of ρ.
Thus we discretize the position parameter z of the cell with size δz and obtain a
connection with the neighboring cells:
dρz
dt = D ρz+1 −2ρz + ρz−1
(δz)2
.
The transition rate W and the diffusion constant D are related by
Wzz′ = δz′,z±1 D/(δz)2 = Wz′z ,
and from Wz′z ≥0, it then follows that D ≥0.
While open systems may prefer the transition in one direction (for example, they
transmit energy to a colder environment), for closed systems, Wzz′ = Wz′z. Therefore,
the rate equation for closed systems simpliﬁes to ˙ρz = 
z′ Wzz′ (ρz′ −ρz). Hence,
dS
dt = −k

z
d(ρz ln ρz)
dt
= −k

z
dρz
dt
(ln ρz + 1) = k

zz′
Wzz′ (ρz −ρz′)(ln ρz + 1) .

6.2 Entropy Theorem
527
If we now swap the summation indices (z ↔z′) and add the expressions, we obtain
2 dS
dt = k

zz′
Wzz′ (ρz −ρz′)(ln ρz −ln ρz′) .
With ρz > ρz′, we also have ln ρz > ln ρz′, so there are no negative terms here. The
entropy law thus follows from the rate equation if the transition rates Wz′z and Wzz′
are equal, and this applies to closed systems.
The entropy increases until it has taken the largest value compatible with the
remaining constraints. In particular, the rate equation does not change at all if
Wzz′ ρz′ = Wz′z ρz holds for all pairs (z, z′). In this situation, the system is said to be
in detailed equilibrium.
6.2.2
Irreversible Changes of State and Relaxation-Time
Approximation
If the entropy of a closed system has increased in the course of time, then according to
the entropy law, it never ever returns to the initial state by itself, because the entropy
would have to decrease again. The change of state is thus not reversible, and is said
to be irreversible.
We take a two-level system as the simplest example. We already investigated its
rate equation in Sect. 4.6.4. With ρ1 + ρ2 = 1, it may be decoupled to yield
˙ρ1 = W12 ρ2 −W21 ρ1 = W12 −(W12 + W21) ρ1 ,
whence it has the solution
ρ1(t) = W12τ + {ρ1(0) −W12τ} exp −t
τ
,
with the relaxation time
τ =
1
W12 + W21
.
In quantum physics, τ is called the average lifetime. It is occasionally replaced
by the decay time T1/2 = τ ln 2, because 1
2 = exp(t1/2/τ). It is a measure of how
fast equilibrium is reached. The more strongly the two states are coupled to each
other, the faster this happens. The solution ρ1 approaches the limiting value W12τ
monotonically, and ρ2 = 1 −ρ1, the value W21τ. The value with the highest entropy
(here 1/2) is reserved for a closed system, in particular, when W12 = W21.
For Z states, we modify the rate equation into a linear system of equations ˙ρz =

z′ azz′ ρz′, where
azz′ =

Wzz′ ,
for z ̸= z′ ,
−
z′′̸=z Wz′′z ,
for z = z′ .

528
6
Thermodynamics and Statistics
The sums of the columns in its coefﬁcient matrix (azz′) are all zero, whence two
important properties follow. Firstly, the determinant of this matrix must be zero, and
therefore there is a zero eigenvalue, hence a stationary eigen solution. The second
property follows because only the diagonal elements are actually negative: all eigen-
values have a non-positive real part. According to Gerschgorin’s theorem [5], the
position (in the complex plane) of the (suitably ordered) kth eigenvalue of a complex
matrix has a distance from the kth diagonal element which is less than the sum of
the moduli of the non-diagonal elements of the kth column. If the transition rates for
inverse processes are equal (as for each closed system), then the matrix is Hermitian
and thus has only real eigenvalues, which we set equal to −τk−1 (each τk is then a
relaxation time). We presume in the following that the eigenvalue 0 is not degener-
ate, otherwise there may be different ﬁnal states. Then the solutions ρz(t) of the rate
equation each consist of a constant term ρz(∞) and Z −1 terms czk exp(−t/τk).
After a sufﬁciently long time, only the largest value of the τk is important, which we
now denote by τ:
ρz(t) ≈ρz(∞) + cz exp −t
τ
.
In this relaxation-time approximation, the factors cz are determined by the initial
state. If it differs only little from the ﬁnal state, we may approximate by setting
cz ≈ρz(0) −ρz(∞).
The stationary ﬁnal state is given by ˙ρz = 0 (for all z). With

z′
azz′ ρz′(∞) = 0 ,

z
ρz = 1 ,
it may be traced back to the adjoint Azz′ of the matrix (azz′) of coefﬁcients. Then, up
to the sign (−)z+z′, the adjoint Azz′ is the sub-determinant (or ﬁrst minor) generated
by eliminating the zth row and z′th column, and therefore det a = 
z′ azz′ Azz′:
ρz(∞) =
Az′z

z′′ Az′z′′ ,
where here z′ may be chosen arbitrarily. For Wzz′ = Wz′z, the matrix (azz′) is also
symmetric, and therefore 
z azz′ = 0 implies that 
z′ azz′ = 0 and all ρz(∞) are
equally large.
Radioactive decay corresponds to an open system. The decay products move away
from each other and never recombine. Therefore, there is in fact a transition mother
→daughter, but not vice versa. From the differential equation ˙ρ = −ρ/τ for the
probability of the mother state, we obtain the solution ρ(t) = ρ(0) exp(−t/τ). Note
that the solution for the ﬁnal state can be broken up into three factors: two for the
decay products and one for the relative motion. According to p. 525, a great many
possible states with energies between EF −1
2dE and EF + 1
2dE belong to this third
factor, in fact, 4πV h−3m√2mEF dE, implying therefore a high entropy.

6.2 Entropy Theorem
529
Fig. 6.6 Time dependence
of the stepwise decay
1→2→3, and in fact here
for τ2 = 3τ1
For a stepwise decay, we have to set up the following system of equations for
the probabilities of the radiating substances and the ﬁnal products, once again with
Wzz′ ̸= Wz′z:
˙ρ1 = −ρ1
τ1
,
˙ρ2 = ρ1
τ1
−ρ2
τ2
,
˙ρ3 = ρ2
τ2
,
with the solutions
ρ1(t) =
ρ1(0) exp −t
τ1
,
ρ2(t) =
τ2
τ1 −τ2

ρ1(t) −ρ1(0) exp −t
τ2

,
ρ3(t) = ρ1(0) −ρ1(t) −ρ2(t) ,
if we restrict ourselves to ρ1(0) = 1, and therefore ρ2(0) = ρ3(0) = 0 (see Fig. 6.6).
(But note that, with τ1 = τ2, we have ρ2(t) = ρ1(t) t/τ1.) According to the above,
we have ρ3(∞) = ρ1(0). But this does not mean that the entropies of the initial and
ﬁnal states were equal, because once again the relative motion is missing, and this
would lead to an increase in the entropy.
6.2.3
Liouville and Collision-Free Boltzmann Equation
In classical mechanics, we label each N-particle system by a point in the (larger)
phase space and a statistical ensemble of such systems by a swarm of points with the
probability density ρ(t, x, p). The single points move in this space as time goes by,
but their total number remains constant. We then have the Liouville equation
dρ
dt = ∂ρ
∂t +
3N

k=1
 ∂ρ
∂xk ˙xk + ∂ρ
∂pk
˙pk

= 0 .
We proved this in Sect. 2.4.4: a volume element in the phase space keeps its probabil-
ity if it follows the equations of motion (by swimming along the particle trajectories,

530
6
Thermodynamics and Statistics
as it were)—as for an incompressible liquid. Its shape can in fact change, but not
its content. Recall also that, according to p. 342, the von Neumann equation is the
quantum theoretical counterpart of the Liouville equation. This is even more general
than the (time-dependent) Schrödinger equation, because it holds not only for pure
states, but also for mixtures.
Under special conditions, the Liouville equation is also called the collision-free
Boltzmann equation, in particular, if there is a swarm of interaction-free molecules,
which cannot therefore collide. Then the probability distribution ρ(t, r, p) of one
molecule sufﬁces, because any other will have the same distribution. Note that, since
there are no correlations, the probability distribution of the gas factorizes even if not
all the molecules have the same mass, although then that will appear differently in
ρ(t, r, p). Momentum changes may be traced back to an external force F = ˙p via
 ∂
∂t + v · ∇r + F · ∇p

ρ(t, r, p) = 0 .
Note, however, that the canonical momentum then has to be equal to the mechanical
one, but charged particles would also interact with each other. If we take the veloc-
ity v instead of the momentum p, then setting ˙v = a, we obtain the collision-free
Boltzmann equation
 ∂
∂t + v · ∇r + a · ∇v

ρ(t, r, v) = 0 ,
which in plasma physics is also called the Vlasov equation. For a ≡0, it is solved
by any function ρ(r −v t, v).
For all these examples, the total entropy is conserved if there is no friction force.
(Actually, as mentioned before, we cannot take a logarithm of a density, because
it carries a dimension. But we may divide the phase space into cells and associate
probabilities with them.) With ∂(ρ ln ρ)/∂t = (ln ρ + 1) ∂ρ/∂t, the collision-free
Boltzmannequationdelivers∂(ρ ln ρ)/∂t = −(v · ∇r + a · ∇v)ρ ln ρ,andtherefore
dS
dt = −k
 ∂(ρ ln ρ)
∂t
d3r d3v = k

(v · ∇r + a · ∇v) ρ ln ρ d3r d3v .
Since the velocity cannot be arbitrarily high, the surface integral of a ρ ln ρ in the
velocity space vanishes. Therefore, Gauss’s theorem supplies

a · ∇v ρ ln ρ d3v = −

ρ ln ρ ∇v · a d3v .
Since the external force, and hence the acceleration a, should not depend on the
velocity, the last expression vanishes. For a friction force, the situation is different,
but this can be traced back to collisions which we will account for only in the next
section.

6.2 Entropy Theorem
531
In order to determine a local change in the entropy, we integrate the further terms
only over the velocity. Since r and v are mutually independent variables, we ﬁnd

v · ∇r ρ ln ρ d3v = ∇r ·

v ρ ln ρ d3v .
The entropy may thus change locally, but not globally, because then according to
Gauss’s theorem, the surface integral would have to be investigated. But for r →∞,
the factor ρ ln ρ is zero.
6.2.4
Boltzmann Equation
We now consider an example in which the entropy can increase with time. If
molecules of equal mass collide, then further terms appear in the Boltzmann equa-
tion mentioned above, which describe the collision-induced gain and loss of the
probability density ρ(t, r, v):
 ∂
∂t + v · ∇r + a · ∇v

ρ(t, r, v) = R+ −R−.
This relation is also more general than the rate equation initially considered, because
in fact dρ/dt stands on the left, while on the right the gain and loss have not been
split-up into transition rate and density. This will be done later.
Weevaluatethenewtermsusingthefollowingapproximations.Firstly,weaccount
for collisions between only two particles and restrict ourselves to time spans during
which a molecule collides at most once. Both assumptions presume a sufﬁciently
low density. Secondly, we neglect the inﬂuence of the container walls, which is justi-
ﬁed for sufﬁciently large systems. Thirdly, we restrict ourselves to elastic scattering
(point-like collision partners without internal degrees of freedom). The differential
scattering cross-section σ may depend only on the velocities. Finally, in addition
to energy and momentum conservation, we also make use of space-inversion and
time-reversal invariance:
σ(v1, v2 →v′
1, v′
2) = σ(−v1, −v2 →−v′
1, −v′
2) ,
r →−r ,
= σ(−v′
1, −v′
2 →−v1, −v2) ,
t →−t .
Then the scattering cross-sections for inverse collisions are equal,
σ(v1, v2 →v′
1, v′
2) = σ(v′
1, v′
2 →v1, v2) ,
something we shall use to establish the relation between R+ and R−, or to establish
Wzz′ = Wz′z. Due to energy and momentum conservation, v1 and v2 already ﬁx v′
1
and v′
2, except for the direction of the relative velocity. For the proof in the next

532
6
Thermodynamics and Statistics
section, this is of no help. Instead of

σ d, it is better to write

σ(v1, v2 →
v′
1, v′
2) d3v′
1 d3v′
2. Then σ is not actually an area, but it is probably not appropriate
to use another letter.
Here the decrease in the probability density ρ(t, r, v1) is the product of the scat-
tering cross-section and the current strength, which themselves may be calculated
from the probability density and the relative velocity:
R−(t, r, v1) =

σ(v1, v2 →v′
1, v′
2) ρ(t, r, v1, v2) |v1 −v2 | d3v2 d3v′
1 d3v′
2 .
For the gain in the probability density, on the other hand, we obtain
R+(t, r, v1) =

σ(v′
1, v′
2 →v1, v2) ρ(t, r, v′
1, v′
2) |v′
1 −v′
2| d3v2 d3v′
1 d3v′
2 .
Since the scattering cross-sections for inverse collisions are equal and the energy is
conserved, whence also |v′
1 −v′
2| = |v1 −v2|, this may be reformulated as
R+(t, r, v1) =

σ(v1, v2 →v′
1, v′
2) ρ(t, r, v′
1, v′
2) |v1 −v2| d3v2 d3v′
1 d3v′
2 .
Finally, we obtain
 ∂
∂t + v1 · ∇r + a · ∇v1

ρ(t, r, v1)
=

|v1−v2| σ(v1, v2 →v′
1, v′
2) {ρ(t, r, v′
1, v′
2) −ρ(t, r, v1, v2)}
d3v2 d3v1
′ d3v2
′ .
On the left is the unknown probability distribution for a single particle, and on the
right the unknown probability distribution for two particles. This equation is soluble
only by a further approximation, derived from the assumption of molecular chaos:
the probability distribution of two particles (at time t and at the same position r) is
assumed to factorize, the velocities of the colliding molecules being assumed not
to be correlated (such a factorization was already assumed in Sect. 4.6.1 in order to
arrive at a calculable expression for the dissipation in quantum-mechanical systems):
ρ(t, r, v1, v2) = ρ(t, r, v1) · ρ(t, r, v2) .
In this situation, we obtain a non-linear integro-differential equation known as the
Boltzmann equation (Boltzmann transport equation)

6.2 Entropy Theorem
533
 ∂
∂t + v1 · ∇r + a · ∇v1

ρ(t, r, v1)
=

|v1−v2 | σ(v1, v2 →v′
1, v′
2) {ρ(t, r, v′
1) ρ(t, r, v′
2) −ρ(t, r, v1) ρ(t, r, v2)}
d3v2 d3v′
1 d3v′
2 .
The collision integral on the right-hand side can usually be further simpliﬁed by
exploiting energy and momentum conservation (see the previous page). We have thus
derived a balance equation and traced the transition rates back to known notions.
Note that the Boltzmann equation may be used to describe a range of different
transport processes, e.g., in reactors, superﬂuids, or stars [6].
6.2.5
Proof of the Entropy Law Using the Boltzmann
Equation
In order to investigate the inﬂuence of the collision integrals on the entropy, we begin
by excluding external forces (a = 0) and assume that the probability density does
not depend upon the position, so that only ρ(t, v) appears. We then have
S(t) = −k

ρ(t, v) ln ρ(t, v) d3v
and
−1
k
dS
dt =
 ∂ρ
∂t {ln ρ + 1} d3v
=

|v1 −v2 | σ(v1, v2 →v′
1, v′
2) {ρ(t, v′
1) ρ(t, v′
2) −ρ(t, v1) ρ(t, v2)}
{ln ρ(t, v1) + 1} d3v1 d3v2 d3v′
1 d3v′
2 .
With the symmetry of the collision partners 1 and 2, this may also be written as
−2
k
dS
dt =

|v1 −v2 | σ(v1, v2 →v′
1, v′
2) {ρ(t, v′
1) ρ(t, v′
2) −ρ(t, v1) ρ(t, v2)}
{ln (ρ(t, v1) ρ(t, v2)) + 2} d3v1 d3v2 d3v1
′ d3v2
′ .
Since inverse collisions have scattering cross-sections equal to the original ones
and since the modulus of the relative velocity remains conserved, we may swap the
primed and the unprimed velocities, and then, as on p. 527, infer dS/dt ≥0.
If the probability density also depends on the position, we have to respect the addi-
tional term

v · ∇r ρ ln ρ d3r d3v. As shown in the section before last, the entropy
may then change locally, but not globally. Likewise, an external force F(r) would
change nothing in the result.

534
6
Thermodynamics and Statistics
The Boltzmann equation can be used, not only to prove the entropy law, but even
to evaluate the entropy gain, provided that the scattering cross-section is known. It
originates uniquely from the change in the states under collisions. There can be no
entropy gain without collisions.
It is well known that the usual basic equations of mechanics and electromagnetism
do not change under time reversal. To each solution of the basic equations belongs
a “time-reversed” solution, for which everything proceeds in the reverse order, i.e.,
t is replaced by −t. In particular, elastic scattering is invariant under time reversal,
and this has even been used explicitly. Nevertheless, the entropy of a closed system
may only increase with time, never decrease.
In reality, there is no contradiction. In fact, we evaluate the entropy using another
distribution function than the one actually planned for the (time-reversal invariant)
Liouville equation. We describe the system with its vast number of degrees of free-
dom using only a small number of variables, average over the remaining ones, and
thereby lose the time-inversion symmetry. This shows up, e.g., in the derivation of the
Boltzmann equation. Here the entropy changes, because we have assumed molecular
chaos—by doing this, we have averaged out possible correlations and lost informa-
tion! Actually, the one-particle density is related to the two-particle density, this with
the three-particle density, and so on. Collisions couple the one- to the many-body
densities. But in order to be able to proceed at all, we have to terminate this sequence
somewhere and come back to molecular chaos.
Althoughtheseconsiderationswereinitiallyappliedonlytothecalculatedentropy,
the question remains as to whether they might not also apply to the experimental
quantity, if the entropy is used as a state variable like, e.g., energy or volume. In fact,
we always adopt only a few state parameters, far too few to be able to describe a
system microscopically. This will become clear in the next section.
If the allowed states are all equally probable, the return probabilities of a many-
body system (N ≫1) are unbelievably small. If, for example, each particle is inde-
pendent of the others and equally probable in both halves of a container, then all N
particles are in the one half only with the probability 2−N, thus for N = 100 only
with the probability 10−30 (see Problem 6.10).
6.2.6
Molecular Motion and Diffusion
In order to investigate the inﬂuence of correlations in more detail, we consider a
gas at rest, consisting of molecules of the same kind. Then ⟨v ⟩= 0 holds as the
ensemble average and also as the time average. Note that an ensemble is said to be
ergodic, if its ensemble average is equal to its time-average value. But ⟨v2 ⟩is not
zero. According to the equidistribution law on p. 559, the average kinetic energy
per degree of freedom for the absolute temperature T is 1
2kT . We shall allow for
motions along a straight line, in a plane, or in space. Therefore, let n be the number
of dimensions. Consequently, ⟨v2 ⟩= nkT/m.
Collisions alter the velocity of a test particle and lead to an irregularly ﬂuctuating
acceleration a around the mean value zero. Then the auto-correlation function of the

6.2 Entropy Theorem
535
velocity ⟨v (t) · v (t′)⟩for t = t′ is in fact equal to ⟨v2⟩> 0, but for |t −t′| →∞, it
surely approaches ⟨v(t)⟩· ⟨v(t′)⟩, i.e., it must approach zero. We set
⟨v (t) · v (t′)⟩= ⟨v2⟩χ(t −t′) ,
with χ(t−t′) = χ(t′−t), χ(0) = 1, and χ(∞) = 0. Up to the ﬁrst collision, χ keeps
the same value, because until then the velocity does not change. Thus we shall assume
now that each individual collision proceeds very fast (an assumption we drop in the
section after next), and the initial and ﬁnal velocities will no longer be correlated.
The probability of a collision is (supposedly) equally large for equal timespans. If
we call the average time up to a collision τ, then we have
χ(t) = exp −|t|
τ
.
This τ does indeed correspond to a relaxation time. On average, in each time span
τ, the same fraction of the original attributes is removed.
If we choose the origin at r (0), then from r (t) =
 t
0 dt′ v (t′) and χ(t −t′) =
χ(t′ −t) = exp(−|t −t′|/τ), we ﬁnd for the squared ﬂuctuation
⟨r2(t)⟩=
 t
0
dt′
 t
0
dt′′ ⟨v (t′) · v (t′′)⟩= 2 ⟨v2⟩
 t
0
dt′
 t′
0
dt′′ χ(t′−t′′)
= 2 ⟨v2⟩τ 2  t
τ −1 + exp −t
τ

.
For |t| ≪τ, this Ornstein–Fürth relation goes over into ⟨r2⟩≈⟨v2⟩t2, and for t ≫
τ, into ⟨r2⟩≈2⟨v2⟩τt, and both are easy enough to understand: up to the ﬁrst
collision, we have r = v t and thus ⟨r2⟩= ⟨v2⟩t2, but after many collisions ⟨r2⟩
increases only in proportion to t (see Fig. 6.7). This is the same for random walks
and for diffusion, as we shall now show.
Fig. 6.7 Ornstein–Fürth
relation. Distance of a gas
molecule from its initial
position as a function of time
(continuous red curve). For
t ≫τ, the approximation
√t/τ holds, represented by
the dashed blue parabola

536
6
Thermodynamics and Statistics
For the random walk, we assume that the test body after each collision moves
along a new direction which is not correlated with the direction prior to the collision.
For N collisions therefore, using r = N
i=1 si ei with ⟨ei · ek⟩= δik, we obtain the
expression ⟨r2⟩= N
i=1⟨si 2⟩. Here ⟨si 2⟩= ⟨v2⟩⟨ti 2⟩and ⟨ti 2⟩= 2τ 2 is independent
of i, so ⟨r2⟩∝N and hence proportional to the total time.
This squared ﬂuctuation also increases in accordance with the diffusion equation
∂ρ
∂t = D ρ ,
hence linearly with time. In particular, if we set the initial value ρ(0, r) = δ(r), then
for n dimensions, the solution of this differential equation (Problem 6.9) reads
ρ(t, r) = exp{−r2/(4Dt)}
√
4π Dt
n
,
and with ρ(0, r) = f (r), ρ(t, r) =

d3r ′ f (r ′) exp{−|r −r ′|2/4Dt}/
√
4π Dt
n
then solves the diffusion equation (see Fig. 6.8).
From this we obtain ⟨r2⟩= 2nDt. Comparing with the expression ⟨r2⟩≈2⟨v2⟩τt
derived above, we arrive at nD = ⟨v2⟩τ. The relation ⟨v2⟩= nkT/m is generally
used:
D = ⟨v2⟩
n
τ = kT
m τ .
The diffusion constant D is thus related to the relaxation time τ, where the mass of
the test particle and the temperature of its environment are also involved.
As already mentioned, the result ⟨r2⟩∝t can be valid only for sufﬁciently long
times, because up to the ﬁrst collision, ⟨r2⟩∝t2 has to hold. We could also have
derived the relation ⟨r2⟩= 2nDt for all t ≥0 by using the ansatz ⟨v (t) · v (t′)⟩=
2nD δ(t−t′). Although we also make the ansatz for the auto-correlation function
as a delta function, it is only an approximation. The diffusion equation has to be
improved at the outset. Only the differential equation (improved diffusion equation)
Fig. 6.8 One-dimensional
diffusion. Shown is the
distribution function
√
Dτ ρ(t, x) as a function of
x/
√
Dτ at the times t =
1
10τ
(red curve), 1
3τ (blue curve),
and τ (green curve). For
t →∞, we ﬁnd ρ →0

6.2 Entropy Theorem
537
∂ρ
∂t = (1 −e−t/τ) D ρ
is solved, for the initial condition ρ(0, r) = δ(r), by
ρ(t, r) = exp(−r2/4Dt′)
√
4π Dt′n
,
with t′ ≡t −τ (1 −e−t/τ) ,
and this leads to the Ornstein–Fürth relation and to ⟨v2⟩= 2nD t′ ≈⟨v2⟩t2.
These considerations are also valid for Brownian molecular motion, where an
inert particle is struck by much faster ones. However, its velocity in this collision
does not change as much as above and its relaxation time τ is therefore very much
longer than the average time between two collisions.
6.2.7
Langevin Equation
In the preceding section, we determined ⟨r2(t)⟩with a time-dependent probability
density ρ(t, r). This corresponds to the Schrödinger picture (Sect. 4.4.2) in quantum
theory. There we also used the Heisenberg picture—then the probability density does
not depend on time, but rather on the observable r. This picture has the advantage
that derivatives of mean values with respect to time are equal to mean values of
derivatives with respect to time.
If we differentiate the Ornstein–Fürth relation
⟨r2(t)⟩= 2 ⟨v2⟩τ {t −τ (1 −e−t/τ)}
with respect to time, we obtain
⟨r · v ⟩= ⟨v2⟩τ

1 −exp −t
τ

.
If we differentiate this once more with respect to time, then we obtain ⟨v2⟩+ ⟨r · ˙v ⟩
on the left and ⟨v2⟩e−t/τ = ⟨v2⟩−⟨r · v ⟩/τ on the right. It therefore follows that
⟨r · ˙v ⟩= −⟨r · v ⟩
τ
= −⟨v2⟩

1 −exp −t
τ

.
At the beginning, when |t| ≪τ, it is clear that ⟨r · v ⟩≈⟨v2⟩t and ⟨r · ˙v ⟩≈
−⟨v2⟩t/τ, while later, when t ≫τ, the two correlation functions ⟨r · v ⟩≈⟨v2⟩τ >
0 and ⟨r · ˙v ⟩≈−⟨v2⟩< 0 are constant. These properties, including the sign, are
easily understood for diffusion: initially, r, v, and ˙v are independent of each other,
but then a correlation is established, and collisions hinder the diffusion, rather as for
a frictional force.

538
6
Thermodynamics and Statistics
Fig. 6.9 Stochastic force as a function of time. This acts irregularly in time, strength, and direction
(only one component is shown here)
This is taken into account by theLangevin equation:
dv
dt = a −v
τ ,
with ⟨a ⟩= 0 .
It is generally set in the form
F = F ′ −α v ,
with ⟨F ′⟩= 0 ,
and α ≡m/τ is referred to as a frictional constant. We have already investigate a
Stokes frictional force −αv on p. 99. The stochastic force F ′ ﬂuctuates irregularly to
and fro (see Fig. 6.9), and cancels out in the ensemble and the time average. Likewise
the stochastic acceleration a(t), which differs from the derivative of the velocity with
respect to time.
The Langevin equation actually yields the required properties of ⟨r · ˙v ⟩and ⟨r ·
v ⟩. Since no correlations are to be expected between r and a (at equal times), and
since ⟨r · a ⟩vanishes, we deduce ⟨r · ˙v ⟩= −⟨r · v ⟩/τ and in addition
d⟨r · v ⟩
dt
= ⟨v2⟩−⟨r · v ⟩
τ
.
Since ⟨v2⟩does not depend on time and ⟨r · v ⟩vanishes initially,
⟨r · v ⟩= ⟨v2⟩τ

1 −exp −t
τ

solves the problem. Then all requirements are satisﬁed, and the Ornstein–Fürth rela-
tion follows (with ⟨r2(0)⟩= 0) by integrating over time.
We know the solution of the Langevin equation, because in Sect. 2.3.8 we treated
the forced damped oscillation and solved a still more general inhomogeneous differ-
ential equation via a Laplace transformation. The solution to
¨x(t) + 2γ ˙x(t) + ω0
2 x(t) = a(t)

6.2 Entropy Theorem
539
is x(t) = x0(t) +
 t
0 dt′ g(t −t′) a(t′),where x0(t)and g(t)satisfythehomogeneous
differential equation and have the initial values x0(0) = x(0), ˙x0(0) = ˙x(0), and
g(0) = 0, ˙g(0) = 1. We are only interested in the ﬁrst derivative ˙x, for which, using
g(0) = 0, we ﬁnd the expression
˙x(t) = ˙x0(t) +
 t
0
dt′ ˙g(t −t′) a(t′) .
The average force is also absent (ω0 = 0). Now, we have the simple differential
equation ¨g + ˙g/τ = 0 with ˙g(0) = 1, which leads to ˙g(t) = exp(−t/τ). Therefore,
the solution of the Langevin equation for t ≥0 reads
v (t) = v (0) exp −t
τ +
 t
0
dt′ exp −(t −t′)
τ
a (t′) ,
and from ⟨v(0)⟩= 0, it follows that ⟨v(t)⟩= 0. After many collisions, the initial
velocity v (0) is thus “forgotten”, and likewise the acceleration, the longer back it
lies. For τ →∞, nothing is forgotten, but then the diffusion constant from the last
section was much too large.
6.2.8
Generalized Langevin Equation and the
Fluctuation–Dissipation Theorem
So far we have assumed that the collisions are so fast that we could have taken
the correlation to be ⟨a (t) · a (t′)⟩∝δ(t −t′). We now drop this approximation,
assuming that the collisions last for a while. We set
⟨a (t) · a (t′)⟩= ⟨v2⟩γ (|t −t′|) ,
because for an equilibrium distribution, only the time difference |t −t′| may be of
importance, and we leave open the way γ may be affected, although it will surely be
monotonically decreasing towards zero. It is convenient to factorize the ﬁxed factor
⟨v2⟩.
In fact, we only need to modify the solution of the Langevin equation considered
above, viz.,
v (t) = v (0) χ(t) +
 t
0
dt′ χ(t −t′) a (t′) ,
insofar as the linear response function χ to the perturbation a is no longer equal to the
old function ˙g(t) = e−t/τ. In particular, it is determined by ⟨a (t) · a (t′)⟩. Therefore,
we have to generalize the Langevin equation. Note that the linear response function
χ is sometimes called the generalized susceptibility.

540
6
Thermodynamics and Statistics
As before, we assume ⟨a ⟩= 0 and for the equilibrium distribution, i.e., with
⟨v (0) · v (0)⟩= ⟨v2⟩and ⟨v (0) · a (t)⟩= 0, we obtain
⟨v (t) · v (t′)⟩
⟨v2⟩
= χ(t) χ(t′) +
 t
0
dt′′
 t′
0
dt′′′ χ(t−t′′) χ(t′−t′′′) γ (|t′′−t′′′|) .
This expression has to be a function of |t −t′|. But how does γ depend on χ?
This may be answered by doing a Laplace transform. Instead of L {γ } as in
Sect. 2.3.8, we now write γ for the Laplace transform of γ :
γ (s) ≡
 ∞
0
dt e−st γ (t) .
Because γ depends only on |t −t′|, we now consider the double Laplace transform
γ (s, s′) ≡
 ∞
0
dt
 ∞
0
dt′ e−st−s′t′ γ (|t −t′|) ,
and relate it to the single Laplace transform of γ . In particular, using st + s′t′ =
(s + s′) t + s′ (t′ −t) and t′′ = t′ −t, it follows that
γ (s, s′) =
 ∞
0
dt e−(s+s′) t
 ∞
−t
dt′′ e−s′t′′ γ (|t′′|) .
We split the last integral into two, one from 0 to ∞and one from −t to 0, and then
set t′ = −t′′:
γ (s, s′) = γ (s′)
s + s′ +
 ∞
0
dt e−(s+s′) t
 t
0
dt′ es′t′ γ (t′) .
Since exp{−(s + s′) t} is the derivative of −exp{−(s + s′) t}/(s + s′) with respect
to t, we may integrate by parts:
(s + s′) γ (s, s′) = γ (s′) −e−(s+s′) t
 t
0
dt′ es′t′ γ (t′)

t=∞
t=0
+
 ∞
0
dt e−(s+s′) t es′t γ (t) .
Clearly, the “boundary values” do not contribute—the factor exp{−(s + s′) t} kills
the integral for t →∞, and for t = 0 the integral does not contribute. Since all the
functions γ depend only on |t −t′|, we have the “noteworthy property”
γ (s, s′) = γ (s) + γ (s′)
s + s′
.
The double Laplace transform of ⟨v (t) · v (t′)⟩reads accordingly, because for this,
too, only |t −t′| is of importance. It contains the expression

6.2 Entropy Theorem
541
L ≡
 ∞
0
dt
 ∞
0
dt′ e−st−s′t′  t
0
dt′′
 t′
0
dt′′′ χ(t −t′′) χ(t′ −t′′′) γ (|t′′ −t′′′|) .
If we interchange the order of integration here, i.e., swapping t with t′′ and t′ with
t′′′, then t′′ is integrated from 0 to ∞and t from t′′ to ∞, etc. If we then replace
t −t′′ →t and t′ −t′′′ →t′, all four integrals have the limits 0 and ∞and are easily
reformulated:
L =
 ∞
0
dt′′
 ∞
0
dt′′′
 ∞
0
dt
 ∞
0
dt′ e−s (t+t′′)−s′(t′+t′′′) χ(t) χ(t′) γ (|t′′−t′′′|)
= χ(s) χ(s′) γ (s, s′) .
The double Laplace transform of ⟨v (t) · v (t′)⟩/⟨v2⟩is thus equal to
χ(s) χ(s′) {1 + γ (s, s′)} = χ(s) χ(s′) {s′+γ (s′)} + χ(s′) χ(s) {s+γ (s)}
s + s′
.
This has to apply to a function which depends only on |t −t′| and therefore has
the above-mentioned “noteworthy property”. Consequently, χ(s) {s + γ (s)} cannot
depend on s at all, and so has to be a constant. Its value is determined by the require-
ment χ(0) = 1, with v (t) equal to v (0) for t = 0, and is in fact independent of γ . If
we use this for χ(s) in the limit s →∞from χ ≈χ(0)/s, we arrive at the desired
relation
χ(s) =
1
s + γ (s) ,
and hence also obtain the correlation function of the velocities, viz.,
⟨v (t) · v (t′)⟩= ⟨v2⟩χ(|t −t′|) .
The auto-correlation functions of the acceleration and velocity are thus related to each
other uniquely, and so also the ﬂuctuations are related to the diffusion. This impor-
tant discovery is called the ﬂuctuation–dissipation theorem. Instead of the pair of
notions reversible–irreversible (with respect to time), we take the pair conservative–
dissipative with regard to the energy.
For a correlation function
γ (t) = 
2 exp(−2μt) ,
with γ (s) = 
2/(s+ 2μ), the ﬂuctuation–dissipation theorem leads to the func-
tion χ(s) = (s+ 2μ)/{(s+μ)2 −(μ2−
2)}. Since we normally set γ (t) ∝δ(t),
we should expect μ ≫
. Using this and the abbreviation ν =
	
μ2 −
2 < μ, we
obtain the correlation function
χ(t) = exp(−μt) [cosh(νt) + μ
ν sinh(νt)] .

542
6
Thermodynamics and Statistics
For t ≫ν−1, it takes the form μ+ν
2ν
exp{−(μ−ν) t}.
Theconnectionbetweenχ andγ isalsousefulforthederivativeofv withrespectto
t. The starting equation leads tov = v(0) χ + χ a with χ = 1/(s + γ ), and hence to
the equation sv −v(0) = a −γ v. This expression is equal to the Laplace transform
of ˙v. Therefore, we infer the generalized Langevin equation, for which the history
of the object is important:
dv
dt = a(t) −
 t
0
dt′ γ (t −t′) v (t′) ,
if ⟨a ⟩= 0 and ⟨a (t) · a (t′)⟩= ⟨v2⟩γ (|t −t′|).
In the last section, we found χ(t) ≈exp(−t/τ) for t ≥0, which yields χ(s) ≈
1/(s + τ −1). According to the ﬂuctuation–dissipation theorem, γ (s) ≈τ −1 was
obtained, i.e., γ (t) ≈2τ −1 δ(t). This also implies
 ∞
0
dt ⟨a (0) · a (t)⟩≈⟨v2⟩
τ
,
which, according to p. 536, is equal to nD/τ 2. With ⟨v2⟩= nkT/m and α = m/τ,
we also have
 ∞
0
dt ⟨F ′(0) · F ′(t)⟩≈nkT α ,
where F ′ = ma is again the statistically ﬂuctuating force.
Even if we avoid the approximation of the last section, viz., γ (t) ∝δ(t), we may
nevertheless generally rely on γ (t) decreasing almost to zero with increasing t. Then
it seems worthwhile considering a Taylor series expansion of v (t′) about t′ ≈t in
the integrand of the generalized Langevin equation. With t′ instead of t −t′, this
leads to
dv
dt = a −v (t)
 t
0
dt′ γ (t′) + dv
dt
 t
0
dt′ t′ γ (t′) + · · · .
This takes the form of the usual Langevin equation if the ﬁrst integral does not
depend upon t at all (and may be set equal to τ −1) and the remaining integrals do
not contribute. These requirements are satisﬁed if only the average changes in v are
important, averaged over the collision time, so that γ has already decreased to its
ﬁnal value.
6.2.9
Fokker–Planck Equation
Wenowconsiderthedistributionfunctionρ (t, v)forthevelocity.Weexpecttoobtain
a diffusion equation ∂ρ/∂t = Dv vρ with Dv ≥0. The Fokker–Planck equation [7]
also contains a drift term, since it reads

6.2 Entropy Theorem
543
∂ρ
∂t = Dvvρ + ∇v · (ρv)
τ
,
with
Dv = D
τ 2 = kT
mτ ≥0 .
To derive this, we proceed in two steps. To begin with, we consider the Kramers–
Moyal expansion (in one dimension):
∂ρ
∂t =
∞

k=1

−∂
∂v
k
D(k)(v) ρ .
We then justify the claim that it is mainly the ﬁrst two terms that contribute. Here the
general Fokker–Planck equation assumes neither that the drift coefﬁcient is D(1) ∝v,
nor that the diffusion coefﬁcient D(2) has to be constant—it may even also depend
upon t, not only on v. However, D(2) ≥0 has to hold.
If, in the short time t, the velocity changes by w with the probability density
P(t, v ←t−t, v−w), then
ρ (t, v) =

d3w P(t, v ←t−t, v−w) ρ(t−t, v−w) .
If we restrict ourselves for the time being only to motion along a straight line, then
a Taylor expansion about w = 0 delivers
P(t, v ←t−t, v−w) ρ (t−t, v−w)
=
∞

k=0
(−w)k
k!
 ∂
∂v
k
P(t, v+w ←t−t, v) ρ (t−t, v) .
Therefore, we introduce the moments
⟨wk⟩≡

dw P(t, v+w ←t−t, v) wk .
They depend upon v, t, and t. With P(t, v ←t, v −w) = δ(w), all moments with
k > 0 have to vanish for t = 0. In contrast, ⟨w0⟩is always equal to 1. For the
determination of ∂ρ/∂t, we may restrict ourselves to the linear terms in t (the term
k = 0 does not contribute), and using
⟨wk⟩
k!
= D(k)(t, v) t + · · · ,
with k ∈{1, 2, 3, . . .} ,
we arrive at the above-mentioned Kramers–Moyal expansion
∂ρ
∂t =
∞

k=1

−∂
∂v
k
D(k) ρ .

544
6
Thermodynamics and Statistics
Here it is clear that none of the coefﬁcients D(k) with even k are negative, because
the probability density P has this property.
To derive the Fokker–Planck equation, we now have to consider the expansion
coefﬁcients
D(k)(t, v) ≡
1
k!
∂⟨wk⟩
∂t
.
They can be determined from the Langevin equation ˙v = a −v/τ with ⟨a ⟩= 0. If
in the time t, the collision acceleration averages out, and on the other hand t
nevertheless remains so small that we may restrict ourselves to the linear term, we
may conclude that ⟨w⟩= −v t/τ, while for short times, only the auto-correlation
of the collision accelerations contributes to ⟨w2⟩:
⟨w2⟩≈
 t
0
dt′ dt′′ ⟨a(t′) · a(t′′)⟩≈t
 ∞
−∞
dt ⟨a(0) · a(t)⟩= 2Dv t .
Here the expansion coefﬁcients D(k) vanish for k > 2 if for even k, we start from
⟨a(t1) · · · a(tk)⟩=

all pairs
⟨a(ti) a(t j)⟩· · · ⟨a(tl) a(tk)⟩,
and a similar sum for k + 1, where each term also contains a further factor ⟨a⟩. This
ensures that ⟨w2κ+1⟩vanishes for κ > 0. In addition, it then follows that ⟨w2κ⟩∝
(t)κ, so only D(1) and D(2) actually remain different from zero.
With this we can now derive the Fokker–Planck equation (in the three-dimensional
space, correlations between the different directions are not expected):
∂ρ
∂t = ∇v · ρ v
τ
+ Dv v ρ =

3 + v · ∇v + τ Dv v
 ρ
τ .
Reformulation can help us ﬁnd solutions. The average term vanishes if we introduce
the variable u = v exp(t/τ) instead of v (p. 43 is useful for such reformulations):
∂ρ
∂t

v =
∂ρ
∂t

u +

∇u ρ

t ·
∂u
∂t

v =
∂ρ
∂t

u + u
τ ·

∇u ρ

t =
∂ρ
∂t

u + v
τ · ∇v ρ .
Therefore, with ρ now a function of t and u, and with v = exp(2t/τ) u, we arrive
at
∂ρ
∂t =

3 + τ Dv exp 2t
τ u
 ρ
τ .
Theﬁrsttermontheright-handsidedisappearsifweconsiderthedifferentialequation
for f = ρ exp(−3t/τ):
∂f
∂t =
∂ρ
∂t −3ρ
τ

exp −3t
τ
= exp 2t
τ Dv u f .

6.2 Entropy Theorem
545
Fig. 6.10 Fokker–Planck
equation. Diffusion equation
with a drift term (see Fig. 6.8
for the situation without this
term). Also represented are
initially sharp solutions for
the times 1
10τ (red curve), 1
3τ
(blue curve), and τ (green
curve). At the beginning,
⟨v⟩= −3√Dvτ holds. The
stationary ﬁnal distribution is
the dashed curve
Finally, we also set t′ = 1
2τ {exp(2t/τ) −1}, and with dt′ = exp(2t/τ) dt, we obtain
the diffusion equation (in the velocity space)
∂f
∂t′ = Dv u f .
According to p. 536, its solution is f = √4π Dvt′ −3 exp{−(u −u0)2/4Dvt′}. Using
this, and if the initial velocity v0 is given as sharp, the desired solution of the Fokker–
Planck equation reads (see Fig. 6.10)
ρ(t, v) =
1
	
2πτ Dv{1 −exp(−2t/τ)}
3 exp −{v −v0 exp(−t/τ)}2
2τ Dv {1 −exp(−2t/τ)} .
Consequently, the mean value ⟨v ⟩= v0 exp(−t/τ) decreases down to the equi-
librium value 0. But the drift term also limits the squared ﬂuctuation, viz.,
(v)2 = 3τ Dv

1 −exp −2t
τ

,
which then approaches the equilibrium value 3τ Dv twice as fast (with half the relax-
ation time 1
2τ)—otherwise, with τ very large compared to the observation time
t, it would have increased permanently with (v)2 = 6Dvt. This time-dependent
squared ﬂuctuation helps us even for the distribution function:
ρ(t, v) =
1
√2π/3 v(t)
3 exp

−3
2
v −v0 exp(−t/τ)
v(t)
2
.
For t ≫τ with (v)2 →3τ Dv = 3kT/m, it goes over into the equilibrium distri-
bution

546
6
Thermodynamics and Statistics
ρ(v) = exp(−1
2mv2/kT )
√2πkT/m
3
.
We shall derive this Maxwell distribution again in a different way in Sect. 6.3.1.
6.2.10
Summary: Entropy Law
Our aim here was to justify the thermodynamically important entropy law. The
entropy of a closed system can only increase as time goes by, never decrease. This
holds for macroscopic systems with many degrees of freedom if we describe them
with only a small number of variables, and in any case, we could by no means account
for all of them. If the entropy of a closed system increases, it changes irreversibly,
even though all the basic equations of mechanics and electromagnetism remain the
same under time reversal. The entropy law follows from the rate equation. A partic-
ularly impressive example of a rate equation is supplied by the Boltzmann equation.
It holds for a gas of colliding molecules, as long as their probability distributions are
uncorrelated (the assumption of molecular chaos).
The increase in the entropy in closed systems does not contradict the observation
of biological systems, which always become more intricate, and hence less probable.
They are not closed systems.
6.3
Equilibrium Distribution
6.3.1
Maxwell Distribution
The collision integral in the Boltzmann equation vanishes for collisions of identical
molecules, if (see p. 527 for detailed equilibrium)
ρ(t, r, v1) ρ(t, r, v2) = ρ(t, r, v1
′) ρ(t, r, v2
′) .
Energy and momentum conservation also impose the constraints
v1
2 + v2
2 = v1
′2 + v2
′2 ,
v1 + v2 = v1
′ + v2
′ .
Consequently, for elastic collisions, (v1 −v0)2 + (v2 −v0)2 is conserved for arbi-
trary v0. The ﬁrst equation may be brought into this form:
ln ρ(t, r, v1)+ ln ρ(t, r, v2) = ln ρ(t, r, v1
′) + ln ρ(t, r, v2
′) .

6.3 Equilibrium Distribution
547
Note that the sum of two one-particle quantities is conserved. Since both v1 and v2
may be chosen quite arbitrarily, the general solution is
ln ρ = −A (v −v0)2 + ln C ,
and this yields the local Maxwell distribution
ρ(t, r, v) = C(t, r) exp{−A(t, r) (v −v0(t, r))2} ,
with initially arbitrary functions C(t, r), A(t, r), and v0(t, r), provided that it is
normalized correctly, i.e.,

d3r d3v ρ(t, r, v) = 1.
Let us take here the special case in which the probability density depends only
on v. We then have ρ(v) = C exp{−A (v −v0)2} with

d3v ρ(v) = 1. This Gauss
distribution is symmetric with respect to v0. Therefore,
⟨v ⟩= v0 .
Consequently, v0 is the average velocity of a molecule. The normalization requires
C = (A/π)3/2,andtheparameter A isrelatedtothesquaredﬂuctuationinthevelocity
by (v)2 = 3
2 A−1. We thus obtain
ρ(v) =
1
√2π/3 v
3 exp

−3
2
(v −v0)2
(v)2

.
This is the famous Maxwell distribution, if we take (v)2 as a measure of the dis-
orderliness of the motion and relate the associated kinetic energy to the temperature
according to
1
2m (v)2 = 3
2 kT ,
by setting (v)2 = 3kT/m, as discussed on p. 534.
If we restrict ourselves to gases which are on the average at rest (something that
can always be realized with suitable coordinates), then v0 = 0, and the distribution
is isotropic. Only the modulus of v is important in this case. Using d3v = v2 dv dv,
if we require
 ∞
0 dv ρ(v) = 1, then
ρ(v) = 4π v2 exp(−1
2mv2/kT )
√2πkT/m
3
.
Clearly, the maximum of ρ(v) is atv = √2kT/m, and thus 1
2mv2 = kT . The mean
value of the modulus of v lies somewhat higher, namely at ⟨v⟩= (2/√π) v.
But instead of ρ(v), we often consider ρ(E), the distribution with respect to the
kinetic energy E, and use dE = mv dv:

548
6
Thermodynamics and Statistics
Fig. 6.11 Maxwell distributions. ρ(v) (left), ρ(E) (right) in suitable temperature-independent
units: v = √2kT/m and E = 1
2 kT
ρ(E) = 2 √E/kT exp(−E/kT )
√π kT
.
The maximum of this distribution lies at E = 1
2 kT , and its mean value is ⟨E⟩=
3
2 kT = 3E. The uncertainty is E = √3/2 kT (see Fig. 6.11).
6.3.2
Thermal Equilibrium
The Maxwell distribution is an equilibrium distribution, because it was expressly
assumed that collisions do not alter anything. Therefore, in particular, the entropy is
also conserved, despite the collisions.
Generally, thermal (thermodynamic or also statistical) equilibrium exists if the
entropy does not change with time by itself. Such an equilibrium always exists if we
consider closed systems with an entropy as high as possible. Of course, all parameters
which characterize our statistical ensemble must then be given as ﬁxed.
In the Schrödinger picture, a sufﬁcient equilibrium condition is
∂ρ
∂t = 0
=⇒
equilibrium,
since then neither ρ nor the mean values {⟨Ai⟩} depend upon time, including the
entropy. With the Liouville equation, the constraint ∂ρ/∂t = 0 may also be replaced
by the requirement
[H, ρ] = 0 ,
or

k
∂H
∂xk
∂ρ
∂pk
−∂H
∂pk
∂ρ
∂xk = 0 .

6.3 Equilibrium Distribution
549
This is satisﬁed if, instead of the distribution function ρ with its 6N variables, we
take the distribution function ρ(H) with the energy as its only variable. Then,
∂ρ
∂pk
= ∂ρ
∂H
∂H
∂pk
and
∂ρ
∂xk = ∂ρ
∂H
∂H
∂xk ,
and the Poisson bracket [H, ρ(H)] always vanishes.
In quantum theory, stationary states are eigenstates of the Hamilton operator: their
density operator ρ commutes with H. Conversely, from [H, ρ] = 0, in the energy
representation, it follows that (Ez −Ez′) ⟨z| ρ |z′⟩= 0. If there is no degeneracy, i.e.,
Ez ̸= Ez′ for z ̸= z′, then the density operator of an equilibrium state is diagonal:
⟨z| ρ |z′⟩= ρ(Ez) ⟨z|z′⟩, or ρ = 
z |z⟩ρ(Ez) ⟨z|. Here ρ(Ez) is the probability of
the state |z⟩with energy Ez. (We divide possible degeneracies into two classes,
namely those which spring from special symmetries of the Hamilton operator, and
those which are merely accidental. We account for symmetries by further quantum
numbers, or simply multiply ρ(Ez) by the number of degenerate states. However, we
shall disregard accidental degeneracies here. We assume that accidental degeneracies
occur so rarely that they have no statistical weight.)
The above-mentioned equilibrium condition ∂ρ/∂t = 0 may also be replaced by
the sufﬁcient constraint that ρ depend only on the energy. (However, this is not nec-
essary, because according to the Liouville equation, for degenerate states there may
also be entropy-increasing exchanges without energy change.) In the following, we
shall determine several canonical distributions for different equilibrium conditions.
Here we must always make an assumption concerning the energy with reference to
the equilibrium conditions.
6.3.3
Micro-canonical Ensemble
Closed systems belong to a micro-canonical ensemble if they have the same external
parameters, their energy lies in the interval between E and E + dE, and they are in
equilibrium. Their entropy is then as high as possible, otherwise it would not be an
equilibrium. According to Problem 6.6 (Sect. 6.1.6), all ZMC permitted (accessible)
states have the same probability, the values resulting from the normalization of ρ:
ρMC(Ez) =

ZMC−1 , for E ≤Ez ≤E + dE ,
0 ,
otherwise .
The constant ZMC, which is the number of states in the considered energy regime, is
the partition function. Note that, since the letter Z is the generally accepted notation
for the partition function, we count the states with z and the upper boundary is called
Z. Here the partition functions are related to the various ensembles, which is why
we append the subscript “MC” for micro-canonical.

550
6
Thermodynamics and Statistics
The energy values Ez depend on the given problem. We shall take care of this
later. Here we are interested primarily in the question of the probabilities with which
the single energies occur in the ensemble, in order to make the entropy as high as
possible, since this determines the equilibrium.
The idea of requiring equal “a priori probabilities” is suggestive even without
considering the entropy. It is the only sensible assumption, as long as there are no
reasons to prefer certain states over others in the considered regime. For any other
distribution, there are irreversible transitions between the states until equilibrium
is reached, at which point the entropy is maximal. According to Sect. 6.1.6, this
highest entropy is S = k ln ZMC. It belongs to ZMC states with equal probabilities
ρz = ZMC−1.
It is often claimed that the entropy S may be expressed in terms of the thermo-
dynamic probability W in the form S = k ln W, even though it is admitted that this
“probability” might be greater than one, which contradicts the notion of probabil-
ity. In contrast, there is a corresponding equation with the micro-canonical partition
function ZMC rather than the thermodynamic probability W. In some sense though,
this partition function may be connected to an occurrence, and relative occurrences
do lead to probabilities. In this context, we compare two micro-canonical ensembles:
the original one with the partition function ZMC and another, which is less restricted
and also contains other states. Then its partition function ZMC> is greater than ZMC.
According to the basic assumption of equal a priori probabilities, the probability of
a state of the original ensemble in this larger ensemble is given by ZMC/ZMC>. Here
ZMC> is in fact not uniquely ﬁxed, but this freedom “only” relates to the zero of the
entropy: the denominator necessary for the normalization in fact shifts the origin of
the entropy, but what is important are usually only differences in entropy.
The relation S = k ln W is called Boltzmann’s principle. From W = exp(S/k)
and ˙S ≥0, it follows that ˙W ≥0, which tells us that the “disorder” in an isolated
system can only increase as time goes by.
6.3.4
Density of States in the Single-Particle Model
For macroscopic bodies, the density of the energy eigenvalues Ez increases approx-
imately exponentially with the energy, as we shall now show with a particularly
simple example.
We consider a system of very many distinguishable particles which all feel the
same average force, but no rest interaction—thus without correlations between the
particles. (As long as the rest interaction can be treated with perturbation theory, the
results barely change. The levels may move relative to each other, but this affects
neither the partition function nor the average level density.) According to quantum
theory, the one-particle potential ﬁxes the one-particle energies and hence also the
number of states below the energy E, which for the N-particle system we shall denote
by (E, N). Note that, on p. 525, we wrote  for (E, 1). We now have

6.3 Equilibrium Distribution
551
ZMC = (E + dE, N) −(E, N) ,
and instead of summing over z, we may also integrate over the energy, if we take the
density of states ∂/∂E as weight factor: 
z =

dE ∂/∂E.
Since we have assumed only particles that are independent of each other, and
therefore neglect correlations, for this “number of states”, we have
(E, N) ≈N(E/N, 1) .
Here the approximation consists in saying that not all particles have to have the same
energy—only the total energy is given. But we shall soon see that for sufﬁciently
large N, (E, N) depends so strongly on the energy that other energy separations
barely contribute to the density of states. The number of one-particle states does not
in fact depend particularly strongly on the energy, e.g., according to p. 525, for a gas
of interaction-free molecules, we ﬁnd p3
F ∝E3/2. But the huge power N leads to a
very strong energy dependence of (E, N) for the N-particle system. In particular,
if ( 1
2 E, 1
2 N) = a E M holds with M ≫1, then the product is

1
2(E + ε), 1
2 N

· 
1
2(E −ε), 1
2 N

= a2 (E2 −ε2)M .
Even for ε/E = √α/M, this is smaller than a2 E2M by the factor e−α, e.g., with a
millimol and ε/E = 10−9, whence α = 3
2 × 6 × 1020 × 10−18 = 900 by nearly 400
orders of magnitude. Therefore, only (E/N, 1) is actually important. An example
is shown in Fig. 6.12.
Fig. 6.12 The number (E, N) of states up to the energy E of an N-particle system decreases
rapidly if the energy is not distributed evenly over all particles. Here, one half has the energy
E< = 1
2 (E −ε) and the other half the energy E> = 1
2 (E + ε). We plot the ratio (E<, 1
2 N) ·
(E>, 1
2 N)/(E, N) against ε/E for N = 1000 (dashed curve) and for N = 2000 (continuous
curve)

552
6
Thermodynamics and Statistics
Fig. 6.13 Probability distribution ρ(Ez) of a micro-canonical ensemble of 100 particles in a cube
as a function of Ez. Here the density of states increases with Ez. The higher energies in the allowed
regime contribute more strongly than the lower ones
For an energy shift E →E + δE, the function (E, N) changes so much that a
Taylor series makes sense only for its logarithm:
ln (E + δE, N) ≈ln (E, N) + ∂ln (E, N)
∂E
δE .
Here the factor in front of δE is huge, namely 3
2 N/E for  ∝(E3/2)N. Even for one
millimol and δE/E = 10−9, ln  increases by nearly a trillion—and the number of
states increases in this approximation exponentially with the energy δE to
(E + δE, N) ≈(E, N) exp
∂ln (E, N)
∂E
δE

.
This property of the partition function or of the density of states ∂/∂E leads us to a
new problem: for all mean values of the micro-canonical ensemble, the upper energy
regime is much more important than the lower one. Here, only the mean value of the
energy is accessible to us macroscopically, so we should give ⟨E⟩and not start from
the micro-canonical ensemble (see Fig. 6.13).
Note that the density of states also increases with the particle number N and
the volume V as strongly as with the energy E, because the above considerations
may be transferred to all other extensive parameters. By an extensive parameter,
we understand a macroscopic parameter which is proportional to the size of the
system, like the particle number, the energy, and the volume. In contrast, intensive
parameters keep their value under subdivision of the system, e.g., the temperature
T and the pressure p.
6.3.5
Mean Values and Entropy Maximum
For all “canonical ensembles” except for the micro-canonical one, we always ﬁx
average values: for the canonical ensemble, the energy ⟨E⟩, for the grand canoni-
cal ensemble, also the particle number ⟨N⟩, and for the generalized grand canoni-
cal ensemble also other mean values, such as the volume ⟨V ⟩, which for the other
ensembles is given precisely, just as the particle number N is given precisely for the
canonical ensemble.
We now search for the general distribution {ρz} with the highest entropy which
is consistent with the constraints given by the mean values ⟨Ai⟩. Here we take only
mean values of extensive quantities, such that the error widths remain as negligible
as possible.

6.3 Equilibrium Distribution
553
An indispensable constraint is trρ ≡⟨1⟩= 1. Therefore, we begin with i = 0 and
set A0 = 1. For n further constraints, i runs up to n. With the Lagrangian parameters
−kλi for the unknown ρz, we have the variation problem
δ

S −k
n

i=0
λi ⟨Ai⟩

= 0 ,
or δ

z
ρz

ln ρz +
n

i=0
λi Aiz

= 0 .
The extremum is obtained from ln ρz + n
i=0 λi Aiz + 1 = 0 and leads to
ρz = exp

−1 −
n

i=0
λi Aiz

= exp

−
n

i=1
λi Aiz

/ exp(1 + λ0) .
The Lagrangian parameter λ0 follows from the norm trρ = 1. If no further mean
values are given, then the highest entropy belongs to ρz = 1/Z with Z = 
z 1, as
we know already from Sect. 6.1.6. Otherwise, taking the partition function
Z ≡

z
exp

−
n

i=1
λi Aiz

,
with 
z ρz = 1, we have the equation exp(1 + λ0) = Z. Hence, λ0 = ln Z −1 and
ρz = 1
Z exp

−
n

i=1
λi Aiz

.
The remaining Lagrangian parameters λi are related to the corresponding mean
values:
⟨Ai⟩= 1
Z

z
Aiz exp
⎛
⎝−
n

j=1
λ j A jz
⎞
⎠= −1
Z
∂Z
∂λi
= −∂ln Z
∂λi
.
The mean values ⟨Ai⟩thus follow from derivatives of the partition function Z, so
we have to determine Z(λ1, . . . , λn) such that, for all i ∈{1, . . . , n}, the equations
⟨Ai⟩= −∂ln Z/∂λi are satisﬁed, where the remaining Lagrangian parameters λ j
with j ̸= i are to be kept ﬁxed.
We have thus found the constraints for the extremum of S[ρ]. It is a maximum,
because −k ρz (ln ρz + n
i=0 λi Aiz) differentiated twice with respect to ρz is equal to
−k/ρz < 0. We shall investigate the physical meaning of the Lagrangian parameters
λ1, . . . , λn in Sect. 6.3.8. These are adjustable parameters and lead us among other
things to the temperature and the pressure.
Note that the partition function also yields the squared ﬂuctuation of ⟨Ai⟩, because
from

554
6
Thermodynamics and Statistics
⟨A2
i ⟩= 1
Z
∂2Z
∂λi 2 =
∂
∂λi
 1
Z
∂Z
∂λi

+ 1
Z2
 ∂Z
∂λi
2
= −∂⟨Ai⟩
∂λi
+ ⟨Ai⟩2 ,
we deduce
(Ai)2 = −∂⟨Ai⟩
∂λi
.
Since the squared ﬂuctuation is non-negative, the partial derivative must not be pos-
itive. If it is zero, then there is no unique relation ⟨Ai⟩→λi. Otherwise ⟨Ai⟩is a
monotonically decreasing function of λi, and so λi is a monotonically decreasing
function of ⟨Ai⟩. Clearly, also (Ai)2 = ∂2 ln Z/∂λi 2 holds.
If the mixed derivatives ∂2 ln Z/(∂λi ∂λ j) are continuous, then the order of the
derivatives may be interchanged. Then we arrive at the equations
∂⟨Ai⟩
∂λ j
= ∂⟨A j⟩
∂λi
.
These are Maxwell’s integrability conditions (Maxwell relations), which will turn
out to be useful later on.
6.3.6
Canonical and Grand Canonical Ensembles
For the canonical ensemble, the mean value of the energy ⟨E⟩is given, in addition
to the norm ⟨1⟩. According to the last section, we then have the canonical partition
function:
ZC ≡

z
exp(−λE Ez) = tr[exp(−λE E)]
and the probability distribution
ρC = 1
ZC
exp(−λE E) .
Note that the Lagrangian parameter λE is related to the energy, but the letter β is
usually used, even though β will be used for the pressure coefﬁcients (see p. 619).
Here, for brevity, we have left out the index z for ρC and E. For the same reason, the
trace notation is convenient for the partition function. If states are degenerate, we
have to multiply by their degree of degeneracy.
For canonical ensembles, what is important is thus to know how the given mean
value ⟨E⟩depends upon the adjustable parameter λE. According to the last section,
we have (see Fig. 6.14)
⟨E⟩= −∂ln ZC
∂λE
and
(E)2 = −∂⟨E⟩
∂λE
.

6.3 Equilibrium Distribution
555
Fig. 6.14 The level density
increases approximately as
E3N/2, and the occupation
probability decreases as
e−λE. Hence xne−x is
important, with maximum
for x = n. Here this function
is shown for n ∈{2, 4, 8, 16}
relative to its maximum, and
therefore as a function of x/n
We shall later relate the temperature to the parameter λE. Indeed, we shall ﬁnd that
λE is the reciprocal of kT .
For any canonical ensemble of macroscopic bodies, only a small energy range
δE is of importance. If we approximate its partition function ZC by an integral of
the energy with the integrand f (E, N) = exp(−λE E) ∂(E, N)/∂E, then for large
N, f (E, N) has a very sharp maximum at E. For the density of states of a gas of
interaction-free molecules, for example, the integrand f (E) ∝exp(−λE E) E3N/2
is to be considered near its maximum at E = 3
2 N/λE, and after a Taylor series
expansion,
f (E + δE) ≈f (E) exp{−3
4 N (δE/E)2} ,
we ﬁnd a Gauss distribution with the tiny width E/√3N/2 ≪E. (Who would ever
determine the energy up to twelve digits for one mole?) Consequently, we have
E ≈⟨E⟩, and for such a sharp maximum, only the states from the nearest neigh-
borhood are important. Therefore, the canonical and micro-canonical ensembles
are very similar—the energy uncertainty (via λE) is given instead of the energy
range dE. Therefore, a distribution parameter λE may even be assigned to a micro-
canonical ensemble, and with this a temperature, as will be shown in Sect. 6.3.8.
The requirement is that exp(−λE E) ∂/∂E should have its maximum at E, which
requires λE ∂/∂E = ∂2/∂E2, or λE = ∂ln (∂/∂E)/∂E for each E = E, i.e.,
(kT )−1 = ∂ln(∂/∂E)/∂E|E.
For the grand canonical ensemble, in addition to ⟨1⟩and ⟨E⟩, we also ﬁx the
particle number ⟨N⟩only on the average. Then we have
ρGC =
1
ZGC
exp(−λE E −λN N) ,
with
ZGC ≡tr exp(−λE E −λN N) =

N
exp(−λN N) ZC(N) .

556
6
Thermodynamics and Statistics
Even more mean values characterize the generalized grand canonical ensemble.
For this, in addition to ⟨1⟩, ⟨E⟩, and ⟨N⟩, further quantities ⟨Vi⟩are given, e.g., the
average volume. Then we have
ρ = 1
Z exp(−λE E −λN N −
i λiVi) ,
with
Z ≡tr exp(−λE E −λN N −
i λiVi)
and
⟨E ⟩= −∂ln Z
∂λE
,
(E)2 = −∂⟨E ⟩
∂λE
≥0 ,
⟨N⟩= −∂ln Z
∂λN
,
(N)2 = −∂⟨N⟩
∂λN
≥0 ,
⟨Vi⟩= −∂ln Z
∂λi
,
(Vi)2 = −∂⟨Vi⟩
∂λi
≥0 .
In the following, we shall imagine as the other quantities Vi only the volume and
then, instead of 
i λiVi, take only λV V . Here we shall sometimes ﬁx the particle
number, thus give only ⟨1⟩, ⟨E⟩, and ⟨V ⟩as mean values. This ensemble has no
special name.
According to the last section, the entropy is
S = −ktr (ρ ln ρ) = −k

z
ρz ln exp(−n
i=1 λi Aiz)
Z
= k (ln Z +
n

i=1
λi ⟨Ai⟩) .
Using this, for generalized grand canonical ensembles, we obtain
S = k (ln Z + λE ⟨E⟩+ λN⟨N⟩+ λV ⟨V ⟩) ,
with somewhat simpler expressions for canonical and grand canonical ensembles,
which are not so important at the moment, because we also wish to investigate the
dependence on ⟨N⟩and ⟨V ⟩. Here Z is a function of the Lagrangian parameters
λE, λN, and λV (see, e.g., Fig. 6.15). We investigate the canonical partition function
ZC(λE, N, V ) on p. 575 and the grand canonical partition function ZGC(λE, λN, V )
on p. 579.
In the following, we shall usually drop the bracket symbols ⟨⟩, because we
consider only mean values anyway, if not explicitly stated otherwise. In addition,
we adopt the common practice in thermodynamics of writing U for the energy E.
It is referred to as the internal energy, bearing in mind that there are also other
forms of energy. In Sect. 6.3.1, for the Maxwell distribution, we divided the kinetic
energy into the collective part m
2 ⟨v⟩2 and the disordered part m
2 (v)2, since we have
⟨v2⟩= ⟨v⟩2 + (v)2. For such an ideal gas, only the disordered motion counts for

6.3 Equilibrium Distribution
557
Fig. 6.15 If the volume V or another extensive parameter changes, then every energy eigenvalue Ez
also changes, and so therefore does the density of states ∂/∂E, here shown for the same example
as in Fig. 6.13, viz., 100 molecules in a cube
the internal energy, the collective center-of-mass motion being considered as one of
the macroscopic parameters.
6.3.7
Exchange Equilibria
For inhibited (partial) equilibria with special “constraints” (inhibitions), only parts
of the system are in equilibrium (each with an entropy as high as possible), which
for the total system without the inhibition would have a higher entropy. It is not
in total (global) equilibrium. We are not interested in the exact description of the
transition from partial to total equilibrium under removal of the inhibition—for that,
we would have to solve rate equations. Here the initial and ﬁnal state sufﬁce: the new
equilibrium is reached by suitable alterations of the partial systems—an exchange
equilibrium (total equilibrium) then develops.
We exemplify by considering two separate closed systems, each of which is in
equilibrium and has the average energy Un and entropy Sn (n ∈{1, 2}). If the two
systems come into contact, in most cases, the total system will not yet be in equilib-
rium: then the two parts exchange energy, as long as the total entropy increases, i.e.,
Sf ≥Si. Here it is assumed that the coupling is so weak and the energy exchange
so slow that, for the total energy, U = U1 + U2 always holds and the probability
distribution always factorizes (and thus S = S1 + S2 holds). In the new equilibrium
state, the total entropy is then as high as possible: δS = δS1 + δS2 = 0 under the
constraint δU = δU1 + δU2 = 0. Exchange equilibrium with respect to the energy
leads to the requirement δS = 
n(∂Sn/∂Un) δUn = 0, thus to
∂S1
∂U1
= ∂S2
∂U2
,
or λE1 = λE2 ,
since we have S = k (ln Z + λEU)—because Z is a function of λE and hence of U,
thus Z(λE(U))—and this implies that

558
6
Thermodynamics and Statistics
1
k
∂S
∂U =
∂ln Z
∂λE
+ U
 ∂λE
∂U + λE
and also, for U = −∂ln Z/∂λE and ∂U/∂λE = −(U)2,
1
k
∂S
∂U = −U + U
−(U)2 + λE = λE .
Note that, in the partial derivatives, N and V , or λN and λV , are held constant. The
equilibrium state of systems in thermal contact can thus be recognized by all parts
having equal distribution parameter λEn.
These considerations are clearly valid not only for the energy U, but also for the
particle number and the volume. Under the constraint δN = 0, δS = 0 delivers
∂S1
∂N1
= ∂S2
∂N2
,
or λN1 = λN2 ,
and under the constraint δV = 0, δS = 0 delivers
∂S1
∂V1
= ∂S2
∂V2
,
or λV 1 = λV 2 .
The exchange equilibrium is only reached if the Lagrangian parameters in all parts
agree with each other.
Now we can better understand how reversible and irreversible changes of state
are distinguished. In the last section, we removed closed systems of inhibitions and
local differences were then equalized, e.g., by diffusion or temperature adjustment.
Such a change of state proceeds by itself and is not reversible, but irreversible—and
the entropy increases.
However, we may also modify external parameters, e.g., supply energy. This
may also happen reversibly, or one part reversibly and another part irreversibly. The
change is then reversible if it proceeds solely through equilibrium states. However,
this constraint is only satisﬁed if no internal equalization is necessary.
6.3.8
Temperature, Pressure, and Chemical Potential
According to p. 513, the zeroth main theorem of thermodynamics states that: There
is a state variable called temperature T and two parts of a system are only in thermal
equilibrium if they have the same temperature. This equilibrium depends in particular
on the possibility that energy may be exchanged. Like λE, the temperature is the
same in all parts—the two parameters describe the same situation. The larger λE,
the more important are the states of low energy, and the cooler the considered body:
λE is inversely proportional to the temperature. They are related by the Boltzmann
constant k according to

6.3 Equilibrium Distribution
559
λE = 1
kT ,
or
T =
1
kλE
,
as we shall show. This also implies
∂
∂λE
= −kT 2
∂
∂T .
If the average energy is given, then the (thermodynamic) temperature T characterizes
the equilibrium distribution, but if the energy has to be sharp, then the notion of
temperature is useless.
However, the zeroth main theorem only states when two temperatures are equal.
We could also take another function f (T ) as the temperature. In this sense any
uncalibrated mercury thermometer serves its purpose within its measurable range,
but without a gauge, not even temperature differences can be given uniquely. Thus
for a canonical distribution, the thermodynamic temperature is uniquely determined
by T = (kλE)−1. Then the behavior of macroscopic models, e.g., of an ideal gas, can
be determined as a function of the temperature (or of the parameter λE), and hence
a gas thermometer can be constructed as a measuring device. In Sect. 6.5.4, we shall
prove the thermal equation of state for ideal gases (the Gay-Lussac law), viz.,
pV = NkT ,
from which the gas thermometer gauge may be derived. And we shall actually prove
pV = N/λE there!
It is immediately clear that, for T = 0, special situations occur, since then λE =
∞. Now for all equilibria with a ﬁnite energy uncertainty (with T > 0),
(U)2 = −∂U
∂λE
> 0
=⇒
∂U
∂T > 0 .
With decreasing temperature T , the internal energy U thus also decreases, implying
that the states of low energy are preferentially occupied. In this limit, only the ground
state is occupied, if it is not degenerate. Correspondingly, the equilibrium distribution
for T = 0 only depends on whether or not the ground state is degenerate, and likewise
the entropy. If there is no degeneracy, then ρz is different from 0 for only one z and
hence S = 0. This property is called the third main theorem of thermodynamics.
In classical statistical mechanics, the following equidistribution law can be
derived: All canonical variables (positions, momenta) which occur in only one term
in the Hamilton function, and there as squared, contribute the value 1
2 kT to the inter-
nal energy in a canonical ensemble. For the proof, we take the Hamilton function
H = H0 + cx2, where H0 and c do not depend upon the coordinate x. In a canonical
ensemble, this variable x contributes

560
6
Thermodynamics and Statistics
 ∞
−∞
dx ρ(x) cx2 =

dx exp(−λE cx2) cx2

dx exp(−λE cx2)
= −∂
∂λE
ln
 ∞
−∞
dx exp(−λEcx2)
to the internal energy. The integral has the value √π/(λE c), whence 1
2 ln λE has to
be differentiated with respect to λE, which results in 1
2/λE = 1
2 kT . This proves the
equidistribution law.
Then, for example, for force-free motion, the squares of the components of the
momentum for the three space directions enter as separate terms—a single free par-
ticle thus has the energy 1
2m (v)2 = 3
2 kT , as claimed on p. 547 for the Maxwell
distribution, and now proven. Consequently, ideal gases with N atoms (without inter-
nal degrees of freedom) have U = 3
2 NkT . Correspondingly, for the linear harmonic
oscillator, the internal energy is 2
2 kT . The virial theorem in mechanics (see p. 79)
then shows that ⟨Epot⟩= ⟨Ekin⟩. It thus also holds in quantum theory, but it should
be noted, however, that it often delivers discrete energy eigenvalues so the above-
mentioned integrals are then sums 
z ρ(Ez) Ez, which for low temperatures leads
to deviations from classical statistics. This shows up quite clearly in connection with
the freezing of degrees of freedom.
If two parts exchange not only energy, but also volume, then not only do their
temperatures become equal, but also their values of the parameter λV . It is common
to set
λV = p λE =
p
kT ,
because pV is then an energy. This means that p is an energy/volume = force/area
and has the unit N/m2 = Pa = 10−5 bar. In addition, for ﬁxed λE (> 0), (V )2 =
−∂V/∂λV > 0 implies the relations ∂V/∂p < 0 and ∂p/∂V < 0. If the volume
decreases, then p increases, provided that no other parameters change: p is the
pressure with which the system acts on the container walls. It is only when it is the
same in all parts that any volume exchange will cease.
Correspondingly, the Lagrangian parameter λN becomes the same in all parts of
a system if particles can be exchanged. We assume that the temperature becomes
equal and set
λN = −μ λE = −μ
kT .
Then μN is an energy, and so is μ, the chemical potential. Like temperature and
pressure, it is a distribution parameter and important for chemical reactions, as will
be shown below. Since (N)2 > 0, we have ∂N/∂μ > 0 for ﬁxed λE (> 0). As
observed, e.g., in Figs. 6.19 and 6.22, the chemical potential is often, but not always
negative.
For materials involving different types of particles, the expression μN in the
exchange equilibrium is replaced by 
i μi Ni, as will be proven in Sect. 6.5.5.
However, chemical equilibria have to be treated separately, because the molecules
are counted as particles, but in chemical reactions, only the number of atoms is
constant, and not necessarily the number of molecules, e.g., not for 2 H2O →2
H2+O2. If we take Xi as a symbol for the ith sort of molecule, then we have

6.3 Equilibrium Distribution
561

i
νi Xi = 0 ,
where the stoichiometric coefﬁcients νi are positive for reaction products, negative
for reaction partners (and then integers as small as possible)—in the above-mentioned
example, they take the values −2, 2, and 1. After dn reactions, we have dNi = νi dn
(actually n is a natural number, but we may go over to a continuum by referring to the
very large total number). This implies δS = 
i(∂S/∂Ni) νi dn = 0 as equilibrium
condition. Then, according to the last section, 
i λNiνi = 0, and hence,

i
νi μi = 0 .
We shall use this equation on p. 588 for the law of mass action for chemical reactions.
6.3.9
Summary: Equilibrium Distributions
Equilibrium distributions do not change with time—the entropy is as high as possible
given the constraints. This happens if the probability distribution depends only on
the energy. For the micro-canonical ensemble, all states in the energy range from E
to E+dE are occupied with equal probability. For the other canonical ensembles,
some parameters are given only as average values (for macroscopic systems, the
ﬂuctuations about the mean value are normally extremely small). To each mean
value there is a distribution parameter which, in the exchange equilibrium, is the
same for all parts. To the energy corresponds the temperature T , to the volume the
pressure p, and to the particle number the chemical potential μ. Here the Lagrangian
parameter λE = 1/kT , λV = p/kT , and λN = −μ/kT were initially introduced
as distribution parameters. For n given mean values {⟨Ai⟩}, the partition function
Z = tr[exp(−n
i=1 λi Ai)] turns out to be useful because ⟨Ai⟩= −∂ln Z/∂λi and
(Ai)2 = ∂2 ln Z/∂λi 2.
6.4
General Theorems of Thermodynamics
6.4.1
The Basic Relation of Thermodynamics
From the relation for the entropy of a generalized grand canonical ensemble, we shall
now derive the following important equation of macroscopic thermodynamics:
dU = T dS −p dV + μ dN .

562
6
Thermodynamics and Statistics
Since we take the equilibrium expression for S, it holds only for reversible changes
of state, or at least for changes of state in which so far all external parameters have
been kept ﬁxed, so dV = 0 and dN = 0.
In Sect. 6.3.6, we derived the equation
S = k (ln Z + λEU + λV V + λN N)
for the entropy. Here the partition function Z is a function of the three Lagrangian
parametersλE,λV ,andλN,andaccordingtothesamediscussion,⟨Ai⟩= −∂ln Z/∂λi
implies d ln Z = −U dλE −V dλV −N dλN, and hence
dS = k (λE dU + λV dV + λN dN) .
According to Sect. 6.3.8, the Lagrangian parameters λE, λV , and λN are related to
the temperature T , the pressure p, and the chemical potential μ:
λE = 1
kT ,
λV = p λE ,
and λN = −μ λE .
Consequently, for T ̸= 0,
dS = dU + p dV −μ dN
T
,
and we have thus proven the claim that dU = T dS −p dV + μ dN.
For the grand canonical ensemble, the term −p dV does not occur, because the
volume is to be kept constant, and for the canonical ensemble, the term μ dN is
also missing, because the particle number is then also ﬁxed. Particularly often, the
equation is used with dN = 0, namely, in the form dU = T dS −p dV .
If the changes in the state quantities do not proceed purely through equilib-
rium states, but nevertheless begin and end with such states, then, in addition to
the reversible change of state just treated, there will also be an irreversible one.
According to the entropy law—and from now on we always assume dt > 0—the
entropy increases without a change in the other macroscopic parameters. This can
be accounted for by
dS ≥dU + p dV −μ dN
T
,
or again, for T > 0,
dU ≤T dS −p dV + μ dN .
The equations for reversible processes become inequalities for irreversible ones, if
we stay with ﬁxed dt > 0.

6.4 General Theorems of Thermodynamics
563
6.4.2
Mechanical Work and Heat
For ﬁxed particle number (dN = 0), we now consider the inequality
dU ≤T dS −p dV
somewhat more deeply, thus allowing for irreversible changes of state. We think,
for example, of a gas with pressure p in a cylinder with (friction-free) mobile pis-
tons. In order to reduce the volume (dV < 0), we have to do work δA = −p dV
on the system. This energy is buffered in the gas—its pressure increases, because
the molecules hit the walls more often. Alternatively, a spring might be extended or
compressed. Instead of δA = −p dV , we may also take δA = (±) 
k Fk dxk with
generalized coordinates xk and associated generalized forces Fk. The sign has to be
adjusted to the relevant notion.
The work δA is not generally a complete differential, because heat is also trans-
ferred. Even in a cycle process, i.e., going through different states before returning
to the initial state,

δA does not generally vanish. If it did, this would be a sign of
a complete differential dA, or a state variable A, whence the integral

dA would
depend only on the initial and ﬁnal points of the path and not on the path in-between.
We know this situation already from mechanics (p. 56). Only for

F · dr = 0 can
we introduce a potential energy—Lorentz and frictional forces are situations where
this is not possible. At least the Lorentz force (see Sect. 2.3.4) can be derived from a
generalized potential energy q ( −v · A), or q vμAμ if, in addition to the position,
we also allow the velocity as a variable, provided that there is no frictional force. As
is well known, this leads to heat, our subject here.
The internal energy U also increases if we supply energy without changing the
volume V . Here the temperature does not even need to increase notably (latent heat).
Then, e.g., at the normal freezing temperature of water, we need a melting heat of 6
kJ/mole to melt ice. This is often written in the form (H2O) = [H2O] + 6 kJ. If the
solid phase is set in angular brackets, the liquid in round, and the gaseous in curly,
then we have (per mole)
(. . .) = [. . .] + melting heat ,
{. . .} = (. . .) + vaporization heat ,
{. . .} = [. . .] + sublimation heat .
Here, we may neglect the volume change for melting, but not of course for vaporiza-
tion, which is why there are tables, e.g., [8], listing the vaporization enthalpy, i.e.,
the energy difference for constant pressure. We shall return to this in Sect. 6.4.4.
If we set δQ for the amount of heat in an inﬁnitesimal process, the energy con-
servation law for dN = 0 takes the form
dU = δQ + δA ,
with dU = 0 ,
for closed systems .

564
6
Thermodynamics and Statistics
This important equation is called the ﬁrst main theorem of thermodynamics. Here,
irreversible processes are also permitted. The essentially new aspect compared to
mechanics is the kind of energy, i.e., “heat”.
If we restrict ourselves to reversible processes, the comparison with the ﬁrst men-
tioned equation dU = T dS + δA supplies the second main theorem of thermody-
namics, viz.,
δQrev = T dS ,
or dS = δQrev
T
.
After our rather detailed investigation of the entropy, this is almost self-evident, as
soon as the notion of the amount of heat has been clariﬁed by the ﬁrst main theorem.
While the entropy for reversible δQrev may increase or decrease, depending on
its sign, for irreversible processes it always increases. We have already investigated
in detail the entropy law “dS/dt ≥0 for closed systems” as a further constituent
of the second main theorem. Therefore, all the main theorems of thermodynamics
have been explained sufﬁciently—we have already discussed the zeroth and third in
Sect. 6.3.8.
Note that, using the second main theorem, a thermometer can be gauged, which
is a problem, according to p. 559. In particular, by the second main theorem, the
equation

dS =
 δQrev
T
= 0
holds for a cycle.
The Carnot process appears in the (S, T ) diagram in Fig. 6.16 as a rectangle with
0 =

dS = Q+
T+
−Q−
T−
=⇒
T−
T+
= Q−
Q+
.
Hence, via the reversibly exchanged amounts of heat, the temperature can be mea-
sured in arbitrary units—the discussion in Sect. 6.3.8 did not reach this far.
Fig. 6.16 In the Carnot
cycle, the amount of heat
Q+ is reversibly taken in at
the temperature T+ and the
amount of heat Q−is
reversibly taken out at the
temperature T−. No heat is
exchanged in-between, and
the total work taken in is
Q+ −Q−, equal to the
enclosed area in the (S, T )
diagram. For a more general
cycle, see Problem 6.25

6.4 General Theorems of Thermodynamics
565
The Carnot cycle is the ideal of a steam engine. In the combustion chamber, an
amount of heat Q+ is taken in at the temperature T+, and in the condenser, an amount
of heat Q−is taken out at the temperature T−and given off to the cooling water
(usually also at intermediate temperatures, which is not convenient). The difference
Q+ −Q−=

δQ can at most be converted to exploitable work −

δA, the energy
remaining conserved for cyclic systems on the time average, and always for closed
systems. The ratio of this work to the gained (input) energy Q+ is the thermodynamic
efﬁciency η of the machine. (Modern power plants can reach η > 45%, James Watt
had η ≈3%, and its predecessors, e.g., Thomas Savery, a tenth of it.) According
to Carnot, this efﬁciency has an upper limit ηC <1, because η = (Q+−Q−)/Q+ =
1−T−/T+ and the cooling water (without energy input) cannot be cooler than the
environment (and the ﬁre cannot be arbitrarily hot). In reality, the efﬁciency is less,
because heat is exchanged for intermediate temperatures and everything should go
quickly, so changes are not only quasi-stationary.
In essence, the steam engine converts a part of the disordered motion (at high
temperature) into ordered motion (work)—the energy is thereby changed from many
degrees of freedom to a few. Nevertheless, the total entropy does not decrease,
because it moves heat from the ﬁre into the cooling water, and there the entropy
increases more notably.
6.4.3
State Variables and Complete Differentials
State variables characterize a state, e.g., energy U, particle number N, and volume
V are state variables in thermodynamics. They may be taken as functions of other
state variables (x1, . . .) = x. Then,
d f ≡f (x + dx) −f (x) =

i
∂f
∂xi dxi
and

d f = 0 .
This quantity d f is called a complete (or total or exact) differential.
But not every inﬁnitesimal quantity δ f is a complete differential d f . We shall
write δ f for all differential forms of the kind encountered in the variational calculus,
while many use only d f , even for non-exact differentials. Then,
δ f =

i
ai dxi
is a complete differential only if ai = ∂f/∂xi for all i, and on all simply-connected
regions,
∂ai
∂xk = ∂ak
∂xi ,
for all i and k .

566
6
Thermodynamics and Statistics
Thus ∂2 f/∂xk∂xi = ∂2 f/∂xi∂xk is required, but the partial derivatives only com-
mute if they are continuous. If this necessary and sufﬁcient constraint on a complete
differential is violated, then the inﬁnitesimal quantity δ f is a “non-exact differen-
tial”. Then the path becomes decisively important for the integration. For example,
δ f = αx−1 dx + βx dy is not exact, since ∂ax/∂y = 0, but ∂ay/∂x = β. If we inte-
grate here from (1, 1) to (2, 2), going parallel to each axis in turn, then the path via
(2, 1) yields

δ f = α ln 2 + 2β, while the path via (1, 2) yields

δ f = β + α ln 2,
whence

δ f ̸= 0.
In three dimensions, this necessary and sufﬁcient constraint for a complete dif-
ferential can also be expressed by
∇× a = 0 .
In mechanics, therefore, a potential can only be introduced for curl-free forces (see
p. 56).
Note that, always in two dimensions, and in special cases in higher dimensions,
an incomplete differential can be made into a complete differential by multiplying
by a suitable function (the integrating factor, also called Euler’s integrating factor),
which then becomes a state variable. The integrating factor for Qrev is T −1.
Changes of state are named after the conserved variable:
dS = 0
isentropic ,
dV = 0
isochoric ,
dT = 0
isothermal ,
dp = 0
isobaric .
For reversible processes, isotropic means the same as adiabatic, i.e., without heat
exchange. With the ideal Carnot process, the states change either isotropically or
isothermally, so in the (S, T ) diagram, it is easier to represent than in the (V, p)
diagram.
6.4.4
Thermodynamical Potentials and Legendre
Transformations
For the internal energy U, on p. 561, we derived the differential form
dU = T dS −p dV + μ dN
for reversible processes. Consequently, the state variables S, V , and N, the so-called
natural variables, are particularly well suited as independent variables for the internal
energy. We can in particular obtain the associated intensive quantities T , p, and μ
from the internal energy U by differentiation:
∂U
∂S

V,N = T ,
∂U
∂V

S,N = −p ,
∂U
∂N

S,V = μ .

6.4 General Theorems of Thermodynamics
567
Likewise, the potential energy Epot may be differentiated with respect to the general-
ized coordinates xk, which delivers generalized forces ∂Epot/∂xk = −Fk. Therefore,
the internal energy U is one of the thermodynamic potentials.
As already mentioned for the vaporization heat on p. 563, it is often appropriate
to replace the extensive variables S, V , or N by their associated intensive parameters
T , p, or μ, respectively, if, e.g., the temperature and pressure are kept ﬁxed, but not
the entropy and volume.
We have already encountered such transformations of variables in mechanics,
where we replaced the Lagrange function L(t, x, ˙x) by the Hamilton function
H(t, x, p) by p = ∂L/∂˙x. This is made possible using a Legendre transformation:
∂A
∂B = C ,
or dA = C dB ,
=⇒
d (BC−A) = B dC ,
or
∂(BC−A)
∂C
= B .
If we thus want to replace the variable B by C = ∂A/∂B, then we take BC −A
instead of A. So, when H = ˙x p −L was chosen, we obtained ∂H/∂p = ˙x.
We now introduce the following thermodynamic potentials:
U
internal energy ,
H ≡U + pV
enthalpy ,
F ≡U −T S
(Helmholtz) free energy ,
G ≡H −T S = F + pV
free enthalpy (Gibbs free energy) ,
to obtain new natural variables with their differentials:
dU = +T dS −p dV + μ dN ,
dH = +T dS + V dp + μ dN ,
dF = −S dT −p dV + μ dN ,
dG = −S dT + V dp + μ dN .
Clearly, we could also introduce four further grand canonical potentials U −μN,
H −μN, F −μN ≡J, and G −μN. Of these, we shall also need
dJ = −S dT −p dV −N dμ ,
from Sect. 6.5.2 onward. However, we often consider systems with a given particle
number. Then we have dN = 0, the four equations are simpliﬁed (the chemical
potential no longer plays a role), and the grand canonical potential becomes obsolete.
If, on the other hand, further variables are important, then additional terms appear,
e.g., with electric or magnetic ﬁelds.
The expression thermodynamic potential is, however, only justiﬁed if it is taken as
a function of its natural variables, thus, e.g., U(S, V, N). Otherwise, simple partial
derivatives do not result. Then according to p. 43 and this section,

568
6
Thermodynamics and Statistics
∂U
∂V

T,N =
∂U
∂V

S,N +
∂U
∂S

V,N
 ∂S
∂V

T,N = −p + T
 ∂S
∂V

T,N ,
and the last mentioned derivative has still to be determined. We shall return to this
in the next section.
From the Legendre transformation equations above with (∂C/∂B) (∂B/∂C) = 1
for C = ∂A/∂B, it is clear that ∂2 A/∂B2 · ∂2(BC−A)/∂C2 = 1. Taking the ﬁrst
equation, e.g., with A = U, B = V , and C = −p for ﬁxed S, this delivers
−1 =
∂2H
∂p2

S
∂2U
∂V 2

S =
∂2F
∂T 2

V
∂2U
∂S2

V
=
∂2G
∂p2

T
∂2F
∂V 2

T =
∂2G
∂T 2

p
∂2H
∂S2

p ,
each for ﬁxed particle number N. Here we have written ﬁrst the negative and then
the positive factor, and we shall encounter such sign rules in the next section.
6.4.5
Maxwell’s Integrability Conditions and Thermal
Coefﬁcients
The thermodynamic potentials are state variables, and therefore integrability condi-
tions are valid: their mixed derivatives do not depend upon the sequence of differ-
entiations (except for phase transitions). We shall use this now and always keep the
particle number ﬁxed. Then, with f (x, y) instead of ∂2 f/∂x ∂y = ∂2 f/∂y ∂x, we
write more precisely
 ∂
∂x

y
∂f
∂y

x =
 ∂
∂y

x
∂f
∂x

y .
These imply four integrability conditions, depending on which pair of S, T , V , and
p is taken as the natural variables:
dU = + T dS −p dV
−
∂p
∂S

V = +
 ∂T
∂V

S ,
dH = + T dS + V dp
+
∂V
∂S

p = +
∂T
∂p

S ,
dF = −S dT −p dV
−
 ∂p
∂T

V = −
 ∂S
∂V

T ,
dG = −S dT + V dp
+
∂V
∂T

p = −
∂S
∂p

T .
Here derivatives of p and V with respect to S and T are related to the “inverse
derivatives” of S and T with respect to p and V . Here the partner is always kept ﬁxed:

6.4 General Theorems of Thermodynamics
569
p and V form one pair, S and T the other. For the derivative ∂p/∂S = (∂S/∂p)−1,
there occurs a minus sign. For all four derivative pairs, we shall now introduce
abbreviations.
The derivative (∂p/∂T )V is the pressure coefﬁcient. It is denoted by β, but note
that β is often used for (kT )−1. It is related to p by the thermal stress coefﬁcient
αp = β/p, and related to the volume derivative (∂V/∂T )p by the thermal expansion
coefﬁcient α:
α ≡1
V
∂V
∂T

p
= −1
V
∂S
∂p

T
expansion coefﬁcient ,
β ≡
 ∂p
∂T

V
=
 ∂S
∂V

T
pressure coefﬁcient .
The derivative (∂T/∂V )S in the ﬁrst pair −(∂p/∂S)V = (∂T/∂V )S, now referring
to p. 43, can be traced back to
 ∂T
∂V

S = −
∂T
∂S

V
 ∂S
∂V

T = −β
 ∂S
∂T

V ,
and the second in a corresponding manner to
∂T
∂p

S = −
∂T
∂S

p
∂S
∂p

T = α V
 ∂S
∂T

p .
Here the derivatives ∂S/∂T are related to the heat capacities. We avoid the notion
of speciﬁc heat (heat capacity/mass), because in the next section we divide by the
particle number N instead of the mass, which is theoretically more convenient:
C p ≡T
 ∂S
∂T

p =
∂H
∂T

p
isobaric heat capacity ,
CV ≡T
 ∂S
∂T

V =
∂U
∂T

V
isochoric heat capacity .
Besides these, we also introduce the compressibilities:
κT ≡
−1
V
∂V
∂p

T
isothermal compressibility ,
κS ≡
−1
V
∂V
∂p

S
adiabatic (isentropic) compressibility .
The signs for the heat capacities and compressibilities were chosen such that none
of the four coefﬁcients is negative. According to p. 559, we have in particular
(∂U/∂T )V > 0 with (U)2 > 0, and according to p. 560, (∂V/∂p)T < 0 with
(V )2 > 0, whence CV ≥0 and κT ≥0. In addition, we shall soon see that C p ≥CV
and κS = (CV /C p) κT .

570
6
Thermodynamics and Statistics
The expansion coefﬁcient α and the pressure coefﬁcient β are mostly positive,
but they can both be negative (e.g., in water at the freezing temperature). However,
at least their product is always positive.
The adiabatic compressibility can be determined from the sound velocity c and
the mass density ρ. In the case of sound, there is a force density −∇p, and therefore
the impulse density has the modulus dp/c. It is equal to the momentum density c dρ.
Consequently, c2 = dp/dρ holds. Here the entropy is conserved, because there is no
time for heat exchange. With ρ (∂p/∂ρ)S = V −1 (∂p/∂V −1)S = −V (∂p/∂V )S =
κS−1, we see that κS, ρ, and c2 are actually connected:
κS =
1
ρ c2 .
The thermal coefﬁcients for ﬁxed intensive quantities are thus rather easy to mea-
sure,includingtheexpansioncoefﬁcientα andtheheatcapacityC p forﬁxedpressure,
as well as the isothermal compressibility κT . However, the pressure coefﬁcient β and
the heat capacity CV for ﬁxed volume are not. Therefore, the following three relations
are helpful:
• Firstly the equation
β = α
κT
.
For its proof in (∂p/∂T )V , we need only swap the ﬁxed and the altered variable,
according to p. 43.
• Secondly, the equation
C p
CV
= κT
κS
.
The left-hand side is equal to (∂S/∂T )p (∂T/∂S)V , and, according to p. 44, we
may swap the pair (S, T ) with the pair (p, V ) to obtain the right-hand side.
• The third equation
C p −CV = T V αβ
follows immediately (as a product T · β · V α), according to p. 43, from
 ∂S
∂T

p −
 ∂S
∂T

V =
 ∂S
∂V

T
∂V
∂T

p .
With αβ = α2/κT ≥0, we see that α and β have equal sign. Independently of this
sign, we clearly have C p ≥CV and κT ≥κS. Ten derivatives of the potentials can be
traced back to expansion and pressure coefﬁcients in addition to T , S, p, and V (the
remaining thermal coefﬁcients also occur in other derivatives):

6.4 General Theorems of Thermodynamics
571
∂U
∂V

T =
−p + βT =
β
∂U
∂S

T = −β
αV
∂U
∂p

T ,
∂H
∂p

T =
(1 −αT ) V =
−αV
∂H
∂S

T = −αV
β
∂H
∂V

T ,
∂F
∂T

p =
−S −αpV =
αV
∂F
∂V

p ,
∂G
∂T

V =
−S + βV =
β
∂G
∂p

V .
The ﬁrst of these equations was already discussed on p. 567. The remaining ones
follow in a similar way (Problem 6.34).
6.4.6
Homogeneous Systems and the Gibbs–Duhem Relation
How do the different quantities depend on the number of particles N? To answer
this question we restrict ourselves now to particles of one sort and always assume
homogeneous systems: all adjustable parameters have the same value everywhere,
such that everything is in local equilibrium.
As mentioned on p. 552, state variables are said to be extensive if they are propor-
tional to the number of particles, e.g., S, V , and the thermodynamic potentials U, H,
F, and G. In contrast, in equilibrium, intensive state variables have the same value
everywhere, e.g., T , p, and μ are intensive state variables. Except for the tempera-
ture, all extensive quantities will be denoted with upper case letters and all intensive
ones with lower case letters.
Of course, we can also divide the extensive quantities by the particle number and
then arrive at intensive quantities. We denote them by the corresponding lower case
letters—the only exception is the temperature—and then we have no other extensive
quantities than N:
v = V
N ,
s = S
N ,
u = U
N ,
h = H
N ,
f = F
N ,
g = G
N .
This separation is particularly convenient, if in addition to N only the intensive
quantities T and p occur as independent variables, hence the natural variables of the
free enthalpy G.
If the weight of a particle (molecule) or the molecular weight Mr is known,
then a scale sufﬁces for the determination of the particle number N = M/(Mru)
of a macroscopic probe, where u =
1
12 of the mass of 12C is the atomic mass unit
(atomic mass constant) (see Table A.3). Therefore, “speciﬁc” quantities, i.e., divided
by the mass, are normally preferred, e.g., the speciﬁc heats rather than the heat
capacities/particle. (But note that the speciﬁc weight gives the ratio M/V.)
It is common to refer to a special particle number, namely the Loschmidt number
NL. It corresponds to a mole, i.e., Mr gram of the substance. Note that the Avogadro

572
6
Thermodynamics and Statistics
constant NA differs only by dimension: NA = NL/mole. (This constant was intro-
duced by Avogadro in 1811, but the value of this number was ﬁrst determined by
Loschmidt in 1865.) Then, for example, on p. 563, the melting heat was given in
kJ/mole. It is necessary for NL molecules. The product of NA and the Boltzmann
constant k is called the gas constant and denoted by
R ≡NA k .
Quantities referring to one mole are common in physical chemistry and are called
molar quantities. To obtain these, we multiply the quantities valid for a single
molecule by the Avogadro constant NA.
The chemical potential μ is the adjustable parameter corresponding to the particle
number N. According to p. 567, it is obtained from any of the four thermodynamic
potentials by differentiation with respect to N, if the other natural variables are
kept ﬁxed. The free enthalpy is particularly suitable, because it depends otherwise
only on intensive quantities: μ = (∂G/∂N)T p . Hence, for homogeneous systems
in equilibrium, G = N g(T, p) clearly implies μ = g(T, p), and thus the famous
Gibbs–Duhem relation
G = μ N ,
which will prove to be extremely useful. For homogeneous systems, with
G = H −T S = F + pV = U −T S + pV ,
it yields
H = T S + μ N ,
F = −p V + μ N ,
U = T S −p V + μ N .
For homogeneous mixtures of different sorts of particles, μN is to be replaced by

i μi Ni, as shown on p. 587.
Note that the chemical potential always decreases with increasing temperature,
because dF = −S dT −p dV + μ dN implies the integrability condition
(∂μ/∂T )V,N = −(∂S/∂N)T,V = −s (T, V ) ,
and the fact that the entropy is never negative.
6.4.7
Phase Transitions and the Clausius–Clapeyron
Equation
We shall now investigate the equilibrium condition for the exchange of particles,
energy, or volume, in particular the phase equilibrium. As is well known, the same
molecules may exist in different phases (aggregation states): solid, liquid, gaseous,

6.4 General Theorems of Thermodynamics
573
Fig. 6.17 For ﬁrst order phase transitions, the ﬁrst derivative of the free enthalpy G(T, p) makes
a jump, here indicated by the dashed red line. The structure indicated by the dotted blue lines
would have higher G than the stable phase (continuous green lines). Here ∂G/∂T = −S < 0 and
∂G/∂p = V > 0 always hold
etc. In Sect. 6.3.8 we derived the constraints T+ = T−, p+ = p−, and μ+ = μ−.
According to the Gibbs–Duhem relation, we thus also have
g+(T, p) = g−(T, p) .
This equation deﬁnes a coexistence curve p (T ) in the (T, p) plane, where the two
phases are in equilibrium (see Fig. 6.17). Away from this curve, there is only the one
or the other phase, namely the one with the lower free enthalpy, as will be shown
in Sect. 6.4.9. Three phases may exist in simultaneous equilibrium only at the triple
point Ttr, ptr. This is the meeting point of the three branches corresponding to the
phase equilibria for melting, vaporization, and sublimation, or those of other phase
transitions.
For the coexistence curve p (T ), the differential equation of Clausius and Clapey-
ron holds. Along this curve, we have dg+ = dg−. Hence dg = −s dT + v dp leads to
−s+ dT + v+ dp = −s−dT + v−dp ,
and this in turn implies the Clausius–Clapeyron equation:
dp
dT = s+ −s−
v+ −v−
.
The entropy change S+ −S−times the transition temperature T is equal to the
transition heat for the phase change: melting, vaporization, or sublimation heat (see
p. 563). For these heats, we are dealing with transition enthalpies, since we then have
to care for p = 0 and have therefore T S = H:
dp
dT = 1
T
H
V .
We usually have dp/dT > 0, but there are nevertheless also counter-examples,
for instance, for the transition ice →water with H = 6.007 kJ/mol and V =
−0.0900 cm3/g.

574
6
Thermodynamics and Statistics
The different substances in a mixture do not usually transform at the same tem-
perature. If we have, for example, two metals mixed in a melt and then cool it down,
without altering the pressure, then often only one of the metals will freeze, or at least
with a mixing ratio different from the one given for the melt. The mixing ratio of the
melt also changes, and along with it its transition temperature. On further cooling,
the two metals do not necessarily segregate. The lowest melting temperature may
occur for a certain mixing ratio of the two metals, hence higher for neighboring mix-
ing processes. This special mixture is called eutecticum: it freezes (at the eutectic
temperature) like a pure metal, while for other compositions, inhomogeneities are
formed in the alloy.
The mixing entropy is important for such mixtures, where we are concerned, for
example, by things like the lowering of the freezing point and raising of the boiling
point of water by addition of salts. This will be discussed in Sect. 6.5.5, because only
there will we be able to determine the temperature change.
6.4.8
Enthalpy and Free Energy as State Variables
The last two sections have shown the utility of the notion of free enthalpy G for
homogeneous systems and for phase transitions. In particular, it is conserved for
isobaric–isothermal processes, just as the internal energy is for isochoric–isentropic
processes. In contrast, for phase transitions with volume changes, and ﬁxed pressure,
the enthalpy H (not the free enthalpy) is important for the transition heat, in addition
to the internal energy and also the (mechanical) work p dV .
The enthalpy is also important for the isentropic ﬂow
of frictionless liquids
through tube narrowings and widenings: here neither work nor heat is exchanged
through the wall of the tube, but pressure and temperature vary with the tube cross-
section. The idea is to follow a mass element M in a stationary ﬂow, and in addition
to its internal energy U, to account also for its collective kinetic energy 1
2 Mv2,
work pV , and potential energy Mgh in the gravitational ﬁeld of the Earth. Only the
sum of the enthalpy H = U + pV and the center-of-mass energy 1
2 Mv2 + Mgh is
conserved along the path. Here the pressure changes with the tube cross-section, as
is easy to see for incompressible liquids because the continuity equation requires
∇· v = 0. The smaller the tube cross-section, the higher the collective velocity v
parallel to the wall, and the lower the pressure on the wall. The Bernoulli equation
(Daniel Bernoulli, 1738) can be applied here. According to this, 1
2ρv2 + p + ρgh
is conserved along the path, where the pressure dependence of the internal energy
(for ﬁxed volume) is neglected compared to the other contributions, along with the
friction (viscosity).
The enthalpy is conserved in the throttling experiment of Joule and Thomson.
Here a suitable penetrable obstacle (“a piece of cotton wool”) ensures a pressure
difference between the high and low pressure regions, and here again there is no
heat exchange with the environment. The kinetic energy of the center-of-mass is
negligible (v = 0), and therefore the enthalpy is conserved.

6.4 General Theorems of Thermodynamics
575
For real gases in the throttling experiment, the temperature changes (Joule–
Thomson effect). According to p. 43, we have
∂T
∂p

H = −
 ∂T
∂H

p
∂H
∂p

T .
Then, according to Sect. 6.4.5 with dH = T dS + V dp, we have
 ∂T
∂H

p = 1
C p
and
∂H
∂p

T = (1 −α T ) V .
Note that C p and V are extensive quantities and for the Joule–Thomson coefﬁcients
only their ratio is important. Ideal gases have αT = 1 (as shown on p. 582). Hence the
throttle experiment with ideal gases proceeds along an isotherm. But for real gases,
αT may be larger or indeed smaller than 1. (For low temperatures the attractive
forces between the molecules are the stronger ones, so cooling by decompression
is possible, while at high temperatures the repulsive forces are the stronger ones,
so the gas heats up under decompression. However, under normal conditions, only
hydrogen and the noble gases have αT < 1.) In the (T, p) plane the two regions are
separated by the inversion curve. We shall also investigate all this more precisely for
a van der Waals gas (Sect. 6.6.2).
It is not the enthalpy, but the free energy F that is important for isothermal,
reversible processes, e.g., if the system is coupled to a heat bath. With dT = 0, we
have dF = −p dV . Thus the free energy F changes here by performing work. The
free energy is the part of the internal energy which, for an isothermal, reversible
process, can be extracted, while the rest U −F = T S is the energy bound in the
irregular motion. In contrast, for an adiabatic isolated system, dS = 0 holds, and
thus −p dV = dU.
A very important example is the energy density of electromagnetic ﬁelds. Accord-
ing to electrostatics, a potential energy 1
2

dV ρ  = 1
2

dV E · D is associated
with a charge density ρ and a potential  (see Sect. 3.1.8), while the magnetic ﬁeld
is associated with the energy 1
2

dV j · A = 1
2

dV H · B (see Sect. 3.3.5). Here it is
assumed that temperature and volume remain unchanged by (quasi-statically) bring-
ing the charges and currents from inﬁnity to their respective positions—only after-
wards can the charge and current density change. Therefore, with 1
2 (E · D + H · B),
we have identiﬁed the density of the free energy.
We can also arrive at the free energy if we derive the state variables from the
canonical partition function ZC. Sections 6.3.6 and 6.3.8 give in particular S =
k (ln ZC + λEU), with λE = (kT )−1, and thus −kT ln ZC = U −T S = F:
F = −kT ln ZC ,
or
ZC = exp −F
kT .
To compute this, T , V , and N are normally given. The conjugate variables follow
using dF = −S dT −p dV + μ dN:

576
6
Thermodynamics and Statistics
S = −
∂F
∂T

V,N ,
p = −
∂F
∂V

T,N ,
μ = +
 ∂F
∂N

T,V .
The other thermodynamic potentials then result from
U = F + T S ,
G = F + pV ,
H = U + pV ,
but the internal energy U, according to pp. 554 and 559, thus comes directly from
U = −∂ln ZC
∂λE
= kT 2 ∂ln ZC
∂T

V,N .
We can thus derive the thermal equation of state for p, V and T , and likewise the
canonical equation of state for U, F, H and G, from the canonical partition function.
6.4.9
Irreversible Alterations
In this section, we have considered only reversible changes of state, even though at
the beginning, in Sects. 6.4.1 and 6.4.2, we also allowed for irreversible ones. If we
ﬁx dt > 0 as there, then we generally have
dU ≤+T dS −p dV + μ dN ,
dH ≤+T dS + V dp + μ dN ,
dF ≤−S dT −p dV + μ dN ,
dG ≤−S dT + V dp + μ dN .
The ﬁrst inequality was already proven in Sect. 6.4.1. The second follows from there
with H = U + p V , the third with F = U −T S, and the fourth from the third with
G = F + p V .
The last two inequalities are particularly important, because it is not the entropy
changes dS that are of interest, but the temperature differences dT . If we keep, e.g.,
T , p, and N ﬁxed for an irreversible process, then the free enthalpy nevertheless
decreases, i.e., dG < 0, because the system was not yet in equilibrium. Stable equi-
librium states are the minima of the thermodynamic potentials. This means the free
energy for ﬁxed T , V , and N, and the free enthalpy for ﬁxed T , p, and N. Of course,
in each case, the entropy is also then as large as possible. We have already made
use of this for the phase transition (Sect. 6.4.7): only the phase with the smaller free
enthalpy is stable for given T and p.
6.4.10
Summary: General Theorems of Thermodynamics
We have derived relations between the macroscopic state variables T , S, p, V , μ, N,
U, H, F, and G, including equations for equilibrium states and reversible processes

6.4 General Theorems of Thermodynamics
577
and inequalities for non-equilibrium states and irreversible processes. This all follows
from the main theorems of thermodynamics, which can be justiﬁed microscopically
or required axiomatically, but which in either case must be tested by experience.
Basic for the ﬁrst and second main theorems is the relation
dU ≤T dS −p dV + μ dN ,
for dt > 0 ,
where U has the natural variables S, V , and N. This implies, for example, T =
(∂U/∂S)V,N and p = −(∂U/∂V )S,N as well as Maxwell’s integrability condition
(∂T/∂V )S,N = −(∂p/∂S)V,N. Other thermodynamic potentials like F = U −T S,
H = U + pV , and G = H −T S follow from Legendre transformations (with other
natural variables) and deliver further similar constraints.
6.5
Results for the Single-Particle Model
6.5.1
Identical Particles and Symmetry Conditions
In the last section, we presented macroscopic thermodynamics and derived general
relations between observable quantities. Now we want to restrict ourselves to equi-
librium states and special cases with known partition functions. Then according to
p. 576, we may derive all thermal and canonical equations of states.
Identical particles without correlations are particularly simple. Then the same one-
particle potential acts on all particles, and the probability distribution of the many-
particle problem splits into a product of one-particle distributions. These depend on
the one-particle states or on the cells in phase space of each individual particle (μ-
space). We order them with respect to their energy ei, and degenerate ones in some
arbitrary way.
Now it is suggestive to assign to every particle its state, and thus ﬁx the many-body
state. This leads to Maxwell–Boltzmann statistics, although it contains an internal
contradiction. In particular, we have assumed the ability to distinguish between the
individual particles, otherwise we cannot decide how a given particle behaves in the
course of time. Then distinguishing features are necessary, and therefore the particles
cannot be completely identical.
This contradiction does not occur in quantum theory, because there we have to
account for the exchange symmetry. Consider two particles in the states |α⟩and |β⟩.
For bosons, only the symmetric state
|α, β⟩s = +|β, α⟩s
∝
|α⟩|β⟩+ |β⟩|α⟩
is permitted, and for fermions, only the antisymmetric state
|α, β⟩a = −|β, α⟩a
∝
|α⟩|β⟩−|β⟩|α⟩.

578
6
Thermodynamics and Statistics
In both cases, the ﬁrst particle occurs with the same probability in the state |α⟩as in
the state |β⟩, and the second, of course, likewise.
Two bosons may occupy the same one-particle state, but not fermions, because
this contradicts the antisymmetry (Pauli principle). If ni is the occupation number of
the ith one-particle state, then we have the occupation-number representation (see
Sect. 5.3.5):
bosons
|z⟩s = |n1, n2, . . .⟩s
with ni ∈{0, 1, . . .} ,
fermions
|z⟩a = |n1, n2, . . .⟩a
with ni ∈{0, 1} .
Correspondingly, for bosons, we have Bose–Einstein statistics, and for fermions,
Fermi–Dirac statistics.
In the classical Maxwell–Boltzmann statistics, several particles may occupy the
same one-particle state. However, there the many-body state does not have to be
symmetric under particle exchange. There are classically more states (by the factor
N!/n1! . . .) than in Bose–Einstein-statistics, because classically each permutation
counts as a new state. If all states are occupied just a little bit (all ni = 0 or 1), then
according to Stirling’s formula, this produces an additional term k ln N! ≈Nk ln N
in the entropy S = k ln ZMC. This addition does not increase in proportion to N,
even though it has to be an extensive variable. This contradiction, occasionally called
Gibbs’ paradox, can only be removed by replacing Z →Z/N! in classical statistics.
This leads to the corrected Boltzmann statistics.
6.5.2
Partition Functions in Quantum Statistics
This is best evaluated for the grand canonical ensemble, for which the energy and
particlenumberaregivenonlyonaverage.Forasharpparticlenumber,thecalculation
is rather involved (see the textbook by Reif in the reading list on p. 620), and soluble
only with an approximation, which is in effect the transition from the canonical to
the grand canonical ensemble. Note that the volume should also be given, because
the one-particle energies depend on it.
If the ith one-particle state contains ni particles of energy ei, then according to
the single-particle model, we have
N =

i
ni
and
E =

i
ni ei ,
with ni ∈{0, 1, 2, . . .} for bosons and ni = 0 or 1 for fermions. Note that N and
E do not stand for the mean values here. For the grand canonical partition function
ZGC = tr[exp{−(E −μN)/kT }], with z = {n1, n2, . . .}, we obtain
ZGC =

{n1, n2, ...}
exp −
i ni (ei −μ)
kT
.

6.5 Results for the Single-Particle Model
579
The exponential function of a sum is equal to the product of the exponential functions:
ZGC =

{n1, n2, ...}

i
exp −ni (ei −μ)
kT
.
In each term, the ﬁrst the factor is exp{−n1(e1 −μ)/kT }, then we have the factor
with i = 2, and then the remaining ones, whence we may write:
ZGC =

i

ni
exp −ni (ei −μ)
kT
.
For example, with a = exp{−(e1 −μ)/kT } and b = exp{−(e2 −μ)/kT }, we have
initially ZGC = a0b0 + a0b1 + · · · + a1b0 + a1b1 + · · · + · · · , but this sum of prod-
ucts may be written as product of simple sums ZGC = (a0 + a1 + · · · )(b0 + b1 +
· · · ).
For bosons, we thus obtain the geometric series of {1 −exp(−(ei −μ)/kT )}−1,
where the chemical potential μ keeps the average particle number ﬁnite, and thus
the geometric series converges. For fermions, on the other hand, we arrive at the sum
1 + exp(−(ei −μ)/kT ). Therefore, the result may be reformulated as
ZGC =

i

1 ∓exp −(ei −μ)
kT
∓1
,
or again,
ln ZGC = ∓

i
ln

1 ∓exp −(ei −μ)
kT

,
where the upper sign holds for bosons and the lower one for fermions. We will also
keep to this notation in the following.
According to p. 556, the natural variables of the grand canonical partition func-
tion are λE, λN, and V , or according to Sect. 6.3.8, T , μ, and V . Here, according
to Sect. 6.3.6, the entropy S is given by k ln ZGC + (U −μN)/T . Consequently,
−kT ln ZGC = F −μN holds, and by the discussion on p. 567, this is the grand
canonical potential J:
J ≡−kT ln ZGC = F −μN = G −pV −μN ,
with
dJ = −S dT −p dV −N dμ .
Using ZGC(T, V, μ), the quantities S, p, and N may be derived immediately, and
then also the other potentials U, H, F, and G may be determined. According to the
Gibbs–Duhem relation, homogeneous systems have G = μN and thus J = −pV .

580
6
Thermodynamics and Statistics
6.5.3
Occupation of One-Particle States
So far we have viewed the grand canonical partition function as a function of T , V ,
and μ, but in the single-particle model, the energies {ei} replace the volume. These
depend not only on V , but also on the average one-particle potential. Therefore, from
N = −
∂J
∂μ

T,{ei} = kT
∂ln ZGC
∂μ

T,{ei} = −kT

i
∂ln ZGC
∂ei

T,{ek̸=i},μ
=

i
⟨ni⟩,
we deduce the average occupation number of the ith one-particle state as
⟨ni⟩=
 ∂J
∂ei

T,{ek̸=i},μ =

exp ei −μ
kT
∓1
−1
.
One-particle states of high energy (ei ≫μ + kT ) are thus barely occupied. In addi-
tion, as required by the Pauli principle,
0 ≤⟨ni⟩≤1 ,
for fermions ,
while for bosons ⟨ni⟩may be greater than 1. But for the latter, due to the constraint
N ≥⟨ni⟩≥0, the chemical potential μ is restricted to μ < min ei, and so is never
positive for e0 = 0. In a grand canonical ensemble and for ei < e j, for both sorts of
particles, we have ⟨ni⟩> ⟨n j⟩.
Since exp{(ei −μ)/kT } = ⟨ni⟩−1 ± 1 and with the average occupation numbers
⟨ni⟩, the partition function ZGC is given by
ln ZGC = ∓

i
ln

1 ∓
1
⟨ni⟩−1 ± 1

= ∓

i
ln
⟨ni⟩−1
⟨ni⟩−1 ± 1 = ±

i
ln (1 ± ⟨ni⟩) .
Using this for the ith one-particle state, we may also give the probability for its
occupation by n particles. Here we write n instead of ni. The partition function is
clearly equal to (1 ± ⟨n⟩)±1 and from ρ = Z−1 exp{−(E −μN)/kT } (see p. 555
and Sect. 6.3.8), it follows that
ρn = exp{−n (ei −μ)/kT }
(1 ± ⟨n⟩)±1
=
⟨n⟩n
(1 ± ⟨n⟩)n±1 .
For bosons, since ρn+1/ρn = ⟨n⟩/(1 + ⟨n⟩) < 1, the state without particles always
has the highest probability, and that with n ≈⟨n⟩is not special at all. The situation
is quite different for fermions: for them, 0 ≤⟨n⟩≤1 and in addition ρ0 = 1 −⟨n⟩
and ρ1 = ⟨n⟩.

6.5 Results for the Single-Particle Model
581
The relation U −μN = 
i⟨ni⟩(ei −μ) = kT 
i⟨ni⟩ln (⟨ni⟩−1 ± 1) implies
S = k ln ZGC + (U −μN)/T ,
whence
S = ±k

i

ln (1 ± ⟨ni⟩) ± ⟨ni⟩ln 1 ± ⟨ni⟩
⟨ni⟩

= −k

i

⟨ni⟩ln ⟨ni⟩∓(1 ± ⟨ni⟩) ln (1 ± ⟨ni⟩)

.
Since x ln x = 0 for x = 0 and x = 1, the unoccupied states do not contribute to
the entropy, and likewise for fermion states with ⟨ni⟩= 1. This can also be justiﬁed
by considering the uncertainty of the occupation number because, for the squared
ﬂuctuation of the particle number in the ith one-particle state, using λN = −μ/kT
and
(ni)2 = −∂⟨ni⟩
∂λN
= kT ∂⟨ni⟩
∂μ
=
exp{(ei −μ)/kT }
[exp{(ei −μ)/kT } ∓1]2 ,
we obtain the noteworthy result (see Fig. 6.18)
(ni)2 = ⟨ni⟩

1 ± ⟨ni⟩

.
This vanishes when ⟨ni⟩= 0 and also for fermions when ⟨ni⟩= 1, while for bosons,
when ⟨ni⟩≫1, the error width is ni ≈⟨ni⟩, not √⟨ni⟩, as would be expected
classically. Note also that, for fermions with ⟨ni⟩= 1
2, the error width is 1
2.
With decreasing temperature the states of higher energy become ever more depop-
ulated. In the limit T ≈0, fermions only occupy one-particle states with ei ≤μ,
while the states above stay empty. Then we have a degenerate Fermi gas with
Fig. 6.18 Occupation
number of the one-particle
states as a function of
(e−μ)/kT for bosons (red
curve) and fermions (blue
curve). We also show
⟨n⟩±n (dashed curves) for
bosons and for fermions.
With (n)2 = −∂⟨n⟩/∂λN
and λN = −μ/kT, the
uncertainty is greater, the
more rapidly ⟨n(x)⟩
decreases. Note that the base
line here appears shifted to
negative values!

582
6
Thermodynamics and Statistics
μ(T = 0) as the Fermi energy eF. We shall return to this in Sect. 6.5.6. With decreas-
ing temperature, bosons crowd into the one-particle state of lowest energy e0. Their
chemical potential for T ≈0 is thus determined by the constraint ⟨N⟩≈⟨n0⟩, which
yields μ ≈e0 −kT ln (1 + ⟨N⟩−1) ≈e0 −kT/N. More on that in Sect. 6.6.6.
6.5.4
Ideal Gases
For high temperatures, a great many states are occupied with nearly equal probability.
For ⟨N⟩to remain ﬁnite, we must then have exp{(ei −μ)/kT } ≫1 for all i, and
hence −μ ≫kT . But then Bose–Einstein and Fermi–Dirac statistics no longer differ
because the exchange symmetry is no longer respected if all one-particle states are
barely occupied. According to the above remarks, we then have
−J
kT = ln ZGC ≈

i
exp

−ei −μ
kT

≈

i
⟨ni⟩= ⟨N⟩.
If we make use here of the Gibbs–Duhem relation for homogeneous systems, thus
J = −pV , we obtain the Gay-Lussac law, which is just the thermal equation of state
for ideal gases, viz.,
pV = NkT .
Then using the results α ≡V −1(∂V/∂T )p,N, κT ≡−V −1(∂V/∂p)T,N, β = α/κT ,
and C p −CV = αβT V , we obtain
α = 1
T ,
κT = 1
p ,
β = p
T = Nk
V
,
C p −CV = Nk .
Hence for ideal gases with (∂U/∂V )T = −p + βT and (∂H/∂p)T = (1 −αT ) V
(see p. 570), both (∂U/∂V )T and (∂H/∂p)T are zero. For (reversible) isothermal
processes in ideal gases, when the volume changes, the internal energy is conserved,
and when the pressure changes, the enthalpy is conserved. Consequently, for ideal
gases, there is no Joule–Thomson effect, something we commented on already on
p. 575.
Clearly, the canonical partition function for a particle may be extracted from
the above-mentioned equation N ≈
i exp{−(ei −μ)/kT }, and we denote this by
ZC(1), whence ZC(1)/N is an intensive variable:
N = ZC(1) exp μ
kT .
The factor exp(μ/kT ) is called the fugacity, and in physical chemistry, the absolute
activity of the material. We shall soon determine ZC(1) for important examples, and
hence also μ via the Gibbs–Duhem relation G:

6.5 Results for the Single-Particle Model
583
μ = −kT ln ZC(1)
N
and
G = −NkT ln ZC(1)
N
.
Hence we obtain the free energy F = G −pV = G −NkT if we also use the Gay-
Lussac law. The internal energy
U = F + T S = F −T (∂F/∂T )V,N = −T 2(∂(F/T )/∂T )V,N
yields
U = NkT 2 ∂ln ZC(1)
∂T

V,N ,
and the enthalpy H = U + NkT . For the entropy, we obtain
S = −
∂F
∂T

V,N = Nk

ln ZC(1)
N
+ 1 + T
∂ln ZC(1)
∂T

V,N

.
Here we have required −μ ≫kT , and hence ln ZC(1)/N ≫1, but it is not necessary
thatitshouldbeverymuchgreaterthan3,ascanbeseenfromFig.6.19.Thecanonical
partition function ZC(1) is determined according to the internal degrees of freedom
of the given gas.
For the ideal monatomic gases, up to rather high temperatures (1 eV =11 600 K),
there is no internal excitation of the atoms (the electronic degrees of freedom are
frozen), so what is important for ei is only the kinetic energy pi 2/2m of their centers
of mass. Here, according to p. 525, a particle conﬁned to a cube of volume V = L3 has
the momentum eigenvalues pi = niℏπ/L, where ni may have only natural numbers
as Cartesian components, and not even negative integers. If we insert this into the
canonical partition function and replace the sum by an integral, we obtain
ZC(1) =
 d3n
8
exp −(nℏπ/L)2
2m kT
= 4π
8
 ∞
0
dn n2 exp −π2ℏ2 n2
2m kT L2 ,
and therefore, since
 ∞
0 dx x2 exp(−ax2) = 1
4
	
π/a3,
ZC(1) =

kT
4πℏ2/2m
3/2
V ≡V
λ3 ,
where the thermal de Broglie wavelength is deﬁned by
λ ≡
h
√
2πmkT
.
However, the Maxwell distribution for ⟨h/mv⟩delivers twice this value (see Prob-
lem 6.11), so the name is not quite satisfying. The result holds for high temperatures,
and not only for a cube. For V ≫λ3, other restrictions deliver the same value for
the partition function ZC(1).

584
6
Thermodynamics and Statistics
Fig. 6.19 The
single-particle model for
ideal monatomic gases yields
the equations mentioned in
the text, if T ≫T0 with
kT0 ≡4π (N/V )2/3 ℏ2/2m.
But for T ≈T0, the
exchange symmetry
contributes. The upper curve
is for bosons and the lower
curve for fermions. We
return to this in Fig. 6.22
Consequently,foridealmonatomicgases,weﬁnd(∂ln ZC(1)/∂T )V = 3
2T −1 and,
as expected by the equidistribution law (p. 559),
U = 3
2 NkT ,
H = 5
2 NkT ,
S = Nk

ln ZC(1)
N
+ 5
2

.
Hence with CV = (∂U/∂T )V,N = C p −Nk and κS = κT CV /C p, we have
CV = 3
2 Nk ,
C p = 5
2 Nk ,
and κS = 3
5 κT = 3
5 p−1 .
If we relate to a mole, then according to p. 572, we have to take the gas constant R
instead of Nk.
For ideal diatomic gases, the molecules rotate and oscillate. As long as its moment
of inertia does not change notably despite the oscillations, the canonical partition
function of a molecule may be written as the product of the canonical partition func-
tions for the the center-of-mass motion, the rotations, and the oscillations, disregard-
ing electronic degrees of freedom, which do not contribute anything (as established
above).
At room temperature, in addition to the electronic excitations, the oscillations are
also frozen. The rotations of diatomic molecules for constant moment of inertia 
have the energy j( j +1) ℏ2/2, and each level is (2 j +1)-fold degenerate due to the
isotropy. Therefore, we have
ZC rot(1) =

j
(2 j + 1) exp

−ℏ2
2
j( j + 1)
kT

.
We evaluate this sum again via an integral, and use the continuous variable
x = ( j + 1
2) ℏ/
√
2 kT .
For molecules containing two identical atoms, however, the states with odd angular
momentum do not occur, and this halves the partition function. Without this factor of

6.5 Results for the Single-Particle Model
585
1
2 (thus in the case of non-identical atoms), with
 ∞
0 dx 2x exp(−x2) = 1, we obtain
ZC rot(1) =
kT
ℏ2/2 exp ℏ2/2
4kT
≈
kT
ℏ2/2 + 1
4 ,
for kT ≫ℏ2
2 .
For sufﬁciently high temperatures, the product of the partition functions is
ZC(1) =
kT
ℏ2/2

kT
4πℏ2/2m
3/2
V ,
and thus now (∂ln ZC(1)/∂T )V = 5
2 T −1. For all diatomic molecules (of identical
or non-identical atoms) and for sufﬁciently high temperatures, it thus follows that
U = 5
2 NkT ,
H = 7
2 NkT ,
S = Nk

ln ZC(1)
N
+ 7
2

.
This result does not contradict the equidistribution law, because for a diatomic
molecule, the moment of inertia about the symmetry axis is then small compared
to the other two, so this rotation is frozen. Therefore, for the symmetric top (see
p. 145), we only have Htot = (p2
β + p2
α/ sin2 β)/2. Each of the N molecules thus
contributes to the internal energy 3
2 kT from the translational motion and also 2
2 kT
from the rotation. Note that the factor of p2
α is not ﬁxed but depends on β, but this
does not affect the equidistribution law, as was shown by its proof on p. 559. We thus
obtain
CV = 5
2 Nk ,
C p = 7
2 Nk ,
and κS = 5
7 κT ,
with κT = p−1, as for all ideal gases.
These expressions are of course only valid for ideal diatomic gases as long as the
oscillations are frozen. Otherwise, we must consider
ZK vib(1) =

n
exp

−ℏω
kT (n + 1
2)

= exp(−1
2 ℏω/kT )
1−exp(−ℏω/kT ) =
1
2 sinh(ℏω/2kT ) .
If this degree of freedom is fully thawed, i.e., kT ≫ℏω, then this results in kT/ℏω,
whence
ZC(1) = kT
ℏω
kT
ℏ2/2

kT
4πℏ2/2m
3/2
V .
Then,
U = 7
2 NkT ,
H = 9
2 NkT ,
S = Nk

ln ZC(1)
N
+ 9
2

,
and
CV = 7
2 Nk ,
C p = 9
2 Nk ,
κS = 7
9 κT .

586
6
Thermodynamics and Statistics
If the molecules consist of two identical atoms, then in fact the above-mentioned fac-
tor 1
2 changes the expression for the state sum ZC(1) by a factor of 2, which modiﬁes
μ only by μ = kT ln 2 and S by S = −Nk ln 2. Also unimportant according to
the equidistribution law is whether the molecules consist of identical or non-identical
atoms.
6.5.5
Mixing Entropy and the Law of Mass Action
Mixtures of several materials may be evaluated rather simply as long as no cor-
relations have to be accounted for. To begin with, we consider a segregated equi-
librium state, with the same temperature and pressure everywhere. Each part has
its particle number Ni corresponding to its volume Vi and entropy Si(T, p, Ni).
The total volume is V = 
i Vi, the energy U = 
i Ui, and the entropy S = 
i Si.
If we now allow for a complete mixture with ﬁxed U and V , then the entropy
increases, because the number of accessible states increases with the volume. We
restrict ourselves here to ideal gases. Then the chemical potential changes with
μ = −kT ln (ZC(1)/N) and ZC(1) ∝V by −kT ln (V/Vi) = −kT ln (N/Ni), and
the entropy Si by Nik ln (N/Ni). Consequently, the mixing entropy amounts to
SM = −k

i
Ni ln Ni
N > 0 .
The mixing is an irreversible process, because the entropy increases. Since Ni/N
is the probability ρi for the component i, we ﬁnd SM/N ≡sM = −k 
i ρi ln ρi for
the mixing entropy per particle. This ﬁts very well with the notion of information
entropy (Sect. 6.1.6).
The mixing entropy depends only on the different particle numbers, not on the
consistency. This leads to Gibbs’ paradox. According to classical conceptions the
difference between the particle types would have to vanish continuously. Even though
a mixture would then no longer be conceivable, the last equation would still be valid.
According to quantum theory, the transition is not continuous, however.
We found the Gibbs–Duhem relation G = μN for pure homogeneous systems in
Sect. 6.4.6 and now want to generalize it to systems of different materials (as long
as they do not react chemically). For homogeneous mixtures of different particles
(e.g., solutions), the equilibrium condition for given T and p is
μi =
 ∂G
∂Ni

T,p,{Nk̸=i} .
G is a homogeneous function of ﬁrst order in the particle numbers Ni, since thermo-
dynamic potentials of homogeneous systems are extensive variables. For arbitrary
x > 0, we have x G(T, p, N1, N2, . . .) = G(T, p, x N1, x N2, . . .). If we differenti-
ate this with respect to x at the position 1 and make use of Euler’s theorem for

6.5 Results for the Single-Particle Model
587
homogeneous functions, we may deduce the important generalized Gibbs–Duhem
relation
G =

i
μi Ni .
Here the mixing entropy also affects the free enthalpy, and in particular, gi = Gi/Ni
denotes the free enthalpy per particle for pure systems (Gpure = 
i Gi). Then for
mixtures (of ideal gases), we have G = Gpure −T SM, and hence,
G =

i
Ni

gi + kT ln Ni
N

.
From the comparison with the generalized Gibbs–Duhem relation, we conclude that
μi = gi + kT ln Ni
N < gi .
The mixing entropy thus lowers the chemical potential, which is now different from
the free enthalpy.
We can exemplify the above by considering the thawing of ice with salt, assuming
that the salt is dissolved only in the water, but not also in the ice. At the transition
point, both phases have to have the same chemical potential. If, in a similar way to
p. 563, we denote the solid phase by [ ] and the liquid by ( ), then at the freezing
temperature of pure water, we have g[ ](T, p) = g( )(T, p), in contrast to the freezing
temperature of salt water:
g[ ](T + T, p) = g( )(T + T, p) + k (T + T ) ln
NW
NW + NS
.
Therefore, to the ﬁrst approximation,
T
∂(g[ ] −g( ))
∂T

p + k ln

1 + NS
NW

= −kT ln

1 + NS
NW

,
where, since dG = −S dT + V dp and dH = T dS + V dp, we may use
∂(g[ ] −g( ))
∂T

p = s( ) −s[ ] = h
T
(> 0) .
The reduction in the freezing temperature is thus
T = −
kT 2 ln(1 + NS/NW)
h + kT ln(1 + NS/NW) .
For small salt concentrations and for one mole, it follows that

588
6
Thermodynamics and Statistics
T ≈−NS
NW
RT 2
H ,
where H is the melting heat of water per mole (6 kJ). Every percent of salt lowers
the freezing temperature by one degree centigrade.
If we now also allow for chemical reactions, then the equilibrium condition

i νiμi = 0onp.561initiallydeliverstheequation
i νigi = −kT 
i ln (Ni/N)νi.
Hence, we have the law of mass action, viz.,

i
 Ni
N
νi = exp −
i νigi
kT
≡K(T, p) ,
with given ﬁxed temperature and pressure. Of interest is then the difference between
the free enthalpies before and after the reaction, in contrast to the difference between
the free energies for isochoric instead of isobaric processes. The equilibrium constant
K depends on the chemical consistency of the materials, but not on the concentration
(which is of course the important aspect of the law of mass action).
The temperature dependence of the chemical reaction follows from
∂ln K
∂T

p = −

i
νi
∂(gi/kT )
∂T

p .
Hence, with (∂g/∂T )p = −s and g + T s = h, we obtain
∂ln K
∂T

p =

i νihi
kT 2
.
For constant pressure, heating thus shifts the reaction equilibrium in favor of the
enthalpy-rich side (endothermic reaction).
6.5.6
Degenerate Fermi Gas and Conduction Electrons
in Metals
For typical temperatures, the conduction electrons in metals form a degenerate Fermi
gas. According to the considerations on p. 582, their chemical potential μ for the
temperature T = 0 is equal to the Fermi energy eF = p2
F/2m. On p. 525, we deter-
mined the number of motional states whose energies ei are smaller than the Fermi
energy:
 = V
h3 · 4π
3

2m eF
3/2 =
V
6π2
2m
ℏ2 eF
3/2
.
Furthermore, two spin states are associated with each of these states, so for N elec-
trons in the volume V , we obtain the Fermi energy

6.5 Results for the Single-Particle Model
589
Fig. 6.20 Fermi
distributions for T/T0 = 1
2
(red curve), 1 (blue curve),
and 2 (green curve). Note
that, in Fig. 6.18, there is
only a single curve, because
for each temperature a
different energy unit was
taken. Here, the one-particle
ground state energy lies very
far to the left!
eF = ℏ2
2m

3π2 N
V
2/3
.
In metals, this energy is very much higher than kT (even at 1000 K) and the electron
gas is therefore degenerate (see the Fermi distribution function in Fig. 6.20).
When computing mean values for Fermi gases, we always encounter expressions
like
⟨A ⟩=

i
ai ⟨ni⟩=

i
ai

exp ei −μ
kT
+ 1
−1
,
for which we shall now give a useful computational method for low temperatures.
For high temperatures, we would have an ideal gas. If the values ai depend only
weakly on the index i and if sufﬁciently many states contribute, the sum may be
replaced by an integral:
⟨A ⟩=
 ∞
0
a(e) g(e) de
exp{(e −μ)/kT } + 1 ,
where g(e) is the density of states for a particle. Note that we have to add an argument
e in order to avoid confusions with the free enthalpy per particle. For T = 0, and
therefore μ = eF, only the integral from 0 to eF is important—the denominator there
is equal to one. However, with increasing temperature, the states for e ≈eF are
reshufﬂed (see the last ﬁgure).
For the expansion in terms of powers of T , we consider the expression
F =
 ∞
0
f (x) dx
exp{β(x −x0)} + 1 ,
with β > 0 and βx0 ≫1 ,
i.e., actually for μ ≫kT , which applies to a degenerate Fermi gas. With F(x) as
“anti-derivative” to f (x) passing through zero, thus f (x) = dF/dx and F(0) = 0,
after integration by parts, we obtain
F =
F(x)
exp{β(x −x0)} + 1

∞
0
−
 ∞
0
F(x) d
dx
1
exp{β(x −x0)} + 1 dx .

590
6
Thermodynamics and Statistics
The ﬁrst term on the right vanishes because F(0) = 0 and the denominator for
x →∞is too large, while it is clear that only the integrand near x ≈x0 contributes
to the second. Therefore, we expand F(x) in a Taylor expansion about this position
to obtain
F =
∞

n=0
1
n!
dn F
dxn

x=x0
 ∞
0
(x−x0)n d
dx
−1
exp{β(x−x0)}+1 dx .
With z = β(x−x0) and d(ez+1)−1/dz = −(ez+1)−2 ez, it follows that
 ∞
0
(x−x0)n d
dx
−1
exp{β(x−x0)}+1 dx = β−n
 ∞
−βx0
zn dz
(ez+1)(e−z+1) .
Because of the denominator, the important contributions to the integrand come only
from z ≈0, since we assumed βx0 ≫1. Therefore, the lower integration limit may
be taken as −∞. Then terms with n odd do not contribute, and for n even,
 ∞
−∞
zn dz
(ez+1)(e−z+1) = −2
 ∞
0
zn d
dz
1
ez+1 dz ,
which gives 1 for n = 0. For n > 0, we integrate by parts and use zn/(ez+1)|∞
0 = 0:
 ∞
−∞
zn dz
(ez+1)(e−z+1) = 2n
 ∞
0
zn−1 dz
ez+1
.
In the next section (on bosons), we shall arrive at nearly the same integral, except that
there, −1 occurs in the denominator instead of +1. Therefore, for n ∈{1, 2, . . .}, we
consider here the two denominators simultaneously and expand e−z/(1 ∓e−z) in a
geometric series:
 ∞
0
zn−1 dz
ez ∓1 =
∞

k=0
(±)k
 ∞
0
zn−1 e−(1+k)z dz = (n −1)!
∞

k=0
(±)k
(1+k)n .
Both sums lead to Riemann’s zeta function (see Fig. 6.21):
ζ(z) =
∞

k=0
1
(1+k)z ,
for Rez > 1 ,
because the alternating sum (for fermions) is equal to (1 −2( 1
2)n) ζ(n), given that
1+k is even for all negative terms and their sum leads to ( 1
2)nζ(n). We need ζ(2) =
π2/6 and in the next section ζ(4) = π4/90, but later also ζ(3), ζ( 3
2), and ζ( 5
2). The
two values for ζ(2) and ζ(4) result from a Fourier expansion of the meander curve
[9].

6.5 Results for the Single-Particle Model
591
Table 6.2 Riemann zeta
function for 1 ≤x ≤4. See
also Fig. 6.21
x
ζ(x)
1.0
∞
1.5
2.612375
2.0
1.644934
2.5
1.341487
3.0
1.202057
3.5
1.126734
4.0
1.082323
Fig. 6.21 Riemann zeta
function for 1 ≤x ≤4. See
also Table 6.2
We thus obtain the expression up to order n = 2 (and 3):
F = F(x0) + 1
6
π2
β2
d f
dx

x=x0 ,
or for the Fermi distribution, as the weight function2
1
exp{β(x −x0)} + 1 ≈ε(x0 −x) −π2
6β2 δ′(x −x0) + · · ·
in an integral, with the step function ε(x) mentioned on p. 18 and the derivative δ′(x)
of the Delta function.
Putting all this together, we thus have for the degenerate Fermi gas,
⟨A⟩≈A(μ) + π2
6 (kT )2 ∂
∂e

a(e)g(e)

e=μ ,
with A(μ) =
 μ
0 a(e′) g(e′) de′. Here, since dA/de = ag(e), A(μ) differs from
A(eF) = ⟨A ⟩T =0 by approximately (μ −eF) a(eF) g(eF). In order to evaluate the
chemical potential μ(T ), we consider the particle number, which does not depend on
2In nuclear physics, the radial distribution of nuclear matter is similar to a Fermi distribution [10].

592
6
Thermodynamics and Statistics
thetemperature,andhencetakea(e) = 1.Then(μ −eF) g(eF) + 1
6π2 (kT )2 g′(eF) ≈
0. If we use this in
⟨A ⟩−⟨A ⟩T =0 ≈(μ −eF) a(eF) g(eF) + 1
6π2 (kT )2 {a′(eF)g(eF) + a(eF)g′(eF)} ,
then all terms on the right cancel out except for the term 1
6π2 (kT )2 a′(eF) g(eF). The
only thing missing is the density of states g(eF). Here, (e) ∝e3/2, and the further
factor is equal to N eF−3/2, so g(eF) = 3
2 N/eF. From this, for T ≈0, we ﬁnd the
important result
⟨A ⟩≈⟨A ⟩T =0 + π2
4
N
eF
a′(eF) (kT )2 .
If we take this expression for the internal energy, then a(e) = e and thus a′ = 1. Near
the origin, the internal energy of a degenerate Fermi gas increases with the square of
the temperature. Hence,
CV ≡
∂U
∂T

V N ≈π2
2 Nk kT
eF
.
For the chemical potential μ(T ) ≈eF −1
6π2(kT )2g′(eF)/g(eF), using
g(e) ≈3
2 N/eF
	
e/eF ,
and thus g′(e)/g(e) ≈1
2e−1, we ﬁnd (see Fig. 6.22)
μ(T ) ≈eF

1 −π2
12
kT
eF
2
.
Thus, it varies as T 2 for a Fermi gas near the zero temperature, whereas it varies
linearly with T for a Bose gas because according to p. 582, we then have μ(T ) ≈
e0 −kT/N. As expected according to p. 572, the chemical potential decreases with
increasing temperature in both cases.
The “high-temperature expansion” in Fig. 6.22 relies on ZC(1) = V/λ3 (see
p. 583), but uses a more precise expression for the chemical potential, and in par-
ticular, one which differentiates between bosons and fermions. For sufﬁciently high
temperatures in
N =

i

exp ei −μ
kT
∓1
−1
,
we have μ < 0 and hence ei −μ > 0. After multiplying by exp{−(ei −μ)/kT },
each term can be expanded in a geometric series. After reordering the series, it
follows that

6.5 Results for the Single-Particle Model
593
Fig. 6.22 The chemical potential of an ideal monatomic Fermi gas as a function of temperature,
relative to the Fermi energy eF = (9π/16)1/3 kT0 with kT0 from Fig. 6.19. The continuous red curve
corresponds to the high-temperature expansion, the dashed magenta curve to the low-temperature
expansion, and the dotted blue curve to a Bose gas (see Fig. 6.29 for Bose–Einstein condensation)
Fig. 6.23 The logarithm Lix(z) = ∞
n=1
zn
nx for |z| < 1, continuous for x = 1 (green) and 2 (red),
dashed for x = 3
2 (blue) and 5
2 (black). The name stems from Li1(z) = −ln(1 −z). Then Li2(z) is
also called the dilogarithm. Furthermore, Lix(1) = ζ(x) and d Lix
dz
= Lix−1(z)
z
(also for |z| ≥1)
N =
∞

n=0
(±)n exp

(n+1) μ
kT
 
i
exp

−(n+1) ei
kT

.
We may write the last sum as V λ−3(T/(n + 1)). With λ(T ) ∝T −1/2 and the abbre-
viation σ = exp(μ/kT ) for the fugacity, we obtain an implicit equation for the
determination of the chemical potential, which contains the polylogarithm Lix(z)
(see Fig. 6.23):
N = ±
V
λ3(T )
∞

n=1
(±σ)n
n3/2
= ±
V
λ3(T ) Li3/2

± exp μ
kT

.

594
6
Thermodynamics and Statistics
6.5.7
Electromagnetic Radiation in a Cavity
An interesting and important system consists of photons in a cavity of volume V .
They may be absorbed or emitted by the walls so the particle number is not ﬁxed, not
even on average. Therefore, there is no chemical potential (μ = 0), and the canonical
ensemble sufﬁces with the free energy as thermodynamic potential:
F = −kT ln ZC = kT

i
ln

1 −exp −ei
kT

.
The second equation holds, because we are dealing with bosons. They move with
the speed of light. Therefore, we have ei = ℏωi = ℏcki with ki = ni π/L, as on
p. 525, so ωi = ni πc/L. Since there are two polarization possibilities (helicities),
the number of states follows from
2 · 4π
8 n2 dn = π
 L
π c
3
ω2 dω =
V
π2 c3 ω2 dω .
If we replace the partition function by an integral, we obtain
F
V =
kT
π2 c3
 ∞
0
ln

1 −exp −ℏω
kT

ω2 dω = kT
π2
kT
ℏc
3  ∞
0
ln (1 −e−x) x2 dx .
According to the last section, integration by parts with x3 ln (1 −e−x)|∞
0 = 0 yields
 ∞
0
ln (1 −e−x) x2 dx = −1
3
 ∞
0
x3 dx
ex −1 = −2 ζ(4) = −π4
45 .
With the Stefan–Boltzmann constant (see p. 623)
σ ≡π2
60
k4
c2ℏ3 ,
the result reads
F = −4 σ
3 c V T 4 .
For the radiation pressure p = −(∂F/∂V )T and the entropy S = −(∂F/∂T )V , this
gives
p = −F
V = 4 σ
3 c T 4
and
S = −4F
T
= 16σ
3c V T 3 .
The pressure does not depend on the volume. For the free enthalpy G = F + pV , we
obtain the value 0, as expected from the Gibbs–Duhem relation with μ = 0. Clearly,
T S = −4F = 4pV and thus

6.5 Results for the Single-Particle Model
595
U = −3F = 4 σ
c V T 4
and
p = 1
3
U
V .
For ideal gases, we also have p ∝U/V , but with the factor 2
3 for the monatomic
gas—for v ≪c the pressure is twice as large as for v ≈c. The frequency of collisions
of the molecules is proportional to their speed, and the recoil proportional to their
momentum. The product of velocity times momentum is important for the pressure.
In the relativistic regime, it is equal to the energy (see p. 245), but twice as large in
the non-relativistic regime.
If the wall has a hole of area A, then the energy per unit time that ﬂows from the
cavity is the area times the light intensity, viz.,
A · I = AcU
V
· 1
4π

2π
cos θ d = A 4σ T 4 · 1
2
 1
0
cos θ d cos θ ,
where θ is the angle between the current direction and the normal to the area. This
then leads to the Stefan–Boltzmann equation
I = σ T 4 ,
where the Stefan–Boltzmann constant σ was already introduced above.
According to p. 580, the average number of (polarized) photons in the ith one-
particle state is given by the Planck distribution:
⟨ni⟩=
1
exp(ℏωi/kT ) −1 .
For the frequency interval dω, the energy density is therefore (see Fig. 6.24)
dU
V
=
ℏω
exp(ℏω/kT ) −1
ω2 dω
π2 c3 .
This Planck radiation formula freezes high frequencies, while for low frequencies
it goes over to the Rayleigh–Jeans law
dU
V
≈kT ω2 dω
π2 c3 ,
which was originally derived for classical oscillators. According to the equidistribu-
tion law, each one contributes kT to the internal energy. But this led to the ultraviolet
catastrophe: U/V was not ﬁnite.
The maximum of the energy density as a function of the wavelength λ = 2πc/ω
follows with |ω3dω| = (2πc)4λ−5dλ from x ≡hc/(kTλ) = 5 {1 −exp(−x)} as
x = 4.965114231745. Together with the second radiation constant c2 ≡hc/k (see
Fig. 6.24), this leads to

596
6
Thermodynamics and Statistics
Fig. 6.24 Planck’s radiation distribution ϕ(λ, T ) = c1λ−5/{exp(c2/(λT)) −1} with the ﬁrst radi-
ation constant c1 ≡2πhc2 and the second radiation constant c2 ≡hc/k. Here ϕ is the radiation ﬂux
density emitted into a half space, viz., ϕ = 1
4c du/dλ. The factor 1
4c was derived for the Stefan–
Boltzmann equation. Three isotherms are shown. The visible light range (400 nm ≤λ ≤750 nm)
is indicated by dashed lines. The temperature of the surface of the Sun is such that a lot of visible
light is emitted (adaption of the eye)
λ = 1
x
hc
kT =
c2
4.965114231745 T .
This is Wien’s displacement law—the higher the temperature, the shorter the most
intense wavelength. As a function of the angular frequency ω (or the energy ℏω), the
maximum follows from x ≡ℏω/(kT ) as x = 3{1 −exp(−x)} = 2.821439372122.
Incidentally, according to the above equation for ⟨ni⟩, the total number of photons
in the volume V may be evaluated from N/V = 2ζ(3)π−2 (kT/ℏc)3 with ζ(3) =
1.202. This depends strongly on the temperature. With this value, we ﬁnd U =
π4/(30ζ(3)) NkT ≈2.7 NkT and hence the average energy per photon.
6.5.8
Lattice Vibrations
In a solid, each of the N atoms may oscillate about its equilibrium site. Here we may
restrict ourselves to harmonic oscillations with small displacements and introduce
3N normalcoordinates(seeSect.2.3.9).Wecanthendescribethemotionoftheatoms
as 3N decoupled oscillations—sound waves, corresponding to phonons as quanta,
without ﬁxing their number. They obey Bose–Einstein statistics. In contrast to the
photons in the last section, we have only a ﬁnite number (3N) of eigen frequencies,
in particular a limiting frequency ωmax.
The excitation energy of the states |n1, n2, . . .⟩s is 3N
i=1 ni ℏωi. Since the number
of phonons is not limited, we consider—as for photons—the canonical partition

6.5 Results for the Single-Particle Model
597
function
ZC =

{...ni...}
exp −
i ni ℏωi
kT
=
3N

i=1
1
1 −exp(−ℏωi/kT ) ,
or ln ZC = −3N
i=1 ln

1 −exp(−ℏωi/kT )

. The energy is therefore
U = −∂ln ZC
∂λE
=
3N

i=1
ℏωi
exp(ℏωi/kT ) −1 ,
and the heat capacity at constant volume (ﬁxed frequencies) is
CV =
∂U
∂T

V =
1
kT 2
3N

i=1

ℏωi
exp(ℏωi/kT ) −1
2
exp ℏωi
kT .
For kT ≫ℏωmax, we have the Dulong–Petit law CV ≈3 Nk, which follows from
the equidistribution law for all temperatures.
With decreasing temperature, ever more degrees of freedom freeze, and for low
temperatures, only the low frequency eigen oscillations are important, i.e., the normal
oscillations with longer wavelength. These wavelengths are essentially longer than
the interatomic distances, and we may make an ansatz for the density of states
∝ω2 (according to Debye) like the one for the electromagnetic radiation in a cavity.
However, we have to account for the fact that there is an upper bound ωmax for the
eigen frequencies:
gD(ω) =
9N ωD−3 ω2
for ω ≤ωD ≡ωmax ,
0
otherwise .
The factor 9NωD−3 follows from the constraint 3N =
 ∞
0 gD(ω) dω. This yields
U =
 ∞
0
ℏω {exp(ℏω/kT ) −1}−1 gD(ω) dω ,
for the energy, or
U = 9 NkT fD(ℏωD/kT ) ,
with the Debye function fD(x), which is displayed in Fig. 6.25.

598
6
Thermodynamics and Statistics
Fig. 6.25 Debye function (continuous red curve) and its approximation
1
15π4/x3 (dashed blue
curve)
Fig. 6.26 Temperature
dependence of the lattice
energy. For T ≪TD, we
have U ≈−3F ≈
3
5π4NkTD (T/TD)4
It is also common to introduce a Debye temperature TD ≡ℏωD/k (200–300 K).
For T ≪TD, the last integral is not important. Then, for the heat capacity,
CV ≈12π4
5
Nk
 T
TD
3
.
In fact, for low temperatures, CV ∝T 3 is observed, except for metals at very low
temperature. (There the conduction electrons contribute, and their heat capacity is
proportional to T according to p. 592.) Integrating by parts, the free energy is obtained
from
F = −kT ln ZC = kT
 ∞
0
ln

1 −exp −ℏω
kT

gD(ω) dω
= 3NkT

ln

1 −exp −TD
T

−fD
TD
T

.
For low temperatures, F = −1
3U and S = 1
3CV ∝T 3 (see Fig. 6.26), like for elec-
tromagnetic radiation in a cavity for all temperatures. Note that, for the harmonic
oscillations about ﬁxed positions we are concerned with here, F does not depend on
the volume, so a pressure cannot be derived for phonons.

6.5 Results for the Single-Particle Model
599
6.5.9
Summary: Results for the Single-Particle Model
In this section, we calculated partition functions for several examples and thereby
derived the equation of states, thus veriﬁable statements, which were not always
obvious for the original many-particle problem, where quantum theory was always
necessary. Classical physics leads to internal contradictions, e.g., to Gibbs’ paradox
(the entropy has to be an extensive variable) and to the ultraviolet catastrophe. Here
we have restricted ourselves to examples which can all be described in the single-
particle model of independent quanta: gases, conduction electrons, electromagnetic
radiation, and lattice oscillations. Here the ﬁrst two examples were treated as grand
canonical ensembles, because the particle number is an important parameter for them,
and the last two as canonical ensembles, because the number of oscillation quanta
(photons, phonons) cannot be given as a ﬁxed variable in those cases.
6.6
Phase Transitions
6.6.1
Van der Waals Equation
The equation of state of ideal gases assumes sufﬁciently high temperatures, because
real gases behave differently at lower temperatures, when interactions between the
molecules may no longer be neglected. These interactions are strongly repulsive for
small distances and weakly attractive for large distances. If the electronic shells of
two molecules overlap, they repel each other strongly, so we assign a volume b to each
molecule which is inaccessible to the others. Then the volume in the gas equation
must be replaced by V −Nb = N (v −b). At greater distances, on the other hand,
the molecules attract each other weakly like electric dipoles. It is not necessary for
permanent dipole moments to exist here. Before the quantum mechanical average, all
molecules have dipole moments, whose coupling does not vanish under the averaging
process. This attraction reduces the pressure on the outer walls and is proportional to
the product of the molecular densities in the interior of the volume and at the surface,
hence proportional to v−2. Therefore, in the gas equation, we have to replace the
pressure by p + av−2. We thus generalize the equation pv = kT for ideal gases to
the van der Waals equation

p + a
v2

v −b

= kT .
These additional terms contribute only for comparably small v = V/N.
Of course, the equation only makes sense for v ≥b. But it does not generally hold
even then, because it is an equation of third order in v(p, T ), viz.,
pv3 −(bp + kT ) v2 + av −ab = 0 ,

600
6
Thermodynamics and Statistics
and therefore allows for three different densities N/V. In fact, the van der Waals
equation describes not only real gases rather well, but to some extent also liquids. It
only gets things wrong for the phase transition. This is not so surprising, because so
far we have assumed homogeneous systems rather than a spatially separated gas and
liquid with their different densities.
How should the van der Waals solution be modiﬁed in order to describe the phase
transition without contradictions? Here we argue that, of three real solutions v(p, T ),
the one with the highest density (lowest v) should hold for the liquid and the one with
the lowest density (highest v) for the gas. For given p and T , the two phases exist
simultaneously between these densities. For the phase transition, despite a change
in v, we nevertheless expect p and T to remain constant. If we take, e.g., isotherms
as functions p (v), then the van der Waals solution in this ambiguous regime should
be replaced by a horizontal straight line segment.
In order to determine the pressure at which this straight line segment is to be taken,
we have to respect the free enthalpy and the equilibrium condition μ1 = μ2 for the
phase transition. We have dN1 = −dN2 and dT = 0 and therefore dG = V dp. The
area

V dp betweenthevanderWaalsisothermandthestraightlinesegmenthastobe
(Maxwell construction) chosensuchthat

dG vanishes, because G is astatevariable.
The van der Waals equation does not therefore always deliver (∂p/∂v)T < 0, as
it actually should according to p. 560 with (V )2 > 0. Given that
(∂p/∂v)T = −kT/(v−b)2 + 2a/v3 ,
in particular, the stability condition requires 2a (v−b)2/v3 ≤kT . This is not always
satisﬁed for low temperatures. The stable phase becomes unstable if we have equality
here and in addition (∂2 p/∂v2)T vanishes, which leads to kT = 3a (v −b)3/v4. At
the critical point for the stability, it is clear that kTc = 2a (vc−b)2/vc3 = 3a (vc −
b)3/vc4, whence
vc = 3b ,
kTc = 8a
27b ,
and
pc =
a
27b2 ,
and thereby pcvc = 3
8 kTc, in contrast to an ideal gas. Note that the van der Waals
equation holds only approximately here. Instead of 3
8 = 0.375, we observe 0.31 for
O2, 0.29 for N2, and 0.23 for H2O. With the reduced quantities vr = v/vc, Tr = T/Tc,
and pr = p/pc, the van der Waals equation reads (see Fig. 6.27)

pr + 3
vr2

(3vr −1) = 8Tr .
The parameters a and b are then hidden.

6.6 Phase Transitions
601
Fig. 6.27 Van der Waals
isotherms with Tr = 1.2, 1.0,
and 0.8. The middle red
curve is the critical one,
while the lower curve
corresponds to a phase
transition. Also shown here
is the unstable solution of the
van der Waals equation
(dashed curve)
6.6.2
Conclusions Regarding the van der Waals Equation
For the stress coefﬁcients β = (∂p/∂T )v, the van der Waals equation implies
β =
k
v −b = 1
T

p + a
v2

.
According to p. 570, (∂U/∂V )T = −p + βT . This is now equal to a/v2. Thus the
potential energy of the cohesive forces between the molecules contributes to the
internal energy. This addition depends in fact on the volume per particle, but not on
the temperature. Therefore, we also ﬁnd
∂CV
∂V
=
∂2U
∂V ∂T = 0 ,
as for an ideal gas.
On the other hand, according to the equation for (∂p/∂v)T mentioned in the last
section, the isothermal compressibility is
κT = −

v
∂p
∂v

T
−1
= f
(v −b)2
vkT −2a (1 −b/v)2 ,
so for the expansion coefﬁcient, we have
α = β κT = 1
T
v −b
v −(2a/kT )(1 −b/v)2 .
According to p. 575, 1−αT is important for the Joule–Thomson experiment, because
(∂T/∂p)H contains only the extra factor −V/C p:

602
6
Thermodynamics and Statistics
1 −αT = b −(2a/kT )(1 −b/v)2
v −(2a/kT )(1 −b/v)2 .
If we keep only terms of ﬁrst order in a and b, then this is equal to (b−2a/kT )/v.
It is negative for low temperatures and delivers (∂T/∂p)H >0. All real gases may
be cooled to low temperatures by decompression (dp<0). But for normal temper-
atures, this does not hold for hydrogen and the noble gases. Their cohesive forces
are then weak (a is small), so for normal temperatures these gases heat up under
decompression. Indeed, highly compressed hydrogen ignites upon streaming out of
leaks.
We can only differentiate the remaining thermal coefﬁcients if we know the
entropy or one of the thermodynamic potentials. As for the ideal gases, the internal
degrees of freedom of the molecules are important, and here we proceed as for the
ideal gases. For the change, we account only for the center-of-mass motion.
Here we disregard the feedback of a given molecule on the others and describe
the coupling by an effective one-particle potential V (r). Note that, in order to avoid
confusion we shall always indicate the position with the volume V . Then the classical
canonical partition function due to the center-of-mass motion of a molecule is
ZC(1) = 1
h3

exp

−1
kT
 p2
2m + V (r)

d3r d3 p ,
and according to p. 583,
ZC(1) = λ−3

exp −V (r)
kT
d3r ,
with λ =
h
√
2π mkT
.
If at ﬁrst we neglect the attractive forces and account only for the strong repulsion,
then the integral yields N (v −b). The weak attraction is approximated by the mean
value V (r) ≈−a/v:
ZC(1) = λ−3 N (v −b) exp a/v
kT .
In addition, for independent particles, according to the corrected Boltzmann statistics
(see p. 578), we have
ln ZC(N) = N ln ZC(1)
N
.
With this we obtain the free energy
F = −kT ln ZC = N

kT ln
λ3
v −b −a
v

,
and p = −(∂F/∂V )T,N = −N −1 (∂F/∂v)T,N = kT/(v −b) −a/v2 for the pres-
sure. Thus we have derived the van der Waals equation in a different way. (For

6.6 Phase Transitions
603
molecules containing more atoms, F also contains additions, and according to
Sect. 6.5.4, these in fact depend upon T , but not on V , whence we obtain the same
pressure.) But the entropy S = −(∂F/∂T )V,N for a real gas is lower than for an ideal
one:
Sreal −Sideal = Nk ln v −b
v
= Nk ln

1 −b
v

.
In addition, the chemical potential μ = (∂F/∂N)T,V is different:
μreal −μideal = −kT ln v −b
v
+ kT
b
v −b −2a
v .
6.6.3
Critical Behavior
The free enthalpy depends on the aggregation state and determines whether a probe
exists in the form of gas or liquid (or solid)—only the phase with the lowest free
enthalpy is stable, as we already stressed in Fig. 6.17. For ﬁxed pressure p < pc, the
(monotonically decreasing) function G(T ) has a kink at the transition temperature,
and likewise, for ﬁxed temperature T < Tc, the function G(p) has a kink at the tran-
sition pressure. The ﬁrst derivatives (∂G/∂T )p and (∂G/∂p)T have a discontinuity
for this discontinuous phase transition, and likewise the entropy and the volume:
S+ −S−= −
∂G+
∂T

p,N +
∂G−
∂T

p,N ,
V+ −V−=
∂G+
∂p

T,N −
∂G−
∂p

T,N .
Here we also speak of a ﬁrst order phase transition, because the ﬁrst derivatives of
G are discontinuous. Such phase transitions have a transition enthalpy (the pressure
remains constant) H+−H−= T (S+−S−) ̸= 0 and obey the Clausius–Clapeyron
equation
dp
dT = S+ −S−
V+ −V−
= 1
T
H+ −H−
V+ −V−
,
discussed on p. 573.
According to Sect. 6.6.1, the isotherm p (V ) has a horizontal tangent at the phase
transition, i.e., (∂p/∂V )T = 0. Therefore, the volume (and density) uncertainty is
inﬁnitelylargethere.Otherwise,itisnegligiblysmallformacroscopicbodies,e.g.,for
an ideal gas, we have (V/V )2 = 1/N (since (∂V/∂p)T = −V/p = −V 2/NkT ):
(V )2 = −
 ∂V
∂λV

T = −kT
∂V
∂p

T = −kT
 ∂p
∂V

T .

604
6
Thermodynamics and Statistics
The density therefore ﬂuctuates enormously at the phase transition. Hence, the
isothermal compressibility κT = −V −1(∂V/∂p)T is inﬁnite there, too, and likewise
(if a transition enthalpy is involved) the isobaric heat capacity C p = T (∂S/∂T )p
and the expansion coefﬁcient α = V −1 (∂V/∂T )p = −V −1(∂S/∂p)T .
At the critical point, S+ and S−agree with each other, as do V+ and V−. A
transition heat is unnecessary, and the ﬁrst derivatives of G are continuous. But
with (∂V/∂p)T = (∂2G/∂p2)T , the second derivative of the free enthalpy is inﬁnite.
Then we have a second order phase transition (a continuous phase transition). At
the critical point, the volume is very unsharp, as for a phase transition of ﬁrst order—
the density ﬂuctuates strongly. At the critical point, an otherwise transparent body
scatters light very strongly and appears opaque (critical opalescence).
We shall now investigate the behavior near the critical point. According to Car-
dani’s formula, the cubic equation v3 + 3Av2 + Bv + C = 0 has the three solutions
vi = xi −A with
x0 = R+ + R−
and
x±1 = −R+ + R−
2
± i
√
3 R+ −R−
2
,
and the abbreviations
R± =
3
−Q ±
	
Q2 + P3 ,
with
Q = A3 + C −AB
2
,
P = B
3 −A2 ,
where the third root is taken such that R+R−= −P. For real coefﬁcients, there are
three real solutions with Q2 + P3 < 0, and hence R−= R+∗. For the reduced van
der Waals equation, we have A = −8
9Tr/pr −1
9, B = 3/pr, and C = −1/pr, and
hence, Q = A3 −1
2(3A + 1)/pr and P = 1/pr −A2. Therefore, near the critical
point with T = Tr −1 and p = pr −1, we have
A ≈−1 + 8
9p −8
9T ,
Q ≈1
3p −4
3T ,
P ≈7
9p −16
9 T .
We reach the critical point along Q = 0, i.e., p = 4T . This delivers R± ≈
±2√T/3, and hence for T < 0, i.e., T < Tc, the two solutions vr −1 ≈
±2√1 −Tr at the phase boundary. For the density ρr ∝vr−1, it follows that
|ρ −ρc| ∝(Tc −T )1/2 .
The density ρ is called an order parameter for the considered system since it has a
discontinuity at the phase transition, and from the last relation, the critical exponent
1
2 for this order parameter is extracted from the van der Waals equation.
For the isothermal compressibility, pr = 8Tr/(3vr −1) −3vr−2 implies
∂pr
∂vr

T = −
24Tr
(3vr −1)2 + 6
vr3 ≈−6Tr

1 −3v + 27
4 (v)2
+ 6

1 −3v + 6(v)2
.

6.6 Phase Transitions
605
For T ≥Tc and v ≈0, this leads to κT −1 = 6pc (Tr −1), but for T ≤Tc and
(v)2 ≈4(1 −Tr), to κT −1 = 12pc (1 −Tr). In total, this gives
κT ∝|T −Tc|−1 ,
where the proportionality factor for T > Tc is equal to 1
6Tc/pc and for T < Tc half
as large. We usually set κT ∝|T −Tc|−γ . According to the van der Waals equation,
the critical exponent here is γ = 1.
6.6.4
Paramagnetism
Magnetism also provides an example of a phase transition. As for gases, we begin
by neglecting the interaction between the atoms (paramagnetism), and include them
in the next section in the molecular ﬁeld approximation due to Weiss.
We thus start from the magnetic moment mgμB of an atom with μB the Bohr
magneton (see p. 327), g the Landé factor, which is equal to (2 j +1)/(2l+1) for the
angular momentum j = l ± 1
2, according to p. 373, and m the directional (magnetic)
quantum number along the magnetic-ﬁeld direction. The potential energy is then
Wpot = −mgμBμ0H = −m η kT ,
with η ≡g μBμ0H
kT
.
In vacuum, we have B = μ0H and the energy −µ · B due to the coupling of the
magnetic moment to the magnetic ﬁeld. Nevertheless, here we investigate the mag-
netization induced by the magnetic ﬁeld, and use now μ0H instead of B (see Sect.
3.2.6).
For a given magnetic ﬁeld, the eigenstates of the energy are evenly spaced at
distances η kT from each other. However, there are only 2 j + 1 of them and not
inﬁnitely many as for a harmonic oscillator. Hence the directional quantum number
m in the canonical partition function 
m exp(mη) takes the values from −j to + j
in even-numbered steps. Now
x−j (1 + x + · · · + x2 j) = x−j 1 −x2 j+1
1 −x
= x j+1/2 −x−j−1/2
x1/2 −x−1/2
.
Hence, for the canonical partition function, we ﬁnd
ZC =
j

m=−j
emη = sinh(( j + 1
2)η)
sinh( 1
2η)
,
and clearly, ρm = ZC−1 exp(mη) for the occupation probability of the states with
the directional quantum number m.

606
6
Thermodynamics and Statistics
Fig. 6.28 Brillouin function
B j(η) for j = 1
2, 3
2, and 5
2.
For η ≈0, it depends
linearly on j, viz.,
B j(η) ≈1
3 ( j +1) η , and for
η ≫1, B j(η) ≈1
(saturation)
For the average magnetic moment, we obtain
m =

m m exp(mη)

m exp(mη)
= d
dη ln sinh(( j + 1
2)η)
sinh( 1
2η)
.
The polarization m/j is therefore given by the Brillouin function (see Fig. 6.28)
B j(η) ≡1
j
d
dη ln sinh(( j + 1
2)η)
sinh( 1
2η)
= ( j + 1
2) coth(( j + 1
2)η) −1
2 coth( 1
2η)
j
.
For j = 1
2, in particular, B1/2(η) = tanh( 1
2η) holds. Generally, B j(η) is a mono-
tonically increasing function—the stronger the magnetic ﬁeld H and the lower the
temperature T , the better the orientation.
For the magnetization from mutually independent moments, we obtain N/V times
the mean value of mgμB:
M = N
V m gμB = N jgμB
V
B j
gμBμ0H
kT

.
So for paramagnetism at low temperatures (η ≫1),
M ≈N
V j gμB ,
for kT ≪gμBμ0H .
Then it depends neither on the temperature nor on the magnetic ﬁeld, and the sys-
tem has reached saturation: all moments are oriented and the magnetization cannot
increase any further. In contrast, at high temperatures, we obtain M ∝H and hence
for the magnetic susceptibility
χ ≡M
H ≈N
V
j( j + 1) (gμB)2μ0
3kT
for kT ≫gμBμ0H .
It is thus proportional to the reciprocal of the temperature, which is Curie’s law.

6.6 Phase Transitions
607
6.6.5
Ferromagnetism
The correlation between the atoms neglected so far (for paramagnetism) is decisive
for ferromagnetism. Here what is important is not so much the magnetic coupling
between the dipole moments, as the exchange symmetry of the fermion states, where
position and spin states are important, because their product has to be antisymmetric
under particle exchange. For this reason even the electric coupling of two electrons
depends on the spin states. This leads to the Ising model
Wik = −2J mi mk ,
where only nearest neighbors i and k interact, although actually the parameter J
depends on the distances. It is adjusted, and even the sign is not the same for all
materials.
We follow P. Weiss with the molecular ﬁeld approximation and assume an average
one-particle potential. The coupling to the n nearest neighbors is then given simply
by −2nJmm, and for the average directional quantum number m, we found j B j(η)
in the last section. The ﬁeld at the position of the test particle is now composed of
the external ﬁeld and the remaining part. Thus we obtain
Wpot = −m {gμBμ0H + 2nj B j(η) J} .
As we have already done for paramagnetism, we may therefore set
Wpot = −m η kT
and
M = N
V gμB j B j(η) ,
but where η now follows from a new equation:
η = gμBμ0H + 2nj B j(η)J
kT
⇐⇒
B j(η) = kT η −gμBμ0H
2nj J
.
We have thus to ﬁnd the points where the Brillouin curve crosses a straight line.
Here the solution with the largest η > 0 is stable, because it has the smallest free
energy, given that the partition function ZC = sinh(( j + 1
2)η)/sinh( 1
2η) increases
monotonically with η, and therefore F = −kT ln ZC decreases.
The case J > 0 is particularly instructive, so we shall now restrict ourselves to
this. For H = 0, in addition to the crossing point for η = 0, there is another for η > 0
if
dB j(η)
dη

0 = j + 1
3
>
kT
2nj J = j + 1
3
T
TC
,
with kTC ≡2
3nj( j +1) J .
Below the Curie temperature TC, we also ﬁnd spontaneous magnetization for H = 0,
because for J > 0 the parallel orientation is convenient for the magnetic moments.

608
6
Thermodynamics and Statistics
The slope of the above-mentioned straight line is proportional to the temperature,
and therefore its crossing point with the Brillouin curve moves from T →0 to ever
higher values of η. But then we may set B j(η) ≈1 and ﬁnd again the saturation
magnetization. In contrast, for T →TC, the crossing point moves towards the origin.
The magnetization vanishes for T = TC. In this case, we have to evaluate B j(η) to
a higher accuracy than we have done so far, because now also the curvature of the
Brillouin curve is important:
B j(η) ≈j + 1
3
η −( j + 1
2)4 −( 1
2)4
45 j
η3 .
The crossing point with the straight line 1
3( j +1) (T/TC) η then leads to
η2 ∝1 −T/TC ,
and therefore to
M ∝
	
TC −T .
For T > TC and H = 0, there is no solution η ̸= 0.
For H ̸= 0 this changes, because then the straight line is shifted downwards and
therefore always cuts the Brillouin curve with η > 0, thus for T > TC. At least for
these temperatures and for H ≈0, we also ﬁnd η ≈0, and therefore we may set
B j(η) ≈1
3 ( j +1) η. This delivers η = gμBμ0H/(k(T −TC)), and hence for the
magnetic susceptibility,
χ = N
V
j( j +1) (gμB)2μ0
3k (T −TC)
,
for
T > TC .
This Curie–Weiss law reproduces the observation for T ≫TC very well, but not close
to the Curie temperature, where the molecular ﬁeld approximation is too coarse. This
means that the phase transition occurs not exactly at TC, if we have determined this
parameter using the Curie–Weiss law for higher temperatures. For T < TC, η is larger
than for H = 0 and the same temperature. Furthermore, the magnetization and the
susceptibility are larger, but the saturation values remain the same.
6.6.6
Bose–Einstein Condensation
We have in fact already considered a photon gas and lattice vibrations, both examples
of Bose gases, but in both cases the (average) particle number was not given. Now
we shall go back to that case, but start with the grand canonical ensemble and take
J = −kT ln ZGC = kT

i=0
ln

1 −exp −(ei −μ)
kT

.

6.6 Phase Transitions
609
We choose e0 as the zero energy and once again write σ for the fugacity exp(μ/kT ).
The term i =0 then contributes kT ln(1−σ), with 0≤σ <1. So far we have not
accounted for this in the high-temperature expansions in Sects. 6.5.4 and 6.5.6,
because replacing the partition function by an integral with the density of states, a
state with the zero energy has no weight:
g(e) = d
de
V
6π2
2me
ℏ2
3/2
=
V
(2π)2
2m
ℏ2
3/2√e = V
λ3
2
√π
√e/kT
kT
.
The internal degrees of freedom are in fact frozen at low temperatures and do not
need to be considered here, but a potential energy would have an effect. In this sense,
we are greatly simplifying here. We now obtain
ln ZGC = −ln(1 −σ) −V
λ3
2
√π
 ∞
0
√x ln(1 −σ e−x) dx ,
where x ≡e/kT . Here, integrating by parts, we ﬁnd
 ∞
0
√x ln(1 −σ e−x) dx = −2σ
3
 ∞
0
x3/2 dx
ex −σ = −
√π
2
∞

n=1
σ n
n5/2 .
Thus with the polylogarithm Li5/2(σ) (see Fig. 6.23), we obtain
J = kT ln(1 −σ) −kT V
λ3 Li5/2(σ) .
Hence it follows that
⟨N⟩= −
∂J
∂μ

T,V = −
∂J
∂σ

T,V
∂σ
∂μ

T,V =
σ
1 −σ + V
λ3 Li3/2(σ) .
The ﬁrst term on the right gives the particle number ⟨n0⟩in the ground state and the
rest then the number of particles in excited states (N ∗). We divide this equation by
N and introduce a critical temperature
Tc ≡
λ3
V
N
Li3/2(1)
2/3
=
h2
2πmk
 N/V
ζ( 3
2)
2/3
.
This increases with increasing density N/V. Hence,
1 −
σ
N (1 −σ) = Li3/2(σ)
ζ( 3
2)
 T
Tc
3/2
.
This equation ﬁxes σ(T ) for given Tc. In particular, σ(0) = N/(N +1) ≈1. For
N ≫1, this barely changes up to T = Tc. In particular, on the left-hand side,

610
6
Thermodynamics and Statistics
Fig. 6.29 Bose–Einstein condensation and its dependence on the temperature T relative to the
critical temperature Tc. Left: The number of particles in the ground state ⟨n0⟩or in excited states
N ∗relative to the total number N. Right: The chemical potential μ, represented for N =100
σ = 1 −1/
√
N delivers approximately 1 −1/
√
N ≈1, and the right-hand side with
T = Tc and σ = 1 thus yields 1. Here with ⟨n0⟩= σ/(1 −σ), the whole expression
is equal to 1 −⟨n0⟩/N = N ∗/N. Thus for T ≥Tc, it always stays equal to one, and
compared with the number N of particles in the ground state, i.e., ⟨n0⟩, this is clearly
negligible (see Fig. 6.29):
N ∗
N =
(T/Tc)3/2
for
T ≤Tc ,
1
for
T ≥Tc .
Here, of course, there are always more bosons in the ground state than in any other
one-particle state—only the sum of numbers over the many excited states may be
greater than the number in the ground state for higher temperatures.
These considerations thus lead to σ ≈1 for T ≤Tc and to Li3/2(σ) = λ3N ∗/V
for T ≥Tc, so Li3/2(σ) = ζ( 3
2) (Tc/T )3/2. If we differentiate this with respect to T ,
then on the left, we have σ −1 Li1/2(σ) · dσ/dT according to the chain rule, and the
polylogarithm diverges for σ = 1 (more strongly than −ln x at the origin). On the
right, for T = Tc, we obtain the ﬁnite value −3
2ζ( 3
2)/Tc. The derivative of σ with
respect to T thus vanishes at Tc, and is continuous (as is the chemical potential μ).
From the generalized grand canonical potential, the pressure and entropy may
also be derived:
p = −
 ∂J
∂V

T,μ = kT
λ3 Li5/2(σ) ,
S = −
 ∂J
∂T

V,μ = −k ln(1 −σ) +
5
2 pV −μN
T
.
The bosons in the ground state do not contribute to the pressure, and for ﬁxed T and
μ, σ is also constant. For T ≤Tc, it depends only on the temperature (and the mass
of the bosons) (proportional to T 5/2), but not on the density. With decreasing volume,

6.6 Phase Transitions
611
Fig. 6.30 Inﬂuence of the
Bose–Einstein condensation
on the pressure coefﬁcients β
(and the isochoric heat
capacity CV = 3
2 V β). At
T = Tc, we have
β = 5
2ζ( 5
2)/ζ( 3
2) Nk/V .
The dashed line is for an
ideal gas
Tc increases and hence also ⟨n0⟩. In other words, the particles condense. This also
holds for the internal energy. From U = J + T S + μN, we obtain U = 3
2 pV .
Clearly, the second derivatives of p and U with respect to T are discontinuous at
Tc, and so also are the ﬁrst derivative of the pressure coefﬁcients β and the isochoric
heat capacity CV , as well as the isothermal compressibility κT . Then, for the pressure
coefﬁcients, we obtain β = (∂p/∂T )V N (see Fig. 6.30)
β = Nk
V
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
5
2
ζ( 5
2)
ζ( 3
2)
 T
Tc
3/2
for
T ≤Tc ,
5
2
Li5/2(σ)
ζ( 3
2)
 T
Tc
3/2
−3
2
Li3/2(σ)
Li1/2(σ)
for T ≥Tc .
From this, we also have the isochoric heat capacity CV , because with U = 3
2 pV , this
is equal to 3
2 Vβ here.
6.6.7
Summary: Phase Transitions
As examples of phase transitions and critical behavior, we have investigated in some
detail the van der Waals gas, magnetism in Weiss’s molecular ﬁeld approximation,
and Bose–Einstein condensation. Here the van der Waals equation had to be amended
by the Maxwell construction, to make the volume a unique function of pressure and
temperature.
A phase transition of nth order has a discontinuity in the nth derivative of the free
enthalpy. The Clausius–Clapeyron equation holds for phase transitions of ﬁrst order.
At the critical point, there is a phase transition of second order. Here the density ρ or
the magnetization M are taken as the order parameter. Below the critical temperature,
their value jumps at the phase transition, but it is continuous above. At the critical
temperature, the isothermal compressibility κT and the susceptibility χ are inﬁnite.

612
6
Thermodynamics and Statistics
Problems
Problem 6.1 Legend tells us that the inventor of chess asked for S = 63
z=0 2z grains
of rice as a wage: one grain on the ﬁrst square, two on the second, and twice as many
on each subsequent square. Compare the sum S for all the squares with the Loschmidt
number NL ≈6 × 1023. How often can the surface of the Earth be covered with S
grains, if 10 of them are equivalent to 1 square centimeter? By the way, 29% of the
surface of the Earth is covered by land.
(3 P)
Problem 6.2 Justify Stirling’s formula n! = (n/e)n √
2πn with the help of the equa-
tion n! =
 ∞
0
xn exp(−x) dx, using a power series expansion of n ln x −x about the
maximum and also by comparing with ln(n!), n ln(n/e), and n ln(n/e) + 1
2 ln(2πn)
for n = 5, 10, and 50.
(9 P)
Problem 6.3 Draw the binomial distribution ρz =
Z
z

pz(1 −p)Z−z when Z = 10
for p = 0.5 and p = 0.1. Compare this with the associated Gauss distribution (equal
to ⟨z⟩and z) and for p = 0.1 with the associated Poisson distribution. Note that the
Gauss and Poisson distributions also assign values for z > 10, but which we do not
want to consider. For comparisons, set up tables with three digits after the decimal
point, no drawings.
(8 P)
Problem 6.4 From the binomial distribution for Z ≫1, derive the Gauss distribu-
tion if the probabilities p and q = 1 −p are not too small compared to one.
Hint: Here it is useful to investigate the properties of the binomial distribution near
its maximum and let ρ depend continuously on z.
(8 P)
Problem 6.5 How high is the probability for z decays in 10 seconds in a radioactive
source with an activity of 0.4 Bq? Give in particular the values ρ(z) for z = 0 to 10
with two digits after the decimal point.
(6 P)
Problem 6.6 Which probability distribution {ρz} delivers the highest information
measure I = −Z
z=1 ρz lb ρz?
Hint: Note the constraint Z
z=1 ρz = 1.
How does I change if initially Z1 states are occupied with equal probability and then
Z2 < Z1? Freezing of degrees of freedom: Determine I for Z1 = 10 and Z2 = 2.
For two possibilities, I may be written as a function of just p = ρ(z1). Set up a table
of values with the step width 0.05.
(6 P)
Problem 6.7 In phase space, every linear harmonic oscillation proceeds along an
ellipse. How does the area of this ellipse depend on the energy and oscillation period?
By how much do the areas of the ellipses of two oscillators differ when their energies
differ by ℏω? Determine the probability density ρ(x) for a given oscillation ampli-
tude x0 and equally distributed phases ϕ.

Problems
613
Hint: Thus we may set x = x0 sin(ωt+ϕ). Actually, the probability density should
be taken at time t. Why is this unnecessary here?
(7 P)
Problem 6.8 A molecule in a gas travels equal distances l between collisions with
other molecules. We assume that the molecules are of the same kind, but always at
rest, a useful simpliﬁcation which does not falsify the result. Here all directions occur
with equal probability. Determine the average square of the distance from the initial
point after n elastic collisions, and express the result as a function of time.
(4 P)
Problem 6.9 Does ρ(t, r) =
√
4π Dt
−3 exp(−r2/4Dt) solve the diffusion equa-
tion ∂ρ/∂t = Dρ, and does it obey the initial condition ρ(0, r) = δ(r)? What is the
time dependence of ⟨r2⟩? Compare with Problem 6.8. How do the solutions ρ(t, r)
read in one and two dimensions?
(9 P)
Problem 6.10 Consider N interaction-free molecules each of which is equally prob-
able in any of two equal sections of a container. What is the probability for all N
molecules to be in just one of the sections? If each of the possibilities since the
existence of the world (2 × 1010) has occurred corresponding to its probability, how
long have 100 molecules (very, very few for macroscopic processes!) been in one
section?
(2 P)
Problem 6.11 Given the Maxwell distribution
ρ (v) = 4π v2(2πkT/m)−3/2 exp(−mv2/2kT ) ,
determine the most frequent and the average velocities (v, ⟨v⟩), kinetic energies (E,
⟨E⟩), and de Broglie wavelengths (λ, ⟨λ⟩).
Hint:
 ∞
0
exp(−αx2) dx = 1
2

π
α ,
 ∞
0
x2n exp(−αx2) dx = (−)n ∂n
∂αn
 ∞
0
exp(−αx2) dx = (2n −1)!!
2n+1αn

π
α ,
 ∞
0
x2n+1 exp(−αx2) dx = 1
2
 ∞
0
yn exp(−αy) dy =
n!
2αn+1 .
The ﬁrst integral is half as large as
 ∞
−∞and the latter equal to the square root of the
surface integral
 ∞
−∞
exp{−α(x2 + y2)} dx dy = 2π
 ∞
0
exp(−αr2)r dr = π
 ∞
0
e−αxdx .
(8 P)

614
6
Thermodynamics and Statistics
Problem 6.12 Consider the 1D diffusion equation ∂y/∂t = D ∂2y/∂x2 with the
boundary condition y(t, 0) = c(0) exp(−iωt). Which differential equation follows
for c(x), and what are its physical solutions for x > 0? (Example: seasonal ground
temperature.)
(3 P)
Problem 6.13 UnderwhatcircumstancesdotheMaxwellequationsyieldadiffusion
equation for the electric ﬁeld strength? How large is the diffusion constant under such
circumstances?
(3 P)
Problem 6.14 For a molecular beam, all velocities v outside of a small solid angle
d around the beam direction are suppressed. How large is the number of sup-
pressed molecules with velocities between v and v + dv per unit time and unit area?
Determine the most frequent and the average velocity in the beam.
(4 P)
Problem 6.15 According to quantum theory, the phase space cells cover the area h.
Therefore, according to Problem 6.7, the number of states of one linear oscillator up
to the highest excitation energy E is equal to (E, 1) = E/ℏω + 1 = n + 1, with
the oscillator quantum number n. Determine (E, 2) for distinguishable oscillators
and then (E, N) by counting. Simplify the result for the case n ≫N. Is the density
of states for this system equal to
1
N! E N−1 (ℏω)−N?
Hint: The binomial coefﬁcients for natural m and arbitrary x are given by
x
m

= x · (x −1) · · · (x −m + 1)
m!
= x −m + 1
m

x
m −1

.
Consequently,
x
1

= x = x
x
0

,
m
m

= 1 = 1
m

m
m −1

,
and for n < m ,
n
m

= 0 .
In addition,
x + 1
m

=
x
m

+

x
m −1

,
and hence
n + 1
m + 1

=
n−m

k=0
n −k
m

.
(6 P)
Problem 6.16 From the expression found for (E, N) in Problem 6.15, determine
the canonical partition function and hence the average energy ⟨E⟩and the squared
relative ﬂuctuation (E/⟨E⟩)2.
(4 P)
Problem 6.17 The energy of N non-interacting spin- 1
2 particles with magnetic
moments µ in the magnetic ﬁeld is E = (n↓↑−n↑↑) μB. What is the micro-
canonical partition function of this system?
(4 P)

Problems
615
Problem 6.18 Take the result of the last problem as a binomial distribution (with
the energy as state variable), and approximate it by the Gauss distribution for μB ≪
dE ≪E. Thereby determine the entropy. How does the entropy differ from the one
found in Problem 6.17, obtained with the Stirling formula for E ≪NμB?
(6 P)
Problem 6.19 Determine, as for the equidistribution law, ⟨pn ˙xm⟩and ⟨xm ˙pn⟩for
canonical ensembles of particles which are enclosed between impenetrable walls.
Why are these considerations not also valid for unbound particles?
(4 P)
Problem 6.20 For an N-particle system, the expression N
i=1 ri · Fi is called the
virial of the force. What follows for its expectation value? Compare the result with
⟨Ekin⟩= N m
2 ⟨˙x · ˙x⟩and with the virial theorem of classical mechanics. Note that this
holds for the mean value over the time (!), and in fact for “quasi-periodic” systems,
i.e., x and p always have to stay ﬁnite.
(5 P)
Problem 6.21 Consider the 1D diffusion equation ∂y/∂t = D ∂2y/∂x2. How do
its solutions read with the initial condition y(0, x) = f (x) instead of the boundary
condition of Problem 6.12?
(2 P)
Problem 6.22 The gas pressure p on the walls can be determined from the momen-
tum change due to the elastic collision of the molecules. Determine the pressure as a
function of the average energy of the individual molecules. Here the same assump-
tions are made as for the derivation of the Boltzmann equation. Do we need the
Maxwell distribution? What follows for ⟨E⟩if the ideal gas equation pV = NkT
holds?
(6 P)
Problem 6.23 In a galvanometer, a quartz ﬁber with the torque δ = 10−13 J supports
a plane mirror. How large is the directional uncertainty at 20 ◦C from the Brownian
motion of the air molecules? How much does a reﬂected light beam ﬂuctuate on a
target scale at 1 m distance?
(3 P)
Problem 6.24 For an ideal monatomic gas, pV 5/3 is a constant for isentropic pro-
cesses. How much does the internal energy U change if the volume increases from
V0 to V ? Does U increase or decrease?
(3 P)
Problem 6.25 Consider a cycle in an (S, T ) diagram. What area corresponds to the
usable work and what area to the heat energy input? Consider a heat engine with
the heat input Q+ = T+S1 at the temperature T+ and Q0 = T0S2 at T0 < T+,
as well as heat output Q−= T−(S1 + S2) at T−< T0. Determine the efﬁciency
η(Q+, Q0, Q−) and compare it with the efﬁciency of an ideal Carnot process (ηC
with Q0 = 0). Express the result as a function of ηC, Q0/Q+, and T0/T+. Determine
a least upper bound for the efﬁciency of a cycle process with heat reservoirs at several
input and output temperatures.
(9 P)

616
6
Thermodynamics and Statistics
Problem 6.26 Why do we have to do work to pump heat from a cold to a hot
medium? Investigate this with an ideal cycle. Under ideal constraints, let the work
A be necessary in order to keep a house at the temperature T+ inside, while the
temperature outside is T−. How are these three quantities connected with the heat
loss Q+? How is the input heat Q′
+ in an ideal power plant related to the heat loss Q+
considered above if it works between the temperatures T ′
+ and T ′
−? Neglect the losses
in the power plant that delivers the electric energy. Take as an example T ′
+ = 800 ◦C,
T+ = 20 ◦C, and T−= T ′
−= 0 ◦C.
(8 P)
Problem 6.27 Determine the functional determinant
∂(S, T )
∂(V, p) =
 ∂S
∂V

p
∂T
∂p

V
−
∂S
∂p

V
 ∂T
∂V

p
.
(2 P)
Problem 6.28 Express the derivatives of S with respect to T , V , and p, with the other
parameters kept ﬁxed, in terms of the thermal coefﬁcients and V and T . Express the
derivatives of T with respect to S, V , and p in terms of the quantities above. Express
(∂F/∂T )p and (∂G/∂T )V in terms of these quantities.
(6 P)
Problem 6.29 Are (∂2U/∂S2)V , (∂2U/∂V 2)S, (∂2G/∂T 2)p, and (∂2G/∂p2)T
always positive?
(4 P)
Problem 6.30 If a charge dq is inserted isothermally and isochorically into a
reversibly working galvanic element at the open circuit voltage , the work δA =
 dq is done. How does its internal energy change for given (T )?
Hint: Note the integrability condition for the free energy F. In addition, we should
have δA = ϕdQ, if upper-case letters always stand for extensive quantities and lower-
case letters for intensive quantities.
(4 P)
Problem 6.31 What vapor pressure p(T ) is obtained from the Clausius–Clapeyron
equation if we assume a constant transition heat Q, neglecting the volume of the
liquid compared to the volume of the gas, and using the equation pV = NkT for an
ideal gas?
(4 P)
Problem 6.32 One liter of water at 20 ◦C and normal pressure (1013 hPa) is subject
to a pressure twenty times the normal pressure. Here the compressibility is 0.5/GPa
on average and the expansion coefﬁcient 2 × 10−4/K. Determine V/V0 as a function
of p and p0 (give values in numbers as well). How much work is necessary for the
change of state? By how much does the internal energy change?
(6 P)

Problems
617
Problem 6.33 At the freezing temperature, ice has the density 0.918 g/cm3 and
water the density 0.99984 g/cm3. An energy of 6.007 kJ/mole is needed to melt ice.
How large are the discontinuities in the four thermodynamic potentials for this phase
transition (relative to one mole)?
(4 P)
Problem 6.34 What is the connection between (∂U/∂V )T and (∂p
T /∂T )V ? Can
(∂CV /∂V )T be uniquely determined for a given thermal equation of state? Transfer
the results to the enthalpy and C p.
(6 P)
Problem 6.35 For a given heat capacity CV (T, V ) and thermal equation of state,
is the entropy uniquely deﬁned? Can we then also determine the thermodynamic
potentials?
(4 P)
Problem 6.36 From thermal coefﬁcients for ideal gases, derive the relation
pV κT /κS = const. ,
for isentropic processes. Determine V (T ) and p(T ) for adiabatic changes in ideal
gases. How does the sound velocity c in an ideal gas depend on T , and what is
obtained for nitrogen at 290 K?
(6 P)
Problem 6.37 For a mole of 4He at 1 bar and 290 K, determine the thermal de
Broglie wavelength λ, the fugacity exp(μ/kT ), the free enthalpy (in J), and the
entropy (in J/K). Here, helium may be taken as an ideal gas.
(4 P)
Problem 6.38 How is the thermal equation of state for ideal monatomic gases to
be modiﬁed in order to account to ﬁrst order for the difference in ln ZGC between
bosons and fermions?
Hint: We may expand pV/kT in powers of the fugacity and express this in terms of
N, V , and λ.
Compare the pressures of the Bose and Fermi gases with that of a classical gas. (8 P)
Problem 6.39 How do the pressure and temperature of the air depend on the height
for constant gravitational acceleration if heat conduction is negligible compared to
convection and therefore each mass element keeps its entropy? This is more realistic
than the assumption of constant temperature.
(2 P)
Problem 6.40 Consider the heating of a house as an isobaric–isochoric situation:
the air expands with increasing temperature and escapes through leakages. Assuming
an ideal gas, how does the number of molecules in the house change, and how does
the internal energy change, assuming that there are no internal excitations of the
molecules? Does the entropy increase or decrease. Or is this clear anyway from the
entropy law? (Heating is not an energy problem, but an entropy problem!)
(5 P)

618
6
Thermodynamics and Statistics
Fig. 6.31 Diesel cycle.
Idealized cycle from 1 to 2
and from 3 to 4 along
isentropic (adiabatic) curves
of an ideal gas, between
either isobaric (2 →3) or
isochoric (4 →1) curves.
Contrast with twice isochoric
for the Otto cycle and twice
isobaric for the Joule cycle
(gas turbine)
Problem 6.41 To extend a surface by dA, work δW = σ dA has to be done against
the attraction between the molecules, where σ is the surface tension. What sign
does (∂σ/∂T )A have? How does the free energy change for an isothermal surface
(without volume change) and how does the internal energy change? How much heat
is involved in an isothermal surface extension assuming that σ(T, A) is given? (6 P)
Problem 6.42 For four-stroke engines (suction, compression, combustion, ejec-
tion), only two cycles are assumed to be idealized. For example, Fig. 6.31 shows
the diesel cycle. Note that diesel engines are “compression–ignition engines”: the
fuel burns at approximately constant pressure. Which two cycles are related to the
diesel cycle (why?), and which path is taken by the one and the other in Fig. 6.31?
What is the efﬁciency of the idealized diesel engine as a function of the compres-
sion K = V1/V2 and expansion E = V4/V3, assuming a single ideal diatomic gas,
i.e., assuming the air to be pure nitrogen? Note that, clearly, K > E > 1. Begin by
expressing Q± in terms of the relevant temperatures. The compression depends on
the construction, but the expansion does not. It is determined by the “heat of com-
bustion” (combustion enthalpy). Determine the ratio K/E of the enthalpies.
(9 P)
List of Symbols
We stick closely to the recommendations of the International Union of Pure and
Applied Physics (IUPAP) and the Deutsches Institut für Normung (DIN). These
are listed in Symbole, Einheiten und Nomenklatur in der Physik (Physik-Verlag,
Weinheim 1980) and are marked here with an asterisk. However, one and the same
symbol may represent different quantities in different branches of physics. Therefore,
we have to divide the list of symbols into different parts (Table 6.3).

List of Symbols
619
Table 6.3 Symbols used in thermodynamics and statistics
Symbol
Name
Page number
∗
Q
Amount of heat
513
∗
A
Work
513
∗
V
Volume
9
∗
p
Pressure
560
∗
N
Particle number
552
∗
μ
Chemical potential
560
∗
S
Entropy
523
∗
T
Temperature
558
∗
U
Internal energy
556
∗
F = U −T S (Helmholtz) Free energy
567
∗
H =
U + pV
Enthalpy
567
∗
G =
H −T S
(Gibbs) Free enthalpy
567
J = F −μN
Grand canonical potential
567
∗
α =
1
V
∂V
∂T

p
(Volume-) Expansion coefﬁcient
569
∗
β =
 ∂p
∂T

V
Pressure coefﬁcient
569
∗
C p =
T
 ∂S
∂T

p
Isobaric heat capacity
569
∗
CV =
T
 ∂S
∂T

V
Isochoric heat capacity
569
∗a
κT =
−1
V
∂V
∂p

T
Isothermal compressibility
569
κS =
−1
V
∂V
∂p

S
Adiabatic compressibility
569
∗
c
Sound velocity
570
ρz
Probability for the state z
515

Partition function up to limiting energy
525, 550
Z
Partition function
549, 556
∗b
ZC
Canonical partition function
554
∗b
ZMC
Micro-canonical partition function
549
∗b
ZGC
Macro-canonical partition function
555
(continued)

620
6
Thermodynamics and Statistics
Table 6.3 (continued)
Symbol
Name
Page number
∗
τ
Relaxation time
527
∗
k
Boltzmann constant
623
∗
NA
Avogadro constant
623
∗
R
Gas constant
572
∗
ν
Stoichiometric coefﬁcient
561
aFor this compressibility, the abbreviation κ is recommended. However, we also use it for the
isentropic exponent −(V/p) (∂p/∂V )S = 1/(pκS). For an ideal gas it is equal to the ratio κT /κS =
Cp/CV
bThe abbreviations “C”, “MCC”, and “GC” stand for canonical, micro-canonical, and grand canon-
ical, and we also use them for the probabilities ρC, ρMC, and ρGC
References
1. C. Caratheodory, Sitzungsber. Preu. Akad. 33 (3 July 1919)
2. A. Sommerfeld, Lectures on Theoretical Physics 5–Thermodynamics and Statistical Mechanics
(Academic, London-Elsevier, Amsterdam, 1964)
3. Ch. Kittel, H. Krämer, Thermal Physics, 2nd edn. (W.H. Freeman, San Francisco, 1980)
4. F.Reif, Fundamentalsof Statistical andThermal Physics (McGraw-Hill,NewYorkNY,1965—
Waveland Press, Long Grove, 2010)
5. R. Zurmühl, Matrizen (Springer, Berlin, 1964). in German
6. C. Syros, The linear Boltzmann equation properties and solutions. Phys. Rep. 45, 211–300
(1978)
7. H. Risken, The Fokker-Planck Equation (Springer, Berlin, 1989)
8. J.R. Rumble (Ed.), CRC Handbook of Chemistry and Physics, 98th edn. (CRC Press, Taylor
& Francis, London, 2017)
9. A. Sommerfeld, Lectures on Theoretical Physics 6–Partial Differential Equations in Physics
(Academic, London-Elsevier, Amsterdam, 1964)
10. A. Bohr, B.R. Mottelson, Nuclear structure, Vol. 1 (Benjamin 1969—World Scientiﬁc 1998)
Suggestions for Textbooks and Further Reading
11. R. Baierlein, Thermal Physics (Cambridge University Press, Cambridge, 1999)
12. S.J. Blundell, K.M. Blundell, Concepts in Thermal Physics, 2nd edn. (Oxford University Press,
Oxford, 2010)
13. N.N. Bogolubov, N.N. Bogolubov Jr., Introduction to Quantum Statistical Mechanics (World
Scientiﬁc, Singapore, 1982)
14. W. Greiner, L. Heise, H. Stöcker, Thermodynamics and Statistical Mechanics (Springer, New
York, 1995)
15. L.P. Kadanov, G. Baym, Quantum Statistical Mechanics (Benjamin, New York, 1982)
16. D. Kondepudi, Introduction to Modern Thermodynamics (Wiley, Chichester, 2008)
17. L.D. Landau, E.M. Lifshitz, Course of Theoretical Physics Vol. 5—Statistical Physics 3rd edn.,
(Butterworth–Heinemann, Oxford, 1980)
18. E.M. Lifshitz, L.P. Pitaevskii, Course of Theoretical Physics Vol. 9—Statistical Physics Part 2—
Theory of the Condensed State (Butterworth–Heinemann, Oxford, 1980)
19. W. Nolting, Theoretical Physics 5–Thermodynamics (Springer, Berlin, 2017)

References
621
20. B.N. Roy, Fundamentals of Classical and Statistical Thermodynamics (Wiley, Chichester,
2002)
21. F. Scheck, Statistical Theory of Heat (Springer, Berlin, 2016)
22. D.V. Schroeder, An Introduction to Thermal Physics (Addison-Wesley, San Francisco, 2000)

Appendix A
Important Constants
This appendix contains four tables. Table A.1 gives the names for different pow-
ers of 10, Tables A.2 and A.3 give some important constants, and Table A.4 gives
some derived quantities. The generally accepted CODATA values are taken from
http://www.physics.nist.gov/cuu/Constants/Table/allascii.txt
Energy conversion units: J = kgm2/s2 = Nm = Ws = VAs = VC = AWb = Pam3.
Table A.1 Terminology for powers of 10
Factor
Preﬁx
Abbreviation
Factor
Preﬁx
Abbreviation
10−1
deci
d
10+1
deca
da
10−2
centi
c
10+2
hecto
h
10−3
milli
m
10+3
kilo
k
10−6
micro
µ
10+6
mega
M
10−9
nano
n
10+9
giga
G
10−12
pico
p
10+12
tera
T
10−15
femto
f
10+15
peta
P
10−18
atto
a
10+18
exa
E
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5
623

624
Appendix A: Important Constants
Table A.2 Important constants in vacuum by choice of the units (m, A). The mass unit (like the
units of meter, second, and ampere) is expected to be a quantity deﬁned by independent elementary
quantities in the near future as from May 20, 2019 on
Quantity
Symbol
Value
Unit
Light velocity
c0
299,792,458
m/s
Magnetic ﬁeld constant
μ0
4π × 10−7
N/A2
12.566370614359 × 10−7
N/A2 = H/m
Electric ﬁeld constant
ε0 = 1/μ0c02
8.854187817622 × 10−12
F/m
Elementary charge
e
1.602176634 × 10−19
C
Planck constant
h
6.62607015 × 10−34
J s
Action quantum
ℏ= h/2π
1.054571818 . . . × 10−34
J s
Boltzmann constant
k
1.380649 × 10−23
J/K
Avogadro constant
NA
6.02214076 × 1023
1/mol
Atomic mass constant
u
1.66053922 . . . × 10−27
kg
Table A.3 Further constants
Quantity
Symbol
Value
Unit
Gravitational constant
G
6.67408(31) × 10−11
m3/kg s2
Electron mass
me
9.10938356(16) × 10−31
kg
5.48579909070(16) × 10−4
u
Proton mass
mp
1.672621898(21) × 10−27
kg
1.007276466789(91)
u
Neutron mass
mn
1.674927471(21) × 10−27
kg
1.00866491588(49)
u
Table A.4 Derived quantities
Quantity
Symbol
Value
Unit
Fine structure constant
α = μ0c0e2/2h
7.2973525664(17) × 10−3
= 1/137.0359991 . . . 4
Bohr magneton
μB = eℏ/2me
9.2740089994(57) × 10−24
J/T
Stefan–Boltzmann constant
σ = π2k4/60ℏ3c2
0
5.670367(13) × 10−8
W/m2 K4

Index
A
A (ampere), 164, 200, 623
Aberration, 236
Absorption circuit, 214
Absorption, forced, 482
Acceleration ﬁeld, 257
Action
action function, 135–140, 245
action variable (phase integral), 136
reduced, 136–141
Action principle, 140
Action quantum, 276, 341, 524, 624
Active resistance, 213
Activity, absolute (fugacity), 582
Addition law for velocities, 234
Addition theorem for spherical harmonics,
400
Adiabatic theorem, 296
Aggregation state (phase), 572
Alloy, 574
Amount of heat, 563–565
Ampère’s circuital law, 195
Angular frequency, 137
Angular momentum, 70, 100
conservation, 77
coupling, 335–337
of the radiation ﬁeld, 464
of two particles, 72
operator, 328–329
rigid body, 86
Annihilation operator, 330–331, 470
for bosons, 302, 440–443
for fermions, 438–440, 442–443
Anomaly, magneto-mechanical, 327
Anti-correlation, 520
Anti-normal order, 477
Anti-particle, 501
Approximation
adiabatic, 347
Born, 405
better (DWBA), 420
Area–velocity law, 64, 142
Atomic mass constant, 571, 624
Atomic model, Bohr’s, 367
Attractor, 107
Auto-correlation, 520
Avogadro constant, 572, 624
Azimuth, 30, 31
B
Balance equation, 526
Base vector, 31–33
contravariant, 32
covariant, 32
Basic relation of thermodynamics, 561–562
irreversible, 562, 576
BCS theory, 457–462
Beats, 115
Behavior, critical, 603–605
Bernoulli distribution, 517
Bernoulli equation, 574
Bessel function
integer, 480
spherical (half integer), 400
Bi-orthogonal system, 426
Binomial coefﬁcient, 365, 614
Binomial distribution, 516–518
Binormal vector, 7
Biot–Savart law, 193
© Springer Nature Switzerland AG 2018
A. Lindner and D. Strauch, A Complete Course
on Theoretical Physics, Undergraduate Lecture Notes in Physics,
https://doi.org/10.1007/978-3-030-04360-5
625

626
Index
Bit, 521
Bloch equation, 486
Bloch function, 116
Bloch vector, 313, 343
Blue sky, 264
Body
massive, 79
rigid, 85–90,
Euler equations, 92
Bogoliubov transformation
for bosons, 473–475
for fermions, 458
Bohr magneton, 327, 624
Bohr radius, 362
Bohr–Sommerfeld quantization, 136
Boiling point, raising, 574
Boltzmann constant, 523, 624
Boltzmann equation, 531–533
collision-free, 345, 530
Boltzmann statistics, corrected, 578
Bose–Einstein condensation, 608–611
Bose–Einstein statistics, 578–582
Bosons, 435–438, 440–442
Boundary condition
asymptotic, 424
periodic, 452–453
Boundary conditions (conductor/insulator),
225
electrostatics, 177
Box potential, 354–358
Bra-vector, 283
Braking radiation, 265
Breit–Wigner formula, 426–427
Brewster angle, 223
Brillouin function, 606
C
Capacitor
cylindrical, 179
plate, 180
spherical, 179
Capacity, 179–180
Cauchy–Riemann equations, 177
Cauchy sequence, 284
C (coulomb), 164, 623
Center-of-mass law, 70–71
Central ﬁeld, 142–144
Central force, 55
Centrifugal force, 91
Centrifugal potential, 142
Change of representation, 286
Change of state
adiabatic, 566
irreversible, 576
reversible, 558
Channel
closed, 424
open, 423
Channel Hamilton operator, 429
Channel radius, 424
Channel resolvent, 429
Chaos, molecular, 532
Characteristic equation of an eigenvalue
problem, 88, 114
Characteristic function
(anti)normal-ordered, 479
(reduced action), 135–141
Charge
apparent, 174
electric, 165–166
Charge conjugation, 500–501
Charge density, 166
Christoffel symbols, 41–42
Circuit, oscillating, 213–214
Circular orbit, 67
Circulation voltage, 206
Clausius–Clapeyron equation, 573
Clausius–Mosotti formula, 175
Clebsch–Gordan coefﬁcient, 337
Clifford algebra, 490
Coefﬁcient
stoichiometric, 561
thermal, 568–571
Coexistence curve, 573
Coherence, 312
Collapse of the wave function, 389
Collision integral, 533
Collision, inverse, 531
Collision law, 73–76
Collision parameter, 67
Column vector, 3
Commutation relation, 315–317
Compass needle, 102
Completeness relation, 285
Compressibility, 569–571
Conduction electrons, 588
Conductivity, electric, 187
Conﬁguration mixture, 456
Conﬁguration space, 59
Conservation law, 238
of angular momentum, 77
of charge, 186, 204
of energy, 78
of momentum, 69
Conserved quantity, 69, 77–79

Index
627
Constant of the motion, 101
Constraint, 94–95
bilateral, 94
holonomic (integrable), 94
rheonomous, 94
scleronomous, 94
unilateral, 94
Contact voltage, 178
Continuity equation, 187
Continuum, normalization in the, 287
Convolution integral, 22
Coordinate
Cartesian, 3
curvilinear, 31–44
cyclic, 99
general, 31–44
generalized, 59–62
oblique, 31–44
Coordinate transformation, 33–34
Core electrons, 362
Coriolis force, 91
Correlation, 520–521
Correlation coefﬁcient, 520
Correlation function, 534
Correspondence principle, 325–327
Coulomb force, 410
Coulomb gauge, 197, 210
Coulomb law, 165–169
Coulomb parameter, 422
Coulomb scattering amplitude, 422
Coulomb scattering phase, 422
Coulomb wave functions, 422
Counter-force, 55
Coupling of angular momenta, 335–337
Covariant, bilinear, 498
CPT theorem, 500
Creation operator, 330–331, 470
for bosons, 302, 440–443
for fermions, 438–440, 442–443
Curie law, 606
Curie temperature, 607
Curie–Weiss law, 608
Curl, 13–14
Curl density, 13–14
Current
electric, 186–189
quasi-static, 205
stationary, 187
Current density, 186, 348–350
Current strength, 186
Curvature, 7–9
second, 8
Cycle, Diesel/Otto/Joule, 618
Cycle process, 563–565, 615
Carnot, 564
Cyclotron frequency, 78, 189
Cylindrical capacitor, 179
Cylindrical coordinates, 40
Cylindrical symmetry, 40
D
D’Alembert operator, 239
Damping, aperiodic, 108
Darboux vector, 8
De Broglie relation, 319
De Broglie wavelength, thermal, 583
Debye function, 597–598
Debye temperature, 598
Decay coefﬁcient, 106
Decay length, 224
Decay, radioactive, 528
Decay time, 106, 527
Decoherence, 389
Decoupling, 114
Degeneracy, 114, 295
accidental, 355
Degrees of freedom, 59
frozen, 522, 560
of a system, 374
Delta function, 18–22
transverse, 469–470
Density of states, 550–552
Density operator, 311–313
reduced, 375–389
time dependence, 342–344
Derivative
covariant, 42
partial, 11
Determinant, 5
Detuning, 483–486
Deviation, 87
Deviation, average (square), 516
Diamagnet, 196
Dielectric constant (permittivity), 176
Diesel cycle, 618
Differential equation
Euler’s, 140
Hill’s, 116–120
Mathieu’s, 117–118
Differential, exact (complete, total), 565
Differential quotient, partial, 43–44
Diffraction law
for force lines, 177
Snellius, 221
Diffusion coefﬁcient, 543

628
Index
Diffusion equation, 526, 536–537
improved, 536
Dipole moment
electric, 171
magnetic, 190–192
Dipole radiation, 264
Dirac bracket, 282–283
Dirac equation, 489
adjoint, 497
Dirac matrix, 490–494
Dirac picture, 345–348
Direct term, 445
Dispersion, 221
squared ﬂuctuation, 516
Dispersion relation, Kramers–Kronig, 23–
24
Displacement current, Maxwell’s, 164, 204,
205
Displacement, electric, 174–176
Displacement ﬁeld
electric, 174–176
magnetic, 193–195
Displacement law (Wien’s), 596
Displacement operator, 317
for Glauber state, 471
Dissipation, 374–389
Dissipation function (Rayleigh’s), 99
Dissipative behavior, 541
Distribution (generalized function), 18
Divergence (source density), 11–12
in general coordinates, 38–41
Doppler effect, 236, 264
quadratic, 236
transverse, 236
Double factorial, 401
Double slit experiment, 280–281
Doublet (two-level system), 308–310, 368
density operator, 312
Drag coefﬁcient, 84, 236
Drift term, 542
Dulong–Petit law, 597
E
Eccentricity of an ellipse, 63
Efﬁciency, thermal, 565
Ehrenfest’s theorem, 339
Eigen angular momentum, 324–325
Eigen-representation, 295
Eigenvalue, 87–90, 294–296
Eigenvalue equation
for the angular momentum, 329
for the energy, 351–374
Eigenvalue problem, 113–114
Eigenvector, 87–90, 294–296
Eikonal, 137
Electron, outer, 362
Elementary charge, 165–166, 624
Ellipse, 63
Elliptic functions (Jacobi)
amplitude, 105–106
cosinus amplitudinis, 146
delta amplitudinis, 146
sinus amplitudinis, 105, 146
Elliptic integral, 103–106
complete
ﬁrst kind, 104–105
third kind, 149
incomplete
ﬁrst kind, 103–105, 203
third kind, 148–149
Emission
forced, 482
spontaneous, 380, 485–486
Energy
bound, 575
free, 567, 575
of the electric ﬁeld, 182
internal, 513, 556
kinetic, 70
for time-dependent force, 151
of two bodies, 72
rigid body, 86
potential, 56–58, 151
generalized, 97–99
of dipoles, 171–172, 198
Energy conservation law, 78
Energy density
of the electric ﬁeld, 182
of the magnetic ﬁeld, 211
Energy ﬂux density, 211
Energy gap, 461
Energy–momentum stress tensor, 248–249
Energy representation, 417
Ensemble, 515
canonical, 554
ergodic, 534
grand canonical, 555
generalized, 556–561
micro-canonical, 549–550
statistical, 279, 515–520
Enthalpy, 567, 574–575
free, 567, 572
Entropy, 514
Entropy law, 514, 525–546
Entropy maximum, 552–561

Index
629
Equation, cubic, 604
Equation of state
canonical, 576
thermal, 576, 582
Equidistribution law, 559
Equilibrium
chemical, 560
detailed, 527
inhibited (partial), 557
thermal
(thermodynamic,
statistical),
548
total, 557
Equilibrium constant, 588
Equilibrium distribution, 546–561
Equilibrium state, stable, 576
Error analysis, 50–51
Error (average), 46–52
of the single measurement, 50
Error distribution, 47–49
Error integral, 48
Error limits, 44–52
Error propagation, 49
Error width, 516
Euler angles, 30–31
Euler–Lagrange equations
generalized, 241
Euler’s curvature radius, 7
Euler’s theorem for homogeneous functions,
587
Eutecticum, 574
Event, 228
Exchange equilibrium, 557–561
Exchange hole, 453
Exchange symmetry, 434–436
Exchange term, 445
Excitation, magnetic, 193–195
Expansion
in terms of Glauber states, 476–478
in terms of Legendre polynomials, 181
in terms of orthonormal system, 286
of operators, 297
plane wave in terms of spherical waves,
399
Expansion coefﬁcient, 569–571
Expectation value, 47, 299
Exponent, critical, 604
F
Factor, integrating (Euler’s), 566
Faddeev equations, 432
Faraday induction law, 205
Fermi–Dirac statistics, 578–582
Fermi energy, 355, 582
Fermi gas, degenerate, 588–593
Fermi gas model, 355
Fermions, 435–440
Fermi’s golden rule, 347
Ferroelectric, 176
Ferromagnet, 196
Ferromagnetism, 607–608
Feshbach theory, 423–426
F (farad), 164
Fictitious force, 90–92
Fictitious resistance, 214
Field constant
electric, 165, 166
magnetic, 164–165, 201
Field, electromagnetic, 206–227
Field equations
electrostatics, 176–178
magnetostatic, 195–197
Field-line tube, 12
Field operator, 301–303
Field quantization, 278
Field strength
electric, 166
magnetic, 193–195
Field tensor, electromagnetic, 240–244
Final-state interaction, 429
Fine structure constant, 362, 624
Fizeau experiment, 236
Floquet operator, 117
Floquet solution, 117
Flow, isentropic, 574
Fluctuation–dissipation theorem, 539–542
Fluctuation, relative, 516
Flux, 12
Fock space, 438
Fock state, 473
Fokker–Planck equation, 542–546
Foldy–Wouthuysen transformation, 503
Force, 55–62
generalized, 59–62
stochastic, 538
velocity-dependent, 97–99
Force ﬁeld, 77
homogeneous, 57
Force law, Ampère’s, 200–201
Force of constraint, 58
Fourier series, integral, 21
Fourier transform, 22–25, 216–220
Four-momentum, canonical conjugate, 247
Four-potential, 239
Four-vector, 231–238
Free-fall laws, 83–85

630
Index
Freezing point, lowering, 574, 587
Frenet–Serret formulas, 8
Fresnel’s equations, 222
Friction, 97–99
Newtonian, 84
Stokes, 99
Frictional constant, 538
Fugacity, 582
Functional derivative, 251
Functional matrix, 34
Function space, Hilbert, 286–287
Fundamental solution, 116
G
Galilean transformation, 227
-space, 523
Gap condition, 461
Gas
ideal, 582–586
real, 599
Gas constant, 572
Gauge transformation, 98, 209
Gauss distribution, 47, 519
Gauss force, 410
Gauss’s theorem, 12
Gay-Lussac law, 582
Gell-Mann and Goldberger
two-potential formula, 420
Gell-Mann matrix, 297
Generalized function (distribution), 18
Generating function
canonical transformations, 130–133
of the Bessel functions, 480
of the Hermite polynomials, 360
of the Laguerre polynomials, 365
of the Legendre polynomials, 82
Gerschgorin’s theorem, 528
G (gauss), 165
Gibbs–Duhem relation, 572
generalized, 587
Glauber state, 471–473
Golden rule, Fermi’s, 347, 382–386
Gradient, 10–11
in general coordinates, 38–41
Graph
connected, 432–433
unconnected, 430
Gravitation, 79
Gravitational acceleration, 81–85
Gravitational constant, 624
Gravitational force, 55, 79
Green function, 111
of the Laplace operator, 27
of the time-dependent oscillator, 119
propagator, 406
Green theorems, 17
Group velocity, 354
H
Hamilton equations
canonical, 122
for a ﬁeld, 252
Hamilton function, 122–124
Hamilton–Jacobi theory, 135–138
Hamilton operator, 326, 351–374
effective, 424
non-Hermitian, 381–382
Hankel function, 401
Hartree–Fock–Bogoliubov equations, 459–
462
Hartree–Fock equations, 454–455
Heat, 563–565
Joule, 188
latent, 563
speciﬁc, 569
Heat capacity, 569–571
Heat tone, 73
Heisenberg equation, 339–340
Heisenberg picture, 340–341
Heisenberg’s uncertainty relation, 275–276
Helicity, 219, 325
Hellmann–Feynman theorem, 296
Hermite polynomial, 359–361
Herpolhode cone, 90
H (henry), 164
Hilbert space, 282–284
convergence in, 283–284
Hilbert vector, 282–287
improper, 287
orthogonal, 283
parallel, 283
Hill’s differential equation, 149
Hole operator, 462
Hooke’s law, 52
H-theorem, 525
Husimi function, 479
Hydrogen atom, 361–367
Hyperbolic orbit, 67
Hysteresis curve, 196
I
Identity
Euler’s, 101
Jacobi

Index
631
for commutators, 289
for Poisson brackets, 124
for vector products, 4
Image charge, 180–181
Impedance, 213
Impulse, 77
Induced charge, 180
Inductance, 201–203
Induction, magnetic, 193–195
Induction voltage, 206
Inequality
Bessel’s, 285
Schwarz, 283
Inertial ellipsoid, 89
Inertial force, 90
Inertial frame, 69
Inertial law, 69
Information entropy, 521–523
Insertion of intermediate states, 285
Insulator, 176–178
Integrability condition
Maxwell’s, 554, 568
Integral principles, 139–142
Integraltheoremsforvectorexpressions,16–
17
Interaction
average, 449
magnetic, 198–201
non-local, 425
separable, 425
time-dependent, 345–348
Interaction representation, 345–348
Interface, and vector ﬁeld, 27
Inversion curve, 575
Ising model, 607
Isotropy, 40
J
Jacobi coordinates, 71
Jacobi matrix, 34
Jaynes–Cummings model, 482–486
J (joule), 623
Joule cycle, 618
Joule–Thomson effect, 575
K
Kepler problem, 62–68
Kepler’s law
ﬁrst, 63
second, 64
third, 65
Ket-vector, 283
Kirchhoff’s lawn, 189
Klein–Gordon equation, 501
Koopman’s theorem, 455
Kramers–Kronig (dispersion) relation, 23–
24
Kramers–Moyal expansion, 543–544
Kramers theorem, 314
Kronecker symbol, 18
L
Ladder operator, 330
Lagrange density, 241
Lagrange equations
ﬁrst kind, 61–62
second kind, 95–99
Lagrange function, 96–100, 247
generalized, 97
Lagrangian multiplier, 61
Laguerre polynomial, 365
generalized, 364–366
Lamb shift, 380
Landau levels, 359
Landé factor, 373
Langevin equation, 537–539
generalized, 542
Laplace equation, 16, 176–177
Laplace operator, 15
Laplace transform, 110–111
Larmor formula, 263
Larmor precession, 343–344
Lattice oscillation, 596–598
Lattice vector, 31
reciprocal, 31
Lattice vibration (phonon), 359–361
Law of mass action, 588
Law of motion, Newton’s, 76
Legendre polynomial, 81–83, 333–335
Legendre transformation, 121, 567
Leibniz formula, 364
Lenz’s rule, 205
Level repulsion, 310
Level shift, 425
Level splitting, 371–373
Level width, 425
Lever law, 59
Levi-Civita tensor, 36
Levinson theorem, 421
Libration, 103
Lie algebra, 290
Liénard–Wiechert potential, 260–261
Lifetime (average), 426, 527
Light cone, 230

632
Index
Light quantum (photon), 466–470
Line integral, 9
Line of nodes, 30, 31
Line width, natural, 264
Liouville equation, 125, 129, 343, 381–389,
529
Lippmann–Schwinger equation, 406, 411–
413
Lorentz contraction, 229, 230
Lorentz distribution, 47, 425, 519
Lorentz force, 78, 189–190, 244
Lorentz gauge, 210
Lorentz group, extended, 228
Lorentz invariance, 216
Lorentz transformation
homogeneous, 228–231
improper, 228
inhomogeneous, 227, 228
orthochronous, 228, 496
proper, 228, 254
Loschmidt number, 571
Low equation, 415
M
Macro state, 515
Magnetization, 191
Magnetization current, 242
Magneton, Bohr, 191, 327, 624
Magnetostatics, 193–199
Main theorem
ﬁrst, 513, 564
second, 514, 564
third, 514, 559
zeroth, 513, 558
Many-body state, 433–438
Markov approximation, 379
Mass
inertial, 69
reduced, 72
relativistic, 245
Mass unit, atomic, 571, 624
Master equation, 526
Matrix, 5
Matrix element, 290
reduced, 385
Matrix mechanics, 287
Maxwell–Boltzmann statistics, 577
Maxwell distribution, 546–548
local, 547
Maxwell equations
macroscopic, 206–208
covariance, 241–244
microscopic
covariance, 239–241
Maxwell relations, 554
Maxwell’s
construction (ﬁeld lines), 167
construction (van der Waals), 600
Mean square ﬂuctuation, 47
Mean value, 46
over time, 79
Measurable quantity, 298
Measurement process, 374
Meissner–Ochsenfeld effect, 195
Melting, 573
Melting heat, 563
Method of least squares, 51–52
Metric, Hermitian, 282
Metric tensor, 36
Micro state, 515
Minkowski diagram, 231
Minkowski force, 248
Minkowski metric, 232
Mixing entropy, 574, 586
Mixture
of materials, 586–588
of states, 280, 311–313
complete, 312
Mole, 571
Molecular ﬁeld approximation, 607
Molecular motion, Brownian, 537
Moment, magnetic, 190–192
Moment of inertia, 86–90
Momentum, 69–70
canonical conjugate, 99–101
mechanical, 100
of two bodies, 72
Momentum conservation law, 69
Momentum density of the radiation ﬁeld,
215
Momentum representation, 317–323, 417
Monopole (charge distribution), 171
Motional quantity (momentum), 69
Motion, force-free, 69–73
Multipole moment, 171, 181
μ-space, 523
Mutual inductance, 201–203
N
Nabla, 10
Negative-frequency part, 469
Neumann formula (inductance), 201
Neumann function, 401
Newton’s axiom

Index
633
ﬁrst, 69
second, 76
third, 55
N (newton), 623
Normal acceleration, 7
Normal coordinates, 113–115
Normal distribution, 47, 519
Normalizable (function, state), 286
Normal order, 477
Normal stress (pressure/tension), 183
Normal vector, 7
Norm (length of a Hilbert vector), 283–284
Nutation, 89
O
Observable, 298–299
Occupation number, average, 580
Occupation-number
representation,
440,
578
Oe (oersted), 165
Ohm’s law, 187
for AC current, 213
 (ohm), 164
One-particle density operator, 445
One-particle state, 433
Opalescence, critical, 604
Operator, 288–315
adjoint, 292
anti-linear, 289, 313
commuting, 289
diagonalization, 295
expansion, 297
Hermitian, 292–293
idempotent, 291
inverse, 292
linear, 289–315
local, 299
orthogonal, 297
representation, 290
self-adjoint, 292–293
trace, 294
unitary, 293
Optical theorem, 418
Optics, geometrical, 135–138
Order parameter, 604
Ornstein–Fürth relation, 535–537
Ørsted law, 195
Orthogonal system of the Legendre polyno-
mials, 82
Orthonormal set of functions, 21
Orthonormal system, 284
Oscillating circuit, 213–214
Oscillation
coupled, 112–115
damped, 106–112
forced, 108–112
harmonic, 102
differential equation, 106
quantum-mechanical, 358–361
Oscillator (see also oscillation)
time-dependent, 116–120, 149–151
Otto cycle, 618
Outer electron, 362
Over-complete basis, 472
P
Pair force, 456
Pa (pascal), 623
Paradox
Gibbs’, 578, 586
Zeno’s, 382
Paraelectric, 175
Parallel connection, 189
Paramagnet, 196
Paramagnetism, 605–606
Parameter
extensive, 552, 571
intensive, 552, 571
Parametric ampliﬁcation, 475
Parity, 314
Parity operation, 29, 228
Parseval’s equation, 23
Partial system, 520–521
Particle, free, 353
Particles, identical, 577
Partition function, 549–556
canonical, 554
Path curvature, 7–9
Pauli equation, 327, 504
rate equation, 382
Pauli operator, 308
Pauli principle, 303, 435
Pendulum, 101–106
Foucault’s, 91
mathematical, 101
oscillation period, 104
spherical, 145–149
Permeability, 196
Permittivity (dielectric constant), 176
Perturbation theory, 134
of Schrödinger and Rayleigh, 369
of Wigner and Brillouin, 369
time-dependent, 346
time-independent, 368–370

634
Index
P-function, 479
Phase (aggregation states), 572
Phase convention
for fermion states, 439
of Condon and Shortley, 331, 337
Phase integral, 136
Phase operator, 304–307
Phase shift, 102, 109
Phase space, 121
larger, 523
Phase space cell, 523–525
Phase transition, 572, 599–611
ﬁrst order, 603
second order, 604
Phase velocity, 137, 225, 354
Phonon, 359
Photon, 359, 466–470
Planck distribution, 595
Planck’s action quantum, 276, 624
Plane
invariant, 89
reﬂection and diffraction at, 220–223
Plane of incidence, 221
Planetary motion (Kepler problem)
as two-body problem, 79–80
Plate capacitor, 180
Poincaré group, 228
Poinsot’s construction, 89
Point, critical, 600
Poisson bracket, 124–125
Poisson distribution, 519
Poisson equation, 27, 169
Polar distance, 39
Polarizability of molecules, 175
Polarization
electric, 174–176
for doublets, 312
magnetic, 191
Polarization direction, 218–220
Polhode cone, 90
Polylogarithm, 593
Position vector, 1
Positive-frequency part, 469
Potential, 77
chemical, 560
electrostatic, 168–170
gauge, 169
grand canonical, 567, 579
thermodynamic, 566–569
time-dependent, 208–211
Power of electric currents, 188
Poynting’s theorem, 211–213
Poynting vector, 211–213
Precession, 90
pseudo-regular, 148
regular, 148
Pressure, 560
Pressure coefﬁcient, 569–571
Principal axes, dielectric, 176
Principal axis transformation, 87–90
Principal moment of inertia, 87–90
Principal quantum number, 363
Principal theorem of vector analysis, 25–27
Principal-value integral, 19
Principle
Boltzmann’s, 550
d’Alembert’s, 93–97
Fermat’s, 141, 246
geodesic, 246
Hamilton’s, 140
of least action, 141
of least time, 141
of virtual work, 58–59
Probability, 279
thermodynamic, 550
Probability wave, 277–279
Problem, inverse, 62
Product
dyadic (tensor product), 11
inner (scalar product), 3
of states, 282–283
of one-particle states, 433
outer (vector product), 4
Projection operator, 291
Propagation of waves
in conductors, 224–226
in insulators, 215–220
Propagator, 369
energy-dependent, 406–413
time-dependent, 405
Proper length, 230
Proper time, 230
Pseudo-momentum, 100
Pseudo-scalar, 6
Pseudo-vector, 6
Q
Q-function, 479
Quabla, 239
Quanta, 279
Quantity
complementary, 275
physical, 1
Quantization, 278
second, 278, 450

Index
635
Quantization direction, 328
Quantum electrodynamics, 463–487
Quantum number, 294
good, 339, 370
Quantum statistics, 578–582
Quasi-particles, 458
Quasi-probability, 324
Quasi-probability density, 479
Quasi-static current, 205
Quenched state, 473–476
R
Rabi frequency, 483
Radial equation, 353
Radial quantum number, 363
Radiation constant, 596
Radiation, electromagnetic, 594–596
Radiation energy, 258–259
Radiation ﬁeld, 256–258
of a dipole, 261–266
of a point charge, 260–261
Radiation formula (Planck), 595
Radiation gauge, 210, 256
Radiation pressure, 594
Radiation source, 253
Radiative reaction, 264
Radius, Bohr, 362
Random walk, 536
Rapidity, 235
Rate equation, 382–386, 526
Ray in Hilbert space, 282
Rayleigh–Jeans law, 595
Ray optics, 135–138
Reactance, 213
Reaction, endothermic, 588
Real-space representation, 317–323
Recursion relation
for Bessel functions, 400
for Hermite polynomials, 360
for Laguerre polynomials, 365–366
for Legendre polynomials, 82
for spherical harmonics, 333
Reference frame, accelerated, 90–92
Reﬂectivity of steps, 357
Refractive index, 137–138, 221
Relativistic dynamics
of free particles, 244–246
with external forces, 247–248
Relaxation time, 106, 527–529
Representation
coupled, 336
of a Hilbert vector, 285
uncoupled, 336
Repulsion of the current, 224, 225
Residual interaction, 449, 456–457
Residue theorem, 20
Resistance, electric, 187
Resolvent, 406
Resonance, 425–427, 486
parametric, 119
Response function, 539–542
Rest energy, 245
Rest mass, 245
Right-hand rule, 195
Rodrigues’ formula
for Hermite polynomials, 359
for Laguerre polynomials, 364
for Legendre polynomials, 334
Rotating-wave approximation, 379, 485
Rotation, 13
Rotational energy, 86
Rotation (curl density), 13–14
in general coordinates, 38–41
Rotation matrix, 30–31, 153
Rotation (vortex density), 13–14
Row vector, 3
Rutherford cross-section, 67, 423
Rydberg energy, 362
Rydberg state, 362
S
Saturation intensity, 486
Saturation magnetization, 608
Scalar product, 3
of states, 282–283
Scalar (tensor of zeroth rank), 35
Scalar triple product, 4
Scattering amplitude, 399–402, 416
Scattering angle, 67
Scattering cross-section, 417–418
Scattering operator, 414–415
Scattering phase, 421
Schrödinger equation
time-dependent, 341
time-independent, 351–374
Schrödinger picture, 340–345
Self-inductance, 212
Semi-classical ansatz, 485
Separatrix, 103
Sequence space, Hilbert, 285
Series
Hausdorff, 290
Neumann, 405
semi-convergent, 49

636
Index
Series connection, 189
Set of ﬁeld lines, 9–10
Shear stress, 183
Single-particle model, 550–552
Singlet state, 337
Skin effect, 225
Slater determinant, 438
Sommerfeld parameter, 422
Sound velocity, 570
Source density, 11–12
Space, 1
Space-like interval, 230, 231
Space reﬂection, 29, 228
Spherical capacitor, 179
Spherical coordinates, 39
Spherical harmonic, 331–335
Spin, 324–325
Spin angular momentum, 324–325
Spinor, 325
adjoint, 497
Spin–orbit coupling, 244, 371–373
Squared ﬂuctuation, 516
S (siemens), 164
Standard deviation, 516
Standard representation of Dirac matrices,
492
State, 565–566
coherent, 471
degenerate, 554
entangled, 375
pure, 280–281, 311
quantum-mechanical, 280–281
stationary, 342
irreversible change of, 527, 558
State variable, 513, 563, 565–566
Static friction, 58
Statistics, 513–525
classical, 523–524
Stefan–Boltzmann constant, 594, 624
Stefan–Boltzmann equation, 595
Steiner’s theorem, 86
Step function (theta function), 18
Stepwise decay, 429, 529
Stirling formula, 518
Stokes’s theorem, 13
Stress coefﬁcient, 569
Stress tensor, 183
Maxwell’s, 184
Structure constant (Lie algebra), 297
Sublimation, 573
Sublimation heat, 563
Summation convention (Einstein), 33, 231,
232
Sum rule, 372
Superconductor, 188, 195
Superposition principle, 279–281
Surface divergence, 27
Surface element, 9
Surface rotation, 27
Surface tension, 183–184
Susceptibility
electric, 175–176
generalized, 539–542
magnetic, 196, 606
Synchrotron radiation, 265–266
System
closed, 526
homogeneous, 571–572
open, 375
T
Tangential acceleration, 7
Tangent vector, 7
Taylor series, 11
Telegraph equation, 224
Temperature, 513, 558
micro-canonical, 555
Tension, mechanical, 183
Tensor, 35–42, 183–184
totally anti-symmetric, 36
Tensor contraction, 35
Tensor extension, 41
Tensor force, 56, 199–201
Tensor product, 3
Theta function (step function), 18
Throttling experiment, 574–575
Time, 1
Time dilation, relativistic, 230
Time-like interval, 230
Time-ordering operator, 346
Time reversal, 228
Time-shift matrix, 117
Time shift operator, 340, 403–405
Top
force-free, 92, 147
heavy, 144–149
Torque, 58
on dipole, 171–172
Torsion, 8–9
Total reﬂection (limiting angle), 223
Trace
of a matrix, 36
of an operator, 294
Trajectory, 6–9
Transformation

Index
637
canonical, 125–138
inﬁnitesimal, 129
inﬁnitesimal, 293
isometric, 293
Landen’s (elliptic integrals), 203
of electromagnetic ﬁelds, 243–244
orthogonal, 29
unitary, 29, 293–294
Transition amplitude, 299, 402
Transition operator, 415–417
Transition probability, 383
Transition rate, 383
Transmittance at steps, 357–358
Transverse gauge, 210
Trap circuit, 214
Triangle inequality, 283
Triple point, 573
Triple product, 4
Triplet state, 337
T (tesla), 164
Tunnel effect, 358, 361
Two-body problem, 79
Two-body system, 443–445
Two-by-two matrix
inverse, 71
Pauli matrices, 308
Two-potential formula, 419–420
2-spinor, 327
U
Uncertainty, 50, 516
quantum-mechanical, 299–301
Uncertainty relation, 275–276, 525
particle number–phase, 307
time–energy, 426
Unit operator, 289
Unit system
Gauss, 165
international, 164–165
Unit vector, 3
complex, 219
V
Van der Waals equation, 599–605
Vaporization, 573
Vaporization enthalpy, 563
Vaporization heat, 563
Variable
conjugate, 122
natural, 567
Variance, 47, 516
Variation, 58
Variational method, 370
Vector, 2–28
axial, 6
Lenz, 63
polar, 6
tensor of ﬁrst rank, 35
Vector algebra, 2–6
Vector ﬁeld, 9
interface, 27
longitudinal, transverse, 25
Vector potential, 98, 197–198
gauge, 197
Vector product, 4
Vectors
in function space, 286–287
in sequence space, 285
orthogonal, 3
Velocity ﬁeld, 257
Velocity four-vector, 234–236
Velocity of light in vacuum, 227
Velocity parameter, 235
Virial theorem, 79
Virtual displacement, 58
Viscosity, 574
Vlasov equation, 530
Voltage, electric, 169
Von Neumann equation, 342–345
Vortex, 14
Vortex density, 13–14
V (volt), 164, 623
W
Wave
evanescent, 223
polarized
circularly, 219
elliptically, 219
linearly, 219
propagation in insulators, 215–220
Wave equation
homogeneous, 216
inhomogeneous, 253–256
solution
advanced, 254
retarded, 254
Wave function, 320–323
probability amplitude, 279
Wave mechanics, 287
Wave operators (Möller’s), 413–414
Wave packet, 321, 354
Wave–particle duality, 276–277

638
Index
Wave resistance, 222
Wave vector, 24, 137
Wave vector representation, 320, 417
Wb (weber), 164, 623
Weber’s equation, 216
Weight, 80
speciﬁc, 571
Weight function, 366
Weyl correspondence, 326
Weyl representation (Dirac matrices), 492
Wigner–Eckart theorem, 385
Wigner force, 409
Wigner function, 321–324
time dependence, 344
Winding, 8
Work, 56
mechanical, 563–565, 574
World point, 230
Wronski determinant, 116
W (watt), 623
Y
Yukawa force, 410
Z
Zero operator, 288
Zero-point energy, 359
Zero vector, 2
Zeta function (Riemann), 590–591
Zitterbewegung, 498, 505

