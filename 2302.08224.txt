DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization
Zhiqing Sun 1 Yiming Yang 1
Abstract
Neural network-based Combinatorial Optimiza-
tion (CO) methods have shown promising results
in solving various NP-complete (NPC) problems
without relying on hand-crafted domain knowl-
edge.
This paper broadens the current scope
of neural solvers for NPC problems by intro-
ducing a new graph-based diffusion framework,
namely DIFUSCO. Our framework casts NPC
problems as discrete {0, 1}-vector optimization
problems and leverages graph-based denoising
diffusion models to generate high-quality solu-
tions. We investigate two types of diffusion mod-
els with Gaussian and Bernoulli noise, respec-
tively, and devise an effective inference schedule
to enhance the solution quality. We evaluate our
methods on two well-studied NPC combinato-
rial optimization problems: Traveling Salesman
Problem (TSP) and Maximal Independent Set
(MIS). Experimental results show that DIFUSCO
strongly outperforms the previous state-of-the-art
neural solvers, improving the performance gap be-
tween ground-truth and neural solvers from 1.76%
to 0.46% on TSP-500, from 2.46% to 1.17% on
TSP-1000, and from 3.19% to 2.58% on TSP-
10000. For the MIS problem, DIFUSCO outper-
forms the previous state-of-the-art neural solver
on the challenging SATLIB benchmark.
Our
code is available at https://github.com/
Edward-Sun/DIFUSCO.
1. Introduction
Combinatorial Optimization (CO) problems are mathemat-
ical problems that involve ﬁnding the optimal solution in
a discrete space. They are fundamental challenges in com-
puter science, especially the NP-Complete (NPC) class of
problems, which are believed to be intractable in polynomial
time. Traditionally, NPC solvers rely on integer program-
ming (IP) or hand-crafted heuristics, which demand signif-
1Carnegie Mellon University, Pittsburgh, PA 15213, USA. Cor-
respondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>.
Preprint.
icant expert efforts to approximate near-optimal solutions
(Arora, 1996; Gonzalez, 2007).
Recent development in deep learning has shown new
promise in solving NPC problems. Existing neural CO
solvers for NPC problems can be roughly classiﬁed into
three categories based on how the solutions are gener-
ated, i.e., the autoregressive constructive solvers, the non-
autoregressive constructive solvers, and the improvement
heuristics solvers. Methods in the ﬁrst category use autore-
gressive factorization to sequentially grow a valid partial
solution (Bello et al., 2016; Kool et al., 2019a). Those
methods typically suffer from the costly computation in
their sequential decoding parts and hence are difﬁcult to
scale up to large problems (Fu et al., 2021). Methods in
the second category rely on non-autoregressive modeling
for scaling up, with a conditional independence assumption
among variables as typical (Joshi et al., 2019; Karalias &
Loukas, 2020; Qiu et al., 2022). Such an assumption, how-
ever, unavoidably limits the capability of those methods
to capture the multimodal nature of the problems (Khalil
et al., 2017; Gu et al., 2018). Methods in the third category
(improvement heuristics solvers) use a Markov decision
process (MDP) to iteratively reﬁnes an existing feasible so-
lution with neural network-guided local operations such as
2-opt (Lin & Kernighan, 1973; Andrade et al., 2012) and
node swap (Chen & Tian, 2019; Wu et al., 2021). These
methods have also suffered from the difﬁculty in scaling up
and the latency in inference, partly due to the sparse rewards
and sample efﬁciency issues when learning improvement
heuristics in a reinforcement learning (RL) framework (Wu
et al., 2021; Ma et al., 2021).
Motivated by the recent remarkable success of diffusion
models in probabilistic generation (Song & Ermon, 2019;
Ho et al., 2020; Rombach et al., 2022; Yu et al.; Saharia et al.,
2022b), we introduce a novel approach named DIFUSCO,
which stands for the graph-based DIFfUsion Solvers for
Combinatorial Optimization. To apply the iterative denois-
ing process of diffusion models to graph-based settings, we
formulate each NPC problem as a {0, 1}-valued vector with
N variables that indicate the selection of nodes or edges in
the candidate solutions for the task. Then we use a message
passing-based graph neural network (Kipf & Welling, 2016;
Hamilton et al., 2017; Gilmer et al., 2017; Veliˇckovi´c et al.,
2018) to encode each instance graph and to denoise the
arXiv:2302.08224v1  [cs.LG]  16 Feb 2023

Graph-based Diffusion Solvers for Combinatorial Optimization
corrupted variables. Such a graph-based diffusion model
overcomes the limitations of previous neural NPC solvers
from a new perspective. Firstly, DIFUSCO can perform
inference on all variables in parallel with a few (≪N) de-
noising steps (Sec. 3.3), avoiding the sequential generation
problem of autoregressive constructive solvers. Secondly,
DIFUSCO can model a multimodal distribution via iterative
reﬁnements, which alleviates the expressiveness limitation
of previous non-autoregressive constructive models. Last
but not least, DIFUSCO is trained in an efﬁcient and sta-
ble manner with supervised denoising (Sec. 3.2), which
solves the training scalability issue of RL-based improve-
ment heuristics methods.
We should point out that the idea of utilizing a diffusion-
based generative model for NPC problems has been ex-
plored recently in the literature. In particular, Graikos et al.
(2022) proposed an image-based diffusion model to solve
Euclidean Traveling Salesman problems by projecting each
TSP instance onto a 64×64 greyscale image space and then
using a Convolutional Neural Network (CNN) to generate
the predicted solution image. The main difference between
such image-based diffusion solver and our graph-based
diffusion solver is that the latter can explicitly model the
node/edge selection process via the corresponding random
variables, which is a natural design choice for formulating
NPC problems (since most of them are deﬁned over a graph),
while the former does not support such a desirable formal-
ism. Although graph-based modeling has been employed
with both constructive (Kool et al., 2019a) and improvement
heuristics (d O Costa et al., 2020) solvers, how to use graph-
based diffusion models for solving NPC problems has not
been studied before, to the best of our knowledge.
We investigate two types of probabilistic diffusion within
the DIFUSCO framework: continuous diffusion with Gaus-
sian noise (Chen et al., 2022) and discrete diffusion with
Bernoulli noise (Austin et al., 2021; Hoogeboom et al.,
2021). These two types of diffusion models have been
applied to image processing but not to NPC problems. We
systematically compare the two types of modeling and ﬁnd
that discrete diffusion performs better than continuous dif-
fusion by a signiﬁcant margin (Section 4). We also design
an effective inference strategy to enhance the generation
quality of the discrete diffusion solvers.
Finally, we demonstrate that a single graph neural network
architecture, namely the Anisotropic Graph Neural Network
(Bresson & Laurent, 2018; Joshi et al., 2022), can be used
as the backbone network for two different NP-complete
combinatorial optimization problems: Traveling Salesman
Problem (TSP) and Maximal Independent Set (MIS). Our
experimental results show that DIFUSCO outperforms pre-
vious probabilistic NPC solvers on benchmark datasets of
TSP and MIS problems with various sizes.
2. Related Work
Let us outline the related work in several categories below.
Additional related work can be found in Appendix A.
2.1. Autoregressive Construction Heuristics Solvers
Autoregressive models have achieved state-of-the-art re-
sults as constructive heuristic solvers for combinatorial opti-
mization (CO) problems, following their recent success in
the language modeling or text generation domain (Vaswani
et al., 2017; Brown et al., 2020). The approach, ﬁrst pro-
posed by Bello et al. (2016) for CO problems, uses a neural
network and reinforcement learning to append one new vari-
able to the partial solution at each decoding step until a
complete solution is generated. However, autoregressive
models (Kool et al., 2019a) face high time and space com-
plexity challenges for large-scale NPC problems due to their
sequential generation scheme and quadratic complexity in
the self-attention mechanism (Vaswani et al., 2017).
2.2. Non-autoregressive Construction Heuristics
Solvers
Non-autoregressive (or heatmap) constructive heuristics
solvers (Joshi et al., 2019; Fu et al., 2021; Geisler et al.,
2022; Qiu et al., 2022) are recently proposed to address
this scalability issue by assuming conditional independence
among variables in NPC problems, but this assumption lim-
its the ability to capture the multimodal nature (Khalil et al.,
2017; Gu et al., 2018) of high-quality solution distributions.
Therefore, additional active search (Bello et al., 2016; Qiu
et al., 2022) or Monte-Carlo Tree Search (MCTS) (Fu et al.,
2021; Silver et al., 2016) are needed to further improve the
expressive power of the non-autoregressive scheme.
DIFUSCO can be regarded as a member in the non-
autoregressive constructive heuristics category and thus can
be beneﬁted from heatmap search techniques such as MCTS.
But DIFUSCO uses an iterative denoising scheme to gen-
erate the ﬁnal heatmap, which signiﬁcantly enhances its
expressive power compared to previous non-autoregressive
methods.
2.3. Diffusion Models for Discrete Data
Typical diffusion models (Sohl-Dickstein et al., 2015; Song
& Ermon, 2019; Ho et al., 2020; Song & Ermon, 2020;
Nichol & Dhariwal, 2021; Karras et al., 2022) operate in the
continuous domain, progressively adding Gaussian noise to
the clean data in the forward process, and learning to remove
noises in the reverse process in a discrete-time framework.
Discrete diffusion models have been proposed for the gener-
ation of discrete image bits or texts using binomial noises
(Sohl-Dickstein et al., 2015) and multinomial/categorical

Graph-based Diffusion Solvers for Combinatorial Optimization
noises (Austin et al., 2021; Hoogeboom et al., 2021). Recent
research has also shown the potential of discrete diffusion
models in sound generation (Yang et al., 2022), protein
structure generation (Luo et al., 2022), molecule generation
(Vignac et al., 2022), and better text generation (Johnson
et al., 2021; He et al., 2022).
Another line of work studies diffusion models for discrete
data by applying continuous diffusion models with Gaus-
sian noise on the embedding space of discrete data (Gong
et al., 2022; Li et al., 2022; Dieleman et al., 2022), the
{−1.0, 1.0} real-number vector space (Chen et al., 2022),
and the simplex space (Han et al., 2022). The most rele-
vant work might be Niu et al. (2020), which proposed a
continuous score-based generative framework for graphs,
but they only evaluated simple non-NP-hard CO tasks such
as Shortest Path and Maximum Spanning Tree.
3. DIFUSCO: Proposed Approach
3.1. Problem Deﬁnition
Following a conventional notation (Papadimitriou & Stei-
glitz, 1998), we deﬁne Xs = {0, 1}N as the space of
discrete solutions {x} for a CO problem instance s, and
cs : Xs →R as the objective function for solutions x ∈Xs:
cs(x) = cost(x, s) + valid(x, s)
(1)
where cost(·) is the cost function for a feasible solution
and is simply a is linear function of x in most NP-complete
problems; valid(·) is the validation function that returns 0
for feasible solutions and +∞for invalid solutions. The
optimization objective is to ﬁnd the optimal solution for a
given instance s:
xs∗= argmin
x∈Xs
cs(x).
(2)
This framework is generically applicable to different NPC
problems. For example, for the Traveling Salesman Problem
(TSP), x ∈{0, 1}N is the indicator vector for selecting a
subset from N edges; the cost of this subset is calculated as:
costTSP(x, s) = P
i xi·d(s)
i , where d(s)
i
denotes the weight
of the i-th edge in problem instance s, and the valid(·) part
of Formula (1) ensures that x is a tour that visits each node
exactly once and returns to the starting node at the end. For
the Maximal Independent Set (MIS) problem, x ∈{0, 1}N
is the indicator vector for selecting a subset from N nodes;
the cost of the subset is calculated as: costMIS(x, s) =
P
i(1 −xi),, and the corresponding valid(·) validates x
is an independent set where each node in the set has no
connection to any other node in the set.
Probabilistic neural NPC solvers (Bello et al., 2016) tackle
instance problem s by deﬁning a parameterized condi-
tional distribution pθ(x|s), such that the expected cost
P
x∈Xs cs(x) · p(x|s) is minimized. Such probabilistic gen-
erative models are usually optimized by reinforcement learn-
ing algorithms (Williams, 1992; Konda & Tsitsiklis, 2000).
In this paper, assuming the optimal (or high-quality) solu-
tion x∗
s is available for each training instance s, we optimize
the model through supervised learning. Let S = {si}N
1 be
independent and identically distributed (IID) training sam-
ples for a type of NPC problem, we aim to maximize the
likelihood of optimal (or high-quality) solutions, where the
loss function L is deﬁned as:
L(θ) = Es∈S [−log pθ(xs∗|s)]
(3)
Next, we describe how to use diffusion models to parame-
terize the generative distribution pθ. For brevity, we omit
the conditional notations of s and denote xs∗as x0 as a
convention in diffusion models for all formulas in the next
two sub-sections.
3.2. Diffusion Models in DIFUSCO
From the variational inference perspective (Kingma et al.,
2021), diffusion models (Sohl-Dickstein et al., 2015; Ho
et al., 2020; Song & Ermon, 2019) are latent variable models
of the form pθ(x0) :=
R
pθ(x0:T )dx1:T , where x1, . . . , xT
are latents of the same dimensionality as the data x0 =
q(x0). The joint distribution
pθ(x0:T ) = p(xT )
T
Y
t=1
pθ(xt−1|xt)
(4)
is the learned reverse (denoising) process that gradually
denoises the latent variables toward the data distribution,
while the forward process
q(x1:T |x0) =
T
Y
t=1
q(xt|xt−1)
(5)
gradually corrupts the data into noised latent variables.
Training is performed by optimizing the usual variational
bound on negative log-likelihood:
E [−log pθ(x0)] ≤Eq

−log
pθ(x0:T )
qθ(x1:T |x0)

= Eq
X
t>1
DKL[q(xt−1|xt, x0)∥pθ(xt−1|xt)]
−log pθ(x0|x1)

+ C
(6)
where C is a constant.
Discrete Diffusion
In discrete diffusion models with
multinomial noises (Austin et al., 2021; Hoogeboom et al.,
2021), the forward process is deﬁned as:
q(xt|xt−1) = Cat (xt; p = xt−1Qt) ,
(7)

Graph-based Diffusion Solvers for Combinatorial Optimization
where Qt =
(1 −βt)
βt
βt
(1 −βt)

is the transition proba-
bility matrix. xQ is a row vector-matrix product where x
is to be understood as one-hot vectors {0, 1}N×2 converted
from the original x ∈{0, 1}N.
Here, βt denotes the corruption ratio.
Also, we want
QT
t=1(1 −βt) ≈0 such that xT ∼Uniform(·). The t-step
marginal can thus be written as:
q(xt|x0) = Cat
 xt; p = x0Qt

,
(8)
where Qt = Q1Q2 . . . Qt. And the posterior at time t −1
can be obtained by Bayes’ theorem:
q(xt−1|xt, x0) = q(xt|xt−1, x0)q(xt−1|x0)
q(xt|x0)
= Cat
 
xt−1; p = xtQ⊤
t ⊙x0Qt−1
x0Qtx⊤
t
!
,
(9)
where ⊙denotes the element-wise multiplication.
According to Austin et al. (2021), the denoising neural
network is trained to predict the clean data pθ(ex0|xt), and
the reverse process is obtained by substituting the predicted
ex0 as x0 in Eq. 9:
pθ(xt−1|xt) =
X
ex
q(xt−1|xt, ex0)pθ(ex0|xt)
(10)
Continuous Diffusion for Discrete Data
The continuous
diffusion models (Song & Ermon, 2019; Ho et al., 2020)
can also be directly applied to discrete data by lifting the
discrete input into a continuous space (Chen et al., 2022).
Since the continuous diffusion models usually start from
a standard Gaussian distribution ϵ ∼N(0, I), Chen et al.
(2022) proposed to ﬁrst rescale the {0, 1}-valued variables
x0 to the {−1, 1} domain as ˆx0, and then treat them as
real values. The forward process in continuous diffusion is
deﬁned as:
q(ˆxt|ˆxt−1) := N(ˆxt;
p
1 −βtˆxt−1, βtI)
(11)
Again, βt denotes the corruption ratio, and we want
QT
t=1(1 −βt) ≈0 such that xT
∼N(·).
The t-step
marginal can thus be written as:
q(ˆxt|ˆx0) := N(ˆxt; √¯αtˆx0, (1 −¯αt)I)
(12)
where αt = 1−βt and ¯αt = Qt
τ=1 ατ. Similar to Eq. 9, the
posterior at time t −1 can be obtained by Bayes’ theorem:
q(ˆxt−1|ˆxt, x0) = q(ˆxt|ˆxt−1, ˆx0)q(ˆxt−1|ˆx0)
q(ˆxt|ˆx0)
,
(13)
which is a closed-form Gaussian distribution (Ho et al.,
2020). In continuous diffusion, the denoising neural net-
work is trained to predict the unscaled Gaussian noise
eϵt = (ˆxt −√¯αtˆx0)/√1 −¯αt = fθ(ˆxt, t). The reverse
process (Ho et al., 2020) can use a point estimation of ˆx0 in
the posterior:
pθ(ˆxt−1|ˆxt) = q(ˆxt−1|ˆxt, ˆxt −√1 −¯αtfθ(ˆxt, t)
√¯αt
) (14)
For generating discrete data, after the continuous data ˆx0 is
generated, a thresholding/quantization operation is applied
to convert them back to {0, 1}-valued variables x0 as the
model’s prediction.
3.3. Denoising Schedule for Fast Inference
One way to speed up the inference of denoising diffusion
models is to reduce the number of steps in the reverse dif-
fusion process, which also reduces the number of neural
network evaluations. The denoising diffusion implicit mod-
els (DDIMs) (Song et al., 2021a) are a class of models that
apply this strategy in the continuous domain, and a similar
approach can be used for discrete diffusion models (Austin
et al., 2021).
Formally, when the forward process is deﬁned not on all
the latent variables x1:T , but on a subset {xτ1, . . . , xτM },
where τ is an increasing sub-sequence of [1, . . . , T] with
length M, xτ1 = 1 and xτM = T, the fast sampling algo-
rithms directly models q(xτi−1|xτi, x0). Due to the space
limit, the detailed algorithms are described in Appendix F.
We consider two types of denoising scheduled for τ given
the desired card(τ) < T: linear and cosine. The for-
mer uses timesteps such that τi = ⌊ci⌋for some c, and the
latter uses timesteps such that τi = ⌊cos( (1−ci)π
2
) · T⌋for
some c. The intuition for the cosine schedule is that diffu-
sion models can achieve better generation quality when iter-
ating more steps in the low-noise regime (Nichol & Dhari-
wal, 2021; Yu et al., 2022; Chang et al., 2022).
3.4. Graph-based Denoising Network
The denoising network takes as input a set of noisy variables
xt and the problem instance s and predicts the clean data
ex0. To balance both scalability and performance consider-
ations, we adopt an anisotropic graph neural network with
edge gating mechanisms (Bresson & Laurent, 2018; Joshi
et al., 2022) as the backbone network for both discrete and
continuous diffusion models, and the variables in the net-
work output can be the states of either nodes, as in the case
of Maximum Independent Set (MIS) problems, or edges,
as in the case of Traveling Salesman Problems (TSP). The
detailed description of neural network architectures can be
found in Appendix G.
For TSP, the initial edge embeddings are initialized as the
corresponding values in xt, and the initial node embeddings
are initialized as sinusoidal positional features of the nodes.

Graph-based Diffusion Solvers for Combinatorial Optimization
For MIS, the initial edge embeddings are initialized as ze-
ros, and the initial node embeddings are initialized as the
corresponding values in xt. A 2-neuron and 1-neuron clas-
siﬁcation/regression head is applied to the ﬁnal embeddings
of for edges (or nodes) for discrete and continuous diffusion
models, respectively.
3.5. Decoding Strategies for Diffusion-based Solvers
After the training of the parameterized denoising network
according to Eq. 6, the solutions are sampled from the diffu-
sion models pθ(x0|s) for ﬁnal evaluation. However, proba-
bilistic generative models such as DIFUSCO cannot guaran-
tee that the sampled solutions are feasible according to the
deﬁnition of CO problems. Therefore, specialized decoding
strategies are designed for the two CO problems studied in
this paper.
Heatmap Generation
The diffusion models pθ(·|s) pro-
duce discrete variables x as the ﬁnal predictions by apply-
ing Bernoulli sampling (Eq. 10) for discrete diffusion or
quantization for continuous diffusion. However, this pro-
cess discards the comparative information that reﬂects the
conﬁdence of the predicted variables, which is crucial for
resolving conﬂicts in the decoding process. To preserve
this information, we adapt the diffusion models to generate
heatmaps (Joshi et al., 2019; Qiu et al., 2022) by making
the following appropriate modiﬁcations: 1) For discrete dif-
fusion, the ﬁnal score of pθ(x0 = 1|s) is preserved as the
heatmap scores; 2) For continuous diffusion, we remove the
ﬁnal quantization and use 0.5(ˆx0+1) as the heatmap scores.
Note that different from previous heatmap approaches (Joshi
et al., 2019; Qiu et al., 2022) that produce a single condition-
ally independent distribution for all variables, DIFUSCO
can produce diverse multimodal output distribution by using
different random seeds.
TSP Decoding
Let {Aij} be the heatmap scores gener-
ated by DIFUSCO denoting the conﬁdence of each edge.
We evaluate two approaches as the decoding method follow-
ing previous work (Graikos et al., 2022; Qiu et al., 2022): 1)
Greedy decoding (Graikos et al., 2022), where all the edges
are ranked by (Aij + Aji)/∥ci −cj∥, and are inserted into
the partial solution if there are no conﬂicts. 2-opt heuristics
(Lin & Kernighan, 1973) are optionally applied. 2) Monte
Carlo Tree Search (MCTS) (Fu et al., 2021), where k-opt
transformation actions are sampled guided by the heatmap
scores to improve the current solutions. Due to the space
limit, a detailed description of two decoding strategies can
be found in Appendix E.
MIS Decoding
Let {ai} be the heatmap scores gener-
ated by DIFUSCO denoting the conﬁdence of each node.
A greedy decoding strategy is used for the MIS problem,
where the nodes are ranked by ai and inserted into the partial
solution if there are no conﬂicts. Recent research (B¨other
et al., 2022) pointed out that the graph reduction and 2-opt
search (Andrade et al., 2012) can ﬁnd near-optimal solu-
tions even starting from a randomly generated solution, so
we do not use any post-processing for the greedy-decoded
solutions.
Solution Sampling
A common practice for probabilistic
CO solvers (Kool et al., 2019a) is to sample multiple so-
lutions and report the best one. For DIFUSCO, we follow
this practice by sampling multiple heatmaps from pθ(x0|s)
with different random seeds and then applying the greedy
decoding algorithm described above to each heatmap.
4. Experiments with TSP
We use 2-D Euclidean TSP instances to test our models.
We generate these instances by randomly sampling nodes
from a uniform distribution over the unit square. We use
TSP-50 (with 50 nodes) as the main benchmark to compare
different model conﬁgurations. We also evaluate our method
on larger TSP instances with 100, 500, 1000, and 10000
nodes to demonstrate its scalability and performance against
other state-of-the-art methods.
4.1. Experimental Settings
Datasets
We generate and label the training instances
using the Concorde exact solver (Applegate et al., 2006)
for TSP-50/100 and the LKH-3 heuristic solver (Helsgaun,
2017) for TSP-500/1000/10000. We use the same test in-
stances as (Joshi et al., 2022; Kool et al., 2019a) for TSP-
50/100 and (Fu et al., 2021) for TSP-500/1000/10000.
Graph Sparsiﬁcation
We use sparse graphs for large-
scale TSP problems to reduce the computational complexity.
We sparsify the graphs by limiting each node to have only
k edges to its nearest neighbors based on the Euclidean
distances. We set k to 50 for TSP-500 and 100 for TSP-
1000/10000. This way, we avoid the quadratic growth of
edges in dense graphs as the number of nodes increases.
Model Settings
T = 1000 denoising steps are used for
the training of DIFUSCO on all datasets. Following Ho et al.
(2020); Graikos et al. (2022), we use a simple linear noise
schedule for {βt}T
t=1, where β1 = 10−4 and βT = 0.02.
We follow Graikos et al. (2022) and use the Greedy decoding
+ 2-opt scheme (Sec. 3.5) as the default decoding scheme
for experiments.
Evaluation Metrics
In order to compare the performance
of different models, we present three metrics: average tour
length (Length), average relative performance drop (Drop),

Graph-based Diffusion Solvers for Combinatorial Optimization
Figure 1: Comparison of continuous (Gaussian
noise) and discrete (Bernoulli noise) diffusion
models with different inference diffusion steps
and inference schedule (linear v.s. cosine).
Table 1: Comparing results on TSP-50 and TSP-100. ∗denotes the baseline for
computing the performance drop. † indicates that the diffusion model samples a single
solution as its greedy decoding scheme. Please refer to Sec. 4 for details.
ALGORITHM
TYPE
TSP-50
TSP-100
LENGTH↓DROP(%)↓LENGTH ↓DROP(%)↓
CONCORDE∗
EXACT
5.69
0.00
7.76
0.00
2-OPT
HEURISTICS
5.86
2.95
8.03
3.54
AM
GREEDY
5.80
1.76
8.12
4.53
GCN
GREEDY
5.87
3.10
8.41
8.38
TRANSFORMER
GREEDY
5.71
0.31
7.88
1.42
POMO
GREEDY
5.73
0.64
7.84
1.07
SYM-NCO
GREEDY
-
-
7.84
0.94
DPDP
1k-IMPROVEMENTS
5.70
0.14
7.89
1.62
IMAGE DIFFUSION
GREEDY†
5.76
1.23
7.92
2.11
OURS
GREEDY†
5.70
0.10
7.78
0.24
AM
1k×SAMPLING
5.73
0.52
7.94
2.26
GCN
2k×SAMPLING
5.70
0.01
7.87
1.39
TRANSFORMER
2k×SAMPLING
5.69
0.00
7.76
0.39
POMO
8×AUGMENT
5.69
0.03
7.77
0.14
SYM-NCO
100×SAMPLING
-
-
7.79
0.39
MDAM
50×SAMPLING
5.70
0.03
7.79
0.38
DPDP
100k-IMPROVEMENTS
5.70
0.00
7.77
0.00
OURS
16×SAMPLING
5.69
-0.01
7.76
-0.01
(a) Continuous diffusion
(b) Discrete diffusion
(c) Runtime
Figure 2: The performance Drop (%) are shown for continuous diffusion (a) and discrete diffusion (b) models on TSP-50 with different
diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis
(neural network + greedy decoding + 2-opt) can be found in the appendix (Tab. 5).
and total run time (Time). The detailed description can be
found in Appendix D.
4.2. Design Analysis
Discrete Diffusion v.s. Continuous Diffusion
We ﬁrst
investigate the suitability of two diffusion approaches for
combinatorial optimization, namely continuous diffusion
with Gaussian noise and discrete diffusion with Bernoulli
noise (Sec. 3.2). Additionally, we explore the effective-
ness of different denoising schedules, such as linear and
cosine schedules (Sec. 3.3), on CO problems. To efﬁ-
ciently evaluate these model choices, we utilize the TSP-50
benchmark.
Note that although all the diffusion models are trained with
a T = 1000 noise schedule, the inference schedule can be
shorter than T, as described in Sec. 3.3. Speciﬁcally, we are
interested in diffusion models with 1, 2, 5, 10, 20, 50, 100,
and 200 diffusion steps.
Fig. 1 demonstrates the performance of two types of dif-
fusion models with two types of inference schedules and
various diffusion steps. We can see that discrete diffusion
consistently outperforms the continuous diffusion models by
a large margin when there are more than 5 diffusion steps1.
Besides, the cosine schedule is superior to linear on
discrete diffusion and performs similarly on continuous dif-
fusion. Therefore, we use cosine for the rest of the paper.
More Diffusion Iterations v.s. More Sampling
By uti-
lizing effective denoising schedules, diffusion models are
able to adaptively infer based on the available computa-
tion budget by predetermining the total number of diffusion
steps. This is similar to changing the number of samples
in previous probabilistic neural NPC solvers (Kool et al.,
2019a). Therefore, we investigate the trade-off between the
number of diffusion iterations and the number of samples
1We also observe similar patterns on TSP-100, where the re-
sults are reported in the appendix (Tab. 4).

Graph-based Diffusion Solvers for Combinatorial Optimization
Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning,
Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline
for computing the performance drop. Results of baselines are taken from Fu et al. (2021) and Qiu et al. (2022), so the runtime may not be
directly comparable. See Section 4 and Appendix H for detailed descriptions.
ALGORITHM
TYPE
TSP-500
TSP-1000
TSP-10000
LENGTH ↓DROP ↓
TIME ↓
LENGTH ↓
DROP ↓
TIME ↓
LENGTH ↓DROP ↓
TIME ↓
CONCORDE
EXACT
16.55∗
—
37.66m
23.12∗
—
6.65h
N/A
N/A
N/A
GUROBI
EXACT
16.55
0.00%
45.63h
N/A
N/A
N/A
N/A
N/A
N/A
LKH-3 (DEFAULT)
HEURISTICS
16.55
0.00%
46.28m
23.12
0.00%
2.57h
71.77∗
—
8.8h
LKH-3 (LESS TRAILS) HEURISTICS
16.55
0.00%
3.03m
23.12
0.00%
7.73m
71.79
—
51.27m
FARTHEST INSERTION
HEURISTICS
18.30
10.57%
0s
25.72
11.25%
0s
80.59
12.29%
6s
AM
RL+G
20.02
20.99%
1.51m
31.15
34.75%
3.18m
141.68
97.39%
5.99m
GCN
SL+G
29.72
79.61%
6.67m
48.62
110.29% 28.52m
N/A
N/A
N/A
POMO+EAS-EMB
RL+AS+G
19.24
16.25%
12.80h
N/A
N/A
N/A
N/A
N/A
N/A
POMO+EAS-TAB
RL+AS+G
24.54
48.22%
11.61h
49.56
114.36%
63.45h
N/A
N/A
N/A
DIMES
RL+G
18.93
14.38%
0.97m
26.58
14.97%
2.08m
86.44
20.44%
4.65m
DIMES
RL+AS+G
17.81
7.61%
2.10h
24.91
7.74%
4.49h
80.45
12.09%
3.07h
OURS (DIFUSCO)
SL+G†
18.35
10.85%
3.61m
26.14
13.06%
11.86m
98.15
36.75% 28.51m
OURS (DIFUSCO)
SL+G†+2-OPT
16.80
1.49%
3.65m
23.56
1.90%
12.06m
73.99
3.10%
35.38m
EAN
RL+S+2-OPT
23.75
43.57% 57.76m
47.73
106.46%
5.39h
N/A
N/A
N/A
AM
RL+BS
19.53
18.03% 21.99m
29.90
29.23%
1.64h
129.40
80.28%
1.81h
GCN
SL+BS
30.37
83.55% 38.02m
51.26
121.73% 51.67m
N/A
N/A
N/A
DIMES
RL+S
18.84
13.84%
1.06m
26.36
14.01%
2.38m
85.75
19.48%
4.80m
DIMES
RL+AS+S
17.80
7.55%
2.11h
24.89
7.70%
4.53h
80.42
12.05%
3.12h
OURS (DIFUSCO)
SL+S
17.23
4.08%
11.02m
25.19
8.95%
46.08m
95.52
33.09%
6.59h
OURS (DIFUSCO)
SL+S+2-OPT
16.65
0.57%
11.46m
23.45
1.43%
48.09m
73.89
2.95%
6.72h
ATT-GCN
SL+MCTS
16.97
2.54%
2.20m
23.86
3.22%
4.10m
74.93
4.39%
21.49m
DIMES
RL+MCTS
16.87
1.93%
2.92m
23.73
2.64%
6.87m
74.63
3.98%
29.83m
DIMES
RL+AS+MCTS
16.84
1.76%
2.15h
23.69
2.46%
4.62h
74.06
3.19%
3.57h
OURS (DIFUSCO)
SL+MCTS
16.63
0.46%
10.13m
23.39
1.17%
24.47m
73.62
2.58%
47.36m
for diffusion-based NPC solvers.
Fig. 2 shows the results of continuous diffusion and dis-
crete diffusion with various diffusion steps and number
of parallel sampling, as well as their corresponding to-
tal run-time.
The cosine denoising schedule is used
for fast inference. Again, we ﬁnd that discrete diffusion
outperforms continuous diffusion across various settings.
Besides, we ﬁnd performing more diffusion iterations is
generally more effective than sampling more solutions,
even when the former uses less computation. For example,
20 (diffusion steps) × 4 (samples) performs competitive to
1 (diffusion steps) × 1024 (samples), while the runtime of
the former is 18.5× less than the latter.
In general, we ﬁnd that 50 (diffusion steps) × 1 (samples)
policy and 10 (diffusion steps) × 16 (samples) policy make
a good balance between exploration and exploitation for
discrete DIFUSCO models and use them as the Greedy and
Sampling strategies for the rest of the experiments.
4.3. Main Results
Comparison to SOTA Methods
We compare discrete DI-
FUSCO to other state-of-the-art neural NPC solvers on TSP
problems across various scales. Due to the space limit,
the description of other baseline models can be found in
Appendix H.
Tab. 1 compare discrete DIFUSCO with other models on
TSP-50 and TSP-100, where DIFUSCO achieves the state-
of-the-art performance in both greedy and sampling settings
for probabilistic solvers.
Tab. 2 compare discrete DIFUSCO with other models on
the larger-scale TSP-500, TSP-1000, and TSP-10000 prob-
lems. Most previous probabilistic solvers (except DIMES
(Qiu et al., 2022)) becomes untrainable on TSP problems
of these scales, so the results of these models are reported
with TSP-100 trained models. The results are reported with
greedy, sampling, and MCTS decoding strategies, respec-
tively. For fair comparisons (Fu et al., 2021; Qiu et al.,
2022), MCTS decoding for TSP is always evaluated with
only one sampled heatmap. From the table, we can see that
DIFUSCO strongly outperforms the previous neural solvers
on all three settings. In particular, with MCTS-based de-
coding, DIFUSCO signiﬁcantly improving the performance
gap between ground-truth and neural solvers from 1.76%
to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000,
and from 3.19% to 2.58% on TSP-10000.
Generalization Tests
Finally, we study the generalization
ability of discrete DIFUSCO trained on a set of TSP prob-
lems of a speciﬁc problem scale and evaluated on other prob-

Graph-based Diffusion Solvers for Combinatorial Optimization
Figure 3: Generalization tests of DIFUSCO trained and
evaluated on TSP problems across various scales. The
performance Drop (%) with greedy decoding and 2-opt
is reported.
Table 3: Results on MIS problems. ∗indicates the baseline for computing the
performance drop. RL, SL, G, S, and TS denotes Reinforcement Learning,
Supervised Learning, Greedy decoding, Sampling decoding, and Tree Search,
respectively. Please refer to Sec. 5 and Appendix H for details.
METHOD
TYPE
SATLIB
ER-[700-800]
SIZE ↑
DROP ↓TIME ↓
SIZE ↑DROP ↓
TIME ↓
KAMIS
HEURISTICS 425.96∗
—
37.58m 44.87∗
—
52.13m
GUROBI
EXACT
425.95
0.00% 26.00m
41.38
7.78%
50.00m
INTEL
SL+G
420.66
1.48% 23.05m
34.86
22.31%
6.06m
INTEL
SL+TS
N/A
N/A
N/A
38.80
13.43% 20.00M
DGL
SL+TS
N/A
N/A
N/A
37.26
16.96% 22.71m
LWD
RL+S
422.22
0.88% 18.83m
41.17
8.25%
6.33m
DIMES
RL+G
421.24
1.11% 24.17m
38.24
14.78%
6.12m
DIMES
RL+S
423.28
0.63% 20.26m
42.06
6.26% 12.01m
OURS
SL+G
424.50
0.34%
8.76m
38.83
12.40%
8.80m
OURS
SL+S
425.13
0.21% 23.74m
41.12
8.36%
26.67m
lem scales. From Fig. 3, we can see that DIFUSCO has a
strong generalization ability. In particular, the model trained
with TSP-50 perform well on even TSP-1000 and TSP0-
10000. This pattern is different from the bad generalization
ability of RL-trained or SL-trained non-autoregressive meth-
ods as reported in previous work (Joshi et al., 2022).
5. Experiments with MIS
For Maximal Independent Set (MIS), we experiment on
two types of graphs that recent work (Li et al., 2018; Ahn
et al., 2020; B¨other et al., 2022; Qiu et al., 2022) shows
struggles against, i.e., SATLIB (Hoos & St¨utzle, 2000) and
Erd˝os-R´enyi (ER) graphs (Erd˝os et al., 1960). The former is
a set of graphs reduced from SAT instances in CNF, while
the latter are random graphs. We use ER-[700-800] for
evaluation, where ER-[n-N] indicates the graph contains
n to N nodes. Following Qiu et al. (2022), the pairwise
connection probability p is set to 0.15.
Datasets
The training instances of labeled by the KaMIS2
heuristic solver. The split of test instances on SAT datasets
and the random-generated ER test graphs are taken from
Qiu et al. (2022).
Model Settings
The training schedule is the same as the
TSP solver (Sec. 4.1). For SATLIB, we use discrete dif-
fusion with 50 (diffusion steps) × 1 (samples) policy and
50 (diffusion steps)×4 (samples) policy as the Greedy and
Sampling strategies, respectively. For ER graphs, we use
continuous diffusion with 50 (diffusion steps)×1 (samples)
policy and 20 (diffusion steps) × 8 (samples) policy as the
Greedy and Sampling strategies, respectively.
2https://github.com/KarlsruheMIS/KaMIS
(MIT License)
Evaluation Metrics
We report the average size of the in-
dependent set (Size), average performance drop (Drop), and
latency time (Time). The detailed description can be found
in Appendix D. Notice that we disable graph reduction and
2-opt local search in all models for a fair comparison since it
is pointed out by (B¨other et al., 2022) that all models would
perform similarly with local search post-processing.
Results and Analysis
Tab. 3 compare discrete DIFUSCO
with other baselines on SATLIB and ER-[700-800] bench-
marks. We can see that DIFUSCO strongly outperforms
previous state-of-the-art methods on SATLIB benchmark,
reducing the gap between ground-truth and neural solvers
from 0.63% to 0.21%. However, we also found that DI-
FUSCO (especially with discrete diffusion in our prelimi-
nary experiments) does not perform well on the ER-[700-
800] data. We hypothesize that this is because the previous
methods usually use the node-based graph neural networks
such as GCN (Kipf & Welling, 2017) or GraphSage (Hamil-
ton et al., 2017) as the backbone network, while we use an
edge-based Anisotropic GNN (Sec. 3.4), whose inductive
bias may be not suitable for ER graphs.
6. Concluding Remarks
We proposed DIFUSCO, a novel graph-based diffusion
model for solving NP-complete combinatorial optimiza-
tion problems. We compared two variants of graph-based
diffusion models: one with continuous Gaussian noise and
one with discrete Bernoulli noise. We found that the discrete
variant performs better than the continuous one. Moreover,
we designed a cosine inference schedule that enhances
the effectiveness of our model. DIFUSCO achieves state-
of-the-art results on TSP and MIS problems, surpassing
previous probabilistic NPC solvers in both accuracy and
scalability.
For future work, we would like to explore the potential of

Graph-based Diffusion Solvers for Combinatorial Optimization
DIFUSCO in solving a broader range of NPC problems,
including Mixed Integer Programming (Appendix C). We
would also like to explore the use of equivariant graph neural
networks (Xu et al., 2021; Hoogeboom et al., 2022) for
further improvement of the diffusion models on geometrical
NP-complete combinatorial optimization problems such as
Euclidean TSP. Finally, we are interested in utilizing (higher-
order) accelerated inference techniques for diffusion model-
based solvers, such as those inspired by the continuous time
framework for discrete diffusion (Campbell et al., 2022; Sun
et al., 2022).
References
Ahn, S., Seo, Y., and Shin, J. Learning what to defer for
maximum independent sets. In International Conference
on Machine Learning, pp. 134–144. PMLR, 2020.
Andrade, D. V., Resende, M. G., and Werneck, R. F. Fast
local search for the maximum independent set problem.
Journal of Heuristics, 18(4):525–547, 2012.
Applegate,
D.,
Bixby,
R.,
Chvatal,
V.,
and Cook,
W.
Concorde TSP solver.
https://www.math.
uwaterloo.ca/tsp/concorde/index.html,
2006.
Arora, S. Polynomial time approximation schemes for eu-
clidean tsp and other geometric problems. In Proceedings
of 37th Conference on Foundations of Computer Science,
pp. 2–11. IEEE, 1996.
Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den
Berg, R. Structured denoising diffusion models in discrete
state-spaces. Advances in Neural Information Processing
Systems, 34:17981–17993, 2021.
Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio,
S. Neural combinatorial optimization with reinforcement
learning. arXiv preprint arXiv:1611.09940, 2016.
Bi, J., Ma, Y., Wang, J., Cao, Z., Chen, J., Sun, Y., and
Chee, Y. M. Learning generalizable models for vehicle
routing problems via knowledge distillation. In Advances
in Neural Information Processing Systems, 2022.
B¨other, M., Kißig, O., Taraz, M., Cohen, S., Seidel, K.,
and Friedrich, T.
What’s wrong with deep learning
in tree search for combinatorial optimization.
In In-
ternational Conference on Learning Representations,
2022. URL https://openreview.net/forum?
id=mk0HzdqY7i1.
Bresson, X. and Laurent, T. An experimental study of neural
networks for variable graphs. 2018.
Bresson, X. and Laurent, T.
The transformer network
for the traveling salesman problem.
arXiv preprint
arXiv:2103.03012, 2021.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.
Campbell, A., Benton, J., Bortoli, V. D., Rainforth, T.,
Deligiannidis, G., and Doucet, A. A continuous time
framework for discrete denoising models.
In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
Advances in Neural Information Processing Systems,
2022. URL https://openreview.net/forum?
id=DmT862YAieY.
Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,
W. T. Maskgit: Masked generative image transformer. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 11315–11325, 2022.
Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and
Chan, W. Wavegrad: Estimating gradients for waveform
generation. In International Conference on Learning
Representations, 2020.
Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M.,
Dehak, N., and Chan, W.
Wavegrad 2: Iterative re-
ﬁnement for text-to-speech synthesis.
arXiv preprint
arXiv:2106.09660, 2021.
Chen, T., Zhang, R., and Hinton, G. Analog bits: Gen-
erating discrete data using diffusion models with self-
conditioning. arXiv preprint arXiv:2208.04202, 2022.
Chen, X. and Tian, Y. Learning to perform local rewrit-
ing for combinatorial optimization. Advances in Neural
Information Processing Systems, 32, 2019.
Choo, J., Kwon, Y.-D., Kim, J., Jae, J., Hottung, A., Tierney,
K., and Gwon, Y. Simulation-guided beam search for
neural combinatorial optimization. In Oh, A. H., Agarwal,
A., Belgrave, D., and Cho, K. (eds.), Advances in Neural
Information Processing Systems, 2022. URL https:
//openreview.net/forum?id=tYAS1Rpys5.
Croes, G. A.
A method for solving traveling-salesman
problems. Operations research, 6(6):791–812, 1958.
d O Costa, P. R., Rhuggenaath, J., Zhang, Y., and Akcay, A.
Learning 2-opt heuristics for the traveling salesman prob-
lem via deep reinforcement learning. In Asian Conference
on Machine Learning, pp. 465–480. PMLR, 2020.
da Costa, P. R. d. O., Rhuggenaath, J., Zhang, Y., and Akcay,
A. Learning 2-OPT heuristics for the traveling salesman

Graph-based Diffusion Solvers for Combinatorial Optimization
problem via deep reinforcement learning. arXiv preprint
arXiv:2004.01608, 2020.
Deudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., and
Rousseau, L.-M. Learning heuristics for the TSP by
policy gradient. In International conference on the inte-
gration of constraint programming, artiﬁcial intelligence,
and operations research, pp. 170–181. Springer, 2018.
Dhariwal, P. and Nichol, A. Diffusion models beat gans
on image synthesis. Advances in Neural Information
Processing Systems, 34:8780–8794, 2021.
Dieleman, S., Sartran, L., Roshannai, A., Savinov, N.,
Ganin, Y., Richemond, P. H., Doucet, A., Strudel, R.,
Dyer, C., Durkan, C., et al. Continuous diffusion for
categorical data. arXiv preprint arXiv:2211.15089, 2022.
Drori, I., Kharkar, A., Sickinger, W. R., Kates, B., Ma, Q.,
Ge, S., Dolev, E., Dietrich, B., Williamson, D. P., and
Udell, M. Learning to solve combinatorial optimization
problems on real-world graphs in linear time. In 2020
19th IEEE International Conference on Machine Learn-
ing and Applications (ICMLA), pp. 19–24. IEEE, 2020.
Erd˝os, P., R´enyi, A., et al. On the evolution of random
graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17–60,
1960.
Fu, Z.-H., Qiu, K.-B., and Zha, H. Generalize a small
pre-trained model to arbitrarily large tsp instances. In
Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 35, pp. 7474–7482, 2021.
Geisler, S., Sommer, J., Schuchardt, J., Bojchevski, A., and
G¨unnemann, S. Generalization of neural combinatorial
solvers through the lens of adversarial robustness. In
International Conference on Learning Representations,
2022. URL https://openreview.net/forum?
id=vJZ7dPIjip3.
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and
Dahl, G. E. Neural message passing for quantum chem-
istry. In International conference on machine learning,
pp. 1263–1272. PMLR, 2017.
Gong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:
Sequence to sequence text generation with diffusion mod-
els. arXiv preprint arXiv:2210.08933, 2022.
Gonzalez, T. F. Handbook of approximation algorithms and
metaheuristics. Chapman and Hall/CRC, 2007.
Graikos, A., Malkin, N., Jojic, N., and Samaras, D.
Diffusion models as plug-and-play priors.
In Thirty-
Sixth Conference on Neural Information Processing
Systems, 2022. URL https://arxiv.org/pdf/
2206.09012.pdf.
Gu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R.
Non-autoregressive neural machine translation. In Inter-
national Conference on Learning Representations, 2018.
Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen,
D., Yuan, L., and Guo, B. Vector quantized diffusion
model for text-to-image synthesis. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 10696–10706, 2022.
Gurobi Optimization, L. Gurobi optimizer reference manual,
2018.
Hamilton, W., Ying, Z., and Leskovec, J. Inductive repre-
sentation learning on large graphs. Advances in neural
information processing systems, 30, 2017.
Han, X., Kumar, S., and Tsvetkov, Y.
Ssd-lm: Semi-
autoregressive simplex-based diffusion language model
for text generation and modular control. arXiv preprint
arXiv:2210.17432, 2022.
He, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-
sionbert: Improving generative masked language models
with diffusion models. arXiv preprint arXiv:2211.15029,
2022.
Helsgaun, K. An extension of the Lin-Kernighan-Helsgaun
TSP solver for constrained traveling salesman and vehicle
routing problems. Technical report, Roskilde University,
2017.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-
bilistic models. Advances in Neural Information Process-
ing Systems, 33:6840–6851, 2020.
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,
A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,
et al. Imagen video: High deﬁnition video generation
with diffusion models. arXiv preprint arXiv:2210.02303,
2022a.
Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and
Salimans, T. Cascaded diffusion models for high ﬁdelity
image generation. J. Mach. Learn. Res., 23:47–1, 2022b.
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,
and Fleet, D. J. Video diffusion models. arXiv preprint
arXiv:2204.03458, 2022c.
Hoogeboom, E., Nielsen, D., Jaini, P., Forr´e, P., and Welling,
M. Argmax ﬂows and multinomial diffusion: Learning
categorical distributions. Advances in Neural Information
Processing Systems, 34:12454–12465, 2021.
Hoogeboom, E., Satorras, V. G., Vignac, C., and Welling,
M. Equivariant diffusion for molecule generation in 3d.
In International Conference on Machine Learning, pp.
8867–8887. PMLR, 2022.

Graph-based Diffusion Solvers for Combinatorial Optimization
Hoos, H. H. and St¨utzle, T. SATLIB: An online resource
for research on SAT. Sat, 2000:283–292, 2000.
Hottung, A. and Tierney, K. Neural large neighborhood
search for the capacitated vehicle routing problem. arXiv
preprint arXiv:1911.09539, 2019.
Hottung, A., Kwon, Y.-D., and Tierney, K. Efﬁcient active
search for combinatorial optimization problems. arXiv
preprint arXiv:2106.05126, 2021.
Hudson, B., Li, Q., Malencia, M., and Prorok, A. Graph
neural network guided local search for the traveling sales-
person problem. In International Conference on Learning
Representations, 2022. URL https://openreview.
net/forum?id=ar92oEosBIg.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In International conference on machine learning, pp. 448–
456. PMLR, 2015.
Johnson, D. D., Austin, J., Berg, R. v. d., and Tarlow,
D.
Beyond in-place corruption: Insertion and dele-
tion in denoising probabilistic models. arXiv preprint
arXiv:2107.07675, 2021.
Joshi, C. K., Laurent, T., and Bresson, X. An efﬁcient
graph convolutional network technique for the travelling
salesman problem. arXiv preprint arXiv:1906.01227,
2019.
Joshi, C. K., Cappart, Q., Rousseau, L.-M., and Laurent,
T. Learning the travelling salesperson problem requires
rethinking generalization. Constraints, pp. 1–29, 2022.
Karalias, N. and Loukas, A. Erdos goes neural: an unsuper-
vised learning framework for combinatorial optimization
on graphs. Advances in Neural Information Processing
Systems, 33:6659–6672, 2020.
Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating
the design space of diffusion-based generative models.
arXiv preprint arXiv:2206.00364, 2022.
Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song,
L. Learning combinatorial optimization algorithms over
graphs. Advances in neural information processing sys-
tems, 30, 2017.
Kim, M., Park, J., et al. Learning collaborative policies
to solve NP-hard routing problems. Advances in Neural
Information Processing Systems, 34, 2021.
Kim, M., Park, J., and Park, J. Sym-NCO: Leveraging sym-
metricity for neural combinatorial optimization. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
Advances in Neural Information Processing Systems,
2022. URL https://openreview.net/forum?
id=kHrE2vi5Rvs.
Kingma, D., Salimans, T., Poole, B., and Ho, J. Varia-
tional diffusion models. Advances in neural information
processing systems, 34:21696–21707, 2021.
Kipf, T. N. and Welling, M. Semi-supervised classiﬁca-
tion with graph convolutional networks. arXiv preprint
arXiv:1609.02907, 2016.
Kipf, T. N. and Welling, M.
Semi-supervised classi-
ﬁcation with graph convolutional networks.
In In-
ternational Conference on Learning Representations,
2017. URL https://openreview.net/forum?
id=SJU4ayYgl.
Konda, V. R. and Tsitsiklis, J. N. Actor-critic algorithms. In
Advances in neural information processing systems, pp.
1008–1014, 2000.
Kool, W., van Hoof, H., and Welling, M. Attention, learn to
solve routing problems! In International Conference on
Learning Representations, 2019a.
Kool, W., van Hoof, H., and Welling, M. Buy 4 REIN-
FORCE samples, get a baseline for free! In Deep Rein-
forcement Learning Meets Structured Prediction, ICLR
2019 Workshop, 2019b.
Krizhevsky, A. and Hinton, G. Convolutional deep belief
networks on cifar-10. Unpublished manuscript, 40(7):
1–9, 2010.
Kwon, Y.-D., Choo, J., Kim, B., Yoon, I., Gwon, Y.,
and Min, S. POMO: Policy optimization with multi-
ple optima for reinforcement learning. arXiv preprint
arXiv:2010.16011, 2020.
Kwon, Y.-D., Choo, J., Yoon, I., Park, M., Park, D., and
Gwon, Y. Matrix encoding networks for neural combi-
natorial optimization. Advances in Neural Information
Processing Systems, 34, 2021.
Li, X. L., Thickstun, J., Gulrajani, I., Liang, P., and
Hashimoto, T. Diffusion-LM improves controllable text
generation. In Oh, A. H., Agarwal, A., Belgrave, D.,
and Cho, K. (eds.), Advances in Neural Information Pro-
cessing Systems, 2022. URL https://openreview.
net/forum?id=3s9IrEsjLyk.
Li, Z., Chen, Q., and Koltun, V. Combinatorial optimization
with graph convolutional networks and guided tree search.
Advances in neural information processing systems, 31,
2018.
Lin, S. and Kernighan, B. W. An effective heuristic algo-
rithm for the traveling-salesman problem. Operations
research, 21(2):498–516, 1973.

Graph-based Diffusion Solvers for Combinatorial Optimization
Liu, J., Li, C., Ren, Y., Chen, F., and Zhao, Z. Diffsinger:
Singing voice synthesis via shallow diffusion mechanism.
In Proceedings of the AAAI Conference on Artiﬁcial In-
telligence, volume 36, pp. 11020–11028, 2022.
Liu, L., Ren, Y., Lin, Z., and Zhao, Z. Pseudo numerical
methods for diffusion models on manifolds. In Interna-
tional Conference on Learning Representations, 2021.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
Dpm-solver: A fast ode solver for diffusion probabilis-
tic model sampling in around 10 steps. arXiv preprint
arXiv:2206.00927, 2022a.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-
solver++: Fast solver for guided sampling of diffusion
probabilistic models. arXiv preprint arXiv:2211.01095,
2022b.
Lu, H., Zhang, X., and Yang, S. A learning-based iter-
ative method for solving vehicle routing problems. In
International Conference on Learning Representations,
2020.
Luo, S., Su, Y., Peng, X., Wang, S., Peng, J., and Ma,
J. Antigen-speciﬁc antibody design and optimization
with diffusion-based generative models for protein struc-
tures. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,
K. (eds.), Advances in Neural Information Processing
Systems, 2022. URL https://openreview.net/
forum?id=jSorGn2Tjg.
Ma, Q., Ge, S., He, D., Thaker, D., and Drori, I. Com-
binatorial optimization by graph pointer networks and
hierarchical reinforcement learning.
arXiv preprint
arXiv:1911.04936, 2019.
Ma, Y., Li, J., Cao, Z., Song, W., Zhang, L., Chen, Z., and
Tang, J. Learning to iteratively solve routing problems
with dual-aspect collaborative transformer. Advances
in Neural Information Processing Systems, 34:11096–
11107, 2021.
Malherbe, C., Grosnit, A., Tutunov, R., Ammar, H. B.,
and Wang, J. Optimistic tree searches for combinato-
rial black-box optimization. In Oh, A. H., Agarwal, A.,
Belgrave, D., and Cho, K. (eds.), Advances in Neural
Information Processing Systems, 2022. URL https:
//openreview.net/forum?id=JGLW4DvX11F.
Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,
S. Sdedit: Image synthesis and editing with stochastic
differential equations. arXiv preprint arXiv:2108.01073,
2021.
Nair, V., Bartunov, S., Gimeno, F., von Glehn, I., Li-
chocki, P., Lobov, I., O’Donoghue, B., Sonnerat, N.,
Tjandraatmadja, C., Wang, P., et al. Solving mixed in-
teger programs using neural networks. arXiv preprint
arXiv:2012.13349, 2020.
Nazari, M., Oroojlooy, A., Snyder, L., and Tak´ac, M. Rein-
forcement learning for solving the vehicle routing prob-
lem. Advances in neural information processing systems,
31, 2018.
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,
P., McGrew, B., Sutskever, I., and Chen, M.
Glide:
Towards photorealistic image generation and editing
with text-guided diffusion models.
arXiv preprint
arXiv:2112.10741, 2021.
Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion
probabilistic models. In International Conference on
Machine Learning, pp. 8162–8171. PMLR, 2021.
Niu, C., Song, Y., Song, J., Zhao, S., Grover, A., and Ermon,
S. Permutation invariant graph generation via score-based
generative modeling. In International Conference on Ar-
tiﬁcial Intelligence and Statistics, pp. 4474–4484. PMLR,
2020.
Ouyang, W., Wang, Y., Han, S., Jin, Z., and Weng, P. Im-
proving generalization of deep reinforcement learning-
based tsp solvers.
arXiv preprint arXiv:2110.02843,
2021.
Papadimitriou, C. H. and Steiglitz, K. Combinatorial opti-
mization: algorithms and complexity. Courier Corpora-
tion, 1998.
Park, J., Chun, J., Kim, S. H., Kim, Y., and Park, J. Learning
to schedule job-shop problems: representation and policy
learning using graph neural network and reinforcement
learning. International Journal of Production Research,
59(11):3360–3377, 2021.
Peng, B., Wang, J., and Zhang, Z. A deep reinforcement
learning algorithm using dynamic attention model for
vehicle routing problems. In International Symposium on
Intelligence Computation and Applications, pp. 636–650.
Springer, 2019.
Qiu, R., Sun, Z., and Yang, Y. Dimes: A differentiable meta
solver for combinatorial optimization problems. In Ad-
vances in Neural Information Processing Systems, 2022.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,
M. Hierarchical text-conditional image generation with
clip latents. arXiv preprint arXiv:2204.06125, 2022.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp.
10684–10695, 2022.

Graph-based Diffusion Solvers for Combinatorial Optimization
Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,
T., Fleet, D., and Norouzi, M. Palette: Image-to-image
diffusion models. In ACM SIGGRAPH 2022 Conference
Proceedings, pp. 1–10, 2022a.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,
E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,
Lopes, R. G., et al. Photorealistic text-to-image diffusion
models with deep language understanding. arXiv preprint
arXiv:2205.11487, 2022b.
Shaw, P.
A new local search algorithm providing high
quality solutions to vehicle routing problems.
APES
Group, Dept of Computer Science, University of Strath-
clyde, Glasgow, Scotland, UK, 46, 1997.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
nature, 529(7587):484–489, 2016.
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S.,
Hu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-a-
video: Text-to-video generation without text-video data.
arXiv preprint arXiv:2209.14792, 2022.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli, S. Deep unsupervised learning using nonequi-
librium thermodynamics. In International Conference on
Machine Learning, pp. 2256–2265. PMLR, 2015.
Song, J., Meng, C., and Ermon, S.
Denoising diffu-
sion implicit models. In International Conference on
Learning Representations, 2021a.
URL https://
openreview.net/forum?id=St1giarCHLP.
Song, Y. and Ermon, S. Generative modeling by estimating
gradients of the data distribution. Advances in Neural
Information Processing Systems, 32, 2019.
Song, Y. and Ermon, S. Improved techniques for train-
ing score-based generative models. Advances in neural
information processing systems, 33:12438–12448, 2020.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-
mon, S., and Poole, B. Score-based generative modeling
through stochastic differential equations. In International
Conference on Learning Representations, 2021b.
Sun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-
based continuous-time discrete diffusion models. arXiv
preprint arXiv:2211.16750, 2022.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.
Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
Li`o, P., and Bengio, Y. Graph attention networks. In
International Conference on Learning Representations,
2018.
Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher,
V., and Frossard, P. Digress: Discrete denoising diffusion
for graph generation. arXiv preprint arXiv:2209.14734,
2022.
Wang, C., Yang, Y., Slumbers, O., Han, C., Guo, T., Zhang,
H., and Wang, J. A game-theoretic approach for improv-
ing generalization ability of TSP solvers. arXiv preprint
arXiv:2110.15105, 2021a.
Wang, R., Hua, Z., Liu, G., Zhang, J., Yan, J., Qi, F., Yang,
S., Zhou, J., and Yang, X. A bi-level framework for
learning to solve combinatorial optimization on graphs.
arXiv preprint arXiv:2106.04927, 2021b.
Williams, R. J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning, 8(3):229–256, 1992.
Wu, L., Gong, C., Liu, X., Ye, M., and Liu, Q. Diffusion-
based molecule generation with informative prior bridges.
arXiv preprint arXiv:2209.00865, 2022.
Wu, Y., Song, W., Cao, Z., Zhang, J., and Lim, A. Learn-
ing improvement heuristics for solving routing problems..
IEEE transactions on neural networks and learning sys-
tems, 2021.
Xin, L., Song, W., Cao, Z., and Zhang, J. Multi-decoder
attention model with embedding glimpse for solving ve-
hicle routing problems. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, volume 35, pp. 12042–
12049, 2021a.
Xin, L., Song, W., Cao, Z., and Zhang, J. NeuroLKH:
Combining deep learning model with Lin–Kernighan–
Helsgaun heuristic for solving the traveling salesman
problem. Advances in Neural Information Processing
Systems, 34, 2021b.
Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful
are graph neural networks? In International Conference
on Learning Representations, 2019. URL https://
openreview.net/forum?id=ryGs6iA5Km.
Xu, M., Yu, L., Song, Y., Shi, C., Ermon, S., and Tang,
J. Geodiff: A geometric diffusion model for molecular
conformation generation. In International Conference on
Learning Representations, 2021.
Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y.,
and Yu, D. Diffsound: Discrete diffusion model for text-
to-sound generation. arXiv preprint arXiv:2207.09983,
2022.

Graph-based Diffusion Solvers for Combinatorial Optimization
Yolcu, E. and P´oczos, B. Learning local search heuristics
for boolean satisﬁability. In NeurIPS, pp. 7990–8001,
2019.
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z.,
Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scal-
ing autoregressive models for content-rich text-to-image
generation. Transactions on Machine Learning Research.
Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang,
H., Hauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I.,
et al.
Magvit: Masked generative video transformer.
arXiv preprint arXiv:2212.05199, 2022.
Zhang, C., Song, W., Cao, Z., Zhang, J., Tan, P. S., and Xu,
C. Learning to dispatch for job shop scheduling via deep
reinforcement learning. arXiv preprint arXiv:2010.12367,
2020.

Graph-based Diffusion Solvers for Combinatorial Optimization
(a)
(b)
Figure 4: The performance Drop (%) of continuous diffusion (a) and discrete diffusion (b) models on TSP-50 with different diffusion
steps and number of samples. (c): The results are reported with greedy decoding without 2-opt post-processing.
(a)
(b)
(c)
Figure 5: The inference per-instance run-time (sec) of diffusion models on TSP-50, where the total run-time is decomposed to neural
network (a) + greedy decoding (b) + 2-opt (c).
A. Additional Related Work
Autoregressive Constructive Solvers
Since Bello et al. (2016) proposed the ﬁrst autoregressive CO solver, more advanced
models have been developed in the years since (Deudon et al., 2018; Kool et al., 2019a; Peng et al., 2019; Drori et al., 2020;
Kwon et al., 2021), including better network backbones (Vaswani et al., 2017; Kool et al., 2019a; Bresson & Laurent, 2018)),
more advanced deep reinforcement learning algorithms (Khalil et al., 2017; Ma et al., 2019; Kool et al., 2019b; Kwon et al.,
2020; Ouyang et al., 2021; Xin et al., 2021a; Wang et al., 2021a; Choo et al., 2022), improved training strategies (Kim
et al., 2022; Bi et al., 2022), and for a wider range of NPC problems such as Capacitated Vehicle Routing Problem (CVRP)
(Nazari et al., 2018), Job Shop Scheduling Problem (JSSP) (Zhang et al., 2020; Park et al., 2021), Maximal Independent
Set (MIS) problem (Khalil et al., 2017; Ahn et al., 2020; Malherbe et al., 2022; Qiu et al., 2022), and boolean satisﬁability
problem (SAT) (Yolcu & P´oczos, 2019).
Improvement Heuristics Solvers
Unlike construction heuristics, DRL-based improvement heuristics solvers use neural
networks to iteratively enhance the quality of the current solution until the computational budget is exhausted. Such
DRL-based improvement heuristics methods are usually inspired by classical local search algorithms such as 2-opt (Croes,
1958) and the large neighborhood search (LNS) (Shaw, 1997), and have been demonstrated with outstanding results by
many previous works (Chen & Tian, 2019; Hottung & Tierney, 2019; da Costa et al., 2020; d O Costa et al., 2020; Xin et al.,
2021b; Ma et al., 2021; Wu et al., 2021; Lu et al., 2020; Wang et al., 2021b; Kim et al., 2021; Hudson et al., 2022).
Improvement heuristics methods, while showing superior performance compared to construction heuristics methods, come at
the cost of increased computational time, often requiring thousands of actions even for small-scale problems with hundreds
of nodes (d O Costa et al., 2020; Wang et al., 2021b). This is due to the sequential application of local operations, such
as 2-opt, on existing solutions, resulting in a bottleneck for latency. On the other hand, DIFUSCO has the advantage of
denoising all variables in parallel, which leads to a reduction in the number of network evaluations required.

Graph-based Diffusion Solvers for Combinatorial Optimization
(a)
(b)
Figure 6: Generalization tests of discrete DIFUSCO trained and evaluated on TSP problems across various scales. The results are reported
with (a) and without (b) 2-opt post-processing.
Table 4: Comparing discrete diffusion and continuous diffusion on TSP-100 with various diffusion steps and number of parallel sampling.
cosine schedule is used for fast sampling.
DIFFUSION STEPS
#SAMPLE
DISCRETE DIFFUSION (Drop%)
CONTINUOUS DIFFUSION (Drop%)
PER-INSTANCE RUNTIME (sec)
W/ 2OPT
W/O 2OPT
W/ 2OPT
W/O 2OPT
NN
GD
2-OPT
50
1
0.23869
1.45574
1.46146
7.66379
0.50633
0.00171
0.00210
100
1
0.23366
1.48161
1.32573
7.02117
1.00762
0.00170
0.00207
50
4
0.02253
0.09280
0.42741
1.65264
1.52401
0.00643
0.00575
10
16
-0.01870
0.00519
0.13015
0.54983
1.12550
0.02581
0.02228
50
16
-0.02322
-0.00699
0.09407
0.30712
5.63712
0.02525
0.02037
Continuous Diffusion Models
Diffusion models were ﬁrst proposed by Sohl-Dickstein et al. (2015) and recently achieved
impressive success on various tasks, such as high-resolution image synthesis (Dhariwal & Nichol, 2021; Ho et al., 2022b),
image editing (Meng et al., 2021; Saharia et al., 2022a), text-to-image generation (Nichol et al., 2021; Saharia et al., 2022b;
Rombach et al., 2022; Ramesh et al., 2022; Gu et al., 2022), waveform generation (Chen et al., 2020; 2021; Liu et al., 2022),
video generation (Ho et al., 2022c;a; Singer et al., 2022), and molecule generation (Xu et al., 2021; Hoogeboom et al., 2022;
Wu et al., 2022).
Recent works have also drawn connections to stochastic differential equations (SDEs) (Song et al., 2021b) and ordinary
differential equations (ODEs) (Song et al., 2021a) in a continuous time framework, leading to improved sampling algorithms
by solving discretized SDEs/ODEs with higher-order solvers (Liu et al., 2021; Lu et al., 2022b;a) or implicit diffusion (Song
et al., 2021a).
B. Additional Results
Discrete Diffusion v.s. Continuous Diffusion on TSP-100
We also compare discrete diffusion and continuous diffusion
on the TSP-100 benchmark and report the results in Tab. 4. We can see that on TSP-100, discrete diffusion models still
consistently outperform their continuous counterparts in various settings.
More Diffusion Steps v.s. More Sampling (w/o 2-opt)
Fig. 4 report the results of continuous diffusion and discrete
diffusion with various diffusion steps and numbers of parallel sampling, without using 2-opt post-processing. The cosine
denoising schedule is used for fast inference. Again, we ﬁnd that discrete diffusion outperforms continuous diffusion across
various settings.
Besides, we ﬁnd that without the 2-opt post-processing, performing more diffusion iterations is much more effective than
sampling more solutions, even when the former uses less computation. For example, 20 (diffusion steps) × 4 (samples) not
only signiﬁcantly outperforms 1 (diffusion steps) × 1024 (samples), but also has a 18.5× less runtime.

Graph-based Diffusion Solvers for Combinatorial Optimization
Runtime Analysis
We report the decomposed runtime (neural network + greedy decoding + 2-opt) for diffusion models
on TSP-50 in Fig. 5. We can see that while neural network execution takes the majority of total runtime, 2-opt also takes a
non-negligible portion of the runtime, especially when only a few diffusion steps (like 1 or 2) are used.
Generalization Tests (w/o 2opt)
We also report the generalization tests of discrete DIFUSCO without 2-opt post-
processing in Fig.6 (b).
C. Discussion on the {0, 1}N Vector Space of CO Problems
The design of the {0, 1}N vector space can also represent non-graph-based NP-complete combinatorial optimization
problems. For example, on the more general Mixed Integer Programming (MIP) problem, we can let Xs be a 0/1 indication
set of all extended variables3. cost(·) can be deﬁned as a linear/quadratic function of x, and valid(·) is a function validating
all linear/quadratic constraints, bound constraints, and integrality constraints.
D. Additional Experiment Details
Metrics: TSP
For TSP, Length is deﬁned as the average length of the system-predicted tour for each test-set instance.
Drop is the average of the relative decrease in performance compared to a baseline method. Time is the total clock time
required to generate solutions for all test instances, and is presented in seconds (s), minutes (m), or hours (h).
Metrics: MIS
For MIS, Size (the larger, the better) is the average size of the system-predicted maximal independent set
for each test-set graph, Drop and Time are deﬁned similarly as in the TSP case.
Hardware
All the methods are trained with 8× NVIDIA Tesla V100 Volta GPUs and evaluated on a single NVIDIA
Tesla V100 Volta GPU, with 40× Intel(R) Xeon(R) Gold 6248 CPUs @ 2.50GHz.
Random Seeds
Since the diffusion models can generate an arbitrary sample from its distribution even with the greedy
decoding scheme, we report the averaged results across 5 different random seeds when reporting all results.
Training Details
All DIFUSCO models are trained with a cosine learning rate schedule starting from 2e-4 and ending at
0.
• TSP-50: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 512.
• TSP-100: We use 1502000 random instances and train DIFUSCO for 50 epochs with a batch size of 256.
• TSP-500: We use 128000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply
curriculum learning and initialize the model from the TSP-100 checkpoint.
• TSP-1000: We use 64000 random instances and train DIFUSCO for 50 epochs with a batch size of 64. We also apply
curriculum learning and initialize the model from the TSP-100 checkpoint.
• TSP-10000: We use 6400 random instances and train DIFUSCO for 50 epochs with a batch size of 8. We also apply
curriculum learning and initialize the model from the TSP-500 checkpoint.
• SATLIB: We use the training split of 49500 examples from (Hoos & St¨utzle, 2000; Qiu et al., 2022) and train DIFUSCO
for 50 epochs with a batch size of 128.
• ER-[700-800]: We use 163840 random instances and train DIFUSCO for 50 epochs with a batch size of 32.
E. Decoding Strategies
Greedy Decoding
We use a simple greedy decoding scheme for diffusion models, where we sample one solution from
the learned distribution pθ(x0). We compare this scheme with other autoregressive constructive solvers that also use greedy
decoding.
Sampling
Following Kool et al. (2019a), we also use a sampling scheme where we sample multiple solutions in parallel
(i.e., each diffusion model starts with a different noise xT) and report the best one.
3For an integer variable z that can be assigned values from a ﬁnite set with cardinality card(z), any target value can be represented as
a sequence of ⌈log2(card(z))⌉bits (Nair et al., 2020; Chen et al., 2022).

Graph-based Diffusion Solvers for Combinatorial Optimization
Monte Carlo Tree Search
For the TSP task, we adopt a more advanced reinforcement learning-based search approach,
i.e., Monte Carlo tree search (MCTS), to ﬁnd high-quality solutions. In MCTS, we sample k-opt transformation actions
guided by the heatmap generated by the diffusion model to improve the current solutions. The MCTS repeats the simulation,
selection, and back-propagation steps until no improving actions are found in the sampling pool. For more details, please
refer to (Fu et al., 2021).
F. Fast Inference for Continuous and Discrete Diffusion Models
We ﬁrst describe the denoising diffusion implicit models (DDIMs) (Song et al., 2021a) algorithm for accelerating inference
for continuous diffusion models.
Formally, consider the forward process deﬁned not on all the latent variables x1:T , but on a subset {xτ1, . . . , xτM }, where τ
is an increasing sub-sequence of [1, . . . , T] with length M, xτ1 = 1 and xτM = T.
For continuous diffusion, the marginal can still be deﬁned as:
q(xτi|x0) := N(xτi;
p
¯ατix0, (1 −¯ατi)I)
(15)
And it’s (deterministic) posterior is deﬁned by:
q(xτi−1|xτi, x0) := N(xτi−1;
s
¯ατi−1
¯ατi

xτi −
p
1 −¯ατi · eϵτi

+
p
1 −¯ατi−1 · eϵτi), 0)
(16)
where eϵτi = (xτi −√¯ατix0)/√1 −¯ατi is the (predicted) diffusion noise.
Next, we describe its analogy in the discrete domain, which is ﬁrst proposed by Austin et al. (2021).
For discrete diffusion, the marginal can still be deﬁned as:
q(xτi|x0) = Cat
 xτi; p = x0Qτi

,
(17)
while the posterior becomes:
q(xτi−1|xτi, x0) = q(xτi|xτi−1, x0)q(xτi−1|x0)
q(xτi|x0)
= Cat

xτi−1; p =
xτiQ
⊤
τi−1,τi ⊙x0Qτi−1
x0Qτix⊤
τi

,
(18)
where Qt′,t = Qt′+1 . . . Qt.
G. Neural Network Architecture
We adopt an anisotropic graph neural network with edge gating mechanisms (Bresson & Laurent, 2018; Joshi et al., 2022)
as the backbone network for both discrete and continuous diffusion models.
Anisotropic Graph Neural Networks
Let hℓ
i and eℓ
ij denote the node and edge features at layer ℓassociated with node i
and edge ij, respectively. t is the sinusoidal features (Vaswani et al., 2017) of denoising timestep t. The features at the next
layer is propagated with an anisotropic message passing scheme:
ˆeℓ+1
ij
= P ℓeℓ
ij + Qℓhℓ
i + Rℓhℓ
j,
eℓ+1
ij
= eℓ
ij + MLPe(BN(ˆeℓ+1
ij )) + MLPt(t),
hℓ+1
i
= hℓ
i + α(BN(U ℓhℓ
i + Aj∈Ni(σ(ˆeℓ+1
ij ) ⊙V ℓhℓ
j))),
where U ℓ, V ℓ, P ℓ, Qℓ, Rℓ∈Rd×d are the learnable parameters of layer ℓ, α denotes the ReLU (Krizhevsky & Hinton,
2010) activation, BN denotes the Batch Normalization operator (Ioffe & Szegedy, 2015), A denotes the aggregation function

Graph-based Diffusion Solvers for Combinatorial Optimization
SUM pooling (Xu et al., 2019), σ is the sigmoid function, ⊙is the Hadamard product, Ni denotes the neighborhoods of
node i, and MLP(·) denotes a 2-layer multi-layer perceptron.
For TSP, e0
ij are initialized as the corresponding values in xt, and h0
i are initialized as sinusoidal features of the nodes.
For MIS, e0
ij are initialized as zeros, and h0
i are initialized as the corresponding values in xt. A 2-neuron and 1-neuron
classiﬁcation/regression head is applied to the ﬁnal embeddings of xt ({eij} for edges and {hi} for nodes) for discrete and
continuous diffusion models, respectively.
Hyper-parameters
For all TSP and MIS benchmarks, we use a 12-layer Anisotropic GNN with a width of 256 as
described above.
H. Experiment Baselines
TSP-50/100
We evaluate DIFUSCO on TSP-50 and TSP-100 against 10 baselines, which belong to two categories:
traditional Operations Research (OR) methods and learning-based methods.
• Traditional OR methods include Concorde (Applegate et al., 2006), an exact solver, and 2-opt (Lin & Kernighan, 1973), a
heuristic method.
• Learning-based methods include AM (Kool et al., 2019a), GCN (Joshi et al., 2019), Transformer (Bresson & Laurent,
2021), POMO (Kwon et al., 2020), Sym-NCO(Kim et al., 2022), DPDP (Ma et al., 2021), Image Diffusion (Graikos
et al., 2022), and MDAM (Xin et al., 2021a). These are the state-of-the-art methods in recent benchmark studies.
TSP-500/1000/10000
We compare DIFUSCO on large-scale TSP problems with 9 baseline methods, which fall into two
categories: traditional Operations Research (OR) methods and learning-based methods.
• Traditional OR methods include Concorde (Applegate et al., 2006) and Gurobi (Gurobi Optimization, 2018), which
are exact solvers, and LKH-3 (Helsgaun, 2017), which is a strong heuristic solver. We use two settings for LKH-3: (i)
default: 10000 trials (the default conﬁguration of LKH-3); (ii) less trials: 500 trials for TSP-500/1000 and 250 trials for
TSP-10000. We also include Farthest Insertion, a simple heuristic method, as a baseline.
• Learning-based methods include EAN (Deudon et al., 2018), AM (Kool et al., 2019a), GCN (Joshi et al., 2019),
POMO+EAS (Kwon et al., 2020; Hottung et al., 2021), Att-GCN (Fu et al., 2021), and DIMES (Qiu et al., 2022). These
are the state-of-the-art methods in recent benchmark studies. They can be further divided into reinforcement learning
(RL) and supervised learning (SL) methods. Some RL methods can also use an Active Search (AS) stage (Bello et al.,
2016) to ﬁne-tune each instance. We take the results of the baselines from Fu et al. (2021) and Qiu et al. (2022). Note that
except for Att-GCN and DIMES, the baselines are trained on small graphs and tested on large graphs.
MIS
For MIS, we compare DIFUSCO with 6 other MIS solvers on the same test sets, including two traditional OR
methods (i.e., Gurobi and KaMIS) and four learning-based methods. Gurobi solves MIS as an integer linear program, while
KaMIS is a heuristics solver for MIS. The four learning-based methods can be divided into the reinforcement learning (RL)
category, i.e., LwD (Ahn et al., 2020)) and DIMES (Qiu et al., 2022), and the supervised learning (SL) category, i.e., Intel
(Li et al., 2018) and DGL (B¨other et al., 2022).

Graph-based Diffusion Solvers for Combinatorial Optimization
Figure 7: Qualitative illustration of how diffusion steps affect the generation quality of continuous diffusion models. The results are
reported without any post-processing. Continuous DIFUSCO with 50 (ﬁrst row), 20 (second row), 10 (third row), and 5 (last row)
diffusion steps in cosine schedule are shown.
Figure 8: Qualitative illustration of how diffusion steps affect the generation quality of discrete diffusion models. The results are reported
without any post-processing. Discrete DIFUSCO with 50 (ﬁrst row), 20 (second row), 10 (third row), and 5 (last row) diffusion steps in
cosine schedule are shown.

Graph-based Diffusion Solvers for Combinatorial Optimization
Figure 9: Qualitative illustration of how diffusion schedules affect the generation quality of continuous diffusion models. The results are
reported without any post-processing. Continuous DIFUSCO with linear schedule (ﬁrst row) and cosine schedule (second row)
with 20 diffusion steps are shown.
Figure 10: Qualitative illustration of how diffusion schedules affect the generation quality of discrete diffusion models. The results are
reported without any post-processing. Discrete DIFUSCO with linear schedule (ﬁrst row) and cosine schedule (second row) with
20 diffusion steps are shown.
Figure 11: Qualitative illustration of discrete DIFUSCO on TSP-50, TSP-100 and TSP-500 with 50 diffusion steps and cosine schedule.

Graph-based Diffusion Solvers for Combinatorial Optimization
Figure 12: Success (left) and failure (right) examples on TSP-100, where the latter fails to form a single tour that visits each node exactly
once. The results are reported without any post-processing.

