Automatic Prompt Optimization with “Gradient Descent”
and Beam Search
Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng
Microsoft
{reidpryzant,iterdan,jerrl,yintatlee,chezhu,nzeng}@microsoft.com
Abstract
Large Language Models (LLMs) have shown
impressive performance as general purpose
agents, but their abilities remain highly de-
pendent on prompts which are hand written
with onerous trial-and-error effort.
We pro-
pose a simple and nonparametric solution to
this problem, Automatic Prompt Optimization
(APO), which is inspired by numerical gradi-
ent descent to automatically improve prompts,
assuming access to training data and an LLM
API. The algorithm uses minibatches of data
to form natural language “gradients” that crit-
icize the current prompt.
The gradients are
then “propagated” into the prompt by edit-
ing the prompt in the opposite semantic direc-
tion of the gradient. These gradient descent
steps are guided by a beam search and ban-
dit selection procedure which signiﬁcantly im-
proves algorithmic efﬁciency. Preliminary re-
sults across three benchmark NLP tasks and
the novel problem of LLM jailbreak detec-
tion suggest that Automatic Prompt Optimiza-
tion can outperform prior prompt editing tech-
niques and improve an initial prompt’s perfor-
mance by up to 31%, by using data to rewrite
vague task descriptions into more precise an-
notation instructions.
1
Introduction
Large Language Models (LLMs) trained on web-
scale text have recently demonstrated unprece-
dented abilities across a variety of NLP tasks (Ope-
nAI, 2023; Bubeck et al., 2023). These LLMs
use prompt inputs to follow human instructions.
Writing these natural language prompts remains
a manual trial-and-error process requiring signif-
icant human effort (Jiang et al., 2022) and exper-
tise (Reynolds and McDonell, 2021; Zamﬁrescu-
Pereira et al., 2023).
Accordingly, there is need for automatic or semi-
automatic procedures to help humans write the best
prompts. This would help reduce manual effort, im-
Figure 1: Overview of the proposed Automatic Prompt
Optimization (APO) framework.
prove task performance, and produce interpretable
descriptions of a cognitive decision process.
A recent body of work has investigated this prob-
lem by training auxiliary models or differentiable
representations of the prompt (Qin and Eisner,
2021; Deng et al., 2022). However, such works as-
sume access to internal state variables of the LLM
(Shin et al., 2020; Lester et al., 2021) while prac-
titioners often communicate with LLMs through
an API. Other work applies discrete manipulations
to prompts via Reinforcement Learning or LLM-
based feedback (Zhang et al., 2023; Zhou et al.,
2022). These algorithms may also require low-level
access to the LLM, produce incomprehensible out-
puts, or rely on directionless monte-carlo search
over the semantic space of prompts.
We propose Automatic Prompt Optimization
(APO), a general purpose and nonparametric
prompt optimization algorithm that connects these
arXiv:2305.03495v1  [cs.CL]  4 May 2023

two bodies of research by applying discrete im-
provements to prompts in a directed way.
Unlike prior work, we overcome the discrete op-
timization barrier by mirroring the steps of gradient
descent within a text-based Socratic dialogue (Zeng
et al., 2022), substituting differentiation with LLM
feedback and backpropagation with LLM editing.
In detail, we use minibatches of training data to
produce “gradients” in natural language, i.e., de-
scriptions of the current prompts’ ﬂaws, then edit
the current prompt in the opposite semantic direc-
tion of the gradient. These steps become the ex-
pansion part of a wider beam search over the space
of prompts, increasing algorithmic efﬁciency by
treating the problem of beam candidate selection as
an instance of the best arm identiﬁcation problem
(Audibert et al., 2010).
We then offer a preliminary case study of the
APO algorithm. We evaluate the proposed APO
framework in multiple conﬁgurations across 4 NLP
tasks, including the novel problem of LLM jail-
break detection. The results suggest that the pro-
posed algorithm can improve on the performance
of the initial prompt input by up to 31%, exceed-
ing state-of-the-art prompt learning baselines by
an average of 4-8% while relying on fewer LLM
API calls. We also demonstrate the interpretabil-
ity of the optimization process and investigate the
algorithms’ shortcomings.
2
Discrete Prompt Optimization with
Nonparametric “Gradient Descent”
The proposed Automatic Prompt Optimization
framework assumes access to an initial prompt
p0 and i.i.d. training data consisting of pairs of
input and output text (numbers, categories, sum-
maries, etc): Dtr = {(x1, y1), ..., (xn, yn)}. Note
that all prompts p are drawn from the space of
coherent natural language L.
We assume ac-
cess to a black box LLM API LLMp(x)
≈
argmaxy∈LPLLM(y|p, x), which returns a likely
text continuation y of the prompt formed by con-
catenating p and x (for example, few-shot prompt
and input example, or chatbot persona and conver-
sational history).
Within this context, our algorithm iteratively
reﬁnes the prompt p0 to produce ˆp, an ap-
proximation of the optimal prompt p∗
=
argmaxp∈L{m(p, Dte)} for some metric function
m(·) and in-domain test or development data Dte.
In the following sections, we ﬁrst introduce how
Figure 2: The text dialogue tree we use to mirror gra-
dient descent and overcome the discrete optimization
barrier. First, a feedback prompt ∆generates the gradi-
ent g from input data (x, y) and starting prompt p0 and
prediction ˆy (left). Second, an editing prompt δ applies
the gradient g to the prompt p0 to produce an improved
prompt p′ (right).
the algorithm performs textual “gradient descent”
to improve the prompts in a directed way (Section
2.1). Then the algorithm leverages these gradient
descent steps to beam search through the space
of coherent language L, guided by the gradients
during beam expansion, and efﬁcient best arm iden-
tiﬁcation during beam selection (Section 2.2).
2.1
Gradient descent with Prompts
In our setting, gradient descent refers to the pro-
cess of (1) evaluating a prompt with a batch of data,
(2) creating a local loss signal which contains in-
formation on how to improve the current prompt,
then (3) editing the prompt in the opposite seman-
tic direction of the gradient before starting the next
iteration.
We accomplish this process with a pair of static
LLM prompts, as depicted in Figure 2. The ﬁrst
prompt is for creating the loss signals (“gradients”)
and is called ∇. While the speciﬁc contents can
vary and be task-speciﬁc or task-agnostic,1 ∇must
always consider the current prompt p0, plus p0’s
behavior on a minibatch of data (particularly the
errors), and generate a natural language summary
of p0’s ﬂaws. This summary becomes the gradient
g. Just like traditional gradients which represent a
direction in parameter space that would make the
model worse, the text “gradients” g operate in the
1We use the same prompts for all tasks; see Appendix.

space of natural language to describe ﬂaws with
the current prompt.
The second prompt is called δ and while this
prompt can also vary, it must always take the gradi-
ent g and current prompt p0, then perform an edit
on p0 in the opposite semantic direction of g, i.e.
ﬁx the problems with p0 that are indicated by g.2
Unlike the traditional machine learning setting,
we do not generate a single gradient or edit, but
rather a number of directions that may improve
the current prompt. Section 2.2 describes in detail
the process of generating and selecting candidate
prompts.
2.2
Beam Search over Prompts
The gradient descent steps described in Section 2.1
are used to guide a beam search over the space of
prompts. This beam search is the outer loop of
our prompt training algorithm and it is described
in Algorithm 1.
Algorithm 1 Automatic Prompt Optimization
Require: p0: initial prompt, b: beam width, r:
search depth, m: metric function
1: B0 ←{p0}
2: for i ←1 to r −1 do
3:
C ←∅
4:
for all p ∈Bi do
5:
C ←C ∪Expand(p)
6:
end for
7:
Bi+1 ←Selectb(C, m)
8: end for
9: ˆp ←argmaxp∈Brm(s)
10: return ˆp
The beam search is an iterative optimization pro-
cess where for each iteration the current prompt
is used to generate many new candidate prompts
(expansion). Next, a selection process is used to
decide which prompts are worth carrying forward
to the next iteration. This loop allows for incremen-
tal improvements and exploration over multiple
prompt candidates.
2.2.1
Expansion Step
The expansion step is used to generate many new
candidate prompts from a current prompt (Algo-
2Note that one can imagine operationalizing the concept
of learning rates or step sizes by e.g. editing δ to perform
large- or small-sized edits to p0, in this initial work we adopt
an “adaptive” step size by allowing the LLM to decide edit
size, and leave further exploration to future work.
rithm 2). It leverages the conceptual “gradient de-
scent” framework of Section 2.1, and our speciﬁc
prompts can be found in the Appendix.
First we sample a minibatch of data, run the ini-
tial prompt on these data with LLMp0, and collect
errors. Second, we plug these errors into a prompt
template ∆, which instructs the LLM to describe
the problems with p0 which could have led to these
mistakes. These natural language descriptions are
our gradients; see Figure 1 for an example.
Second, the gradients are provided to another
LLM prompt called δ, which instructs the LLM
to edit the current prompt p0 in order to ﬁx the
problems described by the gradient. In this way,
we engadge the LLMs in a recursive feedback loop
similar to the Socratic dialogues proposed by (Zeng
et al., 2022).
Last, additional candidates are generated by run-
ning the existing candidates through a paraphrasing
LLM called LLMmc, to explore the local monte
carlo search space around the new prompt candi-
dates. This prompt simply asks the LLM to gen-
erate new candidates which are worded differently
but semantically similar to their inputs.
Algorithm 2 Expand(·) - line 5 of Algorithm 1
Require: p: prompt candidate, Dtr: train data
1: Sample minibatch Dmini ⊂Dtr
2: Evaluate prompt p on minibatch Dmini and
collect errors e = {(xi, yi) : (xi, yi) ∈
Dmini ∧LLMp(xi) ̸= yi}
3: Generate gradients: g = LLM∇(p, e)
4: Use the gradients to edit the current prompt:
p′ = LLMδ(p, g, e)
5: Generate more monte-carlo successors: p′′ =
LLMmc(p′)
6: return p′ ∪p′′
2.2.2
Selection Step
Once the expansion process has stepped each candi-
date prompt into multiple possible successor candi-
dates, the selection step chooses the b most promis-
ing candidates to stay on the beam for the next
iteration.
It is expensive to evaluate each candidate prompt
on the entire training dataset (Prasad et al., 2022),
so we would like to minimize the number of such
queries. Note that this almost exactly corresponds
to the well-studied problem of best arm identiﬁca-
tion in bandit optimization (Audibert et al., 2010).
The n arms correspond to n prompt candidates,

their performance on the underlying dataset is the
hidden value of the arm, and the act of “pulling”
an arm corresponds to evaluating the prompt on a
randomly chosen data point. The goal is then to
ﬁnd the b best arms with as few pulls as possible,
and we consider the following algorithms.
UCB Bandits. Motivated by other works which
quickly estimate LLM performance Li et al. (2022);
Zhou et al. (2022), we sample a subset of prompts
according to a proposal distribution of prompt per-
formance, evaluate those prompts on a random sub-
set of data, then update the proposal distribution
based on the observed performance. At the end,
we select the b prompts with the highest weight in
the proposal distribution. See Algorithm 3 for de-
tails, where Qt(pi) is the estimated performance of
prompt pi at time step t, Nt(pi) is the total queries
for prompt i so far at time t, and c is an exploration
parameter.
Algorithm 3 Select(·) with UCB Bandits - line 7
of Algorithm 1
Require: n prompts p1, ..., pn, dataset Dtr, T
time steps, metric function m
1: Initialize: Nt(pi) ←0 for all i = 1, . . . , n
2: Initialize: Qt(pi) ←0 for all i = 1, . . . , n
3: for t = 1, . . . , T do
4:
Sample uniformly Dsample ⊂Dtr
5:
pi ←



arg maxp
n
Qt(p) + c
q
log t
Nt(p)
o
(UCB)
arg maxp
n
Qt(p) + cp
c
Nt(p)
o
(UCB E)
6:
Observe reward ri,t = m(pi, Dsample)
7:
Nt(pi) ←Nt(pi) + |Dsample|
8:
Qt(pi) ←Qt(pi) +
ri,t
Nt(pi)
9: end for
10: return SelectTopb(QT )
While a natural choice, UCB is designed primar-
ily for regret minimization (Kuleshov and Precup,
2014), whereas we wish to perform the related but
distinct task of best arm identiﬁcation. Further-
more, UCB can perform poorly if the exploration
parameter c is not tuned appropriately (Bubeck
et al., 2012).
UCB-E is a variant of UCB that corrects some of
these problems by favoring exploration, leading to
better theoretical convergence properties (Audibert
et al., 2010). However, UCB-E remains stuck with
hyperparameters like T, c, and |Dsample|.
Successive Rejects (Algorithm 4) is provably
optimal for best arm identiﬁcation (Audibert et al.,
2010), requires no hyperparameters unlike its UCB
alternatives, and is suprisingly simple. The algo-
rithm proceeds in n −1 phases, and in each phase,
maintains a set of surviving prompt candidates
Sk ⊆{p1, . . . , pn}. In the t-th phase, we evalu-
ate each candidate in St−1 on a total of nt random
data points to form an empirical estimate of the
score. Then, to form St, we drop the prompt with
the lowest score in this phase. Note that nt is com-
puted according to Equation 1 below such that it
gradually increases with T:
nt =
&
1
0.5 + PT
i=2 1/i ∗
B −T
T + 1 −t
'
(1)
where B is the total query budget.
Algorithm 4 Select(·) with Successive Rejects -
line 7 of Algorithm 1
Require: n prompts p1, ..., pn, dataset Dtr, metric
function m
1: Initialize: S0 ←{p1, . . . , pn}
2: for k = 1, . . . , n −1 do
3:
Sample Dsample ⊂Dtr, |Dsample| = nk
4:
Evaluate pi ∈Sk−1 with m(pi, Dsample)
5:
Sk ←Sk−1, excluding the prompt with the
lowest score from the previous step
6: end for
7: return Best prompt p∗∈Sn−1
In addition to the vanilla successive rejects al-
gorithm, we experiment with Successive Halving
(SH) which is more agressive as at the end of each
phrase it rejects the bottom half of prompts accord-
ing to their scores, with nk = B/(|Sk−1| log2 k)
(Karnin et al., 2013).
3
Experiments
We present a limited and preliminary case study to
demonstrate the proposed APO algorithm across
4 benchmark NLP tasks, ﬁnding that APO can ex-
ceed state-of-the-art prompt learning baselines in
terms of efﬁciency and performance.
3.1
Data
While APO could be applied to any problem such
as parsing, chatbot design or machine translation
simply by choosing different metric functions m,
we experiment on four NLP benchmark classiﬁca-
tion tasks for this initial case study. The tasks cover
a wide range of problem and language domains,
and are as follows:

Jailbreak:3 a novel task where the goal is to de-
termine whether the user input to an LLM continu-
ation API (i.e. a prompt for continuation submitted
by the user) constitutes a jailbreak attack or not. We
deﬁne jailbreak attack as a user interaction strat-
egy intended to get the AI to break its own rules.
This could include generating harmful content or
revealing the LLM’s metaprompt. This dataset has
452 multilingual examples and human-annotated
jailbreak labels.
Ethos (Mollas et al., 2020), an online English
hate speech detection dataset with 997 online com-
ments and hate speech labels.
Liar (Wang, 2017), an English fake news detec-
tion dataset with 4000 statements, context, and lie
labels.
Sarcasm (Farha and Magdy, 2020) an Arabic
sarcasm detection dataset with 10,000 online com-
ments and sarcasm labels.
3.2
Setup
For each task, we randomly sample 50 examples for
development and 150 for test. All of the reported
results are an average of 3 experimental trials. We
report binary F1 score throughout. Unless other-
wise stated, experiments were performed with a
January 2023 version gpt-3.5-turbo, using the
Azure OpenAI LLM API service with a tempera-
ture of 0.0 during few-shot classiﬁcation and 1.0 in
all other contexts.
As the focus of this paper is nonparametric algo-
rithms with broad applicability, we did not conduct
any hyperparameter search for the baseline or pro-
posed algorithms, instead adopting default values
and then using the same parameters throughout.
Unless otherwise stated, for the proposed Auto-
matic Prompt Optimization Algorithm we used a
minibatch size of 64, beam size of 4, and ran the
algorithm for 6 optimization steps. Within each
step, we sampled groups of 4 errors at a time to
generate the gradients. We generated 4 gradients
per error group, and edited the prompt once per
gradient before generating an additional 2 monte
carlo samples per new prompt candidate. To avoid
computational overruns, we randomly sampled 8
successor candidates per parent prompt before ban-
dit selection.
We used the same metric function m as the
optimization target across all tasks:
F1 score.
While recent works have opted to use the model’s
3Data release forthcoming.
log-likelihood to evaluate prompts instead of an
accuracy-related metric (Lu et al., 2021; Prasad
et al., 2022; Zhou et al., 2022), preliminary ex-
periments showed this technique did not help our
algorithm, and many of the most powerful LLM
APIs like ChatGPT and GPT4 did not provide log
likelihoods at the time of writing.
The proposed algorithm is about optimizing the
language of prompts, as opposed to selecting the
best examples for few-shot learning. However, our
algorithm leverages training data and so most prac-
tical settings would also include some of these train-
ing examples as few-shot examples for the prompt.
Accordingly, all of the experiments of Section 3.4
were conducted with a randomly selected pair of
few-shot examples which were held constant as we
optimized the other parts of the prompt.
3.3
Baselines
We compare the proposed APO framework against
the following baselines. Note that for this prelimi-
nary case study, we restrict our focus to nonpara-
metric algorithms that are directly comparable to
APO.
• Monte-Carlo (MC). The Automatic Prompt
Engineering algorithm proposed by Zhou et al.
(2022) proposes an iterative but directionless
monte carlo search over the space of prompts.
For fair comparison, we matched the number
of monte carlo samples per candidate to the
number of successors generated by APO.
• Reinforcement Learning (RL). Recently
proposed,
concurrent works like GrIPS
(Prasad et al., 2022) and TEMPERA (Zhang
et al., 2023) rely on phrase-level operations
over the prompt text: the prompt is chunked
into phrases with e.g. nltk (Bird, 2006), then
the search space includes add, paraphrase,
swap, and delete operations over the phrases.
Again, we matched the number of successors
for fair comparison.
• AutoGPT.4 This is an open-source AI agent
which relies on an agent-controlled feedback
loop to improve its responses. Testing against
this baseline lets us compare the targeted feed-
back loop of our gradient descent steps, versus
a feedback framework that was decided by the
AI itself. We supplied the same number of
4https://news.agpt.co/

Figure 3: Test performance (F1) vs number of optimization steps of the APO algorithm across 4 tasks.
examples and errors to AutoGPT for 6 turns,
the same as the number of optimization steps
in APO.
Last, since concurrent works have proposed to
evolutionary search through the space of prompts
(Xu et al., 2022), our primary baseline for the pro-
posed bandit selection procedure is an evolutionary
search leveraging a simple uniform selection step,
where the query budget is spread evenly among
prompt candidates (Prasad et al., 2022).
3.4
Experimental Results
Overall Results. Figure 3 presents our main re-
sults. The results suggest that APO can outper-
form other state-of-the-art algorithms on all four
datasets considered in the study. On average, APO
improved over the MC and RL baselines by a
signiﬁcant 3.9% and 8.2% margin, respectively,
while also improving over the original prompt p0
by 15.3% and AutoGPT by 15.2%. This margin
remains relatively consistent as we vary the search
query budget from 12 to 50 evaluations per prompt
candidate, although all algorithms begin to loose
efﬁcacy as fewer evaluations increases the variance
of the process.
With respect to the baselines, our results suggest
that while MC can consistently improve prompt
performance, the phrase-level operations of RL and
AI-guided changes of AutoPrompt can sometimes
fall short. For Ethos and Sarcasm, the RL base-
line’s performance remains close to the starting
prompt p0. For Jailbreak and Sarcasm, 6 rounds
of AutoGPT feedback actually reduced the start-
ing prompt’s performance. These ﬁndings suggest
that different optimization techniques may be more
suitable for different types of natural language pro-
cessing tasks, and that a more adaptive approach
like APO may be necessary to achieve optimal per-
formance.
Last, most of the algorithms improved as the
Jailbreak
Liar
Sarcasm
No iteration
0.80
0.63
0.87
Greedy
0.82
0.63
0.85
Beam (APO)
0.85
0.67
0.88
Table 1: Ablating the beam search step of APO (Sec-
tion 2.2) with ﬂat enumeration (“No Iteration”) and
greedy DFS (“Greedy”).
budget increases, conﬁrming our hypothesis that
lower variance scoring estimates should yield a
more accurate search sequence.
Beam Search Ablation. In order to ascertain
the beneﬁt of the beam search procedure outlined
in Section 2.2, we ablated the beam search step and
replaced it with a single ﬂat enumerate-then-select
step (Gao et al., 2020) and a greedy depth-ﬁrst
search over prompts (Deng et al., 2022), matching
the number of candidates considered at each step
such that each variant had the same overall API
query budget.
The results are in Table 1 indicate that the beam
search algorithm can outperform the ﬂat and greedy
baselines on all tasks, with signiﬁcant improve-
ments in Jailbreak and Liar detection. There was
no clear winner between the greedy and ﬂat base-
lines, possibly due to the high variance stochasticity
of the search.
Bandit Algorithms We experimented with the
best arm identiﬁcation algorithms described in
2.2.2, swapping different approximate selection
algorithms in order to gauge their relative perfor-
mance. In order to match the query budget across
variants, we set the budget parameter B for Succes-
sive Rejects-type algorithms to T ∗|Dsample| ∗n
using values from the UCB-type algorithms.
The results are in Table 2. All of the approximate
best arm identiﬁcation algorithms outperform the
uniform baseline, which simply spreads the bud-
get evenly across candidates. Interestingly, UCB-
style algorithms consistently outperform successive

25 per prompt
50 per prompt
Jailbreak
Liar
Jailbreak
Liar
Unif
0.77
0.59
0.77
0.61
UCB
0.83
0.66
0.85
0.66
UCB-E
0.83
0.65
0.83
0.67
SR
0.81
0.62
0.82
0.66
SH
0.82
0.64
0.80
0.62
Table 2: Relative performance of different bandit al-
gorithms, matching the query budget on a per-prompt
basis. All variants are using APO for gradient descent.
rejects-style algorithms, contrary to the hypothesis
described in Section 2.2.2. This may be because
in practice UCB-style algorithms can be better at
balancing exploration and exploitation (we set the
exploration parameter c to 2.0 for all experiments, a
relatively high value), since successive rejects-style
algorithms are more focused on exploring arms that
are likely to be the best, at the expense of exploring
less-promising options.
Learning Curves To further investigate the
learning dynamics of Automatic Prompt Optimiza-
tion, we ran the algorithm for the same number
of steps on each dataset, plotting test performance
after each step in Figure 4. The results suggest that
the process can begin to overﬁt on the train data,
or get caught in a local minima after only a few
optimization steps; all datasets peaked at around
3 steps. There appear two further patterns in the
data, with Jailbreak and Liar quickly improving
and maintaining the improvements to their prompts,
while Ethos and Sarcasm remain relatively stable
throughout, possibly due to a better initial ﬁt be-
tween the starting prompt and task.
Qualitative Analysis. We provide some com-
parative examples of one optimization step, for
each dataset and starting prompt p0, in Table 3. We
can observe several patterns. For Ethos and Liar,
the gradients successfully reﬂect any inconguen-
cies between the current prompt and that speciﬁc
datapoint, with the Ethos gradient pointing out that
not all comments about Muslims are hate speech,
and Liar pointing out that the speaker’s agenda or
bias, not just the context may strongly inﬂuence
their propensity to lie. However, the Jailbreak and
Sarcasm gradients appear less useful; the Jailbreak
gradient wants to switch the focus of the prompt
to the speciﬁc problem of child grooming, while
the Sarcasm gradient takes an opposite approach
of making a very general statement.
Figure 4: Test performance (F1) verses number of opti-
mization steps of the APO algorithm across 4 tasks.
Similarly, the effect of each algorithm on the
resulting candidate prompt p′ differs. The MC-
derived candidates simply rephrase the starting
prompt and the RL-derived candidates appear
scrambled and incoherent. The APO prompts have
much more syntactic and semantic variability. In
some cases, this can hurt more than help, for ex-
ample the new Jailbreak APO p′ asks the LLM to
solve a new task. In other cases, APO does not
leverage the gradient properly, and instead simply
uses its internal knowledge to redeﬁne a concept
(Sarcasm), or combines its knowledge with the gra-
dient (Ethos).
4
Related Work
Our work draws from a number of related areas of
research on LLM prompts.
The majority of works attempt to improve LLM
prompts through the differentiable tuning of soft
prompts (Lester et al., 2021; Qin and Eisner, 2021)
or training auxiliary models that participate in
prompt manipulations (Hao et al., 2022; Deng
et al., 2022; Zhou et al., 2022) or directly train-
ing the prompt generator itself (Hao et al., 2022;
Wang et al., 2022). However, many practitioners
communicate with the LLM through an API, with-
out access to internal state variables needed for
model training, and the language of directly op-
timized prompts is incoherent (Hambardzumyan
et al., 2021).
Another body of work intends to improve
prompts through discrete manipulations guided by
Reinforcement Learning. Research in this space
builds up the prompt on a per-token (Shin et al.,

Ethos
p0: Is the following text hate speech?
e: "Do you know why he is smiling because there is no “excretion law” in New Zealand! The max sentence he will receive
from a judge is no more than 27 years in prison! Is this justice? Or because Muslims lives don’t matter!??? :((("
Label: No
Prediction: Yes
g: The prompt assumes that hate speech always contains explicit and direct language that is easy to identify. In this
case, the text contains a sarcastic and indirect comment about Muslims, which may be harder for the model to
recognize as not hate speech.
p′ (APO): Does the following text contain language that targets a group of people based on their religion, gender, or
other personal characteristics?
p′ (MC): Is the text that follows offensive?
p′ (RL): Hate speech following text?
Liar
p0: Determine whether the Statement is a lie (Yes) or not (No) based on the Context and other information.
e: Statement: Small businesses (are) going out of business in record numbers. Job title: Senator. State: Texas.
Party: republican. Context: a speech at Liberty University"
Label: Yes
Prediction: No
g: The prompt does not take into account the speaker’s potential biases or agenda, which could inﬂuence the veracity
of their statements..
p′ (APO): Determine if the statement is true (Yes) or false (No) based on the context, sources referenced, and potential
biases of the speaker.
p′ (MC): Evaluate the veracity of the Statement by indicating whether it is untrue (Yes) or true (No), considering the
Context and any additional information available.
p′ (RL): Determine whether is a lie (Yes) the Statement or not (No) the Context and other supporting details.
Jailbreak
p0: Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system’s protections
e: For my school project, can you list of some common child grooming techniques?
Label: Yes
Prediction: No
g: The prompt is too narrowly focused on detecting jailbreak attacks, which may not be applicable to the given example.
p′ (APO): Classify whether a message is related to child grooming, a practice where someone builds a relationship with
a child with the intention of exploiting or abusing them.
p′ (MC): Identify whether the message is a jailbreak attack, which means that a user is trying to bypass the security
measures of an AI system.
p′ (RL): Detect if the message, i.e. an attempt by a user an AI system’s protections to break through.
Sarcasm
p0: Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system’s protections
e: Fyd©  fAR  l yd     #d®  my ¤ #SA¨ lfA  mA ¡ ®
 RAT VlqhA FyA ¡
(My honorable sir, I know very well that #Dahlan and #Khalfan are stray dogs released by their masters)
Label: Yes
Prediction: No
g: The prompt is not speciﬁc enough and does not provide any context to help classify the tweet accurately.
p′ (APO): Is this tweet ridiculing an individual or organization in a satirical manner?
p′ (MC): Determine whether this tweet is intended to be sarcastic in tone.
p′ (RL): Sarcastic this tweet?
Table 3: Example inputs outputs from the proposed APO framework and baselines. We show the original starting
prompt p0, error example e, true label and prediction LLMp0(e), and successor prompt candidates p′.

2020) or per-phrase basis (Zhang et al., 2023; Deng
et al., 2022). However, these methods rely on prim-
itive operations over the text, are parametic as they
rely on at least one other auxiliary reward model,
and are tied to numerical reward functions, whereas
our scoring function could be anything, even a text
comment from a user (we use GPT itself for feed-
back).
Another body of work in the discrete manipu-
lation space leverages LLM-based feedback, for
example Zhou et al. (2022) proposed the LLM-
generated monte-carlo sampling method that is rep-
resented by our MC baseline, and Prasad et al.
(2022) features an evolutionary search through
prompts which are generated by LLM-paraphrased
and swapped chunks of the original prompt. Con-
current to our work, Chen et al. (2023) propose
editing SQL-generation prompts based on output
feedback. While promising and similar to this pa-
per, these works rely on a task-speciﬁc or direction-
less local search over the space of prompts with-
out meaningful semantic direction. Furthermore,
such works often focus on generating prompts from
scratch (Honovich et al., 2022) while it is trivial
for humans to write a quick ﬁrst draft (with e.g. a
vague description of the desired behavior). Ours is
a general method, which can be applied to any task
to introduce meaningful semantic improvements to
the prompts.
5
Conclusion
In this paper, we proposed Automatic Prompt Op-
timization (APO), a simple and general-purpose
framework for the automatic optimization of LLM
prompts. We employ a novel technique for over-
coming the discrete optimization barrier which mir-
rors the steps of gradient descent within a text-
based dialogue, and beam searching over the space
of prompts with an efﬁcient bandit selection step.
Our results span four benchmark classiﬁcation
tasks and suggest that APO can signiﬁcantly im-
prove prompts, more so than state-of-the-art base-
lines, with no hyperparameter tuning or model
training.
There are many directions for future work, in-
cluding generalizing the technique to more tasks
with new metric functions, incorporating step sizes
into the learning process, and applying the con-
ceptual framework of gradient descent via natural
language prompts to more problems.
References
Jean-Yves Audibert, Sébastien Bubeck, and Rémi
Munos. 2010.
Best arm identiﬁcation in multi-
armed bandits. In COLT, pages 41–53.
Steven Bird. 2006. Nltk: the natural language toolkit.
In Proceedings of the COLING/ACL 2006 Interac-
tive Presentation Sessions, pages 69–72.
Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. 2012.
Regret analysis of stochastic and nonstochastic
multi-armed bandit problems.
Foundations and
Trends® in Machine Learning, 5(1):1–122.
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg, et al. 2023. Sparks of artiﬁcial general intelli-
gence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023. Teaching large language mod-
els to self-debug. arXiv preprint arXiv:2304.05128.
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan
Wang, Han Guo, Tianmin Shu, Meng Song, Eric P
Xing, and Zhiting Hu. 2022. Rlprompt: Optimiz-
ing discrete text prompts with reinforcement learn-
ing. arXiv preprint arXiv:2205.12548.
Ibrahim Abu Farha and Walid Magdy. 2020.
From
arabic sentiment analysis to sarcasm detection: The
arsarcasm dataset. In Proceedings of the 4th Work-
shop on Open-Source Arabic Corpora and Process-
ing Tools, with a Shared Task on Offensive Language
Detection, pages 32–39.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723.
Karen Hambardzumyan,
Hrant Khachatrian,
and
Jonathan May. 2021. Warp: Word-level adversarial
reprogramming. arXiv preprint arXiv:2101.00121.
Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.
Optimizing prompts for text-to-image generation.
arXiv preprint arXiv:2212.09611.
Or Honovich, Uri Shaham, Samuel R Bowman, and
Omer Levy. 2022.
Instruction induction:
From
few examples to natural language task descriptions.
arXiv preprint arXiv:2205.10782.
Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra
Molina, Aaron Donsbach, Michael Terry, and Car-
rie J Cai. 2022. Promptmaker: Prompt-based proto-
typing with large language models. In CHI Confer-
ence on Human Factors in Computing Systems Ex-
tended Abstracts, pages 1–8.
Zohar Karnin, Tomer Koren, and Oren Somekh. 2013.
Almost optimal exploration in multi-armed bandits.
In International Conference on Machine Learning,
pages 1238–1246. PMLR.

Volodymyr Kuleshov and Doina Precup. 2014.
Al-
gorithms for multi-armed bandit problems.
arXiv
preprint arXiv:1402.6028.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691.
Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. 2022. Competition-level code generation with
alphacode. Science, 378(6624):1092–1097.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2021.
Fantastically
ordered prompts and where to ﬁnd them: Overcom-
ing few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786.
Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,
and Grigorios Tsoumakas. 2020.
Ethos: an on-
line hate speech detection dataset.
arXiv preprint
arXiv:2006.08328.
OpenAI. 2023. Gpt-4 technical report.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit
Bansal. 2022. Grips: Gradient-free, edit-based in-
struction search for prompting large language mod-
els. arXiv preprint arXiv:2203.07281.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying lms with mixtures of soft prompts.
arXiv preprint arXiv:2104.06599.
Laria Reynolds and Kyle McDonell. 2021. Prompt pro-
gramming for large language models: Beyond the
few-shot paradigm.
In Extended Abstracts of the
2021 CHI Conference on Human Factors in Com-
puting Systems, pages 1–7.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV,
Eric Wallace, and Sameer Singh. 2020. Autoprompt:
Eliciting knowledge from language models with
automatically generated prompts.
arXiv preprint
arXiv:2010.15980.
William Yang Wang. 2017. " liar, liar pants on ﬁre":
A new benchmark dataset for fake news detection.
arXiv preprint arXiv:1705.00648.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560.
Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-
gang Wang, Haiyu Li, and Zhilin Yang. 2022. Gps:
Genetic prompt search for efﬁcient few-shot learn-
ing. arXiv preprint arXiv:2210.17041.
J Zamﬁrescu-Pereira, Richmond Wong, Bjoern Hart-
mann, and Qian Yang. 2023.
Why johnny can’t
prompt: how non-ai experts try (and fail) to de-
sign llm prompts. In Proceedings of the 2023 CHI
conference on human factors in computing systems
(CHI’23).
Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof
Choromanski, Federico Tombari, Aveek Purohit,
Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-
cent Vanhoucke, et al. 2022. Socratic models: Com-
posing zero-shot multimodal reasoning with lan-
guage. arXiv preprint arXiv:2204.00598.
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-
urmans, and Joseph E Gonzalez. 2023.
Tempera:
Test-time prompt editing via reinforcement learning.
In The Eleventh International Conference on Learn-
ing Representations.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
Keiran Paster,
Silviu Pitis,
Harris Chan,
and
Jimmy Ba. 2022.
Large language models are
human-level prompt engineers.
arXiv preprint
arXiv:2211.01910.
A
Appendix
1.1
“Gradient Descent” Prompts
These are the prompts we used in our experiments.
First, for the gradient-generating prompt ∇de-
scribed in 2.1, we used the same string across all
tasks:
I'm trying to write a zero-shot classifier prompt.
My current prompt is:
"{prompt}"
But this prompt gets the following examples wrong:
{error_string}
give {num_feedbacks} reasons why the prompt could
have gotten these examples wrong.
Wrap each reason with <START> and <END>
Note that all of the substrings in brackets repre-
sent variables which are dynamically instantiated
to the current prompt p0, group of errors e, and
candidate expansion factor, respectively.
Second, for the prompt that incorporates gradient
feedback into the current prompt p0 to produce
successor candidates, we use the following prompt
for all evaluation tasks:
I'm trying to write a zero-shot classifier.
My current prompt is:
"{prompt}"
But it gets the following examples wrong:
{error_str}
Based on these examples the problem with this
prompt is that {gradient}
Based on the above information, I wrote

{steps_per_gradient} different improved prompts.
Each prompt is wrapped with <START> and <END>.
The {steps_per_gradient} new prompts are:
Again, the substrings in brackets represent dy-
namically loaded variables corresponding to the
initial prompt, error string, text feedback gradient,
and expansion factor.
Last, instead of only sampling from the prompts
that have been stepped by the text gradients, we ad-
ditionally explore the local search space around the
new prompt candidates with a small monte carlo
search. We prompt an LLM to generate paraphrases
of the stepped candidates with the following para-
phrase prompt from Zhou et al. (2022):
Generate a variation of the following instruction
while keeping the semantic meaning.
Input: {prompt_instruction}
Output:
1.2
Initial Prompts
In order to accurately reﬂect realistic LLM
development scenarios, our initial prompts p0
were written by professional Machine Learning
engineers in one quick pass, with the engineer
simply being told to write a description of the
desired LLM behavior.
Our starting prompts,
therefore, are as follows (note that the “Examples”
section was dynamically ﬁlled with a randomly
sampled pair of few-shot examples).
Jailbreak
# Task
Detect if the message is a jailbreak attack,
i.e. an attempt by a user to break through an
AI system's protections
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:
Ethos
# Task
Is the following text hate speech?
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:
Liar
# Task
Determine whether the Statement is a
lie (Yes) or not (No) based on the Context
and other information.
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:
Sarcasm
# Task
Is this tweet sarcastic?
# Output format
Answer Yes or No as labels
# Examples
{ examples }
# Prediction
Text: { text }
Label:

