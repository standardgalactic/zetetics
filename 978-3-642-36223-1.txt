123
Vincenzo Manca
     EMERGENCE,
   COMPLEXITY 
                   AND
COMPUTATION
Infobiotics
Information in Biotic Systems
ECC

Emergence, Complexity and Computation
3
Series Editors
Prof. Ivan Zelinka
Technical University of Ostrava
Czech Republic
Prof. Andrew Adamatzky
Unconventional Computing Centre
University of the West of England
Bristol
United Kingdom
Prof. Guanrong Chen
City University of Hong Kong
For further volumes:
http://www.springer.com/series/10624

Vincenzo Manca
Infobiotics
Information in Biotic Systems
ABC

Author
Prof. Vincenzo Manca
Dipartimento di Informatica
Universita’ degli Studi di Verona
Verona
Italy
ISSN 2194-7287
e-ISSN 2194-7295
ISBN 978-3-642-36222-4
e-ISBN 978-3-642-36223-1
DOI 10.1007/978-3-642-36223-1
Springer Heidelberg New York Dordrecht London
Library of Congress Control Number: 2012956231
c⃝Springer-Verlag Berlin Heidelberg 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher’s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

To little Elena

Preface
This book presents topics in discrete biomathematics, by promoting and exploring
new approaches at the borders between biology and discrete mathematics. Its main
claim is that, starting from the general principles of molecular biology discovered
in the last century, an informational perspective could give hints and solutions to
some of the challenges posed by biological problems. This approach is not new in
biology. A remarkable example of deductive reasoning, in biological investigation,
is the booklet written by Schr¨odinger in 1945 entitled “What is life?”. In a few
pages, by means of a lucid chain of general arguments, Schr¨odinger deduced that
the information responsible for heredity has to be encoded inside cells by a kind of
molecular structure that he deﬁned “aperiodic crystals” (in essence, a surprising an-
ticipation of what was discovered a few years later). The scientists who discovered
the DNA structure in 1953 were impressed by Schr¨odinger’s analysis, and surely it
was a strong motivation in the achievement of their results. The current situation of
science encourages a revival of such an approach. In fact, in the last century the cru-
ciality of information in living processes became more and more clear. Mathematics
has been widely used in modeling biological phenomena. However, the molecular
and discrete nature of basic life processes suggests that many aspects underlying the
logic of these processes follow principles which are intrinsically based on informa-
tional mechanisms.
Computer science is essential for the elaboration of huge amounts of biological
data. For example, it was essential in genome sequencing. However, science is not
only data processing. If we want to disclose the deep logic of basic mechanisms of
life, we need new scientiﬁc theories, and therefore new conceptual frameworks. Dis-
crete mathematics, algorithms, and computational approaches are good candidates
for introducing new scientiﬁc ideas in life sciences. For this reason the discipline
evoked by the title of this text, Infobiotics, is viewed as the reverse side of Bioinfor-
matics. The two roots “info” and “bio” are inverted in these words. In bioinformat-
ics the biologists ask computer scientists to assist them in elaborating the data they
obtain. Conversly, in infobiotics computer scientists and mathematicians provide bi-
ologists with explanations and theories which biologists need to verify by means of
speciﬁc experiments. As I like to say, life is too important to be investigated only

VIII
Preface
by biologists. Computers are essential for processing data coming from biological
laboratories, but many crucial questions about life can be appropriately answered by
a substantial intervention of mathematicians, computer scientists, and physicists, in
order to complement the work of chemists, biochemists, and biologists, The role of
computer scientists cannot be limited to the use of computers, in the same manner
as physicists are essential to astronomy not only in connection with telescopes.
An example of “mental experiment”, similar to Schr¨odinger’s approach, is
Galileo’s argument (reported by Alfred Whitehead in his Introduction to Mathe-
matics) against the Aristotelian principle that heavier bodies fall faster than lighter
ones. Galileo’s reasoning is a masterpiece of logical analysis, which was the be-
ginning of mechanics and of the scientiﬁc revolution initiated in the 17th century.
Galileo showed that Aristotle was wrong by means of the following argument. Let
H and L be two bodies, such that H is heavier than L. Let us assume that Aristotle
is right, and let us link H with L, by means of very thin string, thus obtaining a
new body H+L, which we drop to the ground from a higher position. If L is slower
to fall, then it provides a force which slows down the fall of H. This means that
H+L falls slower than H. But, this contradicts Aristotle’s claim because, of course,
H+L is heavier than H. This contradiction confutes Aristotle’s claim. Galileo proved
experimentally the validity of his argument (with the famous experiment from the
leaning tower in Pisa), however the motivation of that physical experiment had its
conceptual support in the previous mental experiment.
In many aspects Darwin’s arguments for his theory of evolution, based on natural
selection, have an analogous conceptual rigor in taking into account the evidence of
crucial factors related to species (the ﬁtness principle of living organisms, the geo-
logical transformations, the inheritance of biological characters, and the Malthusian
population growth).
When we read texts of biology describing cell structures and functions, we are
immediately overwhelmed by the huge amount of things and facts. But let us re-
verse the situation, by assuming that we are asked to create life starting from some
basic kinds of organic molecules. How can we manage in order to provide some ba-
sic functions? How can we keep them together by ensuring some transformations?
How can we maintain them, by ensuring some input/output channels, and by inte-
grating them in a stable manner? Any answer to these general questions inevitably
leads to the notions of metabolism and replication, which, in any possible biologi-
cal realization, are logically necessary for basic phenomena on which life relies. A
basic feature of cell function is the genes/proteins duality. Does this duality have
an intrinsic necessity, implied by some more basic principles? Solving such kind
of problems is not an abstract logico-mathematical exercise. Appropriate answers
to these questions could provide important clues to facts which are relevant from a
biological and medical viewpoint. These answers cannot be obtained from millions
of data, or better, they may require a lot of biological and computational experi-
ments, but surely they need to be driven by mental experiments resembling Galilean
analysis of falling bodies.
Paraphrasing Spinoza’s famous masterpiece (“Ethica more geometrico demon-
strata”), infobiotics could be regarded as a a kind of biology “more geometrico

Preface
IX
demonstrata” and should be strictly related to artiﬁcial life, computational synthetic
biology, and natural computing. However, these other disciplines follow different
lines of investigation. Artiﬁcial life, initiated by John von Neumann and Robert
Langton, concerns the realization of artiﬁcial computational systems exhibiting typ-
ical properties of living organisms (von Neumann’s cellular automata were designed
as auto-replicating devices). Computational synthetic biology is more speciﬁcally
involved in the simulation, in silico, of life phenomena, while natural computing in-
vestigates the computational capabilities of natural systems. Of course, infobiotics
can take advantage of all these disciplines, but its main aim is the identiﬁcation of
the informational mechanisms driving life phenomena. We know that life relies on
information, but so far we know very little about the speciﬁc aspects of biological
information.
Starting from the basic organic molecules, the ﬁrst attempts at life appeared only
when some special biopolymers emerged, which were able to encode information.
Thus, if life is based on information, discrete mathematics and informatics must be
crucial in the analysis of biotic systems as forms of life that we know, but also of
forms that we do not know, or that are so far unrealized.
Infobiotics is aimed at considering life, in a wide sense, with the “glasses” of
information. This book tries to show that discrete structures, algorithms, languages,
grammars, and automata are strictly related to life structures and processes, to genes
and proteins, enzymes and ribosomes, species and biological dynamics.
Verona, May 2012
Vincenzo Manca

Acknowledgements
I would like to express my gratitude to many collaborators, who contributed to de-
velop the research lines presented in this book, and who read preliminary versions
of this book by giving important suggestions toward improving its coherence and
quality.
Marco Domenico Martino, and Francesco Bernardini were my students in Pisa,
where my biological investigation started in 1996. Some papers that I published with
them were the seeds of the development of a research line in algorithmic modeling
of biological phenomena. Giuditta Franco, who was PhD student in Verona, has
been a precious research collaborator of mine along the last ten years, in particular,
she has worked with me on many problems of DNA computing, and developed with
me the XPCR algorithm presented in the second chapter. Luca Bianco, when he was
Phd student in Verona, participated in the ﬁrst steps in the deﬁnition of metabolic P
systems and developed the Java implementation of the “metabolic P algorithm”, the
ﬁrst method for computing the evolution of metabolic systems in terms of multiset
rewriting grammars (P systems). Luca Marchetti, who was PhD student in Verona,
worked with me on solving the dynamical inverse problem in metabolic P system
(MP systems, related to P˘aun’s P systems). He developed the Matlab implementa-
tion of LGSS, the main algorithm presented in the third chapter. Alberto Castellini
and Roberto Pagliarini, who were PhD students in Verona, collaborated, in many as-
pects, on software projects for the simulation, design, and analysis of MP systems.
Recently, Giuditta Franco and Alberto Castellini are collaborating with me in a re-
search project, outlined at end of the second Chapter, called Infogenomics, which
is based on the notion of genomic dictionary. In this context, interesting ideas, in
course of investigation, were suggested by Rosario Lombardo, and Vincenzo Bon-
nici, PhD students in Verona, who are developing software tools focused on non-
conventional representation and visualization of genomes. Zsuzsanna Liptak, who
recently joined our research group, and Iain Halliday suggested improvements for
the exposition of some chapters of the book.
Ennio De Giorgi and Alfonso Caracciolo di Forino were my mentors in Pisa.
They gave me a scientiﬁc imprinting and were “models” in the development of my
research.

XII
Acknowledgements
Gheorghe P˘aun, my friend and collaborator, was the scholar who introduced P
systems and founded the scientiﬁc community of Membrane Computing. His pa-
pers and books on DNA Computing and Membrane Computing were inﬂuential
for my research. Membrane Computing, DNA Computing, and Natural Computing
meetings, which I regularly attended since their beginnings, were a constant source
of inspiration. Discussions and ideas, during the scientiﬁc meetings of these com-
munities, were “rivers” ﬂowing in the underground of my mind which, after mixing
their waters, very often sprang up during my conscious research activity.
Giuseppe Scollo and Angelo Spena, my colleagues in Verona, from the math-
ematical and and biological points of view respectively, stimulated my theoretical
analyses of life phenomena from an algorithmic perspective. Ivan Zelinka, at VSB
Technical University in Ostrava, supported enthusiastically the project of this book.
Many other colleagues at the University of Verona helped me very much in projects
and activity promoting bioinformatics curricula in Verona. This activity surely had
a “catalytic” effect on many research topics related to this book.
The content of this book is based on my research and teaching mostly developed
in the last ten years. In this period, I taught and coordinated many courses at the
Department of Computer Science of the University of Verona (Information theory,
Unconventional models of computation, Informational methods, Models of natural
computing, Discrete biological models, Algorithms and languages of bioinformat-
ics) for the curricula in Computer Science and Bioinformatics (bachelor, master and
PhD courses). I am grateful to many students, for their questions, projects, and the-
ses, which were a source of continuous clariﬁcation of my research and teaching. I
taught regularly at the universities of Pisa, Udine, and Verona, where I met a huge
number of students. I remember clearly a lot of them, and I know that they were
essential in the construction of my scientiﬁc proﬁle.
The seven chapters of the book are associated with the ﬁve Platonic solids, the
sphere, and the Archimedean truncated icosahedron (the soccer ball), which, very
often, I contemplate as symbols of rational harmony.

Contents
Part I: Topics in Discrete Biomathematics
1
Discrete Information and Life . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Symbols and Molecules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.1
Molecules and Protocells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Sequences and Polymers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
LUCA and the Sequence Paradox . . . . . . . . . . . . . . . . . . . . . .
8
1.3
Multisets and Membranes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.4
Chemistry Multisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.5
Liposome Membranes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.6
Populations and Hypermultisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.7
Trees and Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.8
Graphs and Interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.8.1
Graphs of Molecule Structures . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.8.2
Graphs of Reactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.8.3
Neuron Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2
Strings and Genomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.1
Biological Monomers and Polymers . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.2
DNA Strings and DNA Helix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.2.1
DNA Notation and Double String Operations . . . . . . . . . . . . . 29
2.2.2
DNA Helix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.3
DNA Pool Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.3.1
Writing and Reading DNA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.3.2
Plasmide Cloning Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.4
DNA Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.4.1
Adleman’s Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.5
PCR and XPCR Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.5.1
XPCR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
2.5.2
DNA Extraction by XPCR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
2.5.3
DNA Recombination by XPCR . . . . . . . . . . . . . . . . . . . . . . . . 66

XIV
Contents
2.6
L-Systems and Morphogenesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
2.6.1
String Models and Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.7
Membrane Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
2.8
Informational Analysis of Genomes . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3
Algorithms and Biorhythms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.1
Metabolic Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.1.1
Lotka-Volterra Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.1.2
The Brusselator (Belousov-Zhabotinsky Reaction) . . . . . . . . 115
3.2
Time Series and Inverse Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.3
Metabolic Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.3.1
Goniometricus: A Metabolic Grammar of Sine
and Cosine. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.3.2
Generalization of the Goniometricus Model . . . . . . . . . . . . . . 122
3.4
Stoichiometric Expansion and Stepwise Regression . . . . . . . . . . . . . . 123
3.4.1
Log-Gain Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
3.4.2
The Stepwise Regression LGSS . . . . . . . . . . . . . . . . . . . . . . . . 130
3.4.3
Problems Related to the Regression with LGSS . . . . . . . . . . . 135
3.4.4
Models of Mitosis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
3.5
Reactivity and Inertia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.6
Metabolic Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
3.7
Metabolic Oscillators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
3.7.1
Anabolism and Catabolism . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
3.8
The Ubiquity of Metabolism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
3.8.1
Metabolic Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
3.8.2
Revisiting MP Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
4
Life Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
4.1
The Enzymatic Paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
4.1.1
Genes and Proteins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
4.2
Bio-inspired Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.3
Replication and Autopoiesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
4.4
Main Informational Steps from LUCA to OVUM . . . . . . . . . . . . . . . . 185
4.5
Life Analysis and Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
4.6
Life Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
4.7
Time’s Arrow and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.8
Life and Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
Part II: Discrete Mathematical Backgrounds
5
Numbers and Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5.1
Sets and Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5.1.1
Relations and Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
5.1.2
Functions and Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
5.2
Numbers and Digits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
5.2.1
Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

Contents
XV
5.2.2
Sums and Positional Representations . . . . . . . . . . . . . . . . . . . . 221
5.2.3
Integer Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
5.3
Rational and Real Numbers: Approximation and Inﬁnite . . . . . . . . . . 224
5.3.1
Incommensurability, Divisibility, and Distinguishability . . . . 225
5.4
Complex Numbers and Real Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
5.4.1
Euler’s Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
5.4.2
Polar Representation of Complex Numbers . . . . . . . . . . . . . . 234
5.4.3
Some of Euler’s Jewels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
5.5
Induction and Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
5.5.1
Fibonacci Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
5.5.2
Arithmetic and Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
5.5.3
A Glimpse of the History of Formal Logic . . . . . . . . . . . . . . . 253
5.6
Series and Growths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.6.1
Arithmetical Progressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.6.2
Figurate Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
5.6.3
Geometrical Progressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
5.6.4
Logistic Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
5.6.5
Natural Exponential Growth . . . . . . . . . . . . . . . . . . . . . . . . . . 261
5.6.6
Asymptotic Orders and Inﬁnitesimals . . . . . . . . . . . . . . . . . . . 262
5.7
Scales of Time, Space, and Matter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
5.8
Discrete Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
6
Languages and Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
6.1
Strings and Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
6.2
Grammars and Chomsky Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
6.3
Regular Expressions and Finite Automata . . . . . . . . . . . . . . . . . . . . . . 282
6.4
Patterns and Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
6.5
Turing Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
6.6
Decidability and Undecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
6.7
Register Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
6.8
Information, Codes, and Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
6.8.1
Shannon’s Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
6.8.2
Optimal Codes and Compression . . . . . . . . . . . . . . . . . . . . . . . 303
6.8.3
Typicality and Transmission . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
7
Combinations and Chances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
7.1
Factorials and Binomial Coefﬁcients . . . . . . . . . . . . . . . . . . . . . . . . . . 309
7.1.1
Permutations and Arrangements . . . . . . . . . . . . . . . . . . . . . . . . 310
7.1.2
Combinations and Binomial Coefﬁcients . . . . . . . . . . . . . . . . 311
7.1.3
Inductive Derivation of Binomial Coefﬁcients . . . . . . . . . . . . 312
7.2
Distributions and Discrete Probability . . . . . . . . . . . . . . . . . . . . . . . . . 314
7.3
Allocations and Partitions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
7.3.1
Multinomial Coefﬁcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
7.3.2
Partitions and Multisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
7.3.3
Integer Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319

XVI
Contents
7.4
Stirling, Bell, Catalan, and Bernoulli Numbers . . . . . . . . . . . . . . . . . . 323
7.4.1
Stirling and Bell Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
7.4.2
Catalan Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
7.4.3
Bernoulli Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
7.5
Stirling Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
7.6
“Ars Conjectandi” and Statistical Tests . . . . . . . . . . . . . . . . . . . . . . . . . 328
7.6.1
Statistics and Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
7.6.2
Statistical Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
7.6.3
Sample Signiﬁcance Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
7.7
Least Square Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
7.7.1
The k-Variable Multiple Regression Model . . . . . . . . . . . . . . . 340
7.7.2
The Classical Stepwise Regression . . . . . . . . . . . . . . . . . . . . . 345
7.8
Trees and Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
7.8.1
Types of Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
7.8.2
Types of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
7.8.3
Graph Planarity and Colorability . . . . . . . . . . . . . . . . . . . . . . . 352
7.8.4
Pointers and Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
7.8.5
Parentheses and Tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
7.8.6
Tree Enumerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
7.8.7
Binary Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
7.8.8
Phylogenetic Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379

Acronyms
ACO
Ant Colony Optimization
ADA
Avogadro Dalton Action
ANN
Artiﬁcial Neural Network
BT
Binary rooted Tree
BZ
Belousov-Zhabotinsky reaction
CF
Context-Free languages
CS
Context-Sensitive languages
DNA
DeoxyriboNucleic Acid
DIP
Dynamical Inverse Problem
EDVAC
Electronic Discrete Variable Automatic Computer
EMA
Equation Metabolic Algorithm
EMBL
European Molecular Biology Laboratory
EPD
Eukaryotic Promoter Database
FIN
FINite languages
FLT
Formal Language Theory
FOL
First Order Logic
FSA
Finite State Automaton
HTML
Hyper Text Markup Language
IFF
IF and only iF
KEGG
Kyoto Encyclopedia of Genes and Genomes
k-mer
DNA string of length k
LGSS
Log Gain Stoichiometric Stepwise regression
LSE
Least Square Evaluation
LUCA
Last Common Universal Ancestor
LV
Lotka-Volterra dynamics
MATLAB
MATrix LABoratory, developed by MathWorks
MetaPlab
Metabolic P systems virtual laboratory
MF
Minimal Forbidden length
MP
Metabolic P (Multiset Processing, Minus Plus), preﬁx related to MP
grammars (MP system, MP rule, ...)
MR
Maximal Repeat length

XVIII
Acronyms
MPF
Metabolic P preﬁx related to ﬂux regulation maps (regulators)
MPR
Metabolic P preﬁx related to reaction maps
MSE
Mean Squared Error
NCBI
National Center for Biotechnology Information
NP
languages decidable in Polynomial time by a Non-deterministic TM
NPQ
Non-Photochemical Quenching
P
Languages decidable in Polynomial time by a deterministic TM
PCR
Polymerase Chain Reaction
PSim
Java Simulator of MP dynamics
P systems
P˘aun systems
REG
REGular languages
REC
RECursive, or decidable, languages
RE
Recursively Enumerable, or semidecidable, languages
RNA
RiboNucleic Acid
SAT
Boolean SATisﬁability problem
SIR
Susceptible-Infective- Removed epidemic
SSE
Sum of Squared Errors
SSR
Sum of Squared Regression deviations
SST
Sum of Squared Total deviations
TSS
Transcription Start Site
TTL
Test Tube Language
TM
Turing Machine
XML
eXtensible Markup Language
XPCR
Cross pairing Polymerase Chain Reaction
UB
Unrooted three-Branched tree
UCSC
University of California Santa Cruz website
UTR
Untranslated Terminal Region
VIF
Variance Inﬂation Factor

Part I
Topics in Discrete Biomathematics

Platonic Tetrahedron
1
Discrete Information and Life
Abstract. Information and life are intrinsically related and many challenges for
a deep understanding of life phenomena require analyzing the role of informa-
tion, and its speciﬁc mechanisms, in biological systems. In this chapter, the main
kinds of discrete informational structures will be informally presented in many bi-
ological contexts where they occur: sets, sequences, multisets, trees, and graphs,
which are mathematically deﬁned and analyzed in the second part of the book, are
here seen as fundamental mechanisms of aggregation and organization of biological
components.
1.1
Symbols and Molecules
A univocal and clear deﬁnition of information is almost impossible. Information,
as energy, matter, time, space, and life are such pervasive notions that any precise
characterization of them necessarily misses aspects which are crucial for any com-
prehensive understanding of their nature. For this reason, it is useful to start with
an informal idea to which we can anchor our initial intuition. We encounter in-
formation in data, which are physical objects, and can be transformed, stored, and
transmitted. But they are meaningful when seen as containers of some kind of “in-
formation”. This sounds clearly tautological, therefore, we need to add something
to avoid vacuity. Data, recognized by some “agents”, have the capability of direct-
ing and regulating their actions. In other words, they direct actions. When a ﬂag
movement or a trumpet sound is executed, then soldiers attack the enemy. The more
complex are the actions, the more the information system regulating them has to be
V. Manca: Infobiotics, ECC 3, pp. 3–26.
DOI: 10.1007/978-3-642-36223-1_1
c⃝Springer-Verlag Berlin Heidelberg 2013

4
1 Discrete Information and Life
complex. No system can exhibit a complex behavior, without a reliable system of
data processing. This is the reason why living organisms need sophisticated mecha-
nisms of data manipulation.
Information occurs with data transformations, while energy occurs with states
transformations. Information needs energy for transforming, sending, receiving, and
storing data, but simultaneously, it may control actions. In those cases where it does
not provide a direct control of actions, it can be always located in a chain of effects
which terminates with a control, or a potential control. For example, some keys in
a computer keyboard send signals with direct effects to some device (for example,
a printer), while pushing other keys provides only the memorization of characters
somewhere (by forgetting their side effects on the screen). Typically, a document
does not have a direct effect of data transformation, but it is aimed at provoking
or evoking actions. Moreover, very often, the effects of some data are not easily
deﬁnable, because their information can act at different levels. For example, the
production of a book could have effects at the level of the book market, but also
effects in the diffusion of ideas and consequent behaviors.
The term “discrete” in mathematics refers to any aggregation of objects where
components are clearly distinguishable. A ﬁnite set of objects is the simplest exam-
ple of a discrete structure. A segment, considered as a subset of points on a line is an
example of “continuous” structure, because the geometrical representation of lines
assumes that in the middle between two points there are other points, and moreover,
that inﬁnite intermediate points can be found with an endless process of resolution
reﬁnement. Space and time are usually modeled as continuous magnitudes. This is
probably an idealization, with respect to the physical reality, but it provides pow-
erful mathematical concepts for their analysis. The surprising results of classical
physics (mechanics and electromagnetism) are based on continuous mathematical
structures (real numbers, limit process, differential calculus). The 19th century con-
cluded (Planck’s famous quantum theory is dated November 1900) with the discov-
ery of the discrete nature of matter and energy (the origin of Dalton’s atomic theory
is dated September 1803).
Information is discrete when data are based on discrete mathematical structures
(the fundamental ones are presented in the second part of this book). Digital infor-
mation is the discrete information, which refers to data realized by sequences of
digits (elements of a ﬁnite set).
Data are represented by means of symbols, which are physical entities. Their
physicality is an intrinsic aspect of their nature; even the abstract symbols of math-
ematics introduced for algebraic manipulation need a physical realization, that is, a
quantity of matter/energy in order to be produced and perceived. A ﬁnite set of sym-
bols (digits, characters, letters, signs) is called an alphabet. The English alphabet
has 26 letters. Most alphabetic writing systems have around 20-30 characters (Ar-
chaic Latin has 21 letters). The chemistry alphabet of Mendeleev’s table is given by
the symbols of atomic species (around 100 elements): {H,He,Br,...,C,O,N,...}.
The alphabet of amino acids is of 20 symbols (21 including one stop symbol). The
alphabet of the usual Indo-Arabic decimal notation has ten digit symbols.

1.1 Symbols and Molecules
5
The DNA nucleotide alphabet consists of four symbols {A,T,C,G}, while the
RNA nucleotide alphabet is {A,U,C,G}. The binary alphabet has two symbols. A
bit, usually denoted by an element of {0,1}, is an element of a binary alphabet.
Computer keyboards have around 80 keys. Striking them (one, two, or sometimes
three of them) a character is generated belonging to an alphabet of 128 standard
elements (ASCII characters) plus another 128 characters with speciﬁc usages. Any
of these 256 characters can be represented by a sequence of eight binary symbols,
called a byte. Table 1.1 collects some important examples of alphabets.
Table 1.1 Important alphabets
Binary Alphabet
{0,1}
Number Decimal Alphabet
{0,1,...,9}
DNA Alphabet
{A,T,C,G}
Archaic Latin Alphabet
21 letters
Amino acids Alphabet
21 letters (including stop symbol)
Chemistry Alphabet
Around 100 symbols
Minimal Computer Keyboard Alphabet Around 80 characters (text + control)
A deﬁnition of life is even more difﬁcult than that a deﬁnition of information.
Life, in a full sense, includes at least seven essential features: i) birth, ii)
nutrition (feeding resources from the environment, expelling degraded sub-
stances), iii) growth, iv) interaction (with stimuli from/to the environment),
v) reproduction, vi) death, and vii) evolution.
The ﬁrst ﬁve features refer to the individual living organisms, seen as particular
instances of life, in time and space. The last two features refer to populations of
living organisms instantiating forms of life, which by means of their cycles of birth-
reproduction-death realize a second level dynamics, providing, along the arrow of
time, a tree of life, that is, a “genealogy” of life species (at least one individual must
reproduce for the survival of the population and all individuals eventually die).
Life exhibits a tremendous complexity of interacting processes which, at dif-
ferent levels, have to communicate for reaching successful realizations of speciﬁc
functionalities. The complexity of this underlying dynamics needs information.
Life emerged only when an efﬁcient system of data processing was possible
at molecular level.
In fact, the fundamental discovery of the past century was the molecular struc-
tures and processes which life is based on. Molecules are discrete structures built on
atoms.

6
1 Discrete Information and Life
In the middle of the 19th century, the discovery of DNA structure [63], and its
informational and digital nature, introduced a new level in the analysis of life phe-
nomena [1]. From this perspective, discrete information becomes the basis of crucial
molecular processes inside the cell, and it follows that their logics can be completely
understood if their symbolic role is recognized. In fact, DNA molecules represent
symbols, and the discrete nature of these molecules implies the digital nature of pro-
cesses involving them. Biochemistry provides nanotechnological support to sym-
bolic elaborations, which follow principles analogous to those of formal systems
investigated in formal language theory or mathematical logic.
1.1.1
Molecules and Protocells
According to some cosmological evaluations, at very beginning, about 1010 years
ago, and in a time interval between around 10−44 and 10−35 seconds, at a tempera-
ture near 1028 Celsius degrees, matter appeared as subatomic particles. After a very
short fraction of one second, they aggregated into the simplest atoms, from Hydro-
gen to Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine,
and Neon (the list is only an exempliﬁcation reporting the ﬁrst period of Mendeleev’s
table) [3]. Then, the simplest organic molecules of hydrocarbons(carbon aggregated
with hydrogens) appeared. In fact, the carbon atom has a structure which implies an
enormous combinatorial power of aggregations (we do not enter into more details,
but this aspect is related to its speciﬁc “tetravalent” nature). From hydrocarbons the
organic molecular evolution starts, which is the basis for the emergence of life.
The more complex organic molecules are constituted by carbon, hydrogen, oxy-
gen, nitrogen, and other crucial atoms (for example, phosphorus, sulfur, iron, cal-
cium, potassium, sodium, chlorine). They are capable of providing a very rich
catalog of chemical aggregations (hydrocarbons, alcohols, ethers, esters, amides,
amines, fatty acids, and other organic groups) [4].
The prebiotic molecular evolution is the process at the end of which, from these
basic organic molecules, more complex molecules emerged, which provided the
basic pieces for the chemical realization of polymers and membranes [10, 6, 199].
Protocells are biomolecular aggregations which:
i) separate an inner region from the external environment, by means of mem-
branes, ii) select within this internal region some kinds of molecules, iii) concen-
trate the molecules which are inside, for a better chemical interaction, iv) protect
the internal space from external disturbances to the internal activity, and v) take
matter from outside and expel matter outside for feeding the internal processes and
eliminating matter which degrades or is dangerous to the internal activity.
These ﬁve points, referring to the compartmentalization of molecules, are real-
ized by membranes. However, point ii) is not enough for an efﬁcient realization of
chemical transformation. In fact, vicinity in space is important, but some agents are
necessary which recognize reactants and speed up their reactions, that is, which play
a role of catalysts. This role can be played by complex molecules, able to discrimi-
nate forms. Linear polymers provide a solution to this complex task. By using some

1.2 Sequences and Polymers
7
“ports” controlling the input/output ﬂux of molecules, protocells maintain a stable
ﬂux of in-coming and out-going matter, in order to ensure that internal chemical
species are kept within certain speciﬁc quantitative ranges. However, in this kind of
protocell, two essential requirements of life are missing: growth and reproduction.
When protocells exhibit some primitive form of these features, then they are close
to cells.
The basic problem of protocell realization is: keeping the introduction-trans-
formation-expulsion ability, and keeping the membrane structure separating the ex-
ternal environment from the interior space. It is important to realize that even these
easy actions are not as simple as they may appear at a ﬁrst glance. In fact, they
are performed without any intervention of external agents. This is not so trivial if
only molecules are available for performing all these tasks. In fact, some kinds of
molecules have to be arranged in order to construct membranes, some have to con-
trol selectively the input/output (ensuring that only certain molecules can enter and
only certain others can exit), and some molecules have to direct the chemical trans-
formations. The problem becomes even more challenging if we consider that any
kind of matter aggregation, the more complex it is, the more easily it is subjected to
degradation, therefore, after some time (depending on the kind of aggregation), it is
naturally destined to be destroyed, or to lose its functionality.
1.2
Sequences and Polymers
Sets or classes (in this context we use these terms as synonyms) are collections of
distinct objects. A set is completely speciﬁed by the elements which belong to it,
and if it has a ﬁnite number of elements, then it is completely described by a list of
them. In this list, the appearance order of the elements is not relevant, and moreover,
each element has to appear once.
Consider the set of decimal digits {0,1,2,3,4,5,6,7,8,9}. If we arrange them in
the following way:
314159265358979
we have the ﬁrst 15 digits (decimal dot is omitted) of the decimal representation
of π (the ratio between any circle and its diameter). Here, two aspects are different
with respect to sets: some symbols occur many times and the order of symbol oc-
currences is relevant. Structures with a linear arrangement of components, possibly
occurring many times, where the order and the number of occurrences is relevant
are called sequences. They are expressed in many ways, for example, by putting the
occurrences between parentheses and by separating them by commas:
(3,1,4,1,5,9,2,6,5,3,5,8,9,7,9)
however, what is essential is a clear indication of the order and the number of oc-
currences. The total number of the elements occurring in a sequence, counting the
repetitions, constitutes its length. In general, sequences are referred to a preliminary
ﬁxed set. For example, the sequence above is a sequence over the set ofdecimal digits.

8
1 Discrete Information and Life
It is easy to realize that the number of sequences grows exponentially with re-
spect to their length. For example, the number of sequences of length 100 over
20 elements is 20100, that is, a number overcoming any possibility of enumeration,
within the whole existence of the universe. Therefore, the aggregation of basic kinds
of molecules, in linear arrangements, opens an enormous possibility in the search of
complex molecules. In fact, it provides a space of molecular variability which life
can explore in the search of the complex molecular functions for assembling living
organisms, starting from molecules.
Polymers are sequences over a set of component molecules called
monomers. They have a crucial relevance in fundamental mechanisms of
life, and are the molecular basis of replication, which is essential to life
reproduction.
1.2.1
LUCA and the Sequence Paradox
What we described in Sect. 1.1.1 is more precisely a prebiotic protocell. It is only
able to maintain, for some time, an introduction-transformation-expulsion ability.
However, another feature is missing for the passage from a prebiotic protocell to a
cell in a full sense: the reproduction in two similar organisms which, in turn, are able
to perform the same task. Surely, many attempts of reproduction happened, but over
time, the reproductive ability of descendants degraded in two possible ways: i) they
lost the reproductive ability due to some internal degradation, or ii) the environment
changed and they were not able to survive. This means that, among a huge number
of attempts, possibly in different places, and in different times, life emerged, in a
full sense, when a protocell was able to produce descendants for a sufﬁcient number
of generations. One lineage acquired the property of generating stable progenies.
Therefore:
Cells originating from that ancestor protocell, called LUCA (Least Universal
Common Ancestor) are the cells of the life we know. This event is dated
around 3.8 billions of years ago.
A different possibility for the origin of life would suggest a different scenario
where the process outlined above happened many times, providing different sources
of life. In this case, it is possible that different LUCAs cooperated, in some way,
by exchanging some molecules, by keeping a sort of similarity, and surely a kind of
competition was established, in sharing common resources [7]. On the contrary, if no
cooperation/competition had been established among biotic protocells, then almost

1.3 Multisets and Membranes
9
certainly, the lineages originating from them would have followed very different
paths. This situation seems nowadays confuted by the fact that all the known forms
of life exhibit an evident common molecular basis.
A famous paradox related to polymers, attributable to Eigen [128, 137], is called
the sequence paradox and is related to the impossibility of producing replicating
polymers with a length greater than the inverse percentage of replication error. That
implies the impossibility of improving the replication efﬁciency beyond a ﬁxed
threshold (around 100 units according to Eigen’s theory). In fact, the ability of a
faithful auto-replication improves with the complexity of the polymers, and con-
sequently, with an increment of their length (longer programs are more powerful
than short programs). This means that replicating polymers need to be sufﬁciently
long in order to replicate without errors, and to keep this special property. But, at
the same time, the longer sequences are, the more errors are cumulated during their
replication (increasing the length of a text, the errors in copying it increase too).
Life had to solve the sequence paradox, and other paradoxes, in order to emerge
from the prebiotic chemistry, and in order to evolve toward more complex
organisms.
However, even if science discloses some important aspects of these paradoxes,
probably no rational reconstruction can completely disclose all the reality of life’s
mystery.
1.3
Multisets and Membranes
The notion of multiset is something between sets and sequences. In fact in a multiset
an element can occur more then once (zero times means that it does not occur), but
the order of occurrences is not relevant. A standard way for denoting this multiset
is the multiset polynomial notation such as:
2a + 3b + c.
The non-negative number of occurrences of an element a in a multiset X is also
called the multiplicity of a in X and we denote it by:
X(a).
We write:
a ∈X
if X(a) > 0, that is, when a occurs in X. Of course, a set is a multiset where all
its elements occur with multiplicity 1. A multiset Y is included in a multiset X if
Y(a) ≤X(a), for every a ∈X. In this case, we write also Y ⊆X.
Sequences and multisets over a set A, called also the support of these structures,
can be seen as particular functions. In fact, a sequence can be seen as a function from
a subset of natural numbers (position from 1 to the sequence length) to elements of

10
1 Discrete Information and Life
a set, while a multiset can be seen as a function from a set to a subset of natural
numbers (the multiplicities assigned to the elements of the set).
A sequence α of length n over A is a function: α : {1,2,...n} →A
A multiset X over A, where any element occurs at most n times, is a function:
X : A →{0,1,2,...n}.
A very useful notation which assumes many meanings, in dependence on the
context where it occurs, is the absolute value sign | |. When it is applied to a (ﬁnite)
set, X, then |X| means the number of elements of X, also called cardinality (in set
theory it extends also to inﬁnite sets, providing transﬁnite cardinal numbers). For a
sequence α, the expression |α| denotes the length of α, while for a (ﬁnite) multiset
X, notation |X| denotes its size, that is, the sum of the multiplicities of all elements
occurring in X.
Given an order among the elements of a ﬁnite set A of cardinality n, any ﬁnite
multiset X over A, is completely identiﬁed by the numeric sequence, of length
n, of the multiplicities that the elements of A have in X (according to the order
ﬁxed over A). In this sense, ﬁnite multisets coincide with ﬁnite sequences of
numbers.
The main principles of aggregation and selection mechanisms, on which life is
based, rely on the basic discrete mathematical structures of sequences and multisets.
We claim that many choices of life for realizing its strategies of expansion and
development are intrinsic to the informational logic of these fundamental structures.
1.4
Chemistry Multisets
Any molecule is a multiset of atoms providing a stable physical structure. In
molecules, multiplicities are indicated by indexes, thus CO2 has the same meaning
of C + 2O. Chemical substances are multisets of molecules. In fact, 100 molecules
of CO2 correspond to a multiset of molecules (100 copies of the same molecule).
A quantity of 12 grams of Carbon dioxide CO2 is a multiset of about 6.2 × 1023 of
CO2 molecules.
Chemistry suggests some natural operation on multisets. Firstly, given two mul-
tisets X,Y over a set A, their multiset sum X +Y can be deﬁned by setting that, for
any a ∈A, the multiplicity of a in X +Y is given by:
(X +Y)(a) = X(a)+Y(a)

1.4 Chemistry Multisets
11
where, for the sake of simplicity, the same symbol +, which aggregates elements in
multiset polynomial notation, here denotes the multiset sum operation.
Multiset sum is, of course, commutative and associative, that is for any multisets
X,Y,Z:
X +Y = Y + X
(X +Y)+ Z = X + (Y + Z).
The multiset without elements is denoted by /0 (which also denotes the empty set).
Multiset multiplication m·X of a multiset X, over a set A, by a natural number
m is deﬁned by the following equation:
(m·X)(a) = m·X(a)
where symbol · on the right hand denotes the product between numbers. If we
identify an element a as a multiset of a single object (with multiplicity 1), then
m·a = ma, that is, for single multisets, multiplication can be identiﬁed with multi-
plicity.
Multiset operations naturally occur in chemistry. In fact, a chemical reaction is
an operation transforming a multiset of molecules (reactants) into another multiset
of molecules (products).
A chemical reaction is usually denoted by an ordered pair of two multisets, with
an arrow between them:
Reactants
→
Products
A reaction such as:
X +Y →2Z
can be viewed as an operation which takes the molecules, viewed as multisets of
atoms, X and Y and transforms them into the multiset 2Z. The elements on the left
of the arrow are also called substrates of the reaction.
Let us consider a reaction of n reactants X1,X2,...Xn and m productsY1,Y2,...Ym:
The so called stoichiometric balance of such a reaction is the procedure which
provides the minimum multiplicities (if they exist) h1,h2,...hn,k1,k2,...km of reac-
tants and products such that:
(h1 ·X1) + (h2 ·X2) + ...(hn ·Xn) = (k1 ·Y1) + (k2 ·Y2) + ...(km ·Ym)
The law of multiple proportions is one of the fundamental laws of stoichiometry
and was ﬁrst discovered by the English chemist John Dalton in 1803. The law states
that when chemical elements combine, they do so in a ratio of whole numbers:

12
1 Discrete Information and Life
In any chemical reaction, the coefﬁcients of the reactants and products are
always multiples of some positive integer coefﬁcients, which are called stoi-
chiometric coefﬁcients.
This principle is a consequence of the fact that molecules are multisets of atoms
and the atoms in each molecule occur with an integer positive multiplicity, therefore,
for the conservation principle, the multisets of atoms corresponding to molecules
have to occur with positive integer multiplicities. The Italian scientist Avogadro
introduced a number as a standard value for indicating a population of molecules.
Its value corresponds to the number of hydrogen molecules contained in one gram
of this substance, its value was estimated as (the two digits between parentheses are
the standard deviation of the last two digits):
6.02214179(30)× 1023.
A concrete example of stoichiometric coefﬁcients, for balancing a chemical reac-
tion, is given by one of the most important reactions for life. The sugar glucose
C6(H2O)6 and oxygen O2 are formed from water H2O and carbon dioxide CO2, by
means of chlorophyll synthesis. In this case, the stoichiometric coefﬁcients which
provide the chemical balance are the following (the light energy is converted into
the chemical energy of glucose, which, burning in presence of oxygen, reverts the
process by producing energy):
6CO2 + 6H2O →C6(H2O)6 + 6O2.
1.5
Liposome Membranes
If we need to put together a number of molecules of some types, we need to collect
them in a space containing them. A realization of this compartmentalization is bio-
logically provided by liposomes. They are membranes realized in a very simple and
efﬁcient manner, by using a special kind of organic molecules, called phospholipids,
which, put in the water, are subjected to two opposite forces. In fact, they are asym-
metric, with one head and one tail. The head part is hydrophilic, while the tail part is
hydrophobic. In the water, they can solve the hydrophilicity versus hydrophobicity
contradiction by aggregating each one, side by side, in such a way that all heads re-
main externally in contact with water, while the tail of any phospholipid is opposite
to the tail of another phospholipid (see Fig. 1.1). In this way, a “bilayer” structure is
realized, where tails remain dry, while heads are wet.
The emergence of life requires that a number of reactions may work, and persist
for some time. Reactions need membranes where reactants are collected as multisets

1.5 Liposome Membranes
13
of molecules, spatially concentrated and protected from external noises. In conclu-
sion, the following statement summarizes the biological cruciality of membranes.
Membranes built by phospholipids need water, and stable reactions need
membranes, therefore life, as we know it, needs water.
Fig. 1.1 The schema of a liposome (GNU Free Documentation License)
Fig. 1.2 Aggregation according to phospholipid bonds (top) and to phosphodiester bonds
(bottom)
Figure 1.2 shows the two fundamental kinds of biological aggregations. The one
at the top is typical of biological membranes. The one at bottom refers to DNA
molecules. Both aggregations are based on double links. But, in phospholipids
we have a bilayer schema where ﬁrstly bilayer double monomers of type: (head-
tail)+(tail-head) are realized, and then these double monomers become elements
of aggregations along all directions (almost) parallel with the axis passing through
their heads. In this way, two surfaces, having heads externally, are in contact with

14
1 Discrete Information and Life
Fig. 1.3 Stylized images of phospholipid aggregation of bilayer monomers around their
head-head axis
Fig. 1.4 A semi-complete liposome aggregation with a tail-uncovered border
water, while double tails are waterproofed by the double tails around them in the
bilayer aggregation. The two surfaces showing the heads of double monomers pro-
vide one internal and one external spherical surface (sphere is the minimum surface
enclosing a given volume). The second arrangement develops according to a bi-
linear schema. In this case, ﬁrstly a linear catenative arrangement is realized by
sequences of type: (head-tail)+(head-tail)...+(head-tail), then two lines are paired
(at least in regular situations). In this case, pairing can be established only between
two corresponding monomers of the paired lines, by means of a speciﬁc molecular
bond between them (having complementary types, see Fig. 1.13). A crucial aspect

1.6 Populations and Hypermultisets
15
Fig. 1.5 Two semi-complete liposome aggregations that provide a membrane by joining their
tail-uncovered borders
explaining the difference between the spherical and bilinear shapes is given by the
symmetric nature of double monomers with respect to their head-head axis, in op-
position with the asymmetric nature of the head-tail monomers which are catenated
in the two lines. DNA monomers, as will be explained in the next chapter, are asym-
metric with respect to the head-tail axis, and along this direction can be established
a pairing only with another monomer. Therefore, the double DNA arrangement is
a “bilinear concatenation”, in opposition to the liposome arrangement which is a
“bilayer (closed) convolution”. As will be explained in the next chapter, under very
general hypotheses, bilinear catenative arrangements will provide helical shapes.
Another crucial difference between phospholipid and DNA aggregations is due
to the kind of molecules that they aggregate. In the phospholipid case, there is only
one type of head-tail monomers, while in the DNA case there are four different
head-tail monomers. This is the reason for the enormous number of combinatorial
possibilities for DNA molecules.
We conclude with the following statement.
Both aggregations of phospholipids and DNA molecules are based on asym-
metric monomers. Phospholipid aggregation is based on homogeneous bilayer
double monomers, which are symmetric with respect to the head-head axis.
DNA aggregation is based on heterogeneous double linear concatenation of
monomers, which are asymmetric with respect to the head-tail axis.
1.6
Populations and Hypermultisets
Chemistry deals with multisets at two different levels. In fact CO2 is a multiset over
the set {C,O}, while 6CO2 + 6H2O is a multiset over the set {CO2,H2O}. In other
words:
6CO2 + 6H2O = 6(C + 2O)+ 6(2H + O).

16
1 Discrete Information and Life
In general, the notion of multiset can be considered at any level, because multisets
of second level can be considered as objects which can be aggregated by providing
multisets of third level, and the same kind of construction can be iterated. We re-
serve the word population for denoting multisets of the ﬁrst level, while the term
hypermultisets is used for multisets at a level greater than one. Therefore, a pop-
ulation over a set A is a multiset of elements which belong to A, and a multiset of
level k > 0, over a set A, is a multiset of elements which are multisets of levels lower
than k (over A).
Populations are ubiquitous in life phenomena, at any level, from populations of
molecules, to populations of organisms and species. However, the peculiarity of
membranes is that they provide a biochemical counterpart of parentheses of mathe-
matical constructs. In fact, when molecules are put inside membranes, these become
elements of other aggregation levels, where they are elements (possibly occurring
in many copies) of a second level multiset. We note the following statement empha-
sizing the structural role of membranes.
Membranes constitute the biochemical realization of hierarchical multiset ag-
gregation of biological components.
If we consider an atom as a multiset of subatomic particles, that is, protons, neu-
trons, and electrons, then a molecule such as CO2 is a multiset of two levels:
CO2 = C + 2O = (6 ·(p + n + e))+ 2(8 ·(p +n+e)).
Analogously to hypermultisets, we can consider hypersets and hypersequences
(however, this is not standard terminology). A hyperset is a set including sets as
its elements. For example, {a,{a,b}} includes as element {a,b} which is the set
of elements a and b. A hypersequence is a sequence of sequences. For example,
(a,(a,b),(a,(a,b))) is a hypersequence. Any sequence can be represented by a hy-
persequence constituted by pairs (sequences of length 2). For example (a,b,c) can
be represented by ((a,b),c). The notion of level for hypersets and hypersequences
can be deﬁned as in the case of hypermultisets. It is easy to realize that hyperstruc-
tures are expressed, up to here, using parentheses. For example, a hypersequence of
level two has two levels of parentheses. This concept is the basis of hierarchies, and
trees are the mathematical concept behind any notion of hierarchy, as we show in
the next section.
Before considering trees and graphs in basic structures of life, let us conclude
regarding the intertwined roles of membranes and polymers. The basic mathemati-
cal structure underlying molecules and reactions are multisets. Efﬁcient realizations
of chemical reactions require complex molecules facilitating and driving them (en-
zymes), and a compartmentalization of molecules in order to select, concentrate, and
protect all the elements involved in reactions. Membranes provide the biological so-
lution to compartmentalization, and polymers, which are sequences of monomers,
provide the enormous molecular variety and complexity within which molecular

1.6 Populations and Hypermultisets
17
functionalities can be found for life ﬁnalities. Membranes and polymers are gener-
ated by suitable mechanisms of biochemical aggregation. However, only the aggre-
gations that are involved in processes of organisms able to survive, reproduce, and
evolve, are maintained by life genealogies. In conclusion, aggregations in space,
based on the mathematical structures of multiset and sequence, are selected along
time for life propagation and evolution. Tables 1.2 and 1.3, and Fig. 1.6 summarize
and visualize basic aspects of biological aggregation.
Table 1.2 Basic mechanisms of matter organization
Aggregation
Stability in space
Selection
Stability in time
Table 1.3 Correspondence between basic mathematical and biochemical aggregations
Multisets
Membranes
Sequences
Polymers
Fig. 1.6 The basic aggregation mechanisms of life: membranes and polymers

18
1 Discrete Information and Life
1.7
Trees and Hierarchies
A natural way to express a hypermultiset, built from some initial elements, is by
means of diagrams such as those given in Fig. 1.7. Each level of aggregation cor-
responds to a closed plane curve embracing the elements which are aggregated,
possibly occurring in many identical copies. Moreover, the 2-dimensional nature of
this kind of diagram represents adequately the lack of order among the components
inside each closed curve. The diagram at the bottom of Fig. 1.7 is a diagram of a
rooted tree, a typical structure representing genealogies. The root is the node at
the top of the diagram (the origin of the genealogy), and each element that is the
parent of some child node is an internal node (the root is an internal node), while
other nodes, called leaves, do not have child nodes, and the nodes having the same
parent are sibling nodes (some authors use the masculine designation ”father, son,
brother”, while others use the feminine designation ”mother, daughter, sister” in-
stead of ”parent, child, sibling”). Terms ancestor and descendant refer to nodes that
are the beginning and the end, respectively, of a chain of parent-child relationship
(another way of expressing a tree is by means of a parent function, assigning to any
node different from the root its parent node).
Life tree represents the hierarchical structure of classiﬁcation of living organ-
isms. The tree of Fig. 1.8 (http://en.wikipedia.org/wiki/Phylogenetic−tree) is a life
evolution tree.
The trees of Fig. 1.9 show the difference between prebiotic and biotic trees. The
biotic tree has leaves connected to the Last Universal Common Ancestor by an un-
interrupted genealogical chain going from it down to the current living organisms.
Fig. 1.7 The membrane diagram of the multiset 3[2a+b] +2c+3[a+c] on the top, and the
tree diagram of the same multiset on the buttom

1.7 Trees and Hierarchies
19
Fig. 1.8 Life as a tree, reconstructed by ribosome RNA sequencing (Credit: Nasa Astrobiol-
ogy Institute, Public Domain)
Fig. 1.9 The left tree is a pre-biotic evolution which ends. The right tree is a biotic evolution
with non-ending paths.

20
1 Discrete Information and Life
1.8
Graphs and Interactions
A molecule is built over a multiset of atoms, but this multiset does not provide all
the information about the relations holding among its atoms. In chemistry, the mul-
tiset of a molecule is also called molecular formula, while the structural formula is
a graph, that is a set of nodes connected by edges or arcs (usually drawn by lines
or curves, oriented or not, and possibly tagged with speciﬁc data). Two molecules
having the same molecular formula, but different structural formulae are called
isomers.
A graph is the mathematical abstraction of point-line (or point-arrow) diagrams.
In any context where some entities are related, possibly in many different ways,
graphs naturally emerge. Mathematically speaking, a graph is a set of “arrows”
(edges, arcs), each of which connects two nodes (tags, or marks may be associated
to nodes and/or arrows). The terminology of graphs is very intuitive and all the
important features of a graph, such as path, cycle, degree, component can be mathe-
matically expressed by using the basic notions of set, sequence, multiset, operation,
and relation. For example, a path is a sequence of nodes, where consecutive nodes
are connected by an edge. A graph is connected if any two nodes are connected by
some path. A cycle is a path where the ﬁrst node coincides with the last one. A graph
is acyclic if it does not have cycles. The graphs which are connected and without
cycles are also called unrooted trees.
From an unrooted tree, a rooted tree can be univocally obtained by choosing one
node as root. Connection and absence of cycles ensure this possibility. In fact, any
node has to be connected to some other node. Therefore, from the chosen root we get
its sons and, iteratively, from them we get other nodes until we reach some leaves.
No node can be reached twice, because the graph is assumed without cycles.
1.8.1
Graphs of Molecule Structures
Molecules are graphs where nodes are atoms (or atom aggregates) and edges are the
chemical bonds among atoms (some tags can be added to nodes and/or edges for
expressing speciﬁc properties of chemical interest, such as the types or strengths of
bonds). For example, a basic components of RNA and DNA molecules are pentoses,
sugar molecules constituted by carbons and water molecules. Ribose C5H10O5 is a
pentose which includes ﬁve carbons and ﬁve molecules of water (H2O). The posi-
tions of the ﬁve carbons in pentoses are identiﬁed by primed numbers. Deoxyribose
C5H10O4 derives from ribose by removing an Oxygen in position 2′.
Figure 1.10 represents the graph of the structural formula of a nucleotide, the
basic component of DNA polymers. It is constituted by three parts:
1. deoxyribose,
2. nitrogenous base (adenine in Fig. 1.10),
3. phosphate group PO4.

1.8 Graphs and Interactions
21
In the graph of Fig. 1.10, nodes are atoms or groups of atoms, and edges represent
the chemical bonds between them. Table 1.4 gives the complete list of Nitrogen
Bases, which correspond to the letters of DNA and RNA alphabets.
In the deoxyribose molecule, Carbon at position 1′ is connected to the nitroge-
nous base. Position 2′ is where Oxygen is missing, respect to ribose, and position 5′
is the position where Carbon shares an Oxygen with the phosphate group. Bases are
connected by means of a bond between Nitrogen and Carbon (a glycosidic bond)
which is established by liberating a water molecule.
When a nucleotide is linked to another nucleotide, then phosphate group becomes
the bridge between the 5′ Carbon with the 3′ Carbon of the other nucleotide. In this
link, OH molecule at position 3′ of pentose (deoxyribose in DNA case, or ribose in
RNA case) is replaced by an Oxygen of the phosphate group.
Table 1.4 The nitrogen Bases of DNA. Uracil substitutes Thymine in RNA.
Adenine
A
C5H5N5
Guanine
G
C5H5N2O2
Thymine
T
C5H6N2O2
Cytosine
C
C4H5N3O
Uracil
U
C4H4N2O2
It is interesting that the nucleotide corresponding to the letter A of the DNA al-
phabet has a crucial role from the point of view of biological energy. In fact, the
Adenosine triphosphate is constituted by the adenine nucleotide, where deoxyri-
bose is replaced by ribose and the phosphate group is a triphosphate (see Fig. 1.12).
This molecule is the so-called unit of energetic currency in biomolecular trans-
formations. When energy is required, then a reaction, from ATP to the molecule
ADT (Adenosine diphosphate, with a diphosphate group) is simultaneously associ-
ated, which liberates both a phosphate group and an energy quantity (7 Kcal/mole
at 37o Celsius). Figure 1.12 shows this transformation, which is essentially an
operation transforming a connected graph into another one with two connected
components.
RNA structure is similar to that of DNA, with the only difference that in RNA
the basic components are ribonucleotides (in a ribose the Carbon at 2’ binds
H2O instead of H2). Ribo-nucleotides concatenate each other, by providing RNA
strands.
DNA nucleotide concatenate in double strands, according to the schema of Fig.
1.13, where a nucleotide of a strand is paired with a nucleotide of the other strand
if the two corresponding bases are a complementary pair. The two complementary
pairs are {A,T} and {C,G} (a pairing of a DNA strand with a RNA strand is also
possible, according to the complementarity {A,U}, {C,G}). The three principles of
this molecule arrangement are: bilinearity, complementarity, and antiparallelism.

22
1 Discrete Information and Life
Fig. 1.10 The graph of the structural formula of Adenine nucleotide. The atoms of Carbon
in positions 1′,2′,3′,4′ are not indicated.
1.8.2
Graphs of Reactions
Molecule structures are represented by graphs, but even chemical reactions acting
cooperatively and competitively over some substances are helpfully represented by
reaction graphs. A chemical reaction is naturally represented by means of a multi-
edge, which extends the notion of edge, because it has a set of source nodes and a set
of target nodes (it can be represented by a body with possibly many tails and many ar-
rows, as it is illustrated by Fig. 1.14). Namely, in a chemical reaction many reactants
are connected to many products. Multigraphs, which are based on Multi-edges, can
be also seen as graphs with two kinds of nodes (circles representing substances and
full circles representing reactions), and with two types of edges (tail edges and head
edges). Figure 1.14 describes chemical reactions expressed by a multigraph.
Fig. 1.11 The Nitrogenous bases

1.8 Graphs and Interactions
23
Fig. 1.12 The transformation from ATP to ADT molecule. On the right, an excess of Hydro-
gen ions is present, which is not indicated.
Fig. 1.13 The schema of a DNA double strand, where Z denotes the sugar, P the phosphate,
and A, T. C, G the bases
Fig. 1.14 A chemical reaction as a multigraph

24
1 Discrete Information and Life
1.8.3
Neuron Graph
The structure of our brain is a graph built on a networks of about 1011 nodes (neu-
rons) connected by means of 1014 edges (synapses). Figure 1.15 reports an origi-
nal drawing by Ramon y Cajal, the Spanish scientist who discovered the neuron,
on the basis of a visualization technique introduced by the Italian scientist Camillo
Golgi (both scientists received the Nobel prize for their discoveries), while Fig. 1.16
(http://www.sciencecases.org/split−brain/split−brain.asp) provides a schematic rep-
resentation of its structure.
Fig. 1.15 An original drawing of a neuron by Ramon y Cajal
Fig. 1.16 A schematic structure of a neuron (Credit: National Institute on Drug Abuse)

1.8 Graphs and Interactions
25
Fig. 1.17 A kind of graph corresponding to a neuron structure
Fig. 1.18 The structure of an artiﬁcial neural network
Neurons suggested one of the ﬁrst computation models elaborated by McCul-
louch and Pitts in 1943 [9], which was the basis of circuits of the EDVAC com-
puter designed by John von Neumann in 1945 [207]. Artiﬁcial Neural Networks
(ANN) became popular, in many variants, in typical problems of artiﬁcial intel-
ligence (recognition, learning, classiﬁcation, and robotics) [8]. Fig. 1.18 shows a
graph representing a general kind of neural network. In this case, nodes are dis-
posed in successive levels (columns of nodes, usually from left to right). Edges
connect nodes of one level to nodes of the next level, and are labeled with numerical
coefﬁcients called weights. A node contains a current value (initially it may be some
default value). The current values of nodes change in time according to the follow-
ing strategy. A activation function is associated to each node. The current value of
a node is passed, to the connected nodes of the next level, after multiplying it by
the corresponding weights of the connecting edges. The weighted values passed to
a node are given as arguments of the activation function of the node, and the result
becomes the new current value of the node. In this way, when some input values

26
1 Discrete Information and Life
are given as current values of the leftmost nodes, after a number of steps, depend-
ing on the number of levels of a given ANN, the current values of the rightmost
nodes provide the output values of the ANN in correspondence to the given input
values. In this sense, an ANN with n input nodes and m output nodes computes
a function from numerical n-sequences to numerical m-sequences. A typical prob-
lem of ANNs is the following inverse problem: given a set of k pairs (Xi,Yi), for
i = 1,2,...,k, where Xi and Yi are input and output sequences, ﬁnding an ANN able
to provide, in the most faithful way, the given correspondence, that is, providing the
best approximating function that underlies the given input-output pairs.

Platonic Octahedron
2
Strings and Genomes
Abstract. Strings constitute the mathematical structures of informational biopoly-
mers. Genomes are long strings (hundreds of thousands, or millions, or billions
of characters) built over the four nucleotides, and many typical operations over
genomes are naturally expressed by string operations. In this chapter we present
basic concepts about DNA molecules and genomes in algorithmic terms, by empha-
sizing the roles of strings, formal languages, and multisets of strings in the anal-
ysis of typical biological and biotechnological DNA manipulations. We conclude
by outlining some research lines of genome analysis which are based on genomic
dictionaries. The chapter is mostly based on the author’s published papers (see
References for Chapter 2).
2.1
Biological Monomers and Polymers
Words of written alphabetic languages are the most usual intuition of symbolic lin-
ear forms. However, if we were to create, at a biomolecular level, structures similar
to words, then we would experience an essential difﬁculty. In fact, letters are ar-
ranged over an external rigid support (paper) maintaining their linear arrangement
stable and robust despite the movements of the support, while molecules are ﬂoat-
ing in a liquid environment. Therefore, we need a different way to arrange them. In
other words, the linearity has to be implemented by means of a feature internal to
molecules. This is the reason for the following structure which is common to the
most important biological monomers: body, head, tail, ﬂag, and bridge. The body is
the component of the monomer to which head, tail, and ﬂag are connected. The link
V. Manca: Infobiotics, ECC 3, pp. 27–105.
DOI: 10.1007/978-3-642-36223-1_2
c⃝Springer-Verlag Berlin Heidelberg 2013

28
2 Strings and Genomes
between two monomers is obtained by connecting the head of a monomer with the
tail of another monomer in a resulting component which is the bridge of the com-
posed structure. When the link is performed we get a polymer, or more precisely a
dimer, having a head and a tail: the head is the head of the monomer which fused its
tail in the bridge, while the tail is the tail of the monomer which fused its head in the
bridge (see Fig. 2.1 and Fig. 2.2). In this construction we have a result the linear ar-
rangement of the two ﬂags of the linked monomers, and we can continue the process
by linking, in the same way, the obtained polymer with another monomer or with
another polymer. This means that the ﬂag is the variable component of monomers,
or what makes its speciﬁc role in the construction of polymers.
The reader is invited to verify that this structure is present in the nucleotide given
in Fig. 1.10, where the body is the sugar, the head and also the bridge is the phos-
phate group, the tail is the oxhydryl group OH in 3’, while ﬂags are the nitrogenous
bases.
For peptides we have that the body is a Carbon-Hydrogen group, the head is the
amine group NH2, the tail is the carboxyl group COOH, the bridge is the group
CONH, and ﬂags are the 20 peptide residues.
Fig. 2.1 The structure of a biological monomer
2.2
DNA Strings and DNA Helix
DNA molecules realize a special structure of strings. For this reason, DNA ma-
nipulation can be formally described in terms of string operations. However, DNA
strings are more precisely double strings and their physical nature implies their spe-
ciﬁc geometric form of a helix. In the following section we discuss these aspects.

2.2 DNA Strings and DNA Helix
29
Fig. 2.2 The structure of a peptide link. The peptide structure above, the peptide bond below.
The bridge links two peptides, by fusing the head NH2 of one peptide with the tail COOH of
the other in CONH (a water molecule H2O is released).
2.2.1
DNA Notation and Double String Operations
Let us recall and extend some standard notation from Formal Language Theory
(see [213], and Chap. 6) in order to formalize fundamental operations related to the
structure of DNA molecules.
Let us consider the usual alphabet of bases Γ = {A,T,C,G}. The set Γ ∗of strings
over this alphabet is comprised of the sequences (words) that can be arranged with
these four symbols (letters). Strings of Γ ∗will be indicated by Greek letters. On
these strings, a binary associative operation of concatenation is deﬁned, that given
two strings of Γ ∗yields a new string where all the symbols of the second sequence
are put, in the given order, after the last symbol of the ﬁrst one. Concatenation be-
tween two strings α and β is denoted by the juxtaposition αβ. The length of a string
α is the number of its symbol occurrences (each symbol is counted as many times as
it occurs) and is indicated by |α|. A special string is that of length 0, denoted by λ,
that is, the empty string, where no symbol occurs (an abstract notion similar to zero
for numbers). Symbols are special strings of length 1. Mathematically speaking, the
structure Γ ∗is referred to as the free monoid over the alphabet Γ . Any subset of Γ ∗
is a (formal) language over the alphabet Γ . Since languages are sets, all the usual set
theoretical notions extend to languages (such as membership ∈, inclusion ⊆, empty
set /0).
The symbol of α that occurs in position i (1 ≤i ≤|α|) is denoted by α(i), the
sequence of symbols of α occurring (in the given order) from position i to position j

30
2 Strings and Genomes
(1 ≤i ≤j ≤|α|) is denoted by α[i, j] and is called a substring of α. In particular, it
is called a preﬁx if i = 1, and a sufﬁx if j = |α|. A string with preﬁx α and sufﬁx β is
also denoted by α ...β. The complementation function, denoted by the superscript
c, is deﬁned on Γ by Ac = T,T c = A,Cc = G,Gc = C. It extends naturally to Γ ∗by
the conditions λ c = λ and (αβ)c = αcβ c. Another operation on Γ ∗is the reverse
operation rev such that, for every X ∈Γ , rev(X) = X, rev(αX) = Xrev(α), and
rev(λ) = λ.
Reversing and complementation operations are involutive and commute, that is:
rev(rev(α)) = α
(αc)c = α
and
rev(αc) = (rev(α))c.
The mirroring of α is deﬁned by:
mir(α) = rev(αc)
and is an involutive operation. We abbreviate mir(α) as ¯α.
In DNA molecules, an intrinsic concatenation verse is given which goes from the
Phosphate to the Oxydryl (that respectively correspond to Carbon 5′ and 3′ positions
in the sugar DNA backbone). This verse is symbolically denoted by an arrow at one
side of non-null strings α, as in:
α →
that can be seen as an indication of the extremity where the Oxydryl terminal is
located (while Phosphate is at the other extremity). However, when we omit the
arrow, the usual reading from left to right is assumed, which corresponds to the
5′ −3′ verse.
An exact pairing (symmetric) relation || is deﬁned over Γ ∗such that:
α||β
if β = mir(α).
Let γ be the longest string such that α and β include γ and ¯γ respectively. Let
hhybr be a non-null length that we call hybridization threshold (it represents a bi-
ological parameter we leave unspeciﬁed). If |γ| > hhybr, then we say that α and β
pair by hybridizing on γ (or shortly, that they hybridize) and write:
α]γ[β.
In this case a double DNA string composed of α and β is deﬁned, that will be
denoted by:
α
rev(β).

2.2 DNA Strings and DNA Helix
31
In this fraction notation the following concatenation verses are implicitly assumed:
α →
←rev(β)
that is, the Oxydryl terminal of the inferior string is located on the opposite side
with respect to that of the superior string. Sometimes, fraction notation is a way to
make explicit the verse of single strings. In fact, we put (superior notation):
α
λ = α →
and (inferior notation):
λ
α = ←α = rev(α).
For notation completeness it is assumed that:
λ
λ = λ.
We write:
α][β
if α and β hybridize (with an exact pairing on some internal portions) and
α ̸][ β
when they do not hybridize. Pairing is a crucial operation of double strings forma-
tion, where the phenomena of bilinearity, complementarity, and antiparallelism are
jointly involved. It is a partial operation that is deﬁned on two strings only when they
hybridize. The set of single or double DNA strings will be denoted by Γ ∗/Γ ∗. By
the symmetry of the pairing relation, double strings satisfy the following equation:
α
rev(β) =
β
rev(α).
This is an important aspect of double strings, that formalizes the mobility of DNA
strands in a ﬂuid environment (superior and inferior positions are relative concepts).
On the other hand, any α hybridizes exactly with ¯α by deﬁnition of exact pairing,
hence the double string:
α
rev( ¯α)

32
2 Strings and Genomes
is deﬁned, which is called a blunt double string and is denoted by < α >. We note
that:
< α > =
α
rev( ¯α) =
α
rev(rev(αc)) = α
αc
and, as a consequence of the equations above we obtain:
< ¯α > = < α >
in fact, we have that:
< ¯α >=
¯α
rev( ¯α) =
¯α
rev(α) =
α
rev( ¯α) =
α
rev(rev(αc)) =< α > .
Operations on DNA strings Γ ∗/Γ ∗are summarized in Table 2.1.
We call strand any object s on which an operation type is deﬁned which assigns
to it a string in Γ ∗/Γ ∗. In particular, if type(s) is a single string, then we say that s is a
single strand, while if type(s) is a double string, then we say that s is a double strand.
Intuitively, strands are DNA molecules which, as physical objects, are different from
the base sequence they realize. Therefore, in our terminology, strands having the
same base sequence are strands with the same type. We restrict ourselves to consider
only single or double strands. In fact, for the needs of the following discussion, we
may avoid considering more complex forms of DNA molecules that combine more
than two DNA strands.
We consider a DNA pool P as a set of strands, or equivalently, as a multiset of
single or double strings of Γ ∗/Γ ∗, which is speciﬁed by a multiplicity function multP
from Γ ∗/Γ ∗to natural numbers. In fact, multP(η) = n means that the pool P contains
n (indiscernible) strands of type η. We write P = {n1 : η1, n2 : η2, ..., nk : ηk}
when multP(η1) = n1,multP(η2) = n2,...,multP(ηk) = nk and multP(η) = 0 for
η ̸∈{η1,η2,...,ηk}. Mixing and splitting DNA pools correspond to the standard
multiset operations of sum and difference, denoted by +,−( sum and difference of
their multiplicity functions).
In virtue of these two ways of considering a pool, we can use (ambiguously) both
notations: η ∈P, for a string η ∈Γ ∗/Γ ∗, meaning multP(η) ̸= 0, and s ∈P, when s
is a strand of P. The type of pool P is the set of strings (a language):
Type(P) = {η ∈Γ ∗/Γ ∗| η ∈P} = {type(s) ∈Γ ∗/Γ ∗| s ∈P}.
We remark the difference between type, which assigns a string to a strand, and
Type (with capital T) which assigns, to a pool of DNA strands, the set of types of its
strands. Although strings and strands are different things, very often the two terms
are used almost synonymously. In fact, any string is physically implemented by
strands having its type, and conversely, the expression “a string”, in a given context,
could refer to a physical occurrence of a string, which is just a strand.

2.2 DNA Strings and DNA Helix
33
Table 2.1 Basic DNA operations
α[i, j]
Substring
αβ
Concatenation
(α)c
Complementation
rev(α) Reversing
mir(α) Mirroring
α][β
Hybridization
α
β
Pairing
< α >
Blunt Pairing
2.2.2
DNA Helix
DNA molecules are constituted by nucleotides arranged in a bilinear structure.
Bilinearity is accompanied by two other features: complementarity and anti-
parallelism. Both these features are consequences of the template driven dupli-
cation of DNA, which seems to be essential to the nature of this molecule. In fact,
template duplication is performed in two basic steps: removing the bonds which tie
the two paired sequences and then using each of them as a template by restoring the
missing paired sequence with the appropriate nucleotides matching the templates. In
this manner the two templates produce two equal double strands. In this procedure,
it is essential that paired strands could be unpaired. For this reason any nucleotide
has to be paired by a weak chemical bond, which is better realized between different
(complementary) molecules. In biological organisms this kind of duplication is per-
formed by a class of enzymes called Polymerase. Their action is essentially called
extension, in fact they extend a strand in the verse 5′ to 3′ by copying the missing
(complementary) nucleotides according to the sequence speciﬁed by the template
strand (by using nucleotides ﬂoating in the environment). However, their action can
be performed only when an initial part of the missing strand, usually called primer
is already present (see Fig. 2.3).
A bilinear structure which realizes a template driven duplication is described in
Fig. 2.4.
From an abstract viewpoint, a monomer M of a bilinear structure is charac-
terized by a triangle, say a monomeric triangle. In fact, let us consider an
internal point P of this monomer, for example its barycenter, which we call its
X-point. Then, the monomeric triangle is deﬁned by the three point P,P′,P′′
where P′ is the X-point of the monomer M′ concatenated to M, and P′′ is the
middle point between P and the X-point of the monomer M′′ paired with M
(see Fig. 2.4).

34
2 Strings and Genomes
Fig. 2.3 Template driven sequential duplication
Fig. 2.4 The bilinear arrangement of monomeric triangles

2.2 DNA Strings and DNA Helix
35
A monomeric triangle deﬁnes a Cartesian system where abscissa, or x-axis, goes
from P to P′, ordinate or y-axis goes from P′ to P′′, and altitude or z-axis is
orthogonal to the xy-plane and oriented according to a counterclockwise rotation
of x-axis toward y-axis. The pairing has a strictly dual nature: only two monomers
can be paired, because this relationship is exclusive, and when two monomers are
paired, both of them are unavailable to be paired with something else. In conclu-
sion, monomers need to be asymmetric with respect to two distinct directions, the
concatenation direction and the pairing direction. In fact, both these two relations
are intrinsically oriented. For this reason, monomers are chiral (from a Greek term
for hand), that is, each monomer deﬁnes univocally a three-dimensional Cartesian
coordinate system.
A monomeric triangle clearly deﬁnes a plane. Therefore, we can consider dif-
ferent possibilities for the planes where concatenated and paired monomeric
triangles lie.
Figure 2.5 shows some possible planar bilinear arrangements of monomers in the
different cases of right, acute, or obtuse monomeric triangles in parallel and antipar-
allel arrangements. Apart from the difﬁculty of keeping planar structures in a ﬂuid
environment, the space occupancy of these structures would become prohibitive for
long DNA molecules. In all these cases, no rotation of paired monomeric triangles
is allowed around any axis lying in the plane of concatenation. Namely, as indicated
at the bottom of the ﬁgure (in the case of a right angle parallel arrangement) such a
kind of rotation would be in conﬂict with the parallelism (or anti-parallelism) of the
these concatenated structures.
If concatenated triangles, lying on the same or different planes, form an angle
along the concatenation line, then we can have the possibilities illustrated in Fig.
2.6. In both possibilities the concatenation angles vary along the concatenation line,
because in a spiral the curvature increases from the periphery to the center (a log-
arithmic spiral could avoid the angular variability, but its space occupancy would
be prohibitive). Therefore, this kind of arrangement is impossible, because it would
imply an angle between two concatenated monomers that depend on their positions
in concatenation line.
In the cases of acute or obtuse monomeric triangles, parallel arrangements are
impossible, as indicated in Fig. 2.7, because the bilinear structure cannot be realized.
When monomeric triangles are acute or obtuse, they can be arranged in antipar-
allel way (see Fig. 2.8), and the arrangement of both concatenation lines in the same
plane can be avoided by means of a rotation along the pairing line, as indicated at
the bottom of the ﬁgure. Of course, acute monomeric triangles realize more compact
arrangements than obtuse monomeric triangle, therefore are preferable.

36
2 Strings and Genomes
Fig. 2.5 Parallel and antiparallel arrangements of bilinear monomeric triangles
Fig. 2.6 Impossible linear arrangement of right monomeric triangles with a rotation an-
gle between concatenated monomers: Top: rotation of concatenated triangles around z-axis;
Bottom: rotation of concatenated triangles around y-axis (xyz, the Cartesian system of the
monomeric triangles)

2.2 DNA Strings and DNA Helix
37
Fig. 2.7 Parallel arrangement of acute (top) or obtuse (bottom) monomeric triangles are im-
possible
Figures 2.9 and 2.10 are different looks of the same structure, where paired
monomeric triangles form a rotation angle around the pairing line. This implies
that they can be located inside a cylinder shape that is completely characterized by
a radius and by three angles: the rotation angle ρ, between two concatenated trian-
gles, the rotation angle Φ between two paired triangles, and the angle τ between
the concatenation segment of the triangle with the cylinder axis. From an evaluation
of these values, and of cylinder radius, the average length of edges of monomeric
triangles can be estimated (fractions of a nanometer) [44].
When many modules of the kind given in Fig. 2.10 are arranged, we obtain the
bilinear structure with a spiral shape, which grows internally to a cylinder, as shown
in Fig. 2.11, that is, the DNA double helix structure appears in its pure geometrical
form.
In Figure 2.12, we can see the empty internal cylinder formed by the paired
monomeric triangles along the DNA helix.

38
2 Strings and Genomes
Fig. 2.8 Planar arrangement of acute or obtuse monomeric triangles is possible in an antipar-
allel way
Fig. 2.9 Two paired monomers inside a cylinder

2.2 DNA Strings and DNA Helix
39
Fig. 2.10 Another look of two paired monomers inside a cylinder
Fig. 2.11 The helix of paired monomers inside a cylinder

40
2 Strings and Genomes
Fig. 2.12 A view of the helix from upstairs
We know that DNA helix presents two groves: a major grove and a minor grove.
Figure 2.13 presents a stylized view of this structure, as a consequence of the double
rotation of the two paired lines around the same cylinder axis. This is the ideal
geometric structure underlying the fundamental discovery by Watson and Crick in
1953 [63].
Fig. 2.13 A stylized representation of DNA groves
In conclusion, anti-parallelism is a direct consequence of the bilinear nonpla-
nar arrangement of nucleotides, as deduced by the analysis developed in terms
of monomeric triangles. Moreover, the chirality of monomers joined to their anti-
parallelism implies that if they have the same chirality, then a reading agent can
read both the two concatenation lines along the same reading plane. Therefore,
DNA structure is implied by algorithmic arguments related to the fundamental

2.2 DNA Strings and DNA Helix
41
duplication mechanism of this molecule, and DNA helix is implied by simple ge-
ometrical principles aimed at satisfying economy in space occupancy and an efﬁ-
cient associative mechanism based on hybridization. An important consequence of
the helix arrangement of the two strands is the possibility of a second level of DNA
packing where the DNA “rope”, packed in the helix cylinder, can be again wound by
wrapping it around some support (histone proteins, where DNA is wrapped around
forming spherical packing units, glued by chromatin proteins, called nucleosomes).
As is well, known in the craft of rope-making, even since ancient times, a “screw
thread” is a helical ridge on the outside of a screw, or bolt, or on the inside of a
cylindrical hole, to allow two parts to be screwed together. The same logic underlies
a DNA formation, where strands are twisted together along a helix. This structure,
not only provides the bilinearity postulated by the template driven duplication, but
increases the robustness of the linear structure, making it able to keep its integrity in
spite of its length, and in spite of other wrapping levels. Another illuminating com-
parison regarding DNA anti-parallelism can be found in folk dances where many
dancers are arranged in two paired rows. Each dancer D corresponds to the dancer
D’ in front of him/her in the paired row, but due to the chirality of the human body,
the dancer who is on the right of D corresponds to the dancer who is on the left
of D’. It is surprising that rope technology and the art of dancing share important
aspects with DNA structure.
Figure 2.14 shows 10 different forms of double DNA strings, where parallelism
between lines refers to hybridization between strings, while Y forms correspond to
the cases where extremal parts of single strings do not hybridize (in YY forms this
happens on both sides). Of course, these forms do not cover all possible DNA forms,
but they identify all the possibilities of pairing two different strands. Circular forms,
hetero-duplex and hairpins are considered in Fig. 2.15.
A DNA simple branch is formed by three linear strands such that each of them
hybridize with the other two. Such kinds of branches can produce any kind of graph,
if we identify a cycle of branches as a single node. Figure 2.17 shows the way
branches are combined for producing a node with degree ﬁve.
Table 2.2 Basic Requirements in DNA Pool Operations
mix(P1,P2) = P1 +P2
split(P) = (P1,P2) ⇔P = P1 +P2, Type(P1) = Type(P2) = Type(P)
length(P) = {|η| | < η > ∈Type(P)}
separate(P,n) = {s ∈P | |type(s)| = n}
Type(denature(P)) ⊇{α | α
β ∈Type(P)}
Type(hybridize(P))⊇{
α
rev(β) | α,β ∈Type(P),α][β}
Type(extend(P))⊇{
αγβ
(δγβ)c |
αγ
(δγβ)c ∈P}
Type(infix(P,γ,δ))⊇{< γαδ > | < α > ∈Type(P)}

42
2 Strings and Genomes
Fig. 2.14 Ten bilinear antiparallel forms
Fig. 2.15 Linear, circular, hairpin and heteroduplex DNA

2.3 DNA Pool Operations
43
Fig. 2.16 A composition of heteroduplex, hairpin, and circular DNA
Fig. 2.17 Branching DNA and a composition of ﬁve branches
2.3
DNA Pool Operations
A pool of DNA molecules can be mathematically simpliﬁed by a multiset of double
or single strings. In Table 2.2, some DNA pool operations are deﬁned which have to
be considered as high level mechanisms, analogous to the basic operations of a high
level programming language. This implies that the algorithms we specify by means
of them are not exactly experimental protocols, but rather computational procedures,
implementable by laboratory procedures. These DNA algorithms are naturally ex-
pressed in terms of a Test Tube register Language (TTL for short), where registers

44
2 Strings and Genomes
are Test Tubes which contain DNA pools, rather than numbers, and DNA pool oper-
ations apply to them. The symbol := denotes the usual assignment command typical
of imperative languages. In an assignment P := op(Q), the operation op is intended
to be applied to the content of the test tube Q and the resulting DNA pool becomes
the content of the test tube P.
The exact “microscopic” effect of operations in Table 2.2 is not deﬁned, because
in many cases it is hard to say exactly what are the multisets before and/or after the
application of these operations. However, despite such a kind of incomplete knowl-
edge at a microscopic level, DNA algorithms can be designed which are based on
these operations. In fact, it is enough to assume that, after performing a given DNA
operation, an input pool P is transformed into a pool P′ where a speciﬁc relationship
holds between the types of P and P′ (see Table 2.2 and Figs. 2.18, 2.19, 2.20).
The realization of operations in Table 2.2 is performed by means of standard labo-
ratory procedures based on physical and biological phenomena. Operations mix and
split are simply obtained by merging the content of two test tubes, or by splitting the
content of one test tube in two different test tubes. Operations length and separate
are realized by means of gel-electrophoresis, a speciﬁc electrochemical tool for dis-
criminating DNA strands according to their length. The operation hybridize is ob-
tained by raising temperature, while renature by (slowly) decreasing it. Operations
extend, ligate, and in fix are realized by Polymerase and Ligase enzymes (see Figs.
2.21, 2.22).
P
P 1
P 2
Fig. 2.18 Split operation (Mix is realized in the inverse way)
In Gel-electrophoresis DNA strands are put over a plate with a gel producing
resistance to the movement of DNA molecules. An electrical ﬁeld is applied be-
tween the two extremal borders of the plate (see Fig. 2.19). In column 1, strands
of different (known) length are located at different levels. In the other columns,
strands of unknown lengths stop at some level after the effect of the electrical ﬁeld
(DNA is negatively charged). Their positions, compared with the position of refer-
ence strands, provide a precise estimation of their lengths. The strands at a given

2.3 DNA Pool Operations
45
Fig. 2.19 Gel-electrophoresis implementing Length operation
Fig. 2.20 DNA denaturation and renaturation

46
2 Strings and Genomes
Fig. 2.21 Polymerase extension (Garret & Grisham: Biochemistry, Saunders College Pub-
lishing, 2e)
Fig. 2.22 Ligase joins 5′ phosphate to 3′ hydroxyl
position can be extracted from the plate, by cutting the gel slice where they are
located, and, after removing the gel by washing the selected strands, the separate
operation, providing strands of a required length, can be performed.

2.3 DNA Pool Operations
47
2.3.1
Writing and Reading DNA
A DNA clone is a multiset of DNA strands constituted by many copies of the same
DNA molecule, or equivalently, a set of DNA strands or double strands having the
same type. Two important aspects of DNA manipulation are the basic operation
of any system of information representation: writing and reading. Writing DNA
consists in generating a clone of DNA molecules having as type a given string over
the alphabet of nucleotides. Reading means the inverse operation, that is, given a
clone of DNA molecules, discovering which is the string of bases corresponding to
their type.
The ﬁrst operation, which is called DNA synthesis, is now a routine operation
and is based on the following principle. Consider nucleotides where one of the two
extremities is removed, say the 5′ of the phosphoric group. Let us group in different
vessels containing a given number of each nucleotide deprived of the P-terminal:
a vessel A containing AOH (instead of PAOH), a vessel T containing TOH, a ves-
sel C containing COH, and a vessel G containing GOH. Let us write, for example,
the sequence ATTCG. We start with a nucleotide (or better, a clone of nucleotides)
PGOH bound to a solid support S by means of some afﬁnity mechanism, which an-
chor them to S, say it S+PGOH. Then we put S+PGOH in the vessel C. The loss
of P-extremity of nucleotides in this vessel ensures that when S+PGOH is put in
the vessel only S+COH PGOH can be formed, where C nucleotides are without the
P-terminal. Therefore, we add to S+COH PGOH the missing P-terminal. by obtaining
molecules S+PCOH PGOH. The same process can start again with sequences CG an-
chored to the support S and having both the extremities, for completing the process,
by adding the remaining TCG part. In general, writing is performed by anchoring to
a solid support S the last nucleotide of the sequence we want to write (3′ terminal),
and then, by proceeding in the verse 3′ −5′, by iterating the cycle of: i) introducing
the solid support S in the right vessel for linking molecules inside it to the OH-
terminal of molecules anchored to S, ii) extracting S from the vessel, and iii) adding
the P-terminal to the molecule anchored to S.
Reading DNA is the process usually indicated as DNA sequencing. In fact, it
provides the sequence bases corresponding to the type of a given DNA clone. In the
reading process, it is essential to assume that DNA which has to be read is provided
as a clone. We will explain the main idea of sequencing by using a metaphor. Let us
assume the availability of a sequence of balls of four colors, say A, T, C, and G. Let
us assume also that these balls are so small that we cannot distinguish their colors.
However, let us suppose that we are able to obtain many copies of the given original
sequence of balls. Then, we can distribute these copies in four different vessels. In
each vessel we use some special scissors SA,ST,SC,SG such that SA cuts after A, ST
cuts after T, SC cuts after C, and SG cuts after G. Moreover, in order to keep the right
analogy with DNA, our balls are asymmetric, with a left part and a right part, in such
a way that scissors can act only once for each strand, by cutting and by keeping in
the vessels only the left parts. In these hypotheses, after using the corresponding
scissors in each vessel, we get some strands which are copies of preﬁxes of the
original sequence. If scissors perform their task in a completely random way, then

48
2 Strings and Genomes
in each vessel, say in A, we get sequences of different lengths which correspond
to the positions where a ball of color A is located in the original sequence. In this
manner, by measuring the lengths of the sequences obtained in the four vessels,
we can discover the positions of the original sequence where A are located, those
where T are located, those where C are located, and those where G are located. In
conclusion, the structure of the original sequence can be completely deduced. This
process was realized by Sanger, who won his second Nobel Prize for it, by using
in each vessel a small part of nucleotides without the Oxydryl, in such a way that
when polymerase enzyme uses one of them, then the extension process stops. It is
important that the amount of these nucleotides is not so big and not so small. In fact
in the ﬁrst case only short fragments are obtained, while in the second case only
long fragments are obtained. However, in almost all sequencing methods the role of
polymerase extension is essential. In a new class of methods each step is constituted
by four sub-steps, where polymerase can use only one of the four nucleotides, and
its use is made evident by coupling to it a phenomenon which produces some effect.
In this way according to which sub-step provides the effect, the kind of nucleotides
added by polymerase can be deduced; therefore all the extended sequence can be
deduced.
2.3.2
Plasmide Cloning Algorithm
DNA cloning of double strands is a process that produces many identical copies of
a given double DNA molecule. Plasmid cloning is performed by means of bacteria.
In fact, in many bacteria there are some double circular forms of DNA, called plas-
mids. Therefore, the main idea of plasmid cloning procedure consists in inserting
a target DNA molecule inside a bacterium plasmid and then letting the bacterium
proliferate in many cells which, being descendant of the same cell, have the same
DNA and consequently the same plasmid including the target DNA molecule. In
this way, we can get many copies of the initially inserted DNA, just by recovering
all these plasmids and by extracting from them the copies of the target molecule.
The following is the detailed (abstract) procedure.
1. Choose a plasmid, called also vector which includes a gene encoding the re-
sistance to an antibiotic A and a second gene encoding the resistance to an
antibiotic B.
2. Cut this circular vector by means of a suitable restriction enzyme occurring once
in the plasmid and in the middle of the sequence of gene A (see Fig. 2.23). As
indicated in the ﬁgure, the cut of the restriction enzyme realizes two speciﬁc
single strand ﬂanking terminals.
3. Extend the target molecule in a such way that it begins and ends with sticky
ends that can hybridize with the sticky ends provided by the restriction enzyme
(see Fig. 2.24).

2.3 DNA Pool Operations
49
Fig. 2.23 Cutting the plasmid
A  A  T  T  C
G
C  T  T  A  A
G
Fig. 2.24 Extending target DNA with suitable sticky ends
4. Let the target molecule hybridize with the broken plasmid, and use Ligase en-
zyme in order to obtain a circular double strand DNA molecule (see Fig. 2.25).
A  A  T  T  C
G
C  T  T  A  A
G
Fig. 2.25 Inserting the target DNA molecule into the plasmid

50
2 Strings and Genomes
5. Infect bacteria that are not resistant to the antibiotics A and B with the plasmids
obtained at the end of the previous step. The adopted technology (by suitable
electrical impulses) ensures that only one plasmid can enter in one bacterium.
In this operation three possible results can occur for each bacterium: i) one
plasmid including the target molecule enters in the bacterium, ii) one plasmid
without the target molecule enters in the bacterium, iii) no plasmid enters in the
bacterium (the ﬁrst situation occurs usually only for one in 10,000 bacteria).
6. Put bacteria obtained at the previous step in a microbial culture (each bacterium
is put alone in one compartment) where the antibiotic B is put with the nutrient.
In this way only bacteria including the plasmid can survive.
7. Select the bacteria which survive at the previous step and construct a mir-
ror bacterial colony by choosing from each compartment one bacterium and
putting it in a compartment having the same position it has in the original cul-
ture plate.
8. After a growing phase, put the antibiotic A in the mirror colony and observe
in which position bacteria die. These positions are those where in the original
colony there are bacteria hosting the plasmid which includes the target DNA
molecule. In fact in these plasmids the resistance to the antibiotic A was re-
moved. For example, if in the mirror colony bacteria in wells 2,5,8 die, then
we deduce that in the original colony these bacteria include the altered plasmid.
9. In the original colony keep only bacteria which correspond to dead bacteria,
in the mirror colony. They include the plasmids hosting the target molecule.
Remove the external membrane of these bacteria. and recover their plasmids.
10. Put in the obtained pool of plasmids the same restriction enzyme used at
the second step. In this way copies of the target molecules, which were in-
cluded in the plasmids, are recovered and can be selected by length with a
gel-electrophoresis.
2.4
DNA Computing
In 1994 Leonard Adleman started the new research ﬁeld of DNA Computing. [11,
12]. He showed that an instance of a famous combinatorial problem can be translated
in terms of DNA strands, put in a test tube in such a way that, by means of typical
laboratory manipulations, a ﬁnal DNA pool is obtained where the solution of the
problem is encoded. Since then, a great deal of research has been carried out, and
many technical and theoretical achievements have been reached in DNA computing
[39, 17, 18, 75]. Recently, new research perspectives have emerged that widen the
possibilities of this ﬁeld, among them: DNA self-assembly [61, 55, 58], and DNA
automata [15, 14], as well as tools for DNA and RNA manipulation inspired by
algorithmic analyses. [55, 13].
In the attempt to implement algorithms over a DNA-based “bioware”, along with
the DNA computing trend, it became increasingly apparent that the logic of DNA
operations presents deep combinatorial and algorithmic aspects.

2.4 DNA Computing
51
2.4.1
Adleman’s Experiment
The main idea of Adleman’s experiment is the representation of a graph in a DNA
pool. To this end, it is enough to encode nodes with single DNA strands and edges
as other strands in such a way that when an edge f connects two nodes a and b, then
it is encoded by a strand which hybridizes with one half of the strand encoding a
and one half of the strand encoding b. Figure 2.26 illustrates this idea of realizing
connections with hybridization of single DNA strands. In this manner, when many
copies of strands encoding all the nodes and edges of a graph are put in a DNA pool,
then, in a very short time, they hybridize by providing all the possible paths which
can be formed in the graph. Some speciﬁc operations involving the ligase and poly-
merase enzymes are necessary for having stable and abundand strands. Ligase fuses
all the strands in the paths by performing the missing OH-P bonds between consec-
utive strands which are linearly arranged, while polymerase provides a generation of
many copies of the formed paths, according to the polymerase chain reaction which
we will study in a following section.
The question posed by Adleman, solved by DNA operations, was the determi-
nation of a Hamiltonian path connecting a start node with an end node (a path
passing exactly once for each node of the graph). In terms of DNA strands, this
corresponds to ﬁnding a DNA strand beginning with a given preﬁx and end-
ing with a given sufﬁx, having a given length (nk length if in the graph there
are k nodes, all encoded by strands of length n) and where the encoding of
each node is included. The solution (only one solution was possible in the prob-
lem considered by Adleman) was found by selecting by electrophoresis all the
strands of the required length, and then by keeping only those which are com-
plete, in the sense of including the encoding of all nodes. The check of com-
pleteness is performed by the crucial operation of extraction. Given a DNA pool
P, the extraction of its γ-strands, provides strands of P having a type includ-
ing the string γ. In Adleman’s experiment, if α1,α2,...,αk are the strings en-
coding the nodes, then the strands solving the problem have to include types
α1,α2,...,αk. Let us consider k probes, that is, strands having types comple-
mentary to the strings α1,α2,...,αk. Therefore, by using the ﬁrst probe, all the
strands are selected, by complementarity (we avoid the biochemical details), with
types including the string α1, then, from this selection, with a second probe, the
strands can be selected which include α2, and by proceeding in this way, after
k steps (k the number of nodes of the graph), the required solution can be ob-
tained. Sequencing the ﬁnal strands, the required Hamiltonian path can be easily
determined.
   Ai   Bi 
Bj 
   Bj’  Ai’ 
Fig. 2.26 Adleman experiment

52
2 Strings and Genomes
The main steps of Adleman’s experiments are synthesized in the following list.
1. Encode nodes and edges of the graph under investigation by means of suitable
linear strands which hybridize by providing double strands encoding all the pos-
sible paths of the graph;
2. Add Ligase enzyme in order to transform hybridization paths into double strands
(by concatenating contiguous strands);
3. Amplify, that is, realizes many copies (by using PCR, as will be explained later)
of the double strands starting with the start node and ending with the end node of
the graph;
4. Separate by length the double strands of the previous step encoding paths with 7
nodes (all the nodes of the graph);
5. Extract, from the paths of the previous step, those where all the nodes occur (each
node once, because paths of the previous step contain 7 nodes);
6. The double strands remaining, after the extraction of the previous step, encode
Hamiltonian paths of the given graph.
The procedure of Adleman’s experiment has a general validity. In fact, in any combi-
natorial problem we can distinguish two different phases (see Fig. 2.27): the gener-
ation of a solution space where all possible solutions are produced, and a following
phase where the true solution of the given problem is selected. Selection requires
tools for discriminating between false and true solutions. The conceptual strength
of DNA computing relies in the possibility of nature in performing massive string
recombination processes, on the basis of the massive parallelism of strand hybridiza-
tion in DNA pools with billions of billions of strands. The extraction operation is
technologically more complex, expensive, and usually not very reliable. We will
consider an extraction method in a following section. However, in principle, both
generation and extraction operations can be performed in linear time, with respect
to the size of the combinatorial problems. This general model of DNA computation,
usually called Adleman-Lipton extract model [39] was tested and applied in many
speciﬁc cases. After almost 10 years of research in this context, the initial enthu-
siasm about the computational efﬁciency of DNA computing sensibly diminished,
but many aspects and many related ﬁelds of investigation were pushed by the Adle-
man’s experiment and by all the research following it. We want only to mention the
problem 3-SAT, on which a great deal of research effort was spent in recent years,
for its centrality in the theory of combinatorial problems and for its natural setting
in the context of DNA computing.
The problem 3-SAT consists in the determination of boolean assignments, to the
variables X1,X2,...,Xn, which satisfy a number of boolean equations C1,C2,...,Cm
involving X1,X2,...,Xn. It can be easily shown that without loss of generality
boolean equations can be put in the form of clauses, that is, boolean sums of at
most 3 literals (this explains the name 3-SAT) implicitly equated to the truth value
true, where each literal can be a boolean variable or the negation of a boolean vari-
able. For example, the system of boolean equations 2.1 ( = 1 is implicit in every
line) is a 3-SAT problem of 4 variables and three clauses which has, for example,
the assignment (X1,X2,¬X3,X4) as solution.

2.4 DNA Computing
53
Space 
Generation 
In linear time 
Solution 
Extraction 
In linear time 
Possible 
Solutions 
True 
Solutions 
Fig. 2.27 The extract model
X1 + ¬X2 + X3
(2.1)
X2 + X3 + ¬X4
¬X1 + ¬X2 + ¬X3
X1 + ¬X2 + X3
¬X2 + ¬X3 + X4.
A natural way for generating a DNA space representing all possible assignments of
a 3-SAT is the Mix-and-split method, which is based on a very simple idea depicted
by Fig. 2.28 and expressed in TTL notation in Table 2.3.
Table 2.3 Mix-and-split procedure
Mix X1 and ¬X1 in a tube T
For J := 2 to N do
Split T into A and B
Extend strands of A with X j
Extend strands of B with ¬X j
Mix A and B into T
By using a split-and-mix procedure all assignments of a 3-SAT problem can be
generated and by applying to them m consecutive selection steps for ﬁltering the
assignments satisfying all the clauses, then the required solution can be found. The
algorithm given in Fig. 2.29 is due to Lipton and provides 3-SAT solutions by means
of 3m extraction steps.

54
2 Strings and Genomes























	







Fig. 2.28 Mix-and-split method
Fig. 2.29 Lipton’s algorithm for 3-SAT. Expression T −T ′ denotes the strands of T where
the extracted strands of T ′ are removed.
We conclude by mentioning another three different ideas for solving 3-SAT in
terms of DNA computing. The ﬁrst one due to Jonoska et al. represents a given in-
stance of the 3-Sat problem by producing, by means of DNA branches, a graph of
the type given in Fig. 2.30, where clauses are represented by joining double DNA
encodings of clauses (by means of branches as depicted in Fig. 2.17). Given a great
number of DNA structures representing the graph of a 3-SAT problem, we can pro-
ceed in m steps (j = 1,m) by splitting, at step j, the pool of graphs in two pools and
cutting (by using speciﬁc enzymes able to recognize some strings and to cut them)
the literal Xj in the ﬁrst pool and the literal ¬Xj, in the second pool, and then, by
mixing again the two pools. It is easy to verify that if the problem has solutions,
then a connected graph remains linking all the clauses, where each path of literals
represents a solution of the original problem.
An algorithm due to Sakamoto et al. yields solutions according to an idea similar
to that of Jonoska’s algorithm. Let us suppose to order the clauses of a given prob-
lem. Let us encode the literals of every clause in such a way that they are linkable

2.4 DNA Computing
55
X
Y
Z
Y
Z
X
Y
X
Z
X
Y
Z
S ( C 1 )
S ( C 2 )
T ( C 1 )
T ( C 2 )
S ( C 3 )
T ( C 3 )
S ( C 4 )
T ( C 4 )
Fig. 2.30 3-SAT solutions represented by graphs encoding assignments
by means of two sticky ends (left and right) with the literals of the previous clause
(on the left) and with the literals of the next clause (on the right, see Fig. 2.31).
Moreover, for every variable X, let the literal ¬X be encoded with a string that is the
the mirror of the encoding of X. Of course, literals of the ﬁrst clause are linkable
only with those of the second, while literals of the last clause are linkable only with
those of clause of order m −1 (m number of clauses). In this way, if a a path, of
linked literals, is formed where a variable and its negation are encoded, then a DNA
hairpin is formed. Therefore, in general with n variables, the true solutions are the
paths encoding n literals that are not DNA hairpin.
S ( C 1 )
S ( C 2 )
T ( C 1 )
T ( C 2 )
S ( C 3 )
T ( C 3 )
S ( C 4 )
T ( C 4 )
Lit(C1)
Lit(C2)
Lit(C3)
Lit(C4)
Fig. 2.31 3-SAT solutions represented by sequences encoding literals
S ( X 1 )
S ( X 2 )
T ( X 1 )
T ( X 2 )
S ( X 3 )
T ( X 3 )
C l a ( X 1 )
S ( X 1 )
T ( X 1 )
C l a (
X 1 )
S ( X 2 )
T ( X 2 )
S ( X 3 )
T ( X 3 )
C l a ( X 2 )
C l a ( X 3 )
C l a (
X 2 )
C l a (
X 3 )
Fig. 2.32 3-SAT solutions represented by sequences encoding clauses
An algorithm due to Manca and Zandron [46] provides 2n strings (n number of
boolean variables) Cla(Xi) and Cla(¬Xi), for 1 ≤i ≤n. The string Cla(Xi) is a
sequence (double string with sticky ends) of DNA encodings of all clauses having
the literal Xi, and Cla(¬Xi) is a sequence of DNA encodings of all clauses having
the literal ¬Xi. Moreover, Cla(Xi) and Cla(¬Xi) are left-linkable to Cla(Xi−1) and
to Cla((¬Xi−1), while they are right-linkable to Cla(Xi+1)and to ¬Cla((¬Xi+1) (by
means of suitable sticky ends, see Fig. 2.32). It is easy to show that the solutions
of a 3-SAT problem are represented by those strands which encode sequences of
n strings of type Cla(−−) containing the encodings of all the clauses of the given
problem. In the case of m clauses, m extractions are sufﬁcient to solve a 3-SAT
problem.

56
2 Strings and Genomes
2.5
PCR and XPCR Protocols
Polymerase Chain Reaction process (PCR) is one of the most important and efﬁcient
tools in biotechnological manipulation and analysis of DNA molecules, where the
polymerase enzyme implements a very simple and efﬁcient duplication algorithm
on double oriented strings. Kary Mullis discovered this method in 1983 [56], and
for this reason, the Nobel Prize in chemistry 1993 was awarded to him (jointly with
Michael Smith for contributions to site-directed mutagenesis).
The main result of PCR is the exponential ampliﬁcation of target double strands
having a given sequence of bases. As will be explained in this section, in the PCR
process, an initial number of target double strands (even only one of them) provides
a ﬁnal number of them which is a product of the initial number by a multiplicative
factor consisting of a power of the times a basic PCR step is repeated. The bilinearity
of DNA molecules and the antiparallel orientation of their two linear components
are essential aspects of the logic underlying PCR. The computational schema of
PCR in TTL notation is given in Table 2.4, while its combinatorial schema in terms
of string transformations is given in Fig. 2.33.
Fig. 2.33 The schema of Polymerase Chain Reaction

2.5 PCR and XPCR Protocols
57
A substantial improvement in the efﬁciency and reliability of PCR was obtained
when the process was realized by means of heat-stable DNA polymerase, such
as Taq polymerase (an enzyme isolated from the bacterium Thermus aquaticus).
Namely, in this way the three phases realizing the denaturation, primer hybridiza-
tion, and polymerase extension can be realized by alternately heating and cooling
the PCR sample to a deﬁned series of temperature steps (depending on the length
and nucleotide composition of the hybridization regions of the involved strands).
Polymerase Chain Reaction, which is a milestone in DNA recombinant technol-
ogy [1], shows many interesting combinatorial properties. In fact, it is in some cases
a complicated process, and, when it is used in non-standard ways, it yields very
complex behaviors. Very often, anomalies are ascribed to experimental noise, but,
if we frame PCR within a rigorous symbolic notation, then non-trivial combinato-
rial aspects appear and, under suitable hypotheses, a formula can be derived which
describes the general form of sequences that are exponentially ampliﬁed. This ap-
proach is not of merely mathematical interest. On the contrary, it can suggest new
methods that enjoy biological relevance for in vitro DNA manipulation. In fact,
starting from DNA computing problems [46], we investigated speciﬁc methods for
DNA extraction and recombination [28, 30, 26]. In these attempts, where theoretical
issues were supported by the experiments, we realized that a special kind of PCR,
called Cross Pairing PCR or XPCR for short, can be the basis for new algorithms
that solve a wide class of DNA extraction and recombination problems. Within a
basic DNA symbolic notation, we will show a crucial computational property of
Polymerase Chain Reaction, we call PCR Lemma, which suggested to us the idea of
Cross Pairing PCR.
Table 2.4 PCR Algorithm
PCR(P,n) =
let Type(P) = {< γ ...δ > , γ , ¯δ}, n integer;
input P;
for i = 1,n do
begin
P := denature(P);
P := hybridize(P);
P := extend(P);
end;
output P.
We denote by PCR(P,n) the DNA pool obtained by applying to the pool P
the procedure of Table 2.4 (the usual PCR process). The parameter n denotes the
number of the fundamental PCR steps (also called PCR cycles, which we avoid

58
2 Strings and Genomes
Table 2.5 Combinatorial Schema of Polymerase Chain Reaction
γ ζ δ
γcζ cδ c
→
γζδ
λ
,
λ
γcζ cδ c
template denaturation
γζδ
λ
,
λ
γcζ cδ c ,
γ
λ ,
λ
δ c →
γ
γcζ cδ c ,
γζδ
δ c
primer hybridization
γ
γcζ cδ c ,
γζδ
δ c
→
γ ζ δ
γcζ cδ c ,
γ ζ δ
γcζ cδ c
polymerase extension
mentioning when its value is not essential in the discussion). It is well-known that if
Type(P) = {< γϕδ >,γ, ¯δ}, then PCR(P,n) generates an exponential ampliﬁcation
of the target molecule < γϕδ >, that is, copies of this molecule in a number that
is exponential with respect to the number n of steps, and only a minor quantity of
strands in P have types different from < γϕδ >.
Polymerase extension ext, depicted in Fig. 2.21, of a single string αγ, according
to a template η, is deﬁned as:
ext(αγ, ¯β ¯γ) = αγβ
(assuming that γ does not occur as a substring of α, and β), that is:
ext(αγ,
λ
γcβ c ) = αγβ
If αγ ̸][η, then we set ext(αγ,η) = αγ.
It is useful to consider another extension operation, which we continue to denote
by ext, having a double string as argument, by setting (if the argument has a form
different from that here considered, this ext leaves it unchanged):
ext( αγ
γcβ c ) =
αγβ
αcγcβ c .
PCR processes are easily representable by diagrams like that in Fig. 2.34, where
dotted arrows represent ext operation performed by polymerase enzymes.
All the possible PCR diagrams which result from the different forms of target
strings and from the different positions where primers may hybridize, are close to
100. Therefore, a natural question arises: in which cases does PCR provide exponen-
tial ampliﬁcation? And, in these cases, is it possible to characterize, in general terms,
the form of strings that are exponentially ampliﬁed? Our DNA notation allows us to
answer these questions with the following lemma, which identiﬁes important cases
of exponential ampliﬁcation by PCR processes.
We say that single DNA strings ϕ and ψ overlap when they hybridize according
to the pattern depicted in Fig. 2.35. We deﬁne their overlap concatenation, by
setting:
< ϕ >⌉⌊< ψ >= ext( ϕ
ψc )

2.5 PCR and XPCR Protocols
59
Fig. 2.34
PCR on a 3′3′ form along with one internal and one external primer. Reading
from the top: after the hybridization of the primers (short arrows), and the formation of
their extensions, these extensions overlap and, with the further extensions of the overlapping
strings, a blunt string is formed where primers correspond to the two (forward and backward)
extremities.

60
2 Strings and Genomes
which produces the DNA string < ϕ′γψ′ > depicted in Fig. 2.36, where γ is the
substring of ϕ, ¯γ of ψ, and ϕ′, ψ′ are the strings ϕ and ¯ψ without the substrings γ
and ¯γ, respectively.
Fig. 2.35 Two DNA single strings that overlap
Fig. 2.36 The overlap concatenation of two strings < ϕ > and < ψ > from the (overlap)
hybridization displayed in Fig. 2.35
Lemma 2.1 (PCR Lemma). Let P be a DNA pool such that Type(P) =
{
α
rev(β), γ, ¯δ} and let:
γ ̸][ ¯δ,
γ ][ β,
¯δ ][ α.
In the following cases:
i) γ ][ ext( ¯δ,α),
ii) ¯δ ][ ext(γ,β),
iii) ext(γ,β) and ext( ¯δ,α) overlap,
PCR(P,n) exponentially ampliﬁes blunt strings, where primers hybridize at extremal
regions of their single strands, having the following forms, respectively:
ext( ¯δ,ext(γ,ext( ¯δ,α)))
(2.2)
ext(γ,ext( ¯δ,ext(γ,β)))
(2.3)
< ext(γ,β) >⌉⌊< ext( ¯δ,α) > .
(2.4)
Proof. By the hypotheses, in the cases i) and ii) the extensions (2.2) and (2.3) have
to be proper (giving results different from the primers). It is easy to check that they
are blunt double strings including the primers in the extremal parts of their strands,
therefore they are seeds of exponential ampliﬁcations. In the last case iii), the sit-
uation of Fig. 2.35 occurs, then the overlap concatenation (2.4) (realized by the

2.5 PCR and XPCR Protocols
61
polymerase extension) yields a blunt string (with extremal primers) that is a seed of
exponential ampliﬁcation.
⊓⊔
The phenomenon of the blunt form of PCR exponential ampliﬁcation is empirically
well-known, but it is interesting that it is a mathematical consequence of the com-
binatorial mechanism on which PCR is based. The following corollary is a direct
consequence of PCR lemma.
Corollary 2.2. In the PCR process of lemma above where an exponential ampliﬁ-
cation occurs, then, at most at the third step of the process, a blunt string appears
which is a seed of an exponential ampliﬁcation.
Fig. 2.37 PCR with two internal primers, where curved lines represent pairing

62
2 Strings and Genomes
In Fig. 2.37, a PCR process is displayed, where the ﬁrst two steps are completely
represented, while, for easier reading, only two double strands of the third step
are shown. One of them is a blunt double strand that is a seed of exponential
ampliﬁcation.
We remark that PCR lemma presented here is different from the analogous lemma
given in [45]. Namely, here we do not claim to cover all the possible cases of am-
pliﬁcation. In fact, some laboratory experiments have shown cases of exponential
ampliﬁcation with YY forms, where primers hybridize with ﬂanking regions (and
produce ampliﬁcations having patterns different from those given in Eqs. 2.2, 2.3,
2.4). Here we do not analyze all the cases of PCR with YY forms. The example of
Fig. 2.39 shows the complexity of the combinatorial patterns related to PCR ampliﬁ-
cation. In this ﬁgure, starting with a YY molecule M1 and two primers that hybridize
on two ﬂanking regions, the molecule M2 is realized, which consists of the exten-
sions of the two primers. Molecules M3 and M4 are obtained by hybridization of
molecules of type M2 with the two strands of M1. In these molecules, it easy to
realize that the extensions of the primers stop when polymerase reaches the other
ﬂanking regions. The resulting extensions hybridize and produce the molecule M5,
that, after polymerase extension, becomes the blunt molecule M6, which is seed of
an exponential ampliﬁcation.
An example where PCR lemma does not apply is displayed in Fig. 2.38.
Fig. 2.38 A case where PCR lemma does not apply

2.5 PCR and XPCR Protocols
63
Fig. 2.39 An exponential ampliﬁcation based on a YY form

64
2 Strings and Genomes
2.5.1
XPCR
The role of the overlap concatenation in PCR ampliﬁcation suggested to us an inter-
esting form of PCR where, rather than just one target string, we put two target DNA
double strings of types < αϕγ > and < γψβ > in a test tube.
Let us analyze what happens if a PCR is performed, starting from such a pool,
extended with primers α, ¯β:
< αϕγ > , < γψβ > , α , ¯β
In this case, after denaturation the following strings will be present in the pool:
αϕγ
λ
,
λ
(αϕγ)c , γψβ
λ
,
λ
(γψβ)c , α , ¯β .
If the temperatures at which α,β, and γ hybridize with their mirror strings are ‘close
enough’, then the following hybridizations can occur, where the ﬁrst string, in the
pool given above, overlaps with the fourth one, while the strings in second and third
positions hybridize with the primers (other hybridizations are possible, but it is easy
to realize that they are not “productive” in terms of ampliﬁcation, because they can
only delay the effect of these “canonical” hybridizations, see Fig. 2.40):
αϕγ
(γψβ)c ,
α
( α ϕ γ)c , γ ψ β
β c .
At this point polymerase can extend strings:
ext(
αϕγ
(γψβ)c ) , ext(
α
( α ϕ γ)c ) , ext(γ ψ β
β c )
that become:
< αϕγψβ > , < αϕγ > , < γψβ >
in such a way that the blunt string < αϕγψβ > is produced, which results from the
overlap concatenation of < αϕγ > and < γψβ > with two more strings equal to the
initial target strings. We call this special kind of PCR cross pairing PCR or XPCR
for short; its combinatorial schema implements an operation, which theoretically
can be seen as a special kind of Tom Head’s null context splicing rule, given in the
seminal paper [37] where a Formal Language Theory perspective of DNA strings
was introduced (see also [47, 43]). In conclusion, if
Type(P) = {< αϕγ >, < γψβ >}
then
Type(PCR(P,α,β)) ≈{< αϕγψβ >}.
Types of strands different from < αϕγψβ > are present in a minor quantity,
moreover, recombinations different from those considered above are possible (for

2.5 PCR and XPCR Protocols
65
example, that of type 3′ −3′ between strands γψβ
λ
and
λ
(αϕγ)c ). Figure 2.40 displays
the possibility of hybridizations in a pool with two target strings and two extremal
primers (boxes includes the equal substrings in the two target molecules). It is easy
to check that the overall effect, in any case, is the ampliﬁcation of the overlap con-
catenation of the two target molecules. In fact, either the doubling of the two target
molecules plus a seed of overlap concatenation, or two seeds of overlap concatena-
tion plus the initial pair of target molecules is obtained after hybridization.
Fig. 2.40 Possible hybridizations of XPCR
We tested XPCR in several different experimental conditions and every time it
provided correct results [28, 30]. In most cases, the ampliﬁcation signal is very clear
and a small noise consisting of unspeciﬁc products is reported in the electrophoresis
results. Surprisingly enough, on the basis of the simple mechanism of XPCR, we
have been able to build more sophisticated procedures, which ﬁnd two main kinds
of applications in DNA extraction and DNA recombination.
2.5.2
DNA Extraction by XPCR
DNA extraction is a fundamental procedure where the “good solutions” are discrim-
inated in a space of possible solutions. For example, given a DNA pool consisting
of a family of genes with an indeﬁnite identity (their sequences are not known),
one might be interested in extracting the subfamily of those genes where a given
subsequence γ occurs, which, for instance, refers to an important biological prop-
erty. The classic extraction procedure by afﬁnity uses a probe ¯γ which is “marked”
in such a way that, after denaturation, single strands where γ occurs hybridize with
the probe, and so are selected from the original pool. Here we show a different way
of performing extraction, based on XPCR, outlined in TTL notation, in Table 2.6
(P−separate(P,n) denotes the pool P after removing its strands of length n).
The main idea of the algorithm is as follows. Consider all the lengths of strands
in a given pool. For each length n, pieces of strands which include the substring
γ in their types are copied. This is performed by means of usual PCR to amplify
both strands of type < αγ > and < γβ >. Strings shorter than n are then separated
by length and ﬁnally joined by an overlapping concatenation performed by XPCR.

66
2 Strings and Genomes
Table 2.6 A DNA extraction algorithm providing the strings of length n including γ as sub-
string, in a given a pool P
XPCR−Extract(P,n,γ)
1.
P := infix(P,α,β);
2.
P0 := separate(P,n);
3.
(P1,P2) := split(P0);
4.
P1 := PCR(P1,α, ¯γ);
5.
P2 := PCR(P2,γ, ¯β);
6.
P1 := P1−separate(P1,n);
7.
P2 := P2−separate(P2,n);
8.
Q := mix(P1,P2);
9.
Q := PCR(Q,α, ¯β);
10.
Q := separate(Q,n);
11.
output Q.
In the joining process, pieces which do not have length n are removed. For this rea-
son, the process must be iterated for each length of strings of the initial pool. The
algorithm reported in Table 2.6 provides all the strings where γ occurs, previously
elongated by the preﬁx α and by the sufﬁx β. This algorithm was tested in vitro
where, in a very heterogeneous DNA pool, all the types of strands including sub-
strands of a given type, and only they, were extracted. Therefore, XPCR extraction
proved to be correct and complete [28].
A useful warning about the XPCR-Extract algorithm is given by the follow-
ing observation. If in a family of initial genes there are two different genes, say
< ϕγψ > and < σγρ >, that have the same length and where the substring γ oc-
curs in exactly the same position, then the method will give, as extracted genes, also
their chimeric combinations < σγψ > and < ϕγρ >. In other words, if we deﬁne
Recombine(L,γ) = {< αγβ > | < αγδ > ,< ηγβ > ∈L}, then Extract(P,γ) co-
incides with Recombine(Type(P),γ). In this case, further checks are necessary for
realizing a reliable extraction.
2.5.3
DNA Recombination by XPCR
Let P a DNA pool of type {α1,α2,...,αn} and Q a pool of type {β1,β2,...,βn}. The
problem of generating all possible recombinations of pools P and Q is that of obtain-
ing a pool of type L = {η1η2 ...ηn | η1 ∈{α1,β1},η2 ∈{α2,β2}...ηn ∈{αn,βn}}.
Of course L contains 2n different strings; we call it the n dimensional complete re-
combination of P and Q. In DNA Computing this is an important step for encoding
all the possible solutions of a combinatorial problem. For example, if αi,βi encode

2.5 PCR and XPCR Protocols
67
the two possible values of a Boolean variable xi, then any string of L usually encodes
a possible solution of the problem. True solutions are obtained by generating L and
then by extracting strings which satisfy the requirements of the problem. But, apart
from this DNA computing interest, DNA recombination has an important biological
meaning. For example, in the immunological system recombination is a key feature
for generating the antibody repertory which is essential to the security system pre-
serving the biological identity. However, in general, DNA pools generated by XPCR
recombination could be very useful in the analysis of many aspects related to DNA
hyper-variability.
Let us present a DNA recombination method based on XPCR. We show that a
simple and efﬁcient algorithm based on XPCR can provide the n dimensional com-
plete recombination of P and Q. Let us consider the following four initial sequences,
where n is an odd number (if n is even, the roles of αn and βn are inverted in the last
two sequences):
Positive: η1 =< α1α2α3α4 ...αn >
Negative: η2 =< β1β2β3β4 ...βn >
Positive-Negative: η3 =< α1β2α3β4 ...αn >
Negative-Positive: η4 =< β1α2β3α4 ...βn > .
Let us call α-string any element of {α1,α2,...,αn} and β-string any element of
{β1,β2,...,βn}. The language L is the set of all the possible ordered combinations of
n α-strings and β-strings. We call XPCR rule rγ, and write it as ξ1,ξ2
rγ
−→ζ, the re-
lation between strings ξ1,ξ2,ζ that holds when ξ1 =< αδγ ... >, ξ2 =< ...γθβ >,
and ζ =< αδγθβ >. Any string that is a combination of α-strings and β-strings
can be obtained from η1, η2, η3, η4 by suitable XPCR rules rγ where γ is an α-
string or a β-string. For example, with n = 7, the string α1α2β3α4β5β6β7, can be
obtained in the following way:
η1 , η4
rα2
−→α1α2β3α4β5α6β7 , η2
rβ5
−→α1α2β3α4β5β6β7.
It can be easily shown that the order of application of the rules is not relevant,
because the same string can be also obtained by permuting the order of application
of the rules (from the same initial strings).
Let us consider the set of XPCR rules R = {rα2,rα3,...rαn−1,rβ2, rβ3,...rβn−1}.
A “quaternary XPCR recombination” which produces L from {η1,η2,η3,η4} is ef-
fectively speciﬁed by the algorithm displayed in Table 2.7. A completeness claim of
our quaternary XPCR recombination method is given by the following proposition,
which is an easy consequence of XPCR deﬁnition and of the particular structure of
the pool to which we apply this recombination method [30].
Proposition 2.3 (Recombination Method). If all the XPCR rules of R are applied
to a DNA pool of Type {η1, η2, η3, η4}, then a ﬁnal DNA pool is obtained of Type
{ξ1ξ2 ...ξn | ξ1 ∈{α1,β1},ξ2 ∈{α2,β2}...ξn ∈{αn,βn}}.

68
2 Strings and Genomes
Table 2.7 The Quaternary Recombination Algorithm
XPCR−Recombination({α1,α2,...αn},{β1,β2,...βn}) =
let Type(P) = {η1,η2,η3,η4};
P := infix(P,α,β);
for i = 2,n−1 do
begin
P := PCR(P,α, ¯αi);
P := PCR(P,αi, ¯β);
P := PCR(P,α, ¯βi);
P := PCR(P,βi, ¯β);
P := PCR(P,α, ¯β);
end;
output P.
The quaternary recombination algorithm has the additional advantage of being
equipped with a couple of special strings, called recombination witnesses, such that,
if they are present in the ﬁnal pool then the whole library of possible recombinations
is present too. Namely, let us consider a pool P including the four strings of quater-
nary recombination (preﬁxed by the string α and sufﬁxed by the string β), then a
set W of stings is a XPCR witness set, if when all the strings of W are included into
P, then all the possible XPCR recombinations where realized from the four initial
strings. Let us call i-trio-factor any substring of three components γi−1γiγi+1 where
exactly two consecutive components are positive (α strings) or negative (β strings),
then the following lemmas can be shown [30, 26].
Proposition 2.4. The recombination according to the XPCR rule rγ (γ = αi or
γ = βi for 1 < i < n) was realized in the pool P, at the end of the quaternary recom-
bination algorithm starting with the four initial strings, if the i-trio-factor including
as component γ is present in some string of P.
Proposition 2.5. The set consisting only of the following two strings is a XPCR
witness set (n ≥6): w1 = αα1α2β3β4α5α6 ...β and w2 = αβ1β2α3α4β5β6...β.
2.6
L-Systems and Morphogenesis
L-systems, introduced by Aristid Lindenmayer in 1968 [48, 53, 213], are grammars
with parallel rewriting. In particular, EOL is the class of L-systems which can be de-
ﬁned as CF grammars where rules are applied, by a parallel rewriting of all the sym-
bols occurring in the string. It is really surprising that a wide class of developmental
processes can be described by suitable L-systems, possibly extended with elements

2.6 L-Systems and Morphogenesis
69
Table 2.8 An EOL system generating the tri-somatic language
S
→
ABC
A
→
AA’
B
→
BB’
C
→
CC’
A
→
a
B
→
b
C
→
c
A’
→
A’
B’
→
B’
C’
→
C’
A’
→
a
B’
→
b
C’
→
c
a
→
F
b
→
F’
c
→
F
F
→
F
conveying important morphological parameters (e. g., lengths, angles, sizes). The
EOL system of Table 2.8 generates the tri-somatic language.
The ﬁrst application of L-systems were the developments of some algae (red
alga). In these organisms all cells grow, at each developmental step. This means that,
in symbols, this mechanism corresponds to a parallel rewriting of all the symbols
occurring in a string.
A DOL system (Deterministic OL system) is deﬁned by a triple (A,μ,α) where
A = {a1,a2,··· ,an} is the alphabet, α is the initial word over the alphabet A, and
μ is a string morphism over A, that is a function from A to A∗which extends to a
function from A∗to A∗by the condition
μ(αβ) = μ(α)μ(β).
The following is the usual representation of a DOL system, where the morphism is
expressed in a evident arrow notation.
μ =
⎛
⎝
a1 a2 ··· an
↓↓
↓
β1 β2 ··· βn
⎞
⎠
When μ : A →P(A∗), we call it a poly-morphism, and the system is said to be
an OL system. This means that each symbol can be rewritten with many possible
strings, and, at any (parallel) rewriting step, one of the strings of μ(a) is chosen to
replace the symbol a (for any symbol a of the rewritten string).

70
2 Strings and Genomes
The strings generated by L-systems are translated into forms, according to the
turtle interpretation, where symbols are associated to segments, and other control
symbols are used, say +,−,[,]. In this case, a symbol F stands for a segment hav-
ing a given initial length and angle α0. The turtle encoding of strings is obtained
by translating symbols into movements of a point (the turtle) that can draw while
it moves. When an L-rewriting is applied, the segment of given length l, associated
to the symbol F, is replaced by a poly-segment of sub-segments of length l/k. The
fractal dimension of a rule is given by logk n where n is the number of sub-segments
of the poly-segment replacing the original segment. This means that, at each rewrit-
ing of the rule, the segment associated to symbol F is resized by the factor 1/k.
The symbols + and −stand for a positive or negative deviations of some preﬁxed
angle δ. Brackets represent mechanisms of internal rewriting. In fact, when an open
bracket appears, a form is generated which encodes the string between the open and
closed brackets. After that, the turtle goes back to the same position and angle it had
before the open bracket. In other words, we can assume that during its movement
the turtle can move and draw or can only move (movement without drawing could
be denoted by putting a special symbol F′). With these assumptions, a string within
brackets encodes the generation of a curve such that, after its generation, the turtle
moves back along this curve with the inverse movement it performed in its genera-
tion, but without drawing (all its symbols F become F′). In Figs. 2.41, 2.43, and 2.44
forms generated by L systems are expressed by means of the turtle interpretation.
Fig. 2.41 Example of forms generated by L systems

2.6 L-Systems and Morphogenesis
71
Fig. 2.42 Start string F −−F −−F and L-rule F →F +F −−F +F with δ = 60o
Fig. 2.43 An L form generated by the L system of Fig. 2.42
The classes EDOL and EOL are obtained by adding to the classes DOL and
EOL the feature “D” of determinism and the feature “E” of extended alphabet (with
nonterminal symbols).
Example 2.6. The following EOL system generates the tri-somatic language (lower
case symbols are terminal). In fact, the only case of producing terminal strings is
when symbols A,B,C or A′,B′,C′ are rewritten in a synchronized way. The mor-
phism μ is given below, terminals are {a,b,c}, and abc is the initial word.
⎛
⎝
A
B
C
A′
B′
C′
a b c F
↓
↓
↓
↓
↓
↓
↓↓↓↓
{AA′,a} {BB′,b} {CC′,c} {A′,a} {B′,b} {C′,c} F F F F
⎞
⎠

72
2 Strings and Genomes
Fig. 2.44 Example of other forms generated by L systems

2.6 L-Systems and Morphogenesis
73
ETOL-systems and EDTOL-systems are EOL-systems and EDOL-systems, re-
spectively, where instead of a poly-morphism, a set of morphisms is given, which
is called a table. In this case, at each rewriting step, one morphism of the table is
chosen.
Example 2.7. The following EDTOL-system, with a table of two morphisms, gen-
erates the tri-somatic language.
μ1 =
⎛
⎝
A B C a b c
↓
↓
↓↓↓↓
aA bB cC a b c
⎞
⎠μ2 =
⎛
⎝
A B C a b c
↓↓↓↓↓↓
a b c a b c
⎞
⎠.
2.6.1
String Models and Theories
A string model is a relational structure (see Chap. 5) having a set of strings as do-
main. A powerful way of expressing rewriting rules and generation strategies is by
means of logical formalisms [204].
A string theory or a monoidal theory T , over the alphabet A, is a theory in-
cluding, as terms, the free monoid generated by the alphabet A. This means that the
signature of T includes the symbols of A (as individual constants), a symbol for the
empty string λ, and a symbol for concatenation, which we denoted in the usual way
(by juxtaposition), and its axioms include the monoid axioms (x, y, z variables of
strings):
(xy)z = x(yz)
xλ = λx = x.
In a string theory over the alphabet A terms, possibly with variables, include the
symbols of A and variables ranging over strings of A∗.
A language L is a set of strings, therefore L can be deﬁned by means of a formula;
within a string theory ϕ(x) (with a free variable x):
L = {α | T |= ϕ(α)}.
In this case |= is the logical consequence relation that can be computed by any
logical calculus of predicate logic.
Given a grammar G = (A,T,S,R), the string theory of Table 2.9, over the alphabet
A, provides the logical deduction of a formula Generate(ϕ) if and only if ϕ ∈L(G)
(the language generated by the grammar G).
The following example is an (equational) monoidal theory that deduces the de-
velopment R(n) at stage n of a Red Alga, a primitive organism that grows according
to the following law.

74
2 Strings and Genomes
Table 2.9 A string theory associated to a Chomsky grammar G
Derive(S)
Terminal(a)
for every a ∈T
Rule(α,β)
for every α →β ∈R
∀x((Terminal(x)∧Terminal(y)) →Terminal(xy))
∀x,y,z((Derive(xyz)∧Rule(y,w)) →Derive(xwz))
∀x((Derive(x)∧Terminal(x)) →Generate(x))
Cells of this organism are indicated by the symbol F, cells inside brackets are
“branches”, inclined (alternatively in opposite verses) with respect to a main grow-
ing axis. For n > 5 we distinguish in R(n) two parts B(n),A(n) called a basal part
and an apical part. Every second cell in the basal part carries a non-branching ﬁla-
ment. These ﬁlaments develop linearly in time, they add at each stage one new cell.
At stage 6 the lengths of these ﬁlaments are 3, 2, 1, respectively. The apical part
at stage 6 consists of four cells without any branches. In the following stages, the
apical part is a repeat of the apical part at the previous stage, together with two new
cells at the end of the apical part. The second of these new cells carries a branch,
identical to the whole organism six stages before.
• R(0) = F
• R(1) = FF
• R(2) = FFFF
• R(3) = FF[F]FF
• R(4) = FF[FF]FF[F]FF
• R(5) = FF[FFF]FF[FF]FF[F]FFFF
• R(x+ 6) = B(x+ 6)A(x+ 6)
• A(5) = FF
• A(x+ 6) = FF[R(x)]A(x+ 5)
• B(x+ 6) = FF[L(x+ 3)]FF[L(x+ 2)]FF[L(x+ 1)]
• L(0) = λ
• L(x+ 1) = FL(x).
The following derivation is based on the equations deﬁning Red Algae structure:
• R(10) = B(10)A(10)
• A(10) = FF[R(4)]A(5) = FF[R(4)]FF =
• FF[FF[FF]FF[F]]FF
• B(10) = FF[L(7)]FF[L(6)]FFL(5) =
• FF[FL(6)]FF[FL(5)]FFL(5) =
• FF[F7]FF[F6]FFF5
• R(10) = FF[FF[FF]FF[F]]FFFF[F7]FF[F6]FFF5.

2.7 Membrane Computing
75


X
X
X
X
X
X
X
Q
Q
Q
Q
Q
Q
Basal n-1
s
s
s
Global n-6

*
H
H
Y

*
Apical n-1

b
b
b
b
b
b
P
P
P
P
P
P
@
@
@
@



'
&
$
%
s
s
s
b
b b
b
s


s
s


s
s

s
s
s
s
s
Fig. 2.45 The development of Red Alga
2.7
Membrane Computing
Membranes are one of the basic ingredients in the organization of living organ-
isms. They determine a space partition in internal and external points, separated by
a surface, called skin. Biomolecules inside a membrane are selected, protected, and
concentrated. In this way, the speciﬁcity and efﬁciency of biochemical reactions
is guaranted. Membrane systems are mathematical objects where abstract struc-
tures of membrane organization are investigated in the perspective of information
processing.
Objects and membrane are dual concepts which can be reciprocally reduced (an
analogous situation arises in set theory duality between elements and sets). This
duality is a special case of the space/matter duality formulated in the context of a
discrete framework. In fact a physical object, having a spatial extension comprises
a portion of space, the space occupied by it, that can be delimited by an implicit
membrane delimiting its internal region. Conversely, a membrane is an object with
an internal region which can include other objects. Therefore, we may consider an
object of type a as equivalent to an empty membrane [ ]a. Analogously an object a
inside the membrane of label j, [a]j, is represented by an object a j with the index
denoting the localization of a. In general, we may reverse the relationship of con-
tainment of membranes and objects, by expressing the localization of an object by
labeling it with a membrane address (a string of membrane labels). Here we do not
enter into further details. However, many aspects deserve a careful analysis.
Membrane computing, introduced by Gheorghe P˘aun in 1998 [49, 50, 74, 51],
develops an analysis of computations and languages in terms of multisets of objects
and their localization inside compartments. The class of membrane systems he in-
troduced are also referred as P systems (P is the initial letter of P˘aun). Differently

76
2 Strings and Genomes
from classical Formal Language theory, which is based on strings, in this framework
multisets are the basic structure, and multiset rewriting rules are usually applied in
a maximal parallel way (a set of rules which can be simultaneously applied must
be applied). Rules are located into membranes and can be applied only to objects
inside the membranes where they are located, even if results produced by the rule
applications can be moved into other membranes. Figure 2.46 shows a membrane
system and one step of computation applied to a conﬁguration where contents are in-
dicated for each membrane (0 is the index of the skin, the most external membrane).
In standard membrane computing, multisets are denoted by commutative strings
(strings where the order of symbols is not relevant), however, here for notational co-
herence with the other chapters, we adopt the multiset polynomial notation. We refer
to the books cited above for the main results and application ﬁelds of this theory.
Here, after a short deﬁnition of a membrane system, we want to list some aspects,
related to the notion of membrane, that show its capability of modeling biological
phenomena.
Fig. 2.46 A membrane system
Membrane conﬁgurations and rules can be easily represented by bracket expres-
sions. For example, the membrane structure given in Fig. 2.46 can be written in the
following way:
[0[1[4 ]4 ]1 + [5 ]5 + [6 [2 ]2 + [3 ]3 ]6 ]0
and the system given in Fig. 2.46 (membrane structure with initial conﬁguration and
membrane rules) is synthetically expressed in Table 2.10.

2.7 Membrane Computing
77
Table 2.10 The bracket representation of the membrane system of Fig. 2.46
[0 2a+b+c+ [1 a+c+ [4 ]4 ]1 +[5 ]5 +[6 2a+2b+2c+ [2 ]2 + [3 ]3 ]6]0
[0 a+b →[0 c
[0 c →[5 d
[1 a →[4 b
[6 a →[3 b
[6 b →[2 c
Usually, membrane systems are represented by means of Venn diagrams with
symbols and rules inside them. Many different notations can be used, very often
with a natural interpretation. However, when a multiset m inside a membrane of
index i is denoted by [m]i, then we mean that m is all the content of [ ]i, while we
write [im for indicating that m is a sub-multiset of the content of [ ]i.
This notation is also called boundary notation and was introduced [65] in order
to cope with more general membrane rules. In fact, in P˘aun’s original formulation,
rules are inside membranes and everything outside the membrane where a rule is lo-
cated remains unknown to the rule. But, in many cases a wider visibility is required.
The essential point of boundary representation is the idea of rules with a greater
level of localization knowledge about the objects which they apply to. An important
case of this situation is present in anti-port rules which postulate to consider objects
inside and outside membranes.
The example of Fig. 2.47 is not standard in membrane computing, but it is very
intuitive because it constitutes a membrane representation of a classical abacus for
performing sums.
Multiset rewriting with maximal parallel strategy means that at any step a set
of rules is chosen in a maximal way, because they can be simultaneously applied,
and no other rule can be applied together with them (and rules have to consume as
many objects as they can). This rewriting strategy provides universal computation
even in the case of only one membrane. The proof of this statement is given by
means of a membrane representation of register machines, in a formulation due
to Minsky (which are proved to be computationally universal). They use only two
types of instructions. Any instruction has a number as label (indicated on the left),
and applies when this label is equal to the content of the program counter (a special
register). Instruction i : inck j increments the content of the register Rk and sets to
j the value of the program counter. The instruction i : condeck j,h decrements the
content of the register Rk, by putting j as the value of the program counter, if the
content of Rk is different from zero, while it does not alter any register, and sets to h
the value of the program counter, if the content of Rk is zero.
Let us represent the register Rk containing the value m with the multiset of m
copies of symbol rk, and let us represent by the presence of symbol Pi the fact

78
2 Strings and Genomes
Fig. 2.47 A membrane system which realizes the sum of two decimal numbers of three digits.
Input numbers are represented by multisets of three symbols (a1, a2, a3 for the ﬁrst input and
b1, b2, b3 for the second input). The output is obtained after three steps (by maximal parallel
applications of rules) by the multiplicities of c in membranes [ ]1, [ ]2, [ ]3. The case of the sum
325+478 is indicated in the Venn diagram of the initial and ﬁnal state of the computation.
that instruction of order i is the current instruction. With this representation of the
computation states, the instructions of the register machine will be given by the
following multiset rules.
1. An instruction i : inck j is expressed by the rule Pi →rk + Pj.
2. An instruction i : condeck j,h is expressed by the rules:
Pi →Si + #
Si + rk →Qi
# →$
Qi + $ →Pj
Si + $ →Ph
It is easy to realize that, with maximal parallel rewriting, the above translation pro-
vides, in the membrane register representation, the effects of the corespondent Min-
ski’s instructions in the original registers.
Many features of biological membranes were introduced in membrane computing
in order to analyze their computational relevance; moreover many different mem-
brane systems were introduced for expressing a great variety of distributed, paral-
lel, and non-deterministic computations [49, 50, 74, 51]. Many of them were an-
alyzed in speciﬁc papers of membrane computing (see http://ppage.psystems.eu).

2.7 Membrane Computing
79
A great deal of research in membrane computing was devoted to results estab-
lishing the computational universality of speciﬁc classes of membrane systems.
Many applications of membrane formalisms consist in discrete models of biological
phenomena.
The following list summarizes some important aspects of membrane that are rel-
evant to biological discrete modeling.
1. Membrane or object polarities (positive, negative, and neuter) can be consid-
ered. Therefore, rules can be applied according to the presence of certain polar-
ities and may also transform polarities.
2. Membrane thickness can prevent the membrane permeability, that is, exit/
entrance of objects from/to a membrane and rules may change a membrane
thickness.
3. Rule priorities can establish an order among the rules of a membrane, and rules
with a lower priority can be applied only if no rule with a higher priority can be
applied.
4. Catalysts are objects that participate in a rule as reactant and products, which
are not transformed during its application. They can play an important role as
controllers. In fact, if a catalyst is not available the rule cannot be applied, and
the amount of a catalyst determine the level of activity of a rule where it is
involved.
5. Synport/antiport rules realize the entrance/exit of objects from/to membranes
only in a synchronized way among pair of objects (a can enter/exit only if b
enters/exits or vice versa).
6. Promoters/Inhibitors may be associated to the rules: their presence can be used
for allowing or for avoiding the application of some rules.
7. Multi-membranes may be considered, that is, membranes which occur in many
copies with possibly different contents. In this case when a rule is associated to
this kind of membrane, it may be applied to all its copies, or only to some of
them (a speciﬁc strategy has to be deﬁned).
8. Parameters could be useful in representing physical variables which are not
objects, but may inﬂuence the applicability of rules (temperature or pressure, or
electrical potential).
9. Deterministic strategies could be imposed in several ways, for example, by
combining priority among rules with promoters and inhibitors, or even pro-
grams could be added to membranes which at each step activate at most one
rule, or groups of independent rules. The metabolic P systems, which will be
introduced in the next chapter, are a special case of deterministic P systems.
10. Non-deterministic strategies can be used in a completely free manner or with
some speciﬁc criteria. In the original formulation maximal parallelism was
adopted; however, intermediate forms of non-determinism could be considered,
where maximal parallelism could be conﬁned to some membranes or to some
kinds of rules.
11. Movement of objects could be considered only from a membrane to an imme-
diately included or immediately including membrane.

80
2 Strings and Genomes
12. Special rules may change the membrane structure, for example by removing
a membrane (releasing its content in the membrane including it), or by creat-
ing membranes, or by including a membrane (with its content) inside another
membrane, or expelling a membrane (with its content) from a membrane which
includes it. These operations correspond to basic biological phenomena (dis-
solution, vesiculation, endocytosis, exocytosis). They are easily described in
terms of bracket expressions, and may be extended and specialized in many
ways.
13. Porters or carriers of objects could be used which control the object movement
according to some transport/addressing mechanisms.
14. Many levels of membrane skins could be useful for internal localizations. For
example, only objects which are at the ﬁrst level could have some kind of inter-
action with external objects.
15. Membrane structures could assume relations which are different from mem-
brane containment; for example, speciﬁc channels or communication lines
could be assumed and rules changing these connections dynamically could be
postulated.
16. Hyper-rules refer to rules which involve not only multisets, but more generally
substructures. For example a rule such as [hc[ia[jb →[jb[ia[hc acts on three
membranes and implies a complex transformation where the existence of three
speciﬁc objects in the three membranes changes their containment relationship.
In general, it would be very useful to introduce the notion of P-term, realized
by a bracket expression possibly including variables of multisets or membrane
structures. In this way, some very general operations could be represented in a
very synthetic and powerful way, For example, let X,Y,Z be variables of mul-
tisets, then the following rule changes radically the inclusion structure and the
content of some membranes:
[ia + X + [jb +Y]jZ]i →[ib +Y + [ja + X]jZ]i
17. Structured objects like strings or trees could replace the simple objects of the
original membrane systems, allowing speciﬁc operations over these structures
in the transformations described by rules.
18. Input/output strategies establish where to put the multisets which encode the
input data (according to some encoding mechanism) and where outputs of a
terminating computation have to be read. However, instead of focusing on ter-
minating computations which provide some kind of results, some external rules
(outside the skin) may yield resources, by modeling the typical interactions of
cells with the environment. In this case, the behavior of the system is intended
to cope with the realization of some dynamical regimes of interest, keeping
some variables within certain intervals, while external rules satisfy some spe-
ciﬁc constraints [65].
19. Termination strategies could be deﬁned in many ways; for example, the pres-
ence or the absence of some symbols in some membranes could denote the end
of computation, but even the absence of applicable rules can play the same role.

2.8 Informational Analysis of Genomes
81
20. Loci (Latin word for “places”) as localization mechanisms more general than
membranes, could be added. For example, if speciﬁc zones of membrane skin
are deﬁned, the position of objects could be given in terms of coordinates which
refer to them. Moreover, implicit forms of localization could be based on con-
centration gradients along some directions.
By using some of the features mentioned in the list above it is possible to prove a
great number of results of computational universality [50]. We will mention only
two results of this kind:
NRE = NOP1(cat2)
that is, the recursively enumerable sets of natural numbers coincide with the set
of numbers which can be generated by P systems with one membrane and rules
allowing at most two catalysts. Moreover:
NRE = NOP3(sym1,anti1)
that is, the recursively enumerable sets of natural numbers coincide with the set
of numbers which can be generated by P systems with three membranes and rules
allowing one symport rule and one antiport rule.
2.8
Informational Analysis of Genomes
Genomes are containers of biological information, they direct the functions of the
organisms and they are what is transmitted to the generations of their organisms.
During their transmission variations are introduced, which change the organisms
hosting them. Therefore, according to the evolutionary mechanisms, genomes are
selected indirectly through the effects they produce in the organism they direct.
The selective force driving this process pushes genomes of organisms with the best
capacity of spreading in the environments and of reproducing abundantly. From
this point of view, we could see organisms as means for selecting genomes, instead
of genomes as means for realizing organisms. The situation is a typical case of
duality, in the sense that each of the two realities needs the other one. However,
what it is essential to stress here, is the speciﬁc informational nature of genomes.
If they are responsible for the most important part of biological information, then
they have to follow the general rules which information mechanisms follow. This
simple statement implies some important informational perspectives in the analysis
of genomes, which we will outline in the following.
Many seminal works were developed in recent years where concepts from in-
formation theory and computer science [198, 200, 64, 22], formal language theory
[213], and linguistics [16] were used in the investigation of genomes considered as
particular texts ([19, 37, 52, 57, 59, 60, 24, 33, 31, 54]). These researches applied
information theoretic notions (information, entropy, mutual information, encoding,
compressibility, complexity measures, and randomness), or formal language theory
(grammars, automata, patterns), or linguistic concepts (words, dictionaries, lexical

82
2 Strings and Genomes
categories, distributional classes) for recognizing and analyzing functional roles of
DNA fragments.
Here we brieﬂy outline an approach, which we call Infogenomics [20, 27],
under development, where we apply methods of informational text analysis specif-
ically conceived for genome analysis. Infogenomics is aimed at devising and com-
puting informational indexes able to provide systematic “localization” of genomes
in many-dimension spaces where these indexes may vary. If these indexes are de-
ﬁned in an adequate manner, then the genome characterization by means of them
could correspond to important biological or evolutive properties which reﬂect their
internal organization, and could provide proﬁles for comparing genomes of different
species or even of different physiological/pathological situations.
Bioinformatics had a crucial role in the analysis of biopolymers by means of
the notion of sequence similarity and sequence alignment. In this ﬁeld, many algo-
rithms on strings were essential in a huge number of important applications. The
epochal sequencing of complex genomes was surely impossible without the ex-
istence of these algorithms and of their efﬁcient implementation. However, now
many genomes, of different types of organisms, are available as ﬁles in public sites.
The number of sequenced genomes is near to 1000, from bacteria to Homo sapiens
(without counting viruses). They are treasures and an integrated analysis of their
informational, mathematical, and linguistic features could reveal new clues in the
challenge of understanding their languages. This perspective requires a systemic
approach where genomes are considered not only strings, but structures based on
strings and the components and features of these structures could be discovered by
comparing them. This emerging perspective [35, 36, 34, 62] is based on alignment
free methods of genome analysis, where global properties of genomes are investi-
gated, rather than local similarities based on classical methods of string alignment.
On the side of molecular biology and biochemistry many international projects
are active for deciphering genomes, in order to pass from the knowledge of genome
sequences to their biological functions. In particular, the project ENCODE (ENCy-
clopedia Of DNA Elements) [23] is mainly aimed to extract lexicons, and catalogs
of biochemically annotated DNA elements, in the human genome. In this context,
biochemical functions were assigned to 80% of the genome, mainly outside the
protein-coding regions (with a clear evidence of their crucial role in regulation of
gene expression). A very complex dynamics of interactions results among DNA re-
gions, proteins, and RNAs, with a lot of newly identiﬁed elements, and with a huge
number of data (see websites: http://nature.com/encode, http://epd.vital-it.ch). This
scenario has certainly an informational basis, linked to the DNA strings related to
these elements. Therefore, an integration between biochemical and informational
perspective could provide important synergies, with new possibilities for interpret-
ing data and for discovering principles of genome organizations and functions.
A simple argument can show the crucial aspect that dictionaries play in the anal-
yses of genomes. Let us consider a genome G with a length of 106 bases. All
the sequences of length 40 which we encounter by scanning all the genomes are
(106 −39), moreover possibly many of them occur many times. However, the num-
ber of possible different words of four letters having length 40 is 440 which is a value

2.8 Informational Analysis of Genomes
83
greater than 1024. This means that the part of sequences of length 40 which occur in
that genome is a portion 10−18 smaller than the whole set of possible words. This
simple numerical evaluation tells us that surely sequences of length 40 have to be
meaningful. In fact if they occur, surely they were selected among a huge number
of other possible sequences. The real sequences of this length which we meet in a
genome are a small part of those which were evaluated, during the evolutive process.
In other words, the meaning of words is related to the relationship between the real
words and all possible words.
Let us denote by Γ the genomic alphabet of four symbols (characters, or letters,
associated to nucleotides): Γ = {A,T,C,G} (then Γ ⋆, as usual, denotes the set of all
possible words over Γ ).
A genome G is representable by a sequence over Γ , that is, a table assigning a
symbol of Γ to each position (from 1 to the length of G). Symbols are written in a
linear order, from left to right, according to the standard writing system of western
languages, and to the chemical orientation 5′ −3′ of DNA molecules.
We remark that other equivalent representations of sequences are possible. For
example, we could represent G, by a function associating to each symbol of Γ the set
of positions where it occurs. In this way G is identiﬁed by four sets of numbers, say
N(A), N(T), N(C), N(G). It is also important to distinguish between subsequences
and substrings (also called words, factors, k-mers) of G. Indeed, a subsequence
is a sequence of symbols occurring on a set of positions (considered in their order),
while a substring is a subsequence of symbols which are (contiguously) associated
to all the positions between an initial and a ﬁnal position (of course, any string is
also a sequence). If a genome has length n, then according to the Gauss triangular
formula, it has at most n(n −1)/2 different factors (n factors of length 1, n −1
of length 2, and so on, up to only one factor of length n), while all the possible
subsequences are 2n (the different ways of choosing sets of positions).
A dictionary D of a genome G is a factorization of G when the concatenation of
all the elements of D, possibly with overlapping sub-strings, yields G (the overlap-
ping concatenation of αγ with γβ is αγβ). It is intended that in this concatenation
the elements of D may occur at least once, but possibly more than once. We re-
mark that the problem of genome sequencing can be expressed in the following
way. Given a genomic dictionary D (consisting of words of G, called reads, usu-
ally of average lengths under 1000 bp), ﬁnd the most probable genome G such
that D is a factorization of G, and where the concatenation of elements is always a
proper overlapping concatenation. Despite this simple formulation, this problem is
computationally complex and its solution is not uniquely deﬁned, in mathematical
terms, but can be found, with a certain probabilistic belief (supported by the em-
pirical evidence) by means of different and repeated reconstruction experiments of
G from different factorizations of it. Nowadays, many different sequencing meth-
ods are available, which are based on different technologies. Crucial parameters of
these sequencing methods are the average length of reads and the number of hi-
erarchical phases of string assembling (where fragments of increasing lengths are
reconstructed from factorizations of these fragments).

84
2 Strings and Genomes
Dictionary Based Indexes
We denote by D(G) the set of all factors of a genome G, while we call k-genomic
dictionary of G (for some k ≤|G|), denoted by Dk(G), the set of all the k-long sub-
strings of genome G. Starting from dictionary Dk(G), the k-genomic table Tk(G),
which mathematically corresponds to a multiset, is deﬁned by equipping the words
of Dk(G) with their multiplicities, that is, the number of their respective occurrences
in G. Let α(G) denote the multiplicity of α in a genome G, and posG(α) give the set
of positions of α in G (that is, the positions where the ﬁrst symbol of α is placed).
Of course, it holds α(G) = |posG(α)|. The table Tk(G) may be represented by a
list of associations of strings to their corresponding multiplicities: α →α(G), with
α ∈Dk(G). The sum of all the multiplicities of elements in Dk(G) is called size of
Tk(G), denoted by |Tk(G)|. It is easy to realize that:
|Tk(G)| = |G|−k + 1.
In general, the multiset Tk(G) associated to the genome G does not univocally indi-
viduate G. In fact, let us assume that G has the following string structure:
G = −−−γ1 −−−−xxxxx−−−−γ2−−−−γ1−−−−yyyyy−−−−γ2−−−
Now, if we exchange thetwo fragments included between γ1and γ2 and if their lengths
are equal to, or longer than k, the resulting genome G′ is such that Tk(G) = Tk(G′),
because the k-factors of these two genomes are the same. In fact, the k strings which
occur internally in γ1 −−−−xxxxx−−−−−γ2 and in γ1 −−−−yyyyy−−−−γ2
do not depend on the positions of these strings, while those which are partially inside
and partially outside to the (left and right) borders depend on the k−1-contexts, that
is, the strings of length k −1 which they have on the right and on the left. But, these
contexts in this case are exactly the same, because |γ1| ≥k and |γ2| ≥k.
We say that two genomes G1 and G2 are multiset k-equivalent when Tk(G1) =
Tk(G2).
Given a dictionary D of a genome G, the Multiplicity-Comultiplicity distribu-
tion MC, relative to D and G, may be deﬁned by means of a graphical proﬁle, where
in the abscissa the multiplicities are given, in increasing order (0, 1, 2, ...), and in
the ordinate the number of words of D having a given multiplicity of occurrence in
G is indicated.
All the typical parameters of distributions (mean value, standard deviation, me-
dian, mode, ...) also determine speciﬁc values of distribution MC.
The same information of a multiplicity-comultiplicity distribution may be ex-
pressed as a rank-multiplicity Zipf map (usually employed to study word frequen-
cies in natural languages). Zipf’s distributions have in the abscissa the words in
decreasing order of frequency (in alphabetical order when they have the same fre-
quency), say this order rank, and in the ordinate the value of frequency associated to
a rank. Zipf’s curves prove to be sensibly different for different genomes, but in all

2.8 Informational Analysis of Genomes
85
cases, there are few elements with maximal multiplicity, indeed Zipf curves initially
slope down steeply.
Selectivity, Lexicality, and Forbidden Words
We call k-lexical fraction of a genome G the value |Dk(G)|/4k, which is the per-
centage of different k-mers occurring in G with respect to all the possible ones. Of
course, |Tk(G)|/4k is an upper bound for |Dk(G)|/4k. A better evaluation for such an
upper bound is given by the value 1/(1 + 4k/|G|) which approximates |Dk(G)|/4k
for a random sequence over Γ having length |G|. In fact, let us assume that G is
random, then if q is the fraction of k-mers occurring at least once in G, then the
fraction of k-mers occurring at least twice in G is q2, and in general the fraction of
k-mers occurring at least i times is qi, therefore, assuming q < 1, for a very long
genome G, its length can be approximated in the following way [25]:
|G| = 4k(q + q2 + ...qi ...) = 4k
q
1 −q.
Therefore,
|G|(1 −q) = 4kq
that is:
|G| = q(|G|+ 4k)
which implies:
1/q = (|G|+ 4k)/|G|
or equivalently, the fraction of k-mers occurring in a random genome of length |G|
(of length sensibly shorter than 4k) is:
1/q = (1 + 4k/|G|).
(2.5)
The computations of |Dk(G)|/4k for the genomes of Table 2.11 are in all cases
sensibly under this estimation. For example, for H. sapiens chr. 19, 1/(1+412/|G|)
is equal to 0.791, while |D12|/412 is equal to 0.639. We deﬁne for a genome G its
k-dictionary selectivity DSk(G) as the following difference:
DSk(G) = 1/(1 + 4k/|G|)−|Dk(G)|/4k.
(2.6)
Dictionary selectivity very often proves more indicative than the k-empirical entropy
of Ek(G), which can be deﬁned as:
E(Tk(G))
by applying to Tk(G) the following general deﬁnition of entropy E(X) of a multiset
X of size n with m elements of multiplicities n1,n2,...,nm:

86
2 Strings and Genomes
E(X) = −
m
∑
j=1
n j
n log n j
n .
The k-co-entropy coEk(G) is a measure of the distance from the maximum value
of the empirical k-entropy of a genome with the same dictionary D(G):
coEk(G) = lg(|Dk(G)|)−Ek(G).
(2.7)
On the basis of genomic k-entropy, many other genomic concepts can be developed
which could have a great relevance in the analysis of genomes, such as mutual in-
formation or entropic divergence.
Another related index for a genome G is given by its k-lexicality, that is, the
ratio:
|Dk(G)|/|Tk(G)|
(2.8)
which expresses the percentage of distinct k-factors of G with respect to the all the
k-factors present in G. It is clear that the k-lexicality increases with the word length
k, and does not exhibit any regularity with the genome length. Of course, the inverse
of this ratio provides an average repeatability of the k-factors of G.
When Γ k = Dk(G) we say that a genome G is k-complete, meaning that all the
possible genomic k-long strings occur (at least once) in G. If G is not k-complete,
a non-empty set Fk(G) of “non-appearing”, say forbidden k-words (also called
“nullomers” [34]), is given by the difference of sets:
Fk(G) = Γ k\Dk(G).
(2.9)
Of course, genomic k-completeness is related to the genome length. In fact, it is easy
to ﬁnd a genome length such that surely genomes of that length are k-complete. In
fact we can construct such a genome by concatenating, in any order, all the k-mers.
Therefore, we have 4k! genomes k-complete of length k4k. The search for the mini-
mum length providing genomic k-completeness, and of algorithms for constructing
such minimal genomes, is a non-trivial theoretical investigation of some possible
interest.
For each G, we can deﬁne its minimal forbidden length, denoted by MF(G), as
the minimum k such that G is not k-complete.
The cardinality of Fk(G), for k greater than MF(G) and within a small range over
MF(G), seems to be a very speciﬁc feature of each genome. It is indeed remarkable
that in all genomes we considered MF(G) is at least 6 and below 12 (see Table 2.15),
and it does not appear directly related to the biological complexity of corresponding
organisms. Another interesting character of genomes is the factor length selectiv-
ity LS(G), which expresses the gap between the length of factors which in principle
could be all accommodated in a genome G, and the length of those which are actu-
ally present in G (according to the value of its minimal forbidden length):
LS(G) = ⌊lg4 |G|⌋−(MF(G)−1)
(2.10)

2.8 Informational Analysis of Genomes
87
where ⌊x⌋is the ﬂoor (greatest integer less than the real value) of x. The value of
LS(G) is around 5 in all the unicellular and primitive multicellular organisms, and
is around 10 in the two human chromosomes we analyzed. A clear understanding
of this behavior should be investigated in more general terms; however, LS(G) is
surely related to an evolution selectivity action over the strings constituting genomic
dictionaries.
Hapax and Repeat Analysis
Two important types of factors of genomes are hapaxes and repeats. A hapax of a
genome G is a factor α of G such that α(G) = 1. The term hapax (from a Greek
word meaning once) came from the analysis of literary texts (for stylistic analysis
and text authorship attribution); however, now it is also used in informational text
analysis [64].
A repeat of G is a factor α of G such that α(G) > 1. Of course, the set H(G) of
hapaxes of G and the set R(G) of repeats of G constitute a bipartition of D(G) (at
least one element of Γ is a repeat and G is a hapax, therefore H(G) and R(G) are
non-empty, also disjoint sets, such that their union is D(G)). We set:
Hk(G) = Γ k ∩H(G)
(2.11)
Rk(G) = Γ k ∩R(G)
(2.12)
where ∩is the set-theoretic intersection. Therefore, given a genome G of length n,
for any k ≤n we can read it according to the bi-partition of its k-genomic dictionaries
Hk(G) and Rk(G).
A more reﬁned measure for the average k-factors repeatability in G may be
now given as:
ARk(G) = |Tk(G)\Hk(G)|
|Rk(G)|
(2.13)
where k-hapaxes have been excluded by both the k-genomic multiset and the
k-genomic dictionary (the symbol\represents the set-theoretic difference). Index
ARk(G) counts the proper (average) repeatability of k-repeats in genome G.
The concepts of hapax and repeat provide a great number of related notions.
In the following section we will discuss experimental data, reported in tables, di-
agrams, and ﬁgures, which include the measure of the ratio between |Hk(G)| and
|Rk(G)| as a function of k (that is, how the number of hapax words of a given length
increases or decreases with respect to the number of repeats of that length). An im-
portant phenomenon guided us in the choice of the string lengths for the computed
dictionaries. In fact, we observed a sort of transition phase effect in the passage
from D12(G) to D18(G), in almost all genomes of Table 2.11, where a clear inver-
sion appears in the ratio hapax-cardinality/repeat-cardinality.
Let us mention brieﬂy other relevant indexes, related to hapax and repeat con-
cepts, that will be reconsidered in the following (for deﬁnitions see Table 2.13):
minimal hapax length, denoted by MH, maximal repeat length MR, repeat

88
2 Strings and Genomes
positions RP. An important concept is the maximal k-repeat distance, deﬁned
as (symbols ⊥and ⊤denote respectively minimum and maximum over sets of
numbers):
MDk(G) = ⊤{|j −i| : α ∈Rk(G),i ∈posG(α), j = ⊥{k : k ∈posG(α),k > i}}.
In Table 2.11, some genomes are reported as case studies of infogenomics analyses.
In Table 2.12, sizes of some genomic dictionaries are reported. In Table 2.13 some
infogenomics indexes were collected, while in Tables 2.14, 2.16, 2.17 numerical
values of these indexes are reported which were computed for genomes of Table
2.11 [20]. Sequences were downloaded as fasta ﬁles from NCBI genome database,
UCSC Genome Bioinformatics website, EMBL-EBI website, or KEGG Kyoto En-
cyclopedia of Genes and Genomes website. They constitute biological models
of remarkable relevance in the genomic analyses. A most detailed description of
these genomes may be found in: http://users.rcn.com/jkimball.ma.-
ultranet/BiologyPages/G/GenomeSizes.html.
Table 2.11 A list of genomes
Organism genome
Length
Genes
Type
Nanoarchaeum equitans
490,885
536
Minimal archaeum
Mycoplasma genitalium
580,076
476
Minimal bacterium
Mycoplasma mycoides
1,211,703
1,016
Venter’s experiment bacterium
Haemophilus inﬂuenzae
1,830,138
1,717
First sequenced bacterium
Escherichia coli
4,639,675
4,685
Bacterium model (K-12)
Pseudomonas aeruginosa
6,264,404
5,566
Ubiquitous bacterium
Saccharomyces cerevisiae
12,070,898 6,275
Unicellular eukaryote (Yeast)
Sorangium cellulosum
13,033,779 9,700
Longest genome bacterium
Homo sapiens chr. 19
63,800,000 2,066
Highest gene density H. chromosome
Caenorhabditis elegans
100,267,63219,000 Worm (around 1000 cells)
Drosophila melanogaster
129,663,32714,000 Insect (Fruit ﬂy)
Homo sapiens chr. 1
247,000,0003,511
Longest Human chromosome
For all our genomes of Table 2.11, listed according to the increasing genome
length, we report in Tables 2.14, 2.16, and 2.17 numerical data related to the com-
putation of Dk(G),Fk(G),Hk(G),Rk(G) for k= 6, 12, and 18, respectively.
It is easy to see that any genomic factor containing a hapax as a substring is a
hapax as well. Hence a hapax within the genome may be elongated (by keeping its
property of being a hapax) up to reach the genome itself, which is of course a hapax.
It is then interesting to evaluate, for each genome G: i) how |Hk(G)| varies with k, ii)
the k-hapax positions (that is, how densely hapax words fall in the genetic regions),
and iii) the shortest length of a hapax. Also, a k-similarity between genomes G and
G′ could be measured by |Hk(G)∩Hk(G′)| (we have some work in progress on the
computation of dictionary intersections).

2.8 Informational Analysis of Genomes
89
Table 2.12 Sizes of genomic dictionaries
|Γ 6| = 46
|Γ 12| = 412
|Γ 18| = 418
|Γ 24| = 424
4096
16,777,216
68.719476736×109
> 281×1012
Organism genome
|D6|/46
|D12|/412
|D18|/418
Nanoarchaeum equitans
0.99
0.025
≈7 ×10−9
Mycoplasma genitalium
0.99
0.029
≈8 ×10−9
Mycoplasma mycoides
0.99
0.038
≈14 ×10−9
Haemophilus inﬂuenzae
1
0.089
≈26 ×10−9
Escherichia coli
1
0.207
≈67 ×10−9
Pseudomonas aeruginosa
1
0.175
≈90 ×10−9
Saccharomyces cerevisiae
1
0.393
≈169 ×10−9
Sorangium cellulosum
1
0.23
≈185 ×10−9
H. sapiens chr. 19
1
0.639
≈610 ×10−9
Caenorhabditis elegans
1
0.83
≈1,315 ×10−9
Drosophila melanogaster
1
0.947
≈1,712 ×10−9
Table 2.13 Genomic indexes, dictionaries, and tables
Indexes/Dictionaries/Tables
Notation
Deﬁnition
Genomic Dictionary
D(G)
{α ∈Γ ∗: α ⊂G}
k-Genomic Dictionary
Dk(G)
Γ k ∩D(G)
k-Genomic Table
Tk(G)
α →α(G) : α ∈Dk(G)
k-Lexicality
Lk(G)
|Dk(G)|/|Tk(G)|
k-Dictionary Selectivity
DSk(G)
1/(1+4k/|G|)−|Dk(G)|/4k
Multiplicity-coMultiplicity k-distribution
MCk(G)
j →|{α ∈Dk(G) : α(G) = j}|
Forbidden k-Factors
Fk(G)
Γ k\Dk(G)
Minimal Forbidden Length
MF(G)
⊥{k : Γ k ̸⊆Dk(G)}
Factor Length Selectivity
LS(G)
⌊lg4 |G|⌋−(MF(G)+1)
Hapaxes
H(G)
{α ∈D(G) : α(G) = 1}
k-Hapaxes
Hk(G)
Γ k ∩H(G)
k-Hapax-factor ratio
HDk(G)
|Hk(G)|/|Dk(G)|
Minimal Hapax Length
MH(G)
⊥{|α| : α ∈H(G)}
Repeats
R(G)
{α ∈D(G) : α(G) > 1}
k-Repeats
Rk(G)
Γ k ∩R(G)
Maximal Repeat Length
MR(G)
⊤{|α| : α ∈R(G)}
Repeat Positions
RP(G)
α →posG(α) : α ∈R(G)
Length-Multiplicity Repeatability
LM(G)
j →α(G) : |α| = j , α ∈R(G)
Average k-Repeatability
ARk(G)
|Tk(G)\Hk(G)|/|Rk(G)|
k-Repeat-factor ratio
RDk(G)
|Rk(G)|/|Dk(G)|

90
2 Strings and Genomes
Table 2.14 Indexes related to D6 dictionaries
Genomic sequences
|D6|
L6
|H6|
|R6|
HR6
Nanoarchaeum equitans
4,094
0.008
6
4,086
1.468 ×10−3
Mycoplasma genitalium
4,082
0.007
35
4,047
8.65 ×10−3
Mycoplasma mycoides
4,076
0.003
39
4,037
9.661 ×10−3
Haemophilus inﬂuenzae
4,096
0.002
0
4,096
0
Escherichia coli
0
0.0009 0
4,096
0
Table 2.15 Genomic forbidden lengths and words
Genomic sequences
MF |F6| |F7|
|F8|
|F9|
|F10|
|F11|
|F12|
Nanoarchaeum equitans
6
2
552
13,294 130,705 802,658 3,837,340 16,346,170
Mycoplasma genitalium
6
14
851
14,196 126,707 779,814 3,789,892 16,281,022
Mycoplasma mycoides
6
20
1,269 18,498 141,646 791,717 3,747,529 16,130,251
Haemophilus inﬂuenzae
7
0
12
1,077
33,891
442,572 3,083,682 15,281,515
Escherichia coli
7
0
1
176
5,617
150,468 1,997,469 13,298,293
Pseudomonas aeruginosa
8
0
0
276
21,387
326,432 2,536,746 13,827,364
Saccharomyces cerevisiae
9
0
0
0
99
31,619
1,007,904 10,179,957
Sorangium cellulosum
7
0
1
683
23,327
299,079 2,296,098 12,913,817
Homo sapiens chr. 19
9
0
0
0
53
12,763
469,379
6,041,533
C. elegans
10
0
0
0
0
38
54,011
2,847,301
D. melanogaster
11
0
0
0
0
0
3214
886,004
Homo sapiens chr. 1
9
0
0
0
1
2,698
149,365
2,830,885
Table 2.16 Indexes related to D12 dictionaries
Genomic sequences
|D12|
L12
|H12|
|R12|
RD12 HR12 AR12
Nanoarchaeum equitans
431,046
0.87
385,146
45,900
0.11
8.39
2.30
Mycoplasma genitalium
496,194
0.85
435,502
60,692
0.13
7.175 2.38
Mycoplasma mycoides
646,965
0.53
442,836
204,129
0.32
2.169 3.76
Haemophilus inﬂuenzae
1,495,701
0.81
1,256,043
239,658
0.17
5.240 2.39
Escherichia coli
3,478,923
0.74
2,675,846
803,077
0.24
3.331 2.44
Pseudomonas aeruginosa
2,949,852
0.47
1,799, 637
1,150,215
0.39
1.564 3.88
Saccharomyces cerevisiae
6,597,259
0.54
3,977,392
2,619,867
0.40
1.518 3.08
Sorangium cellulosum
3,863,399
0.29
1,924,969
1,938,430
0.51
0.993 5.73
Homo sapiens chr19
10,735,683 0.19
3,359,705
7,375,978
0.69
0.455 6.99
C. elegans
13,929,915 0.13
3,099,744
10,830,171 0.78
0.286 8.97
D. melanogaster
15,891,212 0.12
1,632,045
14,259,167 0.9
0.114 8.89

2.8 Informational Analysis of Genomes
91
Table 2.17 Indexes related to D18 dictionaries
Genomic sequences
|D18|
L18
|H18|
|R18|
RD18 HR18 AR18
Nanoarchaeum equitans
489,465
0.99
488,802
663
0.001 737.253.11
Mycoplasma genitalium
569,202
0.98
563,045
6,157
0.01
91.44 2.76
Mycoplasma mycoides
987,645
0.81
913,599
74,046
0.07
12.33 4.025
Haemophilus inﬂuenzae
1,795,492
0.98
1,775,531
19,964
0.01
88.93 2.64
Escherichia coli
4,557,590
0.98
4,518,585
39,005
0.008 115.843.10
Pseudomonas aeruginosa
6,183,215
0.98
6,117,968
65,247
0.01
93.76 2.24
Saccharomyces cerevisiae
11,499,795 0.95
11,307,098 192,697
0.01
58.67 3.96
Sorangium cellulosum
12,640,960 0.96
12,340,846 300,114
0.02
41.12 2.30
Homo sapiens chr19
41,529,106 0.75
39,256,297 2,272,809
0.05
17.27 6.91
C. elegans
89,444,661 0.89
85,157,627 4,287,034
0.04
19.86 3.52
D. melanogaster
116,446,6270.90
112,977,0463,469,581
0.02
32.56 4.45
Table 2.18 MR index, positions of the only twice repeating word of length MR, and relative
distance between the two occurrences (with respect to the genome lengths)
Genomic sequences
MR
MDMR/|G|
Nanoarchaeum equitans
139
96.95%
Mycoplasma genitalium
243
0.15 %
Mycoplasma mycoides
10,963 0.019 %
Haemophilus inﬂuenzae
5,563
8.05%
Escherichia coli
2,815
0.89 %
Pseudomonas aeruginosa
5,304
12.37 %
Saccharomyces cerevisiae
8,375
0.07%
Sorangium cellulosum
2,720
27.68 %
Homo sapiens chr19
2,247
0.02%
C. elegans
38,987 0.10 %
D. melanogaster
30,892 0.02 %
The phenomenon regarding hapax statistical distribution may be observed pass-
ing from 12- to 18-genomic dictionaries (see Tables 2.14, 2.16, and 2.17). For all the
genomes, by enlarging the k value, the number of hapax increases, even relatively to
the number of repeats (roughly speaking, “most of the 12-words are repeats while
most of 18-words are hapax”). Indeed, by computing HRk, we see that repeatability
generally almost disappears for k = 18, with respect to the number of hapaxes.
More interestingly, the (relative) amount of hapaxes increases by some order of
magnitude with k passing from 12 to 18. Based on this observation coming from
computational analyses, one could suppose that by increasing the word size, ge-
nomic dictionaries composed by only hapaxes may be computed. This intuition
has been invalidated (see Table 2.18). In fact, repeats having lengths of several
thousands have been found within each of our genomes, and 12 →18 represents

92
2 Strings and Genomes
a sort of transition phase from scarce to abundant hapax/repeat distribution. This
phenomenon would surely deserve a more detailed and generalized analysis.
Any substring of a repeat word is still a repeat, with its own multiplicity along
the genome, and inside the repeat word itself. A further index is thus deﬁned over
genomes G, called MR(G) (maximal repeat length), as the maximal length of
words γ such that γ(G) > 1. An algorithmic way to ﬁnd it (for our genomes) starts
from repeats out of D18(G) (that are less than the hapaxes) and checks how much
they may be elongated on the genome by keeping their status of repeat words. Data
related to the MR index computed over our genomes are reported in Table 2.18,
where the only MR-long repeat of each genome exhibits a non-trivial structure (that
is, different than polymers with a same nucleotide or similar patterns), and complex
repeats are obtained for many lengths.
The importance of word repeatability is crucial to understanding the information
content of texts. A genome analysis in terms of (shortest) hapaxes and (maximal)
repeats, providing their relative distribution within the genome, highlights the asso-
ciative nature of DNA as a container of information. Localization and frequency of
speciﬁc DNA fragments is indeed crucial to understand the information organiza-
tion of genomes. Hapaxes, occurring once in the genome, by their nature have a role
of address for the speciﬁc retrieval of functional elements, characterized by redun-
dancy and repeatability. On the other hand, an important characterization of repeats
may be given by means of their internal structure, that is, by the non-maximal re-
peats which compose them. These represent a second level repeatability, possibly
exhibiting various and rich genomic structural properties of functional sequences
(such as the presence of power strings).
Indexes, dictionaries, and tables given in this section identify a kernel of about
20 basic concepts, and many other notions may be derived from them. Namely, for
any numerical index Ik with parameter k, the distribution k →Ik can be deﬁned,
and its classical statistical parameters (mean, standard deviation, median, mode,
etc.) may be derived as further indexes (the same possibility holds for multiplicity-
comultiplicity factor distribution). Moreover, extending Shannon’s notion of typical
sequence in information theory, for any index I, a minimal I-typical sequence, for
a given genome G, is a portion of G such that the index I, restricted to this portion,
assumes (approximates) the same value which I assumes over the whole G. The
length and number of these sequences are other genomic indexes. The power of
some indexes in characterizing properties, which are relevant in speciﬁc contexts,
is a kind of research requiring computational experiments, mathematical analyses,
and biological interpretations and comparisons.
Bipartition of a genomic dictionary in hapax and repeat words emphasizes the
roots of precise string categories which are related to the functional organization of
genomes. The set of 18-repeats in our genomes has a size which is a couple of or-
ders smaller than the whole genome, and it seems to have a role of “lexical” coding.
Other elements, with a notably bigger size, seem to have a role of addressing, delim-
iting, coordinating, just like position-identiﬁcation tags. While the lexical nature of
repeated elements points out their semantic value, the “relative localization” nature
of the others gives importance to their unrepeatability along the genomic sequence.

2.8 Informational Analysis of Genomes
93
This phenomenon explains, at an informational level, what is apparent in complex
genomes, where it is well-known that the DNA “coding portion” is much smaller
than the rest. Understanding the logic of such a reality is a crucial challenge in
genome analyses. We would like to remark here that such a dichotomy corresponds
to statistical-informational evidence. On the basis of the hapax/repeat distinction,
we started a genome analysis for building synthetic gene networks, called repeat
sharing gene networks where genes (of a given genome) are the nodes, and two
genes are connected by a labeled edge if they share a repeat, which is put as label
of the edge. This procedure has so far been applied to small genomes, but many im-
portant phenomena were individuated and in some cases groups of genes strongly
connected (often as complete subgraphs) correspond to genes which are related to a
speciﬁc cell function.
Figures 2.48, 2.49, 2.50, 2.51, 2.52, and 2.53 are relative to gene networks ob-
tained by connecting genes having some common words (repeats) in N. equitans
and E. coli (for different lengths of words).
Fig. 2.48 The repeat-sharing whole network of Nanoarchaeum equitans (repeat length 16)
The deﬁnition, computation, and analysis of well-characterized dictionary-based
genomic indexes have pointed out some phenomena of genomic regularity and
speciﬁcity. They can enhance our knowledge about the internal logic of genome
structure and organization, as well as about evolutional and functional attributes of
genomes, speciﬁcally devoted to genome clustering. We trust this methodology of
comparative genomics, after a suitable and speciﬁc tuning of our indexes, may be a
basis for discrimination of genomic pathologies.

94
2 Strings and Genomes
Fig. 2.49 A portion of the N. equitans network of Fig. 2.48
Fig. 2.50 A portion of the N. equitans network of Fig. 2.48 with the common repeats as
labels of edges

2.8 Informational Analysis of Genomes
95
Fig. 2.51 The repeat-sharing whole network of Escherichia coli (repeat length 20)

96
2 Strings and Genomes
Fig. 2.52 The repeat-sharing whole network of Escherichia coli (repeat length 150)
Fig. 2.53 A portion of the E. coli network of Fig. 2.52
There are several possible lines of development:
1. A systematic analysis of other genome sequences (for example of all human
chromosomes) should give more arguments and hints for biological evaluations
of the genomic indexes;
2. The systematic analysis of repeat sharing networks, based on speciﬁc dictionar-
ies, could reveal/conﬁrm important functional gene organization explaining the
semantic roles of repeats;
3. Approaches based on genomic dictionaries could be exported to other kinds of
genomic data, for example those obtained by means of RNA-Seq methodology;
4. Genomic dictionaries of k-mers for k > 12 are very often computationally very
hard to be computed. Therefore, speciﬁc algorithms and data representations for
“long” k-mers could be developed;

2.8 Informational Analysis of Genomes
97
5. It would be interesting to compute intersections of genomic dictionaries and to
investigate if words which are common to many genomes are conserved along
evolutive lineages.
6. The inter-genomic character of hapaxes and repeats could be investigated, for
determining which hapaxes (resp repeats) of a given genome keep or not their
status of hapax (resp. repeat) within a given class of genomes.
7. New kinds of genomic representations could be investigated which could be use-
ful for speciﬁc analyses of genomic features relevant in speciﬁc contexts.
The last point of the list above is the basis for possible important developments.
Representing genomes, in non-conventional manners, could open new possibili-
ties in genome analysis. In fact, within a certain genomic representation some
aspects could emerge which are not evident in other conventional ways of ex-
pressing genomes. An interesting issue, in this context, concerns genome com-
pression methods. A genome compression provides a compact way of identifying
genome sequences, without loss of information. Many approaches have been inves-
tigated, where classical compression methods of information theory were applied
and adapted to genomes [31]. In general, genomes are sequences not efﬁciently
compressible, with standard methods.
Now, let us assume we ﬁnd an encoding that, within some class of genomes, is
able to provide efﬁcient compressions, for example, by reducing a genome G of n
bases to a sequence g of n/10 bits (an incredible compression ratio, with respect to
the actual genomic compressions which rarely are below 1.7 bits per base, in the
average case). Even if this encoding were hardly reversible, if injective, no loss of
information would occur in the passage from G to g. We call this kind of encoding
an one-way compression, because it reduces the digital information of G, but the
recovering of G from g is not efﬁcient, or even computationally hard, in the sense of
computational complexity (for example NP-hard). Nevertheless, g could be very im-
portant for representing the global identity of G in comparisons with other genomes
and, for its reduced size, more adequate for some speciﬁc genomic analyses.
Another kind of investigations concerns the lossy compression methods. Let us
suppose that a genome G is represented by a shorter string g that does not identify
G, but g retains some speciﬁc feature of G that is more evident, or more easily
detectable (for the reduced size of g). If this is the case, the compression method
adopted for generating g could provide an important clue, in the context particular
situations of genome classiﬁcation.
It is too early for evaluating the biological interest of these approaches, but it
is reasonable to believe that new methods of genome representation could disclose
relevant aspects in the informational organization of genomes.
Examples of graphics, related to genomic distributions deﬁned above, are given
in Figs 2.54 – 2.58.
Genomes cannot be fully considered as simple sequences, but more properly,
texts in the usual sense of our written texts of natural or artiﬁcial languages. In fact,
what we call a text is very often apparently a linear structure, because information
is organized at several levels that we distinguish by means of different kinds of in-
formation which are added to the basic linear structure: different kinds of alphabets

98
2 Strings and Genomes
Fig. 2.54 Multiplicity-comultiplicity distribution of 6-words of Escherichia coli
Fig. 2.55 Multiplicity-comultiplicity distribution of 6-words of Drosophila
Fig. 2.56 Rank-frequency distribution of 6-words of Escherichia coli (ordinate in logarithm
scale)
Fig. 2.57 Rank-frequency distribution of 6-words of Chromosome 19 of Homo sapiens (or-
dinate in logarithm scale)

2.8 Informational Analysis of Genomes
99
Fig. 2.58 Multiplicity-length distribution of repeats in Escherichia coli
and styles, character dimensions, and structural information (paragraphs, sections,
titles, footnotes, ...). When such a rich structure of text is represented in a pure
textual format, for example in terms of ascii code, then the whole information is
encoded in suitable manners by means of a complex process of representation. For
the sake of clarifying the idea, we can refer to modern languages for text representa-
tion such as Tex, Latex, or XML (see [41, 42] for examples of XML representation
of linguistic structures). However, the encoding process is very strict. If we delete
from a Latex ﬁle only a few characters, then it becomes useless and its compilation
providing the text it represents cannot be successfully realized. Genomes need to
realize a complex informational structure, but they are pure linear structures which
require a high level of ﬂexibility. DNA is a sort of associative memory, no addresses
are available, as in electronic memories, therefore hapaxes can play an essential role
in localizing and coordinating pieces containing speciﬁc meanings. In this perspec-
tive, the semantics of words do not depend on their absolute positions, but on their
relative positions in a system of anchors or markers for correctly determining some
elements within the genomic superstring where they occur.
Tables 2.19, 2.20, 2.21, 2.22, and 2.23 are examples of complex texts, such as a
web page or a mathematical formula. They are represented at different levels. The
information, at any level, is the same, but the “comprehension” at different levels
is very different. In the ﬁrst case the meaning underlying in Tables 2.19 and 2.20
is a famous web page, which everybody uses frequently as Web search engine. Its
meaning is very clear and any component of the page can be easily recognized with
its speciﬁc function. However, it is hard to recognize this meaning in the representa-
tions given in the mentioned tables. The second example is an elementary algebraic
formula (for solving quadratic equations). Its meaning is clear to anybody having
basic mathematical skills. However, also in this case, it is difﬁcult to recognize its
meaning from Tables 2.22 and 2.23. These examples intend to stress the importance
of discovering the structure of a text from its basic linear representation where the

100
2 Strings and Genomes
organization levels are put in a common unidimensional structure. This is surely the
case of genomic texts; therefore the examples help to imagine the kinds of difﬁcul-
ties faced in disclosing the internal logic of genomes.
Figures 2.59, 2.60, 2.61, 2.62, 2.63, and 2.64 are visual representations of the
genome of E. coli. The genome of this organism (a strain different from the more
common K12) is 5,528,445 base pairs. Colors denote bases according to the follow-
ing code: A green, C blue, G yellow, and T red. The size of edges (or sectors, or
circles) is proportional to the multiplicity of paths passing through them. Moreover,
in all these visualization we can focus on different parts of the structure, by enlarging
the view of some portions (Figs. 2.60, 2.62, 2.64 focus on parts of Figs. 2.60, 2.62,
2.64, respectively). The trees represented in these ﬁgures (with different visualiza-
tion methods, see [40]) are obtained by means of an algorithm, called “Crescendo”,
which is based on the elongation of initial seeds (in our case fragments of length 6),
which are chosen among the most frequent 6-mer words occurring in the genome,
plus the initial 6-mer word, or overlapping concatenations of these words occurring
in the genome. In the cases shown in the ﬁgures they are (multiplicities are between
brackets):
AGCTTTCTGGCG[1],CTGGCG[6032],
CTGGCGCTGGCG[22],CTGGCGCTGGCGCTGGCG[1],AGCTTT[1280].
These words were elongated by starting from each of them and adding, in the or-
der, all the bases that in the genome follow the seed, by stopping the elongation
when the beginning of any seed is encountered. In this way the concatenation of
these elongations covers the whole genome. The different elongation fragments are
7208 and occur 7336 times, therefore the majority of them are hapaxes. For a better
visualization, the trees represented in the ﬁgures end when the number of possible
continuations are less than 30. These ﬁgures suggest many possible analyses, but
what we want to remark here is that they suggest perspectives of genome analyses
that probably are hidden when we represent genomes in classical manners.
Genome visualization is not only a matter of presentation. In fact, when colors,
forms, and geometrical shapes are used in visualizations, we use many-dimensional
perspectives, which prove to be more adequate for expressing systemic phenom-
ena. As shown in Tables 2.19, 2.20, 2.21, 2.22, and 2.23 the internal organization
of texts is hardly recognized in their sequential representations, because it is very
often based on many representation levels, and this aspect is what makes a text dif-
ferent from a string (see tables 2.21, 2.22, 2.23). An interesting phenomenon, com-
mon to the genome representations based on Crescendo, is their noise ampliﬁcation.
Namely, if a genome is changed, in a small percentage, for example, by randomly
substituting some bases, then the corresponding (Crescendo) representations prove
to be different (with respect to reasonable metrics) in a percentage that is between 3
and 10 times the original variation percentage. This effect was conﬁrmed by many
experiments, and can be explained by the additional structure of the representation
that becomes more sensible to small variations in the genome linear structure.

2.8 Informational Analysis of Genomes
101
Table 2.19 First 1000 among 98.870 HTML characters of Google home page (spaces in-
cluded)
<!doctype html><html itemscope="itemscope" itemtype="http://schema.or
g/WebPage"><head><meta itemprop="image" content="/images/google_favi
con_128.eps"><title>Google</title><script>window.google={kEI:"cHB1UO7
WKcTGswa754HgBA",getEI:function(a){var b;while(a&&!(a.getAttribute&&(
b=a.getAttribute("eid"))))a=a.parentNode;return b||google.kEI},https:
function(){return window.location.protocol=="https:"},kEXPI:"31215,357
02,37102,38371,39523,39977,3300025,3300119,3300124,3300133,3300135,33
00137,3300152,3310000,4000116,4000354,4000553,4000624,4000648,4000742,
4000833,4000879,4000955,4001064,4001131,4001145,4001188,4001192,400126
7,4001281,4001437,4001441,4001449,4001459,4001568",kCSI:{e:"31215,3570
2,37102,38371,39523,39977,3300025,3300119,3300124,3300133,33001,35,330
01373300152,3310000,4000116,4000354,4000553,4000624,4000648,4000742,40
00833,4000879,4000955,4001064,4001131,4001145,4001188,4001192,4001267,
4001281,400144001441,4001449,4001459,4001568",ei:"cHB1UO7WKcTGswa 754H
g37,BA"},authuseml:funct r:0,
Table 2.20 Frst 500 hexadecimal characters of the HTML Google home page (spaces in-
cluded)
3C 21 64 6F 63 74 79 70 65 20 68 74 6D 6C 3E 3C 68 74 6D 6C 20 69 74 65
6D 73 63 6F 70 65 3D 22 69 74 65 6D 73 63 6F 70 65 22 20 69 74 65 6D 74
79 70 65 3D 22 68 74 74 70 3A 2F 2F 73 63 68 65 6D 61 2E 6F 72 67 2F 57
65 62 50 61 20 67 65 22 3E 3C 68 65 61 64 3E 3C 6D 65 74 61 20 69 74 65
6D 70 72 6F 70 3D 22 69 6D 61 67 65 22 20 63 6F 6E 74 65 6E 74 3D 22 2F
69 6D 61 67 65 73 2F 67 6F 6F 67 6C 65 5F 66 61 76 69 63 6F 6E 5F 31 32
38 2E 70 6E 67 22 3E 3C 74 69 20 74 6C 65 3E 47 6F 6F 67 6C 65 3C 2F 74
69 74 6C 65 3E 3C 73 63 72 69 70 74 3E 77 69 6E 64 6F 77 2E 67 6F 6F 67
6C 65 3D 7B 6B 45 49 3A 22 63 48 42 31 55 4F 37 57 4B 63 54 47 73 77 61
37 35 34 48 67 42 41 22 2C 67 65 74 45 49 3A 66 75 20 6E 63 74 69 6F 6E
28 61 29 7B 76 61 72 20 62 3B 77 68 69 6C 65 28 61 26 26 21 28 61 2E 67
65 74 41 74 74 72 69 62 75 74 65 26 26 28 62 3D 61 2E 67 65 74 41 74 74
72 69 62 75 74 65 28 22 65 69 64 22 29 29 29 29 61 3D 61 2E 70 61 72 65
20 6E 74 4E 6F 64 65 3B 72 65 74 75 72 6E 20 62 7C 7C 67 6F 6F 67 6C 65
2E 6B 45 49 7D 2C 68 74 74 70 73 3A 66 75 6E 63 74 69 6F 6E 28 29 7B 72
65 74 75 72 6E 20 77 69 6E 64 6F 77 2E 6C 6F 63 61 74 69 6F 6E 2E 70 72
6F 74 6F 63 6F 6C 3D 20 3D 22 68 74 74 70 73 3A 22 7D 2C 6B 45 58 50 49
3A 22 33 31 32 31 35 2C 33 35 37 30 32 2C 33 37 31 30 32 2C 33 38 33 37
31 2C 33 39 35 32 33 2C 33 39 39 37 37 2C 33 33 30 30 30 32 35 2C 33 33
30 30 31 31 39 2C 33 33 30 30 31 32 34 2C 33 20 33 30 30 31 33 33 2C 33
33 35 2C 33 33 30 30 31 33 37 2C 33 33 30 30 31 33 30 30 31
Table 2.21 Formula for solving the second degree equation ax2 +bx+c = 0
x = −b±
√
b2 −4ac
2a
(2.14)
Table 2.22 Latex notation of formula of Table 2.21
\begin{equation}
x = \frac{-b \pm \sqrt{bˆ2 - 4ac}}{2a}
\label{Tartaglia}
\end{equation}

102
2 Strings and Genomes
Fig. 2.59 A hyperbolic tree map of the genome of E. coli
Fig. 2.60 Another view of the hyperbolic tree map of the genome of E. coli

2.8 Informational Analysis of Genomes
103
Fig. 2.61 A sector tree map of the genome of E. coli

104
2 Strings and Genomes
Table 2.23 ASCII notation of formula of Table 2.21
92 92 98 101 103 105 110 123 101 113 117 97 116 105 111 110 125 32 120 32 61
32 92 102 114 97 99 123 45 98 32 92 112 109 32 92 115 113 114 116 123 98 94
50 32 45 32 52 97 99 125 125 123 50 97 125 32 92 108 97 98 101 108 123 84 97
114 116 97 103 108 105 97 125 92 101 110 100 123 101 113 117 97 116 105 111
110 125 32
Fig. 2.62 A sector tree map of the genome of E. coli zooming out for a portion (the marked
sector)

2.8 Informational Analysis of Genomes
105
Fig. 2.63 A circle tree map of the genome of E. coli
Fig. 2.64 A portion of the representation given in Fig. 2.63

Platonic Icosahedron
3
Algorithms and Biorhythms
Abstract. Algorithms deﬁne computation processes, while biorhythms constitute
the dynamic of life. This chapter aims at showing the links between these two con-
cepts, by giving examples where biorhythms can be described by suitable algo-
rithms. Metabolism is the starting point of the analysis we develop, but the metabolic
perspective is generalized in many directions, and a methodology is introduced for
algorithmically solving dynamical inverse problems. The chapter is mostly based
on the author’s published papers (see References for Chapter 3).
3.1
Metabolic Grammars
P systems were introduced by Gheorghe P˘aun as a computation model inspired from
the living cell [49, 50]. In 2004 a special kind of P systems, called MP systems,
were introduced [98], speciﬁcally oriented to the symbolic modeling of biological
processes [90, 106, 65, 68, 96]. This chapter is mainly devoted to present the basic
concepts of MP theory and its applications to biological modeling [107, 101, 73,
105, 109].
Metabolism is one of the basic phenomena on which life is based. Any living or-
ganism has to maintain processes which: i) introduce matter of some kinds from the
external environment, ii) transform internal matter by changing the molecule distri-
bution of a number of biochemical species (substances, metabolites), and iii) expel
outside matter that is not useful, or is dangerous for the organism. Molecule distribu-
tion identiﬁes the metabolic state of a system, and can be represented as a multiset
of type nAA + nBB + ...nZZ giving the numbers nA,nB,...,+nZ of molecules for
each of molecular species A,B,...,Z.
V. Manca: Infobiotics, ECC 3, pp. 107–168.
DOI: 10.1007/978-3-642-36223-1_3
c⃝Springer-Verlag Berlin Heidelberg 2013

108
3 Algorithms and Biorhythms
A chemical reaction such as A + B →C means that a number u of molecules of
kind A and the same number u of molecules B are replaced by u molecules of type
C. This means that this reaction is a multiset rewriting rule. The value of u is the
ﬂux of the rule application. Let us observe a system at some time steps 0,1,2,...,i,
and consider a substance x that is produced by rules r1,r3 and is consumed by rule
r2. If u1[i],u2[i],u3[i] are the ﬂuxes of the rules r1,r2,r3, respectively, in the passage
from step i to step i+ 1, then the variation of substance x is given by:
x[i+ 1]−x[i] = u1[i]−u2[i]+ u3[i].
Then, passing from one step to the next (after a ﬁxed time interval), the number
of objects consumed and produced by a reaction ri, for any single occurrence of
reactant/product of ri, is the reaction ﬂux ui, which depends on the state of the
system by means of a map ϕi called the regulator of ri. In this sense, a metabolic
system is a dynamical system, because the quantities of its substances change along
the time. In general, a time series (X[i] | i ∈N) is a sequence of real values intended
as “equally spaced” in time. In the following we will study metabolic systems in
a discrete mathematical perspective, by introducing Metabolic P grammars, or
brieﬂy MP grammars. These grammars are related to P systems introduced in
1998 by Gheorghe P˘aun [49, 50], and are a special kind of multiset processing
grammars [98, 92, 93, 94, 95, 97].
Deﬁnition 3.1 (MP grammar).
An MP grammar G is a generator of time series,
determined by the following structure (n,m ∈N, the set of natural numbers):
G = (M,R,I,Φ)
where:
1. M = {x1,x2,...xn} is a ﬁnite set of elements called metabolites or substances.
A metabolic state is given by a list of n values, each of which is associated to a
metabolite;
2. R = {αj →β j | j = 1,...m} is a set of rules, or reactions, with αj and β j mul-
tisets over M for j = 1,...m;
3. I are initial values of metabolites, that is, a list x1[0],x2[0],...xn[0] providing the
metabolic state at step 0;
4. Φ = {ϕ1,...,ϕm} is a list of functions, called regulators, one for each rule, such
that, for 1 ≤j ≤m:
ϕj : Rn →R.
(3.1)
G deﬁnes, for any x ∈M, a time series:
(x[i] | i ∈N,i > 0)
(3.2)
in the following way. Let
s[i] = (x[i] | x ∈M)
(3.3)

3.1 Metabolic Grammars
109
the state vector of G at step i, which can be seen as a function from the set of
metabolites to R, then the ﬂux u j[i] of rule rj at step i, is given by applying the
regulator ϕj to the state s[i] :
u j[i] = ϕj(s[i]).
(3.4)
For any i ∈N, the value of x[i+ 1], for each x ∈M is given by the following equa-
tion, where αj(x) and β j(x) denote the multiplicities of x in the multiset αj and β j,
respectively:
x[i+ 1] = x[i]+
m
∑
j=1
(β j(x)−αj(x))u j[i].
(3.5)
An MP graph is a natural graphical representation of an MP grammar G (originated
from [91]). An MP grammar G becomes an MP system when values for time factor
τ, population factor ν, and mass factor μ are added to G (however, very often MP
grammar and MP system are used synonymously):
• τ ∈R is the time interval between two consecutive steps;
• ν ∈R is the number of molecules which represents a (conventional) mole in the
model;
• μ ∈Rn is the vector of the mole masses of metabolites.
An MP grammar G is parametric, when a set P of parameters is added to G, and
metabolic states include also elements of P (to which, the state assigns real values).
If G is parametric, the time series of parameters also has to be provided in order to
specify G.
Table 3.1 gives an example of MP grammar, where /0 denotes an empty multiset
(/0-rules correspond to introduction or expulsion of matter) and substance symbols
occurring in regulators denote the corresponding substance quantities. The quan-
tities that occur as arguments of a regulator (substances or parameters) are called
the tuners of that regulator. In Table 3.1 p is a parameter, because it occurs as
a tuner, but does not occur in any reaction. In the following, we are focused es-
sentially on rules and regulators, then, often, initial values and parameters (if they
are present) are given separately. Moreover, X[i] will also be used for denoting the
vector of metabolite values at step i (in the order they are speciﬁed), while s[i] is
the state vector extending X[i] with the values of parameters at step i (if they are
present). For the sake of simpliﬁcation, we consider equivalent expressions such as:
ϕ(x4[i],x3[i],x1[i]) or ϕ(x4,x3,x1)[i], or ϕ(s[i]).
Three aspects are essential in the notion of MP grammar: multiset rewriting,
time discreteness, and molar perspective. In fact:
1. rules transform multisets. We remark that an implicit assumption of Eqs. (3.5) in
Deﬁnition 3.1 is that all the rules can be applied. This is a reasonable hypothesis if
ﬂuxes have values smaller than the quantities of available substances. If this is not
the case, further strategies need to be speciﬁed which establish how to proceed
when the hypothesis of metabolite abundance does not apply. For example, if
the rules consume a metabolite quantity which is not available, then all the rules
consuming it could be blocked at that step (or some of them according to some
speciﬁed criterion);

110
3 Algorithms and Biorhythms
Table 3.1 An MP grammar corresponding to the MP graph of Fig. 3.1
r1 : /0 →A
ϕ1 = 0.05p
r2 : A →B
ϕ2 = 0.2C
r3 : B →/0
ϕ3 = 0.1
r4 : A →C
ϕ4 = 0.6A/C
r5 : C →/0
ϕ5 = 0.4
A[0] = B[0] = C[0] = 1
p[0] = 0.2, p[i+1] = p[i]+0.2
Fig. 3.1 The MP graph corresponding to the MP grammar of Table 3.1
2. ﬂuxes refer to the substances consumed and produced between two consecutive
steps at a ﬁxed time interval with respect to which time is measured in the spe-
ciﬁc case of interest;
3. ﬂuxes are expressed with respect a conventional mole, that is, a population size,
depending on the particular phenomenon in question.
Any rule r of an MP grammar G determines two column vectors: the left vector
r−and the right vector r+. The ﬁrst vector provides the multiplicity of substances
occurring in the multiset of reactants, while the second vector provides the multi-
plicities of the substances occurring in the multiset of products. The stoichiometric
balance r# of the rule r is deﬁned as r+ −r−(the difference of the two vectors).
The matrix consisting of the column vectors r+ −r−(in a conventional order of
rules) is called the stoichiometric matrix of G. For example, the rules of MP gram-
mar of Table 3.1 could be given by Table 3.2 reporting the pair of vectors and the
corresponding stoichiometric balances.
Now, let us suppose we start from a state of a system where substances A,B,C
have a 1-mole of elements (we do not enter into the speciﬁc value of our mole). Let
us denote by A[i],B[i],C[i] the molar values of these substances at step i. Then, the
ﬂuxes of our system are given, according to Table 3.1, by Table 3.3.

3.1 Metabolic Grammars
111
Table 3.2 Rule vectors of the MP grammar of Table 3.1
r1 =
⎛
⎝
⎛
⎝
0
0
0
⎞
⎠,
⎛
⎝
1
0
0
⎞
⎠
⎞
⎠⇒r#
1 =
⎛
⎝
1
0
0
⎞
⎠
r2 =
⎛
⎝
⎛
⎝
1
0
0
⎞
⎠,
⎛
⎝
0
1
0
⎞
⎠
⎞
⎠⇒r#
2 =
⎛
⎝
−1
1
0
⎞
⎠
r3 =
⎛
⎝
⎛
⎝
0
1
0
⎞
⎠,
⎛
⎝
0
0
0
⎞
⎠
⎞
⎠⇒r#
3 =
⎛
⎝
0
−1
0
⎞
⎠
r4 =
⎛
⎝
⎛
⎝
1
0
0
⎞
⎠,
⎛
⎝
0
0
1
⎞
⎠
⎞
⎠⇒r#
4 =
⎛
⎝
−1
0
1
⎞
⎠
r5 =
⎛
⎝
⎛
⎝
0
0
1
⎞
⎠,
⎛
⎝
0
0
0
⎞
⎠
⎞
⎠⇒r#
5 =
⎛
⎝
0
0
−1
⎞
⎠
Table 3.3 The ﬂuxes of MP rules of Table 3.1
u1[i] : ϕ1(s[i]) = 0.05p[i]
u2[i] : ϕ2(s[i]) = 0.2C[i]
u3[i] : ϕ3(s[i]) = 0.1
u4[i] : ϕ4(s[i]) = 0.6A[i]/C[i]
u5[i] : ϕ5(s[i]) = 0.4
This means that if U[0] denotes the (column) vector of ﬂuxes at time 0, then the
corresponding row vector of ﬂuxes at time 0 is given by:
(u1[0],u2[0],u3[0],u4[0],u5[0]) = (0.01,0.2,0.1,0.6,0.4).
(3.6)
In this notational setting it is easy to realize that the substance variation vector
(A[1]−A[0],B[1]−B[0],C[1]−C[0])
is given by the following matrix (row by column) product:
U[0] = (r#
1 r#
2 r#
3 r#
4 r#
5)×U[0]
that is:
⎛
⎝
1 −1
0 −1
0
0
1 −1
0
0
0
0
0
1 −1
⎞
⎠×
⎛
⎜
⎜
⎜
⎜
⎝
u1[0]
u2[0]
u3[0]
u4[0]
u5[0]
⎞
⎟
⎟
⎟
⎟
⎠
.

112
3 Algorithms and Biorhythms
Table 3.4 Substance variations, in the MP grammar of 3.1, in passing from step 0 to step 1
ΔA[0] = A[1]−A[0] = u1[0]−u2[0]−u4[0] = 0.01−0.2−0.6 = −0.79
ΔB[0] = B[1]−B[0] = u2[0]−u3[0] = 0.2−0.1 = 0.1
ΔC[0] = C[1]−C[0] = u4[0]−u5[0] = 0.6−0.4 = 0.2
In fact, according to Eq. (3.6), we can compute the substance variations as reported
in Table 3.4.
In this manner we compute the substance vector at time 1:
A[1] = A[0]+ ΔA[0] = 1.0 −0.79 = 0.21
B[1] = B[0]+ ΔB[0] = 1.0 + 0.1 = 1.1
C[1] = C[0]+ ΔC[0] = 1.0 + 0.2 = 1.2
and, by applying the same method, we get the value of this vector in all the fol-
lowing steps. In other words, the metabolic grammar provides the time series of its
metabolites. If Δ[i] is the vector of substance variation at step i:
Δ[i] = (Δx[i] | x ∈M)
(3.7)
then, for any metabolite x:
x[i+ 1] = x[i]+ Δx[i]
(3.8)
therefore, the dynamics generated by a metabolic grammar with n substances and
m reactions of regulators
ϕ1,ϕ2,...,ϕm
and stoichiometric matrix
A = (r#
1 r#
2 ... r#
m),
(3.9)
is completely expressed by the following Equational Metabolic Algorithm:
Δ[i] = A×U[i]
(3.10)
where:
U[i] =
⎛
⎜
⎜
⎝
ϕ1(s[i])
ϕ2(s[i])
.........
ϕm(s[i])
⎞
⎟
⎟
⎠.
(3.11)
In conclusion, an MP grammar G can be completely represented in four ways:
1. in textual format, by providing its reactions with regulators and its initial values
(see, for example, Fig. 3.1);
2. by its corresponding MP graph (see, for example, Fig. 3.1);

3.1 Metabolic Grammars
113
Fig. 3.2 The MP graph of MP grammar of Table 3.5
3. by the set of Eqs. (3.5) of Deﬁnition 3.1;
4. by the vector equation EMA(see 3.10).
The MP grammar given in Table 3.5 has the dynamics given in Fig. 3.3. It deﬁnes a
synthetic oscillator, very often considered in the MP theory, called Sirius (see [92]).
The oscillator is made of three substances A, B, and C and ﬁve reactions.
Table 3.5 The MP grammar of Sirius oscillator, corresponding to the MP graph of Fig. 3.2.
Its dynamics with initial values A[0] = 100, B[0] = 100, C[0] = 0.02, is given in Fig. 3.3.
r1 : /0 →A
ϕ1 = 0.047+0.087A
r2 : A →B
ϕ2 = 0.002A+0.0002AC
r3 : A →C
ϕ3 = 0.002A+0.0002AB
r4 : B →/0
ϕ4 = 0.4B
r5 : C →/0
ϕ5 = 0.4C
Table 3.6 The stoichiometric matrix of MP grammar of Table 3.5
A =
⎛
⎝
1 −1 −1
0
0
0
1
0 −1
0
0
0
1
0 −1
⎞
⎠
Table 3.7 reports a useful notation concerning MP rules which reveals an intrin-
sic duality between substances and reactions of an MP grammar. This notation can
be easily extended to a set of substances and to a set of reactions. The closure prop-
erty of a reaction graph of substances X and reactions Y can be expressed by the
condition Ro(So(Y)) = Y and So(Ro(X)) = X.

114
3 Algorithms and Biorhythms
Fig. 3.3 The Sirius dynamics
Table 3.7 The duality between substances and reactions
Notation
Description
R−(x)
reactions consuming substance x
R+(x)
reactions producing substance x
Ro(x)
reactions consuming or producing substance x
S−(r)
substances consumed by reaction r
S+(r)
substances produced by reaction r
So(r)
substances consumed or produced by reaction r
3.1.1
Lotka-Volterra Dynamics
The Lotka-Volterra model is the simplest model of predator-prey interactions. The
model was developed independently by Lotka ([89] and Volterra [116] and pertains
to a population of individuals of two species: a prey species x and a predator species
y. It was observed that, in time, the numbers of individuals of the two types oscil-
late according to a regular pattern. The reason for this oscillating dynamics can be
qualitatively explained by considering that predators need prey for increasing their
number but, at the same time, if the number of predators is too big, then prey can-
not satisfy their sustainment, therefore predators decrease, and consequently prey
can survive more easily so that prey population increases. The differential model of
prey-predatordynamics is described by the following equations, where A,B,C,D are
interaction parameters related to the average mortality and reproduction factors of
the two species, and to the predator voracity, that is, the average percentage of prey
eaten by predators in a time unit. In this way, under suitable simpliﬁcations (prey
consumption is due only to predation, no other factors inﬂuence the dynamics, and
these inﬂuences are almost stable within the considered ranges of the population
sizes), the following equations are the invariant of the considered dynamics:

3.1 Metabolic Grammars
115
dx
dt = (A−By)x
(3.12)
dy
dt = (Cx−D)y.
The solution of these equations, starting from suitable initial values of the two pop-
ulations exhibits a typical oscillation of both population sizes.
Now we show an MP analysis of the same phenomenon which provides the same
conclusion. A prey is replicated, by means of a rule x →2x, in proportion to the
number of prey (their average proliferation, and the availability of resources from
the environment determine the proportionality factor). The rate of predator repli-
cation and prey death (rules r2 and r3) depends on both the number of prey and
predators. Finally, predator death is proportional to predator population size (rule
r4). We do not discuss here the method for determining these parameters, which
will be explained in the following sections, however, the MP grammar of Table 3.8
provides a typical Lotka-Volterra dynamics which is depicted on the left part of
Fig. 3.4.
Table 3.8 MP grammar of Lotka-Volterra dynamics
Reactions
Regulators
r1 : x →2x ϕ1 = 0.03x
r2 : x →/0
ϕ2 = 0.0009+0.009xy+0.0001x2y
r3 : y →2y ϕ3 = 0.0009+0.015xy+0.0003x2y
r4 : y →/0
ϕ4 = 0.066y
We recall that, in accordance with our general notation, in Table 3.8, symbols
x,y are used with different meanings. In fact, when they are used in MP-rules, they
represent metabolites, while when they are arguments of regulators, they denote
metabolite quantities in the given metabolic state.
3.1.2
The Brusselator (Belousov-Zhabotinsky Reaction)
Brusselator is an idealization, due to Prigogine’s school [112], of a famous kind
of chemical oscillating reaction, called BZ reactions, discovered by Belousov and
then analyzed by Zhabotinsky. The best-known BZ reaction can be created with a
mixture of potassium bromate KBrO3, malonic acid CH2(COOH)2, and manganese
sulfate MnSO4 prepared in a heated solution of sulfuric acid H2SO4. Brusselator has
the following form:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
a →x
2x+ y →3x
b + x →y+ d
x →e.

116
3 Algorithms and Biorhythms
Fig. 3.4 On the left: observed prey and predator populations with x[0] = 11 and y[0] = 6. The
approximated values are plotted against the ones calculated by the differential model given
in Eq. (3.12) with A = 0.3, B = 0.1, C = 0.18 and D = 0.7. On the right: the Brusselator
dynamics with x[0] = y[0] = 100.
We can further simplify these reactions by focusing on substances x,y, and by con-
sidering a,b,d,e as input/output substances providing the insertion or expulsion of
x,y from/to the environment. Table 3.9 is an MP grammar of BZ reaction. In fact,
its MP dynamics given on the right of Fig. 3.4 exhibits the classical form of Pri-
gogine’s Brusselator differential model. In this form a strict analogy of BZ with
Lotka-Volterra dynamics can be envisaged, with the difference that a kind of y-to-
x transformation and of x-to-y transformation is realized here by rules r2 and r3
respectively.
Table 3.9 MP grammar of Brusselator dynamics
Reactions
Regulators
r1 : /0 →x
ϕ1 = 1
r2 : 2x+y →3x ϕ2 = 10−6x2y
r3 : x →y
ϕ3 = 0.03x
r4 : x →/0
ϕ4 = 0.01x
3.2
Time Series and Inverse Dynamics
Dynamical system is a pervasive concept in mathematics and in all sciences. A real
number, seen as a process of generation of digits is a particular case of dynam-
ical system. Analogously, a computation which starts from some initial data and
transforms them along some elaboration steps determines a dynamical system. We
ﬁnd other examples of dynamical systems in a planet moving around a star, in the

3.2 Time Series and Inverse Dynamics
117
internal interactions of the particles inside an atom, in a chemical process, in a cy-
clone, in an economic process of resource exchange, in the course of an epidemic,
or in the development of an organism.
The Greek philosopher Eracyitus says, “[...] Panta rei” (π `αντα ρ `ει), that is,
“everything is changing” or also “existence is change”. But if change follows a rule,
something has not to change. In fact, in order to localize and describe a process
as an entity, something has to remain stable during it. Existence is a mysterious
mixing of variation and invariance underlying objects and events, at each level of
reality. A dynamical system is a structure hosting a dynamics. The term dynamics
points to the process while the term system points to the structure. A river, a city,
a living organism, are always different in time, nevertheless, their individualities
persist unchanged during their lives.
Since the epochal discovery of the laws of planetary motions, dynamical systems
have been mathematically studied by means of differential equations. Recently the
study of complex systems arising from life sciences, economics, meteorology, and
many other ﬁelds, requires novel ideas and different approaches for the analysis and
modeling of a wider class of dynamical systems, and for a great variety of aspects
concerning the dichotomy structure/behavior. In particular, discrete dynamical sys-
tems seem to open new modeling possibilities and pose new problems where the
algorithmic aspects replace the geometrical and differential perspective of classical
dynamics.
In 1684 the astronomer Edmund Halley traveled to Cambridge to ask Newton
about planetary motions. It was known that planets move around the Sun in ellipses.
The question Halley posed to Newton was about the reason for this elliptic shape. Sir
Isaac replied immediately: “I have calculated it”. The general rigorous proof of this
phenomenon was deﬁnitively given by Euler in 1749 (elliptic shape is a consequence
of the inverse-square law and of the fundamental law of dynamics) [165].
Halley’s question is a ﬁrst example of the dynamical inverse problem, that is,
given an observed behavior, is there a mathematical system that can explain this
behavior? This is a crucial issue, in fact, when we can answer this question, then we
“dominate” the system because we can predict or alter its behavior.
Any system changing in time can be identiﬁed with some variables. As they de-
ﬁne a system, they must verify some relations which are the invariant of the behav-
ior. Newton’s solution to Halley’s question was obtained ﬁrst by determining some
invariants of the planetary orbits, speciﬁed in terms of differential equations, then,
by solving these equations, the mathematical form of these orbits was deduced. In
general, in order to deﬁne a mathematical system related to a process, we need to de-
termine its variables and invariants. This schema of analysis for dynamical systems
is very general and continues to hold in different formal frameworks of dynamical
representations. To suggest the wide range of this paradigm, let us consider the situ-
ation of a discrete phenomenon. Suppose we observe a device generating as output
words (in a particular alphabet). A very natural question is: which is the grammar
of the language generated by this device? And, in which manner does its internal
structure realize the generation of these words? If we assume that any event can
be discretely represented by a suitable word, then the search for rules underlying

118
3 Algorithms and Biorhythms
this process corresponds, in principle, to the search for a grammar. In the case of
metabolic processes, we show that what is usually expressed in terms of differen-
tial equations can be formulated in terms of a special class of grammars, and very
often these grammars are directly related to the biochemical mechanisms of the
phenomenon under investigation.
Let us consider a biochemical system. Its state can be expressed by means of a
multiset built on the set of its chemical species (molecule types). But we know that
a chemical reaction is a rule transforming multisets of molecules into other multi-
sets of molecules. This viewpoint implies that the whole dynamics of a biochemi-
cal system, where some reactions are active, corresponds to a grammar of multiset
rewriting rules. Moreover, in order to provide a quantitative description of the ef-
fects produced by the reactions, these rules have to specify the amount of molecules
they transform. If the evolution of a system is considered along a sequence of dis-
crete steps, then a natural manner for expressing quantitative effects of rules can
be expressed by equipping rules with functions (depending on the state of the sys-
tem) which establish the ﬂuxes, that is the quantities of substances transformed in
any evolution step. This perspective of considering biochemical systems explains
the central role of grammars in the description of biochemical dynamics, and con-
sequently, the dynamical inverse problem for a biochemical system reduces to the
search for suitable multiset grammars generating the sequence of states which we
observe through time in the given system.
Given an MP system, the recurrent equation 3.10 of EMA generates the evolution
of substances, according to the MP grammar of the system (starting from an initial
metabolic state, and with the knowledge of parameter evolution in time). In other
words, when regulators are given, the substance variations follow easily from the set
of reactions deﬁned in the system. The ﬂux maps express the logic of an MP system
in terms of its internal states.
Let us assume we “observe” a metabolic system for a number of steps (separated
by some temporal intervals). The observation is represented by one time series for
each observed substance, that is, the sequence of quantities of the substance in cor-
respondence to the observation steps.
How can we discover an MP system which provides the dynamics which we
observe? The substances and reactions in question are given by the particular phe-
nomenon we want to describe. Moreover, the stoichiometry can be deduced by a
basic knowledge of the phenomenon, but how do we know the ﬂuxes of matter
transformation that are responsible for the observed evolution? This is the prob-
lem of discovering the ﬂux regulation maps, or simply, the Regulation Discovery
Problem. Its solution can be described as the passage from the observation of the
behavior which a system exhibits in time to the determination of an internal
logic that provides rules of transformation of its states. In a metabolic system,
an internal state is naturally representable by a multiset, therefore, we could synthe-
size the way of solving the dynamical inverse problem, for metabolic systems, by
means of the following statement.

3.3 Metabolic Approximation
119
From time series to (multiset) grammars generating them.
A synthetic way to characterize the approach displayed above is Gramma-
dynamics, because the solution to the a dynamical inverse problem is reduced to
the solution of a regulation discovery problem, and then to the search of an MP
grammar able to generate the observed time series. The methods that we develop,
along this research line, are mainly based on algorithmic, algebraic, and statistical
concepts, thus the more general term Infodynamics may be more appropriate.
3.3
Metabolic Approximation
Before introducing the algorithm that solves the regulation discovery problem in the
context of MP systems, we present a procedure which discovers an MP grammar
“approximating” some given time series exhibiting periodical behaviors [100].
Let us consider a real n-dimensional function f : R →Rn. We pose the follow-
ing approximation problem. Let f[x0], f[x1], f[x2],..., f[xt] be the time series of f
along a uniformly distributed sequence of values x0,x1,x2,...,xt. We are interested
in ﬁnding an MP system such that, for some real vector K, and for some real vectors
ε[i] with 0 ≤ε[i] ≤ε and 0 ≤i ≤t, the following equation holds (X is the vector of
substance quantities of the system):
f[xi]+ K = X[i]± ε[i]
(3.13)
where the vector K provides shift constants for avoiding negative values, and vector
ε is the error tolerated by the approximation.
In other words, we want to design an MP system exhibiting a dynamics that,
within an ε-approximation,coincides with the time series f[x0], f[x1], f[x2],..., f[xt]
sampled from f. Another way to state synthetically this task could be: providing an
MP grammar approximating a real function.
The previous formulation of the problem is too general, because no conditions
are required to the structure of the MP system we are searching for. Of course, a
natural indication could be the simplicity of the MP system. This means that the
more the MP system is “parsimonious”, the better our solution is. The parsimony
of an MP system is given by two features: i) the reaction stoichiometry and ii) the
form of its regulators. If we choose a class of regulation functions, we should search
a minimal set of reactions providing the required behavior.
Now we show that there is a simple way, directly related to the deﬁnition of
MP system, to approach our problem by using a mathematical regression method
based on the solution of a linear least-squares problem [225]. The method of least-
squares was ﬁrst described by Carl Friedrich Gauss around 1794 and it is the stan-
dard approach to approximate solutions of overdetermined systems, that is, sets of

120
3 Algorithms and Biorhythms
equations in which there are more equations than unknowns. “Least-squares” means
that the overall solution minimizes the sum of the squares of the errors made in
solving the equation (see Sect. 7.7 for a mathematical explanation of the method).
Let us explain the procedure of metabolic approximation, in the particular case
of a function from R to R2 providing the time series (A[i],B[i]) with i = 0,...,t, by
means of the following MP grammar (/0 is the empty multiset) in which the values
of c1, c2, ..., c6 are unknowns:
/0 →A
ϕ1(A,B) = c1A+ c2AB
2A →B
ϕ2(B) = c3B+ c4
B →/0
ϕ3(A) = c5B+ c6.
The stoichiometric matrix of the system is
A =

1 −2
0
0
1 −1

,
therefore the approximation system of equations will be, for i = 0,1,...,t −1,
ΔA[i] = A[i+ 1]−A[i] = c1A[i]+ c2A[i]B[i]−2(c3B[i]+ c4)
ΔB[i] = B[i+ 1]−B[i] = c3B[i]+ c4 −(c5B[i]+ c6).
(3.14)
If we call M the matrix of coefﬁcients presented on the right-hand side of Eq. (3.14),
the least-squares approximation of the coefﬁcient vector (c1,c2,c3,c4,c5,c6)T , ex-
pressed as a transposed row (exponent T denotes matrix transposition), for which
our MP system exhibits a dynamics close to the time series we started from, is given
by (see Sect. 7.7 and [225]):
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c1
c2
c3
c4
c5
c6
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=

MT × M
−1 × MT ×
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ΔA[0]
ΔB[0]
ΔA[1]
ΔB[1]
......
ΔA[i]
ΔB[i]
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
assuming that M has maximum rank1 (i.e., equal to the number of its columns). At
this point we can apply EMA to the MP system which uses c1, c2,..., c6 as values
for c1,c2,...,c6. If the system provides a dynamics close enough to the time se-
ries (A[i],B[i]) with i = 0,...,t, then the method stops. If the approximation error
is already too big, the method tries to modify the initial MP grammar by intro-
ducing some rules deduced by a suitable correlation analysis of the approximation
error.
1 If M does not have a maximum rank, then the matrix given by the product MT ×M is not
invertible.

3.3 Metabolic Approximation
121
3.3.1
Goniometricus: A Metabolic Grammar of Sine and Cosine
In this section we will outline a method to develop MP approximations of sine and
cosine functions. This will be done by deﬁning an MP model, called Goniometri-
cus, which has two substances, S and C, approximating in time the evolution of the
functions sine and cosine respectively (apart from a constant offset).
We start from two time series of the same length, related to the two target func-
tions sin and cos along two oscillations (from 0 to 4π) with a step of 10−3. As
depicted in Fig. 3.5, each value has been increased by 3 units to make all values
positive:
cos(0)+ 3 , cos(0.001)+ 3 , cos(0.002)+ 3 , ... cos(4π)+ 3
sin(0)+ 3 , sin(0.001)+ 3 , sin(0.002)+ 3 , ... , sin(4π)+ 3.
Fig. 3.5 The plot of the time series TC and TS
When we start with angle 0, then cosine is 1 and sine is 0. Going from 0 to π/2 co-
sine decreases and sine increases. Therefore, we can assume a transformation from
cosine to sine. Moreover, initially the sum of sine and cosine is 1, but, when angle
π/4 is reached, the sum of sine and cosine values is
√
2, which is greater than 1. This
means that rules of increase and decrease of “matter” in the system are appropriate.
This intuition suggests the MP grammar given in Table 3.10, and depicted by the MP
graph of Fig. 3.6, which allows us to ﬁnd, by means of Least Square Evaluation, the
right constants providing a good approximation of the required dynamics.
Table 3.10 A form for Goniometricus’s MP grammar
r1 : /0 →C
ϕ1 = c1 +c2 C
r2 : C →S
ϕ2 = c3 S+c4 C
r3 : S →/0
ϕ3 = c5 +c6 S

122
3 Algorithms and Biorhythms
Fig. 3.6 Goniometricus’s MP graph related to the MP grammar given in Table 3.10
The least square evaluation of the following system of equations with unknowns
c1,...c6 will provide the regulators of the grammar we are searching for:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
c1 + c2 C[0]−c3 S[0]−c4 C[0]
= ΔC[0]
c3 S[0]+ c4 C[0]−c5 −c6 S[0]
= ΔS[0]
.......................................
= ......
c1 + c2 C[n]−c3 S[0]−c4 C[n]
= ΔC[n]
c3 S[n]+ c4 C[n]−c5 −c6 S[0]
= ΔS[n].
(3.15)
The least square evaluation of system (3.15) provides sine and cosine with an ap-
proximation error of 10−14. The initial values of C and S are 4,3 respectively (for
keeping ﬂuxes positive), τ = 10−3 × 2π (the period is 2π), and ν = μ = 1. The
values of the constants are reported in Table 3.11.
Table 3.11 Goniometricus’s MP grammar. Parameters C −3 and S −3 shift the values of C
and S in the interval [−1,1].
r1 : /0 →C
ϕ1 = 0.0030015+0.0009995 C
r2 : C →S
ϕ2 = 0.001 C +0.001 S
r3 : S →/0
ϕ3 = 0.0029985+0.0010005 S
3.3.2
Generalization of the Goniometricus Model
The approximation power of MP grammars may increase by considering “memo-
ries”. Given a substance A, we denote by A−m (m ∈N) the memory of A of level m.
It is a substance obtained from A such that A−m[i] = A[i −m]. Therefore its time
series TA−m is obtained from the time series TA = [tA,1,tA,2,...,tA,n] of A by the

3.4 Stoichiometric Expansion and Stepwise Regression
123
following shifting operation: TA−m = [0,...,0,tA,1,...,tA,n−m]. Usually, two memory
levels are enough for deﬁning MP approximations of very complex functions. MP
grammars M1 and M2 [100], having the form given in Table 3.12, which approximate
(f1(x),g1(x)) = (cos(x)3,sin(x)3) and (f2(x),g2(x)) = (cos(0.99x),sin(0.99x))
respectively (initial values 3,2, shift of each substance 2, τ = 10−3 × 2π and
ν = μ = 1).
M1 provides approximation order 10−6 and uses two memory levels with:
c1 = 0.0133055979719, c2,0 = 38.2585289672269, c2,1 = −76.6872395145061,
c2,2 = 38.4397714886722, c3 = 0.0053099516581, c4,0 = 35.6377416209758,
c4,1 = −70.8226212812310, c4,2 = 35.1955107358582, c5,0 = 14.5027497660384,
c5,1 = −29.4732582998089, c5,2 = 14.9749362227182, c6,0 = 57.3945207457749,
c6,1 = −115.0336102507111, c6,2 = 57.6568032453146.
M2 does not use memories, and provides approximation order 10−14 with:
c1 =0.000196018399012, c2,0 =0.009850833684540, c3=−0.019701667369079,
c4,0=0.009899838284292, c5,0=0.009899838284293, c6,0=0.009948842884046.
Table 3.12 The Goniometricus MP grammar generalized with two memory levels
r1 : /0 →A
ϕ1 = c1 +c2,0 A+c2,1 A−1 +c2,2 A−2
r2 : A →B
ϕ2 = c3 +c4,0 B+c4,1 B−1 +c4,2 B−2+
c5,0 A+c5,1 A−1 +c5,2 A−2
r3 : B →/0
ϕ3 = c6,0 B+c6,1 B−1 +c6,2 B−2
3.4
Stoichiometric Expansion and Stepwise Regression
The method which we presented in the previous section, for approximating real
functions, is based on grammars with very simple rules. In this case, we assumed
regulators having linear forms. In general, an MP grammar generating a given dy-
namics can be very complex, since we cannot a priori restrict the forms of reg-
ulators. For this reason we need to extend our method for solving the Regulation
Discovery Problem within MP grammars.
Let us suppose we know some time series of states2, that is, the vector sequence
(s[i]|i ∈N),
2 Very often the time series from which the inverse dynamical problem starts are not at
regular time intervals. In this case a preprocessing phase is appropriate for determining an
interpolation curve ﬁtting the observed values along the observation points. After this, we
will consider the time series given by the values of the interpolation curve at steps with the
same time interval.

124
3 Algorithms and Biorhythms
then we can read the equation EMA by reversing the known values with the un-
known ones. In fact, by writing the substance variation vector:
s[i+ 1]−s[i] = Δ[i]
and, assuming n substances and m reactions, we get the following system, which we
call ADA (Avogadro-Dalton-Action, see [94]), for remarking that here, differently
from EMA of Eqs. (3.5), the unknown values are ﬂuxes U[i]:
A×U[i] = Δ[i].
(3.16)
For the determination of the regulators which provide the best approximate solution
of Eq. (3.16), we apply a procedure which we call stoichiometric expansion (see
[102, 103, 104]).
Given a positive integer d, let us assume that the regulators we are searching for
can be expressed as linear combinations of some basic regressors
g1,g2,...,gd
which usually include constants, powers, and products of substances, plus some ba-
sic functions which are considered suitable in the speciﬁc cases under investigation:
ϕ1 = c1,1g1 + c1,2g2 + ...+ c1,dgd
(3.17)
ϕ2 = c2,1g1 + c2,2g2 + ...+ c2,dgd
... = ............
ϕm = cm,1g1 + cm,2g2 + ...+ cm,dgd.
Equation (3.17) can be written, in matrix notation, in the following way, where U[i]
is the column vector of regulators evaluated at state si, G[i] the column vector of
regressors evaluated at the same state, and CT is the matrix m × d of the unknown
coefﬁcients of regressors (exponent T denotes matrix transposition):
U[i] = CT × G[i].
(3.18)
Substituting the right member of Eq. (3.18) into Eq. (3.16), we obtain the following
system of equations (A is the stoichiometric matrix):
A× CT × G[i] = Δ[i].
(3.19)
Now, if we consider t systems of type (3.19), for 1 ≤i ≤t, and if n is the number
of variables, we obtain nt equations with md unknown coefﬁcients of C. If nt > md
and the system has maximum rank, then we can apply a Least Square Evaluation
which provides the coefﬁcients that minimize the errors between the left and right
sides of the equations. These coefﬁcients provide the regulator representations that
we are searching for.
Now we provide a compact representation of the unknown coefﬁcients con-
stituting the matrix C, by using suitable matrix operations. Let us consider the

3.4 Stoichiometric Expansion and Stepwise Regression
125
t-expansions φt
1,φt
2,...,φt
m of regulators as the vectors constituted by the right
members of Eqs. (3.17) evaluated along t steps (where the values of all the vari-
ables of the system are known), and analogously the t-expansions Gt
1,Gt
2,...,Gt
d
of the regressors g1,g2,...,gd along the same t steps. With this notation Eqs. 3.17
provide a linear system with d × m unknowns:
φt
1 = c1,1Gt
1 + c1,2Gt
2 + ...+ c1,dGt
d
(3.20)
φt
2 = c2,1Gt
1 + c2,2Gt
2 + ...+ c2,dGt
d
... = ............
φt
m = cm,1Gt
1 + cm,2Gt
2 + ...+ cm,dGt
d.
Now, let Cd
1,Cd
2,...,Cd
m be the unknown column vectors of dimension d constituted
by the coefﬁcients of the regressors providing the linear combinations of regulators
ϕ1,ϕ2,...,ϕm we are searching for, and
C = (Cd
1,Cd
2,...,Cd
m)
the matrix having these vectors as columns.
Let also F be the following matrix constituted by m column vectors of t elements:
F = (φt
1,φt
2,...,φt
m).
Finally, let
G = (Gt
1,Gt
2,...,Gt
d)
be the matrix of dimension t × d having as columns the t-expansions of regressors.
With the notation above Eq. (3.20) becomes:
G × C = F
(3.21)
Moreover, let Δt
1,Δt
2,...,Δt
n be the column vectors of dimension t constituted by
substance variations of substances, from step i to step i+ 1, for 0 ≤i ≤t, and:
D = (Δt
1,Δt
2,...,Δt
n)
the matrix having these vectors as columns.
By using matrix transposition (denoted by the exponent T), Eq. (3.16) A×U[i] =
Δ[i] becomes:
U[i]T × AT = Δ[i]T = (Δ1[i],Δ2[i],...,Δn[i])
which, expanded along the t time points, provides:
F× AT = D
(3.22)

126
3 Algorithms and Biorhythms
Therefore, by combining Eqs. (3.21) and (3.22), it follows that:
G × C× AT = D.
(3.23)
We show that regressor coefﬁcients of C can be obtained by a least square estimation
deduced by Eq. (3.23), by using the direct product ⊗between matrices, also called
the Kronecker product, a special case of tensor product used in linear algebra and
in mathematical physics.
Given two real matrices A,B of dimension n×m and t ×d respectively, the direct
product:
A⊗B
is the matrix of dimension nt × md, constituted by nm blocks (A ⊗B)i,j, such that,
if A = (ai,j | 1 ≤i ≤n, 1 ≤j ≤m), then (A ⊗B)i,j = ai,jB (all the elements of B
are multiplied by ai,j). If we use the block notation, A⊗B can be represented in the
following way:
⎛
⎜
⎜
⎝
a1,1B a1,11B ... a1,mB
a2,1B a2,2B ... a2,mnB
...
...
...
...
an,1B an,2B ... an,mB
⎞
⎟
⎟
⎠.
(3.24)
The Kronecker product is bilinear and associative, that is, it satisﬁes the following
equations:
A⊗(B+C) = (A⊗B)+ (A⊗C)
(A+ B)⊗C = (A⊗B)+ (A⊗C)
(kA)⊗B = A⊗(kB) = k(A⊗B)
(A⊗B)⊗C = A⊗(B⊗C).
Moreover, matrix direct product veriﬁes the following equations (the last equation
when matrices are invertible):
(A⊗B)× (C⊗D) = (A×C)⊗(B× D)
(A⊗B)T = AT ⊗BT
(A⊗B)−1 = A−1 ⊗B−1.
The following Lemma asserts a useful property of the Kronecker product [87].
Lemma 3.2 (Vectorization Lemma). Let us denote by vec(W) the vectorization of
a matrix W, obtained by concatenating all the columns of W, in their order, in a
unique column vector. Then the following equation holds:
vec(A× X × B) = (BT ⊗A)× vec(X).
(3.25)

3.4 Stoichiometric Expansion and Stepwise Regression
127
Proof. Let A be a matrix n×h, X a matrix h×k, and B a matrix k×m. Let us denote
by Ai the i-row vectors of a matrix A and by X j the j-column vectors of X. With this
notation we evaluate both members of Eq. (3.25). The following matrix product:
A× X × B
(3.26)
is equal to:
⎛
⎜
⎜
⎝
A1×X1 A1×X2 ... A1×Xk
A2×X1 A2×X2 ... A2×Xk
...
...
...
...
An×X1 An×X2 ... An×Xk
⎞
⎟
⎟
⎠× B.
Therefore, if AiX j abbreviates the scalar product Ai×X j of the row vector Ai with
the column vector Xj, then the product A× X × B is equal to:
⎛
⎜
⎜
⎝
b1,1A1X1 +b2,1A1X2 +...+bk,1A1Xk
b1,1A2X1 +b2,1A2X2 +...+bk,1A2Xk
.........
b1,1AnX1 +b2,1AnX2 +...+bk,1AnXk
...
...
...
...
b1,mA1X1 +b2,mA1X2 +...+bk,mA1Xk
b1,mA2X1 +b2,mA2X2 +...+bk,mA2Xk
.........
b1,mAnX1 +b2,mAnX2 +...+bk,mAnXk
⎞
⎟
⎟
⎠.
(3.27)
The right member of Eq. (3.25) is:
(BT ⊗A)× vec(X)
(3.28)
which is equal to:
⎛
⎜
⎜
⎝
b1,1A b2,1A ... bk,1A
b1,2A b2,2A ... bk,2A
...
...
...
...
b1,mA b2,mA ... bk,mA
⎞
⎟
⎟
⎠×
⎛
⎜
⎜
⎝
X1
X2
...
Xk
⎞
⎟
⎟
⎠
that is, by making explicit the rows of A:
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎛
⎜
⎜
⎝
b1,1A1 b2,1A1 ... bk,1A1
b1,1A2 b2,1A2 ... bk,1A2
...
...
...
...
b1,1An b2,1An ... bk,1An
⎞
⎟
⎟
⎠×
⎛
⎜
⎜
⎝
X1
X2
...
Xk
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
b1,2A1 b2,2A1 ... bk,2A1
b1,2A2 b2,2A2 ... bk,2A2
...
...
...
...
b1,2An b2,2An ... bk,2An
⎞
⎟
⎟
⎠×
⎛
⎜
⎜
⎝
X1
X2
...
Xk
⎞
⎟
⎟
⎠
..........................................
⎛
⎜
⎜
⎝
b1,mA1 b2,mA1 ... bk,mA1
b1,mA2 b2,mA2 ... bk,mA2
...
...
...
...
b1,mAn b2,mAn ... bk,mAn
⎞
⎟
⎟
⎠×
⎛
⎜
⎜
⎝
X1
X2
...
Xk
⎞
⎟
⎟
⎠
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠

128
3 Algorithms and Biorhythms
but this vector corresponds to the vectorization of matrix (3.27), therefore the proof
is completed.
⊓⊔
If we apply the Vectorization Lemma to Eq. (3.23), we obtain:
(A⊗G)× vec(C) = vec(D)
(3.29)
where the stoichiometric matrix A is multiplied by the Kronecker product with the
regressor matrix G and the result is multiplied with the vectorization of the regressor
coefﬁcient matrix C, and then equated to the vectorization of the substance variation
matrix D, by providing nt equations with md unknown values.
According to the least square approximation method (see Sect. 7.7), if matrix
A ⊗G has maximum rank (that is, matrices A and G have maximum rank), and
nt ≥md, then the best approximation to vec(C) minimizing the difference between
the two members of Eq. (3.29) is given by the following vector:
vec(C) =

(A⊗G)T × (A⊗G)
−1 × (A⊗G)T × vec(D).
(3.30)
Some constraints may be imposed to the ﬂuxes provided by regulators, which may
be of general nature, or may be speciﬁc to some classes of systems (for example,
ﬂuxes should not be negative, and the sum of ﬂuxes of all reactions consuming a
substance x cannot exceed the quantity of x).
However, the approximation given by the matrix expression (3.30) cannot in gen-
eral be considered the best way for solving the dynamical inverse problem which
we started from. In fact, apart from the computational cost of considering all the
d regressors at the same time, several reasons suggest we should follow a gradual
strategy in the determination of a subset of regressors, and their corresponding co-
efﬁcients, which provide the best approximation of the given dynamics. In fact, two
main requirements, which we will discuss in a future section are essential for an
appropriate application of the least square method: the linear independence among
the regressor expansions and the minimality of the set of regressors.
In conclusion, the best approximation is obtained by determining a mini-
mal set of linearly independent regressors ensuring an error under a given
threshold.
Linear independence is a requirement of least square method, while the minimal-
ity avoids problems of over-ﬁtting of the approximate solution. In fact, the more
regressors are considered, the less is the degree of freedom of the solution. This im-
plies that the solution ﬁts very well with the dynamics on the observation points, but
it is too constrained to them for behaving in a satisfactory way outside them. There-
fore, in order to cope with these requirements we integrate the least square method
by developing, in the next sections, an algorithm, which we call LGSS (Log-Gain
Stoichiometric Stepwise) for selecting the best regressors among a set of possible
regressors.

3.4 Stoichiometric Expansion and Stepwise Regression
129
The stoichiometric expansion method allows us to determine the best coefﬁcients
which approximate regulators, as combinations of some speciﬁed regulators. An-
other two crucial aspects of LGSS algorithm concern i) the determination of which
regressors have to be considered for each ﬂux regulation function, ii) how to choose
those which provide the best approximations. We address these aspects by integrat-
ing the stoichiometric expansion with two mechanisms: i) a way for scoring the re-
gressors of an initial dictionary of functions, by using the Log-gain principle, and ii)
a way for choosing the best regressors for each ﬂux regulator, by using an extension
of the classical stepwise regression method, based on the least square approxima-
tion and on the F statistical test. In the following subsections we develop the details
necessary for a complete description of the LGSS algorithm.
3.4.1
Log-Gain Principle
In the previous section, we have explained how stoichiometric expansion approx-
imates the coefﬁcients of regressors which provide the regulators of a given MP
grammar when regressors are given. In the following subsections we will focus on
the problem of ﬁnding which regressors provide the best approximation of regula-
tors, among a set of d basic functions g1,g2,...,gd depending on substance quanti-
ties and parameters. Here we introduce a principle, called Log-gain principle, which
is a special case of a general criterion usually satisﬁed by the variations of quantities
involved in biological phenomena.
Given a time series vt = (v[i] | 0 ≤i ≤t), of non-null real values its discrete
log-gain vector is deﬁned by:
Lg(vt) = (Lg(v[i]) | 0 ≤i ≤t) =
v[i+ 1]−v[i]
v[i]
| 0 ≤i < t

.
(3.31)
The log-gain principle requires that the ﬂux vector ul[i] = ϕl(s[i]) associated to a
reaction rl has to be a linear combination of log-gains of the time series of tuners of
ϕl. Namely, let st = (s[i] | 0 ≤i ≤t) be the time series of states along some uniformly
distributed observation points 0 ≤i ≤t, with a corresponding time series of ﬂuxes
given by the regulator ϕl:
ϕt
l = (ϕl(s[i]) | 0 ≤i ≤t).
For any substance or parameter y, if the corresponding time series of values along
the state time series st is:
yt = (y[i] | 0 ≤i ≤t),
then the log-gain principle is satisﬁed by ϕt
l when the following equation holds for
suitable real coefﬁcients q j, where Tl is the set of tuners of ϕl:
Lg(ϕt
l ) = ∑
yj∈Tl
q jLg(yt
j).

130
3 Algorithms and Biorhythms
This principle is a discrete formulation [92, 93] of a general principle holding very
often in biological systems, in order to keep a global equilibrium among all its com-
ponents. The term log-gain is related to the fact that the continuous version of the
log-gain of a real derivable function f, is the derivative of its logarithm, express-
ing its relative (inﬁnitesimal) variation d f(x)
dt /x = d log f(x)
dt
. In general terms, we can
assume that in a biological system dynamics obeys a global equilibrium among vari-
ables. Namely, the relative variation of a variable, depending on certain independent
variables, has to be a linear combination of its independent variables. In the inﬂu-
ential book [66], the biological signiﬁcance and relevance of this principle, in many
speciﬁc contexts, is widely discussed.
For evaluating to which extent a regressor veriﬁes the log-gain principle, we com-
pute for each of them a log-gain score, by using a least- squares approximation,
where the log-gain of the regressor, along the observation points, is equal to a linear
combination of the log-gain of its tuners. For example, if we want to test the good-
ness of the regressor AB, given by the product of two time series (A and B), then we
need to calculate the least- squares approximation for c1,c2 such that
Lg(A[i]B[i]) = c1Lg(A[i])+ c2Lg([B[i])
where 0 ≤i < t. Then the obtained approximation error determines a criterion for
establishing a log-gain ordering among regressors by assigning greater log-gain
scores to the ones which provide smaller approximation errors.
3.4.2
The Stepwise Regression LGSS
Once regressors are ranked according to their log-gain scores, their choice is based
on a regression procedure which develops the ideas introduced in Sect. 3.3, by ex-
tending the classical method of stepwise regression [216, 76, 85]. Before presenting
LGSS regression, we will recall the classical k-variable multiple regression. The
reader can ﬁnd more details and statistical motivations in Chapter 7 (Sect. 7.7) and
in Aczel and Sounderpandian’s book [216], from which we adopt the notation.
The following equation is the general form of a linear regression. Statistics pro-
vides methods for ﬁnding the right coefﬁcients, possibly null, if they exist, ex-
pressing the kind of relationship between a dependent variable Y and one or more
independent variables X1, X2, ..., Xk (if k > 1, then the regression equation above
is called a multiple regression model):
Y = β0 + β1X1 + β2X2 + ...+ βkXk + ε.
(3.32)
The LGSS algorithm, given in the gray box at the end of this section, is a pro-
cedure extending the one deﬁned in Sect. 3.3, where MP systems were applied to
the problem of approximating real functions. Also, LGSS is based on least squares
estimation, but differently from standard regression, metabolic approximation in
LGSS has the form of a grammar imposed by the phenomenon under investigation.

3.4 Stoichiometric Expansion and Stepwise Regression
131
Moreover, the search for the best regulators of the given reactions is performed by
using suitable forms of F-tests (based on Fisher distribution).
Let us consider the algebraic formulation of the dynamical inverse problem, given
by Eq. (3.23), for determining the vector C of regressor coefﬁcients providing the
best regulators approximating a given dynamics:
G × C× AT = D
(3.33)
which is equivalent to (see Eq. (3.29)):
(A⊗G)× vec(C) = vec(D).
(3.34)
Let us assume that the rank of the matrix associated to the above system is max-
imum. This happens when the stoichiometric matrix has maximum rank and the
expanded regressors are linear independent. The system above can be considered as
a multiple regression model, where vec(D) is the (vector) dependent variable, while
the independent (vector) variables are the columns of matrix G. These assumptions
make it possible to deal with system 3.34 as a multiple regression model, in the
sense deﬁned in Eq. 3.32.
Forabetterunderstanding ofthefollowing discussion wereformulatein Table3.13
the deviations occurring in regression models in terms of the matrix D, where Δh
indicates variation vector of the substance of index h. We also extend the deﬁnition
of MSE, R2 and ¯R2 in order to consider separately each substance of the system.
MSEh = SSEh
t −dh
= ∑t
i=0(Δh[i]−Δh[i])2
t −dh
(3.35)
R2
h = SSRh
SSTh
= 1 −SSEh
SSTh
(3.36)
¯R2
h = 1 −SSEh/[t −dh]
SSTh/t
= 1 −MSEh
MSTh
.
(3.37)
Table 3.13 The three deviations of an LGSS regression for each substance of the system. We
indicate with h the index of the substance, with Δt
h the predicted value of Δt
h by means of the
multiple regression model, and with ¯Δt
h the average of the values of Δt
h.
Δt
h −¯Δt
h
=
Δt
h −Δt
h
+
Δt
h −¯Δt
h
Total
Unexplained
Explained
deviation
deviation (error)
deviation (regression)
for substance h
for substance h
for substance h
∑t
i=0(Δh[i]−¯Δt
h)2 = ∑t
i=0(Δh[i]−Δh[i])2 +
∑t
i=0( Δh[i]−¯Δt
h)2
SSTh
SSEh
SSRh
Sum of Squared
Sum of Squared
Sum of Squared
Total deviations
Errors
Regressions deviations
for substance h
for substance h
for substance h

132
3 Algorithms and Biorhythms
The number of degrees of freedom is calculated here by considering that in Eq. (3.29)
the total number of equations for each substance is equal to t and the number of re-
gressors used for each substance is given by:
dh =


rl∈Ro(h)

g j | ϕl =
d
∑
j=1
cl,jg j

(3.38)
where ϕl is the regulator of the reaction rl and Ro(h) indicates the reactions of the
system consuming or producing the substance of index h (see Table 3.7). Following
the same reasoning, we can also extend the notion of F-ratio:
Fh =
SSRh/dh
SSEh/[t −dh] = MSRh
MSEh
=
R2
h
1 −R2
h
· t −dh
dh
(3.39)
After having ﬁxed a signiﬁcance value α for the test, we can conclude that a linear
relationship exists if (see F-distribution in Sect. 7.7):
Fh > F[α;dh,t−dh].
The evaluation of the conﬁdence intervals for the least squares estimation of each
regressor can be done by computing, in the t-expansion Gt
j of regressor g j, the right
number of degrees of freedom of the t-distribution. It depends on the ADA stoi-
chiometric expansion applied to the considered regressors. In fact, we recall from
the k-Variable Multiple Regression Model that the number of the degrees of freedom
is given by n−(k+1), where n is the number of equations and k+1 is the number
of regressor coefﬁcients. Now, the role of n is played by the number of equations
involved in the regressor Gt
j under consideration, which is given by (see Table 3.7):
n′ = t ·|So(rl)|.
The role of k + 1, instead, is given by:
k′ = ∑
h∈So(rl)
dh
that is, the sum of all the regressors included in the model which modify at least one
substance in So(rl). Finally, the formula that gives the conﬁdence intervals for the
coefﬁcients computed in Eq. (3.29), is given by:
βl,j = cl,j ±t[α/2;n′−k′]

e·MSESo(rl)
(3.40)
where l = 1,2,...,m and j = 1,2,...,d are the indexes related to reactions and re-
gressors, respectively, e is the element related to the regressor under examination on
the ﬁrst diagonal of the matrix

(A⊗G)T × (A⊗G)
−1 used for the least squares
estimation of Eq. (3.30), and:

3.4 Stoichiometric Expansion and Stepwise Regression
133
SSESo(rl) = ∑
h∈So(rl)
SSEh
(3.41)
MSESo(rl) = SSESo(rl)
n′ −k′ .
(3.42)
The same observations made for computing the degrees of freedom in Eq. (3.40)
must to be applied to the formula of the partial F-test. When we want to test the
statistical signiﬁcance of an expanded regressor in LGSS, we apply the following
partial F-test:
F[1,n′−k′] =
SSER
So(rl) −SSEF
So(rl)
MSEF
So(rl)
(3.43)
where rl is the rule to which the regressor g j is added (Gt
j is its expansion), So(rl)
indicates the set of substances produced or consumed by rl, SSRR
So(rl) is the sum of
squares for error of the reduced model (i.e. the model without the regressor under
examination), SSEF
So(rl) is the sum of squares for error of the full model (i.e. the
model with the regressor under examination), and MSEF
So(rl) is the mean square
error of the full model.
After ﬁxing a signiﬁcance value α for the test, we can conclude that the full
model is statistically better than the reduced one (i.e. that the regressor under ex-
amination should be included in the model) when the value of the partial F-test is
greater than the threshold value:
F[α;1,n′−k′].
(3.44)
The partial F-statistics given in Eq. (3.43) makes possible the application of the
stepwise regression in LGSS, because it allows us to select the right set of regressors
that should be used in ADA expansion. We will test only a subset of the expanded
regressors, that is, the ones which have log-gain scores higher than a ﬁxed theshold
(see Sect. 3.4.1). When the stepwise algorithm stops, LGSS saves the computed
multiple regression model and then tries to modify it by running again the stepwise
regression algorithm with a larger set of expanded regressors. We repeat this phase
until there are no regressors that can be considered. The ﬁnal regression model will
be the one, among those saved, which will provide the best approximation. The
estimation of the approximation of models is calculated by means of Eqs. (3.35),
(3.36), (3.37), (3.39), and (3.40).
The least squares estimation used in LGSS gives the best approximations with
respect to the “observed steps”, while when the dynamics is produced by means of
the regulators, substance variations are computed by means of the previous “com-
puted step”. For this reason the dynamics generated by the MP system provided by
LGSS can be improved by a tuning process which systematically searches in small
neighborhoods of the estimated coefﬁcients, inside the range of the conﬁdence in-
tervals computed by means of Eq. (3.40). In this way, usually, values providing an
MP dynamics have a signiﬁcant improvement in the approximation of the observed
dynamics.

134
3 Algorithms and Biorhythms
LGSS Algorithm
1. Let t be the number of observation time points. Let α1,α2 be two user-
deﬁned signiﬁcance values for partial F-tests. Let S = /0, the set of saved
models. Let us identify any t-expanded matrix with the set of vectors con-
stituted by its columns;
2. Compute the matrices D,G of the t-expansions of substance differences
and of regressors, respectively;
3. Compute the log-gain scores for all the (expanded) regressors of G, and
sort them accordingly in decreasing order. Let G′ be the subset/submatrix
of regressors of G having the best log-gain scores (according to a preﬁxed
user-deﬁned threshold);
4. Let M be a user-deﬁned initial subset/submatrix of G′, and compute the
following LSE of unknown values vec(C):
(A⊗M)× vec(C) = vec(D)
(3.45)
5. For each expanded regressor gt ∈G′/M, compute the LSE of Eq. (3.45),
where M = M∪{gt}, then compute the partial F-test of the extended model
M∪{gt} with respect to the current model M, according to Eq.(3.43);
6. Is there some model, among those computed at the previous step, having
a partial F-test value higher than the value of (3.44) where α = α1? If No,
then go to step 11;
7. Update M as the model, among those of step 5, having the highest value of
partial F-test;
8. For each regressor gt ∈M, compute the LSE of Eq. (3.45), where M =
M/{gt}, and then compute the partial F-test of the current model M with
respect to the reduced model M/{gt}, according to Eq.(3.43);
9. Is there some model, among those computed at the previous step, having a
F-test value lower than the value of (3.44) where α = α2? If No, then go to
step 5;
10. Update M as the model, among those of step 8, having the lowest value of
partial F-test, then go to step 5;
11. Add the current model M to the set S of saved models;
12. Is G′ = G? If No, then update G′ as G′ ∪{gt}, where gt is the expanded
regressor among those in G/G′ having the highest log-gain score, and go
to step 5;
13. Among the models of S select the best one, by using the indexes of
Eqs. (3.35), (3.36), (3.37), (3.39);
14. Provide as output the model selected at the previous step with its evaluation
of parameters and conﬁdence intervals of regressors coefﬁcients.

3.4 Stoichiometric Expansion and Stepwise Regression
135
3.4.3
Problems Related to the Regression with LGSS
The size of the systems of equations solved by LGSS ranges from about a thousand
equations to some hundreds of thousands, depending on the number of substances
and reactions of the MP system under examination and on its time interval (a smaller
time interval requires a longer time series and so a larger system of equations)3. The
size of regressor dictionary depends on the complexity of the phenomenon under in-
vestigation, but usually it comprises no more than one hundred regressors. The total
computation time for a regression depends on the size of the regressor dictionary and
on the number of equations to be solved. However, the computation usually ends in
few minutes (less than ﬁve minutes on average, using a common laptop with a single
dual core CPU and 4Gbyte of RAM memory), but it can increase to hours when the
system is very big (i.e. a system with many hundreds of thousands of equations, and
a regression dictionary of hundreds of regressors).
As explained in Sect. 7.7.1, the correctness of a multiple regression model is
based on some assumptions about the independent variables, and about the prob-
ability distribution of the errors associated to observations. When one or more of
these assumptions are not completely satisﬁed, some mistakes may occur in the def-
inition of the regression model. There are three main problems that we need to be
aware of in the context of multiple regression: (i) the problem of heteroscedasticity,
(ii) the problem of residual autocorrelation, and, the most common in LGSS, (iii)
the problem of multicollinearity [76, 216].
The problem of heteroscedasticity occurs when the variance of the residuals of
the regression is not constant and is (directly or indirectly) proportional to the value
of one or more independent variables of the model. This is in contrast with the as-
sumption of uniform error variance, carried out at the beginning of Sect. 7.7.1. When
hereroscedasticity is present, our regression coefﬁcient estimators are not efﬁcient.
This violation of the regression assumptions may sometimes be corrected by the use
of a transformation for the dependent variableY or by substituting the ordinary least
squares estimation method with the method of weighted least squares.
The problem of residual autocorrelation arises when the error ε depends on the
observation points. It is also called residual autocorrelation, because it occurs when
the time series of the error values is highly correlated with the values of the series
at certain previous steps [229].
As in the case of heteroscedasticity, in this case the ordinary least squares may
fail, therefore it can be useful to adopt another procedure called generalized least
squares.
The most common problem related to regression in LGSS is the problem of mul-
ticollinearity. In multiple regression, we hope to have a strong correlation between
each independent variable and the dependent variable Y, but we do not want to have
independent variables correlated among them. In case of perfect collinearity, the re-
gression algorithm breaks down completely. Since in LGSS regulators usually are
3 An implementation of LGSS, as a set of MATLAB functions, has been developed by Luca
Marchetti in 2012 [108].

136
3 Algorithms and Biorhythms
assumed to be linear combinations of polynomial regressors, it is very common to
meet multicollinearity problems.
In order to overcome the problem, the stepwise algorithm implemented in LGSS
was extended by ﬁltering the expanded regressors, during the forward selection
phase, in order to avoid the insertion of a regressor which is highly correlated with
others already inserted in the regression model. To this end, LGSS computes the
variance inﬂation factor (VIF) for each regressor, which gives an idea of the degree
of multicollinearity it introduces with respect to the other regressors in the regres-
sion equation. To calculate the VIF for a regressor g, we need to run a multiple
regression by considering g as the dependent variable and the set of already inserted
regressors as the set of independent variables. The variance inﬂation factor associ-
ated with g is:
VIF(g) =
1
1 −R2g
(3.46)
where R2
g is the coefﬁcient of determination for the multiple regression of g with
respect to the other regressors.
A VIF of 6, for example, means that the variance of the regression coefﬁcient es-
timator for the considered regressor is 6 times what it should be when no collinear-
ity exists. In LGSS the user can select a threshold value for the variance inﬂation
factor in order to avoid the insertion of collinear regressors. This solution, how-
ever, may affect the performance of the algorithm since it requires many additional
computations.
Of course, a way to overcome the problem of multicollinearity is to drop collinear
variables before launching LGSS. We extended LGSS by including a procedure,
based on a hierarchical clustering technique [86, 104], which allows us to cluster
the time series of the regressors associated to the same reaction and to select those
which are less correlated and that best satisfy the log-gain principle. In many cases
this allows us to signiﬁcantly reduce the set of regressors and provides a better
approximation.
3.4.4
Models of Mitosis
LGSS represents a solution, in terms of MP systems, of the inverse dynamics prob-
lem, that is, of the identiﬁcation of (discrete) mathematical models exhibiting an
observed dynamics and satisfying all the constraints required by the speciﬁc knowl-
edge about the modeled phenomenon.
In Table 3.14 a short list of models obtained by LGSS is presented. In this section
we outline an application of LGSS to Golbeter’s oscillator given in Fig. 3.8 [81, 82,
83]. LGSS provided 700 different models of this oscillator, which, for the most
part, provide the same order of approximation of Golbeter’s model. Moreover, by
considering the phenomenon at different timescales, we obtained different models
and in many cases the analytical form of these models is simpler than Golbeter’s
model [101].

3.4 Stoichiometric Expansion and Stepwise Regression
137
Table 3.14 MP models obtained by LGSS
Belousov-Zhabotinsky, Prigogine’s Brusselator (BZ)
Brusselatus ([84, 68], Table 3.9)
Lotka-Volterra, Predator-Prey dynamics (LV)
Volteranus ([89, 116, 78], Table 3.8)
Susceptible-Infected-Recovered Epidemics (SIR)
Epidemicus ([88, 68])
Early Amphybian Mitotic Cycle (AMC)
Mitoticus ([81, 82, 101], Table 3.16)
Drosophila Circadian Rythms (DCR)
Drosophilus ([68])
Non Photochemical Quenching in Photosynthesis (NPQ)
Photochemicus ([110, 107])
Minimal Diabetes Mellitus (MDM)
Mellitus ([105])
Bi-catalytic Synthetic Oscillator
Sirius ([92], Table 3.5)
Circular Synthetic Oscillator
Goniometricus ([100], Table 3.11)
The fundamental mechanism of mitotic oscillations concerns the periodic change
in the activation state of a protein produced by cdc2 gene in ﬁssion yeast or by
homologous genes in other eukaryotes. The simplest form of this mechanism is
found in early amphibian embryos (see [83]). Here cyclin (C) is synthesized at a
constant rate and triggers the transformation of inactive (M+) into active (M) cdc2
protein, which leads to the formation of a complex known as M-phase promoting
factor (MPF). MPF triggers mitosis, but at the same time M elicits the activation
of a protease from state X+ to X. The active protease degrades cyclin resulting in
the inactivation of cdc2. This brings the cell back to initial conditions and a new
division cycle can take place. The ODE presented on the right of Fig. 3.8 is the
differential model of dynamics described on the left of Fig. 3.7, where C,M,X are
the concentrations of C,M,X, respectively, and 1 −M,1 −X are the concentrations
of M+,X+ respectively (the deﬁnitions of the parameters of the ODE model of
Fig. 3.8 are not simple and are not relevant for our further discussion, however they
can be found in [81]). The regulation maps calculated by LGSS are obtained starting
from a dictionary of 20 possible regressors, that is monomials of C, M, and X with
degree less than or equal to 3 (i.e. constants, C, M, X, C2, M2, X2, CM, CX, MX,
C3, M3, X3, C2M, CM2, C2X, CX2, M2X, MX2, and CMX)4.
The 700 MP models that LGSS found for Goldbeter’s mitotic oscillator are dis-
tributed into 40 different grammatical schemata. In Table 3.15 other descriptional
indexes of models are given for the ﬁrst 14 grammatical schemata which deﬁne
621 models from a total of 700 (89%). These indexes are useful for discriminating
interesting aspects of the MP grammars and they comprehend:
1. the number of regressors;
2. the total number of monomials;
3. the temporal grain of dynamics observation, which is expressed by the values of
time interval τ;
4 Substances M+ and X+ are not considered because they depend on M and X respectively.

138
3 Algorithms and Biorhythms
Fig. 3.7 On the left: a numerical solution of the set of differential equations (right part of
Fig. 3.8) comprising the model introduced by A. Goldbeter (ﬁgure taken from [81]). On the
right: an MP graph which represents Golbeter’s oscillator
dC
dt = vi −vdX
C
Kd+C −kdC
dM
dt = VM1
C
Kc+C
(1−M)
K1+(1−M) −V2
M
K2+M
dX
dt = MVM3
(1−X)
K3+(1−X) −V4
X
K4+X
Fig. 3.8 Goldbeter’s oscillator, which has a cycle of about 25 min [81]
4. the best value of τ which is relative to the model, which provides the best dy-
namical approximation of the mitotic phenomenon;
5. the best RMSE which is the average value of the RMSE relative to the substances
curves corresponding to the best τ.
It is worthwhile remarking that the grammatical schemata occurring at the ﬁrst po-
sitions (with high frequency) are also the grammatical schemata having a small
number of regressors and monomials. In Table 3.16 a parsimonious MP model of
mitotic oscillator is given.
3.5
Reactivity and Inertia
The notion of MP grammar is based on regulators, which deﬁne the value of reac-
tion ﬂuxes at a given state. However, a different approach for deﬁning ﬂuxes can be
followed, where ﬂuxes are obtained as results of a competition among the reactions.
In order to formalize this intuition, a reactivity parameter is associated to every
reaction, as a score in the competition for getting the reactants necessary to its real-
ization. This competition, among reactions consuming a given substance, concerns
a part of matter available in a given state, therefore another parameter is necessary
for each substance, which provides the amount of substance that, in a given state,

3.5 Reactivity and Inertia
139
Table 3.15 Descriptional indices of models given for the ﬁrst 14 grammatical schemata
Grammatical
Number of Number of Total n. of
τ interval
Best τ
RMSE
schemata
models
regressors
monomials (10−3 min) (10−3 min)
1
135
6
16
151 – 345
315
1.61·10−2
2
128
6
17
343 – 477
401
1.62·10−2
3
49
6
17
43 – 93
43
1.84·10−2
4
46
6
16
138 – 232
219
1.95·10−2
5
44
8
24
1 – 71
40
1.48·10−2
6
38
6
16
525 – 699
683
1.78·10−2
7
33
6
16
473 – 563
556
1.79·10−2
8
32
5
15
514 – 694
602
2.78·10−2
9
28
7
16
570 – 696
671
1.09·10−2
10
26
6
16
493 – 684
684
1.8·10−2
11
20
8
23
118 – 137
137
5.86·10−2
12
15
9
25
103 – 117
103
9.6·10−3
13
15
6
17
474 – 499
474
1.62·10−2
14
12
7
21
191 – 212
212
1.97·10−2
cannot be transformed. We call it the inertia of the substance (in a given state). The
difference between the whole quantity of a substance and its inertia is the quantity of
substance that can be partitioned among all reactions competing for it. MP systems
with reactivity parameters were the ﬁrst kind of MP systems formally deﬁned [92].
They are a special class of MP systems with regulators, which we call reactive MP
systems or MPR systems. When we want to refer to the MP systems deﬁned in Def-
inition 3.1, we refer to them as MPF systems for distinguishing them from MPR
systems (MP will mean one of the two systems, MPF or MPR, according to the
context). In an MPR system, we denote by ψx the parameter providing the inertia of
substance x, and by fr (r ∈R) which provides the reactivity of the reaction r. Let us
set by R−(x) the set of reactions consuming the substance x, by S−(r) the reactants
of reaction r, by |r−(x)| the multiplicity of the reactant x in the rule r, and by s any
state of the system. Then, another parameter pr,x of (metabolic) reactance of the rule
r, with respect to the substance x, can be associated to any substance x ∈S−(r):
pr,x(s) =
x/|r−(x)|
ψx(s)+ ∑r′∈R−(x) fr′(s).
(3.47)
Let min A be the minimum over a ﬁnite set A of numbers, conventionally extended
to the empty set by min /0 = 1. With these notations, the ﬂux regulation maps of a
reactive MP system are given by:
ϕr(s) = min{pr,x(s) | x ∈S−(r)} · fr(s).
(3.48)

140
3 Algorithms and Biorhythms
Table 3.16 The MP mitotic oscillator with the minimum total number of monomials (τ =
173· 10−3 min, RMSE ≈2.67· 10−2). Constants and initial values: vi = 0.025, k1 = 0.0209,
k2 = 0.0149329, k3 = 0.0351323, k4 = 0.0200062, k5 = 0.000662743, k6 = 0.215816,
k7 = 0.0696881, k8 = 0.0911799, k9 = 0.166106, k10 = 0.569463, k11 = 0.00823672, k12 =
0.252676, k13 = 0.404647, k14 = 0.668527, C[0] = M[0] = X[0] = 0.01, M+[0] = X+[0] =
0.99.
r1 : /0 →C
ϕ1 = vi
r2 : C →/0
ϕ2 = k1 +k2 M +k3 X −k4 CM
r3 : M+ →M
ϕ3 = k5 +k6 CM
r4 : M →M+
ϕ4 = k7 M +k8 X
r5 : X+ →X
ϕ5 = k9 C +k10 M
r6 : X →X+
ϕ6 = k11 +k12 X +k13 C2 +k14 CM
Two MP systems are dynamically equivalent when they have the same substances
and parameters, the same parameter evolution functions, the same scale factors
ν,μ,τ, and, starting from the same initial state, provide the same dynamics.
Any MPR system is a special case of an MPF system, but we will show that, for
some MPF systems, equivalent MPR systems exist. Lemma 3.3 and Theorem 3.4
explain the equivalence between MP systems based on ﬂux functions and MP sys-
tem based on reactivity and inertia parameters. In this lemma, and in the related
theorem, we consider the notion of non-cooperative MP rule. Table 3.17 considers
this and other properties of general interest for an MP rule r given by the multiset
rewriting α →β.
An MP system is non-cooperative if each rule r deﬁned in the system is non-
cooperative:
|S−(r)| ≤1.
Lemma 3.3. For any MPF system there exists a non-cooperative MPF system which
is dynamically equivalent to it.
Proof. If a rule, for example r : a + c →b, has more than one reactant, then we can
split it into two rules: r1 : a →b, r2 : c →/0 by requiring that the ﬂuxes ϕ1, ϕ2 of the
two rules are equal to the ﬂux ϕr of r. We can proceed in this way for all the rules
which are cooperative. The non-cooperativeMP system which we get in this manner
is dynamically equivalent to the original one.
⊓⊔

3.5 Reactivity and Inertia
141
Table 3.17 Types of an MP rule r of form α →β
monic
|α| = 1
monogenic
|S−(r)| = 1
non-cooperative
|S−(r)| ≤1
cooperative
|S−(r)| > 1
synthetic
|S−(r)| > |S+(r)|
transformative
|S−(r)| = |S+(r)|
dissociative
|S−(r)| < |S+(r)|
assimilative
α = /0
dispersive
β = /0
catalytic
S−(r)∩S+(r) ̸= /0
An MP system is positive if, in any state s, reaction ﬂuxes do not consume more
matter than the amount available (for any reaction r and any substance x):
∑
r∈R−(x)
ϕr(s) ≤x
Theorem 3.4. For any positive MPF system there exists a dynamically equivalent
MPR system.
Proof. According to Lemma 3.3, we can start by considering a non-cooperative
MPF system M. Now we transform M into an MPR system M′ which will result
dynamically equivalent to M. The system M′ is given by the same substances, rules,
parameters, ν,μ,τ of M. Moreover, for any ﬂux ϕr of M, we deﬁne the correspond-
ing reaction map fr of M′ as
fr(s) = |r−(x)|
x
·ϕr(s)
where |r−(x)| gives the multiplicity of the reactant x of r (when S−(r) = /0, we
consider |r−(x)|
x
= 1). Finally, for each substance x, we deﬁne its inertia function ψx
as
ψx(s) = 1 −∑
r∈R−(x)
fr(s).
The MPR system M′ equipped with the reaction maps and with the inertia functions
deﬁned above is dynamically equivalent to the MPF system M we started from. In
fact, for any rule r of M′, the reactance pr,x(s) is given by:
pr,x(s) =
x/|r−(x)|
ψx(s)+ ∑r′∈R−(x) fr′(s)

142
3 Algorithms and Biorhythms
and so the corresponding ﬂux ϕ′
r(s) of M′ is
ϕ′
r(s) = min{pr,y(s) | y ∈S−(r)} · fr(s).
Now, if S−(r) = /0, then
ϕ′
r(s) = min{pr,y(s) | y ∈S−(r)} · fr(s)
= min /0 · fr(s) = fr(s) = ϕr(s).
Otherwise, if S−(r) ̸= /0, since the system M is supposed to be non-cooperative, then
|S−(r)| = 1 and, having S−(r) = {x}, we can set:
ϕ′
r(s) = min{pr,y(s) | y ∈S−(r)} · fr(s)
=
x/|r−(x)|
ψx(s)+ ∑r′∈R−(x) fr′(s) · fr(s)
=
x/|r−(x)|

1 −∑r′∈R−(x) fr′(s)

+ ∑r′∈R−(x) fr′(s) · fr(s)
=
x
|r−(x)| · fr(s) =
x
|r−(x)| · |r−(x)|
x
·ϕr(s) = ϕr(s).
Therefore, in any case we have
ϕ′
r(s) = ϕr(s).
In conclusion, the ﬂuxes of the MPR system M′ are the same as the MPF system M,
whence the two systems result equivalent.
⊓⊔
The MP graph of Fig. 3.9 with its corresponding MP grammars (MPF and MPR) of
Table 3.18 provides, in N, the famous Fibonacci’s sequence when we start with one
unit of A (adult) and zero unit of N (newborn).
Fig. 3.9 The MP graph related to Fibonacci’s MP grammars of Table 3.18
Other examples of MPR and equivalent MPF systems are given in Tables 3.19,
3.20, and 3.21.

3.6 Metabolic Patterns
143
Table 3.18 MPF and MPR grammars providing Fibonacci’s sequence
Rules
Fluxes
Reaction maps Inertia
r1 : A →A+N ϕ1 = A f1 = 1
ψA = 0
r2 : N →A
ϕ2 = N f2 = 1
ψN = 0
Table 3.19 An MPR system providing the dynamics given in Fig. 3.3. All inertias are 100,
the initial values are 100 for A,B, and 0.02 for C.
r1 : A →2A
f1 = 10
r2 : A →B
f2 = 0.02C
r3 : A →C
f3 = 0.02B
r4 : B →/0
f4 = 4
r5 : C →/0
f5 = 4
Table 3.20 An MPR dynamically equivalent to the system given in Table 3.19 (the ﬁrst
reaction of duplication becomes here an input reaction). The inertia of A is 110, while inertias
of B and C are 100, the initial values are 100 for A,B, and 0.02 for C.
r1 : /0 →A
f1 = 10A/(0.02B+0.02C+110)
r2 : A →B
f2 = 0.02C
r3 : A →C
f3 = 0.02B
r4 : B →/0
f4 = 4
r5 : C →/0
f5 = 4
Table 3.21 The MPF of the oscillator associated to the MPR of Table 3.20
r1 : /0 →A
ϕ1 = 10A/(0.02B+0.02C +110)
r2 : A →B
ϕ2 = 0.02AC/(0.02B+0.02C +110)
r3 : A →C
ϕ3 = 0.02AB/(0.02B+0.02C +110)
r4 : B →/0
ϕ4 = 4B/(4B+100)
r5 : C →/0
ϕ5 = 4C/(4C +100)
3.6
Metabolic Patterns
In this section we present some basic cases of metabolic phenomena. Their com-
binations of reactions and regulations provide the majority of typical mechanisms
arising in metabolic networks, by means of cascades, antagonisms, feedbacks, and
delays. The simplest mechanism of composing reactions is their sequential compo-
sition, according to which the product of a ﬁrst reaction is the reactant of a second
one. A cascade is essentially given by a sequential composition of many reactions

144
3 Algorithms and Biorhythms
(see Fig. 3.10). The source of a cascade is the reactant of the ﬁrst reaction, while
its destination is the product of the last reaction of its sequential composition. A
cascade can be either perfectly linear, or sharing substances (reactants or products)
with other reactions.
Fig. 3.10 Sequencing
Fig. 3.11 Cycling
A (transformation) cycle is a sequential composition where the destination coin-
cides with the source of the cascade.
Coupling reactions means that their ﬂuxes are correlated. This does not mean
that the two reactions can be considered as a single reaction. In fact, in general their
ﬂuxes can be different, but for example, the ﬂux of one reaction could be the square
of the ﬂux of another one.
Fig. 3.12 A kind of homeostasis
Homeostasis can occur in many forms. The example given in Fig. 3.12 shows a
simple case of homeostasis where the amount of a given substance is automatically
kept to a ﬁxed value K (if a quantity f(X) is consumed, then the difference with
respect to the ﬁxed value K is introduced from outside).

3.6 Metabolic Patterns
145
The next examples of metabolic patterns are related to the regulation aspect of
reactions. The simplest form of regulation composition is shown in Fig. 3.13, which
can be seen as a regulation cascade. In the case of a regulation cascade we can speak
of a source reaction and of a destination reaction.
Fig. 3.13 A regulation chain
A regulation cascade where the source reaction coincides with the destination
reaction is a regulation cycle (see Fig. 3.14). In this case we may have a lot of pos-
sibilities in dependence of the kind of substances (reactants or products) regulating
the reactions. An important aspect of a regulator is given by its direct or inverse
character. In the ﬁrst case, the ﬂux is directly dependent on the value of the reg-
ulating substance. In the second case, it is inversely dependent on it. Two cases of
regulation cycles are represented in Fig. 3.14. Biregulation, represented in Fig. 3.15,
is another recurrent schema of regulation mechanism.
Fig. 3.14 Two regulation cycles
In the case of antiregulation, given in Fig. 3.16, one substance regulates the
production of another substance, which in turn, regulated the consumption of the
ﬁrst one.

146
3 Algorithms and Biorhythms
Fig. 3.15 Biregulation
Fig. 3.16 Antiregulation
Mechanisms of cisregulation and transregulation (Figs. 3.17, 3.18) concern
backward and forward relationships, respectively, between regulation and reaction
composition.
Fig. 3.17 Cisregulation
Fig. 3.18 Transregulation
Self-regulation is a basic mechanism, similar but different from autocatalysis,
where a substance promotes (or inhibits) its production, or its consumption. For
example, in almost all growth phenomena the bigger is the population, the faster is
its growth rate. For example, a commercial product is sold in a bigger quantity when
there is a greater production of it (making it cheaper), and analogously, the more it
is sold, the more it is produced. Negative self-regulation occurs when a substance
provides an inhibition of its production or an inhibition of its consumption.

3.7 Metabolic Oscillators
147
3.7
Metabolic Oscillators
Metabolic oscillators are ubiquitous in life phenomena, and life in itself is an oscil-
latory phenomenon reproducing and propagating in space. Many of the metabolic
patterns we analyzed in the previous section provide, in a simple manner, oscilla-
tory behaviors. In this section we present some examples of basic metabolic phe-
nomena. In almost all cases the possibility of dynamical curves exhibiting values
which range in a given interval, without diverging and without terminating, is corre-
lated to the structure of the MP graphs involving cycles of reactions and regulations.
The oscillators which we present are expressed as MPR (systems with reactivity
maps). In this formulation they assume a simpler form and are also easier to be dis-
covered. For any oscillator, its MPR graph (where reaction maps are indicated) is
given together with its dynamics (where inertias and initial values are indicated).
For the sake of simplicity, we maintain the terminology of the previous section
(cis-regulation, trans-regulation, and so on), even if regulators are replaced by re-
action maps, therefore their meanings are somewhat different from the regulation
mechanisms analyzed in the previous section. Dynamics are computed by means of
software simulating MPF systems (Psim, speciﬁcally developed in years 2007-2009
[67, 69, 71]). In many cases, oscillations are very robust in dependence of the initial
values and of the algebraic form of regulations, while in other cases systems prove
to be very sensitive and only small changes in the third or fourth decimal digit alters
dramatically the dynamical pattern. Figures 3.19, 3.20, 3.21, and 3.22 provide some
ﬁrst examples of interesting metabolic oscillators with their dynamics.
The names of many oscillators derive from names of stars. This fact is due to
many reasons. Namely, stars were the ﬁrst objects of systematical dynamical inves-
tigation (and mathematical formalization). Moreover, the dynamical system Sirius,
the ﬁrst deﬁned by an MP grammar, exhibits a behavior resembling the double star
Sirius, where the movement of one star is hidden by the other, because in its origi-
nal formulation [92], one substance is transformed in the other two, but the curve of
values (along time) of one substance is quite almost completely hidden by the curve
of the other substance (see Fig. 3.3).
Fig. 3.19 Bicatalyticus, a very simple MPF oscillator
The metabolic oscillator of Fig. 3.23 is based on a transformation cycle. Its initial
behavior follows a very irregular pattern, however, on a long run, it seems to reach
a regular dynamical pattern. On the contrary, the metabolic oscillator of Fig. 3.24

148
3 Algorithms and Biorhythms
Fig. 3.20 The dynamics of Bicatalyticus in 1000 steps, with initial values A=100, B = 100,
and all inertia values 100
Fig. 3.21 Paramitoticus, an oscillator based on a kind or regulation mechanism analogous to
that of the mitotic cycle
shows an opposite behavior, because it seems to tend toward a regular dynamics, but
in the long run, it does not reach a complete regularity (see Figures 20.16, 20.17,
20.19, and 20.20 of [95]).
Other examples of oscillating MPF with their dynamics are given in Figs. 3.25,
3.26, 3.27, 3.28, 3.29, and 3.30.

3.7 Metabolic Oscillators
149
Fig. 3.22 The dynamics of the oscillator Paramitoticus in 1000 steps with initial values A =
B = C = 10, and all inertias 100
Fig. 3.23 Vega, an oscillator based on a transformation cycle
Fig. 3.24 Mizar, an oscillator based on two joint cis-regulations

150
3 Algorithms and Biorhythms
Fig. 3.25 Deneb, an oscillator based on a cycle of regulations
Fig. 3.26 The dynamics of 1000 steps of the oscillator Deneb with initial values A = 1, B =
100, C = 200, A-inertia = 50, B-inertia = 200, C-inertia = 1
Fig. 3.27 Orcincus, an oscillator based on a composition of two cis regulations with a trans
regulation

3.7 Metabolic Oscillators
151
Fig. 3.28 The dynamics of 1000 steps of the oscillator Orcincus with initial values A = 100,
B = 10, and inertias 10
Fig. 3.29 Gemini, an oscillator based on an indirect self-regulation
Fig. 3.30 The dynamics of 100 steps of the oscillator Gemini with initial values A=1, B =
100, C = 100, and A-inertia = 50, B-inertia = 200, C-inertia = 1
The MPR oscillators represented in Figs. 3.32 and 3.33 are different forms of
the same metabolic structure, providing the same dynamics described in Fig. 3.31,
which are based on the oscillator Sirius considered at the beginning of the chapter
(see 3.5 and Fig. 3.3). In comparing the two MP graphs it is interesting to realize how

152
3 Algorithms and Biorhythms
duplication can be replaced by an input rule (in the following MP graphs, provided
by PSIM software, ”Environment measures” refer to the parameters, which in these
cases are absent).
Fig. 3.31 The dynamics of Sirius duplicans and of Sirius assimilans, given in Figs. 3.32 and
3.33
Fig. 3.32 The MPR graph of Sirius duplicans
The basic oscillatory mechanism of Sirius can be obtained with only two sub-
stances (see Figs. 3.34 and 3.35). This oscillator is essentially similar to Bicatalyti-
cus (see 3.19). When the regulators of the reactions that transform the substance A
are different, then substances B and C of Sirius exhibit different shapes (see Fig.
3.31 versus Fig. 3.37 with the corresponding dynamics of Fig. 3.37).

3.7 Metabolic Oscillators
153
Fig. 3.33 The MPR graph of Sirius assimilans, dynamically equivalent to Sirius duplicans
Fig. 3.34 The MPR graph of Sirius binarius
Fig. 3.35 The dynamics of Sirius binarius

154
3 Algorithms and Biorhythms
Fig. 3.36 The MPR graph of Sirius discordans
Fig. 3.37 The dynamics of Sirius discordans
3.7.1
Anabolism and Catabolism
A deeper analysis of metabolic phenomena distinguishes two different types of
metabolism: anabolism and catabolism. This distinction is related to the energetic
aspect of biochemical reactions. Anabolism is the part of metabolism related to the
synthesis of substances that are energetically rich, or play a structural role in life
processes. Catabolism is the metabolism of degradation of substances in their chem-
ical components less energetically rich. In this section we introduce some concepts,
related to anabolism and catabolism that are crucial in the analysis of metabolic
systems. Two important consequences of distinction anabolism/catabolism are: i)
the intrinsic verse of life processes which require input substances, coming from
outside, and output substances, expelled to the outside; ii) the deep relationship be-
tween life and oscillations.

3.7 Metabolic Oscillators
155
Let us consider conservative MP systems, which are close to the real metabolic
system, because they satisfy two essential requirements of metabolic phenomena:
matter conservation and energy orientation of metabolic transformations. Let us
give some notation and deﬁnitions for expressing these requirements.
Given a reaction α →β where multisets α,β are not empty, the matter conser-
vation principle requires that the whole matter of reactants has to equate the whole
matter of products, that is, neither loss nor gain of matter can occur in transforma-
tion reactions. This implies that in such a system a gain of matter can be realized
only with input rules of kind /0 →x, and a loss of matter can occur only with out-
put rules of kind x →/0. In symbols, according to notation of MP systems, we have
(μ(x) as the mass of a mole of x):
∑
x∈α
μ(x) = ∑
y∈β
μ(y).
A reaction chain of substances x1,x2,...,xn is given by reactions r1,r2,...,rn−1
such that x1 ∈S−(r1) (x1 is a reactant of r1) xn ∈S+(rn−1) (xn is a product of rn−1)
and, for 1 < i < n, xi ∈S+(ri−1) and xi ∈S−(ri). A reaction cycle is a reaction chain
where xn = x1.
An MP system is time-bounded if there is a bound to the maximum number
of times its reactions can be applied. This means that, after a number of steps, its
reactions do not have the amounts of substances necessary to them. A system is
time-unbounded if it is not time-bounded. The system is matter-bounded if all
the matter available in the system is not greater than a given quantity. A system
is matter-unbounded if it is not matter-bounded. Of course, no physical system
can last an inﬁnite time, or can have an inﬁnite quantity of matter, therefore, when
speaking of time or matter unboundedness, we have to think of the term unbounded
as equivalent to having extreme values (which we avoid to ﬁx now) of duration or
size (in relation to a certain time and matter availability, respectively).
An MP system is assimilative when there is at least one input rule. It is dis-
persive if there is at least one output rule. It is dissipative if it is simultaneously
assimilative and dispersive, and it is open if it is either assimilative or dispersive,
while it is closed if it is not open.
A simple way for providing oscillations in a closed MP system is given by only
two substances A,B and two reactions: r1 : A →B and r2 : B →A with ﬂux maps A
and B respectively. However, no real chemical or biochemical system of this kind
can exist. This impossibility is connected to a crucial aspect of chemical transfor-
mations which have to obey a principle based on Gibbs’ variation of free energy.
In simpler terms, the natural orientation of a chemical transformation is from reac-
tants that are globally energetically richer to correspondent products energetically
poorer. This verse can be inverted only by providing an additional energy, for ex-
ample, by coupling the reaction with another one releasing the necessary energy for
this inversion.

156
3 Algorithms and Biorhythms
For MP systems where matter conservation holds, the energy orientation require-
ment can be stated by means of the following cycle dissipation principle (no free
reaction cycle may exist).
Cycle dissipation principle
When an MP system is time-unbounded and has a reaction cycle, then surely
a substance has to be present in the system that is consumed during the appli-
cation of the reactions of the cycle.
Figure 3.38 shows the abstract idea underlying the cycle dissipation principle.
The intuition of such a phenomenon can be understood with the metaphor of a wa-
termill, where the wheel activity of this device needs an external water ﬂow sup-
porting the necessary mechanical energy providing the dynamics of the system. If
matter-boundedness is assumed in a metabolic system with a reaction cycle, then
according to the cycle dissipation principle, the input matter feeding the internal
cycle needs to be coupled with output rules expelling matter.
Fig. 3.38 The principle of cycle dissipation for metabolic systems, in the form of an (abstract)
watermill
In the following discussion we consider MP systems which are assumed to sat-
isfy the conservation principle and the cycle dissipation principle. The results we
derive show the importance of open systems and their relationship with oscillatory
dynamics.
Lemma 3.5. Any closed MP system has to be time-bounded.

3.7 Metabolic Oscillators
157
Proof. If the system is closed, then it is matter-bounded. From the cycle dissipation
principle no reaction cycles are present in the system. Therefore, after a number of
steps reactants disappear. Therefore, reactions in the system cannot continue to be
applied.
⊓⊔
Lemma 3.6. Any open but non-dispersive MP system which is matter-bounded can-
not be time-unbounded.
Lemma 3.7. Any open, but non-assimilative MP system which is matter-bounded
cannot be time-unbounded.
In the context of MP systems we say that a system oscillates if its substance quanti-
ties vary in time, but their values are always within some ﬁxed ranges.
Proposition 3.8. An MP system having a reaction cycle and which is matter-
bounded but time-unbounded, has to be assimilative, dissipative, and oscillating.
Proof. (outline) An MP system with a reaction cycle needs to feed some substance
from outside, therefore it is assimilative. But if it is also matter-bounded and time-
unbounded, then it has to be dissipative. These requirements imply that surely its
substance quantities have to vary within some intervals, therefore the system is
oscillating.
⊓⊔
Cells host complex metabolic processes and are bounded in matter and unbounded
in time (when considered with their genealogical lineages). In fact, their sizes can-
not be greater than some values and their reactions need to last as long as possible,
because reproduction provides the reiteration of their internal metabolic processes
in time. Of course, they are special conservative metabolic systems. This means
that what was abstractly deduced for conservative MP systems deﬁnitely applies to
cells, which need to realize oscillatory phenomena. In conclusion, life is oscillation
and the analysis of metabolic oscillations is crucial to understanding life. In this
regard, it is interesting to quote the words of the Nobel laureate Ilia Prigogine in
his foreword to Goldbeter’s book [82] (the oscillatory phenomenon to which Pri-
gogine refers is the Belousov-Zhabotinsky phenomenon, which we considered in
Sect. 3.1.2, about time asymmetry see also [147], Sect. 7 of Chap. 4, and Sect. 8 of
Chap. 5):
I remember my astonishment when I was shown for the ﬁrst time a chemical oscillatory
reaction, more than 20 years ago. I still think that this astonishment was justiﬁed, as the
existence of chemical oscillations illustrates a quite unexpected behavior. We are used
to thinking of molecules as traveling in a disordered way through space and colliding
with each other according to the laws of chance. It is clear, however, that this molecular
disorder may not give rise by itself to supramolecular coherent phenomena in which
millions and millions of molecules are correlated over macroscopic dimensions. . . . It
is this synchronization that breaks temporal symmetry.

158
3 Algorithms and Biorhythms
3.8
The Ubiquity of Metabolism
The French mathematician Ren´e Thom quotes in his book Structural Stability and
Morphogenesis [155] a statement by the Greek philosopher Eraclitus saying that
“ﬁre rests by changing” (Metabollon anapauetai). Existence is transformation, and
metabolism is the basis of biochemical transformation. However, metabolism is not
only an important kind of transformation, but can be considered a paradigm within
which many kinds of dynamical systems can be represented. We have already seen
that real functions can be approximated by means of suitable MP systems (see Sect.
3.3), and we discussed in many points the prominent role of metabolism in cell
functionalities and its link with DNA replication. Here, we want to point out some
other ﬁelds, where the metabolic perspective could be naturally applied to analyze
phenomena from observed behaviors.
The solution of many problems consists in the ability to discover discrete dy-
namical systems that generate time series of an observed phenomenon. This yields
the identiﬁcation of an internal state transition logic from an external phe-
nomenalistic evolution. The algorithm LGSS, which we presented in Sect. 3.4.2
provides a method for solving dynamical inverse problems by means of MP gram-
mars. Its resolution schema follows a pattern illustrated in Fig. 3.39. Therefore,
if we are able to describe a phenomenon in “metabolic terms”, within a suitable
class of MP grammars, the discovery of speciﬁc MP grammars modeling it can
be developed by means of LGSS. In Sect. 3.8.2, a very general notion of MP
grammar will be presented, which suggests a large applicability of this concept
for a wide class of dynamical systems. Of course, this enlarges the ﬁelds where
MP theory and LGSS methodology could be applied for solving dynamical in-
verse problems: from the time series of meteorological variables, to time series
of economic, or of ecological nature. What are the evolution rules that are re-
sponsible for the series we observe? Sometimes, some basic principles are known,
but many detailed aspects are missing or are difﬁcult to be deduced in a reliable
manner.
In this section we outline an example, where a crucial phenomenon of gene
expression is analyzed by means of MP grammars. In the next section, examples
are given where distributed computations are expressed in terms of suitable MP
grammars.
Two relevant cases of biological interest are represented by signal transduction
and regulation networks. The ﬁrst case is a sort of metabolism where no matter
conservation principles are applicable. Namely, signals (for their intrinsic nature)
propagate by changing places and forms of appearance, but they can also be ampli-
ﬁed or reduced, and this is a peculiarity very important in their transformation and
communication.
The second case, which we report shortly, concerns a research in progress on
gene expression analysis in a cancer cell [109, 140, 141]. Assume that a given
perturbation, of a pharmacological nature, is activated on this cell, and the gene-
expressions of around 20,000 genes are given along a sequence of time points.
An important problem to solve is the effect of this perturbation on the gene-

3.8 The Ubiquity of Metabolism
159
Fig. 3.39 The general schema of metabolic paradigm in dynamical inverse problems
expressions. Given the huge number of different genes, a clustering phase is nec-
essary to collect all the genes that provide the same response, in time, to the
perturbation. When this is done, after some statistical elaboration of data and a
normalization of the measurements, it is important to discover the logic of inter-
actions of these clusters of genes during the effect of this perturbation. This can be
performed by using an MP grammar, and then by reading it in terms of classical
gene regulation networks. This is an example of the utility of the MP paradigm in
a ﬁeld that apparently does not present metabolic phenomena (in classical sense).
The following Figs. 3.40, 3.41, 3.42 provide seven interpretations of basic gene
regulatory phenomena where the representation of these gene regulation mecha-
nisms are accompanied by the corresponding MP graph representation. From the
results we got, in a speciﬁc analysis of medical interest, this approach provided
new clues in the explanation of the regulatory mechanisms involved with a speciﬁc
cancer pathology (see [109] for more details and for other applications to medical
problems).
In a ﬁrst approximation MP systems can be viewed as discrete approximations
of metabolic models based on ordinary differential equations (ODE), by means of
recurrent difference equations. Namely, for very small temporal interval τ, ﬂuxes

160
3 Algorithms and Biorhythms
Fig. 3.40 Gene-expression networks and MP graphs: promotion and inhibition
Fig. 3.41 Gene-expression networks and MP graphs: co-promotion and co-inhibition

3.8 The Ubiquity of Metabolism
161
Fig. 3.42 Gene-expression networks and MP graphs: co-promotion-inhibition and anti-
promotion-inhibition
of an MP system approximate to differentials and EMA reduces to a numerical in-
tegration performed according to the Euler method [77]. However, when the time
interval cannot be approximated to an inﬁnitesimal time, MP models introduce a
perspective which is radically different from the ODE perspective [83, 115], more
related to the transformations of the systems along observation steps, rather than
to the real kinetic of reactions, and to the chemo-physical mechanisms underlying
biochemical processes. However, the different perspective of MP systems has com-
putational and modeling advantages, by opening the possibility of algorithmic and
algebraic procedures for disclosing regulation logics of metabolic systems. Namely,
dynamical inverse problems can be solved (via the LGSS algorithm) and the math-
ematical form of regulators can be discovered in a systematical way.
In ODE models, and in stochastic models [80, 74, 51, 79] a critical point is con-
stituted by the evaluation of kinetic rates or stochastic parameters. This is a hard
and unreliable task, because very often the exact microscopic dynamics of reactions
is not completely known, or even when it can be completely studied in vitro, its in
vivo behavior could signiﬁcantly differ from its experimental evidence.

162
3 Algorithms and Biorhythms
An MP model assumes that some species are related by means of some trans-
formations, and that the variations are due to the global action of these transfor-
mations. Of course, their execution could involve very complex underlying sub-
transformations, but this is outside the objective of the model. It only tries to ex-
plain what is observed in terms of the chosen species and transformations. If the
choices are not the right one, this means that the model was not adequately deﬁned,
but this is independent from the methodology, it is only a matter of modeling de-
sign. Therefore, MP modeling is deliberately at a different, more abstract, level with
respect to ODE models. This does not means that it is less adherent to the reality,
but simply that it is focused on a different level of reality. Of course, in this way
many important dynamical details can be lost, but in many cases what is relevant
is the global pattern of a behavior and the main correlations among the involved
quantities. Moreover, in many cases, a higher level of analysis is the only possible
approach, because we know only some time series related to a given phenomenon,
and no idea is available about the internal laws ruling the observed phenomenon.
This is the case that we consider above, concerning a situation of gene expressions
network, where thousands of variables are involved.
3.8.1
Metabolic Computing
We have already shown (see Sect. 3.3) that MP systems can be used for approxi-
mating real functions. But they can also generate time series which correspond to
exact values of real functions along sequences of values. In the following, we give
some examples of MP grammars computing (or generating) the values of linear (Ta-
ble 3.22), square (Table 3.23), root square (Table 3.25), and exponential functions
(Fig. 5.13). It is remarkable that in all these cases ﬂuxes are computed by linear
regulators.
In these computations, we assume a criterion of lack of reactant block, that is,
the ﬂux of a rule becomes null when the regulator assumes a value greater than the
current quantity of one of its reactants (there is an insufﬁcient amount of matter for
applying the rule). In this case, also all the rules which share that reactant are forced
to have a null ﬂux.
Elaborating on the idea of ﬂuxes that become null when they exceed the availabil-
ity of reactants, a computation model on natural numbers can be developed which
is computationally universal (equivalent to register machines [99]).
The square root of positive integers is easily computed by the algorithm given in
Table 3.24, where the input value n > 0 is decreased at any step i by odd number
2i + 1, thus by reducing the initial value of increasing squares. This reduction is
continued while the reduced value remains positive (or null) and, at same time, a
counter (B) memorizes the number of steps. When the reduction process stops, the
counter provides the approximate value of the square root of n. This algorithm is
expressed by the MP grammar of Table 3.25. In fact, the number n is put as value
of A, and at any step A is decreased by increasing odd numbers, according to rules

3.8 The Ubiquity of Metabolism
163
r1 and r2. When A cannot be reduced anymore, because its quantity is less than the
ﬂux of rule r1 (lack of reactant block), then D = 0, therefore, in the following step
the value of B is assumed by Y (rule r4), and C = 0 (rule r5). At that moment, the
successor of the rounded square root of n is the value of Y. Therefore, in another
step rule r6 provides the ﬁnal result in Y and puts Z = 1, that is, the computation
ends (no rule can be applied). The obtained result in Y is exact if A = 0, otherwise
A contains the value exceeding the rounded root of n.
In Fig. 3.24 the MP graph of grammar of Table 3.25 is given, where an exter-
nal oval envelops internal substances, while those that host input and output values
are externally visible. In this kind of metabolic computational device, the control of
computation is naturally distributed, because it emerges from the way reactions and
regulators are connected (the correctness of the grammar, with respect to expected
computations of the square roots, can be proved by induction). The device of Fig.
3.24 can easily be represented in a pure visual form (see Fig. 3.45), as regulators
are sums of substances, thus the formulae deﬁning them can be replaced by arrows
connecting the substances that additively contribute to the same regulation. More-
over, Fig. 3.24 suggests a notion of multi-membrane as a structure collecting many
membranes, but where only some of them play the role of interfaces (accessible
internally and externally).
We refer to [99] for a discussion on some topological and biological intuitions
related to these structures. Of course, there are many similarities between this ap-
proach and the well-known model of Petri nets [113, 114, 72], but here regulators
play a speciﬁc role that gives special peculiarities to MP computations.
MP grammars generating/computing linear, quadratic, and exponential functions
are given in Tables 3.22, 3.23, and Fig. 5.13. By combining and extending such
kind of grammars, also MP grammars for polynomials, logarithms, sigmoids, and
asymptotic functions can be deﬁned.
Table 3.22 An MP grammar generating the sequence of natural numbers mn+k for n ∈N
r1 : /0 →A
ϕ1 = m
A[0] = k
Table 3.23 An MP grammar generating the sequence of square natural numbers in the values
of substance A
r1 : /0 →A
ϕ1 = 2B+1
r2 : /0 →B
ϕ2 = 1
A[0] = 0
B[0] = 0

164
3 Algorithms and Biorhythms
Fig. 3.43 MP grammars computing exponential functions
Fig. 3.44 The MP graph of the MP grammar of Table 3.25

3.8 The Ubiquity of Metabolism
165
Fig. 3.45 A pure visual representation of the MP grammar of Fig. 3.24
Table 3.24 An algorithm for computing the square root of positive integers
begin
input A (positive integer);
B := 0;
C := 1;
while A ≥2B+C
do
A := A - (2B + C);
B := B + 1;
od;
output B, A

166
3 Algorithms and Biorhythms
Table 3.25 An MP grammar computing the square root √n of a natural number n. The value
n is the initial value of substance A, while the (rounded) result can be found in Y when Z = 1,
and at the end, the difference between the initial value and result is put in A.
r1 : A →D
ϕ1 = 2B+C
r2 : /0 →B
ϕ2 = C
r3 : D →/0
ϕ3 = D
r4 : B →Y
ϕ4 = B+D+Y
r5 : C →W
ϕ5 = C +D
r6 : Y →Z
ϕ6 = W
r7 : W →/0
ϕ7 = W
A[0] = n
B[0] = 0
C[0] = 1
D[0] = 1
Y[0] = 0
W[0] = 0
Z[0] = 0
3.8.2
Revisiting MP Grammars
We conclude this chapter with a general concept of MP grammar, which extends
the initial metabolic inspiration toward a general dynamical paradigm which can be
applied to any system of variables.
First, let us recall that a dynamical system is given by a set of real variables
changing in time and a set of invariants, that is, conditions or constraints that are
satisﬁed by the variables during their change.
Now, let us assume a generalized notion of reaction among variables such as
X →Y, meaning that X decreases by a certain amount, while Y increases by the
same amount (with respect to some measurement unit). The decreasing/increasing
amount, called ﬂux of the rule, is determined by a regulator (regulation map) de-
pending on some variables of the system.
An MP rule is a regulated reactionamong variables, that is, a pair constituted by
a reaction and a regulator. A reaction is constituted by left variables which decrease,
while some corresponding right variables increase (each variable is equipped with a
multiplicity). A regulator is a function providing the ﬂux of the rule, in dependence
of the values of some regulation variables, called tuners of the rule (possibly includ-
ing parameters). Every left variable decreases of a quantity which is the product of its
multiplicity with the ﬂux of the rule, and every right variable increases analogously.
An MP grammar is a set of MP rules. Given an MP grammar, when we start from
an initial state (the values of its variables at an initial time) by applying all the rules
of the grammar, we obtain the next state and so on, for all the subsequent steps. An
MP grammar becomes an MP system when some numerical values are ﬁxed for the
physical interpretation of the time series: the time interval between two consecutive

3.8 The Ubiquity of Metabolism
167
applications of rules, and other values related to the quantity units (depending on
the physical nature of variables).
In mathematical terms an MP grammar is speciﬁed by: variables, reactions,
regulators, initial values. Variables that do not occur in reactions, but occur
in regulators are called parameters, while variables that are not parameters are
called (proper) variables. Given some initial values of its proper variables, an MP
grammar deterministically generates a time series for each of its proper variables.
If parameters are present in the grammar, then also the time series of the parameters
take part in the generation of dynamics. It is easy to realize, and it will be evident in
the following deﬁnition, that this notion of MP grammar deﬁnes a system of ﬁnite
difference recurrence equations, which represent the invariant of the dynamical sys-
tem generated from it. In the following generalization of MP grammar, the letters
MP may be related to Minus-Plus rules, which express the dynamics by means of
(regulated) decreasing-increasing of (proper) variables.
Deﬁnition 3.9 (Generalized MP Grammar). A (generalized) MP grammar is a
structure:
(X,Y,R,X[0])
where:
• X is a set of real variables;
• Y is a (possibly empty) subset of variables, called parameters. The elements of
set difference X −Y are also called proper variables;
• R is a set of MP rules, that is, pairs of type (reaction, regulator):
αr →βr ; ϕr.
For any r ∈R, a reaction αr →βr is a pair of multisets over the set X/Y of proper
variables, and a regulator ϕr is a function from X-states to R (an X-state is a
function from X to R). Variables that are arguments of ϕr are the tuners of ϕr;
• X[0] = (x[0] | x ∈X) is an initial state constituted by the values of variables at
an initial time 0.
For any X-state s, and for any proper variable x ∈X/Y, the rules determine a
decrease-increase Δx(s) of x, according to the following formula (each reaction de-
creases any left occurrence of x and increases any right occurrence of x):
Δx(s) = ∑
r∈R
(βr(x)−αr(x))ϕr(s).
(3.49)
(αr(x) = βr(x) = 0 if x does not occur in the reaction of r). Let us set, for every
i ∈N:
X[i] = (x[i] | x ∈X)
then, the (proper) variable variation (column) vector Δ[i] is given by (superscript T
denotes matrix transposition):
Δ[i] = (Δx[i] | x ∈X/Y)T

168
3 Algorithms and Biorhythms
Therefore, if A is the stoichiometric reaction matrix (with proper variables as row
indexes, and rules as column indexes):
A = ((βr(x)−αr(x)) | x ∈X/Y,r ∈R)
and the ﬂux (column) vector is given by:
U[i] = (ϕr(X[i]) | r ∈R)T
then, assuming to know at every step i > 0 the values of parameters, Eq. (3.49) can
be expressed in vector form by (× is matrix product):
Δ[i] = A×U[i]
that is, the ﬁnite difference recurrence equation EMA of Deﬁnition 3.1.
The passage from an MP grammar to an MP system is obtained by ﬁxing some
aspects of physical nature: the time interval τ and other physical measurement units
related to the speciﬁc system under investigation. Of course the same idea can be
expressed in other ways, according to the aspects one wants to stress and to the
conceptual organization of the structural and physical elements deﬁning a discrete
dynamical system.
For example, we could deﬁne an MP system M by a structure, where the gram-
matical component G of the system is focused completely on the rules, that is:
M = (X,Y,F,X[0],τ,G)
where:
• X are the variables and Y ⊂X are the parameters (X −Y the proper variables);
• F is the set of physical measurement units of the system;
• X[0] is the vector of the initial values of variables;
• τ is the step interval time (the values of parameters are supposed to be known in
all the points at time multiple distances τ from the initial time 0);
• G is the MP grammar of the systems, that is, a set of MP rules over variables X
(consisting of reaction-regulator pairs).
In this perspective, an MP grammar is the discrete counterpart of an ODE system,
as it identiﬁes the difference recurrence (vector) equation EMA representing the
dynamical invariant of the system, while the other elements X,Y,F,X[0],τ, refer
to the variables and to the other notions giving physical meaning to the numerical
values of the time series associated to them.

Sphere
4
Life Strategies
Abstract. Life is based on complex systems interacting with environments. In order
to guarantee their existence, these systems developed sophisticated strategies that
solve problems by introducing new problems. This is a peculiar aspect of life com-
plexity that sounds paradoxical in many aspects. In this chapter we outline some
basic strategies of life.
4.1
The Enzymatic Paradox
The kernel of Eigen’s paradox [128] points out a crucial aspect of life and its fun-
damental relationship with the basilar mechanism of molecule replication. In fact,
assume that at some point of molecular evolution a polymer α was produced with
the ability of self-replication. Let n be the length of this polymer, of course this repli-
cation ability was characterized by an error rate ε which, after a while, with a great
probability provides polymers which lose the very rare and remarkable property of
self-replication. For improving its auto-replicative capacity, this molecule α surely
had to increase its complexity, that is its length. In fact, a longer replicator can per-
form a better replication. But, at the same time, a longer polymer implies a greater
error rate in the replication process. Therefore, self-replication implies two con-
ﬂicting tendencies which have to meet at an equilibrium point (α0,n0,ε0) with an
error rate which is impossible to diminish in terms of polymer self-replication. This
argument shows, in qualitative terms, that self-replication can be improved along
a wider strategy. This was the strategy which life followed in its evolution from
the biochemical level to the cell level. Rather than single molecule self-replication,
V. Manca: Infobiotics, ECC 3, pp. 169–211.
DOI: 10.1007/978-3-642-36223-1_4
c⃝Springer-Verlag Berlin Heidelberg 2013

170
4 Life Strategies
life was surely forced to explore self-replicating molecular systems. A good self-
replicating system is a system where self-replication is realized in the context of a
wider system of reactions. In other words, replication is coupled with metabolism by
means of a reaction system able to keep stability and robustness with respect to the
external variations. Membranes are the essential devices for the realization of this
coupling. Therefore, cells could have emerged, from the prebiotic attempts, when
cell replication became a synchronized process of polymer and membrane replica-
tion together with all the underlying reactions also connecting the two processes.
In this case, replication is not only a replication of molecules, but a more gen-
eral process of reproduction, where not only molecules, but the entire processes
of their intertwined reactions are replicated. In conclusion, a good replication of
molecules can be reached within a more complex process that includes the replica-
tion of molecules. In this sense, life followed a strategy where a solution is searched
for by framing the original problem in a more general context. This is a resolu-
tion pattern recurrent in many life phenomena, that is, life thinks at large. It is also
interesting to remark that, it corresponds to a well-known strategy, in the solution
of mathematical problems, termed by P´olya [187] Inventor’s paradox: “The more
ambitious plan may have more chances of success”.
We do not know, at present, the details which explain why molecule replication
within cells is able to reach a more reliable and efﬁcient level, by escaping the limi-
tation of Eigen’s paradox. However, surely replication, in the context of cell repro-
duction, beneﬁts from the evolutive mechanism, in the sense that natural selection
pushes toward the best reproductive processes, which also selects the best molecule
replication processes. Now we present another life paradox, the enzymatic para-
dox, where we will consider metabolism and we show that an intrinsic contradic-
tion of this phenomenon can be solved if metabolism is joined with replication in a
reproductive system. Therefore, if replication postulates metabolism, according to
Eigen’s paradox, vice versa metabolism postulates replication and the deep interplay
between these processes in reproductive systems follows as an intrinsic necessity of
their basilar mechanisms. This means that the long debate about a priority between
them seems meaningless, on the basis of their mutual logics. The cell is essentially
a reproductive unit, and in reproduction phenomena replication and metabolism are
the two faces of the same coin.
Another life paradox is the evolutive paradox. Is evolution driven by the search
toward the best ﬁtness/dominance in a given environment, or more precisely its
dynamics has an internal nature which is not caused by the environment, but is only
selected by it? In other words, life realizes its ﬁtness for evolving or evolves for
reaching the best ﬁtness. The ﬁrst case seems to be more appropriate, according to
the error/chance strategy of life, which freely explores possibilities, subsequently
selected by the environment. This means that life evolves just for evolving, and
ﬁtness is a by-product of this tautological, intrinsic necessity.
Analogously, the morphogenetic paradox arises when we consider that differ-
entiation postulates a previously determined form to be realized, but how and when
this form realization does emerge in cells which are not predetermined to express
forms?

4.1 The Enzymatic Paradox
171
Life is a paradoxical phenomenon which evolves for solving paradoxes. In fact,
the solutions it ﬁnds raise other paradoxes, which move the intrinsic mystery of its
nature forward.
Almost the totality of biochemical reactions is performed by means of speciﬁc
catalysts. They are bio-molecules which allow reactants to meet and to react in
the best way by decreasing the intensity of activation energy. These molecules are
called enzymes. Usually, each reaction needs its speciﬁc enzyme, which is often a
protein. However, this notion of enzyme immediately raises a very general problem
of logical nature. In fact, also enzymes are bio-molecules, therefore i) either they
need biochemical reactions producing them; therefore enzymes which in turn need
enzymes along a tautological regression, ii) or they need to be introduced into the
cell from outside. In the second case, either they are introduced in the cell in a free
way, or they need some speciﬁc molecules which select the kinds of molecules that
are allowed to enter. The case of nonselective introduction of enzymes is not rea-
sonable from many points of view, but basically it is biologically unreasonable that
in a cell any enzyme ﬂoating in the environment could enter.
In the case of a regulated introduction of enzymes, the problem of enzyme pro-
duction is only shifted to the problem of enzyme introduction. In fact, enzymes
should need speciﬁc biomolecules that are able to recognize them, by allowing them
to enter in the cell. But where do these introduction enzymes come from? In con-
clusion, enzymes are supposedly produced internally in the cell, but this assumption
implies a petitio principii. In fact, any reaction which synthesizes an enzyme pos-
tulates an enzyme too, therefore any chain of enzymes (A making B making C,
and so on, for a ﬁnite number of steps) needs a starting enzyme which cannot be
produced internally by the cell. In conclusion, some enzymes need to come from
outside, but this seems only to change the terms of the problem without solving it.
Therefore, where and how are enzymes produced/introduced in the cell? We call
this contradictory situation the enzymatic paradox.
A possible solution of the enzymatic paradox is the existence of molecules of a
kind that allows the synthesis of enzymes, but which are not enzymes and which
came from the outside. Where do they come from? Let us call these bio-molecules
I-molecules for their intrinsic informational role. They came into the cell from out-
side, but their externality is in a temporal sense rather than in a spatial sense. They
are passed to the cell by its mother cell. Simple arguments of reliability imply that a
unique I-molecule collecting the whole information for the synthesis of all enzymes
is the best solution of the enzyme puzzle. But this unique I-molecule can be passed
to the daughter cell, after a duplication of it, otherwise the mother cell has to die as
soon as its unique copy of the I-molecule is released outside it.
The enzymatic paradox, and its solution outlined above, constitutes a sort of a
logical proof of the necessity of the two levels in the organization cells: the M-level
of metabolism and the I-level for storing the information of enzyme construction.
According to this perspective, it turns out that metabolism and replication are two in-
tertwined phenomena, and the argument here outlined represents a more geometrico
deduction of the dualism genes and proteins as a logical necessity of the autocat-
alytic nature of the biochemical reactions hosted in the cells. The following list of

172
4 Life Strategies
facts shows in a more evident way the logical nature of the enzymatic paradox and
of its solution:
1. Fact 1. Every biochemical reaction needs an enzyme catalyzing it.
2. Fact 2. Every enzyme is either internally produced by the cell, or introduced
into the cell from the outside.
3. Fact 3. The introduction of an enzyme in the cell has to be selective (not any
enzyme ﬂoating in the external environment can be internally introduced).
4. Fact 4. A selective introduction of an enzyme in the cell needs a speciﬁc
molecule which needs a speciﬁc enzyme producing it.
5. Fact 5. The previous facts imply either a regressio ad inﬁnitum, or a contradic-
tion in both cases of internal production or external introduction of enzymes.
6. Fact 6. A different kind of external introduction can avoid the consequences
of the previous facts: an introduction of special molecules in the cell from the
mother cell. They are I-molecules which are not enzymes, but provide the nec-
essary information for internally producing the enzymes of a cell.
7. Fact 7. I-molecules need to be replicated in the cell by enzymes already present
inside it, otherwise the passage from the mother to the daughter cell should
imply the death of the mother cell.
8. Fact 8. According to the previous fact, life is based on heredity and I-molecules
are the molecular basis of this mechanism.
9. Fact 9. A more reliable and efﬁcient mechanism for molecular heredity is
reached by a unique I-molecule collecting all the information for the synthe-
sis of enzymes.
10. Fact 10. The solution of the enzymatic paradox, outlined by the previous facts,
is based on reproduction and on heredity. In reproduction, the replication of
molecules is realized in the context of metabolism and vice versa metabolism
is realized by means of mechanisms of molecule replication.
4.1.1
Genes and Proteins
So far we have concentrated on enzymes. They provide the basic metabolic activ-
ity of cells, but bio-molecules performing the basic functions of cells constitute the
larger class of proteins. In fact reactions need to be catalyzed, but also regulated.
Moreover, bio-molecules for recognizing, transporting molecules, and for perform-
ing signals and movements are also essential in a cell as well as bio-molecules for
assembling speciﬁc structural components (gates, tunnels, reticula, bases, histones,
cilia, and so on). Table 4.1 summarizes the basic functions of proteins.
Amino acids are the monomers of polypeptide sequences. They are encoded by
triples of RNA nucleotides. However, this encoding is partial and redundant. This
means that some of all possible triples over the alphabets {A,U,C,G}, which do not
encode amino acids, as is the case of triplesUAA,UAG,UGA are not encoding,while
many triples may denote the same amino acid. Table 4.2 gives this correspondence,
called genetic code, where a letter of the English alphabet is associated to any amino
acid, and sets of codons encoding the same amino acid are synthetically denoted by

4.1 The Enzymatic Paradox
173
Table 4.1 Protein functions
Transformation
Regulation
Recognition
Signal
Transport
Movement
Structure
regular expressions where P = U +C, Q = A + G, N = P + Q, ¬G = U + A +C.
The 20 amino acids are encoded by 61 codons while the remaining codons (UAG,
UGA, UAA) encode the stop character. The codon AUG both codes for methionine
and serves as an initiation signal for translation. In fact, the ﬁrst AUG in an mRNA’s
coding region is where translation into protein begins. A circular representation of
genetic code is given in Fig. 4.3.
Table 4.2 The amino acids with their codons and regular expressions
Amino acid
Name
Letter
Codons
Regular expression
Arg
Arginine
R
{CGU,CGC,CGA,CGG,AGA,AGG}
CGN +AGQ
Leu
Leucine
L
{UUA,UUG,CUU,CUC,CUA,CUG}
CUN +UUQ
Ser
Serine
S
{UCC,UCU,UCA,UCG,AGU,AGC}
UCN +AGP
Ala
Alanine
A
{GCU,GCC,GCA,GCG}
GCN
Gly
Glycine
G
{GGU,GGC,GGA,GGG}
GGN
Pro
Proline
P
{CCU,CCC,CCA,CCG}
CCN
Thr
Threonine
T
{ACU,ACC,ACA,ACG}
ACN
Val
Valine
V
{GUU,GUC,GUA,GUG}
GUN
Ile
Isoleucine
I
{AUU,AUC,AUA}
AU¬G
Asn
Aspargine
N
{AAU,AAC}
AAP
Asp
Aspartate
D
{GAU,GAC}
GAP
Cys
Cysteine
C
{UGU,UGC}
UGP
His
Hystidine
H
{CAU,CAC}
CAP
Gln
Glutamine
Q
{GAA,GAG}
GAQ
Glu
Glutamate
E
{CAA,CAG}
CAQ
Lys
Lysine
K
{AAA,AAG}
AAQ
Phe
Phenilananine
F
{UUU,UUC}
UUP
Tyr
Tyrosine
Y
{UAU,UAC}
UAP
Met
Methionine
M
{AUG}
AUG
Trp
Tryptophan
W
{UGG}
UGG
RNA molecules are obtained by transcription from DNA and after that they are
translated by speciﬁc complexes of proteins and RNA which are called ribosomes.
They are particular translation machines, called transducers, in the sense of formal
language theory. A transducer is an extension of a ﬁnite automaton which reads one
letter per transition, possibly changing its internal state, but also replacing the letter
by a suitable string (in dependence from its current state). For example, the iterated
transducer given in Fig. 4.1 generates the tri-somatic language. It translates the
string in input (in the initial state on the top) in the string of the same language that

174
4 Life Strategies
immediately follows it in length. Then, transduction is iterated, because the result
of the translation (when it reaches the ﬁnal state, on the bottom) is given as input of
another translation. The strings obtained in this way, when it reaches the ﬁnal state,
are those of the tri-somatic language.
Fig. 4.1 An iterated transducer performing a tri-somatic growth
Figure 4.2 represents a transducer which realizes the translation of RNA tran-
scripts, according to the genetic code. This transducer has 21 states, which is ex-
actly the number of data encoded by the genetic code (20 aminoacids + the stop
character). In Fig. 4.2, the region constituted by the full circle in the center, together
with the bold line and the bold external circle, represents the initial state of the trans-
ducer. The states having transition to the external circle are the ﬁnal states (where the
transducer is after reading three consecutive nucleotides). All the transitions from
non-ﬁnal states translate the symbol into the empty character (indicated in the ﬁgure
with “-”). There are 25 transitions to 16 ﬁnal states. In fact, in four cases there are
two different transitions providing the same output (R, L, S , and stop). Therefore,
the transducer translates all the 64 codons into 21 outputs (20 aminoacids and stop).
Apart from one case, all the ﬁnal states have one or two transitions reaching again
the initial state. This means that the transitions cover in an almost uniform way all
the possible outputs. In principle, 5 transitions for each group of 4 ﬁnal states, plus
an additional transition, for example, a schema of transitions 5+5+5+6 could encode
the 20 amino acids plus the stop character.
In the transducer of Fig. 4.2, the schema of ﬁnal transitions 5+5+7+8 is near to the
minimal schema, apart from the redundancy of the 4 extra transitions (related to R,
L, S, and stop). We remark that a code of length 3 over an alphabet of four symbols,
where the third symbol is always signiﬁcant, has to encode al least 20 elements.
In fact, the 4 groups of ﬁnal states (each for each alphabet symbol) cannot have
only 4 transitions, otherwise the translation can be done only with strings of length
2. Therefore, at least 5 transitions for each group provide at least 20 transitions. It

4.1 The Enzymatic Paradox
175
would be interesting to establish if the codon length 3 is a consequence of having
at least 20 encoded elements, or otherwise, if 20 is a consequence of having codons
of length 3. Moreover, is the little discordance from the minimality related to other
important aspects of the genetic code, or is it a matter of contingency in the evolutive
process of deﬁnition of the code? Finally, we remark the importance of having an
asymmetric treatment of start and stop signals. The begin signal is given by an AUG
codon which encodes also Methionine (a very sharp encoding with only one codon),
while the stop signal is external to the code and uses 3 different codons. We can
easily realize that stop cannot be encoded by using a codon for one amino acid. In
fact, such a choice would reduce the number of effective amino acids. But, why not
employ speciﬁc codons only for marking the beginning of translation?
Fig. 4.2 The transducer realized by ribosomes
Whatabstractly weindicated I-moleculescorrespondto genes,whileM-molecules
correspond to proteins. Genes and proteins interact in the interplay between the pro-
cesses of translation from I-molecules to M-molecules and the regulation role of M-
molecules in the translation of I-molecules. In fact, a speciﬁc enzyme (in principle
could be many enzymes) performs the transcription from I-molecules into an inter-
mediate form of molecules, which in real cells are RNA molecules, that we can ab-
stractly indicate as IM-molecules. RNA molecules surely, in the ﬁrst stages of cell
evolution, were able to play both the roles of I-molecules and M-molecules. This

176
4 Life Strategies
Fig. 4.3 A (circle) tree representing the genetic code
intermediate role is crucial for a clear distinction between information and action.
The passage from an informative level to the active level cannot be direct and auto-
matic, it has to be modulated according to the speciﬁc context and the global state
of the cell, because no any kind of information has to be important/relevant/usefulin
any situation. The IM-molecules realize the need of making active, in a given con-
text, the information stored in DNA. When IM-molecules are produced,some of them
are R-molecules having speciﬁc regulation functions, while others are converted into
M-molecules. Their conversion into M-molecules is automatically performed by spe-
ciﬁc transducers, or T-molecules (which in real cells are ribosomes, molecular ma-
chine constituted by RNA and proteins). Therefore, some M-molecules play speciﬁc
roles among the seven functions of proteins, but a crucial role is the regulation role
activating the transcription phenomenon. This means that some M-molecules deter-
mine the active state of I-molecules and the speed/intensity of the transcriptional
mechanism according to a complex network of interferences (possibly at many lev-
els) which deﬁne a complex cycle of transcription regulation: the activation of I-
molecules depends on the kinds and the amounts of M-molecules translated in the
current context. But vice versa, the types and the amounts of M-molecules which
are translated depend on the kinds of I-molecules activated and on the amounts of

4.1 The Enzymatic Paradox
177
molecules (which are R-molecules or M-molecules) regulating their transcription.
Another subtle point emerges from this abstract analysis. In fact, activation is a yes/no
phenomenon, but the regulation of speed and efﬁciency of the transcription mecha-
nism provides a quantitative modulation of this activation by means of the amounts
of translated M-molecules.
Figure 4.4 shows the three steps of the passage from a gene to a protein generated
from it. The ﬁrst step, articulated in several phases (starting from DNA unpacking),
is the transcription where factors, activators, enhancers make active the promoter
region associated to the gene in order to start the transcription of one contiguous re-
gion of DNA into an RNA string, called pre-mRNA, by means of an enzyme, which
in eukaryotic cells is RNA polymerase II and works inside the nucleus (where the
genome of an eukaryote cell is). The starting point of the RNA polymerase transcrip-
tion process, called TSS (Transcription Start Site), is characterized by many speciﬁc
strings in its upstream proximal region (TATA box and analogous strings). Speciﬁc
proteins (around 3,000 in the Human genome), together with RNA complexes, di-
rect the entire process by means of a network of positive, negative, cooperative, or
antagonistic interactions (promotion, repression, inhibition, induction, and activa-
tion) with many speciﬁc sites of regulation, distributed in a wide area that may vary
from many thousands to around hundred thousand base pairs.
The transcribed pre-mRNA string is processed in a second phase, called mat-
uration. In this phase, through a splicing operation, performed by spliceosomes,
some substrings, called exons are selected and the remaining, called introns are
removed. Then, the resulting string, called mRNA, is sent outside the nucleus in
the cytoplasm. Here, the ﬁnal step consists in translating a region between a preﬁx
(5-Untranslated Terminal Region) and a sufﬁx (3-Untranslated Terminal Region).
Whenever the translation is performed, according to the genetic code, the result-
ing linear sequence of amino acids assumes a speciﬁc shape which determines the
function of the obtained protein.
The complex interaction between genes and proteins is based on an intrinsic cir-
cularity. In fact, proteins or protein complexes that regulate gene transcription are
coded by some speciﬁc genes. The overall networks of gene transcription regulation
are based on a multiset of proteins, DNA and RNA strings with a composition that is
speciﬁc for each gene. This multiset is responsible not only for the activation of the
process, but establishes the main parameters of the process (speed, duration, etc.).
In this regulative mechanism, fundamental problems are still open about the
mechanisms that determine the links between the coding regions and the related
speciﬁc promoter regions (outside the transcribed region, and surprisingly enough,
often, very far from the protein-coding region). We could suppose that a sort of
genomic code has to be present, which so far is unknown, in order to direct the cor-
respondence of promoting regions with related coding regions. Differently from the
genetic code, a genomic code is speciﬁc to each genome (though general principles
of organization can be reasonably assumed). In passing, probably many “organic
codes” are waiting to be discovered in many biological contexts [118, 154, 151].

178
4 Life Strategies
RNA molecules (in the many different forms, such as pre-mRNA, mRNA,
snRNA, snoRNA, miRNA, lincRNA, siRNA, tRNR, rRNA, shRNA, and ribo-
switches) play crucial roles (often in molecular complexes with proteins) in the
all the phases of the whole process of transcription/translation, which consists of a
cycle (DNA →RNA →Proteins →DNA).
In the phase preceding the translation, other mechanisms of post-transcriptional
modiﬁcation may intervene that alter the mRNA in order to modulate or even block
the normal translation (see, for example, Sect. 4.5). Analogously, even when the
translation is realized, post-translational modiﬁcation may further modulate the
overall result of the long way from genes to proteins. The richness and ﬂexibility of
these collateral processes is probably the key to the role of epigenetics, concerning
the processes that have no direct record in the genes, but modulate gene expres-
sion in correspondence to the different contexts of their realization, and moreover
can acquire certain forms of biologic memory, with possible short-term hereditable
characters.
Fig. 4.4 From a gene to a protein via transcription, splicing, and translation

4.2 Bio-inspired Algorithms
179
4.2
Bio-inspired Algorithms
Nature computes when it elaborates the biological information involved in complex
processes by means of speciﬁc sophisticated mechanisms of control, coordination,
organization, and ﬁnalization of particular objectives. As we have already noticed,
natural computing can be seen as a perspective intrinsically related to computer sci-
ence. If natural systems are the most complex systems we know, and if any complex
system can work on the basis of a related system of information elaboration, the
analysis of living phenomena from a computational perspective is appropriate in the
same manner as it is appropriate to observe birds for designing ﬂying objects.
In this section, we present some relevant algorithmic paradigms directly sug-
gested by nature. The main stress of this analysis is on the algorithms rather than on
the systems for realizing them. The latter pertain to “hardware”, for example, DNA
computing or Membrane computing, but also Neural Computing [9, 8], or Cellu-
lar automata (introduced by von Neumann) [161], while the former pertain to the
methods and the paradigms for designing resolutive strategies for speciﬁc classes of
problems.
The unifying aspects of all the methods we present brieﬂy are the population-
based strategies. In a sense, all the algorithmic methods suggested by nature have
in common, even if at different levels, the notion of populations of individuals and
the solutions are found by means of suitable processes pushing an initial population
(of solutions or of solvers) to evolve toward a ﬁnal state providing the solutions we
search for.
These algorithms apply usually to problems of a combinatorial or optimization
nature. In fact, their strategies are typical of situations which nature has to cope
with, for ﬁnding the best cases satisfying the constraints imposed by life.
Evolutionary algorithms [127] follow a general (abstract) schema of resolution
that is suggested by the basic mechanism of Darwin’s natural selection.
Table 4.3 The general schema of an evolutionary algorithm
Initialize the population S of solutions with a set S0 of possible solutions
Evaluate a ﬁtness level F over S, and while F is under a given threshold
do
Select a subset S′ of S
Expand S′ into a population S according to a generation mechanism
done
Output the population S reaching the ﬁtness threshold.
The ﬁtness function and the threshold value depend on the speciﬁc problem under
investigation. However, crucial and particular features of evolutionary algorithms
are the selective and the evolutive mechanisms. For example, each solution can pro-
vide new solutions by using casual changes or by specializing certain aspects, and
then the solutions which solve, at least a given fraction of test cases, are selected.

180
4 Life Strategies
Genetic algorithms are an important class of evolutionary algorithms, where the
evolutive mechanism is realized by two steps: i) a recombination step where two
(or more) individual solutions are mixed in some way for producing new solutions,
and ii) a mutation step, where some individuals are casually changed in a certain
small percentage. These two ingredients reﬂect the two main forces used by nature
in genetic evolution. Of course, the manners and the extent of these two evolutive
steps greatly inﬂuence the kind of results, and greatly depend on the context of
application of the genetic algorithms we consider. Moreover, the representation of
solutions determines many different forms of recombinations and mutations. Usu-
ally solutions can be represented by strings; therefore recombination corresponds to
the ways of exchanging substrings, while mutations are casual replacement of single
symbols (a certain number of them).
Memetic algorithms are a generalization of genetic algorithms, where the more
abstract notion of meme replaces the notion of gene. Many possible interpretations
of this concept, very often informally deﬁned, have been developed. However, gen-
erally, a meme has a recombinant character and a mobility greater than those exhib-
ited by a gene. In fact, a meme can be spread around in many copies and in many
places, and can take part in mutation and recombinations with other memes where
the number of copies, and the mutation and recombination extent are context de-
pendent. For example, the number and the sizes of components it exchanges and/or
the number of partners with which it interacts result from the “place” where the
meme lives. The metaphor underlying memes is that of cultural units. They spread
and evolve with the interactions among social groups which come in contact, by
exchanging ideas, habits, words, mental attitudes, and so on. The composition of
these groups constantly changes at a very quick rate. Formally, these features can
be expressed in many ways, but their overall effect is to make the evolutive, selec-
tive, and mutational aspects more articulated and complex than the corresponding
genetic mechanisms. This fact relates to the complexity and ﬂuidity of cultural evo-
lution with respect to the genetic evolution. A possible way of formalizing memetic
mechanisms is provided by membrane structures, where we can assume genetic
algorithms operating on internal membranes (genetic pools), and additional rules
exchanging individuals and even sub-membranes, among different membranes, and
where the levels of nested membranes could represent different levels of recombi-
nation.
Immunological algorithms are another type of evolutionary algorithms. Their
speciﬁcity is based on the notions of clonal selection and negative selection, which
are inspired by the corresponding theories developed since the mid 20th century
in immunology. Clonal selection is realized when selection is performed by some
recognition mechanism. For example, only solutions which match, to a certain de-
gree, some target elements are produced in many copies and this confers to them
more chance of becoming chosen in the further evolution process (this is the kind
of selection B cells undergo by means of antibody-antigen matching). Negative
selection occurs when the target elements which solutions have to satisfy constitute
an unknown space, viewed as a set complement of some known space (this is the

4.2 Bio-inspired Algorithms
181
kind of selection T cells undergo, when apoptosis is induced in all the cells while
the other cells survive to recognize the non-self elements as enemies.
Neural algorithms share with evolutionary algorithms the notion of ﬁtness. In
fact, they search for the best neural network connecting a given input/output corre-
spondence. Any neuron is an element which is stimulated by inputs and reacts to
them by producing outputs, generated by means of certain reaction functions, usu-
ally of sigmoid nature. Their outputs are propagated to neurons of a next level. The
solution that a neural network reaches is the function that provides the outputs cor-
responding to a given class of inputs. The reaction function assigned to each neuron
is chosen within a class of functions, but some parameters, usually called weights,
express the speciﬁc kind of reactivity for the given neuron. Formally, if we assume
n neurons for each of m levels, the neural network is represented by a matrix n × m
of weights for the neural connection of the neuron to the neurons of the next level
(zero weight means no connection).
In order to ﬁnd the best neural network providing the best approximation to a
given input/output behavior, some trainings with input/output cases are selected and
the measure of the efﬁcacy of the solution consists of the ability to provide the
right correspondence for all the cases covered by the problem. A different class of
bio-inspired algorithms are the population-based cooperative algorithms which
are designed in a perspective where individuals are agents searching for the best
solution to a given problem. In this case, solutions are searched through cooper-
ative and competitive search or cooperative and distributed generation pro-
cesses. In the ﬁrst case, some agents move in the solution space by evaluating, at
any step, their current solutions, by comparing them with the current solutions of
other agents, and by using this comparison for developing a better current solution.
The different classes of bio-inspired algorithms, realizing cooperative-competitive
procedures, are characterized by different methods of cooperation-competition. The
following are the general phases of these algorithms:
1. Try
2. Compare
3. Modify
4. Communicate.
The communicate module refers to the way agents share their knowledge in order
to improve their current solution. Many schemata can be used for realizing this cru-
cial aspect of cooperation. One possibility is based on some common space where
periodically agents put their current solutions, and some speciﬁc mechanism, in this
space, for assigning a score to all the solutions. In this case, any agent puts his
solution in the common space if it is better than those which are published there.
Another possibility is realized by means of a network, possibly dynamically de-
ﬁned, which ensures a communication between agents who ensure they update their
knowledge and the consequent evaluation of their solutions. The agent who ﬁrst
reaches a solution with a given level of satisfaction publishes it and the process ter-
minates. Therefore, in this case the publication of the solution coincides with the end
of the process. Of course, many other intermediate possibilities can be conceived.

182
4 Life Strategies
However, in both the previous cases, each agent searches for the global solution to
the problem in question, therefore in this sense the cooperation is complementary to
a competition, because each agent wants to win the “solver cup”.
Algorithms based on strategies of cooperation and distribution try to assemble
solutions by composing some partial solutions devolved to the single agents. In
some cases the agent population is articulated in many levels or roles, and even
the structure of a solution strategy can reﬂect this articulation, in such a way that
any agent is required only for some steps and in different phases of construction of
the solution. All these cases are strongly suggested by mechanisms found in nature
(bacteria, ants, and bees).
A paradigm very useful in optimization problems is the swarm intelligence. It
refers to situations where a collective solution emerges from many coordinated in-
dividual behaviors that follow simple rules of action and communication, usually
driven by common ﬁnalities (reaching a place, searching for food, escape from a
danger, and attack an enemy). Special cases of swarm intelligence are: ACO (Ant
Colony Optimization), PSO (Particle Swarm Optimization), and FSO (Flock of Star-
lings Optimization). A similar idea inspired a computational model, called Boids
that was elaborated in 1987 [149], where the ﬂocking behavior of birds (in a stereo-
typical New York pronunciation boids corresponds to birds) is simulated by using
very simple rules aimed at avoiding trajectory conﬂicts, at keeping spatial vicinity,
and at sharing move direction.
Many aspects of animal intelligence, even if based on speciﬁc phenomena, are
easily representable in computational terms. For example, the method of ACO (Ant
Colony Optimization) is used for the search for a minimum path between two points.
Each ant going from a point A to a point B follows, at ﬁrst casually, an itinerary.
However, during the journey each ant releases on the ground a substance, called a
pheromone. If we assume that only some paths can be followed; of course the paths
which are shorter are also the paths traversed by more ants and consequently having
more pheromone along them. In this way, paths richer in this substance have a major
score, and in this way the solution emerges automatically as a consequence of the
population-based strategy the ants follow.
Apart from the application to optimization problems [148], swarm intelligence
and similar paradigms could play a crucial role in the deﬁnition of algorithmic mod-
els of biological behaviors. In fact, if rules and computations can be established that
are able to reproduce some behaviors, this can shed light on the possible internal
mechanisms that are responsible of emergent functionalities in biological popula-
tions (of particles, cells, and animals).
In concluding the section, we mention the self-assembly algorithms, which ap-
ply a natural mechanism that is based on DNA computing algorithms and other
algorithms which are inspired by DNA recombinant power. In this case, a solution
is constructed by connecting some basic pieces which have an intrinsic tendency
to assemble with other pieces, according to forces attracting them toward com-
plementary pieces. In other words, any piece follows a local satisfaction principle
which is reached when it combines with its correct companion. In this way, struc-
tures are generated spontaneously which provide some possible solutions for a given

4.3 Replication and Autopoiesis
183
problem. Usually, this solution generation phase, due to the distributed nature and
to the autonomous character of the elementary pieces, is very efﬁcient and parallel.
When the possible solutions are formed, a phase of solution selection starts for ﬁl-
tering the true solutions of the problem in question. This schema corresponds to the
Adleman–Lipton extract model of DNA computing; however, it generally suggests
possible applications which can be extended to many other situations. These meth-
ods become effective when the self-assembly generation phase is able to provide
the whole (or a great part of the) solution space for a given problem. In this case,
a trade-off between time and matter has to be evaluated. In fact, if we start with
20 elementary pieces where each piece has two different companions, the possible
solutions are 210. This means that any elementary piece has to be provided with this
multiplicity if we want to generate the complete space of the possible solutions.
4.3
Replication and Autopoiesis
As we have often remarked, metabolism and replication are the basis of life and
the synchronization and cooperation of their activities is the key of life’s birth. In
this section, we want to stress again this point by using a metaphor which could
illuminate the main aspects of their crucial relationship. Let us consider an airport
and let us assume that according to some strategy of an economic, industrial, and
social nature it was decided to replicate the same structure within a given time, say
of some years, in the same region, but in a given place at a given distance from
the existing airport. Firstly, this is a problem of a system replication, but not only,
because it is also required that no service interruption can be tolerated, and moreover
that the newborn airport has to work almost instantaneously, even with a gradual
organization process, in such a way that, in a speciﬁed period, it has to be completely
comparable with the airport from which it derives. Let us consider carefully how we
will plan all this phenomenon, or better, how the original airport has to be organized
in order to provide for the realization of its replication.
Let us analyze the main airport structures and functionalities, without aspiring
to any completeness and adequacy regarding real cases, because we are mostly in-
terested in the elements which are relevant to the chosen metaphor. In an airport
we can distinguish a number of basic components and services going from the ter-
minals, control towers, ramps, runways, taxiway, maneuvering and parking areas,
passenger areas, cargo areas, passengers and personnel hosting, internal transport
services, security, surveillance, maintenance, communication, etc.
All the services are controlled and directed by a central management which is
at the top of a hierarchical structure with a complex network of interactions, and
organized at different coordination and subordination levels. Moreover, a system
of evaluation of the activities has to be deﬁned. All the services are performed by
operative units with an internal organization which depends on the type of service.
The structures have a sort of operative kernel devoted to their maintenance, which
is related to suitable stores.

184
4 Life Strategies
In order to realize a well-functioning replication mechanism, each operative unit
has to follow a strategy of acquisition of components over time, with a precise in-
crement rate. The members which join a unit, since their employment, are linked to
other members in order to learn from them speciﬁc tasks and skills, with the aim of
reaching a skill level similar to that of their teachers. The components of groups are
aware of their split, but nobody will know neither the exact time of this event, nor
who will leave for the new airport and who will remain in the original one. This kind
of ignorance, of the speciﬁc details about the separation, is important for avoiding
bias in the way individuals are related to their groups. Of course, this ignorance is
difﬁcult to realize in the central management unit; therefore a casual mechanism has
to be followed on which all the members have to agree. Moreover, a mirror of all the
operative units has to be available to the management unit, in order to appropriately
direct the splitting operation.
In conclusion, if we limit our analysis only to the human resources of an airport,
we clearly distinguish: i) a personnel acquisition ofﬁce, ii) a distributed system of
personnel formation (transforming people in operative members), iii) an ofﬁce for
monitoring the state of affairs of all operative units, iv) an ofﬁce controlling the right
relationships among all the operative units, and v) an ofﬁce providing solutions
of conﬂict and emergency, which require a very quick redistribution of tasks and
responsibilities. These ofﬁces are a sort of “meta operative unit” directly related
to vi) the central management ofﬁce and vii) the maintenance ofﬁce, which has in
charge the migration plan and the installation phase of the newborn airport.
In each group their oversized structure, due to the doubling of abilities, is in many
aspects a way for having a more reliable and efﬁcient realization of the speciﬁc
tasks, but at the same time, it opens the possibility of conﬂicts due to situations
of competition. This means that when the structure is big enough, it is at the right
moment for a separation which provides a reset for a new deal in the life of the
system. Therefore, when the split order is given by the central management unit, all
the groups are split and move, in a speciﬁed number of days, to the place where it
was decided to place the new airport.
Firstly, the structure builders are moved, with the suitable materials they need,
which belong to the operative units for the maintenance of buildings and airport
areas. Then, the groups move that are needed for activating the main structures.
When they are in the new reality the basic functions are organized, by following the
emergency protocols, which were used in the original system. An important aspect
deserves particular attention, which is related to the speciﬁcity of the activities. At
beginning, basic and generic tasks are required, therefore a sort of lack of special-
ization is required to individuals, in order to be able to carry out generic tasks. For
example, a member operating surveillance of luggage, at beginning, when only a
few ﬂights are regularly established, probably has to be willing to perform other
kinds of surveillance activities, when they are required by the superordinate units
on which he/she depends.
As results from the previous metaphor, replication is related with two distinct,
even if related, aspects: the replication of resources which are needed for assembling
the new structure and its processes, but also an aspect which is crucial in another

4.4 Main Informational Steps from LUCA to OVUM
185
sense, different from that of replication of resources. In fact, the ability required
by replication is essentially the same ability of a system which is aware of itself,
of its structure and of the processes it hosts. This awareness is based on a sort of
mirror of the global system (structures and activities) which has to be available to the
management ofﬁce of the system. This awareness is the basis of a self knowledge,
and it is the same kind of awareness responsible for the ability of self maintenance.
For this reason, any function has to be associated with an information which is
an identiﬁcation and description of it, in the context of the overall structure of the
system. Self knowledge is the basis for autopoiesis, a concept originally introduced
by Chilean biologists Humberto Maturana and Francisco Varela [142], as the main
peculiarity of living organisms. A system which is capable of self maintenance, self
restoration, and self construction, has to possess some kind of self knowledge, and
for this reason the replication ability is based on the same mechanism which drives
self control, by means of the double register information/function which life realizes
by means of genes and proteins.
4.4
Main Informational Steps from LUCA to OVUM
In this section we will try to outline brieﬂy the systemic logic driving some main
passages from protocells to complex forms of life.
Let us consider a prototypal cell able to perform the main function of life. For
it everything is minimal and this cell is able to survive and to produce offspring
very similar to it. This process requires matter and energy, which are taken from
the environment, and requires an incorporated knowledge, which is taken from the
heredity, represented by molecules coming from the mother cell along the repro-
ductive line to which the cell belongs. This is not only an individual process, but
it is a process which has to be considered in the context of a population to which
the cell belongs, and more precisely in the context of other populations which play
concurrently the same game. This aspect is based on an intrinsic conﬂict. In fact, a
population or a multi-population of individuals is a better guarantee that some in-
dividual could survive, but is also the reason for a competition among individuals
competing for the same resources, for maintaining their life. If a perfect cell could
exist (whatever “perfect” would mean), this cell would have a better life without any
conﬂict, but cells, as any biological entity, are arrival points in an endless process
of improvement, and a logic of populations seems to be necessary for implementing
evolutionary strategies. In a sense, individuals need to be organized in populations
because they are non “perfect”, but their belonging to populations can improve their
ﬁnalities. Of course, populations do not make sense without individuals, but at the
same time, individuals are products of a population, and even their ﬁnalities emerge
within the populations they belong to. It seems appropriate, in this context, the quo-
tation of a maxim, taken from an African tradition, which claims that “a child needs
a village for growing”.
The duality individuals/population is probably the deepest mechanism underly-
ing life phenomena, and is an internal force driving the aggregation/selection forces,

186
4 Life Strategies
from the subatomic particles to the universe galaxies. This duality is conjugated by
the tendency toward the complexity of forms. In fact, the main seven functions of
Table 4.4, which refer to cells, occur recurrently, and the same game of life starts
again for complex organisms, plants, animals, and animal populations. At the end of
a level of biological organization, new entities emerge which are the starting point of
a further application of the same paradigm. This schema explains the logic underly-
ing a lot of phenomena, going from morphogenesis, speciation, and sexual mating.
These seven steps are basilar in the construction of living individualities, but they
are always immersed in contexts of a population dynamics, where individualities de-
ﬁne speciﬁc roles of interaction, cooperation, and competition. For any individual,
the acquisition of the abilities of Table 4.4 is realized with conﬂicts, and individ-
ual death is an existential necessity. Paradoxically, death which is the antithesis of
individual lives, is the key of the natural selection applied to populations.
Table 4.4 Seven basic functions of living organisms
Nutrition (environment resource acquisition)
Ingestion/digestion (acquired resource transformation)
Movement/autonomy (territory navigation and control)
Respiration (energy acquisition and transformation)
Differentiation (function specialization)
Morphogenesis (form acquisition)
Reproduction (heredity and replication)
Let us explain, in more details, the seven musts of Table 4.4 from an evolutive
point of view. Of course, without taking matter from outside, no cell can survive.
The ﬁrst form of nutrient acquisition was possible in the surroundings of such nu-
trients. However, very often they were available where also enzymes were present
which were able to produce them from some substances. Therefore, a natural pas-
sage to a more efﬁcient form of nutrient acquisition was the ability of cells to acquire
these enzymes in order to catch directly substances and to transform them internally.
This ability surely provided a great advantage, by removing the necessity of
remaining near to nutrients and enzymes (in the case of heterotrophs), or of di-
rectly depending on the ﬂuctuations of nutrient concentrations (in the case of au-
totrophs). In this way, (heterotroph) cells became open to a space exploration ca-
pacity; therefore this was the prerequisite for developing movement functionalities.
From a “technical” point of view, the ingestion and digestion abilities require com-
plex topological skills of membrane manipulations. In fact, the predation of small
molecules can be obtained by channel mechanisms. But, for catching big molecules
(or other cells), more dramatic methods need to be elaborated. This happens when
a membrane realizes invagination and phagocytosis. In invagination, a part of a
cell surface forms a concavity where an external body can be wrapped in order to

4.4 Main Informational Steps from LUCA to OVUM
187
pass through the cell skin within a vesicle (Figs. 4.5, 4.6, and 4.7). As depicted in
Fig. 4.5, after wrapping the external object and after closing the concavity mouth,
a separation of two membranes is realized: the original one from the newly formed
internal membrane. In experiencing such a kind of mechanism, the cell acquired an
ability which became essential in developing its symbiont activity, when it was able
to catch other cells for devolving to them, under its control, speciﬁc tasks and con-
verting its main activity to more directive roles rather than to operative roles. This
step was essential in the passage from prokaryote to eukaryote cells, and follows the
general principle, continuously used by life, of reusing acquired competencies for
going on in the construction of more complex abilities.
Fig. 4.5 Phagocytosis: Invagination (I), Predation (II), Endocytosis (III), Encapsulation (IV)
Phagocytosis is a crucial point in passing from the acquisition from the environ-
ment of small organic molecules, to the ability of acquiring bigger objects which do
not pass through ports realized by protein channels. To this end, it is easy to realize
that a basic mechanism of mitosis is reused for other purposes. In fact, as is shown
in Fig. 4.6, when a cell separates in two distinct cells (mother and daughter cells)
we have a situation depicted at the top of Fig. 4.6, which we call devesiculation,
because the new cell is released in the environment as a separated entity. When, after
invagination and the closure of the invagination mouth, the situation depicted at the
bottom of Fig. 4.6 is realized, then we have a situation analogous to mitosis, with
the difference that now the new formed vesicle can be released internally to the old
one. For this reason, we call it invesiculation. In this case, apart from the realization

188
4 Life Strategies
of an invagination, some additional tools have to be used for performing the separa-
tion of vesicles. In fact, in the ﬁrst case, as we know, by general physical laws about
surface tension and minimal surface constraints, when the size of a vesicle grows
over a certain limit, automatically the separation happens. For invesiculation, some
cutting agents are required (and probably, other agents for closing the invagination
mouth need to be considered). In Fig. 4.7, a model of invagination driven by receptor
adherence is illustrated, which suggests the molecular basis of this phenomenon.
Fig. 4.6 Vesiculation: Devesiculation (top), Invesiculation (bottom)
Fig. 4.7 Receptor driven invagination

4.4 Main Informational Steps from LUCA to OVUM
189
As we already mentioned, in the next evolutive step, respiration was realized by
extending the phagocytosis in a more elaborated way. In fact, the more probable
event, allowing the passage from a prokaryote to an eukaryote cell, was the sym-
biont phenomenon where a cell captured an aerobic cell with a more efﬁcient ATP
production system based on Oxygen metabolism by devolving to the slave cell its
energetic metabolism. In this case, the topological transformation of Fig. 4.5 was
very appropriate, because the captured cell needs to be also compartmentalized in a
way that its reproductive activity could be controlled by the capturing cell. For this
reason, probably, a previous compartmentalization of the replicative DNA mecha-
nisms was realized, that is, the separation between DNA replication, in the nucleus,
and metabolism in the cytoplasm.
Differentiation has its deep roots in asymmetry. Even at molecule levels asym-
metric forms naturally arise. Nucleotides are chiral objects and this is a fundamental
feature for the realization of polymers conveying information. In terms of membrane
structures, asymmetry is represented by concentration gradients. They arise nat-
urally by consequence of general numeric properties of reaction ﬂuxes. This result
was pointed out in a seminal paper by Turing devoted to morphogenesis. How-
ever, two important factors pushing toward differentiation are the pumps and the
gradients. Both are surely related to the input/output direction which drives the
mechanisms of acquisition and expulsion of molecules, and to the organization of
the internal space of cells.
When different cells are integrated in a population articulated in parts, but with a
functional identity, we have a multicellular organism. The problem of this organiza-
tion is how to reproduce this identity in a reliable and efﬁcient manner. In abstract
terms, this means: how to memorize the knowledge underlying the biological struc-
ture of a multicellular organism, in order to be able to replicate it? This is, ﬁrst at
all, an enormous informational problem. The solution which nature found is the dis-
covery of a sort of supercell. This supercell is the ovum. It is a supercell because in
its genetic memory there is a written record of how to reproduce the whole organism
it belongs to.
Multicellularity could not be realized without this formidable biological discov-
ery. From embryogenesis [163] we know that the construction of the organism from
ovum (here there is no commitment with the sexual or asexual reproduction) fol-
lows a progressive differentiation, and this is the basis of the so-called evo-devo
paradigm (the developmental itinerary resembles the evolutive one); therefore the
ovum logic is strongly related to the differentiation logic and is the same kind of
phenomenon underlying stem cells, totipotent, and cellular type reversibility and
irreversibility, all concepts which are crucial in the new perspectives of many thera-
peutic approaches. The human body has around 250 main cellular types. They have
the same genome, but only a part of it is active in each type. This is a great redun-
dancy from the point of view of any system designer. But the motivation of this lack
of economy depends on the evolutive logic of the differentiation process.
The reusing of acquired competencies principle, which we already noticed, is
a general device driving the evolutive steps of life. However, another principle is
more speciﬁcally involved in the differentiation and organization steps along the

190
4 Life Strategies
Fig. 4.8 The logical structure of multicellular organisms and the implicit tautology of the
ovum’s role
processes pushing toward more complex individual organisms. This principle, we
could call it the Recurrent Functionalities principle, consists in the extension to
cells of the functions of proteins. For example, the functions reported in Table 4.1
for proteins are essentially the same we may ﬁnd in different types of cells of a
multicellular organism. Thus, the same principle acts when different tissues and
organs are assembled in complex living organisms, and again the same principle
acts in the population of individuals, when societies are organized.
Table 4.5 Recurrent functionalities in life differentiation and organization phenomena
Transformation
Regulation
Recognition
Signal
Transport
Movement
Structure
In an evolutive perspective, the role of each actor is not established a priori, roles
emerge when the players are ready to play them. In metaphorical terms, we could
say that the plot of the play is written during its development, from the interaction
with the actors when they are put together in situations which stimulate actions.
Another metaphor of differentiation could be suggested by the computers which
we use everyday. In a travel agency they are used for booking tickets, in an ofﬁce
for preparing documents, in a laboratory for elaborating data coming from experi-
ments, and so on. However, the majority of them have an operating system and stan-
dard software packages, which are used in a very small percentage. This depends
on the fact that it is easier to distribute computers with standard software packages,

4.5 Life Analysis and Synthesis
191
by leaving users to choose what they need. Passing to the biological level, why do
genes include many portions which are transcripts, but are removed when they are
translated, according to the well-known mechanism of the alternative splicing? Let
us suggest a possible reason, which of course would need a biological validation.
Namely, assume that in the course of bio-molecular evolution some DNA sequences
were selected having some speciﬁc biological meaning. The aggregation principle
toward complexity could have pushed these pieces to link in bigger sequences in-
cluding them, and this mechanism could have used sequences having the function of
connectors. When these pieces were joined, some new functions emerged, resulting
from their combinations, but the connectors which had the merit of putting them
together, in this new scenario, were useless or even disturbing, therefore an inverse
process was activated for their removal (during the translation phase).
Gametes are the sexual discovery, which is another big step toward biological
complexity. They are recombination biological machines, based on duality, which
is a very recurrent biological paradigm.
Mind, consciousness, language, and society, belong to other levels, but are rooted
in the protocell-multicell evolutive path.
4.5
Life Analysis and Synthesis
The short account of the previous section seeks only to outline a possible, in-
formationally based, interpretation key of biological evolution. Many sub-steps
could/should be considered, but the aim of these short remarks is to argue that logi-
cally based analyses could help in disclosing principles of life strategies. In any case,
we can discover the key of life when we know how replication and metabolism in-
teract. In other words, how is it logically possible to design functional units where
these two processes interact by producing autonomous systems? How do they suc-
cessfully replicate with the entirety of the processes they host, including replication?
The next step toward comprehension should concern a logical model of
multicellular reproduction based on the ovum mechanism, which adds a further
complexity level to the single cell miracle. Maybe, the logic which explains the
replication/metabolism integration is the same which explains ovum/multicellular
interaction, and maybe we need a new kind of logic for dealing adequately with
these two problems. However, we want to stress that we need such a kind of investi-
gation. In fact, it is impossible to reach a rational understanding, only by cumulating
partial and detailed knowledge about single phenomena and speciﬁc aspects. Such
marvelous devices, as cells and multicellular organisms, cannot exist without a uni-
fying logic and general principles. Even if this logic results from an emergence
phenomenon, this only changes the terms of the investigation, because the elements
which underly it need to be preformed for allowing such a result.
Time and chance surely play a crucial role, and even if a very sophisticated and
evolutionary process of self assembly is involved, some kinds of behavior attitudes
and frames are built in the elements which participate in the life game, in order to
make effective the role of time and chance. In his famous book Science et hypoth´ese,

192
4 Life Strategies
the mathematician Henri Poincar´e develops a beautiful reﬂection about chance. Very
often we say that some events happen by chance when there is a gap between the
causes of them and the effects that we observe. In fact, we “see” clearly the latter
ones, but are not able to identify the former ones, which are usually a great num-
ber and on different scales of observability. The discovery of deterministic chaos,
around the middle of the 20th century, is based on the phenomenon, called initial
condition sensitivity, that occurs in certain dynamics. In such cases, although the
evolution of a system is completely deterministic, it reacts, in an exponential way,
to any perturbation affecting the state of the system. This implies that, even if we
determine a certain dynamical parameter with a very high precision, we eventually
cumulate such a big error that whatever we predict is completely useless. The only
possibility of overcoming this limitation would be an inﬁnite precision of that pa-
rameter determination, that, apart from its practical impossibility, would imply a
computational power which cannot be reached by any computational system. This
profound discovery is the last of a long series of limitative results which mathemat-
ics and mathematical logic discovered, such as the existence of unsolvable problems
and of non-axiomatizable theories. The lesson of deterministic chaos points out the
need for a new scientiﬁc attitude regarding predictability and scientiﬁc models. The
so-called Laplacian paradigm of scientiﬁc explanation, was surprisingly applicable
to classical physics. According to it, if the initial state of a system is given, by us-
ing a deterministic law ruling the dynamics of the system, we should be able to
predict the behavior of the system at any future time. This powerful predictability
cannot continue to hold for complex phenomena. Therefore, the conceptual anal-
yses of complex phenomena, such as those occurring in living organisms, cannot
expect, in general, to exhibit some kind of Laplacian predictability. This means that
rational comprehension of biological phenomena does not imply the discovery of
a machine which tells us what will happen in the future, but will disclose, when it
is good enough, some internal relations which explain how a given system works,
and could help us in the evaluation of parameters adding new levels of knowledge
about life phenomena. A cell is a system where many coordinated molecular events
produce macroscopic effects. Therefore, the logic of cells is the logic which under-
lies this coordination. Discovering it has to be surely based on the biological and
experimental evidence, but there is the need of logical, mathematical, and rational
speculation which cannot be disregarded if we hope that such an enterprise could be
successful.
An important point concerning biological analyses, concerns a clear distinction
between description and explanation. The exact knowledge of the parts of a sys-
tem and of the processes they participate in does not imply an understanding of
the phenomenon. Paradoxically, too many details could hide its real logic. Abstrac-
tion is not common in biological investigations, while the descriptive approach has
dominated the life sciences. Finally, the possibility of discovering principles, inde-
pendently from the biological level, is an important issue to take into account. For
example, the logic of the human immune system, which is a very complex informa-
tional device, is probably based on security mechanisms and dynamical memories,
therefore a rigorous approach, in terms of abstract concepts developed in this ﬁeld,

4.5 Life Analysis and Synthesis
193
could provide fundamental clues for a new understanding of their components and
their connections with other biological levels.
Formal analyses of basic life mechanisms are strictly related to the computational
approaches. In fact, the discrete nature of basic molecular dynamics and their com-
plexity naturally requires algorithmic formulations of phenomena. Moreover, com-
putational simulations and analyses are essential tools when they are hard or even
impossible to reproduce in laboratories. In some cases the outcomes of computa-
tional experiments can suggest biological experiments, or could evaluate, exclude,
or conﬁrm some possibilities which are expected in a given conceptual explicative
framework.
Synthetic biology, which is often advantageously associated to the computational
approaches, is a recent perspective of biological investigation which seems to be
very promising in the near future. The ﬁrst objective of this discipline is the synthe-
sis of simpliﬁed cell structures, which can be components of minimal cells, that is,
living organisms, according to some minimal deﬁnition of life. In this context, the
notion of ribocell was elaborated [143, 152], which is constituted by a lipid vesicle
containing: i) nucleotides, ii) a ribozyme which can catalyze its own polymeriza-
tion, by means of an RNA template, and iii) another enzyme able to synthesize
the membrane lipids from some suitable precursor molecules. This cell could ex-
hibit the capacity of self reproduction, in the moment that their synthesis reactions
are synchronized in the right way with their ribozymes replication. Some computa-
tional experiments show that this possibility is coherent with the data coming from
the known mathematical models of synthesis reactions (differential models). The
membrane is essentially regulated by a mechanism which, in a simple way, can be
expressed in terms of relative surface. If V is the volume of a cell and S is its sur-
face, then its relative surface Φ is the ratio between S and the surface of a sphere
having volume V. In fact, the sphere of radius
3
V
π
3
4 is the solid with the minimal
surface comprising the volume V, and the tension/stretch proﬁle in the lipid bilayer,
originated by the membrane curvature plays a fundamental role in the mechanosen-
sitivity of the cell. When the relative surface Φ is equal to 1, then the cell is a perfect
sphere, while if Φ < 1 the membrane is subjected to a tension, and if Φ > 1, then
the membrane surface is relaxed and can be split. The tolerance for the membrane
tension can be expressed by the value ε such that under 1 −ε a crisis occurs and
the surface breaks. It depends on the kind of lipid structure. For example in oleic
acid, which is used in ribocell experiments, ε = 0.21. A sphere of radius R has the
following volume:
V = 4
3πR3
therefore:
R =
3

V
π
3
4

194
4 Life Strategies
and its surface, expressed by means of V is:
S = 4πR2 = 4π
3

V 2
π2
9
16 =
3√
36πV2.
A reasonable value for having a split is when Φ =
3√
2, in fact in this case:
3√
2 = S/
3√
36πV2
that is:
S =
3
2 × 36πV2 =
=
3
8 × 36π(V/2)2 =
= 2 3
36π(V/2)2
which means that the surface of the cell corresponds to that of two spheres having
volume V/2; therefore there is enough surface for splitting the volume in two equal
spheres.
In the attempt to realize minimal cells having autopoietic ability, the starting point
is the capacity to produce lipid vesicles including the basic ingredients of biochem-
ical dynamics: water, DNA and RNA nucleotides and oligo-polymers, enzymes,
ribozymes, peptides, PNA, catalysts, and so on. Of course, interesting byproducts,
of biomedical interest, of these researches are related to crucial mechanisms in in-
novative therapies and drug development. The main objectives in this ﬁeld concern
the realization within artiﬁcial vesicles of the following functionalities.
1. Realization of basic metabolic functionalities, and speciﬁc resources acquisition
expulsion;
2. Biochemical energy production;
3. Speciﬁc protein expressions mechanisms with some given context dependencies;
4. Complex vesicle internal articulations;
5. Realization of sensorial abilities with activation of environment response mech-
anisms;
6. Communication among synthetic cells;
7. Movement and mechanical activities.
In a wide sense, the essence of this synthetic trend is the realization of supramolec-
ular biochemical machines with speciﬁc functions, similar to those performed by
natural cells, capable of self assembly, and self organization.
Another recent trend in synthetic biology was inaugurated on May 20th 2010
by J. Craig Venter, the same researcher who ﬁrst completed the human genome se-
quencing. The experiment reported by Venter consists ﬁrst in the synthetic assembly
of the entire genome of a bacterium, Mycoplasma mycoides. For security reasons,
some genes were removed, for controlling possible unexpected problems, and other
genes were introduced for expressing proteins which confer a blue color to the bac-
terium colonies. Then, this synthetic genome was replaced in another bacterium,

4.5 Life Analysis and Synthesis
195
Mycoplasma capricolum, of the same family but with a genetic distance from the
M. mycoides, comparable to the distance between human and mouse genomes (this
bacterium was made DNA-free, by using a technique already available). The result-
ing artiﬁcial cell was named Mycoplasma mycoides JCVI-syn1.0. The new cell was
capable of self reproduction (a colony with blue color was obtained) and, as ex-
pected, it expressed the same proteins of the M. mycoides. In conclusion, the main
relevant facts were that: i) the genome of M. mycoides JCVI-syn1.0 was synthet-
ically assembled (with small changes), ii) M. mycoides JCVI-syn1.0 was a living
organism, iii) M. mycoides JCVI-syn1.0’s life was driven by the replaced genome
which proved to be the real “operating system” of the cell. The crucial point of
the experiment was the M. mycoides JCVI-syn1.0’s start-up. In fact, as we know
from computer operating systems, in order to make a system really operative for a
machine, a small set of instructions has to be executed which passes control to the
operating system, by activating its execution. This is a key point of the potency/act
passage, and it is not clear, as far as we know, what is the biomolecular basis of this
delicate passage. Of course, other steps need to be realized for a complete acquisi-
tion of the conceptual framework underlying this experiment of synthetic biology,
but in any case, this seems to be a great achievement toward a comprehension of life
by means of synthetic biology.
Cell genomes strongly resemble computer operating systems. Even in terms of
digital size, there is a remarkable similarity between the number of bytes of genomes
of complex organisms and the number of bytes of modern computer operating sys-
tems. If it is true that genomes establish how cells work, it is also true that the ways
they work are not completely determined by them. Therefore, the problem remains
of a better understanding of the relationship between these programs and their execu-
tion. In fact, the same genomes (genotypes) can produce different observable aspects
(phenotypes). Surely, the genetic determinism is not so strict as the analogous deter-
minism between computer programs and their executions. This suggests the research
of models which could disclose a more complex dialectics relating genotypes with
phenotypes. This is a key point for another aspect that is fundamental in life strategy:
the relationship between programmability and evolvability. The expressions of
genes (the programs executed in a given situation) result from a complex network of
interactions where external inputs play a crucial role. This means that, analogously
with complex computer programs, many conditions on control variables inﬂuence
the kinds of executions which can be performed. The simplest way of imagining
such a phenomenon is based on instructions which, at runtime, instantiate the value
of certain variables by values taken from external input channels, therefore, if condi-
tional instructions such as “if X= ...then do ...else ...” are present in the program,
then the contingent value of such control variables can dramatically change the ob-
servable program executions. Everything which inﬂuences the realization of genetic
programs, but is not directly related to genes, is deﬁned as epigenetics. However,
this is only a negative deﬁnition, which does not say anything about the internal
mechanism which underlies the phenomenon of gene expression and regulation and
the way phenotypical patterns emerge from them [130, 153, 144, 119, 121, 132].
The model based on the external variable checkpoints is probably too simpliﬁed.

196
4 Life Strategies
The essential point here is probably related to some forms of biological memories
which are not genetically driven. Of course, different phenotypesdetermine different
individuals, and different individuals determine different ways which the natural se-
lection can act on them. However, are there some kind of execution memories which
add kinds of articulation to this basic evolutive dynamics?
These questions raise, in biological terms, a problematic which relates to well-
known philosophical debates going back to Aristotle, full of implications with many
contexts and disciplines. In this sense, the dichotomy genetics/epigenetics relates
to Aristotle’s potency/act or potentiality/actuality distinction, and more recently to
Ren´e Thom’s salience/pregnance in the form emergence and development [156].
A form emerges in a context which is necessary to its identiﬁcation. An object in
space is a form of discontinuity, as different from what is external to it, but at the
same time, a form becomes a center of pertinent relations which determine its func-
tion giving sense to its existence. If genes encode forms, the functions they activate
are related to the forms they provide, after transcription and translation, and to the
forms which are involved in these processes. Here an essential difference with pro-
grams is due to the conformational, geometric, and physical character of the forms
these programs realize. In this sense the dialectics genetic/epigenetic links naturally
to morphogenesis, embryogenesis, and developmental mechanisms of form emer-
gence in biological organisms. Perhaps, a uniﬁed logic underlies these phenomena,
and discovering the main rules of this logic will be a great gain in knowledge of life.
An important ﬁeld of application of synthetic biology is the so-called gene ther-
apy. This means synthetic methods that transform portions of genomes, aimed
at contrasting or blocking pathological phenomena. In particular, oncolytic aden-
oviruses are an emerging therapeutic approach for cancer [124, 123, 120, 159, 157,
135, 136]. The general schema of this method is based on the speciﬁc relationship
between a gene and its promoter region. An activation regulative conﬁguration is
realized in the promotion region of a gene, in terms of protein and regulative RNA
fragments binding to sites (at the end of a complex network of intermediate levels,
where for example, A promotes B that inhibits C, and so on, through a chain of ef-
fects and counter-effects). When this happens, then the transcription process of the
corresponding gene starts, in several phases, from the DNA unpacking to the gene
transcription into pre-mRNA (performed by RNA-polymerase enzyme of a speciﬁc
type).
Let us consider an adenovirus A having a gene X controlling the virus prolifera-
tion in the infected cells (causing their destruction). The cancer cells are the selective
target of the therapy, by avoiding, at same time, the viral effect against the normal
cells. Let us assume that some cancer cells express a speciﬁc protein, for example
a marker of cancer proliferation. Let us suppose also, for the sake of simpliﬁcation,
that this protein occurs only in these cancer cells. Let Y be the gene that expresses
this marker protein. Now, let us replace the promoter region P(X) of the gene X of
A with the promoter region P(Y) of the gene Y.
In this synthetic virus A′, the gene X is promoted by P(Y). Therefore, if a pa-
tient with cancer is infected by the engineered virus A′, then it remains silent in
the normal cells, because its promoter is absent and the regulative conﬁguration

4.5 Life Analysis and Synthesis
197
Fig. 4.9 The mechanism of oncolytic gene therapy
promoting Y is not active in these cells. But, when the engineered virus A′ infects
cancer cells, then the biomolecular setting of these cells makes active P(Y) that pro-
motes X. This means that virus A′ become aggressive and its destructive activity
kills the cancer cells hosting it (see Fig. 4.9). Of course, this is only an ideal be-
havior, but the logic of this mechanism, apart from its enormous clinical interest,
discloses general principles of a crucial process of life.
In the case of clinical applications, unfortunately the expression of gene Y is not
exclusive of cancer cells. A way for improving the selectivity of viral attack [136]
is based on post-transcriptional modiﬁcation, by using the mechanism of RNA si-
lencing [119, 153, 130] of small fragments of RNA that interfere with transcripts
(by pairing with portions of them) by contrasting or blocking the translation process
of mRNA into proteins. In fact, let us choose some micro RNAs (miRNA) that are
speciﬁc to some cells, say liver cells, which we want to protect from the virus attack,
but where Y is produced, thus P(Y) is active. If many copies complementary to these
RNA strands are inserted in the 3′-UTR region (Untranslated Terminal Region) of
the gene X of A′, even if the transcript of X is translated when A′ is in the liver cells

198
4 Life Strategies
(where the P(Y) region is active), nevertheless when their terminal region is recog-
nized by the liver speciﬁc miRNA, they activate a silencing process that prevents the
translation of this transcript, therefore the gene X cannot be expressed. Silencing is
a very complex process, studied some time ago in plants, which is very important in
gene regulation and defence against viral infections (often based on double-stranded
RNA vectors). Many biomolecular elements and complexes are involved in RNA si-
lencing (RNAase, Dicer, Risc-complex, and argonaute protein), which are activated
by the double RNA strands of miRNA paired to complementary target regions, and
eventually cleaved in the process.
4.6
Life Evolution
The following are Darwin’s main arguments at the basis of his evolution theory
[125]. On putting them together, there follows a confutation of the ﬁxity of biolog-
ical species over time (a mathematical analysis of natural selection was developed
by Fisher [131] who was also one of the founders of mathematical statistics).
• Geology
Geology tells us that earth changes and consequently life environments can differ
dramatically in different epochs.
• Fitness
Malthusian growth of biological populations shows that, when there is a compe-
tition for resources, individuals with characters which better ﬁt the environment
have more chance of surviving.
• Heredity
Genealogical evidence and breeders’ experience show that the characters of par-
ents are transmitted to the offspring (at that time, Darwin was not aware of
Mendel’s experiments).
• Chance
The appearance of characters which are selected is not forced by their increased
ﬁtness, but they occur freely by chance, and then they are kept by natural
selection.
The Geology argument was an important start of Darwin’s reﬂection and its main
value was the correct temporal scale on which the dynamics of biological charac-
ters should be considered. Arguments Geology+Fitting imply that species change,
while Heredity+Chance imply how they do it. Darwin developed a theory where the
four statements above prove to be integrated in a coherent conceptual framework,
which he supported by means of the ﬁndings he collected during his famous scien-
tiﬁc journey. But, it is important to stress that the motivations for data supporting
his claims followed the logical analysis he started from. In this sense, we can com-
pare the intellectual strength of Darwinian theory of evolution, to Galieo’s mental
experiment about falling bodies, which marked the birth of modern mechanics.
It is surprising that in the Geology+Fitting+Heredity+Chance argument, hered-
ity, which is the mechanism for transmitting biological information, represents also
the way in which biological innovation is realized by means of chance. Maybe, as

4.7 Time’s Arrow and Complexity
199
Einstein said, God does not plays dice, but surely Nature does it. Evolutionary dice
are Mutation and Recombination, they are the tools for exploring biological possi-
bilities. These two golden rules can be implemented in many ways, for example, by
replication errors and by sexual mating. However, what is revolutionary in Darwin’s
approach is that the free creative role of chance transforms a feature of individual
continuity into a source of innovation for species. By reversing Lamark’s perspec-
tive, Darwin disregards any passage from the individual biological experience, to
the species. The biological memory gained by the species passes to the individuals
(by means of genomes, in modern terms), but the reverse does not apply. This means
that if one animal acquires a long neck by trying to eat the best leaves, which reside
on the top of some high tree, then this character does not pass to his offspring, un-
less it is written in his biological memory since his birth. In fact, the giraffe got a
long neck not because of the attempts he or his parents made, but on the contrary,
the long neck occurred, by chance, in his genetic characters, and for this reason this
animal had a better chance of surviving in places where high trees (with good leaves
on the top) are more available than other kinds of vegetable foods.
Species are abstract entities which extract biological population characters
present in reproductive lineages. They are archetypal forms of life, which are sub-
jected to their own dynamical laws, Species originate when a sufﬁcient number of
characters is cumulated (a clear understanding of what sufﬁcient means, is matter of
investigation), which determines a reproductive lineage which separates it from the
lineage where it occurs. It is very difﬁcult to ﬁnd rigorous deﬁnitions and models
of these intuitions. From Linnaeus’s approach to the classiﬁcation of living organ-
isms, to the modern molecular approach based on genome sequencing, and to the
recent proposals of genetic barcoding, enormous problems remain for a clear un-
derstanding of the relationship between individuals and species. Perhaps, a better
solution to this problem has to cope with more complex relations of membership
between instances and genera, within a gamma of levels, where sharp and univo-
cal classiﬁcations need to be abandoned. This is, in many aspects, a mathematical
and computational problem, where phylogenetic distances, clustering of strings, se-
quence ﬁngerprints, and genomic informational indexes could be applied in very
speciﬁc and sophisticated ways.
4.7
Time’s Arrow and Complexity
Time is experienced directly by living organisms as the dimension along which
life dynamics develops with an evident irreversible character. A life starts with a
birth and ends with a death. Maybe humans are the only living organisms with
a clear awareness of this, but no living organism can escape this law. Life ﬂows
along a direction going from birth to death and all life events are arranged in this
order. This notion of time is postulated by Darwin’s evolution theory and, along
the course of time, life follows a process going from the appearance of a prebiotic
stage to the complex multicellular organisms, and to the complex societies of human

200
4 Life Strategies
civilizations. This scenario is so familiar to all of us, that it can hardly be surprising
for the layman.
However, from a scientiﬁc point of view the same scenario raises a paradox so
difﬁcult to analyze and explain, that the most well versed minds of all times were
very often lost in the enormous logical difﬁculties related to time’s arrow. This
paradox links two scientists who enormously inﬂuenced their disciplines, biology
and physics, between the mid 19th and 20th centuries: Charles Darwin and Ludwig
Boltzmann. The ﬁrst chapter of Wiener’s famous book, Cybernetics, is entitled New-
tonian and Bergsonian time and a problematic conciliation is discussed between the
two opposite notions of time in biology and physics. A conciliation which surely
exists, because biology cannot falsify physics on which it is surely based.
The time of classical physics, which successfully explained star and planet mo-
tions, and which is the basis of the modern science inaugurated by Galileo, is a time
where laws are expressed by equations, and therefore where no privileged verse can
be chosen. But people are getting older, and no reliable method has been discov-
ered, so far, for stopping or inverting the aging process. A notion of irreversibility
appeared in physical phenomena concerning thermodynamics. The second princi-
ple of thermodynamics establishes an energy degradation principle pushing isolated
systems, toward states where thermic energy is uniformly distributed. This raised
the problem of relating this principle with classical Newtonian physics, where no
time arrow is postulated. Boltzmann, surely inﬂuenced by Darwin, dedicated his
life to the explanation of the thermodynamical arrow of time. Boltzmann’s research
was greatly opposed by the leading scientiﬁc culture of his time, and the mathemat-
ical proof he presented was criticized in a very aggressive way, surely because of
some technical inadequacies, but essentially because of its revolutionary scientiﬁc
meaning. The intrinsic logical difﬁculty of Boltzmann’s theory was put in evidence
by Henry Poincar´e (we cannot accept an argument where the conclusion is in a
clear contradiction with its premises). Surely, Boltzmann was on the edge of the
paradox, but he was right, as the following science conﬁrmed his intuition. More-
over, the physical theory that he founded, statistical mechanics, is a masterpiece of
all the physics of the 20th century.
A more subtle paradox is related to the two kinds of time of classical mechanics
and of biology. In fact, even if the Boltzmann theory is accepted and thermody-
namics is conciliated within Newtonian mechanics, even in this case, how can we
explain the dichotomy between the thermodynamic arrow and the time of biologi-
cal evolution, where living organisms evolve, against the order degradation required
by the second principle of thermodynamics? In other words, even if time’s arrow is
explained, two opposite arrows rule inorganic and organic phenomena, respectively.
Prigogine’s theory of non-equilibrium thermodynamics tries to explain this appar-
ent contradiction, but no deﬁnitive explanation seems to be clearly settled. In the
following, we will try to explain the relationship between time and complexity. We
will develop arguments and (numerical) experiments showing that time’s arrow is a
consequence of statistical complexity. In the analysis that we develop some crucial
interactions appear among time, complexity, chance, and information.

4.7 Time’s Arrow and Complexity
201
The H-theorem was proved by Boltzmann in 1872. It introduces the function
H, which appears to predict an irreversible increase in entropy, despite the micro-
scopically reversible dynamics of thermodynamical systems. This conclusion was
considered paradoxical by many authoritative thinkers of that time. But many criti-
cisms missed some deep aspects of the question. The main point is that in a popula-
tion dynamics, resulting from a huge number of individual dynamics, some proper-
ties emerge which are new with respect to the simple additions of the individual ef-
fects. In other words, in molecule populations a temporal asymmetry emerges which
breaks the time symmetry. Today, many proofs are available of the H theorem. In
this section we will deﬁne and interpret some numerical experiments which conﬁrm
Boltzmann’s conclusion and suggest the statistical and informational character of
the thermodynamical arrow of time.
Given a partition of an interval in the real line, into subintervals of size τ
(the discretization approximation), we call (discrete) distribution any ﬁnite pop-
ulation of (real) values, which are considered indiscernible when they belong to
the same subinterval of the partition. A distribution, can be considered a (dis-
crete) random variable where the internal multiplicities (how many values fall in
each interval) provide the probability distribution of the random variable. In fact,
a distribution of interval values v1,v2,...,vm with multiplicities k1,k2,...,km re-
spectively, where k = k1 + k2 + ... + km, determines the probability distribution
p1 = k1/k, p2 = k2/k,..., pm = km/k.
Given a value distribution, then (discrete) Boltzmann’s function H for this pop-
ulation has the following deﬁnition, where m is the number of subintervals which
partition the distribution interval, and ni is the number of values belonging to the
i−th subinterval.
H =
m
∑
i=1
ni lgni.
The H function is related to Shannon’s entropy S (already introduced by Gibbs
for thermodynamical systems), and apart from additive and multiplicative constants
they are functions of the same kind, but with opposite signs. Therefore if H de-
creases, then S increases. Both H and S easily extend to continuous variables and
to continuous probability distributions by replacing sums with integrals. Very often
symbol H denotes Shannon entropy. To avoid confusion, in this section, we reserve
symbol H to Boltzmann’s function and S to Shannon’s entropy.
Let us consider an ideal gas where molecules can be assimilated to balls and
their collisions are elastic collisions where the impulse and energy conservation
laws hold. We can describe the evolution of such a gas in a simpliﬁed abstract two-
dimensional setting. In fact, let us start from a distribution of values representing
molecule velocities. Then, apply the following evolution rules which abstractly cor-
respond to molecule collisions.
It is easy to realize that the game described in Table 4.6 represents, in a popula-
tion of molecules, the collision of two molecules (two-dimensional balls), along
a collision line (passing through the centers of the two balls), which, after the

202
4 Life Strategies
Table 4.6 The Pythagorean recombination game
Randomly choose two numbers a,b of the given number population;
Randomly choose a number a1, such a1 ≤a, and split a into a1 and a2 =

a2 −a2
1;
Randomly choose a number b1, such b1 ≤b, and split b into b1 and b2 =

b2 −b2
1;
Replace the pair a,b with the pair a′ =

a2
1 +b2
2, b′ =

b2
1 +a2
2.
collision, exchange the components of their velocities along the collision line, by
leaving unchanged the components orthogonal to that line.
Now if we apply the Pythagorean recombination game to a given distribution,
along a number of steps we observe two facts: 1) the H function decreases (does not
increase), and 2) the distribution approximates to a χ2 distribution, which is typical
of sum of squares of stochastic variables following normal distributions.
By using a simple MATLAB function, a series of numerical experiments were
performed, some of which are reported in Figs. 4.10, 4.11, 4.12, 4.13, and 4.14.
We claim the validity of the following statement which corresponds to an abstract
formulation of the H theorem.
Boltzmann’s function H decreases, or does not increase, during the
Pythagorean recombination game.
In particular, the H theorem follows from the Maxwell’s velocity distribution
theorem, by means of statistical and informational arguments. In fact, the normal
distribution is the distribution holding for casual phenomena where a great num-
ber of independent causes inﬂuence their evolution. Therefore, it is reasonable to
expect that after a great number of collisions the velocity distribution of each com-
ponent follows a normal distribution. Moreover, given the nature of the collisions
which follow the conservation principles of elastic collisions, we can assume that
the variance of the distributions remains constant during the evolution. In fact, the
sum of squares of two colliding “values” does not change after the application of a
Pythagorean recombination (the velocities they represent exchange one component,
see Table 4.6).
Therefore, starting from an initial value distribution, by applying the Pythagorean
recombination game, we get distributions which keep the same variance of the ini-
tial probability distribution. Moreover, these distributions are the sum of the distri-
butions of the two Pythagorean components, which for reasons of symmetry have
the same variance.
From information theory, we can deduce that, given a random variable Z such
that:
Z =

X2 +Y 2

4.7 Time’s Arrow and Complexity
203
if X and Y are independent random variables, from the deﬁnitions of entropy, joint
entropy, conditional entropy, and random variable independence, it follows that the
entropy of Z is the sum of the entropies of its components X and Y.
According to Shannon’s information theory, the entropy of probability dis-
tributions with a given variance reaches the maximum value in the normal
distribution having that variance (see Table 4.8 for a proof in the continuous
case). Therefore, the joint probability distribution of Z =
√
X2 +Y 2 reaches its
maximum, when both components X and Y reach their maxima. As we have
shown above, the Pythagorean recombination game transforms a distribution
in another one having the same variance, therefore if the distributions of the
two Pythagorean components of these distributions evolve toward normal dis-
tributions, then, along the game, the H function evolves toward its minimum
value (and S toward its maximum value).
The entropy of a normal distribution is 1
2 ln(2πeσ2), as deduced in Table 4.7.
Table 4.7 The entropy of the Gaussian distribution N of mean 0 and variance σ2
S(N) = −
 +∞
−∞N(x)lnN(x)dx
=
 +∞
−∞−N(x)ln e−x2
2σ2
√
2πσ2 dx
=
 +∞
−∞−N(x)[−x2
2σ2 −ln

2πσ2]dx
=
 +∞
−∞
N(x)x2
2σ2 dx+ln

2πσ2
 +∞
−∞N(x)dx
= E(x2)
2σ2 +ln

2πσ2 ·1
= 1
2 + 1
2 ln2πσ2
= 1
2(1+ln2πσ2)
= 1
2(lne+ln(2πσ2))
= 1
2(ln(2πeσ2)
The entropic divergence (or Kullback-Leibler divergence) between two proba-
bility distributions p,q, denoted by D(p ∥q) is given by:

204
4 Life Strategies
D(p ∥q) = ∑
x
p(x)log p(x)
q(x)
We know from information theory [198] that entropic divergence D between any
two probability distributions is never negative. Moreover, it can be extended to con-
tinuous probability distributions in a natural way by setting (while keeping its non-
negativity property):
D(p ∥q) =
 +∞
−∞p(x)ln p(x)
q(x) dx.
On the basis of non-negativity of D, Table 4.8 shows that the normal distribution of
variance σ2 has the maximum entropy in the class of the probability distributions
with the same variance.
Table 4.8 The continuous entropy of distributions with variance σ2 reaches the maximum
value for the normal distribution of variance σ2 (f denotes any probability distribution of
variance σ2)
D(f ∥N) =
 +∞
−∞f (x)ln f (x)
N (x)dx
N(x) =
1
√
2πσ e−x2
2σ2
=
 +∞
−∞f (x)ln f (x)dx−
 +∞
−∞f (x)lnN(x)dx
=
 +∞
−∞f (x)ln f (x)dx−
 +∞
−∞f (x)ln e−x2
2σ2
√
2πσ2 dx
=
−S(f )
−
 +∞
−∞f (x)lne−x2
2σ2 dx
+
ln

2πσ2
 +∞
−∞f (x)dx
= −S(f )+
1
2σ2
 +∞
−∞f (x)x2dx+ 1
2 ln2πσ2 ·1
= −S(f )+
1
2σ2 Var(f )+ 1
2 ln(2πσ2)
Var(f ) ≤σ2
≤−S(f )+ 1
2 + 1
2 ln(2πσ2)
= −S(f )+ 1
2(lne+ln(2πσ2))
D(f ∥n) ≤−S(f )+ 1
2 ln(2πeσ2)
but
D(f ∥N) ≥0
therefore
S(f ) ≤1
2 ln(2πeσ2)

4.7 Time’s Arrow and Complexity
205
Fig. 4.10 Initial distribution, Interval 100–200, population size 10000
Fig. 4.11 1000 steps with 200 collisions per step, applied to the distribution of Fig. 4.10 with
approximation level of 15

206
4 Life Strategies
Fig. 4.12 2000 steps with 200 collisions per step, applied to the distribution of Fig. 4.10 with
approximation level of 15
Fig. 4.13 3000 steps with 200 collisions per step, applied to the distribution of Fig. 4.10 with
approximation level of 15
Fig. 4.14 4000 steps with 200 collisions per step, applied to the distribution of Fig. 4.10 with
approximation level of 15

4.7 Time’s Arrow and Complexity
207
In conclusion, the Pythagorean recombination game pushes the system toward
the maximum of entropy, because the more the game goes on, the more velocities
approximate to normal distributions, which maximize the entropy, by keeping the
initial variance (interactions casually increase and velocities depend on an increas-
ing number of small independent effects). In fact, Figs. 4.10, 4.11, 4.12, 4.13, and
4.14 show that our pool of numbers (sums of squares of velocities) distribute ac-
cording tho a χ2 distribution.
This analysis shows that time’s arrow is a population phenomenon strictly related
to the complexity of systems, where the statistical and informational perspective
transforms the reversibility of individual events into an irreversible population pro-
cess. If we realize that also life is a population phenomenon based on bio-molecular
dynamics, then we easily can deduce that life postulates a time arrow. However, de-
spite this point of contact between thermodynamics and life, an essential difference
is immediately evident. The thermodynamical complexity implies an irreversibility
pushing an isolated system to an equilibrium state of maximum entropy, while life
evolution generates an increasing complexity of living organisms pushing species to
evolve toward increasing biological complexity, far from entropic maxima. There-
fore, the two arrows follow opposite directions. The research for answering this kind
of questions is crucial for a clear deﬁnition of biological complexity and for under-
standing the deep reasons on which the origin of life is based [122]. However, two
important remarks are appropriate in this context: i) living organisms cannot be iso-
lated systems, ii) in order to keep their dissipative character (assimilating from the
environment and dispersing to it) they need to be far from thermodynamical equi-
librium states. This non-equilibrium situation is the great discrimination between
organic and nonorganic organizations, and introduces the time arrow opposite to the
thermodynamical arrow, which however prevails when individual deaths occur, by
restoring matter to the organic course.
Two giants of science, Galileo and Darwin are crucially involved in the scien-
tiﬁc analysis of time. The former discovered the basis for a reliable measurement of
time. The latter introduced a “historical” perspective in the scientiﬁc analysis of life.
In fact, starting from considering living organisms according to suitable time scales,
Darwin found an interpretation key of their internal organization and development
in the framework of the evolution processes. Biological evolutions are population-
based dynamics driven by environment, chance (mutation+recombination), and
heredity. Heredity is, of course, an oriented transmission of information from a past
to a future.
As we have shown in this section, Boltzmann introduced an orientation in the
time of physics, by making the ﬁrst step in the conciliation of Galileo’s and Darwin’s
times. Certainly, other steps are necessary in this direction for a better understanding
of time and life [126].

208
4 Life Strategies
4.8
Life and Computation
Life and computation have a very long history of complex interactions. Many funda-
mental steps in the discovery of important principles and methods of computing ma-
chines are related to deep speculations about the typical natural processes involved
in the elaboration of complex information. The main characteristics of theories of
computation, developed in the last century, were the discrete character of dynamics
underlying computation systems [139], different from the continuous notions dom-
inating classical mathematical calculi, and their strong connections with biological
systems. Probably, it is not an overestimation to claim that natural computing was
the basis of modern theories of computations on which the computer science rev-
olution of the last century was performed. In the nature/computation dialectics we
may distinguish the following perspectives:
1. Computing by means of natural objects/phenomena (as in DNA Computing);
2. Computing by means of calculi inspired by natural objects/phenomena (as in
Membrane Computing or Neural Computing);
3. Computing for reproducing typical phenomena of life, with no obligation of
biological adherence (like self-replication phenomena by means of cellular
automata);
4. Computing for analyzing or predicting biological phenomena (like metabolic be-
haviors by means of MP models or growth processes by means of L systems).
About the last point above, let us remarks some important aspects. A model is either
good or bad only to the extent it helps us in predicting and explaining what we can
observe. No other criterion can be discriminant, and it is ingenuous to adopt a mirror
analogy of an absolute character. In fact, reality is different when it is considered
at different levels of observation. A priori it is very hard to chose the “pertinent
aspects” of a phenomenon and to disregard what is not relevant.
What is the adherence to reality in the physical theories at quantum levels, or
at cosmological levels? What is the reality of the probability wave in Schr¨odinger
equation? We trust them because they work. No mirror principle can assist us for
their evaluation. Models are creations of human invention. Modeling is an art, and it
cannot follow easily preﬁxed procedures. This art is based on the right guess of what
has to be observed, what relationships are relevant between the observed features,
how translate them in a chosen conceptual universe, and how to interpret the ﬁndings
which result from this translation.
Let us consider a “logical” link between life and computation. The following is
an equational version of a famous result of lambda-calculus, a formalism elaborated
by Alonzo Church in the context of a foundational theory in mathematical logic.
Consider a set S of operators where an application operation is deﬁned, which will
be denoted by concatenation. Then assume that three operators F,G,Y exist which
satisfy the following equations for every x ∈S:
Gx = F(xx)
Y = GG.

4.8 Life and Computation
209
From the previous equations the following equation holds:
Y = F(Y)
and consequently, for every n ∈N:
Y = Fn(Y).
This means that Y, also called Curry-Feys paradoxical operator, provides an inﬁ-
nite process of self-generation. The paradoxical nature of Y consists in its ability to
exhibit the property of producing a computation with the only result of producing
computations making the same task, along a non-terminating process which does
not return any deﬁnite result.
This argument shows, in abstract, algebraic terms, that application and replication
are the essential ingredients of an endless phenomenon of self-generation. However,
an important issue follows directly from this analysis. In fact, a difference between
life computations and artiﬁcial problem solving computations clearly arises. On the
one hand, the computations performed for solving problems are terminating pro-
cesses providing results when they stop. On the other hand, almost all computations
performed in life processes are aimed at keeping their livening nature over time, by
fulﬁlling some speciﬁc characteristics, but without terminating. Life in itself can be
seen as an endless phenomenon propagating in space and in time and evolving into
forms able to make this propagation ability more efﬁcient and diversiﬁed.
In the development of modern computer science, four giants had a dominant role:
Alan Turing, Nobert Wiener, Claude Shannon, and John von Neumann. Their major
contributions were related to natural computing issues, along a network of ideas
and formalisms developed by other scholars at the borders of many disciplines:
mathematics, physics, logic, linguistics, and biology.
Alan Turing was the ﬁrst scientist who elaborated in 1936 a general mathemat-
ical model of a computational device performing computations [214]. His model
was inspired by a deep behavioral analysis of mind activity of a human agent per-
forming calculations. A less known work of Turing was devoted to a mathematical
analysis of morphogenesis, as a consequence of numerical phenomena ruling the
threshold passage between different dynamical regimes described in terms of differ-
ential equations [158].
A joint work of Wiener with Rosenbluth, a Mexican physiologist, and Bigelow,
an American electrical engineer [150] (Wiener studied mathematics under Russell’s
guidance) was entitled “Behavior, purpose and teleology” and was a philosophical
starting point of his Cybernetics [160], a discipline he founded for studying, in a
uniﬁed perspective, mechanisms of information processing in animal and artiﬁcial
systems (see also [117]).
Claude Shannon founded the modern information theory in his famous book-
let published in 1948 [212], “A mathematical theory of communication”, where
the mathematical analysis of quantitative principles of digital information was de-
ﬁned. It is not well-known that Shannon studied in his PhD thesis some methods for

210
4 Life Strategies
representing genetic information, while in his master’s thesis he deﬁned the electri-
cal circuit representing boolean functions.
John von Neumann, surely inﬂuenced by Turing’s model, elaborated in 1945
the project of the ﬁrst modern computer, realizing the standard von Neumann’s
paradigm: EDVAC (Electronic Digital Variables Automatic Computer) [207]. The
circuits performing the basic arithmetical operations were based on the McCulloch
and Pitts model of neuron [9]. In this perspective, the electronic valve was seen as
the basic device for representing the neuron. In von Neumann’s terminology a neu-
ral network is a graph of E-elements. These elements are connected by edges of two
different types: excitation and inhibition edges (with a small terminal circle) and
are labeled with delays (expressed by angles in the middle of lines). A neural com-
putation is performed by a network of E-elements which transform some boolean
signals entering in input E-elements into signals exiting from output E-elements.
In Figs. 4.15 and 4.15 simple graphs of E-elements are given, from the initial
section of von Neumann’s report about EDVAC.
The thread connecting biological systems with computation can be found not
only in the ﬁrst stages of computer science, but remains a constant throughout
its evolution. (see Kleene’s famous paper “Representation of events in nerve nets
and ﬁnite automata”). L systems were suggested by plant growth, and cellular
automataare related to self-replicating systems (L systems are particular cellular
automata). Splicing systems or H systems introduced by Head in 1987 represent ge-
nomic operations in terms of string rewriting, DNA computing extends the frame-
work of formal language theory to double strings, and membrane computing extends
string manipulations to multisets (strings with commutative concatenation).
Fig. 4.15 A simple connections of three E-elements, taken from von Neumann’s EDVAC
report

4.8 Life and Computation
211
Fig. 4.16 The one-digit addition circuit, taken from the EDVAC report (its complex explana-
tion is not reported here)

Part II
Discrete Mathematical Backgrounds

Platonic Esahedron
5
Numbers and Measures
Abstract. Numbers are the essence of any mathematization process. They measure
entities, but also provide rigorous frameworks for analyzing the notion of inﬁnity;
the set of natural numbers is the ﬁrst example of inﬁnity. Finiteness and inﬁnity,
which very often appear in dialectic aporias, underly the essence of mathematics.
Numbers are essential to the notion of population, and are intrinsically related to
computation processes. In this chapter, after a brief section on sets and functions,
the essentials of numerical systems will be outlined, by emphasizing the concepts
that are most relevant for discrete structures. Then, the principle of induction, and
basic topics of arithmetic and logic will be presented. Two sections conclude the
chapter: one on series and growths (with numbers related to time, space, and matter
aggregation), the other one on basic dynamical concepts.
5.1
Sets and Functions
Sets, numbers, sequences, relations, operations, functions, and variables are the ba-
sic concepts of mathematics, intrinsically intertwined and related to the inﬁnite, a
notion that is the essence of mathematics (“The Art of the Inﬁnite” [179]). Here
we give a brief presentation of these concepts and some standard notation (see
[193, 174, 177, 187, 186, 169] for more extended or advanced presentations).
Sets or classes (in this context we use these terms as synonyms) are collections
of distinct objects. The speciﬁcation of all the elements which belong to a set com-
pletely identiﬁes it. If X is a set, a ∈X means that a belongs to X (a ̸∈X if a does
not belong to X). If a set has a ﬁnite number of elements, it is completely described
by a list, where the appearance order of the elements is not relevant, and multiple
appearances are redundant, that is, equivalent to single appearances. A set {a,b}
V. Manca: Infobiotics, ECC 3, pp. 215–271.
DOI: 10.1007/978-3-642-36223-1_5
c⃝Springer-Verlag Berlin Heidelberg 2013

216
5 Numbers and Measures
of two elements is an unordered pair. There are many ways for introducing an or-
der between the elements a,b of an unordered pair. A standard way to do it is by
means of the set {a,{a,b}}, which is usually denoted by (a,b). A triple (a,b,c) can
be identiﬁed by ((a,b),c), that is, an ordered pair where the ﬁrst element is a pair
too. In this way, any sequence of ﬁnite length can be obtained by iterating the set
construction of (ordered) pairing.
Table 5.1 collects the standard set-theoretic concepts (sometimes set difference
is also denoted by /).
Table 5.1 Fundamental set-theoretic notation
∈
Membership
a ∈B
a is an element of A (a belongs to A)
⊆
Inclusion
A ⊆B
All the elements of set A belong to set B
/0
Empty set
/0 ⊆A
/0 is included in any set A
∪
Union
A∪B
The set of elements belonging to A or B
∩
Intersection
A∩B
The set of elements belonging to both sets A,B
−
Difference
A−B
The set of elements of A which do not belong to B
×
Cartesian product
A×B
The set of pairs (a,b) with a ∈A,b ∈B
()k k-power
Ak
The set of all k-sequences over A
P
Powerset
P(A)
The set of all the subsets of A
N
Naturals
{0,1,2...}
The set of null and positive integers
Z
Integers
{0,1,2.−1,−2...}
The set of integers
Q
Rationals
{0,1/2,2/3.−1/2,−2/3...} The set of rationals
R
Reals
{0,1,1/2,
√
2,...}
The set of reals
C
R×iR
iR = {ix | x ∈R}
(The imaginary unit i =
√
−1)
5.1.1
Relations and Operations
The notion of sequence is connected with the mathematical concept of relation. For
k ∈N, a k-sequence is a sequence of k elements. A k-relation (or a relation of k
arguments) over a set X is a condition which either holds or does not hold for any
given k-sequence of elements in X.
Given two sets A,B, the set of pairs (x,y), where x ∈A and y ∈B, is the carte-
sian product of the two sets, denoted by A × B. A subset of A × B is also called a
correspondence between the sets A and B. The set of all k-sequences over a set A
is denoted by Ak. Therefore, mathematically a k-relation R over A is a subset of Ak,
in symbols, R ⊆Ak. For example, the relation ≤on natural numbers is identiﬁed by
the set of pairs:
{(x,x+ y) | x,y ∈N}.
Binary relations are an important type of relations. They are usually indicated by
a symbol put between the two objects which are related. For example, the equality
symbol = expresses a binary relation between expressions such that E1 = E2 holds
when E1 and E2 denote the same object. The order symbol ≤, when put between
two numbers, expresses a relation which holds if the number on its left is less than
or equal to the number on its right. Very often, when a binary relation does not hold
its symbol is barred, for example, a ̸< b means that a < b does not hold.

5.1 Sets and Functions
217
A binary relation R is said to be reﬂexive in a set A if aRa holds for all a ∈A. R
is symmetric on A if for every pair of elements a,b ∈A, aRb implies that also bRa
holds. R is transitive if for every triple of elements a,b,c ∈A, if aRb and bRc hold,
then also aRc holds. A binary relation on a set A which is reﬂexive, symmetric, and
transitive is called an equivalence relation on A. If ≡is an equivalence on A, then
for any element a ∈A the equivalence class of a is the set [a]≡deﬁned by:
[a]≡= {b ∈A | b ≡a};
the quotient set A/≡of A with respect to ≡is the set of equivalence classes of A:
A/≡= {[a]≡| a ∈A}.
A binary relation R on a set A is an order relation if it is reﬂexive, transitive and
antisymmetric, that is for every a ̸= b ∈A, aRb and bRa cannot both hold. An order
relation is linear or total on A if, for any every a,b ∈A either aRb or bRa holds
(otherwise the ordering is said to be partial). Many important concepts based on
order relations can be deﬁned in general (minimum, maximum, minimal, maximal,
upper bound, lower bound, least upper bound, and greatest lower bound).
The number of sequences over an alphabet of symbols grows exponentially with
their length. For example the number of possible different sequences of length ten,
over an alphabet of twenty symbols is 2010, that is more than ten trillions. In fact
in any of the ten positions we can put one of twenty possible symbols. This simple
observation explains why polymers are the natural way for producing an enormous
chemical variety.
An operation of k arguments over a set X is a rule that, when applied to a k-
sequence of arguments over the set X, may yield one element of X as a result. If it
does not provide any result, then it is undeﬁned on that sequence of arguments.
If we order the elements of a set X, say X = {x1,x2,...,,xn}, then any subset of
X can be expressed by a sequence of 0s and 1s where, at position i of the sequence,
the value 1 occurs in the sequence if the element xi belongs to the subset, otherwise
the value 0 occurs. From this representation it follows that the number of possible
subsets of X is 2n. Table 5.2 provides a few examples of relations and operations.
5.1.2
Functions and Variables
A (total) function from a set A to a set B is an operation, that is, a rule that applies to
all elements in A and for each of them produces exactly one result in B. The under-
lying idea of the function is that of imaging, as a representation device associating
images to given objects. The term “function” goes back to theater representations,
and its etymology is common to Latin words like fungere and ﬁngere, which roughly
refer to the action of playing a role. The standard notation for a function is:
f : A →B

218
5 Numbers and Measures
Table 5.2 Examples of relations and operations (iff means if and only if)
Father-Son relation
F(x,y) iff x is father of y
Son-Father-Grandfather relation
G(x,y,z) iff x is son of y who is son of z
Arithmetical Order
the usual enumeration order ≤over the natural
numbers
Number Divisibility
D(x,y) iff x can be exactly divided by y
Linear betweeness
B(x,y,z) iff point y is internal to the segment x,z
Arithmetical operations
+ , −,· ,/
Area measure
σ(P) is the area of a polygon P
Number Factorization
f actor(n) is the set of prime numbers dividing n
Sequence occurrence
α(i) is the element at position i in sequence α
Sequence Length
|α| is the length of α
and if x ∈A, then f(x) is the image of x, which belongs to B (A is called domain of
f, while B is called codomain of f). The properties of injectivity, and surjectivity
are expressed by the following formulae (where ⇒is the logical implication):
x ̸= y ∈A ⇒f(x) ̸= f(y) (injectivity)
y ∈B ⇒y = f(x) for some x ∈A (surjectivity)
bijectivity of f means both injectivity and surjectivity, that is, a correspondence one-
to-one between the set A and the set B. The imaging principle realized by functions
is one of the most powerful principles in mathematics, which can be found in a wide
variety of situations. In fact, a typical approach in mathematics is the representation
of certain objects (for example, physical states) by means of other objects, very
often more abstract (for example, numbers), in order to derive useful information
about the former ones by working with the latter ones. In some contexts it is useful
to consider partial functions, that is, functions that can be undeﬁned on some el-
ements of their domains. However, unless explicitly stated, functions are implicitly
assumed to be total, that is, always deﬁned on their domain.
Variables are a crucial device of mathematical language. Any general statement
in mathematics uses variables implicitly or explicitly. For example, a geometrical
proof about triangles, begins with a preamble of such a type: “Given three points P,
Q, R, and a line passing through P ...”. In fact, in order to show any relationship
of general nature, it is essential to deal with “generic” elements. Generality means
variability, because a point P can be instantiated with any speciﬁc point when a
deﬁnite context is selected. Variables were essential for the development of algebra.
In fact, when we establish some conditions holding for a number, but we do not
know the exact identity of this number, we use a variable to denote it, and in this
sense that variable corresponds to a an unknown value. A solution of an equation in
one variable corresponds to the determination of a value of the variable that satisﬁes
the equality between the left hand side and the right hand side of the equation.

5.2 Numbers and Digits
219
Very often, different names for indicating variables relate to different levels of
variability, for example: parameter, indeterminate, unknown.
The essence of the notion of variable is that of an entity taking values within a
range of variability, which can be speciﬁed by a set. The simplest kind of variables
are boolean variables, ranging over a set of two values (usually denoted by 0,1). An
important issue concerning variables, attains to the relationship between variables.
For example, let X and Y represent the measures of two quantities related to some
bacterial cells (say, the quantity of a nutrient in the environment and the quantity
of a protein inside the cell, respectively). Let us assume we discover that, in certain
conditions, the value of Y depends on the value of X. In this case we say that Y
is a dependent variable with respect to X, and this dependence deﬁnes a function
f : A →B, from the range A of X to the range B of Y: for any a ∈A, f(a) = b when
b is the value taken by Y in correspondence to the value a taken by X.
5.2
Numbers and Digits
Numbers appear as quantity ratios when measures are considered. Natural numbers
measure sizes of populations; rational numbers measure, with approximations, the
sizes of continuous quantities.
In counting populations, a relevant aspect is the distinguishability of elements. In
fact, the possibility of counting many objects does not imply the ability of recogniz-
ing them as single objects distinguishable from each other. This situation is typical
with molecules. One may be able to ﬁnd 1000 molecules of a protein, but one can
seldom recognize each of them from another of the same group. If we measure pop-
ulations by using a standard population size of 1000 elements, say it a Kilo K (of
elements), as a unit, then we obtain fractions. For example, the size of a population
of 3200 elements is 3 K + 2/10 K, that is, 3.2 K.
Greeks had different notions of numbers. Natural numbers were “arithmoi”, frac-
tions were “megethe”, connected to the geometrical intuition of segments. The four
classical operations on numbers were deﬁned in geometrical terms (by compass and
ruler constructions over segments). “Logoi” were related to the incommensurable
ratios (e. g., between the edge and the diagonal of a square) and to the notion of
inﬁnite.
A deﬁnite, complete understanding of number systems was developed in the 19th
century, at the end of a long process of uniﬁcation of the distinct Greek notions and
of the geometric Greek perspective with the algebraic notion of number [190]. This
process evolved along the new algorithmic perspective of the positional notation,
of Hindu-Arabic origins, that was introduced by Leonardo Fibonacci in his Liber
Abaci (1202), whereby numbers are represented by sequences of digits.
Sequences over an alphabet can always represent numbers. In fact, as we will
explain later on, if the pairwise distinct elements which may occur in a sequence are
fewer than k, then a sequence may be taken as a base-k representation of a number.

220
5 Numbers and Measures
5.2.1
Natural Numbers
The set N = {0,1,2,3,...} of natural numbers can be constructed by starting from 0,
by applying iteratively the successor function, which always yields a new number:
0 →s(0) →s(s(0)) →s(s(s(0)))... .
If we represent numbers by sequences of only one symbol, say 1, then zero cor-
responds to symbol 1 and the successor operation becomes the juxtaposition of a
symbol 1 to its argument:
1 →11 →111 →111 →... .
The sum of two numbers n,m can be obtained by iterating the successor operation
n times, starting from m (s occurring n times):
n + m = s(...s(m)...).
The difference is the inverse operation of the sum, that is, n −m is the number k, if
it exists, such that m+ k = n.
The product of n,m is deﬁned by iterating the m-sum (sum of m) n times, starting
from 0:
n ∗m = (((0 + m)+ m)+ ...m))).
The division is the inverse operation of the product, that is, n
m = k if m∗k = n.
Sum and product are commutative operations, that is, their result does not depend
on the order of their arguments. Difference and division are not commutative. The
product is distributive with respect to the sum: a ∗(b + c) = a ∗b + a ∗c.
The number x raised to power of n, is deﬁned by iterating the x-product n times,
starting from 1: 1 × x × x... × x; it is denoted by xn. Number x is called the base,
and n the exponent.
When a base b is ﬁxed, then the operation yielding bx in correspondence of x is
called exponential (of base b).
The root m√n is the inverse of the m-power, that is,
m√
nm = n, while the logarithm
(of base b) is the inverse of the exponential, so that mlgm k = k.
Inverse operations are not always deﬁned on natural numbers. For example, dif-
ference and division are not deﬁned when the ﬁrst number is smaller than the second
one. The extension of naturals to the set Z of integers guarantees that difference is
always deﬁned. The extension of integers to the set Q of rationals (fractions) guaran-
tees that division is always deﬁned for any pair of rationals (apart from any division
by zero, which is always undeﬁned). Square root is not always deﬁned on positive
rationals, as mathematicians of Pythagoras’ school discovered (no rational num-
ber can equal
√
2). The extension of rationals to the set R of reals guarantees that
square root is always deﬁned on positive reals. However, square roots (in general,
even roots) of negative reals cannot be real numbers. The extension of reals to the
set C of complex numbers guarantees that roots are always deﬁned.

5.2 Numbers and Digits
221
Iteration and inversion are the two basic mechanisms to deﬁne operations on
numbers, starting from the basic operation of successor which generates the inﬁnite
sequence of natural numbers. As we have just seen, sums are iterations of successor,
products are iterations of sum, and powers and exponentials are iterations of prod-
uct. Differences are inverse operations of sums, divisions are inverse operations of
products, and root extractions and logarithms are inverse operations of powers. The
extensions of the number sets: N ⇒Z ⇒Q ⇒R ⇒C guarantee that inverse oper-
ations of difference, division, and root extraction are always deﬁned.
It is interesting that the inverse operations can be also obtained by a chain of
iterations, using the operation pred, for predecessor, which is the inverse of succes-
sor. In fact, difference can be obtained by iterating predecessor, division by iterating
difference, and logarithm by iterating division.
Usual symbols for numbers came from Magreb. In fact, Leonardo Fibonacci
(1180–1250), who introduced them in Europe in his famous book Liber Abaci, was
son of a merchant from Pisa and studied near Tunis. These digits are compact forms,
drawn by a continuous line, where the number of angles corresponds to the numer-
ical value of each digit (zero is a circle without angles), see Fig. 5.1. Hindu-Arabic
positional number representation had an enormous impact on all the western cul-
tures: it can be compared with the discovery of phonetic alphabets (the birth of ef-
ﬁcient writing systems). Algorithms for numerical elaboration based on positional
representation were the beginning of a mathematical attitude from which modern
algebra stems, with systematic methods for solving equations.
Fig. 5.1 The angles of Arabic digits
The usual representation of numbers with decimal digits expresses a number, say
357, as a sum of powers of 10: 357 = 3∗102+5∗101+7∗100. It is possible to show
that this representation is univocal for any natural number, provided the leftmost
digit is nonzero. Moreover, the method may be applied with any base greater than
1. For example, the same number, with respect to base 8, becomes 545, because
357 = 5∗82+4∗81+5∗80 = 357. A more general and compact way for expressing
this kind of notation is given in the next subsection.
5.2.2
Sums and Positional Representations
The sum operation can be extended to any sequence of numbers, by using the sum-
mation symbol ∑:

222
5 Numbers and Measures
n
∑
i=1
ai
standing for a1 +a2,+...+an. The extreme values can be indicated in many ways,
for example:
∑
i=1,n
ai
or similar, clearly understandable, notations. The following equation expresses the
appearance of the sum variable (its identity is not relevant).
n
∑
i=1
ai =
n
∑
j=1
a j
other properties of ∑follow from the properties of sum operation:
n
∑
i=1
ai =
k
∑
i=1
ai +
n
∑
i=k+1
ai
n
∑
i=1
(ai + bi) =
n
∑
i=1
ai +
n
∑
i=1
bi
n
∑
i=1
bai = b
n
∑
i=1
ai
m
∑
j=1
b j

n
∑
i=1
ai

=
n
∑
i=1
ai

m
∑
j=1
b j

=
n
∑
i=1
m
∑
j=1
aib j =
m
∑
j=1
n
∑
i=1
aib j.
Given a natural number b > 1, called base, for any natural number n, there exists a
natural k such that the following univocal representation of base b holds:
n = ∑
i=0,k
cibi
where ci < b for i ≤k and ck > 0. Therefore, sequence (c0,c1,...,ck) (usually writ-
ten in the reverse order) univocally identiﬁes, with respect to base b, the natural
number n. When b different symbols, called digits, are chosen to represent the num-
bers smaller than b, then any sequence of digits, with ck ̸= c0, is a representation of
a natural number.
All classical algorithms to compute the four arithmetical operations can be gen-
eralized to any base, when the tables for products and carries are given for any pair
of values smaller than b (Pythagorean tables).
Other positional representations different from the decimal one are: the binary
representation (introduced by Leibniz), the octal and hexadecimal representations
(basis 8 and 16 respectively), which, besides the binary representation, are mostly
used in computer representations of numbers.

5.2 Numbers and Digits
223
Representations of base 60 and 20 are found in ancient civilizations, the Assyro-
Babilonyan and the Maya, respectively. The latter is a positional representation
proper, for it also had a symbol for zero. This is an essential aspect of positional
representations.
The word zero comes from an Arabic root for zephyr (a gentle wind, or some-
thing ineffable). Zero is the ﬁrst great abstraction of modern algebra (the empty
set is its set-theoretic equivalent). It is essential in positional notation and is the
basis for computations with big numbers, a problem that Greek mathematics ad-
dressed with Archimedes’ Arenarius, but without a complete and deﬁnite solution.
However, the informational power of zero and of positional notation relies on the
reduction of numbers to sequences of digits. It seems noteworthy that the etymology
of the word algorithm is rooted in the positional representation. It is used to mean
the computational procedures (on representation of numbers) after al-Khwarizmi’s
seminal work, which was translated into Latin (several versions in the 12th and 13th
centuries) and published with title “Algorismi de numero Indorum” (in the 19th
century) [170, 180, 167]. This was the beginning of the algebraic methods for elab-
orating numerical information.
5.2.3
Integer Numbers
The set Z of integer numbers was obtained from N to express negative quantities.
An integer number is a pair of a sign and of a natural number providing an absolute
value (usually, the sign is omitted when positive). The extension of the four fun-
damental operations to the integers is straightforward, except for product, which is
obtained by the product of the absolute values equipped with a sign according to
the rules given in Table 5.3, which, at a ﬁrst glance, may appear somewhat arbitrary.
The rules of product sign are necessary for having an extension of the product op-
eration to integers that is coherent with the product deﬁned on naturals. It is easy to
accept the ﬁrst two rules of Table 5.3, what about the third rule?
Let us evaluate the product (−k)(−h). We have (−k) = [n−(n+k)] and (−h) =
[m−(m+ h)] for any pair of natural numbers n,m, therefore:
(−k)(−h) = [n −(n + k)][m−(m+ h)]=
nm−n(m+ h)−m(n+k) ?(n + k)(m+ h) =
nm−nm−nh −mn−mk ?(nm+ nh + km+ hk).
Table 5.3 The sign multiplication rules
+∗+ = +
+∗−= −
−∗−= +

224
5 Numbers and Measures
The symbol ? stands for the sign to assign to the product between −(n + k) and
−(m+h). But, in order to get a result which does not depend on n,m, the sign ? has
got to be +, thereby yielding (−k)(−h) = kh.
5.3
Rational and Real Numbers: Approximation and Inﬁnite
Rational numbers are fractions of integers. When we add a “part of the unit” to
a natural number, we get a rational number. For example, 3 plus 1/7 is equal to
21/7+1/7, that is 22/7. All positive fractions can be viewed as the sum of a natural
number, possibly zero, plus a fraction of the unit. Within fractions, with positive or
negative sign, all the four arithmetical operations are always deﬁned (apart from
division by zero, always undeﬁned). Fractions of the unit can be represented in
decimal notation by sums of negative powers of ten. For example, 1/8 is equal to
1×10−1+2×10−2+5×10−3, that in the usual decimal notation is written as 1/8 =
0.125. However, in some cases these sums are inﬁnite. For example, 1/6 = 1 ×
10−1 + 6 × 10−2+ 6 × 10−3 + ···, that is, in the usual decimal notation it is written
as 1/6 = 0.¯6, meaning that digit 6 is inﬁnitely repeated. Therefore fractions provide
either ﬁnite sequences of decimal digits, separated by a dot (or a comma or some
symbol different from digits) to distinguish the integer part from the fractional part,
or periodical sequences ending with an inﬁnitely repeated subsequence of digits.
It is possible to show that ﬁnite and periodical sequences are the only two kinds
of decimal representations associated to fractions. In fact, the decimal notation of
a fraction is obtained by applying the usual division algorithm learned at primary
school (with many variants, but with the same essential procedure). According to
this algorithm, at each step, a difference is calculated between two decimal num-
bers, and then a digit of the greater number is appended to this difference, by pro-
viding a number of at most n + 1 digits, if n is the number of digits of the divisor.
The comparison of this number with the divisor provides a new digit of the result.
But, the possible decimal sequences of n + 1 digits are 10n+1, therefore surely be-
fore than 10n+1 + 1 times, the same case must occur twice. The digits of the result
between these two occurrences are exactly the digits of the periodical part in the
decimal representation of fractions (ﬁnite decimal representations are a special case
of periodical representations with periodical part consisting of zeros).
The above argument raises a natural question. What kind of numbers are inﬁnite
non-periodical decimal sequences of type 0. −−−···? It is easy to realize that
the four arithmetical operations can be naturally deﬁned for them. Moreover, these
sequences have a direct reading as points of the unitary segment. If a segment is
partitioned into 10 contiguous intervals (for example, including the extreme left
point, but excluding the right extreme point), then each digit can be associated to
each interval. For example, a sequence of type 0.23 individuates the points internal
to the third part (associated to digit 2) of the unitary segment, and then internal to
the fourth part of it (associated to digit 3). In conclusion, inﬁnite non-periodical
sequences correspond to inﬁnite processes generating subintervals of the unitary
segment. These numbers are the real numbers. Here are two examples of irrational

5.3 Rational and Real Numbers: Approximation and Inﬁnite
225
numbers obtained by deﬁning inﬁnite sequences of digits that are not periodical. The
ﬁrst one is an inﬁnite sequence which cannot be periodic because 1 occurs once, 2
twice, ..., 0 ten times, 10 eleven times, and so on.
0.1223334444555556666667777777...
Another inﬁnite sequence is due to Champernowne, a mathematician of the begin-
ning of the 20th century; it has the digits of all natural numbers arranged after the
dot in their enumeration order (of course this prevents any possibility of a periodical
repetition of the same group of digits).
0.1234567891011121314151617...
5.3.1
Incommensurability, Divisibility, and Distinguishability
The way of introducing irrational numbers by means of inﬁnite decimal sequences is
related to the positional representation of numbers (see [179]). However, it does not
correspond to the historical development of the concept of number. In fact, real num-
bers were discovered by the Greeks, when no positional representation of numbers
was available. The Greek discovery of the necessity of real numbers was entirely
based on geometrical arguments that we want to present brieﬂy.
Consider a square having side of length a. Let d denote the length of its diagonal,
as indicated in Fig. 5.10. If the side and the diagonal lengths are in a rational ratio,
then a/d = n/m, where n,m are natural numbers (n < m). Therefore, the discovery
that this ratio is not a rational number implies that, if c is the length of a portion of
the side such that a = nc, even assuming the length of c as small as we want, no
natural number m can exist such that m times this portion could equal the diagonal,
that is, d = mc. For this reason, we say that the lengths of these two segments are
incommensurable. This argument seems completely counterintuitive, because it is
reasonable that at some, even very small level, a common part that is submultiple of
any two segments must exist. As we show, the mathematical power of this argument
is in its logical nature. Of course, our resolution ability has to stop at some level,
therefore the claim of incommensurability may appear meaningless. However, if we
assume a pure geometrical notion of segment where any two internal points deﬁne
a part of it, and any two points can be separated by a point between them, then the
argument holds perfectly. This argument has a pure logical nature; this is the reason
for the name ”logoi” given to numbers deﬁned by such a kind of existential argument
(”logos” means reasoning). However, despite their purely logical existence, these
numbers are fundamental for the development of the whole mathematics.
From a historical viewpoint, we remark that a lot of Greek philosophy was famil-
iar with processes of time and space divisibility. The famous Zeno’s paradoxes were
examples where speciﬁc processes of inﬁnite divisibility are taken into account. The
deep understanding of “inﬁnite” as a key feature of mathematical entities is surely
one of the most original and fruitful aspects of Greek thought (developed in several

226
5 Numbers and Measures
directions by Eudoxus of Cnidus, Euclid of Alexandria, and Archimedes of Syra-
cuse).
The following argument is a modern elaboration of a proof of the incommensura-
bility between the side and the diagonal of a square, developed by the Pythagorean
school (6th century B.C.).
Fig. 5.2 The incommensurability between the side and the diagonal of a square
A proof of geometrical incommensurability. Consider a unitary square (with
side of length one). According to Pythagoras’ theorem the length of its diagonal is
√
2, therefore if the ratio between the diagonal and the side were a rational number,
then two natural numbers p,q should exist such that
√
2 = p/q. We show that this
hypothesis implies a contradiction. In fact, without loss of generality we can assume
that no common factor exists between p and q. If
√
2 = p/q, then 2 = p2/q2, that
is, p2 = 2q2. The last equation implies that p2 has number 2 as a factor, but this
requires that also p has 2 as a factor. Therefore p = 2r for some r, that is, 4r2 = 2q2,
thus 2r2 = q2. But, by the same argument used earlier, this fact implies that q has 2
as a factor. In conclusion, both p and q have 2 as a factor, and this contradicts our
initial assumption that no common factor exists between p and q. Whence, the initial
hypothesis
√
2 = p/q implies a contradiction, thus proving the irrationality of
√
2,
because
√
2 ̸= p/q for any pair of natural numbers p,q. In conclusion, this proof
shows the incommensurability between the side and the diagonal of any square. In
an analogous way it can be shown that √p is irrational for any prime number p.
Real numbers completely express the continuous nature of the straight line and
of segments. If we consider a segment as a set of points arranged in a linear struc-
ture, then we realize that the elements of this set include points that we cannot
singularly individuate. This is a subtle argument implied by the analysis developed
by Georg Cantor in the 19th century. We do not enter in the details of this discus-
sion, we only want to remark that a continuous structure implies inﬁnite divisibility
and undistinguishability of its components. This aspect is a crucial discrimination

5.3 Rational and Real Numbers: Approximation and Inﬁnite
227
between continuous and discrete structures. Compare a segment, which is continu-
ous with a set of golf balls, which is discrete. A subset of balls can be partitioned
into smaller subsets, but if the initial set is ﬁnite, then after a ﬁnite number of parti-
tions you cannot go on because one half of a golf ball is not a golf ball. But if you
start from a segment, which has a ﬁnite size, the partition process can be inﬁnitely
performed, because one half of a segment is still a segment. The difference is in the
composition of the two structures.
However, distinguishability of objects needs a further subtle analysis. In fact,
two balls can be distinguished from three balls, but this does not mean that they
can be individually distinguished. For example, if they have the same shape and
their positions may change when you are not observing them, are you able to tell
whether they exchanged their positions, when you observe them again? Surely, you
can tell whether their number increases or decreases, but in many cases their single
individuality cannot be determined.
Let us mention that arithmetical operations on numbers, when numbers are rep-
resented by segments, are realized by using compass and ruler. Sum and difference
may be performed by putting segments on the same line, in straightforward manners,
while product and division are performed by using similar triangles, at it is shown
in Fig. 5.3. An algorithm is a procedure solving a given problem. Many famous ge-
ometrical constructions are essentially algorithms manipulating geometrical entities
to solve speciﬁc problems.
Fig. 5.3 Segment multiplication by similarity: the lengths of two parallel segments are in the
ratio of their intercepts with the same incident lines
For our further discussion we recall the famous Pythagoras’ theorem about right-
angled triangles. Figure 5.4 is essentially a proof of this theorem (due to Chinese
mathematicians).
Pythagoras’ theorem is the basis of the well-known goniometric equation: sin2 x+
cos2 x = 1. In the goniometric unitary circle, a radius which forms an angle α with
the x-axis has projections, over the two axes, sinα (y-projection) and cosα (x-
projection), which together with the radius form a right triangle. From this circle
all the important goniometric properties can be derived. Figure 5.5 is essentially a

228
5 Numbers and Measures
Fig. 5.4 Chinese proof of Pytagoras’ theorem. The area of the left and right squares L, R is
the same. The internal square of L is obtained by removing four triangles from L. These four
triangles are equivalent to the two rectangles removed from the right rectangle R. Therefore,
the areas of the internal ﬁgures of L and R are equal
proof of the formula providing the sine of the angle α +β. Angles are measured by
radiants, where a radiant is the angle corresponding to an arc with the same length
of the radius (so that 2π is the measure of a whole rotation, of 360 degrees). The
segment between double circles is sin(α +β). It is the sum of two projections (bold
lines). In fact, the segment between the small and big full circles is cosβ, and its
vertical projection (the smaller bold segment) is sinα cosβ, while the segment be-
tween the double circle and the big full circle is sinβ and its vertical projection (the
bigger bold segment) is sinβ cosα. The sum of the two projections (bold lines) is
sin(α + β).
The number
√
2 is irrational, but it is the root of the equation x2 = 2, that is, it
is algebraic, while the number π cannot be obtained as root of some equation with
rational coefﬁcients. Irrational numbers which are not algebraic are called tran-
scendental. However, there are many formulae and algorithms for generating all
the digits of
√
2, as well as all the digits of π. Real numbers for which procedures
are known which generate the sequence of their digits are called computable real
numbers. When the notion of computable number was deﬁnitely clariﬁed, after a
seminal paper of Turing (1936), it was completely clear that computable real num-
bers form a very small (in terms which can be mathematically formalized) subset of
all real numbers. In conclusion, we can deﬁne sets having elements which we can
collectively identify, albeit most of them have no individual identiﬁcation process.
All the sets of numbers N, Z, Q, R, C are inﬁnite. However, there is a crucial
difference between the sets of natural, integer, and rational numbers, from one side,
and the sets of real and complex numbers, on the other side. In fact, elements of
N,Z,Q are represented by suitable ﬁnite sequences of symbols, while those of R
require, in general, inﬁnite sequences of symbols. It can be shown that any inﬁnite
set of ﬁnite sequences is enumerable, that is, it can be put in one-to-one correspon-
dence with the natural numbers. This kind of correspondence cannot be established
between the natural numbers and the set of all inﬁnite sequences over a ﬁnite alpha-
bet. This means that real numbers are many more than the rationals. We can express

5.3 Rational and Real Numbers: Approximation and Inﬁnite
229
Fig. 5.5 sin(α + β) = sinα cosβ + cosα sinβ (analogously cos(α + β) = cosα cosβ −
sinα sinβ holds)
this situation with the following statement (which we do not explain formally): If
you choose randomly a point in the interval [0,1] the probability of getting a rational
number is almost zero, while that of getting a real irrational number is almost one.
Let Sk the set of ﬁnite sequences (over a given ﬁnite (ordered) alphabet) of
length k. For any k ∈N, Sk is a ﬁnite set. Enumerate in the alphabetic order
the elements of S1, S2,... In this way any ﬁnite sequence occurs at some step
of the enumeration.
The set S of all the inﬁnite sequences over a binary alphabet {0,1} cannot be
put in one-to-one correspondence with natural numbers (the same argument can
be easily generalized to the set of sequences over any ﬁnite alphabet). Table 5.4
explains the original intuition of the diagonal argument by Georg Cantor.

230
5 Numbers and Measures
Table 5.4 Cantor’s square of binary sequences
1
0
1
0
0
1
1
. . .
1
0
0
0
0
1
1
. . .
1
0
1
0
0
0
1
. . .
1
1
1
0
0
1
1
. . .
1
0
1
0
1
1
1
. . .
1
0
1
1
0
1
1
. . .
1
0
1
0
1
1
0
. . .
. . . . . . . . . . . . . . . . . . . . . . . .
No inﬁnite list of binary sequences can contain all the inﬁnite binary se-
quences. The anti-diagonal sequence, where each symbol of the diagonal is
changed, must differ from any sequence of an inﬁnite list. For example, 0 1 0
1 0 0 1 ... differs from the seven sequences explicitly indicated in the list of
Table 5.4: from the ﬁrst one in its ﬁrst digit, from the second one in its second
digit, and so on.
An argument similar to Cantor’s diagonal argument, due to Bertrand Russell, shows
that no set A can be put in one-to-one correspondence with its power set P(A). In
fact, assume, that a bijective function f(x) could provide such a correspondence
between A and P(A). Let
D = {x | x ̸∈f(x)}
then, some y ∈A must exist such that D = f(y). But, for such a y either of the
following statements leads to a contradiction:
i) y ∈D
ii) y ̸∈D.
In fact, i) implies y ̸∈f(y), because y has to satisfy the property, holding for all
the elements of D, x ̸∈f(x), and by the equation f(y) = D deﬁning y, we deduce
y ̸∈D, therefore i) implies ii). Analogously, ii) implies, by the equation D = f(y),
that y ̸∈f(y), thus y satisﬁes the property of the elements of D, that is, we deduce
y ∈D, therefore ii) implies i).
According to Russell’s argument, the powerset P(N) cannot be put in one-to-one
correspondence with N. Moreover, any subset of the natural numbers identiﬁes a
binary inﬁnite sequence s (and vice versa) when we consider the numbers in this
subset as the ordinal positions in the sequence where s takes the value 1. But, these
sequences are in one-to-one correspondence with R, therefore by Russel’s argument
we obtain another proof that the set of inﬁnite binary sequences (bijective with the
reals) cannot be put in bijective correspondence with the set of the natural numbers.

5.4 Complex Numbers and Real Vectors
231
5.4
Complex Numbers and Real Vectors
Complex numbers are sums of a real number with an imaginary number. Imaginary
numbers were discovered by the Italian mathematicians of the Renaissance, during
their study of formulas to solve algebraic equations. In fact, the two solutions of
a second degree equation ax2 + bx + c = 0 are given by the following formula (a
solution with +, the other with −):
−b ±
√
b2 −4ac
2a
.
When (b2 −4ac) is negative we get square roots of negative numbers, or numbers
multiplied by √−1. No real number exists having a negative square, therefore √−1
is meaningless in the set of real numbers. However, in formulae giving solutions of
third and fourth degree equations, in some cases, real solutions are obtained by using
formulae which manipulate imaginary numbers. This discovery was shocking for
Italian mathematicians, in the same way as that of incommensurability was shock-
ing for Greek mathematicians. In fact, meaningful solutions to a problem could be
obtained by manipulating meaningless objects. Moreover, if i denotes √−1, then in
the set of numbers a + ib with a,b ∈R the arithmetical operations can be extended
in a way which is coherent with their deﬁnition on the reals. One may just deal with
these numbers as if they were algebraic expressions (sums of two reals) including
an indeterminate i satisfying the condition (impossible for every real number) that
i2 = −1. Therefore (a1 + ib1)(a2 + ib2) = (a1a2 −b1b2) + i(a1b2 + b1a2) (analo-
gously for the other operations).
5.4.1
Euler’s Identity
The imaginary unit i is related to two numbers which are crucial in mathematics.
One of them is Archimedes’ constant π, the ratio between the length of a circle and
its diameter. The second one is the constant e, introduced by John Napier, and fur-
ther deﬁned and fully investigated by Leonhard Euler. The Scottish mathematician
Napier in 1614 published a work where the problem is addressed of reducing the
computation of products to that of suitable sums. An arithmetical progression is a
sequence of numbers where each element (apart the ﬁrst one) is obtained by the
previous one by adding a constant value to it (the common difference of the pro-
gression). Analogously, a geometric progression is a sequence of numbers where
each element (apart from the ﬁrst one) is obtained from the previous one by multi-
plying it by a constant value (the common ratio of the progression). The initial idea
of Napier (suggested by mechanical analogies) was to determine geometrical pro-
gressions coinciding, with a good approximation, with an arithmetical progression
of very small common difference, for example 0.001. In this manner, a product of
two numbers a ≤b, with an approximation to the third decimal, can be obtained by
locating them in the geometrical progression and then by locating, a greater number
c at a distance (number of elements) from a equal to the position of b. In this way,

232
5 Numbers and Measures
summing distances in the geometrical progression corresponds to multiplying ele-
ments of the arithmetical progression. Napier realized that numbers such as 1.01100,
or 1.0011000, that is, having the form (1 + 1/n)n with large n were useful for re-
alizing his intuition. All these numbers are greater than 2 and smaller than 3, and
for increasing values of n they approximate a real number e, later formally deﬁned
by Euler. We will present, in Sect. 5.6.5, an argument showing the importance of e
in many developmental processes. Here, we report an analysis of Euler which links
this number to the imaginary unit and to Archimedes’ constant π. The ﬁrst impor-
tant result of Euler, about e, was its representation as an inﬁnite sum. The starting
point of this result is Newton’s formula:
(a + b)n =
n
∑
k=0
n
k

akbn−k
where
n
k

denotes the binomial coefﬁcient, that is, the number of different k-subsets
of n elements, which is given by the following formula, where n! is the factorial of
n, that is the product 1× 2 × ...× (n −1)× n:
n
k

=
n!
k!(n −k)!.
The argument of Euler is informal, but extremely fascinating. He expresses e by
the following formula, where ω stands for an inﬁnite number (numerus inﬁnite
magnus):
e = (1 + 1/ω)ω
but, being, for any number x, also ω/x an inﬁnite number, we can put:
e = (1 + x/ω)ω/x
therefore
ex = ((1 + x/ω)ω/x)x = (1 + x/ω)ω
and by Newton’s formula, dealing with ω as if it were a natural number, this can be
written as:
ex = (1 + x/ω)ω =
ω
∑
k=0
ω
k

xk/ωk
but, by deﬁnition of binomial coefﬁcients and factorials, we get:
ex =
ω
∑
k=0
ω
k

xk/ωk = 1+ωx/1! ω +ω(ω −1)x2/2! ω2 +ω(ω −1)(ω −2)x3/3! ω3 ...
and for the inﬁnity of ω we can assume ω = (ω −1) = (ω −2)... therefore we can
conclude that:
ex = 1 + x/1!+ x2/2!+ x3/3!...

5.4 Complex Numbers and Real Vectors
233
With similar arguments Euler proved the following identities involving the circular
functions sine and cosine:
sinx = x−x3/3!+ x5/5!−x7/7!...
cosx = 1 −x2/2!+ x4/4!−x6/6!...
Therefore by comparing the series for the exponential function ex and of the circu-
lar functions, it is easy to obtain the famous identity of Euler connecting the three
numbers π,i,e:
Table 5.5 Euler’s trigonometric identity
eix = cosx+isinx.
If we put x = π, from cos(π) = −1 and sin(π) = 0, we easily get the following
celebrated formula, called Euler’s identity.
Table 5.6 Euler’s identity
eiπ +1 = 0.
According to Napier’s logarithms, Euler’s constant e is called the natural base of
logarithms. The operation loga x of a number x, with respect to any base a ∈R,a >
1, yields the value such that aloga x = x. From this deﬁnition, the main properties of
Table 5.7 easily follow.
Table 5.7 Basic properties of logarithms
loga xy = loga x+loga y
loga xy = yloga x
loga b = −logb a
loga 1/x = −loga x
logb x = logb aloga x
All the deﬁnitions of basic arithmetical operations, plus those of exponentiation,
and logarithm, with respect to real bases, easily extend to rational and real numbers.
We conclude by remarking that logarithms, and circular functions, had an enor-
mous impact on the technological and social development of the 17th century. In

234
5 Numbers and Measures
fact, reliable navigation is based on reliable and efﬁcient methods of trigonomet-
ric computations (for the angles to follow along a navigation trajectory). Therefore,
logarithmic tables of sine, and cosine values were essential computational tools for
the geographic revolution of that time.
5.4.2
Polar Representation of Complex Numbers
Euler’s identity suggests a geometrical interpretation of complex numbers in the
plane. Firstly the French mathematician Argand [166], and then Gauss represented
a number a+ib as the point of the Cartesian plane with abscissa a (on the real line)
and ordinate b (on the imaginary line). In this way, the complex number cosx+isinx
is a point on the unitary circle centered in the origin of the Cartesian plane where
the radius connecting it to the origin forms an angle x with the real line. This means
that, according to Euler’s identity, eix corresponds to this point, and any point of
the complex plane has the exponential or polar representation ρeix, where ρ is the
distance between the point and the origin, called the module of the number, while
x is the angle that the radius connecting the point to the origin forms with the real
axis. The following identity is a consequence of the previous analysis, where arcsin
is the inverse function of sin, that is arcsinx is the angle α such that sin(α) = x:
a + ib =

a2 + b2 eiarcsin(b/√
a2+b2).
This interpretation of C gives a (geometrical) meaning to the complex numbers
based on the imaginary unit. Moreover, it opens enormous possibilities of applica-
tions, ranging from electromagnetism to hydrodynamics,or quantum physics, which
show, in almost literal terms, the power of mathematical imagination. The sum of
two complex numbers is easily obtained by separately summing their real and imag-
inary parts, while the exponential representation implies that their multiplication can
be carried out by multiplying their modules and by summing their angles.
An important consequence of the polar representation of complex numbers is the
proof of the fundamental theorem of algebra: Every algebraic equation of degree
n with coefﬁcients in C has n solutions in C (by counting each solution with its
multiplicity).
Firstly, we observe that, given an equation P(x) = 0 where P(x) is a polynomial
of degree n, then α is a solution of it iff P(x) = (x−α)Q(x), where Q(x) has degree
n−1. In fact, if P(x) = (x−α)Q(x), then obviously P(α) = 0. Vice versa, If P(α) =
0, then assume that P(x) = (x −α)Q(x) + k, with k ̸= 0, then 0 = P(α) = (α −
α)Q(x) + k, that is, 0 = k, which is a contradiction. This means that if P(x) has n
solutions α1,α2,...,αn ∈C then P(x) = a(x −α1)(x −α2)···(x −αn) (for some
constant a). Therefore, if any equation has at least one solution, then any equation
of degree n has exactly n solutions (counted with their multiplicities). In fact, if P(x)
has degree n and it has at least one solution, say it α1, then P(x) = (x−α1)Q(x) with
Q(x) of degree n −1, and all solutions of Q(x) = 0 are also solutions of P(x) = 0.
But, also Q(x) has at least one solution, therefore we can apply iteratively the same

5.4 Complex Numbers and Real Vectors
235
argument, and so we eventually reach a polynomial of degree 1, having exactly one
solution.
In conclusion, the essential part of the fundamental theorem is the proof of the
existence of at least one solution for any algebraic equation P(z) = 0, with z ∈C.
After some partial proofs of Euler, a ﬁrst complete attempt of proof of the fun-
damental theorem is that given by d’Alembert in 1746, assuming a lemma, which
was stated without a rigorous proof (Argand proved it in 1806 [166]). During his
lifetime, Carl Friedrich Gauss offered at least four different proofs of this theorem,
covering the timespan of his entire adult life. His ﬁrst proof was published in his doc-
toral dissertation in 1799, at age of 22. In 1849, just a few years before his death,
Gauss gave a fourth proof that bore similarity to his ﬁrst one. These proofs (the
ﬁrst and the fourth one) were grounded on subtle and deep properties of continuous
functions and topological concepts, which at that time were missing of any rigorous
clariﬁcation (the masterpieces of Dedekind and Weierstrass about reals and conti-
nuity were completed after 1870, and topological properties of curves, implicitly
assumed in the ﬁrst and fourth proofs of Gauss, reach a completely rigorous math-
ematical elaboration only in the 20th century). Quoting from [192], Gauss claims
that: “It seems to be well demonstrated that an algebraic curve neither ends abruptly
(as it happens in the transcendental curve y = 1/logx), nor loses itself after an inﬁ-
nite number of windings in a point (like a logarithmic spiral)”.
The informal proof that we outline uses arguments different from those of Gauss
(see [171] for a topological proof). Let us assume the concept of transformation of
the complex plane; for any polynomial P, any point z of the complex plane trans-
forms into point P(z) of the same plane. Let us ﬁx a polynomial P(z) (with complex
coefﬁcients) having degree k > 2 and where the monomial zk has coefﬁcient 1 (this
does not imply any loss of generality). Let us transform the points of a disk D of the
complex plane having as boundary a circle C, the center in the origin, and radius R.
Of course, P(0) belongs to D′ (P(0) ̸= 0, otherwise the proof would be concluded,
being the origin a solution of the equation). The set of points that are P-images of
the points in D form a region D′, with boundary C′. The polynomial P transforms
interior points of D into interior points of D′ and points on the boundary C into
points on the boundary C′. This intuitive property has a rigorous explanation, based
on analytical aspects related to the “open mapping theorem”. Let us deﬁne the ra-
dius R′ of D′ with respect to P(0) as the minimum distance between P(0) and C′.
For large enough and increasing values of the radius R of C, the monomial zk of P
becomes dominant (on the points of C), thus the corresponding radius R′ of D′ (with
respect to P(0)) increases consequently. Therefore, for a sufﬁciently large radius R
of C the radius R′ of D′, is greater than the distance between P(0) and the origin. In
conclusion, when the radius R of C reaches this value, the origin belongs to D′, so
that a point z0 in D has to exist which is transformed into 0, that is, P(z0) = 0.
The representation of complex numbers in the plane is an example of geometri-
cal representation of pairs of numbers as points in a two-dimensional space. This
idea easily generalizes to triples and, in general, to any sequence of k components.
In other words, if R2 corresponds to the plane and R3 corresponds to the three-
dimensional space, then Rk can be seen as a k-dimensional space, where basic

236
5 Numbers and Measures
Fig. 5.6 Transformation of the circles by means of algebraic transformations
geometric concepts can be developed in an abstract setting. In general, a space where
points are identiﬁed by a number (the dimension of the space) of variables ranging
over the real numbers forms a (real) vector space. A more biologically oriented ex-
ample of vectors is constituted by a cell culture plate, divided into k compartments
where for each compartment different nutrients or drugs were placed. The number
of cells for each compartment provides a vector in Nk and the growth process pro-
vides a time series for each compartment. As another example, a DNA microarray is
a small chip where many thousand of spots are arranged in a matrix (any spot has a
row and a column position). Each spot contains DNA probes of the same type (sin-
gle strands of length around 20 bases), which are speciﬁc of genes, in such a way
that each column refers to one gene. When RNA coming from a cell is uniformly
distributed on the microarray, neglecting the details of the process, a quantity can be
assigned to each spot, which determines the overall expression of the genes associ-
ated to the columns for the cell under observation. In conclusion, a DNA microarray
yields a vector in Rk, if k are the columns in the chip (the vector of the overall ex-
pression relative to each of k genes). A population of objects of many different types,
that is a multiset over a given set, where an order is given, provides a vector formed
by the multiplicities of the different types of objects. This means that the state of
a biochemical system where some reactions happen is naturally represented by a
point of a suitable space, and the evolution of the system, starting from this state, is
represented by a motion of this point in this space.

5.4 Complex Numbers and Real Vectors
237
In general, a vector space over an algebraic ﬁeld C (for example real or complex
numbers) is deﬁned when a set of elements, called vectors, are given with an internal
sum operation ⊕which is commutative and associative, and for which a zero vector
0 exists with respect to this sum, and an opposite vector −v exists for any vector v
such that v⊕−v =0 (for two vectors v,w, then v⊕−w is often abbreviated by v−w).
Moreover, a scalar multiplication is deﬁned between any element c ∈C of the ﬁeld
and any vector v such that c·v is a vector too, and some natural axioms hold for this
multiplication (distributivity of scalar multiplication with respect to vector addition
a(v ⊕w) = av ⊕aw, and distributivity of scalar multiplication with respect to ﬁeld
addition (a + b) · v = a · v ⊕b · v). Finally for two scalars a,b, if ab is their product
in the ﬁeld a · (b · v) = (ab) · v and 1 · v = v if 1 is the multiplicative identity of the
ﬁeld C.
In conclusion, we can realize that measures allow us to locate objects and popu-
lations in spaces. In the simplest case this space is the unidimensional real line, but
in general, it is a vector space of many dimensions.
5.4.3
Some of Euler’s Jewels
In this book we encountered some spectacular results due to Leonhard Euler. For
example, the famous identity eiπ + 1 = 0 and the inﬁnite sum representations of
ex, sinx, and cosx. In this section we shortly present one of Euler’s most sensa-
tional discoveries. It is the solution of a problem, called Basel problem, which was
raised by the Italian mathematician Pietro Mengoli and was investigated also by
Jakob Bernoulli. The question concerns the exact determination of the limit of the
p-harmonic series for p = 2:
∞
∑
1
1
np
It was proved that for p = 1 the harmonic series is divergent, but for p > 1 these
series are convergent.
The following is the solution found by Euler in 1735 (since 1731, at the age of
24, he was hardly working on this problem).
According to Euler’s determination of the series for sine we have:
sinx = x−x3
3! + x5
5! −x7
7! + x9
9! ...
therefore:
sinx
x
= 1 −x2
3! + x4
5! −x6
7! + x8
9! ...
Now the main intuition of the argument can be introduced. We know that given
an algebraic equation P(x) = 0 of degree n with non-zero roots a1,a2,...,an and
P(0) = 1 we can express P(x) by means of the following product (which is easily
proved to have the same roots and P(0) = 1):

238
5 Numbers and Measures
P(x) =

1 −x
a1

1 −x
a2

1 −x
a3

...

1 −x
an

Euler assumed that this property extends to “inﬁnite polynomials” and applied it to
sinx/x. Namely, sinx/x is equal to zero in correspondence to the following values
of x:
π,−π,2π,−2π,3π,−3π...
whence:
1 −x2
3! + x4
5! −x6
7! + x8
9! ... =

1 −x
π

1 −x
−π

1 −x
2π

1 −
x
−2π

...
that is:
1 −x2
3! + x4
5! −x6
7! + x8
9! ... =

1 −x2
π2

1 −x2
4π2

1 −x2
9π2

...
or equivalently:
1 −x2
3! + x4
5! −x6
7! + x8
9! ... = 1 −
 1
π2 + 1
4π2 + 1
9π2 + ...

x2 ...
therefore, by equating the coefﬁcients of x2 of the two members (the other powers
do not matter, in this context), we get:
1
3! = −1
π2

1 + 1
4 + 1
9 + 1
16 + ...

thereby concluding that:
1 + 1
4 + 1
9 + 1
16 + ... = π2
6 .
The sinx/x representation, in terms of its inﬁnite roots, provides another important
result due to Wallis, which we use to prove Stirling’s approximation (see Sect. 7.5).
In fact:
sin(π/2)
π/2
=

1 −π2/22
π2

1 −π2/22
4π2

1 −π2/22
9π2

1 −π2/22
16π2

...
that is:
2
π = (4 −1)
4
(16 −1)
16
(36 −1)
36
...
which is exactly Wallis’ identity, usually given by the following equality:
2
π = 1 ·3 ·3 ·5 ·5·7·7·9...
2 ·2 ·4 ·4 ·6·6·8·8....

5.5 Induction and Recurrence
239
After the presentation of this argument, William Dunham [178] concludes by
saying: ”Surely this established that Euler’s train of thought had not derailed. If this
argument could recover previously known results such as this, there seemed all the
more reason to embrace his initial conclusion.” (the inﬁnite product representation
of sinx/x, Euler, Introduction to Analysis of the Inﬁnite, Book I, pp. 154–155)1.
5.5
Induction and Recurrence
Natural numbers are generated by means of the basic counting process. In fact, we
start from an initial number, called zero, and then by applying the successor opera-
tion, at each step, we get a new number from the last one previously generated. Apart
from the speciﬁc manners to identify the elements of this sequence, the essence of
this (inﬁnite) process is the combination of zero and successor. A deﬁnition of nat-
ural number which expresses this perspective is due to the Italian mathematician
Giuseppe Peano at the beginning of the last century: zero is a number, and the suc-
cessor of any number is a number too. We remark that the successor operation is
always deﬁned and its result applied to a number n is always different from all the
numbers from zero to n in the generation process. This structure characterizes the
set N of the natural numbers and is the essence of the mathematical induction, a
very powerful method for proving and deﬁning properties of natural numbers. The
explicit and formal identiﬁcation of the principle of induction was given by Peano,
but it was used, in an implicit or intuitive manner, by many mathematicians be-
fore Peano (primitive forms of induction are present in Greek mathematics) such
as: Maurolico, Tartaglia, Pascal, and Dedekind. The name induction seems to be
a little misleading, because it suggests an analogy to the process of inferring gen-
eral properties from particular cases. However, this connection is not completely
wrong. In fact, the primitive forms of induction emerge when some properties are
veriﬁed in some speciﬁc numerical cases, but a pattern occurs which is not speciﬁc
to single cases, because it follows a common general schema. The principle of in-
duction claims that in order to prove or to deﬁne a property P over N it is sufﬁcient
to provide:
Table 5.8 The initial step and the induction step of an induction schema
i) The proof or deﬁnition of P(0) (validity or deﬁnition of P for zero)
ii) The proof or deﬁnition of P(n + 1) by assuming that P(n) holds or is deﬁned (n
generic).
1 Another famous Euler’s jewel, related to Riemann’s zeta function on the distribution of
prime numbers, is the so called sum-product formula ∑k∈N 1
k = ∏p∈P
p
p−1, where P is the
set of prime numbers.

240
5 Numbers and Measures
This schema can be easily changed and extended in many ways. For example, if
we want to prove P only for all the numbers greater than or equal to k, it applies
by just taking k instead of zero (the property in question is not exactly P(n), but
P(n + k) for all values of n). In other cases it is more convenient to prove or deﬁne
P(n+1) by assuming that P(j) holds or is deﬁned for all j ≤n. Or, more generally,
what we want to prove or to deﬁne is not directly related to natural numbers, but
to a sequence S0,S1,... of structures indexed by natural numbers. In this case the
initial step applies to S0 and the induction step proves or deﬁnes something for
Sn+1 by assuming it holds or is deﬁned for the structure Sn (or for all structures
with indexes smaller than n+1). In the literature there are many methods indicating
different schemas of induction; however, they are all consequences of the basic form
considered above. Often, the numerical variable used in the schema of induction (n
in our formulation) is called the induction variable, and a proof or deﬁnition by
induction is said to be developed over the variable n.
Given a quite obvious validity of the induction, it is surprising that its generality
was discovered in relatively recent times. Moreover,despite its simplicity, very often
induction proofs and deﬁnitions are not so easy to be correctly developed and their
construction and analysis requires a good mathematical maturity.
We provide an initial example of proof by induction, due to Maurolico, of the
fact that n2 is the sum of the ﬁrst n odd numbers:
n2 =
n
∑
i=1
(2i−1).
(5.1)
The initial step trivially holds. For the induction step, we need to prove the following
equation, by assuming the previous one:
(n + 1)2 =
n+1
∑
i=1
(2i−1).
In fact,
(n + 1)2 = n2 + 2n + 1
but, for the induction hypothesis, we can replace n2 by the right member of Eq. (5.1),
thereby obtaining the same equation instantiated at the value n + 1, as required by
the induction step:
(n + 1)2 =
n
∑
i=1
(2i−1)+ (2n + 1) =
n+1
∑
i=1
(2i−1).
The above example shows a peculiarity of induction, that is, a sort of tautological
ﬂavor, which is intrinsic to the induction schema. In fact, it seems that we use in
the proof the same claim we want to prove. This impression is related to the basic
mechanism of the induction step, where essentially an hypothetical statement is
transformed into a universal statement. This phenomenon is more clearly apparent

5.5 Induction and Recurrence
241
if we reformulate the previous schema in a logical form where an arrow denotes the
proof consequence and ∀is the universal quantiﬁcation asserting the validity of a
proposition for all the values of the variable it refers to.
Table 5.9 Proof by induction of ∀n ∈N P(n)
i) P(0)
ii) ∀n ∈N

P(n) ⇒P(n+1)

The above induction proof is of a completely new kind with respect to the well-
known traditional geometric proof of Greek mathematics, developed in the context
of ﬁgurate numbers. For the sake of completeness, we want to mention it. Consider
an arrangement of n2 balls forming a square of side n. This arrangement can be
constructed by putting an initial ball and then by surrounding it by three other balls.
In general, for passing from a square to the next square, that is from n2 to (n + 1)2,
you need to add a row of n balls, a column of n balls and a further common ball
extending the added row and column. Therefore, 2n + 1 is the whole number of
balls for passing from n2 to (n + 1)2. This means that in general a square of n2
balls is obtained by increasingly arranging n odd numbers of balls which extend the
square sides. Table 5.10 shows the construction of a square of 9 objects, according
to the outlined procedure. The following interesting proposition, due to the Greek
mathematician Nichomachus, has a very simple proof by induction.
Table 5.10 The construction of a square of 9 objects by arranging 1+3+5 objects
⋄
∗
o
⋄∗o
⋄
∗∗
−→⋄∗∗
⋄⋄⋄
⋄⋄⋄
Proposition 5.1 (Nichomachus’ Theorem)
n
∑
i=1
i3 =

n
∑
i=1
i
2
Proof. (Proof by induction) The initial step is evident. The induction step is estab-
lished by the following identities (where the passage from the last but one equation
follows by induction hypothesis and by the identity 1+2+...n = n(n+1)/2, which
will be proved in a next section (Sect. 5.6.1, Eq. (5.3)):

242
5 Numbers and Measures
(1 + 2 + ...n + (n + 1))2 = (1 + 2 + ...n)2 + (n + 1)2 + 2(1 + 2 + ...n)(n + 1) =
13 + 23 + ...n3 + (n + 1)2 + n(n + 1)(n + 1) =
13 + 23 + ...n3 + (n + 1)2(n + 1) = 13 + 23 + ...(n + 1)3.
⊓⊔
Tables 5.11, 5.12, 5.13, and 5.14 respectively display classical deﬁnitions by induc-
tion (on the variable n) of the arithmetical sum, product, exponential, and factorial
operations. These deﬁnitions have two equations, the ﬁrst one for the initial case,
while in the second one the left member provides the deﬁnition of the operation
by assuming, by induction hypothesis, that the right member is already deﬁned for
smaller values of the variable. In the deﬁnition of product, the deﬁnition of sum is
assumed, and, in the deﬁnition of exponentiation, the deﬁnition of product is as-
sumed. They can be proved to be correct (they deﬁne the expected operations) by
using the induction principle.
Table 5.11 The inductive deﬁnition of sum
n+0 = n
m+(n+1) = (m+n)+1
Table 5.12 The inductive deﬁnition of product
n∗0 = 0
m∗(n+1) = (m∗n)+m
Table 5.13 The inductive deﬁnition of exponentiation
m0 = 1
mn+1 = mn ∗m
Table 5.14 The inductive deﬁnition of factorial n! (the product of all the numbers equal to or
lower than n)
0! = 1
(n+1)! = n!∗(n+1)
The deﬁnition in Table 5.15 characterizes the property of prime numbers by in-
duction (a prime number is greater than 1 and has no divisor different from itself).

5.5 Induction and Recurrence
243
Table 5.15 The sequence of prime numbers
p(1) = 2
p(n+1) = min{j | j > p(n) and p(i) is not a divisor ofj for i ≤n}
The induction principle is a powerful tool in the analysis of discrete structures.
Tables 5.16, 5.17, and 5.18 provide inductive deﬁnitions of tree and graph over a
set A of labels (see Figs. 5.7 and 5.8). In particular, the deﬁnition of Table 5.17 is a
special form of inductive deﬁnition, usually referred as recursive deﬁnition (a tree
is deﬁned in terms of smaller trees).
Table 5.16 The inductive deﬁnition of rooted ordered trees over the set A
Basis step: A single vertex is a rooted tree.
Induction step: If T1,T2,...,Tk are disjoint rooted trees with roots r1,r2,...,rk ∈A,
respectively, then, starting with an element r ∈A such that r ̸= r1,r ̸= r2,...,r ̸= rk,
and adding an edge from r to each of roots r1,r2,...,rk a new tree is obtained having
r as its root and trees T1,T2,...,Tk as its subtrees.
Table 5.17 The recursive deﬁnition of rooted ordered trees over the set A
A ﬁnite non-empty set T ⊆A of nodes is a tree if the following conditions hold:
i) there is a node r ∈T called the root of T
ii) the remaining nodes of T/{r} are partitioned into m ≥0 sets, and each of them is a
tree over A, called a subtree of T.
An inﬁnite tree is a tree having an inﬁnite set of nodes N. An inﬁnite tree is said
to be ﬁnitary if every node of the tree has a ﬁnite number of children. An important
result about inﬁnite trees, based on induction, is the following proposition known as
K¨onig lemma.
Proposition 5.2. An inﬁnite ﬁnitary rooted tree has an inﬁnite path.
Proof. In fact, if r is the root of the tree, then an inﬁnite path (pn|n ∈N) can be
deﬁned by induction by putting p0 = r, which is the root of an inﬁnite tree, and for
any n ∈N, if pn has been already deﬁned, as the root of an inﬁnite tree, then surely it
has a child q which is the root of an inﬁnite tree, therefore we deﬁne pn+1 = q.
⊓⊔

244
5 Numbers and Measures
Table 5.18 The inductive deﬁnition of simple oriented graph over the set A
If a ∈A then a ∈G(A).
If G ∈G(A), a ∈A and a is not a node of G (a ̸∈G), then the graph denoted
by (G + a) belongs to G(A), where the node a is added as new node disconnected
from the nodes of G. Moreover, if G ∈G(A), a,b ∈G and a is not connected to b in
G, then the graph denoted by (G+(a,b)) belongs to G(A), where the edge connecting
a to b is added to G.
Fig. 5.7 Construction of a new tree from three already given trees
Fig. 5.8 Extending a graph with a new node (top) and with a new arc (down)

5.5 Induction and Recurrence
245
Induction is strictly related to two other notions: iteration and recurrence. It-
eration is a special case of induction obtained by applying an operation a num-
ber of times, and at each step to the result of its previous application. This notion
can be formally deﬁned by means of a sort of exponentiation, as it is expressed in
Table 5.19.
Table 5.19 The inductive deﬁnition of iteration
f 0(x) = x
f n+1(x) = f (f n(x))
For example, the iteration of the successor function s of exponent 3 and argument
4 is:
s3(4) = s(s2(4)) = s(s(s1(4))) = s(s(s(s0(4)))) = s(s(s(4))) = 7.
5.5.1
Fibonacci Sequence
Recurrence is a form of induction where a function is deﬁned on natural numbers
by starting from some initial values, and then by deﬁning the values of the function
in terms of some of the values it takes on smaller arguments.
One of the most famous examples of recurrent deﬁnition is the following, due
to Leonardo Fibonacci (Leonardo Pisano) and presented in his famous Liber Abaci
(1202 A.D.), whereby the Indo-Arabic positional notation for numbers was intro-
duced in Europe:
Table 5.20 The recurrent deﬁnition of Fibonacci sequence
F(1) = F(2) = 1
F(n+2) = F(n+1)+F(n)
Table 5.21 The ﬁrst eight values of Fibonacci sequence
F(1) = 1,F(2) = 1,F(3) = 2,F(4) = 3,F(5) = 5,F(6) = 8,F(7) = 13,F(8) = 21
The Fibonacci sequence is strictly related to the golden ratio φ = (1 +
√
5)/2.
In fact, for increasing values of the sequence, the ratio between a Fibonacci number
and its predecessor approximates to φ.

246
5 Numbers and Measures
Fig. 5.9 The “golden triangle”
Let T be a “golden” isosceles triangle with vertex angle equal to one-tenth of the
full circle angle, that is α = π/5.
The internal triangle of Fig. 5.9 is isosceles too and similar to T because it has
the same angles as T: π/5, 2π/5, and 2π/5 (equal angles at the basis). In the golden
triangle T, if 1 is the length of the basis and φ is the length of the oblique sides, then
it is easy to ﬁnd the following proportion:
φ : 1 = 1 : (φ −1).
Therefore φ is solution of the equation:
φ2 = φ + 1.
This equation has only one positive solution given by φ = (1 +
√
5)/2, that is, the
golden ratio, approximately equal to 1.618. Moreover, from the equation above, it
follows that:
φ = 1 + 1
φ
therefore by applying iteratively its second member to replace φ, we get a continu-
ous fraction approximating to φ:
φ = 1 +
1
1 +
1
1+
1
1+
1
1+...
It can be showed, by induction, that:
F(n) = φn −ϕn
φ −ϕ

5.5 Induction and Recurrence
247
where φ is the positive root and ϕ the negative root of the equation x2 = x + 1
(square = successor) that is:
F(n) = (1 +
√
5)n −(1 −
√
5)n
2n√
5
being ϕ smaller than 1, in absolute value, for n sufﬁciently large, F(n) approximates
to φn/
√
5 (whence F(n + 1)/F(n) approximates to φ).
It is easy to verify that −ϕ = 1
φ and that 1 + ϕ = 1 −1
φ = 1
φ2 . The value 1/φ2 is
also called circle golden ratio, and the angle 2π/φ2 is frequently occurring in nature
structures.
The problem, which suggested to Fibonacci his sequence, was the growth of a
population of rabbits. The (ideal) rule of their reproduction establishes that each
rabbit generates one offspring (a couple of rabbits, an offspring couple), but a new-
born at some generation time, say i, can generate rabbits only when it becomes adult,
that is, at generation i+ 2 (one step is necessary to a newborn to become an adult).
Fig. 5.10 exempliﬁes this kind of development by means of a tree.
Fig. 5.10 Five generations of Fibonacci development
Fibonacci sequence is surprisingly ubiquitous in processes of biological morpho-
genesis and development. In DNA double strands, the angle formed by two consec-
utive nucleotides is approximately π/5, and the ratio between the two grooves of
DNA helix is near to φ. Moreover, Fibonacci sequence and golden ratio are found
in many patterns formed by leaves and ﬂowers during plant developments. In the

248
5 Numbers and Measures
case of phyllotaxis, Fibonacci spirals and “golden angles” frequently occur. A Fi-
bonacci spiral, based on Fibonacci sequence, is given in Fig. 5.11. The most fre-
quent golden angle in phyllotaxis (the arrangement of leaves in stems) is 2π(1/φ2),
approximately equal to 137.52o.
The reason of such geometrical phenomena seems to be due to the optimal bal-
ance, realized by these patterns, between two opposite forces: i) a repulsive force
between nearby individuals (leaves which need to have enough space around to re-
ceive enough light without interference from other leaves), and ii) an attractive force
due to a whole population requirement (a tree tending to have the maximum number
of leaves on its branches).
Fig. 5.11 Fibonacci’s spiral
5.5.2
Arithmetic and Logic
Arithmetic and logic are deeply related. Both have a foundational nature for the
whole mathematics. Here we present some basic concepts of First-Order Logic
(FOL), mainly developed in the last two centuries (books [172, 185, 189] are some
classical references, [184] is a succinct presentation).
A model, or a (relational) structure, M is given by: i) a set D, called domain
of M , ii) some elements a,b,... ∈D, called individual constants of M , and iii)
some operations f,g,... and relations R,Q,... over D (to each operation and relation
an arity is associated that speciﬁes the number of the arguments). Relations are
considered equivalent to predicates, that is, functions that map their arguments to
a truth value (1,0) in correspondence to the fact that the relation holds or does not
hold between them. Usually M is indicated by:
M = (D,a,b,... f,g,...,R,Q,...).
The set Term(M ) of the terms over M is given by all the expressions that can be
constructed, in the usual algebraic sense, by induction, by applying operations and
relations of M to the individual constants of M , or to already constructed terms.
For example, if f has arity 1 and g has arity 2, then the following are terms over the
model M :

5.5 Induction and Recurrence
249
f(a), g(a,b), f(g(a,b)), g(a, f(a)), f(g(a, f(a))).
An equation such as g(a, f(a)) = b means that by applying the operation f to the
constant a, we get an element of D, say c, and by applying g to the pair (a,c) we
get b. This means that g(a, f(a)) is considered as the denotation of the element of
D obtained by applying the operations according to the usual way algebraic expres-
sions are evaluated (in the order speciﬁed by parentheses). However, we can also
consider a term as an uninterpreted expression (a tree) constructed from constants,
operations, commas, and parentheses, disregarding its denotation. If we want to be
precise we write g(a, f(a)) to mean the element of D (if it exists) denoted by the
term, while we write ⌈g(a, f(a))⌉(or something analogous) to denote the uninter-
preted term. In formal logic this distinction is fundamental and a lot of different
terminologies are used to tell this intrinsic difference. Very often a denoting symbol
is said to be used while it is said to be mentioned when it refers to itself.
A ﬁrst-order logic signature (FOL signature) Σ is a set of symbols for objects,
operations, and relations. A Σ-model is a relational structure where objects, opera-
tion and relations are denoted by the symbols of Σ.
We call Term(Σ) the uninterpreted terms over the signature Σ, and TermV (Σ)
the terms over the signature Σ and a set V of variables.
Terms over a signature Σ can be transformed into terms over a model when the
symbols of Σ are interpreted in the model.
We call TermV(M ) the terms over the model M where variables V range over
the domain of M , and Term(M ) the terms of M , where variables do not occur.
Atomic formulae (over a signature) are expressions R(t1,t2,...,tk) where R is a
k-ary relation symbol and t1,t2,...,tk are terms.
First-Order Logic or FOL formulae are constructed by starting from atomic
formulae by using the First-Order Logical operators (FOL), which are:
1. connectives denoted by: ¬,∧,∨,→, ↔; respectively for negation (not), con-
junction (and), disjunction (or), implication (if), equivalence (iff), and
2. quantiﬁers denoted by ∀(for all), the universal quantiﬁer, and ∃(there exists),
the existential quantiﬁer.
Let us associate connectives with the boolean functions described by their truth
tables deﬁned by the following boolean equations:
• ¬1 = 0 , ¬0 = 1
• (1 ∧0) = (0 ∧1) = (0 ∧0) = 0 , (1 ∧1) = 1
• (1 ∨0) = (0 ∨1) = (1 ∨1) = 1 , (0 ∨0) = 0
• (0 →1) = (1 →1) = (0 →0) = 1 , (1 →0) = 0
• (1 ↔1) = (0 ↔0) = 1 , (0 ↔1) = (1 ↔0) = 0.

250
5 Numbers and Measures
Let us deﬁne universal and existential quantiﬁcations in the following way, where
P stands for any unary predicate, D for the domain of a given model, and x for
a variable ranging over D (1 stands for true, 0 stands for false and assume that
0 < 1):
∀DP(x) = min{P(x) | x ∈D}
∃DP(x) = max{P(x) | x ∈D}.
If we apply connectives and quantiﬁers to atomic formulae over a model M and
variables V, then we generate the formulae over M and over a set of variables V. A
variable occurrence is free if it is not quantiﬁed. Formulae denote k-ary predicates
if k different free variables occur in them, while they denote a truth value if no free
variable occurs in them. Formulae without free variables are called sentences over
M (over its signature). A FOL theory is a set of sentences over a FOL signature.
We say that a sentence ϕ over M holds in M if it denotes the true value (say 1)
and we write:
M |= ϕ.
We can obtain a deﬁnition of logical validity by means of the following notion of
interpretation of FOL formulae over FOL models.
Let M be a model of signature Σ and domain D, and F(Σ,V) the FOL formulae
over the signature Σ and the set V of variables. An interpretation of F(Σ,V) in
M is deﬁned by giving to the constant, operation, and relation symbols of Σ their
meaning in the model M , that is, by giving to the terms their denotation in D with
the variables in V ranging over D. Then, for an atomic formula ϕ = R(t1,t2,...,tk)
where no variable occurs, the interpretation τ is deﬁned by stating τ(ϕ) = 1 if and
only if the relation R holds on the terms t1,t2,...,tk of M . The interpretation τ is
extended to all the sentences of F(Σ,V), by requiring the following conditions for
the truth values of all non-atomic sentences of F(Σ,V):
• τ(ϕ ∧ψ) = τ(ϕ)∧τ(ψ)
• τ(ϕ ∨ψ) = τ(ϕ)∨τ(ψ)
• τ(¬ϕ) = ¬τ(ϕ)
• τ(ϕ →ψ) = τ(ϕ) →τ(ψ)
• τ(ϕ ↔ψ) = τ(ϕ) ↔τ(ψ)
• τ(∀P(x)) = ∀Dτ(P(x))
• τ(∃P(x)) = ∃Dτ(P(x)).
A sentence of F(Σ,V) is logically valid iff it is true in every FOL model of signature
Σ. It is satisﬁable if it is has a FOL model.
Given a FOL theory Φ and a FOL sentence ϕ we say that ϕ is a logical conse-
quence of Φ, by writing:
Φ |= ϕ
when ϕ is true in all models where all the sentences of Φ are true.
The discovery of the ﬁrst (informal) counter-model proof, showing the indepen-
dence of the ﬁfth Euclidean axiom (on the unicity of the parallel line) from the
remaining Euclidean axioms was essentially based on the notion of interpretation.

5.5 Induction and Recurrence
251
In fact, an interpretation of the Euclidean axioms was found where all the Euclidean
axioms were true apart from the ﬁfth axiom which was false. This discovery was the
ﬁrst meta-theorem of geometry, proving the mathematical impossibility of deducing
the ﬁfth axiom from the other ones.
Let us consider an example of FOL theory. We use: i) three variables x,y,z rang-
ing over the individuals of some biological population with sexual reproduction;
ii) four predicate symbols G,A,M,F, that is, symbols of relations over the domain
consisting of our individuals, such that:
G(x,y) means x generated y,
A(x,y) means x is an ancestor of y,
M(x) means x is a male,
F(x) means x is a female.
By using logical symbols, predicate symbols, variables, and parentheses we can
put the following sentences in formulae expressing usual facts about sexual
reproduction:
• ∀x(∃y(G(y,x)∧M(y))) (Everybody has a father)
• ∀x(∃y(G(y,x)∧F(y))) (Everybody has a mother)
• ∀x(M(x)∨F(x)) (Everybody is male or female)
• ∀x(∀y(G(x,y) →A(x,y))) (Parents are ancestors)
• ∀x(∀y(∀z(A(x,y)∧G(y,z)) →A(x,z))) (The ancestors of parents are ancestors)
• ∀x(¬(A(x,x))) (Nobody is self-ancestor).
These sentences constitute the axioms of a theory, consisting of all the logical conse-
quences of them. Can we interpret them in a different domain, with different mean-
ings for predicate symbols, in such a way that they turn to be true also in this new
interpretation? With a detailed analysis, we can discover that these axioms cannot
be fulﬁlled by a real biological population, consisting of a ﬁnite number of indi-
viduals. On the other hand, we can interpret coherently these formulae on natural
numbers. However, in which sense can we prove that the father’s unicity is not a
logical consequence of the given axioms? And, in which sense the non-existence of
a common ancestor for all individuals, ¬∃x(∀yA(x,y)), is a logical consequence of
them? Could we ﬁnd an algorithm generating all the logical consequences of these
axioms? What does it mean ∃y∀x(G(y,x)∧M(y))? Can this axiom be added to the
theory, while preserving its consistency (avoiding a contradiction)?
Peano’s axioms of natural numbers are given by the following sentences (P is
any predicate).
0 is a number.
Each number has a successor.
0 is not successor of any number.
Distinct numbers have distinct successors.
If P(0) and, for every number n, the implication P(n) →P(n + 1) holds, then, for
every n, proposition P(n) holds.

252
5 Numbers and Measures
Theory PA is the following theory which expresses Peano’s axioms as FOL formulae
(on symbols 0,1 for distinct constants and + for a binary operation):
¬∃x(0 = x+ 1)
∀x(∀y((x+ 1 = y+ 1) →(x = y)))
P(0)∧∀x(P(x) →P(x+ 1)) →∀x(P(x)).
One of the most important results in formal logic, originally due to Kurt G¨odel es-
tablishes the existence of algorithmically deﬁnable logical calculi that can express
completely the FOL logical validity. In fact, let ⊢be a derivation relation over un-
interpreted formulae deﬁned in some algorithmic way (many of such calculi were
found in the last century), such that we write:
Φ ⊢ϕ
to denote that formula ϕ is derivable from a set of formulae Φ, then G¨odel’s com-
pleteness theorem asserts the following equivalence:
Φ |= ϕ
if and only if
Φ ⊢ϕ.
Some important properties of FOL are brieﬂy reported:
1. The completeness of FOL is related to another important property of this logic
called compactness. It asserts that a theory Φ has a model (there is a model
where all the sentences of Φ hold) iff every ﬁnite subset of Φ has a model.
2. Any sentence of a FOL theory can be transformed into an equivalent formula,
which is called Skolem normal form, where quantiﬁcations are all at the begin-
ning of a propositional formula.
3. By introducing suitable operation symbols, any formula can be transformed into
an equisatisﬁable formula ϕ, called Herbrand expansion of ϕ, where only uni-
versal quantiﬁcations are, at the beginning of a propositional formula.
4. Herbrand expansions imply that any FOL theory is equisatisﬁable with a propo-
sitional theory (which in general may be inﬁnite even if the original FOL theory
is ﬁnite).
5. From the propositional representation of FOL theories, it can be derived that any
satisﬁable FOL theory has a model with numerable domain. This phenomenon
shows a deep relationship between FOL and natural numbers, and it is related to
the existence of non-standard models of real numbers (Skolem’s paradox: Real
numbers that are a non-denumerable set have a denumerable FOL model).
FOL can be considered the formal language of mathematics because any mathemat-
ical theory can be represented as a FOL theory by using a suitable representation of
its axioms as FOL sentences.

5.5 Induction and Recurrence
253
5.5.3
A Glimpse of the History of Formal Logic
Formal Logic originates with Aristotle and concerns the human activity of drawing
inferences. Of course, since the time when language developed, man has deduced
conclusions from premises, but Aristotle inaugurated the systematic study of the
rules involved in the construction of valid reasoning. The ﬁrst important discovery
of this approach was that the logical structure of sentences and deductions is given
by relations between signs in abstraction from their meaning. This aspect motivates
the attribute formal. Modern logic, as it has been developed since the middle of 19th
century, emphasizes this aspect by developing notational systems able to represent
and analyze the logical form of sentences and deductions. In this sense formal logic
is also referred as Symbolic Logic, or also Mathematical Logic, because the emer-
gence of the symbolic perspective was stimulated by certain trends within math-
ematics, namely, the generalization of algebra, the development of the axiomatic
method, especially in geometry, and the reductionism, that is, the tendency, espe-
cially in analysis, to ﬁnd basic concepts for a foundation of the whole mathematics.
The elaboration of the formal method in modern logic was chieﬂy founded by the
works of: Gottfried Wilhelm von Leibniz (1646–1716), De Morgan (1806–1871),
Boole (1815–1864), Peirce (1839–1914), Schr¨oeder (1841–1902), Frege (1848–
1925), Peano (1858–1932),Hilbert (1862–1943),Russell (1872–1970),L¨owenheim
(1878–1957), Skolem (1887–1963), Tarski (1901–1983), G¨odel (1906–1978), Her-
brand (1908–1931), Gentzen (1909–1945), and Turing (1912–1954). The essential
aspect of formal methods, in the representation of sentences and deductions, con-
sists of a clear distinction between syntax and semantics. This is an intrinsic feature
of any formal language, as opposed to natural language. Syntax establishes which
(linear) arrangements of symbols of a speciﬁed alphabet have to be considered well-
formed expressions, the categories in which they are classiﬁed, and symbolic rules
according to which some relations between expressions are deﬁned. Semantics es-
tablishes how to deﬁne the general concepts of interpretation, satisﬁability, truth,
compatibility, independence, and entailment. This distinction does not mean that
syntax and semantics are opposed, but rather, complementary. Syntactic categories,
and syntactic rules, are given in such a way that important semantic concepts can
be adequately expressed in syntactic terms. When this is possible those concepts
can be calculated or mechanized. One of the most important questions investigated
in modern logic is that of the completeness of given logical systems. A way of ex-
pressing completeness, as in celebrated G¨odel’s completeness theorem (1930), is
that any sentence that is provable, by means of the syntactic rules of a logical sys-
tem (e.g. Russell’s system of Principia Mathematica), is universally valid, that is,
true in all of its possible interpretations. Therefore, form and content of sentences
are distinguished in order to achieve effective, general methods in elaborating and
understanding sentences. Syntax deﬁned apart from semantics, can individuate pro-
cesses which elaborate formulae by using only symbolic structures, which by virtue
of their ﬁnite nature, can be encoded as physical states of a machine (the famous
G¨odel’s incompleteness theorem was based on a suitable encoding of FOL syn-
tax in arithmetical terms). Semantics, which deal with no particular interpretation

254
5 Numbers and Measures
of symbols, can deﬁne general semantic concepts which are the basis of any no-
tion of logical validity (truth in all possible worlds, according to a deﬁnition due to
Leibnitz).
5.6
Series and Growths
Fibonacci sequence is related to a growth process. Growth processes occur in any
life phenomenon and at each level from the cell to the most complex organisms, and
to biological populations. The mathematical form of a growing process is very often
given by a series, that is, a sequence where at each step an additive term is added to
the term which is the outcome of the preceding step.
5.6.1
Arithmetical Progressions
The simplest form of growth is represented by a constant additive increment. It is
typical of a population where the number of individuals increases at each step by
a constant number of k individuals. If the initial quantity is F(0) then the formula
which provides the population size at step n + 1 is given by:
F(n + 1) = F(n)+ k
that, by a trivial induction argument, gives for n > 0:
F(n) = F(0)+ nk.
If the population, at each step i increments of ki units, then, the formula providing
the size G(n) of such a population, by assuming that G(0) = 0, is:
G(n) =
n
∑
i=1
F(i) =
n
∑
i=1
ik
The value G(n) is thus:
G(n) = kn ·(n + 1)
2
.
(5.2)
This equation can be easily proved by induction. Without loss of generality we con-
sider the case k = 1, by denoting T(n) the corresponding sum. Of course, T(0) = 0,
therefore the formula holds for the value 0 of n. Let us assume that the formula holds
for n, then the following chain of equations shows its validity for n + 1:
T(n + 1) = n(n + 1)
2
+ 2(n + 1)
2
= (n + 1)(n + 2)
2
(5.3)
(the reader is advised to search for a very beautiful geometrical proof of this formula,
due to Gauss, when he was a child).

5.6 Series and Growths
255
5.6.2
Figurate Numbers
The study of ﬁgurate numbers was an old subject of Greek mathematics. However,
its investigation in the 17th century was the beginning of important discoveries
in discrete mathematics. Here, we will show a phenomenon somewhat related to
Nichomachus’ theorem.
A triangular conﬁguration of objects has the following structure:
o
o o
o o o
o o o o
o o o o o
The total number of objects of a triangular conﬁguration Tn of basis n is given,
according to formula 5.2:
Tn = n(n + 1)/2
(5.4)
Of course, Tn + Tn−1 = n2 therefore Tn + Tn −n = n2, that is, 2Tn = n2 + n, which
provides another way of proving the equation above.
A square number n2 can be represented by arranging n2 objects in a conﬁguration
Qn of n rows of objects each of them with n objects. Now let us evaluate the sum of
the squares Qi for i ranging from 1 to n (see Fig. 5.12).
Fig. 5.12 Square conﬁgurations of increasing side (top) and their extension with added (num-
bered) objects (bottom)
The following equation expresses the way of obtaining the top conﬁguration of
Fig. 5.12 from the bottom conﬁguration of the same ﬁgure, by removing three sums
of triangular numbers (the number of circles with number 1, with number 2, and
with number 3):

256
5 Numbers and Measures
∑
i=1,n
i2 = nTn −∑
i=1,n−1
Ti
(5.5)
but by formula 5.4 we have:
∑
i=1,n
Ti = ∑
i=1,n
i(i+ 1)/2 = 1/2

∑
i=1,n
i2 + ∑
i=1,n
i

that is:
∑
i=1,n
i2 = 2 ∑
i=1,n
Ti −∑
i=1,n
i
(5.6)
therefore, by equating the right members of Eqs. (5.5) and (5.6), we have:
nTn −∑
i=1,n−1
Ti = 2∑
i=1,n
Ti −∑
i=1,n
i
that is:
nTn −∑
i=1,n−1
Ti = 2 ∑
i=1,n−1
Ti + 2Tn −Tn
3 ∑
i=1,n−1
Ti = (n −1)Tn
∑
i=1,n−1
Ti = (n −1)Tn
3
.
Finally, from Eq. (5.5) it follows:
∑
i=1,n
i2 = nTn −(n −1)Tn
3
= 3nTn −(n −1)Tn
3
= Tn(3n −n + 1)
3
= n(n + 1)(2n + 1)
6
.
Formulae for the sums of powers (up to the 17th power) were discovered by Faul-
haber, a mathematician of the 17th century. Faulhaber’s formulae follow a general
pattern which can be proved by induction.
Theorem 5.3 (Faulhaber). The sum of powers of degree k, for k > 0, has the fol-
lowing general form, where Pk(n) is a polynomial of degree k:
n
∑
i=1
ik = nk+1
k + 1 + Pk(n)

5.6 Series and Growths
257
Proof. (by induction). Let
Sk(n) =
n
∑
i=1
ik
If k = 1, then Sk(n) = T(n) = n(n + 1)/2 and the statement of the theorem holds.
Assume that a polynomial of degree (k −1) was determined such that:
Sk−1(i) = ik/k + Pk−1(i)
(5.7)
then, we can put:
n−1
∑
i=1
Sk−1(i) =
n−1
∑
i=1
ik/k +
n−1
∑
i=1
Pk−1(i)
therefore:
n−1
∑
i=1
Sk−1(i) = Sk(n)
k
+ Qk(n −1)
(5.8)
where Qk(n −1) = Pk−1(n −1) + Pk−1(n −2) + ...Pk−1(1) is, by induction hy-
pothesis, a polynomial of variable n having degree k. On the other hand, the sum
∑n−1
i=1 Sk−1(i) can be put in the following way:
1
+1 +2k−1
+1 +2k−1 +3k−1
+...+
...
...
...
...
+1 +2k−1 +3k−1 +...+ (n −2)k−1
+1 +2k−1 +3k−1 +...+ (n −2)k−1 +(n −1)k−1
or equivalently:
(n −1)1k−1 + (n −2)2k−1+ (n −3)3k−1+ ...+ [n −(n −1)](n −1)k−1
which is the difference of two terms:
⎧
⎨
⎩
n1k−1 + n2k−1 + n3k−1 + ... = nSk−1(n −1)
1k + 2k + 3k + ...
= Sk(n −1)
In conclusion:
n−1
∑
i=1
Sk−1(i) = nSk−1(n −1)−Sk(n −1)
(5.9)
and if we equate the right members of Eqs. (5.8) and (5.9), we get:
nSk−1(n −1)−Sk(n −1) = Sk(n)
k
+ Qk(n −1)
that is:

258
5 Numbers and Measures
nSk−1(n −1)−Qk(n −1) = Sk(n)
k
+ Sk(n −1)
which, adding nk to the both members, becomes:
nSk−1(n −1)−Qk(n −1)+ nk = Sk(n)
k
+ Sk(n −1)+ nk
that is
k + 1
k
Sk(n) = nSk−1(n −1)−Qk(n −1)+ nk
Sk(n) =
k
k + 1

nSk−1(n −1)−Qk(n −1)+ nk
and, by Eq. (5.7):
Sk(n) =
k
k + 1

n[(n −1)k/k + Pk−1(n −1)]−Qk(n −1)+ nk
and ﬁnally:
Sk(n) =
1
k + 1

n(n −1)k + kPk−1(n −1)−kQk(n −1)+ knk
which corresponds to the statement of the theorem.
⊓⊔
5.6.3
Geometrical Progressions
A second case of growth is that of a population where at each step each individual
generates k offspring, therefore at each step the population size increases of its size
multiplied by k (no external effect and no death is considered). In this case, assuming
an initial size equal to 1, the formula providing the population size at generation step
n is:
G(n) =
n
∑
i=0
ki.
We prove that
G(n) = kn −1
k −1 .
In fact, the proof of this equation is the following:
kG(n)−G(n) =
n+1
∑
i=1
ki −
n
∑
i=0
ki = kn+1 −1
that is,
G(n)(k −1) = kn+1 −1

5.6 Series and Growths
259
Fig. 5.13 The curves of some simple growths
therefore, the ﬁnal formula is obtained:
G(n) = kn+1 −1
k −1 .
Figure 5.13 shows many kinds of growth functions.
Sigmoid functions occur when growth processes are between two thresholds and
the growth rate is inversely proportional to the growth level. This happens, for ex-
ample, in the exponential sigmoid 2/(1 + e−t) displayed in Fig. 5.14.
Hill functions are another kind of sigmoid functions with shapes such as that
one displayed in Fig. 5.15:
H(x) =
Axn
Bn + xn .
5.6.4
Logistic Maps
The logistic map is a a growth rule which provides the fraction of living individuals,
in a population having a maximum size denoted by 1. If x denotes the fraction of liv-
ing individuals, then f(x) represents the fraction of them at the next generation (like
in the previous cases, the process evolves along a discrete sequence of generation
steps):

260
5 Numbers and Measures
Fig. 5.14 An exponential sigmoid function
Fig. 5.15 A Hill function where A = 10, B = 5, n = 4
f(x) = ax(1 −x).
This form expresses the size at the next step, as depending at the same time on the
number of living individuals x and on the number of potential living individuals
which can be added for reaching the maximum size. The parameter a is speciﬁc of
any particular kind of logistic rule. If we want to move from the function f to the
time series Gf (n), along the generation steps, we need to consider the iteration of
f, therefore:
Gf (n) = f n(x0)
where x0 is the size of the population at the initial time.

5.6 Series and Growths
261
Despite the simplicity of their form, logistic maps provide very rich behavioral
forms. When the parameter a is close to a value around 3.57, logistic growths be-
come chaotic. This implies many typical phenomena: sensitivity (starting from very
close values, then very different dynamics could be generated), transitivity (the dy-
namics takes, in an erratic way, almost all of its possible values), and dense peri-
odicity (any value is close, at any distance, to periodical states). A detailed analysis
of chaotic phenomena, and of the chaotic aspect of logistic maps, is outside the
scope of our discussion (see [183]), the literature about logistic map and related as-
pects is very rich and full of spectacular phenomena (very interesting images can
easily found on the web), we refer to [162] for a general analysis of iterated maps
and historical notes. In a very intuitive manner, chaos means impossibility of reli-
able predictions. The surprising fact disclosed by phenomena such as logistic maps
is deterministic chaos. The discovery of the existence of this possibility is one of
the deepest mathematical results of the last century, and can be found not only in
complex systems, but in very simple cases, as logistic growth shows. Then, in a de-
terministic chaotic phenomenon, unpredictability is not due to a lack of rule, but to
the impossibility of applying it with the exactness required for a reliable adequacy
to the reality modeled by it.
5.6.5
Natural Exponential Growth
Geometrical progressions are growths of exponential type. A pure exponential law
is of kind an. This happens in a population where, starting from one individual, at
each generation step every individual generates a individuals, but no individual at
a generation step can survive at the next generation step. Between arithmetical and
exponential growths there are intermediate levels, for example power laws such as
an2,an3,.... Of course, growths can be modeled with continuous time, and the base
of an exponential law can be lower than one, or equivalently, the exponents can take
negative values. However, here we want to mention the relationship between growth
and the Euler constant e, by following an analysis due to the Italian mathematician
of the last century Bruno De Finetti.
Let us consider a simple process where in a unitary time an initial quantity a0 be-
comes 2a0. We can assume that this process is due to a uniform increment along the
considered time interval. This means that if we partition the interval into n subin-
tervals, then during each of them a quantity a0/n is added to the initial quantity. In
fact, a0 + n(a0/n) = 2a0. Now, let us change this increment rule, by assuming that
at each step the increment by the 1/n factor is not applied to the initial quantity,
but to the global quantity cumulated along the preceding steps. We can express this
natural growth rule by the following inductive deﬁnition (i > 0):
ai+1 = ai + ai/n
this means that:
ai+1 = ai(1 + 1/n)

262
5 Numbers and Measures
therefore
an = a0(1 + 1/n)n.
It can be shown that, for any n, 2 < (1 + 1/n)n < 3 and, for increasing values of n,
the value (1 + 1/n)n approaches the limit value e = 2,71828....
In conclusion, the hypothesis of inﬁnitesimally distributed increment (in time and
in quantity) implies that after a unitary time, an initial quantity a0 become ea0, and
the growth law, along time t, is a0et.
5.6.6
Asymptotic Orders and Inﬁnitesimals
Growth processes can be compared according to their speeds. An exponential
growth with base greater than 1 seems to be faster than a quadratic growth law.
This is not completely right, because comparing 100t2 with 2t for t = 2 yields the
values 400 and 4 respectively, although the ﬁrst expression is quadratic and the sec-
ond one is exponential. However, for all the values t ≥15 the exponential function
overtakes the quadratic one by a gap increasing with the value of t. This means that
a comparison of growths is correct when it refers to the overall growth behavior,
possibly by forgetting a ﬁnite number of values. This intuition can be formalized by
the notion of almost everywhere comparison of positive non-decreasing functions.
A positive non-decreasing function f deﬁned on the set N is less than (or equal
to) a function g of the same type when, for some n0 ∈N, f(n) ≤g(n) for every value
n > n0. We write f(n) ≤∞g(n) to denote this fact.
O(g) denotes the class of positive non-decreasing functions { f | f(n) ≤∞g(n)}.
Often, with an abuse of notation, f ∈O(g) is indicated by writing f = O(g) (by
saying that f is a O of g). Symmetrically, Ω(g) denotes the class of functions
{ f | g(n) ≤∞f(n)}.
Any positive real number ε can be seen as a constant function yielding the value
ε everywhere.
An inﬁnitesimal is a function f deﬁned on the set N and taking values in the set
of real numbers such that their absolute values (the function yielding the absolute
values of f(n)) belong to O(ε) for any ε > 0 ∈R. The function f(n) is an inﬁnite
if 1/ f(n) is an inﬁnitesimal. Two functions f(n),g(n) are asymptotically equiva-
lent if 1 −f(n)/g(n) is an inﬁnitesimal. The class of functions f(n) such that, for
some positive constant k ∈R,k ̸= 0, k f(n) and g(n) are asymptotically equivalent
is denoted by θ(g).
All the concepts of classical inﬁnitesimal calculus can be developed in terms
of inﬁnitesimals. For example, the classical condition deﬁning the number z as the
limit of a real function f(x), for x approaching a value x0, usually denoted by the
following notation:
lim
x→x0 f(x) = z
is equivalent to the requirement that:
f(x0 + 1/n)−z

5.6 Series and Growths
263
and:
f(x0 −1/n)−z
are two asymptotically equivalent inﬁnitesimals.
The function f(n) is a zero of g(n) if f(n)/g(n) is an inﬁnitesimal, and o(g)
denotes the class of the zeros of g. If f ∈θ(g), then f and g are said to be of the
same asymptotic order, written f =o g. If f ∈o(g), then f is said to be of a smaller
asymptotic order with respect to g, and we also write f ≤o g.
The growth of functions from reals to reals can be compared by considering their
restrictions to the set of the natural numbers. It can be easily shown that polynomials
of the same degree have the same asymptotic order, while those of smaller degrees
have smaller asymptotic orders. Moreover, logb(x) ≤o x (for any b ≥2), and x ≤o
ax (for any a > 1). The following rules can be also proved. If f(x) ≤o g(x), then
f(x) + g(x) =o g(x). If f(x) ≤o g(x) and h(x) is an increasing real function, then
f(x)h(x) ≤o g(x)h(x). If f(x) ≤o g(x) and h(x) is a non-decreasing real function,
then f(h(x)) ≤o g(h(x)) and h(f(x)) ≤o h(g(x)). By means of these rules (about
polynomials, logarithms, exponentials, sums, products, and compositions) it is easy
to asymptotically compare a large class of real functions.
However, manipulating inﬁnitesimals and inﬁnites requires one to be very care-
ful, because very often the intuition may lead to wrong consequences. Let us men-
tion, for example, a surprising phenomenon related to the following harmonic se-
quence:
1, 1
2, 1
3, 1
4, 1
5,...
of course it is an inﬁnitesimal, but it can be easily shown that the following har-
monic series Hn is an inﬁnite:
Hn =
n
∑
i=1
1
i .
Table 5.22 The inﬁnity of harmonic series (Oresme, 1323 –1382)
Hn = 1+ 1
2 + 1
3 + 1
4 + 1
5 +...
=
1+ 1
2 +( 1
3 + 1
4)+( 1
5 +...+ 1
8)+... >
1+ 1
2 +( 1
4 + 1
4)+( 1
8 +...+ 1
8)+... =
1+ 1
2 + 2
4 + 4
8 +...
=
1+ 1
2 + 1
2 + 1
2 +...
= an inﬁnite
There exists a constant γ = 0.577215... (it is unknown whether γ is irrational),
called Euler-Mascheroni constant such that:
Hn = ln(n)+ γ.

264
5 Numbers and Measures
It can be shown that the function
ζ(k) =
∞
∑
i=1
1
ik
converges for any k > 1. The function ζ is extremely important in number theory.
Here we want to recall, from Sect. 5.4.3, the relationship, discovered by Euler, be-
tween ζ(2) and π:
ζ(2) =
∞
∑
i=1
1
i2 = π2
6 .
We remark that the name of Euler is strictly connected to almost all important spe-
cial numbers of mathematics. His crucial role in the deﬁnition of e (called Euler
number or Napier constant) and his famous formula connecting e with i were al-
ready explained. However, apart from the constant γ mentioned above, which fre-
quently occurs in many contexts, he was also the mathematician who popularized
the symbol π for the famous Archimedes’ number 3.1415926535... which, maybe,
is the irrational (trascendental) number of which we know the largest number of dec-
imal digits (about 1012 digits). This possibility is due to the BaileyBorweinPlouffe
formula (BBP formula) discovered in 1995, allowing for the computation of the nth
digit of π without calculating the ﬁrst n −1 digits.
5.7
Scales of Time, Space, and Matter
Any mathematization of a natural phenomenon is based on the measurement of
quantities and on the evaluation of their orders, that is, the powers, with respect to
a given base, that better approximate their values. Table 5.23 provides the decimal
scale of powers with their corresponding symbols and preﬁxes (it is interesting to
realize that the tenth power of 2 is 1024 which is close to the third power of 10).
Let us evaluate some measure orders. What is the magnitude order of a solar
year? A year is consists of 365 days of 24 hours of 3600 seconds. Therefore, one
year is 365×24×3,6×103 < 3,2×107 seconds, that is only a few tens of millions
of seconds.
What is the magnitude order of universe life? Assuming the universe started 15
billions of years ago, the number of seconds from the so-called “big-bang” are less
than 5 × 1017.
How many corn grains can be put on a chessboard where its 64 squares are
numbered and where one grain is put on the ﬁrst square, two on the second one,
and in general the double of the number of grains placed in a square is placed on
the next square (chessboard puzzle)? It is easy to evaluate this number as 264 −1.
If we approximate 210 with 103, and 24 with 15, then the overall evaluation gives
264 ≈(210)6×15 = 1.5×1019. This means that if a grain is one gram, we need about
15000 billions of tons of corn to cover the chessboard according to this procedure.
You can never ﬁnd such a corn quantity on Earth.

5.7 Scales of Time, Space, and Matter
265
Table 5.23 Decimal orders
Preﬁx
Symbol Power
yotta
Y
1024
zetta
Z
1021
exa
E
1018
peta
P
1015
tera
T
1012
giga
G
109
mega
M
106
kilo
K
103
hecto
h
102
deka
da
101
deci
d
10−1
centi
c
10−2
milli
m
10−3
micro
μ
10−6
nano
n
10−9
pico
p
10−12
femto
f
10−15
atto
a
10−18
zepto
z
10−21
yocto
y
10−24
Tables 5.24, 5.25, 5.26, 5.27, and 5.28 summarize some of the most impor-
tant measures on which universe, life and evolution are based on. The web site
http://bionumbers.hms.harvard.edu hosts a huge collection of num-
bers corresponding to measures of quantities of biological organisms. The numbers
of regarding measurements that relate to bacterium Escherichia coli constitute a ﬁle
of size near to one megabyte.
Usually, the macroscopic quantities of time, space, and mass are around three
or four orders above second, meter, and gram, respectively. The microscopic mag-
nitudes have the same range but six orders below these orders, while three or-
ders below the microscopic world we have the nanoscale. The visual resolution
of our eyes can distinguish at the level of one-tenth of a millimeter. The micro-
scope can increase that by around 2000 times, while more powerful instruments can
increase our sight by millions of times. Telescopes have visibility ranges around
1020, and by means of radio telescopes we can receive signals from the extreme
part of the universe. But structures of small molecules and atoms are placed at
nano scales. Therefore, we can see cells, but we cannot see inside molecules and
atoms, even if we can indirectly inspect their structure by using suitable physical
phenomena.

266
5 Numbers and Measures
Table 5.24 Matter aggregation levels (distances in meters)
Level
Distances
Aggregation Forces
Subatomic particles 10−15
Subnuclear
Atoms
10−12
Nuclear
Inorganic Molecules 10−10
Atomic-Electrical
Organic Molecules
10−8
Electrical-Chemical
Cells
10−6
Electrical-Chemical-Mechanical
Microorganisms
10−5
Electrical-Chemical-Mechanical
Organisms
101
Electrical-Chemical-Mechanical
Ecosystems
105
Electrical-Chemical-Mechanical
Planets
107
Gravitational
Stars
1014
Gravitational
Constellations
1017
Gravitational
Nebulosas
1021
Gravitational
Galaxies
1024
Gravitational
Universe
1027
Gravitational
Time-Space, Matter-Energy, Life-Evolution, Mind-Consciousness are the great
problems of science. They are always revisited to gain better understandings of their
nature, but since Plato’s allegory of the cave, presented in his Republic, our pictures
of reality are partial, inadequate, and continuously changing. The scientist, like the
philosopher, cannot directly see the reality because he cannot stand the truth dazzle.
But the truth is under this dazzling light, everything else are only shadows, indirect
and defective realities.
Table 5.25 Mass numbers in grams
Electron
10−28
Hydrogen
1.67·10−24
Nucleotide
10−21
Human genome 6·10−12
Cell
10−6
Human brain
1.5·103
Human body
6·104
Earth
1027
Sun
1033

5.7 Scales of Time, Space, and Matter
267
Table 5.26 Ages in years
Universe
15·109
Earth
4.5·109
Life
3.8·109
Eukariotic Life
1.8·109
Multicellular life
6·108
Mammals
2·106
Neanderthal Homo
105
Table 5.27 Physical and Biological numbers (space in meters, time in seconds)
Plank scale time
10−43
Big-bang time interval
10−40
Big-bang temperature
1028 Co
Light speed
3·108 m/s
Light Year
3·1023
Electromagnetic wave lengths
10−6 −−10−10
Gravitational/Electromagnetic force ratio
1036
One gram mass
3·1016 Joule
Atom types
102
Cell molecules
1010
Human body cells
1014
Human Brain neurons
1011
Human Brain synapses
1014
Human Brain elementary operation energy
10−16 Joule
Human genes
3·105
Human genome
3·109 base pairs
Mycoplasma G. genome
5.8·104 bp
Escherichia C. genome
4·106 bp
Gene lengths
103 −−104 bp
Human cell types
254
Human protein types
105
Human cell divisions per minute
2·107
Human immunological space
108 shapes
Table 5.28 Big numbers
Avogadro number
6.2·1023
Universe Atoms
1078
Universe Particles
1087
Gogol
10100
Chessboard Games 10120
Gogolplex
1010100

268
5 Numbers and Measures
5.8
Discrete Dynamical Systems
The mathematical formalizations of dynamical systems, in spite of their elementary
character, reveal many subtle points related to the puzzling nature of time. Here
we give a brief outline of some basic concepts (see [168, 181, 165, 182] for more
advanced concepts).
Given a function δ : S →S and i ∈N, the ith-iterates of δ are inductively deﬁned
by the following equations, for every s ∈S:
δ 0(s) = s,
δ i+1(s) = δ(δ i(s)).
Deﬁnition 5.4. An autonomous dynamical system is a pair (S,δ) where the set of
states S is also called the phase space and δ : S →S.
In an autonomous dynamical system, the dynamics δ is a next-state-function from
the phase space S to itself. According to the deﬁnition above, an autonomous dy-
namical system with an initial state s0 “represents” N if δ n(s0) ̸= δ m(s0) for every
n,m ∈N, with n ̸= m. In this perspective, the inﬁnite enumeration process is based
on the possibility of starting from s0 and generating, with the next-state-function, a
state that is new at every application of this function. If the set of states generated by
the next-state-function is ﬁnite, the counting process cannot be developed beyond a
certain bound, because surely the dynamics has a cycle (reaching an already gener-
ated state). In this case, it is possible to go beyond the bound by restarting the cycle
after its end, and by counting, in some way, the number of times the cycle is iterated
(this is the principle of calendars and chronologies, based on astronomical cycles).
In autonomous dynamical systems, time is a consequence of their internal dy-
namics. Instead, in the following deﬁnition of timed dynamical system, an external
notion of time is assumed.
Deﬁnition 5.5. A timed dynamical system D is a triple D = (S,T,δ) given by a set
S of states, called phase space, T ⊆R is a set of time instants, and δ : T →S is a
dynamics, which assigns to any time instant the state which the system takes at that
instant.
Any function f : R →R determines a dynamical system, in a trivial way. Motions,
developments, growth processes, and morphogenesis are representable as dynamical
systems. In planetary motions phase space is R6 because the position and velocity
of the planet with respect to three cartesian axes identify the state of the system, and
the dynamics is the motion law giving the planet positions and velocities at every
instant (a system of two planets needs a phase state included in R12). A dynamical
system D = (S,T,δ) is discrete if T is a subset of Z.
Proposition 5.6. An autonomous dynamical system (S,δ) determines a family of
timed dynamical systems Ds = {(S,N,δs) | s ∈S}, where dynamics δs is called the
timing of δ based on the initial state s ∈S, and it is deﬁned by δs(i) = δ i(s).

5.8 Discrete Dynamical Systems
269
The following deﬁnition of ﬂow generalizes the concept of autonomous system, by
using a next-state-function equipped with a real number as further parameter.
Deﬁnition 5.7. A ﬂow D is a triple D = (S,T,δ) given by i) a set S of states, called
phase space, ii) a set T ⊆R of time intervals such that 0 ∈T and when t,t′ ∈
T, then also t + t′ ∈T, and iii) a dynamics δ, which is a function δ : S × T →S
providing the state δ(s,t) which the system assumes after time t, starting from s. It
is required that δ(s,0) = s, and for any s ∈S and t,t′ ∈T we have δ(δ(s,t),t′) =
δ(s,t +t′). When T is a subset of Z, then D is a discrete ﬂow.
A timed dynamical system (S,T,δ) is totally discrete when both sets S and T are
ﬁnite or enumerable sets. Even though it sounds a little strange, a real number dec-
imal representation can be seen as a totally discrete dynamical system, whenever
we identify it with a sequence providing its decimal digits. In fact in this case
S = {0,1,2,...9} and T = N. Inﬁnite words, trees, or graphs are totally discrete
dynamical systems on suitable discrete phase spaces, where the dynamics is the
function giving the structure at some step of its generation. Examples of totally dis-
crete dynamical systems are: deductive and rewriting systems, Chomsky grammars,
automata, L systems, cellular automata (see Chap. 6).
Any computation is a kind of dynamics. However, the two concepts are based
on different perspectives. In a computation an input is provided to the system (in
an initial state) and an output is expected at the end of the process if the system
reaches a state which is considered as ﬁnal, according to some termination crite-
rion. This means that the dynamics underlying the move of a computation from an
initial to a ﬁnal state is functional to the desired relation between input and output,
and the sooner a ﬁnal state is obtained, the better the computation is realized. On
the contrary, in a dynamical system the focus of the process is on the dynamics
itself. Input and output can be even disposed of, but when they are present, they
are used in order to guarantee that dynamics could proceed by satisfying some be-
havioral requirements (e.g., acquiring or expelling substances). A major example
of this dynamical perspective are living organisms. They are open systems, that is,
they need inputs and outputs, to ensure that a dynamics, with speciﬁc features, can
be maintained in time, as long as possible. In this perspective, it is very important
to individuate dynamical properties which are relevant in life phenomena, and to
discover principles according to which some artiﬁcial dynamics exhibits behaviors
with these properties.
The deﬁnition of autonomous systems implicitly assumes that the application of
δ can be considered uniform, that is, the same temporal interval is assumed for
any application of δ. In the late 16th century, Galileo discovered the pendulum, as
a physical phenomenon for which this uniformity feature may be assumed. This
discovery is the beginning of the experimental science, which is intrinsically based
on the measure of time.
Does time come before any dynamics, or vice versa is time an abstraction orig-
inated by dynamical phenomena? In fact, time apart from dynamics seems to be
a metaphysical abstraction. From an epistemological point of view, we only have
clocks, and any physical notion of time is always related to a clock, that is, a system

270
5 Numbers and Measures
(S,δ,s0) where (S,δ) is an autonomous dynamical system, and s0 ∈S is an initial
state, conventionally associated to time 0. With a clock, it is intrinsically assumed
that the application of its next-state-function can be considered as a uniform pro-
cess, in such a way that the transition from a state to the next one happens according
to the same law, with no appreciable difference in the quantity of time (whatever
this could mean) necessary to perform that transition. In other words, clocks are
physical entities where we can observe regular events of transition from a state to
another state. If k is a bound such that no physical clock is available at a level inferior
to 10−k seconds (the time measurement unit), what reason can be used to assume
the existence of time at smaller temporal scales?
One of the most important concepts in dynamical systems is that of attractor.
There are many ways to formalize this notion. Intuitively, an attractor is a quasi
state (a set of states) such that when a trajectory reaches it (a state in it), then this
trajectory remains inside it. Moreover, this quasi state is included in a bigger quasi
state, called its basin, such that any trajectory running through the basin, after a
while (or according to a limit process), tends to “fall” into the attractor. A basin
is a case of dynamically invariant set B, that is, in a dynamical system (S,δ), if
x ∈B, then δ(x) ∈B. An attractor of basin B is a sort of minimal or limit subset of B
where dynamics could remains eternally conﬁned. This means that attractors can be
viewed as “dynamical states”, or even, as second level states. For example, a living
organism in a stable situation, performing life functions, moves along a trajectory,
which macroscopically seems just a state, but corresponds essentially to an attractor.
In mammals there are around 200 cell types. The stable states of these cells can be
seen as different attractors where cell dynamics may fall, in order to satisfy some
speciﬁc conditions corresponding to their biological role.
Example 5.8. The Collatz dynamical system [191] is the autonomous system on pos-
itive natural numbers where dynamics is given, for any x ∈N, by 3x+ 1 if x is odd,
and x/2 if x is even. . The Collatz conjecture claims that {1,2,4} is the attractor
of this system (starting from any number you fall into the 1 −4 −2 −1 loop, and
then you remain there). It has been proved to be true for numbers until around 260
(a web computational project on this topic is actively testing the Collatz conjecture,
see http://www.ericr.nl/wondrous/).
A simple notion of attractor can be deﬁned in the following way.
Deﬁnition 5.9. Given a dynamical system of dynamics δ, an attractor A of basin B,
with A ⊆B, is a set of states such that, for any x ∈A, δ(x) ∈A, and for any x ∈B
there exist a natural number n such that δ n(x) ∈A.
An attractor A of basin B is called minimal if A ⊆C for any attractor C of basin B.
Usually, attractors are intended as minimal attractors. It can be easily proved that a
minimal attractor is consisting of only one state or of a cycle.
Time measurement is present as soon as the ﬁrst human civilizations are devel-
oped. In any organized society, especially when it reaches a certain level of com-
plexity, humans need to dominate the ﬂow of time, in order to plan, project and
organize events and activities. The common experience of the day/night alternation,

5.8 Discrete Dynamical Systems
271
the seasons, and the natural cycles are the primitive points of reference in the ﬂow
of time. The discovery of astronomical regularities represents a more sophisticated
way of dominating time. Stars give the spatial orientation and the temporal rhythms
to human activities. But, in a sense the time of stars, in the primitive societies on
the earth, depends on the sky, and a reliable and precise synchronization of events
is possible only at local level.
Clocks were available even before Galileo’s discovery. In the middle ages, the
improvement of some technical devices is the basis of clocks, usually put in public
buildings, which have a precision acceptable for everyday life. However, the preci-
sion necessary for scientiﬁc measurements was possible after Galileo’s discovery of
the isochrony of small oscillations. Galileo observed that the oscillation period of
a pendulum is constant, regardless of the angle of the swing, if it is around 3 de-
grees. On the basis of this phenomenon, further important technical improvements
allowed the construction of reliable clocks (for example, Christiaan Huygens’ and
Robert Hooke’s balance spring).
This was the passage from an astronomical time to a physical time, and was
the basis of modern physics where motion laws are expressed by equations relating
space measurements with the measurements of times necessary to cover them. Phys-
ical measurable time was also the basis for dominating the ﬂow of time at nonlocal
ranges, for example, for the needs of reliable and precise methods of sea navigation.

Platonic Dodecahedron
6
Languages and Grammars
Abstract. Strings are sequences on which some speciﬁc operations are deﬁned. The
most important operation over strings is the concatenation. If α and β are two
strings, their concatenation, usually denoted by αβ, is the sequence where the last
element of α is followed by the elements of β, in the order they have in β. Formal
languages are sets of strings. In this chapter we present some basic concepts of
formal language theory: languages, grammars, automata, expressions, and patterns.
We conclude by outlining the notions of decidability and undecidability, and with
two famous kinds of computation automata: Turing machines and register machines.
6.1
Strings and Languages
Formal language theory (FLT) started at the beginning of the 20th century with
some seminal papers by Thue [195], where strings are considered as mathematical
objects deﬁning the algebraic structure of monoid, based on the binary operation
of concatenation (juxtaposition of strings) that is associative. When the theory of
computation (and computability) started and symbolic processes were investigated
in mathematical terms [214, 208, 209, 196, 197, 203], then formal languages, as sets
of strings, and classes of formal languages proved to be crucial for a wide class of
concepts that became of fundamental importance for computer science [201, 199,
206, 210, 211, 213].
Given an alphabet A, we denote by A∗the set of ﬁnite sequences of symbols of A
equipped by the operation of concatenation such that, if:
α = α1,α2,...,αn
β = β1,β2,...,βm
V. Manca: Infobiotics, ECC 3, pp. 273–307.
DOI: 10.1007/978-3-642-36223-1_6
c⃝Springer-Verlag Berlin Heidelberg 2013

274
6 Languages and Grammars
then their concatenation α ·β is deﬁned as:
α ·β = α1,α2,...,αn,β1,β2,...,βm.
This means that α ·β is constituted by the symbols of α in the order they have in α
followed by the symbols of β (in the order they have in β). In this case, sequences
are more properly called strings (over the alphabet A) and very often α · β is ab-
breviated as αβ, by omitting the symbol of concatenation (juxtaposition notation).
Moreover, very often, comma between symbols is omitted. Strings will be usually
denoted by Greek letters. We recall that when we say symbols of a string, we mean
symbol occurrences, because the same symbol may occur many times. The length
of a string α, denoted by |α|, is the number of symbols occurring in α (the sum of
their multiplicities).
The concatenation of strings is an associative operation:
α(βγ) = (αβ)γ.
It is also useful to introduce a special string, called the empty string and denoted
by λ with no symbol occurring in it, such that:
αλ = λα = α.
If n ∈N, the iteration αn of a string α is deﬁned by concatenating n copies of the
string α, by assuming that α0 = λ. If for some (possibly empty) strings β,γ,δ it
holds that:
α = βγδ
then β,γ, and δ are substrings of α and in particular β is a preﬁx of α, δ is a sufﬁx
of α, and γ is an inﬁx of α.
Notation γ ⊆α means that γ is a substring of α, and γ ⊂α means that γ is a
substring of α which is different from α.
The reverse of a string α = α1α2 ...αn is the string where symbols occur in the
reverse order, with respect to α:
rev(α) = αnαn−1 ...α1.
Given a string α, then, if i ≤j ≤|α|, then α[i, j] denotes the substring selection of
α, which is the string constituted by the symbols occurring in α from i to j, in the
same order they occur in α. We denote by α(i) = α[i,i] the symbol of α occurring
in its position i.
Many kinds of operations of insertion, deletion, and permutation can be deﬁned
in terms of concatenation, length, and substring selection.
Given a string α, its left derivation ∂α is an operation such that:
∂α(αβ) = β

6.1 Strings and Languages
275
and ∂α(β) = λ if α is not a preﬁx of β (an analogous deﬁnition can be given for the
right derivation, by replacing preﬁxes by sufﬁxes).
A (formal) language L over an alphabet A is a set of strings over this alphabet,
that is, L ∈P(A∗). Of course, the alphabet A is a language over A, A∗is also a
language over A, and any ﬁnite (or empty) subset of A∗is a language over A.
For example, all the strings over an alphabet A of length equal to or smaller than
n deﬁne a (ﬁnite) language over the alphabet A:
{α ∈A∗| |α| ≤n}
The following are languages over an alphabet A:
{αα | α ∈A∗}
{α ∈A∗| α ̸= β n for any β ∈A∗,n ∈N}
Any string α ∈A∗provides the following languages:
Pref(α) = {β | βγ = α,γ ∈A∗}
Su f(α) = {β | γβ = α,γ ∈A∗}
Sub(α) = {β | β ⊆α}
If ω is an operation deﬁned on strings over an alphabet A, then we can extend it to
any language L over A by putting:
ω(L) = {ω(α) | α ∈L}.
The deﬁnition of languages as particular sets, allows us to apply to them all the usual
boolean operation on set (union, intersection, and difference). However, the catena-
tive nature of strings provides other natural operations. Table 6.1 reports the main
operations on languages, where L1,L2 ∈P(A∗). In the context of formal languages,
for reasons of similarity with the notation that will be introduced, set-theoretic union
will be also called sum and will be denoted by +.
Abbreviation αL stands for {α} ·L and α + L stands for {α} + L.
A language expression E is constructed by applying language operations on
some initially given languages. A grammar G for a language L is an algorithm
Table 6.1 Operations on languages
L1 +L2 = {α |α ∈L1, or α ∈L2}
language sum
L1 −L2 = {α |α ∈L1,α ̸∈L2}
language difference
L1 ∩L2 = {α |α ∈L1,α ∈L2}
language intersection
L1 ·L2 = {αβ |α ∈L1,β ∈L2}
language concatenatiion
Ln = {α1α2 ...αn | αi ∈L, for i = 1,2,...,n}
language iteration (n ≥1)
L∗= {α1α2 ...αn | αi ∈L, for i = 1,2,...,n,and n ∈N}
Kleene star

276
6 Languages and Grammars
which generates all and only the strings of L. An automaton M for L is an algorithm
which recognizes all and only the strings of L. A string pattern P is a linear structure
(represented by an algebraic expression, as we see later on) which is common to all
the strings of L.
We will denote by L(E),L(G),L(M),L(P) the language speciﬁed by the gram-
mar G, by the automaton M, and by the pattern P respectively. A typical problem of
formal language theory is the passage from one method of speciﬁcation of a given
language, to another method which speciﬁes the same language.
The intuition behind a grammar is that of a set of rules for writing words (syntac-
tic forms). An automation can be seen as a device which, when it takes a symbolic
form as an input, it answers yes/not in correspondence of a recognized/unrecognized
structure. The term pattern comes from Latin paternitas and expresses an origin
which is common to all the strings with a speciﬁc imprinting. These strings are said
to be instances of the pattern. In formal terms, a pattern (of strings) can be deﬁned
as an expression built by means of string operations, from some strings, and vari-
ables ranging in some speciﬁc sets, and/or satisfying some conditions. When values
are given to the variables of a pattern P, the corresponding values that P assume are
the strings of L(P).
An analogy can be useful to clarify the notion of (string) pattern. In fact, a poly-
nomial with positive integer coefﬁcients and positive integer variables can be con-
sidered a number pattern. For example,
3x+ 4y
is a pattern such that number 10 is an instance of it, in correspondence to the values
x = 2,y = 1, but 5 is not an instance of this pattern because the equation 3x+4y = 5
does not hold for any pair of positive integer values of x,y. Analogously, the string
pattern axx with x ∈A∗has the string abb as its instance, but the string aab cannot
be an instance of it.
The following language is constructed by starting from languages {a} and {b}
by Kleene star and language concatenation:
{a}∗·{b}∗.
The same language is speciﬁed by the pattern anbm with n,m ∈N.
Some important patterns deﬁning languages (over alphabets {a,b} and {a,b,c}
respectively) are the following, where term soma (meaning body in Greek) refers to
their interpretations, as growing structures (of one, two, or three components):
Mono-somatic Pattern = an with n ∈N,n > 0
Bi-somatic Pattern = anbn with n ∈N,n > 0
Tri-somatic Pattern = anbncn with n ∈N,n > 0.

6.2 Grammars and Chomsky Hierarchy
277
The ﬁrst one expresses a simple homogeneous linear form. The second one ex-
presses a symmetric linear form constituted by two parts having the same size, but
of two different types. The third one has an analogous structure with three parts.
This kind of form corresponds to an idealized form of development which is typi-
cal of many animal embryos (ectoderm, mesoderm, endoderm). In the next section,
we will present some grammars generating the languages corresponding to these
patterns.
6.2
Grammars and Chomsky Hierarchy
In Formal Language Theory a grammar is a formal device generating strings. In
this sense, a (formal) grammar is similar to a logical calculus equipped with axioms
from which it derives theorems. In fact, the ﬁrst kinds of formal systems generating
strings, introduced by Post [208, 209], were inspired from the deduction systems of
formal logic. The following deﬁnition, due to Chomsky [196] is a particular case of
Post system.
Deﬁnition 6.1. A (Chomsky) grammar G is a generative device. It is speciﬁed by
four components:
G = (A,T,S,R)
• A ﬁnite alphabet A;
• A subset T of A of terminal symbols;
• an initial symbol S belonging to nonterminal symbols in A−T;
• a ﬁnite set R of rules α →β with α,β ∈A∗and α ̸= λ, that is, pairs of strings
over A.
A grammar G = (A,T,S,R) deﬁnes a language by means of two relations: a direct
rewriting relation ⇒G, and a transitive rewriting relation ⇒∗
G:
ϕ ⇒G ψ
iff
ϕ = γαδ
ψ = γβδ
α →β ∈R.
while
ϕ ⇒∗
G ψ
iff for some sequence ϕ1,ϕ2,...,ϕn of strings:
ϕ = ϕ1 ⇒G ϕ2 ⇒G ... ⇒G ϕn = ψ.
The language L(G) generated by the grammar G is the set of strings of terminal
symbols obtained by transitive rewriting from the initial symbol S of G.
The language deﬁned by the bi-somatic pattern is generated by the grammar of
Table 6.3 (S is the initial symbol, and terminal symbols are lower case letters).

278
6 Languages and Grammars
Table 6.2 The language L(G) generated by the grammar G
L(G) = {α ∈T ∗|S ⇒∗
G α}
Table 6.3 A grammar for the bi-somatic language
S →ab
S →aSb
In fact, by applying the second rule a certain number n of times, n symbols a are
generated as preﬁx of S followed by n symbols b as sufﬁx. With the ﬁrst rule S is
replaced by ab, therefore instances of the bi-somatic pattern are always produced.
The language deﬁned by the tri-somatic pattern is generated by the grammar of
Table 6.4.
Table 6.4 A grammar for the tri-somatic language
S →abc
S →aSBc
cB →Bc
bB →bb
In fact, by applying the ﬁrst two rules we get a string with a number n of symbols
a as a preﬁx, followed by a string abc and by the same number n of pairs Bc. The
last two rules of the grammar move symbols B in the middle and symbols c at the
end of the string. When a symbol B comes on the right of a symbol b it transforms
into b, according to the last rule. In this way all the B which reach the middle of the
string become b, but the number of B is equal to the number of a and of c. Therefore,
instances of the tri-somatic pattern are always produced by the grammar of Table
6.4. Moreover, it is easy to verify that, for whatever is the order of application of the
last two rules, the same terminal string is generated.
The mono-somatic language deﬁned by the pattern an (n > 0) is generated by the
grammar of Table 6.5.
Table 6.5 A grammar for the mono-somatic language
S →aS
S →a

6.2 Grammars and Chomsky Hierarchy
279
Table 6.6 A grammar for the bipartite language
S →aS
S →Sb
S →a
S →b
Table 6.7 Another grammar for the tri-somatic language
S →abc
S →Saabc
Sa →aSb
Sba →aSb
Sbb →bbSc
Scb →bSc
Scc →S′cc
Scc →cc
bS′ →S′b
aS′ →aaSb
The language deﬁned by the bipartite pattern anbm (n,m ∈N,n+m ̸= 0) is gener-
ated by the grammar of Table 6.6.
Another grammar generating the tri-somatic language is given in Table 6.7. The
idea behind its construction is that a special symbol “writes” a new terminal symbol
a and transforms in a symbol moving on the right and searching for a symbol b.
When a b is reached it writes a new symbol b and transforms in a symbol search-
ing (on the right) for c. When c is reached a new symbol c is written and either
rewriting concludes, or a search (on the left) for a begins with another analogous
rewriting cycle. In this way, the synchronized rewriting of the three symbol, in the
right positions, is performed which guarantees the generation of tri-somatic forms.
The same strategy can be adapted for generating a lot of interesting linear patterns.
For example, the pattern of duplicated strings L(α) can be generated with a similar
kind of strategy. In this kind of generative mechanism two possibilities are essen-
tial: i) moving a symbol along different parts of a string, which are possibly far and
at unbounded distances, and ii) transform a non-terminal symbol according to the
symbols occurring around it. These kinds of mechanisms can be realized only with
rules which rewrite more than one symbol.
The grammars given above correspond to different models of development of
linear structures. For example, in the bipartite pattern the only requirement is the
distinction of two parts an a-part on the left and a b-part on the right. In the case
of bi-somatic pattern, the two distinct parts are required to have the same size. This
means that a-part and b-part have to grow in a synchronized way. Finally, the tri-
somatic pattern extends, from two to three, the requirement of equal size and distinct

280
6 Languages and Grammars
parts. This implies that the structure growth needs to use long distance relationships
or symbol transport mechanisms which guarantee the required symmetry.
The rules of a grammar can be classiﬁed according to four basic types individu-
ated by Chomsky and given in Table 6.8.
Table 6.8 Types of grammatical rules
Rule Types
G = (A,T,S,R)
α →β ∈R
Type 0
α ∈A∗−T ∗
General
Type 1
Type 0 + condition: |α| ≤|β|
Monotone (Context-sensitive)
Type 2
Type 1 + condition: α ∈A−T
Context-free
Type 3
Type 2 + condition: β ∈T +T(A−T)
Right-linear
According to the previous classiﬁcation, types are inclusive, in the sense that a
rule of type i + 1 satisﬁes the conditions of type i, but the proper type of a rule
is the maximum type which we can assign to it. A grammar G is of type i with
i ∈{0,1,2,3} if i is the minimum of the proper types which can be assigned to the
rules of G. A language is of type i if it can be generated by a grammar of type i. It
is properly of type i (i < 3) if it is generated by a grammar of type i and cannot be
generated by a grammar of type i+ 1.
Let us indicate by Li the class of languages generated by grammars of type i.
A fundamental result about these grammatical types is the following strict
inclusion of these classes of languages, constituting the well known Chomsky
Hierarchy.
L3 ⊂L2 ⊂L1 ⊂L0
In fact, all the ﬁnite languages are of type 3, but there are inﬁnite languages
of type 3, for example, the language L(an) (n > 0) is of type 3, but is not ﬁnite.
Moreover it can be proved that: i) the bi-somatic language is properly of type 2; ii)
the tri-somatic language is properly of type 1, and a class of languages there exists
(the class REC will mention below), which is properly included in L0, and which
properly includes L1.
Let RE be the class of recursively enumerable languages, for which an algo-
rithm exists which enumerate all its strings and only them. An important result about
Chomsky grammars states that:
L0 = RE.
A classical result in computation theory, which was one of the most important
achievements of Turing analysis in his fundamental paper of 1936 (see Sect. 6.5),
establishes that, if REC are the recursive languages, for which the membership of a
string can be algorithmically decided (in ﬁnite number of computation steps), then:

6.2 Grammars and Chomsky Hierarchy
281
REC ⊂RE.
The inclusion of the classCS = L1 in the class RE is a consequence of the monotony
condition for the grammars of type 1. Moreover, the existence of a language of REC
which does not belong to CS can be obtained by a kind of reasoning going back to
Cantor and Russell (the diagonal argument).
It can be shown that any rule of type 1 is equivalent, from the generative point
of view, to a set of rules having the following context-sensitive form, where X is a
non-terminal symbol and α,β,γ are strings of the alphabet A with γ ̸= λ:
αXβ →αγβ.
Languages generated by grammars with context-sensitive rules are also called
context-sensitive and their class is denoted by CS. Therefore, CS = L1. Rules of
type 2 are called context-free and languages generated by grammars of type 2 are
also denoted by CF. Therefore, CF = L2.
In the next section we show that L3 coincides with the class REG of regular
languages obtainable from ﬁnite languages by applying the language operations of
concatenation, sum, and Kleene star.
If X,Y are non-terminal symbols and a is terminal symbol, then X →aY is a
right-linear rule, while X →Ya is a left-linear rule, and X →a is at same time
right-linear and left-linear.
Grammars with both types of rules right-linear and left-linear generate the class
LIN of linear languages which is strictly included in CF and which strictly includes
REG.
In conclusion, Chomsky hierarchy can be reformulated and extended in the fol-
lowing way:
FIN ⊂REG ⊂LIN ⊂CF ⊂CS ⊂REC ⊂RE.
Two important aspects of grammatical generation which are interrelated are cru-
cial: i) rules are applied without no strategy, ii) terminal symbols are the only mech-
anism for discriminating, among the strings generated from S, those which belong
to the language L(G). It can be shown that if we do not use terminal symbols,
then the generation power of grammars is reduced. In this way, the terminal/non-
terminal distinction plays the role of integrating the rules toward the realization of
forms which are speciﬁc of a given language. In other words, only string genera-
tions where rules are applied in a “correct” way are able to “terminalize”. The other
generations can be viewed as unsuccessful trials in a speciﬁc process of string gen-
eration.
Natural languages, programming languages, and strings representing linear forms
that occur in natural processes require grammars having a complexity between
context-free and context-sensitive grammars.

282
6 Languages and Grammars
The theory of formal languages studies four main phenomena concerning formal
languages:
1. equivalence results, showing that different methods and formal devices deﬁne
the same languages. An example of such a kind of results is given in the next
section where an equivalence will be given which implies an equivalence be-
tween regular grammars and the class of ﬁnite state automata;
2. normalization results, showing that some formal devices deﬁning languages can
be put in some speciﬁc formats which allow to simplify the analysis of speciﬁc
aspects. For example, any grammar with monotone rules generates the same lan-
guage of a grammar where rules have the context sensitive form.
3. decidability results, showing that some class of problems about languages can
be solved in an algorithmic way, by reaching the solution in a ﬁnite number of
steps. For example, given a grammar CF it is possible to decide if it generates a
ﬁnite or inﬁnite language;
4. closure results showing that some classes of languages are closed with respect to
some language operations. For example, the sum of two CF languages is a CF
language too, but CF is not closed with respect to complementation (with respect
to the set of strings over its terminal alphabet), while REG and CS are closed with
respect to all boolean operations (sum, intersection, and complementation).
6.3
Regular Expressions and Finite Automata
Consider singleton languages, that is, languages constituted by only one string. For
example {a},{b},{c}. Then, let us construct languages by applying to them the
language operation of sum, concatenation, and Kleene star, with an implicit priority
of star with respect to concatenation, and of concatenation, with respect to sum. For
example:
{a}∗+ {b}{c}∗.
Languages built in this manner are called regular languages. If we adopt the ab-
breviation of eliminating set parentheses, by assuming that strings specify singleton
languages, then we get regular expressions which naturally identify regular lan-
guages. For example, the language above is simply denoted by the following regular
expression (parentheses can be introduced for a better reading of expressions):
a∗+ b c∗.
Regular expressions are related to ﬁnite state automata.
A ﬁnite state automaton M is a device equipped with: i) an alphabet, ii) a ﬁnite
set of internal states, one of which is the initial state, and some of which are the
ﬁnal states (see Fig. 6.1), iii) plus a set of transition rules which determine the
following behavior. When a string α of an alphabet of the automaton is put on its
tape, then M reads it by starting from the ﬁrst symbol on the left of α and in its initial
state. After reading a symbol, in dependence of its transition rules, M changes its
state and moves to read the next symbol on the right. When the whole string α was

6.3 Regular Expressions and Finite Automata
283
M
?
Q
Fig. 6.1 A ﬁnite state automaton at beginning of its functioning
read, M “accepts” α, if the state assumed by M belongs to the set of its ﬁnal states.
Otherwise, M does not accept α. We denote by L(M) the language recognized by
M, that is, the set of strings which are accepted by the automaton M.
Figure 6.2 describes, by means of a graph, an automaton which recognizes the
bipartite language L(anbm). Initial and ﬁnal states are indicated by entering and
exiting arrows respectively that are not linked with other states.
?
b
6
-
-

a
b
a
b
a


?
?




q2
q1
q0


Fig. 6.2 A ﬁnite state automaton recognizing the bipartite language
The formal deﬁnition of ﬁnite state automaton follows.
Deﬁnition 6.2. A (deterministic) ﬁnite state automaton M is given by the following
structure:
M = (A,Q,τ,F,q0)

284
6 Languages and Grammars
where A is the alphabet, Q the set of states, F the set of ﬁnal states, q0 is the initial
state, and τ is the state transition function, from pairs of symbols and states to states:
τ : A× Q →Q.
M reads an input string α, starting from the ﬁrst symbol (from the left) of α and in
the state q0 as current state. The automaton M applies the transition function τ to
the symbol x that it reads and to its current state q, by taking τ(x,q) as next current
state, and by passing to read the symbol on the right of the symbol x previously read.
When the whole input string is read, then M halts. If the state of M, when it halts,
belongs to F, then α is accepted by M, otherwise α is not accepted by M.
We can express the transition function of an automaton by a rewriting relation. For
example the transition of the automaton given in Fig. 6.2 could be given by (q0
initial state and q0,q1 ﬁnal states):
Table 6.9 The automaton recognizing the bipartite language expressed by state transition
rules
q0a →q0
q0b →q1
q1b →q1
q1a →q2
q2a →q2
q2b →q2
In terms of rewriting relation, the language recognized by a ﬁnite automaton M
can be deﬁned in the following way:
L(M) = {α ∈A∗| q0α →∗
M q f ,q f ∈F}.
The transition rules representation of a ﬁnite state automaton can be easily trans-
formed into a grammar if we start from the initial state q0 and in passing from a
state to another one we generate a symbol instead of reading it. For example, in Ta-
ble 6.10, the rules of automaton of Table 6.9 are transformed into generative rules
R which provide the type 3 grammar G = ({a,b,q0,q1,q2},{a,b},q0,R) generating
the bi-partite language. In this case, the initial state of the automaton becomes the
start symbol of the grammar G, and rules q0 →a, q0 →b, and q1 →b are added for
deleting the ﬁnal states q0,q1. Transition rules of the automaton which are relative to
state q2 do not need to be translated into generative rules. This method can be easily
generalized, by proving that any language recognized by a ﬁnite automaton is also

6.3 Regular Expressions and Finite Automata
285
generated by a type 3 grammar. In an analogous way it is simple to deﬁne the inverse
process of constructing a ﬁnite automaton which recognizes the language generated
by a type 3 grammar. In conclusion, the two kinds of formal devices determine the
same class of languages.
Table 6.10 A type 3 grammar generating the bi-partite language
q0 →aq0
q0 →bq1
q1 →bq1
q0 →a
q0 →b
q1 →b
A ﬁnite state automaton M is deterministic when, for every state, only one pos-
sible transition can be applied. This means that the codomain of τ is Q:
τ : A× Q →Q.
A deterministic automaton M recognizes a string α (over its alphabet) when starting
in the initial state and reading α on its tape (from the ﬁrst symbol on the left to the
last on the right), by applying the transition function τ, M ends in a ﬁnal state.
A ﬁnite state automaton M is nondeterministic when, for every state, one among
a set of possible transitions can be applied. This means that the codomain of τ is
P(Q):
τ : A× Q →P(Q).
A nondeterministic automaton M recognizes a string α (over its alphabet) when
starting in the initial state and reading α on its tape, if M chooses at each step one
of the possible transitions, for some sequence of choices, M ends in a ﬁnal state.
Proposition 6.3. The language bi-somatic L(anbn) cannot be recognized by any ﬁ-
nite state deterministic automaton.
Proof. Any ﬁnite state automaton M determines an equivalence relation ≡M be-
tween the strings over the alphabet of M such that α ≡M β if M ﬁnishes reading
both the strings in the same state. Now, let {a,b} the alphabet of M, then having
M a ﬁnite number of states, there exist two distinct natural numbers n,m such that
an ≡M am. If we concatenate both an and am by the same string bn, then we get anbn
and ambn. Assume that M could recognize L(anbn), then, when M reads an and am,
it ends in the same state, according to the equivalence ≡M, therefore, it ends in the
same state also when it reads both strings anbn and ambn. But this is a contradiction
because we assumed that M is able to recognize L(anbn).
⊓⊔

286
6 Languages and Grammars
For the sake of brevity, usually we will avoid to provide all the transition rules
of a ﬁnite automaton. In fact, we adopt the convention that when a transition
is not explicitly given, it is assumed that the state speciﬁed by the missing
transition is a special non-ﬁnal state, say it a failure state, which does not
belong the the ﬁnal states, where the automaton remains for any other symbol
read in that state.
In some cases it is useful to use λ-transition, that is, the automaton can change
its state without reading any symbol. However, it can be shown that any automaton
with λ-transition has an equivalent automaton without λ-transitions.
Let us consider the pattern abxy where x ∈{a,b,c}∗,y ∈{a,c}. The correspond-
ing regular expression is
ab(a + b + c)∗(a + c)
Figure 6.4 gives two automata recognizing the language L(ab(a + b + c)∗(a + c)).
The automaton on the top is nondeterministic because reading the symbols a,c in
the top rightmost state different edges can be followed, by reaching different states.
On the bottom, an equivalent deterministic automaton is given. Usually, the nonder-
ministic automaton equivalent to a deterministic one has a simpler structure, but its
functioning is more complex (all the possible behaviors have to be considered for
recognizing a given string).
A result due to Kleene is expressed by the following proposition which we report
without proof.
Proposition 6.4. Any language which is recognized by a nondeterministic ﬁnite
state automaton is also recognized by a suitable deterministic ﬁnite state automaton.
Finite state automata were inspired by studies in modeling of neurological activity.
The original paper of S. C. Kleene, in 1956, where these automata were introduced,
was entitled Representation of events in nerve nets and ﬁnite automata, which
was based of the fundamental paper of McCulloch and Pitts of 1943 (A logical
calculus of the ideas immanent in nervous activity).
Another result, proved by Kleene in the paper mentioned above, shows the equiv-
alence between FSA (Finite State Automata) and REG. In fact the following propo-
sition holds.
Proposition 6.5. REG ≡FSA, that is, the class of languages generated by ﬁnite
languages by applying, a ﬁnite number of times, the operations of concatenation,
sum, and Kleene star, coincides with the class of languages recognized by ﬁnite
state automata.
Proof. Given a regular language L, a (nondeterministic) ﬁnite automaton recogniz-
ing it can be easily constructed. In fact, if a regular language L1 is recognized by
M1 and a regular language L2 is recognized by M2, then putting together the graphs
representing the transition rules of the automata M1 and M2, and connecting the ﬁnal

6.3 Regular Expressions and Finite Automata
287
Fig. 6.3 A nondeterministic (top) automaton and a deterministic (bottom) automaton recog-
nizing L(ab(a+b+c)∗(a+c))
states if M1 with the initial state of M2, we get an automaton recognizing L1 ·L2. If
the two initial states of M1 and M2 are “fused” in a unique initial states connecting
the remaining parts of the two graphs, then we get an automaton recognizing the
language L1 + L2. Finally, if a graph describes the transitions of an automaton M
recognizing L, when in the graph of M, any transition edge entering in a ﬁnal state,
is replicated by a new transition edge which connects that ﬁnal state with the initial
of M, then a new automaton is obtained which recognizes the language correspond-
ing to L∗. These three constructions deﬁne a general procedure for constructing an
automaton equivalent to a given regular expression.
The more complex part of the proof concerns the identiﬁcation of the regular
language recognized by a given ﬁnite state automaton. We will show that it can
be obtained by applying the operation of sum, iteration, and Kleene star by start-
ing from ﬁnite languages. The proof we report is the original one of Kleene. It is

288
6 Languages and Grammars
based on the deﬁnition of the following language, relative to a given automaton
M = (A,Q,τ,F,q1) with Q = {q1,q2,...,,qn}
Li,jn
of the strings which produces the state passage of M from qi to q j possibly by
passing through q1,q2,...,qn.
Of course, we have:
Li,j0 ⊆A
moreover, for all i, j ∈N, the following equation holds:
Li,jk+1 = Li,jk + Li,k+1k ·(Lk+1,k+1k)
∗·Lk+1,jk
The validity of this equation can be explained if we consider the following example
of chain of state transitions in the automaton M, where dots between states denotes
intermediate states among {q1,q2,...,qk}, and the state qk+1 occurs three times
(clearly any number of times could be considered):
qi .........qk+1 .........qk+1.........qk+1 .........q j.
Now, for passing from qi to q j, possibly by passing through qk+1, there are two
possibilities: or M does not pass through qk+1 (term Li,jk in the equation above)
or M goes from qi to qk+1 (term Li,k+1k in the equation above) by passing again
from qk+1 to qk+1 (two times in the example above) through the other k states (term
(Lk+1,k+1k)∗in the equation above), and ﬁnally, from qk+1 to q j through the other
k states (term Lk+1,jk in the equation above). Therefore, if the ﬁnal states are F =
qm,qm+1,...qn, the language recognized by M is given by:
L(M) = L1,mn + L1,m+1n + ...+ L1,nn.
In conclusion, all the equations above show that the language recognized by M
is obtained by applying regular operations (sum, concatenation, and Kleene star)
starting from ﬁnite languages (the proof uses implicitly the principle of induction
given in Sect. 5.8).
⊓⊔
Very often grammars and automata corresponding to a given regular expression can
be found in a simple way. However, an interesting problem, which occurs in many
contexts, is the search of “minimal” grammars or automata equivalent to some given
(complex) regular expression (according to some speciﬁed notion of size of these
devices). The automaton of Fig. 6.4 provides two equivalent automata recognizing
the (language of) the ﬁrst regular expression of Table 6.11.
Table 6.11 shows a list of regular expressions, while Table 6.12 shows the pat-
terns (with n,m > 0) of languages of types 1 or 2. First four languages are of type 2,
while the last three are of type 1. Classes of languages of interest in natural phenom-
ena (forms of developments, complex communication languages, natural languages)
belong usually to classes between the Chomsky types 1 and 2. All Chomsky classes

6.3 Regular Expressions and Finite Automata
289
Table 6.11 Regular expressions
a(b+c)∗
a∗(c+b)
(ab)∗
(ab)∗+(ac)∗
(a+ab)∗c
((a+ab)∗c+bb)∗
aabcbb
Fig. 6.4 Two equivalent automata recognizing L(a(b+c)∗)
Table 6.12 Patterns of languages of types 1 or 2
anb2n
anbn+3
(aba)n
anbnam
anbnan
anbnanbn
anbman+m
are closed with respect to the sum. In fact. given a grammar G1 for a language L1 and
a grammar G2 for a language L2 it is easy to provide a grammar, generating L1 +L2,
with the lowest type among those of G1 and G2. The class REG is boolean (closed
with respect to sum, intersection, and complementation). An important result of late
1980s has proved that also CS is boolean. However, CF is not boolean because it
is not closed with respect to intersection. In fact, anbncm and anbmcm are patterns

290
6 Languages and Grammars
of CF languages, but their intersection, having pattern anbncn, is not CF. Many im-
portant properties of languages can be algorithmically established. For example, a
language is inﬁnite iff any grammar generating it has an auto-rewriting symbol, that
is, a non-terminal symbol X such that α ⇒∗
G β where X occurs in both strings α,β,
and such that X ⇒∗
G γ with γ ∈T ∗.
However, given two grammars G1,G2 of type 2, it can be shown that it is im-
possible to have an algorithm which, in general, is able to decide whether L(G1) ⊆
L(G2). We mention other two important results about Chomsky hierarchy. The in-
tersection of any language L with a regular language preserves the Chomsky type of
L. Moreover any language L of type 0 can be obtained by a suitable CS language L′
by deleting a special symbol from all the strings of L′.
6.4
Patterns and Rules
The notion of string pattern was already introduced in informal way. Here its formal
deﬁnition follows.
Deﬁnition 6.6. A string pattern over an alphabet A is an algebraic expression P built
by means of symbols (of the alphabet), variables (with speciﬁed ranges of variabil-
ity), and operations. When variables of P are instantiated by values in the respective
ranges, then P denotes a string over the alphabet A. If P = P(x1,x2,...,,xk) is a
pattern of variables x1,x2,...,,xk, and S1,S2,...,,Sk are the respective ranges of its
variables, then the language L(P) deﬁned by P is given by:
L(P) = {P(x1 := α1,x2 := α2,...,,xk := αk) ∈A∗| α1 ∈S1,α2 ∈S2,...,,αk ∈Sk}
where P(x1 := α1,x2 := α2,...,,xk := αk) is the evaluation of the pattern when the
values α1,α2,...,αk are assigned to the corresponding variables (and the operations
are applied according to the structure of the expression P).
For example, let us consider the pattern:
xn a ym b xn+m
with x,y ∈A∗, n,m ∈N. Here we have strings a,b and variables x,y,n,m (with their
ranges of variability). When variables are instantiated and concatenation, iteration,
and arithmetical sum are applied, we get a string of A∗.
The elements of L(P) are also called instances of the pattern. In more complex
cases, speciﬁc constraints can be required to a pattern (for example, the number of
its variables). Patterns can be combined by using the language operations +,·,/,∩.
In fact, if P1 and P2 are two patterns that identify languages, then, in a completely
natural way, P1 + P2 means that:
L(P1 + P2) = L(P1)+ L(P2)
and analogously for concatenation, difference, intersection, and Kleene star.

6.4 Patterns and Rules
291
Some regular expressions can be represented by patterns having only vari-
ables ranging on natural numbers. For example a∗+ b(c∗) can be represented by
an + b(cm) with n,m ∈N (different occurrences of Kleene star deserve different
variables). Patterns built with strings, string operations, variables, and language op-
erations provide a very powerful way of expressing forms of strings. It can be proved
that suitable combinations of patterns with logical conditions on them provide an
expressive power equivalent to the Chomsky grammars of type 0 [204].
The notion of pattern is useful for giving a general deﬁnition of string rewriting
rule. The general idea of such a rule is that of a combinatorial rearrangement of
pieces of some input strings for producing some output strings. This can be repre-
sented by a sequence of patterns, called premises, and a sequence of patterns called
conclusions, such that, in the conclusions variables occur that appear also in the
premises:
Premises
Conclusions.
According to this format, the grammatical rewriting relation has only one premise
and one conclusion, that is, α ⇒G β can be expressed in the following way, where
x,y are variable over A∗(A the alphabet of G):
xαy
xβy.
This kind of rule is also called a replacement. In a string rewriting rule all the
variables occurring in the conclusions have to occur also in the premises, and have
to verify the same constraints.
For example, the following rule has two premises, one conclusion and two vari-
ables (x,y ∈A∗):
xb,by
bxyb .
A string rewriting rule can be applied to a sequence of strings when these strings
are instances of the premises, for suitable values of their variables. In this case, the
application of the rule yields the evaluation of the conclusions, according to the
same values of variables assigned to the variables of premises.
The rule given above can be applied to the strings:
ababab,bcc
according to the following values of premise variables:
x = ababa
y = cc
and provides as a conclusion the following instance:

292
6 Languages and Grammars
bababaccb.
If R is a set of string rewriting rules, over some alphabet, and L a language over the
same alphabet, then the language R∗(L), generated by R from L, is deﬁned as the
minimum language including L and closed with respect to the rules of R. This means
that, when R∗(L) contains some strings, then it contains also the strings obtained
from them by means of an application of a rule of R.
It can be shown that the language R∗(L) can be generated by applying the rules of
R to strings of L by means of string derivations. A string derivation is a sequence
of steps, where in the initial step some strings of L are chosen, and in every follow-
ing step strings are generated that are either strings of L, or strings obtained by an
application of a rule of R to strings generated at preceding steps.
Figure 6.5 represents a string derivation by means of rules with two premises and
two conclusions, by means of a suitable tree (with leaves on the top and the root on
the bottom).
Consider the following rule (x,y ∈{a,b,c}∗):
r =
xaby
xaabbyc
it is easy to verify that:
L(anbncn) = {r}∗({abc}).
In fact, when the rule is applied to a string of the tri-somatic language, then it pro-
vides a string which follows the same pattern. Therefore, starting from the shortest
string of this type (we do not consider string λ), we are able to provide any possible
Fig. 6.5 A string derivation represented as a tree

6.5 Turing Machine
293
string of the language. The above generation of L(anbncn) shows as powerful are
rewriting rules including variables.
In the case of replacement variables remain “ﬁxed”, but when they are rearranged
in the conclusions, then a global transformation is applied to the input strings in
order to generate the output strings. This means that particular kinds of rewriting
rules can be very powerful, and few rules can generate languages which require
Chomsky grammars with greater numbers of rules. For example, consider a sort of
multiple rewriting expressed by the rule:
a, b, c →aa, bb, cc
thatsimultaneously rewritesany threeoccurrencesofthethreesymbolsbytheirdupli-
cates, according to the following formulation by means of patterns (x,y ∈{a,b,c}∗):
axbyc
aaxbbycc.
In this way, starting from the string abc the tri-somatic language is generated by
means of string derivations which use only this multiple rewriting.
6.5
Turing Machine
From a very general point of view, a computation system is deﬁned by seven com-
ponents:
1. A set D of data;
2. a set S of states;
3. an input function (encoding data into states) in : D →S;
4. an output function (decoding states into data) out : S →D;
5. a set Ω of operations on the set S;
6. a set Φ of rules which are functions ρ : S →P(Ω);
7. a set of computations Γ which are sequences of states with an initial state en-
coding the initial data. For any state s a state s′ follows s in a computation if
s′ = ω(s) for some ω ∈ρ(s). A computation halts with a terminal state sf when
ρ(sf ) = /0.
The previous structure describes a system where data are encoded by states and a
dynamics is deﬁned on the states that ends when the system reaches ﬁnal states
encoding results. In this sense, a computation is a special kind of dynamics where
the passage from a state to another is performed by applying one operation among
those which are applicable in a given state.
This general schema can be realized and extended in many ways. Namely, the
rule of applying operations can be external to the system, and at each step an opera-
tor can intervene in the system for its evolution. Otherwise, a description of this rule,
usually called program, can be internal to the system, by making it able to evolve
in a speciﬁc way. In this case the system is a programmed computation system,

294
6 Languages and Grammars
also called automaton, which in its etymological sense means just “autonomously
evolving system”. If the system can input a program, as part of its data, by behaving
according to it, we say that the system is universal with respect to the class of pro-
grams it can input. If at each step the computation rule provides only one applicable
operation, then the resulting computation is deterministic as opposed to the more
general case of a nondeterministic computation. More general kinds of computa-
tions may assume the presence of interactions, that is, of exchanges of data during
the computation. In this case, the behavior of a system depends on the sequence
of data exchanged during these interactions. For example, in some state the system
can require a value from outside. This means that the system stops its computation,
by waiting for a value, and when it is provided (by means of an input device), the
computation continues, possibly providing output values during the computation.
A fundamental model of computation was elaborated by Alan Mathison Turing
in a famous paper published in 1936. It describes a general kind of computation
processes based on a very intuitive basis. This kind of computation develops by
transforming the contents of cells linearly arranged on a tape unbounded in the
two directions (see Fig. 6.6). One symbol is contained in each cell which belongs
to an alphabet A including a special symbol, say it B, called the blank symbol. A
system can assume internal states of a ﬁnite set Q and in each state can read the
symbol put in one cell (the current cell). A program, consisting of a ﬁnite sequence
of instructions, deﬁnes the action that the system can perform at any step of its
evolution. Any instruction has a pre-condition and a post-condition. A pre-condition
is given by a pair (q,a). If the precondition of an instruction corresponds to the
current situation of the system, that is, q is the internal state of the system and a is
the symbol in the current cell, then the instruction can be applied according to its
post-condition, which is a tern (b,q′,m) where b is the symbol which replaces the
symbol a in the current cell. The state q′ is the state which replaces q and m indicates
the current cell at the next step of computation. If m = L the (new) current cell is
located on the left of previous one, while if m = R on the right.
Any computation starts with an initial state (usually indicated by q0), with a se-
quence of symbols over the alphabet A arranged in a contiguous portion of the tape,
and with the blank symbol B in all the cells outside this portion. At the beginning,
the system reads on the ﬁrst cell on the left where a symbol different from B is writ-
ten (this is the initial current cell). When the machine stops, because no instructions
are applicable, the output is the string put on the tape between the two most extreme
symbols on the tape which are different from B.
The formal deﬁnition of A Turing machine follows.
Deﬁnition 6.7. A Turing machine M is given by the following structure:
M = (A,Q,q0,I)
where A is the alphabet of its symbols, including the special blank symbol B, Q is
the ﬁnite set of its states, q0 ∈Q is its initial state, and I the set of its instructions,
that is, quintuples (q,a,b, p,m) where q, p ∈Q, a,b ∈A and m ∈{L,R}. The set I of

6.5 Turing Machine
295

-
B
B
B
B
Q
?
M
Fig. 6.6 The Turing machine
instructions deﬁnes the computations of M, as sequences of conﬁgurations gener-
ated in the following way.
A conﬁguration is a string ϕqxψ (representing the tape content, with an un-
bounded number of blank symbol occurrences implicitly assumed on the left of
ϕ and on the right of ψ), where: ϕ,ψ ∈A∗, q ∈Q is the current state of the con-
ﬁguration, and x ∈A is the current symbol of the conﬁguration. In the conﬁguration
ϕqxψ any instruction having q,x as initial symbols can be applied. M is determinis-
tic when at most one instruction is applicable in every conﬁguration, otherwise it is
nondeterministic. The application of an instruction (q,x,y, p,R) in the conﬁguration:
ϕqxψ
yields a next conﬁguration:
ϕypψ,
while an instruction (q,x,y, p,L) in the conﬁguration:
ϕzqxψ,
where z ∈A is the symbol on the tape before the position of the current symbol x,
yields a next conﬁguration:
ϕ pzyψ.
The machine M starts a computation in an initial conﬁguration q0α, where α ∈A∗
is the input string of the computation.
M halts when no instruction is applicable. In this case, the conﬁguration, say
ξqxχ, is a ﬁnal conﬁguration, and the output of the corresponding computation
of M is the string ξxχ (where the symbol of the state is removed from the ﬁnal
conﬁguration).
The Turing machine of Table 6.13 when starts with a string of {B,1}∗of type
1n+1B1m+1 provides as result 1n+m+1 (1n+1 is a representation of number n).

296
6 Languages and Grammars
Table 6.13 A Turing machine performing the sum of numbers in unary notation
q0,1,1,q0,R
q0,B,1,q1,R
q1,1,1,q1,R
q1,B,B,q2,L
q2,1,B,q3,L
q3,1,B,q4,L
A Turing machine can be used as a computing device providing an input/output
behavior, but it can be also used as a recognizing device when given an input it
starts a computation and ends in some chosen states (a class o ﬁnal states has to be
deﬁned). A Turing machine can be also viewed as generating the language of all the
strings obtained as outputs in correspondence of all possible inputs strings.
We remark that, in principle, computing functions or recognizing and generating
languages are equivalent tasks. In fact, a function f : A →B is univocally identiﬁed
by its graphic, that is, the set {(x, f(x)) ∈A × B | x ∈A}. Therefore, generating or
recognizing the graphic of f is equivalent to computing f and vice versa.
It can be shown that the class of languages recognized by Turing machines is
equal to the class of languages generated by Turing machines, and that the languages
recognized/generated by nondeterministic Turing machines are equal to the class of
languages recognized/generated by deterministic Turing machines. This class coin-
cides with the class generated by Chomsky grammars of type 0.
Table 6.14 A non-terminating Turing machine on {0,1}
q0,1,1,q0,R
q0,0,0,q0,R
q0,B,B,q0,R
The Turing machine given in Table 6.14 describes a computation that does not
ends. The Turing machine given in Table 6.15 recognizes the bi-somatic language
(stops in the state qyes when the input is a bi-somatic string, while stops in the state
qno otherwise). It is easy to generalize the same strategy for recognizing the tri-
somatic language.
We remark that what makes Turing machines more powerful than ﬁnite state
automata is the possibility of movement in the two directions of the tape and the
possibility of reading and writing on it.

6.6 Decidability and Undecidability
297
Table 6.15 A Turing machine on {a,b} recognizing the bi-somatic language
q0,a,B,q1,R
q1,a,a,q1,R
q1,b,b,q2,R
q2,b,b,q2,R
q2,B,B,q3,L
q3,b,B,q4,L
q4,a,a,q4,L
q4,b,b,q4,L
q4,B,B,q0,R
q0,B,B,qyes,R
q0,b,b,qno,R
q2,a,a,qno,R
6.6
Decidability and Undecidability
The original Turing’s paper of 1936 presents a basilar result of the theory of com-
putation: the undecidability of the general halting problem for Turing machines. As
a consequence of it, it is possible to deduce the existence of recursive enumerable
sets which are not decidable. Namely, no Turing machine exists which can, in gen-
eral, establish wether a given Turing machine stops yielding a result in correspon-
dence to a given input. Therefore, we can collect in a set all the outputs provided
by a Turing machine, but there is no way to know whether a given Turing machine
(encoded by its program, which is a string) halts on not. Of course, if it halts, we
know it, but if it does not halt, we cannot exclude that a halting conﬁguration will
be reached in the future. This means that, in general, we can decide positively the
termination if it occurs, but we cannot decide negatively when it does not occur.
A recursively enumerable language is also called semi-decidable. In fact we can
always correctly say yes, when a string belongs to it, but we cannot, in general, say
no when a string does not belong to it. In other words, membership can be asserted
in a ﬁnite amount of time, but non-membership cannot, in general, deﬁnitively as-
certained in a ﬁnite amount of time.
A semi-decidable language can be generated as outputs of a Turing machine,
in correspondence to all input strings over its alphabet. It is easy to show that a
language can be generated by a Turing machine iff it is also recognized by some
Turing machine. The class RE coincides with the class of semi-decidable languages,
while the class REC coincides with the class of decidable languages (a language is
decidable if a Turing machine exists answering “yes” for the (input) strings that
belong to the language and “no” for those that do not belong to it.
A simple result about decidability (often mentioned as Post theorem) says that if a
language L and its complementary ¯L are both recursively enumerable, then they are
both recursive. In fact, let us start, in parallel, the enumerations of the two languages,
then any string will be generated by one of the two enumerations, therefore, in a

298
6 Languages and Grammars
ﬁnite time it will appear, and according to which enumeration will provide it, we
can decide if it belongs to L or not. The converse implication is obvious. In fact if L
is decidable, then we easily can enumerate both L and ¯L.
The existence of languages in RE which are not in REC was an important
achievement of the computation theory founded by Turing. Such a language was
deﬁned by Turing, by using a technique resembling the diagonal argument of Can-
tor showing that natural numbers cannot be put in a bijective correspondence with
the set of real numbers. In fact, if we enumerate all the strings over an alphabet
α1,α2,... and all the Turing machines over the same alphabet M1,M2,... (by enu-
merating strings representing their programs), then the language:
K = {αi | αi ∈L(Mi)}
can be easily recognized as recursively enumerable, but its complementary ¯K can-
not be recursively enumerable because no Turing machine Mj can recognize it. In
fact if this machine could exist, then either αj ∈L(Mj) or αj ̸∈L(Mj), but both
possibilities lead to a contradiction. In conclusion K is recursively enumerable, but
¯K is not recursively enumerable, therefore K cannot be decidable, otherwise both K
and ¯K should be decidable.
The other crucial achievement of Turing’s paper was the notion of universal Tur-
ing machine. In fact, a Turing M is univocally identiﬁed by its program, and it can
be easily encoded with a string of a suitable alphabet. Turing showed that a Turing
machine U can be constructed which, taken as an input an encoding < M,α > of
M with the input α, can provide a computation which corresponds univocally to the
computation of M with the input α. In particular, the computation of U with this
input stops if and only if M stops on α, and this computation yields the same output
produced by M with α as input.
Turing machines resulted to be equivalent to other formalism introduced for
deﬁning general classes of algorithmically effective computation processes. In fact,
reasonable processes of mutual translations were found such that computations per-
formed by Turing machines were encoded by computations performed in these other
formalisms, and vice versa. Just for mentioning some of them: λ-calculus, Post sys-
tems, Herbrand-G¨odel general recursive functions, Kleene-G¨odel partial recursive
functions, and Markov Algorithms are all Turing-equivalent.
In 1936, Alonzo Church published a paper where was formulated the famous
Turing-Church Thesis:
Every computation can be realized, by means of some suitable encoding of
data, by means of a Turing machine.
According to Turing-Church Thesis, the informal notion of computable func-
tion, can be adequately reduced to the rigorous concept of Turing computable func-
tion. Any formalism which results to be equivalent to Turing machines is said to be
computationally universal.

6.6 Decidability and Undecidability
299
We conclude by giving the schema of a sketch of G¨odel’s famous incompleteness
theorem [175, 173, 188], where logical incompleteness is strictly related to the no-
tion undecidability (formulae over a signature can be seen as strings of a particular
formal languages). Basic logical notions of ﬁrst-order logic are outlined in Chap. 5.
Given a model M over a domain D, a theory Φ represents a subset A of D, by
means of a formula ϕ(x) with a free variable x, if a ∈A ⇔M |= ϕ(a).
A theory is g¨odelian if it can represent any language of RE. A theory is ax-
iomatic if it is constituted by all the logical consequences, called theorems, de-
ducible, by means of a logical calculus (a deduction algorithm), from a ﬁnite set of
sentences, called axioms. A theory is complete if for any sentence ϕ of its signature
either ϕ or ¬ϕ belongs to the theory. A theory is consistent if there is no sentence
ϕ such that both ϕ and ¬ϕ belong to the theory.
The following lemmas are the basis of G¨odel’s incompleteness theorem.
Lemma 6.8. The Peano Arithmetic PA is g¨odelian.
The previous lemma was proved by G¨odel via a method, called syntax arithmetiza-
tion, able to translate in arithmetical terms the logical notions of ﬁrst-order theories.
Lemma 6.9 (G¨odel). Any consistent axiomatic theory that is complete is decidable.
An informal argument proving the previous lemma is the following. In order to
decide if ϕ is a theorem of a theory Φ, generate all the theorems you can deduce
from the axioms, by the completeness of Φ either ϕ or ¬ϕ will be generated. In the
ﬁrst case ϕ is a theorem, in the other case ϕ is not a theorem; both cases cannot
occur because Φ is assumed to be consistent.
We know that there is a language K that is a RE language, but it is not decidable.
This fact implies the following proposition.
Lemma 6.10. If a consistent theory Φ is decidable, then it cannot be g¨odelian.
Proof. If Φ were g¨odelian then Φ should represent the set K in RE, which is not
decidable, that is, a ∈K ⇔Φ |= ϕ(a), for some formula ϕ(x) with a free variable.
But, being Φ decidable, the theorems of Φ are a decidable set, therefore also the
subset of sentences ϕ(a), which are theorems of Φ, are a decidable set. Therefore,
K should be decidable, against the hypothesis that K is not decidable.
⊓⊔
No g¨odelian axiomatic theory can deduce all the arithmetical true sentences (that,
of course, are a complete theory), as it is stated by the following theorem.
Theorem 6.11. PA is incomplete.
In fact, PA is axiomatic and g¨odelian. If it would be complete, then it should be
decidable for Lemma 6.9, but this should contradict, by Lemma 6.10, its property
of being g¨odelian.
G¨odel Incompleteness says that there are true arithmetical propositions that can-
not be deduced from PA axioms. In general, arithmetics cannot be axiomatized in
FOL. This incompleteness, in the original proof of G¨odel, was proved by using an

300
6 Languages and Grammars
auto-referential sentence, that is a form of a diagonal construction, related to Can-
tor’s proof of non-enumerability of real numbers (and also to Russell’s paradox). In
the proof sketched above the diagonal argument is implicit in the Turing’s construc-
tion of the language K (directly suggested by Cantor’s diagonal argument).
First-order logic connects arithmetic, computability, and formal language the-
ory [188, 204]. Many logical limitations are due to computational aspects, and at
same time, many limitations of calculi are of logical nature. Analogously, deduction
of consequences by means of a logical calculus is a special case of computation,
but at same time any computation can be represented in terms of suitable logical
deductions. Finally, any FOL (First-Order Logic) theory can be interpreted in mod-
els having natural numbers as domain. These facts show that arithmetic, logic, and
computation are ﬁelds strongly connected.
6.7
Register Machines
Register machines are computation systems strictly related to Turing machines.
They have a structure similar to the central unit of von Neumann’s architecture, rep-
resenting the general schema of a stored-program electronic computers, designed
in the EDVAC (Electronic Digital Variables Automatic Computer) draft of 1945
[207, 194]. The constitutive element of EDVAC project is the electronic valve and
its analogy with the neuron, according to McCulloch and Pitts’ formalization [9].
The document by von Neumann provides the constitutive principles of a universal
electronic computer; however, a systematic analysis of electronic circuits, in terms
of boolean operations, was developed afterwards, on the basis on researches of Emil
Post and Claude Shannon, in the years between 1930 and 1945. Register machines
are computationally universal: for any Turing machine M there exists a register ma-
chine performing the same computations of M (by a suitable encoding of data of M
as natural numbers).
Consider the cells of a Turing machine tape and, instead of arrange them linearly,
associate to each of them an address expressed by a number or by a label. Moreover,
instead of put in these cells symbols, assume to put in them numbers (if any num-
ber can occur, then the size of cells is unbounded). In a register machine, instead of
moving along the cells, it is possible to apply a set of operations to the numbers con-
tained in a cell, called register, by specifying the cell address. A register machine
performs a computation according an internal program which is constituted by a
sequence of instructions. Each instruction has three components: i) an order num-
ber, ii) an operation referred to registers, and iii) the number (or two numbers where
only one is chosen) which has to be put in the (instruction) counter C. This num-
ber establishes the next instruction to execute. In fact, at each computation step, the
machine executes the instruction having the order number contained in the counter.
At the beginning of the computation, the counter contains the number of the ﬁrst
instruction to be executed. When the counter C contains a number which is not the
number of an instruction of the program, then the computation stops. The instruc-
tions, with the related operations, are given in Table 6.16, where Ri,R j denote two

6.7 Register Machines
301
registers of addresses i, j respectively. Three special registers are assumed: an input
register Rin providing data to the machine, an output register Rout providing results,
plus the register C, with the role of counter. Any machine is assumed to have, apart
the three special registers, all the registers necessary for executing the instructions
of its program.
Table 6.16 The instructions of a register machine (applicable when the counter C contains h
h,inputi,k
the value of register Rin is put into the register Rin and k in C.
h,out puti,k
the value of register Ri is put into the register Rout and k in C.
h,inci,k
the value put in the register Ri is incremented of one unit and k in C.
h,deci,k
the value put in the register Ri is decremented of one unit and k in C.
h,zeroi,k
the value 0 is put in the register Ri and k in C.
h,testi, j,k,l
if the values of Ri and R j are equal then k, otherwise l, is put in C.
Table 6.17 A register machine computing the sum of two numbers
1. input1 2
2. input2 3
3. zero3 4
4. test2,3 7, 5
5. dec2 6
6. inc1 4
7. out put1 8
It can be shown that the same computational power of register machines deﬁned
above continues to hold by substituting the two operations of test and decrement
with a unique operation of conditional decrement cdeci k, l such that if the content
of Ri is not zero decrements this register and put k in C, otherwise only puts l in C.
Register machines have the same computational power of structured register
while-programs, which are realized by using only the operations of increment and
decrement, but by structuring operations in blocks of type whilej. This means, all
the operations in a whilej block are iterated while the content of register R j is not
zero. For example, the program of Table 6.17 could be expressed by the register
while-program given in Table 6.18. In this case, instructions do not need to have
order numbers and next instruction numbers, because operations are executed in
the order they are listed in the program, and the program execution ends when the
last instruction is executed. When a while-block is entered, the exit from it happens
when its condition does not continue to hold.
This notion of program is the basis of structured programming, which was a key
feature of high level programming languages developed in 1970s. In a structured
program, a correspondence holds between the order of instructions in the program

302
6 Languages and Grammars
Table 6.18 A structured register while-programs for the sum of two numbers
input1
input2
while
R2 > 0
dec2
inc1
end
out put1
and the order of their execution. In this way, program analysis results to be easier
and may be developed in a more reliable manner.
The algorithm underlying the register program of Table 6.18 is the natural way
to perform a sum, according to the ﬁnger-counting rule.
6.8
Information, Codes, and Entropy
In 1948 Claude Shannon published a booklet entitled “The mathematical theory of
communication” [212], which is a scientiﬁc milestone and the ﬁrst systematic math-
ematical investigation about a quantitative perspective in the analysis of information.
The starting point of Shannon’s approach is a probabilistic perspective based on the
concept of information source, which is constituted by a set D of data with a func-
tion assigning a probability of emission to each element of D. The (probabilistic)
measure of a datum d (with respect to logarithm basis b) is the logarithm of 1/pd
that is −logb pd. The motivation of this deﬁnition is based on the idea that the in-
formation conveyed by an event is proportional to its rarity and that the information
conveyed by a pair of independent events is the sum of the information conveyed by
each of them. It is interesting to remark that the same intuition was the basis of a
method elaborated by the Arabian crypto-analysts Al-Kindi (Ninth century A.D.) in
deciphering the codes based on mechanisms of letter substitution. In fact, if in a text
letter “a” is replaced by letter “p”, then you can guess this encoding by counting the
frequency of “p” in a sufﬁciently long text of the language to which this encoding is
applied.
6.8.1
Shannon’s Entropy
The crucial concept of Shannon’s theory is the (information) entropy of an (infor-
mation) source deﬁned by:
−∑
d∈D
pd logb pd
(6.1)

6.8 Information, Codes, and Entropy
303
Shannon’s entropy is the center of a theoretical framework where codes and digital
information can be rigorously analyzed and applied to an enormous spectrum of
scientiﬁc ﬁelds [198, 200].
The notion of code is easily deﬁned by a function from codewords to data. Code-
words are strings over a preﬁxed ﬁnite alphabet. A code C is proper if there is only
one codeword for every datum, otherwise the code is redundant. Here, we consider
only proper codes. In the case of a proper code, the digital size of the information of
a datum, with respect to the code, is given by the length of the codeword associated
to the datum.
Given a code C we deﬁne digit(C) by the following equation, where |α| denotes
the length of the string α:
digit(C) = ∑
α∈C
|α|
Let us deﬁne digit(n) as the minimum value of digit(C) for C varying in the proper
codes of n codewords. The following result can be shown, which again provides a
strict link between logarithms and information:
n⌈logkn⌉≥digit(n) ≥n(⌊logkn⌋−2).
6.8.2
Optimal Codes and Compression
Codes are univocal when do not exist two different sequences of codewords which
can be concatenated by providing the same string. These codes have important prop-
erties which are essential for providing efﬁcient decoding processes when data are
transmitted along a transmission channel. Kraft norm ||C|| of a code C is deﬁned by:
||C|| = ∑
α∈C
|A|−|α|
where A is the alphabet of the code, and |A| denotes the cardinality of A. Kraft norm
of a univocal code is always equal to or less than 1. A special kind of univocal code
are the instantaneous codes, having the property that no pair of their codewords
α,β can exist such that α is a preﬁx of β. Any univocal code C can be always
transformed into an instantaneous code C′ such that ||C|| = ||C′||. Instantaneous
codes can be deﬁned by means of an encoding tree (an example is given n in Fig.
6.7), where codewords correspond to the leaves of the tree (analogous trees can be
constructed for alphabets with more than two symbols).
An information source is any device emitting data d ∈D with probability p(d),
the mean length LC of a proper code C is given by:
LC = ∑
α∈C
pα |α|.
A code C is optimal, with respect to an information source, if no other code exists
having a shorter mean length with respect to that information source. A very simple
algorithm due to Huffman exist for deﬁning an optimal code with respect to an

304
6 Languages and Grammars





J
J
JJ^





J
J
JJ^









A
AAU
A
AAU
A
AAU



A
AAU



A
AAU







=
QQQQQQQ
Q
s
{a1,a2,a3,a4,a5,a6,a7,a8,a9}
{a1,a2,a3,a4}
{a5,a6,a7,a8,a9}
{a5,a6,a7}
{a8,a9}
{a5}
{a9}
{a8}
{a6,a7}
{a6}
{a7}
0
0
0
0
0
0
1
1
1
1
1
1
100
1010
1011
110
111
{a1,a2,a3}
0
1
{a1,a2}
{a4}
{a3}
{a1} {a2}
01
001
0001
0000
0
1
{a1,a2,a3,a4}
{a5,a6,a7,a8,a9}
{a5,a6,a7}
Fig. 6.7 An encoding tree
information source. Let us recall brieﬂy the Huffman method. Given k data with
k > 1 and with probabilities p1, p2,..., pk respectively, let us deﬁne the following
process of tree construction.
1. Consider two probabilities p1, p2 such that no probability is smaller than them,
then introduce a new probability p1 + p2 and connect this new probability with
their summands and label these edges with 0 and, 1 respectively.
2. Apply again the procedure of the previous step to a set of probabilities where
p1, p2 are replaced by p1 + p2 (see Fig. 6.8), and iterate this step until only a
unique probability is obtained (at each step the set of probabilities decreases by
one unit, see Fig. 6.9).
3. Associate to the datum of probability p j the sequence of labels of the edges
connecting the probability p j with the ﬁnal unique probability.
Fig. 6.8 The initial step of an Huffmann code

6.8 Information, Codes, and Entropy
305
Fig. 6.9 A Huffmann code tree
Shannon’s ﬁrst theorem shows that entropy is a lower bound to the code optimality.
In fact, if H is the entropy of an information source and LC is the mean length of C
with respect to this information source, then:
LC ≥H.
Let T be a text, that is a sequence α1α2 ...αm of codewords, and let us denote by
|α| the length of a codeword α. The digital size of T (with respect to the code of its
codewords) is given by:
digitC(T) = ∑
j=1,m
|αj|.
A compression of T is a text T ′ such that, for some code C′: i) digitC′(T ′) <
digitC(T) and ii) text T can be univocally recovered from T ′. The compression
ratio is digitC′(T ′)/digitC(T). This ratio together with the space and time complex-
ity of a compression algorithm are the basic elements to evaluate a compression
method.
It is easy to realize that no universal compression algorithm can exist. In fact,
no compression method can satisfy the two conditions above for every text. If this
universal method would exist, then by applying a compression, every text of a cer-
tain digital size q could be reduced to a text of digital size q′ < q. But of course
the number of texts of size q′ are less than the number of texts of size q, there-
fore it is impossible to recover, univocally, every q-text from some q′-text. The

306
6 Languages and Grammars
impossibility of universal compression methods implies that we can only try to dis-
cover compression methods which work for speciﬁed classes of texts.
In compression theory there are three main principles: i) encoding more probable
codewords in a shorter way, ii) trying to determine a dictionary of the codewords
occurring in a given text, and iii) changing the order of codewords in such a way
that similar codewords become contiguous. In fact, contiguity of similar codewords
allow for replacing a subtext αn by the pair (α,n), which can be encoded more
shortly than αn.
6.8.3
Typicality and Transmission
Shannon’s second theorem concerns with the transmission error. It shows that, even
if transmission is affected by an error probability, when data are encoded in a suit-
able way, this probability can be made indeﬁnitely small.
After the theoretical result of the second theorem, transmission codes were in-
troduced where the reliability of transmission is obtained by adding extra symbols
which can recover the lost information when some noise corrupts the invoiced mes-
sages. These extra symbols are called control digits. For this reason, an encoding
with m digits plus k control digits encodes, in the binary case, 2m+k messages which
send only 2m different messages. The ratio m/m + k is the transmission rate. The
capacity of a channel transmitting between a sender information source and a re-
ceiver information source is the maximum amount of mutual information (see later)
which can pass between the two sources (the transmitter and the receiver). In more
precise terms, the second theorem claims that when the transmission rate is less than
the capacity of the channel, then the more we reduce the transmission rate, the more
the error probability decreases.
The second theorem is a masterpiece of Shannon’s approach and was the be-
ginning of the theory of the self-correcting transmission codes. Its proof relies on
the notion of typical sequence generated by an information source. When an in-
formation source emits a sequence of data, the longer is the sequence, the more
it approximates to a typical sequence of the source. In fact, a sequence is typical
when the frequency of any datum occurring in it is the same as the emission prob-
ability of the source. This means that sufﬁciently long sequences can assumed to
be typical, with a small error probability. The deep connection between typicality
and entropy is given by the following asymptotic results: i) the probability of any
typical sequence of length n approximates, for increasing n, to 2−Hn, where H is the
entropy of the information source, ii) the number of typical sequences of length n
approximates, for increasing n, to 2Hn.
The theoretical framework necessary to the proof of the second theorem includes
many important notions related to the entropy. In this context, it is useful to consider
an information source as a casual variable, that is, a variable with an associated
probability of assuming their values. If H(X) is the entropy of X:
H(X) = −∑
X=x
p(x)log p(x)

6.8 Information, Codes, and Entropy
307
then the joint entropy of two casual variables X,Y is given by:
H(X,Y) = −∑
X=x,Y=y
p(x,y)log p(x,y)
the entropy of X conditioned to Y is given by the following equation where p(x|y)
is the conditional probability of having X = x when Y = y:
H(X|Y) = −∑
X=x,Y=y
p(x,y)log p(x|y)
it can be shown that:
H(X|Y) = −∑
X=x,Y=y
p(y)H(X|Y = y)
and
H(X,Y) = H(X)+ H(Y|X).
Other two important and related notions of information theory are mutual infor-
mation I(X|Y) between X and Y and the entropic divergence between them.The
mutual information (usually deﬁned by means of the entropic divergence) satisﬁes
the following important equation:
I(X|Y) = H(X)−H(X|Y)
the equation above explains the name used for denoting I(X|Y). In fact, let us iden-
tify the entropy of an information source with the average (probabilistic) informa-
tion of the data that it emits. if we consider X and Y as two processes, where the
ﬁrst one is an information source sending data through a channel, and the second is
the information source of the data as they are received, then I(X|Y) is a (probabilis-
tic) measure of the information passing from the sender to the receiver: the average
information of X (the transmitted information) minus the average information of X
when Y (the received information) is given.
The entropic concepts can be easily extended from discrete to continuous vari-
ables. This extension is necessary to the analysis of transmission by means of sig-
nals. They are functions, periodical in time, realized by electromagnetic waves. In
this case, data are encoded by altering a wave in a suitable way (modulating it),
and when the wave is received its alterations are decoded (demodulated) for recov-
ering the encoded messages. This method mixes discrete methods for representing
information with continuous signals playing the role of channels. Shannon shows
the sampling theorem, extending an already known result, according to which the
capacity of a periodical function is related to its maximum frequency (in its compo-
sition as sum of circular functions, in Fourier representation). By using the sampling
theorem Shannon shows his celebrated third theorem giving the capacity of a contin-
uous signal affected by a noise of power N. This capacity is given by the maximum
frequency of the signal multiplied by a factor which is the logarithm of the ratio
P+ N/N, where P is the power of the signal.

Archimedes’ Truncated
icosahedron
7
Combinations and Chances
Abstract. Combinatorics is the ﬁeld of mathematics which provides methods for
counting discrete structures of a given type. The exact evaluation of these num-
bers has very often a great importance in many theoretical and applicative contexts.
In this chapter the basic formulae of combinatorics will be presented, by empha-
sizing the links among combinatorics, probability, statistics, and biological applica-
tions. The Least Square Evaluation method is outlined, and basic notions about trees
and graphs are also provided with the fundamental enumeration formulae of these
structures.
7.1
Factorials and Binomial Coefﬁcients
The main aim of combinatorics is the evaluation of the numbers of ﬁnite mathe-
matical structures of a given type. In the following sections, we present the basic
combinatorial schemata which occur frequently in the problems of counting ﬁnite
structures. A simple and unifying way of describing combinatorial schemata is the
notion of allocation. An allocation is deﬁned by a set of objects and a set of cells,
and a way of putting objects into cells. Many kinds of allocations can be considered
in correspondence to the following features: i) whether or not objects are distinct,
ii) whether or not cells are distinct, iii) whether or not objects can be repeated, and
iv) whether or not cells can be repeated.
An important aspect in this regard is the explanation of what exactly means “dis-
tinct”. In fact, two objects are of course distinct, otherwise they is only one object.
However, in many contexts it is necessary to discern between distinct and distin-
guishable. Two balls having exactly the same shape are distinct when you put them
together on a table, but they may be not easily distinguishable if you put one of
them alone, and then the other one, or the same, at subsequent points in time. In the
V. Manca: Infobiotics, ECC 3, pp. 309–368.
DOI: 10.1007/978-3-642-36223-1_7
c⃝Springer-Verlag Berlin Heidelberg 2013

310
7 Combinations and Chances
context of combinatorial schemata, objects or cells are always distinct when you
count them, but they are undistinguishable when some of them can occur many
times and these multiple occurrences are counted as the occurrence of the same ob-
ject (distinguishability implies distinctness, but the converse implication does not
hold). In the following, when we speak of objects or cells, we assume that they are
distinguishable, unless the contrary is stated. The combinatorial schemata that we
will analyze will clarify these aspects.
7.1.1
Permutations and Arrangements
Let us consider n objects. How many possibilities we have of arranging them in
a sequence? Such an arrangement is usually called a permutation of n objects.
In set-theoretic terms a permutation is a bijective function. In fact, we determine
this number by using an allocation schema associating the n objects to n cells or
locations numbered from 1 to n (this terminology is used for a better visualization
of the arrangement). In the ﬁrst cell we can choose one of n objects, in the second
one of the remaining n−1 objects, and so on. In the last cell only one possibility of
choice remains, because the other n−1 objects are already allocated. In conclusion,
the number of n-permutations is
n(n −1)(n −2)...2 ·1 = n!
In this sense, factorial numbers count permutations.
The argument for counting permutations can be easily extended to the case of
arrangements, that is, injective functions from n objects to k places, where k < n
(permutations are arrangements where n = k). In this case, we get the following
product, which we call k-subfactorial of n:
n(n −1)(n −2)...(n −k + 1) = (n)k.
Of course:
(n)k = n!/(n −k)!
Let us consider allocations of objects in cells where any object can occur any number
of times, but only one object can be allocated to each cell. How many possible
allocations of this kind, of n objects to k cells are there? Any allocation of this
type corresponds to a function from the cells to the objects. In this case, at any
step of the allocation process, n choices are possible, whence we have nk different
arrangements with repetition.
The number of permutations and arrangements are useful for counting words.
How many words of length 10 can be written with 20 letters? They are 2010. How
many words of length 10 where no letter occurs more than once? They are (20)10.
How many different words can be obtained by rearranging the order of letters in the
word number? 6! = 720.
It is interesting to remark that the ratio between the number of injective functions
and all possible functions becomes very small when n increases. This phenomenon,

7.1 Factorials and Binomial Coefﬁcients
311
referred to as the rarity of injections has very surprising consequences. For example,
in a town where seven accidents occur each week, it is very improbable that these
accidents are distributed one per day. In fact, 7!/77 < 0.00613, therefore unlucky
days and lucky days, most probably, can be experienced. A famous example in this
regard is the so-called birthday phenomenon.If you consider a class with m students,
with m > 23, you have a big probability of ﬁnding two of them who were born on
the same day of the year. In general, the probability that m birthdays coincide is
given by:
p = 1 −(365)m
365m =

1 −1
365

1 −2
365

...

1 −m−1
365

.
If we discard the products between fractions (which are very small) the expression
can be approximated by:
p ≈1 −1 + 2 + 3...+ (m−1)
365
= 1 −m(m−1)
730
.
The previous examples are close to an important combinatorial principle known as
Pigeonhole Principle.
Table 7.1 The Pigeonhole Principle
For any allocation of n objects to m cells, where m < n, at least one cell
has to contain more than one object.
In fact, in a class with more than 366 students, at least two of them were born in
the same day (the date February 29 is included). This principle has very important
consequences. For example, if a dynamical system has a ﬁnite number of possi-
ble states, with no terminal state (where it may remain forever), then it has to be
eternally recurrent, that is, some of its states occurs an unbounded number of times.
7.1.2
Combinations and Binomial Coefﬁcients
How many possibilities are there of choosing k objects among n given objects? That
is, how many k-subset does a set of cardinality n have?
A k-subset of n objects is also called a k-combination over n objects. Also in
this case we can use an allocation schema. In fact, a combination is obtained by
allocating k of the n objects to k undistinguishable cells, with exactly an object
per cell. Therefore, the number of k-combinations of n objects can be deduced by
allocating the n objects to k distinguishable cells, in (n)k ways, and then by dividing
this number for the number of possible different ways these cells can be ordered,
that is k! ways. In conclusions the combinations we are searching are:
(n)k
k! =
n!
k!(n −k)!

312
7 Combinations and Chances
the standard notation for this number is:
n
k

.
These numbers are called binomial coefﬁcients, because (as Newton showed) they
are the coefﬁcients occurring in the expansion of the powers of binomials:
(a + b)n =
n
∑
k=0
n
k

ak bn−k
By using the factorial representation of subfactorials we get the following equation:
n
k

=
n!
(n −k)!k!
The formula above is simple, but it is very complex to be computed. In fact, when
numbers are big it is computationally prohibitive. In the next section we will provide
a recurrent formula for computing
n
k

without using factorials.
7.1.3
Inductive Derivation of Binomial Coefﬁcients
There is only one possibility of choosing n objects among n given objects, and
there are n possibilities of choosing one object among them. Of course, zero ob-
jects among n corresponds to the choice of the empty set, therefore we can put the
following equalities:
n
n

= 1
n
1

= n
n
0

= 1.
Theorem 7.1. For any n ≥k > 0:
n + 1
k + 1

=
n
k

+
 n
k + 1

Proof. Let us consider n + 1 objects (n ≥0). We can select k + 1 of them (k ≤
n) by using the following procedure. Let us ﬁx one object among the given n + 1
objects, call it a. The subsets of the k + 1 chosen elements can belong to one of
two classes: the sets including a and the sets that do not include a. The sets of the
ﬁrst type are in one-to-one correspondence with the choices of k objects among n,
because a can be added to any choice of k. The sets of the second type are in one-to-
one correspondence with the choices of k + 1 objects among n, because a is never

7.1 Factorials and Binomial Coefﬁcients
313
chosen. In conclusion, if binomial coefﬁcients provide the number of j-subsets for
sets of size smaller than n + 1 (with j ≤n), then: the number of combinations with
a is
n
k

, and the number of combinations without a is
 n
k+1

. This concludes the
proof.
⊓⊔
The number of combinations is easily computed by the following triangle due to
Tartaglia and Pascal, but already known by Chinese mathematicians at least 1000
years earlier. It is based on the previous theorem. In fact, the n-th row provides the
values of
n
k

for k going from 0 (left) to n (right). In the triangle, the elements of
the (n+1)-th row are obtained by summing the two elements over it in the n-th row
(the top left and the top right). The ﬁrst seven rows of this triangle are the following:
1
1
1
1
2
1
1
3
3
1
1
4
6
4
1
1
5
10
10
5
1
1
6
15
20 15
6
1
Tartaglia’s (or Pascal’s) triangle is symmetric. In fact, choosing k objects among
n is equivalent to chose n −k among n, which correspond to those which are not
chosen, that is:

n
k

=

n
n −k

.
Of course, the deﬁnition by induction has to coincide with the deﬁnition by means
of factorials, that is:
(n)k
k! + (n)k+1
(k + 1)! = (n + 1)k+1
(k + 1)! .
Newton’s formula, for computing the power of a binomial, is based on binomial
coefﬁcients. In fact, it is easy to realize that the coefﬁcient of a power akbn−k, of de-
gree n, coincides with the number of ways the factor a can be chosen in the product
(a+b)(a+b)...(a+b) of n binomials (so that b is chosen n−k times), therefore:
(a + b)n =
n
∑
k=0
n
k

akbn−k.
When in this formula a = b = 1 we get the number 2n of all possible subsets of n
elements. In fact, k-subsets, with k going from 0 to n, are all possible subsets of n
elements:

314
7 Combinations and Chances
2n =
n
∑
k=0
 n
k

.
Newton’s formula is the starting point of a lot of important mathematical theorems.
An example is the following Fermat’s little theorem which follows easily by in-
duction from Newton’s formula. It is an important property of modular arithmetic,
based on congruences. Notation n ≡m mod k denotes the congruence of n and m
modulo k, holding when the divisions of n and m by k provides the same remainder.
Proposition 7.2 (Fermat’s little Theorem). If p is a prime number then, for any
x ∈N:
xp ≡x mod p
Proof
(x+ 1)p =
n
∑
k=0
p
k

xk
in the right side of the equation, apart 1 and xp, other terms have coefﬁcients with
the following forms with 1 < j < p:
p!/(j!(p −j)!)
and, due to the primality of p, the factor p cannot be removed from the numerator,
therefore they are congruent to zero modulo p. In conclusion, for x = 1 the equation
of the proposition trivially holds, then if we assume that it holds for a value x, then
we have:
(x+ 1)p ≡xp + 1 mod p
but, by induction:
xp ≡x mod p
therefore:
xp + 1 ≡x+ 1 mod p
and, by the transitive property of equality, this implies:
(x+ 1)p ≡x+ 1 mod p.
⊓⊔
7.2
Distributions and Discrete Probability
Diagrams 7.1 e 7.2 are a kind of discrete version of Gaussian distribution shown
in Fig. 7.3. This corresponds to the fact that by using Stirling approximation, ex-
plained in the next section, it is possible to obtain a formula, due to De Moivre and
Laplace, which approximates the binomial coefﬁcients with a bell-shaped curve,
usually called gaussian because Gauss showed its deep probabilistic meaning. In
fact, if we consider the Bernoulli’s law of two possible events, say {1,0}, of proba-
bilities p and q = 1 −p, respectively, then the probability p(n,k) of having k times
1 in n events is given by:

7.2 Distributions and Discrete Probability
315
Fig. 7.1 The 16 subsets of a set of 4 elements, grouped in columns of equinumerous subsets
p(n,k) =
n
k

pkqn−k.
(7.1)
Equation 7.1 is easily explained by following the model of balls extraction from a
urn where p is the percentage of white balls (1 value) and q is the percentage of black
balls (0 value, apart the color, the balls are undistinguishable). Assuming a large
number of extractions, if we replace factorials by their Stirling’s approximations
(we avoid to give here the details), then the probability of extracting n balls where k
are white is given by the De Moivre–Laplace formula:
p(n,k) =
1
√npq
√
2π
e
−

1
2
 (k−np)
√npq
2
.
The mean value is np and the mean value of the deviation from it is √npq. If these
two values are replaced by μ and σ, respectively, we get the famous Gaussian dis-
tribution of mean μ and standard deviation σ:
nμ,σ(x) =
1
σ
√
2π e
−

1
2
 (x−μ)
σ
2
.
The area under this curve, between two extremes, gives the probability of having
values comprised between them. This law can be found in many random phenom-
ena, going from the distribution of the height in a population of individuals, to the
distribution of the errors in the measures of a physical quantity.
Another fundamental law of aleatory nature is the Poisson law (see Fig. 7.4),
which can be derived from Bernoulli’s law under suitable hypotheses, typical of

316
7 Combinations and Chances
Fig. 7.2 The 64 subsets of a set of 6 elements, grouped in columns of equinumerous subsets
rare events: a number n very big of possible events and a probability p very small
of having k successful events . In this case, if we call λ the product np (λ real), the
probability is given by the following Poisson distribution (of parameter λ):
p(k,λ) = e−λ λ k
k! .

7.3 Allocations and Partitions
317
Fig. 7.3 Gauss’s distribution. M denotes the mean. More than 68% of the values is within a
radius σ centered on M; more than 95% is within 2σ, and more than 99% within 3σ
7.3
Allocations and Partitions
In the following we consider other important kinds of allocations and some kinds of
partitions, which are special cases of allocations where cells are assumed undistin-
guishable and no cell is empty.
7.3.1
Multinomial Coefﬁcients
If in the formula of binomial coefﬁcients by means of factorials we put at denom-
inator the product of m factorials such that k1 + k2 + ...km = n, we get the follow-
ing formula for multinomial coefﬁcients, related to the Boltzmann distribution in
physics (of the fractional numbers of particles in a gas having some energy levels):

n
k1,k2,...,km

=
n!
k1!k2!...km!.

318
7 Combinations and Chances
Fig. 7.4 Poisson distribution for different values of λ (if λ > 10 it approximates to a
gaussian)
This formula has a simple combinatorial meaning. It corresponds to the number
of different ways of allocating n distinguishable objects into m distinguishable cells,
where k1,k2,...km objects are allocated, respectively in the m cells.
It gives also the number of permutations with repetitions, that is, the different
ways we can change the positions to the elements of a sequence over an alphabet,
say {a,b,c,d}, with na occurrences of a, nb of b, nc of c, and nd of d. It is easy to
show that:

n
k1,k2,...,km

=
n
k1
n −k1
k2
n −k1 −k2
k3

...
n −k1 −k2...−km−2
km−1

.
7.3.2
Partitions and Multisets
Binomial coefﬁcients provide also the formula for computing the number of parti-
tions of n undistinguishable objects into k distinguishable cells possibly empty.
In fact, for obtaining this number we can add k −1 undistinguishable tokens,
that is, elements which are different from the given n undistinguishable objects.
Then, we consider all permutations of a sequence of n + k −1 objects of two types.
Each permutation provides a partition into different cells. In fact, the k −1 tokens

7.3 Allocations and Partitions
319
determine k different locations in a sequence: before the ﬁrst occurrence, between
the ﬁrst and the second occurrence, and so on. Therefore, by choosing the positions
where to put the n objects of the ﬁrst type, we get the following value corresponding
to this kind of partitions:
n + k −1
n

.
If we want the number of partitions of n undistinguishable objects into exactly k
non-empty distinct cells (n ≥k), then we apply the same method given above, but
we force the presence of one object before each of the k separators. Therefore, in
this case we get the following number of possibilities:
 n −1
k −1

.
With an analogous argument we can determine the number of k-multisets over a
set of n elements. In fact, consider now n −1 undistinguishable separators and k
undistinguishable tokens. Each permutation of n + k −1 objects with n −1 of one
type and k of a different type (tokens and separators), represents a multiset where
the number of tokens before the i −1 separator corresponds to the multiplicity of
the i-th object of the set. For example, the multiset 3a + 2b + 4c of size 9 over 3
different kinds of objects, a,b,c, would be represented by (1 as separator and 0 as
token):
00010010000.
Therefore, the number of k-multisets over a set of n elements is given by:
n + k −1
k

.
7.3.3
Integer Partitions
A partition of a positive integers n is a sum of positive integers equal to n. For
example, 10 = 2+3+5 is a partition of 10 in three parts (the summands 2,3,5). This
kind of partition corresponds to allocate n undistinguishable objects to a number of
undistinguishable, non empty, cells. We denote by P(n) the number of partitions of
the integer n. Euler found a generating function for P(n), that is a function that, when
represented as inﬁnite series of monomials anxn, with n ∈N, satisﬁes the equation
an = P(n). In fact, Euler has proven that:
∞
∑
n=0
P(n)xn =
∞
∏
k=1

1
1 −xk

.
The following asymptotic expression for P(n) was ﬁrst obtained by Hardy and Ra-
manujan in 1918 [224]:

320
7 Combinations and Chances
P(n) ∼
exp

π

2n/3

4n
√
3
as n →∞.
Many other recurrent enumerative formulae are also known. Let us mention only
one of them, due to Euler:
P(n) = P(n −1)+ P(n −2)−P(n−5)−P(n −7)+P(n −12)+P(n −15)−...
where P(n) is recurrently computed by using the values P(n −q) with q ∈{(3k2 +
k)/2 | k ∈Z}, according to the following formula (P(1) = 1,P(i) = 0 for i ≤0):
P(n) = ∑
k∈Z
(−1)k+1P(n −(3k2 + k)/2).
Numbers q above are called pentagonal numbers because in the case of k nega-
tive they correspond to ﬁgurate numbers already known by Greek mathematicians,
which can be arranged in pentagonal shapes (differences of two suitable triangu-
lar numbers). This formula is one of Euler’s jewels, known as Euler’s Pentagonal
Number Theorem.
The following tables show the ﬁrst 30 values of P(n).
n
1 2 3 4 5 6
7
8
9
10 11 12 13
14
15
P(n) 1 2 3 5 7 11 15 22 30 42 56 77 101 135 176
n
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
P(n) 231 297 385 490 627 792 1002 1255 1575 1958 2436 3010 3718 4565 5604
Now we present a new perspective were partitions are classiﬁed according to
their minimum summand. In this way, we discover a recurrent formula comput-
ing P(n) by using elementary combinatorial arguments (see Eq. (7.4)). However,
computational experiments show that the recurrent pentagonal formula of Eulero is
computationally better than our formula. Nevertheless, we brieﬂy present this el-
ementary method for computing integer partitions, as an example of derivation of
recurrent relations in a classical combinatorial context.
Let us denote by Pmin(k)(n) the number of partitions of n having k as minimum
summand.
Lemma 7.3
Pmin(n)(n) = 1
Pmin(k)(n) = 0
for
⌊n/2⌋< k < n
Pmin(k)(n) = 1
for
⌊n/3⌋< k ≤⌊n/2⌋.
Proof. If k = n there is obviously only one partition of n with k as minimum sum-
mand. If k is smaller than n, but greater than ⌊n/2⌋, no partition of n can exist with
minimum part k. If k is comprised between the two extreme values ⌊n/2⌋and ⌊n/3⌋,
then there is exactly one partition of n = k +(n−k) having k as minimum part.
⊓⊔

7.3 Allocations and Partitions
321
Lemma 7.4. For n > 1,
P(n) =
⌊n/3⌋
∑
i=1
Pmin(i)(n)+ ⌊n/2⌋−⌊n/3⌋+1.
(7.2)
Proof. Of course, P(n) = ∑n
i=1 Pmin(i)(n), therefore the equation above follows from
the previous lemma.
⊓⊔
Let us denote by Pk(n) the number of partitions of n with at most k summands,
which is computable by means of the following well-known inductive equation:
Pk(n) = Pk−1(n)+ Pk(n −k)
with P1(n) = Pn(n) = Pk(0) = 1.
The following lemma expands the integer partitions enumeration formula (7.2)
of Lemma 7.4.
Lemma 7.5. Pmin(1)(n) = P(n −1). For k > 1 and n > k,
Pmin(k)(n) =
⌊n/k⌋
∑
i=2
Pi−1(n −ik).
Proof. In general, P(n −m) is the number of partitions of n where the summand
m occurs. Therefore, Pmin(1)(n) = P(n −1). Given an arbitrary partition, say it π,
of n −ik with at most i −1 summands, we can transform it (in a 1-to-1 way) in
a partition of n with i summands, say it π′, having k as its minimum summand.
Namely, consider the ik units of n which are exceeding n−ik, then add k of them to
each of the i−1 (possibly null) terms of π, and use the remaining k units for the i-th
summand of π′ (so that in π′ the presence of a minimum summand k is guaranteed).
In this manner, if i varies from 2 to ⌊n/k⌋(n > k), then we get Pmin(k)(n) by means
of the formula given above.
⊓⊔
However, the previous enumeration, based on minimum summands, can be substan-
tially improved by using the following lemma.
Lemma 7.6. For k < ⌊n/3⌋and 1 ≤n,
Pmin(k+1)(n) = P(n −k −1)−
k
∑
i=1
Pmin(i)(n −k −1)
Proof. Let us consider the partitions of n having k+1 as a part, which equals P(n−
k −1). Now remove from these the partitions that have 1 as a minimum part, those
having 2 as a minimum part, and so on, up those having k as a minimum part. In this
way, we obtain the partitions of n having k + 1 as minimum summand.
⊓⊔

322
7 Combinations and Chances
Let us deﬁne by induction the following partition difference operator Dk, of degree
k, for n > 1 and k ≤n (see Example 7.9 for a computation of D):

D1(n) = P(n −1)
Dk+1(n) = P(n −k −1)−∑k
i=1 Di(n −k −1)
(7.3)
Lemma 7.7. For k < ⌊n/3⌋,
Pmin(k)(n) = Dk(n).
Proof. This lemma follows easily by induction on k, from the deﬁnition of Dk and
the previous lemma. In fact, Pmin(1)(n) = P(n −1) = D1(n). Moreover, from the
previous lemma, Pmin(k+1)(n) = P(n −k −1)−∑i=1,k Pmin(k)(n −k −1). Therefore,
by induction hypothesis, we can replace the terms of the right side by obtaining
Pmin(k+1)(n) = P(n −k −1) −∑i=1,k Dk(n −k −1), from which, according to the
deﬁnition of Dk+1, we get Pmin(k+1)(n) = Dk+1(n).
⊓⊔
Finally, from lemmas 7.4, 7.6, and 7.7 a theorem follows which provides a simple
recurrent formula enumerating integer partitions.
Theorem 7.8. P(1) = 1 and P(2) = 2. For n > 2,
P(n) =
⌊n/3⌋
∑
i=1
Di(n)+ ⌊n/2⌋−⌊n/3⌋+1.
(7.4)
Example 7.9. The computation of P(27) = 3010, according to formula (7.4).
P(27) = D1(27)+ D2(27)+ D3(27)+ D4(27)+ D5(27)+ D6(27)+ D7(27)
+D8(27)+ D9(27)+ ⌊27/2⌋−⌊27/3⌋+1
D1(27) = P(26) = 2436
D2(27) = 383
D3(27) = 110
D4(27) = 39
D5(27) = 18
D6(27) = 9
D7(27) = 5
D8(27) = 3
D9(27) = 2
P(27) = 2436 + 383 +110+39+18+9+5+3+2+13−9+1 = 3010.
The detailed computation of D4(27) = 39
D4(27) = P(23)−D1(23)−D2(23)−D3(23)
P(23) = 1255
D1(23) = P(22) = 1002

7.4 Stirling, Bell, Catalan, and Bernoulli Numbers
323
D2(23) = P(21)−D1(21)
P(21) = 792
D1(21) = P(20) = 627
D2(23) = 165
D3(23) = P(20)−D1(20)−D2(20)
D1(20) = P(19) = 490
D2(20) = P(18)−D1(18)
P(18) = 385
D1(18) = P(17) = 297
D2(20) = 88
D3(23) = 627 −490 −88 = 49
D4(27) = 1255 −1002 −165−49= 39.
7.4
Stirling, Bell, Catalan, and Bernoulli Numbers
In this section we brieﬂy present some important classes of numbers related to fun-
damental enumeration formulae (see [218] for more details).
7.4.1
Stirling and Bell Numbers
Now, let us consider the number of allocations of n distinguishable objects to k
undistinguishable non-empty cells. For counting these combinatorial schemata, we
introduce this deﬁnition, by induction, of Stirling numbers of the second kind, de-
noted by:

n
k
 
.
Stirling numbers of the ﬁrst kind, we only mention here, are denoted by:
!
n
k
"
and count the number of permutations of n objects having exactly k cycles (a cycle
(abc...m) is a permutation where a goes to b, b goes to c ...and so on, and ends
with m going to a).
Stirling numbers (of the second kind) satisfy the following equations:

n
n
 
=

n
1
 
= 1
n + 1
k
 
= k
n
k
 
+

n
k −1
 
.
In fact, when allocating n + 1 objects into k undistinguishable cells, ﬁrstly we can
remove one object from the given object. The remaining objects can be allocated

324
7 Combinations and Chances
to k cells in two possible manners: i) either allocating the remaining objects to k
cells in
n
k
 
ways, and then adding the removed object to one of the k cells, in k
different ways, or ii) allocating the remaining objects to k −1 cells, leaving empty
one cell, and putting the removed object by itself in the empty cell.
By using Stirling numbers, we can also obtain the number of surjective functions
S(n,k) from a set of n elements to a set of k elements ( k ≤n). In fact, these functions
correspond to the allocations of n objects to k distinguishable cells. Therefore, all
ways of distinguishing k cells, correspond to giving them k! different orderings,
which yield the following cardinality of S(n,k):
|S(n,k)| = k!

n
k
 
.
The number of all possible partitions of n objects into any number of non-empty in-
discernible cells is given by the numbers bn, called Bell numbers, after the American
mathematician who studied them:
bn = ∑
i=1,n
 n
i
 
.
These numbers correspond to the different ways of arranging the ﬁrst n non-zero
numbers into monotonic sequences, that is, sequences where the ﬁrst occurrence of
a number cannot follow that of a number greater than it. For example 1 1 2 3 3 4 5
is monotonic, while 1 1 3 2 3 4 5 is not monotonic. In this way we get a partition of
the positions of the sequence into undistinguishable non-empty cells (the different
numbers occurring in the sequence).
7.4.2
Catalan Numbers
Catalan numbers count the possible ways of writing correct expressions of n pairs
of parentheses.
Expression () is correct. If E is a correct expression, then also (E) is correct, and
if E1 and E2 are correct, then E1E2 is correct too. This number corresponds to the
different ways 2n persons at a table can shake hands by avoiding crossing, or the
ways we can connect the two extremes of a diagonal in a square grid, by means
of paths consisting of horizontal and vertical segments on the grid. These numbers
also equal to the numbers of sequences of n occurrences of 1 and n occurrences of
−1, such that the partial sums are never negative (from the beginning to any other
position). Let us call these sequences positive sequences over {1,−1}.
Theorem 7.10. The number of positive sequences over {1,−1} of length 2n is given
by the n-th Catalan number:
1
n + 1
2n
n


7.4 Stirling, Bell, Catalan, and Bernoulli Numbers
325
Proof. The sequences S over {1,−1} of length 2n are
 2n
n

. Let us partition the
sequences of S in the classes S0,S1,S2,...,Sn, where Si, for 0 ≤i ≤n, consists
of the sequences having i as maximum negative value of their partial sums. The
sequences that we want to count coincide with those of class S0. These classes have
the same number of elements. In fact, we can transform, by means of a one-to-one
correspondence, the class Si+1 in the class Si, for 0 ≤i ≤n. This transformation,
given a sequence of Si+1, interchanges the value −1 of the position where the partial
sum −(i+1) is reached with the value 1 of its next position (this value is necessarily
1). This means that |Si+1| = |Si|, for 0 ≤i ≤n. Therefore, |Si| = |S|/(n + 1), for
0 ≤i ≤n, and in particular, for the set S0 whose cardinality we want to count, we
have that |S0| = |S|/(n+1), that is, the number of our sequences is that one asserted
by the theorem.
⊓⊔
7.4.3
Bernoulli Numbers
Jakob Bernoulli developed in his Ars Conjectandi a generalization of Faulhaber’s
method for computing the sums of powers. He introduced an interesting sequence
of numbers, B(i) with i > 0, which now are called Bernoulli numbers. He reported
proudly of having computed the sum of the ﬁrst thousand tenth powers “intra semi-
quadrantem horae” (in seven minutes and a half).
Bernoulli numbers can be deﬁned, by induction, in the following way.
Initial Step. Let us consider the following equation:
B2 −2B1 + 1 = B2
that becomes:
2B1 = 1
by obtaining B1 = 1/2. This value is the ﬁrst Bernoulli number B(1).
Induction Step. For i > 1, consider the equation:
(B−1)i+1 = Bi+1
replace in it, for j < i, powers B j with the Bernoulli numbers B(j), computed at the
previous steps (power Bi+1 disappears). The value of Bi resulting from the obtained
equation is the Bernoulli number B(i).
Bernoulli proved the following theorem (see [218]).
Theorem 7.11 (Bernoulli’s formula for the sums of powers). The sum of powers
∑i=1,n ik−1 is given by the following equation, where double curly brackets mean
that, after applying Newton’s binomial formula, the powers Bi have to be replaced
with the Bernoulli numbers B(i).

326
7 Combinations and Chances
∑
i=1,n
ik−1 = {{(n + B)k −Bk}}
k
.
7.5
Stirling Approximation
The approximation of n! which we prove in this section, due to Stirling, is a key
point in many contexts of discrete mathematics.
Theorem 7.12 (Stirling Approximation).
n! →n→∞
√
2πn(n/e)n
Proof. Let us consider the ratio nn/n!, then
nn/n! = n
n ·
n
n −1 ·
n
n −2 ·... n
1.
Of course:
n
n −i =
n
n −1 · n −1
n −2 · n −2
n −3 · n −i+ 1
n −i
.
If we replace its left member with the right one, in the initial equation, then we get:
nn/n! =

n
n −1
!
n
n −1
n −1
n −2
"!
n
n −1
n −1
n −2
n −2
n −3
"
...
whence, by grouping in decreasing order:
nn/n! =

n
n −1
n−1 n −1
n −2
n−2n −2
n −3
n−3
...
that is:
nn/n! =
n−1
∏
i=1

1 +
1
n −i
n−i
where each factor seems to go to Euler constant e when n goes to inﬁnity, by sug-
gesting this (wrong) equation:
nn ≈n!en−1 (wrong)
This evaluation is wrong because these factors do not tend to e simultaneously. In
fact, when the ﬁrst one is near to e, the last one has to start its approximation.In order
to get a better evaluation we deﬁne the following term b(n) by trying to evaluate its
asymptotic behavior:
enn!/nn = b(n).

7.5 Stirling Approximation
327
The evaluation of b(n) is strictly related to the following Wallis’ product for π/2 (a
consequence of Euler’s jewel given in Sect. 5.4.3):
π/2 = ∏
n>0
4n2
4n2 −1 = lim
n→∞∏
i=1,n
4i2
4i2 −1
(7.5)
π
2 = 2
1 · 2
3 · 4
3 · 4
5 · 6
5 · 6
7 ... ·
2n
2n −1 ·
2n
2n + 1 ...
(7.6)
if we denote by (2n)!! the product of the even numbers equal to or less than 2n, we
get:
(2n)!! = 2n ·n!
(7.7)
moreover, if we denote by (2n+1)!! and by (2n−1)!! the product of the odd num-
bers equal to or less than 2n + 1 and 2n −1 respectively, we get:
(2n −1)!! = (2n)!/(2n ·n!)
(7.8)
(2n + 1)!! = (2n + 1)·(2n)!/(2n·n!)
(7.9)
therefore, Eq. (7.6) becomes:
π/2 →n→∞
(2n)!!·(2n)!!
(2n + 1)!!·(2n −1)!!
whence, according to Eqs. (7.7), (7.8), and (7.9), it follows that:
π/2 →n→∞
2nn!·2nn!
(2n + 1)·(2n)!/(2n·n!)·(2n)!/(2n·n!)
that is:
π
2 →n→∞=
24n(n!)4
((2n)!)2(2n + 1).
Now, let us consider the terms (b(n))2, b(2n) and evaluate the ratio (b(n))2/b(2n).
We have:
(b(n))2 = (n!)2e2n
n2n
b(2n) = (2n)!e2n
(2n)2n
(b(n))2
b(2n) = (n!)2e2n
n2n
(2n)2n
(2n)!e2n = (n!)2
n2n
22nn2n
(2n)! = 22n(n!)2
(2n)!
which, by approximating (2n + 1)1/2 by (2n)1/2, becomes:
(b(n))2
b(2n) = (2n + 1)1/2(π/2)1/2 →n→∞
√πn.

328
7 Combinations and Chances
The evaluation of this ratio allows us the evaluation of b(n) as n goes to inﬁnity.
In fact, b(n) has to be a sub-linear function of n, because otherwise the ratio (b(n))2
b(2n)
could not be of type c√n. Therefore, b(2n) = qb(n), with q < 2, and the following
equation holds:
(b(n))2
b(2n) = b(n)
q .
Now, if the above ratios have to equal √πn, then b(n) has to be asymptotically k√n
for some suitable k. If we replace this value in the above equation, then we get:
(b(n))2
b(2n) =
k2n
k
√
2n →n→∞
√πn
therefore
k√n
√
2
→n→∞
√πn
which means that k =
√
2π and b(n) →n→∞
√
2πn. This concludes the proof of
Stirling’s approximation (more precise forms of this approximation can be found in
Feller’s book [221]).
⊓⊔
7.6
“Ars Conjectandi” and Statistical Tests
A statistical distribution on a population of objects is obtained by associating to
each of them a numerical value. These values over the population naturally provide
a population of numbers, for which many important concepts can be deﬁned.
For example, let us consider a text seen as a multiset of words (ignoring their
order), if we assign to each word its length, we get a statistical distribution (of word
lengths) taking values in the positive integers.
Given a statistical distribution X, the frequency ν(x) of an element x ∈X is the
ratio between the multiplicity of x in X and the size of X. The range of X is the inter-
val between the minimum and the maximum values occurring in X. The majority
and the minority of X are the maximum multiplicity and minimum multiplicity of
X, respectively. The mode is the value (or the values) having the majority as multi-
plicity, while the median is the value in the middle position, when values of X are
arranged in increasing order (if the elements of X are an even number, either the last
value of the ﬁrst half, or the ﬁrst one of the second half is chosen as median).
A discrete statistical distribution or interval statistical distribution, is a statis-
tical distribution where the range of the distribution is partitioned into disjoint inter-
vals (usually having the same length) and only one value for each interval occurs in
the distribution, for example, the middle point of each interval.
Given a statistical distribution X = (x1,x2,...,xn), its mean μ(X) is given by:
μ(X) = x1 + x2 + ...+ xn
n
= ∑
x∈X
ν(x)x.
(7.10)

7.6 “Ars Conjectandi” and Statistical Tests
329
The mean square deviation σ2(X,x0) of X with respect to a reference value
x0 ∈R is a sum, for x varying in X, given by:
σ2(X,x0) = 1
n ∑
x∈X
(x−x0)2 = ∑
x∈X
ν(x)(x−x0)2.
(7.11)
The variance of X, σ2(X) is the mean square deviation of X with respect to the
mean μ(X):
σ2(X) = σ2(X,μ(X)).
(7.12)
It can be shown that variance reaches the minimum value among all possible mean
square deviations.
The standard deviation σ(X) of a statistical distribution X is the square root of
variance:
σ(X) =

σ2(X).
(7.13)
A fundamental (elementary) result, due to the Russian mathematician Chebichev,
states that in any statistical distribution X of mean μ and standard deviation σ, for
any positive real k, the fraction of the values of X which, in absolute value, differ
from μ more than or exactly equal to kσ is a fraction less than or equal to 1/k2.
Formally, setting (x ≥y) = 1 if the ≥relation holds between x,y and zero otherwise,
we have:
∑x∈X(x−μ ≥kσ)
|X|
≤1/k2.
(7.14)
This means that, using k =
√
2, at least half of the values lie in the interval μ −2σ
and μ + 2σ.
The entropy H(X) of a statistical distribution X is a sum, for x varying in X,
given by:
H(X) = −∑
x∈X
ν(x)logν(x).
(7.15)
This value, which is the basis of Shannon’s information theory, corresponds to a
measure of the disorder of a population X.
7.6.1
Statistics and Probability
A sample of a population X is a subpopulation of X, that is, a multiset where any
element occurs with a multiplicity less or equal to its multiplicity in X. A sample of
X is typical if the frequency of any element of the sample is the same frequency that
it has in X. Statistics is the science of discovering properties of populations from its
samples.
In statistical distribution the notion of multiplicity, which is the crucial concept
of multisets, provides naturally the notion of frequency, which can be seen as a sort
of relative multiplicity. Frequencies measure the occurrence of values by means of
fractions, and the sum of all frequencies of a statistical distribution is equal to 1.

330
7 Combinations and Chances
This notion is naturally extended to that of probability measure. Given a set of
elements, usually called events (and the set is a space of events) we may assign
to each of them a probability which determines the possibility of their appearance
with respect to a given (observation) context. An axiomatic approach, establishing
natural conditions which a probability measure has to satisfy, has been developed
in the last century by Kolmogorov. However, it is easy to realize that probability
theory is the natural mathematical setting for studying statistical phenomena. Many
important statistical parameters obey speciﬁc laws expressed by means of concepts
developed in the theory of probability. The basic concepts of probability theory are
those of random variable and probability distribution. A real random variable is
a variable ranging on real numbers that assigns to each real interval I a measure, less
than or equal to 1, to the event that the variable takes values inside that interval. The
theory of probability is a discipline which emerged in the modern age, with some
anticipations by the Italian mathematician Girolamo Cardano (1501–1576) (Liber
de ludo aleae, that is, The book about the dice game), Galileo (1564–1642) (Sopra
le scoperte dei dadi, that is, About discoveries on dice), and Christian Huygens
(1629–1695) (De ratiociniis in ludo aleae, that is, The rules of dice game). The ﬁrst
main intuitions were developed by Pascal (1623–1662) and Fermat (1601–1665),
but a systematic treatise on this subject was the book on Ars Conjectandi, written by
Jacob Bernoulli (1654 –1705), who used binomial coefﬁcients in the distribution of
boolean variables, and found the law of large numbers (frequencies approximate to
probabilities for large populations), called Theorema Aureus. The idea of assigning
degrees of possibility to events, is an aspect of great innovation with respect to the
classical mathematics, and implicitly, it replaces facts in suppositione objecti (farts
as they are) with that of facts in suppositione subjecti (facts as they are judged).
Namely, reality is not what it is, but what an observer judges it could be, according
to a percentage of “actuality”, in the context of a potential space of events.
The most important discoveries in probability theory, after Bernoulli’s work,
were developed since the 17th and18th centuries, starting with De Moivre (1667–
1754), who found the normal distribution as a curve for approximating large bino-
mial coefﬁcients. Bayes (1702 –1752) discovered the theorem named by his name
(independently discovered also by Laplace), concerning the inversion of conditional
probabilities. Laplace (1749 –1827) extended the De Moivre’s result about normal
distribution. Finally, Gauss (1777–1855) recognized the normal distribution as the
law of casual error distribution, and Poisson (1781–1840) introduced the distribu-
tion named by his name as the law of rare events.
A space of events is a special boolean algebra (with 0, 1, sum, product, and nega-
tion, analogous to the operations ¬,∧,∨over propositions). Moreover, a probability
measure can be easily assigned to a proposition, by measuring in some way, with
numbers in [0,1], the class of models where the proposition holds. However, an im-
portant remark about probability is the distinction of two different aspects: i) the
deﬁnition of the space of events which is more appropriate in a given context, and,
ii) the application of probabilistic methods for analyzing and deducing information
within a given event space. The methods of the theory of probability constitute a
conceptual framework which is, for many aspects, independent from the speciﬁc

7.6 “Ars Conjectandi” and Statistical Tests
331
space of events. A comparison may help to distinguish these two levels. The calcu-
lus and the theory of differential equations provide rules and methods for solving
and analyzing differential equations, but the choice of the right equation which ﬁts
the best description of a physical phenomenon is a different thing, which pertains to
the ability of modeling correctly phenomena of a certain type.
The axiomatic approach in probability theory was initiated by Kolmogorov and
was developed within the Russian mathematical tradition. It plays a role which is
comparable to the axiomatic approach in geometry, and it is important for under-
standing the logical basis of probability. However, it cannot replace the intuition and
the analysis of adequacy of probabilistic models with respect to the realities they ap-
ply to. Logical analyses of probability are related to inductive logics and to many
non-classical methods of logical inference (Carnap and Hintikka are two logicians
of the last century who developed theories in this direction). From the mathematical
point of view, probability theory is part of a general ﬁeld of mathematics, referred
to as Measure theory, initiated by French mathematicians of the last century. An im-
portant exponent of this school, Emil Borel, proved the following result about real
numbers, which is related to the intrinsic random character of real numbers: Almost
all numbers, when expressed in any base, contain every possible digit or possible
string of digits. Here, almost all means that the set of numbers without this property
are a set having null measure (a non-empty set may have null measure).
An event can be expressed by means of a proposition asserting that the value of
a variable X belongs to a subset of its range, denoted by range(X). This means that
the usual propositional operations ¬,∧,∨are deﬁned on events. The conditional
probability of an event A, given an event B, is denoted by P(A|B). It expresses a
sort of implicational probability, or the probability of A, under the assumption that
event B has occurred. Formally:
P(A|B) = P(A∧B)
P(B)
.
Events A and B are said to be independent, and we write A||B, if P(A|B) = P(A).
Events A and B are disjoint if P(A ∧B) = 0. The following rules connect proposi-
tional operations to probabilities . Proposition ¬A has to be considered in terms of
complementary set, that is, if A = (X ∈S), then ¬A = (X ∈range(X)−S).
1. P(A) ≥0
2. P(A∨¬A) = 1
3. P(A∧¬A) = 0
4. P(¬A) = 1 −P(A)
5. P(A∨B) = P(A)+ P(B)−P(A∧B)
6. P(A|B) = P(A∧B)/P(B)
7. A||B ⇔P(A∧B) = P(A)P(B).
The theory of probability is the ﬁeld were even professional mathematicians can
be easily wrong, and very often reasoning under probabilistic hypotheses is very
slippery. Let us report some examples, from [215], which can help to grasp main
subtle points about probability. Assume that a pilot has a 2% chance of being shot

332
7 Combinations and Chances
down in a military mission. It seems quite natural to conclude that in 50 missions
we can guess that he is almost sure to die. But this is completely wrong. In fact, let
us apply the rules above, then the probability of surviving is 0.9850, assuming that
each mission is independent from the others. Therefore the probability of dying is
1 −0.9850 = 0.64, which is markedly different from 1. The same argument shows
that the probability of obtaining one head, by tossing a coin twice, is not 1.
This kind of error was in the problem posed by the Chevalier de M´er´e to Pascal,
about a popular dice game: Why the probability of one ace, rolling one die 4 times,
is greater than that of both aces, rolling two dice 24 times? In fact, one could led
to reason in the following way. The probability of one ace is 1/6, and 4/6 = 2/3,
analogously the probability of 2 aces is 1/36, and 24/36 = 2/3, by concluding
that in principle the two events: “1 ace in 4 rolls”, and “2 aces in 24 double rolls”
are equiprobable. But the empirical evidence reported by Chevalier de M´er´e was
against this wrong conclusion. Namely, Pascal (interacting with Fermat on this puz-
zle) solved the apparent paradox by the following computation. In the ﬁrst game,
P(no-ace-in-4-tolls) = (5/6)4, therefore P(ace-in-4-rolls)= 1 −(5/6)4 = 0.5177. In
the second game, P(no-2-aces-in-24-double-rolls)= (35/36)24, therefore P(2-aces-
in-24-double-rolls) = 1 −(35/36)24 = 0.4914. A detailed analysis shows that the
simple mistake in the argument suggesting the same probability for the two events
is due to the violation of rule 5 given above.
Sometimes difﬁculties arise in choosing the right event space. The problem which
was posed to Galileo by the archduke of Tuscany was the following: Why rolling
three dice the value 10 is more frequent than 9?. His question was motivated by the
fact that there are 6 ways for decomposing 10 into three parts: (6+3+1), (6+2+2),
((5+4+1), (5+3+2), (4+4+2), (4+3+3), but even 6 ways of decomposing 9: (6+2+1),
(5+3+1), ((5+2+2), (4+4+1), (4+3+2), (3+3+3). The answer Galileo found (in mod-
ern language) is that the space of events is not given by the partitions of integers in
three parts, but by the possible outcomes of the three dice, and it is easy to realize
that there are 27 ways of obtaining 10 with three dice, while the 25 ways of obtain-
ing 9 with three tolls (for example (3+3+3) is realized in only one way, but (6+3+1)
can be realized in a number of ways corresponding to the 6 permutations of three
objects.
Another interesting case of confusion is related with conditional probability.
Bayes’ theorem, in a simpliﬁed form, asserts the following equation:
P(A|B) = P(A)P(B|A)/P(B).
Its proof is very simple. In fact, by the deﬁnition of conditional probability we have:
P(A|B) = P(A∧B)/P(B)
but
P(A∧B) = P(B∧A) = P(B|A)P(A)

7.6 “Ars Conjectandi” and Statistical Tests
333
therefore, the statement claimed by the theorem follows easily from the above two
equations.
Despite the simplicity of this proof, what the theorem asserts is an inversion of
conditions. In fact, P(A|B) can be computed by means of the inverse conditional
probability P(B|A). This is a very subtle point. Assume that a test T for a disease
D is wrong only in one out to 1000 cases. Assume that a person is positive to this
test. What is the probability of being affected by D? Is it 0.999? Fortunately it is not
the case. In fact, this deduction confuses P(D|T) (the probability of the disease D,
when the test T is positive) with P(T|D) (the probability test T positivity, when the
disease D is present). The right way to reason is the application of Bayes’ theorem.
For this, assume to know the simple probabilities of D and T. D affects one out of
10000 persons, and T provides positivity one out of 1000 cases. Let us assume also
that T is very reliable, with P(T|D) = 1, that is, if D is present, then T is positive
with probability 1. This means that in 10000 persons we have 11 positive cases:
10 because T is wrong 1 to 1000) plus 1, because 1 to 10000 has the disease D
and T discovers it. In conclusion, the probability of having D is less than 10% . In
fact:
P(D|T) = P(D)P(T|D)/P(T) = 1/10000 × 1/(11/10000)= 1/11.
A famous debate about probability is the three doors quiz also known as the Monty
Hall problem, by the name of the TV conductor in a popular American show. There
are three doors A,B, and C. A treasure is behind only one of them, and you are
asked to guess which is the lucky door. Let us assume that you choose the door C.
Then, another chance is given to you. In fact, they open the door B, showing that it
is not the fortunate door, and you are free of conﬁrming your preceding choice C,
or changing by guessing A. The question is: What is the more rational choice? Con-
ﬁrming C or changing it with A? Many famous mathematicians concluded that there
is no reason for changing (Paul Erd¨os, the number one of 20th mathematicians was
among them). However, by using Bayes’ theorem it can be shown that changing the
door is the better choice, because it provides a passage from a probability 1/3 of suc-
cess to a probability of 2/3. This fact was even conﬁrmed by computer simulations
of the game. We do not present the detailed analysis based on Bayes’ theorem, but
we give an intuitive, very convincing reason. If the doors are 100: D1,D2,...D100
and after you guess D100, the doors D2,D3,... are shown to be unlucky, are you
sure to conﬁrm D100, or rather, are you inclined to believe that is wiser change your
initial choice? In fact, what is crucial in this game is that opening doors change the
the space of events.
7.6.2
Statistical Inference
The simplest probabilistic schema is an urn of n balls, where m < n are white and
n −m are black. This model, due to Jakob Bernoulli, assumes a boolean random
variable (the color of the extracted ball), but it is of crucial importance in analyz-
ing and developing general probabilistic concepts. Moreover, it is surprising that

334
7 Combinations and Chances
Laplace, the mathematician who is the symbol of deterministic perspective in sci-
ence, is also the same scientist who grasped the importance and generality of the
normal distribution, as the most extraordinary law of chance. At a ﬁrst glance, this
seems to be a paradox. In fact, if chance is the absence of rules, how is it possible to
discover a law for phenomena which are outside of any law? Maybe, just this appar-
ent paradox can provide a sort of reconciliation between determinism and chance.
In fact, a way of considering the laws of chance, is that of rules for passing from
the nondeterminism, at level of individuals, to a collective determinism. Namely,
chance is not the pure absence of causes, but rather it is a population of (small)
causes such that we cannot distinguish them singularly [230], but in their collective
combination. Probability enlarged the power of mathematical analyses and expla-
nations, by replacing the rigid Newtonian model of an exact predictability with a
conceptual framework where complex phenomena going beyond any kind of New-
tonian explanation can be mathematically framed for discovering mutual inﬂuence
and causal relations. For this reason, presently, probabilistic and statistic methods
are applied everywhere. The discovery, in the second part of the last century, of
deterministic chaos reinforces the limitations of a rigid determinism in science. In
fact, even in simple systems, under suitable conditions, it happens that there is no
way of predicting the behavior of the system even if it is completely deterministic,
because the system is so sensible to the determination of its initial state that only
an (impossible) inﬁnite precision could avoid an error of its future states which will
exponentially amplify with time.
Since the 19th century, statistics begins to be recognized as a discipline, ﬁrstly
related to the quantitative analysis of population phenomena interesting for soci-
ological and economical analyses (birth and death rates, richness distribution, mi-
gration ﬂuxes, and so on). Adolphe Qu´etelet (1796–1874) played a crucial role in
the connection between statistics and probability. He tried to apply the Laplace dis-
covery about the central role of normal distribution in the context of sociological
disciplines. His program was very ambitious (deﬁning a quantitative analysis of so-
ciology based on normal distribution), but his ideas had a strong inﬂuence through
the publication of a volume of Henry Thomas Buckle, who reported, in detailed
terms, the theories of Qu´etelet. This volume had great success, and it was certainly
read by Darwin, Maxwell, and Boltzmann, suggesting to them the power of statis-
tics in the analysis of natural phenomena. If the theory of probability, starting from
a given space of events, provides methods for assigning probabilities to complex
events, statistics plays the inverse game, because it provides methods for inferring
the right probabilities which underlie some statistical observation. This idea had
been emerging since Laplace, but became a crucial aspect of the statistical studies
of 20th century when mathematically advanced methods were developed, in two
complementary directions: i) inferring probabilities from statistical data, ii) discov-
ering the probabilistic laws of statistical parameters. This second trend provided one
of the most powerful tools in the causal analysis of population phenomena, namely,
the methods for testing hypotheses about cause/effect relations. Galton (Darwin’s
cousin), who tried to apply Qu´etelet’s approach to biology, introduced the important
notion of correlation, better formalized by Karl Pearson [229], who discovered the

7.6 “Ars Conjectandi” and Statistical Tests
335
χ2 test, Gosset discovered the Student’s test (Student is Gosset’s pseudonym), and
Fisher the F test. Fisher, who was a great mathematical statistician [222], was also
a great geneticist, he gave a statistical interpretation to natural selection, by means
of the Natural selection theorem showing that, under suitable hypotheses, the force
driving evolution pushes a biological population toward the maximum of possible
ﬁtness (the maximum possible demographic rate reachable in a given context).
The use of correlation indexes and of tests of signiﬁcance, according to speciﬁc
methods, provide effective tools in statistical regression techniques, which, in very
general terms, are aimed at ﬁnding the best analytical forms which underly time
series of observed data, an important ﬁeld of statistical application in the same line
of Gauss’ methods of minimum least squares (by means of which Gauss solved an
astronomical puzzle by determining the orbit of the asteroid Ceres).
It is reported [228] that Henry Poincar´e proved the dishonesty of a baker, by
discovering that the weights of the daily loaves of bread ﬂuctuated without follow-
ing the gaussian distribution (clearly, a guilty defect with respect to the declared
weight). This is a simple example of using a law of chance for ﬁnding the exis-
tence of a non-accidental motivation providing an observed effect. In other words,
Poincar´e’s experiment answered the question: Is there some cause which is not due
only to chance, affecting the weights of loaves of bread?.
Hypothesis testing is largely the product of Ronald Fisher, Jerzy Neyman, Karl
Pearson, and Egon Pearson (Karl’s son), developed in the early 20th century. Fisher
coined the term tests of signiﬁcance for denoting the essence of hypothesis testing.
In general, such tests provide tools for evaluating data coming from sampling popu-
lations or from analytical models for comparing them, in order to discover whether
data of distinct sources signiﬁcantly differ with respect to casual differences. The
distinction between casual and causal differences is the basis for: i) recognizing
the existence of cause providing some observed effects; ii) establishing that some
observed effects can be associated to a given cause acting on individuals of a pop-
ulation, or iii) evaluating if a model explains some observed data, apart from some
approximation errors of casual nature.
For example, in a population of people affected by a disease, does the treatment
with a drug have a positive effect against some pathology? Let us assume that we
want to observe the possible effects of a speciﬁc drug on the blood pressure in
persons affected by blood hypertension. Comparing two samples, one of which was
exposed to the drug treatment, we would deduce whether the effects we observe
could be associated to the treatment or are simply casual ﬂuctuations.
The basic approach of a statistical test consists in the comparison of the casual
hypothesis H0, claiming that the observed effects are casual, against the opposite
hypothesis H1, according to which the observed effects are due to a phenomenon
acting on the population. In this way then we can conclude, on a statistical basis, in
favor of a proved effect caused by the phenomenon in question. If the test concerns
only a parameter of a population, and no comparison is performed with a population
of reference, we can only conclude that a cause is acting on the parameter, apart from
the inﬂuence of chance (the nature of this cause has to be deduced by using other
kinds of information, as in the case of Poincar´e’s experiment).

336
7 Combinations and Chances
The general schema of performing such a test is given by the following
procedure:
1. The measure of a statistical parameter Q in two populations (for example the
mean blood pressure);
2. The determination of the probabilistic law F which the chosen parameter follows
(F gives, for each possible value of the parameter Q, the probability of having
samples where the parameter assumes that value);
3. The calculation of the value V of the chosen parameter, in the population where
the cause under investigation is acting;
4. Deﬁnition of the conﬁdence interval of the test, that is, the level of error proba-
bility E which is tolerated in the choice of the hypothesis (for example, 3% or
5%);
5. The localization of the observed value V in the distribution probability F;
6. The evaluation of the probability PV which F assigns to the event {Q ≥V} (or
{|Q| ≥|V|}, in the case of a symmetric distribution);
7. The rejection of the null hypothesis if PV ≤E, with the conﬁdence E, or accep-
tance in the other case (with conﬁdence E).
In the previous procedure, PV is also called the p-value of V, while the value V0
such that the probability of {Q ≥V0} is less than E, is called the critical value for
Fig. 7.5 Statistical inferential value of test parameters

7.6 “Ars Conjectandi” and Statistical Tests
337
the test (with respect to the conﬁdence E). Equivalently, instead of considering the
p-value, the null hypothesis can be rejected when V ≥V0.
7.6.3
Sample Signiﬁcance Indexes
Analyzing a population can be aimed at selecting individuals of the population hav-
ing some property of interest. In the following we express some sampling situations
in terms of clinical tests [223]. A population D is partitioned in two sub-populations
D+ and D−of people having a disease, and people without the disease, respectively.
The same population is selected in other two sub-populations T + and T −of people
resulting positive to a clinical test T and people resulting negative to the test, respec-
tively. Some indexes can be deﬁned for the test T, which characterize its capacity in
discriminating correctly, a priori and a posteriori, with respect to disease D.
Table 7.2 deﬁnes four important indexes of a clinical test T for a disease D.
Table 7.2 Indexes of adequacy for clinical tests
Sensitivity
|T + ∩D+|/|T+|
Speciﬁcity
|T −∩D−|/|T−|
Positive predictability
|T + ∩D+|/|D+|
Negative predictability |T −∩D−|/|D−|
We can interpret a test as a measure of reliability of statistical inference with
respect to a property of having a pathology. Table 7.3 explains the meaning of sta-
tistical inferences.
Table 7.3 Statistical inferential value of test indexes
High value
Reliability
Sensitivity
Positivity ⇒Disease
Speciﬁcity
Negativity ⇒Health
Positive predictability
Disease ⇒Positivity
Negative predictability
Health ⇒Negativity
Given a population X, the population of the samples of X is a second level pop-
ulation which is very important in the statistical analysis of X. In fact, it provides
some statistical distributions, called sample distributions of the samples taken from
X. The power of statistics is based on the evidence that sample distributions follow
precise mathematical laws. These are the basis for important procedures inferring
properties of populations from their samples.

338
7 Combinations and Chances
The h-index of a population X over a set A (also called Hirsch index, introduced
in the context of bibliometrics), is the maximum number n of elements of A having in
X a multiplicity greater than or equal to n. For example, the h-index of the population
2a + 3b + 4c + 5d is 3, because the multiplicities of 3 of them, b,c,d, are equal to
or greater than 3.
Let us consider the statistical distribution of values provided by a variable Z as-
suming values in correspondence to the individuals of a population. Let us order the
values of this distribution in a decreasing order. In many real cases there is a law
relating the ordering position of a value v with the percentage of elements x of the
population for which Z(x) ≥v. For example, in the statistical distribution of the rich-
ness (expressed in money) of a population, it is very common that around the 80%
of the total richness of the population is covered by around the 20% richest people
(in the richness order). This is the famous Pareto law of 20/80 (from the Italian
economist who discovered this regularity). Similar laws occur in many situations of
resource distributions, and provide patterns of general relevance.
7.7
Least Square Approximation
The method of least squares was introduced by Carl Friedrich Gauss around 1794
as an approach to the approximate solution of overdetermined systems, that is, sets
of equations with more equations than unknowns. The term “least squares” is used,
because the overall solution minimizes the sum of the squares of the errors made
with respect to an exact solution of the system.
A very elegant analysis of this method can be developed in terms of vector spaces
[225]. In a vector space over the ﬁeld R of real numbers an internal scalar product
( | ) of vector pairs is deﬁned such that (v|w) ∈R. In this case we say that the
vector space is a pre-Hilbert real space. The scalar product of a pre-Hilbert real
space deﬁnes a norm by ||v|| =

(v|v) and a notion of convergence with respect
to this norm can be given which generalizes the convergence of real functions. A
pre-Hilbert real space is complete if any Cauchy sequence of vectors converges to
a vector in the space. A pre-Hilbert space which is complete is said to be a Hilbert
space.
A very general and crucial result about pre-Hilbert spaces is the projection the-
orem given in Table 7.4. This theorem is the basis for the Least Square Estimation
given in Table 7.5.
Table 7.4 Existence and unicity of the approximating vector with orthogonal error
The projection theorem. Let H a Hilbert space and M a closed subspace of H. There
exists a unique vector m0 ∈M such that ||x−m0|| ≤||x−m|| for any pair x ∈H,m ∈M,
and the error vector e = x−m0 is orthogonal to M, that is, (e | m) = 0 for every m ∈M.

7.7 Least Square Approximation
339
Given n vectors x1,x2,...,xn of a real Hilbert space H, we know that their linear
combinations with real coefﬁcients provide a closed subspace K (a subset of H
closed under the vector operations) of H. Therefore the vector v ∈H closest to K
has to minimize the norm
||v−c1x1 −c2x2 −...−cnxn||.
According to the projection theorem, the unique minimizing vector x0 ∈K has to be
orthogonal to K. Therefore, for i = 1,2,...,n:
((v−c1x1 −c2x2 −...−cnxn) | xi) = 0.
This means:
c1(x1 | x1)+ c2(x2 | x1)+ ...+ cn(xn | x1) = (v | x1)
(7.16)
c1(x1 | x2)+ c2(x2 | x2)+ ...+ cn(xn | x2) = (v | x2)
.......................................... ... ......
c1(x1 | xn)+ c2(x2 | xn)+ ...+ cn(xn | xn) = (v | xn)
The matrix G which is transpose to the matrix of system 7.16 is called Gram matrix
of the n vectors generating the vector space K.
Given a set of m equations with n unknowns, which expresses a linear combina-
tion of n linearly independent vectors of Rm for some (unknown) coefﬁcients, there
exists a unique n-vector of these coefﬁcients providing the best approximation to a
given m-vector v. The unicity of this n-vector, and its algebraic form, is determined
by the theorem of Table 7.5, as a natural consequence of the projection theorem
for Hilbert spaces, and the solution is expressed in terms of the Gram matrix of the
vectors.
Table 7.5 The Least Square Evaluation of matrix W
Least-Square-Estimate. Let W a m×n matrix (m > n) with linearly independent col-
umn vectors and let Wz be the matrix product of W by z ∈Rn . Then, for any m-vector
v ∈Rm, there exists a unique vector z0 ∈Rn minimizing ||v−Wz|| (the Euclidean m-
dimensional norm) over all z ∈Rn. Moreover, if W T denotes the transpose of W, this
vector z0 is given by z0 = (W TW)−1W T v.
The existence and unicity of vector z0, as the Least-Square-Estimate claims (see
7.5), follows directly from the projection theorem. Moreover, the Gram matrix cor-
responding to the column vectors of matrix W is easily seen to be W TW (see left
sides of Eqs. (7.16)), and W Tv corresponds to the right hand vector of equation

340
7 Combinations and Chances
system (7.16). Therefore, given the linear independence of the column vectors of
W, the matrix W TW is invertible, and the asserted value for z0 can be computed.
7.7.1
The k-Variable Multiple Regression Model
In this section we will recall the classical k-variable multiple regression. The reader
can ﬁnd more details and statistical motivations in Aczel and Sounderpandian’s
book [216], from which we adopt the notation.
In statistics, the regression analysis provides techniques for ﬁnding the relation-
ship between a dependent variable Y and one or more independent variables X1, X2,
..., Xk. The following equation is the general form of a linear regression:
Y = β0 + β1X1 + β2X2 + ...+ βkXk + ε.
(7.17)
When k > 1 the regression equation above is called a multiple regression model.
The correctness of the regression model is subjected to the following assumptions:
1. For each observation, the observation error ε is normally distributed with mean
zero and standard deviation σ and is independent from the errors associated with
all other observations;
2. The variables Xi are independent from the error ε.
Table 7.6 The three deviations associated with the data points (see also Fig. 7.6). We indicate
withY the value of the dependent variable, with Y its predicted value by means of the multiple
regression model, and with ¯Y the average of the values of Y. Finally, we indicate with n the
total number of data points used during the regression
Y −¯Y
=
Y −Y
+
Y −¯Y
Total
Unexplained
Explained
deviation
deviation (error)
deviation (regression)
∑n
j=1(y[ j]−¯y)2 = ∑n
j=1(y[ j]−y[ j])2 +
∑n
j=1(y[ j]−¯y)2
SST
SSE
SSR
Sum of Squared
Sum of Squared
Sum of Squared
Total deviations
Errors
Regression deviations
If the assumptions given above are satisﬁed, then we can compute the coefﬁcients
ci,i = 0,...,k in terms of least squares estimations of the regression parameters βi,
as the values providing the best approximation Y to the value Y:
Y = c0 + c1X1 + c2X2 + ...+ ckXk
(7.18)

7.7 Least Square Approximation
341
Fig. 7.6 Plot representing the three deviations introduced in Table 7.6 for a linear regression
model
which are given by the Least Square approximation:
⎛
⎜
⎜
⎝
c0
c1
...
ck
⎞
⎟
⎟
⎠=

MT × M
−1 × MT ×
⎛
⎜
⎜
⎝
Y[1]
Y[2]
...
Y[n]
⎞
⎟
⎟
⎠
(7.19)
where M is the following matrix:
M =
⎛
⎜
⎜
⎝
1 X1[1] X2[1] ... Xk[1]
1 X1[2] X2[2] ... Xk[2]
...
...
...
... ...
1 X1[n] X2[n] ... Xk[n]
⎞
⎟
⎟
⎠.
The correctness of the estimated values, with respect to the real ones, depends on
the amount of the unexplained deviation (i.e. the regression error) as indicated in
Table 7.6 and displayed in Fig. 7.6.
The Mean Square Error is an unbiased estimator of the variance of the errors.
It is given by the ratio of the sum of squared errors (SSE) with the number of the
degrees of freedom associated to the regression model, that is, the number of data
points minus the number of the regression coefﬁcients used in the model (n is the
number of data points used for the regression):
MSE =
SSE
n −(k + 1) = ∑n
j=1(y[j]−y[j])2
n −(k + 1)
.
(7.20)
We remark here that when we use n data points, a regression model which uses n−1
independent variables always reaches a perfect ﬁt. However, when we do this we are
overﬁtting our data, leaving no degree of freedom for errors. In this case, we will ﬁt

342
7 Combinations and Chances
Fig. 7.7 Comparison between the predicting power of two regression models: a 13-degree
polynomial Y = c0 +c1X +c2X2 +...+c13X13 (depicted by the continuous line) and a least-
square line (depicted by the dotted line). The dataset used to calculate the models is the 14
points depicted as circles, while the last point represented by a star is the value of Y we predict
with our models. The 13-degree polynomial is a perfect example of model which overﬁts the
data. Namely, it fails completely the prediction of Y in the 15th data point
perfectly Y in the n data points, but it will give a very poor prediction of Y in other
points (see Fig. 7.7 for an example of overﬁtting).
The mean square error is a measure of the size of regression error, but does not
give any indication about the explained component of the regression. The multiple
coefﬁcient of determination provides a measure of the capacity of a regression model
in explaining the dependent variable (SSR,SST,SSE as in Table 7.6):
R2 = SSR
SST = 1 −SSE
SST .
(7.21)
The value R2 measures the part of the dependent variable variation which is ex-
plained by the combination of the independent variables in the multiple regression
model. In other words, R2 measures the percentage of Y explained by the regression
model in terms of Xi variables (even if it is not relevant here, the notation R2 is due
to the fact that it corresponds to the square of another statistical index). However,
R2 has also a disadvantage, because it does not take into account the number of
independent variables used in the regression model. For this reason, the adjusted
multiple coefﬁcient of determination is introduced (MST = SST/(n −1)):
¯R2 = 1 −SSE/[n −(k + 1)]
SST/(n −1)
= 1 −MSE
MST
(7.22)
which accounts the degrees of freedom of SSE and SST, by giving a sort of penalty
to those models which ﬁt the data well, but that are not parsimonious, as they use
too many independent variables.
When we deﬁne a new regression model, we need to avoid the use of independent
variables that are not related with the dependent variable Y. In other words, when
we try to deﬁne a regression model, we have to answer to this basic question: is

7.7 Least Square Approximation
343
there a linear regression relationship between the dependent variable Y and any of
the independent variables Xi used by the regression equation? The statistical test F
will answer the question.
The following value:
F =
SSR/k
SSE/[n −(k + 1)] = MSR
MSE =
R2
1 −R2 · n −(k + 1)
k
(7.23)
provides the ratio between the variance explained by the regression model (given
by the MSR) and the unexplained variance (given by the MSE). It is well-known in
the literature that, under the hypothesis that F is a ratio of unrelated variances, its
value follows an F[k,n−k+1]-distribution (with k and n−(k+1) degrees of freedom),
a probability density distribution which takes the name of the English statistician
Sir Ronald A. Fisher and is widely used in statistics to compare the distribution
of two populations and to carry out the analysis of variance. In Fig. 7.8, the plot
of an F-distribution with 5 and 14 degrees of freedom is depicted, the one which
should be used, for example, when we want to test a regression model which ﬁts
n = 20 data points by means of k = 5 independent variables. After having ﬁxed
a signiﬁcance level α for the test, which speciﬁes the probability of error of the
test, we can reject the null hypothesis H0 when the value of the F-ratio of a given
regression model is greater than a threshold value F[α;k,n−(k+1)]. This value divides
the area of the F-distribution in two different parts of cumulative probabilities 1−α
and α, respectively. If we ﬁx α = 0.05, then we can conclude, with 5% probability
of being wrong, that there exists a linear relationship between Y and any of the
independent variables occurring in the regression equation, when the value of the F-
ratio is greater than F[α;k,n−(k+1)]. In the case of considering the distribution F(5,14),
this value is equal to 2.9582. The value of F[α;k,n−(k+1)] is called the critical value
of F[k,n−(k+1)] for α. In the literature, some tables are given which provide the right
values of F[α;k,n−(k+1)] for different values of α and of the degrees of freedom of the
F-distribution.
Fig. 7.8 The plot of the F-distribution F(5,14) with 5 and 14 degrees of freedom. The full
right-tailed area of 0.05 is the one which represents the rejection region for the null hypothesis
H0 deﬁned in Table 7.7

344
7 Combinations and Chances
Table 7.7 A statistical hypothesis test for the existence of a linear relationship between Y
and any of the Xi in the regression model of Eq. 7.17
H0: β1 = β2 = β3 = ... = βk = 0
H1: not all βi (i = 1,...,k) are zero.
In conclusion, if the null hypothesis H0 is true, no linear relationship exists be-
tween Y and any of the independent variables proposed in the regression equation.
On the other hand, if we reject the null hypothesis, there is statistical evidence to
conclude that a regression relationship exists between Y and at least one of the in-
dependent variables Xi, i = 1,...,k (see 7.7). The F test provides what is also called
the analysis of variance, based on the localization of the F-ratio with respect to the
F-distribution (with respect to a signiﬁcance level).
Fig. 7.9 The plot of a t-distribution with 14 degrees of freedom
Another probability distribution which is useful in the context of multiple re-
gression is the Student’s t-distribution (or simply the t-distribution), a continuous
probability distribution that is the standard method for evaluating the conﬁdence in-
tervals for a mean when variance is unknown (assuming that population is normally
distributed). Such a kind of distribution is used for calculating the conﬁdence in-
tervals for the least squares estimations of the regression coefﬁcients calculated in
(7.18). In statistics, a conﬁdence interval provides the evaluation of the range, of an
estimated parameter, where it is likely to ﬁnd the correct value of the parameter. This
evaluation is relative to a signiﬁcance value α which gives the probability of being
wrong in the conﬁdence estimation. The (1 −α)% conﬁdence interval for each βi,
i = 0,...,k in Eq. (7.17) is given by:
βi = ci ±t[α/2;n−(k+1)]

eii ·MSE,
(7.24)
where t[α/2;n−(k+1)] is the critical value of a t-distribution with n−(k+1) degrees of
freedom for α/2, and eii is the element in position (i,i) of the matrix (MT × M)−1
used in Eq. (7.19), which is the sum of the squares of Xi (the term √eii ·MSE is the

7.7 Least Square Approximation
345
“scale factor” in the speciﬁc estimation of the conﬁdence interval of ci). In Fig. 7.9
the t-distribution t(14), with 14 degrees of freedom, is depicted with signiﬁcance
level α = 0.05.
7.7.2
The Classical Stepwise Regression
In the previous section we have deﬁned main concepts about multiple regression,
by indicating some statistical tests which are used to understand the correctness of
a given regression model. In this section, we address the problem of variable selec-
tion, that is, the problem of deciding which independent variables have to enter into
a multiple regression model, among a given set of possible variables. The simplest
method we can deﬁne consists of running all possible regressions, for all possible
choices of independent variables, and then choosing the best model by selecting the
one having the highest ¯R2 or the lowest MSE. This brute-force method has the prob-
lem that it considers a number of models which increases exponentially with respect
to the number of possible variables. In fact, the number of different models that we
can deﬁne by means of k independent variables is 2k.
The stepwise regression algorithm provides a method for variable selection which
allows us to obtain good regression models with a lower complexity in time. This
algorithm does not necessarily ﬁnd the best model among all possible 2k models, but
it allows us to ﬁnd a good model in a feasible time even when we need to consider
a high number of independent variables. The method uses a statistical test, again
based on F-distribution, which is called partial F-test, as it evaluates the relative
signiﬁcance of a subset of all possible variables.
Suppose that a regression model of Y with k independent variables is postulated:
Y = β0 + β1X1 + β2X2 + ...+ βkXk + ε.
(7.25)
We will call this model the full model in the sense that it includes the maximal set
of independent variables. Now, suppose that we want to test the relative signiﬁcance
of a subset of r of the k independent variables in the full model. The partial F-test
provides a statistical criterion for evaluating if the full model given in Eq. (7.25) is
better than the reduced model with only k −r variables:
Y = β0 + β1X1 + β2X2 + ...+ βk−rXk−r + ε.
(7.26)
This corresponds to comparing the two hypotheses given in Table 7.8.
Table 7.8 Hypothesis of the partial F-test
H0: βk−r+1 = βk−r+2 = ... = βk = 0
H1: βk−r+1,βk−r+2,...,βk are not all zero.

346
7 Combinations and Chances
Fig. 7.10 The plot of the F-distribution F[r,n−(k+1)]. The full right-tailed area is the one which
represents the rejection region for the null hypothesis H0 deﬁned in Table 7.8.
The value which is considered in the partial F-test is given by:
F[r,n−(k+1)] = (SSER −SSEF)/r
MSEF
(7.27)
where SSRR is the sum of error squares of the reduced model, SSEF is the sum of
error squares of the full model, MSEF is the mean square error of the full model,
k is the number of independent variables in the full regression model, and r is the
number of variables removed from the full model when it is reduced. The difference
SSER −SSEF is called the extra sum of squares associated to the reduced model.
Since this sum of squares refers to r variables, it has r degrees of freedom. Also the
partial F-test is based on an F-distribution (like the test deﬁned in Table 7.7 in the
previous section). After having ﬁxed a signiﬁcance level α for the test, we can reject
the null hypothesis H0 (concluding that the full model is statistically better than the
reduced one) when the value of the partial F-statistic is greater than the threshold
value F[α;r,n−(k+1)] (see Fig. 7.10).
The algorithm of stepwise regression carries out partial F-tests by using the no-
tion of p-value. The p-value is the probability that the null hypothesis H0 of Ta-
ble 7.8 is correct, and is given by the size of the right-tailed area, in Fig. 7.10, of
the considered F-distribution beyond the critical value given by the partial F-test.
Using of p-values to carry out the test is perfectly equivalent to use critical values
already considered in F-statistics.
The regression algorithm is made of two different parts. The ﬁrst one, called for-
ward selection (steps 2 and 3, below), is devoted to increasing the set of independent
variables used by the regression model, by adding the most signiﬁcant ones among
those which do not occur in the model. The second phase is called backward elimi-
nation (steps 4 and 5, below) and is devoted to the elimination of variables that have
been previously inserted in the regression model, but which are become statistically

7.8 Trees and Graphs
347
less signiﬁcant after the insertion of some variables. Finally, the algorithm ends at
step 3, when it is not possible to further improve the model by adding or remov-
ing variables. The minimum values of signiﬁcance for introducing and for dropping
variables in the model are called Pin and Pout, respectively. They correspond to sig-
niﬁcance levels of the partial F-test. They do not need to be equal, but they must
be set very carefully because if Pin > Pout, then a loop may occur where a variable
enters the model, then leaves it, then reenters, and so on.
The stepwise regression algorithm is based on the following steps:
1. Start from a user deﬁned k-variable multiple regression model (possibly,
k = 0).
2. Compute the partial F-test for each variable not yet included in the regres-
sion model.
3. Check if there is at least one variable whose p-value is less than the user
deﬁned threshold value Pin (this is the same of checking for those variable
whose partial F-statistic is greater than the threshold value F[Pin;1,n−(k+1)]).
If there are some variables which fulﬁll the requirement, add to the model
the most signiﬁcant one (i.e. the one with smaller p-value). If there is no
variable which can be added, the regression algorithm stops.
4. Compute the partial F-test for all variables currently in the regression
model.
5. Check if there is at least one variable whose p-value is greater than the user
deﬁned threshold value Pout. If there are some variables which fulﬁll the
requirement, remove from the model the less signiﬁcant one (i.e. the one
with greater p-value).
6. Restart from step 2.
7.8
Trees and Graphs
Trees and graphs are everywhere in discrete mathematics and in computer science.
Moreover, a huge number of discrete models in biology are based on trees and/or
graphs. Here we present some basic concepts about them (see [224, 217, 220] for
more advanced concepts).
A (rooted) tree T is speciﬁed by a structure T = (N,r, f), where N is a set of
nodes, (or vertices), including a node r, called the root of T, and f is a function,
assigning a father node to any node of N different from r, such that, for any node
x different from r, f n(x) = r for some natural number n (f n(x) denotes the n-fold
application f(... f(f(x))...) of f to x, f 0(x) = x, and f 1(x) = f(x)). The termi-
nology about trees is inspired by the many different contexts where they occur, es-
pecially genealogy and botanic. For example: root, node, leaf, parent, father, child,
son, brother, ancestor, descendant, branch, generation, path, and depth.

348
7 Combinations and Chances
A simple (unoriented) graph G is speciﬁed by a pair G = (N,E), where N is a
set nodes, (or vertices), and E is a set of edges (or arcs), such that, for any e ∈E,
an unordered pair of nodes of N is speciﬁed that are connected trough e (the graph
is oriented if the pairs of nodes associated to edges are ordered pairs, it is a multi-
graph if different edges connect a same pair of notes). Rooted trees can naturally be
expressed as graphs where an edge is set between every node and its father.
Basic terminology and example of trees and graphs are given in Chap. 1 (here
we recall that ”parent, child, sibling” are also used instead of ”father, son, brother”,
or ”mother, daughter, sister”; f(y) = x means that x is father of y, and y is son of x,
and when also f(z) = x that x and y are brothers). In particular, a path of a graph
is a sequence of nodes where two successive nodes are connected by an edge of the
graph. A cycle is a path where the last node is connected with the ﬁrst node. A graph
is connected if there is a path between any pair of nodes in the graph. Moreover, it
can be shown that the previous characterization of rooted tree is equivalent to that of
acyclic (without cycles) and connected graph with one particular node, chosen as its
root (if no root is chosen, the structure is an unrooted tree). Deﬁnitions by induction
of trees and graphs can be found in Sect. 5.5.
Rooted trees and graphs can be easily represented by means of diagrams, where a
node is a point (or a closed region of space) and an edge is a curve connecting points
(regions). In terms of diagrams, a rooted tree has a root point, which branches with
other son points which possibly branch again. The terminal nodes which do not
continue this process are the leaves of the tree. Tags can be added to nodes and
edges of trees and graphs, which represent speciﬁc kinds of information relevant to
particular cases of application.
7.8.1
Types of Trees
The study of trees started with the research of Arthur Cayley who developed an
enumeration method in 1875 for calculating the number of isomers of branched
alkanes.
There are many subtle aspects which identify different notions of trees. Usually,
in the case of trees the attribute “labeled” without further speciﬁcation means that
nodes are distinguishable elements, while “unlabeled” means that nodes are not dis-
tinguishable for themselves, but only for their positions in the tree. In other words,
the individual identity of each node is not relevant in the deﬁnition of the tree (how-
ever this terminology is not uniform, because very often labels are used as tags,
allowing different nodes to have the same label/tag). When speciﬁc values are as-
signed to nodes (and edges) of a tree, for avoiding any confusion with the attributes
labeled and unlabeled, we say that the tree is decorated.(unfortunately this notation
is not uniform in the literature).
The tree given in Fig. 7.11 is a decorated tree (by numbers and operations).
If among the sons of a node an order is assumed, then the tree is called an ordered
tree. Genealogical trees are ordered (this was an important issue in royal families,
for deciding the crown rights).

7.8 Trees and Graphs
349
An ordered unlabeled tree is representable by an expression built only by paren-
theses, for example:
((()())(()(()()))).
In this case, the root correspond to the external parentheses. This root has two sons,
and they have two sons too. The second son of the second son branches again with
two nodes.
Every algebraic expression can be represented by a rooted tree. For example, the
following expression:
(((2 + 3)× 5)+ (23× 5))
can be represented by the tree given in Fig. 7.11.
Fig. 7.11 The tree diagram of the expression (((2+3)×5)+(23 ×5))
We can construct any tree by means of a derivation, developed in consecutive
steps. At any step, an element occurs which is either a leaf of the tree, or the father
of elements occurring at previous steps (#i is the element occurring at step i). The
following derivation provides, at its ﬁnal step, the expression (((2+3)×5)+(23×
5)) (for easier reading, numbers and operations decorating the nodes are indicated,
with the partial expressions corresponding to the steps):
1. 2
2. 3
3. 5
4. #1 + #2 = (2 + 3)
5. #4 × #3 = ((2 + 3)× 5)
6. #1 exp #2 = 23
7. #6 × #3 = (23 × 5)
8. #5 + #7 = (((2 + 3)× 5)+ (23× 5)).

350
7 Combinations and Chances
Fig. 7.12 The bar diagram of the expression (((2+3)×5)+(23 ×5))
The diagram of Fig. 7.12 is another way of expressing the hierarchical structure of
the the expression (((2 + 3)× 5)+ (23× 5)).
Rooted trees, expressions (built with parentheses), branched diagrams, are equiv-
alent concepts, expressing some kind of hierarchy among components.
7.8.2
Types of Graphs
Edges of graphs can be oriented or not (oriented edges express arrows). The degree
of a node in a graph is the number of edges connected to the node (in the oriented
case it is possible to distinguish entering and exiting edges, consequently, in and
out degree). A graph is complete if every node is directly connected by an edge
to every other node. Figure 7.13 shows the graph K3,3, investigated by Kuratowski,
consisting of six nodes where vertices can be partitioned into two disjoint sets of
three nodes, and where every node of one set is connected to every node of the other
set. Figure 7.14 shows the complete graph K5 on ﬁve nodes.
We call hypergraph is a graph where nodes are graphs. This terminology is not
standard, because usually the term refers to a generalization of a graph in which
an edge can connect any number of vertices. Here, the term is used by analogy
with hyperset, hypersequence, and hypermultiset (that is, structures consisting of
Fig. 7.13 Kuratowski’s graph

7.8 Trees and Graphs
351
Fig. 7.14 The complete graph on ﬁve nodes
Fig. 7.15 A hypergraph and its representation with a graph with different types of nodes and
edges
components that are structure of the same kind). Fig. 7.15 shows an hypergraph
with two hyper-nodes and one hyper-edge and its representation as a graph. It is
easy to realize that the different levels of the ﬁrst structure are realized by different
types of nodes and edges in the second structure.

352
7 Combinations and Chances
7.8.3
Graph Planarity and Colorability
The theory of graphs started with Euler (1707–1793) who posed a problem inspired
from the river bridges in the city of K¨onisberg. The question was: “Is it possible
to cross all seven bridges of K¨onisberg in such a way that all of them are crossed
one and only one time?” In general terms, given a (non-oriented) graph we say that
a path along the nodes of the graph is Eulerian if every edge of the graph occurs
exactly once in the path. Euler found a sufﬁcient and necessary condition that a
graph has to fulﬁll for the existence of such a path. Namely, if every node has even
degree, that is, an even number of edges connected to it, then when we enter into a
node we can exit from it by passing through another edge. But if some node has an
odd degree, this manner of visiting a node cannot be realized, and necessarily some
edge has to be crossed more than once.
A graph is planar when it can be drawn in the plane in such a way that its
edges do not intersect. For example, it is easy to realize that the graph of Fig. 7.13,
consisting of two sets A and B having both three nodes and where each node of A
is connected, by one edge, to every node of B is not a planar graph. Any tree is a
planar graph.
A polyhedron is a convex region of space bounded by polygons (for every pair
of points in the region, every point on the straight line segment that joins them is
also within the region). A polyhedron is “equivalent” to a planar graph. The follow-
ing construction provides the graph G(P) naturally associated to a polyhedron P:
remove one face f from P (without removing its edges and vertices), then all ver-
tices and edges of P can be placed on the plane of f (the faces of P correspond to
minimal cycles (no subset of nodes of a minimal cycle is a cycle too). Now, consider
the portion of plane external to G(P) as a face too, then P and G(P) have the same
numbers of vertices, edges, and faces.
Euler proved that convex polyhedra satisfy a relation among the number V of its
vertices, the number E of its edges, and the number F of its faces. According to
the equivalence between polyhedra and planar graphs, this relation, also known as
Euler’s polyhedral formula, can be proved for connected planar graphs.
Fig. 7.16 Adding to a graph a new node and an edge (left) or only one edge (right)

7.8 Trees and Graphs
353
Theorem 7.13. In any connected (ﬁnite) planar graph the following equation relat-
ing the numbers of its vertices, edges, and faces holds:
V −E + F = 2
Proof. This formula trivially holds for the atomic connected graph, consisting of
only one node, where no edge is present, but one node and one face (the external
face) are present. Any (ﬁnite) planar connected graph can be constructed starting
from an atomic graph by applying, a certain number of times, two different pos-
sible operations (see Fig. 7.16): i) adding one node and one edge which connects
the new node to a node already in the graph, or ii) adding one edge which connects
two nodes already in the graph (which are connected by some path, but not already
directly connected by an edge). Of course, in both cases, the value V −E + F does
not change, because when we add one node we add also one edge, and when we add
only one edge we add also a face (because a new cycle is added to the graph). In con-
clusion, the value V −E +F is equal to 2 for the atomic graph and remains the same
after any of the two operations extending graphs. Therefore, the formula holds for
all connected planar graphs.
⊓⊔
The dual graph G∗of a given planar graph G has as nodes the faces of G, and
two nodes of G∗are connected by an edge if the corresponding faces of G share an
edge. An obvious property of planar graphs is that the dual graph of a planar graph
is planar too.
A simple relation concerning connected acyclic graphs (unrooted trees) is that
the number of vertices equals the number of edges plus 1. In fact, when a connected
acyclic graph is constructed, starting from the atomic graph, only the operation on
the left of Fig. 7.16 can be applied, because when a new node is added also a new
edge is added. Therefore, the node put at beginning of the construction is the only
one which does not introduce an edge. Hence, V = E + 1.
A regular polyhedron is a convex polyhedron bounded by a number of equal
regular polygons (convex regions of plane bounded by segments (edges) with equal
lengths and forming equal angles). A famous geometrical proposition asserts that
only ﬁve different kinds of regular polyhedra exist (hedron in Greek means face).
These polyhedra have been investigated by Plato (in the dialogue entitled Timaeus),
whereby they are referred to as the Platonic solids. The existence of only ﬁve Pla-
tonic solids can be proved as a consequence of Euler’s polyhedral formula (a proof,
based on elementary geometry, is present in Euclid’s books). We present this proof
as a paradigmatic example of an argument developed in purely mathematical dis-
crete terms, essentially based on the notion of planar graph.
Theorem 7.14 (Platonic solids). There are only ﬁve Platonic solids.
Proof. In a planar graph associated to regular polyhedron, two properties hold: the
degree g is the same for all nodes, and the number k of edges around a face is the
same for all faces (k > 2). These two facts imply the following equations:
gV = 2E
(7.28)

354
7 Combinations and Chances
2E = kF
(7.29)
In fact, gV counts edges twice because each edge is common to two nodes, and also
kF counts edges twice because each edge is common to two faces.
From Euler’s formula V −E +F = 2 we have F = E −V +2, therefore we have:
2E = k(E −V + 2)
that is:
(k −2)E = kV −2k
whence:
2(k −2)E = 2kV −4k
but from Eq. (7.28) we can replace 2E by gV, therefore:
(k −2)gV = 2kV −4k
and ﬁnally:
[2(k + g)−kg]V = 4k.
(7.30)
In order to have a positive value 4k, we need to have:
2k + 2g > kg
that is:
g <
2k
k −2
Fig. 7.17 Platonic solids

7.8 Trees and Graphs
355
This implies that k < 6. In fact, for k ≥6 the above inequality implies g < 3, but
in the graphs coming from regular polyhedra k and g are at least 3, In fact, a face
needs at least three edges (triangle) and a graph with all nodes with degree 2 is a
cycle (only one face).
For k = 3 Eq. (7.30) gives:
(6 −g)V = 12
therefore, g can assume only the values 3, 4, and 5, which correspond to the ﬁrst
three lines of following Table 7.9, where the values of F correspond to the number
of regular polygons of a polyhedron.
From Eq. (7.30), for k = 4 and k = 5 the only possible value of g is 3, and we
get the last two lines of Table 7.9. We remark that the tetrahedron is self-dual and
that the hexahedron is dual of the octahedron, while the dodecahedron is dual of the
Icosahedron. No other possibility is allowed.
⊓⊔
Table 7.9 The ﬁve Platonic solids
g=3
V = 4
F= 4 (3F = 3V=12)
k=3 Tetrahedron
g=4
V = 6
F = 8 (3F = 4V=24)
k=3 Octahedron
g=5
V = 12
F = 20 (3F = 5V=60)
k=3 Icosahedron
g=3
V = 8
F =6 (dual of Octahedron)
k=4 Hexahedron
g=3
V = 20
F = 12 (dual of Icosahedron)
k=5 Dodecahedron
It is interesting to consider the constructions of the two more complex Platonic
polyhedra, the dodecahedron and the icosahedron. For dodecahedron we can start
from a pentagon surrounded by other ﬁve pentagons (each of them shares an edge
with the central one). If the peripheral pentagons share two edges with their neigh-
bors, then they form a sort of cup, a 6-pentagon cup. By joining two 6-pentagon
cups, we get a dodecahedron.
For an icosahedron we consider a 5-triangle cup with a vertex common to all
triangles and a pentagon as basis. If at each edge of this pentagon a triangle is put
(equal to the triangles of the cup), then between two triangles put at consecutive
edges an equal triangle is comprised, sharing an edge with each of them (and the
vertex common to these edges). In this way 15 triangles are connected providing
a pentagonal border. Then, by adding to this border a reversed 5-triangle cup the
icosahedron is formed having 20 triangles. A planar representation of icosahedron
is given in Fig. 7.20. Platonic solids occur in natural forms, for example, in capsids
of virus. The capsid of Adenovirus, Reovirus, Papovavirus, and Picornavirus is an
icosahedron (the regular polyhedron more similar to a sphere, where the 12 vertexes
are equipped with extruding ﬁbers used to bind target cells).
From Eq. (7.30), it follows that no regular solid can be constructed with hexagons.
However, if 12 pentagons are mixed with hexagons, then convex polyhedra can
be constructed. The minimum of such polyhedron, having 20 hexagons and 12

356
7 Combinations and Chances
pentagons (60 vertices), is one of 13 Archimedes’ solids (realized by two types of
regular polygons). Its shape corresponds to the standard football sphere. In 1985, a
molecule consisting of 60 atoms of carbon, called Fullerene, was discovered which
corresponds exactly to Archimedes’ polyhedron of 60 vertices, 90 edges, and 32
faces, called truncated icosahedron, which is obtained by cutting all 12 vertices of
the icosahedron, and thus replacing them with pentagons as new faces (and con-
temporarily, doubling the edges of the other faces, which pass from triangles to
hexagons). Fullerene molecules and similar polyhedron molecules, with different
number of vertices, have very important chemical properties and the scientists who
discovered C60 (Robert F. Curl Jr., Sir Harold Kroto, Richard E. Smalley) won the
Nobel prize in 1996.
Colorability of graphs is another important subject of investigation. A graph is
k-colorable if it is possible to assign one of k different colors to each of its node, in
such a way that nodes with a common edge have different colors (if a graph is k-
colorable, then it is also k+1-colorable). An important result proves that any planar
graph is 4-colorable. Kenneth Appel and Wolfgang Haken proved this fact in 1976
by means of a computer program for analyzing a huge number of possibilities (1936
cases, and 633 cases in a more recent proof by other authors. Previous proofs, since
the end of 19th century, turned out to be wrong). The reader is advised to check that
the graph of Fig. 7.13 is 2-colorable, while the graph of Fig. 7.14 is not 4-colorable
(it is 5-colorable).
It was proved by Kuratowski in 1930 that a graph is planar if and only if it does
not “include” graph K3,3 of Fig. 7.13 or ﬁve-complete graph K5 of Figure 7.14. In
more formal terms, here the forbidden inclusion means that the given graph does
not include any expansion of K3,3 or K5, where an expansion of a graph is realized
by replacing in it an edge between two nodes with a path between them. Fig. 7.18
shows that non-planar graphs (3D, or three-dimensional) strictly include 4-colorable
graphs (4C), which strictly include planar graphs (2D, or 2-dimensional).
Fig. 7.18 Inclusions among graph planarity, 4-colorability, and non-planarity
We conclude the section by noting that the notion of duality of graphs is a case of
a general phenomenon of mathematics, which occurs in many forms and provides
very often an important key of comprehension of basic concepts. The simplest ex-
ample of duality is between elements and classes. A class contains elements, but an
element can be characterized by the classes to which it belongs. Analogously, a text

7.8 Trees and Graphs
357
Fig. 7.19 A 4-coloration
Fig. 7.20 A planar representation of an icosahedron
consists of words which determine its meaning, but conversely the semantic function
of a word is determined by the class of texts where it occurs. More technical exam-
ples of duality can be found in linear algebra (vectors and linear transformations)
and in geometry (points and lines), but cases of duality can be found in biochemistry
between metabolic states and reactions (both representable as suitable vectors), or
in cell biology where genes and proteins can be considered as dual entities playing
the roles of informational and functional biomolecules. The common aspect of all
forms of duality is the intrinsic necessity of concepts which are paired, and which
can be fully described and analyzed only when they are related, because each of
them postulates the other one in order to be completely deﬁned, as it happens in the
bilateral symmetry of organisms or in the double-stranded DNA molecules.

358
7 Combinations and Chances
7.8.4
Pointers and Graphs
The notion of pointer (address, link, according to the contexts) allows the deﬁnition
of another mechanism of representation of (ﬁnite) graphs by means of sequences,
usually called hypertexts. A crucial aspect of hypertexts is the presence of two sym-
bolic levels: i) the basic level consisting of sequences over a basic alphabet, and ii)
a structural level, where some special symbols or strings, also called metasymbols,
express structural relations (parentheses are structural symbols expressing symbol
aggregations).
A pointer is a value denoting an address, in an addressed space consisting of
locations. Each location contains a value (datum), belonging to a set of possible
values. Moreover, each location can be partitioned into several ﬁelds containing
different kinds of values. The example given in the right part of Fig. 7.21 shows
the main intuition of this idea. An arrow denotes that the content of the location
corresponds to the address of the location from where the arrow originates. In other
words, arrows visualize pointers. This mechanism can be realized, because locations
can also contain addresses of locations. A structure of this type can be completely
determined by the address of the ﬁrst location (in the ﬁgure, the address of location
containing value a). This kind of representation can be modiﬁed and extended in
many ways and corresponds to the way graphs are stored in computer memories (the
term linked list is often used for these structures). Of course, in correspondence to
the different kinds of memories and methods for accessing to the locations, many
important concepts and algorithms have been developed, which are speciﬁc topics
of computer data structures.
7.8.5
Parentheses and Tags
Mathematical expressions are usually built by speciﬁc symbols, variables, and
parentheses. Parentheses are symbols occurring in pairs: opening and closing paren-
thesis. Any open parenthesis has to be paired to a corresponding closed parenthesis.
?
6
u
u
-
z
-
u
u
u
?
?
u
u
a
b
c
d
e
e
d
c
b
a



=
@
@
I
?
6
@
@@
R
u
u
u
u
-
-
u
Fig. 7.21 Pointer representation of a graph

7.8 Trees and Graphs
359
Table 7.10 Formula for solving the second degree equation ax2 +bx+c = 0
x = −b±
√
b2−4ac
2a
In mathematical expressions, two, or three, or sometime more types of parentheses
are used. Let us consider the formula of Table 7.10 giving the solutions of a second
degree equation.
The same formula can be represented by the Latex text [176], reported in Table
7.11. This language developed by Leslie Lamport, is based on the language TEX,
which was introduced by Donald Knuth in 1978. It was essentially a formalism pro-
viding abstract formulae expressing structures of scientiﬁc texts and of scientiﬁc
formulae. In this context, the attribute “abstract” means that such formulae are built
by textual characters (including some special characters), but they intend to repre-
sent the logical structure of documents and of mathematical formulae, and leave to
different levels the speciﬁc stylistic choices of a concrete syntactical realization (see
[42, 41] for abstract syntax representations of linguistic texts).
Table 7.11 Latex text which produces the formula of Table 7.10
$$x = \frac{−b \pm \sqrt{b∧2−4ac}}{2a}$$
One of the basic ideas of TEX is the use of a textual formalism for expressing
parentheses and operation symbols. This idea, developed by many people, in dif-
ferent contexts of programming language formats, led to the deﬁnition of Mark-up
languages, based on the general notion of tag. Tags are a generalization of paren-
thesized expressions. An open marker is a symbol put between two angles, and it
uniquely corresponds to its closing marker, according the following structure:
< marker >
content
< /marker > .
Mark-up languages are the basis of data transfer protocols (for example, HTML,
XML), where information at different levels is enclosed in different types of paren-
thesized expression (XML stands for “eXtensible Markup Language”). When a no-
tion of link is added to tags, by means of keys, which univocally identify tag occur-
rences, then any graph can be represented by a suitable tag expression. In fact, let us,
for example, insert inside the left marker an additional information, after the special
symbol #, which denotes a key of the tag. In this manner, we can easily express the
pointer structure of Fig. 7.13 in the XML style of Table 7.13.
Trees are special graphs, sequences are special trees (where any node different
from a leaf has only one son), and a multiset is a special sequence (the sequence of
multiplicities, with respect to an ordering of the elements). Moreover, membranes
which realize hypermultisets, that is, iterated multiset aggregations, are naturally

360
7 Combinations and Chances
Table 7.12 The pointer structure of Fig. 7.21 in XML format
< list >
< element #p1 >
< content >a< /content >
< pointer −1 >p2< /pointer >
< pointer −2 >p3< /pointer >
< /element >
< element #p2 >
< content >b< /content >
< /element >
< element #p3 >
< content >d< /content >
< pointer −1 >p5< /pointer >
< pointer −2 >p2< /pointer >
< pointer −3 >p4< /pointer >
< /element >
< element #p4 >
< content >c< /content >
< pointer −1 >p2< /pointer >
< pointer −2 >p5< /pointer >
< /element >
< element #p5 >
< content >e< /content >
< /element >
</list>
represented by expressions of parentheses. But if we extend parentheses with tags
possibly including links (or keys), then we can easily represent graphs.
Parentheses were introduced in mathematical notation relatively late (Girard,
17th century). Before them, other methods for aggregating symbols were based on
vincula and dots. Vinculum (used by Vi`ete, the same mathematician who introduced
letters in algebraic manipulations, in 15th century) is a line over/below the symbols
it connects. Table 7.10 expresses the resolution formula of second degree equations
in terms of vincula notations (for root square and fraction).
A dotted notation was systematically used by the Italian mathematician Giuseppe
Peano in his Formulario Mathematico (1908), and this notation was also inherited
by Bertrand Russell and Alfred Whitehead in their Principia Mathematica. When
dots are put next to operation symbols, they can denote a priority of their application
(a smaller number of dots means a greater priority). In speciﬁc contexts, operation

7.8 Trees and Graphs
361
Table 7.13 A 4-node complete graph expressed in XML format
< graph >
< node #1 >
< edges > 2, 3, 4 < /edges >
< /node >
< node #2 >
< edges > 1, 3, 4 < /edges >
< /node >
< node #3 >
< edges > 1, 2, 4 < /edges >
< /node >
< node #4 >
< edges > 1, 2, 3 < /edges >
< /node >
< /graph >
symbols have an implicit priority, therefore some dots can be avoided (for example,
multiplication and division are applied before sum and subtraction).
Dotted notation can be generalized as an alternative device with respect to paren-
theses (or used jointly with them). In general, dots can be used for breaking a
sequence into pieces. Firstly, the pieces without dots are aggregated, then pieces
between one dot are aggregated, and so on. In other words, dots indicate the order
of aggregation of components in a sequence. This provides a linear representation
of hypermultisets, with dots, that is, extra symbols which can be iterated any num-
ber of times. For example, the sequence below is the dot representation of mem-
brane/parenthesis structure: (((a,c)(a,c)(a,c)),(c,c),((a,a,b)(a,a,b)(a,a,b))).
(ac• ac• ac• • cc• • aab • aab • aab).
In the expression below:
(3 + 5)× ((12 × 10)+(7×13))
parentheses are removed by putting near to the operation symbols a number of dots
representing their application priority (fewer dots for greater priorities). Its dotted
representation is the following:
3 + 5 ×•• 12 × 10 +• 7 × 13.

362
7 Combinations and Chances
In considering discrete structures, we discussed very often the passage from a rep-
resentation to another representation of the same structure. This is a crucial point
in their mathematical and computational analyses. A good representation has to
preserve the information. This means that it has to be possible to pass from a rep-
resentation of an object to another one of the same object and vice versa. However,
the equivalence of information content does not mean that two representations are
completely equivalent. In fact, very often, the information can be processed in a
better manner when an object is represented in one format rather than in another.
A typical example of this phenomenon is provided by the elementary arithmetical
algorithms, which can be easily deﬁned within the positional representation of num-
bers, but become sometimes very difﬁcult when numbers are represented in terms
of geometrical entities. In general, the computational evaluation of a representation
is an important issue in the algorithmic analysis of data structures and depends on
aspects related to the ﬁnality of their speciﬁc context of use. For example, the ad-
vantage of tag representations is given by the possibility of expressing, in textual
formats, complex information with graphical, and even multi-medial, features by
using only symbols of the standard 7-bit ASCII alphabet of 128 characters (Ameri-
can Standard Code for Information Interchange).
7.8.6
Tree Enumerations
Tree enumeration formulae are an old subject ﬁrst investigated in connection to
chemical structures. In Knuth’s book [224] many classical results are reported for
different kinds of trees. Different notions of trees require different enumeration
methods. We recall brieﬂy the aspects which are crucial in this regard. Trees can
be rooted or unrooted, ordered or unordered, or labeled or unlabeled. An unrooted
tree is a connected acyclic graph, while a rooted tree is an unrooted tree where a
node is chosen as an ancestor of all nodes of the tree. This means that a genealogi-
cal path with respect to the root can be assigned to every node of the tree. In ordered
trees, a generation order is established among the sons of a node, in unordered trees
no order is given among the sons of a node. In labeled trees all nodes are distinct,
while in unlabeled trees nodes are undistinguishable.
A forest is a sequence of trees (a multiset of trees, if order is not relevant). Of
course, there is a one-to-one correspondence between forests on n nodes and rooted
trees of n + 1 nodes. In fact, any tree becomes a forest after removing its root, and
conversely any forest become a tree if we add a node as father of the roots of the trees
in the forest. Forests of unlabeled ordered rooted trees on n nodes correspond to the
parenthesized expressions of n parenthesis pairs enumerated by Catalan numbers.
From this enumeration, the number (2n −2)!/(n −1)! of labeled ordered rooted
trees of n nodes easily follows.
For unlabeled unordered rooted trees no exact analytical formula is available,
but a recurrent formula, obtained by means of generating functions, was given by

7.8 Trees and Graphs
363
Otter in the middle of last century, together with a complex asymptotic approximate
formula [232]. Recurrent formulae, deduced by means of elementary combinato-
rial arguments, but computationally less efﬁcient than Otter’s formula, are given in
[226, 227].
The number of labeled unordered rooted trees of n nodes is nn−1. This formula
was discovered by Cayley. Here we present a proof of this result, which is based on
an interesting encoding of these trees as sequences of length n −1 constructed over
the n nodes, due to Pr¨ufer [231], as reported in [224].
Encoding trees into sequences.
Let T be a labeled rooted tree of n nodes numbered by 1,2,...n. The following
procedure provides a sequence encoding T. For i = 1,...n−1, take the leaf x
of T having the minimum index; set xi = father(x) and update T, by deleting
from T the node x and its edge. The sequence (x1,x2,...,xn−1) is the encoding
of T.
Fig. 7.22 Representation of a tree of n nodes by means of sequence of length n−1 over the
nodes
We can reverse the encoding process of trees given above. In fact, any sequence
of length n−1 over n nodes uniquely individuates a labeled rooted tree over n nodes.
The following procedure allows us to reconstruct the tree of n nodes encoded by a
sequence of n −1 nodes.

364
7 Combinations and Chances
Decoding sequences into trees
Given a sequence α over a set X of n nodes numbered by 1,2,...n, the fol-
lowing procedure provides another sequence β of nodes over X. Let j be the
smallest index of the nodes of X which does not occur in α, then put β = (xj)
and update α, by removing its ﬁrst element.
For i = 2,...,n −1 do:
let j be the smallest index of the nodes which do not occur in α and do not
already occur in β, then update β by adding to it xj as last element, and update
α by removing its ﬁrst element.
done.
The set of pairs of elements having the same positions in α and β, respec-
tively, provide the edges of the tree encoded by the sequence α.
Figure 7.23 shows a tree and the sequence (1,4,4,3,5,5,3,1) encoding the tree.
The sequence (2,6,7,4,8,9,5,3) is generated by the above procedure applied to
(1,4,4,3,5,5,3,1). In fact, the pairs of elements having the same positions in
(1,4,4,3,5,5,3,1) and (2,6,7,4,8,9,5,3) provide the edges of the tree encoded
by (1,4,4,3,5,5,3,1).
Fig. 7.23 The tree over nodes {1,2,3,4,5,6,7,8,9} is encoded by the sequence
(1,4,4,3,5,5,3,1). This sequence with the sequence (2,6,7,4,8,9,5,3) returns the tree
Cayley’s formula easily follows from the previous encoding. In fact, given n
nodes, there are nn−1 distinct sequences of length n −1. It could be considered
counterintuitive that a two-dimensional structure as a tree could be encoded by a se-
quence. However, at a more careful analysis, we realize that the encoding sequence
of a tree is based on a previous ordering of its nodes, therefore, in more precise

7.8 Trees and Graphs
365
terms, the sequence encoding of trees over n nodes is given by two sequences: the
sequence ordering the nodes, and another sequence, of length n−1, over the nodes.
7.8.7
Binary Trees
A natural correspondence can be easily established between any forest and a binary
tree (each node which is not a leaf has at most two sons). The correspondence is
depicted by Fig. 7.24. The idea is very simple: the roots of the forest are linked in
the order of the corresponding trees of the forest; moreover, every father is linked
only to the ﬁrst son and every son to the following son. The root of the ﬁrst tree of
the forest is the root of the resulting tree. In this way, any forest provides a binary
tree.
Fig. 7.24 Representation of a forest of trees by means of a binary tree
7.8.8
Phylogenetic Trees
Given n organisms how many different phylogenetic trees can be built from them?
Let us assume that we are interested only to the phylogenetic branching processes.
A binary tree is complete if any node different from the root has exactly two sons (a
tree is k-ary if any node different from the root has at most k sons, and it is complete
if any node different from the root has exactly k sons).
The problem of (branching) phylogenetic reconstruction reduces to the enumer-
ation of the set BT(n) of rooted complete binary trees on n leaves [219].

366
7 Combinations and Chances
Firstly, we recall the following proposition, which can be easily shown by induc-
tion on the number of nodes.
Proposition 7.15. In a complete binary tree with n leaves there are n −1 internal
nodes (that is, 2n −1 nodes).
The following proposition is an easy consequence of the the previous one, because
any tree has a number edges equal to the number of its nodes minus one.
Proposition 7.16. In a complete binary tree with n leaves there are 2n −2 edges.
A Unrooted Branches (UB) is a unrooted trees where any internal node has degree
3. Given a BT, if we remove its root, by collapsing the two edges exiting from it in
one edge, we get a UB (see 7.25). Therefore, the following proposition holds.
Proposition 7.17. Any BT can be transformed into a UB with the same number of
leaves and a number of edges decreased by one.
In order to obtain the enumeration of the set BT(n) of BT with n leaves, we consider
the set UB(n) of UB with n leaves. For simplicity we continue to denote by BT(n)
and UB(n) the cardinalities of these sets, respectively.
Fig. 7.25 Transforming a binary tree (top) in an unrooted branch (bottom) with the same
leaves
Proposition 7.18. The number BT(n) of rooted complete binary trees with n leaves
is given by (2n −3)UB(n) where UB(n) is the number of unrooted branchings with
n leaves.

7.8 Trees and Graphs
367
Fig. 7.26 Increasing UB leaves. Top: a minimal UB; Middle: an UB where the left leaf edge
of the top UB is replaced by a minimal UB; Bottom: an UB where the internal edge of the
middle UB is replaced by a minimal UB
Proof. From the previous propositions it follows that any BT with n leaves has
(2n −2) nodes and (2n −3) edges. Moreover, from each UB a BT, with the same
number of leaves can be obtained by de-collapsing one of its edges.
⊓⊔
Proposition 7.19. The number of rooted binary trees of n leaves is
BT(n) = (2n −3)!!
Proof. The minimal UB has one internal node connected with 3 edges to 3 leaves,
therefore UB(3) = 1. We pass from a UB to another UB having a number of leaves
increased by one, by replacing one edge with a minimal UB (see, Fig. 7.26). For
example, UB(4) = 3UB(3), and UB(5) = 5UB(4), because an UB with 3 leaves has
3 edges, and an UB with 4 leaves has 5 edges. From Propositions 7.16 and 7.17, we
know that a UB of n −1 leaves has 2(n −1)−3 edges.

368
7 Combinations and Chances
Therefore, the following equation holds (n > 3):
UB(n) = UB(n −1)(2(n −1)−3)
whence, for n > 3:
UB(n) = 3 ·5 ·...·(2(n −1)−3) = 3 ·5 ·...·(2n −5).
Thus, the number UB(n) given above, for n > 2, can be concisely expressed by
(denoting n ·(n −2)·(n −4)...·3 by n!!):
UB(n) = (2n −5)!!
Therefore, from Proposition 7.18, we get BT(n) = (2n −3)!!, which concludes the
proof.
⊓⊔

References
References for Chapter 1
1. Alberts, B., et al.: Molecular Biology of the Cell. Garland Science, New York (1994)
2. von Baeyer, H.C.: Information: The New Language of Science. Harvard University
Press (2004)
3. Ball, P.: The Elements: A Very Short Introduction. Oxford University Press (2002)
4. Ball, P.: Molecules: A Very Short Introduction. Oxford University Press (2003)
5. Davis, P.: The Origin of Life. Penguin Books (1999)
6. de Duve, C.: Life Evolving. Molecules, Mind, and Meaning. The Christian Ren´e de
Duve Trust (2002)
7. Doolittle, W.F.: Uprooting the tree of life. Scientiﬁc American 6, 90–95 (2000)
8. Hoppﬁeld, J.J.: Neural networks and physical systems with emergent collective com-
putational abilities. Proc. Nat. Acad. Sci. USA, Biophysics 79, 2554–2558 (1982)
9. McCulloch, W., Pitts, W.: A Logical Calculus of Ideas Immanent in Nervous Activity.
Bulletin of Mathematical Biophysics 5, 115–133 (1943)
10. Schr¨odinger, E.: What is Life? The physical Aspect of the Living Cell. Cambridge
University Press (1944)
References for Chapter 2
11. Adleman, L.: Molecular computation of solutions to combinatorial problems 286,
1021–1024 (1994)
12. Adleman, L.: Computing with DNA. Scientiﬁc American 279(2), 54–61 (1998)
13. Amos, M.: Theoretical and Experimental DNA Computation. Springer (2005)
14. Bath, J., Turberﬁeld, A.J.: DNA nanomachines. Nature Nanotechnology 2(5), 275–284
(2007)
15. Benenson, K., Paz-Elitzur, T., Adar, R., Keinan, E., Livneh, Z., Shapiro, E.: Pro-
grammable and autonomous computing machine made of biomolecules. Nature 414,
430–434 (2001)
16. Baker, M.C.: The atoms of language. Basic Books (2001)
17. Braich, R.S., Johnson, C., Rothemund, P.W.K., Hwang, D., Chelyapov, N., Adleman,
L.M.: Solution of a Satisﬁability Problem on a Gel-Based DNA Computer. In: Condon,
A., Rozenberg, G. (eds.) DNA 2000. LNCS, vol. 2054, pp. 27–42. Springer, Heidelberg
(2001)
18. Braich, R.S., Chelyapov, N., Johnson, C., Rothemund, P.W.K., Adleman, L.: Solution
of a 20-variable 3-SAT problem on a DNA computer. Science 296, 417–604 (2002)

370
References
19. Brendel, V., Busse, H.G.: Genome structure described by formal languages. Nucleic
Acids Research 12(5), 2561–2568 (1984)
20. Castellini, A., Franco, G., Manca, V.: A dictionary based informational genome analy-
sis. BMC Genomics 13, 485 (2012)
21. Cristianini, N., Hahn, M.W.: Computational genomics. Cambridge University Press
(2007)
22. Deonier, R.C., Tavar´e, S., Waterman, M.S.: Computational Genome Analysis: An In-
troduction. Springer Science + Business Media, Inc. (2005)
23. The Encode Project Consortium: An integrated encyclopedia of DNA elements in the
human genome. Nature 489, 57–72 (2012)
24. Fici, G., Mignosi, F., Restivo, A., Sciortino, M.: Word Assembly through minimal for-
bidden words. Theoretical Computer Science 359, 214–230 (2006)
25. Fofanov, Y., et al.: How independent are the appearances of n-mers in different
genomes? Bioinformatics 20(15), 2421–2428 (2008)
26. Franco, G.: Biomolecular computing: combinatorial algorithms and laboratory experi-
ments. PhD Thesis, University of Verona (2006)
27. Franco, G.: Perspectives in computational genome analysis. Discrete and topological
models in molecular biology. Springer (2013)
28. Franco, G., Giagulli, C., Laudanna, C., Manca, V.: DNA Extraction by XPCR. In: Fer-
retti, C., Mauri, G., Zandron, C. (eds.) DNA 2004. LNCS, vol. 3384, pp. 104–112.
Springer, Heidelberg (2005)
29. Franco, G., Manca, V.: Algorithmic applications of XPCR. Natural Computing 10, 805–
819 (2011)
30. Franco, G., Manca, V., Giagulli, C., Laudanna, C.: DNA Recombination by XPCR. In:
Carbone, A., Pierce, N.A. (eds.) DNA 2005. LNCS, vol. 3892, pp. 55–66. Springer,
Heidelberg (2006)
31. Giancarlo, R., Scaturro, D., Utro, F.: Textual Data Compression in Computational Bi-
ology: A synopsis. Bioinformatics 25, 1575–1586 (2009)
32. Gibson, G., Muse, S.V.: A Primer of Genome Science. Sinauer Associates Inc. (2009)
33. Gimona, M.: Protein Linguistics — a grammar for modular protein assembly? Nature 7,
68–73 (2006)
34. Hampikian, G., Andersen, T.: Absent sequences: nullomers and primes. In: Pac. Symp.
Biocomputing, vol. 12, pp. 355–366 (2007)
35. Hao, B., Qi, J.: Prokaryote phylogeny without sequence alignment: from avoidance
signature to composition distance. J. Bioinf. and Comput. Biol. 2, 1–19 (2004)
36. Haubold, B., Pierstorff, N., M¨oller, F., Wiehe, T.: Genome comparison without align-
ment using shortest unique substrings. BCM Bioinf. 6, 123 (2005)
37. Head, T.: Formal language Theory and DNA: an analysis of the generative capacity
of speciﬁc recombinant behaviors. Bulletin of Mathematical Biology 9(6), 737–759
(1987)
38. Lindenmayer, A.: Mathematical models for cellular interaction in development. J. The-
oret. Biology 18, 280–315 (1968)
39. Lipton, R.J.: DNA solutions of hard computational problems. Science 268, 542–544
(1995)
40. Lombardo, R.: Unconventional computations and genome representations. PhD Thesis,
University of Verona (2013)
41. Manca, V., Jim´enez-L´opez, M.D.: GNS: Abstract Syntax for Natural Languages. Lan-
guages: mathematical approaches. Triangle 8, 55–80 (2012); Jim´enez-L´opez M.D. (ed.)
42. Manca, V., Suzuki, J., Suzuki, Y.: An XML Representation of Basic Japanese Gram-
mar. In: Bel-Enguix, G., Jim´enez-L´opez, M.D. (eds.) Language as a Complex System:
Interdisciplinary Approaches, pp. 215–244. Cambridge Scholars Publishing (2010)

References
371
43. Manca, V.: A Proof of Regularity for Finite Splicing. In: Jonoska, N., P˘aun, G., Rozen-
berg, G. (eds.) Aspects of Molecular Computing. LNCS, vol. 2950, pp. 309–317.
Springer, Heidelberg (2003)
44. Manca, V.: On the Logic and Geometry of Bilinear Forms. Fundamenta Informati-
cae 64, 261–273 (2005)
45. Manca, V., Franco, G.: Computing by polymerase chain reaction. Mathematical Bio-
sciences 211, 282–298 (2008)
46. Manca, V., Zandron, C.: A Clause String DNA Algorithm for SAT. In: Jonoska, N.,
Seeman, N.C. (eds.) DNA 2001. LNCS, vol. 2340, pp. 172–181. Springer, Heidelberg
(2002)
47. P˘aun, G., Rozenberg, G., Salomaa, A. (eds.): DNA Computing. Springer (2000)
48. Rozenberg, G., Salomaa, A.: Lindenmayer Systems: Impacts on Theoretical Computer
Science, Computer Graphics, and Developmental Biology. Spinger (1992)
49. P˘aun, G.: Computing with Membranes. Journal of Computer and Systems Sci-
ence 61(1), 108–143 (2000)
50. P˘aun, G.: Membrane Computing. An Introduction. Springer (2002)
51. P˘aun, G., Rozenberg, G., Salomaa, A.: Oxford Handbook of Membrane Computing.
Oxford University Press (2010)
52. Percus, J.K.: Mathematics of Genome Analysis. Cambridge University Press (2002)
53. Prusinkiewicz, P., Lindenmayer, A., Hanan, J.S., Fracchia, F.D., Fowler, D., de Boer,
M.J.M., Mercer, L.: The Algorithmic Beauty of Plants. Springer (1990)
54. Puglisi, A., Baronchelli, A., Loreto, V.: Cultural route to the emergence of linguistic
categories. PNAS 105(23), 7936–7940 (2008)
55. Rose, J.A., Hagiya, M., Deaton, R.J., Suyama, A.: A DNA-based in vitro genetic pro-
gram. Journal of Biological Physics 28, 493–498 (2002)
56. Saiki, R.K., Scharf, S., Faloona, F., Mullis, K.B., Horn, G.T., Erlich, H.A., Arnheim, N.:
Enzymatic Ampliﬁcation of β-Globin Genomic Sequences and Restriction Site Analy-
sis for Diagnosis of Sickle Cell Anemia. Science 230, 1350–1354 (1985)
57. Searls, D.B.: String variable grammar: A logic grammar formalism for the biological
language of DNA. Journal of Logic Programming 24, 73–102 (1995)
58. Park, S.H., Yan, H., Reif, J.H., LaBean, T.H., Finkelstein, G.: Electronic Nanostructures
Templated on Self-Assembled DNA Scaffolds. Nanotechnology 15, S525–S527 (2004)
59. Searls, D.B.: The language of genes. Nature 420, 211–217 (2002)
60. Searls, D.B.: Molecules, Languages and Automata. In: Sempere, J.M., Garc´ıa, P. (eds.)
ICGI 2010. LNCS (LNAI), vol. 6339, pp. 5–10. Springer, Heidelberg (2010)
61. Seeman, N.C.: DNA in a material worldblocking. Nature 421, 427–431 (2003)
62. Vinga, S., Almeida, J.: Alignment-free sequence comparison - a review. Bioinfor-
matic 19(4), 513–523 (2003)
63. Watson, J.D., Crick, F.H.G.: Molecular structure of nucleic acids: a structure for de-
oxiribose nucleic acid. Nature 171, 737–738 (1953)
64. Witten, I.H., Moffat, A., Bell, T.C.: Managing gigabytes: compressing and indexing
documents and images, 2nd edn. Academic Press (1999)
References for Chapter 3
65. Bernardini, F., Manca, V.: Dynamical aspects of P systems. Biosystems 70(2), 85–93
(2003)
66. von Bertalanffy, L.: General Systems Theory: Foundations, Developments, Applica-
tions. George Braziller Inc., New York (1967)

372
References
67. Bianco, L.: Membrane models of biological systems. PhD Thesis, University of Verona
(2007)
68. Bianco, L., Fontana, F., Franco, G., Manca, V.: P systems for biological dynamics. In:
[74] (2006)
69. Bianco, L., Manca, V., Marchetti, M., Petterlini, M.: Psim: a simulator for biochem-
ical dynamics based on P systems. In: CEC 2007 - IEEE Congress on Evolutionary
Computation, Singapore, September 25-28 (2007)
70. Castellini, A., Zucchelli, M., Busato, M., Manca, V.: From time series to biological
network regulations. Molecular Biosystems (2012), doi:10.1039/C2MB25191D
71. Castellini, A.: Algorithms and software for biological MP modeling by statistical opti-
mization techniques. PhD Thesis, University of Verona (2010)
72. Castellini, A., Franco, G., Manca, V.: Hybrid Functional Petri Nets as MP systems.
Natural Computing 9, 61–81 (2010)
73. Castellini, A., Franco, G., Pagliarini, R.: Data Analysis pipeline from laboratory to MP
models. Natural Computing 10, 55–76 (2011)
74. Ciobanu, G., P˘aun, G., Perez-Jimenez, M.J.(eds.): Applications of Membrane Comput-
ing. Springer (2006)
75. Condon, A., Harel, D., Kok, J.N., Salomaa, A., Winfree, E. (eds.): Algorithmic Biopro-
cesses. Springer (2009)
76. Draper, N., Smith, H.: Applied Regression Analysis, 2nd edn. John Wiley & Sons, New
York (1981)
77. Fontana, F., Manca, V.: Discrete solutions to differential equations by metabolic P sys-
tems. Theoretical Computer Science 372, 165–182 (2007)
78. Fontana, F., Manca, V.: Predator-prey dynamics in P systems ruled by metabolic algo-
rithm. BioSystems 91, 545–557 (2008)
79. Gheorghe, M., Manca, V., Romero-Campero, F.J.: Deterministic and stochastic P sys-
tems for modeling cellular processes. Natural Computing 9(2), 457–473 (2010)
80. Gillespie, D.T.: A general method for numerically simulating the stochastic time evo-
lution of coupled chemical reactions. J. Comp. Phys. 22, 403–434 (1976)
81. Goldbeter, A.: A minimal cascade model for the mitotic oscillator involving cyclin and
cdc2 kinase. PNAS 88(20)
82. Goldbeter, A.: Biochemical Oscillations and Cellular Rhythms: The molecular bases of
periodic and chaotic behaviour. Cambridge University Press (1996)
83. Goldbeter, A.: Computational approaches to cellular rhythms. Nature 420, 238–245
(2002)
84. Hilborn, R.C.: Chaos and Nonlinear Dynamics, 2nd edn. Oxford University Press
(2000)
85. Hocking, R.R.: The Analysis and Selection of Variables in Linear Regression. Biomet-
rics 32 (1976)
86. Johnson, S.C.: Hierarchical Clustering Schemes. Psychometrika 2, 241–254 (1967)
87. Jain, A.K.: Fundamentals of Digital Image Processing. Prentice Hall (1989)
88. Lambert, J.D.: Computational Methods in Ordinary Differential Equations. Wiley &
Sons (1973)
89. Lotka, A.J.: Analytical Note on Certain Rhythmic Relations in Organic Systems. Proc.
Natl. Acad. Sci. U.S. 6, 410–415 (1920)
90. Manca, V.: String rewriting and metabolism. A logical perspective. In: [111], pp. 36–60
(1998)
91. Manca, V., Bianco, L.: Biological networks in metabolic P systems. BioSystems 91,
489–498 (2008)

References
373
92. Manca, V.: The metabolic algorithm: principles and applications. Theoretical Computer
Science 404, 142–157 (2008)
93. Manca, V.: Log-Gain Principles for Metabolic P Systems. In: [75], ch. 28, pp. 585–605.
Springer (2009)
94. Manca, V.: Fundamentals of Metabolic P Systems. In: [51], ch. 19 (2010)
95. Manca, V.: Metabolic P Dynamics. In: [51], ch. 20 (2010)
96. Manca, V.: From P to MP Systems. In: P˘aun, G., P´erez-Jim´enez, M.J., Riscos-N´u˜nez,
A., Rozenberg, G., Salomaa, A. (eds.) WMC 2009. LNCS, vol. 5957, pp. 74–94.
Springer, Heidelberg (2010)
97. Manca, V.: Metabolic P systems. Scholarpedia 5(3), 9273 (2010)
98. Manca, V., Bianco, L., Fontana, F.: Evolution and Oscillation in P Systems: Appli-
cations to Biological Phenomena. In: Mauri, G., P˘aun, G., Jes´us P´erez-J´ımenez, M.,
Rozenberg, G., Salomaa, A. (eds.) WMC 2004. LNCS, vol. 3365, pp. 63–84. Springer,
Heidelberg (2005)
99. Manca, V., Lombardo, R.: Computing with Multi-membranes. In: Gheorghe, M., P˘aun,
G., Rozenberg, G., Salomaa, A., Verlan, S. (eds.) CMC 2011. LNCS, vol. 7184, pp.
282–299. Springer, Heidelberg (2012)
100. Manca, V., Marchetti, L.: Metabolic approximation of real periodical functions. The
Journal of Logic and Algebraic Programming 79, 363–373 (2010)
101. Manca, V., Marchetti, L.: Goldbeter’s Mitotic Oscillator Entirely Modeled by MP Sys-
tems. In: Gheorghe, M., Hinze, T., P˘aun, G., Rozenberg, G., Salomaa, A. (eds.) CMC
2010. LNCS, vol. 6501, pp. 273–284. Springer, Heidelberg (2010)
102. Manca, V., Marchetti, L.: Log-Gain Stoichiometic Stepwise regression for MP systems.
Int. J. of Foundations of Computer Science 22(1), 97–106 (2011)
103. Manca, V., Marchetti, L.: Solving inverse dynamics problems by means of MP systems.
BioSystems 109, 78–86 (2012)
104. Manca,
V.,
Marchetti,
L.:
An
algebraic
formulation
of
inverse
problems
in
MP
dynamics.
International
Journal
of
Computer
Mathematics
(2012),
doi:10.1080/00207160.2012.735362
105. Manca, V., Marchetti, L., Pagliarini, R.: MP modelling of glucose-insulin interactions
in the Intravenous Glucose Tolerance Test. International Journal of Natural Computing
Research 2(3), 13–24 (2011)
106. Manca, V., Martino, M.D.: From String Rewriting to Logical Metabolic Systems. In:
P˘aun, G., Salomaa, A. (eds.) Grammatical Models of Multiagent Systems, pp. 297–
315. Gordon and Breach (1999)
107. Manca, V., Pagliarini, R., Zorzan, S.: A photosynthetic process modelled by a metabolic
P system. Natural Computing 8, 847–864 (2009)
108. Marchetti, L.: MP representations of biological structures and dynamics. PhD Thesis,
University of Verona (2012)
109. Marchetti, L., Manca, V.: A Methodology Based on MP Theory for Gene Expression
Analysis. In: Gheorghe, M., P˘aun, G., Rozenberg, G., Salomaa, A., Verlan, S. (eds.)
CMC 2011. LNCS, vol. 7184, pp. 300–313. Springer, Heidelberg (2012)
110. Pagliarini, R.: Modeling and reverse engineering of biological phenomena by means of
metabolic P systems. PhD Thesis, University of Verona (2011)
111. P˘aun, G. (ed.): Computing with biomolecules. Theory and Experiments, pp. 36–60.
Spinger, Singapore (1998)
112. Prigogine, I., Lefever, R.: Symmetry Breaking Instabilities in Dissipative Systems II. J.
Chem. Phys. 48, 1695–1700 (1968)
113. Reising, W., Rozenberg, G. (eds.): Lectures on Petri Nets. Springer (1998)

374
References
114. Szallasi, Z., Stelling, J., Periwal, V. (eds.): System Modeling in Cellular Biology Lec-
tures on Petri Nets. The MIT Press (2006)
115. Voit, E.O.: Computational Analysis of Biochemical Systems. Cambridge University
Press (2000)
116. Volterra, V.: Fluctuations in the abundance of a species considered mathematically. Na-
ture 118, 558–560 (1926)
References for Chapter 4
117. Ashby, W.R.: An Introduction to Cybernetics. Chapman & hall Ltd., London (1956)
118. Barbieri, M.: The organic codes: The birth of semantic biology. peQuod (2001)
119. Hamilton, A.J., Baulcombe, D.: A Species of Small Antisense RNA in Post-
transcriptional Gene Silencing in Plants. Science 286(5441), 950–952 (1999)
120. Campos, S.K., Barry, M.A.: Current advances and future challenges in adenoviral vec-
tor biology and targeting. Curr. Gene. Ther. 2, 189–204 (2007)
121. Winkler, W.C., Cohen-Chalamish, S., Breaker, R.R.: An mRNA structure that controls
gene expression by binding FMN. Proc. Natl. Acad. Sci. USA 99(25), 15908–15913
(2002)
122. Casti, J.L.: Complexiﬁcation. Harper Perennial (1994)
123. Cheng, W.S., Dzojic, H., Nilsson, B., Totterman, E.M.: An oncolytic conditionally
replicating adenovirus for hormone dependent and hormone-independent prostate can-
cer. Cancer Gene. Ther. 13, 13–20 (2006)
124. Chu, R.L., Post, D.E., Khuri, F.R., Van Meir, E.G.: Use of replicating oncolytic aden-
oviruses in combination therapy for cancer. Clin. Cancer Res. 10(5), 299–312 (2004)
125. Darwin, C.: The Origin of Species. John Murray, London (1859)
126. Davies, P.: About Time. Orion Productions (1995)
127. Eiben, A.E., Smith, J.E.: Introduction to Evolutionary Computing. Springer (2007)
128. Eigen, M., Schuster, P.: The Hypercycle: A Principle of Natural Self-Organization.
Springer (1979)
129. Gheorghe, M. (ed.): Molecular Computational Models: Unconventional Approachers.
Idea Group Publishing (2005)
130. Fire, A., Xu, S.Q., Montgomery, M.K., Kostas, S.A., Driver, S.E., Mello, C.C.: Potent
and speciﬁc genetic interference by double-stranded RNA in Caenorhabditis elegans.
Nature 391, 806–811 (1998)
131. Fisher, R.A.: The Genetical Theory of Natural Selection. Oxford University Press
(1930)
132. Gelfand, M.S., Mironov, A.A., Jomantas, J., Kozlov, Y.I., Perumov, D.A.: A conserved
RNA structure element involved in the regulation of bacterial riboﬂavin synthesis genes.
Trends Genet. 15(11), 439–442 (1999)
133. Gibson, D.G., et al.: Creation of a Bacterial Cell Controlled by a Chemically Synthe-
sized Genome. Science 329(5987), 52–56 (2010)
134. Goldberg, D.E.: Genetic Algorithms in Search, Optimization, and Machine Learning.
Addison Wesley (1989)
135. Leja, J., Dzojic, H., Gustafson, E.: A novel chromogranine-A promoter-driven oncolytic
adenovirus for midgut carcinoid therapy. Clin. Cancer Res. 13, 2455–2462 (2007)
136. Leja, J., Nilsson, B., Gustafson, E., Akerstr¨om, G., Oberg, K., Giandomenico, V., Es-
sand, M.: Double detargeted oncolytic adenovirus shows replication arrest in liver cells
and retains neuroendocrine cell killing ability. PLOS ONE 5(1), 1–9 (2010)
137. Kaneko, K.: Life: An Introduction to Complex Systems Biology. Springer (2006)

References
375
138. Kauffman, L.H.: The Mathematics of Charles Sanders Peirce. Cybernetics & Human
Knowing 8(1-2), 79–110 (2001)
139. Manca, V., Franco, G., Scollo, G.: State transition dynamics: basic concepts and molec-
ular computing perspectives. In: [129] ch. 2, pp. 32–55 (2005)
140. Manca, V.: An Outline of MP Modeling Framework. In: Csuhaj-Varj´u, E., Gheorghe,
M., Rozenberg, G., Salomaa, A., Vaszil, G. (eds.) CMC 2012. LNCS, vol. 7762, pp.
47–55. Springer, Heidelberg (2013)
141. Marchetti, L., Mitrea, A., Kruger, H., Draghici, S., Manca, V., Bollig-Fisher, A.: Mod-
eling the transcription effects of HER2 oncogene signaling and a role for E2F2 in breast
cancer (submitted for publication, 2013)
142. Maturana, H., Varela, F.J.: Autopoiesis and Cognition: The Realization of the Living.
Reidel, Boston (1980)
143. Mavelli, F.: Ribocell Modeling. In: Proc. of the Artiﬁcial life XII Conference (2010)
144. Mironov, A.S., Gusarov, I., Raﬁkov, R., Lopez, L.E., Shatalin, K., Kreneva, R.A., Pe-
rumov, D.A., Nudler, E.: Sensing small molecules by nascent RNA: a mechanism to
control transcription in bacteria. Cell 111(5), 747–756 (2002)
145. von Neumann, J.: The Computer and the Brain, 2nd edn. Yale University Press (2000)
146. Ohler, U.: Computational promoter recognition in eukaryotic genomic DNA. PhD The-
sis, Technische Facult¨at der Universit¨at Erlagen-N¨urnberg (2001)
147. Prigogine, I., Stengers, I.: Entre Temps et l’´eternit´e. Librairie Arth`eme Fayard, Paris
(1988)
148. Price, K.V., Storn, R.M., Lampinen, J.A.: Differential Evolution: A Practical Approach
to Global Optimization. Springer (2006)
149. Reynolds, C.: Flocks, herds and schools: A distributed behavioral model. In: SIG-
GRAPH 1987, Proceedings of the 14th Annual Conference on Computer Graphics and
Interactive Techniques, pp. 25–34. ACM (1987)
150. Rosenblueth, A., Wiener, N., Bigelow, J.: Behavior, purpose, and teleology. Philosophy
of Science 10, 18–24 (1943)
151. Salmena, L., Poliseno, L., Tay, Y., Kats, L., Pandolﬁ, P.P.: A ceRNA Hypothesis: The
Rosetta Stone of a Hidden RNA Language? Cell 146, 353–358 (2011)
152. Stano, P., Rampioni, G., Carrara, P., Damiano, L., Leoni, L., Luisi, P.L.: Semi-synthetic
minimal cells as a tool for biochemical ICT. BioSystems 109, 24–34 (2012)
153. Stormo, G.D., Ji, Y.: Do mRNAs act as direct sensors of small molecules to control
their expression? Proc. Natl. Acad. Sci. U.S.A. 98(17), 9465–9467 (2001)
154. Tay, Y., Kats, L., Salmena, L., Weiss, D., Tan, S.M., Ala, U., Karreth, F., Poliseno,
L., Provero, P., Di Cunto, F., Lieberman, J., Rigoutsos, I., Pandolﬁ, P.P.: Coding-
Independent Regulation of the Tumor Suppressor PTEN by Competing Endogenous
mRNAs. Cell 147, 344–357 (2011)
155. Thom, R.: Stabilit´e Structurelle et morphog´en`ese. Benjamin (1972)
156. Thom, R.: Semiophysics: a sketch. Addison-Wesley (1990)
157. Toth, K., Dhar, O., Wold, W.S.: Oncolytic (replication-competent) adenoviruses as an-
ticancer agents. Expert Opin. Biol. Ther. 10, 353–368 (2010)
158. Turing, A.M.: The Chemical Basis of Morphogenesis. Philosophical Transactions of
the Royal Society of London. Series B, Biological Sciences 237(641), 37–72 (1952)
159. Wechler, R., Russel, S.J., Curiel, D.T.: Engineering targeted viral vectors for gene ther-
apy. Nat. Rev. Genet. 8, 573–587 (2007)
160. Wiener, N.: Cybernetics: or Control and Communication in the Animal and the Ma-
chine. The MIT Press, Cambridge (1948)
161. Wolfram, S.: Theory and Application of Cellular Automata. Addison-Wesley (1986)
162. Wolfram, S.: A new Kind of science. Wolfram Media (2002)

376
References
163. Wolpert, L.: Developmental Biology: A Very Short Introduction. Oxford University
Press (2011)
164. Gold, L., Brown, D., He, Y., Shtatland, T., Singer, B.S., Wu, Y.: From oligonucleotide
shapes to genomic SELEX: Novel biological regulatory loops. Proc. Natl. Acad. Sci.
U.S.A. 94(1), 59–64 (1997)
References for Chapter 5
165. Acheson, D.: From Calculus to Chaos. Oxford University Press (1997)
166. Argand, J.R.: Essai sur une mani`ere de repr´esenter les quantit´es imaginaires dans les
constructions gr´eomr´etriques. Annales de Math´ematiques, Paris (1806)
167. Boyer, C., Merzbach, U.: A History of Mathematics, 2nd edn. Wiley (1989)
168. Brin, M., Stuck, G.: Introduction to Dynamical Systems. Cambridge University Press
(2002)
169. Balakrishnan, V.K.: Introductory Discrete Mathematics. Dover Publications (1991)
170. Chabert, J.L.: A History of Algorithms. Springer (1999)
171. Courant, R., Robbins, H.: What is Mathematics, 2nd edn. Oxford University Press
(1996)
172. Crossley, J.N., et al.: What is mathematical logic. Oxford University Press (1972)
173. Martin Davis, M. (ed.): The Undecidable: Basic Papers on Undecidable Propositions,
Unsolvable problems and Computable Functions. Raven Press (1965)
174. Fr¨ankel, A.A.: Abstract Set Theory. North-Holland (1976)
175. G¨odel, K.: On Formally Undecidable Propositions of Principia Mathematica and Re-
lated Systems (based on the original paper of 1931, in German). Dover (1962)
176. Gr¨atzer: First Step In Latex. Springer (1999)
177. Halmos, P.R.: Naive set theory. Springer (1960)
178. Dunham, W.: Euler: The Master of Us All. The Mathematical Association of America
(1999)
179. Kaplan, R., Kaplan, E.: The Art of the Inﬁnite. Oxford University Press (2003)
180. Kline, M.: Mathematical Thought from ancient to modern times, vol. 2. Addison Wes-
ley (1972)
181. Kurka, P.: Topological and Symbolic Dynamics. Soci´et´e Math´ematique de France
(2003)
182. Luenberger, D.G.: Introduction to Dynamic Systems. Theory, Models, and Applica-
tions. John Wiley & Sons (1979)
183. May, R.B.: Simple mathematical models with very complicated dynamics. Nature 261,
459–467 (1976)
184. Manca, V.: Formal Logic. In: Webster, J.G. (ed.) Encyclopedia of Electrical and Elec-
tronics Eng., vol. 7, pp. 675–687. John Wiley & Sons (1999)
185. Kalish, D., Montague, R.: Logic: Techiniques of formal reasoning. Harcourt, Brace &
World. Inc. (1964)
186. P´eter, R.: Playing with Inﬁnity (Hungarian edition 1957). Dover (2010)
187. P´olya, G.: How to solve it, 2nd edn. Penguin Books (2010)
188. Smory´nski, C.: Logical Number Theory. Springer (1991)
189. Smullyan, R.M.: First-Order Logic. Dover Publications (1995)
190. Toeplitz, O.: The Calculus: A Genetic Approach. German edition (1949) edited by
Kothe, G. The University of Chicago Press (2007)
191. Wirsching, G.J.: The Dynamical System Generated by the 3n + 1 Function. Lecture
Notes in Mathematics, vol. 1681 (1998)

References
377
192. van der Waerden, B.L.: A History of Algebra. Springer, Berlin (1985)
193. Whitehead, A.N.: An Introduction to Mathematics. Oxford University Press (1948) (1st
edn. 1911)
References for Chapter 6
194. Aspray, W. (ed.): Computing before computers. Iowa State University Press, Ames
(1990)
195. Berstel, J.: Axel Thue’s papers on repetitions in words: a translation. Institut Blaise
Pascal, Universit´e Pierre et Marie Curie, Paris (1994)
196. Chomsky, N.: Three models for the description of language. IRE Transactions on Infor-
mation Theory 2, 113–124 (1956)
197. Chomsky, N.: On certain formal properties of grammars. Information and Control 2(2),
137–167 (1959)
198. Cover, T.M., Thomas, J.A.: Elements of Information Theory. John Wiley & Sons (1991)
199. Davis, M.: Computability and Unsolvability. Dover Publications (1982)
200. Luenberger, D.G.: Information Science. Princeton University Press (2006)
201. Seymour Ginsburg, S.: The mathematical theory of context free languages. McGraw-
Hill (1966)
202. Hopcroft, J.E., Motwani, R., Ullman, J.D.: Introduction to Automata Theory, Lan-
guages, and Computation, 3rd edn. Addison-Wesley (2006)
203. Kleene, S.C.: Representation of Events in Nerve Nets and Finite Automata. In: Shan-
non, C., John McCarthy, J. (eds.) Automata Studies. Princeton University Press (1956)
204. Manca, V.: Logical String Rewriting. Theoretical Computer Science 264, 25–51 (2001)
205. Prusinkiewicz, P., Lindenmayer, A.: The Algorithmic Beauty of Plants (The Virtual
Laboratory). Springer (1990)
206. Minsky, M.L.: Computation: ﬁnite and inﬁnite machines. Prentice Hall (1967)
207. von Neumann, J.: First Draft of a Report on the EDVAC. Moore School of Electrical
Eng. Univ. of Pennsylvania (1945); IEEE Annals of the History of Computing 15(4),
27–75 (1993)
208. Post, E.L.: Finite Combinatory Processes - Formulation 1. Journal of Symbolic Logic 1,
103–105 (1936)
209. Post, E.L.: Formal Reductions of the General Combinatorial Decision Problem. Amer-
ican Journal of Mathematics 65, 197–215 (1943)
210. Salomaa, A.: Formal Languages. Academic Press (1973)
211. Salomaa, A.: Jewels of Formal Language Theory. Computer Science Press (1981)
212. Shannon, C.E.: A Mathematical Theory of Communication. The Bell System Technical
Journal 27, 379–423, 623–656 (1948)
213. Rozenberg, G., Salomaa, A. (eds.): Handbook of formal languages. Springer (1997)
214. Turing, A.M.: On computable numbers with application to the entscheidungsproblem.
Proc. London Math. Soc. 2(42), 230–265 (1936)
References for Chapter 7
215. Actzel, A.: Chance. High Stakes (2006)
216. Aczel, A.D., Sounderpandian, J.: Complete Business Statistics. McGraw Hill, Interna-
tional Edition (2006)
217. Aigner, M.: Discrete Mathematics. American Mathematical Society (2007)

378
References
218. Conway, J.H., Guy, R.K.: The Book of Numbers. Springer (1996)
219. Durbin, R., et al.: Biological Sequence Analysis. Cambridge University Press (1998)
220. Even, S.: Graph Algorithms. Computer Science Press (1979)
221. Feller, W.: An Introduction to Probability Theory and Its Applications, 3rd edn., vol. 1.
John Wiley and Sons (1968)
222. Fisher, R.A.: On The Mathematical Foundation of Theoretical Statistics. Transactions
of the Royal Society of London 222, 309–368 (1922)
223. Kirkwood, B., Sterne, J.: Essential Medical Statistics. Wiley-Blackwell (2003)
224. Knuth, D.E.: The Art of Computer Programming: Fundamental Algorithms. Addison
Wesley (1968)
225. Luenberger, D.G.: Optimization by Vector Space Methods. John Wiley & Sons (1969)
226. Manca, V.: Enumerating Membrane Structures. In: Corne, D.W., Frisco, P., P˘aun,
G., Rozenberg, G., Salomaa, A. (eds.) WMC 2008. LNCS, vol. 5391, pp. 292–298.
Springer, Heidelberg (2009)
227. Manca, V.: A Recurrent Enumeration of Free Hypermultisets. In: Kelemen, J., Kele-
menov´a, A. (eds.) Computation, Cooperation, and Life. LNCS, vol. 6610, pp. 16–23.
Springer, Heidelberg (2011)
228. Mlodinow, L.: The Drunkard’s Walk. Knopf Doubleday Publishing Group (2009)
229. Pearson, K.: Notes on the History of Correlation. Biometrika 13(1), 25–45 (1920)
230. Poincar´e, J.H.: La science et l’hypoth`ese. Flammarion, Paris (1968)
231. Pr¨ufer, H.: Neuer Beweis eines Satzes ¨uber Permutationen. Arch. Math. Phys. 27, 742–
744 (1918)
232. Otter, R.: The number of trees. The Annals of Mathematics, 2nd Ser. 49(3), 583–599
(1948)

Index
Activator, 179
Adenovirus, 198
Adleman, 51
Adleman-Lipton extract model, 52
Algorithm, 131, 134, 163, 223, 225, 229,
230, 253, 277, 282, 292
Alignment free genome analysis, 82
Allocation, 311
Alphabet, 5, 275
Amino acid, 174
Anabolism, 156
Analysis of variance, 346
Ant Colony Optimization, 184
Archimedes, 233
Archimedes’ solids, 357
Argand, 236, 237
Arithmetic, 302
Arrangement, 312
Asymptotic order, 264
Attractor, 272
Automaton, 277
Autonomous dynamical system, 270
Autopoiesis, 185
Avogadro-Dalton-Action, 124
Basel Problem, 239
Basin, 272
Bayes’ theorem, 334
Bell number, 326
Belousov, 158
Belousov-Zhabotinsky, 137
Bergson, 201
Bernoulli, 316, 332
Bernoulli number, 327
Bernoulli’s law, 316
Bi-somatic language, 278
Bigelow, 211
Binomial coefﬁcient, 234, 314, 315
Boltzmann, 201, 203
Boolean function, 251
Brusselator, 137
Brusselatus, 137
Cantor, 231, 302
Catabolism, 156
Catalan number, 326
Catalyst, 173
Cayley, 350, 365
Chaos, 193, 262
Chebichev, 331
Chevalier de M´er´e, 334
Chomsky grammar, 74
Chomsky hierarchy, 282
Church, 211, 300
Clock, 271
Code, 305
Collatz, 272
Colorability, 354
Combination, 313
Complexity, 201
Compression, 305
Computability, 302
Computable function, 300
Conditional probability, 333
Conﬁdence interval, 133, 346
Connective, 251
Crick, 40
Critical value, 339, 345

380
Index
Curry, 211
Cycle dissipation principle, 157
d’Alembert, 237
Darwin, 200, 209
De Finetti, 263
De Moivre, 316, 332
De Moivre-Laplace formula, 317
Decidability, 299
Dedekind, 237
Degree of freedom, 132
Dependent variable, 342
Derivation, 351
Deterministic ﬁnite state automaton, 287
Dictionary based indexes, 84
Digit, 223
Direct rewriting, 279
Distinguishability, 311
DNA
Ampliﬁcation, 56
Antiparallelism, 33, 40
Bilinearity, 33
Chirality, 40
Clone, 47
Complementarity, 33
Computing, 50
Denaturation, 44
Extraction, 65
Gel-electrophoresis, 44
Hairpin, 41
Helix, 33
Hetero-duplex, 41
Hybridization, 41
Ligase enzyme, 44
Mix-and-split, 53
Monomeric triangle, 34
Notation, 29
PCR protocol, 56
Polymerase enzyme, 33, 44
Polymerase extension, 44
Pool, 32
Pool operations, 43
Primer, 33
Recombination, 66
Sequencing, 47
Strand, 29
String operations, 30, 33
Synthesis, 47
DNA computing, 210
DNA microarray, 237
Double strings, 29
Drosophilus, 137
Dynamical inverse problem, 117, 160
Dynamics, 270
EDVAC project, 302
Eigen paradox, 9, 171
Empty string, 276
ENCODE, 82
Enhancer, 179
Entropy, 203, 304, 331
Enzymatic paradox, 173
Enzyme, 173
Epidemicus, 137
Equational Metabolic Algorithm, 112
Eraclytus, 117
Euclid, 355
Euler, 233, 239, 263, 265, 355
Euler pentagonal number theorem, 322
Euler’s polyhedral formula, 354
Evolutionary algorithm, 181
Exon, 179
Exponential growth, 263
F test, 133, 345
F-distribution, 132, 345
Factorial, 312
Faulhaber, 258
Fermat, 316
Fibonacci, 143, 222, 247, 249
Figurate number, 243, 257
Finite automaton, 284
Finite state automaton
Deterministic recognition, 287
Nondeterministic recognition, 287
First Order Logic, 250
Fisher, 200, 337
Formal Language Theory, 64
Formal language theory, 302
Formal logic, 255
Formula, 250
Fullerene, 357
Function, 219
Galileo, 209, 271, 334
Galton, 337
Gauss, 119, 236, 237, 332, 340
Gaussian distribution, 316
Gene, 174

Index
381
Gene expression regulation, 179
Gene transcription, 179
Gene translation, 179, 180
Genetic algorithm, 181
Genetic code, 175, 178
Genome
co-Entropy, 86
Compression, 97
Dictionary, 84, 89
Entropy, 85
Factorization, 83
Forbidden word, 86
Index, 89
Lexicality, 86
Noise ampliﬁcation, 100
Representation, 97
Selectivity, 85
Sequencing, 83
Size, 89
Table, 84, 89
Visualzation, 97
Gibbs, 156, 203
Girard, 362
Gogol, 268
Goldbeter, 137
Golden ratio, 248
Goniometricus, 137
Google, 101
Gram matrix, 341
Grammar, 277
Graph, 20, 246
simple, 349
G¨odel, 301
G¨odel’s incompleteness theorem, 301
H Theorem, 202
Halley, 117
Hapax, 87
Head, 64
Heteroscedasticity, 136
Hierarchical clustering, 137
Hilbert space, 340
Hypothesis testing, 337
Immunological algorithm, 182
Independent variable, 342
Induction, 241
Inertia, 139
Inﬁnite, 264
Inﬁnitesimal, 264
infodynamics, 119
Infogenomics, 81
Information source, 305
Information theory, 205, 304
Inhibitor, 179
Instruction, 302
Integer partition, 321
Interpretation, 251
Intron, 179
Inventor’s paradox, 172
Iterated transducer, 176
Iteration, 222, 247
k-mer, 84
Kleene, 288
Kleene star, 278
Knuth, 361
Kolmogorov, 333
Kuratowski, 352, 358
K¨onig, 245
L-systems, 68
Lamarque, 200
Language, 275
Context-free, 283
Context-sensitive, 283
Decidable, 299
Deﬁned by a string pattern, 292
Generated by a grammar, 280
Recognized by an automaton, 285
Regular, 284
Semidecidable, 299
Undecidable, 299
Language expression, 277
Language operation, 277
Laplace, 316, 332
Latek, 361
Least Square Estimation, 340
Least Square Evaluation, 125, 341
Least square problem, 119
LGSS Algorithm, 134
Lindenmayer, 68
Liposome, 12
Lipton, 52
Location, 360
Log-gain principle, 129
Lotka-Volterra, 137
LUCA, 8

382
Index
Markup language, 361
Mascheroni, 265
MATLAB, 135
Matrix Kronecker product, 126
Maurolico, 241
Maxwell, 204
McCulloch, 288
McCullocht, 302
Mean square error, 343
Mellitus, 137
Membrane, 9
Membrane computing, 75, 210
Memetic algorithm, 182
Mengoli, 239
Metabolic approximation, 119
Metabolic computing, 163
Metabolic Grammar, 107
Metabolic oscillator, 147
Metabolic pattern, 144
Metabolic state, 108
Minimal forbidden length, 86
Minsky, 77
Mitoticus, 137
Molar perspective, 109
Mole, 12
Molecule, 6
Mono-somatic language, 278
Monomer, 28
Monty Hall problem, 335
MP
Minus Plus grammar, 167
Flux, 108
Grammar, 108, 167
Graph, 110
Mass factor, 109
Metabolite, 108
Oscillator, 147
Parameter, 109
Population factor, 109
Reaction, 108
Regulator, 108
Rule, 108
Stoichiometric matrix, 110
System, 109
Time factor, 109
Tuner, 109
MP gene regulation analysis, 161
MP Models
Brusselator, 115
Gonoimetricus, 121
Lotka-Volterra, 114
Mitotic oscillator, 138
Table of references, 137
MP rule type, 141
mRNA, 179
Mullis, 56
Multicollinearity, 136
Multimembrane, 164
Multinomial coefﬁcient, 319
Multiple coefﬁcient of determination, 344
Multiple regression, 342
Multiplicity-Comultiplicity distribution, 84
Multiset, 9
Multiset rewriting, 109
Mutual information, 309
Mycoplasma mycoides, 196
Napier, 233, 235
Natural computing, 210
Neural algorithm, 183
Neural Network, 24
Neuron, 24
Newton, 117, 201
Nichomachus, 243
Nondeterministic ﬁnite state automaton, 287
Nonterminal symbol, 279
Nucleotide, 4, 20
Number
Complex, 233
Greek classiﬁcation, 221
Integer, 225
Natural, 222
Rational, 226
Real, 227
Number of multisets, 320, 321
Number of partitions, 320
Operation, 218
Optimal code, 305
Ordinary Differential Equations, 162
Oresme, 265
Overﬁtting, 343
Ovum, 192
p-value, 339
Parenthesis, 362
Pareto, 340
Partial F test, 133, 348
Pascal, 241, 334

Index
383
PCR lemma, 60
Peano, 241, 362
Peano Arithmetic, 301
Pearson, 337
Permutation, 312
Petri net, 164
Phagocytosis, 189
Photochemicus, 137
Pigeonhole Principle, 313
Pitts, 288, 302
Planarity, 354
Plasmide cloning, 48
Plato, 268, 355
Platonic solids, 355
Poincar´e, 193, 337
Pointer, 360
Poisson, 332
Poisson distribution, 317
Polyhedron, 354
Polymer, 7, 27
Population, 15
Post, 299
pre-mRNA, 179
Prigogine, 158
Prime number, 245
Probability, 331
Program, 302
Progression
Arithmetical, 256
Geometrical, 260
Promoter, 179
Protein, 174
Protein function, 175
Protocell, 6, 187
Pr¨ufer, 365
Pythagoras, 222
Pythagorean recombination game, 203
Qu´etelet, 336
Rank-Multiplicity distribution, 84
Reaction Graph, 22
Reactivity, 139
Recurrence, 241
Recursive language, 282
Recursively enumerable language, 282
Red Alga, 75
Register machine, 77, 302
Regressors, 125
Regular expression, 284
Regulation discovery problem, 118
Relation, 218
Relational Structure, 250
Repeat, 87
Repressor, 179
Residual autocorrelation, 136
Ribocell, 195
Ribosome, 177, 179
RNA polymerase, 179
RNA silencing, 197
Rosenbluth, 211
Russell, 232
Sample signiﬁcance indexes, 339
Sanger, 47
SAT problem, 52
Semidecidability, 299
Sequence, 7, 218
Sequence alignment, 82
Set, 217
Set-theoretic notation, 218
Shannon, 205, 211, 304
Shannon’s ﬁrst theorem, 306
Sirius, 137
Sirius oscillator, 113, 148
Skolem’s paradox, 254
Spliceosome, 179
Splicing, 179
Splicing rule, 64
Squared error, 132, 342
Squared regression deviation, 132
Squared regression dviation, 342
Squared total deviation, 132, 342
Standatd deviation, 331
Statistical Distribution, 330
Statistical inference, 336
Statistics, 331
Stepwise regression, 131, 347, 349
Stirling approximation, 316, 328
Stirling number, 325
String, 275
String concatenation, 276
String pattern, 277
String rewriting rule, 293
String theory, 73
Student’s test, 337
Susceptible Infected Recovered, 137
Synthetic biology, 195

384
Index
t-distribution, 133, 346
t-Expansion, 125
Tag, 360, 362
Tartaglia, 101, 241
Tartaglia-Pascal triangle, 315
Term, 250
Terminal symbol, 279
Text analysis, 97
Thermodynamical system, 203
Thom, 198
Time discreteness, 109
Time series, 119
Time’s arrow, 201
Timed dynamical system, 270
Transcription co-factor, 179
Transcription factor, 179
Transcription Start Site, 179
Transitive rewriting, 279
Tree, 18, 245
Binary, 367
Decorated, 350
Enumeration, 364
Labeled, 350
Linear encoding, 365
Ordered, 350
Phylogenetic, 367
Rooted, 349
Unlabeled, 350
Unrooted three-branched, 368
Tri-somatic language, 69, 278
Turing, 211, 296
Turing machine, 295
Turing-Church Thesis, 300
Typicality, 308
Undecidability, 299
Universal Turing machine, 300
Untranslated Terminal Region, 179
Variable, 220
Variable selection, 347
Variance Inﬂation Factor, 136
Vector, 237
Vectorization Lemma, 127
Venter, 196
Vesiculation, 190
Vi`ete, 362
Volteranus, 137
von Neumann, 211
Wallis’ product, 328
Watson, 40
Weierstrass, 237
Weighted least squares, 136
Wiener, 201, 211
Zero, 225
Zhabotinsky, 158

