
BestMasters

Springer awards „BestMasters“ to the best master’s theses which have been comple-
ted at renowned universities in Germany, Austria, and Switzerland.
Th e studies received highest marks and were recommended for publication by 
supervisors. Th ey address current issues from various fi elds of research in natural 
sciences, psychology, technology, and economics.
Th e series addresses practitioners as well as scientists and, in particular, off ers gui-
dance for early stage researchers.

Matthias Kaeding
Bayesian Analysis of 
Failure Time Data
Using P-Splines

Matthias Kaeding
Hamburg, Germany
BestMasters
ISBN 978-3-658-08392-2    ISBN 978-3-658-08393-9 (eBook)
DOI 10.1007/978-3-658-08393-9
Library of Congress Control Number: 2014958185
Springer Spektrum
© Springer Fachmedien Wiesbaden 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or 
part of the material is concerned, speci¿ cally the rights of translation, reprinting, reuse of illus-
trations, recitation, broadcasting, reproduction on micro¿ lms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by 
similar or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this 
publication does not imply, even in the absence of a speci¿ c statement, that such names are 
exempt from the relevant protective laws and regulations and therefore free for general use. 
The publisher, the authors and the editors are safe to assume that the advice and information in this 
book are believed to be true and accurate at the date of publication. Neither the publisher nor the 
authors or the editors give a warranty, express or implied, with respect to the material contained 
herein or for any errors or omissions that may have been made.
Printed on acid-free paper
Springer Spektrum is a brand of Springer Fachmedien Wiesbaden
Springer Fachmedien Wiesbaden is part of Springer Science+Business Media
(www.springer.com)

Contents
1
Introduction
1
1.1
Outline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Basic Concepts of Failure Time Analysis
5
2.1
Continuous Time . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Discrete Time . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.3
Likelihood Construction
. . . . . . . . . . . . . . . . . . . . . .
8
2.3.1
Censoring and Truncation
. . . . . . . . . . . . . . . . .
8
2.3.2
Time Varying Covariates . . . . . . . . . . . . . . . . . .
12
2.4
Relative Risk and Log-Location-Scale Family . . . . . . . . . . .
14
3
Computation and Inference
17
3.1
MCMC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.1.1
Metropolis-Hastings . . . . . . . . . . . . . . . . . . . .
18
3.1.2
Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . .
19
3.2
Inference from Simulation Output
. . . . . . . . . . . . . . . . .
20
3.3
Model Diagnostics and Comparison . . . . . . . . . . . . . . . .
22
3.3.1
Criterion Based Methods . . . . . . . . . . . . . . . . . .
23
3.3.2
Martingale Residuals . . . . . . . . . . . . . . . . . . . .
26
3.4
Bayesian P-Splines . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.4.1
Basic Concepts . . . . . . . . . . . . . . . . . . . . . . .
27
3.4.2
Extended Linear Predictor . . . . . . . . . . . . . . . . .
36
4
Discrete Time Models
45
4.1
Estimation Based on GLM Methodology . . . . . . . . . . . . . .
46
4.1.1
Grouped Cox . . . . . . . . . . . . . . . . . . . . . . . .
52

VI
Contents
4.1.2
Logistic Model . . . . . . . . . . . . . . . . . . . . . . .
53
4.2
Estimation Based on Latent Variable Representation . . . . . . . .
54
4.2.1
Probit Model . . . . . . . . . . . . . . . . . . . . . . . .
54
4.2.2
Scale Mixtures of Normals . . . . . . . . . . . . . . . . .
56
4.2.3
Grouped Cox II . . . . . . . . . . . . . . . . . . . . . . .
58
5
Application I: Unemployment Durations
61
6
Continuous Time Models
69
6.1
Lognormal and Extensions . . . . . . . . . . . . . . . . . . . . .
69
6.2
Relative Risk Models . . . . . . . . . . . . . . . . . . . . . . . .
72
6.2.1
Exponential Distribution . . . . . . . . . . . . . . . . . .
76
6.2.2
Weibull Distribution
. . . . . . . . . . . . . . . . . . . .
77
6.2.3
Other Baseline Hazards . . . . . . . . . . . . . . . . . . .
79
6.3
Nonparametric Relative Risk Models . . . . . . . . . . . . . . . .
80
6.3.1
Piecewise Exponential Hazard . . . . . . . . . . . . . . .
80
6.3.2
Nonparametric Relative Risk Models
. . . . . . . . . . .
83
7
Application II: Crime Recidivism
87
8
Summary and Outlook
95
Appendix A: Description of R Function
99
Bibliography
103

List of Figures
2.1
Some hazard rates . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2
Some survivor functions
. . . . . . . . . . . . . . . . . . . . . .
6
3.1
B-Spline basis functions of degree 0,1,2,3. . . . . . . . . . . . . .
31
3.2
Function estimation via B-splines
. . . . . . . . . . . . . . . . .
32
3.3
Inﬂuence of smoothing parameter
. . . . . . . . . . . . . . . . .
33
4.1
Common response functions . . . . . . . . . . . . . . . . . . . .
52
4.2
Error approximation logistic
. . . . . . . . . . . . . . . . . . . .
57
5.1
Convergence problems . . . . . . . . . . . . . . . . . . . . . . .
63
5.2
Histograms of numbers of effective parameter draws for both sam-
plers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
5.3
Fixed effects - discrete time . . . . . . . . . . . . . . . . . . . . .
65
5.4
Density curves for martingale residuals at selected points in time.
The grouped Cox model with complementary log-log link is can
be seen to perform slightly better, the distribution of the residuals
is more centered around zero for this model. . . . . . . . . . . . .
66
5.5
Nonlinear effects - discrete time (a)
. . . . . . . . . . . . . . . .
67
5.6
Nonlinear effects - discrete time (b)
. . . . . . . . . . . . . . . .
68
6.1
(a) hazard (b) density (c) survivor function of the lognormal distri-
bution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
6.2
(a) hazard (b) density (c) survivor function of the Weibull distribu-
tion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
6.3
Example of a bathtub shaped hazard . . . . . . . . . . . . . . . .
80

VIII
List of Figures
7.1
Fixed effects continuous time . . . . . . . . . . . . . . . . . . . .
89
7.2
Effective number of parameters - continuous times
. . . . . . . .
90
7.3
Baseline hazard for the piecewise constant model with 5% and
95% quantiles. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
7.4
Baseline hazard for the lognormal model with 5% and 95% poste-
rior quantiles. Note that the posterior quantiles for the lognormal
model are very narrow and barely visible.
. . . . . . . . . . . . .
90
7.5
Estimated nonlinear effects - continuous time (a)
. . . . . . . . .
91
7.6
Estimated nonlinear effects - continuous time (b)
. . . . . . . . .
93
7.7
Selected sampling paths . . . . . . . . . . . . . . . . . . . . . . .
94

List of Tables
2.1
Discrete data
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.2
Discrete data - longitudinal . . . . . . . . . . . . . . . . . . . . .
11
2.3
Episode splitting
. . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.1
Loss functions and corresponding estimators . . . . . . . . . . . .
21
4.1
Some mixtures
. . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.1
Model assessment
. . . . . . . . . . . . . . . . . . . . . . . . .
62
6.1
Data set expansion for piecewise constant model, given knots 0, 2,
5, 8
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
7.1
Model assessment
. . . . . . . . . . . . . . . . . . . . . . . . .
88

1 Introduction
Failure time analysis is a form of regression analysis where the time until an event
occurs is of interest. The event is generically referred to as failure in this thesis,
the observational units are referred to as individuals.
Unlike most regression models the model is not formulated for the conditional
expectation. Most regression models for failure time analysis are formulated in
terms of the hazard rate, giving the risk of failure and will be deﬁned precisely in
the following. A general formulation for the hazard rate is (Cox and Oakes 1984,
p. 70):
h(t|zzzi,βββ) = h0(t)ρ(β1z1 +...,βkzk) = h0(t)ρ(ηi).
Here, the baseline hazard h0(t) gives the hazard of an individual with standard
conditions, corresponding to zzz = 000, ηi = zzz⊤
i βββ is the linear predictor and ρ(·) is a
nonnegative function satisfying ρ(0) = 1. Splines allow the replacement of linear
effects of the form zzz⊤
i βββ in the linear predictor by more general functions. This is
useful for ﬂexible modeling of the baseline hazard, treating time formally like a
covariate. A spline is a function consisting of local polynomials that are joined
together at points in the domain of the covariate. Splines can be understood as a
regression model: Every spline can be written as a weighted sum of basis func-
tions depending on a covariate, hence a regression model where the regression
coefﬁcients are given by the weigths.
The aim of this thesis is to present Bayesian methods for models where either
the hazard rate, covariates, or both are modeled via splines, in discrete and contin-
uous time. B-spline basis functions in combination with a penalty to avoid over-
ﬁtting (usually called P-splines) are the main building blocks used for modeling.
P-splines have good numerical properties, and allow fast computation. Addition-
ally, other useful basis functions for failure time analysis will be given. Failures are
always assumed to be nonrecurrent. A fully Bayesian perspective using MCMC
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_1, © Springer Fachmedien Wiesbaden 2015

2
1 Introduction
methods is taken.
1.1 Outline
This thesis is structured as follows. At ﬁrst the basic concepts of failure time
analysis are introduced. For the statistical analysis of failure time data, time is
represented by a random variable which is characterized by quantities that are
speciﬁc for failure time modeling. These quantities can be used to construct the
likelihood by taking into account special properties of failure time data, such as
censoring, which refers to failure times that are not fully observed. Next, two
central model families are introduced; the relative risk and the log-location-scale
model family. The subsequent chapter gives an overview of computational and
inferential methods as they are relevant for model building. The chapter concludes
with the introduction of Bayesian P-splines using the Gaussian likelihood as an
example. The sampling scheme for Gaussian responses can be adjusted for the
probit model for discrete time and the lognormal model for continuous time. Sub-
sequently, models for the analysis of discrete time are introduced. Gibbs sampler
for these models are categorized here by methods embedded in the generalized lin-
ear model (GLM) and the latent variable framework. Based on those frameworks
efﬁcient Bayesian sampling schemes can be constructed. From the GLM frame-
work iteratively weighted least squares (IWLS) proposals based on ﬁsher scoring
for the Metropolis-Hastings algorithm can be derived. Many sampling schemes
for models using P-splines were developed on the basis on IWLS proposals, in-
cluding sampling schemes for continuous time models. Discrete time models are
illustrated using data of unemployment durations. Subsequently, estimation for
continuous time is described. The focus is on relative risk models but the lognor-
mal and extensions based on will also be discussed. The methods are illustrated
using a data set on crime-recidivism. As ﬁnal chapter, a summary with outlook
will be given.

1.2 Notation
3
1.2 Notation
In this thesis standard notation as often used in the literature is used. The distinc-
tion between a random variable Y and its realizations y will be made in the intro-
ductory chapters and ignored for the later chapters when the meaning is obvious.
The conventions used in this thesis are listed here. Conditioning on parameters
will often be surpressed for notational simplicity.

4
1 Introduction
Symbol
Explanation
x
scalar
xxx = (x1,...,xn)⊤
vector
XXX
matrix
I[·]
indicator function
diag(x1,...,xn)
diagonal matrix obtained from xxx
bdiag(AAA,BBB)
block diagonal matrix out of matrices AAA,BBB
The following table gives an overview over important ﬁxed symbols.
Symbol
Explanation
h(t)
hazard rate
H(t)
cumulative hazard rate
h0(t)
baseline hazard
H0(t)
cumulative baseline hazard
G(t)
survivor function
D
available data
L(θθθ|D)
likelihood
vi
censoring indicator
η
linear predictor
The following table gives an overview over the shorthand used for the distributions.
Distribution
Shorthand
Parameter
normal
N(μ,σ2)
expectation μ, variance σ2
truncated normal
TN(a,b)(μ,σ2)
expectation μ, variance σ2, support (a,b)
lognormal
LN(μ,σ2)
location μ,shape σ
inverse gamma
IG(α,β)
shape α, scale β
gamma
G(α,β)
shape α, rate β
Poisson
P(λ)
mean/variance λ
inverse Wishart
IW(a,BBB)
degrees of freedom a, scale matrix BBB

2 Basic Concepts of Failure Time Analysis
2.1 Continuous Time
Time is represented by the nonnegative random variable T with cumulative density
function
F(t) = P(T ≤t),
and density
f(t) = dF(t)/dt.
For failure time analysis, T is generally characterized by other quantities. The
survivor function gives the probability that T exceeds t:
G(t) = P(T > t) = 1−F(t).
It always holds that no individual has failed at T = 0
G(0) = 1,
(2.1)
and it is usually assumed that every subject will fail eventually
lim
t→∞G(t) = 0.
(2.2)
Variables with survivor function not satisfying 2.2 are called defective, for those
it follows that E[T]=∞. The probability of failure in the small interval [t,t+dt) can
be approximated by h(t)dt (Aalen et al. 2008, pp. 5–17). The function h(t) is the
hazard rate, deﬁned as:
h(t) = lim
Δ→0
P(t ≤T < t +Δ|T ≥t)
Δ
.
(2.3)
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_2, © Springer Fachmedien Wiesbaden 2015

6
2 Basic Concepts of Failure Time Analysis










W
KD]DUG
Figure 2.1: Some hazard rates










W
VXUYLYRUIXQFWLRQ
Figure 2.2: Some survivor functions
The probability P(t ≤T < t +Δ|T ≥t) is
F(t +Δ)−F(t)
G(t)
.
Hence 2.3 is
1
G(t) lim
Δ→0
F(t +Δ)−F(t)
Δ
= F′(t)
G(t) = f(t)
G(t),
showing that the hazard is a conditional density.
The cumulative hazard rate is
H(t) =
 t
0 h(t) =
 t
0
f(u)
G(u) du = [−logG(u)]t
0 = −logG(t),
due to 2.1. Hence, the survivor function can be written in terms of the hazard rate:
G(t) = exp(−
 t
0 h(t)) = exp(−H(t)).
(2.4)
The same applies for the density:
f(t) = h(t)G(t) = h(t)exp(−H(t)).
Because of these relationships, the random variable T is fully speciﬁed by one of
the given quantities. From 2.4, it can be seen that the function h(t) only needs to

2.2 Discrete Time
7
satisfy
 t
0 h(s)ds < ∞,
for all t and
 ∞
0 h(s)ds = ∞
to be the hazard rate of a nondefective continuous variable (Kalbﬂeisch and Pren-
tice 2002, p. 9). Many models in failure time modeling are formulated in terms of
the hazard rate ﬁrst.
2.2 Discrete Time
In the case of grouped failure times, an unobservable continous random variable T ⋆
is partitioned into m+1 intervals [a0 = 0,a1),[a1,a2),...,[am,am+1 = ∞), (Lawless
2003, p. 370). Observed are discrete failure times from the random variable T =
{1,2,..,m+1}, so that T= t corresponds to T⋆∈[at−1,at). The hazard in terms of
T is
h(t) = P(T = t|T ≥t) = P(T = t)
P(T ≥t) =
f(t)
G(t −1).
(2.5)
Expressing 2.5 in terms of T ⋆gives:
h(t) = G⋆(at)−G⋆(at−1)
G⋆(at)
= 1−exp(−
 at
at−1
h⋆(u)du).
This is the probability of failure in interval t, conditional on reaching the interval.
A discrete time model can be speciﬁed in terms of T or T⋆. Failure after interval t
is a result of a sequence of binary trials unfolding in time (Kalbﬂeisch and Prentice
2002, p. 9):
G(t) = P(T > t) = P(T ̸= 1∩T ̸= 2...∩T ̸= t) =
P(T ̸= 1)P(T ̸= 2|T ̸= 1)P(T ̸= 3|T ̸= 1,T ̸= 2)...P(T ̸= t|T ̸= 1,...,T ̸= t −1).
The probability P(T ̸= x|T ̸= x−1) is given by 1−h(x), it follows that in analogy
to the continuous case the survivor function can be expressed in terms of the hazard

8
2 Basic Concepts of Failure Time Analysis
rate:
G(t) =
t
∏
j=1
(1−h( j)).
Assuming grouped failure times might not be appropriate in all cases, as some
random variables are intrinsically discrete. Some helpful results follow from this
assumption however, and estimation is easier by deriving inferences on the like-
lihood contributions following from 2.2, leading to an identical modeling frame-
work.
2.3 Likelihood Construction
Failure time data have some special characteristics which have to be accounted
for in the construction of the likelihood. A failure time is referred to as censored
when the actual failure time is not observed but it is only known to fall into an
interval. Failure times are left-truncated if they are only observable if they exceed
a truncation time. Time varying covariates are often available in the data set. In
the following sections, based on Klein and Moeschberger (2003, pp. 63-77), it will
be clariﬁed how these conditions are accounted for in the formulation of the likeli-
hood. Conceptually, these adjustments can be represented in an uniﬁed framework
by varying the likelihood contributions. As a consequence, the likelihood becomes
more difﬁcult to work with but there are computational methods which simplify
estimation.
2.3.1 Censoring and Truncation
In the presence of right-censoring, the observed failure time for an individual is
ti = min(˙ti,ci).
Here, ˙ti is the true failure time and ci is the censoring time. The indicator variable
vi is deﬁned as
vi =
⎧
⎨
⎩
1
if ti ≤ci,
0
if ti > ci,

2.3 Likelihood Construction
9
and is usually referred to as censoring indicator. The available data is given by:
D = {(ti,δi,zzz⊤
i )n
i=1}.
Here, zzzi = (zi1,zi2,..)⊤is the vector of covariates of individual i. To proceed, it is
necessary to make assumptions about the process generating the censoring times.
Under the assumption of random censoring, C1,...,Cn are i.i.d. random variables
with survivor function S() and pdf s() depending on φφφ, independent of T1,...,Tn and
each other. Let θθθ be the parameter vector of interest on which the survivor function
of T1,...,Tn depends. Under random censoring, the full likelihood contribution L⋆
i
is
L⋆
i = G(ti|zzzi) = P(Ti > ti|zzzi)s(ti)
for a right-censored failure time and
L⋆
i = G(ti|zzzi)h(ti|zzzi)S(ti) = f(ti|zzzi)S(ti)
for a completely observed failure time. Under noninformative censoring, we have
G(ti|θθθ,φφφ) = G(ti|θθθ). Further we assume that T1,...,Tn are i.i.d. or independent
given the covariates. Under those assumptions, the likelihood is given by:
L(θθθ|D) = c
n
∏
i=1
h(ti|zzzi,θθθ)viG(ti|zzzi,θθθ)
∝
n
∏
i=1
Li,
where c = ∏n
i=1 S(ti)vis(ti)1−vi is a multiplicative constant and
Li = h(ti|zzzi,θθθ)viG(ti|zzzi,θθθ).
For Bayesian analysis, the assumption f(θθθ,φφφ) = f(θθθ) f(φφφ) is also necessary so
that f(φφφ) factors out of the posterior. In this thesis it is always assumed that
censoring is random and noninformative. For discrete failure time data a failure
indicator y is introduced for every interval before and including the failure time,

10
2 Basic Concepts of Failure Time Analysis
so that P(yij = h( j|zzzi)):
yij =
⎧
⎨
⎩
1
if individual i fails in interval [a j−1,a j),
0
if ti > a j.
Assuming that censoring occurs at the end of the interval, the likelihood contribu-
tion of an uncensored and a right-censored individual respectively equal
ti
∏
i=1
P(yij = 0)1−yijP(yi j = 1)yij = h(ti|zzzi)
ti−1
∏
i=1
(1−h(i|zzzi)) =
h(ti|zzzi)G(ti −1|zzzi) = f(ti|zzzi),
and
ti
∏
i=1
P(yi j = 0) =
ti
∏
i=1
(1−h(j|zzzi)) = G(ti|zzzi).
The likelihood is:
L(θ|D) =
n
∏
i=1
ti
∏
j=1
(1−h(j|zzzi))1−yi jh( j|zzzi)yij,
which is the likelihood of a Bernoulli distribution. The same result would have
been obtained for an intrinsically discrete random variable. Discrete failure times
can be analyzed by methods for this distribution. In practice, this is achieved by
changing a data set given in the usual form for failure time analysis 2.1 into the lon-
gitudinal form 2.2. Under interval censoring, it is only known that failure occurred
Table 2.1: Discrete data
id
t
z1
v
1
3
9
1
2
2
12
0
during the interval [li,ri). Interval-censoring can be viewed as generalization of
right-censoring, the interval corresponding to right-censoring is [li,∞), while by

2.3 Likelihood Construction
11
Table 2.2: Discrete data - longitudinal
id
y
z1
1
0
9
1
0
9
1
1
9
2
0
12
2
0
12
convention the interval of a uncensored individual is [li = ti,ri = ti). For continu-
ous time, the interval can be set to (li,ri] [li,ri] or (li,ri) as the same information
about the failure time is represented (Sun 2006, p. 15), this is not true for discrete
time. In this thesis, the notation [li,ri) is used. Grouped failure times are a special
case of interval-censoring where all no intervals overlap (Lawless 2003, p. 64).
The available data is given by
D = {([li,ri),zzz⊤
i )n
i=1}.
The likelihood contribution is given by:
Li = P(li ≤Ti < ri|zzzi) = G(li|zzzi)−G(ri|zzzi).
Under left-truncation, a failure time can only be observed if it exceeds a truncation
time tri. Analysis of those cases proceeds by conditioning on failure after tri. For
example, for continuous time, the contribution of an uncensored, left-truncated
individual is:
f(ti|zzzi)
S(tri|zzzi) = h(t|zzzi)exp(−
 ti
0 h(t|zzzi)dt
exp(−
 tri
0 h(t|zzzi)dt
= h(t|zzzi)exp(−
 ti
tri
h(t|zzzi)dt.

12
2 Basic Concepts of Failure Time Analysis
For discrete time:
f(ti|zzzi)
S(tri|zzzi) =
h(ti|zzzi)∏
ti−1
j=1(1−h( j|zzzi))
∏tri
j=1(1−h( j|zzzi))
= h(ti|zzzi)
ti−1
∏
j=tri+1
(1−h( j|zzzi)),
so conveniently by deleting failure indicators up to and including the truncation
time, the likelihood contribution is correct. Combining the concepts, the data is
given by
{(li,ri,tri,zzzi)n
i=1}.
2.3.2 Time Varying Covariates
For covariates depending on time a distinction must be made between internal and
external covariates (Kalbﬂeisch and Prentice 2002, pp. 196–199): For the former
case it holds that
h(t|ZZZ(t),βββ,T ≥u) = h(t|ZZZ(t),βββ,T = u),0 < u ≤t,
this implies that the covariates ZZZ(t) affect the hazard, but failure does not affect
the covariate path ZZZ(t). Internal covariates are those that are directly involved with
failure: As such, G(t|ZZZ(t),βββ) can no longer be interpreted as a survivor function.
Inclusion of internal covariates is problematic and here attention is restricted to
external covariates. For example in the context of unemployment durations, an
example of an internal covariate would be the amount of unemployment bene-
ﬁts an individual receives. Given that the amount is > 0 at month t, we have1
G(T > t|ZZZ(t)) = 1. A (strictly seen, approximate) external variable might be the
current rate of unemployment. Treatment of time varying covariates proceeds by
taking them to be a stochastic process, an important property of which is pre-
dictability; informally, covariates are predictable if the values which explain vari-
ation in the hazard rate at time t are (to the researcher) known at an inﬁnitesimal
short moment before t (Berg 2001). The author gives as an example the case of an
individual making a decision, e.g. accepting a job offer, under the anticipation of
1A model could in fact not even be ﬁt with usual procedures here because of perfect seperation.

2.3 Likelihood Construction
13
the realization of T. If this is unknown to the analyst, predictability is not given.
In this thesis it is always assumed that covariates are predictable. Under this as-
sumption - given regularity conditions - standard methods can be used with time
varying covariates for relative risk models introduced in the next section.
While general sampling paths are possible for ZZZ(t), changes in covariates values
are usually observed at discrete points (t0 = 0 < t1 < ... < tni,k). The survivor func-
tion of an individual can be written as product of conditional survivor functions
without changing the likelihood:
G(ti|zzzi(t)) = exp{−
 t
0 h(u|zzzi(u))du} = exp{−
 t1
t0
h(u|zzzi(u))du−
 t2
t1
h(u|zzzi(u))du−...−
 tni,k
tnk−1 h(u|zzzi(u))du}
=
ni,k
∏
j=1
G(t j|tj > t j−1,zzzi(j)).
Every term G(tj|tj > t j−1,zzzi(j)) is the likelihood contribution of an right-censored
individual with covariate zzzi(j) and failure time t j left-truncated at tj−1. As a conse-
quence, the data set can be adjusted by a process called episode splitting (Blossfeld
and Rohwer 2002, pp. 140–142), which can be seen in table 2.3. On the left is the
Table 2.3: Episode splitting
id
t
v
z1
tl
id
t
v
z1
z2
tl
1
5
1
3
0
1
3
0
3
11
0
2
7
0
5
0
1
5
1
3
15
3
2
2
0
5
9
0
2
5
0
5
4
2
2
7
0
5
6
5
data set before, on the right the data set after episode splitting. After the split, the
time varying covariate zzz2 can be included. For discrete time, time-varying covari-
ates can be inluded by varying the covariate values across the intervals.

14
2 Basic Concepts of Failure Time Analysis
2.4 Relative Risk and Log-Location-Scale Family
The Cox model or relative risk model, due to Cox (1972) is probably the most often
used model for failure time modeling in continuous time. For models belonging to
the relative risk family, the hazard rate is speciﬁed as:
h(t|zzz(t),βββ) = h0(t)exp(βββ Tzzz(t)),
(2.6)
implying a loglinear model for the hazard rate:
logh(t|zzz(t),βββ) = logh0(t)+zzz(t)⊤βββ.
Here, the hazard rate consists of the baseline hazard h0(t), corresponding to zzz = 000
on which the covariates act multiplicatively by exp(βββ ⊤zzz(t)). For time-constant
covariates the model 2.6 is also known as proportional hazards model. In this
case, the ratio of hazards of two individuals with covariate vectors zzzi and zzz j is
hi(t|zzzi,β)
hj(t|zzzj,β) = h0(t)exp(βββ ⊤zzzi)
h0(t)exp(βββ ⊤zzz j) = exp(βββ ⊤(zzzi −zzz j)),
(2.7)
so hi(t|zzzi) ∝hj(t|zzzj), independent of t. A one unit change in covariate zp - given the
other covariates are ﬁxed - corresponds to multiplication of the hazard rate by the
factor exp(βp). This factor is also called the hazard ratio or relative risk of covari-
ate p (Aalen et al. 2008, p. 9) as it gives the ratio 2.7 if zzzi −zzz j = (0,..,0,1,0,..,0)⊤,
where the one is in position p. Frequentist inference is often based on the partial
likelihood, which is free of the baseline hazard:
Lp(β|D) =
d
∏
i=1
exp(zzziβββ)
∑j∈R(i) exp(zzzjβββ),
(2.8)
t(1),t(2),...,t(d) are the ordered failure times and
R(x) = {i : ti ≥x}

2.4 Relative Risk and Log-Location-Scale Family
15
is the risk set at time x. The estimator derived by maximization of 2.8 has proper-
ties that are very close to the optimality of maximum likelihood estimators. This
- in addition to the simplicity of the model - explains the central role hold by the
relative risk model in failure time modeling. As shown by Kalbﬂeisch (1979) by
marginalizing over the baseline hazard a marginal posterior proportional to the par-
tial likelihood is obtained if a gamma process - a stochastic process with gamma
distributed increments - is used as prior for H0 . Much literature on Bayesian
failure time analysis is based on this approach - under which the baseline hazard
is mainly viewed as nuisance parameter to be averaged over - motivated by the
properties of the partial likelihood. This thesis is instead concerned with models
based on the full posterior where the baseline hazard is estimated from the data.
In this thesis, extensions of the basic Cox model with the following properties are
discussed:
• The baseline hazard is either completely speciﬁed or the log-baseline is ﬂex-
ibly modeled via P-splines so that inferences are always based on the full
posterior.
• Effects of the form z⊤
i jβ j can be generalized to smooth functions f j(zij).
• Time-dependent covariates and time-dependent effects can be included so
that the often restrictive proportionality property is not satisﬁed.
Including these extensions is much easier working with the full posterior instead
of the partial posterior (Fahrmeir et al. 2013, p. 430). Furthermore, very often
an estimate of the baseline hazard is needed - e.g. for the prediction of survival
probabilites - or the shape of the baseline hazard is of main interest. For example in
econometric applications, there is often great interest if negative (dh(t)/dt < 0) or
positive (dh(t)/dt > 0) duration dependence prevails (Heckman and Singer 1984).
Another important model family for continuous time is the log-location-scale
family (Lawless 2003, pp. 26–28). It consists of distributions where Y = logT can
be written as
Y = logT = μ +σW,

16
2 Basic Concepts of Failure Time Analysis
where μ is a location parameter, for regression analysis taken as ZZZ⊤βββ, σ is a scale
parameter and W has density f0, belonging to the location-scale family of the form
fW(y) = 1
σ f0(y−μ
σ
).
By the relation 2.4 regression coefﬁcients can be interpreted directly as marginal
effects in terms of the conditional expectation of Y, or in terms of T via the factor
exp(β j). The effect of zj is to accelerate or slow down time until failure, therefore
models of this form are also called accelerated failure time models (AFT). This
effect can be understood to be relative to an individual with covariate vector 000.
The survivor function of T can be represented in terms of the survivor Function of
W, denoted G0 in analogy to the relative risk model, as it corresponds to zzzi = 000:
P(T > t|z,βββ) = P(exp(ZZZ⊤βββ)exp(σW)) > t)
= P(W > logt −μ
σ
) = G0(logt −μ
σ
).
The only intersection of the log-location and relative risk family is the Weibull
distribution and by proxy the exponential distribution, which is a special case of
the Weibull.
This thesis is focused mainly on the relative risk family and only inference for
the lognormal (and some extensions based on it) and the Weibull distribution from
the family is discussed. For the lognormal model discussion is restricted to time-
constant covariates and time-constant effects as inclusion of those is a very special-
ized topic and rarely done, but inclusion of smooth effects f j(zij) will be discussed.

3 Computation and Inference
Bayesian inference is based on the posterior distribution of the parameters θθθ =
(θ1,θ2,...)⊤, given by
f(θθθ|D) = cL(θθθ|D)f(θθθ),
with normalizing constant c = [
 L(θθθ|D)f(θθθ)dθθθ]−1. In rare cases c can be given
in closed form. In most cases the posterior distribution can not be normalized and
inference is based on a simulation of the posterior distribution, usually obtained
via Markov chain simulation Monte Carlo (MCMC) methods.
The purpose of this chapter is to give an overview over computational and in-
ferential methods relevant for this thesis.
3.1 MCMC
Following Roberts (1996, pp. 205–209) a Markov chain is a sequence of random
variables X0,X1,... where the distribution of Xt given Xt−1,Xt−2,...,X0 only de-
pends on Xt−1, so that
Xt|Xt−1,Xt−2,...,X0 ∼K(Xt|Xt−1).
The conditional probability density K(Xt|Xt−1) is referred to as transition kernel.
If the marginal distribution of Xt+1 is π, so that

K(Xt+1|Xt)π(Xt)dXt = π(Xt+1)
holds (given that Xt ∼π), the chain has stationary distribution π; implying that
if one sample is obtained from π, all subsequent samples are from π. For this to
be the case the transition kernel has to allow free moves over the state space. For
Bayesian inference the transition kernel is chosen so that π equals the posterior
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_3, © Springer Fachmedien Wiesbaden 2015

18
3 Computation and Inference
Algorithm 1: Metropolis-Hastings algorithm
1 Choose start values (θθθ (0)
1 ,...,θθθ (0)
k )
2 Draw a proposal θθθ ⋆from g(θθθ ⋆|θθθ (s−1))
3 Set θθθ (s+1) = θθθ ⋆with probability α(θθθ (s−1),θθθ ⋆),θθθ (s−1)) and θθθ (s+1) = θθθ (s) with
probability 1-α(θθθ (s−1),θθθ ⋆), where
α(θθθ (s−1),θθθ ⋆) = min

1,
π(θθθ ⋆|y)g(θθθ (s−1)|θθθ ⋆)
π(θθθ (s−1)|y)g(θθθ ⋆|θθθ (s−1))

4 Set s=s+1 and move to step 2
distribution.
If certain properties are met by the chain, averages S−1 ∑S
i=1 h(xi), where S de-
notes the chainlength, converge to expectation Eπ[h(Xi)]. These properties - which
for the methods used here are usually met - and deeper coverage of MCMC theory
are given e.g. in Tierney (1996) or Robert and Casella (2004).
3.1.1 Metropolis-Hastings
The Metropolis-Hastings (MH) algorithm is an algorithm that allows samling from
a density that is only known up to a proportionality constant. In pseudocode type
notation, the algorithm is given by algorithm 1.
Proportionality constants that
are independent of θθθ cancel out in the ratio α(·,·) and need not to be known.
The transition kernel of the MH-algorithm at iteration s decomposes into a term
corresponding to acceptance, and a term corresponding to rejection of a draw:
K(X(t+1)|X(t)) =π(X(t+1)|X⋆)α(X(t+1),X⋆)+
I[X(s+1) = X⋆]{1−

α(X(t+1),X⋆)π(X⋆|X(t))dX⋆}.
It can be shown that
π(Xt)K(X⋆|X(t)) = π(X⋆)K(X(t)|X⋆)
(3.1)

3.1 MCMC
19
Algorithm 2: Gibbs sampler
1 Choose start values (θθθ (0)
1 ,...,θθθ (0)
k )
2
sample θθθ (s)
1 ∼f(θθθ 1|θθθ (s−1)
2
,...,θθθ (s−1)
k
,D)
sample θθθ (s)
2 ∼f(θθθ 2|θθθ (s)
1 ,θθθ (s−1)
3
,...,θθθ (s−1)
k
,D)
...
sample θθθ (s)
k
∼f(θθθ k|θθθ (s)
1 ,...,θθθ (s)
k−1,D)
3 Set s=s+1 and move to step 2
holds. By integrating 3.1 in Xt it can be proofed that the stationary distribution is
π:

π(Xt)K(X⋆|X(t)) = π(X⋆).
The choice of proposal density is crucial for the speed of convergence and the mix-
ing of the chain (Rosenthal 2011): A Markov chain with transition kernel P1 mixes
faster compared to a Markov chain with transition kernel P2 if EP1(Xn −Xn−1)2 >
EP2(Xn −Xn−1)2. Faster mixing also implies lower variance for functionals of
π as autocorrelation is lower. In general there is a tradeoff between high accep-
tance rates and good coverage of the parameter space (Gamerman and Lopes 2006,
p. 196). A method to bypass this tradeoff is to use a proposal density that approx-
imates the target distribution. Here, a high acceptance rate is an indicator for a
good approximation. Most Metropolis-Hastings sampler used in this thesis take
this approach.
3.1.2 Gibbs Sampler
To obtain a sample of θθθ from π, the Gibbs sampler proceeds by sequentially sam-
pling from the full conditional distributions of parameter blocks θθθ i,i = 1,...,k
formed from θθθ. The Gibbs sampler is given by algorithm 2.
For the full condi-
tional distribution of block i conditional on θθθ excluding θθθ i the notation f(θθθ i|·,D)

20
3 Computation and Inference
will be used. The Gibbs sampler can be seen as a special case of the MH-algorithm,
in which the proposal distribution for θθθ i is the full conditional distribution. In-
serting f(θθθ i|·,D) in α(θθθ (⋆)|θθθ (s−1)) gives an acceptance probability of 1 for all i.
Optimally the blocks consist of correlated parameters.
For some (conditional conjugate) distributions draws from f(θθθ i|·,D) can be
obtained directly. For conditional distribution for which this is not the case a
Metropolis-Hastings subchain can be used to obtain draws from the full condi-
tionals. This will be often the case for the models in this thesis. A subchain of
length one is sufﬁcient, as for this choice the joint distribution converges to the
stationary distribution, and further exploration of the full conditional would not
necessarily accelerate convergence (Robert and Casella 2004, p. 393).
An important type of Gibbs sampler for failure time modeling is based on the data
augmentation algorithm by Tanner and Wong (1987). Here, the parameter space is
augmented by latent variables Z1,...,Zp so that
f(θθθ|D) =

g(θθθ,ZZZ|D)dZZZ,
holds, so f(θθθ|D) can be expressed as EZZZ[g(θθθ,ZZZ|D)]. The pdf g(((θθθ,ZZZ|D) is called
a completion of f(θθθ|D). There are inﬁnite possible completitions for a given den-
sity, g is chosen so that sampling from the full conditionals g(θθθ i|ZZZ,D) is simpliﬁed
(Robert and Casella 2004, p. 374). Completion Gibbs sampler are often used for
missing data problems where g(·|D) is taken as f(yobserved,ymissing|D). Censor-
ing can be seen as special case of missing data, where the available data provides
information about the range of missing values (Sun 2006, p. 249). Another impor-
tant application for data augmentation are discrete failure time models, where the
latent data is given by an underlying continuous variable, determining the value of
the binary indicator.
3.2 Inference from Simulation Output
The simulation of the posterior distribution must be summarized in an appropriate
manner. Point estimators for a parameter θ ∈Θ can be motivated from decision

3.2 Inference from Simulation Output
21
theory by deﬁning a loss function L(γ,θ), and minimizing the posterior expected
loss

Θ L(γ,θ)f(θ|D)dθ.
(3.2)
It can be shown that the minimizer of 3.2 equals the minimizer of the integrated
risk

Θ

X f(x|θ)L(q(x),θ)dx f(θ)dθ,
where x ∈X and q(x) is an estimate of θ.
Table 3.1: Loss functions and corresponding estimators
L(γ,θ)
Bayes estimator
(θ −γ)2
Posterior mean
|θ −γ|
Posterior median
I[γ ̸= θ]
Posterior mode1
Such a minimizer is called a Bayes estimator (Robert 2001, pp. 52–54). Dif-
ferent loss functions correspond to different Bayes estimators. The most common
ones are given in table 3.1. The corresponding Bayes estimators can be obtained
from MCMC output as empirical counterpart.
To communicate estimation uncertainty the interval containing 100(1−α)% of
the posterior density is often reported. This interval can be directly obtained from
MCMC output, invariant to one-to-one transformations of the parameter, and is
directly interpretable. An alternative is the set of highest posterior density, for a
given α deﬁned as the set C = {θ ∈Θ : f(θ|D) ≥k(α)}, where k(α) is the largest
constant satisfying
f(C|D) ≥1−α
(Carlin and Louis 2011, p. 49). C is more difﬁcult to determine but usually gives
a shorter interval (given that the set can be described as interval). MCMC output
is correlated, a measure for the resulting loss in precision is given by the effective
sample size ne f f (Kass et al. 1998). This measure would equal the number of draws
1More precise, the corresponding loss function is deﬁned as a sequence of losses, as

Θ I[γ ̸=
θ]π(θ|x) = 1 (Robert 2001, p. 166).

22
3 Computation and Inference
if there would be no autocorrelation. It is deﬁned as:
ne f f =
S
1+2∑∞
t=1 ρt
,
(3.3)
where ρt is the autocorrelation at lag t. A simple method to estimate the denom-
inator of 3.3 is to use all empirical autocorrelations larger than a cutoff, say 0.1
(Carlin and Louis 2011, p. 151). Furthermore convergence of the chain should be
monitored. Convergence diagnostics seems to be rarely discussed in the context
of the models used here. It should be noted that the sampling schemes discussed
here have empirically shown to work well in practice. Overviews of convergence
assessment methods are given in Cowles and Carlin (1996) and Robert and Casella
(2004, pp. 459-510).
3.3 Model Diagnostics and Comparison
Bayesian methods for model diagnostics and assessment are an active and unset-
tled area of research. Most methods are based on assessing prediction quality,
either by directly evaluating point or probabilistic prediction, or in terms of the
predictive distribution

p(ttt|θθθ)f(θθθ|DDD)dθθθ
(3.4)
(Gelman et al. 2013). The ﬁrst approach is harder to implement for failure time
data; for censored observations predicted failure times can not be directly com-
pared to an observed value. In addition, for models based on the Cox model,
obtaining draws from the predictive distribution can be difﬁcult. Usually,
p(ttt|θθθ) =
n
∏
i=1
p(ti|θθθ,zzzi)
in equation 3.4 is a probability density function. For failure time data, p(ti|θθθ,zzzi) is
given for discrete failure time data analyzed via failure indicators by
ti
∏
i=1
p(yit = 1|θθθ,zzzit)yit(1−p(yit = 0|θθθ,zzzit))yit,

3.3 Model Diagnostics and Comparison
23
for continuous failure time by
p(ti|θθθ,zzzi) =
⎧
⎨
⎩
G(ti|θθθ,zzzi)h(ti|θθθ,zzzi)vi
if ti is not interval censored
G(li|θθθ,zzzi)−G(ri|θθθ,zzzi)
else.
For some criterions proper priors are necessary. These methods can not be used
as Bayesian P-splines are based on improper priors. For failure time analysis, if
there is qualitative knowledge about the failure process, this can be used for model
assessment by inspecting the shape of the hazard. An overview over methods for
failure time data is presented in this section.
3.3.1 Criterion Based Methods
A criterion based on prediction quality is the L-measure, embedded in the frame-
work of minimizing posterior predictive loss developed by Gelfand and Ghosh
(1998). The L-measure by Ibrahim et al. (2001a) uses a decomposition in predic-
tion variance and a bias term weighted by 0 < u < 1:
L⋆(ttt,u) =
n
∑
i=1
Var(g(ti)rep|yi)+u
n
∑
i=1
(g(ti)−μi)2,
(3.5)
where (ti)rep denotes replications of failure times. The function g(·) is usually
taken as log(·) for failure times are usually heavily skewed, and μi = E[g(trep,i)|ti].
For censored data, 3.5 is replaced by 3.6, here lll and rrr are censoring times.
Lobs(ttt,u) =
  rrr
lll L2(ttt,u) f(g(tttcens)|θθθ)f(θθθ|D)dg(tttcens)dθθθ,
(3.6)
The L-measure can be estimated from MCMC output via:
ˆLobs(ttt,u) =
n
∑
i=1
{1
S
S
∑
j=1
(E[(g(trep, j))2|θj]−ˆμ2
i )}+u{ ∑
{i:yiobs}
( ˆμi −g(ti))2+
1
S
S
∑
j=1
[ ∑
{i:ticens}
(μi −g(trep,is))2].

24
3 Computation and Inference
E[(g(trep,j)) and E[(g(trep, j))2 are often not available in closed form for failure
time data, they can be replaced by their respective average obtained from a MCMC
run. The criterion can be used to compare arbitrary models under right and interval
censoring, as long as draws from the posterior predictive distribution can be ob-
tained. Several methods for obtaining random deviates from failure time random
variables using hazard rates are given by Devroye (1986, pp. 260–285). The sim-
plest method is based on inverting the cumulative hazard. If H(t) can be inverted,
a draw from f(t) = h(t)exp(−H(t)) can be obtained by H−1(e), where e is a draw
from a standard exponential distribution with cdf 1−exp(−t);
P(H−1(e) ≤t) = P(e ≤H(t)) = 1−exp(−H(t)) = F(t).
This can also be used for to sample from f(t|t > l) as necessary for the estimation
of the L-measure in case of right censoring, in this case the hazard is given by
h(t|t > l) = f(t|t > l)/G(t|t > l) = f(t)I[t > l]
G(l)
	G(t)
G(l) = f(t)I[t > l]
G(t)
and H(t|t > l) is
 t
0 h(t|t > l). For interval censoring the conditional survivor
function G(t|t > l,t < r) can be inverted. If H−1 is not available in closed form,
a numerical solution can be used. In general obtaining draws from the predictive
distribution is not always easy or feasible, especially for some nonparametric mod-
els considered here where the log-baseline is modeled via P-splines. Apart from
this difﬁculty the criterion suffers from the disadvantage of using the data twice
for estimation and model evaluation; every data point inﬂuences its own predic-
tion positively, which can lead to an overestimation of prediction accuracy (Hastie
et al. 2009, p. 228). Crossvalidation based criterions, measuring out-of-sample
prediction accuracy avoid this.
The pseudomarginal likelihood (LPMPL) (Geisser and Eddy 1979) is often used
for failure time modeling, it is deﬁned as:
LPML =
n
∑
i=1
log(CPOi),

3.3 Model Diagnostics and Comparison
25
where CPOi is the conditional predictive ordinate
CPOi =

p(ti|θθθ,zzzi)f(θθθ|D−i)dθθθ
for individual i, where D−i denotes the data excluding individual i. The CPOi is
usually estimated by a sample from a single MCMC run. A simple estimator is
(Gelfand 1996):

CPOi =

1
S
n
∑
i=1
1
p(ti|θθθ (s))
−1
,
(3.7)
where θθθ (s) denotes draw s of the MCMC run. The estimator 3.7 can be unstable
for low values of p(ti|θθθ (s)), alternative estimators using importance sampling have
been proposed (Vehtari and Ojanen 2012), although the estimator has been found
to work well in practice by Hanson (2006). It can be shown that asymptotically the
pseudomarginal likelihood contains a penalty term for the number of parameters
similar to the AIC (Gelfand and Dey 1994).
For Bayesian models, a penalty term is necessary that accounts for what is es-
timated from the data and how much information is provided by the prior. The
deviance information criterion (DIC), proposed by Spiegelhalter et al. (2002),con-
tains such a penalty term. It is deﬁned as
DIC = DEV( ˆθˆθˆθ)+ pD
(3.8)
where DEV( ˆθˆθˆθ) = −2log p(ttt| ˆθˆθˆθ) is the deviance at ˆθˆθˆθ, measuring goodness of ﬁt
and the penalty term is
pD = E f(θθθ|D)[DEV]−DEV( ˆθˆθˆθ).
A simple estimate for the penalty term is
ˆpD = 1
S
S
∑
s=1
DEV(θθθ s)−DEV( ˆθˆθˆθ),
(3.9)
plugging 3.9 into 3.8 yields an estimator for the DIC, so that the DIC is very easy

26
3 Computation and Inference
to compute via MCMC output. Under strong prior information ˆpD is much less
than the absolute number of parameters. The likelihood used in failure time mod-
eling is not a joint density, Banerjee and Carlin (2004) argue that under random,
noninformative censoring the likelihood is still appropriate for Kullback-Leitner
divergence on which the DIC is based.
3.3.2 Martingale Residuals
Several types of residuals have been proposed for failure time analysis. Useful for
right-censored data are martingale residuals (MR) which can be derived from an
alternative representation of the likelihood in terms of a stochastic process called
counting process. Counting process theory is not necessary for this thesis but
some rudimentary concepts are necessary to properly introduce martingale residu-
als. Following Therneau and Grambsch (2000), a counting process Ni(t) at time t
gives the number of failures of individuals i in the interval [0,t]. For nonrecurrent
events Ni(t) is either one or zero. Yi(t) = 1[i ∈R(t)] is the at-risk process of indi-
vidual i, indicating membership in the risk set at time t. An individuals counting
process Ni(ti) can be decomposed into the sum of two processes:
Ni(ti) = Ai(ti)+Mi(ti),
Ai(ti) =
⎧
⎨
⎩
 ti
0 Yi(u)hi(u)du
if Ti is continous
∑ti
l=1Yi(l)hi(l)
if Ti is discrete.
Mi(t) is a martingale:
E[Mi(ti)|Fs] = Mi(s),
where Fs is (informally) the data available at time s. The estimated residual process
at time t is Ni(x)−ˆAi(x), the martingale residual ˆMi is estimated by
ˆMi(∞) = vi −ˆAi(ti),

3.4 Bayesian P-Splines
27
where vi is the censoring indicator. The value of ˆMi(∞) represents the difference
between observed and expected number of events for individual i. Martingale
residuals have properties that are similar to residuals from linear regression: Un-
der the true model, they have expectation zero and are uncorrelated. Often the
residuals are used to determine the functional form of a covariate by plotting it
against the covariate, but they can also be used to directly compare models, e.g.
Kneib and Hennerfeind (2008) use kernel densities of estimated martingale resid-
uals at selected points in time to detect patterns of low predictive performance.
3.4 Bayesian P-Splines
3.4.1 Basic Concepts
In this section Bayesian smoothing via penalized splines (P-splines) is introduced
by the example of linear regression with normally distributed errors. This section
is based on Fahrmeir et al. (2013, pp. 413–452), more formal introductions can be
found in Dierckx (2006) or Boor (2001). The Gibbs sampler given in this section
can (with slight adjustments) directly be used for the probit model in the context
of discrete time, and the lognormal for continuous time. Regression models using
P-splines have the structure
yyy = f(xxx)+eee,
(3.10)
where eee ∼N(000,σ 2III). Interest lies in the function f, which is assumed to be
an unknown smooth function of the continuous covariate x. The idea of spline-
smoothing is to approximate f by a polynomial spline of degree l ≥0, with knot
sequence xmin = k1 < k2,.. < km = xmax spread in the domain of x. A function f is
called a polynomial spline of degree h if it satisﬁes the following conditions:
1. f is (l −1) times continuous differentiable
2. f is a polynomial of degree l in every interval [kj,k j+1) determined by the
knots
A spline is a function consisting of local polynomials joined together at the knots
satisfying smoothness restrictions guaranteeing overall smoothness of the spline.

28
3 Computation and Inference
For the special case l = 0 there are no smoothness requirements for f. The overall
smoothness is determined by the degree l, while ﬂexibility is controlled by number
and placement of the knots. Every polynomial spline can be written as a weighted
sum of u=l+m-1 basis functions, which deﬁne the polynomial spline:
f(x) =
u
∑
i=1
Bi(x)ζi.
Given ﬁxed basis functions, estimation can proceed as for classical linear regres-
sion (with a large number of coefﬁcients), where the parameters to be estimated
are given by ζi,i = 1,...u. Probably best known is truncated power series basis (TP
basis):
B1(x) = 1, B2(x) = z,...,Bl+1(x) = zl,
(3.11)
Bl+2(x) = (z−k2)lI[z > k2],...,Bl+m−1 = (z−km−1)lI[z > km−1].
(3.12)
Here, the design-matrix BBB(xxx) for the regression model 3.10 has the form:
⎡
⎢⎢⎢⎢⎢⎣
1
x1
x2
1
...
xl
1
(z1 −k2)lI[z1 > k2]
...
(z1 −km−1)lI[z1 > km−1]
1
x2
x2
2
...
xl
2
(z2 −k2)lI[z2 > k2]
...
(z2 −km−1)lI[z2 > km−1]
...
...
...
...
...
...
1
xn
x2
n
...
xl
n
(zn −k2)lI[zn > k2]
...
(zn −km−1)lI[zn > km−1]
⎤
⎥⎥⎥⎥⎥⎦
.
In general, the design matrix BBB consists of evaluations of the basis functions, so
that BBBij gives the value of the jth basis function, evaluated at the covariate value xi.
Given an estimate for ζζζ, for example the least squares estimate (BBB⊤BBB)−1BBB⊤yyy,
the function value f(·) at covariate value xi is estimated by

f(xi) = x0
i ζ1 +...+xl
iζl+1



term 1
+
(xi −k2)lI[xi > k2]ζl+2 +...+(xi −km−1)lI[z > km−1]ζl+m−1



term 2

3.4 Bayesian P-Splines
29
and the estimated function can be plotted by evaluation of f at a large number of
points in the domain of x and interpolating between the function evaluations. Term
1 one can be understood as a global polynomial from which there are random de-
viations represented by term 2. Bayesian inference for the TPS basis functions
proceeds by treating it like a mixed model, ζ1,...,ζl+1 being viewed as ﬁxed ef-
fects, ζl+2,...,ζl+m−1 being viewed as random effects. The TPS basis is intuitive,
but numerically unstable; as the basis functions are unbounded they can become
numerically imprecise for large covariate values. Additionally, basis functions
become almost linear dependent for knots that are close to each other. A better
alternative is given by B-spline basis functions (see ﬁgure 3.1), given by:
B0
j(x) = I(k j ≤x < k j+1), j = 1,...,u−1
B1
j(x) = x−kj−1
kj −kj−1
I(kj−1 ≤x < k j)+ kj+1 −x
kj+1 −kj
I(kj ≤x < k j+1)
...
Bl
j(x) = x−kj−l
kj −kj−l
Bl−1
j−1(x)+
kj+1 −x
k j+1 −kj+1−l
Bl−1
j
(x).
Due to the recursive deﬁnition, the inner knot sequence k1 < ... < km, must be
expanded to k1−l < ... < km+l. For equidistant knots, the knots are found by
kj = min(x)+( j −1)∗s, j = 1−l,...,m+l,
with interval width s = (max(x)−min(x))/(m−1). Here, knots are always taken
as equidistant knots for reasons that will be made clear in the following. B-spline
functions exhibit the following properties from which their numerical stability fol-
lows:
• Nonnegativity:
B(y) ≥0, for all y and every degree l.
• Local deﬁnition:
B-splines basis function are at maximum positive in an interval spanned by
l+2 knots. Every basis function overlaps with 2l basis functions.

30
3 Computation and Inference
• Normalization:
It holds that ∑u
i=1 B j(xi) = 1,for every xi ∈[min(xxx),max(xxx)]. This implies
that an intercept is implicitly included in the design matrix.
• Bounded range:
B-spline basis functions are bounded from above.
Although B-splines have superior numerical properties compared to the TPS basis,
they deﬁne exactly the same set of functions for a given knot sequence (Fahrmeir
and Kneib 2011, p. 38).
B-Spline basis functions can be derived as normalized differences of TPS basis
functions. For equidistant knots, the formula is (Eilers and Marx 2010):
Bl
j(x) = (−1)l+1Δd+1Bl
j,TPS(x)/(sll!),
where s is the distance between knots and Δd is the difference operator of order d,
deﬁned by:
Δ1ζi = ζi −ζi−1
Δdζi = Δd−1ζi −Δd−1ζi−1.
To avoid overﬁtting, the ﬂexibility of the ﬁt is usually restricted. For frequentist
estimation based on least squares, this is achieved by adding a quadratic penalty
term to the least squares criterion:
c(λ) = (yyy−Bζ
Bζ
Bζ)⊤(yyy−Bζ
Bζ
Bζ)+λζζζ ⊤KKKζζζ,
where λ ∈[0,∞) is a weight term controlling smoothness of the ﬁt and KKK is a
penalty matrix, penalizing rough function estimates. The inﬂuence of λ is visual-
ized in ﬁgure 3.3. O’Sullivan (1986) uses the integrated second derivative of the
ﬁtted curve as penalty term,
p(λ) = λ
 max(x)
min(x) {
n
∑
i=1
f ′′(xi)}2 du,
(3.13)

3.4 Bayesian P-Splines
31
0.00
0.25
0.50
0.75
1.00
1
2
3
4
5
6
7
8
9
10
11
12
x
B(x)
0.00
0.25
0.50
0.75
1.00
1
2
3
4
5
6
7
8
9
10
11
12
x
B(x)
0.0
0.2
0.4
0.6
1
2
3
4
5
6
7
8
9
10
11
12
x
B(x)
0.0
0.2
0.4
0.6
1
2
3
4
5
6
7
8
9
10
11
12
x
B(x)
Figure 3.1: B-Spline basis functions of degree 0,1,2,3.
which can be taken as a measure for the curvature of a function. The penalty 3.13
can be written as ζζζ ⊤KKKζζζ with a penalty matrix KKK where
KKKi j =
 max(x)
min(x) B′′
i (u)B′′
j(u)du.

32
3 Computation and Inference
í



í



Figure 3.2: Function estimation via B-splines. The thick curve is the estimated function, given by the
sum of scaled basis functions depicted in black.
The minimizer of 3.13 is (BBB⊤BBB+λKKK)−1BBB⊤yyy, which under the model
yyy = f(xxx)+εεε = BBB(xxx)ζζζ +εεε
is clearly biased due to the penalty term. In terms of mean squared error this is
generally compensated by the reduction of variance. A minimizer of 3.13 can
be analytically found, even if the criterion is not represented in terms of basis
functions but is instead represented as
(yyy−f(xxx))T(yyy−f(xxx))+ p(λ).
(3.14)
The minimizer of 3.14 is given by a spline having knots at every unique covariate
value of x, called smoothing spline with basis functions very similar to B-Spline
basis functions. Smoothing splines have in general excellent properties, they are
implemented in a Bayesian framework by Hastie and Tibshirani (2000). Due to
the high dimension of the design matrix, computational burden is very high. A
compromise is used by Eilers and Marx (1996), where a penalty is based on the
derivative of the estimated function. The ﬁrst derivative of f(x) for the B-Spline
basis is given by
u
∑
j=1
Bl′
j (x)ζi = −s−1
u
∑
j=1
Δζ j+1Bl−1
j
(x),

3.4 Bayesian P-Splines
33
●●●●●
●●●●●●●●●●●●●
●●●
●●●●●
●
●●
●●
●
●●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●●●
●
●●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●
●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
í100
í50
0
50
0
20
40
60
time (ms)
acceleration (g)
λ=1eí5
●●●●●
●●●●●●●●●●●●●
●●●
●●●●●
●
●●
●●
●
●●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●●●
●
●●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●
●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
í100
í50
0
50
0
20
40
60
time (ms)
acceleration (g)
λ=1
●●●●●
●●●●●●●●●●●●●
●●●
●●●●●
●
●●
●●
●
●●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●●●
●
●●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●●●
●●
●
●
●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
í100
í50
0
50
0
20
40
60
time (ms)
acceleration (g)
λ=1e5
Figure 3.3: Inﬂuence of smoothing parameter. The ﬁrst choice of λ leads to over- the third to under-
ﬁtting. Used is the mcycle data set from R package MASS (Venables and Ripley 2002) on simulated
motorcycle accidents. X-axis is time after impact, Y-axis is acceleration in g.
where s is the distance between the knots. In general it can be shown that the dth
derivative depends on the dth order difference of neighbouring parameters. The
summed squared difference of order d of adjacent coefﬁcients approximates the
integrated square of the dth derivative. This leads to the penalty:
u
∑
i=1
(Δdζi)2 = ζζζ ⊤DDD⊤
d DDDdζζζ = ζζζ ⊤KKKdζζζ,
(3.15)

34
3 Computation and Inference
where DDDt is a difference matrix, indexed by the order of the difference used, and
KKKt = DDD⊤
t DDDt. This combination of basis function and penalty is usually referred to
as P-splines. In analogy to the scalar case, the difference matrices are recursively
deﬁned:
DDDt = DDD1DDDt−1
For instance, the relevant quantities of 3.15 for a difference penalty of order 2 are
given by:
DDD2 =
⎡
⎢⎢⎢⎢⎢⎣
1
−1
1
−1
...
...
1
−1
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
1
−1
1
−1
...
...
1
−1
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
1
−2
1
1
−2
1
...
...
...
1
−2
1
⎤
⎥⎥⎥⎥⎥⎦
,DDD2ζζζ =
⎡
⎢⎢⎣
ζ3 −2ζ2 +ζ1
...
ζK −2ζK−1 +ζK−2
⎤
⎥⎥⎦,
,
KKK2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
−2
1
−2
5
−4
1
1
−4
6
−4
1
...
...
...
1
−4
6
−4
1
1
−4
5
−2
1
−2
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Compared to smoothing splines, the dimension of ζζζ has been reduced from the
number of unique values of x to u = l +m−1, depending on the choice of l and the
number of knots m. A general strategy to choose m is to use a large enough number
of knots so that the data would be overﬁt without the penalty, for example m=30.

3.4 Bayesian P-Splines
35
For equidistant knots, this strategy gives good results in sparse-data situations, as
the estimated function interpolates better as with unequally spaced knots, while in
regions there would hardly be any gain in precision using a larger number of knots
(Eilers and Marx 2010). As such, the simple choice of equdistant knots gives good
results. For nonequdistant knots the penalty matrix KKK = DDD⊤DDD can be adjusted to
include weights via KKK = DDD⊤W
W
WDDD.
To transfer the difference penalty 3.15 to a Bayesian framework, the difference
penalty can be interpreted as prior knowledge regarding the smoothness of the
function f(·). A smoothness prior for ζζζ has been developed by Lang and Brezger
(2004) in the form of the random walk priors. The difference penalty of order d
translates to a random walk of order d of the regression coefﬁcient:
ζk = Δdζk +uk,
with uk ∼N(0,ξ 2) and diffuse priors ∝c for ζ1,...ζd. e.g. for a random walk of
order 1 and 2:
ζk = ζk−1 +uk, ζk = 2ζk−1 −ζk−2 +uk.
This prior can be written as:
ζζζ|ξ 2 ∝( 1
ξ 2 )rank(KKK)/2 exp(−1
2ξ 2ζζζ ⊤KKKζζζ)
(3.16)
The prior 3.16 is improper as a result of the diffuse prior for the initial values. It has
rank(KKK) = k−d. All columns and rows of the penalty matrix K add up to 000. It can
be shown that the nullspace of the penalty matrix is spanned by global polynomial
of order d-1, which therefore is not penalized (Kneib 2006, p. 36). The global vari-
ance parameter ξ 2 controls the smoothness of the ﬁt. For limξ 2 →0, no deviations
from the global polynomial would be allowed, for a difference penalty of order 2,
a straight line would be obtained. For limξ 2 →∞, the function estimate would
become extremly rough and would ﬁt the data at hand extremly well. As prior
knowledge about ξ 2 is usually rare, ξ 2 is assigned a hyperprior, so the smooth-
ing parameter is estimated from the data. A conditional conjugate inverse gamma
hyperprior ξ 2 ∼IG(aξ
0,bξ
0) is normally used. Very low hyperparameters aξ
0,bξ
0,

36
3 Computation and Inference
e.g. 0.001 for both are usually chosen. A diffuse prior for the variance parameter
would result in a improper posterior. For unknown σ2, the prior σ2 ∼IG(aσ
0 ,bσ
0 )
is a convenient choice as it is conditionally conjugate. Under the Gaussian like-
lihood the resulting full conditionals are all conjugate. A Gibbs sampler for the
model parameters of 3.10 alternates between sampling from the conditional dis-
tributions of the regression coefﬁcients, the smoothing variance, and the residual
variance. The full conditional of ζζζ is
N( 1
σ2BBB⊤BBB+ 1
ξ 2KKK)−1 1
σ2BBB⊤yyy,( 1
σ2BBB⊤BBB+ 1
ξ 2KKK)−1).
(3.17)
The full conditional distributions for σ2 and ξ 2 are both IG(·,·), with parameters
aσ
0 +n/2,bσ
0 +εεε⊤εεε/2
for the residual variance, and
aξ
0 +rank(KKK)/2,bξ
0 +ζζζ ⊤KKKζζζ/2
for the smoothing variance. The computation of the moments of 3.17 can be done
very quickly: Due to the local deﬁnition of the B-splines, the design matrix BBB
is sparse and the precision matrix ΣΣΣ−1
ζζζ
=
1
σ2BBB⊤BBB + 1
ξ 2KKK is a band matrix with
bandwidth max(d,l) where h is the degree of the spline and d is the order of the
random walk. Specialized algorithms can be used to quickly invert ΣΣΣ−1
ζζζ
which
is highly relevant for an iterative algorithm such as the Gibbs sampler. In the
following section, extensions from univariate smoothing are introduced.
3.4.2 Extended Linear Predictor
The model 3.10 can be written in terms of the linear predictor as:
yyy = ηηη +εεε
ηηη = BBBζζζ.

3.4 Bayesian P-Splines
37
Extensions of the model can be represented in terms of extensions of the linear
predictor. Following Fahrmeir et al. (2013, p. 183) the linear predictor including
all effects can generically be written as
ηηη = ZZZβββ +W
W
Wααα + f1(xxx1)+...+ fr(xxxr) =
ZZZβββ +W
W
Wααα +BBB1ζζζ 1 +....BBBrζζζ r.
(3.18)
where ZZZβββ are ﬁxed effects, W
W
W = bdiag(W
W
W 1,...,W
W
W p) is the design matrix corre-
sponding to random effects ααα = (ααα⊤
1 ,...,ααα⊤
p )⊤, BBBiζζζ i,i = 1,...,m are effects repre-
sented via basis functions, B-spline or other. Description of estimation techniques
is easier and conciser via this representation. The extensions will be discussed in
detail in the following.
3.4.2.1 Fixed and Random Effects
For Bayesian inference all parameter are taken as random variables so that the
distinction between ﬁxed and random effects is somewhat misleading, as these
labels are common in the literature they are nonetheless used here.
For ﬁxed effects ZZZβββ, the prior distributions βββ ∼N(ttt,TTT) and diffuse priors βββ ∝
constant are common. ZZZ is the usual design matrix for ﬁxed effects. The full
conditional is multivariate normal with variance
ΣΣΣβββ = σ2(ZZZ⊤ZZZ +σ2TTT −1)−1
and expectation
μμμβββ = ΣΣΣβββ/σ2(ZZZ⊤[yyy−ηηη +ZZZβββ]+σ2TTT −1ttt),
(Fahrmeir and Kneib 2011, p. 213). The case TTT −1 = 000 corresponds to a diffuse
prior. To account for cluster-speciﬁc heterogenity, Gaussian random effects can be
included in the model. These can also be brought into the basis function framework
by viewing e.g. a random intercept αi as function of a index variable x⋆
j, taking
values 1,...,p, so that g(x⋆
i j) = αi, see Brezger and Lang (2006), but the classical

38
3 Computation and Inference
perspective is taken here.
For random effects, the prior αααi ∼N(000,QQQ),i = 1,..., p can be used. An inverse
Wishart hyperprior denoted by IW(a0,BBB0) for the covariance matrix QQQ is condi-
tionally conjugate and allows within-cluster correlation of random effects, while
random effects are indendent across clusters. Gyperparameters are a0 and BBB. The
full conditonal distribution of random effects αααi is of the exact same form as for
ﬁxed effects, the distribution is N(μμμαααi,ΣΣΣαααi) where
ΣΣΣαααi = σ2(W
W
W ⊤
i W
W
W i +σ2QQQ−1)−1
and
μμμαααi = ΣΣΣαααi/σ2(ZZZ⊤
i [yyyi −ηηηi +W
W
W iαααi]+σ2QQQ−1).
The vector ααα could also be updated in one large block, but this is not necessary
as random effects are conditionally independent across individuals (Gamerman
1997). The full conditional distribution of QQQ is IW(a⋆,BBB⋆), where
a⋆= a0 + p/2, BBB⋆= BBB0 + 1
2
p
∑
i=1
αααiααα⊤
i .
As an alternative, QQQ can be taken as diag(υ1,...,υq). Using the prior
υi ∼IG(aαi
0 ,bαi
0 ),
the full conditionals are
υi ∼IG(aαi
0 + p/2,bαi
0 + 1
2
n
∑
i=1
α2
ij)),
for j=1,...p, where αi j is the jth element of αi. There are parallels between random
effects and P-spline coefﬁcients. It can be shown that for a model
yyy = BBB(xxx)ζζζ
(3.19)
with a prior of the form f(ζζζ|ξ 2) ∝exp{−1
2ξ 2ζζζKKKζζζ}, where KKK is rank deﬁcient, and

3.4 Bayesian P-Splines
39
BBB(xxx) is matrix of B-spline basis functions, the model 3.19 can be written as mixed
model
yyy =UUUγγγ +GGGυυυ,
where γγγ are ﬁxed and υυυ are random effects. For the B-Spline basis, the random
effects υυυ represent deviations from a global polynomial described by UUUγγγ, in com-
plete analogy to the TPS-basis. It follows that var(yyy|UUUγγγ) is not a diagonal matrix,
so that B-splines marginally induce correlation (Fahrmeir and Kneib 2011, p. 230).
For Gaussian yyy, cov(yi,y j|UUUγγγ) can be given in closed form, in general the correla-
tion is a function of the closeness of covariate values xi,x j.
The marginal and conditional hazard usually differ under the presence of ran-
dom effects, as do the marginal and conditional hazard, so that parameter inter-
pretation differs. This is a specialized topic for failure time analysis and this point
will not be stressed in this thesis, in general the implementation of random ef-
fects, in this context called frailties will be mentioned and implicitly a conditional
perspective is taken.
3.4.2.2 Multiple Functions f1(xxx1)+...+ fr(xxxr)
Models with linear predictor containing several functions f1,...fr have an idenﬁ-
cation problem: As the linear predictor
ηηη⋆= f1(xxx1)⋆+... fr(xxxr)⋆, with f1(xxx1)⋆= f1(xxx1)+c, f2(xxx2)⋆+c = f2(xxx2)−c
equals the linear predictor
ηηη = f1(xxx1)+...+ fr(xxxr),
the mean level of the functions f1(xxx1) + f2(xxx2)+,...,+ fr(xxxr) is not identiﬁed and
identiﬁcation constraints must be imposed. Using constraints of the form CCC jζζζ j =
000, the prior 3.16 can be adjusted to
ζζζ j|ξ 2
j ∝( 1
ξ 2
j
)rank(KKK j)/2 exp(−1
2ξ 2
j
ζζζ ⊤
j KKKζζζ j)I[CCC jζζζ j = 0], for j = 1,...,r,
(3.20)

40
3 Computation and Inference
(Lang et al. 2014). A natural choice is CCC j = BBB j(xj) so that
n
∑
i=1
ˆf1(x1i) =
n
∑
i=1
ˆf2(x2i) = ... =
n
∑
i=1
ˆfr(xri) = 0.
(3.21)
Here, the functions f j(·), j = 1,..,r represent deviations from the global mean
set by the intercept βββ 0 which is always assumed to be included. For the normal
likelihood, the conditional distribution of ζζζ j is still normal with moments 3.22,
constrained to CCC jζζζ j = 000. In general, draws from
xxx|μμμ,QQQ−1 ∝exp(−1
2

xxx−μμμ)⊤QQQ(xxx−μμμ)

I[AAAxxx = 000],
can be generated by the following steps (Rue and Held 2005, pp. 37–39):
1. Sample xxx⋆from N(μμμ,QQQ−1).
2. Compute xxx = xxx⋆−QQQ−1AAA⊤(AAAQQQ−1AAA⊤)−1AAAxxx .
For the restriction 3.21, step 2 can be done quickly as AAA = 111⊤BBBi is a row vector.
Another, perhaps less elegant approach, is to center the functions in every iteration
of the Gibbs sampler. The mean of each function subsequently is added to the
intercept so that the posterior is unchanged. The full conditional of all regression
coefﬁcientes corresponding to basis function matrices BBB j(xxx j) is given by:
ΣΣΣ j = ( 1
σ2BBB⊤
j BBB j + 1
ξ 2
j
KKK j)−1,μμμ j = ΣΣΣ j
1
σ2
j
BBB⊤
j [yyy−ηηη +BBBj(xxx j)ζζζ j],
(3.22)
(Lang and Brezger 2004). The full conditional distribution of ξ 2
j is
ξ 2
j ∼IG(a
ξ j
0 + rank(KKK j)
2
,b
ξj
0 + 1
2ζζζ ⊤
j KKK jζζζ j).
Some further extensions useful for failure time analysis can be included by varying
the elements of BBB j and KKK j which are given in the following.

3.4 Bayesian P-Splines
41
Varying Coefﬁcients
Varying coefﬁcients have the form
zzz1βββ 1(xxx1)+...+zzzkβββ k(xxxk)
(Hastie and Tibshirani 1993). Interaction between the covariates zj and x j are mod-
eled by allowing the effect of the interaction variable to z j vary over the domain of
the effect modiﬁer xj. Conditional on the effect modiﬁer, the effect is linear. This
can be estimated in the given framework by deﬁning the matrix of basis functions
diag(z j)BBB⋆
j(xj) = BBB j,
where BBB⋆is the usual matrix of B-splines basis functions. The most interesting
effect modiﬁer for failure time analysis is time, allowing time varying effects. Of-
ten the interaction variables are binary variables, deﬁning nonlinear deviations for
subgroups.
Seasonal Effect and Time Trends
Suppose that historical time is observed as 1,2,...,N, with period length g. A ﬂex-
ible formulation that allows for time-varying seasonal effects assumes, (Kneib
2006, pp. 39–40):
ζseas,i = −(ζseas,1 +...+ζseas,g−1) ∼N(0,ξ 2
seas).
This can be written as
ζζζ seas|ξ 2
seas ∝exp(−
1
2ξ 2seas
ζζζ ⊤
seasKKKseasζseas),
by deﬁning
DDDseason =
⎡
⎢⎢⎣
1
...
1
...
...
...
1
...
1
⎤
⎥⎥⎦,

42
3 Computation and Inference
KKKseason = DDD⊤
seasonDDDseason, where DDDseason has dimension (N −g)×N and the entry 1
is repeated g times in every row of DDDseason. The rank of KKKseason is given by N-g+1.
The parameter ξ 2
seas controls the variation of the seasonal effects over time. For
limξ 2
seas →0, the seasonal effects become constant over time. The corresponding
design matrix BBBseas is a matrix of binary indicators. It is also possible to include
time trends, to this end historical time can be treated exactly like a time-varying
covariate.
3.4.2.3 Sampling Scheme
The sampling scheme for the normal model is given by sampling scheme 1. At
every step the most recent values of all conditioning parameters are used. As can
be seen the steps for random effects, ﬁxed effects and regression coefﬁcients for
P-splines respectively basis function coefﬁcients are very similar to each other.
In general the main estimation problem will lie in determining regression coefﬁ-
cients.
3.4.2.4 Alternative Priors for Variance Parameter
The inverse gamma prior for the smoothing and random effect variance is a con-
venient choice due to its conjugacy to the normal distribution, but the distribution
exhibits some undesirable properties (Polson and Scott 2011), (Gelman 2006): (1)
For small samples the hyperparameter for the variance parameter can have large
inﬂuence on estimates, making it a bad choice if the prior is supposed to be non-
informative. (2) The distribution is biased against zero. For random effects and
P-spline coefﬁcients this corresponds to completely linear effects and the absence
of heterogenity, both situations are important to detect. As an alternative, the use
of the Cauchy distribution truncated from below at zero, with pdf
f(σ|d) ∝(1+σ2/d)−(d+1)/2
called half-cauchy distribution, has been advocated by Gustafson (1997) for the
variance parameter and by Gelman (2006) for the standard deviation. A half-

3.4 Bayesian P-Splines
43
Sampling scheme 1
1 Draw ﬁxed effects from N(μμμβββ,ΣΣΣβββ) where
ΣΣΣβββ = σ2(ZZZ⊤ZZZ +σ2TTT −1)−1, and
μμμβββ = 1
σ2ΣΣΣβββ(ZZZ⊤[yyy−ηηη +ZZZβββ]+σ2TTT −1ttt)
2 For j=1,...,m: Draw regression coefﬁcients for basis functions coefﬁcients
from N(μμμζζζ j,ΣΣΣζζζ j) where
ΣΣΣζζζ j = ( 1
σ2BBB⊤
j BBB j + 1
ξ 2
j
KKK j)−1,μμμζζζ j = ΣΣΣζζζ j
1
σ2BBB⊤
j [yyy−ηηη +BBBj(xxx j)ζζζ j],
correct the samples by the steps given in section 3.4.2.2
3 For i=1,...,p: Draw random effects from N(μμμαααi,ΣΣΣαααi) with
ΣΣΣαααi = σ2(W
W
W ⊤
i W
W
W i +σ2QQQ−1)−1 and
μμμαααi = 1
σ2ΣΣΣαααi(ZZZ⊤
i [yyyi −ηηηi +W
W
W iαααi]+σ2QQQ−1)
4 For i=1,...,r: Draw smoothing variance from IG(aξi
0 + rank(KKKi)
2
,bξi
0 + 1
2ζζζ ⊤
i KKKiζζζ i)
5 Draw σ2 from IG(aσ
0 + n
2,bσ
0 + εεε⊤εεε
2 )
6 Draw QQQ from IW(a0 + p/2, 1
2 ∑p
i=1αααiααα⊤
i ), or for nondiagional QQQ:
For j=1,...p: Draw var(αi) from IG(aαi
0 + p/2,bαi
0 + 1
2 ∑n
i=1 α2
ij))
cauchy prior with d=1 for a standard deviation corresponds to an inverted-beta
distribution ∝(σ2)(b−1)(1+σ2)−(a+b) with parameters a=b=1/2 for the variance
(Polson and Scott 2011). For some highly variable functions a global variance
parameter can be inappropriate. Variable function estimates can be obtained by
letting the variance vary across coefﬁcients. This can for example be done by
introducing gamma distributed weights, leading to a marginal t-distribution (Lang
and Brezger 2004). In general the inverse gamma prior is used as default choice
here.

44
3 Computation and Inference
3.4.2.5 Extrapolation
If there is interest in predicting a function beyond the boundaries of the data set,
say for a point x⋆> max(xxx j), samples for the necessary regression coefﬁcients
ζ j,u+1,ζ j,u+2... are obtained by continuing the random walk of order d of the re-
gression coefﬁcents ζζζ. Equidistant knots are added to the knots sequence and the
corresponding basis functions have to be computed. For example for a difference
penalty or order one, draws for ζ j,u+1 are obtained from N(ζ s
u,ξ 2(s)), for s=1,...,S.
The function value at x⋆can be estimated by
ˆf(x⋆) = 1
S
S
∑
i=1
u⋆
∑
k=1
B jk(x⋆)
(Brezger and Lang 2006), where u⋆is the number of columns of B jk after adding
enough knots so that ∑u⋆
k=1 B jk = 1.

4 Discrete Time Models
As noted in section 2.3, by the introduction of failure indicators yij,i = 1,...,n, j =
1,...,ti a Bernoulli likelihood is obtained and estimation can proceed as for binary
regression - allowing that time can be treated like an arbitrary covariate whose
effect can be smoothed. This is not the case for continuous time models. The
linear predictor for discrete time models has the generic form
ηηη = ηηη +ZZZβββ +W
W
Wααα +BBB1(xxx1)+...+BBBm(xxxm),
where for this chapter it is always assumed that xxx1 = ttt, so that BBB1(ttt) describes the
baseline hazard. The basis function matrices are allowed to have the forms given
in section 3.4.1 and may further depend on process time.
This chapter is structured as follows: At ﬁrst the generalized linear model frame-
work is given, embedded in which iteratively weighted least squares (IWLS) pro-
posals can be derived which can be used for continuous time models as well. Some
important models for discrete time are discussed. Subsequently estimation based
on data augmentation scheme is discussed.
The baseline hazard is estimated uniﬁed via P-splines for all discrete time mod-
els. The simplest speciﬁcation is a constant baseline hazard in every period t, using
P-splines of order 0 where the columns of BBB(ttt) are given by binary indicators. The
difference penalty suppresses large jumps in sparse data settings where estimation
is unstable. By design, these occur towards the end of the observation time for
failure time data. Splines of higher order can be used as well to obtain smoother
estimates, allowing the baseline hazard to vary within intervals.
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_4, © Springer Fachmedien Wiesbaden 2015

46
4 Discrete Time Models
4.1 Estimation Based on GLM Methodology
Following McCulloch and Searle (2001, pp. 135–143) the Bernoulli distribution
belongs to the exponential family of distributions, which can be analyzed in the
generalized linear model framework. Distributions in the exponential family can
be written as
f(yi|θ,φ) = exp{(yθi −b(θi))/φ +c(yi,φ)},
(4.1)
where θ and φ are referred to as the natural and the scale parameter and b(·) and
c(·) are functions speciﬁc to the distribution. The expectation and variance depend
on the natural parameter:
E[yi] = μi = b′(θ),
var(yi) = b′′(θ) = φv(μi).
For regression analysis, μi is related to the linear predictor ηi by the link function
η = g(μi),
with inverse r(ηi) = μi. The Bernoulli with pmf
pyi
i (1−pi)1−yi
has natural parameter θi = log(pi/1−pi), the pmf written in the form 4.1 is:
f(yi|θ) = exp(yiθi −log(1+exp(θi)),
where b(θi) = log(1 + exp(θi)), φ = 1 and c(y,φ) = 0, so b′(θ) = EYi = pi and
var(yi) = b′′(θi) = pi(1−pi). IWLS proposals will be used here for the Bernoulli
and the Poisson likelihood, both of which have φ = 1, so with no loss of generality
φ = 1 is taken hereafter.
Suppose the model only includes ﬁxed effects, so that the linear predictor ηi is
given by zzz⊤
i βββ. The full conditional of βββ is in general not conditional conjugate for
GLMs and a Metropolis-Hastings update can be used. For maximum likelihood

4.1 Estimation Based on GLM Methodology
47
estimation, maximizing the loglikelihood
L(βββ) =
n
∑
i=1
(yiθi −log(1+exp(θi)))
with respect to βββ usually can not be done analytically. Iterative numerical methods
can be used, using local information about the surface of the loglikelihood to ﬁnd
the maximum. This information is valuable in ﬁnding a proposal distribution.
Fisher scoring computes the maximum via the following iteration rule:
βββ (s+1) = βββ (s) +III(βββ (s))−1s(βββ),
(4.2)
where s(βββ) is the score vector and III(βββ (s)) is the Fisher information at βββ. It can
be shown that the Fisher information and the score vector for exponential family
distributions can always be written as
III(βββ (s)) = ZZZ⊤W
W
WZZZ, and
(4.3)
s(βββ) =
n
∑
i=1
(yi −μi)wig′(μi)zzz⊤
i = ZZZ⊤W
W
WΔΔΔ(yyy−μμμ),
(4.4)
whereW
W
W = diag(w1,...wn), wi = [v(μi)(g′(μi))2]−1 and ΔΔΔ = diag(g′(μ1),...g′(μn))
depend on βββ (s). For canonical link functions it holds that θ = η and g′(μi) =
1/var(μi), hence the expressions for the diagonal of W are simpliﬁed. Plugging
the expressions 4.3 and 4.4 into 4.2, this can be viewed as a weighted least square
regression
βββ (k+1) = (ZZZ⊤W
W
WZZZ)−1ZZZ⊤W
W
W ˙y˙y˙y(ηηη(k)),
of working observations
˙y˙y˙y(ηηη(k)) = ηηη(k) +ΔΔΔ(ηηη(k))[yyy−μμμ(ηηη(k))],
on ZZZ, as var(˙y˙y˙y) =W
W
W −1. This estimator also would be obtained under the working
model
˙y˙y˙y( ˆηηη(k)) ∼N(ZZZ⊤βββ,W
W
W −1).
(4.5)

48
4 Discrete Time Models
The idea behind IWLS proposals by Gamerman (1997) is to approximate the full
conditional distribution of βββ
∝exp(log(f(yyy|θθθ))+log( f(θθθ)))
by replacing the likelihood f(yyy|θθθ) by the likelihood corresponding to the model
4.5, hence approximating the likelihood by a Gaussian likelihood obtained via one
Fisher scoring step. Under a prior distribution
∝exp(−1
2(βββ −ttt)⊤TTT −1(βββ −ttt)),
where T −1 = 000 represents a diffuse prior βββ ∝const, this gives
∝exp(−1
2

(˙y˙y˙y−ZZZ⊤βββ)⊤[ZZZ⊤W
W
WZZZ]−1(˙y˙y˙y−ZZZ⊤βββ)+(βββ −ttt)⊤TTT −1(βββ −ttt)

),
as proposal distribution. This is proportional to a normal distribution with covari-
ance matrix and expectation:
ΣΣΣβββ(ηηη(s)) = (ZZZ⊤W
W
WZZZ +TTT −1)−1
mmmβββ(ηηη(s)) = ΣΣΣβββ(ηηη(s))(ZZZ⊤W
W
W ˙y˙y˙y+TTT −1ttt),
where W
W
W =W
W
W(ηηη(s)) and ˙y˙y˙y = ˙y˙y˙y(ηηη(s)). The extension to the full predictor
ηηη +ZZZβββ +W
W
Wααα +BBB1(xxx1)+...+BBBr(xxxr)
is straightforward. To update ζζζ j, with corresponding penalty matrix KKK j/ξ 2(s−1)
at iteration s, the mean of the proposal is changed to the mean obtained under the
working model ˙y˙y˙y ∼N(ηηη,W
W
W −1), which is:
mmmζζζ j(ηηη(s)) = ΣΣΣζζζ j(ηηη(s))(ZZZ⊤W
W
W[˙y˙y˙y−ηηη(s) +BBBjζζζ (s)
j ]),
(4.6)

4.1 Estimation Based on GLM Methodology
49
with covariance matrix
ΣΣΣζζζ j(ηηη(s)) = [ZZZ⊤W
W
WZZZ +KKK j/ξ 2(s−1)]−1,
(4.7)
where the weights and working observations are evaluated at ηηη(s),
ηηη(s) =ZZZβββ (s) +W
W
Wααα(s) +BBB1ζζζ (s)
1 +....+BBB j−1ζζζ (s)
j−1+
BBB jζζζ (s−1)
j
+BBBj+1ζζζ (s−1)
j+1 +...+BBBrζζζ (s−1)
r
.
This has the form of a regression on partial residuals ˙y˙y˙y(ηηη(s)) −ηηη(s) +BBB jζζζ (s)
j . To
update ζζζ (s) at iteration s, a proposal βββ ⋆from a multivariate normal distribution
with covariance and expectation 4.6 is drawn and and accepted with probability
α(ζζζ (s),ζζζ ⋆) = min(
L(yyy|ζζζ ⋆)f(ζζζ ⋆|(ξ 2
i )(s−1))φ(ζζζ (s)|mmmζζζ j(ηηη⋆),ΣΣΣζζζ j(ηηη⋆))
L(yyy|ζζζ (s))f(ζζζ (s)|(ξ 2
i )(s−1))φ(ζζζ ⋆|mmmζζζ j(ηηη(s)),ΣΣΣζζζ j(ηηη(s))),1).
(4.8)
where ηηη⋆= ηηη(s) +BBB j(ζζζ ⋆
j −ζζζ (s)
j ) and φ(xxx|aaa,BBB) denotes the density of the multi-
variate normal distribution with moments aaa,BBB evaluated at xxx. Implementing the
identiﬁcation restriction ∑n
i=1 f(x) = 0 is easier here by centering the functions
in every iteration, as adjusting the proposal distribution to the conditional density
N(mmm,ΣΣΣ|AAAζζζ = 000) would involve evaluating furthermore normalizing constants due
to the conditioning. Updating ﬁxed and random effects is done in analogy, the
only differences are that random effects ααα1,...,αααn are updated seperately and the
prior is ﬁxed for ﬁxed effects. This leads to sampling scheme 2 (Brezger and Lang
2006).
To evaluate α(ζζζ (s),ζζζ ⋆), the moments mmmζζζ j(ηηη⋆) and ΣΣΣζζζ j(ηηη⋆) conditional on
ηηη⋆= ηηη(s) +BBBj(ζζζ ⋆
j −ζζζ (s)
j )
have to be computed. This is computationally costly as it involves calculation
of W
W
W(ηηη⋆), ΔΔΔ(ηηη⋆), ˙y˙y˙y(ηηη⋆) and the inversion of (ZZZ⊤W
W
W(ηηη⋆)ZZZ +TTT −1). Brezger and
Lang (2006) give a modiﬁcation of the proposal distribution. Here ζζζ (s) in the
linear predictor is replaced by mmmζζζ j(ηηη(s−1)), the mean of the proposal distribu-

50
4 Discrete Time Models
Sampling scheme 2
1 For i=1,...,m:
Draw a proposal ζζζ ⋆
j from a normal distribution with expectation 4.7 and
covariance 4.6, compute 4.8,
set

ζζζ (s)
j
= ζζζ ⋆
j
with probability α(ζζζ (s),ζζζ ⋆))
ζζζ (s)
j
= ζζζ (s−1)
j
with probability 1−α(ζζζ (s),ζζζ ⋆))
2 For i=1,...,p: Draw random effects by steps analog to basis function
coefﬁcients
3 Draw ﬁxed effects by steps analog to basis function coefﬁcients
4 Draw smoothing variance by step 4, sampling scheme 1
5 Draw variance of random effects by step 6, sampling scheme 1
tion used at iteration s −1, in the calculation of the moments of the proposal
distribution, independent if the proposal was accepted or not. Writing ηηη(s)
ζζζ j,m =
ηηη(s) +BBB j(mmmζζζ j(ηηη(s−1)) −ζζζ (s)
j ), the moments of the proposal distribution for BBB jζζζ j
are given by
ΣΣΣζζζ j(ηηη(s)
ζζζ j,m) = (ZZZ⊤W
W
WZZZ +KKK−1
j /ξ 2(s−1))−1
(4.9)
mmmζζζ j(ηηη(s)
ζζζ j,m) = ΣΣΣζζζ j(ηηη(s)
ζζζ j,m)(ZZZ⊤W
W
W[˙y˙y˙y−ηηη(s)
ζζζ j,m +BBBjmmmζζζ j(ηηη(s−1))]),
(4.10)
withW
W
W =W
W
W(ηηη(s)
ζζζ j,m) and ˙y˙y˙y = ˙y˙y˙y(ηηη(s)
ζζζ j,m), so that the proposal distribution is indepen-
dent from the current state ζζζ (s). Hence the moments of the proposal distribution
have to be computed only once. In addition to reducing computation time, accep-
tance rates are often higher.
A draw is accepted with probability
α(ζζζ (s),ζζζ ⋆) = min(1,
L(yyy|ζζζ ⋆)f(ζζζ ⋆|ξ 2(s−1))φ(ζζζ (s)|mmm(ηηη(s)
ζζζ j,m),ΣΣΣζζζ j(ηηη(s)
ζζζ j,m))
L(yyy|ζζζ (s))f(ζζζ (s)|ξ 2(s−1))φ(ζζζ ⋆|mmm(ηηη(s)
ζζζ j,m),ΣΣΣζζζ j(ηηη(s)
ζζζ j,m)
.
(4.11)
Adjusting the proposal distribution to N(aaa,BBB|AAAζζζ = 000) does not change the accep-

4.1 Estimation Based on GLM Methodology
51
tance probability: We have
φ(ζζζ ⋆|mmmζζζ j,ΣΣΣζζζ j,AAAζζζ ⋆= 0) =
φ(ζζζ ⋆|mmmζζζ j,ΣΣΣζζζ j) f(AAAxxx|xxx)
φ(AAAζζζ ⋆|AAAmmmζζζ j,AAAΣΣΣζζζ jAAA⊤) ,
(4.12)
it can be shown (Rue and Held 2005, p. 38) that
log f(AAAxxx|xxx) = −1
2 log|AAAAAA⊤|.
(4.13)
As AAAζζζ ⋆= AAAζζζ (s) = 000, the only factor not canceling in 4.12 if plugged into 4.11 is
φ(ζζζ ⋆|mmmζζζ j,ΣΣΣζζζ j). The sampling scheme is given in the following.
Sampling scheme 3
1 For i=1,...,m:
Draw a proposal ζζζ ⋆
j from a normal distribution with expectation 4.10 and
covariance 4.9, compute 4.11,
set

ζζζ (s)
j
= ζζζ ⋆
j
with probability α(ζζζ (s),ζζζ ⋆))
ζζζ (s)
j
= ζζζ (s−1)
j
with probability 1−α(ζζζ (s),ζζζ ⋆))
2 For i=1,...,p: Draw random effects by steps analog to basis function
coefﬁcients
3 Draw ﬁxed effects by steps analog to basis function coefﬁcients
4 Draw smoothing variance by step 4, sampling scheme 1
5 Draw variance of random effects by step 6, sampling scheme 1
Another adjustment is to compute the weight matrix only every kth iteration, or
even keep it ﬁxed, which usually does not impair convergence.
IWLS proposals allow fully automatic blockwise updates and usually have high
acceptance rate. In terms of mixing better solutions are possible, for example a
closer approximation of the full conditional distribution can be constructed. How-
ever the approach of IWLS proposals is quite general and the literature on the
models this thesis deals with mainly uses this approach. The idea behind IWLS
proposals was also used by Hennerfeind (2006) as blueprint for the development
of proposal distributions for continuous time models which will be discussed in

52
4 Discrete Time Models





í
í



[
SUREDELOLW\
OLQHW\SH
ORJLW
SURELW
JFR[
Figure 4.1: The most common response functions
section 6.3. As such, all models in this thesis can be estimated via adjustments of
sampling schemes 1-3.
In the following sections, some important models corresponding to different
choices of link functions are discussed.
4.1.1 Grouped Cox
The grouped Cox model is the discrete time analog to the Cox model in continuous
time with hazard
h(ti|ηi) = h0(ti)exp(ηi)
(4.14)
(Kalbﬂeisch and Prentice 1973). Given an unobservable continuous random vari-
able T⋆, the probability of failure in the interval [aj−1,a j), given survival until
aj−1, under the Cox model with relative risk hazard h0(ti)exp(ηi), is given by:
= P(T ∗∈[aj−1,a j)|ηi j,T ∗> aj−1) = 1−exp{−exp(ηij)
 a j
a j−1
h0(u)du}. (4.15)
Equation 4.15 holds for covariates that are time-constant or constant in every in-
terval [a j−1,a j), j = 1,...,m. The notation ηi j denotes the linear predictor where

4.1 Estimation Based on GLM Methodology
53
the covariates are evaluated at aj−1. Grouping T⋆, gives
P(T = t|ηit) = 1−exp{−exp{ηit +α0(t)}},
where α0(t) = log
 t
t−1 h0(u)du and η is the linear predictor without the baseline
hazard and without time-varying effects. In a GLM context the corresponding link
function g(x) = log(−log(1 −x)) is referred to as complementary log-log link.
As η is unchanged by the discretization of the time scale, regression coefﬁcients
and estimated effects can be directly interpreted in terms of T⋆(Fahrmeir and Tutz
2001, p. 140).
Independent of the connection to the relative risk model, the complementary
log-log link provides a good ﬁt for binary models if one of the categories is more
frequent than the other, due to the asymmetry of the response function (Cameron
and Trivedi 2005, p. 466). This holds obviously for most data sets for failure time.
For IWLS proposals, the computation of
W
W
W it = [exp(ηit −exp(ηit))]2/(hit(1−hit))
and
ΔΔΔit = ([exp(ηit −exp(ηit)]−1)
is necessary which can be unstable and slow. An alternative method will be given
in section 4.2.3.
4.1.2 Logistic Model
Under the logistic model the hazard is given by
h(ti|ηi j) =
exp(ηi j)
1+exp(ηi j),
and covariates act multiplicatively on the conditional odds:
P(T = t|T ≥t)
P(T > t|T ≥t) = exp(ηit).
(4.16)

54
4 Discrete Time Models
The factor exp(βk) can be interpreted as effect of an one unit change of covariate
zk in terms of the odds (given that the remaining covariates are held ﬁxed). The
working observations have the easy form
ηit + (yit −μit)
μit(1−μit),
and the weights are given by μit(1−μit), where
μit = P(T = t|T ≥t) =
exp(ηit)
1+exp(ηit).
(4.17)
For an underlying discretized variable T⋆, where δ is the interval width, 4.16 im-
plies that in the limiting case limδ →0 the hazard of the Cox model is recovered
(Fleming and Harrington 1991, pp. 127–128).
The right side of 4.17 is the cdf of a logistic distribution, this distribution has
similar shape to the normal distribution with slightly wider tails. The logit model
gives results which are very similar to the probit model where the response func-
tion is given by the cdf of a standard normal distribution. For the latter model, a
completion Gibbs sampler can be set up that bypasses the necessity for Metroplis-
Hastings updates completely as all conditional distributions are fully conjugate
(under the priors considered here). Such sampling schemes for various models are
discussed in the following.
4.2 Estimation Based on Latent Variable Representation
4.2.1 Probit Model
A completion Gibbs sampler for the probit model was developed by Albert and
Chib (1993). Let ˜Yit,i = 1,...,n be random variables, related to Yit by:
Yit =

1
˜Yit > 0
0
˜Yit ≤0

4.2 Estimation Based on Latent Variable Representation
55
so that P(Yit = 1) = P(˜Yit > 0). Under the model
˜Y˜Y˜Y i = ηηηi +σεεεi
(4.18)
where εi ∼N(0,III), we have P(Yit = 1) = Φ(ηit/σ). As
Φ(ηηη⋆/σ⋆) = Φ(ηηη/σ), ∀c ̸= 0,
where η⋆= cη⋆and σ⋆= σc, only the ratio η/σ is identiﬁed and σ is usually
ﬁxed at σ = 1 (Greene 2012, p. 686). Due to the deterministic relationship between
Y and ˜Y, f(yit|ηit, ˜yit) = 1[˜yit > 0] and
˜Yit ∼TN(0,∞)(ηit,1),
where TN(0,∞)(a,b) denotes a normal distribution, truncated to the interval (0,∞)
with mean a and variance b.
Suppose again for introductory purposes there are only ﬁxed effects so that ηit =
zzz⊤
it βββ. The joint posterior distribution of ˜YYY = (˜Y˜Y˜Y 1,..., ˜Y˜Y˜Y n)⊤and βββ is
g(˜YYY,βββ|D) ∝f(βββ)
n
∏
i=1
ti
∏
j=1
(I[˜Yit > 0]I[yit = 1]+I[˜Yit ≤0]I[yit = 0])φ(˜Yit|zzz⊤
ijβββ,1).
(4.19)
Integrating ˜YYY out of 4.19 gives the marginal distribution
∝f(βββ)
n
∏
i=1
ti
∏
j=1
Φ(zzz⊤
it βββ)yit(1−Φ(zzz⊤
it βββ))1−yit
so g( ˜YYY,βββ|D) is a completion of the posterior distribution f(βββ|D). The model is
equivalent a Bernoulli GLM with response function Φ(x) =
 x
−∞φ(u|0,1)du. Sam-
pling from truncated normal distributions can be quickly done, in Robert (1995)
and Geweke (1991) efﬁcient methods are given. As the necessity of Metropolis-
Hastings steps is completely bypassed, sampling scheme 4 is computationally
much cheaper than the use of IWLS proposals. The sampling scheme for the full
linear predictor 3.18 is then given as:

56
4 Discrete Time Models
Sampling scheme 4
1 For i = 1,...,n, j = 1,...,ti:
Draw ˜y(s)
ij from TNIij(ηi j,1), where
Ii j =

(0,∞)
if yit = 1
(−∞,0]
if yit = 0
2 Draw parameter using sampling scheme 1, with σ ﬁxed at 1
4.2.2 Scale Mixtures of Normals
The family of scale mixtures of normal distributions consists of distributions that
can be represented as
 f(Yλ|λ)h(λ)dλ, where Y ∼N(0,1) and λ > 0 (Andrews
and Mallows 1974). Some useful scale mixtures are given in table 4.1. Conditional
on λ, Y is distributed as N(0,λ 2). If g is a scale mixture, f(Y|λ)h(λ) is by
deﬁnition a completion of g(·). This leads to the representation
˜yit = ηit +εit,
εit|λit ∼N(0,λit),
for distributions from the scale mixture family. Sampling scheme 4 for the probit
model can be extended by an additional block for λλλ (Albert and Chib 1993). The
conditional distributions are adjusted to account for the scale factor but conditional
conjugacy is preserved: The conditional distribution of e.g. ﬁxed effects with prior
N(βββ 0,BBB0) is
N((ZZZ⊤W
W
WZZZ)−1[ZZZ⊤W
W
W(˜y˜y˜y−ηηη +ZZZβββ)+BBB−1
0 βββ 0]),
where W =diag(λλλ −1
1 ,...,λλλ −1
n ). In general the moments of the conditional distri-
bution are the same with the exception of the additional weight matrix W
W
W. An
important choice is λit ∼IG(ω/2,ω/2), resulting in a marginal t-distribution with
ω degrees of freedom. The parameter a can be estimated from the data, it is usually
updated via a Metropolis-Hastings step, but is often kept ﬁxed at a low value. The
t-link is a robust alternative to more common link functions, as it can be shown to
be less inﬂuenced by outliers. In addition, as Ft
7(x/1.5484) ≈Flogistic(x), where

4.2 Estimation Based on Latent Variable Representation
57
í4eí04
0e+00
4eí04
í20
0
20
x
error
Figure 4.2: Ft
7(x/1.5484)−Flogistic(x)
Ft
v(x) denotes the cdf of a t-distributed variable with ω degrees of freedom, the lo-
gistic model can be very well approximated via the t model (Liu 2004), see ﬁgure
4.2. For the t-link, the distribution h(λit|·) is available in closed form and given by
λit ∼IG(ω +1
2
,
2
ω +(˜yit −ηit)2 ).
An exact solution is available as well as the logistic distribution belongs to the
scale mixture family, but sampling λ is difﬁcult for this model as λ follows a
Kolmogorov-Smirnov distribution for which the cdf and pdf are only available in
an inﬁnite series representation (Devroye 1986, p. 161).
Table 4.1: Some mixtures
Marginal distribution of eit
Mixture component λi
Student-t(v)
λi ∼IG(v/2,v/2)
Laplace(J)
λi ∼Exp(0.5J)
Standard logistic
λi = (2K)2,K ∼Kolmogoro-Smirnov

58
4 Discrete Time Models
Sampling scheme 5: Scale Mixtures
1 For i = 1,...,n, j = 1,...,ti:
Draw λit from f(λit|·)
2 Draw parameters and latent data as for sampling scheme 4, with additional
weight matrix W
W
W=diag(λλλ −1
1 ,...,λλλ −1
n )
4.2.3 Grouped Cox II
For the grouped Cox model with link function log(−log(1−μit)) we have P(Yit =
0|ηit) = exp(−exp(ηit)) and P(Yit = 1|ηit) = 1−exp(−exp(ηit)). Assuming un-
derlying Poisson variables Y ⋆
it ,i = 1,...,n,t = 1,...,ti, so that Yit = I[Yit > 0], it
follows that
P(Yit = 0|ηit) = P(Y ⋆
it = 0|ηit) = exp(−exp(ηit))
and
P(Yit = 1|ηit) = P(Y ⋆
it > 0|ηit) = 1−exp(−exp(ηit)).
The full conditional distribution of Y ⋆
it is a Poisson distribution with single pa-
rameter exp(ηit) truncated from below at zero if yit = 1 and degenerate at zero
otherwise. Integrating f(YYY ⋆,θθθ|D) in Y ⋆yields f(θθθ|D), so the introduction of the
latent variable into a Gibbs sampler yields again a valid data augmentation scheme
(Dunson and Herring 2004). An efﬁcient method to obtain random deviates from
the truncated Poisson distribution is given by Cumbus et al. (1996). While con-
ditioning on Poisson distributed variables does not result in conditional conjugate
distributions for most paramaters, conditioning on YYY ⋆nevertheless yields simpli-
cation as inference the Poisson likelihood is generally easier than dealing with the
cloglog link. For IWLS proposals the expressions for the weights and working ob-
servations are very simple for Poisson data as the log(·) is the natural link function
for the Poisson distribution and due to the equivariance property of the Poisson
distribution. The weight matrix is given by diag(μ11,...,μntn) and the working

4.2 Estimation Based on Latent Variable Representation
59
observations are
˙yit = ηit +yit/μit −1,i = 1,...,n,t = 1,...,ti,
where μit = exp(ηit) = var(Y ⋆
it |ηit). Furthermore the Poisson likelihood is condi-
tionally conjugate to a gamma prior, so that gamma frailties are an alternative to
Gaussian random intercept models. Under the hazard
h(t|wi,θθθ,ηi j) = 1+exp(−wi exp(ηi j)),i = 1,...,n, j = 1,...,ti,
where a priori wi ∼G(κ−1,κ−1) with mean one and variance κ, the full condi-
tional distribution of wi,i = 1,...,n is
wi ∼G(κ−1 +
ti
∑
j=1
yi j,κ−1 +
ti
∑
j=1
exp(ηij))).
A hyperprior is usually assigned for κ, the full conditional distribution f(κ|·,D)
is not conditionally conjugate. Drawing from the latent variables does not add
much computational burden as draws only have to be obtained if yit = 1, which for
failure time data usually is the vast minority of data points.

5 Application I: Unemployment Durations
Discrete time models are applied on a data set of the NEPS panel study (Blossfeld
et al. 2011). The data was obtained as retroperspective interviews. Analyzed are
unemployment durations. As unemployment spells can be repeated, only the ﬁrst
unemployment spells are analyzed. Additionally, the transition from unemploy-
ment to employment is not exclusive; there are several types of failure. Applying
the methods of this thesis is valid under the latent failure time approach: Here,
spells that did not result into employment are taken as censored, hence it is as-
sumed that there exists a latent employment failure time which is not observed
(Crowder 2001, pp. 37–38). Some possible model improvements using methods
not covered in this thesis are the inclusion of all spells via recurrent failure meth-
ods and the inclusion of all modes of failure. A complete case analysis is carried
out. 773 individuals remain in the sample. The analysis was limited to spells
whose beginning lies in the years 1995-2005. Spells lasting longer than 24 months
are seen as censored. Estimation was carried out using a R-function appended as
the thesis. Sampling of truncated normal deviates was done via the rtruncnorm
function from the truncnorm package (Trautmann et al. 2012), which implements
the sampler by Geweke (1991) and was found to be very stable. A ﬁrst try, based
on a inverse cdf approach was found to be unstable, sometimes producing inﬁnite
values.
The covariates used as ﬁxed effects were:
• sex binary variable indicating sex=female
• foreign binary variable indicating for individuals with migration background
• training binary variable indicating that an individual was not born in Ger-
many
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_5, © Springer Fachmedien Wiesbaden 2015

62
5 Application I: Unemployment Durations
Table 5.1: Model assessment
LPML
DIC
pD
deviance
grouped Cox
-2063.6
4093.5
33.4
4060.1
probit
-2085.7
4138.1
32.1
4105.9
• registered binary variable indicating registration of unemployment at begin
of unemployment
These covariates were used for modeling of smooth effects via P-splines:
• casmin education years coded via casmin classiﬁcation
• time process time
• historical time historical time, ranging from 2001 to 2011
• age age in years at beginning of unemployment spell
Furthermore, seasonality with perior length 12 was included, leading to the model
g(P(T = t|T ≥t,η)) =β0 +β1 ∗sex+β2 ∗foreigner +β3 ∗training+
β4 ∗registered + f0(time)+ f(age)+ f(casmin)+
f(season)+ f(historicaltime)
A grouped Cox and a probit model were used. For the grouped Cox, sampling
scheme 3 was used, additionally a completion Gibbs sampler with latent Poisson
variables as described in section 4.2.3 in combination with sampling scheme 3 was
tried. Truncated Poisson variables were sampled via the inverse cdf method using
standard R-functions for the Poisson distribution. For the grouped Cox sampler
there were very high correlations between the seasonal and the baseline hazard
coefﬁcients causing the sampler to get stuck (see ﬁgure 5.1), those parameters
were drawn in one block. This was done by combining the design, penalty matri-
ces and restriction matrices into blockmatrices. This resulted in lower acceptance
rates (around 20%) which was found to be acceptable for a vector of size 171.

5 Application I: Unemployment Durations
63
í1.0
í0.5
0.0
0.5
1.0
0
2500
5000
7500
10000
iteration
parameter
Figure 5.1: Convergence problems for unblocked parameter.
Sampling scheme 3 was found to behave somewhat erratically: There were long
stretches where no draws were accepted, often after convergence seemed to have
been reached see ﬁgure 5.6. Furthermore there were numerical problems due to the
exponential functions involved in the complementary log-log link. It might be the
case that sampling was not done long enough, for the completion Gibbs sampler
this behaviour was not observed however, as such the results of this sampler are
reported here. All in all, this sampler was much more stable. For other parameters,
results of the samplers were very similar. Acceptance rates were around 50% for
all parameters excluding baseline hazard coefﬁcients.
For the probit model, no further adjustments were made and the standard sam-
pler was used, using sampling scheme 4. The probit model achieves more inde-
pedent draws than both sampler. A fair comparison would furthermore include
execution times, as the sampler for the probit model is about 2 times faster al-
though this depends on the implementation. For both models, 12000 iterations
were used, the ﬁrst 2000 were taken as burnin. Convergence could probably be
accelerated by using better start values, here all parameters were set to zero at the

64
5 Application I: Unemployment Durations
0
20
40
60
0
1000
2000
3000
neff
count
group
gcox
probit
Figure 5.2: Histograms of numbers of effective parameter draws for both samplers.
beginning. It was noticed that convergence was heavily inﬂuenced by start values
for the basis function coefﬁcients and the corresponding variance parameter, which
is part of the reason why one long chain was run instead of multiple independent
chains. All in all, the inﬂuence of start-values seems to be a problem of IWLS
proposals, this was also mentioned by Brezger and Lang (2006) who used a mode
ﬁnding algorithm to ﬁnd start values. Under all model diagnostics shown here, the
grouped Cox model is preferable. This is not suprising as the complementary log-
log link usually ﬁts failure time data better to its asymmetry. The LPML statistic
was computed using the harmonic mean estimator. Estimated nonlinear are given
only for the grouped Cox model in ﬁgure 5.5, as the model ﬁts better and results
can be directly interpreted in terms of a latent failure time variable. Furthermore,
results were very similar to the probit. All nonlinear effects were estimated using
B-splines of degree 3 with 30 knots and a difference penalty of order 2.
The posterior quantiles of the baseline hazard are somewhat wide, probably
owing the fact that time is decomposed into three effects, the estimates of which
are not uncorrelated. We ﬁnd slight negative duration dependence for the ﬁrst

5 Application I: Unemployment Durations
65
FORJORJ
SURELW
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
í






FRHIILFLHQW
UHODWLYHKD]DUG
IRUHLJQ
OHKU
UHJLVWHUHG
VH[
IRUHLJQ
OHKU
UHJLVWHUHG
VH[
Figure 5.3: Relative hazards P(T = t,z = 1|T ≥t)/P(T = t,z = 0|T ≥t) for both models. Estimates are
averaged over time, keeping the other covariates at zero, including the constant at the posterior mean.
unemployment durations, so that the chance of leaving unemployment gets worse
with the length of the unemployment spell. For seasonality, it is found that chance
of ﬁnding employment is biggest at december, this seems implausible and might
be related to data collection, this might well be a heaping effect. The effect of
age shows that the B-spline ﬁt could very well be replaced by a linear effect. For
the effect of education it can be seen that the slope of the function starts out at
approximately zero, followed by a positive effect around year 12. This however
should not be overinterpreted due to the wide posterior quantiles. For the time
trend, the effect peaks strongly around year 1999. The estimated seasonal effects
are almost constant over time.

66
5 Application I: Unemployment Durations
0
2
4
0.0
0.5
martingale residuals
group
cloglog
probit
t=1
0.0
0.2
0.4
0.6
í2
í1
0
martingale residuals
density
group
cloglog
probit
t=6
0.0
0.2
0.4
0.6
í2
í1
0
group
cloglog
probit
t=12
0.0
0.1
0.2
0.3
0.4
0.5
í3
í2
í1
0
density
group
cloglog
probit
t=24
Figure 5.4: Density curves for martingale residuals at selected points in time. The grouped Cox model
with complementary log-log link is can be seen to perform slightly better, the distribution of the resid-
uals is more centered around zero for this model.

5 Application I: Unemployment Durations
67
í0.50
í0.25
 0.00
 0.25
0
5
10
15
20
25
month
í1.0
 0.0
 1.0
feb
apr
jun
aug
oct
dec
month
1997
í1.0
 0.0
 1.0
1996
1998
2000
2002
2004
2006
year
í0.3
 0.0
 0.3
 0.6
1996
1998
2000
2002
2004
2006
historical time
f(historical time)
í1.0
í0.5
 0.0
 0.5
20
30
40
50
í0.50
í0.25
 0.00
 0.25
7.5
10.0
12.5
15.0
17.5
f(casmin)
Figure 5.5: Estimated nonlinear effects with 5% and 95% posterior quantiles

68
5 Application I: Unemployment Durations
gcoxda
gcoxnoda
probit
í0.6
í0.3
0.0
0.3
0.6
í0.25
0.00
0.25
0.50
0.75
í0.2
0.0
0.2
0.4
í0.6
í0.4
í0.2
0.0
0.2
0.4
í0.2
0.0
0.2
í4
í2
0
2
4
6
í2.5
0.0
2.5
β1
ζ1
ζ2
ζ3
ζ4
logxi4
logxi2
0
2500
5000
7500
10000
0
2500
5000
7500
10000
0
2500
5000
7500
10000
Figure 5.6: Trace plots for selected parameters. The results have been centered for visualization pur-
poses. Left column, IWLS proposals with data augmentation, middle column IWLS proposals without,
right column probit model.

6 Continuous Time Models
In this chapter, models for continuous time will be discussed. It is structured as
follows: The lognormal model and some extensions based on it are discussed ﬁrst,
followed by relative risk models. For relative risk models, there is a uniﬁed ap-
proach for estimation based on IWLS proposals. The section on relative risk is
split into a section on parametric relative risk models, where the baseline hazard is
fully parameterized and nonparametric relative risk models, where the (log) base-
line hazard is modeled via P-splines.
6.1 Lognormal and Extensions
The lognormal distribution is characterized by the following quantities:
probability density function f(ti|μ,σ) =
1
ti
√
2πσ2 exp(−1
2σ2 (logti −μ)2)
survivor function g(ti|μ,σ) =1−Φ(logti −μ
σ
).
Unlike many distributions which will be discussed in the following, the hazard of
the lognormal distribution is nonmonotone and the support is given by (0,∞). The
shape of the hazard is controlled by σ > 0, μ ∈(−∞,∞) is a location parameter. As
can be seen in ﬁgure 6.1 the lognormal hazard is zero at t=0, reaches a maximum
and decreases in the following. This shape is often observed, e.g. for marriage dis-
solution by divorce (Lawless 2003, pp. 22–23). Covariates are usually introduced
through μi = ηi. As the expectation and median are given by exp(μi +σ2/2) and
exp(μi), parameter interpretation is straightforward in terms of those quantities,
bot clearly not in terms of the hazard rate given by,
h(ti|μ,σ2) =

1
ti
√
2πσ2 exp(−1
2σ2 (logti −μi)2)
	
1−Φ(logti −μi
σ
)

.
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_6, © Springer Fachmedien Wiesbaden 2015

70
6 Continuous Time Models
σ = 0.25
σ = 0.25
σ = 0.5
σ = 0.5
σ = 1.2
σ = 1.2
0
2
4
6
0
2
4
6
t
hazard
0.0
0.5
1.0
1.5
0
2
4
6
t
density
(a)
(b)
0.00
0.25
0.50
0.75
1.00
0
2
4
6
t
survivor function
(c)
Figure 6.1: (a) hazard (b) density (c) survivor function of the lognormal distribution
Here the linear predictor for the lognormal model has the form
ηηη = ZZZβββ +W
W
Wααα +BBB1(xxx1)+...+BBBm(xxxm),
where no effect is allowed to depend on time and no time varying covariates are
included. Working with the observed data likelihood under censoring usually in-
volves Metropolis-Hastings steps, evaluation of the likelihood involves computa-

6.1 Lognormal and Extensions
71
tion of ΦΦΦ for censored failure times which must be done via numerical integration.
A completion Gibbs sampler bypasses this problem, using the unknown failure
times as latent variables. As logT = Y is distributed as N(μ,σ2), a Gibbs sampler
can be set on the log scale, which is easier (Sha et al. 2006). Sampling proceeds
using sampling scheme 1 with an additional block for the latent failure times. This
can be done for right and interval censoring, with data given by
D = {([li,ri),zzz⊤
i )n
i=1},
where it is known that the failure occurs in the interval [li,ri). For all censored in-
dividuals, the latent data is drawn from TN[logli,logri)(ηi,σ2). Given the complete
data, the Gibbs sampler can proceed exactly as sampling scheme 1. A distribution
from the family of scale-mixtures can be used, adding additional blocks for scale
parameters. As robust alternative the t-distribution can be used for logT. Fur-
thermore using that logT is Gaussian, the available methodology for the normal
distribution can be applied. Some relevant methods, which are beyond the scope
of this thesis but should be mentioned are:
• Mixture of normal distributions for logT to achieve more ﬂexibility.
• Multivariate modeling of failure time and longitudinal data using the multi-
variate normal distribution.
It should be noted that given complete data, many other distributions addition-
ally used in failure time analysis belong to the exponential family. A completion
Gibbs sampler combined with IWLS proposals for untractable conditionals might
be applied, adding a block for additional parameters, using the response function
E[Ti|ηi] = exp(ηi). This applies e.g. for the gamma and the inverse Gaussian
distribution, which are sometimes used for failure time modeling.

72
6 Continuous Time Models
6.2 Relative Risk Models
This section is concerned with regression models where the hazard can be written
in the form
h(ti|ηi,ΨΨΨ) = h0(ti|ΨΨΨ)exp(ηi) = exp(ηi),
(6.1)
where ηi is the linear predictor without the effect of time, ηi = ηi +log(h0(ti|ΨΨΨ))
and h0(ti|ΨΨΨ) is the baseline hazard, depending on ΨΨΨ. Time varying variables are
possible but for now it is assumed that all covariates are constant over time. In this
case, the likelihood for models with hazard rate 6.1 is
L =
n
∏
i=1
Li =
n
∏
i=1
{h0(ti)exp(ηi)}vi exp(−exp(ηi)H0(ti)),
where the conditioning in the likelihood on parameters is surpressed for notational
simplicity and H0(ti) =
 ti
0 h0(u)du is the cumulative baseline hazard. As shown
by Aitkin and Clayton (1980), the likelihood can be written as:
L =
n
∏
i=1
Li[H0(ti)
H0(ti)]vi
=
n
∏
i=1
[H(ti)vi exp(−H(ti))]
 h0(ti)
H0(ti)
vi
,
(6.2)
here
H(ti) = exp(ηi +oi) =
 ti
0 exp(η),
and oi = logH0(ti|ΨΨΨ). Deﬁning the censoring indicators as dependent variables,
the likelihood 6.2 is proportional to a Poisson likelihood with log link and mean
H(ti). For parametric relative risk models, H0(ti) can usually be given in closed
form. For left-truncated failure times, H(ti) is changed to
log
 ti
tri
h0(u)du,
where tri is the truncation time and equals 0 for individuals with untruncated fail-
ure times. Hence time-varying covariates can be included by episode-splitting

6.2 Relative Risk Models
73
as described in section 2.3.2. Estimation can be done using sampling scheme
2 or 3 for Poisson responses, using linear predictor ηi + oi, for dependent vari-
ables vi,i = 1,...,n with an additional block introduced for ΨΨΨ (Gamerman 1997).
Weights are given by diag(H(t1),...,H(tn)) and working observations
˙yi = ηi +vi/H(ti)−1,i = 1,...,n,
(6.3)
the proposal distribution, for example for ζζζ j with prior
f(ζζζ|ξ 2) ∝exp(−1
2ξ 2ζζζtKKKζζζ)
is given as N(mmmζζζ j,ΣΣΣζζζ j), with covariance matrix
ΣΣΣζζζ j = [ZZZ⊤W
W
WZZZ +KKK j/ξ 2]−1,
and expectation
mmmζζζ j = ΣΣΣζζζ j(ZZZ⊤W
W
W[˙y˙y˙y−ηηη +BBB jζζζ j]).
A tailored proposal for nonparametric relative risk models with linear predictor
ηηη = ZZZβββ +W
W
Wααα +BBB1(x1)+...+BBBm(xxxm),
(6.4)
as for discrete time models was developed using steps analog to the development
of IWLS proposals by Hennerfeind (2006). Here xxx1 = ttt, BBB(t) describes the log-
baseline and all basis functions are allowed to further depend on time, so that
h(ηi) = exp(ηi). The proposal distribution was obtained by approximating the
loglikelihood by a second order Taylor expansion around the current value ζζζ c
j,
giving the kernel of a multivariate normal distribution with covariance matrix and
expectation given by
ΣΣΣ j = (KKK j −HHH(ζζζ c
j))−1,μμμ j = ΣΣΣ j[s(ζζζ c
j)−HHH(ζζζ c
j)ζζζ c
j],
where HHH is the Hesse matrix and s(ζζζ c
j) is the score vector. Plugging the expressions
for the derivates, the proposal distribution can be represented in terms of working

74
6 Continuous Time Models
observations, which have the same form as 6.3; where
H(ti) =
 ti
0 exp(ηi(u))du
for an untruncated and
 ti
tri exp(ηi(u))du for a truncated failure time. A general
sampling scheme for relative risk models is given in the following:
Sampling scheme 6
1 Draw parameters by sampling scheme 2 using weights H(t1),...,H(tn)) and
working observations
˙yi = ηi +vi/H(ti)−1,i = 1,...,n
2 Draw ΨΨΨ
Sampling scheme 3 might also be used for the ﬁrst step although this does not seem
to have been done in the literature. If there are no time varying effects and if the
cumulative baseline hazard can be given in closed form, all necessary quantities
can be calculated exactly. Otherwise, integrals of the form
 ti
0 exp(logh0(ti|ΨΨΨ(s))+∑∑xi jζ (s)
i
Bi(ti))
have to be solved for the evaluation of the likelihood and to compute the weights.
Hennerfeind (2006) uses numerical integration based on a trapezoid rule, which
approximates an integral of the form
 u
l f(u)du by
J
∑
i=2
[k⋆
i −k⋆
i−1] f(k⋆
i )+ f(k⋆
i−1)
2
,
with knots k1 = l < k2 < ... < kJ = u. For interval-censored data, data augmenta-
tion might principally be used if it is possible to obtain draws from the conditional
distribution of the failure times, which conditional on the data are always truncated
to [li,ri). For parameteric relative risk models ΨΨΨ is usually given by 1-2 positive
parameters. A simple method to update these parameters via Metropolis-Hastings

6.2 Relative Risk Models
75
updates is to use a random walk on the log scale, which corresponds to a multi-
plicative random walk on the original scale (Dellaportas and Roberts 2003, p. 7).
The acceptance probability is
min( f(α p,θθθ|D)α p
f(αc,θθθ|D)αc ,1).
Another possibility, used for example by Konrath (2013) for the Weibull distri-
bution, is the use of a gamma distribution centered around the current value as
proposal distribution. More advanced algorithms like the slice sampler or adaptive
reject sampling might also be used.
As for the grouped Cox model, frailties distributed a priori as G(κ−1,κ−1) -
with an appropriate hyperprior for κ - can always be used as the gamma distribu-
tion is conditional conjugate to the Poisson distribution (Henschel et al. 2009). For
the hazard
hi(t j) = h0(t j)exp(ηi j)wi,i = 1,...,I, j = 1,...,ni,
the conditional distribution for frailties wi,i = 1,...,J is
wi ∼G(κ−1 +
ni
∑
j=1
vi j,κ−1 +
ni
∑
j=1
exp(oi +ηij)),
where oi is the integral 6.2 for the full predictor 6.4. In the following, some impor-
tant distributions from the relative risk family are discussed. Estimation for these
can always proceed using sampling scheme 6, with linear predictor 6.3 where BBB(ttt)
is replaced by a closed form expression for log(h0(ttt)). The support of all discussed
distributions is given by [0,∞).

76
6 Continuous Time Models
6.2.1 Exponential Distribution
Under the exponential distribution the hazard rate is given by the single parameter
h(ti|ϒ) = ϒ. The distribution is characterized by the following quantities:
probability density function f(ti|ϒ) =ϒexp(−tiϒ)
survivor function G(ti|ϒ) =exp(−tiϒ)
cumulative hazard rate H(ti|ϒ) =tiϒ
hazard rate h(ti|ϒ) =ϒ
expectation E(Ti|ϒ) = 1
ϒ.
Due to the fact that the hazard rate is constant over time, it holds that the distribu-
tion has a property called lack of memory:
G(T > t +x|T > x) = G(t).
The exponential distribution is a special case of the Weibull distribution if the
corresponding shape parameter equals 1, see ﬁgure 6.2. Writing ϒ = exp(β0), the
hazard is exp(η), so that h0(ti) = 1 and oi is given by log
 t
0 1du = logti,i = 1,...,n
for time-constant effects, which is free of additional paramaters, hence for a Gibbs
sampler oi,.i = 1,...,n can be computed at iteration 1 and stay unchanged. The
ratio h0(ti)/H0(ti) = 1/ti in the likelihood 6.2 is a multiplicative constant that is
free of parameters and can be ignored.
Assuming a time-constant hazard is often a stark oversimpliﬁcation, the distribu-
tion is nevertheless of practical importance: Predictions based on the exponential
have been shown to be more robust against misspeciﬁcation, compared to distri-
butions with more complicated hazard rates. The distribution is often used in this
context, especially in the absence of covariates or when the inclusion of covariates
is problematic (Thall et al. 2005). Under right censoring for a homogenous popu-
lation, the distribution is conjugate to the gamma distribution. Under a G(α0,ψ0)
prior for ϒ, the posterior distribution of ϒ is G(α0 +d,ψ0 +∑n
i ti). Furthermore the
posterior predictive distribution is available in closed form and given by an inverse

6.2 Relative Risk Models
77
beta distribution (Ibrahim et al. 2001b, pp. 31–32).
The distribution can be used as building block to more complex models. The ex-
ponential model can be generalized to the piecewise exponential model discussed
in 6.3.1, where the time axis is partitioned into intervals and the hazard is assumed
to be constant in every interval. Another generalization is the hyperexponential
distribution, which is a mixture of exponential distribution, and does not have a
relative risk hazard.
6.2.2 Weibull Distribution
The Weibull distribution is probably the most widely used distribution for fail-
ure time analysis. Use of the distribution can be motivated by the fact that the
minimum of i.i.d. random variables can be shown to be approximately Weibull
distributed. The random variables can be viewed as failure times associated with
different causes of failure competing with each other, so that their minimum is the
observed failure time (Wienke 2010, p. 31). In a relative risk context, the distribu-
tion is characterized by the following quantities:
probability density function f(ti|α,γ) = αγtα−1
i
exp(−γtα
i )
survivor function G(ti|α,γ) = exp(−γtα
i )
cumulative hazard rate H(ti|α,γ) = γtα
i
hazard rate h(ti|α,γ) = γαt(α−1)
i
expectation E[Ti|α,γ] = α−1/γΓ(1+ 1
γ ),
where Γ(s) =
 ∞
0 xs−1 exp(−x)dx is the gamma function. The linear predictor
is usually introduced through log(γi) = ηi so that h0(ti) = αtα−1
i
. The Weibull
distribution is the only distribution with relative risk hazard that is also a member
of the log-location-scale family. The variable logTi = Yi can be written as
Yi = η⋆
i +σWi,
where σ = 1/α and η⋆
i = −σηi, and Wi follows the standard extrem value distri-

78
6 Continuous Time Models
α  
α  
α  
α  
α  
α  









W
KD]DUG










W
GHQVLW\
(a)
(b)










W
VXUYLYRUIXQFWLRQ
(c)
Figure 6.2: (a) hazard (b) density (c) survivor function of the Weibull distribution
bution from the location-scale distribution with pdf fW(y) = exp(y−exp(y)).
As such parameter interpretation is possible in terms of the hazard and the fail-
ure time. The distribution is relatively ﬂexible. The parameter α controls the shape
of the hazard rate: For α > 1 the hazard is monotonically increasing, for 0 < α < 1
it is monotonically decreasing (see ﬁgure 6.2), for α = 1 a time-constant hazard is
obtained giving the exponential distribution as special case. A gamma prior is of-
ten used for α. To capture hazards of other shapes, many extensions of the Weibull

6.2 Relative Risk Models
79
distribution have been proposed, these are discussed by Murthy et al. (2004).
6.2.3 Other Baseline Hazards
Other distributions with relative risk hazards but somewhat limited usability are the
Gompertz and the Pareto distribution. The Gompertz baseline hazard is exp(ρt),
ρ ∈(−∞,∞). As such the distribution is appropriate exclusively for applications
with exponentially growing hazard - a major application is modeling of human
adult mortality. The Pareto distribution has baseline hazard (w+t)−1 and survivor
function ((1 +t/w)−1)exp(ηi), w>0. This hazard shape is not very interesting, the
deﬁning property of the distribution are its wide tails which might make the dis-
tribution attractive for robust modeling of outliers. Both the Gompertz and the
Pareto distribution can be defective for certain parameter values (Wienke 2010,
pp. 33–37). A relative risk hazard with a nonmonotone baseline hazard which is
always nondefective is given by the sickle model by Diekmann and Mitter (1983).
The hazard is given by h(t|a,b) = aexp(logt −t
b), a,b>0. The shape of the haz-
ard is similar to the lognormal distribution. The survival fraction is exp(−ab2).
Covariates can be introduced via ai = exp(ηi).
A hazard of the form 6.1 for a nondefective random variable can always be pro-
duced by using an appropriate function for h0(ti|ΨΨΨ), so the approach is actually
fairly ﬂexible. The baseline hazard might be generated by theoretical considera-
tions, for example Flinn and Heckman (1982) use
h0(ti|ΨΨΨ) = exp(γ0 +γ1
tλ1
i
−1
λ1
+γ2
tλ2
i
−1
λ2
),
based on the Box-Cox-transformation, derived from considerations regarding du-
ration dependence. A convenient choice is to use the hazard of an arbitrary dis-
tribution for h0 (Cox and Oakes 1984, p. 73). Under this approach the resulting
density in general does not correspond to a known distribution. Draws from the
predictive distributions can still be obtained by the method described in 3.3 in the
context of the L-measure, using the hazard rate.

80
6 Continuous Time Models
1
2
3
4
5
0.00
0.25
0.50
0.75
1.00
t
hazard
Figure 6.3: Example of a bathtub shaped hazard
6.3 Nonparametric Relative Risk Models
The ﬂexibility of parametric models is for many applications insufﬁcient. For
example many hazard shapes observed in practice can be described as bathtub-
shaped (Bebbington et al. 2006): Here the hazard consists of a decreasing part,
followed by approximately constant hazard, followed by an increasing hazard, see
ﬁgure 6.3. None of the aforementioned distributions can give such a hazard with-
out adjustments. While a distribution corresponding to a speciﬁc shape of hazard
usually can be found, it is often preferable to estimate the hazard from the data by
ﬂexible methods, especially under the absence of information about the shape of
the hazard. In the following such nonparametric models are discussed.
6.3.1 Piecewise Exponential Hazard
The piecewise constant model is a simple but useful model, for which estimation
is relatively easy and interval-censoring can be accounted for. The time axis is par-
tioned into m intervals [s0 = 0,s1),[s1,s2),...,[sm−1,sm = ∞). The baseline hazard
in every interval is approximated by a constant, the hazard of an exponential dis-
tribution:

6.3 Nonparametric Relative Risk Models
81
h0(t) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
λ1
if t ∈[s0,s1)
λ2
if t ∈[s1,s2)
...
λm
if t ∈[sm−1,sm).
Lawless (2003, p. 385) shows that if the number of knots tends to inﬁnity, the
conditional likelihood of all regression coefﬁcients is given by
d
∏
i=1
exp(ηi)
∑j∈R(i) exp(ηi),
the partial likelihood, see chapter 2.4. Hence for a large number of knots, estimates
from piecewise constant model should approximately agree with estimates based
on partial likelihood.
The baseline hazard depends on ΨΨΨ = (λ1,...,λm)⊤. The likelihood contributions
under right censoring are
hvi
i exp(−
m
∑
j=1
d jhi j),
where hi = h0(ti)exp(η(ti)), ∑m
j=1 d jhi j is the cumulative hazard rate and dij =
min(ti,sb)−sb−1 is the time individual i spent in interval j. The likelihood is
L =
n
∏
i=1
m
∏
j=1
(hi)vij exp(−di jhij).
(6.5)
As shown by Laird and Olivier (1981) and Holford (1980), 6.5 is proportional to
a Poisson likelihood with dependent variables given by vij = I[ti ∈[s j−1,s j)] and
offsets logdij,i = 1,...,n, j = 1,...,m. Hence, in analogy to the models of section
6.2, GLM methodology as described in 4.1 can be applied. The data set must be
expanded similar as for discrete time models, as can be seen in table 6.1. On the
left is a data set before expansion, on the right after expansion.
Time varying covariates can be included by varying covariate values between
the intervals. The parameters logλ1,logλ2,...,logλm and piecewise constant vary-
ing time-dependent effects can be estimated using P-splines of order 0. The knot

82
6 Continuous Time Models
Table 6.1: Data set expansion for piecewise constant model, given knots 0, 2, 5, 8
id
t
v
z1
id
interval
v
z1
offset
1
4
0
3
1
1
0
3
1.1
2
5.3
1
5
1
2
0
3
.7
2
1
0
5
1.1
2
2
0
5
1.1
2
3
1
5
-1.2
placement strategy given in section 3.4.1 can be used to determine the intervals. Of
course the model can also be estimated without the data expansion. An alternative
to the normal prior for logλ1,...,logλm is the use of a gamma prior for λ1,...,λm.
Similar to the random walk prior for P-splines, correlation can be induced to pe-
nalized abrupt jumps. For example, Aslanidou et al. (1998) use
λk|λk−1,λk−2,...,λ1 ∼G(κk, κk
λk−1
),
the variance parameter κk controls the penalization.
Strictly speaking, the piecewise constant model is a fully parametric model, as a
piecewise exponential distribution generalizing the exponential distribution can be
deﬁned. Under interval-censoring, unknown failure times can be drawn from this
distribution for the construction of a completion Gibbs sampler. This approach has
been used by Henschel et al. (2009) in connection with IWLS proposals. A draw
from f(ti|D,θθθ) can be obtained by a two-step procedure. Suppose it is known that
ti lies between a0 = Li < a1 < ... < ap = Ri, where a2,...,ap−1 are knots. A draw
can be obtained as follows (Wang et al. 2012b):
Sampling from the truncated exponential distribution
1 Determine the interval by drawing (i1,...,ip−1) from a multinomial
distribution with parameter ααα = (α1,...,αp−1)⊤and size 1, where
αk ∝G(ak−1)−G(ak), if ik = 1, the failure time is drawn from [ak−1,ak)
2 Conditional on ik, draw the failure time from an exponential distribution with
rate λi exp(ηik), truncated to the interval [ak−1,ak)

6.3 Nonparametric Relative Risk Models
83
For step 2, draws from a truncated piecewise exponential distribution can be eas-
ily obtained by the inverse cdf method as the inverse of a cdf of the truncated
exponential distribution can be found closed form.
6.3.2 Nonparametric Relative Risk Models
The underlying baseline hazard is rarely a step function, so approximating it by
a smooth function is often more appropriate. A natural extension of the piece-
wise exponential model is the use of splines of higher order. Estimation proceeds
using sampling scheme 6, here ΨΨΨ is given by a block of spline-coefﬁcients corre-
sponding to the matrix of basis functions B(t). Proof of propriety of the posterior
distribution under fairly general conditions is given by Hennerfeind et al. (2006).
Noted by Hennerfeind (2006) are some computational complications and adjust-
ments. For splines of order > 1, numerical integration is always necessary. For
P-splines of order 1, corresponding to a piecewise Gompertz model, an analytical
solution is possible but very cumbersome to compute so that numerical integration
is preferable. As a simpler alternative to IWLS proposals, conditional prior pro-
posals (Knorr-Held 1999) can be used for regression coefﬁcients corresponding to
basis functions depending on time. Giving the conditional mean of the prior in
terms of standart formulae for the multivariate normal distribution is not possible,
as the prior is not proper. The conditional moments can instead be given in terms
of the penalty matrix (Fahrmeir and Lang 2000):
Supressing most indices for simplicity, suppose the subblock
ζζζ[r : s] = (ζr,ζr+1,...,ζs)⊤,r ≤s
from ζζζ = (ζ1,...,ζr−1,ζr...ζs,ζs+1,...,ζm)⊤is updated. Let KKK[r : s,r : s] denote the
submatrix of the penalty matrix corresponding to ζζζ[r : s], so that KKK[r : s,1 : r −1]
and KKK[r : s,s + 1 : m] are the matrices to the left and to the right. The conditional
expectation of ζζζ[r : s] given all remaining ζζζ is:

84
6 Continuous Time Models
E[ζζζ[r : s]|·] =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
−ξ 2KKK[r : s,r : s]−1KKK[r : s,(s+1) : m]ζζζ[s+1 : m]
if r = 1
−ξ 2KKK[r : s,r : s]−1KKK[r : s,1 : (r −1)]ζζζ[1 : (r −1)]
if s=m
−ξ 2KKK[r : s,r : s]−1
KKK[r : s,(s+1) : m]ζζζ[s+1 : m]+
KKK[r : s,1 : (r −1)]ζζζ[1 : (r −1)]

else.
The conditional variance is ξ 2KKK[r : s,r : s]−1. Here, the acceptance probability for
Metropolis-Hastings update steps simpliﬁes to
α(βββ p,βββ c) = min(L(βββ β
p)
L(βββ β
c )
,1),
the ratio of likelihoods. This simpliﬁes computation but mixing is worse compared
to IWLS proposals. Autocorrelation can be decreased by letting the blocksize vary
randomly between iterations. An alternative approach might be to use sampling
scheme 3 where the moments of the proposal distribution only have to be computed
once.
A simpler, approximate approach is used by Lambert and Eilers (2005). Here
the time axis [0,τ], where τ is the maximal observed failure time, is partitioned
into intervals - here called bins - Ij, j = 1,...,J with midpoints Δ1,...,ΔJ. In every
interval the number of failures follows a Binomial distribution with parameters
e j given by the number of failures in bin j and pj given by the probability of
failure in bin j. The authors use a large number of intervals and approximate the
binomial distribution by a Poisson distribution with parameter μj = rjpj, where rj
are the number of individuals at risk during interval j and use P-splines to smooth
p1,..., pJ. Based on a similar approach, Yavuz and Lambert (2011) estimate the
parameters of the basic proportional hazard model under interval censoring. Under
the PH model we have,
G(ti|zzz,βββ) = exp(−
 ti
0 h(u|zzz,βββ)) = exp(−H0(ti)exp(ηi)) = G0(ti)exp(ηi),
and
G0(ti) = 1−
 ti
0 f0(u)du.
(6.6)

6.3 Nonparametric Relative Risk Models
85
The binning approach is used to model f0. Here
pj = P(Li < Ti < Ri|zzz = 0) =

Ij
f0(u)du
is approximated by f0(u j)Δj, where Δj is the midpoint of interval Ij. Then, the
likelihood contribution of an individual with standard conditions zzz = 0 can be writ-
ten as
li = ∑
i=J
pjvi j +dj(1−α),
(6.7)
where vij = I[ti ∈Ij], d j = I[Ri > τ] and α = P(Ti > τ|zzz = 0). Then, P(ti ∈Ij|zzz = 0)
is estimated via P-splines coefﬁcients ζ1,...,ζk by
ˆp j = ˆα
exp(γ j)
exp(γ1)+...+exp(γJ),
where γj = ∑k
p=1 ˆζpBp(uj). It follows that f0(t) ≈p j/Δ j, so that the likelihood
contribution
li = G(li|zzz,βββ)−G(ri|zzz,βββ) = G0(li)exp(ηi) −G0(ri)exp(ηi)
is derived by plugging pj/Δ j = f0 into 6.6. Yavuz and Lambert (2011) use Gaus-
sian random walk proposals on a reparametrized posterior distribution to explore
the posterior. An exact, fully Bayesian application of the relative risk model under
interval censoring does not seem to have been developed yet.

7 Application II: Crime Recidivism
In the second application, a data set on crime recidivism is used. The data set
has been previously analyzed and obtained by Schmidt and Witte (1988). The data
was obtained from ﬁles on inmates of a North Carolina prison who were released
between July 1979 until June 1980. It can be freely downloaded from the web-
page https://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/8987?q=
ICPSR as of 06/27/2014. The data set was assembled in 1984, the observation
windows equals 81 month. The original sample is of size 9327, after deleting
missing values 4628 cases remain. Failure time is deﬁned as time until return to
prison. Due to rounding, the failure times are interval-censored. Failure times
are rounded to the nearest month, so that return to prison after 15 days would be
coded 1, return after 16 days until the 15th of the following month would be coded
2. The month of release in addition to the month of return to prison are available,
so the exact interval in which return to prison occured can be reconstructed. The
covariates used for ﬁxed effects in the model are
• white binary variable indicating if an individual is white1 (referance cate-
gory)
• drugs binary variable indicating if an individual’s prison records indicates a
serious problem with hard drugs or alcohol (before entering prison)
• married binary variable indicating if an individual was married at the time
of prison entry
• felony binary variable indicating if an individual’s crime was a felony (ref-
erence category: crime against a person)
1Using the coding of Schmidt and Witte (1988), all non-blacks are coded as whites.
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_7, © Springer Fachmedien Wiesbaden 2015

88
7 Application II: Crime Recidivism
Table 7.1: Model assessment
LPML
DIC
pD
deviance
piecewise constant
-8691.622
17357.72
25.35715
17332.36
lognormal
-9109.098
18118
54.52985
18064.26
• workprogram binary variable indicating if an individual’s took part in a
program aiding job search
• supervised binary variable indicating if an individual was supervised after
release (for example for parole)
• property binary variable indicating if an individuals crime was against a
property (reference category: crime against a person)
• other binary variable indicating crimes neither against a person nore against
a property (reference category: crime against a person)
Furthermore, following covariates are modeled as smooth functions via P-splines:
• rule violations number of prison rule violations per month during the prison
stay
• age age, in years at time of release
• school years number of formal schooling years
• time served time served (in months) for prison sentence
For the nonlinear effects, a difference penalty of order 2 with 30 knots were used
for all function estimates. 8000 iterations were run, the ﬁrst 1000 were taken as
burnin.
A lognormal and a piecewise constant model were ﬁtted. The lognormal model
was chosen as a nonmonotone hazard seems more plausible than a monotone for
this process. The usual inverse gamma prior was used for σ2 with hyperparam-
eter set to 0.001. For the piecewise constant model, a completion gibbs sampler

7 Application II: Crime Recidivism
89
lognormal
piecewise constant
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
í0.4
0.0
0.4
0.8
0.5
1.0
1.5
2.0
beta
exp(beta)
drugs
felony 
married
other crime
property crime
supervised
white
workprogram
drugs
felony 
married
other crime
property crime
supervised
white
workprogram
fixed effects
Figure 7.1: Estimated ﬁxed effects for both models with 5% and 95% quantiles. The effects for the
lognormal model are on the failure time, the effects of the piecewise constant hazard on the hazard rate.
based on a data-expansion with an additional block for failure times as described
in section 6.3.1 was used. Sampling of failure times slows down the sampler con-
siderably, however. Finding the interval in which the failure time lies was the most
elaborate part, after this is achieved the sampling of the truncated exponential dis-
tribution can be quickly done by the inverse cdf method. The sampling scheme for
the lognormal model is based on sampling scheme 1, with additional block for the
logfailure times. As for the probit model, sampling of the latent failure time adds

90
7 Application II: Crime Recidivism
0
10
20
30
40
0
2000
4000
neff
count
group
lognormal
piecewise constant
H
Hí
Hí






QHII
GHQVLW\
JURXS
ORJQRUPDO
SLHFHZLVHFRQVWDQW
Figure 7.2: Histograms and kernel density estimates for the effective number of parameters, obtained
via the coda package (Plummer et al. 2006).
0
1
2
3
0
500
1000
1500
2000
2500
days
baseline hazard
Figure 7.3: Baseline hazard for the piecewise
constant model with 5% and 95% quantiles.
0.000
0.025
0.050
0.075
0.100
0
500
1000 1500 2000 2500
days
baseline hazard
Figure 7.4: Baseline hazard for the lognormal
model with 5% and 95% posterior quantiles.
Note that the posterior quantiles for the log-
normal model are very narrow and barely vis-
ible.
hardly and computational burden is done via the function rtruncnorm. Both were
again implemented in R.
Diffuse priors were assigned to all ﬁxed effects. The baseline hazard was esti-
mated using a sum-to-zero constraint which seemed more natural here.

7 Application II: Crime Recidivism
91
í2.0
í1.0
 0.0
 1.0
0.0
0.4
0.8
1.2
rule violations/month
f(rule violations/month)
í1.0
í0.5
 0.0
 0.5
 1.0
0.0
0.4
0.8
1.2
rule violations/month
f(rule violations/month)
í6.0
í4.0
í2.0
 0.0
0
100
200
months served
f(months served)
0.0
2.0
4.0
6.0
0
100
200
months served
f(months served)
Figure 7.5: Estimated effects for both models with 5% and 95% posterior quantiles. The stripes repre-
sent unique observations. Left column: lognormal, right column: piecewise constant.
Mixing wise, the sampling scheme for the piecewise constant model did not
perform very well, some selected sampling paths showing this can be seen in 7.7.
This is probably due to the fact that the baseline coefﬁcients are very strongly
inﬂuenced by the latent failure times and vice versa. Estimated baseline hazard
can be seen in 7.3 and 7.4. The LPMPL and the DIC favor strongly the more
ﬂexible piecewise constant model.

92
7 Application II: Crime Recidivism
Surprisingly, the model complexity term of the DIC is lower for the piecewise
constant model which is due to lower smoothing variances of this model. Estimates
of ﬁxed effects are visualized in ﬁgure 7.1. They are not directly comparable, still
it can be seen that the models agree on the direction of effects, in the sense that a
positive effect on the baseline hazard corresponds to a negative effect for the failure
time. The effects seem mostly sensible; the negative inﬂuence of the workprogram
on time until return to prison does seem suspect, it might be an artifact or might be
related to the assignment procedure for the workprogram.
The estimated nonlinear effects can be seen in ﬁgure 7.6 and 7.5. The same in-
verse relation between hazard of piecewise constant and failure time of lognormal
variable holds here. As can be seen some of these estimated effects are driven by
individuals with very large covariate values, leading to very large posterior inter-
vals for sparse data regions as it is the case for the variable rule violations, where
the effects for sparse data regions become very variable. For regions with more
data (<0.5 rule violations/month), the estimated effect is almost linear. For the
time served, it can be seen that the effect levels off around circa 5 years of jail time,
the change in slope for the piecewise constant model around 150 months should
not be overinterpreted due to the large posterior interval. The nonlinear effect of
age is estimated with very little variance and seems very sensible: Imprisonment
during younger years should be much more disruptive compared to later years. In
conclusion it can be said that the models agree with the general direction of effects
but that the piecewise constant model ﬁts better.
An alternative modeling approach should be mentioned. An interesting question
might be if individuals return to prison at all. While some defective distributions
mentioned in this thesis could be used, the more interesting approach would be the
use of so called cure rate models, which were not discussed in this thesis. Here
the probability that an individual is "cured", so that failure becomes impossible, is
explicitly modeled via a two-component mixture distribution.

7 Application II: Crime Recidivism
93
0.0
0.5
1.0
1.5
2.0
5
10
15
education years
f(education years)
í2.0
í1.5
í1.0
í0.5
 0.0
5
10
15
education years
f(education years)
í1.0
 0.0
 1.0
 2.0
 3.0
20
40
60
80
age
f(age)
í3.0
í2.0
í1.0
 0.0
 1.0
20
40
60
80
age
f(age)
Figure 7.6: Estimated effects for both models with 5% and 95% posterior quantiles. The stripes repre-
sent unique observations. Left column: lognormal, right column: piecewise constant.

94
7 Application II: Crime Recidivism
lognormal
piecewise_constant
í0.8
í0.4
0.0
0.4
í0.2
í0.1
0.0
0.1
0.2
í0.2
0.0
0.2
0.4
í0.2
í0.1
0.0
0.1
0.2
0.3
í1
0
1
í3
í2
í1
0
1
2
3
í3
í2
í1
0
1
2
β1
baseline
ζ3
ζ4
ζ1
logxi4
logxi2
0
500
1000
1500
0
500
1000
1500
Figure 7.7: Selected sampling paths, both samplers for the last 1000 iterations. Blue vertical line is the
effective number of draws. As can be seen, acceptance rates are low for piecewise constant model and
mixing is better for the lognormal model. Note that all parameters have been centred for visualization
purposes. Convergence for the smoothing variances is slow.

8 Summary and Outlook
Bayesian methods for the analysis of failure time data in continuous and discrete
time using P-splines have been presented and applied. The sampling schemes can
be summarized in the following way: (1) Sampling schemes where by introduc-
tion of latent variables Gaussian conditionals are obtained, then using the basic
sampling scheme for the Gaussian likelihood. (2) Sampling schemes using IWLS
proposals when full conditionals are not conditional conjugate. It turned out that
with these samplers a broad range of models can be estimated with relative ease.
In the applications, the methods were applied. For the ﬁrst application, IWLS
proposals were tried out in connection with a data augmentation sampling scheme
for the grouped Cox model. The performance of this sampler was found to be
acceptable, the standard sampler using sampling scheme 3 showed some problems
which could not be resolved. For the second application the piecewise constant
model was estimated via completion Gibbs samplers. The mixing of the lognor-
mal sampler was found to be satisfactory. The mixing of the piecewise constant
sampler based on IWLS proposals was found to be improvable but good enough
to base inferences on. In all cases smoothing variances converged slowly.
There are several possible directions for future research. Due to the often men-
tioned Poisson likelihood, a worthwile perspective might be to apply further meth-
ods for the Poisson likelihood. For example, a data augmentation scheme for the
Poisson distribution has been developed by Frühwirth-Schnatter et al. (2009) using
a normal mixture as approximation for two introduced latent variables. This ap-
proximation might be used directly or might be used as a proposal distribution for
Metropolis-Hastings updates. Wang et al. (2012a) use a completion Gibbs sam-
pler for interval-censored data where two latent Poisson variables are introduced,
one of which is distributed as Zi ∼P([H0(ri) −H0(li)]exp(ηi)). An interesting
approach would be to use P-splines for H0(t) instead of h0(t). The function would
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9_8, © Springer Fachmedien Wiesbaden 2015

96
8 Summary and Outlook
have to be restricted to be nonincreasing, this can in fact be achieved for P-splines
by restricting the coefﬁcients to be nonincreasing. This approach would have the
advantage of not requiring numerical integration. The baseline could easily be
obtained by standard formulae for the derivatives of P-splines. In a related note,
other functions might be used for the log-baseline to bypass numerical intergration
for acceptance steps. An interesting idea might be to use a weighted mixture of
hazards; Komárek et al. (2005) use an approach similar to P-splines where a pe-
nalized mixture distribution is used for modeling of the log failure times. In their
approach, the means of the component distribution are treated like knots and the
mixture weights are penalized. This approach might be used for the log-baseline
hazard, using e.g. a penalized mixture of Weibull hazards.
For failure time modeling, the data collection is often very important. An inter-
esting research topic might be modeling techniques that correct for bias resulting
from this by using the methods of this thesis, for example by using ﬂexible mea-
surement error models or a ﬂexible multiple imputation algorithm to counteract
heaping. The probit model might be an interesting choice, heteroskedasty might
be introduced, for example variance might be related nonlinearly to the time dif-
ference between the interview and the beginning of exposure of failure.
Natural extensions are furthermore multiple modes of failure and recurrent fail-
ures. Extending the models to multiple modes of failure is straightforward in the
discrete case; instead of models for binary data, models for polytomous data can be
used. For continuous data, this is not straightforward and the modeling framework
is somewhat different in this context. For recurrent failure models the situation is
similar: Discrete time models for recurrent failures are often based around stan-
dard random effect models, while for continuous time again a different modeling
framework is necessary, here counting process notation, as shortly discussed in the
context of martingale residuals, is very useful.
Other approaches would be to research and compare alternative priors for vari-
ance parameters and different functions for the hazard rate in combination with
P-splines. Alternatives to the relative risk hazard which have been suggested (Sun

8 Summary and Outlook
97
2006, pp. 19–20) are the proportional odds model
G(ti)
1+G(ti) = exp(−ηi)
G0(ti)
1+G0(ti)
and the additive hazards model
h(t|ηi) = h0(ti)+ηi,
which could be analyzed by adjusting the methods of this thesis. For discrete time
there have been suggestions of specialized response functions accounting for the
nature of failure time data. Promising is the response function
P(T = t|T ≥t) = 1−(1+ζ exp(η))−1/ζ,
(8.1)
used by Hess (2009) for discrete failure time data. The function 8.1 ﬁts heavily
skewed data well and nests the response function of the grouped Cox as special
case, inference could be carried out using IWLS proposals.

Appendix A: Description of R Function
To run the Gibbs sampler, a function was implemented in R. The necessary scripts are
available on a CD appended to this thesis. The functions take are optimized exclusively
for the applications and might not generalize well. The usage is outlined here. It should
be noted that regarding computational speed, the functions could probably be improved.
Several packages are necessary to run the functions:
• Matrix (Bates and Maechler 2013) for sparse matrix implementation which was
essentiel for efﬁcient computation and storing of B-spline design matrices.
• splines (R Core Team 2013) for fast creation of basis function matrices. The pack-
age conviently outputs sparse matrices
• rtruncnorm for draws from the truncated normal distribution as discussed in section
5
• Rcpp (Eddelbuettel and François 2011) this package provides a R interface to imple-
ment simple C++ functions which give computational advantages1
• RcppArmadillo (Eddelbuettel and Sanderson 2014) this package includes some
functions necessary for fast matrix manipluation via rcpp
• reshape2 this implements the melt function essential for fast creation of offsets for
the piecewise constant sampler
Furthermore for the rcpp package installation of the program Rtools is necessary. The
functions are implemented in the following ﬁles:
• functions.R
• gibbs_all.R
• cppfunctions.cpp
The ﬁle functions.R contains supporting functions implemented directly in R, the ﬁle
cppfunctions.cpp contains supporting functions implemented in R via the rcpp pack-
age, the ﬁle gibbs_all.R contains the function gibbs_all used for estimation. The ﬁle
1Faster execution speeds could be possible using this more.
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9, © Springer Fachmedien Wiesbaden 2015

100
Appendix A: Description of R Function
main.R can be adjusted for sourcing. After sourcing these ﬁles the function can be used if
all pre-requisites are installed. The function takes the following arguments for all hazards,
in brackets default values are given:
Z
design matrix
Z.splines
design matrix for splines
r(1000)
number of iterations
beta0.Precision (000)
prior precision of ﬁxed effects
beta0
prior mean of ﬁxed effects
l.vector
vector of degrees for splines
d.vector
vector of difference penalty for splines
inter.act.list
list for interactios
lambda.start (0.05)
start value for spline precision
m.vector (30)
vector for number of knots
hyper.a0.vector (0.0001)
vector of hyperparameter a0 for splines
variances
hyper.b0.vector (0.0001)
vector of hyperparameter b0 for splines
variances
hazard
hazard as string:
available are piece-
wise_constant,lognormal, probit, cloglog,
cloglog.poisson (for cloglog via Poisson
data augmentation)
just.objekts(FALSE)
boolean indicating if gibbs sampler should
only be run to give out objekts such as de-
sign matrices for splines
It is always assumes that there is a constant, if no design matrix for ﬁxed effects is given,
it is created. If a design matrix for ﬁxed effects is given and contains no column of ones,
a column one will be created. Counting for all basis function matrices start with ﬁrst col-
umn of will.splines, followed by the basis matrices for varying coefﬁcient terms. The list
inter.act.list is organized as follows: Entries consist of matrices where the ﬁrst col-
umn is effect modiﬁer and the right column is the interaction variable. Following arguments
are speciﬁc for discrete time models:

Appendix A: Description of R Function
101
Z
design matrix
y
vector of failure indicators
time.var
variable giving process time
time.blocked
boolean indicating if seasonal and base-
line hazard parameters are sampled in one
block
seas.var
variable of time measurements, from 1 to
maximal measurement of historical time
per
period length
time.blocked
boolean indicating seasonal coefﬁcients
and baseline hazard paramater are updated
blockwise
Seasonal variables could also be used for continuous time, but this has not been tested.
For the lognormal and piecewise constant model, interval censoring is assumed. Following
arguments are speciﬁc for these hazard:
upper
vector of upper interval boundaries for
failure time
lower
vector of upper interval boundaries for
failure time
res.var.a0 0.001
hyperparameter a for the residual variance
res.var.b0 0.001
hyperparameter b for the residual variance
The ﬁrst entries of d.vector and m.vector for the piecewise constant model are reserved
for the baseline hazard. The function returns a list with the following objects:
gamma
list with matrices of B-spline coefﬁcients
with
beta
matrix of coefﬁcients for ﬁxed effects
taus
matrix of smoothing variances
spline.design.mat
list of entries necessary for plotting of
splines
Furthermore, for the piecewise constant model, the function returns the list baseline, con-

102
Appendix A: Description of R Function
taining a matrix of regression coefﬁcients, a vector of smoothing variances for the baseline
coefﬁcients and a vector of knots. For the lognormal model, the vector sigma containing the
squared shape parameter σ2 is given out. Splines can be plotted via the function plot.splines
which can be found in functions.R.
In the following an example for a function call for discrete time is given:
gibbs.splines(Z=Z,id=data$id,per=12,seas.var=data$season,
time.var=data$t, hazard="probit", r=r,y=data$y,
Z.splines=Z.splines,inter.act.list=inter.act.list,
lambda.start=0.1, l.vector=c(3,3,3,3,3,3,3,3),
d.vector=c(2,2,2,2,2,2,2,2,2))
It has been observed that the start parameter lambda.start is inﬂuental regarding accep-
tance rates, if the function is run with low acceptance rates changing this parameter often
helps. All IWLS proposals are based around sampling scheme 2. The parameters for basis
function coefﬁcients are sampled using the standard sum to zero (over the sample) restric-
tion.

Bibliography
Aalen, O., Ø. Borgan, and S. Gjessing (2008). Survival and event history analysis:
A process point of view. New York: Springer.
Aitkin, M. and D. Clayton (1980). “The ﬁtting of exponential, Weibull and ex-
treme value distributions to complex censored survival data using GLIM”.
In: Applied Statistics 29, pp. 156–163.
Albert, J. H. and S. Chib (1993). “Bayesian analysis of binary and polychoto-
mous response data”. In: Journal of the American Statistical Association 88,
pp. 669–679.
Andrews, D. F. and C. L. Mallows (1974). “Scale Mixtures of Normal Distribu-
tions”. In: Journal of the Royal Statistical Society. Series B (Methodological)
36, pp. 99–102.
Aslanidou, H., D. K. Dey, and D. Sinha (1998). “Bayesian Analysis of Multi-
variate Survival Data Using Monte Carlo Methods”. In: Canadian Journal of
Statistics 26, pp. 33–38.
Banerjee, S. and B. P. Carlin (2004). “Parametric Spatial Cure Rate Models for
Interval-Censored Time-to-Relapse Data”. In: Biometrics 60, pp. 268–275.
Bates, D. and M. Maechler (2013). Matrix: Sparse and Dense Matrix Classes and
Methods. R package version 1.1-1.1.
Bebbington, M., C.-D. Lai, and R. Zitikis (2006). “Useful periods for lifetime dis-
tributions with bathtub shaped hazard rate functions.” In: IEEE Transactions
on Reliability 55, pp. 245–251.
Berg, G. J. Van den (2001). “Duration models: speciﬁcation, identiﬁcation and
multiple durations”. In: Handbook of Econometrics. Ed. by J. Heckman and
E. Leamer. Vol. 5. Amsterdam: Elsevier, pp. 3380–3460.
Blossfeld, H.-P. and G. Rohwer (2002). Techniques of event history modeling: New
approaches to causal analysis. 2nd ed. Mahwah: Lawrence Erlbaum.
M. Kaeding, Bayesian Analysis of Failure Time Data Using P-Splines, BestMasters,
DOI 10.1007/978-3-658-08393-9, © Springer Fachmedien Wiesbaden 2015

104
Bibliography
Blossfeld, H.-P., H. G. Roßbach, and J. von Maurice (2011). “Education as a Life-
long Process: The German National Educational Panel Study (NEPS)”. In:
Zeitschrift für Erziehungswissenschaft, pp. 19–34.
Boor, C. de (2001). A Practical Guide to Splines. New York: Springer.
Brezger, A. and S. Lang (2006). “Generalized structured additive regression based
on Bayesian P-splines”. In: Computational Statistics & Data Analysis 50,
pp. 967–991.
Cameron, A. C. and P. K. Trivedi (2005). Microeconometrics: Methods and appli-
cations. Cambridge: Cambridge University Press.
Carlin, B. and T. Louis (2011). Bayesian Methods for Data Analysis, Third Edition.
Boca Raton: Taylor & Francis.
Cowles, M. K. and B. P. Carlin (1996). “Markov Chain Monte Carlo Convergence
Diagnostics: A Comparative Review”. In: Journal of the American Statistical
Association 91, pp. 883–904.
Cox, D. R. (1972). “Regression Models and Life-Tables”. In: Journal of the Royal
Statistical Society. Series B (Methodological) 34, pp. 187–220.
Cox, D. R. and D. Oakes (1984). Analysis of survival data. London: Chapman and
Hall.
Crowder, M. (2001). Classical Competing Risks. Boca Raton: Taylor & Francis.
Cumbus, C., P. Damien, and S. Walker (1996). Sampling truncated poisson and
multivariate normal densities via the gibbs sampler. Technical Report. Uni-
versity of Michigan Business School.
Dellaportas, P. and G. O. Roberts (2003). “An introduction to MCMC”. In: Spatial
Statistics and Computational Methods. Ed. by J. Moller. New York: Springer,
pp. 1–42.
Devroye, L. (1986). Non-uniform random variate generation. New York: Springer.
Diekmann, A. and P. Mitter (1983). “The "Sickle-Hypothesis": A Time-Dependent
Poisson Model with Applications to Deviant Behaviour and Occupational
Mobility”. In: Journal of Mathematical Sociology 9, pp. 85–101.
Dierckx, P. (2006). Curve and surface splitting with splines. Oxford: Clarendon
Press.

Bibliography
105
Dunson, D. B. and A. H. Herring (2004). “Bayesian latent variable models for
mixed discrete outcomes”. In: Biostatistics 6, pp. 11–25.
Eddelbuettel, D. and R. François (2011). “Rcpp: Seamless R and C++ Integration”.
In: Journal of Statistical Software 40, pp. 1–18.
Eddelbuettel, D. and C. Sanderson (2014). “RcppArmadillo: Accelerating R with
high-performance C++ linear algebra”. In: Computational Statistics and Data
Analysis 71, pp. 1054–1063.
Eilers, P. H. C. and B. D. Marx (1996). “Flexible smoothing with B-splines and
penalties”. In: Statistical Science 11, pp. 89–121.
—
(2010). “Splines, knots, and penalties”. In: Wiley Interdisciplinary Reviews:
Computational Statistics 2, pp. 637–653.
Fahrmeir, L. and T. Kneib (2011). Bayesian smoothing and regression for longitu-
dinal, spatial and event history data. Oxford: Oxford University Press.
Fahrmeir, L. and G. Tutz (2001). Multivariate statistical modelling based on gen-
eralized linear models. 2nd ed. New York: Springer.
Fahrmeir, L., T. Kneib, S. Lang, and B. Marx (2013). Regression: Models, Methods
and Applications. Heidelberg: Springer.
Fahrmeir, L. and S. Lang (2000). “Bayesian Inference for Generalized Additive
Mixed Models Based on Markov Random Field Priors”. In: Journal of the
Royal Statistical Society. Series C 50, pp. 201–220.
Fleming, T. R. and D. P. Harrington (1991). Counting processes and survival anal-
ysis. New York: Wiley.
Flinn, C. and J. Heckman (1982). “Models for the Analysis of Labor Force Dy-
namics”. In: Advances in Econometrics 1, pp. 35–95.
Frühwirth-Schnatter, S., R. Frühwirth, L. Held, and H. Rue (2009). “Improved
auxiliary mixture sampling for hierarchical models of non-Gaussian data”.
In: Statistics and Computing 19, pp. 479–492.
Gamerman, D. and H. Lopes (2006). Markov Chain Monte Carlo: Stochastic Sim-
ulation for Bayesian Inference. 2nd ed. Boca Raton: Taylor & Francis.
Gamerman, D. (1997). “Sampling from the posterior distribution in generalized
linear mixed models”. In: Statistics and Computing 7, pp. 57–68.

106
Bibliography
Geisser, S. and W. F. Eddy (1979). “A Predictive Approach to Model Selection”.
In: Journal of the American Statistical Association 74, p. 153.
Gelfand, A. E. and D. K. Dey (1994). “Bayesian Model Choice: Asymptotics and
Exact Calculations”. In: Journal of the Royal Statistical Society. Series B
(Methodological) 56, pp. 501–514.
Gelfand, A. (1996). “Model determination using sampling-based methods”. In:
Markov Chain Monte Carlo in Practice. Ed. by W. Gilks, S. Richardson, and
D. Spiegelhalter. London: Chapman & Hall, pp. 145–162.
Gelfand, A. E. and S. K. Ghosh (1998). “Model Choice: A Minimum Posterior
Predictive Loss Approach”. In: Biometrika 85, pp. 1–11.
Gelman, A. (2006). “Prior distributions for variance parameters in hierarchical
models”. In: Bayesian Analysis 1, pp. 1–19.
Gelman, A., J. Hwang, and A. Vehtari (2013). “Understanding predictive informa-
tion criteria for Bayesian models”. In: Statistics and Computing, pp. 1–20.
Geweke, J. (1991). “Efﬁcient simulation from the multivariate normal and student-
t distributions subject to linear constraints and the evaluation of constraint
probabilities”. In: Computing Science and Statistics: Proceedings of the 23rd
Symposium on the Interface. Fairfax Station: Interface Foundation, pp. 571–
578.
Greene, W. H. (2012). Econometric analysis. 7th ed. Boston: Prentice Hall.
Gustafson, P. (1997). “Large Hierarchical Bayesian Analysis of Multivariate Sur-
vival Data”. In: Biometrics 53, pp. 230–242.
Hanson, T. E. (2006). “Inference for Mixtures of Finite Polya Tree Models”. In:
Journal of the American Statistical Association 101, pp. 1548–1565.
Hastie, T. and R. Tibshirani (1993). “Varying-Coefﬁcient Models”. In: Journal of
the Royal Statistical Society. Series B (Methodological) 55, pp. 757–796.
—
(2000). “Bayesian Backﬁtting”. In: Statistical Science 15, pp. 196–223.
Hastie, T., R. Tibshirani, and J. H. Friedman (2009). The elements of statisti-
cal learning: Data mining, inference, and prediction. 2nd ed. New York:
Springer.
Heckman, J. J. and B. Singer (1984). “Econometric duration analysis”. In: Journal
of Econometrics 24, pp. 63–132.

Bibliography
107
Hennerfeind, A. (2006). “Bayesian nonparametric regression for survival and
event history data”. PhD thesis. Ludwig-Maximilians-Universität München.
Hennerfeind, A., A. Brezger, and L. Fahrmeir (2006). “Geoadditive Survival Mod-
els”. In: Journal of the American Statistical Association 101, pp. 1065–1075.
Henschel, V., J. Engel, D. Hölzel, and U. Mansmann (2009). “A semiparametric
Bayesian proportional hazards model for interval censored data with frailty
effects”. In: BMC Medical Research Methodology 9, p. 9.
Hess, W. (2009). A Flexible Hazard Rate Model for Grouped Duration Data. Tech-
nical Report. Lund University.
Holford, T. (1980). “The analysis of rates and of survivorship using log-linear
models”. In: Biometrics 36, pp. 299–305.
Ibrahim, J. G., M. H. Chen, and S. Debajyoti (2001a). “Criterion-based methods
for Bayesian model assessment”. In: Statistica Sinica 11, pp. 419–443.
Ibrahim, J. G., M.-H. Chen, and D. Sinha (2001b). Bayesian survival analysis.
New York: Springer.
Kalbﬂeisch, J. D. and R. L. Prentice (2002). The statistical analysis of failure time
data. 2nd ed. New York: Wiley.
Kalbﬂeisch, J. D. (1979). “Non-parametric Bayesian Analysis of Survival Time
Data”. In: Journal of the Royal Statistical Society, Series B 40, pp. 214–221.
Kalbﬂeisch, J. D. and R. L. Prentice (1973). “Marginal Likelihoods Based on
Cox’s Regression and Life Model”. In: Biometrika 60, pp. 256–278.
Kass, R. E., B. P. Carlin, A. Gelman, and R. M. Neal (1998). “Markov Chain Monte
Carlo in Practice: A Roundtable Discussion”. In: The American Statistician
52, pp. 93–100.
Klein, J. P. and M. L. Moeschberger (2003). Survival analysis: Techniques for
censored and truncated data. 2nd ed. New York: Springer.
Kneib, T. and A. Hennerfeind (2008). “Bayesian semi parametric multi-state mod-
els”. In: Statistical Modelling 8, pp. 169–198.
Kneib, T. (2006). “Mixed model based inference in structured additive regression”.
PhD thesis. Ludwig-Maximilians-Universität München.
Knorr-Held, L. (1999). “Conditional Prior Proposals in Dynamic Models”. In:
Scandinavian Journal of Statistics 26, pp. 129–144.

108
Bibliography
Komárek, A., E. Lesaffre, and J. F. Hilton (2005). “Accelerated Failure Time
Model for Arbitrarily Censored Data With Smoothed Error Distribution”. In:
Journal of Computational and Graphical Statistics 14, pp. 726–745.
Konrath, S. (2013). “Bayesian regularization in regression models for survival
data”. PhD thesis. Ludwig-Maximilians-Universität München.
Laird, N. and D. Olivier (1981). “Covariance Analysis of Censored Survival Data
Using Log-linear Analysis Techniques”. In: Journal of the American Statisti-
cal Association 76, pp. 231–240.
Lambert, P. and P. H. C. Eilers (2005). “Bayesian proportional hazards model
with time-varying regression coefﬁcients: A penalized Poisson regression ap-
proach.” In: Statistics in Medicine 24, pp. 3977–3989.
Lang, S. and A. Brezger (2004). “Bayesian P-Splines”. In: Journal of Computa-
tional and Graphical Statistics 13, pp. 183–212.
Lang, S., N. Umlauf, P. Wechselberger, K. Harttgen, and T. Kneib (2014).
“Multilevel structured additive regression”. In: Statistics and Computing 24,
pp. 223–238.
Lawless, J. F. (2003). Statistical models and methods for lifetime data. 2nd ed.
New York: Wiley.
Liu, C. (2004). “Robit Regression: A Simple Robust Alternative to Logistic and
Probit Regression”. In: Applied Bayesian Modeling and Causal Inference
from an Incomplete-Data Perspective. Ed. by A. Gelman and X. L. Meng.
New York: Wiley, pp. 227–238.
McCulloch, C. and S. Searle (2001). Generalized, Linear, and Mixed Models. New
York: Wiley.
Murthy, D., M. Xie, and R. Jiang (2004). Weibull Models. New York: Wiley.
O’Sullivan, F. (1986). “A Statistical Perspective on Ill-Posed Inverse Problems”.
In: Statistical Science 1, pp. 502–518.
Plummer, M., N. Best, K. Cowles, and K. Vines (2006). “CODA: Convergence
Diagnosis and Output Analysis for MCMC”. In: R News 6, pp. 7–11.
Polson, N. G. and J. G. Scott (2011). “Shrink Globally, Act Locally: Sparse
Bayesian Regularization and Prediction”. In: Bayesian Statistics 9. Ed. by

Bibliography
109
J. Bernardo, J. Bayarri, J. Berger, A. Dawid, and D. Heckerman. Oxford: Ox-
ford University Press, pp. 501–538.
R Core Team (2013). R: A Language and Environment for Statistical Computing.
R Foundation for Statistical Computing. Vienna, Austria.
Robert, C. P. (2001). The Bayesian Choice: From Decision-Theoretic Foundations
to Computational Implementation. New York: Springer.
Robert, C. P. (1995). “Simulation of truncated normal variables”. In: Statistics and
Computing 5, pp. 121–125.
Robert, C. P. and G. Casella (2004). Monte Carlo statistical methods. 2nd ed. New
York: Springer.
Roberts, G. (1996). “Markov chain concepts related to sampling algorithms”. In:
Markov Chain Monte Carlo in Practice. Ed. by W. Gilks, S. Richardson, and
D. Spiegelhalter. London: Chapman & Hall, pp. 45–54.
Rosenthal, J. S. (2011). “Optimal Proposal Distributions and Adaptive MCMC”.
In: Handbook of Markov Chain Monte Carlo. Ed. by S. Brooks, A. Gelman,
G. L. Jones, and X.-L. Meng. First. Chapman & Hall, CRC.
Rue, H. and L. Held (2005). Gaussian Markov Random Fields: Theory and Appli-
cations. Boca Raton: Taylor & Francis.
Schmidt, P. and A. D. Witte (1988). Predicting Recidivism Using Survival Models.
New York: Springer.
Sha, N., M. G. Tadesse, and M. Vannucci (2006). “Bayesian variable selection for
the analysis of microarray data with censored outcomes”. In: Bioinformatics
22, pp. 2262–2268.
Spiegelhalter, S. D., N. G. Best, B. P. Carlin, and A. V. D. Linde (2002). “Bayesian
measures of model complexity and ﬁt”. In: Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 64, pp. 583–639.
Sun, J. (2006). The statistical analysis of interval-censored failure time data. New
York: Springer.
Tanner, M. A. and W. H. Wong (1987). “The Calculation of Posterior Distributions
by Data Augmentation”. In: Journal of the American Statistical Association
82, pp. 528–540.

110
Bibliography
Thall, P. F., L. H. Wooten, and N. M. Tannir (2005). “Monitoring event times
in early phase clinical trials: some practical issues.” In: Clinical Trials 2,
pp. 467–478.
Therneau, T. and P. Grambsch (2000). Modeling Survival Data: Extending the Cox
Model. New York: Springer.
Tierney, L. (1996). “Introduction to general state-space Markov chain theory”. In:
Markov Chain Monte Carlo in Practice. Ed. by W. Gilks, S. Richardson, and
D. Spiegelhalter. London: Chapman & Hall, pp. 59–74.
Trautmann, H., D. Steuer, O. Mersmann, and B. Bornkamp (2012). truncnorm:
Truncated normal distribution. R package version 1.0-6.
Vehtari, A. and J. Ojanen (2012). “A survey of Bayesian predictive methods
for model assessment, selection and comparison”. In: Statistics Surveys 6,
pp. 142–228.
Venables, W. N. and B. D. Ripley (2002). Modern Applied Statistics with S. 4th ed.
New York: Springer.
Wang, L., X. I. Lin, and B. Cai (2012a). “Bayesian Semiparametic Regression
Analysis of Interval-Censored data with Monotone Splines”. In: Interval-
Censored Time-to-Event Data: Methods and Applications. Ed. by D. Chen,
J. Sun, and K. Peace. Boca Raton: Taylor & Francis, pp. 149–165.
Wang, X., A. Sinha, J. Yan, and M.-H. Chen (2012b). “Bayesian Inference of
Interval-Censored data”. In: Interval-Censored Time-to-Event Data: Methods
and Applications. Ed. by D. Chen, J. Sun, and K. Peace. Boca Raton: Taylor
& Francis, pp. 167–195.
Wienke, A. (2010). Frailty Models in Survival Analysis. Boca Raton: Taylor &
Francis.
Yavuz, A. C. and P. Lambert (2011). “Smooth estimation of survival functions
and hazard ratios from interval-censored data using Bayesian penalized B-
splines.” In: Statistics in medicine 30, pp. 75–90.

