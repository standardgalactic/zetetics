NOISE
A Flaw in Human Judgment
DANIEL KAHNEMAN
OLIVIER SIBONY
CASS R. SUNSTEIN
Copyright © 2021 by Daniel Kahneman,  
Olivier Sibony, and Cass R. Sunstein


2
Figure 2: Looking at the back of the target
Team A
Team B
Team C
Team D

3
4
14
24
34
44
Noise =
1 SD
54
64
74
84
Figure 3: Distribution of GoodSell’s market share forecasts for one region

4
–30
–20
–10
30
40
50
Bias
Zero
error
Noise =
1 SD
0
10
20
Figure 4: Distribution of errors in GoodSell’s forecasts for one region

5
970
971
972
973
974
975
976
977
978
979
980
Figure 5: Five measurements of the same length

6
30
23
18
15
14
15
18
23
30
39
971
972
973
974
975
976
977
978
979
980
Figure 6: Mean squared error (MSE) for ten possible values of the true length

7
Figure 7: Two decompositions of MSE
Noise2
Bias2
MSE
Noise2
Bias2
MSE

8
–30
–20
–10
Panel A: Same Noise, Less Bias
0
Zero
error
Zero
error
10
20
Before
Afer
Before
Afer
30
40
50
–30
–20
–10
Panel B: Same Bias, Less Noise
0
10
20
30
40
50
Figure 8:  Distribution of errors with bias reduced by half vs. noise reduced 
by half


10
Figure 10: Decomposing system noise
System Noise2
Level
Noise2
Pattern
Noise2

11
Table 1: Correlation coefficient and percentage concordant (PC)
Correlation coefficient
Percentage concordant (PC)
.00
50%
.10
53%
.20
56%
.30
60%
.40
63%
.60
71%
.80
79%
1.00
100%

12
Simpler
More
complex
Simple
rules
Improper
linear
models
Linear
regression
models
Machine-
learning
models
Figure 11: Four types of rules and algorithms

13
Panel 1
Aiming at the same bull’s-eye,
but hitting diﬀerent spots
Panel 2
Aiming at diﬀerent bull’s-eyes,
but hitting the same area
A
A
AAA
A
A
A A
AB
B
B
BBB
B
B
BB
Figure 12: A look at the back of the target in an experiment to test for bias

14
Level
Noise
32%
44%
57%
37%
6%
27%
29%
19%
49%
Just
Punishment
Punitive intent
71%
94%
Outrage
Dollar award
51%
Pattern
Noise
Figure 13: Components of judgment variance


16
Figure 15: Decomposing pattern noise
Stable
Pattern
Noise2
Occasion
Noise2
Pattern Noise2


18
Table 3: Apgar Scoring Guidelines
Category
Number of points assigned
Appearance  
(skin color)
0: Entire body is blue or pale
1: Good color in body but blue hands or feet
2: Completely pink or normal color
Pulse (heart rate)
0: No heart rate
1: <100 beats per minute
2: >100 beats per minute
Grimace (reflexes)
0: No response to airways being stimulated
1: Grimace during stimulation
2:  Grimace and cough or sneeze during 
stimulation
Activity  
(muscle tone)
0: Limp
1: Some flexing (bending) of arms and legs
2: Active motion
Respiration 
(breathing rate 
and effort)
0: Not breathing
1: Weak cry (whimpering, grunting)
2: Good, strong cry

19
APPENDIX A
How to Conduct a Noise Audit
T
his appendix provides a practical guide for conducting a noise 
audit. You should read it from the perspective of a consultant 
who has been engaged by an organization to examine the quality of 
the professional judgments its employees produce by conducting a 
noise audit in one of its units.
As implied by its name, the focus of the audit is the prevalence of 
noise. However, a well‑conducted audit will provide valuable infor‑
mation about biases, blind spots, and specific deficiencies in the 
training of employees and in the supervision of their work. A suc‑
cessful audit should stimulate changes in the operations of the unit, 
including in the doctrine that guides professionals’ judgments, the 
training they receive, the tools they use to support their judgments, 
and the routine supervision of their work. If the effort is considered 
successful, it may be extended to other units of the organization.
A noise audit requires a substantial amount of work and much 
attention to detail because its credibility will surely be questioned if 
its findings reveal significant flaws. Every detail of the cases and the 
procedure should therefore be considered with hostile scrutiny in 
mind. The process we describe aims to reduce opposition by enlisting 

20
the professionals who are the most significant potential critics of the 
audit to be its authors.
Alongside the consultant (who may be external or internal), the 
relevant cast of characters includes the following:
• 
Project team. The project team will be responsible for all phases 
of the study. If the consultants are internal, they will form the 
core of the project team. If the consultants are external, an 
internal project team will work closely with them. This will 
ensure that people in the company view the audit as their 
project and consider the consultants as playing a supporting 
role. In addition to the consultants who administer the 
collection of data, analyze the results, and prepare a final 
report, the project team should include subject matter experts 
who can construct the cases that the judges will assess. All the 
members of the project team should have high professional 
credibility.
• 
Clients. A noise audit will only be useful if it leads to significant 
changes, which requires early involvement of the leadership of 
the organization, which is the “client” of the project. You can 
expect clients to be initially skeptical about the prevalence of 
noise. This initial skepticism is actually an advantage if it is 
accompanied by an open‑minded attitude, curiosity about the 
results of the audit, and a commitment to remedy the situation 
if the consultant’s pessimistic expectations are confirmed.
• 
Judges. The clients will designate one or more units to be 
audited. The selected unit should consist of a substantial 
number of “judges,” the professionals who make similar 
judgments and decisions on behalf of the company. The judges 
should be effectively interchangeable; i.e., if one person was 
unavailable to handle a case, another would be assigned to it 
and expected to arrive at a similar judgment. The examples that 
introduced this book were sentencing decisions of federal 

21
judges and the setting of risk premiums and claims reserves in 
an insurance company. For a noise audit, it is best to select a 
judgment task that (1) can be completed on the basis of written 
information, and (2) is expressed numerically (e.g., in dollars, 
probabilities, or ratings).
• 
Project manager. A high‑level manager in the administrative 
staff should be designated as project manager. Specific 
professional expertise is not required for that task. However, a 
high position in the organization is of practical significance in 
overcoming administrative hurdles and is also a demonstration 
of the importance that the company attaches to the project. The 
task of the project manager is to provide administrative support 
to facilitate all phases of the project, including the preparation 
of the final report and the communication of its conclusions to 
the leadership of the company.
Construction of Case Materials
The subject matter experts who are part of the project team should 
have recognized expertise in the task of the unit (e.g., setting premi‑
ums for risks or evaluating the potential of possible investments). 
They will be in charge of developing the cases that will be used in the 
audit. Designing a credible simulation of the judgments professionals 
make on the job is a delicate task — especially given the scrutiny that 
the study will undergo if it reveals serious problems. The team must 
consider this question: if the results of our simulation indicate a high 
level of noise, will people in the company accept that there is noise in 
the actual judgments of the unit? The noise audit is only worth carry‑
ing out if the answer is a clear yes.
There is more than one way to achieve a positive response. The 
noise audit of sentencing described in chapter 1 summarized each 
case by a brief schematic list of relevant attributes and obtained 
assessments of sixteen cases in ninety minutes. The noise audit in the 

22
insurance company described in chapter 2 used detailed and realistic 
summaries of complex cases. Findings of high noise in both instances 
provided acceptable evidence because of the argument that if much 
disagreement was found in simplified cases, noise could only be 
worse in real cases.
A questionnaire should be prepared for each case, to provide a 
deeper understanding of the reasoning that led each judge to a judg‑
ment of that case. The questionnaire should be administered only 
after the completion of all cases. It should include:
• 
Open questions about the key factors that led the participant to 
her response.
• 
A list of the facts of the case, allowing the participant to rate 
their importance.
• 
Questions that call for an “outside view” of the category to 
which the case belongs. For instance, if the cases call for dollar 
valuations, participants should provide an estimate of how 
much below or above average the case is compared to all 
valuations for cases of the same category.
Prelaunch Meeting with Executives
When the case materials to be used in the audit are assembled, a 
meeting should be scheduled in which the project team will present 
the audit to the leadership of the company. The discussion in that 
meeting should consider possible outcomes of the study, including a 
finding of unacceptable system noise. The purpose of the meeting is 
to hear objections to the planned study and to obtain from the lead‑
ership a commitment to accept its results, whatever they are: there is 
no point moving on to the next stage without such a commitment. If 
serious objections are raised, the project team may be required to 
improve the case materials and try again.
Once the executives accept the design of the noise audit, the 

23
project team should ask them to state their expectations about the 
results of the study. They should discuss questions such as:
• 
“What level of disagreement do you expect between a randomly 
selected pair of answers to each case?”
• 
“What is the maximum level of disagreement that would be 
acceptable from a business perspective?”
• 
“What is the estimated cost of getting an evaluation wrong in 
either direction (too high or low) by a specified amount (e.g., 
15%)?”
The answers to these questions should be documented to ensure 
that they are remembered and believed when the actual results of the 
audit come in.
Administration of the Study
The managers of the audited unit should be, from the beginning, 
informed in general terms that their unit has been selected for spe‑
cial study. However, it is important that the term noise audit not be 
used to describe the project. The words noise and noisy should be 
avoided, especially as descriptions of people. A neutral term such as 
decision-making study should be used instead.
The managers of the unit will be immediately in charge of the 
data collection and responsible for briefing the participants about the 
task, with the participation of the project manager and members of 
the project team. The intent of the exercise should be described to the 
participants in general terms, as in “The organization is interested in 
how [decision makers] reach their conclusions.”
It is essential to reassure the professionals who participate in the 
study that individual answers will not be known to anyone in the 
organization, including the project team. If necessary, an outside 
firm may be hired to anonymize the data. It is also important to 

24
stress that there will be no specific consequences for the unit, which 
was merely selected as representative of units that perform judgment 
tasks on behalf of the organization. To ensure the credibility of the 
results, all qualified professionals in the unit should participate in the 
study. The allocation of half a working day to the exercise will help 
convince the participants of its importance.
All participants should complete the exercise at the same time, 
but they should be kept physically separate and asked not to commu‑
nicate while the study is in progress. The project team will be avail‑
able to answer questions during the study.
Analyses and Conclusions
The project team will be in charge of the statistical analyses of the 
multiple cases evaluated by each participant, including the mea‑
surement of the overall amount of noise and its constituents, level 
noise and pattern noise. If the case materials allow it, it will also 
identify statistical biases in the responses. The project team will 
have the equally important task of trying to understand the 
sources of variability in judgments by examining responses to the 
questionnaire in which participants explained their reasoning and 
identified the facts that most influenced their decisions. Focusing 
mainly on extreme responses at both ends of the distribution, the 
team will search for patterns in the data. It will look for indica‑
tions of possible deficiencies in the training of employees, the 
procedures of the organization, and the information that it pro‑
vides to its employees.
The consultant and the internal project team will work together 
to develop tools and procedures that apply principles of decision 
hygiene and debiasing to improve the judgments and decisions made 
in the unit. This step of the process is likely to extend over several 
months. In parallel, the consultant and the professional team will 
also prepare a report on the project, which they will present to the 
leadership of the organization.

25
At this point, the organization will have carried out a sample 
noise audit in one of its units. If the effort is considered successful, 
the executive team may decide on a broader effort to evaluate and 
improve the quality of the judgments and decisions that are produced 
in the organization.

26
APPENDIX B
A Checklist for a Decision Observer
T
his appendix presents a generic example of a checklist to be 
used by a decision observer (see chapter 19). The checklist pre‑
sented here roughly follows the chronological sequence of the discus‑
sion that leads to an important decision.
The suggested questions that follow each item in the checklist 
bring additional clarifications. Decision observers should ask them‑
selves these questions while observing the decision process.
This checklist is not intended to be used as it stands. Rather, we 
hope that it will serve as an inspiration and a starting point for deci‑
sion observers who will design a custom bias observation checklist of 
their own.

27
Bias Observation Checklist
1. Approach to Judgment
1a. Substitution
___ “Did the group’s choice of evidence and the focus of their dis‑
cussion indicate substitution of an easier question for the dif‑
ficult one they were assigned?”
___ “Did the group neglect an important factor (or appear to give 
weight to an irrelevant one)?”
1b. Inside view
___ “Did the group adopt the outside view for part of its delibera‑
tions and seriously attempt to apply comparative rather than 
absolute judgment?”
1c. Diversity of views
___ “Is there any reason to suspect that members of the group 
share biases, which could lead their errors to be correlated? 
Conversely, can you think of a relevant point of view or exper‑
tise that is not represented in this group?
2. Prejudgments and Premature Closure
2a. Initial prejudgments
___ “Do (any of ) the decision makers stand to gain more from one 
conclusion than another?”
___ “Was anyone already committed to a conclusion? Is there any 
reason to suspect prejudice?”
___ “Did dissenters express their views?”
___ “Is there a risk of escalating commitment to a losing course of 
action?”
2b. Premature closure; excessive coherence
___ “Was there accidental bias in the choice of considerations 
that were discussed early?”

28
___ “Were alternatives fully considered, and was evidence that 
would support them actively sought?”
___ “Were uncomfortable data or opinions suppressed or 
neglected?”
3. Information Processing
3a. Availability and salience
___ “Are the participants exaggerating the relevance of an event 
because of its recency, its dramatic quality, or its personal rel‑
evance, even if it is not diagnostic?”
3b. Inattention to quality of information
___ “Did the judgment rely heavily on anecdotes, stories, or anal‑
ogies? Did the data confirm them?”
3c. Anchoring
___ “Did numbers of uncertain accuracy or relevance play an 
important role in the final judgment?”
3d. Nonregressive prediction
___ “Did the participants make nonregressive extrapolations, 
estimates, or forecasts?”
4. Decision
4a. Planning fallacy
___ “When forecasts were used, did people question their sources 
and validity? Was the outside view used to challenge the 
forecasts?”
___ “Were confidence intervals used for uncertain numbers? Are 
they wide enough?”
4b. Loss aversion
___ “Is the risk appetite of the decision makers aligned with that 
of the organization? Is the decision team overly cautious?”
4c. Present bias
___ “Do the calculations (including the discount rate used) reflect 
the organization’s balance of  short‑ and  long‑ term priorities?”

29
APPENDIX C
Correcting Predictions
M
atching predictions are errors caused by our reliance on the 
intuitive matching process (see chapter 14). We make match‑
ing predictions when we rely on the information we have to make a 
forecast and behave as if this information were perfectly (or very 
highly) predictive of the outcome.
Recall the example of Julie, who could “read fluently when she was 
four years old.” The question was, what is her GPA? If you predicted 
3.8 for Julie’s college GPA, you intuitively judged that the  four‑  year‑ 
 old Julie was in the top 10% of her age group by reading age (although 
not in the top  3–  5%). You then, implicitly, assumed that Julie would 
also rank somewhere around the 90th percentile of her class in terms 
of GPA. This corresponds to a GPA of 3.7 or 3. 8 —  hence the popular‑
ity of these answers.
What makes this reasoning statistically incorrect is that it grossly 
overstates the diagnostic value of the information available about 
Julie. A precocious  four‑  year‑  old does not always become an aca‑
demic overachiever (and, fortunately, a child who initially struggles 
with reading will not languish at the bottom of the class forever).
More often than not, in fact, outstanding performance will 


31
make if the information you have were perfectly predictive. Write 
it down.
2. Look for the mean.
Now, step back and forget what you know about Julie for a 
moment. What would you say about Julie’s GPA if you knew abso-
lutely nothing about her? The answer, of course, is straightforward: 
in the absence of any information, your best guess of Julie’s GPA 
would have to be the mean GPA in her graduating  class —   probably 
somewhere around 3.2.
Looking at Julie this way is an application of the broader principle 
we have discussed above, the outside view. When we take the outside 
view, we think of the case we are considering as an instance of a class, 
and we think about that class in statistical terms. Recall, for instance, 
how taking the outside view about the Gambardi problem leads us to 
ask what the base rate of success is for a new CEO (see chapter 4).
3. Estimate the diagnostic value of the information you have.
This is the difficult step, where you need to ask yourself, “What is 
the predictive value of the information I have?” The reason this ques‑
tion matters should be clear by now. If all you knew about Julie was 
her shoe size, you would correctly give this information zero weight 
and stick to the mean GPA prediction. If, on the other hand, you had 
the list of grades Julie has obtained in every subject, this information 
would be perfectly predictive of her GPA (which is their average). 
There are many shades of gray between these two extremes. If you 
had data about Julie’s exceptional intellectual achievements in high 
school, this information would be much more diagnostic than her 
reading age, but less than her college grades.
Your task here is to quantify the diagnostic value of the data you 
have, expressed as a correlation with the outcome you are predicting. 
Except in rare cases, this number will have to be a back‑of‑ the‑ 
 envelope estimate.
To make a sensible estimate, remember some of the examples we 
listed in chapter 12. In the social sciences, correlations of more than 

32
.50 are very rare. Many correlations that we recognize as meaningful 
are in the .20 range. In Julie’s case, a correlation of .20 is probably an 
upper bound.
4. Adjust from the outside view in the direction of your intuitive 
guess, to an extent that reflects the diagnostic value of 
the information you have.
The final step is a simple arithmetic combination of the three 
numbers you have now produced: you must adjust from the mean, in 
the direction of your intuitive guess, in proportion to the correlation 
you have estimated.
This step simply extends the observation we have just made: 
if the correlation were 0, you would stick to the mean; if it were 1, you 
would disregard the mean and happily make a matching prediction. 
In Julie’s case, then, the best prediction you can make of GPA is one 
that lies no more than 20% of the way from the mean of the class in 
the direction of the intuitive estimate that her reading age suggested 
to you. This computation leads you to a prediction of about 3.3.
We have used Julie’s example, but this method can be applied just 
as easily to many of the judgment problems we have discussed in this 
book. Consider, for instance, a vice president of sales who is hiring a 
new salesperson and has just had an interview with an absolutely 
outstanding candidate. Based on this strong impression, the execu‑
tive estimates that the candidate should book sales of $1 million in 
the first year on the  job —   twice the mean amount achieved by new 
hires during their first year on the job. How could the vice president 
make this estimate regressive? The calculation depends on the diag‑
nostic value of the interview. How well does a recruiting interview 
predict on‑ the‑  job success in this case? Based on the evidence we 
have reviewed, a correlation of .40 is a very generous estimate. 
Accordingly, a regressive estimate of the new hire’s  first‑  year sales 
would be, at most, $500K + ($1 million − $500K) × .40 = $700K.
This process, again, is not at all intuitive. Notably, as the exam‑
ples illustrate, corrected predictions will always be more 

33
conservative than intuitive ones: they will never be as extreme as 
intuitive predictions, but instead closer, often much closer, to the 
mean. If you correct your predictions, you will never bet that the ten‑
nis champion who has won ten Grand Slam titles will win another 
ten. Neither will you foresee that a highly successful start‑up worth 
$1 billion will become a behemoth worth several hundred times that. 
Corrected predictions do not take bets on outliers.
This means that, in hindsight, corrected predictions will inevita‑
bly result in some highly visible failures. However, prediction is not 
done in hindsight. You should remember that outliers are, by defini‑
tion, extremely rare. The opposite error is much more frequent: when 
we predict that outliers will remain outliers, they generally don’t, 
because of regression to the mean. That is why, whenever the aim is 
to maximize accuracy (i.e., minimize MSE), corrected predictions 
are superior to intuitive, matching predictions.

