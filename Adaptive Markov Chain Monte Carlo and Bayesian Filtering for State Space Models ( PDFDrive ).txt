Isambi Sailon Mbalawata
ADAPTIVE MARKOV CHAIN MONTE CARLO 
AND BAYESIAN FILTERING FOR STATE 
SPACE MODELS
Acta Universitatis 
Lappeenrantaensis 614
Thesis for the degree of Doctor of Science (Technology) to be presented with 
due permission for public examination and criticism in Auditorium 1383 at 
Lappeenranta University of Technology, Lappeenranta, Finland on the 12th of 
December, 2014, at 12 pm.

Supervisors
Docent Simo Särkkä
School of Science
Department of Biomedical Engineering and Computational Science
Aalto University
Finland
Professor Heikki Haario
LUT School of Technology
Department of Computational Engineering 
Lappeenranta University of Technology 
Finland
Reviewers
Professor Eric Moulines
Département TSI / CNRS UMR 5141
Télécom ParisTech (ENST)
France
Professor Lennart Svensson
Signals and Systems Department
Chalmers University of Technology
Sweden
Opponent
Professor Eric Moulines
Département TSI / CNRS UMR 5141
Télécom ParisTech (ENST)
France
ISBN 978-952-265-708-4
ISBN 978-952-265-709-1 (PDF)
ISSN-L 1456-4491
ISSN 1456-4491
Lappeenrannan teknillinen yliopisto
Yliopistopaino 2014

Abstract
Isambi Sailon Mbalawata
ADAPTIVE MARKOV CHAIN MONTE CARLO AND BAYESIAN FILTERING FOR STATE
SPACE MODELS
Lappeenranta, 2014
77 p.
Acta Universitatis Lappeenrantaensis 614
Diss. Lappeenranta University of Technology
ISBN 978-952-265-708-4, ISBN 978-952-265-709-1 (PDF), ISSN-L 1456-4491, ISSN 1456-4491
This thesis is concerned with the state and parameter estimation in state space models. The estima-
tion of states and parameters is an important task when mathematical modeling is applied to many
different application areas such as the global positioning systems, target tracking, navigation, brain
imaging, spread of infectious diseases, biological processes, telecommunications, audio signal pro-
cessing, stochastic optimal control, machine learning, and physical systems. In Bayesian settings,
the estimation of states or parameters amounts to computation of the posterior probability density
function. Except for a very restricted number of models, it is impossible to compute this density
function in a closed form. Hence, we need approximation methods.
A state estimation problem involves estimating the states (latent variables) that are not directly ob-
served in the output of the system. In this thesis, we use the Kalman ﬁlter, extended Kalman ﬁlter,
Gauss–Hermite ﬁlters, and particle ﬁlters to estimate the states based on available measurements.
Among these ﬁlters, particle ﬁlters are numerical methods for approximating the ﬁltering distribu-
tions of non-linear non-Gaussian state space models via Monte Carlo. The performance of a particle
ﬁlter heavily depends on the chosen importance distribution. For instance, inappropriate choice of
the importance distribution can lead to the failure of convergence of the particle ﬁlter algorithm.
In this thesis, we analyze the theoretical Lp particle ﬁlter convergence with general importance
distributions, where p ≥2 is an integer.
A parameter estimation problem is considered with inferring the model parameters from measure-
ments. For high-dimensional complex models, estimation of parameters can be done by Markov
chain Monte Carlo (MCMC) methods. In its operation, the MCMC method requires the unnormal-
ized posterior distribution of the parameters and a proposal distribution. In this thesis, we show how
the posterior density function of the parameters of a state space model can be computed by ﬁlter-
ing based methods, where the states are integrated out. This type of computation is then applied
to estimate parameters of stochastic differential equations. Furthermore, we compute the partial
derivatives of the log-posterior density function and use the hybrid Monte Carlo and scaled conju-
gate gradient methods to infer the parameters of stochastic differential equations.
The computational efﬁciency of MCMC methods is highly depend on the chosen proposal distribu-
tion. A commonly used proposal distribution is Gaussian. In this kind of proposal, the covariance
matrix must be well tuned. To tune it, adaptive MCMC methods can be used. In this thesis, we pro-
pose a new way of updating the covariance matrix using the variational Bayesian adaptive Kalman
ﬁlter algorithm.

Keywords: Parameter and state estimation, stochastic differential equations, adaptive Markov
chain Monte Carlo, linear and nonlinear ﬁlters, Lp particle ﬁlter convergence
UDC 519.217.2:519.226:681.5.015.44:004.942:519.2:303.733.3

Preface
This doctoral thesis contains the results of research undertaken at the Department of computational
engineering, Lapeenranta University of Technology. I would like to acknowledge Lapeenranta Uni-
versity of Technology (LUT), Aalto University and University of Dar Es Salaam, for their ﬁnancial
support, without which the completion of this work would not have been possible. The doctoral
years have been a challenging journey, with both ups and downs, but fortunately, I have not been
alone on this road. I have been fortunate to have the company of an extended team of experts, always
willing to coach, sponsor, help, and motivate me. For this, I am most grateful. First and foremost, I
would like to thank God for giving me the grace and privilege to pursue my studies and successfully
complete in spite of the many challenges I faced.
My most sincere gratitude goes to my supervisor Docent Simo Särkkä, who is full of knowledge
and constructive ideas, always eager to share both his knowledge and time. In him, I found an
exemplary academic coach and true pillar of support especially when things seem to be moving in
a wrong direction. In general, I count myself as one of the most fortunate doctoral students in the
world for having a supervisor who cared so much about my academic and non-academic life, and
who responded to my questions and queries so promptly.
I would also like to thank Professor, Ph.D. Heikki Haario for giving me the opportunity to study
a doctoral degree in LUT, and taking his time to discuss problems with me. I also want to thank
Professor, Ph.D. Matti Heiliö for introducing the East Africa Technomathematics Network, from
which I got to know about LUT, where I pursued my Masters in Technomathetics. My gratitude
also goes to Professor, Ph.D Jouko Lampinen and Docent Aki Vehtari for hosting me at Department
of Biomedical Engineering and Computational Science, Aalto University during my several visits.
I would also like to thank the reviewers of this dissertation, Professor Eric Moulines and Professor
Lennart Svensson for the time spent reviewing the dissertation, and for the helpful and generous
comments – always greatly appreciated.
I wish to express my gratitude to Saunders Oliver, Matylda Jabło´nska, Marko Laine, Gasper Mwanga,
Betty Nanyonga and Jane Aduda for proofreading my work. I owe appreciation to the non-academic
staff who have been of great support during my doctoral studies, in particular Riita Laari, Sari
Damsten, and Eeva Häyrinen of LUT, and Eeva Lampinen, Marita Stenman and Susanna Väänänen
of Aalto University.
Special thanks go to all other people who have worked with me over the years, in particular the
co-authors Jouni Hartikainen, Matti Vihola, Betty Nanyonga, Gasper G Mwanga and Matti Heiliö.
I would like to thank my colleagues and friends: Pitos Biganda, David Kolosen, Pendo Kivyiro,
Zubeda Mussa, Idrisa Said, Ashvinkumar Chaudhari, Ville Manninen, Miika Tolonen, Almasi
Maguya, Frank Seth, Arno Solin, and Juho Kokkala, among others, for many useful discussions,
comments and suggestions. I very much enjoyed our daily exchange of academic and non-academic
ideas and thoughts, and the fun moments together.
Thanks and love to my parents Sailon Mbalawata and Tabia Ntangu, my in-laws Zacharia Mwantaji
and Verenatha Mwanawima, and all my other relatives for their encouragement and support. During
my doctoral studies, I was lucky to have Finnish host families. My sincere thanks go to Antti

Vanhanen and his wife Marketta Vanhanen, Pertti Kälviäinen and his wife Maiju Kälviäinen, Jari
Berg and his wife Maarit Berg, Pastor Sakari Kiiskinen and his wife Seija Kiiskinen as well as Varpu
Lehmusjarvi and her husband Reino Lehmusjarvi for exposing me and my wife to non-academic
life during our stay in Finland.
Finally, I would like to thank my wife Jestina Zacharia Ndolela. Her support, encouragement, quiet
patience and unwavering love were undeniably the bedrock upon which the past years of my life
have been built. She already has my heart so I will just give her a heartfelt THANKS.
Lappeenranta, November 2014
Isambi Sailon Mbalawata

To my beloved, lovely wife,
JESTINA ZACHARIA NDOLELA


CONTENTS
Abstract
Preface
Contents
List of the original articles and the author’s contribution
Part I: Overview of the thesis
11
1
Introduction
13
2
Linear and Non-linear Filtering Techniques
17
2.1
State Space Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Bayesian Optimal Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3
Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.4
Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.5
Gaussian Filter with Gauss–Hermite Cubature . . . . . . . . . . . . . . . . . . . .
24
2.6
Sequential Importance Resampling . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3
Theoretical Results for Particle Filters
31
3.1
Mean Square Error Convergence for Unbounded Importance Weights
. . . . . . .
33
3.2
Convergence Results with Bounded Weights for Unbounded Test Functions . . . .
36
3.3
Convergence Results with Unbounded Weights for Unbounded Test Functions . . .
38
3.4
General Particle Filter Convergence Results for Bounded Test Functions . . . . . .
38
4
Markov Chain Monte Carlo
45
4.1
Metropolis–Hastings Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4.2
Adaptive Markov Chain Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . . .
46
4.2.1
Stochastic Approximation-Based Adaptive Algorithms . . . . . . . . . . .
47
4.2.2
Variational Bayesian Adaptive Metropolis Algorithm . . . . . . . . . . . .
50
4.3
Hybrid Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
5
Parameter Estimation in Stochastic Differential Equations
57
5.1
Stochastic Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.2
Non-ﬁltering Likelihood Based Methods . . . . . . . . . . . . . . . . . . . . . . .
58
5.3
Evaluation of Marginal Likelihood and its Derivatives by Kalman Filter . . . . . .
60
5.4
Approximation of Marginal Likelihood and its Derivatives by Extended Kalman Filter 63

5.5
Approximation of Marginal Likelihood by Gaussian Cubature Filters . . . . . . . .
63
5.6
Particle Marginal Metropolis–Hastings Algorithm . . . . . . . . . . . . . . . . . .
65
6
Conclusion and Discussion
67
Bibliography
69
Part II: Publications
79

LIST OF THE ORIGINAL ARTICLES AND THE AUTHOR’S CONTRIBUTION
This thesis consists of an introductory part and 7 scientiﬁc papers. The articles and the author’s
contributions in them are summarized below.
I
Mbalawata, I. S., Särkkä, S., and Haario, H., 2013. Parameter Estimation in Stochastic
Differential Equations with Markov Chain Monte Carlo and Non-Linear Kalman Filtering,
Computational statistics, 28(3), 1195–1223, 2013.
II
Särkkä, S., Hartikainen, J., Mbalawata, I. S., and Haario, H., 2014. Posterior Inference
on Parameters of Stochastic Differential Equations via Non-Linear Gaussian Filtering and
Adaptive MCMC. Accepted for publication in Statistics and Computing.
III
Mbalawata, I. S and Särkkä, S., 2014. On The L4 Convergence of Particle Filters with
General Importance Distributions. In Proceedings of IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), 8048–8052, 2014.
IV
Mbalawata, I. S., Särkkä, S., Vihola, M., and Haario, H., 2015 Adaptive Metropolis
Algorithm Using Variational Bayesian Adaptive Kalman Filter. Computational Statistics &
Data Analysis, 83, 101–115, 2015.
V
Mbalawata, I. S and Särkkä, S., 2014. Weight Moment Conditions for L4 Convergence of
Particle Filters for Unbounded Test Functions. In 2014 Proceedings of the 22nd European
Signal Processing Conference (EUSIPCO). IEEE, 1207–1211, 2014.
VI
Mbalawata, I. S and Särkkä, S., 2014. Moment Conditions for Convergence of Particle
Filters with Unbounded Importance Weights. Submitted to Signal Processing.
VII
Nannyonga, B, Mwanga, G. G, Haario H, Mbalawata, I. S and Heiliö, M., 2014. Deter-
mining parameter distribution in within-host severe P. falciparum malaria. BioSystems, 126,
76–84, 2014.
Isambi Sailon Mbalawata is the principal author of the Papers I, III, IV, V and VI and co-author of
the Papers II and VII. In Papers I, III, IV, V and VI, the author is responsible for writing and exper-
imentation. In Paper II, the author contributed (and wrote) the adaptive Markov chain Monte Carlo
part. For Paper VII, the author has participated in the discussions, writing and experimentation.


PART I: OVERVIEW OF THE THESIS


CHAPTER I
Introduction
Data analysis for real life problems often involves estimating unknown quantities given partial or
noisy measurements. For instance, the estimation problem of the state and parameters of a stochastic
system, given noisy measurements, is of central importance in engineering, telecommunication, nat-
ural science, epidemiology, signal processing, ﬁnance, to mention a few. As pointed out by Särkkä
(2013), applications of such estimation can be found in global positioning systems, target tracking,
navigation, brain imaging, spread of infectious diseases, biological processes, telecommunication,
audio signal processing, stochastic optimal control, learning, and physical systems.
In the state estimation task, when parameters are known, the conditional probability density function
of the state, given all measurements up to time step k, is of interest. We denote this density function
as p(xk | y1:k), where xk is the state at k and y1:k = (y1,y2,...,yk) are the measurements up to
step k. Usually, the measurements arrive sequentially in time and, thus, the conditional probability
density function is computed recursively in time, leading to a Bayesian ﬁltering problem (see, e.g.,
Jazwinski, 1970). For a few models, like in linear Gaussian systems, it is possible to evaluate
p(xk | y1:k) analytically. However, in most cases, p(xk | y1:k) is intractable, hence approximation
methods, such as the extended Kalman ﬁlter, Gaussian ﬁlters, and particle ﬁlters, are needed. In this
thesis, we review some Bayesian ﬁlters and also analyze the Lp-convergence of particle ﬁlters in the
case of a general important distribution with bounded and unbounded importance weights, where
p ≥2 is an integer.
In the parameter estimation task, the conditional probability density function of parameters, given
all measurements up to the time step k, is of interest. We denote this density function as p(θ | y1:k),
where θ is a parameter vector. Given p(θ | y1:k), the estimation of parameters can be done by
either point estimation methods (like maximum a posteriori) or interval estimation methods (like
Markov chain Monte Carlo (MCMC)) (see, e.g., Gelman et al., 2004). In this thesis, we mainly
deal with interval estimation, particularly MCMC methods. Note that, in most cases, p(θ | y1:k) is
intractable, hence approximation methods are needed. We propose new ways of evaluating p(θ |
y1:k) by Kalman ﬁlter for linear models, and approximating p(θ | y1:k) by non-linear ﬁlters for non-
linear models. For computational efﬁciency of the MCMC algorithm, we propose a new adaptation
of the Gaussian proposal distribution, where the covariance matrix is computed by the variational
Bayesian adaptive Kalman ﬁlter algorithm.
Generally, we can summarize the main contributions of this thesis as follows.
13

14
1. Introduction
1. Parameter estimation of stochastic differential equations (SDEs). As stated, the estimation of
parameters requires p(θ | y1:k) which is intractable. Most of the existing methods for parame-
ter estimation of SDEs suffer from invertability of diffusion coefﬁcient and high computational
expense (see Chapter V for details). In this thesis, we propose the computation of
(a) Marginal likelihood by linear and non-linear Kalman ﬁlter. In Paper I, we propose the
efﬁcient computation of the marginal likelihood function by the Kalman ﬁlter (for linear
SDEs) and extended Kalman ﬁlter (for non-linear SDEs). For gradient based parameter
estimation methods, we also compute the gradients of the marginal likelihood functions.
(b) Marginal likelihood by sigma-point based approximations. In Paper II, the approximation
of the marginal likelihood function for parameter estimation of SDEs is done by sigma-
point methods like the 3rd order spherical cubature rule or Gauss–Hermite quadratures.
2. Adaptive Markov chain Monte Carlo. The efﬁciency of MCMC algorithms depends on the cho-
sen proposal distribution. In most cases the chosen proposal distribution is Gaussian, in which
the covariance matrix needs to be well tuned. We have applied the adaptive MCMC methods in
Papers II and VII to estimate parameters. In Paper IV, we propose a new way of tuning (updat-
ing) the covariance matrix for Gaussian proposal distribution by a variational Bayesian adaptive
Metropolis algorithm.
3. Theoretical convergence of the particle ﬁlter. The existing convergence proofs of particle ﬁlters
have mainly been considered for bootstrap ﬁlters, particularly for the Lp-convergence mode. In
this thesis (see Chapter III), we extend some of the existing proofs to more general cases. That
is:
(a) In Paper III, we prove the L4 particle ﬁlter convergence for unbounded test functions with
general importance distributions, where the importance weights are assumed to be bounded.
This proof is an extension of the existing proof of (Hu et al., 2008), who dealt with bootstrap
importance distribution only.
(b) In Paper V, we extend the L4 particle ﬁlter convergence results of Paper III to the case of
potentially unbounded importance weights. This paper addresses the fact that there exists
importance distributions which do not ensure that the importance weights are uniformly
bounded. In such cases, we need further assumptions on the weights, for instance, the
seventh moments of the importance weights can be assumed to be bounded.
(c) In Paper VI, for bounded test functions, we prove the L2 and L4 particle ﬁlter convergence
for the ﬁltering posterior distribution of the current state of the particle ﬁlter. We also
provided the empirical convergence of probability measure where the importance weights
are assumed unbounded.
(d) We provide a full detailed Lp particle ﬁlter convergence proof for the case of unbounded
importance weights and bounded test functions.
The work is organized as follows. Chapter II gives an overview of how linear and non-linear ﬁlter-
ing problems are solved. The chapter reviews some of Bayesian ﬁlters like Kalman ﬁlter, extended
Kalman ﬁlter, Gaussian ﬁlter with Gauss–Hermite cubature and sequential importance resampling.
Chapter III deals with the theoretical Lp-convergence results for particle ﬁlters with general impor-
tance distributions where the importance weights are assumed bounded and also unbounded. Adap-
tive Markov chain Monte Carlo methods are discussed in Chapter IV. It is in this chapter where our

15
proposed method called variational Bayesian adaptive Metropolis method is discussed. Chapter V
discusses stochastic differential equations (SDEs) and existing methods for estimating parameters
in SDEs. The chapter includes the proposed ﬁltering based likelihood methods for estimating the
parameters of linear and non-linear SDEs. Chapter VI concludes the thesis.

16
1. Introduction

CHAPTER II
Linear and Non-linear Filtering Techniques
2.1
State Space Models
In order to analyze deterministic and stochastic systems that are observed through a stochastic pro-
cess, one can use state space models. State space models (SSMs) are widely used in a range of
ﬁelds, including economics, ﬁnance, environmental science, natural science, medicine and engi-
neering (Liu, 2002; Cappé et al., 2005; Petris et al., 2009). They describe the probabilistic de-
pendence between two stochastic processes: dynamic and measurement (observation) processes
(Jazwinski, 1970). The aim of state space modeling is often to estimate the state of the dynamic
process given the measurements. The general discrete dynamic and measurement processes in time
are often written, respectively, in probabilistic form as
xk ∼p(xk | xk−1,θ),
yk ∼p(yk | xk,θ);
for
k = 1,2,3,...,M,
(2.1)
where xk ∈Rn is the state, θ ∈Φ ⊆Rd are parameters and yk ∈Rm is the measurement. In this
chapter, we assume that the parameters are known and, for notational convenience, we drop them.
For instance, we write p(xk | xk−1) for p(xk | xk−1,θ). Also, we assume that the SSM is Markovian,
that is, the dynamic states satisfy the Markov property and the measurements are conditionally
independent (Jazwinski, 1970).
The corresponding continuous-discrete processes of (2.1) are given in Deﬁnitions 2.1.2 and 2.1.4.
Discrete SSMs imply that both dynamic and measurement processes are discrete processes in time,
while continuous-discrete SSMs imply that the dynamic process is a continuous process in time, and
a measurement process is a discrete process in time. The SSMs are grouped into linear SSMs and
non-linear SSMs, depending on the structure of dynamic and measurement processes. The simplest
SSM is a linear Gaussian model whose dynamic and measurement processes are deﬁned as follows.
Deﬁnition 2.1.1 (Discrete linear SSM). The discrete dynamic and measurement models are respec-
tively written as
xk = Ak−1 xk−1 +qk−1,
yk = Hk xk +rk,
(2.2)
17

18
2. Linear and Non-linear Filtering Techniques
where Ak−1 is the transition matrix of the dynamic process, qk−1 ∼N(0, Qk−1) is the dynamic
process noise with covariance Qk−1, Hk is the measurement model matrix, rk ∼N(0, Rk) is the
measurement noise with covariance Rk, and the initial condition x0 is normally distributed with
mean m0 and covariance P0.
Deﬁnition 2.1.2 (Continuous–discrete linear SSM). The continuous dynamic and discrete measure-
ment models are respectively written as
dx(t) = F(t)x(t)dt +L(t)dB(t),
yk = Hk x(tk)+rk,
(2.3)
where F : [0,∞) →Rn×n and L : [0,∞) →Rn×s are matrix valued functions, B(t) is an s-dimensional
Brownian motion process with spectral density Qc ∈Rs×s, Hk is a measurement model matrix and
rk ∼N(0,Rk) is the measurement noise with covariance Rk. Brownian motion is discussed in
Chapter V. Note that x(tk) and x(tk−1) can be written as xk and xk−1 respectively.
Practically there are few linear Gaussian phenomena but their ﬁltering distributions can be solved
analytically with the Kalman ﬁlter algorithm discussed in Section 2.3. Most of the real systems are
non-linear with dynamic and measurement processes deﬁned as follows.
Deﬁnition 2.1.3 (Discrete non-linear SSM). The discrete dynamic and measurement models are
respectively written as
xk = f(xk−1)+qk−1,
yk = h(xk)+rk,
(2.4)
where f : Rn →Rn is the dynamic model function, h : Rn →Rm is the measurement model function,
and qk−1 and rk are noises which can be Gaussian or non-Gaussian.
Example 2.1.1 (Non-linear state space model). A concrete example of a discrete non-linear SSM is
a non-linear time series model deﬁned as follows (Gordon et al., 1993).
xk = xk−1
2
+25
xk−1
1+x2
k−1
+8cos(1.2k −1)+uk,
yk = x2
k
20 +vk,
(2.5)
where uk ∼N(0,10) and vk ∼N(0,1). Equation (2.5) can be written in probabilistic form as
p(xk | xk−1) = N
 
xk | xk−1
2
+25
xk−1
1+x2
k−1
+8cos(1.2(k −1)),10
!
,
p(yk | xk) = N

yk | x2
k
20,1

.
(2.6)
Let x0 = 0.1. Then Equation (2.5) can be numerically simulated and the results are shown in Figure
2.1. The ﬁgure shows the states and the measurements for 100 step realizations. If the states
x1,x2,... are unknown, there are a number of methods that can be used to estimate them, given
measurements y1,y2,.... These methods, particularly Kalman ﬁlter based methods and particle
ﬁlters, are discussed in this thesis.

2.1 State Space Models
19
0
10
20
30
40
50
60
70
80
90
100
−25
−20
−15
−10
−5
0
5
10
15
20
25
Iterations
Realisations
 
 
States
Measurements
Figure 2.1: 100-point realizations of states and measurements of Equation (2.5).
Deﬁnition 2.1.4 (Continuous–discrete non-linear SSM). The continuous dynamic and discrete mea-
surement models are respectively written as
dx(t) = f(x(t),t)dt +L(x(t),t)dB(t),
yk = h(x(tk))+rk,
(2.7)
where f : Rn × [0,∞) →Rn is the non-linear dynamic model function, L : Rn × [0,∞) →Rn×s is
the matrix valued function, B(t) is the Brownian motion process, h : Rn →Rm is the measurement
model function, and rk is the Gaussian or non-Gaussian noise.
When we integrate the continuous dynamic process (2.7) from initial time t0 to ﬁnal time t we get
the following
xi(t)−xi(t0) =
Z t
t0
fi(x(τ),τ)dτ +
Z t
t0
m
∑
j=1
Li j(x(τ),τ)dBj(τ);
i = 1,...,n,
(2.8)
where the ﬁrst integral on the right hand side is a Lebesgue integral (Chung, 2001) while the second
integral is an Itô integral (Karatzas and Shreve, 1991; Øksendal, 2003). Therefore, the continu-
ous dynamic process is an Itô stochastic differential equation (SDE) (Jazwinski, 1970). SDEs are
differential equations with random noises and their solutions have non-differentiable sample paths
(Kloeden and Platen, 1999). The history of SDEs starts from a paper by Einstein (1905), who gave a
mathematical connection between microscopic random motion of particles (microscopic motion of
Brownian particles) and the macroscopic diffusion equation. Currently, SDEs have attracted a lot of

20
2. Linear and Non-linear Filtering Techniques
attention, due to physical processes in real life systems experiencing random forcing and stochastic
inputs that cannot be captured by ordinary differential equations. For example, the SDEs are com-
monly used to model the diverse phenomena in neural networks, ecosystem dynamics, population
genetics, macroeconomics, and physical systems.
Given measurements y1,y2,...,yM, the estimation of xk is called ﬁltering if k = M, smoothing if
k < M and prediction if k > M. This thesis deals mainly with the ﬁltering problem whose complete
solution is the conditional probability density function of xk given all measurements up to k (Jazwin-
ski, 1970). It is difﬁcult to compute this function although, for linear SSMs, it is possible because
the function is characterized by mean vectors and covariance matrices. In the following section, we
review the Bayesian ﬁltering approach which is used to evaluate the conditional probability density
function recursively.
2.2
Bayesian Optimal Filter
Given a state space model, we are often interested in estimating the parameters θ and the state vec-
tor at time k, given all the measurements up to k. However, if it happens that θ is known then the
remaining task of estimating xk becomes easier, and reduces to approximation of the marginal poste-
rior distribution of states p(xk | y1,...,yk). The computation of p(xk | y1,...,yk) is done recursively
since the measurements are received sequentially. This recursive computation leads to a Bayesian
optimal ﬁlter. The recursive process is initialized by the probability density associated with prior
information of states to any observations. The computation of posterior density for one iteration in
this Bayesian recursive procedure is shown in Figure 2.2, and in algorithmic form is described in
Algorithm 2.1 (Jazwinski, 1970; Särkkä, 2013).
Figure 2.2: Recursive Bayesian posterior density estimation procedure for one iteration.
In most cases, the integrals in the Bayesian optimal ﬁlter algorithm are intractable. This problem
is of great interest as it has lead to the invention of a great number of different approximations
of the ﬁlter. The ﬁlters can be grouped into linear ﬁlters for linear SSM problems and non-linear
ﬁlters for non-linear SSM problems. In the following sections, we brieﬂy describe the Kalman ﬁlter,
extended Kalman ﬁlter, Gaussian ﬁlter with Gauss–Hermite quadrature and sequential importance

2.3 Kalman Filter
21
Algorithm 2.1 Bayesian optimal ﬁlter
i. Initialization: start from the prior p(x0).
ii. for k = 1,2,... perform the following.
• Prediction step: the prediction distribution of the state xk can be computed by
– for the discrete case: the Chapman-Kolmogorov equation:
p(xk | y1,...,yk−1) =
Z
p(xk | xk−1) p(xk−1 | y1,...,yk−1)dxk−1
(2.9)
– For the continuous-discrete case: integrate the Fokker–Planck equation (FPE):
∂p
∂t = −
n
∑
i=1
∂[p fi]
∂xi
+ 1
2
n
∑
i,j=1
∂2[(LQc LT)ijp]
∂xi ∂xj
,
(2.10)
where p ≜p(x,t | y1,...,yk−1), fi ≜fi(x,tk), and L ≜L(x,tk).
• Update step: Given the next measurement yk, the predictive distribution above is updated
to a posterior distribution by the Bayes’ rule as
p(xk | y1,...,yk) =
p(yk | xk) p(xk | y1,...,yk−1)
R p(yk | xk) p(xk | y1,...,yk−1)dxk
.
(2.11)
resampling (particle ﬁlter) which have been used in this research. For each section, the prediction
steps for ﬁlters are presented in discrete and continuous-discrete cases. However, note that in most
cases, for instance linear SSMs, it is possible to discretize the continuous dynamic system exactly.
2.3
Kalman Filter
The breakthrough of the ﬁlter theory started from the work of Peter Swerling (1958) and Rudolf
Kalman (1960). For instance, Kalman (1960b) solved Equations (2.2) and (2.3) in linear case,
which led to Kalman ﬁlter. The Kalman ﬁlter (KF; Kalman, 1960b) is an optimal recursive data
processing algorithm for estimating the dynamic state from noisy measurements in linear Gaussian
SSMs. It operates by propagating the mean and covariance of the state through time.
Suppose m0 and P0 denote the prior mean and covariance which are used to deﬁne the prior distri-
bution of x0. Algorithm 2.2 is the KF algorithm which provides a recursive efﬁcient computation
of the mean and covariance of the dynamic states x1,x2,...,xM. For the derivation of the ﬁltering
steps for KF algorithm see, for instance, Särkkä (2013).
From Algorithm 2.2, m−
k is a priori state estimate, mk is a posteriori state estimate, P−
k is a pri-
ori estimate error covariance and Pk is a posteriori estimate error covariance. Note that, for the
continuous-discrete case, m−(tk) and P−(tk) are written as m−
k and P−
k respectively. The differen-
tial equations for the continuous-discrete prediction step can be solved by any numerical scheme,
such that Runge–Kutta scheme (Butcher, 2003) or matrix fraction decomposition (see. e.g., Grewal

22
2. Linear and Non-linear Filtering Techniques
and Andrews, 2001, and Paper I). The initial conditions are m−
k (tk−1) = mk−1, P−
k (tk−1) = Pk−1,
and the prediction result is given as m−
k ≜m−
k (tk), P−
k ≜P−
k (tk).
Algorithm 2.2 Kalman ﬁlter
i. Initialize the mean m0 and covariance P0.
ii. For k = 1,2,..., perform the following.
• Prediction step:
– for the discrete case;
m−
k = Ak−1 mk−1,
P−
k = Ak−1Pk−1AT
k−1 +Qk−1.
(2.12)
– for the continuous-discrete case, solve
dm−
k (t)
dt
= F(t)m−
k (t),
dP−
k (t)
dt
= F(t)P−
k (t)+P−
k (t)FT(t)+L(t)Qc LT(t).
(2.13)
• Update step:
Sk = Hk P−
k HT
k +Rk,
Kk = P−
k HT
k S−1
k ,
mk = m−
k +Kk
 yk −Hk m−
k

,
Pk = P−
k −Kk Sk KT
k .
(2.14)
The existence of the ﬁltering solution depends on the controllability and observability properties
of the SSM (Jazwinski, 1970; Grewal and Andrews, 2001). These two properties were originally
introduced by Kalman (1959, 1960a). Since then, the two concepts have received strong attention
when designing a ﬁlter. Roughly speaking, the SSM is said to be controllable if, by altering the
inputs, the state vector can be made to go to zero from any initial condition (Grewal and Andrews,
2001). The SSM is said to be observable if the unknown initial condition can be computed given the
history of inputs and outputs (Grewal and Andrews, 2001). In a stochastic context, controllability
with respect to noise ensures that the prior distribution deﬁned in the SSM is nonsingular whereas
observability ensures that the variance is ﬁnite. There are various ways of testing whether the SSM
is controllable and observable (see Kailath et al., 2000).
Consider the discrete SSM (2.2). In order to be able to estimate the state of the dynamic system,
the controllabilty and information matrices must be positive deﬁnite. These matrices are deﬁned as
follows.
Deﬁnition 2.3.1 (Controllabilty matrix). The controllability matrix ℶis deﬁned as (Jazwinski, 1970)
ℶ(M,M0) =
M
∑
k=M0
Φ(M,k)Qk ΦT(M,k),
(2.15)
where Φ(M,k) = AM−1 AM−2 AM−3 ··· Ak+1 Ak Ak−1 for k ≤M. The SSM is said to be completely
controllable if ℶ(M,1) > 0.

2.4 Extended Kalman Filter
23
Deﬁnition 2.3.2 (Information matrix). The information matrix ϝ is deﬁned as (Jazwinski, 1970)
ϝ(M,M0) =
M
∑
k=M0
ΦT(k,M)HT
k Σ−1
k Hk Φ(k,M),
(2.16)
where Φ(k,M) = A−1
k−1 A−1
k A−1
k+1 ··· A−1
M−2 A−1
M−1 for k ≤M. The SSM is said to be completely
observable if ϝ(M,1) > 0.
In Paper IV, we proposed variational Bayesian adaptive Metropolis (VBAM) algorithm as one of
the adaptive Markov chain Monte Carlo methods. To study the validity of VBAM algorithm, we
required the boundedness of the mean and covariance of Kalman ﬁlter. For that case the SSM must
be uniformly completely controllable and observable (Jazwinski, 1970).
Deﬁnition 2.3.3 (Uniform complete controllability). A system (2.2) is uniformly completely con-
trollable if there exist a positive integer L, and constants β1,β2 > 0 such that for all M ≥L we
have
β1 I ≤ℶ(M,M −L) ≤β2 I,
(2.17)
where I is an identity matrix.
Deﬁnition 2.3.4 (Uniform complete observability). A system (2.2) is uniformly completely observ-
able if there exist a positive integer L, and constants β1,β2 > 0 such that for all M ≥L we have
β1 I ≤ϝ(M,M −L) ≤β2 I.
(2.18)
The complete analogous deﬁnitions for the continuous linear SSM (2.3) are found in (Jazwinski,
1970). For the case of non-linear SSMs, the controllability and observability are discussed, for
instance in (Hermann and Krener, 1977; Hangos et al., 2004).
The Kalman ﬁlter can only be used when the SSM is linear and the noise is Gaussian. When the
assumption of linearity is violated, the KF cannot be used and hence non-linear ﬁlters such as the
extended Kalman ﬁlter, unscented Kalman ﬁlter, ensemble Kalman ﬁlter, quadrature Kalman ﬁlter,
and particle ﬁlters (Jazwinski, 1970; Julier and Uhlmann, 2004; Evensen, 2003; Doucet et al., 2001;
Ito and Xiong, 2000) are used instead. In the next section, the extended Kalman ﬁlter (used in Paper
I) is discussed.
2.4
Extended Kalman Filter
The extended Kalman ﬁlter (EKF) forms a Gaussian approximation to distribution of states and
measurements using a Taylor series expansion (Jazwinski, 1970; Grewal and Andrews, 2001). The
idea is that the non-linear SSM is linearized with the Taylor series expansion and then the KF is
applied. The estimation of states is thus done recursively as shown in Algorithm 2.3. For the
derivation of the ﬁltering steps for the EKF algorithm see, for instance, Särkkä (2013).
In Algorithm 2.3, Fx(x,t) is the Jacobian matrix of f(x,t) and Hx(x) is the Jacobian matrix of
h(x). In the continuous-discrete case, the initial conditions are m−
k (tk−1) = mk−1, P−
k (tk−1) = Pk−1,
and the prediction result is given as m−
k ≜m−
k (tk), P−
k ≜P−
k (tk). Note that the accuracy of EKF

24
2. Linear and Non-linear Filtering Techniques
depends on how severe the SSM non-linearities are (Jazwinski, 1970). For high dimension models,
the linearization of the dynamic model can lead to poor error covariance evolution which, in some
models, leads to unstable error covariance growth (Evensen, 2009). As pointed out by Särkkä
(2013), the EKF is simple to implement though it requires the dynamic and measurement function
to be differentiable which can be impossible in some cases. Improving the EKF performance can be
done by retaining higher order terms of the Taylor series expansion. However, the computations of
the resulting Jacobians and Hessians are often numerically unstable and intensive. Similarly, models
with abruptly changing behavior do not have derivatives. To avoid such problem of differentiability,
a gradient free ﬁlter is needed, for example, Gaussian ﬁlter based on Gauss–Hermite integration
(used in Paper II).
Algorithm 2.3 Extended Kalman ﬁlter
i. Initialize the mean m0 and covariance P0.
ii. For k = 1,2,..., perform the following.
• Prediction step:
– for the discrete case;
m−
k = f(mk−1),
P−
k = Fx(mk−1)Pk−1 FT
x (mk−1)+Qk−1.
(2.19)
– for the continuous-discrete case, solve
dm−
k (t)
dt
= f(m−
k (t),t),
dP−
k (t)
dt
= Fx(m−
k (t),t)P−
k (t)+P−
k (t)FT
x (m−
k (t),t)
+L(m−
k (t),t)Qc LT(m−
k (t),t).
(2.20)
• Update step:
Sk = Hx(m−
k )P−
k HT
x (m−
k )+Rk,
Kk = P−
k HT
x (m−
k ,t)S−1
k ,
mk = m−
k +Kk
 yk −h(m−
k )

,
Pk = P−
k −Kk Sk KT
k .
(2.21)
2.5
Gaussian Filter with Gauss–Hermite Cubature
In this section, the non-linear ﬁlter problem is solved by Gaussian assumed density approxima-
tions (Maybeck, 1982; Ito and Xiong, 2000; Särkkä, 2013), where the mean and covariance are
approximated by Gaussian moment matching. The prediction and update steps for general additive
Gaussian ﬁlter are given in Algorithm 2.4. The algorithm involves Gaussian integrals which, in most
cases, are analytically intractable. Thus numerical methods for evaluating such integrals are needed.
In this section the Gauss–Hermite integration method (Abramowitz and Stegun, 1964; Cools, 1997;

2.5 Gaussian Filter with Gauss–Hermite Cubature
25
Ito and Xiong, 2000; Wu et al., 2006; Arasaratnam, 2009) is discussed. In the one-dimensional
case, the Gauss–Hermite integration is called quadrature while for higher dimensional cases it is
cubature. There exist other integration techniques that have been used to approximate the Gaussian
integrals in Algorithm 2.4 (see Arasaratnam and Haykin, 2009; Wu et al., 2006).
Algorithm 2.4 Gaussian ﬁlter
i. Initialize the predictive mean m0 and covariance P0.
ii. For k = 1,2,..., perform the following.
• Prediction step:
– for the discrete case;
m−
k =
Z
f(xk−1)N(xk−1 | mk−1,Pk−1)dxk−1
P−
k =
Z 
f(xk−1)−m−
k
 
f(xk−1)−m−
k
T N(xk−1 | mk−1,Pk−1)dxk−1
+Qk−1
(2.22)
– for the continuous-discrete case, solve
dm−
k
dt
=
Z
f(x,t)N(x | m−
k (t),P−
k (t))dx,
dP−
k
dt
=
Z
(x−m−
k (t))fT(x,t)N(x | m−
k (t),P−
k (t))dx
+
Z
f(x,t)(x−m−
k (t))T N(x | m−
k (t),P−
k (t))dx
+
Z
L(x,t)Qc LT(x,t)N(x | m−
k (t),P−
k (t))dx.
(2.23)
• Update step:
µk =
Z
h(xk)N(xk | m−
k ,P−
k )dxk,
Sk =
Z
[h(xk)−µk] [h(xk)−µk]T N(xk | m−
k ,P−
k )dxk +Rk,
Ck =
Z 
xk −m−
k

[h(xk)−µk]T N(xk | m−
k ,P−
k )dxk,
Kk = Ck S−1
k ,
mk = m−
k +Kk [yk −µk],
Pk = P−
k −Kk Sk KT
k .
(2.24)
Let φ(x) be a function of interest. Then the unit Gauss–Hermite quadrature is deﬁned as
Z
φ(x)N(x | 0,1)dx ≈
p
∑
i=1
wi φ(xi),
(2.25)

26
2. Linear and Non-linear Filtering Techniques
where xi are the sample points and wi are the associated weights. The sample points are the p roots
of (probabilists) Hermite polynomials deﬁned as (Särkkä, 2013)
Hp(x) = (−1)p exp(x2/2) dp
dxp exp(−x2/2),
(2.26)
and the weights are
wi =
2p p!√π
2p2 (Hp−1(xi))2.
(2.27)
The same weights and sample points can be used to compute a non-unit Gauss–Hermite quadrature
as
Z
φ(x)N(x | m,P)dx ≈
p
∑
i=1
wi φ(P1/2ξ i +m),
(2.28)
where ξ i are the unit sample points computed as the roots of Hermite polynomial. Computing the
quadrature points using root-ﬁnding method may be difﬁcult in some cases (Arasaratnam et al.,
2007). Instead, a better approach which exploits the relationship between orthogonal polynomials
and tridiagonal matrices can be used (Golub and Welsch, 1969).
For the case of multidimensional problems, the Gauss–Hermite cubature is deﬁned as
Z
φ(x)N(m,P)dx ≈
p
∑
i=1
wi1,...,in φ(m+
√
Pξ (i1,...,in)),
(2.29)
where ξ (i1,...,in) are multidimensional unit sample points which are Cartesian product of univariate
sample points and wi1,...,in are multidimensional weights which are products of univariate weights.
When using the Gauss–Hermite integration, the resulting Gaussian ﬁlter is named the Gauss–
Hermite Kalman ﬁlter (Ito and Xiong, 2000; Särkkä, 2013) or quadrature Kalman ﬁlter (Arasarat-
nam et al., 2007).
The Kalman ﬁlter based methods discussed above have storage and computational problems (Evensen,
2009; Jia et al., 2011). Also, the ﬁlters assume that the noises are Gaussian. To cope with such prob-
lems, the particle ﬁlters are used. Particle ﬁlters are Monte Carlo based approximation ﬁlters whose
history can be traced back to the work of Handschin and Mayne (1969) and Handschin (1970).
They approximate the distribution of states using particles that are re-weighted according to their
likelihoods (Doucet et al., 2001; Gordon et al., 1993; Ristic et al., 2004). There are several versions
of particle ﬁlters: sequential importance resampling, auxiliary particle ﬁlter, marginalized particle
ﬁlter, regularized auxiliary particle ﬁlter, and Rao-Blackwellized particle ﬁlter, to name a few (Gor-
don et al., 1993; Pitt and Shephard, 1999; Ristic et al., 2004; Cappé et al., 2007). In the next section
the basic particle ﬁltering algorithm based on sequential importance sampling (used in Paper VI),
for the discrete case, is discussed.
To understand the particle ﬁlter theory, one has to understand the basic Monte Carlo methods like
Monte Carlo integration, importance sampling, and sequential importance sampling, which requires
basic knowledge of applied mathematics and probability (which can be found in Gelman et al., 2004;
Ross, 2010; Chung, 2001).

2.6 Sequential Importance Resampling
27
2.6
Sequential Importance Resampling
In Bayesian inference (Bernardo and Smith, 1994; Gelman et al., 2004), the estimation of the
state xk at time step k amounts to computation of its posterior distribution p(xk | y1:k), where
y1:k = {y1,...,yk} are the observed measurements. The posterior distribution can be computed
with Bayes’ rule as
p(xk | y1:k) =
p(y1:M | xk) p(xk)
R p(y1:k | xk) p(xk)dxk
.
(2.30)
Here p(y1:k | xk) is the observation model and p(xk) is the prior distribution of states. The posterior
distribution can be used to compute the moments, quantiles, highest posterior density regions, etc.
These quantities can be expressed as a conditional expectation of function φ of state xk:
E[φ(xk) | y1:k] =
Z
φ(xk) p(xk | y1:k)dxk.
(2.31)
To estimate xk, it is often enough to evaluate (2.31) for φ(xk) = xk where the integration amounts to
ﬁnding the mean of samples. One of the difﬁculties of using Bayes’ formula is that it is impossible
to ﬁnd the closed-form solutions of integrals (2.30) and (2.31). There are approximation methods to
solve the problem of such intractability of the integrals. For instance, the quadrature methods and
other numerical integration schemes have been used to approximate the integrals, but they perform
badly in higher dimensional problems. For higher dimensional problems, one can use, for example,
Monte Carlo methods to approximate the integrals. Monte Carlo based methods are largely applied
in stochastic non-linear dynamic systems in physics, biology, chemistry, computational ﬁnance, to
mention but a few (Lemieux, 2009; Dimov, 2008; Gilks et al., 1996; Robert and Casella, 2004).
Suppose it is possible to sample from p(xk | y1:k), then Monte Carlo integration (see., e.g. Liu,
2002) is a simple and easy method that can be used to approximate the integrals in Equations (2.30)
and (2.31) by drawing samples from the distribution, and computing sample averages. If it is im-
possible to sample from p(xk | y1:k) then one can use, for example, rejection sampling, which uses a
new distribution called the proposal distribution from which N samples x(1)
k ,x(2)
k ,··· ,x(N)
k
are gener-
ated (von Neumann, 1951; Liu, 2002; Koop et al., 2007; Lemieux, 2009; Gilks et al., 1996; Gelman
et al., 1996; Bishop, 2006) or importance sampling which is related to rejection sampling, except
that here all generated samples from proposal distribution are used in approximations (Liu, 2002;
Koop et al., 2007; Lemieux, 2009; Gilks et al., 1996; Gelman et al., 1996; Bishop, 2006). When
importance sampling is performed sequentially for SSMs, it leads to sequential importance sam-
pling (Doucet et al., 2001), which generates the importance sampling approximations to ﬁltering
distributions of (2.1) from which at time step k, the approximation to the expectation of a function
can be calculated as the weighted sample average (Doucet et al., 2001; Särkkä, 2013):
E[φ(xk) | y1:k] ≈
N
∑
i=1
˜w(i)
k φ(˜x(i)
k ),
(2.32)
where ˜w(i)
k is the normalized importance weight computed by Equation (2.34) and x(i)
k is a particle
generated from the importance distribution q(x(i)
k | x(i)
0:k−1,y1:k). If the model is Markovian, the
importance distribution can be selected as q(x(i)
k | x(i)
0:k−1,y1:k) = q(x(i)
k | x(i)
k−1,y1:k) which implies
that only xk−1 should be stored. One problem encountered by the sequential importance sampling

28
2. Linear and Non-linear Filtering Techniques
is the degeneracy problem where almost all the particles have zero or nearly zero weights. Thus,
the distribution of the importance weights becomes more skewed and this results in most of the
particles carrying very insigniﬁcant weights, whereas most weights are carried by a few particles
(Doucet et al., 2000, 2001; Ristic et al., 2004; Cappé et al., 2007). To measure the degeneracy
problem, Kong et al. (1994) introduced an effective sample size Nef f deﬁned in Equation (2.33).
Ristic et al. (2004) observed that the smaller the value of Nef f , the more severe the degeneracy
problem.
Nef f =
1
∑N
i=1

w(i)
k
2,
where
1 ≤Nef f ≤N.
(2.33)
To avoid the problem of degeneracy, one can introduce a resampling step in the sequential impor-
tance sampling, leading to the sequential importance resampling (SIR) 1 algorithm described in
Algorithm 2.5 (Gordon et al., 1993; Kitagawa, 1996; Doucet et al., 2000, 2001; Ristic et al., 2004;
Cappé et al., 2007). Intuitively, the idea of introducing this step is to eliminate particles with low
importance weights and replicate particles with higher importance weights. The resampling step
is not necessarily performed at every time step, but only when it is needed. For instance, it can
be performed when Nef f is too small, say Nef f < N/10 (Särkkä, 2013). Different unbiased resam-
pling schemes (for example systematic resampling, residual resampling, adaptive resampling, and
multinomial resampling) have been proposed in literature (Kitagawa, 1996; Hol et al., 2006).
Algorithm 2.5 Sequential importance resampling
• At k = 0, for i = 1,...,N, sample x(i)
0 ∼p(x0), where p(x0) is the prior distribution.
• At k = 1,2,3,..., perform the following.
– Sample ˜x(i)
k ∼q(xk | x(i)
k−1,y1:k), for i = 1,...,N.
– Calculate the unnormalized weights by
w(i)
k = p(yk | ˜x(i)
k ) p(˜x(i)
k | x(i)
k−1)
q(˜x(i)
k | x(i)
k−1,y1:k)
,
for
i = 1,...,N.
(2.34)
– Normalize the weights by ˜w(i)
k =
w(i)
k
∑N
i=1 w(i)
k
.
– Compute Nef f and if it is too low, perform resampling.
The SIR Algorithm 2.5 is a generic particle ﬁlter from which a large number of variations and
extensions have been developed. The performance of any particle ﬁlter depends heavily on the
choice of importance distribution which determines the efﬁciency and complexity of the ﬁlter (Guo
et al., 2005; Doucet et al., 2000). There are no universal criteria used to choose a good importance
distribution. However, one has to study the structure of the SSM and choose the importance dis-
tribution that gives appropriate theoretical and numerical approximations which converge to a true
distribution.
1SIR is also often referred to as sampling importance resampling or sequential importance sampling resampling
(Särkkä, 2013).

2.6 Sequential Importance Resampling
29
The one-step posterior importance distribution is the (conditionally) optimal importance distribution
that minimizes the variance of the importance weights (Doucet et al., 2000; Kong et al., 1994; Chen
and Liu, 1996, 2000). It is deﬁned as
q(xk | xk−1,y1:k) = p(xk | xk−1,yk).
In order to use the posterior importance distribution, one requires the ability to sample from p(xk |
xk−1,yk). Despite these difﬁculties, researchers have been approximating p(xk | xk−1,yk) with sub-
optimal approaches like Markov chain Monte Carlo (Doucet et al., 2000; Hürzeler and Künsch,
1998; Tanizakia and Mariano, 1998; Künsch, 2005). To avoid sampling and computation problems
experienced in the posterior importance distribution, the prior importance distribution (Handschin
and Mayne, 1969; Handschin, 1970; Gordon et al., 1993; Tanizakia and Mariano, 1998; Doucet
et al., 2000) can be used. It is deﬁned as
q(xk | xk−1,y1:k) = p(xk | xk−1).
The prior importance distribution is simple to use as one can easily sample from prior densities
and compute the weights. However this is sometimes inefﬁcient since it does not consider infor-
mation from the current measurement. To incorporate the more recent observations, the posterior
importance distribution can be approximated using a Kalman based ﬁlter leading to the Gaussian
importance distribution which takes the following form
q(xk | xk−1,y1:k) = N(xk | mk|k,Σk),
where mk|k and Σk are Gaussian distribution parameters. The two parameters can be computed using
non-linear Kalman ﬁlters (Doucet et al., 2000; Van der Merwe et al., 2000; Guo et al., 2005). The
Gaussian importance distribution should have heavier tails than p(xk | xk−1,yk) with Σk bounded
below. These restrictions can be resolved by replacing the Gaussian importance distribution with a
Student’s t-distribution (Cappé et al., 2005). The effect of importance distribution is described in the
following example (taken from Paper III) where two scaled distributions (Gaussian and Student’s
t) are used.
Example 2.6.1 (Linear Gaussian random walk model2). Consider the following SSM.
xk = xk−1 +qk−1,
where
qk−1 ∼N(0,Q),
(2.35)
yk = xk +rk,
where
rk ∼N(0,R),
(2.36)
with the optimal Gaussian importance distribution N(xk | mk,Pk) where mk and Pk are given as
mk = xk−1 +
Q
Q+R(yk −xk−1),
and
P = Q−
Q2
Q+R.
The importance distribution is replaced by N(xk | mk,cPk) where c is a positive integer. It is observed
that for c < 1, the weights become unbounded and thus the particle ﬁlter approximation is not
guaranteed to converge.
For simulation purposes, the initial value x0 is set to 0 and the values for Q and R are 1 and
0.5 respectively. The c-values are chosen to be 0.1, 0.5, 1, 2, and 5 while degree of freedom (for
2We are grateful to Juho Kokkala for the numerical experiments.

30
2. Linear and Non-linear Filtering Techniques
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
0
0.1
0.2
0.3
0.4
0.5
Estimate of E[|x−µ|4]
Number of particles N
 
 
c=0.1
c=0.5
c=1
c=2
c=5
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
0
0.1
0.2
0.3
0.4
0.5
Estimate of E[|x−µ|4]
Number of particles N
 
 
c=0.1
c=0.5
c=1
c=2
c=5
Figure 2.3: Illustration of the effect of importance distribution. Left: the Gaussian importance
distribution. Right: Student’s t-importance distribution. The scaling of variance signiﬁcantly affects
the performance of the particle ﬁlter with a Gaussian importance distribution while the effect to a
Student’s t-distribution based particle ﬁlter is smaller. The ﬁgures are taken from Paper III.
Student’s t-distribution) is 3. Figure 2.3 illustrates the effect of the value c to the estimates of
the fourth order central moment of the ﬁltering distribution for M = 24 with varying number of
particles. The analytical value for fourth order central moment is 0.4. It can be seen that scaling
of the variance in Gaussian importance distribution affects the convergence of the fourth moment
estimate whereas the effect is smaller for the Student’s t-distribution .

CHAPTER III
Theoretical Results for Particle Filters
In Chapter II, we saw that particle ﬁlters are powerful methods for approximating Bayesian ﬁlter-
ing distributions in non-linear non-Gaussian state space models by forming a weighted set of Monte
Carlo samples {(˜x(i)
k , ˜w(i)
k ) : i = 1,...,N}, where {˜x(i)
k
: i = 1,...,N} denotes the particles and
{ ˜w(i)
k
: i = 1,...,N} the importance weights. Different statistics, such as the posterior expecta-
tion E[φ(xk) | y1:k] of a test function φ can be approximated from the formed weighted set with
Equation (2.32). As pointed out by Hu et al. (2008), it is very important to address the conver-
gence of particle ﬁlter approximation to the true solution and state the assumptions which allow the
convergence to exist. A particle ﬁlter converges if, in a suitable sense, Equation (2.32) becomes
exact as N →∞. There exist a vast amount of literature for convergence of particle ﬁlter algorithms
(Crisan and Doucet, 2002; Douc et al., 2009; Douc and Moulines, 2008; Van der Merwe et al., 2000;
Del Moral and Guionnet, 2001; Del Moral, 2004; Hu et al., 2008, 2011; Crisan and Míquez, 2014).
The mode of convergence presented in the aforementioned references include, among others, the
Lp-convergence, where p ≥2.
In the existing literature, the proposed Lp-convergence results for the case of unbounded test func-
tions have been mainly restricted to the bootstrap ﬁlter (a bootstrap ﬁlter is a particle ﬁlter with prior
importance distribution (Gordon et al., 1993)). Also, the unbounded importance weights have not
been considered in either bounded or unbounded test function cases. To ﬁll these gaps, we have
extended the existing L4-convergence results of Hu et al. (2008) to the case of general importance
distributions with bounded importance weights (see Paper III) and with unbounded importance
weights (see Paper V). Similarly, we have extended the mean square convergence, L4-convergence,
and empirical measure convergence results for bounded test functions to the case of unbounded
importance weights (see Paper VI).
In this chapter we brieﬂy review the results from Papers III, V and VI and provide the detailed
proof for the Lp-convergence of particle ﬁlters with bounded test function. For theoretical analysis,
we write Equation (2.1) in the following form
xk ∼fk(xk | xk−1),
yk ∼gk(yk | xk).
(3.1)
Here fk(xk | xk−1) is the transition probability density modeling the dynamics of the system, and
gk(yk | xk) is the conditional probability density of measurements modeling the distribution of mea-
31

32
3. Theoretical Results for Particle Filters
surements. Also, we rewrite Algorithm 2.5 as Algorithm 3.1 to include measures that are used in
the proofs.
Algorithm 3.1 Standard particle ﬁlter
• At k = 0, for i = 1,...,N, sample x(i)
0 ∼π0|0(dx0).
• At k ≥1,
– Sample ˜x(i)
k ∼q(xk | x(i)
k−1,y1:k), for i = 1,...,N.
– Calculate the unnormalized weights by
wk(˜x(i)
k ,x(i)
k−1) = gk(yk | ˜x(i)
k ) fk(˜x(i)
k | x(i)
k−1)
q(˜x(i)
k | x(i)
k−1,y1:k)
,
(3.2)
for i = 1,...,N, and deﬁne unnormalized empirical measure ˆπN
k|k as
ˆπN
k|k = 1
N
N
∑
i=1
wt(˜x(i)
k ,x(i)
k−1)δ˜x(i)
k ,
(3.3)
where δx denotes a Dirac delta measure concentrated at x.
– Normalize the weights by ˜w(i)
k =
w(i)
k
∑N
i=1 w(i)
k
, where w(i)
k = wk(˜x(i)
k ,x(i)
k−1), and deﬁne empir-
ical probability measure ˜πN
k|k as
˜πN
k|k =
N
∑
i=1
˜w(i)
k δ˜x(i)
k .
(3.4)
– Do resampling to obtain the resampled particles x(i)
k , and deﬁne empirical probability
measure πN
k|k, which is the approximation to the ﬁltering distribution, as
πN
k|k = 1
N
N
∑
i=1
δx(i)
k .
(3.5)
– k ←k +1
Let Fk−1 be the σ-ﬁeld generated by the particles {x(i)
k−1}N
i=1, πk|k−1 the measure corresponding to
the probability density p(xk | y1:k−1) and πk|k the measure corresponding to the density p(xk | y1:k).
If we denote g ≜gk and f ≜fk, Then the Bayesian ﬁltering equations (2.9) and (2.11), for (3.1),
can be written as (see, e.g., Del Moral, 2004; Hu et al., 2008, 2011, and Papers III, V and VI)
(πk|k−1,φ) = (πk−1|k−1, f φ),
(πk|k,φ) = (πk|k−1,φ g)
(πk|k−1,g) ,
(3.6)

3.1 Mean Square Error Convergence for Unbounded Importance Weights
33
where (πk,φ) ≜
Z
φ dπk, and f φ(x) ≜
Z
f(z | x)φ(z)dz. However, to cope with general impor-
tance distribution, the Bayesian ﬁltering equations (3.6) are written in such a way that the prediction
and update steps are combined, resulting to (see Papers III, V and VI)
(πk|k,φ) = (πk−1|k−1, f φ g)
(πk−1|k−1, f g) = ( ˆπk|k,φ)
( ˆπk|k,1) ,
(3.7)
where ˆπk|k(dxk) = (
R wk(xk,xk−1)q(xk | xk−1,y1:k)dπk−1|k−1)dxk. In order to establish the theo-
retical convergence results, we need some assumptions to the state space model (3.1), probability
measures, and importance weights. Also, the derivations require the use of some mathematical
results, such as the Minkowski inequality, Hölder’s inequality, Jensen’s inequality, Markov inequal-
ity, Borel-Cantelli argument, and Cauchy–Schwarz inequality which can be found for instance in
(Chung, 2001).
3.1
Mean Square Error Convergence for Unbounded Importance Weights
The mean square error convergence is attained by computing the bound for E
h
|(πN
k|k,φ)−(πk|k,φ)|2i
,
where E[φ(xk) | y1:k] ≈(πN
k|k,φ). For bounded test functions φ ∈B(Rn), where B(Rn) is the set of
bounded Borel measurable functions on Rn, most of the proofs of convergence of the mean square
error have assumed that the importance weights are bounded. However, there are importance dis-
tributions which can lead to unbounded importance weights. In Example 3.1.1, we show such
importance distribution that produce unbounded importance weights.
Example 3.1.1 (Unbounded importance weights). Suppose a priori dynamics of the state in the
Cox process is modelled as x(τ) ≜η1/2 |B(τ)|, where B(τ) is a standard Brownian motion. If
measurements are Poisson distributed with an intensity parameter λ(τ) = cx(τ), where c > 0 is a
constant, then the density f(xk | xk−1) with respect to the Lebesgue measure and the density g(yk | xk)
with respect to the counting measure are deﬁned as (see Paper VI)
f(xk | xk−1) =
1
√2π η

e−
(xk−xk−1)2
2η
+e−
(xk+xk−1)2
2η

,
g(yk | xk) =
(
limxk→0+ g(yk | xk),
if xk = 0,
(cxk)yk exp(−cxk)
yk!
,
otherwise.
If we select a Gamma distribution with constant parameters α,β > 0 as the importance distribution
for a particle ﬁlter, then the importance sampling density (with respect to the Lebesgue measure) is
q(xk) = β α
Γ(α) xα−1
k
exp(−β xk).
Clearly, for α > 1, the importance weight
w(xt,xt−1) =
1
√2π η
exp(−cxt)

e−
(xt−xt−1)2
2η
+e−
(xt+xt−1)2
2η

β α
Γ(α) xα−1
t
exp(−β xt)
,
becomes unbounded at origin, because q(0) = 0.

34
3. Theoretical Results for Particle Filters
From Example 3.1.1, based on existing proofs (see. e.g., Crisan and Doucet, 2000; Doucet et al.,
2001; Crisan and Doucet, 2002) we can conclude that the convergence of classical particle ﬁlter
Algorithm 3.1 is not guaranteed because the weights are unbounded. In this section, we impose
assumptions which lead to the guarantee of the convergence of the particle ﬁlter regardless of the
unboundedness of the weights. Particularly, we brieﬂy give the proofs for the mean square con-
vergence, L4-convergence and empirical measures convergence of Algorithm 3.1 in the case of
potentially unbounded importance weights.
In order to prove the mean square convergence, we impose the following assumptions (see Paper
VI).
Assumption 3.1. The measurement model gk is bounded, that is, there exist a constant cg < ∞such
that ∀k ∈N, ∀x ∈Rn, and ∀y ∈Rm we have gk(y | x) ≤cg < ∞.
Assumption 3.2. The resampling procedure satisﬁes (see, e.g., Crisan and Doucet (2000, 2002) for
the sufﬁcient conditions for this):
E
(πN
k|k,φ)−( ˜πN
k|k,φ)

2
≤Ck
∥φ∥2
N
,
(3.8)
where Ck is the constant and ∥φ∥≜supx∈Rn |φ(x)|.
Assumption 3.3. Let wk(xk,xk−1) be the unnormalized importance weight function. There exists a
constant Cw < ∞such that ∀k ∈N and ∀xk−1 ∈Rn, we have E[(w(i)
k )2(xk,xk−1) | xk−1] ≤Cw, with
the expectation taken over q(xk | xk−1,y1:k).
Having the above assumptions, we prove the following main theorem for the mean square error
convergence, which shows that the mean square error introduced by sampling is of the order 1/N.
Theorem 3.4. Provided that Assumptions 3.1, 3.2 and 3.3 hold for all k ≥0, then there exist a
constant ck such that, for any φ ∈B(Rn)
E
(πN
k|k,φ)−(πk|k,φ)

2
≤ck
∥φ∥2
N
.
(3.9)
Proof. For each step (initialization, prediction-update and resampling steps) of Algorithm 3.1, we
compute the bound for the mean square error.
−At k = 0, since the particles are independent and identically distributed variables, then
E
(πN
0|0,φ)−(π0|0,φ)

2
≤c0
∥φ∥2
N
.
(3.10)
−For k ≥1, the result follows by proving the following Lemmas 3.5 and 3.6.

3.1 Mean Square Error Convergence for Unbounded Importance Weights
35
Lemma 3.5. Let us assume that for φ ∈B(Rn) and Assumptions 3.1, 3.2 and 3.3 hold, we have
E
(πN
k−1|k−1,φ)−(πk−1|k−1,φ)

2
≤ck−1
∥φ∥2
N
.
(3.11)
Then
E
( ˜πN
k|k,φ)−(πk|k,φ)

2
≤˜ck
∥φ∥2
N
.
(3.12)
Proof. For the proof of this lemma, see Paper VI. However, the idea of the proof starts by deﬁning
( ˜πN
k|k,φ)−(πk|k,φ) ≤
∥φ∥
( ˆπk|k,1)
h
( ˆπk|k,1)−( ˆπN
k|k,1)
i
+
1
( ˆπk|k,1)
h
( ˆπN
k|k,φ)−( ˆπk|k,φ)
i
,
(3.13)
and split ( ˆπN
k|k,φ)−( ˆπk|k,φ) = Π1+Π2, where Π1 = ( ˆπN
k|k,φ)−E[( ˆπN
k|k,φ) | Fk−1] and Π2 = E[( ˆπN
k|k,φ) |
Fk−1] −( ˆπk|k,φ). Using the boundedness of φ, Assumption 3.3 and Equation (3.11), we can eas-
ily show that E[|Π1|2 | Fk−1] ≤¯c1 ∥φ∥2/N and E[|Π2|2 | Fk−1] ≤¯c2∥φ∥2/N, where ¯c1 and ¯c2 are
the constants. Also, using Minkowski inequality, we can show that E[|( ˆπN
k|k,φ) −( ˆπk|k,φ)|2] ≤
ˆck∥φ∥2/N, from which, further computations complete the proof.
Lemma 3.6. Assume that Assumptions 3.1, 3.2 and 3.3 hold and that
E
( ˜πN
k|k,φ)−(πk|k,φ)

2
≤˜ck
∥φ∥2
N
.
Then
E
(πN
k|k,φ)−(πk|k,φ)

2
≤ck
∥φ∥2
N
.
(3.14)
Proof. See Paper VI for the proof.
The application of the assumption imposed to guarantee the convergence of particle ﬁlter with un-
bounded importance weights can be seen in Example 3.4.1 by setting p = 2. Note that, the mean
square convergence results can be generalized to L4-convergence and empirical measure conver-
gence (see Paper VI). To guarantee the L4-convergence results, we use Assumption 3.1 together
with the following assumptions.
Assumption 3.7. The resampling procedure satisﬁes the condition (Crisan and Doucet, 2000):
E
(πN
k|k,φ)−( ˜πN
k|k,φ)

4
≤Ck
∥φ∥4
N2 .
(3.15)
Assumption 3.8. There exists a constant Cw < ∞such that ∀k ∈N and ∀xk−1 ∈Rn, we have
E[(w(i)
k )4(xk,xk−1) | xk−1] ≤Cw, with the expectation taken over q(xk | xk−1,y1:k).
Using Assumptions 3.1, 3.7 and 3.8, we can prove the following L4-convergence theorem.
Theorem 3.9. Provided that Assumptions 3.1, 3.7 and 3.8 hold for all k ≥0, then for φ ∈B(Rn) we
have
E
(πN
k|k,φ)−(πk|k,φ)

4
≤ck
∥φ∥4
N2 .
(3.16)

36
3. Theoretical Results for Particle Filters
Proof. See Paper VI.
The generalization of L4-convergence results to Lp-results, for bounded test functions, is possible.
This is done in Section 3.4 below, where we give a complete proof, showing all intermediate details
needed to establish the Lp-convergence results of particle ﬁlter with unbounded importance weights.
Also, the L4-convergence results can be used to deduce the empirical measure convergence given in
the following theorem which shows that, almost surely, limN→∞πN
k|k = πk|k.
Theorem 3.10. Provided that Assumptions 3.1, 3.7 and 3.8 hold for all k ≥0, then we have, almost
surely,
lim
N→∞πN
k|k = πk|k.
(3.17)
Proof. See Paper VI.
3.2
Convergence Results with Bounded Weights for Unbounded Test Func-
tions
In this section, we provide the L4-convergence results for the case where the test function is not
bounded. To guarantee the convergence of particle ﬁlter with unbounded test function, Hu et al.
(2008) modiﬁed the standard particle ﬁlter algorithm. The modiﬁed particle ﬁlter Algorithm 3.2
is constructed such that we always have ((πN
k−1|k−1,wk q),1) ≥γk > 0, where γk > 0 is a chosen
threshold (Hu et al., 2008). The choice of γk is described in (Hu et al., 2008). However, choosing
γk can be a challenging task since it is compared with the term ((πN
k−1|k−1,wk q),1) which involves
particles and measurements.
The L4-convergence results of Hu et al. (2008) covers only bootstrap type of importance distribu-
tions with bounded importance weights. In Paper III, we extended the Hu et al. (2008) convergence
proof to cover a general importance distribution with bounded importance weights. To get the re-
sults, we imposed the following assumptions.
Assumption 3.11. For any given y1:s, we have ((πs−1|s−1,ws−1 q),1) > 0, where s = 1,...,k.
Assumption 3.12. The dynamic model f, and measurement model g, are bounded.
Assumption 3.13. The importance weights are bounded.
Assumption 3.14. The function φk(·) satisﬁes supxs |φ(xs)|4g(ys | xs) < C(y1:s).
Using Assumptions 3.11, 3.12, 3.13 and 3.14, we brieﬂy discuss the convergence results by proving
the following main convergence theorem.
Theorem 3.15. Suppose Assumptions 3.11, 3.12, 3.13 and 3.14 hold. Then we have the following.
i. For a sufﬁciently large N, Algorithm 3.2 will not run into an inﬁnite loop.

3.2 Convergence Results with Bounded Weights for Unbounded Test Functions
37
Algorithm 3.2 Modiﬁed Particle Filter
• Initialize the particles {x(i)
0 }N
i=1 ∼π0(dx0), where i = 1,2,...,N
• For k = 1,2,..., perform the following.
– Draw samples ¯xi
k ∼∑N
j=1 αi
j q(xk | x j
k−1,yk), where α j
i are non-negative weights such
that
N
∑
j=1
αi
j = 1,
N
∑
i=1
αi
j = 1
and
1
N
N
∑
i=1
N
∑
j=1
αi
j q(xk | x j
k−1,yk) = 1
N
N
∑
j=1
q(xk | xj
k−1,yk).
– If ((πN
k−1|k−1, ¯w(i)
k ¯q),1) ≥γk, proceed to the next step otherwise return to the previous
step. Note that ¯wk and ¯q are the values evaluated at ¯x(i)
k .
– Rename ˜x(i)
k = ¯x(i)
k , and compute the importance weights by
w(i)
k = g(yk | ˜xk) f(˜xk | xk−1)
q(˜xk | xk−1,y1:k)
,
for i = 1,2,...,N.
– Normalize the weights as done in Algorithm 3.1.
– Perform resampling.
ii. Let L4
k(g) be the class of functions satisfying Assumption 3.14. For any φ ∈L4
k(g), there exists
a constant Ck|k, independent of N such that
E
(πN
k|k,φ)−(πk|k,φ)

4
≤Ck|k
∥φ∥4
k,4
N2
,
(3.18)
where ∥φ∥4
k,4 is deﬁned as (Hu et al., 2008)
∥φ∥k,4 = max
n
1,(πs|s,|φ|4)1/4, s = 0,1,...,k
o
.
(3.19)
Proof. The proof of Theorem 3.15, is found in Paper III. As a brieﬂy, from Equation (3.13), we
compute the bounds for E[|( ˆπN
k|k,φ) −( ˆπk|k,φ)|4] and E[( ˆπN
k|k,|φ|4)]. Again, we denote Fk−1 the
σ-algebra generated by x(i)
k−1 and split ( ˆπN
k|k,φ)−( ˆπk|k,φ) = Π1 +Π2 +Π3, where
Π1 = ( ˆπN
k|k,φ)−1
N
N
∑
i=1
E[φ(˜x(i)
k )ρ(˜x(i)
k ,xk−1) | Fk−1],
Π2 = 1
N
N
∑
i=1
E[φ(˜xi
k)wk(˜x(i)
k ,xk−1) | Fk−1]−1
N
N
∑
i=1
(πN,αi
k−1|k−1, f φ g),
Π3 = 1
N
N
∑
i=1
(πN,αi
k−1|k−1, f φ g)−( ˆπk|k,φ).
(3.20)

38
3. Theoretical Results for Particle Filters
Using Lemmas 7.1, 7.2, 7.3, 7.4, and 7.5 from Hu et al. (2008), we can easily deduce E[|Π1|4 |
Fk−1] ≤˜CΠ1
∥φ∥4
k−1,4
N2
, E[|Π2|4 | Fk−1] ≤˜CΠ2
∥φ∥4
k−1,4
N2
and E[|Π3|4 | Fk−1] ≤˜CΠ3
∥φ∥4
k−1,4
N2
, which can be
combined by Minkowski’s inequality, to get
E
( ˆπN
k|k,φ)−( ˆπk|k,φ)

4
≤ˆCk|k
∥φ∥4
k−1,4
N2
.
(3.21)
Similarly, we can show that
E
h( ˆπN
k|k,|φ|4)

i
≤Mk|k||φ||4
k−1,4.
(3.22)
Using Equations (3.21) and (3.22), with φ = 1, as well as Minkowski’s inequality, the results of the
theorem follow.
3.3
Convergence Results with Unbounded Weights for Unbounded Test Func-
tions
As pointed out in the preceding section, there exist importance distributions which do not ensure that
the importance weights are uniformly bounded. For example, the importance weights can become
inﬁnite in isolated points provided that the required expectations of them remain bounded (see Paper
III). In Paper V, we provide the convergence results of the modiﬁed particle ﬁlter algorithm 3.2
with the assumption that the importance weights are not bounded. The idea in Paper V is to prove
Theorem 3.15 by computing the bounds for E[|Π1|4 | Fk−1], E[|Π2|4 | Fk−1] and E[|Π3|4 | Fk−1],
where Π1, Π2 and Π3 are deﬁned as Equations (3.20). To establish the convergence results, we use
Assumptions 3.11, 3.12, and 3.14, together with the following
Assumption 3.16. For any potentially unbounded importance weights wk(xk,xk−1) deﬁned as
wk(xk,xk−1) = g(yk | xk) f(xk | xk−1)
q(xk | xk−1,yk)
,
(3.23)
the seventh order moment E[(w(i)
k (xk,xk−1))7 | xk−1] is ﬁnite, where the expectation is over q(.).
Therefore, using Assumptions 3.11, 3.12, 3.14, and 3.16 as well as Lemmas 7.1, 7.2, 7.3, 7.4 and
7.5 from Hu et al. (2008), we can prove Theorem 3.15 for the case of unbounded importance weight
(see Paper V for the proof).
3.4
General Particle Filter Convergence Results for Bounded Test Functions
As mentioned in Section 3.1, the L4-convergence results of the particle ﬁlter Algorithm 3.1 with
unbounded importance weights to the case of bounded test functions can be generalized to general
Lp-convergence results. In order to obtain such results, we use Assumption 3.1 and impose the
following assumptions.

3.4 General Particle Filter Convergence Results for Bounded Test Functions
39
Assumption 3.17. The resampling procedure satisﬁes (see, e.g., Crisan and Doucet, 2000, for the
sufﬁcient conditions for this):
E
h(πN
k|k,φ)−( ˜πN
k|k,φ)

pi
≤Ck
∥φ∥p
N p/2 ,
(3.24)
where ∥φ∥≜supx∈Rn |φ(x)|.
Assumption 3.18. The importance density q satisﬁes the following condition. Let
wk(xk,xk−1) = g(yk | xk) f(xk | xk−1)
q(xk | xt−1,y1:k)
,
be the unnormalized importance weight function. There exists a constant Cw < ∞such that ∀k ∈N
and ∀xk−1 ∈Rn, we have E[(w(i)
k )p(xk,xk−1) | xk−1] ≤Cw, with the expectation taken over q(xk |
xk−1,y1:k).
Given the assumptions above, we are now in the position to prove the following theorem.
Theorem 3.19. Provided that Assumptions 3.1, 3.17 and 3.18 hold for all k ≥0, then there exist a
constant ck such that, for any φ ∈B(Rn) and p ≥2 we have
E
h(πN
k|k,φ)−(πk|k,φ)

pi
≤ck
∥φ∥p
N p/2 .
(3.25)
Proof. We give a detailed proof of this theorem for each particle ﬁlter algorithm steps as follows.
−For k = 0 (the initialization step), the result follows because the N-particles from prior distribu-
tion are assumed to be independent and identically distributed variables. That is,
E
h(πN
0|0,φ)−(π0|0,φ)

pi
≤c0
∥φ∥p
N p/2 .
−For k ≥1 (the prediction-update and resampling steps), the results follow by proving the follow-
ing Lemmas 3.20 and 3.24, which correspond to the prediction-update step and resampling
step, respectively.
Lemma 3.20. Assume that Assumptions 3.1, 3.17 and 3.18 hold. For any φ ∈B(Rn), and
E
h(πN
k−1|k−1,φ)−(πk−1|k−1,φ)

pi
≤ck−1
∥φ∥p
N p/2 ,
(3.26)
then
E
h( ˜πN
k|k,φ)−(πk|k,φ)

pi
≤˜ck
∥φ∥p
N p/2 .
(3.27)
Proof. In order to prove this lemma, we impose the following auxiliary lemmas (see, Hu et al.,
2011, for their proofs).

40
3. Theoretical Results for Particle Filters
Lemma 3.21. Let p ≥2 and {ξi,
i = 1,...,N} be conditionally independent random variables
given σ-algebra G such that E[ξi|G ] = 0 and E[|ξi|p|G ] < ∞. Then
E
"
N
∑
i=1
ξi

p
| G
#
≤C(p)


N
∑
i=1
E[|ξi|p | G ]+
 
N
∑
i=1
E[|ξi|2 | G ]
!p/2
,
where C(p) is the constant that grows exponentially or polynomially (Härdle et al., 1998; Kantas
et al., 2009).
Lemma 3.22. If E[|ξ|p] < ∞, then
E
hξ −E[ξ]

pi
≤2p E[|ξ|p],
for
p ≥1.
Having the above auxiliary lemmas, we proceed with the proof of Lemma 3.20 as follows. Let Fk−1
be the σ-ﬁeld generated by {x(i)
k−1}N
i=1. Then
E[( ˆπN
k|k,φ) | Fk−1] = E
h
(πN
k−1|k−1,φ wk) | Fk−1
i
= (πN
k−1|k−1, f φ g).
(3.28)
From Assumption 3.18, we can show the boundedness of E[(w(i)
k )p | Fk−1] as follows
Remark 3.23. Provided that E[(w(i)
k )p | xk−1] is bounded, then E[(w(i)
k )p | Fk−1] is bounded as well.
This can easily be shown as
E[(w(i)
k )p | Fk−1] = E
"
1
N
N
∑
i=1

w(i)
k
p
| xk−1
#
≤E
"
sup
i=1,...,N

w(i)
k
p
| xk−1
#
≤sup
x0:k

E
h
w(i)
k
p
| xk−1
i
≤sup
x0:k
cw ≤Cp
w.
Using Equation (3.7), we can simplify ( ˜πN
k|k,φ)−(πk|k,φ) as follows.
( ˜πN
k|k,φ)−(πk|k,φ)
=
( ˆπN
k|k,φ)
( ˆπN
k|k,1) −( ˆπk|k,φ)
( ˆπk|k,1) =
( ˆπN
k|k,φ)( ˆπk|k,1)−( ˆπk|k,φ)( ˆπN
k|k,1)
( ˆπN
k|k,1)( ˆπk|k,1)
=
( ˆπN
k|k,φ)( ˆπk|k,1)−( ˆπN
k|k,φ)( ˆπN
k|k,1)
( ˆπN
k|k,1)( ˆπk|k,1)
+
( ˆπN
k|k,φ)( ˆπN
k|k,1)−( ˆπk|k,φ)( ˆπN
k|k,1)
( ˆπN
k|k,1)( ˆπk|k,1)
=
( ˆπN
k|k,φ)
( ˆπN
k|k,1)( ˆπk|k,1)
h
( ˆπk|k,1)−( ˆπN
k|k,1)
i
+
( ˆπN
k|k,1)
( ˆπN
k|k,1)( ˆπk|k,1)
h
( ˆπN
k|k,φ)−( ˆπk|k,φ)
i
=
( ˜πN
k|k,φ)
( ˆπk|k,1)
h
( ˆπk|k,1)−( ˆπN
k|k,1)
i
+
1
( ˆπk|k,1)
h
( ˆπN
k|k,φ)−( ˆπk|k,φ)
i
≤
∥φ∥
( ˆπk|k,1)
h
( ˆπk|k,1)−( ˆπN
k|k,1)
i
+
1
( ˆπk|k,1)
h
( ˆπN
k|k,φ)−( ˆπk|k,φ)
i
(3.29)
From Equation (3.29), we see that to prove Equation (3.27) we need to evaluate the bounds for the
terms E[|( ˆπN
k|k,φ)−( ˆπk|k,φ)|p] and E[|( ˆπN
k|k,1)−( ˆπk|k,1)|p]. However, we only need to evaluate the

3.4 General Particle Filter Convergence Results for Bounded Test Functions
41
bound for the former term. The bound for the latter term follows by setting φ = 1 to the former term.
Thus, consider the expression ( ˆπN
k|k,φ) −( ˆπk|k,φ), which can be written as ( ˆπN
k|k,φ) −( ˆπk|k,φ) =
Π1 +Π2, where
Π1 = ( ˆπN
k|k,φ)−E[( ˆπN
k|k,φ) | Fk−1]
and
Π2 = E[( ˆπN
k|k,φ) | Fk−1]−( ˆπk|k,φ).
To evaluate the bound for E[|( ˆπN
k|k,φ) −( ˆπk|k,φ)|p], we need to compute E[|Π1|p] and E[|Π2|p] as
follows.
E
h( ˆπN
k|k,φ)−E[( ˆπN
k|k,φ) | Fk−1]

p
| Fk−1
i
= E
" 1
N
N
∑
i=1
φ(x(i)
k )w(i)
k −E
"
1
N
N
∑
i=1
φ(x(i)
k )w(i)
k | Fk−1
#
p
| Fk−1
#
= 1
N p E
"
N
∑
i=1

φ(x(i)
k )w(i)
k −E
h
φ(x(i)
k )w(i)
k | Fk−1
i
p
| Fk−1
#
≤C(p)
N p
N
∑
i=1
E
hφ(x(i)
k )w(i)
k −E
h
φ(x(i)
k )w(i)
k | Fk−1
i
p
| Fk−1
i
+ C(p)
N p
 
N
∑
i=1
E
φ(x(i)
k )w(i)
k −E
h
φ(x(i)
k )w(i)
k | Fk−1
i
2
| Fk−1
!p/2
Lemma 3.21
≤2pC(p)
N p


N
∑
i=1
E
hφ(x(i)
k )w(i)
k

p
| Fk−1
i
+
 
N
∑
i=1
E
φ(x(i)
k )w(i)
k

2
| Fk−1
!p/2
Lemma 3.22
= 2pC(p)
N p


N
∑
i=1
E
hφ(x(i)
k )

pw(i)
k

p
| Fk−1
i
+
 
N
∑
i=1
E
φ(x(i)
k )

2w(i)
k

2
| Fk−1
!p/2

≤2pC(p)∥φ∥p
N p


N
∑
i=1
E
hw(i)
k

p
| Fk−1
i
+
 
N
∑
i=1
E
w(i)
k

2
| Fk−1
!p/2

because |φ|p ≤∥φ∥p
≤2pC(p)∥φ∥p
N p


N
∑
i=1
Cp
w +
 
N
∑
i=1
C2
w
!p/2

By Remark 3.23
= 2pC(p)∥φ∥p
N p

NCp
w +N p/2Cp
w

= 2pC(p)∥φ∥p
N p/2
 Cp
w
N p/2 +Cp
w

≤2pC(p)∥φ∥p
N p/2
(Cp
w +Cp
w)
= 2pC(p)Cp
w∥φ∥p
N p/2
= ˜c1
∥φ∥p
N p/2 .
(3.30)

42
3. Theoretical Results for Particle Filters
E
hE[( ˆπN
k|k,φ) | Fk−1]−( ˆπk|k,φ)

p
| Fk−1
i
= E
h(πN
k−1|k−1, fφg)−(πk−1|k−1, fφg)

p
| Fk−1
i
By Equation (3.28)
≤ck−1
∥f φ g∥p
N p/2
≤ck−1
∥φ g∥p
N p/2
By Equation (3.26) and ∥fφ∥≤∥φ∥
= ck−1 ∥g∥p∥φ∥p
N p/2 = ˜c2
∥φ∥p
N p/2 .
(3.31)
We combine Equations (3.30) and (3.31), by Minkowski inequality, to have
(E[|( ˆπN
k|k,φ)−( ˆπk|k,φ)|p])
1
p ≤

˜c1
∥φ∥p
N p/2
 1
p
+

˜c2
∥φ∥p
N p/2
 1
p
=

˜c
1
p
1 + ˜c
1
p
2
 ∥φ∥
N1/2 = ˜c
1
p
k1
∥φ∥
N1/2,
which implies
E[|( ˆπN
k|k,φ)−( ˆπk|k,φ)|p] ≤˜ck1
∥φ∥p
N p/2 ,
(3.32)
and, for φ = 1, we have
E[|( ˆπN
k|k,1)−( ˆπk|k,1)|p] ≤
˜ck1
N p/2.
(3.33)
Introduce the Minkowski inequality to (3.29), we have

E
h( ˜πN
k ,φ)−(πk,φ)

pi1/p
≤

∥φ∥
( ˆπk|k,1)


E
h( ˆπN
k|k,1)−( ˆπk|k,1)

pi1/p
+

1
( ˆπk|k,1)


E
h( ˆπN
k|k,φ)−( ˆπk|k,φ)

pi1/p
≤

∥φ∥
( ˆπk|k,1)


˜ck1
1
N p/2
1/p
+

1
( ˆπk|k,1)


˜ck1
∥φ∥p
N p/2
1/p
=

∥φ∥
( ˆπk|k,1)
˜c1/p
k1
1
N1/2 +

1
( ˆπk|k,1)
˜c1/p
k1
∥φ∥
N1/2
=
 
1
( ˆπk|k,1)
˜c1/p
k1 +

1
( ˆπk|k,1)
˜c1/p
k1
!
∥φ∥
N1/2 = ˜c1/p
k
∥φ∥
N1/2.
(3.34)
Therefore
E
h( ˜πN
k ,φ)−(πN
k ,φ)

pi
≤˜ck
∥φ∥p
N p/2 .
Lemma 3.24. Let us assume that for any φ ∈B(Rn), we have
E
h( ˜πN
k ,φ)−(πN
k ,φ)

pi
≤˜ck
∥φ∥p
N p/2 .
(3.35)
Then
E
h(πN
k|k,φ)−(πk|k,φ)

pi
≤ck
∥φ∥p
N p/2 .
(3.36)

3.4 General Particle Filter Convergence Results for Bounded Test Functions
43
Proof. Deﬁne (πN
k|k,φ)−(πk|k,φ) = (πN
k|k,φ)−( ˜πN
k|k,φ)+( ˜πN
k|k,φ)−(πk|k,φ). Then, using Minkowski
inequality together with Assumption 3.17 and Equation (3.35) we have

E
h(πN
k|k,φ)−(πk|k,φ)

pi1/p
≤

E
h(πN
k|k,φ)−( ˜πN
k|k,φ)

pi1/p
+

E
h( ˜πN
k|k,φ)−(πk|k,φ)

pi1/p
≤
p
Ck
∥φ∥
N1/2 +√˜ck
∥φ∥
N1/2 = √ck
∥φ∥
N1/2,
which implies that
E
h(πN
k|k,φ)−(πk|k,φ)

pi
≤ck
∥φ∥p
N p/2 .
Example 3.4.1 (Continuation of Example 3.1.1). If we select some α ∈(1,∞)\Z and β ∈(0,4c/3),
then it is now easy to show that even when yk = 0, we have
Z ∞
0 (w(xk,xk−1))p q(xk)dxk ≤cw < ∞,
for p ≥1.
(3.37)
Hence according to the present theory the particle ﬁlter converges in Lp sense for bounded Borel
functions regardless the unboundedness of importance weights (see Paper VI for numerical results).

44
3. Theoretical Results for Particle Filters

CHAPTER IV
Markov Chain Monte Carlo
The Markov chain Monte Carlo (MCMC) approach is a Monte Carlo sampling technique which
utilizes Monte Carlo integration and Markov chains (Gilks et al., 1996). It was ﬁrst introduced by
Metropolis et al. (1953) as a simulation method of energy levels of atoms in a crystalline structure
and later on, Hastings (1970) applied the idea to statistical problems. The main idea in many MCMC
methods is ﬁrst to draw samples from a proposal distribution. The proposal distribution can depend
on the current value, thus forms a Markov chain. Secondly, a speciﬁcally constructed accept/reject
mechanism is used to correct the arbitrary proposal mechanism and simulate from the invariant
distribution of interest called the target distribution. At the end, a sequence of samples is obtained
from which the distribution or integrals can be approximated.
There are various MCMC methods, some of the most well-known are the Metropolis algorithm,
Metropolis–Hastings, Gibbs sampler, hybrid Monte Carlo, reversible jump Monte Carlo (see Gilks
et al., 1996; Neal, 2011; Gelman et al., 2004). In this chapter, Metropolis–Hastings algorithm,
adaptive MCMC algorithms, and hybrid Monte Carlo are discussed. These are the only MCMC
methods that have been used in this work (see Papers I, II, IV and VII)
4.1
Metropolis–Hastings Algorithm
The impossibility of sampling from the posterior distribution p(x | y1,...,yM) is one of the limi-
tations, which leads to indirect sampling. To circumvent this problem, sampling techniques, like
rejection and importance sampling, are used (Liu, 2002). These techniques require a new distri-
bution called a proposal distribution, from which samples are generated. We denote the proposal
distribution as q(x∗| xi), where x∗is a candidate point and xi is the current sample at iteration i.
The rejection and importance sampling techniques require that the proposal and posterior distribu-
tions should be similar. In the large and complex problems, it is difﬁcult to come-up with proposal
distribution with such property. To solve this problem, the Metropolis–Hastings method, which in
principle can use any form of proposal distribution which depends on the current sample xi, can be
used. The algorithmic version of the Metropolis–Hastings method is shown in Algorithm 4.1 (Gilks
et al., 1996). Note that if the proposal distribution is symmetric such as Gaussian, the Metropolis–
Hastings algorithm reduces to Metropolis algorithm, where q(xi−1 | x∗) = q(x∗| xi−1).
The proposal distribution is the crucial ingredient affecting the Metropolis–Hastings performance
as it explores the intractable distribution and ensures that irreducibility holds at each iteration. Ir-
45

46
4. Markov Chain Monte Carlo
Algorithm 4.1 Metropolis–Hastings
i. Draw the starting point, x0, from an initial distribution p0(x).
ii. For i = 1,2,..., perform the following.
• Sample a candidate point x∗from the proposal distribution q(x∗| xi−1).
• Accept the candidate point and set xi = x∗with the probability given by:
α(xi−1,x∗) = min

1, p(x∗| y1,...,yM)q(xi−1 | x∗)
p(xi−1 | y1,...,yM)q(x∗| xi−1)

.
(4.1)
This random acceptance is done by generating u from a standard uniform distribution,
u ∼U[1, 0], and accepting x∗if u ≤α(xi−1,x∗).
reducibility is a property of Markov chain to reach all interesting parts of the state space model
(SSM) (Gilks et al., 1996). In general, the proposal distributions used should result in good mixing
of samples and in a suitable acceptance rate. Determining which proposal distribution is best for a
particular target distribution is very important, but also a difﬁcult task, because it involves a lot of
trial and error. In the next section, the adaptive MCMC method, which addresses solve this problem,
is discussed.
4.2
Adaptive Markov Chain Monte Carlo
The Metropolis–Hastings algorithm converges reasonably well when the proposal distribution is
well tuned. In most cases the tuning is done by hand, that is, the tuning is a trial-error technique.
For instance, Gelman et al. (1996) and Roberts et al. (1997) proposed a method to monitor the
acceptance rate and scale the proposal distribution manually. To avoid the trial-error tuning, adaptive
Markov chain Monte Carlo algorithms can be used. The idea behind the adaptive MCMC algorithms
is that the proposal distribution is tuned automatically during the MCMC run (Gilks et al., 1996;
Haario et al., 1999, 2006; Cai et al., 2006; Laine, 2008; Andrieu and Thoms, 2008; Liang et al., 2010;
Barber et al., 2011; Rosenthal, 2011; Vihola, 2012). There are many adaptive MCMC algorithms
proposed by different authors but they can be categorized, into six classes:
1) Stochastic approximation-based adaptive algorithms. This class contain all adaptive MCMC
algorithms that require the diminishing adaptation property (Andrieu and Thoms, 2008; Liang
et al., 2010).
2) Adaptive independent Metropolis–Hastings algorithms. In this class the proposal is updated with
past samples, avoiding the requirement of diminishing adaptation (Keith et al., 2008; Holden
et al., 2009; Liang et al., 2010).
3) Regeneration-based adaptive algorithms. The algorithms in this class are designed on a basic
property of the Markov chain, whose future outputs become independent of the past after each
regeneration point (Gilks et al., 1998; Liang et al., 2010).

4.2 Adaptive Markov Chain Monte Carlo
47
4) Population-based adaptive algorithms. Here, the algorithms work on an enlarged state space,
which allow the practitioners to design adaptive proposals and to incorporate sophisticated com-
putational techniques (Liang and Wong, 2000, 2001; Liang et al., 2010).
5) Differential evolution adaptive MCMC. In this class, the algorithms permit proper accounting of
all sources of simulation error, including parameter, model structural and forcing data uncertainty
(Ter Braak, 2006).
6) Filtering adaptive MCMC. This is a new adaptive MCMC where the adaptation of the proposal
distribution is done by the variational Bayesian adaptive Kalman ﬁlters (see Paper IV). This
algorithm may also fall to the ﬁrst category above because it has diminishing adaptation property.
In this thesis, we have used the stochastic approximation-based adaptive algorithm (Papers II and
VII) and developed the ﬁltering adaptive MCMC called variational Bayesian adaptive Metropolis
algorithm (Paper IV). Thus, only these adaptive versions are discussed in the following sections.
4.2.1
Stochastic Approximation-Based Adaptive Algorithms
The Gaussian distribution is often used as a proposal distribution. Thus, it is the covariance matrix
that needs to be well tuned for a good acceptance rate and mixing of the samples. Under certain
settings, Gelman et al. (1996) showed that the optimal covariance matrix is λΣ, where Σ is the n×n
covariance matrix of the target distribution and λ is the proposal covariance scaling factor which
optimizes the mixing property of the MCMC algorithm. When the proposal and target distributions
are both Gaussian, the value of λ is 2.382/n (Gelman et al., 1996).
In the adaptive Metropolis algorithm of Haario et al. (2001), the covariance matrix Σ is learned “on
the ﬂy”, that is, Σ is computed from the empirical distribution of the available generated MCMC
samples by using the following formula.
Σi = cov(x0,x1,...,xi)+ε In.
(4.2)
Here, In is an n × n identity matrix, ε is a small positive value whose role is to make sure that Σi
is not singular (Haario et al., 1999, 2001). At the initial step, i = 0, the initial covariance Σ0 is
initialized depending on prior information available. If the initial guess of the covariance is poor, it
may hinder the learning of the target distribution in the initial stages of the algorithm. Similarly, if
λΣi is too large in some directions or too small in all directions, the algorithm will have a very small
or very large acceptance probability, which will result in very slow learning of the target distribution
covariance (Andrieu and Thoms, 2008). These problems have motivated the use of delayed rejection
technique (Haario et al., 2006) or, for large dimension (say n ≤1000), the adaptation is performed
component by component (Haario et al., 2005; Andrieu and Thoms, 2008; Liang et al., 2010).
Alternatively, simultaneous adaptation of λ and Σ forces the acceptance probability to a sensible
value (Andrieu and Thoms, 2008; Liang et al., 2010). The adaptation of λ can be done by Robbins–
Monro recursion as (Robbins and Monro, 1951)
logλi+1 = logλi +γi+1

α(xi,x∗)−¯α∗

,
(4.3)
where ¯α∗is the targeted acceptance rate, for instance, ¯α∗= 0.234 (Roberts and Rosenthal, 2001),
α(xi,x∗) is the acceptance probability deﬁned as (4.1) and γi+1 is a gain factor sequence satisfying

48
4. Markov Chain Monte Carlo
the following conditions (Andrieu and Thoms, 2008; Liang et al., 2010):
∞
∑
i=1
γi = ∞
and
∞
∑
i=1
γ1+δ
i
< ∞,
for some δ ∈(0,1].
A good example of the algorithm, where the aim is to attain ¯α∗, is the robust adaptive Metropolis
Algorithm 4.2 proposed by (Vihola, 2012).
Algorithm 4.2 Robust adaptive Metropolis
i. Draw x0 from an initial distribution and initilize Σ0.
ii. For i = 1,2,..., perform the following.
• Compute xi = xi−1 +Si−1Ui, where Σi−1 = Si−1ST
i−1 and Ui ∼q is an independent ran-
dom vector sampled from spherically symmetric proposal distribution q.
• Accept xi+1 = xi with the probability
α(xi−1,xi) = min

1,
p(xi | y1,...,yM)
p(xi−1 | y1,...,yM)

.
(4.4)
Otherwise the proposal is rejected and xi+1 = xi−1.
• Compute Σi from
Σi = Si−1

Id +ηi (αi −¯α∗) UiUT
i
∥Ui∥2

ST
i−1,
where ηi is adaptation step size sequence.
Example 4.2.1 (Blood stage malaria with double infection). In Paper VII, the classical models of
blood stage malaria are reviewed and a new model, which incorporates double infection is pro-
posed. The proposed model is (see Paper VII for more details)
dx
dt
=
Λ−µx−β1xm,
dy1
dt
=
β1xm−υ1y1 −δ1y1 −β2y1m,
(4.5)
dy2
dt
=
β2y1m−υ2y2 −δ2y2,
dm
dt
=
r1δ1y1 +r2δ2y2 −νm−β1xm−β2y1m,
where x is the concentration of uninfected RBCs, y1 and y2 are the infected RBCs, and m is the num-
ber of free malaria parasites. Here, we need to ﬁt R1 = r1δ1, σ1 = υ1 +δ1, R2 = r2δ2, σ2 = υ2 +δ2,
β1 and β2 using adaptive MCMC approach. Due to absence of data, the model was numerically
simulated and 10000 MCMC samples were generated. Figures 4.1 and 4.2 show the autocorrela-
tion functions and marginal distributions of MCMC samples. It can be concluded that the MCMC
samples converge to the target distribution, hence the parameters are identiﬁable (see Paper VII for
more analytical and numerical analysis of the model (4.5)).

4.2 Adaptive Markov Chain Monte Carlo
49
10
20
30
40
50
0
0.5
1
R1
10
20
30
40
50
0
0.5
1
σ1
10
20
30
40
50
0
0.5
1
R2
10
20
30
40
50
0
0.5
1
σ2
10
20
30
40
50
0
0.5
1
β1
10
20
30
40
50
0
0.5
1
β2
Figure 4.1: Autocorrelation functions of MCMC parameter samples. The autocorrelation coefﬁ-
cients (x-axis) decay toward zero, and stabilize around zero as the number of lags (y-axis) increases.
15.4
15.6
15.8
16
16.2
16.4
0
100
200
300
R1
0.515
0.52
0.525
0.53
0
100
200
300
σ1
13
13.5
14
14.5
0
100
200
300
400
500
R2
0.59
0.595
0.6
0.605
0.61
0
100
200
300
σ2
2.45
2.5
2.55
2.6
x 10
−10
0
100
200
β1
1.9
1.95
2
2.05
2.1
2.15
x 10
−10
0
100
200
300
400
β2
Figure 4.2: Marginal distribution plot together with mean of MCMC samples (green) and the origi-
nal parameter values (black).

50
4. Markov Chain Monte Carlo
4.2.2
Variational Bayesian Adaptive Metropolis Algorithm
It is possible for the covariance matrix of the Gaussian proposal for Metropolis algorithm to be
updated using adaptive Kalman ﬁlter based on variational Bayesian approximation. The resulting
algorithm is called variational Bayesian adaptive Metropolis algorithm (see Paper IV). The idea of
the algorithm starts by allowing the unknown measurement noise covariance to vary at every time
step. That is, we deﬁne the linear state space model as
xk ∼N(Ak xk−1,Qk−1),
yk ∼N(Hkxk,Σk),
Σk ∼p(Σk | Σk−1),
(4.6)
where Ak, Qk−1 and Hk are known matrices and are as deﬁned in Deﬁnition 2.1.1. In Equation (4.6),
the ﬁrst two equations are uniformly completely controllable and observable with any bounded se-
quence of Σk. The last equation deﬁnes the Markovian dynamic model for the unknown measure-
ment noise covariance. To get a proposal covariance matrix for the Metropolis algorithm, we need
to approximate Σk. The approximation starts by deﬁning the joint ﬁltering distribution of the state
and covariance matrix, which is denoted as p(xk,Σk | y1:k) and, in most cases, intractable (Särkkä
and Nummenmaa, 2009; Särkkä and Hartikainen, 2013). Thus, we approximate it by variational
Bayesian (VB) method (cf. Šmídl and Quinn, 2006). The idea of the VB method is to approximate
p(xk,Σk | y1:k) in terms of approximate marginals Qx(xk) and QΣ(Σk), that is
p(xk,Σk | y1:k) ≈Qx(xk)QΣ(Σk).
Next, we have to compute the Qx(xk) and QΣ(Σk) using Kullback–Leibler divergence equation and
the methods from calculus of variations (for more details see, e.g,. Särkkä and Nummenmaa, 2009).
The computations result in
Qx(xk) = N(xk |,mk,Pk)
and
QΣ(Σk) = IW(Σk | νk,Vk),
where mk and Pk are given by the standard Kalman ﬁlter, and νk and Vk are the parameters of the
inverse Wishart (IW) distribution. The proposal covariance can, for example, be computed as the
mean of the inverse Wishart distribution (see, e.g,. Särkkä and Nummenmaa, 2009):
Σk =
1
νk −n−1Vk.
(4.7)
The computation of the covariance Σk is done recursively by variational Bayesian adaptive Kalman
ﬁlter Algorithm 4.3 (See Paper IV). Thus, we can present the adaptive Metropolis algorithm where
the proposal covaraince is updated by (4.7). Suppose m0 and P0 are the initial states parameters for
the Gaussian distribution and ν0 and V0 are initial parameters for the inverse-Wishart distribution.
Then, the variational Bayesian adaptive Metropolis algorithm (VBAM) is performed recursively as
shown in Algorithm 4.4. To check the convergence of VBAM algorithm, it is enough to prove the
following theorem (see Paper IV).
Theorem 4.1. Suppose that the target density π(x) = p(x | y1,...,yM) is bounded and has a
bounded support. Furthermore, suppose that ∑k≥1 k−1γk < ∞. Then, for bounded functions φ,
the strong law of large numbers holds, that is,
1
Nsamp
Nsamp
∑
k=1
φ(xk) a.s.
−−→
Z
Rd φ(x)π(x)dx,

4.2 Adaptive Markov Chain Monte Carlo
51
Algorithm 4.3 variational Bayesian adaptive Kalman ﬁlter
• Initialize ν0, m0, P0 and Σ0.
• For k = 1,2,...
– Prediction: compute the parameters of the predicted distribution:
m−
k = Ak−1 mk−1,
P−
k = Ak−1 Pk−1 AT
k−1 +Qk−1,
ν−
k = ρ(νk−1 −n−1)+n+1,
Σ−
k = BΣk−1 BT,
where ρ is a real number 0 < ρ ≤1 and B is a matrix 0 < |B| ≤1.
– Update: set νk = ν−
k +1 and Σ(1)
k
= Σ−
k . Iterate the following until the convergence (say,
c times for j = 1,...,c):
S(j+1)
k
= Hk P−
k HT
k +Σ(j)
k ,
K(j+1)
k
= P−
k HT
k

S(j+1)
k
−1
,
m(j+1)
k
= m−
k +K(j+1)
k
 yk −Hk m−
k

,
P(j+1)
k
= P−
k −K(j+1)
k
S(j+1)
k

K(j+1)
k
T
,
Σ(j+1)
k
=
νk−1 −n−1
νk −n−1

Σ−
k +

1
νk −n−1

Hk P(j+1)
k
HT
k
+

1
νk −n−1

yk −Hk m(j+1)
k

yk −Hk m(j+1)
k
T
.
(4.8)
– Set Σk = Σ(c)
k , mk = m(c)
k
and Pk = P(c)
k .

52
4. Markov Chain Monte Carlo
Algorithm 4.4 Variation Bayesian adaptive Metropolis
• Initialize x0, Σ0, m0, P0, ν0, and λ0.
• For i = 1,2,...
– Sample a candidate from the Gaussian proposal distribution x∗∼N(xi−1,λi−1 Σi−1).
– Calculate the acceptance probability α(xi−1,xi) using Equation (4.4)
– Set
xi =
 x∗
if u < α(xi−1,xi),
where u ∼U(0,1)
xi−1
Otherwise.
– Update the proposal covariance matrix by Algorithm 4.3, where xi is the measurement
at i. Check that Σi > µ1In and Σi < µ2In, where µ1 and µ2 are predeﬁned constants.
If this is not true, set Σi = Σi−1 and do a single iteration update step (ignoring the last
equation) of Algorithm 4.3 to compute the updated mean and covariance corresponding
to this noise covariance.
– Optionally, update λi using Equation (4.3). If λi /∈[δλ,δ −1
λ ], then force it to the interval
[δλ,δ −1
λ ].
where Nsamp is the number of samples, and x1,...,xNsamp are VBAM generated samples assuming the
uniform complete controllability and observability properties of the state space model are satisﬁed.
Proof. For full derivation of the convergence of the VBAM algorithm see Paper IV. However, here
we brieﬂy give hints on the main issues.
Using uniform complete observability and uniform complete controllability properties for the ﬁrst
two equations of (4.6), the sequence of mk and Pk produced by Algorithm 4.3 are bounded. To
check this, one needs to use Lemmas 7.1 and 7.2, and Theorem 7.4 from Jazwinski (1970). Note
that the measurements are bounded because they are sampled from a compactly supported target dis-
tribution. The boundedness of mk and Pk, together with the assumption that νk −n−1 > 0, guaran-
tees that the sequence Ω= {Σ(1)
k ,Σ(2)
k ,...,Σ(c−1)
k
,Σ(c)
k ,Σ(c+1)
k
,...}, produced by Algorithm 4.3 con-
verges. Using the boundedness of mk and Pk, we can ﬁnd an appropriate bound for ∥Σk −Σk−1∥F,
where ∥·∥F is the Frobenius norm, and the bound for the difference of transition probabilities.
Next we show how the VBAM algorithm works in the following 2-dimensional banana–shaped
distribution example. For more examples, we refer the reader to Paper IV, from where the VBAM
and classical adaptive MCMC results are compared.
BANANA–SHAPED DISTRIBUTION EXAMPLE
This example constructs a non-Gaussian target distribution by twisting a 2-dimensional Gaussian
distribution. The Gaussian distribution has unit component variances and the correlation between
the components equals to 0.9. This “banana”-shaped distribution example is often used as an exam-
ple to study the adaptive MCMC algorithm performance (see, e.g., Haario et al., 1999; Roberts and
Rosenthal, 2001; Haario et al., 2001; Bornkamp, 2011).

4.2 Adaptive Markov Chain Monte Carlo
53
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
x 10
4
−4
−2
0
2
4
x−samples
Iterations
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
x 10
4
−15
−10
−5
0
5
y−samples
Iterations
Figure 4.3: Trace plots for x-samples and y-samples. Mixing of the samples in both coordinates is
relatively good, which means that the covariance matrix of the proposal distribution can be adapted
using VBAKF algorithm.
The Gaussian coordinates x and y are twisted with a banana function to produce more non-linear
target. As the determinant of the transformation is 1, we calculate the probability regions of the
nonlinear target and study the trace and scatter plots of generated samples. The banana function
f(x,y), its inverse g(x,y) and the unnormalized probability density function (banana distribution)
p(x,y) are deﬁned as follows:
f(x,y) =

ax
y
a −b((ax)2 +a2)

,
g(x,y) =

x
a
ya+ab(x2 +a2)

,
p(x,y) = exp

−g(x,y)C−1g(x,y)T
2

,
where
C =
 1
0.9
0.9
1

.
With a = b = 1, we generated 20000 samples using the following VBAKF initial parameters: Q =
0.12In, ν = n2 and the initial covariance is 5In. The trends of the samples, for each coordinate, are
shown in Figure 4.3. It is observed that each chain mixes well though the x-samples mix better than
the y-samples.
Figure 4.4 shows the scatter plot, contours with 50% and 95% regions and the percentages of sam-
ples that lie inside the regions. In this type of example, we expect that the scatter plot should show

54
4. Markov Chain Monte Carlo
−6
−4
−2
0
2
4
6
−10
−8
−6
−4
−2
0
R1 = 49.8%, R2 = 95.5%
y−samples
x−samples
Figure 4.4: Scatter plot: the relationship between the x-samples and y-samples. The distribution
is a "banana shaped". Percentages in the title represents the number of samples that lie inside the
regions.
a banana-like shape. This is depicted in Figure 4.4, which implies that VBAM works. The mixing
of the chain can be analyzed by counting the number of samples that are inside the 50% and 95%
probability regions, respectively. From the title of Figure 4.4, R1 (for red contour) and R2 (for green
contour) represent the percentage of samples present in the 50% and 95% contour regions. From
the ﬁgure it can be seen that there is a right proportion of samples accepted in the regions R1 and
R2, which means that the method gives promising results.
4.3
Hybrid Monte Carlo
Most of MCMC algorithms discussed above are not suitable for strong multi-modal targets, and they
suffer from random walk behavior. To avoid random walk behavior, we can use another sampling
technique called hybrid Monte Carlo. We have used this type of MCMC in Paper I to estimate
parameters of the SDEs.
The hybrid Monte Carlo (HMC, Duane et al., 1987; Neal, 2011) is an efﬁcient gradient based
MCMC method which combine MCMC and molecular dynamics approaches. It is based on uti-
lization of the gradient information together with the plain density values. It avoids the random
walk behavior typically encountered in Metropolis-Hastings method (Beskos et al., 2013; Neal,
2011). Instead of the posterior probability density function, the HMC algorithm uses a Hamiltonian
function deﬁned as
H(x,ρ) = ϕ(x)+ 1
2ρTρ,
(4.9)

4.3 Hybrid Monte Carlo
55
where ρ is the momentum of the particle and ϕ(x) = −ln p(x | y1,...,yM) −p(x) is the negative
logarithm of the unnormalized posterior distribution. If the system evolves in ﬁctious time τ, the
particle coordinates and momenta will change according to the following Hamiltonian dynamics
equations which are approximated by Leapfrog method (Birdsall and Langdon, 2004),
dx
dτ = ∂H
∂ρ = ρ,
dρ
dτ = −∂H
∂x = −∂ϕ(x)
∂x
.
(4.10)
From Equation (4.10), we see that the HMC method requires the partial derivative of ϕ(x). Due
to this, if the derivatives do not exist then the HMC method can not be used. In Chapter V, we
propose the computation of ϕ(x) and its partial derivative by using Kalman ﬁlter algorithm (for
linear system) and extended Kalman ﬁlter (for non-linear system).
Suppose the conﬁguration at ith iteration is (xi, ρi), the next state is generated in HMC Algorithm
4.5.
Algorithm 4.5 Hybrid Monte Carlo
• Generate new momenta ρi as follows:
ρi ←α ρi +ξ
p
1−α2,
where
α ∈[0,1] and ξ ∼N(0,I).
(4.11)
• Run Leapfrog algorithm for L steps with step size ε and then let the ﬁnal conﬁguration be
(x∗, ρ∗). The choice of ε, L and α is a hard task, but the mechanisms of tuning can be found
in (Neal, 2011; Beskos et al., 2013; Chen et al., 2001; Holder et al., 2001).
• Accept (xi+1,ρi+1) = (x∗, −ρ∗) with acceptance probability
α
 (xi, ρi),(x∗, ρ∗)

= min{1,exp
 −H(x∗, ρ∗)+H(xi, ρi)

},
(4.12)
otherwise let (xi+1,ρi+1) = (xi, ρi).

56
4. Markov Chain Monte Carlo

CHAPTER V
Parameter Estimation in Stochastic Differential Equations
5.1
Stochastic Differential Equations
In Chapter II, we saw that it is possible to estimate the states of state space models (SSMs) provided
that the measurements and parameters are known. However, in many physical problems, the param-
eters are not always known. In this chapter, we describe how the parameters of the continuous-
discrete SSMs can be estimated. For the discrete SSMs, the parameter estimation is covered in
Särkkä (2013). If θ ∈Φ ⊆Rd are the parameters to be estimated, to include these parameters, the
non-linear continuous dynamic process Equation (2.7) can be written as
dx(t) = f(x(t),t,θ)dt +L(x(t),t,θ)dB(t),
(5.1)
where f : Rn×[0,∞)×Φ →Rn is a dynamic model function called drift coefﬁcient, L : Rn×[0,∞)×
Φ →Rn×s is a matrix-valued function called diffusion coefﬁcient (Kloeden and Platen, 1999; Øk-
sendal, 2003) and B(t) is a Brownian motion. As stated in Chapter II, Equation (5.1) is called Itô
stochastic differential equation (SDE). We assume that the global Lipschitz and growth conditions
are satisﬁed to ensure the existence and uniqueness of solutions of the SDEs (Karatzas and Shreve,
1991; Øksendal, 2003). However note that, the exact solutions of most SDEs are difﬁcult to obtain.
For that case, the solution of the SDE (5.1) needs to be approximated by, for example, Taylor-based
approximation methods (see Peter E. Kloeden, 1991; Kloeden and Platen, 1999; Shoji and Ozaki,
1997) or Gaussian approximation based methods (Jazwinski, 1970; Särkkä, 2006).
A Brownian motion B(t) is a process that represents the evolution of some random value or system
over time, whose value at time t is a vector of s independent Brownian motions, that is,
B(t) =

B1(t),...,Bs(t)

,
where {B j(t) : j = 1,...,s} is a one-dimensional Brownian motion. Suppose 0 ≤t1 < t2 ≤t3 < t4.
Then some of the properties of the Brownian motion are: B(0) = 0, the increments B(t2) −B(t1),
and B(t4)−B(t3) are the independent random variables while the increment B(t2)−B(t1) is a normal
random variable with zero mean and variance t2 −t1 (Karatzas and Shreve, 1991; Øksendal, 2003).
Figure 5.1 shows a few example paths of a one-dimensional standard Brownian motion generated
from the same initial point. As seen from the ﬁgure, each trajectory has its own path even if they
are generated using the same initial and parameter values.
57

58
5. Parameter Estimation in Stochastic Differential Equations
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
t
B(t)
Figure 5.1: Five trajectories of one-dimensional sampled standard Brownian motions.
A comprehensive review and evaluation of existing methods for estimating the SDEs parameters
have been surveyed in Young (1980); Shoji and Ozaki (1997); Nielsen et al. (2000); Singer (2004);
Jensen and Poulsen (2002); Jeisman (2005); Iacus (2007). In general, the estimators can be grouped
into two classes, namely likelihood based estimators and non-likelihood estimators. The difference
between the two classes is based on the use of the likelihood functions, that is the latter class does
not use likelihood function whereas the former class uses. In this chapter, only likelihood based
estimators are discussed.
If the likelihood function is known, one can use Bayesian methods such as maximum likelihood
method or Markov chain Monte Carlo (MCMC) method to get the estimates of parameters. Un-
fortunately, the likelihood function of most SDEs can not be computed in a closed form. Thus,
approximate methods are needed. The methods are grouped into non-ﬁltering likelihood based
methods and ﬁltering likelihood based methods.
5.2
Non-ﬁltering Likelihood Based Methods
Suppose that x(t0),x(t1),...,x(tM) are points of the random variable x(t) obtained from solving
Equation (5.1). If p(x(t0) | θ) is the density of the initial state and p(x(tk) | x(tk−1),θ) is the tran-
sition probability density function (TPDF) from x(tk−1) to x(tk), then the negative log–likelihood

5.2 Non-ﬁltering Likelihood Based Methods
59
function is (Dacunha-Castelle and Florens-Zmirou, 1986; Kim and Nelson, 1999)
L(θ) = −log(p0(x(t0) | θ))−
M
∑
k=1
log(p(x(tk) | x(tk−1),θ)).
(5.2)
If the states are observed and p(x(tk) | x(tk−1),θ) is known, then θ can be estimated, for instance,
by the exact maximum–likelihood estimation, which is given by minimizing the function (5.2) with
respect to the parameters (Iacus, 2007). However, most of the SDEs do not have a closed form
solution for p(x(tk) | x(tk−1),θ). Their transition density functions need to be approximated. Some
of the methods used to approximate the TPDF are as follows.
Solution of Fokker–Planck equation provides the TPDF of the SDE. The evolution of the TPDF is
described by a Fokker–Planck Equation (FPE) (Jazwinski, 1970; Øksendal, 2003). There exist SDEs
where the solutions of their corresponding FPEs can be obtained analytically. Some of the SDEs
have FPEs which are solved numerically, for instance, by ﬁnite difference methods (Poulsen, 1999;
Jensen and Poulsen, 2002) or spectral approximation (Hurn and Lindsay, 1999). Such numerical
approximations are only possible for low dimensional SDEs.
In the discrete maximum likelihood (or pseudo-likelihood) methods, the problem of unknown TPDF
is solved by discretizing the SDE using numerical schemes (Elerian, 1998; Kloeden and Platen,
1999), or by approximating the SDE with linear SDE via local linearization (Shoji and Ozaki, 1997,
1998). For example, Euler–Maruyama scheme is the simplest numerical scheme derived from Itô-
Taylor expansion, where the TPDF is approximated as a Gaussian density function (Iacus, 2007;
Sørensen, 2004, 1997; Pedersen, 1995). Such stochastic Itô–Taylor schemes, in terms of multiple
Wiener integrals, do not give the exact solutions and, also, they do not always preserve the qualita-
tive characteristics of the exact solutions (Ozaki, 1992; Biscay et al., 1996). These limitations lead to
the invention of local linearization methods (Ozaki, 1985, 1992, 1993; Biscay et al., 1996; Shoji and
Ozaki, 1997, 1998; Jiménez et al., 1999) which require the diffusion coefﬁcients to be invertible.
The linearization is done either deterministically, which leads to Ozaki scheme or stochastically,
leading to Shoji–Ozaki scheme. In both cases, the TPDFs are approximated as Gaussian (Shoji and
Ozaki, 1997, 1998).
Unlike the classical numerical schemes for simulating SDEs, which all involve some kind of dis-
cretization errors, the exact algorithm is constructed in such a way that the discretization errors
are avoided (Beskos and Roberts., 2005; Beskos et al., 2006a, 2008). The exact algorithm uses
rejection sampling on two probability measures for random paths retrospectively. It requires the
unitary diffusion coefﬁcient, hence for non-unitary diffusion, which are invertible, one needs to use
Lamperti transformation (see,e.g., Møller, 2010, for Lamperti transformation). After simulating the
SDE exactly, an unbiased TPDF is derived (Beskos et al., 2006b).
The simulated maximum likelihood method was proposed independently by Pedersen (1995); Brandt
and Santa–Clara (2002) and later Hurn et al. (2003) gave an alternative algorithm. The idea behind
this is to obtain an estimate of the TPDF by kernel estimate method, Monte Carlo-simulation based
estimate method or by importance sampling based estimate method (for review of these methods
see Jeisman, 2005; Iacus, 2007). However, the simulated maximum likelihood methods are compu-
tationally very expensive.
Aït-Sahalia (2002, 2008) constructed an explicit sequence of approximations to the TPDF using
the Hermite polynomials, which resulted in Hermite Polynomial Expansion Approaches. For the
univariate SDE, Aït-Sahalia (2002), with the use of the Hermite polynomial, proposed the approxi-
mation of the TPDF via transformations. His method has been extended to univariate models driven

60
5. Parameter Estimation in Stochastic Differential Equations
by Lévy processes (Schaumburg, 2001), univariate time inhomogeneous diffusion models (Egorov
et al., 2003), Bayesian setting (DiPietro, 2001), damped diffusion (Li, 2010), and regime-switching
univariate diffusion models for the short-term interest rate (Choi, 2010). However, the Hermite
series expansion coefﬁcients cannot be computed explicitly but could be replaced by analytical ap-
proximations in terms of the inﬁnitesimal generator.
For the multivariate SDEs, Aït-Sahalia (2008) classiﬁed the SDEs into reducible and irreducible.
For the reducible SDEs, the multivariate Hermite expansions coefﬁcients are used to compute the
TPDF. For the irreducible SDEs, Aït-Sahalia (2008) discusses how to handle the irreducibility by
postulating the form of the density expansion, and then using the Kolmogorov equations to get
the partial differential equations that are solved by Taylor expansion approximations. This method
has been applied to stochastic volatility and afﬁne term structure models (Aït-Sahalia and Kimmel,
2007, 2010; Egorov et al., 2011). Note that the computation of the expansion coefﬁcients required
by this method quickly becomes tedious for higher dimensional SDEs. Also, in the irreducible SDE
case, an additional Taylor expansion approximation is required.
The above methods do not consider measurement noise in the likelihood computation. To incorpo-
rate measurement noise in estimation process, the idea is to consider the given SDE (5.1) and in-
troduce the measurement model. For the linear SDEs, the likelihoods are computed by the Kalman
ﬁlter, while in the non-linear SDEs, the likelihoods are computed by the non-linear ﬁlters like ex-
tended Kalman ﬁlter (EKF). In the following sections, we discuss the computation of likelihoods
and their partial derivatives. For more details, and numerical examples see Papers I and II.
5.3
Evaluation of Marginal Likelihood and its Derivatives by Kalman Filter
Bayesian estimation of the unknown parameters θ in Itô linear SDEs is the process of modeling the
parameters as random variables in the following model:
dx(t) = F(t,θ)x(t)dt +L(t,θ)dB(t),
yk = Hk x(tk)+rk.
(5.3)
Here F : [0,∞)×Φ →Rn×n is the drift coefﬁcient and L : [0,∞)×Φ →Rn×s is the diffusion coef-
ﬁcient. The deﬁnitions of B(t), Hk, and rk are the same as in previous chapters. Due to the Markov
properties of the dynamics, we can compute the joint probability of measurements and states, given
parameters as
p(y1,...,yM,x0,...,xM | θ) = p(x0)
M
∏
k=1
p(yk | xk)p(xk | xk−1,θ).
(5.4)
If we integrate out the states from Equation (5.4), we get the marginal likelihood of the measurement
deﬁned as
p(y1,...,yM | θ) =
Z
···
Z
p(x0)
M
∏
k=1
p(yk | xk)p(xk | xk−1,θ)dx0 ···dxM.
(5.5)
If we know the prior distribution of parameters, θ ∼p(θ), then, using Bayes rule, we can form the
posterior distribution of the parameters as
p(θ | y1,...,yM) ∝p(y1,...,yM | θ) p(θ).
(5.6)

5.3 Evaluation of Marginal Likelihood and its Derivatives by Kalman Filter
61
The Bayesian solution to parameter estimation in the SDE models of the form (5.3) is given by
Equations (5.5) and (5.6). Different point estimate methods can be used provided (5.5) and (5.6) are
known. For instance, the maximum a posteriori (MAP) estimate maximizes (5.6) and the maximum
likelihood (ML) estimate maximizes (5.5). In this thesis, we assume that the prior distribution for
the parameters is ﬂat or is uniform within some bounds. Thus ML and MAP coincide.
We have used the Kalman ﬁlter algorithm to estimate states of the linear SSMs provided parameters
are known. Similarly, the Kalman ﬁlter can be used to compute the marginal likelihood for param-
eter estimation in (5.3) (Schweppe, 1965; Singer, 2004, and Paper I). In this situation the Kalman
ﬁlter Algorithm 2.2 remain the same but the matrices now depend on parameters, that is, here we
have, for instance, F(t,θ), m−
k (θ), P−
k (θ), Sk(θ), and Kk(θ) instead of F(t), m−
k , P−
k , Sk, and Kk
respectively. The Kalman ﬁlter computes the likelihood in terms of Gaussian distribution, where
the computation is done recursively (Jazwinski, 1970). The idea behind is that the Kalman ﬁlter is
recursively run through the data and the following distributions are obtained (See Paper I),
p(xk | y1:k−1,θ) = N(xk | m−
k (θ),P−
k (θ)),
p(xk | y1:k,θ) = N(xk | mk(θ),Pk(θ)),
p(yk | y1:k−1,θ) = N(yk | Hk m−
k (θ),Sk(θ)),
(5.7)
where m−
k (θ), mk(θ), P−
k (θ), Pk(θ), and Sk(θ) are computed from Algorithm 2.2. From the
distributions (5.7), it becomes easy to compute the marginal likelihood of the measurements given
θ, that is (cf. Singer, 2002, 2011; Särkkä, 2013, and Paper I),
p(y1,...,yM | θ) =
M
∏
k=1
p(yk | y1:k−1,θ) =
M
∏
k=1
N(yk | Hk m−
k (θ),Sk(θ)).
(5.8)
For simplicity, we denote ϕ(θ) as a negative log-marginal likelihood (sometimes called Energy
function (Särkkä, 2013)). Thus
ϕ(θ) = −log p(y1,...,yM | θ).
(5.9)
With (5.9), we can use any gradient free methods (like ML, MAP, MCMC) to estimate parameters.
However, for gradient based methods (like HMC, conjugate gradient) the derivative of (5.9), deﬁned
as ∂ϕ(θ)/∂θ, must be known. If we take the partial derivative of (5.9), the resulting derivative has
two partial derivative terms: ∂m−
k /∂θi and ∂Sk/∂θi. The two partial derivatives are derived by
taking the derivative of the Kalman ﬁlter prediction and update equations. The evaluation of partial
derivatives results in so called sensitivity equations (Gupta and Mehra, 1974). The computations of
partial derivatives of m−
k (t) and Kalman ﬁlter update equations are straightforward. However, the
computation of P−
k (t) and its partial derivatives ∂P−
k (t)/∂θi is more tedious, but can be done by
the matrix fraction decomposition if drift and diffusion coefﬁcients do not depend on time (see, e.g,
Grewal and Andrews, 2001, and Paper I). The idea of the matrix fraction decomposition is to express
P−
k (t) as a fraction decomposition of auxiliary matrices C(t,θ) and D(t,θ). The auxiliary matrices
are solutions of a two differential equations which are solved by matrix exponential function (see
Paper I). The results from the matrix fraction decomposition are shown in Algorithm 5.1, which are
taken from Paper I.
Suppose we deﬁne n−
k (t) = ∂m−
k (t)/∂θi, G(t,θ) = ∂F(t,θ)/∂θi, B−
k (t) = ∂P−
k (t)/∂θi, W(t,θ) =
∂(L(t,θ)Qc LT(t,θ))/∂θi, X(t,θ) = ∂C(t,θ)/∂θi, Y(t,θ) = ∂D(t,θ)/∂θi, and ∆tk = tk −tk−1,

62
5. Parameter Estimation in Stochastic Differential Equations
where i = 1,2,...,d. Using matrix fraction decomposition, the computation of ϕ(θ) and ∂ϕ(θ)/∂θ
can be done in algorithmic form as shown in Algorithm 5.1 (see Paper I). Note that the initial condi-
tions for ordinary differential equations are m−
k (tk−1) = mk−1, P−
k (tk−1) = Pk−1, and the prediction
result is given as m−
k ≜m−
k (tk), P−
k ≜P−
k (tk).
Algorithm 5.1 Evaluation of likelihood and its derivative by Kalman ﬁlter
i. Initialize m0, P0, ϕ0(θ) = 0, and ∂ϕ0(θ)/∂θi = 0.
ii. For each measurement yk ∈{y1,...,yM}, perform the following
• Prediction step:
m−
k (tk)
n−
k (tk)

= exp
F(θ)
0
G(θ)
F(θ)

∆tk
m−
k (tk−1)
n−
k (tk−1)

,


C−
k (tk)
D−
k (tk)
X−
k (tk)
Y−
k (tk)

= exp






F(θ)
Σ(θ)
0
0
0
−FT(θ)
0
0
G(θ)
W(θ)
F(θ)
Σ(θ)
0
−GT(θ)
0
−FT(θ)

∆tk






P−
k (tk−1)
I
B−
k (tk−1)
0

.
• Update step: evaluate the Kalman ﬁlter continuous-discrete update equation (2.14) and
the following
∂Sk
∂θi
= HkB−
k (tk)HT
k + ∂Rk
∂θi
,
∂Kk
∂θi
= B−
k (tk)HT
k S−1
k
−P−
k (tk)HT
k S−1
k
∂Sk
∂θi
S−1
k ,
nk(tk) = n−
k (tk)+ ∂Kk
∂θi
 yk −Hkm−
k

−KkHkn−
k (tk),
Bk(tk) = B−
k (tk)−∂Kk
∂θi
SkKT
k −Kk
∂Sk
∂θi
KT
k −KkSk
∂Kk
∂θi
T
.
• Evaluate the new negative log-marginal likelihood and its partial derivative.
ϕk(θ) = ϕk−1(θ)+ 1
2
M
∑
k=1
ln|2πSk|+ 1
2
M
∑
k=1
(yk −Hkm−
k )TS−1
k (yk −Hkm−
k ),
∂ϕk(θ)
∂θi
= ∂ϕk−1(θ)
∂θi
+ 1
2
M
∑
k=1

Tr

S−1
k
∂Sk
∂θi

−1
2
M
∑
k=1
(yk −Hkm−
k )TS−1
k Hkn−
k (tk)
−1
2
M
∑
k=1
 yk −Hk m−
k
T S−1
k
∂Sk
∂θi
S−1
k (yk −Hk m−
k )
−1
2
M
∑
k=1
 Hk n−
k (tk)
T S−1
k (yk −Hkm−
k ).
From Algorithm 5.1, the dependence on parameters for some terms, for example Sk, has been

5.4 Approximation of Marginal Likelihood and its Derivatives by Extended Kalman Filter 63
dropped out for notational convenience. Algorithm 5.1 gives the negative log-marginal likelihood
and its derivative for the linear SDEs. For the non-linear SDEs, whose dynamic and measurement
functions derivatives exist, the extended Kalman ﬁlter algorithm is used to compute the negative
log-marginal likelihood and its partial derivative.
5.4
Approximation of Marginal Likelihood and its Derivatives by Extended
Kalman Filter
The continuous-discrete extended Kalman ﬁlter Algorithm 2.3 has been used to approximate the
states of the non-linear SSM (2.7) where the parameters are known. If the non-linear SSM de-
pends on unknown parameters and the derivatives of the dynamic and measurement functions exist,
we can use the extended Kalman ﬁlter algorithm to approximate the marginal likelihood. If the
measurement noises are additive, then the non-linear SDEs can be stated as follows.
dx(t) = f(x(t),t,θ)dt +L(x(t),t,θ)dB(t),
yk = h(xk)+rk.
(5.10)
The extended Kalman ﬁlter is recursively run through the measurements and the following distribu-
tions are approximated as the Gaussian approximations,
p(xk | y1:k−1,θ) ≈N(xk | m−
k (θ),P−
k (θ)),
p(xk | y1:k,θ) ≈N(xk | mk(θ),Pk(θ)),
p(yk | y1:k−1,θ) ≈N(yk | h(m−
k ),Sk(θ)).
(5.11)
The marginal likelihood of the measurements is thus Gaussian deﬁned as
p(y1,...,yM | θ) =
M
∏
k=1
p(yk | y1:k−1,θ) ≈
M
∏
k=1
N(yk | h(m−
k ),Sk(θ)).
(5.12)
We again deﬁne ϕ(θ) as an approximation of negative log-marginal likelihood whose partial deriva-
tive is ∂ϕ(θ)/∂θ. The terms for ∂ϕ(θ)/∂θ are obtained by taking partial derivatives of the ex-
tended Kalman ﬁlter equations. If G(x(t),t,θ) = ∂f(x(t),t,θ)/∂θi, n−
k (t) = ∂m−
k (t)/∂θi, B−
k (t) =
∂P−
k (t)/∂θi, W(θ) = ∂(L(x(t),t,θ)Qc LT(x(t),t,θ))/∂θi, Ex(t) = ∂Fx(m−
k ,θ,t)/∂x and the ini-
tial conditions are m−
k (tk−1) = mk−1, P−
k (tk−1) = Pk−1, and the prediction results are m−
k = m−
k (tk),
P−
k = P−
k (tk), then the computation of ϕ(θ) and ∂ϕ(θ)/∂θ can be done in algorithmic form as
shown in Algorithm 5.2 (see Paper I for details).
5.5
Approximation of Marginal Likelihood by Gaussian Cubature Filters
In Chapter II, we saw that the extended Kalman ﬁlter does not work if the derivatives of dynamic
and/or measurement functions do not exist. The same limitation applies to the computation of
marginal likelihood, that is, we can not compute the marginal likelihood for non-linear SDEs us-
ing Algorithm 5.2 if the derivatives of dynamic and/or measurement functions do not exist. In that
situation, the Gaussian quadrature ﬁlters can be used instead. In estimating parameters of the SDE
(5.10) in Paper II, we used the 3rd order spherical cubature rule and Gauss–Hermite ﬁlters to ap-
proximate the marginal likelihood p(y1,...,yM | θ). The approximation of this marginal likelihood

64
5. Parameter Estimation in Stochastic Differential Equations
Algorithm 5.2 Approximation of likelihood and its derivative by extended Kalman ﬁlter
i. Initialize ϕ0(θ), m0, P0, ϕ0(θ) = 0 and ∂ϕ0(θ)/∂θi = 0.
ii. For each measurement yk ∈{y1,...,yM}, perform the following.
• Integrate the EKF continuous-discrete prediction equations (2.20) and the following
dn−
k (t)
dt
= Fx(m−
k ,θ,t)n−
k j(t)+G(m−
k ,t,θ)
dB−
k (t)
dt
= P−
k
 
n
∑
j=1
Em−
k j(t)n−
k j(t)+Eθi(t)
!T
+
 
n
∑
j=1
Em−
k j(t)n−
k j(t)+Eθi(t)
!
P−
k
+B−
k (t)FT
x (m−
k ,θ,t)+Fx(m−
k ,θ,t)B−
k (t)+W(θ).
• Evaluate the EKF continuous-discrete update equations (2.21) and the following
∂Sk
∂θi
= Hk B−
k (t)HT
k + ∂Rk
∂θi
∂Kk
∂θi
= B−
k (t)HT
x (m−
k ,t)S−1
k
+P−
k
 
n
∑
j=1
 
n−
k j(t)∂Hx(m−
k ,t)
∂m−
k j
!!T
S−1
k
−P−
k HT
x (m−
k ,t)S−1
k
∂Sk
∂θi
S−1
k
∂mk
∂θi
= ∂m−
k
∂θi
+ ∂Kk
∂θi
 yk −h(m−
k )

−Kk
∂h(xk)
∂θi
Bk(t) = B−
k (t)−∂Kk
∂θi
SkKT
k −Kk
∂Sk
∂θi
KT
k −KkSk
∂Kk
∂θi
T
.
• Compute new negative log-marginal likelihood function and its derivative.
ϕk(θ) = ϕk−1(θ)+ 1
2
M
∑
k=1
ln|2πSk|+ 1
2
M
∑
k=1
 yk −h(m−
k )
T S−1
k
 yk −h(m−
k )

∂ϕk(θ)
∂θi
= 1
2
M
∑
k=1
Tr

S−1
k
∂Sk
∂θi

−1
2
M
∑
k=1
 yk −h(m−
k )
T S−1
k
∂h(xk)
∂θi
+ ∂ϕk−1(θ)
∂θi
−1
2
M
∑
k=1
∂µk
∂θi
T
S−1
k
 yk −h(m−
k )

−1
2
M
∑
k=1
 yk −h(m−
k )
T S−1
k
∂Sk
∂θi
S−1
k
 yk −h(m−
k )

.

5.6 Particle Marginal Metropolis–Hastings Algorithm
65
Algorithm 5.3 Approximation of likelihood by Gaussian Cubature ﬁlter
i. Initialize ϕ0(θ) = 0, m0, and P0.
ii. For each measurement yk ∈{y1,...,yM}, evaluate the likelihood function as follows.
• Run Algorithm 2.4 using Equation (2.29).
• Compute negative log-marginal likelihood function as
ϕk(θ) = ϕk−1(θ)+ 1
2
M
∑
k=1
ln|2πSk|+ 1
2
M
∑
k=1
 yk −h(m−
k )
T S−1
k
 yk −h(m−
k )

.
Note that it is possible to compute the gradient of the negative marginal likelihood func-
tion (Särkkä, 2013).
is Gaussian and is deﬁned as (5.12). In algorithmic form, the approximation is recursively done as
shown in Algorithm 5.3.
The likelihood computations using Algorithms 5.1, 5.2, and 5.3 assume that the noise is Gaus-
sian. Thus, for non-Gaussian noise, the algorithms can not be used. Instead, the likelihood can be
computed using particle ﬁlter, for instance, the one discussed in Section 2.6. This is done simply
by running the particle ﬁlter algorithm and, at each time step, computing an approximation to the
marginal likelihood p(y1,...,yM | θ) as (Andrieu et al., 2010)
p(y1,...,y1 | θ) ≈p(y1 | θ)
M
∏
k=2
 
1
N
N
∑
i=1
w(i)
k
!
.
(5.13)
5.6
Particle Marginal Metropolis–Hastings Algorithm
We have seen that the MCMC method is a powerful computational tool for analysis of complex
statistical problems. However, in practice there are two major problems of MCMC: how to get
efﬁcient proposal distribution, and how to compute the marginal likelihood. These problems can
be solved by combining the MCMC and particle ﬁlter algorithms resulting to a particle Markov
chain Monte Carlo (PMCMC) algorithms. The PMCMC methods are algorithms which use parti-
cles from the particle ﬁlter algorithm to construct a proposal distribution and likelihood for MCMC.
They are used to sample from a high dimensional probability distribution where neither particle
ﬁlter method nor MCMC method can sample satisfactorily. In general, PMCMC can be thought of
as approximations to MCMC. Andrieu et al. (2010) proposed three kinds of PMCMC: particle inde-
pendent Metropolis–Hastings sampler, particle marginal Metropolis–Hastings sampler, and particle
Gibbs sampler. In this chapter, we only discuss the particle marginal Metropolis–Hastings algo-
rithm, where both parameters θ and states x1,...,xM are simultaneously estimated. We have used
this type of PMCMC in Paper II.
In particle marginal Metropolis–Hastings (PMMH) sampler, the particle ﬁlter is used to construct
the Monte Carlo estimate of likelihood, used in the acceptance probability (4.1), for marginal
Metropolis–Hastings algorithm. The new values for θ are ﬁrst sampled from the proposal density

66
5. Parameter Estimation in Stochastic Differential Equations
q(θ∗| θ) while those of x1,...,xM are sequentially sampled from p(x1,...,xM | θ∗,y1,...,yM).
If the conventional Metropolis–Hastings algorithm is used in the state space model and the prior
distribution of parameter is not ﬂat, then acceptance probability can be written as
min

1, p(θ∗,x∗
1,...,x∗
M | y1,...,yM)q(θ,x1,...,xM | x∗
1,...,x∗
M,θ∗)
p(θ,x1,...,xM | y1,...,yM)q(θ∗,x∗
1,...,x∗
M | x1,...,xM,θ)

,
(5.14)
where the proposal distribution q(θ∗,x∗
1,...,x∗
M | x1,...,xM,θ) and p(θ,x1,...,xM | y1,...,yM) can
be factored as
q(θ∗,x∗
1,...,x∗
M | x1,...,xM,θ) = q(θ∗| θ) ˆp(x∗
1,...,x∗
M | y1,...,yM,θ∗),
p(θ,x1,...,xM | y1,...,yM) = p(θ | y1,...,yM) p(x1,...,xM | y1,...,yM,θ).
(5.15)
Here ˆp(x∗
1,...,x∗
M | y1,...,yM,θ∗) is a Monte Carlo estimate of p(x∗
1,...,x∗
M | y1,...,yM,θ∗) com-
puted from particle ﬁlter algorithm. If (5.15) and (5.6) are substituted to (5.14), then
min
(
1,
ˆp(y1,...,yM | θ∗) p(θ∗)q(θi−1 | θ∗)
ˆp(y1,...,yM | θi−1)p(θi−1)q(θ∗| θi−1)
)
.
(5.16)
The particle marginal Metropolis–Hasting algorithm is shown in Algorithm 5.4. Note that each iter-
Algorithm 5.4 Particle marginal Metropolis–Hastings
i. Initialization, i = 0
• Set θ0 arbitrarily
• Run the particle ﬁlter algorithm to sample x0
1,...,x0
M ∼ˆp(x1,...,xM | y1,...,yM,θ0)
and let ˆp(y1,...,yM | θ0) be the marginal likelihood estimate.
ii. For i = 1,2,... perform the following
• Sample θ∗∼q(θ∗| θi−1)
• Run the particle ﬁlter algorithm to sample x∗
1,...,x∗
M ∼ˆp(x1,...,xM | y1,...,yM,θ∗)
and let ˆp(y1,...,yM | θ∗) be the marginal likelihood estimate.
• With probability (5.16), set
θi = θ∗,
xi
1,...,xi
M = x∗
1,...,x∗
M, and ˆp(y1,...,yM | θi) = ˆp(y1,...,yM | θ∗);
otherwise set
θi = θi−1,
xi
1,...,xi
M = xi−1
1
,...,xi−1
M
and ˆp(y1,...,yM | θi) = ˆp(y1,...,yM | θi−1)
ation of MCMC part in the PMMH algorithm requires an estimate of marginal likelihood computed
from the output of particle ﬁlter method. Consequently, the PMMH method can be computationally
extremely intensive. This is because a fairly high number of particles is required in the underlying
particle ﬁlter samplers to obtain reasonable mixing of the resulting MCMC likelihood functions,
which are stochastic (Andrieu et al., 2010). The efﬁciency of PMCMC depends on the number of
particles that are used. Doucet et al. (2012) proposes how to choose the number of particles.

CHAPTER VI
Conclusion and Discussion
The main purpose of this thesis work was to develop methods to estimate the states and parameters
of stochastic state space models, given the measurements. We saw that the estimation process
requires a posterior distribution function which is analytically intractable.
When parameters are known, for linear state space models, we have used Kalman ﬁlter to approxi-
mate the posterior distribution of a state given all measurements whereas, for non-linear state space
models, we have used non-linear Kalman ﬁlters (Gaussian ﬁlters) and particle ﬁlters. We have also
analyzed the theoretical Lp-convergence of particle ﬁlters with general importance distributions by
ﬁnding the appropriate bounds of the pth moment of the condition mean error.
We have used the Markov chain Monte Carlo (MCMC) methods to estimate the parameters of the
Itô stochastic differential equations (SDEs). Since, before this work, most of the existing likelihood
parameter estimation methods for SDEs suffered from invertability of diffusion coefﬁcients and
computational time, we have proposed the ﬁltering likelihood based method which solves these
problems. The idea of the method is to use the Kalman ﬁlter, for linear SDEs, to evaluate the
ﬁltering distributions which are used to compute the marginal likelihood. Similarly, we computed
the marginal likelihood, for non-linear SDEs, using non-linear Kalman ﬁlter (extended Kalman
ﬁlter). Also, we approximated the marginal likelihood using sigma-point methods, in particular we
used 3rd order spherical cubature and Gauss–Hermite quadrature methods. Having the marginal
likelihood functions, we were able to derive the corresponding partial derivatives which were used
in gradient based methods (hybrid Monte Carlo and scaled conjugate gradient methods).
One of the main ingredients of the MCMC methods are the proposal distributions. To cope with
proposal distribution, adaptive MCMC methods, which tune the parameters of proposal distribu-
tion automatically, were used. We have reviewed some of the adaptive MCMC methods which use
Gaussian distribution as a proposal distribution, from which the covariance matrix was automati-
cally updated. Apart from reviewing adaptive MCMC methods, we proposed a new adaptation of
covariance matrix by a variational Bayesian adaptive Kalman ﬁlter. We proved that the proposed
adaptive MCMC method is valid as the samples produced satisﬁed the strong law of large numbers
and also the empirical results agreed with other MCMC methods.
For future research, we intend to apply these proposed methods to real life problems. For instance,
we have started applying the ﬁltering methods to modelling EBOLA disease. Here the idea is to
analyze the parameters, initial population of the infected, and other compartments of the epidemic
67

68
6. Conclusion and Discussion
model. The analysis is done in both a deterministic and a stochastic context. We also expect to
form other adaptive Metropolis algorithms which utilize different kinds of ﬁlters in the proposal
adaptation. We for instance propose replacing the linear Kalman ﬁlter (in VBAM) with non-linear
Kalman ﬁlters such as the extended Kalman ﬁlter, a sigma-point (unscented) ﬁlter, or even particle
ﬁlters. The study of theoretical convergence of particle ﬁlter with respect to dimensionality of a
model is another topic for future work.

BIBLIOGRAPHY
Abramowitz, M., Stegun, I. A., 1964. Handbook of Mathematical Functions: with Formulas,
Graphs, and Mathematical Tables. U.S. Department of Commerce, National Bureau of Standards.
Aït-Sahalia, Y., 2002. Maximum Likelihood Estimation of Discretely-Sampled Diffusions: A
Closed-Form Approximation Approach. Econometrica 70 (1), 223–262.
Aït-Sahalia, Y., 2008. Closed-form likelihood expansions for multivariate diffusions. The Annals of
Statistics 36 (2), 906–937.
Aït-Sahalia, Y., Kimmel, R. L., 2007. Maximum likelihood estimation of stochastic volatility mod-
els. Journal of Financial Economics 83 (2), 413–452.
Aït-Sahalia, Y., Kimmel, R. L., 2010. Estimating afﬁne multifactor term structure models using
closed-form likelihood expansions. Journal of Financial Economics 98 (1), 113–144.
Andrieu, C., Doucet, A., Holenstein, R., 2010. Particle Markov chain Monte Carlo methods. Journal
of the Royal Statistical Society: Series B (Statistical Methodology) 72 (3), 269–342.
Andrieu, C., Thoms, J., 2008. A tutorial on adaptive MCMC. Statistics and Computing 18 (4),
343–373.
Arasaratnam, I., 2009. Cubature Kalman Filtering: Theory & Applications. Doctoral dissertation,
Department of Electrical & Computer Engineering, McMaster University.
Arasaratnam, I., Haykin, S., 2009. Cubature Kalman ﬁlters. IEEE Transactions on Automatic Con-
trol 54 (6), 1254–1269.
Arasaratnam, I., Haykin, S., Elliott, R., 2007. Discrete-time nonlinear ﬁltering algorithms using
Gauss–Hermite quadrature. In: Proceedings of the IEEE. pp. 953–977.
Barber, D., Cemgil, A. T., Chiappa, S., 2011. Bayesian Time Series Models. Cambridge University
Press.
Bernardo, J. M., Smith, A. F. M., 1994. Bayesian Theory. John Wiley & Sons.
Beskos, A., Papaspiliopoulos, O., Roberts, G. O., 2006a. Retrospective exact simulation of diffusion
sample paths with applications. Bernoulli 12 (6), 1077–1098.
Beskos, A., Papaspiliopoulos, O., Roberts, G. O., 2008. A factorisation of diffusion measure and
ﬁnite sample path constructions. Methodology and Computing in Applied Probability 10 (1),
85–104.
69

70
Bibliography
Beskos, A., Papaspiliopoulos, O., Roberts, G. O., Fearnhead, P., 2006b. Exact and computation-
ally efﬁcient likelihood-based estimation for discretely observed diffusion processes (with dis-
cussion). Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (3),
333–382.
Beskos, A., Pillai, N. S., Roberts, G. O., Sanz-Serna, J., Stuart, A. M., 2013. Optimal tuning of the
Hybrid Monte Carlo algorithm. Bernoulli 19 (5A), 1501–1534.
Beskos, A., Roberts., G. O., 2005. Exact simulation of diffusions. The Annals of Applied Probability
15 (4), 2422–2444.
Birdsall, C. K., Langdon, A. B., 2004. Plasma physics via computer simulation. CRC Press.
Biscay, R., Jiménez, J. C., Riera, J., Valdes, P. A., 1996. Local Linearization method for the numeri-
cal solution of stochastic differential equations. Annals of the Institute of Statistical Mathematics
48 (4), 631–644.
Bishop, C. M., 2006. Pattern Recognition and Machine Learning. Springer–Verlag New York.
Bornkamp, B., 2011. Approximating Probability Densities by Iterated Laplace Approximations.
Computational and Graphical Statistics.
Brandt, M. W., Santa–Clara, P., 2002. Simulated likelihood estimation of diffusions with an appli-
cation to exchange rate dynamics in incomplete markets. Journal of Financial Economics 63 (2),
161–210.
Butcher, J. C., 2003. Numerical Methods for Ordinary Differential Equations. John Wiley & Sons.
Cai, B., Meyer, R., Perron, F., 2006. Metropolis-Hastings algorithms with adaptive proposals. Statis-
tics and Computing 18 (4), 421–433.
Cappé, O., Godsill, S. J., Moulines, E., 2007. An overview of existing methods and recent advances
in sequential Monte Carlo. In: Proceedings of the IEEE. pp. 899–924.
Cappé, O., Moulines, E., Rydén, T., 2005. Inference in Hidden Markov Models. Springer.
Chen, L., Qin, Z., Liu, J. S., 2001. Exploring Hybrid Monte Carlo in Bayesian Computation. In:
Proceedings of ISBA 2000. pp. 1–5.
Chen, R., Liu, J. S., 1996. Predictive updating methods with application to Bayesian classiﬁcation.
Journal of Royal Statistical Society: Series B (Methodological) 58 (2), 397–415.
Chen, R., Liu, J. S., 2000. Mixture Kalman ﬁlters. Journal of the Royal Statistical Society: Series B
(Statistical Methodology) 62 (3), 493–508.
Choi, S., 2010. Regime-Switching Univariate Diffusion Models of the Short-Term Interest Rate.
Studies in Nonlinear Dynamics and Econometrics 13 (1), 132–157.
Chung, K. L., 2001. A Course in Probability Theory, 3rd Edition. Academic Press.
Cools, R., 1997. Constructing cubature formulae: the science behind the art. Acta Numerica 6 (4),
1–54.

71
Crisan, D., Doucet, A., 2000. Convergence of sequential Monte Carlo methods. Tech. Rep.
CUEDIF-INFENGrrR38, Signal Processing Group, Department of Engineering, University of
Cambridge.
Crisan, D., Doucet, A., 2002. A survey of convergence results on particle ﬁltering methods for
practitioners. IEEE Transactions Signal Processing 50 (3), 736–746.
Crisan, D., Míquez, J., 2014. Particle-kernel estimation of the ﬁlter density in state-space models.
Bernoulli 20 (4), 1879–1929.
Dacunha-Castelle, D., Florens-Zmirou, D., 1986. Estimation of the coefﬁcients of a diffusion from
discrete observations. Stochastics: An International Journal of Probability and Stochastic Pro-
cesses 19 (4), 263–284.
Del Moral, P., 2004. Feynman–Kac Formulae: Genealogical and Interacting Particle Systems with
Applications. Springer.
Del Moral, P., Guionnet, A., 2001. On the stability of interacting processes with applications to
ﬁltering and genetic algorithms. Annales de l’Institut Henri Poincare (B) Probability and Statistics
37 (2), 155–194.
Dimov, I. T., 2008. Monte Carlo Methods for Applied Scientists. World Scientiﬁc Publishing Co.
Pte. Ltd.
DiPietro, M., 2001. Bayesian Inference for Discretely Sampled Diffusion Processes with Financial
Applications. Doctoral dissertation, Carnegie-Mellon University.
Douc, R., Moulines, E., 2008. Limit theorems for weighted samples with applications to sequential
Monte Carlo methods. Annals Statistics 36 (5), 2344–2376.
Douc, R., Moulines, E., Olsson, J., 2009. Optimality of the auxiliary particle ﬁlter. Probability and
Mathematical Statistics 29 (1), 1–28.
Doucet, A., Freitas, N. D., Gordon, N., 2001. Sequential Monte Carlo Methods in Practice. Springer.
Doucet, A., Godsill, S. J., Andrieu, C., 2000. On Sequential Monte Carlo Sampling Methods for
Bayesian Filtering. Statistics and Computing 10 (3), 197–208.
Doucet, A., Pitt, M., Kohn, R., 2012. Efﬁcient implementation of Markov chain Monte Carlo when
using an unbiased likelihood estimator. arXiv preprint arXiv:1210.1871.
Duane, S., Kennedy, A. D., Pendleton, B. J., Roweth, D., September 1987. Hybrid Monte Carlo.
Physics Letters B 195, 216–222.
Egorov, A. V., Li, H., Ng, D., 2011. A tale of two yield curves: Modeling the joint term structure of
dollar and euro interest rates. Journal of Econometrics 162 (1), 55–70.
Egorov, A. V., Li, H., Xu, Y., 2003. Maximum likelihood estimation of time-inhomogeneous diffu-
sions. Journal of Econometrics 114 (1), 107–139.
Einstein, A., 1905. Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung
von in ruhenden Flüssigkeiten suspendierten Teilchen. Annalen der Physik 322 (8), 549–560.

72
Bibliography
Elerian, O., 1998. A Note on the Existence of a Closed Form Conditional Transition Density for the
Milstein Scheme. Working paper, Nufﬁeld College, Oxford University.
Evensen, G., 2003. The Ensemble Kalman Filter: theoretical formulation and practicle implemen-
tation. Ocean Dynamics 53, 343–367, dOI: 10.1007/s10236-003-0036-9.
Evensen, G., 2009. Data Assimilation: The Ensemble Kalman Filter. Jonh Wiley and Sons.
Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B., 2004. Bayesian Data Analysis, 2nd Edition.
CRC/Chapman and Hall.
Gelman, A., Roberts, G. O., Gilks, W. R., 1996. Efﬁcient Metropolis jumping rules. Bayesian Statis-
tics 5 5, 599–607.
Gilks, W. R., Richardson, S., Spiegelhalter, D. J., 1996. Markov chain Monte Carlo in Practice.
Chapman and Hall/CRC.
Gilks, W. R., Roberts, G. O., Sahu, S. K., 1998. Adaptive Markov chain Monte Carlo through
regeneration. Journal of the American Statistical Association 93 (443), 1045–1054.
Golub, G. H., Welsch, J. H., 1969. Calculation of Gauss quadrature rules. Mathematics of Compu-
tation 23 (106), 221–230.
Gordon, N. J., Salmond, D. J., Smith, A. F. M., 1993. Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. Statistics and Computing 140 (2), 107–113.
Grewal, M. S., Andrews, A. P., 2001. Kalman Filtering, Theory and Practice Using MATLAB, 2nd
Edition. Wiley–IEEE Press.
Guo, D., Wang, X., Chen, R., 2005. New sequential Monte Carlo methods for nonlinear dynamic
systems. Statistics and Computing 15 (2), 135–147.
Gupta, N. K., Mehra, R. K., 1974. Computational aspects of maximum likelihood estimation and
reduction in sensitivity function calculations. Automatic Control, IEEE Transactions on 19 (6),
774–783.
Haario, H., Laine, M., Mira, A., Saksman, E., 2006. DRAM: Efﬁcient adaptive MCMC. Statistics
and Computing 16 (4), 339–354.
Haario, H., Saksman, E., Tamminen, J., 1999. Adaptive proposal distribution for random walk
Metropolis algorithm. Computational Statistics 14 (3), 375–395.
Haario, H., Saksman, E., Tamminen, J., 2001. An adaptive Metropolis algorithm. Bernoulli 7 (2),
223–242.
Haario, H., Saksman, E., Tamminen, J., 2005. Componentwise adaptation for high dimensional
MCMC. Computational Statistics 20 (2), 265–273.
Handschin, J. E., 1970. Monte Carlo techniques for prediction and ﬁltering of non-linear stochastic
processes. Automatica 6 (4), 555–563.

73
Handschin, J. E., Mayne, D., 1969. Monte carlo techniques to estimate the conditional expectation
in multi-stage non-linear ﬁltering. International Journal of Control 9 (5), 547–559.
Hangos, K. M., Bokor, J., Szederkényi, G., 2004. Analysis and control of nonlinear process systems.
Springer.
Härdle, W., Kerkyacharian, G., Tsybakov, A., Picard, D., 1998. Wavelets, approximation, and sta-
tistical applications. Springer.
Hastings, W. K., 1970. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika 57 (1), 97–109.
Hermann, R., Krener, A. J., 1977. Nonlinear controllability and observability. Automatic Control,
IEEE Transactions on 22 (5), 728–740.
Hol, J. D., Schön, T. B., Gustafsson, F., 2006. On Resampling Algorithms for Particle Filters. In:
Nonlinear Statistical Signal Processing Workshop, 2006 IEEE. pp. 79–82.
Holden, L., Hauge, R., Holden, M., 2009. Adaptive independent Metropolis–Hastings. Annals of
Applied Probability 19 (1), 395–413.
Holder, T., Leimkuhler, B., Reich, S., 2001. Explicit variable step-size and time-reversible integra-
tion. Applied Numerical Mathematics 39 (3-4), 367–377.
Hu, X., Schön, T. B., Ljung, L., 2008. A basic convergence result for particle ﬁltering. IEEE Trans-
actions on Signal Processing 56 (4), 1337–1348.
Hu, X., Schön, T. B., Ljung, L., 2011. Correspondence: A basic convergence result for particle
ﬁltering. IEEE Transactions on Signal Processing 59 (7).
Hurn, A. S., Lindsay, K. A., 1999. Estimating the Parameters of Stochastic Differential Equations.
Mathematics and Computers in Simulation 48, 373–384.
Hurn, A. S., Lindsay, K. A., Martin, V. L., 2003. On the efﬁcacy of simulated maximum likelihood
for estimating the parameters of stochastic differential Equations. Time Series Analysis 24, 45–
63.
Hürzeler, M., Künsch, H. R., 1998. Monte Carlo Approximations for General State-Space Models.
Journal of Computational and Graphical Statistics 7 (2), 175–193.
Iacus, S. M., 2007. Simulation and Inference for Stochastic Differential Equations. Springer.
Ito, K., Xiong, K., 2000. Gaussian Filters for Nonlinear Filtering Problems. IEEE Transactions on
Automatic Control 45 (5), 910–927.
Jazwinski, A. H., 1970. Stochastic Processes and Filtering Theory. Academic Press.
Jeisman, J., 2005. Estimation of the Parameters of Stochastic Differential Equations. Doctoral dis-
sertation, Queensland University of Technology.
Jensen, B., Poulsen, R., 2002. Transition Densities of Diffusion Processes: Numerical Comparison
of Approximation Techniques. Journal of Derivatives 9, 18–32.

74
Bibliography
Jia, B., Xin, M., Cheng, Y., 2011. Sparse Gauss–Hermite Quadrature Filter with Application to
Spacecraft Attitude Estimation. Journal of Guidance, Control and Dynamics 34 (2), 367–379.
Jiménez, J. C., Shoji, I., Ozaki, T., 1999. Simulation of Stochastic Differential Equations Through
the Local Linearization Method. A Comparative Study. Journal of Statistical Physics 94 (3/4),
587–602.
Julier, S. J., Uhlmann, J. K., March 2004. Unscented Filtering and Nonlinear Estimation. Proceed-
ings of the IEEE 92 (3), 401–422.
Kailath, T., Sayed, A. H., Hassibi, B., 2000. Linear Estimation. Prentice Hall.
Kalman, R. E., 1959. On the general theory of control systems. Automatic Control, IRE Transac-
tions on 4 (3), 481–491.
Kalman, R. E., 1960a. Contributions to the theory of optimal control. Boletin de la Sociedad Matem-
atica Mexicana 5 (2), 102–119.
Kalman, R. E., 1960b. A New Approach to Linear Filtering and Prediction Problems. Transactions
of the ASME–Journal of Basic Engineering 82, 35–45.
Kantas, N., Doucet, A., Singh, S. S., Maciejowski, J. M., 2009. An overview of sequential Monte
Carlo methods for parameter estimation in general state-space models. In: 15th IFAC Symposium
on System Identiﬁcation. Vol. 15. pp. 774–785.
Karatzas, I., Shreve, S. E., 1991. Brownian Motion and Stochastic Calculus. Springer.
Keith, J. M., Kroese, D. P., Sofronov, G. Y., 2008. Adaptive independence samplers. Statistics and
Computing 18 (4), 409–420.
Kim, C., Nelson, C. R., 1999. State-space models with regime switching: classical and Gibbs-
sampling approaches with applications. Vol. 1. MIT Press.
Kitagawa, G., 1996. Monte Carlo Filter and Smoother for Non-Gaussian Nonlinear State Space
Models. Journal of Computational and Graphical Statistics 5 (1), 1–25.
Kloeden, P. E., Platen, E., 1999. Numerical Solution to Stochastic Differential Equations. Springer.
Kong, A., Liu, J. S., Wong, W. H., 1994. Sequential Imputations and Bayesian Missing Data Prob-
lems. Journal of the American Statistical Association 89 (425), 278–288.
Koop, G., Poirier, D. J., Tobias, J. L., 2007. Bayesian Econometric Methods. Cambridge University
Press.
Künsch, H. R., 2005. Recursive Monte Carlo ﬁlters: Algorithms and theoretical analysis. Annals of
Statistics 33 (5), 1983–2021.
Laine, M., 2008. Adaptive MCMC methods with applications in environmental and geophysical
models. Doctoral dissertation, Lappeenranta University of Technology.
Lemieux, C., 2009. Monte Carlo and Quasi–Monte Carlo Sampling. Springer Science+Business
Media New York.

75
Li, M., 2010. A damped diffusion framework for ﬁnancial modeling and closed-form maximum
likelihood estimation. Journal of Economic Dynamics and Control 34 (2), 132–157.
Liang, F., Liu, C., Carroll, R. J., 2010. Advanced Markov chain Monte Carlo Methods: Learning
from Past Samples. John Wiley & Sons, Ltd, Chichester, UK.
Liang, F., Wong, W. H., 2000. Evolutionary Monte Carlo: application to Cp model sampling and
change point problem. Statistica Sinica 10, 317–342.
Liang, F., Wong, W. H., 2001. Real parameter evolutionary Monte Carlo with applications in
Bayesian mixture models. Journal of the American Statistical Association 96, 653–666.
Liu, J. S., 2002. Monte Carlo Methods Strategies in Scientiﬁc Computing. Springer–Verlag New
York.
Maybeck, P. S., 1982. Stochastic Models, Estimating, and Control. Vol. 2. Academic Press.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., Teller, E., 1953. Equation of
State Calculations by Fast Computing Machines. Chemical Physics 21 (6), 1087–1092.
Møller, J. K., 2010. Stochastic State Space Modelling of Nonlinear systems - With application to
Marine Ecosystems. Doctoral dissertation, DTechnical University of Denmark Informatics and
Mathematical Modelling.
Neal, R. M., 2011. MCMC using Hamiltonian dynamics. In: Brooks, S., Gelman, A., Jones, G.,
Meng, X.-L. (Eds.), Handbook of Markov Chain Monte Carlo. Chapman & Hall / CRC Press, pp.
113–162.
Nielsen, J. N., Madsen, H., Young, P. C., 2000. Parameter estimation in stochastic differential equa-
tions: An overview. SIAM Journal on Numerical Analysis 24, 83–94.
Øksendal, B., 2003. Stochastic Differential Equations: An Introduction with Applications, 6th Edi-
tion. Springer.
Ozaki, T., 1985. Non-linear time series models and dynamical systems. In: Hannan, E. J., Krish-
naiah, P. R., Rao, M. M. (Eds.), Handbook of Statistics. Vol. 5. Elsevier, Amsterdam, Ch. 2, pp.
25–83.
Ozaki, T., 1992. A bridge between nonlinear time series models and nonlinear stochastic dynamical
systems: a local linearization approach. Statistica Sinica 2, 113–135.
Ozaki, T., 1993. A local linearization approach to nonlinear ﬁltering. International Journal of Con-
trol 57 (1), 75–96.
Pedersen, A. R., 1995. A new approach to maximum likelihood estimation for stochastic differential
equations based on discrete observations. Scandinavian Journal of Statistics 22 (1), 55–71.
Peter E. Kloeden, Eckhard Platen, H. S., 1991. Numerical solution of SDE through computer ex-
periments. Springer.
Petris, G., Petrone, S., Campagnoli, P., 2009. Dynamic linear models with R. Springer.

76
Bibliography
Pitt, M. K., Shephard, N., 1999. Filtering via Simulation: Auxiliary Particle Filters. Journal of the
American Statistical Association, 94 (446), 590–599.
Poulsen, R., 1999. Approximate Maximum Likelihood Estimation of Discretely Observed Diffusion
Processes. Working Paper 29, University of Aarhus.
Ristic, B., Arulampalam, S., Gordon, N., 2004. Beyond the Kalman Filter: Particle Filters for
Tracking Applications. Artech House Publishers.
Robbins, H., Monro, S., 1951. A Stochastic Approximation Method. The Annals of Mathematical
Statistics 22 (3), 400–407.
Robert, C. P., Casella, G., 2004. Monte Carlo Statistical Methods. Springer texts in statistics.
Roberts, G. O., Gelman, A., Gilks, W. R., 1997. Weak convergence and optimal scaling of random
walk Metropolis algorithms. Annals of Applied Probability 7 (1), 110–120.
Roberts, G. O., Rosenthal, J. S., 2001. Optimal scaling for various Metropolis–Hastings algorithms.
Statistical Science 16 (4), 351–367.
Rosenthal, J. S., 2011. Optimal Proposal Distributions and Adaptive MCMC. In: Brooks, S., Gel-
man, A., Jones, G., Meng, X.-L. (Eds.), Handbook of Markov Chain Monte Carlo. Chapman &
Hall / CRC Press, pp. 93–111.
Ross, S. M., 2010. Introduction to probability models, 10th Edition. London: Academic Press.
Särkkä, S., 2006. Recursive Bayesian Inference on Stochastic Differential Equations. Doctoral dis-
sertation, Helsinki University of Technology.
Särkkä, S., 2013. Bayesian Filtering and Smoothing. Cambridge University Press.
Särkkä, S., Hartikainen, J., 2013. Non-linear noise adaptive Kalman ﬁltering via variational Bayes.
In: Proceedings of MLSP 2013. pp. 1–6.
Särkkä, S., Nummenmaa, A., 2009. Recursive Noise Adaptive Kalman Filtering by Variational
Bayesian Approximations. IEEE Transactions on Automatic Control 54 (3), 596–600.
Schaumburg, E., 2001. Maximum Likelihood Estimation of Jump Processes with Applications to
Finance. Doctoral dissertation, Princeton University.
Schweppe, F. C., 1965. Evaluation of likelihood functions for Gaussian signals. Information Theory,
IEEE Transactions on 11 (1), 61–70.
Shoji, I., Ozaki, T., 1997. Comparative study of estimation methods for continuous time stochastic
processes. Journal of Time Series Analysis 18 (5), 485–506.
Shoji, I., Ozaki, T., 1998. Estimation for nonlinear stochastic differential equations by a local lin-
earization method. Stochastic Analysis and Applications 16, 733–752.
Singer, H., December 2002. Parameter Estimation of Nonlinear Stochastic Differential Equations:
Simulated Maximum Likelihood versus Extended Kalman ﬁlter and Itô-Taylor expansion. Jour-
nal of Computational and Graphical Statistics 11, 972–995.

77
Singer, H., 2004. A survey of estimation methods for stochastic differential equastions. In: Proceed-
ings of 6th International Conference on Social Science Methodology, Amsterdam. pp. 1–12.
Singer, H., 2011. Continuous-discrete state-space modeling of panel data with nonlinear ﬁlter algo-
rithms. Advances in Statistical Analysis 95 (4), 375–413.
Šmídl, V., Quinn, A., 2006. The variational Bayes method in signal processing. Springer.
Sørensen, H., 2004. Parametric inference for diffusion processes observed at discrete points in time:
a survey. International Statistical Review 72 (3), 337–354.
Sørensen, M., 1997. Estimating functions for discretely observed diffusions: a review. In: Selected
Proceedings of the Symposium on Estimating Functions. Vol. 32. pp. 305–325.
Tanizakia, H., Mariano, R. S., 1998. Nonlinear and non-Gaussian state-space modeling with Monte
Carlo simulations. Journal of Econometrics 83 (1–2), 263–290.
Ter Braak, C. J. F., 2006. A Markov chain Monte Carlo version of the genetic algorithm differential
evolution: easy Bayesian computing for real parameter spaces. Statistics and Computing 16 (3),
239–249.
Van der Merwe, R., Doucet, A., de Freitas, N., Wan, E., 2000. The Unscented Particle Filter. Work-
ing paper, Cambridge University Engineering Department.
Vihola, M., 2012. Robust adaptive Metropolis algorithm with coerced acceptance rate. Statistics and
Computing 22 (5), 997–1008.
von Neumann, J., 1951. Various techniques used in connection with random digits. National Bureau
of Standards Applied Mathematics Series 12, 36–38.
Wu, Y., Hu, D., Wu, M., Hu, X., 2006. A Numerical-Integration Perspective on Gaussian Filters.
IEEE Transactions on Signal Processing 54 (8), 2910–2921.
Young, P., January 1980. Parameter estimation for continuous-time models: a survey. Automatica
17, 23–39.


PART II: PUBLICATIONS


PUBL. I
Isambi S. Mbalawata, Simo Särkkä and Heikki Haario, Parameter Estimation in Stochastic Dif-
ferential Equations with Markov Chain Monte Carlo and Non-Linear Kalman Filtering, computa-
tional statistics, 28(3), 1195–1223, 2013.
c⃝2013 computational statistics. All rights reserved.
Reprinted, with the permission of computational statistics


Comput Stat
DOI 10.1007/s00180-012-0352-y
ORIGINAL PAPER
Parameter estimation in stochastic differential
equations with Markov chain Monte Carlo
and non-linear Kalman ﬁltering
Isambi S. Mbalawata · Simo Särkkä ·
Heikki Haario
Received: 19 September 2011 / Accepted: 13 July 2012
© Springer-Verlag 2012
Abstract
This paper is concerned with parameter estimation in linear and non-linear
Itô type stochastic differential equations using Markov chain Monte Carlo (MCMC)
methods. The MCMC methods studied in this paper are the Metropolis–Hastings and
Hamiltonian Monte Carlo (HMC) algorithms. In these kind of models, the computation
of the energy function gradient needed by HMC and gradient based optimization
methods is non-trivial, and here we show how the gradient can be computed with
a linear or non-linear Kalman ﬁlter-like recursion. We shall also show how in the
linear case the differential equations in the gradient recursion equations can be solved
using the matrix fraction decomposition. Numerical results for simulated examples
are presented and discussed in detail.
Keywords
Hamiltonian Monte Carlo · Stochastic differential equation ·
Parameter estimation · Markov chain Monte Carlo · Kalman ﬁlter ·
Matrix fraction decomposition
I. S. Mbalawata (B)· H. Haario
Department of Mathematics and Physics, Lappeenranta University of Technology,
P.O.Box 20, 53851 Lappeenranta, Finland
e-mail: isambi.mbalawata@lut.ﬁ
H. Haario
e-mail: heikki.haario@lut.ﬁ
S. Särkkä
Department of Biomedical Engineering and Computational Science, Aalto University,
P.O.Box 12200, 00076 Aalto, Finland
e-mail: simo.sarkka@aalto.ﬁ
123

I. S. Mbalawata et al.
1 Introduction
Stochastic differential equations (SDEs, Karatzas and Shreve 1991; Øksendal 2003),
are non-deterministic equations which can be used for modeling uncertain dynamic
phenomena in various applications such as navigation, telecommunications, control
engineering, biology, ﬁnance, and natural sciences. In these applications, the dynam-
ical system or the physical phenomenon can often be modeled as an SDE which is
observed at discrete instants of time either directly or with certain physical sensors.
The sensors may not measure the whole state of the SDE directly and the measure-
ments can also contain errors, which needs to be taken into account in modeling. From
the modeling point of view SDEs can be considered as extensions of ordinary differ-
ential equations, where the driving force is a non-deterministic stochastic process.
The mathematical theory of SDEs is called Itô calculus or stochastic calculus (see,
e.g., Karatzas and Shreve 1991; Øksendal 2003) which is an extension of the classical
differential and integral calculus to stochastic processes.
Although often it is easy to form a stochastic differential equation which mod-
els the dynamic phenomenon in hand, in practice, the parameters of the SDE model
remain uncertain or completely unknown. The parameters may be, for example, phys-
ical constants related to the environment or densities of ﬂuctuations (noise variances,
volatilities) in the phenomenon. Therefore parameter estimation in SDE models is a
crucial area in many applications where SDEs are used.
The objective of this paper is to estimate the parameters of linear and non-linear
SDEs. The linear SDEs have the following form:
dx(t) = F(t, θ) x(t) dt + L(t, θ) dB(t),
(1)
where x(t) ∈Rn is the state of the SDE, θ ∈Φ ⊆Rd is the vector of parameters to
be estimated, F : [0, ∞) × Φ →Rn×n and L : [0, ∞) × Φ →Rn×s are matrix
valued functions, and t →B(t) is s-dimension Brownian motion with diffusion
matrix Qc ∈Rs×s. In most cases (especially in physical models) the matrix L(t, θ)
has less columns than rows, which implies that the SDE is irreducible.
The measurement yk ∈Rm at time tk for k = 1, . . . , M is
yk = Hk x(tk) + rk,
(2)
where x(tk) is the state at time tk, rk ∼N(0, Rk) is the Gaussian measurement noise
with Rk ∈Rm×m being the covariance matrix of the measurement error at tk, and Hk
is the measurement model matrix. At time t0 the state is assumed to have the prior
distribution p(x(t0)) = N(x(t0) | m0, P0).
For non-linear SDEs we consider the following:
dx(t) = f(x(t), t, θ) dt + L(x(t), t, θ) dB(t)
(3)
yk = h(x(tk)) + rk,
(4)
where f : Rn × [0, ∞) × Φ →Rn is the dynamic model function, L : Rn × [0, ∞) ×
Φ →Rn×s is a matrix valued function, and h : Rn →Rm is the measurement model
123

Parameter estimation in SDEs with MCMC and non-linear KF
function. Again, most of the models that we consider are irreducible models, that is,
have non-square L.
The key quantity for the computation of the likelihood function to the models of
the above form is the transition probability density p(x(tk) | x(tk−1), θ) which is the
solution of the Kolmogorov forward equation or Fokker–Planck partial differential
equation (see, e.g., Jazwinski 1970; Øksendal 2003). For example, the Kolmogorov
forward equation corresponding to the non-linear SDE (3) is the following:
∂p(x, t, θ)
∂t
= −

i
∂
∂xi
( fi(x, t, θ) p(x, t, θ))
+1
2

i j
∂2
∂xi ∂x j

L(x, t, θ)Qc(t)LT (x, t, θ)

i j p(x, t, θ)

, (5)
where the boundary condition is given as p(x, tk−1, θ) = δ(x−x(tk−1)), where δ(·) is
the Dirac delta distribution. The transition density is then given as p(x(tk) | x(tk−1), θ)
≜
p(x, tk, θ). If we deﬁne the measurement model density as p(yk | xk)
=
N(yk | h(xk), rk), where xk ≜x(tk), then due to the Markov properties of the dynam-
ics the joint distribution of all the measurements, the states at the measurement times,
and the initial state, given the parameters can be written as
p(y1, . . . , yM, x0, . . . , xM | θ) = p(x0)
M

k=1
p(yk | xk)p(xk | xk−1, θ).
(6)
Integrating out the states gives the marginal likelihood of the measurement:
p(y1, . . . , yM | θ) =

· · ·

p(x0)
M

k=1
p(yk | xk)p(xk | xk−1, θ)dx0 · · · dxM. (7)
Using the Bayes rule we can then form the posterior distribution of the parameters:
p(θ | y1, . . . , yM) =
p(y1, . . . , yM | θ) p(θ)
	
p(y1, . . . , yM | θ) p(θ) dθ
.
(8)
The Bayesian solution to parameter estimation in SDE models of the form (1) and (3)
is, in principle, given by the Eqs. (5), (7), and (8). However, none of these equations
can be evaluated in closed form except for a few special cases; the transition density
is not known, because the forward Kolmogorov Equation (5) does not have a closed
form solution, and the integrals over the states in Eq. (7) are intractable, as well as
the numerator integral in Eq. (8). In practical computations, the intractable integral
in the numerator is less of a problem, because it is not needed in the most Bayesian
computational methods. In the case of linear Gaussian models the ﬁrst two of the
equationscanbesolvedandtheKalmanﬁlterprovidescomputationallyefﬁcientmeans
for evaluating the likelihood. In the case of non-linear models we need to approximate
123

I. S. Mbalawata et al.
the integrals over the states as well as the transition densities, and forming these
approximations is a central problem in parameter estimation in non-linear SDEs.
Various methods for coping with these problems and for estimating the parameters
of linear and non-linear SDEs have been studied in literature. However, most of the
methods have originally been developed for scalar models and the extension to mul-
tidimensional models—which is our main concern here—has been done separately.
Due to this, the methods often use the Lamperti transform, which causes a problem
in multidimensional case, because it requires the SDE to be reducible. Unfortunately,
physics based SDEs are almost always non-reducible, which makes the usage of these
methods challenging. These methods also rarely consider the case of noisy or partial
observation of the state. In the case of physical models, it is quite rare to observe the
full state of the system and there always is some noise in the measurements as well.
Furthermore, the sensor readings often are non-linear transformations of the state,
which cannot be handled with these methods.
In discrete maximum likelihood (or pseudo-likelihood) methods the problem of
unknown transition density is solved by discretizing the SDE using some numerical
method such as in (Elerian 1998; Kloeden and Platen 1999; Shoji and Ozaki 1998)
and the discrete likelihood is used in the place of the actual likelihood. Non-linear
Kalman ﬁltering based approximations to the likelihood (Singer 2002, 2011) use
the extended Kalman ﬁlter (EKF) and other types of Gaussian approximations for
approximating the transition densities and the integrals. Local linearization (Shoji and
Ozaki 1998) can be seen as a simple approximation to the EKF approach, where the
linearization is done only once in the beginning of each discretization interval (Singer
2002). According to Singer (2002) the performance of EKF and local linearization
is very similar and the usage of other non-linear Kalman ﬁlters instead of EKF was
proposed by Singer (2011). The advantage of the non-linear Kalman ﬁltering approach
over the local linearization and SDE discretization based approaches is that it can
easily cope with partial state measurements, measurement noise and non-linearities
in the measurement model. Although Singer (2002, 2011) only considered the usage
of the likelihood approximation in maximum likelihood estimation of parameters, the
likelihood approximations can also be used in maximum a posterior estimates and in
Bayesian MCMC computations. This is the route that we take in this article.
In simulated maximum likelihood methods the idea is to simulate trajectories from
the transition density of the SDE (Pedersen 1995; Brandt and Santa-Clara 2002; Hurn
et al. 2003), which typically only works with fully observed state and no measurement
noise. Markov chain Monte Carlo (MCMC) methods (Jones 1998; Elerian et al. 2001;
Eraker 2001; Golightly and Wilkinson 2006, 2008; Stramer et al. 2010; Stramer and
Bognar 2011) draw samples from the joint Bayesian posterior distribution of states
and parameters or calculate maximum likelihood or Bayesian estimates using Exact
Algorithm based Monte Carlo sampling (Beskos et al. 2006, 2009). These methods
are also typically suitable only for reducible models, but some methods for irreducible
models exist as well (Stramer et al. 2010; Stramer and Bognar 2011). Furthermore,
sampling the state means Monte Carlo sampling in a very high-dimensional space.
When the likelihood is computed with the non-linear Kalman ﬁltering approach of
Singer (2002, 2011), the method can be seen as an approximately Rao-Blackwellized
(Berger 1985; Doucet et al. 2000) version of a full MCMC sampling of the state and
123

Parameter estimation in SDEs with MCMC and non-linear KF
the parameters. The approximate Rao-Blackwellization approach has also turned out
to be useful in target tracking applications (Särkkä et al. 2007). Thus the MCMC
algorithms developed in this article can be seen as Rao-Blackwellized versions of the
full MCMC approaches. Furthermore, this way we can also consider the usage of the
HMC algorithm, which would not be possible without the Rao-Blackwellization.
Partial differential equation (PDE) based methods approximate solutions to the
Fokker–Planck equations (Hurn and Lindsay 1999; Jensen and Poulsen 2002; Jeis-
man 2005), for example, by using Hermite expansions (Aït-Sahalia 2002, 2008) or
other PDE methods in (Jensen and Poulsen 2002). Other possible approaches are, for
example, the generalized method of moments, indirect estimation, characteristic func-
tions, estimating functions, matching to marginal density (see Young 1980; Nielsen
et al. 2000; Singer 2004; Sørensen 2004; Jeisman 2005). These methods are different
in the sense that they are not based on approximating the likelihood at all. One more
general approach is to simply discretize the SDE using methods in Kloeden and Platen
(1999), and then use discrete-time parameter estimation methods (see, e.g., Cappé et
al. 2005; Ninness and Henriksen 2010; Andrieu et al. 2010).
In this article, we develop a new parameter estimation method for SDEs which
is based on the powerful MCMC method called Hamiltonian Monte Carlo (HMC,
Duane et al. 1987; Neal 2011). We use the approach of Singer (2002, 2011) and
utilize (extended) Kalman ﬁlters for evaluating the (approximate) marginal likelihood
of the parameters. We show how derivatives of the energy function (negative log-
likelihood) can be computed in a computationally efﬁcient manner using a Kalman
ﬁlter kind of recursion. This enables the use of HMC, as well as various gradient based
optimization methods. In this paper, we also develop parameter estimation methods for
SDEs based on the Metropolis–Hastings (MH) method and we compare the MCMC
results to maximum a posteriori (MAP) estimates computed with the scaled conjugate
gradient method (see, e.g., Bishop 1996).
2 Bayesian parameter estimation in statistical models
2.1 Maximum a posteriori
The maximum a posteriori (MAP) estimate (see, e.g., Gelman et al. 2004) of the
parameter θ is given by the minimum of the (unnormalized) negative log-posterior
(energy function) of the observed data:
ˆθ = arg min
θ ϕ(θ),
(9)
where the energy function is deﬁned as
ϕ(θ) = −ln p(y1, . . . , yM | θ) −ln p(θ).
(10)
The moments can be then approximated with point-wise evaluations as follows:
E[g(θ) | y1, . . . , yM] ≈g(ˆθ).
(11)
123

I. S. Mbalawata et al.
In this article, we use a gradient based optimization method called scaled conjugate
gradient (SCG) method (see, e.g., Bishop 1996) for ﬁnding the minimum of the energy
function. But, in principle, any optimization method could be used.
2.2 Markov chain Monte Carlo
Markov chain Monte Carlo (MCMC) methods (Gilks et al. 1996; Gelman et al. 2004)
are numerical methods for computing multidimensional integrals of the above form by
using Monte Carlo. The idea is to draw samples θ(1), θ(2), . . . , θ(N) from the posterior
distribution p(θ | y1, . . . , yM) and approximate the expectation as the sample average
E[g(θ) | y1, . . . , yM] ≈1
N
N

i=1
g(θ(i)).
(12)
One difﬁculty in drawing samples from the posterior distribution is that even for the
evaluation of the posterior probability density, we would need to be able to evaluate
the normalization constant integral in (8). MCMC methods are a class of Monte Carlo
methods which can draw the samples without the knowledge of the normalization
constant. These methods are based on simulating a multidimensional Markov chain
which has been constructed such that it has the posterior distribution as its stationary
distribution. In the simulation of the Markov chain we only need to evaluate the ratios
of posterior probability densities and thus the evaluation of the normalization constant
is not required.
The most well-known MCMC methods are the Metropolis, Metropolis–Hastings
and Gibbs sampler algorithms (see, e.g., Gilks et al. 1996; Gelman et al. 2004). In this
article, we are going to use the Metropolis–Hastings algorithms which require the use
of a good proposal distribution which can be hard to ﬁnd. The Metropolis–Hastings
(MH) algorithm works by sampling a candidate point θ∗from a proposal distribution
q(θ∗| θ) and then accepting the proposal with a suitable acceptance probability (Gilks
et al. 1996). In general, the proposal distributions used in MCMC algorithms should
result in well mixing of chains and in a suitable acceptance rate. Determining which
proposal distribution is the best one for a particular target distribution is a very impor-
tant, but also a difﬁcult task, because it involves much trial and error. The most used
proposal distribution is the Gaussian distribution, due to its attractive computational
and theoretical properties. However, in general, we do not know how to obtain a suit-
able covariance matrix for a Gaussian proposal distribution. One way to overcome this
problem is to use adaptive MCMC, where the proposal distribution is automatically
adapted during the MCMC run (for surveys of Adaptive MCMC algorithms see Liang
et al. 2010; Andrieu and Thoms 2008).
2.3 Hamiltonian Monte Carlo
The Hamiltonian Monte Carlo1 (HMC, Duane et al. 1987; Neal 2011) is a gradi-
ent based MCMC method which combines the molecular dynamics approach from
1 Originally called hybrid Monte Carlo (see, Neal 2011).
123

Parameter estimation in SDEs with MCMC and non-linear KF
statistical physics with MCMC. HMC is based on utilization of gradient information
and therefore it is capable of avoiding the random walk behavior typically encountered
in Metropolis–Hastings methods (Beskos et al. 2010; Neal 2011).
Consider a unit mass m = 1 particle, whose generalized coordinates are given by the
parameter vector θ ∈Rd. Let the corresponding momentum of the particle be ρ ∈Rd.
Assume that the potential energy of the system is given by the negative logarithm of the
(unnormalized) posterior distribution ϕ(θ) = −ln p(θ | y1, . . . , yM)−ln p(θ) and that
the kinetic energy is given by the usual formula T (ρ) = ρT ρ/2. The corresponding
Hamiltonian function is then given as
H(θ, ρ) = ϕ(θ) + 1
2
ρT ρ.
(13)
Assuming a suitable temperature of the system, the joint probability density of a
particle system with the Hamiltonian function above is now given as
p(θ, ρ) =
1
Z H
e−H(θ,ρ)
= p(θ | y1, . . . , yM) N(ρ | 0, I),
(14)
that is, the marginal distribution of θ will be the posterior distribution and the marginal
of ρ is a unit Gaussian distribution.
If we now let the system evolve in ﬁctious time τ, the particle coordinates and
momenta will change according to the Hamiltonian equations
dθ
dτ = ∂H
∂ρ = ρ
dρ
dτ = −∂H
∂θ
= −∂ϕ(θ)
∂θ
.
(15)
Note that θ and ρ are functions of time τ. It can be shown (Neal 2011) that these
equations keep the Hamiltonian constant, preserve the volume of the phase space
and are time-reversible. From these properties it follows that the joint distribution of
θ and ρ will always be p(θ, ρ) = p(θ | y1, . . . , yM) N(ρ | 0, I). That is, by letting
the particles evolve in time, the distribution of generalized coordinate θ will be the
posterior distribution p(θ | y1, . . . , yM).
In practice, the Hamiltonian dynamics cannot be simulated exactly, but we need to
use a numerical differential equation solver with a ﬁnite step size ϵ for solving them.
A useful method for doing that is the leapfrog method which has the fortunate property
that it also has the volume-conservation and time-reversibility properties (Neal 2011).
Due to the ﬁnite discretization step, the Hamiltonian H does not stay exactly con-
stant, but this can be taken into account in the MCMC method by adding a Metropo-
lis accept/reject step after each simulation of a trajectory, which leads to the HMC
algorithm.
The HMC algorithm has two main steps in each iteration. The ﬁrst step deals with
ρ only, where the momentum values are randomly drawn. We could simply draw the
123

I. S. Mbalawata et al.
momenta from Gaussian distribution with mean zero and unit variance, but here we
present a more general method which uses momentum persistence, where the old
momenta are replace only partially. The momentum persistence or partial momentum
refreshment (see, e.g., Neal 2011) can be used for avoiding random walk behavior
in HMC when L is small. The persistence is controlled by the parameter α ∈[0, 1],
where the value α = 0 corresponds to no persistence at all. The second step deals with
Metropolis updates, where the new state is proposed by Hamiltonian dynamics that
are simulated by L leapfrog steps. At the end of the L-step trajectory, the momentum
variables are negated that makes the Metropolis proposal symmetrical. The two steps
leave the distribution of position and momentum invariant (Neal 2011).
Suppose the conﬁguration at the nth iteration is (θ(n), ρ(n)), the next state is gener-
ated in HMC as follows:
– Generate new momenta ρ(n) as follows:
ρ(n) ←α ρ(n) + ξ

1 −α2,
(16)
where α ∈[0, 1] and ξ ∼N(0, I).
– Run the leapfrog algorithm for L steps with step size ϵ. Let the ﬁnal conﬁguration
be (θ∗, ρ∗).
– Accept (θ(n+1), ρ(n+1)) = (θ∗, −ρ∗) with acceptance probability
A

(θ(n), ρ(n)), (θ∗, ρ∗)

= min

1, e−H(θ∗,ρ∗)+H(θ(n),ρ(n))
,
(17)
otherwise let (θ(n+1), ρ(n+1)) = (θ(n), ρ(n)).
The generated sequence of random samples θ(1), θ(2), . . . will now be approximately
distributed according to the posterior distribution p(θ | y1, . . . , yM). Note that for
the evaluation process the gradient is needed in the Hamiltonian equations, and for
the acceptance probability we do not need to know the normalization constant of the
posterior distribution.
The choice of the leapfrog step size ϵ, the number of leapfrog steps L, and the
momentum persistence parameter α is a hard task. One way of tuning the HMC
parameters is to run the HMC algorithm for different values of ϵ, L and α, monitor
the acceptance rates, and do visual inspection of the convergence using trace plots,
scatter plots, and by investigating the autocorrelation function (Neal 2011). From the
plots and the acceptance rates, one should choose the values of ϵ, L and α that give
the best results. For more details on mechanisms to tune the HMC parameters, see
Neal (2011), Beskos et al. (2010), Chen et al. (2001), Holder et al. (2001).
3 Parameter estimation in linear SDEs
3.1 Continuous-discrete-time Kalman ﬁlter
The continuous-discrete-time Kalman ﬁlter (see, e.g., Grewal and Andrews 2001) is
an algorithm which can be used for computing the exact posterior distribution of the
state in linear Gaussian state space models of the following form (cf. Sect. 1):
123

Parameter estimation in SDEs with MCMC and non-linear KF
dx(t) = F(t, θ) x(t) dt + L(t, θ) dB(t)
(18)
yk = Hk x(tk) + rk.
(19)
Parameter estimation of linear SDEs could also be recast as parameter estimation in
discrete-time Gauss–Markov models by forming the equivalent discrete-time system
by using the well known results from linear systems theory (see, e.g., Grewal and
Andrews 2001; Särkkä 2006). However, this will lead to problems in computation
of the gradient of the energy function, because it would require computation of the
gradients of the discretization matrices, for which there does not exist a simple formula
(cf. Najfeld and Havel 1995). Instead of discretizing (solving) the dynamic model it is
advantageous to use the continuous-discrete formulation explicitly. The continuous-
discrete Kalman ﬁlter is the following:
1. Prediction step:
dm−
k (t)
dt
= F(t, θ)m−
k (t)
(20)
dP−
k (t)
dt
= F(t, θ)P−
k (t) + P−
k (t)FT (t, θ) + 	(t, θ),
(21)
where 	(t, θ) = L(t, θ) Qc LT (t, θ) and the initial conditions are m−
k (tk−1) =
mk−1, P−
k (tk−1)
=
Pk−1, and the prediction result is given as m−
k
≜
m−
k (tk), P−
k ≜P−
k (tk).
2. Update step:
Sk = HkP−
k HT
k + Rk
(22)
Kk = P−
k HT
k S−1
k
(23)
mk = m−
k + Kk

yk −Hkm−
k

(24)
Pk = P−
k −KkSkKT
k .
(25)
Note that the predicted and updated means and covariances together with Sk and Kk
above depend on the parameter θ, but the dependency has been dropped for notational
convenience. By running the Kalman ﬁlter through the data with ﬁxed parameters, θ
we get the following distributions
p(xk|y1:k−1, θ) = N(xk|m−
k (θ), P−
k (θ))
p(xk|y1:k, θ) = N(xk|mk(θ), Pk(θ))
p(yk|y1:k−1, θ) = N(yk|Hk m−
k (θ), Sk(θ)).
From these results we can now easily compute the marginal likelihood of the
measurements given θ as follows (cf. Singer 2002, 2011; Särkkä 2006):
p(y1, . . . , yM | θ) =
M

k=1
N(yk|Hk m−
k (θ), Sk(θ)),
(26)
and hence the energy function from Eq. (10) can be written as
123

I. S. Mbalawata et al.
ϕ(θ) =
M

k=1
1
2 ln |2πSk(θ)| + 1
2
M

k=1
(yk −Hkm−
k (θ))T S−1
k (θ)(yk −Hkm−
k (θ)).
(27)
3.2 Energy function derivative
The MCMC methods utilize the energy function ϕ(θ) during the sampling process,
and in the case of Metropolis–Hastings this is indeed all that we need. However, in the
gradient based optimization methods for ﬁnding the MAP estimate and in the HMC
we also need the derivative of the energy function. We can now formally differentiate
the energy function in Eq. (27) as follows:
∂ϕ(θ)
∂θi
= 1
2
M

k=1

Tr

S−1
k
∂Sk
∂θi

−1
2
M

k=1

Hk
∂m−
k
∂θi
T
S−1
k (yk −Hkm−
k )
−1
2
M

k=1

yk −Hkm−
k
T S−1
k
∂Sk
∂θi
S−1
k (yk −Hkm−
k )
−1
2
M

k=1
(yk −Hkm−
k )T S−1
k Hk
∂m−
k
∂θi
−∂(ln(p(θ)))
∂θi
,
(28)
where we have used the chain rule to account for the implicit dependence of m−
k and
Sk on the parameter θ. From Eq. (22) we get that the derivative of Sk can be expressed
as ∂Sk/∂θi = Hk ∂P−
k /∂θi HT
k . Next we need to derive equations for computing the
two unknown values ∂m−
k /∂θi and ∂P−
k /∂θi.
3.3 Kalman ﬁlter prediction step derivatives
In principle, it is possible to compute ∂m−
k /∂θi and ∂P−
k /∂θi from discrete Kalman
ﬁlter prediction equations however, it is not easy to calculate the resulting partial
derivativesofAk andQk,becausethereexistnosimpleformulaforsuchcalculation(cf.
Najfeld and Havel 1995). To solve this problem we use the continuous-discrete-time
Kalman ﬁlter prediction Eqs. (20) and (21). The partial derivatives of the differential
Eqs. (20) and (21) are
∂
∂θi

dm−
k (t)
dt

= ∂F(t, θ)
∂θi
m−
k (t) + F(t, θ)∂m−
k (t)
∂θi
∂
∂θi

dP−
k (t)
dt

= ∂F(t, θ)
∂θi
P−
k (t) + F(t, θ)∂P−
k (t)
∂θi
+∂P−
k (t)
∂θi
FT (t, θ) + P−
k (t)
∂F(t, θ)
∂θi
T
+ ∂	(t, θ)
∂θi
.
(29)
123

Parameter estimation in SDEs with MCMC and non-linear KF
For the purposes of simplifying the notation, we deﬁne n−
k (t) = ∂m−
k (t)/∂θi,
G(t, θ) = ∂F(t, θ)/∂θi, B−
k (t) = ∂P−
k (t)/∂θi, and W(t, θ) = ∂	(t, θ)/∂θi. Note
that n−
k (t) and B−
k (t) depend on θ since the predicted and updated mean and covari-
ance depend on the parameter θ, but the dependency has been dropped for notational
convenience.
The Eqs. (20), (21) and (29) form the following set of differential equations:
dm−
k (t)
dt
= F(t, θ)m−
k (t)
dn−
k (t)
dt
= G(t, θ)mk−1(t) + F(t, θ)n−
k (t)
dP−
k (t)
dt
= F(t, θ)P−
k (t) + P−
k (t)FT + 	(t, θ)
dB−
k (t)
dt
= G(t, θ)P−
k (t)+P−
k (t)GT (t, θ)+F(t, θ)B−
k (t)+B−
k (t)FT (t, θ)+W(t, θ).
(30)
The above differential equations are used to compute the values m−
k (t) and P−
k (t)
as well as the derivatives ∂m−
k (t)/∂θi and ∂P−
k (t)/∂θi given that the quantities
mk−1, Pk−1, ∂mk−1/∂θi, and ∂Pk−1/∂θi from the previous step are known.
The differential equations above can now be integrated using any ODE numerical
integration method such as the Runge–Kutta method (see, e.g., Butcher 2003). This
is the only practical way in the general case, where the matrices depend on time.
However, if the matrices are time independent, we can use the so-called matrix fraction
decomposition for much more computationally efﬁcient solving of the equations.
3.4 Matrix fraction decomposition based computation of derivatives
This section deals with the computation of the values m−
k (t) and P−
k (t) as well as their
derivatives ∂m−
k (t)/∂θi and ∂P−
k (t)/∂θi which are used in the numerical computation
of energy function and its derivative. In this section, matrices F and 	 do not depend
on t.
To compute m−
k (t) and ∂m−
k (t)/∂θi, we solve the ﬁrst two equations of Eq. (30)
and the resulting solution is
m−
k (t)
n−
k (t)

= exp
 F(θ)
0
G(θ) F(θ)

tk
  mk−1
nk−1

,
(31)
where n−
k (t) = ∂m−
k (t)/∂θi, nk = ∂mk/∂θi, and tk = tk −tk−1.
To compute P−
k (t) and ∂P−
k (t)/∂θi, we solve the third and fourth differential equa-
tions of Eq. (30). This computation is not as straight forward as for m−
k (t) and
∂m−
k (t)/∂θi, but it can be done by using the matrix fraction decomposition (see,
e.g., Grewal and Andrews 2001). We start by assuming that
123

I. S. Mbalawata et al.
P(t, θ) = C(t, θ)D−1(t, θ),
(32)
where C(t, θ) and D(t, θ) are auxiliary matrices differentiable with respect to t. The
matrix product is called matrix fraction and the representation of the matrix P(t, θ) is
calledafractiondecompositionofP(t, θ)(GrewalandAndrews2001).Inthefollowing
we shall deﬁne P−
k (t) ≜P(t, θ). To get the value of P(t, θ) we need to compute C(t, θ)
and D(t, θ) and use Eq. (32). If we assume that C(t, θ) and D(t, θ) are solutions of
the following differential equations:
dC(t, θ)
dt
= F(θ)C(t, θ) + 	(θ)D(t, θ)
(33)
dD(t, θ)
dt
= −FT (θ)D(t, θ),
(34)
then by taking time derivative of the Eq. (32), and substituting Eqs. (33) and (34) to
the right hand side of (32), and then simplifying, we get the following:
dP(t, θ)
dt
= F(θ)P(t, θ) + P(t, θ)FT (θ) + 	(θ).
(35)
BecauseEqs.(33) and (34)formasystemoflineartimeinvariantdifferentialequations,
they could be easily solved by using the matrix exponential function:
C(t, θ)
D(t, θ)

= exp
F(θ)
(θ)
0
−FT (θ)

tk
 Pk−1
I

,
(36)
where the initial values for C and D have been set to Pk−1 and I, respectively. Hence,
we are in the position of ﬁnding P−
k (t) ≜P(t, θ) using Eqs. (32) and (36). But we
have to keep in mind that we also need to compute the partial derivative of P(t, θ). If
we take the partial derivative of Eq. (32) with respect to θi we get the following:
∂P(t, θ)
∂θi
= ∂C(t,θ)
∂θi
D−1(t, θ) −C(t, θ)D−1(t, θ) ∂D(t,θ)
∂θi
D−1(t, θ)
(37)
The next task is to ﬁnd the partial derivative of P(t, θ) from Eq. (37). The computa-
tion of ∂P(t, θ)/∂θi needs ∂C(t, θ)/∂θi and ∂D(t, θ)/∂θi which can be computed by
differentiating Eqs. (33) and (34) with respect to θi:
∂
∂θi
dC(t, θ)
dt

= ∂F(θ)
∂θi
C(t, θ) + F(θ)∂C(t, θ)
∂θi
+ ∂	(θ)
∂θi
D(t, θ) + 	(θ)∂D(t, θ)
∂θi
(38)
∂
∂θi
dD(t, θ)
dt

= −
∂F(θ)
∂θi
T
D(t, θ) −FT (θ)∂D(t, θ)
∂θi
.
(39)
The initial values for ∂C(t, θ)/∂θi and ∂D(t, θ)/∂θi are ∂Pk−1/∂θi and 0, respectively.
Fornotationalconvenience,wesetX(t, θ) = ∂C(t, θ)/∂θi andy(t, θ) = ∂D(t, θ)/∂θi.
123

Parameter estimation in SDEs with MCMC and non-linear KF
Hence Eqs. (33), (34), (38) and (39) form a system of differential equations which has
the following solution:
⎡
⎢⎢⎣
C(t, θ)
D(t, θ)
X(t, θ)
y(t, θ)
⎤
⎥⎥⎦= exp
⎛
⎜⎜⎝
⎡
⎢⎢⎣
F(θ)
(θ)
0
0
0
−FT (θ)
0
0
G(θ)
W(θ)
F(θ)
(θ)
0
−GT (θ)
0
−FT (θ)
⎤
⎥⎥⎦tk
⎞
⎟⎟⎠
⎡
⎢⎢⎣
Pk−1
I
∂Pk−1
∂θi
0
⎤
⎥⎥⎦,
(40)
from which ∂P(t, θ)/∂θi can easily be computed using Eq. (37). Therefore we have
computed ∂m−
k (t)/∂θi and ∂P−
k (t)/∂θi and the next step is the computation of their
measurement update from the Kalman ﬁlter update equation.
3.5 Kalman ﬁlter update step derivatives
By differentiating Eqs. (23), (24) and (25), we get the following equations for updating
n−
k (t) = ∂m−
k (t)/∂θi and B−
k (t) = ∂P−
k (t)/∂θi into nk(t) = ∂mk(t)/∂θi and Bk(t) =
∂Pk(t)/∂θi:
∂Kk
∂θi
= B−
k (t)HT
k S−1
k
−P−
k (t)HT
k S−1
k
∂Sk
∂θi
S−1
k
(41)
nk(t) = n−
k (t) + ∂Kk
∂θi

yk −Hkm−
k

−KkHkn−
k (t)
(42)
Bk(t) = B−
k (t) −∂Kk
∂θi
SkKT
k −Kk
∂Sk
∂θi
KT
k −KkSk
∂Kk
∂θi
T
,
(43)
which completes the derivative recursion. Thus the energy ϕ(θ) and its derivative can
be sequentially computed along the Kalman ﬁltering.
4 Parameter estimation in non-linear SDEs
4.1 Continuous-discrete extended Kalman ﬁlter
The continuous-discrete extended Kalman ﬁlter (EKF) is a Taylor series expansion
based approximation of the general Bayesian continuous-discrete ﬁlter (Jazwinski
1970). It is an extension of the continuous-discrete Kalman ﬁlter to non-linear optimal
ﬁltering problems (Jazwinski 1970; Grewal and Andrews 2001). If the measurement
noises are additive then the non-linear continuous discrete ﬁltering problem can be
stated as Eqs. (3) and (4). The prediction and update steps of the (ﬁrst order) additive
noise continuous-discrete extended Kalman ﬁlter are:
1. Prediction step:
dm−
k (t)
dt
= f(m−
k (t), t, θ)
(44)
123

I. S. Mbalawata et al.
dP−
k (t)
dt
= Fx(m−
k (t), t, θ)P−
k (t) + P−
k (t)FT
x (m−
k (t), t, θ) + 	(m−
k (t), t, θ),
(45)
where 	(m, t, θ) = L(m, t, θ) Qc LT (m, t, θ) and Fx(x, θ, t) is the Jacobian
matrix of f(x, θ, t) with respect to x. The initial conditions are m−
k (tk−1) = mk−1,
P−
k (tk−1) = Pk−1, and the prediction result is given as m−
k ≜m−
k (tk), P−
k ≜
P−
k (tk).
2. Update step:
μk = h(m−
k , t)
(46)
Sk = Hx(m−
k , t)P−
k HT
x (m−
k , t) + rk
(47)
Kk = P−
k HT
x (m−
k , t)S−1
k
(48)
mk = m−
k + Kk

yk −μk

(49)
Pk = P−
k −KkSkKT
k ,
(50)
where Hx(x, t) is the Jacobian matrix of h(x, t).
In the case of non-linear ﬁltering model it is not possible to write down the energy
function in closed form. However by using the Gaussian approximation of the pos-
terior distribution computed by the continuous-discrete EKF, we can write down the
approximate energy function as follows:
ϕ(θ) =
N

k=1
1
2 ln |2πSk| + 1
2
N

k=1

yk −μk
T S−1
k

yk −μk

−ln p(θ).
(51)
4.2 Extended Kalman ﬁlter energy function derivative
The energy function derivative needed by the HMC and SCG methods can be derived
in analogous manner to the linear case (see Sect. 3.2):
∂ϕ(θ)
∂θi
= 1
2
N

k=1
Tr

S−1
k
∂Sk
∂θi

−1
2
N

k=1
∂μk
∂θi
T
S−1
k

yk −μk

−1
2
N

k=1

yk −μk
T S−1
k
∂Sk
∂θi
S−1
k

yk −μk

−1
2
N

k=1

yk −μk
T S−1
k
∂μk
∂θi
−∂(ln(p(θ)))
∂θi
.
(52)
The computation of the energy gradients needs the partial derivatives ∂Sk/∂θi and
∂μk/∂θi which can be computed by differentiating the continuous-discrete extended
Kalman ﬁlter Equations (46) and (47) as follows:
123

Parameter estimation in SDEs with MCMC and non-linear KF
∂μk
∂θi
= Hx(m−
k , t)
∂m−
kj
∂θi
(53)
∂Sk
∂θi
=
n

j=1

∂m−
kj
∂θi
∂Hx(m−
k , t)
∂m−
kj

P−
k HT
x (m−
k , t) + Hx(m−
k , t)∂P−
k
∂θi
HT
x (m−
k , t)
+ Hx(m−
k , t)P−
k
⎛
⎝
n

j=1

∂m−
kj
∂θi
∂Hx(m−
k , t)
∂m−
kj
⎞
⎠
T
.
(54)
4.3 Continuous-discrete extended Kalman ﬁlter prediction derivatives
If we compute the partial derivatives of the Eqs. (44) and (45), we get the following
differential equations:
d
dt

∂m−
k
∂θi

= Fx(m−
k , θ, t)
∂m−
kj
∂θi
+ ∂f(m−
k , θ, t)
∂θi
(55)
d
dt

∂P−
k
∂θi

= ∂P−
k
∂θi
FT
x (m−
k , θ, t) + P−
k
⎛
⎝
n

j=1

∂Fx(m−
k , θ, t)
∂m−
kj
∂m−
kj
∂θi

+∂Fx(m−
k , θ, t)
∂θi
T
+
⎛
⎝
n

j=1

∂Fx(m−
k , θ, t)
∂m−
kj
∂m−
kj
∂θi

+ ∂Fx(m−
k , θ, t)
∂θi
⎞
⎠P−
k
+ Fx(m−
k , θ, t)∂P−
k
∂θi
+ ∂	(θ, t)
∂θi
.
(56)
The above equations, together with the prediction Eqs. (44), (45) can now be used to
compute the prediction mean m−
k (t) and covariance P−
k (t) as well as their derivatives
∂m−
k (t)/∂θi and ∂P−
k (t)/∂θi. As the differential equations are non-linear, they need
to be integrated with a numerical ODE solution method such as Runge–Kutta.
4.4 Continuous-discrete extended Kalman ﬁlter update derivatives
To complete the recursion for the non-linear case, we still need to differentiate the
Eqs. (48), (49) and (50), which gives:
∂Kk
∂θi
= ∂P−
k
∂θi
HT
x (m−
k , t) S−1
k
+ P−
k
⎛
⎝
n

j=1

∂m−
kj
∂θi
∂Hx(m−
k , t)
∂m−
kj
⎞
⎠
T
S−1
k
123

I. S. Mbalawata et al.
−P−
k HT
x (m−
k , t)S−1
k
∂Sk
∂θi
S−1
k
(57)
∂mk
∂θi
= ∂m−
k
∂θi
+ ∂Kk
∂θi

yk −μk

−Kk
∂μk
∂θi
(58)
∂Pk
∂θi
= ∂P−
k
∂θi
−∂Kk
∂θi
SkKT
k −Kk
∂Sk
∂θi
KT
k −KkSk
∂Kk
∂θi
T
.
(59)
5 Numerical results
In this section we apply the HMC, MH and MAP methods to parameter estimation of
linear and non-linear SDEs. The main objective of this section is to assess the accuracy
and performance of the three estimation methods. In the MAP case, the point estimate
is all that we have. In the MCMC methods we can also compute the credible intervals
of the parameters which are the most probable intervals where the true parameter
should be. If the credible interval contains the parameter, the result can still be as good
as the model and data allows, even when the point estimates are far away from the true
values. But although the results of the methods are important, they do not tell much
about the methods itself.
In MCMC methods the main interest is how the MCMC sampling is performing.
For that purpose we have used the following assessment methods:
– TheautocorrelationfunctionmeasureshowwelltheMCMCsamplerisperforming
by measuring the dependence of the adjacent samples. Given a set of samples
x1, x2, x3, . . . , xN, an estimate of the autocorrelation function can be computed
as
φs ≈1
ˆσ 2
1
N
N−s

i=1

xi −ˆμ
 
xi+s −ˆμ

,
where ˆμ is the average of the samples, and ˆσ 2 is the sample variance. If the samples
were completely independent, we would have φ0 = 1 and φs = 0 for s > 0. In
general, smaller values in φs, for s > 0 indicate better mixing of the Markov chain
in the MCMC sampler.
– The autocorrelation time is the minimum distance of uncorrelated samples in
MCMC samples. It can be used as a measure of total dependency in the samples
and thus it summarizes the whole autocorrelation function into a single number.
The autocorrelation time can be deﬁned as
τ = 1 + 2
∞

s=1
φs,
where φs is the autocorrelation function. There are many ways to estimate the
autocorrelation time from a ﬁnite set of samples and here we have used the method
proposed by Geyer (1992). The smaller the value of τ, the less dependent the
samples are, and better the method is doing.
123

Parameter estimation in SDEs with MCMC and non-linear KF
There are three examples studied in this section: the ﬁrst two are linear SDEs models
and the third example is non-linear. All computations were done in Matlab.2 The
models can be summarized as follows:
1. The ﬁrst one is the classical Ornstein–Uhlenbeck process model which is a one-
dimensional linear reducible SDE, and the SDE is directly measured at discrete
instants of time. The parameters of the simulation have been selected such that
the posterior distribution has quite complicated form. Because the model is linear,
we can compute its likelihood exactly and a very computationally efﬁcient way of
doing this is the Kalman ﬁlter approach presented in this article. The derivatives of
the energy function can also be exactly computed with the methods presented in
this article. The purpose of this simple example is to show that the present method
produces meaningful results and demonstrate the usage of the assessment criteria.
2. The second example is a multidimensional linear SDE model, where the main
difﬁculty comes from the higher number of parameters. Although we can reduce
the number of samples parameters from 10 to 7 by including the linear parameters
as parts of the state, we still need to sample 7 of them. This example shows the
advantage of HMC over MH and also demonstrates the advantage of reporting
credible intervals of MCMC methods instead of the plain point estimates.
3. The third example is a multidimensional non-linear irreducible SDE with a
non-linear measurement model. There do not exist many methods for estimat-
ing parameters in this class of models. Furthermore, even with known parameters
the corresponding state estimation problem is strongly non-linear. This example
demonstrates that the present methods can be practically applied to this class of
non-linear irreducible SDE models and they produce good results.
5.1 Ornstein–Uhlenbeck model
In this section we demonstrate how to estimate the parameters λ and α of linear
reducible SDE called the Ornstein–Uhlenbeck process (Ornstein and Uhlenbeck 1930)
deﬁned as:
dx(t) = −λ x(t) dt + α dB(t),
(60)
where α is the degree of volatility caused by shocks and λ is the rate by which these
shocks dissipate and the variable reverts towards zero mean. The Ornstein–Uhlenbeck
process has many applications such as in ﬁnancial mathematics where it is used in
modeling interest rates, currency exchange rates, as well as prices for commodities.
The parameters λ and α are strictly positive.
We simulated 100 observations using λ = 4, α = 2, a sampling period of 0.1
and the measurement equation yk = x(tk), that is, x(tk) is measured perfectly. We
generated 10,000 samples from the posterior distribution of λ and α using Hamiltonian
Monte Carlo (HMC) and Metropolis–Hastings (MH), and computed the maximum a
posteriori (MAP) estimates of the parameters using the scaled conjugate gradient
2 The Matlab codes can be obtained from the corresponding author on request.
123

I. S. Mbalawata et al.
0
5000
10000
0
5
10
15
HMC samples of λ 
Samples
Iterations
0
5000
10000
1
2
3
HMC samples of α 
Samples
Iterations
0
5000
10000
0
5
10
15
MH samples of λ 
Samples
Iterations
0
5000
10000
1
2
3
MH samples of α
Samples
Iterations
Fig. 1 Ornstein–Uhlenbeck model: trace plots for λ and α samples simulated from HMC and MH sampling
techniques. For both methods, the samples have relatively good mixing
(SCG) optimization. In HMC sampling we used 0.01 for the leapfrog step size, 50
leapfrog steps at each iteration and we had no momentum persistence in use. In MH, we
used a symmetric Gaussian distribution with standard deviation 0.35 as the proposal
distribution, which gave the acceptance rate of 18%.
For the purpose of studying the performance of MCMC simulations, we examined
the trends and autocorrelations of samples. The trends of the parameter samples are
shown in Fig. 1, where the vertical axes represent the sample values. It is observed that
the parameters have relatively good mixing for both methods HMC and MH though
HMC mixes a bit better. The MH sample trends show a slight slow periodic movement
especially for the case of λ samples.
The autocorrelation functions of HMC and MH samples are shown in Fig. 2. The
autocorrelation functions of HMC decay to zero much earlier than the autocorrelation
functions of MH. For instance, HMC samples for α have the autocorrelation function
that drops to zero around lag 8, while for MH samples the autocorrelation function
drops to zero around lag 35, and does not stabilize to zero throughout the lags. The
same conclusion can be drawn for λ samples, although the exact lags are different.
Table 1 shows the autocorrelation times for HMC and MH samples. As it is seen, the
MH samples have greater autocorrelation times than HMC samples. Thus it can be
123

Parameter estimation in SDEs with MCMC and non-linear KF
5
10
15
20
0
0.5
1
HMC samples of λ 
Sample autocorrelations
Lags
5
10
15
20
0
0.5
1
HMC samples of α 
Sample autocorrelations
Lags
5
10
15
20
0
0.5
1
MH samples of λ 
Sample autocorrelations
Lags
5
10
15
20
0
0.5
1
MH samples of  α 
Sample autocorrelations
Lags
Fig. 2 Ornstein–Uhlenbeck model: the autocorrelation functions of HMC and MH samples computed
up to lag 20. The HMC simulations have lower autocorrelations than MH simulations with the HMC
autocorrelations stabilizing around zero after decay. The HMC autocorrelation times are lower than MH
autocorrelation times, which means that HMC simulations mix better
Table 1 Ornstein–Uhlenbeck model: the autocorrelation times for HMC and MH samples
Parameter
HMC method
MH method
λ
1.04
25.91
α
2.62
15.10
The better mixing of HMC is implied by the lower autocorrelation times of the samples
concluded that HMC has better mixing than MH with this model, and it is in that sense
performing better.
We computed the mean and median of the HMC samples and MH samples and
then compared them to the MAP estimates and original values. The mean, median
and MAP estimates are displayed in Table 2, together with the original values of λ
and α. The HMC sample median for λ is close to the original value compared to the
other λ estimates. For the case of α, the MH sample mean estimate is better than other
methods. To check the consistency of the uncertainty estimates in parameters, we
computed the 95% credible intervals corresponding to the 2.5 and 97.5% quantiles as
shown in Table 3 for each parameter. It is observed that the original values are within
the range.
123

I. S. Mbalawata et al.
Table 2 Ornstein–Uhlenbeck model: the original parameters values, HMC mean and median, MH mean
and median, and MAP estimates
Parameter
Original values
HMC mean
HMC median
MH mean
MH median
MAP
λ
4.00
4.52
4.44
4.64
4.59
4.63
α
2.00
1.82
1.80
1.83
1.82
1.81
The estimates of the parameters are indeed close to the true values
Table3 Ornstein–Uhlenbeckmodel:thecredibleintervalsforHMCandMHsamplesshowingthequantiles
corresponding to 2.5 and 97.5%
Parameter
Original values
HMC 2.5%
HMC 97.5%
MH 2.5%
MH 97.5%
λ
4.00
3.59
7.41
2.28
7.27
α
2.00
1.70
2.20
1.53
2.16
The original values are shown to be within the credible interval, which means that the methods perform
well
Figure 3 shows the scatter plots for HMC and MH samples together with the original
values of λ and α marked by a star and the MAP estimates marked by a circle. In each
method, we found that the original values are among the sample points and also the
MAP estimates are within the samples.
The above results are for 100 observations. However the results in Table 2 show
that there seems to be a bias: for example the HMC means seem to differ much
from the original values. However, because the true values are inside the credible
intervals, this bias is not actually statistically signiﬁcant and is only caused by the low
number of observations. And we should expect the bias to decrease when the number
of observations is increased. We studied the behavior of the method with increasing
number of observations by computing the estimates based on the range of 100–2000
observations. At each number of observations, the number of HMC samples produced
was 1000 using 50 leapfrog steps of size 0.01, and then the HMC mean, median and
credible intervals of samples were computed. From Fig. 4 we can generally conclude
that as the number of observations increases, the HMC means approach the true value,
which demonstrates the fact that the bias was only due to the low number of noisy
observations.
5.2 Stochastic biochemical oxygen demand model
Biochemical oxygen demand (BOD) model is a model for the concentration of oxygen
needed by aerobic microorganisms. The concentration is purposely for stabilization
of waste water organic matters. The water quality in bodies of water is generally
measured by BOD and oxygen concentration. Let b, o and n be the concentration
levels, measured in mg/l, for Carbonaceous Biochemical Oxygen Demand (CBOD),
Dissolved Oxygen (DO) and Nitrogenous Biochemical Oxygen Demand (NBOD)
respectively(Bergetal.2006).TheCBOD,DOandNBODaredeﬁnedbythefollowing
differential equations:
123

Parameter estimation in SDEs with MCMC and non-linear KF
0
5
10
15
1
1.5
2
2.5
3
HMC samples of λ and α  
λ
α
0
5
10
15
1
1.5
2
2.5
3
MH samples of λ and α  
λ
α
Fig. 3 Ornstein–Uhlenbeck model: the scatter plots, original parameter values (star) and the MAP estimates
(circle). The samples enclose the original values and MAP estimates. There exist correlation between the
parameters
0
500
1000
1500
2000
2
3
4
5
6
7
8
HMC λ estimates
Observations
λ
True value
HMC mean
HMC median
2.5 CI
97.5 CI
0
1000
2000
1.5
2
2.5
HMC α estimates
Observations
α
True value
HMC mean
HMC median
2.5 CI
97.5 CI
Fig. 4 Ornstein–Uhlenbeck model: the plots of HMC mean, HMC median and credible intervals (2.5 and
97.5 CI) for parameters λ and α respectively. As the number of observations increases, the HMC mean
estimates approach the true values
db
dt = −kbb + s1
do
dt = −kcb −k2o −knn + s∗
2
dn
dt = −knn + s3,
(61)
where kb = kc +k3, s∗
2 = k2ds + p0 −r0 +s2, kc is the reaction rate coefﬁcient, k2 is
the reaeration rate coefﬁcient, k3 is the sedimentation and adsorption loss rate for CBO
kn is the decay rate of NBOD, p0 is photosynthesis of oxygen, r0 is the respiration
of oxygen, ds is the saturation concentration of oxygen, s1 is the independent source
for CBOD, s3 is the independent source for OD, and s3 is the independent source for
NBOD. The BOD is an environmental model, thus uncertainties are unavoidable. To
123

I. S. Mbalawata et al.
take into account the uncertainties, the differential Eq. (61) is transformed to SDE by
adding white noise processes. The resulting linear SDE is:
⎡
⎣
dB
dO
dN
⎤
⎦=
⎡
⎣
−kb 0
0
−kc −k2 −kn
0
0
−kn
⎤
⎦
⎡
⎣
B
O
N
⎤
⎦dt +
⎡
⎣
s1
s∗
2
s3
⎤
⎦dt +
⎡
⎣
σB 0
0
0
σO 0
0
0
σN
⎤
⎦dB, (62)
where σB, σO, and σN are diffusion coefﬁcients in the noises of CBOD, OD and
NBOD respectively. dB =
!dB1 dB2 dB3
"T is the vector of Brownian motion process
increments. We are interested only in estimating the parameters of the BOD model,
for more information about this model see Berg et al. (2006).
If we deﬁne the state x =
! B O N s1 s∗
2 s3
"T , we can write the Eq. (62) and the
measurement model for the noisy values of B, O, N in the form
dx = Fx dt + L dB
y = H x + r,
(63)
where
F =
⎡
⎢⎢⎢⎢⎢⎢⎣
−kb 0
0
1 0 0
−kc −k2 −kn 0 1 0
0
0
−kn 0 0 1
0
0
0
0 0 0
0
0
0
0 0 0
0
0
0
0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎦
,
L =
⎡
⎢⎢⎢⎢⎢⎢⎣
σB 0
0
0
σO 0
0
0
σN
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎦
,
H =
⎡
⎣
1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0
⎤
⎦,
and r ∼N(0, 0.01 I), where I is the identity matrix. The task is to estimate the
parameters kc, k2, k3, kn, s1, s∗
2, s3, σB, σO, and σN. In this model HMC, MH and
MAP were used on the parameters kc, k2, k3, kn, σB, σO and σN while the Kalman
ﬁlter produced the estimates for s1, s∗
2 and s3. The simulation of data was done by
considering the typical values of the parameters used in Berg et al. (2006). These
values (the original values) are shown in Tables 5 and 7.
We generated 10,000 samples with HMC and MH sampling methods. For MH,
we used Gaussian distribution with covariance matrix 0.152I as the proposal distri-
bution. The acceptance rate was found to be 20.43%. In HMC, the step adjustment
used in leapfrog scheme was 0.1 and the leapfrog trajectory length was 100. The auto-
correlation times of the samples are shown in Table 4, where it can be seen that the
autocorrelation times for HMC are smaller than those of MH samples. Thus, the HMC
samples are statistically less dependent than MH samples.
The means, medians and MAP estimates of the parameters (excluding s1, s∗
2 and
s3) together with the original values are shown in Table 5. It can be seen that all the
estimates are fairly close to the true parameters values and thus all the methods work
quite well. To check the consistency of the uncertainty estimates in parameters, we
computed the 95% credible intervals corresponding to 2.5 and 97.5% quantiles as
shown in Table 6 for each parameter. It is observed that the original values are within
the ranges.
123

Parameter estimation in SDEs with MCMC and non-linear KF
Table 4 BOD model: the autocorrelation times for HMC and MH samples
Method
kc
k2
k3
kn
σ 2
B
σ 2
O
σ 2
N
HMC
0.55
0.51
0.97
1.09
2.41
16.89
3.17
MH
124.43
54.39
125.22
106.12
31.73
36.94
37.56
The HMC samples are statistically less dependent and converge better than MH samples as they have lower
values of autocorrelation times
Table 5 BOD model: the original parameter values, means and medians of HMC and MH sampling
methods and MAP estimates
Parameter
Original values
HMC mean
HMC median
MH mean
MH median
MAP
kc
0.76
0.70
0.70
0.71
0.71
0.71
k2
4.25
4.32
4.31
4.32
4.31
4.31
k3
0.25
0.19
0.19
0.18
0.18
0.18
kn
0.98
0.75
0.75
0.73
0.73
0.74
σ 2
B
1.50
1.55
1.53
1.54
1.53
1.50
σ 2
O
1.50
1.41
1.39
1.44
1.41
1.37
σ 2
N
1.00
1.03
1.01
1.03
1.02
0.10
The means, medians and MAP estimates for all the parameters are fairly close to the original values
Table 6 BOD model: the credible intervals for HMC and MH samples showing the quartiles corresponding
to 2.5 and 97.5%
Parameter
Original values
HMC 2.5%
HMC 97.5%
MH 2.5%
MH 97.5%
kc
0.76
0.40
1.01
0.38
1.06
k2
4.25
4.08
4.56
4.05
4.57
k3
0.25
−0.21
0.60
−0.25
0.62
kn
0.98
0.32
1.17
0.27
1.20
σ 2
B
1.50
1.12
2.12
1.07
2.09
σ 2
O
1.50
1.03
1.90
0.99
1.91
σ 2
N
1.00
0.73
1.42
0.72
1.41
The original values are within the credible intervals, which means that the methods perform well
The parameters s1, s∗
2, and s3 were treated as states in the recursive Kalman ﬁlter,
and hence the last step results of the ﬁlters give the estimates and their covariances
(at each Monte Carlo sample). Table 7 shows the results. At the ﬁrst glance it might
seem that the estimates of s1 and s3 are biased. We also computed the 95% credible
intervals corresponding to 2.5 and 97.5% quantiles as shown in Table 8 for each
parameter. It is observed that the original values are within the ranges and thus the
MCMC results are good in that sense. Thus the apparent biases in fact indicate the
corresponding posterior distribution is asymmetric, and thus the point estimates do
not well summarize the posterior distribution. In the MAP case, the original value of
123

I. S. Mbalawata et al.
Table 7 BOD model: the original parameter values and their estimates
Parameter
Original values
HMC-KF
MH-KF
MAP-KF
s1
3.00
2.29
2.27
2.27
s∗
2
42.03
42.12
42.13
42.12
s3
0.00
-0.37
-0.37
-0.37
Table 8 BOD model: the original parameter values, HMC, MH and MAP credible intervals
Par.
Orig.
HMC 2.5%
HMC 97.5%
MH 2.5%
MH 97.5%
MAP 2.5%
MAP 97.5%
s1
3.00
1.21
3.40
1.19
3.39
1.56
2.98
s∗
2
42.03
40.16
44.11
40.24
44.13
41.43
42.81
s3
0.00 -0.98
0.22
-0.97
0.22
-0.96
0.22
In HMC and MH cases the intervals enclose the original values, but in MAP case not
Fig. 5 Illustration of the
re-entry vehicle tracking. The
dashed line shows the velocity
of a radar
6200
6300
6400
6500
6600
0
100
200
300
400
Radar
Earth
Trajectory
Target
s1 is not inside the credible interval, which is most likely due to the fact that the MAP
estimate ignores the parameter uncertainty in computation of the credible interval.
5.3 Re-entry vehicle tracking
As the non-linear example, we consider the re-entry vehicle tracking problem which
has been used as an example of highly non-linear tracking problem in (Julier and
Uhlmann 2004; Särkkä 2006, 2007). In the problem, a high-speed vehicle is doing
a re-entry to Earth’s atmosphere and we wish to estimate its trajectory from radar
measurements (see Fig. 5). Here we assume that the drag parameter is unknown, but
instead of including it as an additional state variable as in (Julier and Uhlmann 2004;
Särkkä 2006, 2007), we consider it as a parameter of the model. We shall also assume
123

Parameter estimation in SDEs with MCMC and non-linear KF
that the process noise spectral densities are unknown. With these modiﬁcations the
model can be written into the following form (Särkkä 2006):
R(t) =
#
x2
1(t) + x2
2(t)
V (t) =
#
x2
3(t) + x2
4(t)
D(t) = b0 θ1 exp
 R0 −R
H0

V (t)
G(t) = −Gm0
R3(t)
dx1 = x3 dt
dx2 = x4 dt
dx3 = D(t) x3 dt + G(t) x1 dt + dB1(t)
dx4 = D(t) x4 dt + G(t) x2 dt + dB2(t)
(64)
where x1(t) and x2(t) are positions of the body, x3(t) and x4(t) are velocities of the
body, D(t) is part of the drag-related force term, G(t) is part of the gravity-related
force term, R(t) is the distance from the center of Earth, V (t) is the speed, and B(t) =
(B1(t), B2(t)) is a Brownian motion with diffusion matrix Qc = diag(θ2, θ3). The
known parameters are b0 = −0.59783, H0 = 13.406, Gm0 = 3.9860 × 105, R0 =
6374, and θ1, θ2 and θ3 are the unknown parameters to be estimated. The state is
measured by a radar which produces range and bearings measurements as follows:
dk =
#
(x1(tk) −xr)2 + (x2(tk) −yr)2 + rd
k
θk = tan−1
x2(tk) −yr
x1(tk) −xr

+ ra
k ,
(65)
the location of the radar is (xr, yr) = (R0, 0). The standard deviations of the mea-
surements were chosen to be 10 times the ones used in (Julier and Uhlmann 2004):
σr = 10−2 km, σa = 1.7 mrad. The initial distribution of the state is assumed to be
multidimensional Gaussian with mean and covariance:
m0 =
⎡
⎢⎢⎣
6500.4
349.14
−1.8093
−6.7967
⎤
⎥⎥⎦,
P0 =
⎡
⎢⎢⎣
10−6
0
0
0
0
10−6
0
0
0
0
10−6
0
0
0
0
10−6
⎤
⎥⎥⎦.
(66)
We simulated the trajectory only for the ﬁrst 100s to induce some additional uncer-
tainty to the parameters. The sampling period was t = 2 seconds. In the simu-
lations we used the following diffusion matrix for the Brownian motion: Qtrue
c
=
diag
!
2.4064 × 10−4, 2.4064 × 10−4 "
, and the true value of the drag parameter was
θ1 = 2.
The autocorrelation times for HMC and MH samples are shown in Table 9 and
they show that the mixing is much better in HMC samples than in MH samples. The
123

I. S. Mbalawata et al.
Table 9 Re-entry tracking: the autocorrelation times for HMC and MH samples
Parameter
HMC method
MH method
θ1
0.78
8.75
θ2
0.60
83.64
θ3
1.38
89.74
The autocorrelation times of HMC are lower than of MH
Table 10 Re-entry tracking: the original parameters values, HMC mean and median, MH mean and median,
and MAP estimates
Parameter
Original
HMC mean
HMC median
MH mean
MH median
MAP
θ1
2.00
2.07
2.07
2.07
2.08
2.07
θ2
2.4×10−4
3.8×10−4
3.5×10−4
3.8×10−4
3.6×10−4
3.4×10−4
θ3
2.4×10−4
1.4×10−4
1.3×10−4
1.4×10−4
1.3×10−4
1.2×10−4
All the estimates can be seen to be fairly close to the true values
Table 11 Re-entry tracking: the credible intervals for HMC and MH samples showing the quantiles cor-
responding to 2.5 and 97.5%
Parameter
Original
HMC 2.5%
HMC 97.5%
MH 2.5%
MH 97.5%
θ1
2.00
1.66
2.20
1.69
2.12
θ2
2.4×10−4
0.0×10−4
6.4×10−4
3×10−8
1.2×10−3
θ3
2.4×10−4
1.5×10−4
7.8×10−4
3×10−5
7.7×10−4
The parameter estimates are observed to be within the ranges
mean, median and MAP estimates are shown in Table 10 and the credible intervals in
Table 11. The ﬁrst parameter θ1 is well estimated in both MCMC sampling methods
with the estimates being close to the true parameter value. The MAP estimates can
be seen to be closer to the true values than the mean and median, which might be
due to the particular form of the posterior distribution. In the parameters θ2 and θ3
there appears to be a bias when looking at the point estimates. One explanation to
the bias in these process noise parameters is that the linearization error causes them
to be overestimated. However, because the true values are indeed inside the credible
intervals, this can also be the result of their posterior distribution being asymmetric.
6 Conclusion and discussion
In this paper we have presented parameter estimation methods for linear and non-linear
stochastic differential equations (SDE). We have shown how linear and non-linear
(extended) Kalman ﬁltering can be used for computationally efﬁcient implementation
of a Metropolis–Hastings based Markov chain Monte Carlo (MCMC) algorithm for
parameter estimation. We have also derived a recursive algorithm for computation
of the energy function parameter gradients needed by the Hamiltonian Monte Carlo
123

Parameter estimation in SDEs with MCMC and non-linear KF
(HMC) based MCMC and gradient based optimization methods. In the linear time-
invariant case the differential equations involved in the recursive algorithm can be
solved in a very computationally efﬁcient manner using an extension of the matrix
fraction decomposition and in general case using a numerical differential equation
solver such as Runge–Kutta.
The performance of the methods was illustrated in simulated linear and non-linear
examples. For linear SDE examples, we applied the methods to a classical Ornstein–
Uhlenbeck process and the stochastic biochemical oxygen demand model. The results
show that the methods are able to estimate the model parameters and their posterior
uncertainties accurately. As a non-linear example we used a re-entry vehicle tracking
problem which has previously been used as an example of demanding non-linear
tracking problem. The SDE in the model is multivariate, non-linear and irreducible,
and the measurement model is non-linear. There exists only a few methods which
can be used for parameter estimation in this type of models and as shown by the
results, the present method gives good estimates of the parameters and their posterior
uncertainties.
We also evaluated the accuracies and performances of the MAP, MH and HMC
based estimates. The relative performances (speed of mixing) of MH and HMC were
evaluated by using autocorrelation functions and autocorrelation times. The relative
merits of the evaluated optimization and MCMC based methods can be summarized
as follows:
– The scaled conjugate gradient (SCG) based MAP estimation is quite computa-
tionally efﬁcient method to get a rough estimate of the model parameters. With
the presented recursive algorithm the computation of gradients is computationally
efﬁcient enough to favor gradient based optimization over non-gradient based.
However, the optimization only provides a point estimate and no uncertainty esti-
mates at all.
– The Metropolis–Hastings (MH) based MCMC method turned out to be quite com-
putationally efﬁcient in the simulated examples. However, the tuning of the pro-
posal density can be hard and the performance of the method is likely to be worse
in higher-dimensional problems. Adaptive MCMC methods could be used for
dealing with the tuning problem.
– The Hamiltonian Monte Carlo (HMC) based MCMC had quite much the same
performance as MH based MCMC. Although the autocorrelation lengths of the
MCMC samples of HMC were much shorter than those of MH, the required
computations during the Hamiltonian simulation were quite high, especially
when Runge–Kutta integration of differential equations was used. The tuning
of the leapfrog steps counts and lengths turned out to be quite challenging
as well.
Acknowledgments
We would like to acknowledge Aki Vehtari, Antti Solonen, Jouni Hartikainen, and
Arno Solin for their contributions. We thank the Department of Mathematics and Physics in Lappeenranta
University of Technology, the Department of Biomedical Engineering and Computational Science in Aalto
University and the Academy of Finland for their ﬁnancial support.
123

I. S. Mbalawata et al.
References
Aït-Sahalia Y (2002) Maximum likelihood estimation of discretely sampled diffusions: a closed-form
approximation approach. Econometrica 70(1):223–262
Aït-Sahalia Y (2008) Closed-form expansion for multivariate diffusions. Ann Stat 36(2):906–937
Andrieu C, Thoms J (2008) A tutorial on adaptive MCMC. Stat Comput 18(4):343–373
Andrieu C, Doucet A, Holenstein R (2010) Particle Markov chain Monte Carlo methods. R Stat Soc Ser B
Stat Methodol 72(3):269–342
Berg EVD, Heemink AW, Lin HX, Schoenmakers J (2006) Probability density estimation in stochastic
environmental models using reverse representations. Stoch Environ Res Risk Assess 20(1–2):126–139
Berger JO (1985) Statistical decision theory and Bayesian analysis. Springer, Berlin
Beskos A, Papaspiliopoulos O, Roberts G, Fearnhead P (2006) Exact and computationally efﬁcient
likelihood-based estimation for discretely observed diffusion processes (with discussion). J R Stat Soc
Ser B Stat Methodol 68(3):333–382
Beskos A, Papaspiliopoulos O, Roberts G (2009) Monte Carlo maximum likelihood estimation for discretely
observed diffusion processes. Ann Stat 37(1):223–245
Beskos A, Pillai NS, Roberts GO, Sanz-Serna JM, Stuart AM (2010) Optimal tuning of the hybrid Monte
Carlo algorithm (submitted)
Bishop CM (1996) Neural networks for pattern recognition. Oxford University, Oxford
Brandt M, Santa-Clara P (2002) Simulated likelihood estimation of diffusions with an application to
exchange rate dynamics in incomplete markets. J Financ Econ 63(2):161–210
Butcher JC (2003) Numerical methods for ordinary differential equations. Wiley, New York
Cappé O, Moulines E, Rydén T (2005) Inference in Hidden Markov models. Springer, Berlin
Chen L, Qin Z, Liu JS (2001) Exploring hybrid Monte Carlo in Bayesian computation. In: Proceedings of
ISBA
Doucet A, Godsill SJ, Andrieu C (2000) On sequential Monte Carlo sampling methods for Bayesian ﬁltering.
Stat Comput 10(3):197–208
Duane S, Kennedy AD, Pendleton BJ, Roweth D (1987) Hybrid Monte Carlo. Phys Lett B 195(2):216–222
Elerian O (1998) A note on the existence of a closed form conditional transition density for the Milstein
scheme. Working paper, Nufﬁeld College, Oxford University
Elerian O, Chib S, Shephard N (2001) Likelihood inference for discretely observed non-linear diffusions.
Econometrica 69(4):959–993
Eraker B (2001) MCMC analysis of diffusion models with application to ﬁnance. J Bus Econ Stat 19(2):177–
191
Gelman A, Carlin JB, Stern HS, Rubin DB (2004) Bayesian data analysis. CRC/Chapman and Hall, London
Geyer CJ (1992) Practical Markov chain Monte Carlo. Stat Sci 7(4):473–483
Gilks WR, Richardson S, Spiegelhalter DJ (1996) Markov chain Monte Carlo in practice. Chapman and
Hall/CRC, London
Golightly A, Wilkinson DJ (2006) Bayesian sequential inference for nonlinear multivariate diffusions. Stat
Comput 16(4):323–338
Golightly A, Wilkinson DJ (2008) Bayesian inference for nonlinear multivariate diffusion models observed
with error. Comput Stat Data Anal 52(3):1674–1693
Grewal MS, Andrews AP (2001) Kalman ﬁltering, theory and practice using MATLAB. Wiley-IEEE Press,
New York
Holder T, Leimkuhler B, Reich S (2001) Explicit variable step-size and time-reversible integration. Appl
Numer Math 39(3–4):367–377
Hurn AS, Lindsay KA (1999) Estimating the parameters of stochastic differential equations. Mathe Comput
Simul 48(4–6):373–384
Hurn AS, Lindsay KA, Martin VL (2003) On the efﬁcacy of simulated maximum likelihood for estimating
the parameters of stochastic differential equations. J Time Ser Anal 24(1):45–63
Jazwinski AH (1970) Stochastic processes and ﬁltering theory. Academic Press, Edinburgh
Jeisman J (2005) Estimation of the Parameters of stochastic differential equations. Doctoral dissertation,
Queensland University of Technology
Jensen B, Poulsen R (2002) Transition densities of diffusion processes: numerical comparison of approxi-
mation techniques. J Deriv 9(4):18–32
Jones CS (1998) A simple Bayesian method for the analysis of diffusion processes. Working paper, Uni-
versity of Pennsylvania
123

Parameter estimation in SDEs with MCMC and non-linear KF
Julier SJ, Uhlmann JK (2004) Unscented ﬁltering and nonlinear estimation. Proc IEEE 92(3):401–422
Karatzas I, Shreve SE (1991) Brownian motion and stochastic calculus. Springer, Berlin
Kloeden PE, Platen E (1999) Numerical solution to stochastic differential equations. Springer, Berlin
Liang F, Liu C, Carroll RJ (2010) Advanced Markov chain Monte Carlo methods: learning from past
samples. Wiley, Chichester
Najfeld I, Havel TF (1995) Derivatives of the matrix exponential and their computation. Adv Appl Math
16(3):321–375
Neal RM (2011) MCMC using Hamiltonian dynamics. In: Brooks S, Gelman A, Jones GL, Meng XL (eds)
Handbook of Markov chain Monte Carlo, chap 5. Chapman & Hall/CRC, London
Nielsen JN, Madsen H, Young PC (2000) Parameter estimation in stochastic differential equations: an
overview. SIAM J Numer Anal 24:83–94
Ninness B, Henriksen S (2010) Bayesian system identiﬁcation via Markov chain Monte Carlo techniques.
Automatica 46(1):40–51
Øksendal B (2003) Stochastic differential equations: an introduction with applications. Springer, Berlin
Ornstein LS, Uhlenbeck GE (1930) On the theory of the Brownian motion. Phys Rev 36(5):823–841
Pedersen AR (1995) A new approach to maximum likelihood estimation for stochastic differential equations
based on discrete observations. Scand J Stat 22(1):55–71
Särkkä S (2006) Recursive Bayesian inference on stochastic differential equations. Doctoral dissertation,
Helsinki University of Technology
Särkkä S (2007) On unscented Kalman ﬁltering for state estimation of continuous-time nonlinear systems.
IEEE Trans Autom Control 52(9):1631–1641
Särkkä S, Vehtari A, Lampinen J (2007) Rao-Blackwellized particle ﬁlter for multiple target tracking. Inf
Fusion J 8(1):2–15
Shoji I, Ozaki T (1998) Estimation for nonlinear stochastic differential equations by a local linearization
method. Stoch Anal Appl 16(4):733–752
Singer H (2002) Parameter estimation of nonlinear stochastic differential equations: simulated maximum
likelihood versus extended Kalman ﬁlter and Itô-Taylor expansion. J Comput Graph Stat 11(4):972–995
Singer H (2004) A survey of estimation methods for stochastic differential equastions. In: Proceedings of
6th international conference on social science methodology, Amsterdam
Singer H (2011) Continuous-discrete state-space modeling of panel data with nonlinear ﬁlter algorithms.
AStA Adv Stat Anal 95(4):375–413
Sørensen H (2004) Parametric inference for diffusion processes observed at discrete points in time: a survey.
Int Stat Rev 72(3):337–354
Stramer O, Bognar M (2011) Bayesian inference for irreducible diffusion processes using the pseudo-
marginal approach. Bayesian Anal 6(2):231–258
Stramer O, Bognar M, Schneider P (2010) Bayesian inference for discretely sampled Markov processes
with closed-form likelihood expansions. J Financ Econ 8(4):450–480
Young P (1980) Parameter estimation for continuous-time models: a survey. Automatica 17(1):23–39
123

PUBL. II
Simo Särkkä, Jouni Hartikainen, Isambi S. Mbalawata and Heikki Haario, Posterior Inference
on Parameters of Stochastic Differential Equations via Non-Linear Gaussian Filtering and Adaptive
MCMC, Statistics and Computing
c⃝2014 Statistics and Computing. All rights reserved.
Reprinted, with the permission of Statistics and Computing


Stat Comput
DOI 10.1007/s11222-013-9441-1
Posterior inference on parameters of stochastic differential
equations via non-linear Gaussian ﬁltering and adaptive MCMC
Simo Särkkä · Jouni Hartikainen ·
Isambi Sailon Mbalawata · Heikki Haario
Received: 13 November 2012 / Accepted: 26 November 2013
© Springer Science+Business Media New York 2013
Abstract This article is concerned with Bayesian estima-
tion of parameters in non-linear multivariate stochastic dif-
ferential equation (SDE) models occurring, for example, in
physics, engineering, and ﬁnancial applications. In particu-
lar, we study the use of adaptive Markov chain Monte Carlo
(AMCMC) based numerical integration methods with non-
linear Kalman-type approximate Gaussian ﬁlters for param-
eter estimation in non-linear SDEs. We study the accuracy
and computational efﬁciency of gradient-free sigma-point
approximations (Gaussian quadratures) in the context of pa-
rameter estimation, and compare them with Taylor series
and particle MCMC approximations. The results indicate
that the sigma-point based Gaussian approximations lead to
better approximations of the parameter posterior distribution
than the Taylor series, and the accuracy of the approxima-
tions is comparable to that of the computationally signiﬁ-
cantly heavier particle MCMC approximations.
Keywords Stochastic differential equation · Parameter
estimation · Gaussian approximation · Non-linear Kalman
ﬁlter · Adaptive Markov chain Monte Carlo
1 Introduction
This article is concerned with the use of non-linear Kalman-
type approximate Bayesian ﬁlters in adaptive Markov chain
Electronic supplementary material The online version of this article
(doi:10.1007/s11222-013-9441-1) contains supplementary material,
which is available to authorized users.
S. Särkkä (B) · J. Hartikainen · I.S. Mbalawata · H. Haario
P.O. Box 12200, 00076 Aalto, Finland
e-mail: simo.sarkka@aalto.ﬁ
Monte Carlo (AMCMC) estimation of the unknown param-
eters θ ∈Rd in Itô stochastic differential equations (SDEs,
see, e.g., Øksendal 2003) of the form
dx = f(x,t;θ)dt + L(x,t;θ)dW,
(1)
where x(t) ∈Rn, and W(t) is an s-dimensional vector of
independent standard Wiener processes. Above, f(·) is the
non-linear drift function and L(·) is the dispersion matrix of
the SDE. The assumed model for the measurements yk ∈Rm
is linear Gaussian:
yk = H(θ)x(tk) + rk,
rk ∼N

0,R(θ)

,
(2)
where the matrices H(θ) and R(θ) are deterministic func-
tions of the parameters and the measurement are sampled at
given time steps t1,...,tM. The generalization to non-linear
Gaussian measurement models is straightforward.
The central difﬁculty in the parameter estimation prob-
lem is that the transition density of the SDE in Eq. (1)
cannot be evaluated in closed form. There exists a wide
range of methods for estimating parameters of SDE mod-
els, which circumvent this problem. These include simu-
lated maximum likelihood based methods (Pedersen 1995;
Brandt and Santa-Clara 2002; Hurn et al. 2003), MCMC
based methods (Jones 1998; Elerian et al. 2001; Eraker
2001; Golightly and Wilkinson 2006, 2008; Stramer et al.
2010; Stramer and Bognar 2011) as well as Exact Algorithm
based methods (Beskos et al. 2006, 2009). It is also possible
to directly approximate the solutions of the Fokker–Planck–
Kolmogorov (FPK) equation by using standard numerical
methods for solving PDEs (Hurn and Lindsay 1999; Jensen
and Poulsen 2002; Jeisman 2005) such as Hermite expan-
sions (Aït-Sahalia 2002, 2008). An overview of other meth-
ods can be found in Sørensen (2004).

Stat Comput
In this article, we study another class of methods, which
is based on forming a Gaussian (process) approximation to
the parameter conditioned diffusion process. This approach
is connected to the Taylor series approximations used in the
extended Kalman ﬁlter (EKF, Jazwinski 1970) which is a
widely used approach in guidance, control, target tracking,
and other applications (see, e.g., Bar-Shalom et al. 2001).
Given the Gaussian approximation, it is possible to evalu-
ate the corresponding marginal likelihood of the parameters
and further the (approximate) unnormalized marginal pos-
terior density by using so-called prediction error decompo-
sition (Schweppe 1965). Similar approximations have also
been proposed in the context of linear noise approximations
(LNA) of jump Markov processes and the related master
equations (Kurtz 1970, 1971; Van Kampen 2007; Ferm et al.
2008; Komorowski et al. 2009; Ross et al. 2009).
Although the idea of using Gaussian approximations of
diffusion processes is old (see, e.g., Kushner 1967; Jazwin-
ski 1970), in recent years, new ﬁltering and smoothing algo-
rithms based on Gaussian quadrature integration and sigma-
point methods have been developed (Singer 2008b; Särkkä
2006, 2007, 2010; Arasaratnam et al. 2010; Archambeau
and Opper 2011; Särkkä and Sarmavuori 2013). These new
methods are more accurate than the classical Taylor series
based EKF methods.
Using Gaussian approximations in the context of maxi-
mum likelihood estimation of parameters in SDEs was stud-
ied by Singer (2002), who compared simulated maximum
likelihood, Itô–Taylor series, and Taylor series based EKF
approximations in maximum likelihood (ML) estimation of
parameters. Singer (2011) extended the results and proposed
the use of numerical integration and quadrature type of ap-
proximations instead of the Taylor series based EKF approx-
imation. The use of Markov chain Monte Carlo (MCMC)
methods instead of the ML estimates in the EKF based
SDE parameter estimation framework was recently inves-
tigated by Mbalawata et al. (2013). To improve the perfor-
mance of the basic Metropolis–Hastings based MCMC sam-
pling, Mbalawata et al. (2013) proposed the use of Hamil-
tonian Monte Carlo (HMC) method. Maximum likelihood
estimation in SDE models using Gauss–Hermite quadrature
approximations of diffusions was also recently studied by
Hurn et al. (2013).
In this article, the aim is to study the accuracy and com-
putational requirements of Gaussian approximation based
parameter estimation methods when they are combined with
MCMC methods, and in particular, with adaptive MCMC
methods. More speciﬁcally, we study the use of the recently
developed Gaussian quadrature (sigma-point) based Gaus-
sian approximations developed for continuous-discrete ﬁl-
tering and smoothing in the SDE parameter estimation prob-
lem. We evaluate the effect of the Gaussian state approx-
imations on the parameter posterior distributions by com-
paring them to the grid based “exact” solutions, previously
proposed Taylor series (or LNA) based approximations, and
to particle MCMC approximations (Andrieu et al. 2010; Go-
lightly and Wilkinson 2011).
2 Adaptive Markov chain Monte Carlo methods
Markov chain Monte Carlo (MCMC) methods (see, e.g., Liu
2001) are powerful methods for approximate computation of
expectation integrals of the form
E

g(θ)|YM

=

Rd g(θ)p(θ|YM)dθ,
(3)
where the posterior distribution p(θ|YM) ∝p(YM|θ)p(θ)
is known only up to a normalization constant. This kind of
integrals typically arises in the context of Bayesian com-
putations (Gelman et al. 2004; Bernardo and Smith 1994).
Above, p(YM|θ) is the marginal likelihood of the measure-
ments YM = {y1,...,yM} and p(θ) is the prior distribution
of the parameters θ ∈Rd.
The sampling properties of MCMC heavily depend on
the choice of the proposal distribution and unfortunately,
choosing a suitable one is hard (see, e.g., Liu 2001). In
the Metropolis algorithm, a Gaussian distribution is often
used, in which case the covariance matrix of the Gaus-
sian proposal distribution needs to be well tuned. If the co-
variance is too small or too large, the Markov chains in
MCMC will be highly correlated and hence the estimators
will have a large variance (Andrieu and Thoms 2008). To
overcome this problem we can use adaptive Markov chain
Monte Carlo (AMCMC) methods where the covariance of
the Gaussian proposal in the Metropolis algorithm is au-
tomatically adapted during the MCMC run (Haario et al.
1999, 2001, 2006; Liang et al. 2010; Andrieu and Thoms
2008; Roberts and Rosenthal 2007, 2009; Fort et al. 2011;
Andrieu and Moulines 2006; Atchade and Rosenthal 2005;
Vihola 2012).
In this article, we use the robust adaptive Metropolis
(RAM) algorithm introduced by Vihola (2012), where the
adaptation method is an extension of the algorithm of Haario
et al. (2001). The RAM algorithm of Vihola (2012) is similar
to the adaptive Metropolis algorithm of Haario et al. (2001)
except that the adaptation of the covariance Σi is done in
a slightly different way. RAM also has an additional mech-
anism to reach a given target acceptance rate, for example,
the rate ¯α∗= 0.234, which can be shown to be optimal un-
der certain (ideal) conditions (Roberts and Rosenthal 2001).
The RAM algorithm can be found in the supplementary ma-
terial of this article.

Stat Comput
3 Posterior inference via Bayesian ﬁltering and
smoothing
To estimate the parameters with MCMC methods, we now
want to ﬁnd a way to evaluate the marginal posterior proba-
bility density of the parameters p(θ|YM) ∝p(YM|θ)p(θ)
up to a normalization constant. The prior is usually easy
to evaluate, but in the case of SDE parameter estimation
the difﬁcult part is the evaluation of the marginal likelihood
p(YM|θ), where the states have been integrated out. One
generally applicable approach is to approximate this likeli-
hood by using particle ﬁltering, which leads to so-called par-
ticle Markov chain Monte Carlo (PMCMC) methods (An-
drieu et al. 2010). However, their disadvantage is that they
tend to require a high amount of computational resources as
we will see later in this article.
The marginal likelihood p(YM|θ) can be, in principle,
computed efﬁciently as follows. The classical ﬁltering the-
ory (Jazwinski 1970) states that we can compute the pre-
diction densities p(x(tk)|Yk−1,θ) and the ﬁltering densities
p(x(tk)|Yk,θ) for k = 1,...,T via the following recursive
algorithm:
– Start from the prior p(x(t0)|θ) ≡p(x(t0)|Y0,θ).
– For each measurement yk for k = 1,...,M do:
1. Prediction step: Integrate the FPK equation from the
initial condition p(x(tk−1)|Yk−1,θ) to time tk, which
results in the predicted density p(x(tk)|Yk−1,θ).
2. Update step: Use Bayes’ rule for computing the ﬁlter-
ing density for step k:
p

x(tk)|Yk,θ

=
p(yk|x(tk),θ)p(x(tk)|Yk−1,θ)

p(yk|x(tk),θ)p(x(tk)|Yk−1,θ)dx(tk).
(4)
The ﬁltering results can be used for constructing an expres-
sion for the marginal likelihood as follows. The marginal
likelihood can be factored as:
p(YM|θ) =
M

k=1
p(yk|Yk−1,θ).
(5)
Given the ﬁltering solution, the terms in the product can be
computed recursively as
p(yk|Yk−1,θ) =

p

yk|x(tk),θ

p

x(tk)|Yk−1,θ

dx(tk).
(6)
Once we have obtained the expression for the marginal like-
lihood, we can easily evaluate the posterior distribution up
to the normalization constant, which is all we need for the
implementation of MCMC sampling.
Sometimes we also want to generate samples from the
posterior distribution of the states. Due to the Markov prop-
erties of the state, the conditional distribution of the state
given all the measurements obeys the following backward-
simulation (smoothing) recursion (cf. Godsill et al. 2004):
p

x(tk)|x(tk+1),YM,θ

= p(x(tk+1)|x(tk),θ)p(x(tk)|Yk,θ)
p(x(tk+1)|Yk,θ)
,
(7)
where the right hand side only involves results from ﬁltering.
Thus once we have computed the ﬁltering distributions, we
can simulate posterior trajectories x(t0),...,x(tM) using the
above recursion. Although we have formulated the above re-
cursion only for the states at the measurement times, we can
easily augment additional times to the trajectory and obtain
samples from them as well.
Multiplying the above equation with p(x(tk+1)|YM,θ)
and integrating over x(tk+1) gives the following backward
Bayesian optimal smoothing recursion (Kitagawa 1987):
p

x(tk)|YM,θ

= p

x(tk)|Yk,θ

×
 p(x(tk+1)|x(tk),θ)p(x(tk+1)|YM,θ)
p(x(tk+1)|Yk,θ)
dx(tk+1),
(8)
which can be used for computing the posterior marginals
of x(tk) (i.e., the smoothing distributions) for k = M −1,
...,0.
4 Gaussian ﬁltering and smoothing based
approximation
In this section, we show how to form Gaussian approxima-
tions to the general recursion equations given in the previous
section using the recent numerical approximation methods
developed for state estimation (e.g., Särkkä and Solin 2012;
Särkkä and Sarmavuori 2013). We derive the approxima-
tions directly using results from Itô calculus which makes
the underlying approximations more explicit than the typ-
ical Euler–Maruyama discretization based derivation (e.g.,
Singer 2011). We also derive equations for sampling trajec-
tories from the approximate full smoothing posterior.
Assume that after the step k −1 at time tk−1 we have a
Gaussian approximation to the ﬁltering distribution as fol-
lows:
p

x(tk−1)|Yk−1,θ

≈N

x(tk−1)|m(tk−1),P(tk−1)

,
(9)
with some mean m(tk−1) and covariance P(tk−1). We are
now interested in what happens to this distribution in the

Stat Comput
evolution from time tk−1 to time tk. Because we are inter-
ested in forming a Gaussian approximation, we are speciﬁ-
cally interested in knowing the mean and covariance of the
resulting distribution. Note that the mean and covariance are
actually functions of the parameter θ as well, but we have
dropped that dependence for notational convenience.
By using the generator of the SDE (e.g., Øksendal 2003)
we get the following identity of an arbitrary function g:
E
	dg
dt


Yk−1,θ

= E
	
j
∂g
∂xj
fj(x,t)|Yk−1,θ

+ 1
2E
	
ij

L(x,t;θ)LT (x,t;θ)

ij
∂2g
∂xi∂xj


Yk−1,θ

.
(10)
We are interested in the mean and covariance of the solu-
tion and so we ﬁrst set g(x) = xi, giving rise to a solution
mi, and then set g(x) = (xi −mi)(xj −mj), which leads
to the following ordinary differential equations (ODEs) for
the conditional mean m(t) = E[x(t)|Yk−1,θ] and covari-
ance P(t) = E[(x(t) −m(t))(x(t) −m(t))T |Yk−1,θ]:
dm(t)
dt
= E

f(x,t;θ)|Yk−1,θ

dP(t)
dt
= E

x −m(t)

fT (x,t;θ)|Yk−1,θ

+ E

f(x,t;θ)

x −m(t)
T |Yk−1,θ

+ E

L(x,t;θ)LT (x,t;θ)|Yk−1,θ

.
(11)
Note that the expectations above are taken with respect to the
conditional distribution of the state p(x(t)|Yk−1,θ). Thus
the equations are not true ODEs in the sense that they cannot
be solved without ﬁrst solving the full FPK partial differen-
tial equation (Jazwinski 1970).
We now employ a simple but widely used approximation
of x(t) as a Gaussian process such that at every time step
t ∈[tk−1,tk) we have:
p

x(t)|Yk−1,θ

≈N

x(t)|m(t),P(t)

,
(12)
where m(t) and P(t) are the mean and covariance of the
process x(t), respectively. That is, we replace the expected
values with respect to the true distribution of x(t) with ex-
pectations over the Gaussian approximation. The approxi-
mations to the mean and covariance integrals can then be
evaluated numerically. One way is to use Taylor expansions
as in extended Kalman ﬁlter (EKF) (Jazwinski 1970):
f(x,t;θ) ≈f(m,t;θ) + Fx(m,t;θ)(x −m)
L(x,t;θ) ≈L(m,t;θ),
(13)
where Fx is the Jacobian matrix of the mapping x →
f(x,t;θ), which gives the Taylor series or LNA approxima-
tions (e.g., Mbalawata et al. 2013; Singer 2002; Ferm et al.
2008):
dm
dt = f(m,t;θ)
dP
dt = PFT
x (m,t;θ) + Fx(m,t;θ)P
+ L(m,t;θ)LT (m,t;θ).
One particularly useful class of methods are Gaussian
quadrature methods, also called sigma-point methods, which
approximate the Gaussian integrals in the following way:

g(x)N(x|m,P)dx ≈
Nξ

i=1
W (i)g

m +
√
Pξ(i)
,
(14)
where ξ(i) are a suitable set of method speciﬁc unit sigma
points and W (i) are some deterministic weights. The ma-
trix square root is any matrix, which satisﬁes
√
P
√
P
T = P.
With this approximation the mean and covariance equations
become (Särkkä and Sarmavuori 2013):
dm
dt =

i
W (i)f(m +
√
Pξi,t;θ)
dP
dt =

i
W (i)f(m +
√
Pξi,t;θ)ξT
i
√
P
T
+

i
W (i)√
PξifT (m +
√
Pξi,t;θ)
+

i
W (i)L

m+
√
Pξ(i),t;θ

LT 
m+
√
Pξ(i),t;θ

.
One useful sigma-point method is the 3rd order spherical cu-
bature rule (“cubature” is just another term for multidimen-
sional quadrature) used in the cubature Kalman ﬁlter (CKF)
of Arasaratnam et al. (2010), which uses Nξ = 2n unit sigma
points deﬁned as
ξi =
√nei,
i = 1,...,n
−√nei−n,
i = n + 1,...,2n,
(15)
where ei denotes the unit vector to the direction of coordi-
nate axis i, and the weights are deﬁned as W (i) = 1/(2n) for
i = 1,...,2n. The 3rd order spherical cubature rule can be
seen as a special case of the unscented transform used in the
unscented Kalman ﬁlter (UKF) (Julier et al. 2000; Särkkä
2007). The third order here means that it integrates correctly
any linear combination of up to third order monomials (e.g.,
x1x2x3, x2
1x2, x3
1).

Stat Comput
One may also form the rule (14) as a Cartesian product of
Gauss–Hermite quadratures as is done in the Gauss–Hermite
ﬁlters (Ito and Xiong 2000; Singer 2008b). This leads to
methods which are able to integrate linear combinations of
higher order polynomials correctly.
By integrating the mean and covariance equations from
the initial conditions m(tk−1) and P(tk−1) to time tk, we get
the following Gaussian approximation to the solution of the
FPK equation:
p

x(tk)|Yk−1,θ

≈N

x(tk)|m

t−
k

,P

t−
k

.
(16)
Above, the results of prediction are denoted as m(t−
k ),
P(t−
k ), where the minus at superscript means “inﬁnitesi-
mally before the time tk”. We use this notation to distinguish
these from m(tk), P(tk), which denote the mean and covari-
ance of the Gaussian approximation to p(x(tk)|Yk,θ).
Because of the Gaussianity of the above distribution and
the measurement model, Bayes’ rule in Eq. (4) gives
p

x(tk)|Yk,θ

≈N

x(tk)|m(tk),P(tk)

,
(17)
where the mean m(tk) and covariance P(tk) are given by the
standard Kalman ﬁlter update equations (see the supplemen-
tary material).
The conditional marginal likelihood in Eq. (6) is also
Gaussian and can be evaluated simply as:
p(yk|Yk−1,θ) ≈N

yk|H(θ)m

t−
k

,Sk

,
(18)
where Sk = H(θ)P(t−
k )HT (θ) + R(θ). The prediction error
decomposition (5) then gives:
p(YM|θ) ≈
M

k=1
N

yk|H(θ)m

t−
k

,Sk

.
(19)
For the smoothing recursions, we can utilize the results de-
rived by Särkkä and Sarmavuori (2013), who give the equa-
tions for the approximate means ms(tk) and covariances
Ps(tk) of the (marginal) smoothing distributions in Eq. (8).
The equations can be found in the supplementary material.
Thus we approximately have
p

x(tk)|YM,θ

≈N

x(tk)|ms(tk),Ps(tk)

.
(20)
The backward-simulation recursion in Eq. (7) can be ob-
tained
by
formally
setting
m(tk+1) ←x(tk+1)
and
P(tk+1) ←0 in the smoothing recursions, which gives:
p

x(tk)|x(tk+1),YM,θ

≈N

x(tk)|m(tk) + Gk+1

x(tk+1) −m

t−
k+1

,
P(tk) −Gk+1P

t−
k+1

GT
k+1

.
(21)
5 Experimental evaluation
5.1 Ginzburg–Landau double well potential
As the ﬁrst numerical example we consider a diffusion pro-
cess in a double well potential, described by the Ginzburg–
Landau equation, which is a popular benchmark model for
comparing different approximation schemes (e.g., Singer
2002). The Ginzburg–Landau equation reads
dx = −

αx + βx3
dt + σdW,
(22)
where θ = (α,β,σ) are the parameters to be estimated from
data. The Ginzburg–Landau model corresponds to a diffu-
sion at the double-well potential Φ(x,θ) = α
2 x2 + β
4 x4, with
minima at ±√−α/β when α is negative. We measure the
equation with a sampling period t from the model
yk = x(tk) + rk,
rk ∼N(0,R).
(23)
We aim to compare the performance of Taylor series,
3rd order spherical cubature rule, Gauss–Hermite quadra-
ture in forming the Gaussian state approximation, and com-
bine them with the adaptive MCMC method (RAM) of (Vi-
hola 2012). As the particle MCMC method we use the parti-
cle marginal Metropolis–Hastings (PMMH) (Andrieu et al.
2010) with 5000 particles, which we also combine with the
RAM adaptation method. This number of particles was cho-
sen on the basis that the method of Doucet et al. (2012) (with
the parameters ﬁxed to true ones) suggests that 500 particles
should be enough for this model—we chose still to multiply
this number by ten. In the particle ﬁlter of PMMH we use
the dynamic model as the importance distribution, which has
the advantage that this way the evaluation of the transition
density is not needed (Golightly and Wilkinson 2011). The
trajectories of the SDE were simulated using the stochas-
tic Runge–Kutta method (Kloeden and Platen 1999; Rößler
2006).
For comparison, we also implemented a benchmark ﬁl-
tering and smoothing solution, where the transition density
is numerically solved from the FPK by using ﬁnite differ-
ences method on a relatively dense grid (the range [−7,7]
with x = 1/10), and the Bayesian ﬁltering and smoothing
equations were approximated on the same grid via direct nu-
merical integration. This provides a quite accurate “ground
truth” solution.
We simulated a state trajectory with this model using
the parameters θ = (−1,0.1,2) on time interval t ∈[0,40]
and generated observations every t = 2 with variance R =
0.12. With these parameter settings the potential has min-
ima at ±
√
10, which are also the modes of the stationary
(bi-modal) state distribution.
To evaluate the accuracies of different approximations in
estimating the posterior distributions of the parameters we

Stat Comput
Fig. 1 Ginzburg–Landau: conditional posteriors of each parameter.
Here, light gray shade denotes the true distribution (FP-based solu-
tion), black line the particle MCMC estimate, light gray line the Tay-
lor series-based estimate, dotted line Gauss–Hermite quadrature and
dashed line spherical cubature. As can be seen, Taylor series-based
method is clearly inconsistent with the true posterior, while the other
methods are slightly off, but still much closer to the truth
calculated the conditional posterior distribution of each pa-
rameter in a one-dimensional grid while keeping the other
parameters ﬁxed to true values. These are plotted in Fig. 1,
where we have also plotted the estimate provided by the par-
ticle MCMC method. As can be seen in the ﬁgure, the con-
ditional distribution approximations provided by the Taylor
series approach are quite far away from the true posteri-
ors. The approximations are somewhat sensitive to particu-
lar realization of the state trajectory and measurements, but
a similar phenomenon is observed each time. With the other
methods the distribution approximations are much closer to
the true ones although they also differ slightly from the true
distribution. From the ﬁgure one can also see the stochastic-
ity of the particle MCMC estimate, which is on average the
same as the one provided by the numerical FPK solution.
We ran MCMC sampling with RAM algorithm on the un-
known parameters while using the spherical cubature, Tay-
lor series and particle MCMC based approximations. Esti-
mates of the parameters are visualized in Fig. 2 as pair-wise
marginal distributions. The FPK method was not used, be-
cause its computational requirements are far too huge to
be used in MCMC sampling. As was the case with con-
ditional distributions above, Taylor series-based method is
not able to approximate the parameter posterior distribution
correctly. Even though the parameter posteriors are heav-
ily non-Gaussian, the cubature based method approximates
the shape of true posterior distribution surprisingly well (as-
suming that PMCMC is close to the truth). The ﬁgure also
shows the maximum a posteriori (MAP) estimates of param-
eters obtained with each of the methods separately, as well
as the PMCMC-based MAP estimate, which can assumed to
be close to the exact MAP estimate.
5.2 Van der Pol oscillator
As a higher dimensional non-linear example we consider pa-
rameter estimation in the Van der Pol oscillator model (see,
e.g., Kandepu et al. 2008), which is described by the second
order non-linear ODE
d2x(t)
dt2
−μ

1 −ϵx2(t)
dx(t)
dt
+ ω2x(t) = f (t),
(24)
where the coefﬁcients μ and ϵ as well as the angular velocity
ω are unknown parameters. Above, we have also included an
additional unknown forcing term f (t).
We model the forcing term f (t) as a sum of white noise
and a stochastic resonator c(t), which is formed as a sum of
N harmonic components cn(t),n = 1,...,N:
d2cn(t)
dt2
= −(nωc)2cn(t) + σnϵn(t),
(25)
where ωc is the angular velocity of the force process, σn the
strength of the noise process driving the nth harmonic, and
ϵn(t) is a white noise process. For simplicity, we assume
here that σn = σc for all n = 1,...,N. Thus, the param-
eter vector θ = (ϵ,μ,ω,σ,ωc,σc) is six dimensional, and
the state variable x = (x, ˙x,c1, ˙c1,...,cN, ˙cN) has 2 + 2N
components. The full SDE model is of the form
dx(t) = ˙x(t)dt
d˙x(t) = μ

1 −ϵx2(t)

˙x(t)dt −ω2x(t)dt
+

c1(t) + ··· + cN(t)

dt
dcn(t) = ˙cn(t)dt
d˙cn(t) = −(ωc)2cn(t)dt + σcdWn(t),
n = 1,...,N.
(26)
The measurement is the state of the Van der Pol oscillator
plus noise:
yk = x(tk) + rk,
rk ∼N(0,R),
(27)
which corresponds to H = (1 0 ... 0) in Eq. (2).
We simulated the system with parameters
θ = (1,1/2,1,1/100,π/5,1/100)
on the time interval t ∈[0,40] and generated observations
with sampling period t = 1 and variance R = 1/102. We

Stat Comput
Fig. 2 Ginzburg–Landau: MCMC estimates of parameters. The pan-
els show the pair-wise marginal distributions of α,β and σ. The results
for β and σ are shown in log-space since that was used in sampling.
Crosses (×) in the panels denote the MAP-estimates obtained from
PMCMC (which should be close to the true MAP-estimates) while
pluses (+) denote the MAP estimates of each of the methods
used N = 2 harmonic components in the resonator c(t).
Given the simulated measurements, we ran MCMC sam-
pling with the RAM algorithm on the parameters with
spherical cubature, Taylor series, and particle ﬁlter (parti-
cle MCMC) with 5000 particles approximating the marginal
likelihood. This number of particles was chosen using the
criterion proposed by Doucet et al. (2012). We did not test
the Gauss–Hermite quadrature based method, because of
its high computational requirements due to the exponential
scaling of the number of computations in the state dimen-
sionality (i.e., 7th order method would need 76 = 117649
sigma-points). The FPK method is not feasible either, be-
cause it scales exponentially in the state dimensions and as
stated in the previous section, it is too expensive for MCMC
even in one dimension.
Figure 3 shows the pair-wise marginals of each of the
parameters. It can be seen that the parameters ϵ and μ are
highly correlated with each other. The parameters ω and
ωc seem to be quite well identiﬁed with all the methods,
while the parameters σ and σc do not identify as well. The
posterior approximation of the latter parameters also shows
the MCMC method has difﬁculties in sampling of these pa-
rameters under the Gaussian approximations. However, the
shapes of all the parameter posteriors are still well captured
by all the methods.
5.3 Computational requirements
In this section, we evaluate the computational requirements
of the methods used in the Van der Pol example with varying
number of oscillator components N. The computational re-
quirements of the different methods in parameter estimation
heavily depend on the underlying algorithm used for eval-
uation of the marginal likelihood. They in turn depend on

Stat Comput
Fig. 3 Van der Pol Oscillator: MCMC estimates of parameters. The
panels show the pair-wise marginal distributions of the parameters pro-
vided by the spherical cubature, Taylor series, and PMCMC methods.
Crosses (×) in the panels denote the MAP-estimates obtained from
PMCMC (assumed to be close to the true MAP-estimates) and pluses
(+) denote the MAP estimates of each of the methods
the number of evaluation points (i.e., the number of sigma-
points in the cubature method and particles in the particle
ﬁlter), which in turn depends on the dimensionality of the
state. The scaling of all the methods with the state dimen-
sionality is roughly quadratic and they scale roughly linearly
in the number of evaluation points. However, the overhead
and the proportionality constants are quite different in each
of the methods.
We start by investigating the number of evaluation points
needed in each of the methods. We used the criterion of
Doucet et al. (2012) for determining the number of particles
for PMCMC (the likelihood were evaluated at the true pa-
rameter values). These and the number of evaluation points
for the cubature method are shown in Table 1. The corre-
sponding number of evaluations for the Taylor series is one
although the proportionality constant is quite high. This re-
veals one challenge in PMCMC—the required number of
particles seems to grow exponentially with the state dimen-
Table 1 The number of evaluation points needed in the Van der Pol
oscillator model as function of oscillator components N
N
PMCMC particles
Cubature sigma-points
0
500
4
1
2000
8
2
5000
12
3
50000
16
4
100000
20
sion, which indeed is a well-known problem in the context of
particle ﬁltering (Snyder et al. 2008). In contrast, the number
of evaluations need by the cubature method grows linearly
with the number of dimensions.
The numbers of evaluation points in Table 1 do not di-
rectly give the computation time needed by the methods. To
evaluate the practical speed we measured the times need for

Stat Comput
Fig. 4 Computation times as function of the number of oscillator com-
ponents in the Van der Pol model. The computational requirements
of the particle approximation (PMCMC) are signiﬁcantly higher than
those of cubature and Taylor series approximations
a single evaluation of the likelihood function with each of
the methods. It is worth noting that as the evaluation time
was measured using MATLAB, due to implementation de-
tails, the results slightly favor the particle ﬁlter. This would
change if, for example, a C++ implementation were used.
The measured times of the likelihood evaluations are
shown in Fig. 4. It can be seen that the computation time
required by the particle-based method in PMCMC is much
higher than the computation times of the cubature and Tay-
lor based methods—despite the fact that the implementation
favors the particle-based method. The steep scaling of the
computational requirements in the particle approximation is
mainly due to the almost exponential growth of the required
number of particles (given in Table 1). However, without
increasing the number of particles with the state dimension-
ality the PMCMC would not give a reasonable result at all.
6 Conclusion and discussion
In this article, we have studied the use of quadrature or
sigma-point based non-linear Kalman-type Gaussian ﬁlters
with adaptive Markov chain Monte Carlo (AMCMC) meth-
ods in approximate full Bayesian estimation of parameters
in partially observed non-linear stochastic differential equa-
tions (SDEs). In particular, we have tested the accuracy
and computational requirements of sigma-point based ap-
proximation methods against Taylor series, grid, and parti-
cle MCMC based methods. The results indicate that sigma-
point based Gaussian approximations provide a surprisingly
accurate approximation of the parameter posterior distribu-
tion, while the methods are computationally light-weight.
The advantage of the Gaussian approximation based
methods over many other methods is that although the model
family is quite general, due to the utilization of the Gaus-
sian state approximations, the required computations remain
light. The computations are order of magnitude lighter when
compared to, for example, the particle MCMC method. The
use of sigma-point methods makes the method black-box in
the sense that to implement the method, we do not need
to compute derivatives, closed form expectations or any
other such quantities—being able to evaluate the drift, dif-
fusion and measurement model functions is sufﬁcient. Fur-
thermore, the sigma-point approximations are more accurate
than Taylor series approximations, and the use of AMCMC
makes the manual tuning of proposal distributions unneces-
sary.
The main weakness of the methods is that they use a
Gaussian approximation for the state and thus might not
produce accurate results when the state posterior is strongly
non-Gaussian. However, as the our experiments show, the
results can be accurate even when the SDE is strongly non-
linear. The non-Gaussianity of the parameter posterior is not
a problem as such (cf. Fig. 2), because it is approximated
using the MCMC method. The method also works well with
the multivariate non-linear Van Der Pol SDE model. The
results are also in good agreement with PMCMC method
which produces an asymptotically exact Monte Carlo ap-
proximation of the parameter posterior, but often requires
a large number of particles.
An important point to note is that the analysis of com-
putational requirements in Sect. 5.3 is based on the implicit
assumption that the accuracy of the spherical cubature rule
remains the same regardless of the state dimensionality, pro-
vided that we add sigma-points with the linear rule. It is pos-
sible (and likely) that this is not strictly true. Although Gaus-
sian approximations tend to be robust to the state dimension-
ality, it is possible that in some cases the exponential scaling
of the space causes the integration rule to become inaccu-
rate. However, theoretical analysis of this phenomenon is
hard and heavily depends on the problem at hand.
Another thing affecting the computational requirements
in practice is also the speed of mixing of the MCMC sam-
pling. This is turn also depends on the target acceptance rate
which is used in the AMCMC algorithm. For (A)PMCMC
it is likely that the choice ¯α∗= 0.234 which we used is not
optimal. However, this choice does not affect the efﬁciency
comparisons here.
The Gaussian approximation based methods presented in
this article could be easily extended to non-linear measure-
ment models simply by replacing the update step with the
non-linear Gaussian ﬁlter update step (Ito and Xiong 2000;
Wu et al. 2006; Särkkä and Sarmavuori 2013). Instead of the
ﬁrst order Taylor series (or LNA) it would also be possible
to use higher order expansions (see, e.g., Jazwinski 1970;
Maybeck 1982; Ferm et al. 2008), which might sometimes
work better than the ﬁrst order expansion. However, unlike
the sigma-point methods, they require closed form evalua-
tion of the derivatives of the functions and thus are not black-
box in that sense. It would also be possible to use more than

Stat Comput
the ﬁrst two moments in the SDE approximation (cf. Jazwin-
ski 1970; Maybeck 1982; Socha 2008; Singer 2008a).
7 Supplementary materials
Supplementary materials are available. Appendix A contains
the full RAM algorithm, and the Kalman ﬁlter update and
non-linear continuous-discrete Gaussian smoothing equa-
tions are given in Appendix B.
Acknowledgements
The authors would like to thank Marko Laine
for his valuable help in AMCMC methods, and the anonymous referees
for their suggestions for improving the paper.
References
Aït-Sahalia, Y.: Maximum likelihood estimation of discretely sampled
diffusions: a closed-form approximation approach. Econometrica
70(1), 223–262 (2002)
Aït-Sahalia, Y.: Closed-form expansion for multivariate diffusions.
Ann. Stat. 36(2), 906–937 (2008)
Andrieu, C., Moulines, E.: On the ergodicity properties of some adap-
tive MCMC algorithms. Ann. Appl. Probab. 16(3), 1462–1505
(2006)
Andrieu, C., Thoms, J.: A tutorial on adaptive MCMC. Stat. Comput.
18(4), 343–373 (2008)
Andrieu, C., Doucet, A., Holenstein, R.: Particle Markov chain Monte
Carlo methods. J. R. Stat. Soc., Ser. B, Stat. Methodol. 72(3),
269–342 (2010)
Arasaratnam, I., Haykin, S., Hurd, T.R.: Cubature Kalman ﬁltering for
continuous-discrete systems: theory and simulations. IEEE Trans.
Signal Process. 58(10), 4977–4993 (2010)
Archambeau, C., Opper, M.: Approximate inference for continuous-
time Markov processes. In: Inference and Learning in Dynamic
Models. Cambridge University Press, Cambridge (2011)
Atchade, Y.F., Rosenthal, J.S.: On adaptive Markov chain Monte Carlo
algorithms. Bernoulli 11(5), 815–828 (2005)
Bar-Shalom, Y., Li, X.R., Kirubarajan, T.: Estimation with Applica-
tions to Tracking and Navigation. Wiley, New York (2001)
Bernardo, J.M., Smith, A.F.M.: Bayesian Theory. Wiley, New York
(1994)
Beskos, A., Papaspiliopoulos, O., Roberts, G., Fearnhead, P.: Exact
and computationally efﬁcient likelihood-based estimation for dis-
cretely observed diffusion processes (with discussion). J. R. Stat.
Soc., B 68(3), 333–382 (2006)
Beskos, A., Papaspiliopoulos, O., Roberts, G.: Monte Carlo maximum
likelihood estimation for discretely observed diffusion processes.
Ann. Stat. 37(1), 223–245 (2009)
Brandt, M., Santa-Clara, P.: Simulated likelihood estimation of diffu-
sions with an application to exchange rate dynamics in incomplete
markets. J. Financ. Econ. 63(2), 161–210 (2002)
Doucet, A., Pitt, M., Kohn, R.: Efﬁcient implementation of Markov
chain Monte Carlo when using an unbiased likelihood estimator
(2012). arXiv:1210.1871
Elerian, O., Chib, S., Shephard, N.: Likelihood inference for dis-
cretely observed non-linear diffusions. Econometrica 69(4), 959–
993 (2001)
Eraker, B.: MCMC analysis of diffusion models with application to
ﬁnance. J. Bus. Econ. Stat. 19(2), 177–191 (2001)
Ferm, L., Lötstedt, P., Hellander, A.: A hierarchy of approximations of
the master equation scaled by a size parameter. J. Sci. Comput.
34(2), 127–151 (2008)
Fort, G., Moulines, E., Priouret, P.: Convergence of adaptive MCMC
algorithms: ergodicity and law of large number. Tech. Rep., LTCI
(2011)
Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B.: Bayesian Data
Analysis, 2nd edn. CRC Press/Chapman & Hall, Boca Ra-
ton/London (2004)
Godsill, S.J., Doucet, A., West, M.: Monte Carlo smoothing for non-
linear time series. J. Am. Stat. Assoc. 99(465), 156–168 (2004)
Golightly, A., Wilkinson, D.J.: Bayesian sequential inference for
nonlinear multivariate diffusions. Stat. Comput. 16(4), 323–338
(2006)
Golightly, A., Wilkinson, D.J.: Bayesian inference for nonlinear multi-
variate diffusion models observed with error. Comput. Stat. Data
Anal. 52(3), 1674–1693 (2008)
Golightly, A., Wilkinson, D.J.: Bayesian parameter inference for
stochastic biochemical network models using particle Markov
chain Monte Carlo. Interface Focus 1(6), 807–820 (2011)
Haario, H., Saksman, E., Tamminen, J.: Adaptive proposal distribution
for random walk Metropolis algorithm. Comput. Stat. 14(3), 375–
395 (1999)
Haario, H., Saksman, E., Tamminen, J.: An adaptive Metropolis algo-
rithm. Bernoulli 7(2), 223–242 (2001)
Haario, H., Laine, M., Mira, A., Saksman, E.: DRAM: efﬁcient adap-
tive MCMC. Stat. Comput. 16(4), 339–354 (2006)
Hurn, A.S., Lindsay, K.A.: Estimating the parameters of stochastic
differential equations. Math. Comput. Simul. 48(4–6), 373–384
(1999)
Hurn, A.S., Lindsay, K.A., Martin, V.L.: On the efﬁcacy of simulated
maximum likelihood for estimating the parameters of stochastic
differential equations. J. Time Ser. Anal. 24(1), 45–63 (2003)
Hurn, A.S., Lindsay, K.A., McClelland, A.J.: A quasi-maximum like-
lihood method for estimating the parameters of multivariate dif-
fusions. J. Econom. 172, 106–126 (2013)
Ito, K., Xiong, K.: Gaussian ﬁlters for nonlinear ﬁltering problems.
IEEE Trans. Autom. Control 45(5), 910–927 (2000)
Jazwinski, A.H.: Stochastic Processes and Filtering Theory. Academic
Press, San Diego (1970)
Jeisman, J.: Estimation of the Parameters of Stochastic Differential
Equations. Doctoral dissertation, Queensland University of Tech-
nology (2005)
Jensen, B., Poulsen, R.: Transition densities of diffusion processes: nu-
merical comparison of approximation techniques. J. Deriv. 9(4),
18–32 (2002)
Jones, C.S.: A simple Bayesian method for the analysis of diffusion
processes. Working paper, University of Pennsylvania (1998)
Julier, S.J., Uhlmann, J.K., Durrant-Whyte, H.F.: A new method for the
nonlinear transformation of means and covariances in ﬁlters and
estimators. IEEE Trans. Autom. Control 45(3), 477–482 (2000)
Kandepu, R., Foss, B., Imsland, L.: Applying the unscented Kalman
ﬁlter for nonlinear state estimation. J. Process Control 18(7–8),
753–768 (2008)
Kitagawa, G.: Non-Gaussian state-space modeling of nonstationary
time series. J. Am. Stat. Assoc. 82(400), 1032–1041 (1987)
Kloeden, P.E., Platen, E.: Numerical Solution to Stochastic Differential
Equations. Springer, Berlin (1999)
Komorowski, M., Finkenstädt, B., Harper, C., Rand, D.: Bayesian in-
ference of biochemical kinetic parameters using the linear noise
approximation. BMC Bioinform. 10(1), 343 (2009)
Kurtz, T.G.: Solutions of ordinary differential equations as limits of
pure jump Markov processes. J. Appl. Probab. 7(1), 49–58 (1970)
Kurtz, T.G.: Limit theorems for sequences of jump Markov processes
approximating ordinary differential processes. J. Appl. Probab.
8(2), 344–356 (1971)

Stat Comput
Kushner, H.J.: Approximations to optimal nonlinear ﬁlters. IEEE
Trans. Autom. Control 12(5), 546–556 (1967)
Liang, F., Liu, C., Carroll, R.J.: Advanced Markov Chain Monte Carlo
Methods: Learning from Past Samples. Wiley, Chichester (2010)
Liu, J.S.: Monte Carlo Strategies in Scientiﬁc Computing. Springer,
Berlin (2001)
Maybeck, P.: Stochastic Models, Estimation and Control vol. 2. Aca-
demic Press, San Diego (1982)
Mbalawata, I.S., Särkkä, S., Haario, H.: Parameter estimation in
stochastic differential equations with Markov chain Monte Carlo
and non-linear Kalman ﬁltering. Comput. Stat. 28(3), 1195–1223
(2013)
Øksendal, B.: Stochastic Differential Equations: An Introduction with
Applications. Springer, Berlin (2003)
Pedersen, A.R.: A new approach to maximum likelihood estimation for
stochastic differential equations based on discrete observations.
Scand. J. Stat. 22(1), 55–71 (1995)
Roberts,
G.O.,
Rosenthal,
J.S.:
Optimal
scaling
for
various
Metropolis–Hastings algorithms. Stat. Sci. 16(4), 351–367
(2001)
Roberts, G.O., Rosenthal, J.S.: Coupling and ergodicity of adaptive
Markov chain Monte Carlo algorithms. Applied probability 44(2),
458–475 (2007)
Roberts, G.O., Rosenthal, J.S.: Examples of adaptive MCMC. J. Com-
put. Graph. Stat. 18(2), 349–367 (2009)
Ross, J.V., Pagendam, D.E., Pollett, P.K.: On parameter estimation in
population models II: multi-dimensional processes and transient
dynamics. Theor. Popul. Biol. 75(2), 123–132 (2009)
Rößler, A.: Runge-Kutta methods for Ito stochastic differential equa-
tions with scalar noise. BIT Numer. Math. 46, 97–110 (2006)
Särkkä, S.: Recursive Bayesian inference on stochastic differential
equations. Doctoral dissertation, Helsinki University of Technol-
ogy (2006)
Särkkä, S.: On unscented Kalman ﬁltering for state estimation of
continuous-time nonlinear systems. IEEE Trans. Autom. Control
52(9), 1631–1641 (2007)
Särkkä, S.: Continuous-time and continuous-discrete-time unscented
Rauch-Tung-Striebel smoothers. Signal Process. 90(1), 225–235
(2010)
Särkkä, S., Sarmavuori, J.: Gaussian ﬁltering and smoothing for
continuous-discrete dynamic systems. Signal Process. 93, 500–
510 (2013)
Särkkä, S., Solin, A.: On continuous-discrete cubature Kalman ﬁlter-
ing. In: Proceedings of SYSID 2012, pp. 1210–1215 (2012)
Schweppe, F.C.: Evaluation of likelihood functions for Gaussian sig-
nals. IEEE Trans. Inf. Theory 11, 61–70 (1965)
Singer, H.: Parameter estimation of nonlinear stochastic differen-
tial equations: simulated maximum likelihood versus extended
Kalman ﬁlter and Itô-Taylor expansion. J. Comput. Graph. Stat.
11, 972–995 (2002)
Singer, H.: Generalized Gauss-Hermite ﬁltering. AStA Adv. Stat.
Anal. 92(2), 179–195 (2008a)
Singer, H.: Nonlinear continuous time modeling approaches in panel
research. Stat. Neerl. 62(1), 29–57 (2008b)
Singer, H.: Continuous-discrete state-space modeling of panel data
with nonlinear ﬁlter algorithms. AStA Adv. Stat. Anal. 95(4),
375–413 (2011)
Snyder, C., Bengtsson, T., Bickel, P., Anderson, J.: Obstacles to high-
dimensional particle ﬁltering. Mon. Weather Rev. 136(12), 4629–
4640 (2008)
Sørensen, H.: Parametric inference for diffusion processes observed at
discrete points in time: a survey. Int. Stat. Rev. 72(3), 337–354
(2004)
Socha, L.: Linearization Methods for Stochastic Dynamic Systems.
Springer, Berlin (2008)
Stramer, O., Bognar, M.: Bayesian inference for irreducible diffusion
processes using the pseudo-marginal approach. Bayesian Anal.
6(2), 231–258 (2011)
Stramer, O., Bognar, M., Schneider, P.: Bayesian inference for dis-
cretely sampled Markov processes with closed-form likelihood
expansions. J. Financ. Econ. 8(4), 450–480 (2010)
Van Kampen, N.G.: Stochastic processes in physics and chemistry, 3rd
edn. Elsevier, Amsterdam (2007)
Vihola, M.: Robust adaptive Metropolis algorithm with coerced accep-
tance rate. Stat. Comput. 22(5), 997–1008 (2012)
Wu, Y., Hu, D., Wu, M., Hu, X.: A numerical-integration perspec-
tive on Gaussian ﬁlters. IEEE Trans. Signal Process. 54(8), 2910–
2921 (2006)

Statistics and Computing manuscript No.
(will be inserted by the editor)
Supplementary Material for “Posterior Inference on
Parameters of Stochastic Differential Equations via
Non-Linear Gaussian Filtering and Adaptive MCMC”
Simo S¨arkk¨a · Jouni Hartikainen · Isambi
Sailon Mbalawata · Heikki Haario
Received: date / Accepted: date
A RAM Algorithm
If we let ϕ(·) be the negative logarithm of the unnormalized posterior probability density:
ϕ(θ) = −ln p(YM |θ)−ln p(θ),
(1)
then the RAM algorithm of Vihola (2012) can be written in the following form:
1. Draw θ0 from an initial distribution p0(θ), and initialize S0 to be the lower-triangular Cholesky factor
of the initial covariance Σ0. Then set i = 1.
2. Sample a candidate point by θ∗= θi−1 +Si−1 ri, where ri ∼N(0,I).
3. Compute the acceptance probability
αi = min{1, exp(ϕ(θi−1)−ϕ(θ∗))}.
(2)
4. Sample a uniform random variable u from the uniform distribution U(0,1).
5. If u ≤αi, set θ(i) = θ∗. Otherwise set θ(i) = θ(i−1)
6. Compute a lower-triangular matrix Si with positive diagonal elements satisfying the equation
Si ST
i = Si−1

I+ηi (αi −¯α∗) ri rT
i
||ri||2

ST
i−1,
(3)
where {ηi}i≥1 ⊂(0,1] is an adaptation step size sequence decaying to zero. Although any such se-
quence will do, Vihola (2012) suggests ηi = i−γ with a suitable exponent γ ∈(1/2,1].
7. Set i ←i+1 and go to step 2 until the desired number of samples has been generated.
Note that the general RAM algorithm (Vihola, 2012) allows for the usage of more general symmetric
distribution q(r) = q(||r||) instead of the unit Gaussian distribution N(0,I) above.
S. S¨arkk¨a
P.O.Box 12200, FI-00076 Aalto, Finland
Tel.: +358-50-5124393
E-mail: simo.sarkka@aalto.ﬁ

2
Simo S¨arkk¨a et al.
B Kalman Filter and RTS Smoother Equations
The Kalman ﬁlter update equations (see, e.g., Bar-Shalom et al, 2001) are:
Sk = H(θ)P(t−
k )HT (θ)+R(θ)
Kk = P(t−
k )HT (θ)S−1
k
m(tk) = m(t−
k )+Kk (yk −H(θ)m(t−
k ))
P(tk) = P(t−
k )−Kk Sk KT
k .
(4)
Note that in the above equations, all the means and covariances as well as the terms Sk and Kk implicitly
depend on the parameters θ, but we have dropped this dependence for notational convenience.
The approximate smoothed means ms(tk) and covariances Ps(tk) can be computed as follows (S¨arkk¨a
and Sarmavuori, 2013):
dCk(t)
dt
= Ck(t)P−1(t)
Z
f(x,t;θ)(x−m(t))T N(x|m(t),P(t))dx
Gk+1 = Ck(t−
k+1)P−1(t−
k+1)
ms(tk) = m(tk)+Gk+1 [ms(tk+1)−m(t−
k+1)]
Ps(tk) = P(tk)+Gk+1 [Ps(tk+1)−P(t−
k+1)]GT
k+1,
(5)
where the differential equation for C should be integrated from the initial condition C(tk) = P(tk) to time
tk+1 (which is denoted as t−
k+1 for consistency).
References
Bar-Shalom Y, Li XR, Kirubarajan T (2001) Estimation with Applications to Tracking and Navigation.
Wiley Interscience
S¨arkk¨a S, Sarmavuori J (2013) Gaussian ﬁltering and smoothing for continuous-discrete dynamic systems.
Signal Processing 93:500–510
Vihola M (2012) Robust adaptive Metropolis algorithm with coerced acceptance rate. Statistics and Com-
puting 22(5):997–1008

PUBL. III
Isambi S. Mbalawata and Simo Särkkä., 2014. On The L4 Convergence of Particle Filters with
General Importance Distributions, Proceedings of IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), 8048–8052, 2014.
c⃝2014 IEEE International Conference on Acoustics, Speech, and Signal Processing. All rights
reserved.
Reprinted, with the permission of IEEE International Conference on Acoustics, Speech, and Signal
Processing


ON THE L4 CONVERGENCE OF PARTICLE FILTERS
WITH GENERAL IMPORTANCE DISTRIBUTIONS
Isambi S. Mbalawata
Lappeenranta University of Technology
P.O.Box 20, FI-53851 Lappeenranta, Finland
Simo S¨arkk¨a
Aalto University
P.O.Box 12200, FI-00076 Aalto, Finland
ABSTRACT
In this paper we extend the L4 proof of Hu et al. (2008) from
bootstrap type of particle ﬁlters to particle ﬁlters with gen-
eral importance distributions. The result essentially shows
that with general importance distributions the particle ﬁlter
converges provided that the importance weights are bounded.
By numerical simulations we also show that this condition is
often also a practical requirement for a good performance of
a particle ﬁlter.
Index Terms— Particle ﬁlter; convergence; importance
distribution; unbounded function
1. INTRODUCTION
Particle ﬁlters [1, 2, 3] are powerful methods for approximate
Bayesian ﬁltering in state space models of the form
xt ∼f(xt | xt−1),
yt ∼g(yt | xt),
(1)
where xt ∈Rn is the state of the system, yt ∈Rm is the
measurement, f(xt | xt−1) is the transition probability den-
sity (w.r.t. Lebesgue measure) modeling the dynamics of the
system, and g(yt | xt) is the conditional probability density
of measurements modeling the distribution of measurements.
Particle ﬁlters form a weighted set of Monte Carlo sam-
ples {(xi
t, wi
t) : i = 1, . . . , N} such that the posterior ex-
pectation of a test function φ(·) can be approximated as
E[φ(xt) | y1:t] ≈
N

i=1
wi
t φ(xi
t).
(2)
A particle ﬁlter converges if, in a suitable sense, the above
approximation becomes exact when N →∞.
Various types of convergence result for particle ﬁlters with
general importance distributions, but with bounded test func-
tions can be found in the survey article [4]. Long-term sta-
bility results and central limit theorem type of convergence
theorems for particle ﬁlters (also for unbounded functions),
can be found in [5, 6, 7, 8] and references therein. L4 type
This work was supported by the Academy of Finland (266940, 273475).
We are grateful to Juho Kokkala for the numerical experiments.
of convergence results for the unbounded case have recently
been studied in [9, 10], but only in the case of bootstrap type
of importance distributions.
In this paper, we extend the proof of Hu et al. (2008) [9] to
the case of general importance distributions. The results show
that the boundedness of importance weights (along with the
model densities) is a sufﬁcient condition for the convergence
also in the case of unbounded test functions, which is also a
sufﬁcient condition in the bounded case [4]. We also discuss
the practical implications of the condition to certain impor-
tance distributions proposed in literature.
2. PARTICLE FILTERING
Recall that the Bayesian ﬁlter for the state space model in (1)
can be written in the abstract form [9]:
(πt|t−1, φ) = (πt−1|t−1, f φ),
(3)
(πt|t, φ) = (πt|t−1, φ g)
(πt|t−1, g) ,
(4)
where we have deﬁned
(π, φ) =

φ(x) π(dx),
f φ =

f(xt | xt−1) φ(xt) dxt.
With this notation, we obtain the bootstrap ﬁlter simply by
replacing the measures π with their ﬁnite-sample approxima-
tions and by introducing an additional resampling step. This
was the starting point of the analysis in [9].
However, here we wish to analyze the convergence of
the more general particle ﬁlter which does not correspond
to a direct ﬁnite-sample approximation of the prediction and
update steps above. Instead of sampling from the dynamic
model distribution we sample from an importance distribu-
tion q(xt | xt−1, yt) and then compute weights for the sam-
ples. For this purpose it is convenient to rewrite the Bayesian
ﬁlter as a single step
(πt|t, φ) = ((πt−1|t−1, ρ q), φ)
((πt−1|t−1, ρ q), 1) ,
(5)
where we have deﬁned the importance weights
ρ(xt, xt−1) = g(yt | xt) f(xt | xt−1)
q(xt | xt−1, yt)
.
(6)
2014 IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)
978-1-4799-2893-4/14/$31.00 ©2014 IEEE
8048

As in [9, 10], to be able to cope with unbounded functions, we
need to use a slightly modiﬁed version of the standard parti-
cle ﬁlter in order to guarantee the convergence. The mod-
iﬁed particle ﬁlter is constructed such that we always have
((πN
t−1|t−1, ρ q), 1) ≥γt > 0, where γt > 0 is a chosen
threshold [9]. The modiﬁed algorithm is the following.
Algorithm 2.1 (General Modiﬁed Particle Filter). The algo-
rithm is similar to [9], but includes importance distributions.
1. Initialize the particles, {xi
0}N
i=1 ∼π0(dx0)
2. Draw samples according to ¯xi
t ∼N
j=1 αi
j q(xt |
xj
t−1, yt), where αj
i are non-negative weights such that
N
j=1 αi
j = 1, N
i=1 αi
j = 1, and
1
N
N

i=1
N

j=1
αi
j q(xt | xj
t−1, yt) = 1
N
N

j=1
q(xt | xj
t−1, yt).
3. If ((πN
t−1|t−1, ¯ρ ¯q), 1) ≥γt, proceed to step 4 otherwise
return to step 2. Note that ¯ρ and ¯q are the values eval-
uated at ¯xi
t.
4. Rename ˜xi
t = ¯xi
t, and compute and normalize the
weights wi
t = ρ(˜xi
t, xi
t−1), ˜wi
t = wi
t/ N
j=1 wj
t.
5. Resample, xi
t ∼˜πN
t|t(dxt) = N
i=1 ˜wi
t δ˜xi
t(dxt)
6. Set t = t + 1 and repeat from step 2).
3. CONVERGENCE WITH GENERAL
IMPORTANCE DISTRIBUTION
To prove the convergence of the particle ﬁlter, we need to
impose the following conditions (cf. [9]).
• H0: For any given y1:s, we have ((πs−1|s−1, ρ q), 1) >
0, where s = 1, . . . , t.
• H1: The dynamic model f, measurement model g,
and the importance weights ρ(xt, xt−1) are bounded.
That is, there exist constants Cf, Cg, and Cρ such that
∥f∥≤Cf, ∥g∥≤Cg, and ∥ρ∥≤Cρ, where the ﬁrst
norm is an operator norm induced by the supremum
norm, and the second two are supremum norms of the
functions.
• H2: The function φ(·) satisﬁes supxs |φ(xs)|4g(ys |
xs) < C(y1:s).
The main convergence theorem is the following.
Theorem 3.1. Consider the general modiﬁed particle ﬁlter
algorithm and suppose that the conditions H0, H1, and H2
above hold. Then we have the following.
1. For a sufﬁciently large N, the algorithm will not run
into an inﬁnite loop on steps 2-3.
2. Let L4
t(g) be the class of functions satisfying H2. For
any φ ∈L4
t(g), there exists a constant Ct|t, indepen-
dent of N such that
E
(πN
t|t, φ) −(πt|t, φ)

4
≤Ct|t
||φ||4
t,4
N 2
,
(7)
where ||φ||4
t,4 is deﬁned as [9]
||φ||t,4 = max

1, (πs|s, |φ|4)1/4, s = 0, 1, . . . , t
	
.
(8)
Proof. The proofs for initialization and resampling steps are
the same as in [9]. Thus, here, we only prove the convergence
of the (combined) prediction and update steps. That is, we
prove the convergence of the following:
(πN
t|t, φ) −(πt|t, φ) =
(ˆπN
t|t, φ)
(ˆπN
t|t, 1) −(ˆπt|t, φ)
(ˆπt|t, 1) ,
(9)
where ˆπN
t|t = (πN
t−1|t−1, ρ qN) and ˆπt|t = (πt−1|t−1, ρ q). It
is now enough to study the bounded for the following terms:
E
(πN
t|t, φ) −(πt|t, φ)

4
and
E[(πN
t|t, |φ|4)].
(10)
At t = 0, we have the initialization step for which the proof
can be found in [9]. Next, we assume that there exist constants
Ct−1|t−1 and Mt−1|t−1 such that the following is true:
E
(πN
t−1|t−1, φ) −(πt−1|t−1, φ)

4
≤Ct−1|t−1
||φ||4
t−1,4
N 2
,
(11)
and
E

(πN
t−1|t−1, |φ|4)


≤Mt−1|t−1||φ||4
t−1,4.
(12)
To study the boundedness of (10), we start by studying
the numerator terms in (9). We ﬁrst derive the bound for
E[|(ˆπN
t|t, φ) −(ˆπt|t, φ)|4] and then for E[(ˆπN
t|t, |φ|4)].
Let Ft−1 be the σ-algebra generated by xi
t−1. Then we
can write (ˆπN
t|t, φ) −(ˆπt|t, φ) = Π1 + Π2 + Π3, where
Π1 = (ˆπN
t|t, φ) −1
N
N

i=1
E[φ(˜xi
t) ρ(˜xi
t, xt−1) | Ft−1], (13)
Π2 = 1
N
N

i=1
E[φ(˜xi
t) ρ(˜xi
t, xt−1) | Ft−1]
−1
N
N

i=1
(πN,αi
t−1|t−1, f φ g),
(14)
Π3 = 1
N
N

i=1
(πN,αi
t−1|t−1, f φ g) −(ˆπt|t, φ).
(15)
8049

We treat the terms of Π1, Π2 and Π3 separately and, in each
case, we assume that ∥ρ∥≤Cρ, and that ∥f∥and ∥g∥are
bounded by some constants. Let ¯xi
t ∼(πN,αi
t−1|t−1, q), then
E[φ(¯xi
t) ρ(¯xi
t, xi
t−1) | Ft−1] = (πN,αi
t−1|t−1, f φ g),
(πN
t−1|t−1, f φ g) = 1
N
N

i=1
(πN,αi
t−1|t−1, f φ g).
(16)
The probability of the threshold γt corresponds to event At
deﬁned as At = {(πN
t−1|t−1, f g) ≥γt}, where, by (16), we
have
E

1
N
N

i=1
ρ(¯xi
t, xi
t−1) | Ft−1

= (πN,αi
t−1|t−1, f g).
(17)
Suppose ∥f∥and ∥g∥are bounded, and H0 holds. Then, using
Markov’s inequality and (11), it implies that
P

1
N
N

i=1
ρ(¯xi
s, xi
s−1) < γt
Ft−1

≤
Ct−1|t−1∥f∥4 ∥g∥4
N 2
γt −(πt|t−1, g)

4
=
˜Cγt
N 2 = ϵ.
(18)
To bound (13), we use Lemmas 7.1, 7.2, 7.3, and 7.5 from
[9], (16), and (12), which leads to
E[|Π1|4 | Ft−1]
≤24
N 4
 N

i=1
E

|φ(¯xi
t) ρ(¯xi
t, xi
t−1)|4|Ft−1

1 −ϵ

+ 24
N 4
⎡
⎣
 N

i=1
E

|φ(¯xi
t) ρ(¯xi
t, xi
t−1)|2|Ft−1

1 −ϵ
2⎤
⎦
≤
24
(1 −ϵ)2
⎡
⎣C3
ρ

πN
t−1|t−1, f |φ|4 g

N 2
⎤
⎦
+
24
(1 −ϵ)2
⎡
⎣C2
ρ

πN
t−1|t−1, f |φ|2 g

N 2
⎤
⎦
≤
25 ˜Cρ
(1 −ϵ)2

Mt−1|t−1∥φ∥4
t−1,4
N 2

= ˜CΠ1
∥φ∥4
t−1,4
N 2
. (19)
For the bound of (14), we use Lemmas 7.3 and 7.5, from [9],
Eqs. (12), (16), and (18), as well as Jensen’s inequality, which
leads to
E
Π2

4
| Ft−1

≤
24ϵ2C2
ρ
(1 −ϵ)4N 2 E

E

(N (πN
t−1|t−1, f |φ|2 g))2
≤
24ϵ2
(1 −ϵ)4N 2 C2
ρ∥f∥2 ∥g∥2 
πN
t−1|t−1, |φ|4
≤˜CΠ2
∥φ∥4
t−1,4
N 2
.
(20)
For the bound of (15), we use (11), which gives:
E
Π3

4
| Ft−1

≤||f||4||g||4E
(πN
t−1|t−1, φ) −(πt−1|t−1, φ)

4
≤˜Ct−1|t−1
∥f∥4∥g∥4∥φ∥4
t−1,4
N 2
= ˜CΠ3
∥φ∥4
t−1,4
N 2
.
(21)
By combining Eqs. (19), (20), and (21) via Minkowski’s in-
equality, we get
E
(ˆπN
t|t, φ) −(ˆπt|t, φ)

4 1
4
≤

˜C1/4
Π1 + ˜C1/4
Π2 + ˜C1/4
Π3
 ∥φ∥t−1,4
N 1/2
= ˆC1/4
t|t
∥φ∥t−1,4
N 1/2
,
which implies
E
(ˆπN
t|t, φ) −(ˆπt|t, φ)

4
≤ˆCt|t
∥φ∥4
t−1,4
N 2
.
(22)
The bound for E[(ˆπN
t|t, |φ|4)] can be derived using the same
technique as above, which leads to
E

(ˆπN
t|t, |φ|4)


≤Mt|t||φ||4
t−1,4.
(23)
Note that if we set φ = 1 in (22) and (23), we get similar
bounds for the difference of the denominators.
We ﬁnally study the boundedness of (10). For E[|(πN
t|t, φ)−
(πt|t, φ)|4], we use (22) and (23) with φ = 1 along with
Minkowski’s inequality, to get
E
(πN
t|t, φ) −(πt|t, φ)

4 1
4
≤

(ˆπN
t|t, φ) ˆC1/4
t|t
γt(ˆπt|t, 1)

 1
N 2
 1
4
+

ˆC1/4
t|t
(ˆπt|t, 1)


∥φ∥4
t−1,4
N 2
 1
4
≤
1
N 1/2 C
1
4
t|t∥φ∥t−1,4,
which thus gives
E
(πN
t|t, φ) −(πt|t, φ)

4
≤Ct|t
∥φ∥4
t−1,4
N 2
.
8050

For E[(πN
t|t, |φ|4)], we similarly get
E

(πN
t|t, |φ|4) −(πt|t, |φ|4)

≤¯
Mt|t∥φ∥4
t−1,4,
(24)
which thus completes the proof.
4. PRACTICAL IMPLICATIONS
Our result states that provided that ||f||, ||g||, and ||ρ|| are
bounded, we can ensure the convergence. The boundedness
of f and g is indeed quite natural, but let’s take a closer look
at the boundedness of the term ρ, which we deﬁned in (6), on
some commonly used importance distributions.
• The optimal importance distribution [2] q(xt | xt−1, yt) =
p(xt | xt−1, yt) leads to ρ(xt, xt−1) =

g(yt |
xt) f(xt | xt−1) dxt which is guaranteed to be bounded
provided that f and g are bounded.
• In the bootstrap ﬁlter [1, 9] we select q(xt | xt−1, yt) =
f(xt | xt−1), which gives ρ(xt | xt−1) = g(yt | xt)
and thus is ensured to be bounded.
• Using non-linear Kalman ﬁlters to approximate the op-
timal importance distribution [2, 11, 12] gives q(xt |
xt−1, yt) = N(xt | mt, Pt) were, mt and Pt are mean
and covariance computed by the Kalman ﬁlter.
We
can now assure convergence only if the ratio of the
optimal importance distribution and its approximation
p(xt | xt−1, yt)/N(xt | mt, Pt) is bounded. This re-
quires that the tails of p(xt | xt−1, yt) are not heavier
than the tails of the Gaussian distribution and that the
covariance Pt is bounded from below.
• We can also use a multivariate Student’s t-distribution
with the parameters mt and Pt above instead of the
Gaussian distribution [13]. If we choose the degrees
of freedom in the Student’s t-distribution to be low
enough, then ρ can be assured to be bounded.
Example 4.1 (Linear Gaussian state space model). Consider
the one-dimensional Gaussian random walk model
xt = xt−1 + qt−1,
qt−1 ∼N(0, Q),
(25)
yt = xt + rt,
rt ∼N(0, R).
(26)
The optimal importance distribution is now Gaussian N(xt |
mt, Pt) with mt = xt−1 + Q/(Q + R) [yt −xt−1], P =
Q −Q2
(Q + R). If we replace the importance distribution
with N(xt | mt, c Pt) where c < 1, then the weights become
unbounded and thus the particle ﬁlter is not guaranteed to
converge.
Figure 1 illustrates the effect of the value c to the estimates
of the fourth order central moment of the ﬁltering distribution
at t = 24 with varying number of particles with the param-
eter values Q = 1, R = 1/2, and x0 = 0. It can be seen
0
2000
4000
6000
8000
10000
0
0.1
0.2
0.3
0.4
0.5
Estimate of E[|x−μ|4]
Number of particles N
 
 
c=0.1
c=0.5
c=1.0
exact
0
2000
4000
6000
8000
10000
0
0.1
0.2
0.3
0.4
0.5
Estimate of E[|x−μ|4]
Number of particles N
 
 
c=0.1
c=0.5
c=1.0
exact
Fig. 1. Illustration of the effect of scaling the importance dis-
tributions with c in linear Gaussian example. Left: Gaussian
distribution. Right: Student’s t-distribution with ν = 3 de-
grees of freedom. The scaling of variance signiﬁcantly af-
fects the performance of the particle ﬁlter with a Gaussian
importance distribution whereas the effect to a Student’s t-
distribution based particle ﬁlter is smaller.
that scaling of the variance in Gaussian importance distri-
bution affects the convergence of the fourth moment estimate
whereas with the Student’s t-distribution the effect is smaller.
Example 4.2 (Non-linear state space model). A typically used
example of a non-linear model is the following system (see,
e.g., [2]):
xt = 1
2 xt−1 + 25
xt−1
1 + (xt−1)2 + 8 cos(1.2t) + qt−1, (27)
yt = x2
t
20 + rt,
(28)
where qt−1 ∼N(0, 10) and rt ∼N(0, 1). It is now easy to
show that the ratio of the optimal importance distribution and
any Gaussian distribution will be uniformly bounded. Thus
using a non-linear Kalman ﬁlter based Gaussian importance
distribution leads to a converging particle ﬁlter provided that
we do not allow the Gaussian distribution to become singular.
5. CONCLUSION
In this paper, we extended the proof of Hu et al. (2008) [9] to
the case of general importance distributions. Our proof shows
the L4 convergence of the particle ﬁlter estimates for a general
class of unbounded functions provided that the importance
weights are bounded. This also implies the probability-one
convergence of the estimates [9]. We have analyzed the con-
ditions set by the proof on importance distributions proposed
in literature and tested them numerically.
6. REFERENCES
[1] Neil J. Gordon, David J. Salmond, and Adrian F. M.
Smith,
“Novel approach to nonlinear/non-Gaussian
8051

Bayesian state estimation,” Radar and Signal Process-
ing, IEE Proceedings F, vol. 140, no. 2, pp. 107–113,
1993.
[2] Arnaud Doucet, Simon Godsill, and Christophe An-
drieu, “On sequential Monte Carlo sampling methods
for Bayesian ﬁltering,” Statistics and Computing, vol.
10, no. 3, pp. 197–208, 2000.
[3] Olivier Capp´e, Simon J. Godsill, and Eric Moulines,
“An overview of existing methods and recent advances
in sequential Monte Carlo,” in Proceedings of the IEEE,
2007, pp. 899–924.
[4] Dan Crisan and Arnaud Doucet, “A survey of conver-
gence results on particle ﬁltering methods for practition-
ers,” Signal Processing, IEEE Transactions, vol. 50, no.
3, pp. 736–746, 2002.
[5] Pierre Del Moral, Feynman-Kac Formulae: Genealog-
ical and Interacting Particle Systems with Applications,
Springer, 2004.
[6] Randal Douc and Eric Moulines, “Limit theorems for
weighted samples with applications to sequential Monte
Carlo methods,” Annals Statistics, vol. 36, no. 5, pp.
2344–2376, 2008.
[7] Randal Douc, Eric Moulines, and Jimmy Olsson, “Op-
timality of the auxiliary particle ﬁlter,” Probability and
Mathematical Statistics, vol. 29, no. 1, pp. 1–28, 2009.
[8] Nick Whiteley,
“Stability properties of some particle
ﬁlters,” The Annals of Applied Probability, 2013, (to
appear).
[9] Xiao-li Hu, Thomas B. Sch¨on, and Lennart Ljung, “A
basic convergence result for particle ﬁltering,”
IEEE
Transactions on Signal Processing, vol. 56, no. 4, pp.
1337–1348, 2008.
[10] Xiao-li Hu, Thomas B. Sch¨on, and Lennart Ljung, “A
general convergence result for particle ﬁltering,” IEEE
Transactions on Signal Processing, vol. 59, no. 7, 2011.
[11] Rudolph Van Der Merwe, Arnaud Doucet, Nando
de Freitas, and Eric Wan, “The unscented particle ﬁl-
ter,” in In Advances in Neural Information Processing
Systems, 2000, pp. 584–590.
[12] Dong Guo, Xiaodong Wang, and Rong Chen,
“New
sequential Monte Carlo methods for nonlinear dynamic
systems,” Statistics and Computing, vol. 15, no. 2, pp.
135–147, 2005.
[13] Olivier Capp´e, Eric Moulines, and Tobias Ryd´en, Infer-
ence in Hidden Markov Models, Springer, 2005.
8052

PUBL. IV
Isambi S. Mbalawata, Simo Särkkä, Matti Vihola and Heikki Haario, Adaptive Metropolis Al-
gorithm Using Variational Bayesian Adaptive Kalman Filter. Computational Statistics and Data
Analysis, 83, 101–115, 2015.
c⃝2014 Computational Statistics and Data Analysis. All rights reserved.
Reprinted, with the permission of Computational Statistics and Data Analysis


Computational Statistics and Data Analysis 83 (2015) 101–115
Contents lists available at ScienceDirect
Computational Statistics and Data Analysis
journal homepage: www.elsevier.com/locate/csda
Adaptive Metropolis algorithm using variational Bayesian
adaptive Kalman filter
Isambi S. Mbalawata a,∗, Simo Särkkä b, Matti Vihola c, Heikki Haario a
a Department of Mathematics and Physics, Lappeenranta University of Technology, P.O. Box 20, FI-53851 Lappeenranta, Finland
b Department of Biomedical Engineering and Computational Science, Aalto University, P.O. Box 12200, FI-00076 Aalto, Finland
c Department of Statistics, University of Oxford, 1 South Parks Road, Oxford, OX1 3TG, United Kingdom
a r t i c l e
i n f o
Article history:
Received 27 August 2013
Received in revised form 1 October 2014
Accepted 5 October 2014
Available online 16 October 2014
Keywords:
Markov chain Monte Carlo
Adaptive Metropolis algorithm
Adaptive Kalman filter
Variational Bayes
a b s t r a c t
Markov chain Monte Carlo (MCMC) methods are powerful computational tools for analysis
of complex statistical problems. However, their computational efficiency is highly depen-
dent on the chosen proposal distribution, which is generally difficult to find. One way to
solve this problem is to use adaptive MCMC algorithms which automatically tune the statis-
tics of a proposal distribution during the MCMC run. A new adaptive MCMC algorithm,
called the variational Bayesian adaptive Metropolis (VBAM) algorithm, is developed. The
VBAM algorithm updates the proposal covariance matrix using the variational Bayesian
adaptive Kalman filter (VB-AKF). A strong law of large numbers for the VBAM algorithm is
proven. The empirical convergence results for three simulated examples and for two real
data examples are also provided.
© 2014 Elsevier B.V. All rights reserved.
1. Introduction
Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are an important class of numerical tools for approxi-
mating multidimensional integrals over complicated probability distributions in Bayesian computations and in various other
fields. The computational efficiency of MCMC sampling depends on the choice of the proposal distribution. A challenge of
MCMC methods is that in complicated high-dimensional models it is very hard to find a good proposal distribution.
The Gaussian distribution is often used as a proposal distribution due to its theoretical and computational properties.
However, the Gaussian proposal distribution needs a well tuned covariance matrix for optimal acceptance rate and good
mixing of the Markov chain. If the covariance matrix is too small, too large or has improper correlation structure, the Markov
chains will be highly positively correlated and hence the estimators will have a large variance. Because manual tuning is la-
borious, several adaptive MCMC algorithms have been suggested (Haario et al., 1999, 2001, 2006; Vihola, 2012; Andrieu
and Thoms, 2008; Roberts and Rosenthal, 2007, 2009; Atchadé and Rosenthal, 2005; Gelman et al., 1996) to update the
covariance matrix during the MCMC run.
In this article, we propose a new adaptive Metropolis algorithm, where we update the covariance matrix of the Gaus-
sian proposal distribution of the Metropolis algorithm using the variational Bayesian adaptive Kalman filter (VB-AKF) pro-
posed by Särkkä and Nummenmaa (2009) and Särkkä and Hartikainen (2013). The idea of the classical Metropolis algorithm
(Haario et al., 1999) is essentially to empirically estimate the covariance of the samples and use this estimate to construct
the proposal distribution. However, as we point out here, such a covariance estimation problem can also be formulated
∗Corresponding author.
E-mail address: Isambi.Mbalawata@lut.fi (I.S. Mbalawata).
http://dx.doi.org/10.1016/j.csda.2014.10.006
0167-9473/© 2014 Elsevier B.V. All rights reserved.

102
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
as an instance of recursive Bayesian estimation, where the term used for this kind of recursive estimation is Bayesian fil-
tering (Särkkä, 2013). This reinterpretation allows one to construct alternative and potentially more effective adaptation
mechanisms by utilizing the various Bayesian filtering algorithms developed over the years for doing the covariance esti-
mation. The aim of this article is to propose a practical algorithm which is constructed from this underlying idea, prove its
convergence, and test its performance empirically.
The structure of this article is the following: in Section 2 we review the existing adaptive MCMC methods. Section 3 is
dedicated to our new adaptive Metropolis algorithm. Theoretical validity of the proposed algorithm is shown in Section 4 by
proving a strong law of large numbers. In Section 5, we study the empirical convergence of the method in three simulated
examples and then apply the method to two real data examples.
2. Adaptive Markov chain Monte Carlo methods
Markov chain Monte Carlo (MCMC) methods are widely used algorithms for drawing samples from complicated mul-
tidimensional probability distributions. For example, in Bayesian analysis (Gelman et al., 2013), we are often interested in
computing the posterior expectation of a function g(θ) given the measurements z1, . . . , zM:
E[g(θ) | z1, . . . , zM] =

Rd
g(θ) p(θ | z1, . . . , zM) dθ.
(1)
We can use MCMC methods to approximate the expectation by drawing samples from the posterior distribution
θ1, θ2, . . . , θn ∼p(θ | z1, . . . , zM),
(2)
and then by employing the approximation
E[g(θ) | z1, . . . , zM] ≈
1
n
n

i=1
g(θi).
(3)
A common construction for MCMC uses a random walk that explores the state space through local moves. The most well-
known traditional MCMC method is the Metropolis algorithm. In the Metropolis algorithm we draw a candidate point θ∗from
a symmetric proposal distribution q(θ∗| θ) and use an accept/reject rule to accept or reject the sampled point (Gilks et al.,
1996; Gelman et al., 2013; Brooks et al., 2011).
The efficiency of an MCMC algorithm can be improved by carefully tuning the proposal distribution. Adaptive MCMC
methods are a family of algorithms, which take care of the tuning automatically. This proposal is often chosen to be a Gaus-
sian distribution, in which case it is the covariance matrix that needs to be tuned. Under certain settings Gelman et al. (1996)
show that the optimal covariance matrix for an MCMC algorithm with Gaussian proposal is λ Σ, with λ = 2.382/d, where
d is the dimension and Σ is the d × d covariance matrix of the target distribution.
In the adaptive Metropolis (AM) algorithm by Haario et al. (2001), the covariance matrix Σk−1 for the step k is estimated
as follows:
Σk−1 = cov(θ0, θ1, . . . , θk−1) + εI,
(4)
where I is the d×d identity matrix and ε is a small positive value whose role is to make sure that Σk−1 is not singular (Haario
et al., 1999, 2001). The AM algorithm of Haario et al. (2001) can be summarized as follows:
• Initialize θ0, Σ0.
• For k = 1, 2, 3, . . .
– Sample a candidate point θ∗from a Gaussian distribution
θ∗∼N(θk−1, λ Σk−1).
(5)
– Compute the acceptance probability
αk = min

1,
p(θ∗| z1, . . . , zM)
p(θk−1 | z1, . . . , zM)

.
(6)
– Sample a random variable u from the uniform distribution U(0, 1).
– If u < αk, set θk = θ∗. Otherwise set θk = θk−1.
– Compute the covariance matrix Σk using Eq. (4).
Different adaptive algorithms have been proposed as improved versions of the AM algorithm above. Good surveys of such
algorithms are found in Andrieu and Thoms (2008) and Liang et al. (2010), where the authors present ways to implement
the algorithms and then show why the algorithms preserve the correct stationary distributions. For instance, apart from
updating the covariance alone, one can adapt λ using the following Robbins–Monro algorithm, which alleviates the problem
of Σk being systematically too large or too small (Andrieu and Thoms, 2008; Atchadé and Fort, 2010; Vihola, 2011):
log(λk) = log(λk−1) + γk (αk −α).
(7)
In Eq. (7), α is the target acceptance rate which is commonly set to 0.234 and γk is a gain factor sequence satisfying the
following conditions:
∞

k=1
γk = ∞
and
∞

k=1
γ 1+δ
k
< ∞
for some δ ∈(0, 1].

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
103
In a recent paper, Vihola (2012) introduced the robust adaptive Metropolis (RAM) algorithm with an online rule for adapting
the covariance matrix and a mechanism for maintaining the mean acceptance rate at a pre-determined level. Holden et al.
(2009) describe an adaptive independent Metropolis–Hastings algorithm, where the proposal is adapted with past samples.
The limitation of this adaptation is that the information gained from doing the local steps cannot be used, so the algorithm
iterations do not improve the proposal (Holden et al., 2009; Liang et al., 2010). Gilks et al. (1998) proposed regeneration-
based adaptive algorithms, where after each regeneration point the proposal distribution is modified based on all the past
samples and the future outputs become independent of the past. In the population-based adaptive algorithms the proposal
distributions are designed such that computational techniques are incorporated into simulations and the covariance matrix
is adapted using a population of independent and identically distributed samples from the adaptive direction sampler (Gilks
et al., 1994) or the evolutionary Monte Carlo (Ren et al., 2008).
Another type of adaptive MCMC is proposed by Vrugt et al. (2009) and Vrugt and Braak (2011), where they integrate the
MCMC algorithm and differential evolution. This type of algorithm generates multiple different chains simultaneously for
global exploration, and automatically tunes the scale and orientation of the proposal distribution in randomized subspaces
during the search. One way of adapting the MCMC proposal distribution is by using multiple copies of the target density and
Gibbs sampling (Cai et al., 2006; Griffin and Walker, 2013). The idea is that a product of the proposal density and copies of
the target density is used to define a joint density which is sampled by Metropolis–Hastings-within-Gibbs algorithm.
When constructing an adaptive algorithm one should be sure that the conditions for ergodicity are satisfied to ensure
that the algorithm converges to the target distribution (Andrieu and Moulines, 2006; Roberts and Rosenthal, 2007; Saksman
and Vihola, 2010; Atchadé and Fort, 2010; Bai et al., 2011). One way to ensure the ergodicity property of adaptive MCMC
is in terms of the two general conditions (Bai, 2009; Bai et al., 2011): diminishing adaptation and containment condition.
However, these conditions are not necessary and there exist valid adaptation mechanisms that do not have these proper-
ties. In our proposed algorithm, to ensure ergodicity, we impose slightly stronger conditions that imply the diminishing and
containment conditions. The imposed conditions also imply a strong law of large numbers rather than just a weak law of
large numbers.
3. Adaptive metropolis algorithm with variational Bayesian adaptive Kalman filter based covariance update
In this section, we first briefly review the noise adaptive Kalman filter (Särkkä and Hartikainen, 2013) which is used
to adapt the covariance matrix of the proposal distribution, and then present the proposed variational Bayesian adaptive
Metropolis (VBAM) algorithm.
3.1. Kalman filter
Kalman filter (Kalman, 1960) is the classical algorithm for estimation of the dynamic state from noisy measurements in
linear Gaussian state space models. In probabilistic terms the model can be expressed as (Jazwinski, 1970; Särkkä, 2013)
xk ∼N(Ak−1 xk−1, Qk−1),
yk ∼N(Hk xk, Σk),
where N(·) denotes the multivariate Gaussian distribution, xk ∈Rn is the dynamic state, Ak−1 is the dynamic model matrix,
Qk−1 is the process noise covariance, yk ∈Rd is the measurement, Hk is the measurement matrix, and Σk is the measurement
noise covariance matrix. Here, xk is an unknown variable and yk is an observed variable, whereas the matrices Ak−1, Qk−1,
Hk, and Σk are assumed known. We further assume that x0 ∼N(m0, P0), where m0 and P0 are the known prior mean and
covariance. The estimation of states is recursively performed using two Kalman filter steps:
1. Prediction step:
m−
k = Ak−1 mk−1,
P−
k = Ak−1 Pk−1 AT
k−1 + Qk−1.
(8)
2. Update step:
Sk = Hk P−
k HT
k + Σk,
Kk = P−
k HT
k S−1
k ,
mk = m−
k + Kk

yk −Hk m−
k

,
Pk = P−
k −Kk Sk KT
k,
(9)
where m−
k is the a priori state mean, mk is the a posteriori state mean, P−
k is the a priori state covariance, and Pk is the a pos-
teriori state covariance. In Bayesian sense the Kalman filter computes the statistics for the following conditional distribution
of the state given the measurements:
p(xk | y1:k) = N(xk | mk, Pk).
(10)

104
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
If Ak−1 = I and the process noise is zero Qk−1 = 0, the Kalman filter reduces to the so called recursive least squares (RLS)
algorithm which solves a general multivariate linear estimation (regression) problem recursively. The matrices Ak−1 and
Qk−1 can be used for modeling the dynamics of the state when it is not assumed to be static (as in RLS).
3.2. Variational noise adaptation
In the Kalman filter discussed above, the model matrices in the state space model are assumed to be known. The varia-
tional Bayesian adaptive Kalman filter (VB-AKF, Särkkä and Nummenmaa, 2009 and Särkkä and Hartikainen, 2013) is con-
sidered with the case where the noise covariance Σk is unknown. The model is assumed to be of the form:
xk ∼N(Ak−1 xk−1, Qk−1),
(11)
yk ∼N(Hkxk, Σk),
(12)
Σk ∼p(Σk | Σk−1),
(13)
where Eq. (13) defines the Markovian dynamic model prior for the unknown measurement noise covariances. If we were
able to implement the optimal (non-Gaussian) Bayesian filter for this model, it would compute the distribution
p(xk, Σk | y1:k).
(14)
Recall that the Kalman filter can be considered as a generalization of the RLS algorithm. In the same way we can consider the
Bayesian filter for the above model as a generalization of the RLS algorithm where the noise variance is estimated together
with the linear regression solution. We can also interpret the AM covariance adaption rule in Eq. (4) as a simple linear re-
gression problem where we estimate the noise covariance together with the linear regression solution. However, in AM we
throw out the linear regression solution and only retain the covariance.
Because the above model is a generalization of RLS with covariance estimation, it can be seen to provide a recursive so-
lution for estimation of the covariance in Eq. (4) as a special case. This is the idea of our method. However, there is no reason
to only use the parameters which reduce the Bayesian filter to the RLS, but one can use the full state space model and the
corresponding filter to construct an adaptive Metropolis algorithm.
Unfortunately, the exact Bayesian filter for the above model is computationally intractable. However, the joint filtering
distribution of the state and covariance matrix can be approximated with the free-form variational Bayesian approximation
as follows (Särkkä and Nummenmaa, 2009; Särkkä and Hartikainen, 2013):
p(xk, Σk | y1:k) ≈Qx(xk) QΣ(Σk),
(15)
where Qx(xk) and QΣ(Σk) are unknown approximating densities formed by minimizing the Kullback–Leibler (KL) diver-
gence between the true distribution and the approximation:
KL [Qx(xk)QΣ(Σk) | |p(xk, Σk | y1:k)] =

Qx(xk) QΣ(Σk) log

Qx(xk) QΣ(Σk)
p(xk, Σk | y1:k)

dx dΣk.
(16)
The KL divergence above can be minimized with respect to Qx(xk) and QΣ(Σk) using the methods from calculus of variations,
which results in the following approximations (Särkkä and Nummenmaa, 2009; Särkkä and Hartikainen, 2013):
Qx(xk) ∝exp

log p(xk, Σk | y1:k) QΣ(Σk) dΣk

,
(17)
QΣ(Σk) ∝exp

log p(xk, Σk | y1:k) Qx(xk) dxk

.
(18)
Solving the above equations leads to the following approximation (Särkkä and Nummenmaa, 2009; Särkkä and Hartikainen,
2013):
p(xk, Σk | y1:k−1) ≈N(xk |, mk, Pk) IW(Σk | νk, Vk),
(19)
where mk and Pk are given by the standard Kalman filter, and νk and Vk are the parameters of the inverse Wishart (IW)
distribution. The proposal covariance can, for example, be computed as the mean of the inverse Wishart distribution:
Σk =
1
νk −d −1
Vk.
(20)
The dynamic model p(Σk | Σk−1) needs to be chosen such that it produces inverse Wishart distribution on the Bayesian filter
prediction step. As pointed out by Särkkä and Nummenmaa (2009), this kind of dynamical model is hard to construct explic-
itly, and hence they proposed heuristic dynamics for the covariances, which was then extended by Särkkä and Hartikainen
(2013). The following dynamic model is obtained:
ν−
k = ρ(νk−1 −d −1) + d + 1,
Σ−
k = B Σk−1 BT,
(21)

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
105
where ν−
k and Σ−
k are prior parameters, ρ is a real number 0 < ρ ≤1 and B is a matrix 0 < |B| ≤1. Here the parameter ρ
controls the forgetting of the previous estimates of the covariance matrix by decreasing the degrees of freedom exponen-
tially. The matrix B can be used to model the deterministic dynamics of the covariance matrix.
For our purposes, it is useful to write the VB-AKF algorithm in a slightly modified form from Särkkä and Hartikainen
(2013), such that it involves the covariance matrix in (20) explicitly. The result is Algorithm 1. Note that in the algorithm,
the matrices Ak, Qk and Hk are defined by the selected state space model, and their use in VBAM will be shown in the nu-
merical examples section.
Algorithm 1 VB-AKF algorithm
• Initialize ν0, m0, P0 and Σ0.
• For k = 1, 2, . . .
– Prediction: compute the parameters of the predicted distribution:
m−
k = Ak−1 mk−1,
P−
k = Ak−1 Pk−1 AT
k−1 + Qk−1,
ν−
k = ρ(νk−1 −d −1) + d + 1,
Σ−
k = B Σk−1 BT,
– Update: set νk = ν−
k + 1 and Σ(1)
k
= Σ−
k . Iterate the following until the convergence (say, N times for j = 1, . . . , N):
S(j+1)
k
= Hk P−
k HT
k + Σ(j)
k ,
K(j+1)
k
= P−
k HT
k

S(j+1)
k
−1
,
m(j+1)
k
= m−
k + K(j+1)
k

yk −Hk m−
k

,
P(j+1)
k
= P−
k −K(j+1)
k
S(j+1)
k

K(j+1)
k
T
,
Σ(j+1)
k
=
νk−1 −d −1
νk −d −1

Σ−
k +

1
νk −d −1

Hk P(j+1)
k
HT
k
+

1
νk −d −1

yk −Hk m(j+1)
k

yk −Hk m(j+1)
k
T
.
22
– Set Σk = Σ(N)
k
, mk = m(N)
k
and Pk = P(N)
k
.
In Algorithm 1, the choice of the number of iterations N depends on the problem at hand. However, in the numerical
examples we tested, we found out that the algorithm requires only a few iterations to converge (we used N = 5). However,
it would also be possible to use a stopping criterion which determines a suitable time to stop by monitoring the changes in
the estimates at each iteration. In the next section we will use the above algorithm to compute the covariance matrix Σk for
a proposal distribution.
3.3. Variational Bayesian adaptive metropolis algorithm
We are now ready to describe the variational Bayesian adaptive Metropolis (VBAM) algorithm where the covariance
matrix is updated with VB-AKF. The idea is simply to replace the covariance computation in Eq. (4) with the estimate of
covariance computed by VB-AKF Algorithm 1. However, to ensure the convergence of the method, we need the following
restrictions (see Section 4):
1. The state space model in Eqs. (11) and (12) needs to be uniformly completely controllable and observable with any
bounded sequence of Σk. This is quite natural, because otherwise the state estimation problem would not make sense as
such. For the definitions of uniformly completely controllable and observable models, see Definitions 4 and 5.
2. We need to have ρ = 1 and B = I in our dynamic model for the covariance matrix. This is needed to enforce diminishing
adaptation of the covariance matrix.
3. The target distribution needs to be compactly supported. In principle, this is a restriction on the application domain of the
method, but in practice, we can imagine to truncate the distribution at some value which exceeds to maximum floating
point number that can be represented in the computer system.
4. We need to force uniform lower and upper bounds for the covariances Σk and therefore we need to include an additional
boundary check to the method. For that purpose, we fix some constants 0 < µ1 ≤µ2 < ∞determining the feasible
values for Σk, and enforce by projection whenever necessary that µ1 I ≤Σk ≤µ2 I. In practice, we can set µ1 to be very

106
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
small and µ2 very high so that we practically never hit the boundaries. Note that for two matrices A and B, the ordering
A ≤B here means that B −A ≥0 is positive semidefinite.
5. We also ensure that the values λk stay within [δλ, δ−1
λ ] for some constant δλ ∈(0, 1] by using a truncation procedure.
The following Algorithm 2 is the variational Bayesian adaptive Metropolis (VBAM) algorithm.
Algorithm 2 Adaptive Metropolis algorithm with VB-AKF algorithm
• Initialize θ0, Σ0, m0, P0, ν0, and λ0. The choice of initial parameters depends on the problem at hand. However, note that
m0 = 0, P0 = I, λ0 = 2.382/d and ν0 = d + 2 are often used.
• For k = 1, 2, . . .
– Sample the candidate from the Gaussian proposal distribution θ∗
∼
N(θk−1, λk−1 Σk−1) or from a Student’s
t-distribution with location θk−1 and scale λk−1 Σk−1.
– Calculate the acceptance probability:
αk = min

1,
p(θ∗| z1, . . . , zM)
p(θk−1 | z1, . . . , zM)

.
– Generate u ∼U(0, 1)
– Set
θk =

θ∗
if u < αk,
θk−1
Otherwise.
– Update the proposal covariance matrix by computing it with the VB-AKF Algorithm 1 where yk = θk. Check that
Σk > µ1I and Σk < µ2I. If this is not true, set Σk = Σk−1 and do a single iteration of the VB-AKF update step
(ignoring the last equation) to compute the updated mean and covariance corresponding to this noise covariance.
– Optionally, update λk using Eq. (7). If λk /∈[δλ, δ−1
λ ], then force it to the interval [δλ, δ−1
λ ].
4. Proof of convergence of VBAM
In this section we show the convergence of our variational Bayesian adaptive Metropolis algorithm (VBAM) using the law
of large numbers. We consider two variants of the algorithm, with and without λk, simultaneously. To prove the convergence
of our proposed VBAM algorithm, it is enough to prove the following theorem.
Theorem 1. Suppose that the target density π(θ) = p(θ | z1, . . . , zM) is bounded and has a bounded support. Furthermore,
suppose that 
k≥1 k−1γk < ∞. Then, for bounded functions f , the strong law of large numbers holds,
1
n
n

k=1
f (θk)
a.s.
−→

Rd
f (θ)π(θ)dθ,
where θ1, . . . , θn are VBAM generated samples assuming the uniform complete controllability and observability properties
(see Definitions 4 and 5) of the state space model are satisfied.
Before proving Theorem 1, we introduce some concepts in the filtering theory and some supporting lemmas that are used
in the proof. We briefly introduce the concepts of information matrix and controllability matrix, as well as the concepts of
uniform complete observability and uniform complete controllability in terms of them. These concepts are very well known
in the field of statistical estimation and filtering of stochastic processes (Jazwinski, 1970), and they ensure the conditions
that the prior distribution is non-degenerate and that the posterior covariance of the state is bounded and hence computing
an estimate of the state is possible in statistical sense.
Definition 2 (Information Matrix). The information matrix ϝ is defined as (Jazwinski, 1970)
ϝ(M, M0) =
M

k=M0
ΦT(k, M) HT
k Σ−1
k
Hk Φ(k, M),
(23)
where Φ(k, M) = A−1
k−1 A−1
k
A−1
k+1 . . . A−1
M−2 A−1
M−1, for k ≤M.
Definition 3 (Controllability Matrix). The controllability matrix ℶis defined as (Jazwinski, 1970)
ℶ(M, M0) =
M

k=M0
Φ(M, k) Qk ΦT(M, k),
(24)
where Φ(M, k) = AM−1 AM−2 AM−3 . . . Ak+1 Ak Ak−1, for k ≤M.

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
107
The state space model (11) and (12) is said to be completely observable if ϝ(M, 1) > 0. Similarly, it is completely control-
lable if ℶ(M, 1) > 0. We now introduce the concepts of uniform complete observability and uniform complete controllability
as defined by Jazwinski (1970). These concepts are required in Lemma 6.
Definition 4 (Uniform Complete Observability). A system is uniformly completely observable if there exist a positive integer
L, and constants β1, β2 > 0 such that for all M ≥L we have
β1 I ≤ϝ(M, M −L) ≤β2 I.
(25)
Definition 5 (Uniform Complete Controllability). A system is uniformly completely controllable if there exist a positive
integer L, and constants β1, β2 > 0 such that for all M ≥L we have
β1 I ≤ℶ(M, M −L) ≤β2 I.
(26)
Having introduced the two important concepts for the proof, we next state and prove the lemma which shows the
boundedness of mk and Pk. After that we state and prove the lemma for diminishing of the difference of VBAM covariance
matrices. Lemmas 6 and 8 are required in the proof of Theorem 1.
Lemma 6. Assume that the state space system defined by Eqs. (11) and (12) is uniformly completely observable and uniformly
completely controllable for any sequence Σk which is bounded below by µ1I and above by µ2I. If P0 > 0, then mk and Pk produced
by the VBAM are uniformly bounded.
Proof. The sequence Σk produced by VBAM algorithm is always bounded below by µ1I and above by µ2I by construction.
Given the bounded sequence Σk, the computation of the means mk and covariances Pk reduces to conventional Kalman
filtering. Because the state space model is uniformly completely observable and controllable, by Lemmas 7.1 and 7.2, and
Theorem 7.4 in Jazwinski (1970), the mean and covariance sequences are uniformly bounded provided that the measure-
ments are bounded. The measurements are the samples from the target distribution which is assumed to be compactly
supported and thus are bounded.
□
The VB-AKF algorithm itself contains a fixed point iteration and for the VBAM algorithm to converge to something sen-
sible, we need this iteration to converge. This is ensured by Theorem 7 which is proved below.
Theorem 7. The sequence {Σ(1)
k , Σ(2)
k , . . . , Σ(j−1)
k
, Σ(j)
k , Σ(j+1)
k
, . . .} produced by the VB-AKF algorithm iteration converges to
a below and above bounded matrix Σ(j)
k
→Σk with j →∞provided that the system is uniformly completely observable and
controllable, νk −d −1 > 0, mk−1, Pk−1, and Σk−1 are bounded from above and below.
Proof. Recall (Särkkä and Nummenmaa, 2009; Särkkä and Hartikainen, 2013) that Eqs. (17) and (18) actually arise in the
solution to the minimization of the KL divergence functional
J [Qx, QΣ] = KL [Qx(xk) QΣ(Σk) | |p(xk, Σk | y1:k)] ,
=

Qx(xk) QΣ(Σk) log

Qx(xk) QΣ(Σk)
p(xk, Σk | y1:k)

dx dΣk,
under the constraints that Qx and QΣ integrate to unity. The minimization with respect to Qx then gives (17) and the min-
imization with respect to QΣ gives (18).
The fixed point iteration in VB-AKF is just an implementation of the following coordinate descend iteration with j =
1, 2, 3, . . .:
Q (j+1)
x
= arg min
Qx
J [Qx, Q (j)
Σ ],
Q (j+1)
Σ
= arg min
QΣ
J [Q (j+1)
x
, QΣ].
It is now easy to show that J is a convex functional in both the arguments and hence the coordinate descend is guaran-
teed to converge (Luenberger and Ye, 2008). When the probability density of a Gaussian distribution converges, its mean
and covariance converge as well. Similarly, the convergence of the probability density of an inverse-Wishart distribution
implies that its parameters converge as well. The given conditions on the means and covariances ensure that the posterior
distribution is well-behaved, which further ensures that the limiting approximating distributions and their statistics are
well-behaved (finite and non-zero) as well. Hence the result follows.
□
Next we will show that Σk −Σk−1 converges to zero provided that we have ρ = 1 and B = I. Intuitively, the selection
ρ = 1 ensures that ak = 1/(νk −d −1) converges to zero and B = I ensures that the prediction step does not alter the
converged covariance matrix.

108
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
Lemma 8. The sequences mk and Pk produced by the VBAM satisfy
∥Σk −Σk−1∥F ≤c ak,
where c is a constant and ∥· ∥F is the Frobenius norm.
Proof. We will start by considering the case where in Algorithm 2, we have resorted to truncation and have set Σk = Σk−1.
Because this implies that ∥Σk −Σk−1∥F = 0, the condition is trivially satisfied. In the case that the truncation is not done
we proceed as follows.
Let us denote Ξk = yk −H mk. Since by Lemma 6, Σk, mk and Pk are bounded, we have ∥Σk∥F ≤cΣ, ∥Ξk Ξ T
k ∥F ≤cΞ
and ∥Hk Pk HT
k∥F ≤cP for some constants cΣ, cΞ and cP. According to Theorem 7 the iteration in VB-AKF converges and thus
we can assume Σk is the solution to the fixed point equation. If we set B = I, Eq. (22) then gives
∥Σk −Σk−1∥F = ∥−ak Σk−1 + ak Ξk Ξ T
k + ak Hk Pk HT
k∥F,
≤ak

∥Σk∥F + ∥ΞkΞ T
k ∥F + ∥HkPkHT
k∥F

,
≤ak(cΣ + cΞ + cP).
□
Next we state and prove the lemma which shows the boundedness of the difference of transition probabilities. Let us
denote by Πλ,Σ the Markov transition probability of a random-walk Metropolis algorithm with the increment proposal
distribution N(0, λΣ).
Lemma 9. For the VBAM algorithm, there exists a constant cD < ∞such that
∥ΠλkΣk −Πλk−1Σk−1∥:= sup
θ
sup
|f |≤1
|ΠλkΣkf (θ) −Πλk−1Σk−1f (θ)| ≤cD (ak + γk),
where ΠλΣf (θ) :=

f (θ′) ΠλΣ(θ, dθ′), and the latter supremum is taken with respect to measurable functions.
Proof. By construction, the eigenvalues of λk Σk are bounded and bounded away from zero, uniform in k. Therefore, Propo-
sition 26(i) of Vihola (2011) implies the existence of a constant c∆< ∞such that
|Πλk Σkf (θ) −Πλk−1 Σk−1f (θ)| ≤c∆∥λk Σk −λk−1 Σk−1∥F,
= c∆∥(λk −λk−1) Σk + λk−1 (Σk −Σk−1)∥F,
≤c∆|λk −λk−1| ∥Σk∥F + λk−1 ∥Σk −Σk−1∥F.
Both ∥Σk∥F and λk−1 are bounded, and an easy computation shows that |λk−λk−1| ≤cλγk for some constant cλ = cλ(β0, β1)
< ∞, because (λk)k≥1 is bounded. The proof is concluded by applying Lemma 8 to bound ∥Σk −Σk−1∥F.
□
Finally, we give the proof of Theorem 1.
Proof of Theorem 1. We use Corollary 2.8 in Fort et al. (2011), with Θn = λn Σn, V ≡1 and πΘn = π. We need to check
that the conditions (A3)–(A5) are satisfied. It is easy to show that (A3) holds for example with λΘ = bΘ = 1/2, because
the eigenvalues of feasible covariance matrices Θ are bounded and bounded away from zero by construction, and one can
find δΘ ≥δ > 0 (see Theorem 7 in Vihola, 2011). Therefore, supΘ LΘ < ∞and (A5) holds trivially. For (A4), it is enough to
observe that
∞

k=1
k−1∥ΠΘk −ΠΘk−1∥≤
∞

k=1
k−1cP

γk + ak

< ∞,
where we have used Lemma 9, our assumption on γk and the fact that supk≥1 k ak < ∞, because νk increases linearly if we
select ρ = 1 which gives νk = ν0 + k.
□
Remark 10. We note that our result generalizes immediately if we replace the Gaussian proposal distribution with a multi-
variate Student’s t-distribution. It is possible to elaborate our result by omitting the bounds for the scaling adaptation by ad-
ditional regularity conditions onπ. This can be achieved by showing first the stability of the scaling adaptation (Vihola, 2011).
5. Numerical results
In this section, the convergence of the algorithm is assessed empirically.1 We first present three simulated examples
which are often used in literature to study the performance and convergence of adaptive MCMC algorithms. In the first
example, we compare our proposed VBAM algorithm with the AM algorithm proposed by Haario et al. (1999, 2001) using
an example from the articles. We use a Gaussian random walk model as the state space model in the VBAM. In the second
1 The Matlab codes can be obtained from the corresponding author on request.

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
109
Fig. 1.
Left column: the upper plot is the adaptive Metropolis (AM) density and the lower plot is the variational Bayesian adaptive Metropolis (VBAM)
density. The dashed line shows the true density while the black line shows the approximated density from samples. Right column: the upper plot shows
the AM density difference between the true and approximated densities while the lower plot shows the density difference for the VBAM method. The
density difference for the VBAM algorithm fluctuates around zero while the AM density difference does not. Therefore VBAM algorithm performs better
than AM.
example, we apply AM and VBAM to the 100-dimensional example from Roberts and Rosenthal (2009). In the third example,
we apply VBAM to a well-known benchmark of sampling from a 20-dimensional banana shaped distribution and compare
the empirical results with the AM results of Roberts and Rosenthal (2009). Finally, we apply the VBAM algorithm to two real
data examples. In those examples we analyze the chemical reaction model found in Himmelblau (1970) and the bacteria
growth model found in Berthouex and Brown (2002).
5.1. One-dimensional projection of the density function
In this example, we consider a one-dimensional projection of the density function similar to that of Haario et al. (2001).
We aim to sample from a density π on the rectangle R = [−18, 18]×[−3, 3] ⊂R2 as follows. Let S = [−0.5, 0.5]×[−3, 3]
and set
π(x) =

1,
if x ∈S,
36,
if x ∈R \ S.
For the AM algorithm, we initialized the proposal covariance as Σ0 = I and updated it using Eq. (4) with the values of ε
and λk set to 0.0001 and 2.8322, respectively. For the VBAM algorithm we used a random walk model defined as
xk = xk−1 + qk−1,
yk = xk + rk,
(27)
where A = I and Q = 0.0012 I, H = I. It is easy to show that this model is uniformly completely observable and controllable
for any bounded sequence of Σk. We initialized the prior mean and covariance as m0 =

0
0T and P0 = I while we set
ν0 = 4.
To compare the performance of AM and VBAM algorithms, we generated 106 samples from each algorithm. As in Haario
et al. (2001), the comparison is done through computing the density differences of each method and the results are shown
in Fig. 1. The left column plots show the empirical densities produced by the adaptive Metropolis (AM) algorithm and
the proposed variational Bayesian adaptive Metropolis (VBAM) algorithm, respectively. The right column shows, for each
method, the difference between the real target density and the empirical densities. The VBAM algorithm indeed seems to
give samples that follow well the true distribution, that is, the empirical density approximates well the true density. Because
the density differences are around zero the results show the VBAM algorithm performs better than the AM algorithm.
5.2. 100-dimensional Gaussian target distribution
In this example, we consider a high-dimensional Gaussian target distribution as discussed by Roberts and Rosenthal
(2009). Here, the target distribution is N(0, MMT), where the entries of the d×d matrix M are generated from unit Gaussian

110
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
Fig. 2. First coordinate of the MCMC chains in the 100-dimensional Gaussian target distribution example.
distributions (Roberts and Rosenthal, 2009). We compare our results with a version of the AM algorithm considered in the ar-
ticle using the C-language implementation (adaptchol.c) provided by the authors. For the experiment we also implemented
a C-language version of our VBAM adaptation so that the only difference in the runs is the covariance adaptation.
In the AM algorithm of Roberts and Rosenthal (2009), at iteration k, the proposal distribution is defined as
qk(θ∗| θ) =



N

θ,
0.12
d
I

,
if k ≤2d,
(1 −β) N

θ,
2.382
d
Σk

+ β N

θ,
0.12
d
I

,
if k > 2d,
(28)
where Σk is the current empirical covariance and β is a small positive constant. In the numerical experiment, we chose the
dimensionality to be d = 100 and β = 0.05 following Roberts and Rosenthal (2009). For the VBAM algorithm, the state
space model used is the Gaussian random walk model (27) with A = I, H = I, and Q = 10−9 I, and λk is updated using Eq. (7).
We initialized λ0 = 2.382/2, the value of α was 0.234 and we used the following γk (Liang et al., 2010):
γk =
k0
max{k0, kτ},
where k0 > 1 is the pre-specified value and τ ∈(1/2, 1]. The used values for k0 and τ were 1000 and 0.99, respectively.
Trace plots of the first coordinate of the AM and VBAM chains are shown in Fig. 2. It can be seen that the VBAM seems
to stabilize to the stationary distribution a bit faster than the AM algorithm. This is confirmed by the suboptimality factors
(Roberts and Rosenthal, 2009) shown in Fig. 3. In this examples, the suboptimality factor of VBAM reaches the value of
around one faster than AM which implies that its adaptation works faster.
5.3. 20-dimensional banana-shaped distribution
In this example, we consider a banana-shaped distribution which is also often used as an example to study the perfor-
mance of adaptive MCMC algorithms (Haario et al., 1999; Roberts and Rosenthal, 2009; Haario et al., 2001; Bornkamp, 2011).
The banana-shaped distribution density function is given as
f(x1, x2, . . . , xd) ∝exp

−
x2
1
200 −
1
2(x2 + B x2
1 −100 B)2 −
1
2(x2
3 + x2
4 + · · · + x2
d)

,
where B > 0 is the ‘bananicity’ constant.
We used the VBAM algorithm to sample from this distribution and compared the results with the AM algorithm of Roberts
and Rosenthal (2001) whose proposal distribution was (28). The VBAM state space model was the random walk model (27)
with A = I, H = I and Q = 10−9 I. As in Roberts and Rosenthal (2001), we set d = 20 and B = 0.1 and ran the AM and
VBAM algorithms for 106 iterations. Fig. 4 shows the trace plots for the first components x1 and x2. It can be observed that
the AM and VBAM algorithms both mix with an approximately same speed.

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
111
Fig. 3. Suboptimality factors in the Gaussian target distribution example.
Fig. 5 shows the scatter plots for AM and VBAM algorithms. As can be seen, the shapes of the plots indeed have a banana-
like shape and they cover the support of the distribution well. In this example we did not find any significant difference
between AM and VBAM, but still it shows that also the VBAM algorithm works well in this challenging sampling problem.
5.4. Chemical reaction model
As the first real data example2 we analyze a chemical reaction model studied in the book by Himmelblau (1970). The
model found on pp. 326–327 (Himmelblau, 1970) is a result of deterministic modeling of chemical reactions which involve
six species (A, B, C, D, E and F) and three type of reactions.
The chemical reactions are
A + B
k1
−→C + F,
A + C
k2
−→D + F,
A + D
k3
−→E + F,
(29)
and they are modeled with the differential equations
dA
dt = −k1A B −k2A C −k3A D,
dB
dt = −k1A B,
dC
dt = k1A B −k2A C,
dD
dt = k2A C −k3A D,
dE
dt = k3A D.
(30)
The task is to estimate the parameters k1, k2 and k3 from experimental data having the initial values given as C(0) = D(0) =
0, A(0) = 0.02090 mol/l and B(0) = A(0)/3. The data are shown in Fig. 6.
The reported parameter values were k1 = 14.7, k2 = 1.53 and k3 = 0.294 (Himmelblau, 1970). We applied the VBAM
algorithm to sample the parameters and then compared the results with the reported ones. We used the random walk state
space model (27) with Ak = I, Hk = I and Qk = 10−9I.
2 Both the real data models can also be found in http://helios.fmi.fi/~lainema/mcmc/examples.html, where the author analyzes them using MCMC.

112
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
Fig. 4. Trace plots of the first component of the banana distribution. The first row is the AM plot and the second is for VBAM. The mixing of the samples
in both AM and VBAM is good.
Fig. 5. Scatter plots for the first two components in the banana example. The first plot is for AM, the second plot is for VBAM.
Fig. 6. Experimental data for concentration of A in the chemical reaction model.

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
113
Fig. 7. Marginal distributions for parameters k1, k2, and k3 in the chemical reaction model estimated from the samples of VBAM plotted together with the
sample means, maximum a posterior (MAP) estimates, and reported parameter values (RPV) in the chemical reaction model. Here, the gray lines denote
the distributions, the black dotted lines the sample means, the light dotted lines the RPV, and the black lines the MAP estimates.
Fig. 8. VBAM scatter plots for the chemical reaction model. It is observed that there is some correlation between the parameters.
The marginal distribution plots estimated from 100,000 VBAM samples are shown in Fig. 7. In the figure, it can be seen
that the estimates are well consistent with the reported parameter values. The scatter plots for VBAM samples are shown
in Fig. 8. It is observed that there exists correlation between parameters, for example, k2 and k3 seems to have negative
correlation. However, the correlation is not particularly strong and hence the parameters are quite well identifiable from
the data.
5.5. Monod model
As the second real data example, we analyze the bacteria growth models studied in the book by Berthouex and Brown
(2002). The estimation of the parameters of the Monod model has been studied for instance, in Chapter 35 of the book
(Berthouex and Brown, 2002), where the authors used the experimental data shown in Fig. 9. The data were obtained by
operating a continuous flow biological reactor at steady-state conditions and the following Monod model was proposed to
fit the data:
y =
θ1 x
θ2 + x + ε.
(31)
In Eq. (31), y is the growth rate expressed per hour and is obtained at substrate concentration x, θ1 is the maximum growth
rate expressed per hour, θ2 is the saturation constant, and ε is a Gaussian noise (Berthouex and Brown, 2002). In Berthouex
and Brown (2002), the parameters were estimated to be θ1 = 0.153 and θ2 = 55.4.
We estimated these parameters using the VBAM algorithm. We used the random walk state space model (27) for VBAM
algorithm, where Ak = I, Hk = I and Qk = 10−9I.
The marginal distribution estimates computed from 100,000 VBAM samples together with the sample means, MAP
estimates, and reported parameter values as well as the scatter plot are shown in Fig. 10. The plots show that the reported
parameter values are well within the estimated parameter distribution and that there exists strong correlation between the
parameters.

114
I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
Fig. 9. Data for the bacteria growth (Monod) model.
Fig. 10. The first two plots from left show the marginal distributions for parameters θ1 and θ2 in the Monod model estimated from the samples of VBAM,
plotted along with the sample means, MAP, and reported parameter values (RPV). The gray lines denote the distributions, the black dotted lines the sample
means, the light dotted lines the RPV, and the black lines the MAP estimates. As can be seen, the parameter estimates are consistent with RPV. The plot on
the right shows the scatter plot. The parameters θ1 and θ2 seem to have a strong positive correlation.
6. Conclusion and discussion
In this paper, we have proposed a new adaptive Markov chain Monte Carlo (MCMC) method called variational Bayesian
adaptive Metropolis (VBAM) algorithm, which adapts the covariance matrix of the Gaussian proposal distribution in the
Metropolis algorithm with the variational Bayesian adaptive Kalman filter (VB-AKF, Särkkä and Hartikainen, 2013). We
have shown that the method is indeed a valid adaptive MCMC method in the sense that it samples from the correct target
distribution by proving a strong law of large numbers for it. We have numerically tested the performance of the method in
widely used example models and compared it to two other adaptive MCMC schemes. In the first two simulated experiments,
our method turned out to perform better than the AM algorithm of Haario et al. (1999, 2001). In the third simulated example,
the performance was similar to the performance of the AM algorithm of Roberts and Rosenthal (2001). In the two real data
examples, VBAM also produced results which are consistent with results reported in the literature.
The advantage of the proposed method is that it has more parameters to tune, which gives more freedom. In particular,
the tight relationship with the linear systems theory and Kalman filtering allows one to borrow good state space models from
target tracking literature (Jazwinski, 1970; Bar-Shalom et al., 2004) and use them as the models in VBAM. The correctness
of the method can be easily verified by checking that the resulting state space model is uniformly completely observable
and controllable, which is a standard step in building state space models. Sometimes, however, the freedom of choosing
algorithm parameters can be seen as a disadvantage, because manual tuning of the VB-AKF model parameters can turn out
to be challenging. Fortunately, in many cases a simple Gaussian random-walk state space model is a good default choice.
The computational requirements of the VBAM method are typically O(d3), where d is the parameter dimensionality,
while the complexity of a usual implementation of AM is O(d2). This is because the VB-AKF step is needed, which amounts

I.S. Mbalawata et al. / Computational Statistics and Data Analysis 83 (2015) 101–115
115
to a (constant) number of Kalman filter updates at each iteration and these operations are computationally more demanding
than what is needed in the standard AM. However, these operations are still quite cheap and when the model is complex
enough to require MCMC sampling, the evaluation of the distribution can be expected to dominate the computation time
anyway. Furthermore, these operations can be typically optimized for a given state space model. For example, in the random
walk model we do not actually need to perform all the matrix operations in full generality, because the model matrices are
diagonal. Even though the basic implementation of the method is straightforward and not significantly harder than imple-
mentation of an AM algorithm, developing an optimized version of the VBAM method for a particular type of state space
model can be more complicated.
An advantage of the method is that it can also easily be generalized in various ways. For example, we could extend
it by replacing the linear Kalman filter with non-linear Kalman filters such as the extended Kalman filter, a sigma-point
(unscented) filter, or even particle filters (Särkkä, 2013). In fact, provided that we can ensure that the mean and covariance
of the corresponding non-linear Kalman filter remain bounded, replacing the linear VB-AKF with a non-linear one (Särkkä
and Hartikainen, 2013) should lead to a valid VBAM algorithm as well. Similarly, a (Rao–Blackwellized) particle filter could
be used for estimating the noise covariance (Särkkä, 2013) and provided that the adaptation can be shown to diminish in
time. However, with non-linear state space model it will be hard to find good state space models for the algorithm. This path
is interesting though, because it can lead to a completely new family of adaptive MCMC algorithms, which utilize different
kinds of filters in the proposal adaptation.
Acknowledgments
We are grateful to Arno Solin for proofreading this paper. Isambi S. Mbalawata was supported by the Finnish Centre
of Excellence on Inverse Problems Research of Academy of Finland project 23B510A. Matti Vihola was supported by the
Academy of Finland project 250575. Simo Särkkä was supported by the Academy of Finland projects 266940 and 273475.
References
Andrieu, C., Moulines, E., 2006. On the ergodicity properties of some adaptive MCMC algorithms. Ann. Appl. Probab. 16, 1462–1505.
Andrieu, C., Thoms, J., 2008. A tutorial on adaptive MCMC. Stat. Comput. 18, 343–373.
Atchadé, Y., Fort, G., 2010. Limit theorems for some adaptive MCMC algorithms with subgeometric kernels. Bernoulli 16, 116–154.
Atchadé, Y., Rosenthal, J.S., 2005. On adaptive Markov chain Monte Carlo algorithms. Bernoulli 11, 815–828.
Bai, Y., 2009. Convergence of Adaptive Markov chain Monte Carlo algorithms (Doctoral dissertation). University of Toronto.
Bai, Y., Roberts, G.O., Rosenthal, J.S., 2011. On the containment condition for adaptive Markov chain Monte Carlo algorithms. Adv. Appl. Stat. 21, 1–54.
Bar-Shalom, Y., Li, X.R., Kirubarajan, T., 2004. Estimation with Applications to Tracking and Navigation: Theory, Algorithms and Software. John Wiley &
Sons.
Berthouex, P.M., Brown, L.C., 2002. Statistics for Environmental Engineers, second ed. CRC Press.
Bornkamp, B., 2011. Approximating probability densities by iterated Laplace approximations. J. Comput. Graph. Statist. 20.
Brooks, S., Gelman, A., Jones, G., Meng, X., 2011. Handbook of Markov Chain Monte Carlo. Chapman Hall/CRC.
Cai, B., Meyer, R., Perron, F., 2006. Metropolis–Hastings algorithms with adaptive proposals. Stat. Comput. 18, 421–433.
Fort, G., Moulines, E., Priouret, P., 2011. Convergence of adaptive and interacting Markov chain Monte Carlo algorithms. Ann. Statist. 39, 3262–3289.
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B., 2013. Bayesian Data Analysis, third ed. Chapman & Hall/CRC Press.
Gelman, A., Roberts, G.O., Gilks, W.R., 1996. Efficient Metropolis jumping rules. In: Bayesian Statistics, Vol. 5. pp. 599–607.
Gilks, W.R., Richardson, S., Spiegelhalter, D.J., 1996. Markov Chain Monte Carlo in Practice. Chapman & Hall/CRC Press.
Gilks, W.R., Roberts, G.O., George, E.I., 1994. Adaptive direction sampling. Statistician 43, 179–189.
Gilks, W.R., Roberts, G.O., Sahu, S.K., 1998. Adaptive Markov chain Monte Carlo through regeneration. J. Amer. Statist. Assoc. 93, 1045–1054.
Griffin, J.E., Walker, S.G., 2013. On adaptive Metropolis–Hastings methods. Stat. Comput. 23, 123–134.
Haario, H., Laine, M., Mira, A., Saksman, E., 2006. DRAM: efficient adaptive MCMC. Stat. Comput. 16, 339–354.
Haario, H., Saksman, E., Tamminen, J., 1999. Adaptive proposal distribution for random walk Metropolis algorithm. Comput. Statist. 14, 375–396.
Haario, H., Saksman, E., Tamminen, J., 2001. An adaptive Metropolis algorithm. Bernoulli 7, 223–242.
Himmelblau, D.M., 1970. Process Analysis by Statistical Methods. John Wiley & Sons.
Holden, L., Hauge, R., Holden, M., 2009. Adaptive independent Metropolis–Hastings. Ann. Appl. Probab. 19, 395–413.
Jazwinski, A.H., 1970. Stochastic Processes and Filtering Theory. Academic Press.
Kalman, R.E., 1960. A new approach to linear filtering and prediction problems. J. Basic Eng. 82, 35–45.
Liang, F., Liu, C., Carroll, R.J., 2010. Advanced Markov Chain Monte Carlo Methods: Learning from Past Samples. John Wiley & Sons.
Luenberger, D.G., Ye, Y., 2008. Linear and Nonlinear Programming, third ed. Springer.
Ren, Y., Ding, Y., Liang, F., 2008. Adaptive evolutionary Monte Carlo algorithm for optimization with applications to sensor placement problems. Stat.
Comput. 18, 375–390.
Roberts, G.O., Rosenthal, J.S., 2001. Optimal scaling for various Metropolis–Hastings algorithms. Statist. Sci. 16, 351–367.
Roberts, G.O., Rosenthal, J.S., 2007. Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms. J. Appl. Probab. 44, 458–475.
Roberts, G.O., Rosenthal, J.S., 2009. Examples of adaptive MCMC. J. Comput. Graph. Statist. 18, 349–367.
Saksman, E., Vihola, M., 2010. On the ergodicity of the adaptive Metropolis algorithm on unbounded domains. Ann. Appl. Probab. 20, 2178–2203.
Särkkä, S., 2013. Bayesian Filtering and Smoothing. Cambridge University Press.
Särkkä, S., Hartikainen, J., Non-linear noise adaptive Kalman filtering via variational Bayes. In: Proceedings of MLSP 2013, pp. 1–6.
Särkkä, S., Nummenmaa, A., 2009. Recursive noise adaptive Kalman filtering by variational Bayesian approximations. IEEE Trans. Automat. Control 54,
596–600.
Vihola, M., 2011. On the stability and ergodicity of adaptive scaling Metropolis algorithms. Stochastic Process. Appl. 121, 2839–2860.
Vihola, M., 2012. Robust adaptive Metropolis algorithm with coerced acceptance rate. Stat. Comput. 22, 997–1008.
Vrugt, J.A., Braak, C.J.T., 2011. DREAM(D): an adaptive Markov chain Monte Carlo simulation algorithm to solve discrete, noncontinuous, and combinatorial
posterior parameter estimation problems. Hydrol. Earth Syst. Sci. 15, 3701–3713.
Vrugt, J.A., Braak, C.J.T., Diks, C., Robinson, B.A., Hyman, J.M., Higdon, D., 2009. Accelerating Markov chain Monte Carlo simulation by differential evolution
with self-adaptive randomized subspace sampling. Int. J. Nonlinear Sci. Numer. Simul. 10, 273–290.

PUBL. V
Isambi S.Mbalawata and Simo Särkkä, Weight Moment Conditions for L4 Convergence of Parti-
cle Filters for Unbounded Test Functions. In 2014 Proceedings of the 22nd European Signal Pro-
cessing Conference (EUSIPCO). IEEE, 1207–1211, 2014.
c⃝2014 IEEE European Signal Processing Conference. All rights reserved.
Reprinted, with the permission of IEEE European Signal Processing Conference


WEIGHT MOMENT CONDITIONS FOR L4 CONVERGENCE OF PARTICLE FILTERS FOR
UNBOUNDED TEST FUNCTIONS
Isambi S. Mbalawata
Lappeenranta University of Technology
P.O. Box 20, FI-53851 Lappeenranta, Finland
Simo S¨arkk¨a
Aalto University
P.O.Box 12200, FI-00076 Aalto, Finland
ABSTRACT
Particle ﬁlters are important approximation methods for solv-
ing probabilistic optimal ﬁltering problems on nonlinear
non-Gaussian dynamical systems. In this paper, we derive
novel moment conditions for importance weights of sequen-
tial Monte Carlo based particle ﬁlters, which ensure the L4
convergence of particle ﬁlter approximations of unbounded
test functions. This paper extends the particle ﬁlter conver-
gence results of Hu & Sch¨on & Ljung (2008) and Mbalawata
& S¨arkk¨a (2014) by allowing for a general class of potentially
unbounded importance weights and hence more general im-
portance distributions. The result shows that provided that the
seventh order moment is ﬁnite, then a particle ﬁlter for un-
bounded test functions with unbounded importance weights
are ensured to converge.
Index Terms— Particle ﬁlter convergence, unbounded
importance weights, moment conditions
1. INTRODUCTION
Dynamic state estimation problems are of great interest in
many real life applications such as navigation, target tracking,
brain imaging, spread of infectious diseases, biological pro-
cesses, telecommunication, audio signal processing, stochas-
tic optimal control, machine learning, and physical systems
[1]. In these problems, the state vector at time t is represented
by xt ∈Rn and satisﬁes the stochastic equation
xt ∼f(xt | xt−1),
(1)
where f(xt | xt−1) is the transition probability density of
the corresponding Markovian stochastic process modeling the
dynamics of the system. At each time step t we get a measure-
ment yt ∈Rm from the measurement model
yt ∼g(yt | xt),
(2)
This work was supported by the projects 266940 and 273475, and the
Finnish Centre of Excellence on Inverse Problems Research of the Academy
of Finland.
where g(yt | xt) is a conditional probability density modeling
the distribution of measurements.
The Bayesian approach to dynamic state estimation prob-
lem involves the construction of the probability density func-
tion of xt, given y1:t ≜(y1, y2, . . . , yt) [2]. This problem
is known as the Bayesian ﬁltering problem. If we denote the
probability density function of xt given y1:t by p(xt | y1:t),
then the construction of p(xt | y1:t) can be recursively done
using the Bayesian ﬁltering equations [1]
p(xt | y1:t−1) =
Z
f(xt | xt−1) p(xt−1 | y1:t−1) dxt−1,
p(xt | y1:t) =
g(yt | xt) p(xt | y1:t−1)
R
g(yt | xt) p(xt | y1:t−1) dxt
,
(3)
where the ﬁrst step is often referred to as the prediction step
(or time update) and the second step is the measurement up-
date step (or correction step).
In this paper, we aim to analyze the theoretical conver-
gence properties of particle ﬁlters. In such theoretical analy-
sis, it is convenient to rewrite the Bayesian ﬁltering equations
in terms of probability measures as follows (see., e.g, [3–6]).
Let ν be a measure and φ be a measurable function. Then we
denote
(ν, φ) ≜
Z
φ dν,
and
f φ(x) ≜
Z
f(dz | x)φ(z). (4)
Let πt|t−1 denote the measure corresponding to the probabil-
ity density p(xt | y1:t−1) and πt|t the measure correspond-
ing to the density p(xt | y1:t), then, using notations (4), the
Bayesian ﬁltering equations (3) can be written as
(πt|t−1, φt) = (πt−1|t−1, f φt),
(πt|t, φt) = (πt|t−1, φt g)
(πt|t−1, g) .
(5)
2. PARTICLE FILTERING
In the most practical cases, especially in nonlinear or non-
Gaussian models, the closed form solution of (3) or (5) is in-
tractable. Thus, several approximate methods have been pro-
posed and the most used classes of approximate methods are

Gaussian approximation based extended/non-linear Kalman
ﬁlters (e.g., [1,2]), and sequential Monte Carlo based particle
ﬁlters (e.g., [1, 7, 8]). In this paper we study particle ﬁlters,
where the main idea is to approximate πt|t by a weighted set
of Monte Carlo samples {(xi
t, wi
t)
:
i = 1, . . . , N}, and,
based on these samples, we can approximate the statistics of
the distribution via (weighted) sample averages.
Given a set of assumptions, it is sometimes possible to
show that a particle ﬁlter converges to the exact ﬁltering dis-
tribution, when the number of particles N tends to inﬁnity [9].
Typically, a particle ﬁlter is said to convergence if the expec-
tations of a suitable class of test functions φ(.) converges in
this limit in some suitable topology:
lim
N→∞
 N
X
i=1
wi
t φ(xi
t)
!
= E[φ(xt) | y1:t].
(6)
General convergence results for particle ﬁlters for test bounded
functions have been given, for example, in references [3, 9–
15] while for unbounded test functions, results can be found
in [4–6].
3. MODIFIED PARTICLE FILTER
The L4-convergence of particle ﬁlter in the paper [4] required
the modiﬁcation of standard bootstrap ﬁlter algorithm to cope
with unbounded test functions. The convergence results were
obtained by computing the bounds for the conditional expec-
tation of the fourth power of error (L4) in the test function
estimates.
These results of [4] were extended in the paper [6] to
the case of more general importance distributions q(xt |
xt−1, y1:t). The results of [6] showed that with general im-
portance distributions the (modiﬁed) particle ﬁlter converges
provided that the importance weights are bounded.
The modiﬁed particle ﬁlter algorithm as presented in [6]
is given in Algorithm 1. The modiﬁed particle ﬁlter is con-
structed such that we always have
(πt|t−1, wt) ≈(˜πN
t|t−1, wt) = 1
N
N
X
i=1
w(i)
t
≥γt > 0, (8)
where γt > 0 is a chosen threshold [4,6].
In this paper, we extend the L4 particle ﬁlter convergence
proof of [6] to the case of (potentially) unbounded importance
weights. We use the same techniques and some assumptions
from [6], but impose a weaker assumption that the seventh
order moment is ﬁnite.
4. CONVERGENCE RESULT
The convergence proof of Algorithm 1 with bounded impor-
tance weights is found in the paper [4] for bootstrap type of
Algorithm 1 General Modiﬁed Particle Filter
1. Initialize the particles, {x(i)
0 }N
i=1 ∼π0(dx0)
2. Predict the particles by drawing independent samples ac-
cording to
¯x(i)
t
∼
N
X
j=1
αi
j q(xt | xt−1, yt),
where αi = (α1
1, α2
2, . . . , αi
N) are the weights such that
αi
j ≥0,
N
X
j=1
αi
j = 1,
N
X
i=1
αi
j = 1,
and
1
N
N
X
i=1
N
X
j=1
αi
j q(xt | x(j)
t−1, yt) = 1
N
N
X
j=1
q(xt | x(j)
t−1, yt)
(7)
3. If (1/N) PN
i=1 ¯wt ≥γt, proceed to step 4 otherwise re-
turn to step 2. Note that ¯wt is the value computed at ¯x(i)
t .
4. Rename ˜x(i)
t
= ¯x(i)
t , and compute the importance weights
{w(i)
t }N
i=1 at ˜xi
t, and then normalize them.
5. Resample, x(i)
t
∼˜πN
t|t(dxt) = PN
i=1 ˜w(i)
t
δ˜xi
t(dxt)
6. Set t = t + 1 and repeat from step 2).
importance distributions and in the paper [6] for general im-
portance distributions. Here we follow a similar path as in
the proof in [6], but modify it such that we can replace the
assumption on the boundeness of the important weights with
a moment condition.
To guarantee the convergence, we impose the following
assumptions.
Assumption 4.1. For any given y1:s we have (πs|s−1, gs) >
γs > 0, where s = 1, . . . , t.
Assumption 4.2. The dynamical model density f and mea-
surement model density g are bounded, that is, there exists
constants cf and cg such that ∥f∥≤cf and ∥g∥≤cg, where
∥· ∥denotes the supremum norm.
Assumption 4.3. The test function of interest φ(·) satisﬁes
supxs |φ(xs)|4g(ys | xs) < C(y1:s).
Assumption 4.4. For any potentially unbounded importance
weights wt(xt, xt−1) deﬁned as
wt(xt, xt−1) = g(yt | xt) f(xt | xt−1)
q(xt | xt−1, yt)
,
(9)
the seventh order moment E[(wt(xt, xt−1))7 | xt−1] is ﬁnite,
where the expectation is over q(.).

We now present the following convergence theorem,
which shows the bound for error of the fourth moment condi-
tional mean.
Theorem 4.5. Consider the modiﬁed particle ﬁlter in Algo-
rithm 1 and suppose that Assumptions 4.1–4.4 are satisﬁed.
Then
i. For sufﬁciently large N, the algorithm will not run into
an inﬁnite loop in steps 2-3.
ii. Let L4
t(g) be the class of functions satisfying Assumption
4.3. For any φ ∈L4
t(g), there exists a constant ct|t,
independent of N such that
E
(πN
t|t, φ) −(πt|t, φ)

4
≤ct|t
||φ||4
t,4
N 2
,
(10)
where
∥φ∥t,4 = max
n
1, (πs|s, |φ|4)1/4, s = 0, 1, . . . , t
o
.
Proof. The proofs for initialization and resampling steps are
the same as in [4]. Therefore, here, we only prove the conver-
gence of the (combined) prediction and update steps as in [6].
That is, we prove the convergence of
(πN
t|t, φ) −(πt|t, φ) =
(ˆπN
t|t, φ)
(ˆπN
t|t, 1) −(ˆπt|t, φ)
(ˆπt|t, 1) ,
(11)
where ˆπN
t|t = (πN
t−1|t−1, w qN) and ˆπt|t = (πt−1|t−1, w q).
This is attained by ﬁnding the bounds for the following terms:
E
(πN
t|t, φ) −(πt|t, φ)

4
and
E[(πN
t|t, |φ|4)].
(12)
As in [6], it is enough to ﬁnd the bounds for the following
terms:
E
(ˆπN
t|t, φ) −(ˆπt|t, φ)

4
and
E
h
(ˆπN
t|t, |φ|4)
i
, (13)
and
E
(ˆπN
t|t, 1) −(ˆπt|t, 1)

4
and
E
h
(ˆπN
t|t, 1)
i
.
(14)
We only study the boundedness of (13). The bounds for (14)
are obtained by setting φ = 1 in (13). We denote Ft−1 as the
σ-algebra generated by xi
t−1. We write (ˆπN
t|t, φ)−(ˆπt|t, φ) as
Π1 + Π2 + Π3, where
Π1 = (ˆπN
t|t, φ) −1
N
N
X
i=1
E[φ(˜xi
t) w(˜xi
t, xt−1) | Ft−1], (15)
Π2 = 1
N
N
X
i=1
E[φ(˜xi
t) w(˜xi
t, xt−1) | Ft−1]
−1
N
N
X
i=1
(πN,αi
t−1|t−1, f φ g),
(16)
Π3 = 1
N
N
X
i=1
(πN,αi
t−1|t−1, f φ g) −(ˆπt|t, φ).
(17)
Let ¯xi
t ∼(πN,αi
t−1|t−1, q), then
E[φ(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1] = (πN,αi
t−1|t−1, f φ g).
(18)
We next compute the bounds for E[|Π1|4, E[|Π2|4 and
E[|Π3|4, as in [6]. For E[|Π1|4, we use Lemmas 7.1, 7.2,
7.3, 7.4 and 7.5 from [4] and Equation (18) to get
E[|Π1|4|Ft−1]
≤24
N 4
N
X
i=1
E


φ(˜xi
t) w(˜xi
t, xi
t−1)

4
| Ft−1


+ 24
N 4


N
X
i=1
E


φ(˜xi
t) w(˜xi
t, xi
t−1)

2
| Ft−1




2
≤
24
N 4(1 −ϵ)2
N
X
i=1
E

|φ(¯xi
t) w(¯xi
t, xi
t−1)|4|Ft−1

+
24
N 4(1 −ϵ)2
 N
X
i=1
E

|φ(¯xi
t) w(¯xi
t, xi
t−1)|2|Ft−1

!2
.
From Assumption 4.4, we can deduce the following.
Lemma 4.6. Provided that E[(wt(¯xi
t, xi
t−1))7 | xt−1] is
bounded, then E[(wt(¯xi
t, xi
t−1))7 | Ft−1] is bounded too.
Proof.
E[(wt(x(i)
t , xt−1))7 | Ft−1]
≤sup
x0:t

E

wt(x(i)
t , xt−1)
7
| xt−1

≤sup
x0:t
c7
w ≤C7
w.
Remark 4.7. If the seventh moment is ﬁnite then the lower
moments are ﬁnite too.
Proof. Results are easily obtained from H¨older’s and Jensen’s
inequalities,
With Lemma 4.6 and the Cauchy–Schwarz inequality, we
get
E

|φ(¯xi
t) w(¯xi
t, xi
t−1)|4|Ft−1

≤
q
E

φ8(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

×
q
E

w7(¯xi
t, xi
t−1) | Ft−1

≤C7/2
w
q
E

φ8(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

.
E

|φ(¯xi
t) w(¯xi
t, xi
t−1)|2|Ft−1

≤
q
E

φ4(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

×
q
E

w3(¯xi
t, xi
t−1) | Ft−1

≤C3/2
w
q
E

φ4(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

.

Thus
E[|Π1|4|Ft−1]
≤
24C7/2
w
N 4(1 −ϵ)2
N
X
i=1
q
E

φ8(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

+
24C3
w
N 4(1 −ϵ)2
 N
X
i=1
q
E

φ4(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

!2
.
But
N
X
i=1
q
E

φ8(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

≤N +
N
X
i=1
E

φ8(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

 N
X
i=1
q
E

φ4(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

!2
≤(1 + N)
N
X
i=1
E

φ4(¯xi
t) w(¯xi
t, xi
t−1) | Ft−1

.
Then
E[|Π1|4|Ft−1]
≤
24C7/2
w
N 2(1 −ϵ)2 + 24C7/2
w ∥f∥∥g∥
N 2(1 −ϵ)2
Mt−1|t−1∥φ∥8
t−1,4
+ 24C3
w∥f∥∥g∥
N 2(1 −ϵ)2 Mt−1|t−1∥φ∥4
t−1,4
+ 24C3
w∥f∥∥g∥
N 2(1 −ϵ)2 Mt−1|t−1∥φ∥4
t−1,4
=
 
c1
∥φ∥4
t−1,4
+ c2∥φ∥4
t−1,4 + c5
!
∥φ∥4
t−1,4
N 2
= ˜CΠ1
∥φ∥4
t−1,4
N 2
.
(19)
The computation of bounds for E[|Π2|4 and E[|Π3|4 is the
same as in [6]. Therefore
E
Π2

4
| Ft−1

≤˜CΠ2
∥φ∥4
t−1,4
N 2
.
(20)
E
Π3

4
| Ft−1

≤˜CΠ3
∥φ∥4
t−1,4
N 2
.
(21)
By combining Equations (19), (20), and (21) via Minkowski’s
inequality, we get
E
(ˆπN
t|t, φ) −(ˆπt|t, φ)

4 1
4
≤ˆC1/4
t|t
∥φ∥t−1,4
N 1/2
,
which implies
E
(ˆπN
t|t, φ) −(ˆπt|t, φ)

4
≤ˆCt|t
∥φ∥4
t−1,4
N 2
.
(22)
From [6], the bound for E[(ˆπN
t|t, |φ|4)] is
E
h(ˆπN
t|t, |φ|4)

i
≤Mt|t||φ||4
t−1,4.
(23)
Note that if we set φ = 1 in (22) and (23), we get bounds for
(14). Hence the remaining task is to ﬁnd the bounds for (12),
which is done exactly the same way as in [6]. Thus
E
(πN
t|t, φ) −(πt|t, φ)

4
≤Ct|t
∥φ∥4
t−1,4
N 2
,
E
h
(πN
t|t, |φ|4) −(πt|t, |φ|4)
i
≤¯
Mt|t∥φ∥4
t−1,4,
which complete the proof of Theorem 4.5.
5. NUMERICAL EXAMPLE
A relevant question is now that what is the actual beneﬁt of the
current extension in practical particle ﬁltering models. The
clear beneﬁt is that it extends the class of allowed importance
distributions to the class which does not ensure that the im-
portance weights are uniformly bounded. For example, the
weights might become inﬁnite in isolated points provided that
the required expectations of them remain bounded.
However, to get an idea what kind of importance weights
have this kind of property, consider
v(x) = |x|−1/2 exp(−|x|).
(24)
Clearly this function is everywhere positive, but it also con-
tains an inﬁnite value at x = 0, and hence it is not bounded.
However, its integral is ﬁnite, which can be seen by comput-
ing its integral by reducing it into the deﬁnition of the Gamma
function:
Z ∞
−∞
|x|−1/2 exp(−|x|) dx = 2 √π.
(25)
The function deﬁned by (24) is thus an example of a positive
function which is unbounded, but has a bounded integral (see
Figure 1). It is now easy to see that it is possible to construct
models for which the importance weights are point-wise un-
bounded but still satisfy Assumption 4.4. Examples of practi-
cal models which lead to this kind of importance weights will
be considered in future work.
6. CONCLUSION
In this paper, we have extended the L4 particle ﬁlter conver-
gence proof of [6] to the case of potentially unbounded impor-
tance weights, by replacing the boundedness condition with

Fig. 1. Example of a point-wise unbounded function with a
ﬁnite integral over (−∞, ∞).
ﬁniteness of conditional weight moments. Our proof shows
that provided that the seventh order moment is ﬁnite, then a
particle ﬁlter for unbounded test functions with unbounded
importance weights are ensured to converge.
REFERENCES
[1] Simo S¨arkk¨a, Bayesian ﬁltering and smoothing, Cam-
bridge University Press, 2013.
[2] Andrew H. Jazwinski, Stochastic Processes and Filter-
ing Theory, Academic Press, 1970.
[3] Pierre Del Moral, Feynman–Kac Formulae: Genealog-
ical and Interacting Particle Systems with Applications,
Springer, 2004.
[4] Xiao-Li Hu, Thomas B. Sch¨on, and Lennart Ljung, “A
basic convergence result for particle ﬁltering,”
IEEE
Transactions on Signal Processing, vol. 56, no. 4, pp.
1337–1348, 2008.
[5] Xiao-Li Hu, Thomas B. Sch¨on, and Lennart Ljung, “A
general convergence result for particle ﬁltering,” IEEE
Transactions on Signal Processing, vol. 59, no. 7, 2011.
[6] Isambi Mbalawata and Simo S¨arkk¨a, “On the L4 con-
vergence of particle ﬁlters with general importance dis-
tributions,” in Proceedings of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), 2014, (to appear).
[7] Arnaud Doucet, Simon J. Godsill, and Christophe An-
drieu, “On sequential Monte Carlo sampling methods
for Bayesian ﬁltering,” Statistics and Computing, vol.
10, no. 3, pp. 197–208, 2000.
[8] Arnaud Doucet, Nando de Freitas, and Neil Gordon, Se-
quential Monte Carlo methods in practice, Springer,
2001.
[9] Dan Crisan and Arnaud Doucet, “A survey of conver-
gence results on particle ﬁltering methods for practi-
tioners,” IEEE Transactions Signal Processing, vol. 50,
no. 3, pp. 736–746, 2002.
[10] Dan Crisan and Arnaud Doucet, “Convergence of se-
quential Monte Carlo methods,” Tech. Rep. CUEDIF-
INFENGrrR38, Signal Processing Group, Department
of Engineering, University of Cambridge, 2000.
[11] Pierre Del Moral and Alice Guionnet, “On the stabil-
ity of interacting processes with applications to ﬁlter-
ing and genetic algorithms,” Annales de l’Institut Henri
Poincare (B) Probability and Statistics, vol. 37, no. 2,
pp. 155–194, 2001.
[12] Randal Douc and Eric Moulines, “Limit theorems for
weighted samples with applications to sequential Monte
Carlo methods,” Annals Statistics, vol. 36, no. 5, pp.
2344–2376, 2008.
[13] Randal Douc, Eric Moulines, and Jimmy Olsson, “Op-
timality of the auxiliary particle ﬁlter,” Probability and
Mathematical Statistics, vol. 29, no. 1, pp. 1–28, 2009.
[14] Alan Bain and Dan Crisan, Fundamentals of stochastic
ﬁltering, vol. 60, Springer, 2009.
[15] Pierre Del Moral,
Mean ﬁeld simulation for Monte
Carlo integration, Chapman & Hall/CRC, 2013.

PUBL. VI
Isambi S. Mbalawata and Simo Särkkä, Moment Conditions for Mean Square Convergence of
Particle Filters with Unbounded Importance Weights. Submitted to Signal Processing.

Signal Processing 00 (2014) 1–??
Signal Pro-
cessing
Moment Conditions for Convergence of Particle Filters with
Unbounded Importance Weights
Isambi S. Mbalawataa,∗, Simo S¨arkk¨ab
aDepartment of Computational Engineering, Lappeenranta University of Technology, P.O.Box 20, FI-53851 Lappeenranta, Finland
bDepartment of Biomedical Engineering and Computational Science, Aalto University, P.O.Box 12200, FI-00076 Aalto, Finland
Abstract
In this paper, we derive moment conditions for particle ﬁlter importance weights, which ensure that the particle ﬁlter estimates of
the expectations of bounded Borel functions converge in mean square and L4 sense, and that the empirical measure of the particle
ﬁlter converges weakly to the true ﬁltering measure. The result extends the previously derived conditions by not requiring the
boundedness of the importance weights, but only boundedness of second or fourth order moments. We show that the boundedness
of the second order moments of the weights implies the convergence of the estimates bounded functions in the mean square sense,
and the L4 convergence as well as the empirical measure convergence are assured by the boundedness of the fourth order moments
of the weights. We also present an example class of models and importance distributions where the moment conditions hold, but
the boundedness does not. The unboundedness in these models is caused by point-singularities in the weights which still leave the
weight moments bounded. We show by using simulated data that the particle ﬁlter for this kind of model also performs well in
practice.
c⃝2011 Published by Elsevier Ltd.
Keywords: Particle ﬁlter, convergence, unbounded importance weights, moment condition.
1. Introduction
Particle ﬁlters are sequential Monte Carlo based methods for numerically solving Bayesian ﬁltering problems by
approximating the ﬁltering distribution using a weighted set of Monte Carlo samples {(˜x(i)
t , ˜w(i)
t ) : i = 1, . . . , N} (see,
e.g., [1, 2]). They approximate the ﬁltering probability measure as a linear combination of delta measures located at
the particles ˜x(i)
t with the weights given by ˜w(i)
t .
In probabilistic sense, the Bayesian estimation problem can be expressed as state inference in a state space model
∗Corresponding author
Email addresses: Isambi.Mbalawata@lut.fi (Isambi S. Mbalawata), Simo.Sarkka@aalto.fi (Simo S¨arkk¨a)
1

/ Signal Processing 00 (2014) 1–??
2
of the form
x0 ∼f0(x0),
xt ∼ft(xt | xt−1),
yt ∼gt(yt | xt),
(1)
where t = 0, 1, 2, . . ., xt ∈Rn is the state of the system, yt ∈Rm is the measurement, f0(x0) is the prior probability
distribution of x0 at initial time step t = 0, ft(xt | xt−1) is the transition probability density modeling the dynamics
of the system, and gt(yt | xt) is the conditional probability density of measurements modeling the distribution of
measurements. In applications, the densities are usually with respect to the Lebesgue measure or the counting measure,
but other reference measures are possible as well.
An important feature of any particle ﬁlter algorithm is that it should converge to the correct distribution as the
number of particles N →∞. This property of particle ﬁlters is well studied and there exists a number of convergence
results for particle ﬁlters (see, e.g., [3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] and references therein). However, the
eﬀect of importance distribution on the convergence is less studied and it is typical either to assume that the dynamic
model is used as the importance distribution, leading to so called bootstrap ﬁlter, or that the unnormalized importance
weights are point-wise bounded. Although in central limit theorem type analysis of particle ﬁlters this point-wise
boundedness is not always assumed (see, e.g., [7]), it is a standard assumption in Lp-type analysis of particle ﬁlters
[5, 1, 6].
In [16], we derived novel moment conditions for importance weights which ensured the L4-convergence of the
modiﬁed particle ﬁlter of [11] for the case of unbounded test functions. Unfortunately, the results in [16] are not
directly applicable to standard particle ﬁlters. In this paper, we give the proofs for the mean square convergence,
L4-convergence, and the empirical measure convergence for the standard particle ﬁlter in the case of potentially
unbounded importance weights and bounded test function. This enlarges the class of state space models in which
particle ﬁlters are ensured to converge. Our proof follows the spirit and many of the ideas of the proofs in [5, 6]
although the assumptions and the main results are diﬀerent.
2. Particle Filtering
Particle ﬁlters are related to the Bayesian ﬁltering problem, which refers to the construction of the ﬁltering prob-
ability density function p(xt | y1:t). The construction of p(xt | y1:t) is done recursively by Bayesian ﬁltering equations
(see, e.g., [2]). Let B(Rn) be the set of bounded Borel measurable functions on Rn, φ ∈B(Rn), πt|t−1 the measure
corresponding to the probability density p(xt | y1:t−1), and πt|t the measure corresponding to the density p(xt | y1:t).
Then the Bayesian ﬁltering equations for state space model (1) can be written as
(πt|t−1, φ) = (πt−1|t−1, ft φ),
(πt|t, φ) = (πt|t−1, φ gt)
(πt|t−1, gt) ,
(2)
2

/ Signal Processing 00 (2014) 1–??
3
where (π, φ) ≜
R
φ dπ, f φ(x) ≜
R
f(z | x) φ(z) dνf (z), and νf is the reference measure used for the density f.
We assume that the state-space model satisﬁes the suﬃcient conditions for the Bayesian ﬁltering equations to have
solutions which are regular densities with respect to the chosen reference measure. For existence of solution to (2),
we have to require that (πt|t−1, gt) > 0.
Due to intractability of Equations (2), for most state space models, we usually need to approximate them. A
particle ﬁlter for approximating the solutions of (2) is given in Algorithm 1. In this paper, we provide the mean
square, L4, and empirical measure convergence results for general importance distribution q(xt | xt−1, y1:t), regardless
of the boundedness of the importance weights.
Algorithm 1 Standard particle ﬁlter
• At t = 0, for i = 1, . . . , N, sample x(i)
0 ∼π0|0(dx0).
• At t ≥1,
– Sample ˜x(i)
t ∼q(xt | x(i)
t−1, y1:t), for i = 1, . . . , N.
– Calculate the unnormalized weights by
wt(˜x(i)
t , x(i)
t−1) = gt(yt | ˜x(i)
t ) ft(˜x(i)
t | x(i)
t−1)
q(˜x(i)
t | x(i)
t−1, y1:t)
,
(3)
for i = 1, . . . , N, and deﬁne unnormalized empirical measure ˆπN
t|t as
ˆπN
t|t = 1
N
N
X
i=1
wt(˜x(i)
t , x(i)
t−1) δ˜x(i)
t ,
(4)
where δx denotes a Dirac delta measure concentrated at x.
– Normalize the weights by ˜w(i)
t
=
w(i)
t
PN
i=1 w(i)
t , where w(i)
t
= wt(˜x(i)
t , x(i)
t−1), and deﬁne empirical probability
measure ˜πN
t|t as
˜πN
t|t =
N
X
i=1
˜w(i)
t δ˜x(i)
t .
(5)
– Do resampling to obtain the resampled particles x(i)
t , and deﬁne empirical probability measure πN
t|t, which
is the approximation to the ﬁltering distribution, as
πN
t|t = 1
N
N
X
i=1
δx(i)
t .
(6)
– t ←t + 1
3

/ Signal Processing 00 (2014) 1–??
4
3. Convergence of Mean Square Error
In this section, we derive a novel mean square convergence theorem for particle ﬁlters. For the Theorem 3.4, we
impose the following assumptions.
Assumption 3.1. The measurement model gt is bounded, that is, there exist a constant cg < ∞such that ∀t ∈N,
∀x ∈Rn, and ∀y ∈Rm we have gt(y | x) ≤cg < ∞.
Assumption 3.2. The resampling procedure satisﬁes (see, e.g., [5, 6] for the suﬃcient conditions for this):
E
(πN
t|t, φ) −(˜πN
t|t, φ)

2
≤Ct
∥φ∥2
N ,
(7)
where ∥φ∥≜supx∈Rn |φ(x)| and Ct < ∞is a constant.
Assumption 3.3. The importance density q is satisﬁes the following condition. Let
wt(xt, xt−1) = g(yt | xt) f(xt | xt−1)
q(xt | xt−1, y1:t)
(8)
be the unnormalized importance weight function, ∀t ∈N and xt−1 ∈Rn. Then E[w2
t (xt, xt−1) | xt−1] ≤Cw < ∞, with
the expectation taken over q(xt | xt−1, y1:t).
Theorem 3.4. Provided that Assumptions 3.1, 3.2 and 3.3 hold for all t ≥0, then there exist a constant ct < ∞such
that, for bounded function φ ∈B(Rn)
E
(πN
t|t, φ) −(πt|t, φ)

2
≤ct
∥φ∥2
N .
(9)
Using Assumptions 3.1, 3.2 and 3.3, we now aim to prove Theorem 3.4, for the case where the importance weights
are not necessarily (point-wise) bounded. In the following we use the notation g ≜gt, f ≜ft and wt ≜wt(xt, xt−1).
Additionally, Ft−1 denotes the σ-ﬁeld generated by the particles {x(i)
t−1}N
i=1 and ˜πt|t the empirical measure before the
resampling step.
Proof. For each step (initialization, prediction, update and resampling step) of Algorithm 1, we compute the bound for
mean square error. However, to cope with general importance distribution as in [15, 16], we combine the prediction
and update steps, hence the Bayesian ﬁltering equations (2) can be re-written as
(πt|t, φ) = (πt−1|t−1, f φ g)
(πt−1|t−1, f g) = (ˆπt|t, φ)
(ˆπt|t, 1),
(10)
where ˆπt|t(dxt) = (
R
wt(xt, xt−1) q(xt | xt−1, y1:t) dπt−1|t−1) dxt.
At initial step, t = 0, we have E
(πN
0|0, φ) −(π0|0, φ)

2
≤c0
∥φ∥2
N , because the N particles from the prior distribution
(πN
0|0) are assumed to be independent and identically distributed. We now aim to prove the corresponding result for all
t ≥1, by using an induction argument. The result follows by proving Lemma 3.5 (for combined prediction-update
steps) and Lemma 3.7 (for resampling step).
4

/ Signal Processing 00 (2014) 1–??
5
Lemma 3.5. Let us assume that for φ ∈B(Rn) and Assumptions 3.1, 3.2 and 3.3 hold, we have
E
(πN
t−1|t−1, φ) −(πt−1|t−1, φ)

2
≤ct−1
∥φ∥2
N .
(11)
Then
E
(˜πN
t|t, φ) −(πt|t, φ)

2
≤˜ct
∥φ∥2
N .
(12)
Proof. Given Ft−1, the σ-ﬁeld generated by {x(i)
t−1}N
i=1, then
E[(ˆπN
t|t, φ) | Ft−1] = (πN
t−1|t−1, f φ g)
(13)
and, from Assumption 3.3, we can easily show the boundedness of E[(w(i)
t )2 | Ft−1]:
Remark 3.6. Provided that E[(w(i)
t )2 | xt−1] is bounded, then E[(w(i)
t )2 | Ft−1] is bounded as well.
We know that
(˜πN
t|t, φ) −(πt|t, φ) ≤
∥φ∥
(ˆπt|t, 1)
h
(ˆπt|t, 1) −(ˆπN
t|t, 1)
i
+
1
(ˆπt|t, 1)
h
(ˆπN
t|t, φ) −(ˆπt|t, φ)
i
,
(14)
where (ˆπt|t, 1) = (πt|t−1, g) > 0 by our assumptions. To prove Equation (12), we need to evaluate the bounds for
E[|(ˆπN
t|t, φ) −(ˆπt|t, φ)|2] and E[|(ˆπN
t|t, 1) −(ˆπt|t, 1)|2]. We ﬁrst evaluate the bound for the former expression, from which
the latter will follow by setting φ = 1. We deﬁne (ˆπN
t|t, φ) −(ˆπt|t, φ) = Π1 + Π2, where
Π1 = (ˆπN
t|t, φ) −E[(ˆπN
t|t, φ) | Ft−1],
Π2 = E[(ˆπN
t|t, φ) | Ft−1] −(ˆπt|t, φ).
We compute E[|Π1|2] and E[|Π2|2] as follows. Using the boundedness of φ, Equation (13) and Remark 3.6, we get
E
hΠ1

2
| Ft−1
i
≤1
N E

1
N

N
X
i=1
φ(x(i)
t ) w(i)
t

2
| Ft−1

≤∥φ∥2
N2
N
X
i=1
E
h
|w(i)
t |2 | Ft−1
i
≤˜ct1
∥φ∥2
N .
(15)
For the second part, using Equations (11) and (13) as well as ∥fφ∥≤∥φ∥, we get
E
hΠ2

2
| Ft−1
i
= E
h(πN
t−1|t−1, fφg) −(πt−1|t−1, fφg)

2
| Ft−1
i
≤ct−1
∥φ∥2 ∥g∥2
N
= ˜ct2
∥φ∥2
N .
(16)
Using Minkowski inequality, we combine (15) and (16) to get
E
h(ˆπN
t|t, φ) −(ˆπt|t, φ)

2i
≤ˆct
∥φ∥2
N ,
(17)
5

/ Signal Processing 00 (2014) 1–??
6
which, with φ = 1, implies E
h(ˆπN
t|t, 1) −(ˆπt|t, 1)

2i
≤ˆct 1
N . Using these results and the Minkowski inequality to (14):

E
h(˜πN
t|t, φ) −(πt|t, φ)

2i1/2
≤
 
1
(ˆπt|t, 1)
p
ˆct +
1
(ˆπt|t, 1)
p
ˆct
! ∥φ∥
N1/2 =
p
˜ct
∥φ∥
N1/2 ,
which completes the proof of Lemma 3.5.
Lemma 3.7. Assume that Assumptions 3.1, 3.2 and 3.3 hold and that
E
(˜πN
t|t, φ) −(πt|t, φ)

2
≤˜ct
∥φ∥2
N .
Then
E
(πN
t|t, φ) −(πt|t, φ)

2
≤ct
∥φ∥2
N .
(18)
Proof. If we deﬁne (πN
t|t, φ) −(πt|t, φ) = (πN
t|t, φ) −(˜πN
t|t, φ) + (˜πN
t|t, φ) −(πt|t, φ), then, using Minkowski inequality together
with Assumption 3.2 and results of Lemma 3.5 we have

E
(πN
t|t, φ) −(πt|t, φ)

21/2
≤
p
Ct
∥φ∥
N1/2 +
p
˜ct
∥φ∥
N1/2 = √ct
∥φ∥
N1/2 ,
hence, the result follows.
4. The L4 and Empirical Measure Convergence
In this section we generalize the above L2-convergence results to L4-convergence and empirical measure conver-
gence.
4.1. The L4-Convergence
To guarantee the L4-convergence results, we use Assumption 3.1 together with the following assumptions.
Assumption 4.1. The resampling procedure satisﬁes the condition [5]:
E
(πN
t|t, φ) −(˜πN
t|t, φ)

4
≤Ct
∥φ∥4
N2 .
(19)
Assumption 4.2. Let wt(xt, xt−1) be the unnormalized importance weight function deﬁned in (8), ∀t ∈N and xt−1 ∈Rn,
then E[w4
t (xt, xt−1) | xt−1] ≤Cw < ∞, with the expectation taken over q(xt | xt−1, y1:t).
Remark 4.3. Provided that E[(w(i)
t )4 | xt−1] is bounded, then E[(w(i)
t )4 | Ft−1] is bounded as well.
For the L4-convergence, we need to prove the following theorem.
6

/ Signal Processing 00 (2014) 1–??
7
Theorem 4.4. Provided that Assumptions 3.1, 4.1 and 4.2 hold for all t ≥0, then for φ ∈B(Rn) we have
E
(πN
t|t, φ) −(πt|t, φ)

4
≤ct
∥φ∥4
N2 .
(20)
Proof. Certainly, this is true for t = 0 and the cases for t ≥1 result follows from Lemmas 4.5 and 4.6 below together
with an induction argument.
Lemma 4.5. Assume Assumptions 3.1, 4.1 and 4.2 hold and we have
E
(πN
t−1|t−1, φ) −(πt−1|t−1, φ)

4
≤ct−1
∥φ∥4
N2 .
(21)
Then
E
(˜πN
t|t, φ) −(πt|t, φ)

4
≤˜ct
∥φ∥4
N2 .
(22)
Proof. Recall that we have deﬁned (˜πN
t|t, φ) −(πN
t|t, φ) in Equation (14) and consider
(ˆπN
t|t, φ) −(ˆπt|t, φ) =
h
(ˆπN
t|t, φ) −E[(ˆπN
t|t, φ) | Ft−1]
i
+
h
E[(ˆπN
t|t, φ) | Ft−1] −(ˆπt|t, φ)
i
. Using Lemmas 7.1 and 7.2 from [11]
together with Remark 4.3, we can easily deduce
E
(ˆπN
t|t, φ) −E[(ˆπN
t|t, φ) | Ft−1]

4
| Ft−1

≤16∥φ∥4
N4

N
X
i=1
C4
w +

N
X
i=1
C2
w

2= ˜c1
∥φ∥4
N2 .
(23)
Proceeding as in (16) we get that
E
 E[(ˆπN
t|t, φ) | Ft−1] −(ˆπt|t, φ)

4
| Ft−1

≤˜c2
∥φ∥4
N2 .
(24)
The result follows by combining (23) and (24) with Minkowski’s inequality.
Lemma 4.6. Assume Assumptions 3.1, 4.1 and 4.2 hold and we have
E
(˜πN
t|t, φ) −(πt|t, φ)

4
≤˜ct
∥φ∥4
N2 .
(25)
Then
E
(πN
t|t, φ) −(πt|t, φ)

4
≤ct
∥φ∥4
N2 .
(26)
Proof. Proceeding as the proof of Lemma 3.7 but using Assumption 4.1 and Lemma 4.5, the results follows.
4.2. Empirical Measure Convergence
In this section, we use the L4-results to deduce the empirical measure convergence given in the following theorem.
Theorem 4.7. Provided that Assumptions 3.1, 4.1 and 4.2 hold for all t ≥0, then we have, almost surely,
lim
N→∞πN
t|t = πt|t.
(27)
Proof. Using the L4-convergence results, then the result follows by using the Markov inequality and Borel-Cantelli
argument [5].
7

/ Signal Processing 00 (2014) 1–??
8
5. Analytical and Numerical Example
Assume that we have a Cox process, where the a priori dynamics of the state can be modeled as a reﬂected
Brownian motion x(τ) ≜η1/2 |W(τ)|, where W(τ) is a standard Brownian motion, and the measurements are Poisson
distributed with an intensity parameter λ(τ) = c x(τ), where c > 0 is a constant, and the measurements are obtained
at discrete times t ∈{1, 2, 3, 4, . . .}. The model can now be formulated as a discrete-time model for the measurement
times:
f(xt | xt−1) =
1
p
2 π η
"
e−
(xt−xt−1)2
2η
+ e−
(xt+xt−1)2
2η
#
,
g(yt | xt) =

limxt→0+ g(yt | xt),
if xt = 0,
(c xt)yt exp(−c xt)
yt!
,
otherwise,
where f(xt | xt−1) is a density with respect to the Lebesgue measure and g(yt | xt) with respect to the counting measure.
Above, we require that xt ≥0 for all t. The purpose of including xt = 0 as the special case in g is to ensure that it is
continuous and bounded in the domain xt ≥0.
Let us now select a Gamma distribution with constant parameters α, β > 0 as the importance distribution for a
particle ﬁlter. Thus the importance sampling density (w.r.t. the Lebesgue measure) is
q(xt) =
βα
Γ(α) xα−1
t
exp(−β xt).
(28)
At some point of time we eventually reach a zero measurement yt = 0. In this case we have for xt > 0
w(xt, xt−1) =
1
p
2 π η
exp(−c xt)
"
e−
(xt−xt−1)2
2η
+ e−
(xt+xt−1)2
2η
#
βα
Γ(α) xα−1
t
exp(−β xt)
.
(29)
Let us assume that α > 1. It is now easy to show that for any (ﬁnite) selection of xt−1 we have
lim
xt→0+ w(xt, xt−1) = ∞.
(30)
This happens, because q(0) = 0, but the numerator is nonzero. Thus according to the classical result for particle ﬁlters
[5, 1, 6] the particle ﬁlter is not guaranteed to converge in mean square, L4, or empirical measure sense.
By using f ≤1/
p
2 π η and combining terms gives
w(xt, xt−1) ≤
Γ(α)
p
2 π η β−α exp ((β −c) xt) x−α+1
t
.
(31)
Thus we have
E[wp(xt, xt−1) | xt−1] =
Z ∞
0
w2(xt, xt−1) q(xt) dxt
≤
βα
Γ(α)

Γ(α)
p
2 π η β−α

p
×
Z ∞
0
exp ([(p −1) β −p c] xt) x(1−p) α+p−1
t
.
(32)
8

/ Signal Processing 00 (2014) 1–??
9
Provided that (p−1) β−p c < 0, the above expression is just a constant times the gamma function value Γ((1−p) α+p).
Recalling that the gamma function is ﬁnite for negative arguments other than integers, we can now deduce that even
when yt = 0, we have for p = 2, 4:
Z ∞
0
(w(xt, xt−1))p q(xt) dxt ≤cw < ∞
(33)
provided that β < c p/(p −1), α > 1, and (1 −p) α + p is not a negative integer. Thus, according to the present theory,
the particle ﬁlter converges in mean square and L4 sense for bounded Borel functions, and its empirical measure
converges.
[Figure 1 about here.]
[Figure 2 about here.]
Because this model is single-dimensional, we can use numerical integration (naive Riemann sum in this case) to
approximate the ﬁltering solution in a dense grid. The result of applying the grid ﬁlter to a simulated process with
c = 1/2, q = 1/10, x0 = |ξ|, where ξ is unit Gaussian, is shown in Figure 1 on the left. The right hand side of Figure 1
shows the result of a particle ﬁlter with 10000 particles and with the importance distribution parameters α = 1.5 and
β = 0.5. For visualization the number of particles is reduced to 100 per time step. As can be seen, the result is well in
line with the grid based result. Figure 2 shows the ﬁltering distribution approximations at step t = 11, where y11 = 0
and hence the importance weight is unbounded. The particle ﬁlter result is well in line with the grid based result
despite the unboundedness of the weight.
6. Conclusion and Discussion
We have derived moment conditions for importance weights of particle ﬁlters, which ensure that the particle ﬁlter
estimates of the expectations of bounded Borel functions converge in mean square and L4 sense, and that the empirical
measure of the particle ﬁlter converges weakly to the true ﬁltering measure. The novel result is that the importance
weights do not need to be point-wise bounded. We have also provided an example of a model and a particle ﬁlter for
which the present theory guarantees the particle ﬁlter convergence although the previously developed particle ﬁlter
theory does not.
The numerical example showed an example situation when the weight moments can be bounded when the weights
are not point-wise bounded. Similar phenomenon is possible whenever there are point-singularities in the weights
caused by nulls in the importance distribution. An advantage of the moment conditions is that when the importance
distribution is constructed indirectly (as in, e.g., [17]), the weight moment condition can be easier to check than the
point-wise boundedness.
9

/ Signal Processing 00 (2014) 1–??
10
Acknowledgment
This work was supported by the projects 266940 and 273475, and the Finnish Centre of Excellence on Inverse
Problems Research of the Academy of Finland.
References
[1] A. Doucet, N. Freitas, N. Gordon, Sequential Monte Carlo methods in practice, Springer, 2001.
[2] S. S¨arkk¨a, Bayesian ﬁltering and smoothing, Cambridge University Press, 2013.
[3] P. Del Moral, A. Guionnet, Central limit theorem for nonlinear ﬁltering and interacting particle systems, The Annals of Applied Probability
9 (2) (1999) 275–297.
[4] P. Del Moral, A. Guionnet, On the stability of interacting processes with applications to ﬁltering and genetic algorithms, Annales de l’Institut
Henri Poincare (B) Probability and Statistics 37 (2) (2001) 155–194.
[5] D. Crisan, A. Doucet, Convergence of sequential Monte Carlo methods, Tech. Rep. CUEDIF-INFENGrrR38, Signal Processing Group,
Department of Engineering, University of Cambridge (2000).
[6] D. Crisan, A. Doucet, A survey of convergence results on particle ﬁltering methods for practitioners, IEEE Transactions Signal Processing
50 (3) (2002) 736–746.
[7] N. Chopin, Central limit theorem for nonlinear ﬁltering and interacting particle systems, The Annals of Statistics 32 (6) (2004) 2385–2411.
[8] P. Del Moral, Feynman–Kac Formulae: Genealogical and Interacting Particle Systems with Applications, Springer, 2004.
[9] R. Douc, E. Moulines, Limit theorems for weighted samples with applications to sequential Monte Carlo methods, Annals Statistics 36 (5)
(2008) 2344–2376.
[10] R. Douc, E. Moulines, J. Olsson, Optimality of the auxiliary particle ﬁlter, Probability and Mathematical Statistics 29 (1) (2009) 1–28.
[11] X. Hu, T. B. Sch¨on, L. Ljung, A basic convergence result for particle ﬁltering, IEEE Transactions on Signal Processing 56 (4) (2008) 1337–
1348.
[12] X. Hu, T. B. Sch¨on, L. Ljung, A general convergence result for particle ﬁltering, IEEE Transactions on Signal Processing 59 (7).
[13] P. Del Moral, Mean ﬁeld simulation for Monte Carlo integration, Chapman & Hall/CRC, 2013.
[14] N. Whiteley, Stability properties of some particle ﬁlters, The Annals of Applied Probability 23 (6) (2013) 2161–2603.
[15] I. Mbalawata, S. S¨arkk¨a, On the L4 convergence of particle ﬁlters with general importance distributions, in: Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014, pp. 8048 – 8052.
[16] I. Mbalawata, S. S¨arkk¨a, Weight moment conditions for L4 convergence of particle ﬁlters for unbounded test functions, in: Proceedings of
IEEE EUSIPCO 2014, 2014, pp. 1–5, (to appear).
[17] S. S¨arkk¨a, T. Sottinen, Application of Girsanov theorem to particle ﬁltering of discretely observed continuous-time non-linear systems,
Bayesian Analysis 3 (3) (2008) 555–584.
10

/ Signal Processing 00 (2014) 1–??
11
0
20
40
60
80
100
0
1
2
3
4
5
6
Time step
 
 
True xt
Mean
95% Quantiles
Figure 1. Left: Grid based state estimate of the Cox process. Right: Particle ﬁlter estimate.
11

/ Signal Processing 00 (2014) 1–??
12
0
1
2
3
0
0.1
0.2
0.3
0.4
0.5
0.6
0
1
2
3
0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 2. Left: Grid based ﬁlter distribution at t = 11. Right: Particle ﬁlter histogram for the same step.
12

PUBL. VII
Betty Nannyonga, Gasper G. Mwanga, Heikki Haario, Isambi S Mbalawata and Matti Heiliö,
Determining parameter distribution in within-host severe P. falciparum malaria. BioSystems , 126,
76–84, 2014.
c⃝2014 BioSystems. All rights reserved.
Reprinted, with the permission of BioSystems


BioSystems
 126
 (2014)
 76–84
Contents
 lists
 available
 at
 ScienceDirect
BioSystems
jo
 u
 r
 n
 al
 homep
 age:
 www.elsevier.com/locate/biosystems
Determining
 parameter
 distribution
 in
 within-host
 severe
P.
 falciparum
 malaria
B.  Nannyonga a,∗,  G.G.
 Mwanga b,  H.
 Haario b, I.S.
 Mbalawata b, M.
 Heilio b
a Department
 of
 Mathematics,
 Makerere
 University,
 P.O.
 Box
 7062,
 Kampala,
 Uganda
b Department
 of
 Mathematics
 and
 Physics,
 Lappeenranta
 University
 of
 Technology,
 Box
 20,
 FIN-53851
 Lappeenranta,
 Finland
a
 r  t
 i
 c  l e  
i  n  f  o
Article
 history:
Received
 26
 June
 2013
Received
 in
 revised
 form
 25
 August
 2014
Accepted
 18
 September
 2014
Available
 online
 26
 September
 2014
Keywords:
Within-host
 malaria
Double
 infection
MCMC
a
 b
 s
 t  r  a
 c  t
Numerous
 studies
 have  been  carried
 out
 on  within-host
 Plasmodium
 falciparum
 malaria
 with  varying
results.
 Some
 studies
 have  suggested
 over  estimation
 of parasite
 growth  within  an  infected  host
 while
others
 stated  that  evolution
 of  parasitaemia
 seems
 to  be quelled
 by parasite  load.  Various  mathematical
models
 have  been
 designed
 to  understand
 the dynamics
 of  evolution
 of  within-host
 malaria.
 The  basic
ingredient
 in  most
 of  the models
 is  that  the  availability
 of uninfected
 red  blood  cells  (RBCs)
 in  which
the
 parasite
 develops
 is  a limiting
 factor  in the propagation
 of  the  parasite
 population.
 We  hypothesize
that
 in  severe
 malaria,
 due  to  parasite  quest
 for  survival  and  rapid  multiplication,
 the  vicious malaria
parasite
 is sophisticated
 and  can  be  absorbed
 in  an  already
 infected
 RBC  and  speeds
 up  rapture
 rate.  The
study
 reviews
 the  classical
 models
 of  blood
 stage
 malaria
 and  proposes
 a new
 model
 which  incorporates
double
 infection.
 Analysis
 of the  model  and  parameter
 identiﬁability
 using  Markov
 chain
 Monte  Carlo
(MCMC)
 are  presented.
 MCMC  uses
 distribution
 of parameters
 to study  the model
 behavior  instead
 of
single
 points.
 Results
 indicate
 that  most  infected
 RBCs
 rupture  quickly
 due  to  the  disease
 instead.
 This
may
 explain  anemia
 in  malaria  patients
 and  lack  of uniformity
 of  oscillations
 in within-host
 malaria.
Therefore,
 more
 needs
 to be  done  as far  as  within-host
 malaria  is concerned,
 to  provide  step  by
 step
evolution
 of  malaria  within  a host.
©
 2014 Elsevier
 Ireland  Ltd.
 All
 rights
 reserved.
1.
 Introduction
Malaria
 is
 a
 tropic
 disease
 caused
 by
 a
 Plasmodium, which
 is
 a
genus
 of
 parasitic
 protozoa.
 The
 symptoms
 of
 an
 individual
 with
a
 malaria
 episode
 include
 high
 fevers,
 shaking
 chills
 and
 anemia
(Krogstad,
 2012).
 It
 is
 a
 major
 threat
 to
 the
 lives
 and
 health
 of
millions
 of
 people
 living
 in
 the
 tropics
 and
 subtropics
 (Marsh,
 2012).
Millions
 of
 episodes
 of
 the
 disease
 and
 deaths
 of
 more
 than
 a
 million
people
 each
 year
 are
 reported,
 many
 of
 them
 children
 in
 sub-
Saharan
 Africa.
 Of
 the
 four
 species
 of
 malaria
 parasite
 that
 infect
humans,
 severe
 disease
 and
 deaths
 are
 overwhelmingly
 attributed
to
 Plasmodium
 falciparum.  It
 is
 this
 species,
 speciﬁcally
 the
 dynam-
ics
 within
 its
 host
 that
 is
 the
 center
 of
 our
 study.
 The
 life
 cycle
of
 the
 human-infecting
 Plasmodium
 species
 can
 be
 broken
 down
into
 the
 vector
 (mosquito)
 stage
 and
 the
 host
 (human)
 stage.
 The
sexual
 stage
 of
 the
 parasite
 takes
 place
 in
 the
 female
 mosquito
of
 the
 genus
 Anopheles
 and
 the
 asexual
 stage
 in
 humans.
 Malaria
transmission
 occurs
 when
 a
 female
 mosquito
 vector
 takes
 a
 blood
∗Corresponding
 author.
 Tel.:
 +256
 772
 347347.
E-mail
 addresses:
 bnk@cns.mak.ac.ug, bnk@cns.mak.ac.ugy
 (B.
 Nannyonga).
meal
 from
 an
 infected
 human
 host.
 During
 a
 subsequent
 blood
meal,
 the
 mosquito
 injects
 between
 10
 and
 100
 sporozoites
 into
the
 blood
 stream
 of
 the
 human
 via
 saliva
 (Ponnudurai
 et
 al.,
 1982;
Rosenberg
 et
 al.,
 1990).
 Once
 inside
 the
 human
 host,
 the
 sporo-
zoites
 travel
 to
 liver
 cells,
 and
 this
 marks
 the
 beginning
 of
 the
exo-erythrocytic
 phase
 of
 the
 malaria
 parasite.
 Within
 the
 liver,
the
 parasites
 change
 form
 again
 into
 trophozoites,
 beginning
 the
pre-erythrocytic
 stage
 (Krier
 and
 Baker,
 1980).
 The
 trophozoite
duplicates
 asexually
 producing
 thousands
 of
 merozoites.
 These
 ﬁll
the
 tissue
 of
 the
 trophozoite
 cell
 which
 ruptures
 when
 fully
 devel-
oped.
 The
 merozoites
 then
 ﬂood
 the
 surrounding
 tissue
 and
 enter
the
 blood
 stream,
 although
 some
 of
 them
 re-enter
 the
 liver
 cells
 for
a
 secondary
 pre-erythrocytic
 stage
 (Bruce-Chwatt,
 1980;
 Garnham,
1966).
The
 development
 of
 P.
 falciparum
 malaria
 is
 a
 complex
 process
(Hoshen
 et
 al.,
 2000)
 involving
 multiple
 stages.
 It
 is
 characterized
by
 an
 exponential
 growth
 in
 the
 number
 of
 infected
 erythro-
cytes,
 followed
 by
 oscillations
 with
 a
 period
 of
 48
 h,
 which
 are
eventually
 dampened.
 An
 inherent
 feature
 of
 infection
 is
 syn-
chronicity,
 which
 is
 irrespective
 of
 the
 duration
 of
 merozoite
release
 from
 the
 liver,
 causing
 periodic
 symptoms
 in
 the
 infected
human.
 The
 malaria
 parasite
 is
 a
 unicellular
 organism
 belonging
http://dx.doi.org/10.1016/j.biosystems.2014.09.009
0303-2647/©
 2014
 Elsevier
 Ireland
 Ltd.
 All
 rights
 reserved.

B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
 
77
to
 the
 phylum
 Apicomplexa.
 Development
 of
 new
 approaches
 to
control
 malaria
 through
 prevention
 and
 treatment
 is
 dependent
on
 thorough
 understanding
 of
 how
 the
 malaria
 parasite
 interacts
with
 the
 human
 host
 causing
 its
 damage
 (Marsh,
 2012).
 All
 Plas-
modium
 species
 take
 up
 cell
 cytoplasm
 by
 feeding
 on
 hemoglobin
in
 the
 red
 blood
 cells
 (RBCs)
 or
 erythrocytes
 of
 their
 host
 (Slater
 and
Cerami,
 1992).
 For
 untreated
 P.
 falciparum
 malaria,
 large
 number
 of
infected
 RBCs
 rupture
 every
 48
 h
 (Medical
 Biology-diseases,
 2012),
releasing
 fever-causing
 chemicals
 into
 the
 blood
 at
 regular
 inter-
vals
 (Marsh,
 2012;
 Malaria,
 2012a).
 The
 released
 merozoites
 can
either
 re-enter
 a
 naive
 erythrocyte
 and
 begin
 a
 new
 erythrocytic
cycle,
 or
 become
 a
 gametocyte,
 remaining
 inactive
 in
 the
 blood-
stream
 of
 the
 human
 host.
 It
 begins
 to
 differentiate
 if
 taken
 up
by
 a
 mosquito
 and
 this
 starts
 the
 whole
 cycle
 again.
 Whether
 the
late
 relapses
 of
 malaria
 episodes
 are
 caused
 by
 a
 dormant
 primary
pre-erythrocytic
 parasite
 or
 a
 clinically
 undetectable
 secondary
pre-erythrocytic
 system
 (Bruce-Chwatt,
 1980)
 is
 not
 known.
 The
cycle
 for
 the
 pre-erythrocytic
 stage
 is
 two
 weeks
 at
 most
 (Clyde,
1987),
 but
 individuals
 go
 without
 symptoms
 for
 decades
 between
the
 time
 of
 primary
 infection,
 ﬁrst
 exposure
 to
 the
 parasite,
 and
 a
relapse.
Parasite
 
load
 
is
 
regulated
 
according
 
to
 
parasite
 
density
(Ginsburg
 and
 Hoshen,
 2002).
 The
 autoregulation
 maintains
 an
equilibrium
 between
 activated
 host
 defense
 processes,
 the
 sensitiv-
ity
 of
 the
 erythrocytes
 to
 invasion
 and
 the
 degree
 of
 pathogenicity
of
 the
 parasite
 (White
 and
 Ho,
 1992).
 Developing
 a
 suitable
 mathe-
matical
 model
 is
 imperative
 to
 understanding
 such
 auto-regulation.
Numerous
 mathematical
 models
 have
 been
 designed
 to
 under-
stand
 the
 process
 of
 malaria
 infection
 in
 non-immune
 individuals
(Gravenor
 and
 Kwiatkowski,
 1998;
 Gravenor
 et
 al.,
 1998,
 1995;
Gupta
 et
 al.,
 1994;
 Hetzel
 and
 Anderson,
 1996;
 Saul,
 1998)
 with
mixed
 results.
 Such
 models
 are
 essential
 since
 the
 malaria
 para-
sites
 cannot
 be
 seen
 at
 some
 stages
 within-host.
 It
 is
 obvious
that
 the
 models
 could
 disclose
 the
 signiﬁcance
 of
 antigenic
 vari-
ation
 to
 the
 evolution
 of
 malaria
 (Ginsburg
 and
 Hoshen,
 2002) at
cellular
 and
 population
 levels.
 Studies
 such
 as
 those
 in
 Okosun
et
 al.
 (2011,
 2013)
 and
 Makinde
 and
 Okosun
 (2011)
 have
 modeled
optimal
 control
 strategies
 and
 analyzed
 cost
 effectiveness
 of
 the
disease.
 Okosun
 et
 al.
 (2011)
 studied
 optimal
 control
 analysis
 of
 a
malaria
 disease
 transmission
 model
 that
 includes
 treatment
 and
vaccination
 with
 waning
 immunity,
 while
 Makinde
 and
 Okosun
(2011)
 looked
 at
 the
 impact
 of
 chemotherapy
 on
 optimal
 con-
trol
 of
 malaria
 disease
 with
 infected
 immigrants.
 These
 studies
model
 malaria
 control
 at
 population
 level.
 More
 often
 than
 not,
the
 impact
 of
 a
 disease
 at
 population
 level
 may
 or
 may
 not
 be
driven
 by
 the
 cellular
 level
 dynamics
 of
 the
 infected
 individ-
uals.
 Understanding
 within-host
 dynamics
 of
 malaria
 may
 provide
needed
 information
 on
 such
 studies
 of
 malaria
 at
 population
level.
A
 lot
 of
 study
 has
 been
 done
 on
 blood-stage
 malaria
 with
 inter-
esting
 results
 (Hetzel
 and
 Anderson,
 1996;
 Saul,
 1998;
 Achcar
 et
 al.,
2011;
 Johnston
 et
 al.,
 2014;
 Dondorp
 et
 al.,
 2000;
 Thibodeaux,
2010).
 In
 this
 study,
 a
 new
 approach
 that
 perhaps
 makes
 it
 pos-
sible
 for
 P.
 falciparum
 malaria
 to
 multiply
 rapidly
 within
 the
 host
(Slater
 and
 Cerami,
 1992) is
 suggested.
 First,
 the
 basic
 within-host
model
 by
 Hetzel
 and
 Anderson
 (1996)
 and
 Dondorp
 et
 al.
 (2000)
is
 analyzed,
 and
 steady
 states
 together
 with
 conditions
 for
 their
existence
 and
 persistence
 explored.
 The
 results
 are
 compared
 to
those
 from
 the
 Saul
 (1998)
 model,
 although
 not
 shown
 in
 the
 paper.
Until
 recently,
 malaria
 parasites
 were
 considered
 to
 prefer
 new
cells
 to
 already
 infected
 ones.
 Here,
 it
 is
 hypothesized
 that
 mero-
zoites
 do
 enter
 infected
 red
 cells
 due
 to
 restriction
 of
 blood
 ﬂow
(see
 Ginsburg
 and
 Hoshen,
 2002
 and
 the
 references
 there
 in).
 This
is
 based
 on
 an
 assumption
 that
 availability
 of
 uninfected
 RBCs
 to
merozoites
 is
 restrained
 when
 total
 clogging
 of
 venules
 occurs
 in
severe
 malaria.
 Although
 the
 situation
 in
 severe
 malaria
 may
 be
much
 more
 complex
 (Dondorp
 et
 al.,
 2000),
 the
 possibilities
 that
could
 lead
 to
 anemia
 are
 explored.
The
 hypothesis
 is
 studied
 from
 a
 mathematical
 point
 of
 view
 and
an
 attempt
 to
 answer
 the
 following
 questions
 is
 made:
 In
 cases
 of
severe
 malaria
 and
 restricted
 supply
 of
 uninfected
 RBCs,
 is
 it
 plausi-
ble
 that
 merozoites
 get
 absorbed
 in
 infected
 cells?
 If so
 how
 would
such
 dynamics
 affect
 the
 entire
 blood
 stage
 malaria
 infection
 pro-
cess?
 Does
 it
 have
 any
 marginal
 effect
 on
 persistence
 within-host?
To
 answer
 these
 questions
 the
 basic
 model
 by
 Hetzel
 and
 Anderson
(1996)
 and
 Dondorp
 et
 al.
 (2000)
 is
 analyzed
 and
 results
 compared
with
 the
 case
 of
 additional
 infection.
 The
 rupture
 function
 by
 Saul
(1998)
 is
 used
 for
 comparison
 to
 the
 known
 dynamics
 of
 malaria.
2.
 Model
 formulation
Three
 models
 based
 on
 earlier
 work
 such
 as
 in
 Hetzel
and
 Anderson
 (1996),
 Saul
 (1998),
 Dondorp
 et
 al.
 (2000)
 and
Thibodeaux
 (2010)
 and
 more
 recently,
 Cuadros
 and
 García-Ramos
(2012)
 are
 used.
 The
 models
 designed
 are
 susceptible-infective
 (SI),
in
 the
 human
 host
 and
 an
 additional
 compartment
 for
 the
 malaria
parasites.
 In
 the
 ﬁrst
 model,
 malaria
 parasites
 are
 attacking
 non
infected
 RBCs
 and
 then
 after
 a
 given
 time,
 rupture
 to
 release
 more
merozoites
 and
 the
 infection
 process
 continues.
 In
 this
 case,
 the
number
 of
 merozoites
 released
 are
 dependent
 on
 the
 number
 of
red
 cells
 destroyed.
 Upon
 rupture,
 r merozoites
 are
 released.
 In
 the
proposed
 model,
 similar
 assumptions
 as
 in
 the
 ﬁrst
 model
 are
 made.
The
 difference
 arises
 out
 of
 the
 hypothesis
 that
 merozoites
 can
 be
absorbed
 in
 an
 already
 infected
 RBC.
 More
 details
 about
 this
 process
are
 given
 in
 the
 proceeding
 sections.
2.1.
 Original
 model
In
 this
 section,
 a
 mathematical
 model
 of
 blood
 stage
 infec-
tion
 with
 a
 single
 strain
 of
 malaria
 is
 investigated.
 Emphasis
 is to
study
 the
 cell
 population
 dynamics
 to
 determine
 the
 parameters
for
 a successful
 invasion
 and
 persistence
 of
 the
 parasite.
 Important
parameters
 in
 the
 invasion/persistence,
 and
 whether
 the
 density
of
 susceptible
 erythrocytes
 is
 an
 important
 determinant
 of
 the
 ini-
tial
 pattern
 of
 infection
 in
 vivo
 are
 studied.
 The
 basic
 model
 of
blood-stage
 malaria
 dynamics
 implemented
 here
 has
 three
 vari-
ables:
 x,
 the
 concentrations
 of
 uninfected
 RBCs,
 y,
 the
 infected
 RBCs,
and
 m,
 the
 free
 malaria
 parasites
 (Hoshen
 et
 al.,
 2000;
 Hetzel
 and
Anderson,
 1996;
 Dondorp
 et
 al.,
 2000).
 These
 quantities
 denote
 the
total
 abundance
 (cells
 or
 parasites)
 of
 the
 given
 population
 per
milliliter
 (mL)
 of
 blood,
 and
 their
 dynamics
 are
 described
 in
 Fig.
 1.
Fig.
 1A
 shows
 the
 known
 biological
 dynamics
 of
 malaria
 within
a
 host;
 extracted
 from
 Nature,
 see
 Winzeler
 (2008). Fig.
 1B
 is
 the
mathematical
 translation
 of
 these
 dynamics
 within
 the
 RBCs.
 In
the
 study
 we
 model
 the
 asexual
 cycle
 c.
 The
 mathematical
 transla-
tion
 of
 stage
 c
 is
 described
 by
 the
 following
 system
 of
 differential
equations:
dx
dt =
 
 −
 x
 −
 ˇxm;
dy
dt =
 ˇxm
 −
 y
 −
 ıy;
dm
dt =
 rıy
 −
 m
 −
 ˇxm.
 
(2.1)
In
 the
 model,
 
 is
 the
 rate
 at
 which
 new
 red
 cells
 are
 formed
(cells/ml/day),
 ,
  and
 
 are
 the
 death
 rates
 of
 uninfected
 red
cells,
 infected
 red
 cells
 and
 merozoites,
 respectively
 (/day).
 Let
 ı be
the
 differentiation
 rate
 of
 merozoites.
 The
 rate
 at
 which
 merozoites
invade
 red
 cells
 (/cell/ml/day)
 is
 ˇ
 while
 r
 is
 the
 number
 of
 mero-
zoites
 released
 per
 rupturing
 schizont.
 All
 parameters
 of
 the
 model
are
 non
 negative.
 The
 magnitudes,
 dimensions
 and
 sources
 of
 these
parameters
 are
 summarized
 in
 Table
 1, and
 a
 compartmentalized

78
 
B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
Fig.
 1.
 Compartmentalized
 diagram
 of
 the
 basic
 malaria
 model.
Table
 1
Parameter
 values
 and
 their
 dimensions.
Parameter
 
Description
 
Value
 
Dimension
 
Source

 
The
 rate
 at
 which
 new
 RBCs
 are
 formed
 
2.5
 ×
 108
cells/ml/day
 
Hetzel
 and
 Anderson
 (1996), Saul
 (1998)
 and
 Thibodeaux
 (2010)

 
The
 per
 capita
 death
 rate
 for
 non
 infected
 RBCs
0.0083
 
/day
 
Hetzel
 and
 Anderson
 (1996)
 and
 Dondorp
 et
 al.
 (2000)
ı
 
Differentiation
 rate
 of
 merozoites
 
0.5
 
/day
 
Dondorp
 et
 al.
 (2000)
 and
 Cuadros
 and
 García-Ramos
 (2012)
 
Natural
 death
 rate
 of
 infected
 red
 blood
 cells
 
0.025
 
/day
 
Dondorp
 et
 al.
 (2000)
 and
 Cuadros
 and
 García-Ramos
 (2012)

 
The
 per
 capita
 death
 rate
 for
 free
 merozites
 
48
 
/day
 
Dondorp
 et
 al.
 (2000)
 and
 Cuadros
 and
 García-Ramos
 (2012)
ˇ
 
Infection
 rate
 of
 non
 infected
 red
 blood
 cells
 by
merozoites
2.5
 ×
 10−10
/merozoite/day
 
Dondorp
 et
 al.
 (2000)
r 
Merozoite
 production
 rate
 
16–20,
 32
 
/day
 
Malaria
 (2012b)
diagram
 in
 Fig.
 1. From
 this
 system
 of
 differential
 equations,
 it
 is
possible
 to
 determine
 the
 concentration
 of
 the
 malaria
 parasite
population
 at
 equilibrium.
 Setting
 all
 the
 infected
 classes,
 that
 is,
y
 =
 0
 =
 m,
 and
 solving
 the
 resulting
 system
 at
 this
 steady
 state
 gives
the
 disease-free
 equilibrium
 DFE,
 as
 E1(0)
 =
 (/,
 0,
 0).
The
 endemic
 equilibrium
 state
 is
 obtained
 by
 setting
 the
 left
hand-side
 of
 expressions
 in
 Eq.
 (2.1)
 to
 zero
 to
 obtain
 the
 following
solution
 set:
˜E1 =
 {˜x, ˜y, ˜m}
 =

rı
(ı(r
 −
 1)
 −
 )(ˇ
 +
 )R01
,
(ˇ
 +
 )(R01 −
 1)
ˇ(ı(r
 −
 1)
 −
 )
, (ˇ
 +
 )(R01 −
 1)
ˇ
,
 
(2.2)
where
R01 =
rıˇ
(ı
 +
 )(ˇ
 +
 ) ,
 
(2.3)
is
 the
 basic
 reproductive
 number
 for
 the
 infection.
 Note
 that
 when
R01 =
 1,
 we
 have
rc = (ı
 +
 )(ˇ
 +
 )
ıˇ
.  
(2.4)
This
 is
 the
 critical
 number
 of
 released
 merozoites
 per
 rupturing
infected
 erythrocyte
 above
 which
 the
 infection
 persists.
 Therefore,
the
 uninfected
 steady
 state
 exists
 if
 r
 <
 rc whereas
 the
 endemic
 equi-
librium
 exists
 otherwise.
Stability
 of
 the
 disease-free
 state
 is
 investigated
 using
 the
 fol-
lowing
 theorem:
Theorem
 1.
(i)
 The
 disease-free
 state
 E0 =
 (/,
 0,
 0)
 exists
 for
 all
 non-negative
values
 of
 its
 parameters
 and
 is
 globally
 asymptotically
 stable
 when
R01 ≤
 1
 and
 unstable
 when
 R01 >
 1.
(ii)
 If
 R01 >
 1,
 solutions
 to
 the
 system
 (2.1)
 starting
 closely
 to
 E0
move
 away
 from
 E0 except
 those
 starting
 close
 to
 the
 invariant
x-axis
 which
 approach
 E0 along
 this
 axis.
(iii)
 If
 R01 >
 1,
 system
 (2.1)
 has
 a
 unique
 endemic
 equilibrium ˆE1 =
(ˆx, ˆy, ˆm)
 and
 its
 coordinates
 are
 given
 by
 Eq.
 (2.2).
The
 proof
 of
 this
 theorem
 follows
 from
 a
 Lyapunov
 function
L
 =
 (ˇ
 +
 )y
 +
 ˇm,
 which
 can
 be
 shown
 that
 L
 →
 ∞
 as
 t→
 ∞
(Hale,
 1969;
 Hsu,
 2005;
 Khalil,
 2002).
 The
 derivative
 of
 L
 along
 the
solutions
 to
 system
 (2.1)
 is
L′  =
 (ˇ
 +
 )y′
 +
 ˇm′
 =
 (ˇ
 +
 )(ˇxm
 −
 y −
 ıy) +
 ˇ(ıry
 −
 m
 −
 ˇxm),
=
 rıˇy
 −
 ˇxm
 −
 (ˇ
 +
 )(ı
 +
 )y
 −
 ˇ
 ≤
rıˇ−(ˇ
 +
 )(ı
 +
 )
y,
=
 (ˇ
 +
 )(ı
 +
 )

rıˇ
(ˇ
 +
 )(ı
 +
 ) −
 1

y,
=
 (ˇ
 +
 )(ı
 +
 )[R01 −
 1]y.
Then
 L′ ≤
 0
 if
 R01 ≤
 1.
 In
 addition,
 L′  =
 0
 only
 if
 y
 =
 m
 =
 0
 . Thus,
the
 maximum
 invariant
 set
 in
 {(x,
 y,
 m)
 :
 L′  =
 0}
 is
 the
 singleton
E1(0).
To
 study
 the
 stability
 of
 the
 endemic
 equilibrium,
 system
 (2.1)
is
 linearized.
 The
 Jacobian
 matrix
 of
 the
 system
 is
 given
 by
J1 =
⎡
⎣
− −
 ˇ ˜m
 
0
 
−ˇ˜x
ˇ ˜m
−ı
 −
  
ˇ˜x
−ˇ ˜m
rı
 
−
 −
 ˇ˜x
⎤
⎦.
 
(2.5)
Note
 that
 Jacobian
 (2.5)
 has
 negative
 trace.
 For
 stability,
 the
determinant
 det(J1) >
 0.
 The
 determinant
 of
 the
 matrix
 det(J1)
 is
given
 by
 det(J1) =
 rıˇ˜x −
 (ı
 +
 )(
 +
 ˇ˜x +  ˇ ˜m). Let
 us
 fur-
ther
 simplify
 the
 determinant
 using
 the
 equilibrium
 condition
 (2.2).
This
 implies
 that
det(J1) >
 0
 ⇒
r2ı2ˇ
(ı(r
 −
 1)
 −
 )(ˇ
 +
 )R01
>
 (ı
 +
 )


 +
rıˇ
(ı(r
 −
 1)
 −
 )(ˇ
 +
 )R01
+
 (ˇ
 +
 )(R01 −
 1)

.
Simplifying
 this
 expression
 implies
 that
det(J1) >
 0
 if
 and
 only
 if
ı
 +
 
ı(r
 −
 1)
 −
  >
 
 +
 (ˇ
 +
 )(R01 −
 1).
(2.6)

B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
 
79
Thus
 the
 determinant
 is
 positive
 if
 the
 relation
 in
 Eq.
 (2.6)
 is
satisﬁed.
 Since
 the
 trace
 is
 negative,
 the
 steady
 states
 are
 asymp-
totically
 stable.
2.2.
 Proposed
 model
In
 this
 model,
 in
 addition
 to
 the
 assumptions
 made
 in
 the
 pre-
vious
 model
 in
 Eq.
 (2.1), it
 is
 hypothesized
 that
 merozoites
 can
 be
absorbed
 in
 an
 already
 infected
 RBC
 at
 a
 rate
 ˇ1y1m.
 It
 is
 important
to
 note
 that
 P.
 falciparum
 possesses
 a
 rapid
 reproduction
 rate
 and
not
 preferential
 for
 what
 age
 of
 the
 erythrocyte
 it
 infects
 (Clyde,
1987).
 It
 is
 proposed
 therefore,
 that
 the
 plasmodium
 parasite
 is
sophisticated
 in
 that
 it
 prefers
 an
 infected
 RBC
 that
 is
 about
 to
rupture
 in
 order
 to
 multiply
 faster.
 This
 assumption
 leads
 to
 an
additional
 subgroup
 y2, and
 rates
 ˇ2, 2, and
 ı2 with
 the
 same
descriptions
 as
 those
 in
 the
 referenced
 equations
 above.
 On
 the
other
 hand,
 the
 newly
 infected
 RBCs
 die
 off
 at
 a
 rate
 1,
 differenti-
ates
 at
 a
 rate
 ı1 and
 release
 r1 new
 merozoites
 upon
 rupture.
 The
model
 is
 given
 in
 Eq.
 (2.7). Note
 here
 that
 when
 there
 is
 no
 second
infection
 the
 system
 returns
 the
 standard
 models
 in
 Eq.
 (2.1). These
modiﬁcation
 lead
 to
 the
 following
 systems
 of
 equations:
dx
dt =
 

 −
 x
 −
 ˇ1xm;
dy1
dt
=
 
ˇ1xm
 −
 1y1 −
 ı1y1 −
 ˇ2y1m;
(2.7)
dy2
dt
=
 
ˇ2y1m
 −
 2y2 −
 ı2y2;
dm
dt =
 
r1ı1y1 +
 r2ı2y2 −
 m
 −
 ˇ1xm
 −
 ˇ2y1m,
The
 basic
 reproductive
 ratio
 for
 Eq.
 (2.7)
 as
 in
 the
 previous
 model
is
 computed
 for
 comparison
 to
 prior
 results.
 This
 is
 given
 by
R2
02 =
r1ı1ˇ1
(1 +
 ı1)(ˇ1
 +
 ) .
(2.8)
Note
 that
 whether
 this
 value
 of
 R02 is
 larger
 than
 R01 depends
on
 the
 magnitude
 of
 the
 differences
 between
 ı,
 ,
 ˇ,
 r
 and
 ı1, 1, ˇ1,
r1 respectively.
 The
 square
 sign
 signiﬁes
 double
 infection
 of
 x
 and
 y1
by
 the
 merozoites.
 Comparison
 will
 be
 made
 later
 using
 numerical
simulations.
 When
 R2
02 =
 1,
 the
 critical
 value
 of
 r1, which
 gives
 the
minimum
 number
 of
 merozoites
 below
 which
 malaria
 would
 not
prevail
 in
 this
 case
 is
 obtained.
 This
 is
 given
 by
rc
1 = (1 +
 ı1)(ˇ
 +
 )
ı1ˇ1
. 
(2.9)
The
 disease-free
 equilibrium
 state
 is
 obtained
 by
 setting
y1 =
 y2 =
 m
 =
 0
 in
 system
 (2.7)
 to
 obtain
 E2(0)
 =
 (/,
 0,
 0,
 0)
 . At
endemic
 equilibrium
 the
 right
 hand
 side
 of
 Eq.
 (2.7)
 is
 set
 to
 zero
and
 the
 resulting
 system
 solved
 at
 the
 point
 E∗
2 = (˜x, ˜y, ˜m) gives  the
following
 relations,
˜x =


 +
 ˇ1 ˜m ,  
(2.10)
˜y1 =
ˇ1 ˜m
(
 +
 ˇ1 ˜m)(1 +
 ı1 +
 ˇ2 ˜m) ,
 
(2.11)
˜y2 =
ˇ1ˇ2 ˜m2
(
 +
 ˇ1 ˜m)(1 +
 ı1 +
 ˇ2 ˜m)(2 +
 ı2) ,
(2.12)
0
 =
 [A ˜m2 +
 B ˜m
 +
 C] ˜m,
 
(2.13)
0 
100 
200 
300
21
22
23
24
(a) Health RBCs
 
 
0 
100 
200 
300
0
1
2
x 10
10 (b) Differences: Health RBCs
0 
100 
200 
300
0
5
10
15
20
(c) Infected RBCs
 
 
0 
100 
200 
300
−1
−0.5
0
0.5
1
x 10
10 (d) Differences: Infected RBCs
0 
100
 
200
 
300
5
10
15
20
(e) Free Merozoite
s
 
 
0 
100
 
200
 
300
−2
0
2
x 10
9 (f) Differences: Free Merozoite
s
Basic
Proposed Basi
 
c
Basic
Proposed Basic (y1)
Proposed Basic (y2)
Basic
Proposed Basi
 
c
Fig.
 2.
 Simulation
 of
 the
 proposed
 basic
 model
 in
 Eq.
 (2.7). The
 horizontal
 axis
 represent
 the
 time
 (days)
 while
 the
 vertical
 axis
 represent
 compartments
 in
 logarithmic
 scale.

80
 
B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
 
 
 
 
 
15
15.5
16
16.5
R1
 
 
 
 
 
0.51
0.52
0.53
0.54
σ1
  
  
  
  
 
13
13.5
14
14.5
15
R2
  
  
  
  
 
0.58
0.6
0.62
σ2
2000 
4000 
6000 
8000 
10000
2.4
2.45
2.5
2.55
2.6
x 10
−10
β1
2000 
4000 
6000 
8000 
10000
1.9
2
2.1
2.2
x 10
−10
β2
Fig.
 3.
 Markov
 chain
 Monte
 Carlo
 (MCMC)
 scatter
 plots.
 The
 vertical
 axis
 represent
 samples
 and
 the
 horizontal
 axis
 represent
 number
 of
 iterations.
 The
 ﬁtted
 parameters
oscillate
 within
 their
 conﬁdence
 interval
 and
 show
 a strong
 correlation.
0.515
0.52
0.525
0.53
0.535
σ1
R1
13
14
15
R2
σ1
0.59
0.6
0.61
σ2
R2
2.45
2.5
2.55
2.6
x 10
−10
β1
σ2
15.5 16 16.5
2
2.2
x 10
−10
β2
0.515
0.52
0.525
0.53
0.535
13
14
15
0.59 0.6 0.61
2.452.52.552.6
x 10
−10
β1
Fig.
 4.
 Figure
 showing
 correlation
 of
 parameters.

B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
 
81
with
A
 =
 
ˇ1ˇ2(2 +
 ı2),
 B
 =
 (2 +
 ı2)[ˇ1ˇ2 +
 ˇ1(1 +
 ı1)
+ˇ2]
 −
 ˇ1ˇ2r2ı2,
C
 =
 
(2 +
 ı2)[(1 +
 ı1)(ˇ1 +
 ) −
 ˇ1r1ı1].
(2.14)
It
 is
 easily
 seen
 that
 A
 >
 0
 for
 positive
 parameter
 values.
 There-
fore,
 there
 exists
 two
 distinct
 positive
 roots
 of
 Eq.
 (2.13)
 if
 B
 <
 0,
C
 >
 0
 and

B2 −
 4AC
 >
 0
 given
 by
˜m∞
1,2 = −B
 ±

B2 −
 4AC
2A
.
(2.15)
These
 two
 roots
 are
 bifurcation
 points
 of
 system
 (2.7). Stability
and
 existence
 of
 equilibrium
 states
 is
 done
 using
 numerical
 simula-
tion.
 The
 steady
 state
 not
 shown
 here,
 is
 a
 spiral
 point.
 This
 implies
that
 the
 system
 has
 complex
 eigenvalues
 with
 non-zero
 real
 part.
This
 point
 is
 unstable
 if
 the
 real
 part
 is
 positive
 and
 asymptotically
stable
 if
 the
 real
 part
 is
 negative.
 Therefore,
 either
 all
 the
 trajecto-
ries
 will
 move
 in
 toward
 the
 equilibrium
 point
 as
 time
 increases,
or
 move
 away
 from
 the
 point.
As
 mentioned
 in
 preceding
 sections,
 the
 model
 in
 Eq.
 (2.7)
 is
now
 ﬁt
 using
 the
 known
 parameter
 values.
 In
 the
 ﬁtting,
 ,
 ,
 ,
and
 ı
 and
 are
 ﬁxed
 because
 they
 are
 known.
 The
 parameters
 to
 be
ﬁtted
 are
 R1
 =
 r1ı1, 1 =
 1 +
 ı1, R2
 =
 r2ı2, 2 =
 2 +
 ı2, ˇ1 and
 ˇ2.
3.
 Numerical
 results
In
 this
 section,
 simulation
 and
 parameter
 identiﬁability
 of
 the
proposed
 model
 in
 (2.7)
 is
 carried
 using
 delayed
 rejection
 adap-
tive
 Markov
 chain
 Monte
 Carlo
 (DRAM)
 method
 as
 in
 Haario
 et
 al.
(2001)
 and
 Marko
 (2008).
 To
 generate
 Fig.
 2, ﬁrst,
 data
 was
 simu-
lated
 using
 the
 literature
 values
 from
 Table
 1
 using
 the
 basic
 model.
Fig.
 2a,
 c
 and
 e
 shows
 the
 simulated
 data
 in
 logarithmic
 scale,
 from
which
 it
 is
 observed
 that
 the
 typical
 oscillations
 known
 for
 within
host
 malaria
 as
 in
 Hetzel
 and
 Anderson
 (1996)
 and
 Dondorp
 et
 al.
(2000)
 are
 preserved,
 but
 with
 different
 merozoite
 growth
 rates.
In
 this
 ﬁgure,
 both
 the
 basic
 and
 proposed
 models
 are
 plotted
 to
highlight
 the
 change
 in
 disease
 dynamics
 in
 presence
 of
 double
infection.
 Fig.
 2b
 and
 d
 shows
 differences
 between
 the
 healthy
and
 infected
 RBCs
 in
 the
 proposed
 and
 basic
 model,
 while
 Fig.
 2f
shows
 the
 differences
 in
 the
 merozoite
 abundance
 when
 a
 dou-
ble
 infection
 is
 assumed.
 From
 these
 differences,
 it
 is
 observed
 that
the
 model
 over
 estimates
 the
 rupture
 rate
 and
 number
 of
 infected
RBCs.
For
 parameter
 identiﬁability,
 using
 DRAM
 algorithm,
 the
 simu-
lated
 data
 are
 corrupted
 with
 Gaussian
 noise.
 The
 MCMC
 samples
are
 analyzed
 using
 scatter
 plots
 and
 autocorrelation.
 The
 scat-
ter
 plot
 shows
 the
 correctness
 of
 the
 chain
 and
 hence,
 one
 can
determine
 the
 goodness
 of
 performance
 of
 the
 MCMC
 method.
Fig.
 3
 shows
 the
 trend
 of
 the
 generated
 samples.
 All
 sam-
ples
 mix
 well
 though
 at
 the
 beginning
 in
 some
 of
 the
 plots
10
20
30
40
50
0
0.2
0.4
0.6
0.8
1
R1
10
20
30
40
50
0
0.2
0.4
0.6
0.8
1
σ1
10
20
30
40
50
0
0.2
0.4
0.6
0.8
1
R2
10
20
30
40
50
0
0.2
0.4
0.6
0.8
1
σ2
10 
20
30
 
40
50
0
0.2
0.4
0.6
0.8
1
β1
10 
20
30
 
40
50
0
0.2
0.4
0.6
0.8
1
β2
Fig.
 5.
 MCMC
 trace
 plots.
 The
 vertical
 axis
 represent
 samples
 and
 the
 horizontal
 axis
 represent
 number
 of
 iterations.
 The
 ﬁtted
 parameters
 oscillate
 within
 their
 conﬁdence
interval
 and
 show
 a strong
 correlation.

82
 
B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
0.515
0.52
0.525
0.53
0
100
200
300
σ1
15.4
15.6
15.8
16
16.2
16.4
0
100
200
300
R1
13
13.5
14
 
14.5
0
100
200
300
400
500
R2
0.59 
0.595 
0.6 
0.605 
0.61
0
100
200
300
σ2
2.45 
2.5 
2.55 
2.6
x 10
−10
0
50
100
150
200
250
β1
1.9 
1.95 
2 
2.05 
2.1 
2.15
x 10
−10
0
100
200
300
400
β2
Fig.
 6.
 Marginal
 distribution
 plot
 together
 with
 mean
 of
 MCMC
 samples
 (green)
 and
 the
 original
 parameter
 values
 (black).
 (For
 interpretation
 of
 the
 references
 to
 color
 in
this
 ﬁgure
 legend,
 the
 reader
 is
 referred
 to
 the
 web
 version
 of
 the
 article.)
(for
 instance
 R1,
 1 and
 ˇ1),
 the
 mixing
 is
 not
 good.
 This
might
 be
 caused
 by
 poor
 initial
 covariance
 matrix
 which
 is
later
 updated.
 The
 parameters
 are
 well
 correlated
 as
 depicted
 in
Fig.
 4.
The
 MCMC
 samples
 converge
 to
 a
 right
 distribution
 as
 shown
 in
Fig.
 5
 where
 the
 autocorrelation
 coefﬁcients
 drop
 to
 and
 stabilizes
around
 zero.
Fig.
 6
 shows
 the
 marginal
 distribution
 together
 with
 the
 sam-
ple
 means
 and
 original
 parameters.
 It
 is
 observed
 that
 all
 marginal
distributions
 (except
 that
 of
 ˇ1)
 are
 skewed
 to
 the
 right.
 The
 Kur-
tosis
 coefﬁcients
 for
 all
 sample
 parameters
 are
 approximate
 to
three,
 which
 means
 their
 marginal
 distributions
 are
 close
 to
 nor-
mal  distribution.
 As
 seen
 from
 Fig.
 6, the
 sample
 mean
 for
 R1
 =
 r1ı1,
1 =
 1 +
 ı1, R2
 =
 r2ı2, 2 =
 2 +
 ı2, ˇ1 and
 ˇ2 are
 15.8962,
 0.5241,
13.8188,
 0.6014,
 2.5119
 ×
 10−10 and
 2.0108
 ×
 10−10 respectively.
These
 mean
 estimates
 give
 r1 =
 31.7924,
 1 =
 0.0241,
 r2 =
 27.6375
and
 2 =
 0.1014.
 From
 these
 results,
 if,
 as
 stated
 by
 Ginsburg
 and
Hoshen
 (2002), the
 uninfected
 RBCs
 are
 restricted
 due
 to
 capillary
logging,
 then
 two
 merozoites
 may
 enter
 the
 uninfected
 red
 blood
cells
 and
 release
 the
 maximum
 possible
 number
 of
 new
 mero-
zoites
 (equal
 to
 32
 (Malaria,
 2012b)).
 In
 this
 case,
 the
 infected
RBCs
 will
 die
 due
 to
 infection
 at
 the
 same
 rate
 as
 that
 known
from
 prior
 studies
 (≈0.025
 (Dondorp
 et
 al.,
 2000;
 Cuadros
 and
García-Ramos,
 2012)).
 If
 the
 merozoites
 enter
 already
 infected
 RBC,
they
 produce
 more
 merozoites
 in
 the
 48
 h
 cycle,
 outside
 the
 range
(16–20
 (Cuadros
 and
 García-Ramos,
 2012))
 as
 determined
 from
prior
 studies.
 Therefore,
 there
 is
 increased
 chance
 of
 more
 infection
in
 this
 case.
 Further,
 from
 the
 results,
 2 =
 0.1006,
 which
 is
 higher
than
 in
 usual
 cases
 (Dondorp
 et
 al.,
 2000;
 Thibodeaux,
 2010).
 This
implies
 that
 fewer
 infected
 RBCs
 die
 off
 due
 to
 the
 disease
 (from
 40
(Dondorp
 et
 al.,
 2000;
 Cuadros
 and
 García-Ramos,
 2012)
 to
 approx-
imately
 8
 per
 day).
 This
 result
 signiﬁes
 that
 more
 infected
 red
 cells
rupture
 due
 to
 infection
 instead
 of
 simply
 dying
 off.
 This
 stresses
increased
 chances
 of
 anemia
 in
 the
 host.
 It
 may
 also
 indicate
 why
there
 is
 lack
 of
 uniformity
 of
 the
 oscillations
 in
 malaria
 mod-
els
 in
 contrast
 to
 what
 literature
 suggests
 (48
 h
 rupture
 (Medical
Biology-diseases,
 2012) at
 regular
 intervals
 (Marsh,
 2012;
 Malaria,
2012a)).
From
 the
 results,
 it
 can
 therefore
 be
 suggested
 that
 in
 case
 of
additional
 infection,
 more
 merozoites
 are
 released
 and
 this
 may
result
 in
 anemia
 of
 the
 host
 especially
 since
 at
 this
 time
 malaria
is
 severe
 and
 there
 is
 restricted
 abundance
 of
 uninfected
 RBCs.
This
 leads
 to
 reduction
 in
 the
 number
 of
 infected
 RBCs
 destroyed
naturally.
 Thus,
 most
 of
 the
 infected
 RBCs
 rupture
 due
 to
 infec-
tion.
 In
 this
 ﬁtting,
 it
 was
 assumed
 that
 merozoites
 do
 not
 freely
attack
 RBCs
 and
 therefore
 ˇ1 and
 ˇ2 were
 ﬁxed.
 If
 one
 of
 the

B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
 
83
parameters
 (ˇ1)
 was
 ﬁxed
 and
 ˇ2 ﬁt,
 correlation
 of
 parameters
is
 not
 as
 much
 as
 when
 both
 are
 ﬁxed.
 Thus,
 it
 can
 be
 concluded
that
 during
 within-host
 episodes
 of
 malaria,
 there
 are
 restrictions
on
 how
 the
 merozoites
 attack
 RBCs.
 Understanding
 the
 underly-
ing
 mechanisms
 of
 such
 attacks
 can
 best
 be
 explained
 by
 clinical
experiments.
4.
 Discussion
 and
 conclusions
This
 paper
 studies
 mathematical
 models
 for
 within-host
 malaria
with
 different
 mechanisms
 of
 parasite
 growth.
 It
 shows
 how
 a
model
 can
 be
 ﬁt
 to
 data
 and
 parameter
 variation
 and
 distribution
studied
 using
 the
 Markov
 chain
 Monte
 Carlo
 (MCMC)
 algorithm.
Two
 models
 were
 used;
 the
 ﬁrst,
 a
 basic
 malaria
 model
 where
 the
life
 of
 an
 infected
 red
 cell
 is
 assumed
 to
 follow
 an
 exponential
 decay,
with
 an
 average
 span
 equal
 to
 the
 parasite
 growth
 cycle
 (Hetzel
 and
Anderson,
 1996;
 Dondorp
 et
 al.,
 2000).
 This
 model
 describes
 the
population
 at
 discrete
 times
 in
 terms
 of
 the
 multiplication
 factors
which
 occur
 between
 different
 stages
 and
 the
 probability
 of
 surviv-
ing
 from
 one
 time
 point
 to
 the
 next
 and
 the
 size
 of
 the
 population
at
 a
 previous
 time.
 The
 second
 model
 hypothesizes
 a
 case
 of
 severe
malaria
 where
 there
 is
 limited
 supply
 of
 uninfected
 RBCs
 such
 that
parasites,
 in
 their
 quest
 for
 survival
 and
 to
 multiply
 faster,
 get
absorbed
 in
 already
 infected
 RBCs.
 Using
 the
 two
 models,
 reliable
parameter
 distributions
 were
 obtained.
 From
 these,
 the
 likelihood
of
 double
 infection
 was
 provided.
 The
 parasite
 growth
 rate
 in
 the
case
 of
 exponential
 growth
 was
 found
 to
 be
 highest
 when
 there
is
 double
 infection,
 signifying
 an
 increased
 rupture
 rate
 that
 may
happen
 before
 48
 h.
 This
 is
 also
 explained
 by
 the
 increased
 oscilla-
tions
 in
 this
 model,
 more
 than
 that
 in
 the
 discrete
 case.
 The
 rate
at
 which
 infected
 RBCs
 die
 off
 was
 lowered
 in
 the
 case
 of
 dou-
ble
 infection.
 This
 means
 that
 more
 infected
 red
 cells
 rupture
 to
release
 new
 merozoites
 for
 more
 infection.
 Most
 of
 the
 ﬁt
 param-
eters
 were
 shown
 to
 be
 highly
 correlated.
 However,
 in
 all
 cases,
 r
and
 ı
 were
 found
 to
 be
 extremely
 correlated
 with
 each
 other
 and
this
 made
 it
 hard
 to
 estimate
 them
 separately
 and
 thus
 were
 ﬁt
together.
We  also
 considered
 effect
 of
 additional
 infection
 at
 population
level.
 Assuming
 that
 double
 infection
 increases
 parasite
 load
 in
 an
individual,
 the
 population
 attributes
 constant
 was
 estimated
 to
 be
44%.
 This
 would
 imply
 that
 increase
 in
 parasite
 load
 within
 a
 host
may
 affect
 the
 outcome
 of
 between-hosts
 at
 population
 level.
 This
result
 is
 possible
 in
 cases
 of
 severe
 malaria
 where
 there
 is
 limited
supply
 of
 uninfected
 RBCs
 (Ginsburg
 and
 Hoshen,
 2002).
 Prior
 study
such
 as
 that
 of
 Saul
 (1998)
 have
 indicated
 over
 estimation
 of
 the
well
 known
 and
 widely
 used
 models
 of
 within-host
 malaria.
 His
suggested
 function
 of
 merozoite
 growth
 showed
 loss
 of
 the
 known
periodicity
 in
 malaria
 within
 a
 host
 and
 needs
 further
 analysis.
 His
results
 have
 since
 been
 criticized
 by
 other
 researchers
 and
 a
 better
approaches
 suggested.
A
 possible
 way
 that
 could
 provide
 best
 estimates
 for
 para-
site
 growth
 within
 a
 host
 may
 be
 using
 a
 discrete
 approach.
The
 results
 in
 this
 study
 provide
 good
 insights
 into
 the
 complex
system
 of
 malaria
 and
 may
 vary
 based
 on
 numerous
 factors,
 how-
ever,
 they
 are
 speculative
 and
 actual
 experiments
 would
 give
 a
better
 phenomenon
 of
 what
 happens
 within
 a
 host
 in
 cases
 of
severe
 malaria.
 It
 is
 important
 to
 note
 however
 that
 the
 dynam-
ics
 of
 the
 infection
 changed
 greatly
 in
 this
 setting
 and
 that
 calls
for
 in-depth
 research
 of
 the
 phenomenon.
 Our
 results
 highlight
the
 well
 known
 fact
 that
 within-host
 malaria
 infection
 is
 a
 com-
plex
 system
 that
 may
 never
 be
 fully
 understood.
 This
 is
 greatly
due
 to
 the
 complicated
 life
 cycle
 and
 behavior
 of
 the
 plasmod-
ium
 parasite.
 Controlling
 malaria
 within
 a
 host
 may
 save
 the
host,
 and
 also
 reduce
 on
 the
 between-host
 transmission.
 How-
ever,
 these
 results
 are
 from
 a
 mathematical
 perspective
 and
 clinical
research
 may
 provide
 a
 better
 description
 of
 such
 a
 complex
 sys-
tem.
Acknowledgments
The
 authors
 wish
 to
 thank
 the
 anonymous
 reviewers
 for
 the
insightful
 comments
 that
 made
 this
 manuscript
 better.
References
Achcar,
 J.A.,
 Martinez,
 E.Z.,
 Souza,
 A.D.,
 Tachibana,
 V.M.,
 Flores,
 E.F.,
 2011.
 Use
 of
 Pois-
son spatiotemporal
 regression
 models
 for
 the
 Brazilian
 Amazon
 Forest:
 malaria
count
 data.
 Rev.
 Soc.
 Bras.
 Med.
 Trop.
 44
 (6),
 749–754.
Bruce-Chwatt,
 L.J.,
 1980.
 Essential
 Malariology.
 Heinemann
 Medical
 Books,
 London.
Clyde,
 D.F.,
 1987.
 Recent
 trends
 in
 the
 epidemiology
 and
 control
 of
 malaria.
 Epi-
demiol.
 Rev.
 9,
 21–43.
Cuadros,
 F.D.,
 García-Ramos,
 G.,
 2012.
 Variable
 effect
 of
 co-infection
 on
 the
 HIV
 infec-
tivity:
 within-host
 dynamics
 and
 epidemiological
 signiﬁcance.
 Theoret.
 Biol.
Med.
 Model.
 9,
 9.
Dondorp,
 A.M.,
 Kager,
 P.A.,
 Vreeken,
 J.,
 White,
 N.J.,
 2000.
 Abnormal
 blood
 ﬂow
and
 red
 blood
 cell
 deformability
 in
 severe
 malaria.
 Parasitol.
 Today
 16,
228–232.
Garnham,
 P.C.C.,
 1966.
 Malaria
 Parasites
 and
 Other
 Haemosporidia.
 Blackwell,
Oxford.
Ginsburg,
 H.,
 Hoshen,
 M.B.,
 2002.
 Is
 the
 development
 of
 falciparum
 malaria
 in
 the
human
 host
 limited
 by
 the
 availability
 of
 uninfected
 erythrocytes?
 Malar.
 J. 1,
18.
Gravenor,
 M.B.,
 Kwiatkowski,
 D.,
 1998.
 An
 analysis
 of
 the
 temperature
 effects
 of
 fever
on  the
 intra-host
 population
 dynamics
 of
 Plasmodium
 falciparum. Parasitology
117,
 97–105.
Gravenor,
 M.B.,
 McLean,
 A.R.,
 Kwiatkowski,
 D.,
 1995.
 The
 regulation
 of
 malaria
parasitaemia:
 parameter
 estimates
 for
 a
 population
 model.
 Parasitology
 110,
115–122.
Gravenor,
 M.B.,
 van
 Hensbroek,
 M.B.,
 Kwiatkowski,
 D.,
 1998.
 Estimating
 sequestered
parasite
 population
 dynamics
 in
 cerebral
 malaria.
 Proc.
 Natl.
 Acad.
 Sci.
 U.
 S.
 A.
95,
 7620–7624.
Gupta,
 S.,
 Hill,
 A.V.S.,
 Kwiatkowski,
 D.,
 Greenwood,
 A.M.,
 Greenwood,
 B.M.,
 Day,
 K.P.,
1994.
 Parasite
 virulence
 and
 disease
 patterns
 in
 Plasmodium
 falciparum
 malaria.
Proc.
 Natl.
 Acad.
 Sci.
 U.
 S.
 A.
 91,
 3715–3719.
Haario,
 H.,
 Saksman,
 E.,
 Tamminen,
 J.,
 2001.
 An
 adaptive
 metropolis
 algorithm.
Bernoulli
 7,
 223–242.
Hale,
 J.K.,
 1969.
 Ordinary
 Differential
 Equations.
 John
 Wiley,
 New
 York.
Hetzel,
 C.,
 Anderson,
 R.M.,
 1996.
 The
 within-host
 cellular
 dynamics
 of
 blood
stage
 malaria
 –
 theoretical
 and
 experimental
 studies.
 Parasitology
 113,
25–38.
Hoshen,
 M.B.,
 Heinrich,
 R.,
 Stein,
 W.D.,
 Ginsburg,
 H.,
 2000.
 Mathematical
 mod-
elling
 of
 the
 within-host
 dynamics
 of
 Plasmodium
 falciparum. Parasitology
 121,
227–235.
Hsu,
 B.,
 2005.
 A
 survey
 of
 constructing
 Lyapunov
 functions
 for
 mathematical
 models
in
 population
 biology.
 Taiwan.
 J.
 Math.
 9,
 151–175.
Johnston,
 G.L.,
 Gething,
 P.W.,
 Hay,
 S.I.,
 Smith,
 D.L.,
 Fidock,
 D.A.,
 2014.
 Model-
ing
 within-host
 effects
 of
 drugs
 on
 Plasmodium
 falciparum
 transmission
 and
prospects
 for
 malaria
 elimination.
 PLOS
 Comput.
 Biol.
 10
 (1),
 e1003434.
Khalil,
 H.K.,
 2002.
 Nonlinear
 Systems.
 Prentice
 Hall,
 New
 York.
Krier,
 J.P.,
 Baker,
 J.R.,
 1980.
 Parasitic
 Protozoa.
 Academic
 Press,
 New
 York,
 NY.
Krogstad,
 D.J.,
 2012.
 Malaria.
 http://www.123esaaf.com/Diseases/Malaria/Malaria
en.html
 (accessed
 15.06.12).
Makinde,
 O.D.,
 Okosun,
 K.O.,
 2011.
 Impact
 of
 chemo-therapy
 on
 optimal
 con-
trol of
 malaria
 disease
 with
 infected
 immigrants.
 Biosystems
 104
 (1),
32–41.
2012a.
 
Malaria,
 
Malaria
 
History
 
and
 
the
 
Malaria
 
Parasite
 
and
 
Life
 
Cycle.
http://factsanddetails.com/world.php?itemid=2145&catid=57&subcatid=381
(accessed
 25.06.12).
2012b.
 Malaria
 –
 A
 Growing
 Threat.
 Science
 in
 the
 News,
 Australian
 Academy
of  Science,
 Nova
 http://www.science.org.au/nova/011/011box01.htm
 (accessed
14.06.12).
Marko,
 E,
 2008.
 Adaptive
 MCMC
 Methods
 with
 Applications
 in
 Environmental
 and
Geographical
 Models.
 Lappeenranta
 University
 of
 Technology
 (PhD
 thesis).
Marsh,
 K.,
 2012.
 Malaria
 and
 the
 Human
 Body,
 Part
 1:
 Danger
 Cycle.
 Wellcome
 Trust
http://malaria.wellcome.ac.uk/doc
 WTD023879.html
 (accessed
 17.06.12).
2012.
 Medical
 Biology-Diseases.
 http://microbio-quiz.wikispaces.com/8a.+Medical+
Microbiology+-+diseases
 (accessed
 18.06.12).
Okosun,
 K.O.,
 Ouifki,
 R.,
 Marcus,
 N.,
 2011.
 Optimal
 control
 analysis
 of
 a
 malaria
 dis-
ease  transmission
 model
 that
 includes
 treatment
 and
 vaccination
 with
 waning
immunity.
 Biosystems
 106
 (2-3),
 136–145.
Okosun,
 K.O.,
 Rachid,
 O.,
 Marcus,
 N.,
 2013.
 Optimal
 control
 strategies
 and
cost-effectiveness
 
analysis
 
of
 
a 
malaria
 
model.
 
Biosystems
 
111
 
(2),
83–101.
Ponnudurai,
 T.,
 Verhave,
 J.P.,
 Meuwissen,
 J.H.,
 1982.
 Mosquito
 transmission
of cultured
 Plasmodium
 falciparum. Trans.
 R.
 Soc.
 Trop.
 Med.
 Hyg.
 76,
278–279.

84
 
B.
 Nannyonga
 et
 al.
 / BioSystems
 126
 (2014)
 76–84
Rosenberg,
 R.,
 Wirtz,
 R.A.,
 Schneider,
 I.,
 Burge,
 R.,
 1990.
 An
 estimation
 of
 the
 number
of  malaria
 sporozoites
 ejected
 by
 a feeding
 mosquito.
 Trans.
 R.
 Soc.
 Trop.
 Med.
Hyg.
 84,
 209–212.
Saul,
 A.,
 1998.
 Models
 for
 the
 in-host
 dynamics
 of
 malaria
 revisited:
 errors
 in
 some
basic
 models
 lead
 to
 large
 over-estimates
 of
 growth
 rates.
 Parasitology
 115,
405–407.
Slater,
 A.F.G.,
 Cerami,
 A.,
 1992.
 Inhibition
 by
 chloroquine
 of
 a
 novel
 haem
 polymerase
enzyme
 activity
 in
 malaria
 trophozoites.
 Nature
 355,
 167–169.
Thibodeaux,
 J.,
 2010.
 Modeling
 erythropoiesis
 subject
 to
 malaria
 infection.
 Math.
Biosci.
 225,
 59–67.
White,
 N.J.,
 Ho,
 M.,
 1992.
 The
 pathophysiology
 of
 malaria.
 Adv.
 Parasitol.
 31,
83–173.
Winzeler,
 E.A.,
 2008.
 Malaria
 research
 in
 the
 post-genomic
 era.
 Nature
 455,
751–756
 http://www.nature.com/nature/journal/v455/n7214/full/nature07361.
html

ACTA UNIVERSITATIS LAPPEENRANTAENSIS 
 
 
 
 
573.      KINNARINEN, TEEMU. Pressure filtration characteristics of enzymatically hydralyzed biomass 
suspensions. 2014. Diss. 
 
574.      LAMMASSAARI, TIMO. Muutos kuntaorganisaatiossa – tapaustutkimus erään kunnan teknisestä 
toimialasta. 2014. Diss. 
 
575.      KALWAR, SANTOSH KUMAR. Conceptualizing and measuring human anxiety on the Internet. 
2014. Diss. 
 
576.      LANKINEN, JUKKA. Local features in image and video processing – object class matching and 
video shot detection. 2014. Diss. 
 
577.      AL-SAEDI, MAZIN. Flexible multibody dynamics and intelligent control of a hydraulically driven 
hybrid redundant robot machine. 2014. Diss. 
 
578.      TYSTER, JUHO. Power semiconductor nonlinearities in active du/dt output filtering. 2014. Diss. 
 
579.      KERÄNEN, JOONA. Customer value assessment in business markets. 2014. Diss. 
 
580.      ALEXANDROVA, YULIA. Wind turbine direct-drive permanent-magnet generator with direct liquid 
cooling for mass reduction. 2014. Diss. 
 
581.      HUHTALA, MERJA. PDM system functions and utilizations analysis to improve the efficiency of 
sheet metal product design and manufacturing. 2014. Diss. 
 
582.      SAUNILA, MINNA. Performance management through innovation capability in SMEs. 2014. Diss. 
 
583.      LANA, ANDREY. LVDC power distribution system: computational modelling. 2014. Diss. 
 
584.      PEKKARINEN, JOONAS. Laser cladding with scanning optics. 2014. Diss. 
 
585.      PELTOMAA, JYRKI. The early activities of front end of innovation in OEM companies using a new 
FEI platform as a framework for renewal. 2014. Diss. 
 
586.      ROZHANSKY, IGOR. Resonant tunneling effects in semiconductor heterostructures. 2014. Diss. 
 
587.      PHAM, THUY DUONG. Ultrasonic and electrokinetic remediation of low permeability soil 
contaminated with persistent organic pollutants. 2014. Diss. 
 
588.      HOKKANEN, SANNA. Modified nano- and microcellulose based adsorption materials in water 
treatment. 2014. Diss. 
 
589.      HINKKANEN, JUHA. Cooperative strategy in emerging markets – analysis of interfirm R&D 
cooperation and performance in Russian manufacturing companies. 2014. Diss. 
 
590.      RUSKOVAARA, ELENA. Entrepreneurship education in basic and upper secondary education – 
measurement and empirical evidence. 2014. Diss. 
 
591.      IKÄHEIMONEN, TUULI. The board of directors as a part of family business governance – multilevel 
participation and board development. 2014. Diss. 
 
592.      HAJIALI, ZUNED. Computational modeling of stented coronary arteries. 2014. Diss. 
 
593.      UUSITALO, VILLE. Potential for greenhouse gas emission reductions by using biomethane as road 
transportation fuel. 2014. Diss. 
 

594.      HAVUKAINEN, JOUNI. Biogas production in regional biodegradable waste treatment – possibilities 
for improving energy performance and reducing GHG emissions. 2014. Diss. 
 
595.      HEIKKINEN, JANNE. Vibrations in rotating machinery arising from minor imperfections in component 
geometries. 2014. Diss. 
 
596.      GHALAMCHI, BEHNAM. Dynamic analysis model of spherical roller bearings with defects. 2014. 
Diss. 
 
597.      POLIKARPOVA, MARIIA.Liquid cooling solutions for rotating permanent magnet synchronous 
machines. 2014. Diss. 
 
598.      CHAUDHARI, ASHVINKUMAR. Large-eddy simulation of wind flows over complex terrains for wind 
energy applications. 2014. Diss. 
 
599.      PURHONEN, MIKKO. Minimizing circulating current in parallel-connected photovoltaic inverters. 
2014. Diss. 
 
600.      SAUKKONEN, ESA. Effects of the partial removal of wood hemicelluloses on the properties of kraft 
pulp. 2014. Diss. 
 
601.      GUDARZI, DAVOOD. Catalytic direct synthesis of hydrogen peroxide in a novel microstructured 
reactor. 2014. Diss. 
 
602.      VALKEAPÄÄ, ANTTI. Development of finite elements for analysis of biomechanical structures using 
flexible multibody formulations. 2014. Diss. 
 
603.      SSEBUGERE, PATRICK. Persistent organic pollutants in sediments and fish from Lake Victoria, 
East Africa. 2014. Diss. 
 
604.      STOKLASA, JAN. Linguistic models for decision support. 2014. Diss. 
 
605.      VEPSÄLÄINEN, ARI. Heterogenous mass transfer in fluidized beds by computational fluid dynamics. 
2014. Diss. 
 
606.      JUVONEN, PASI. Learning information technology business in a changing industry landscape. The 
case of introducing team entreneurship in renewing bachelor education in information technology in 
a university of applied sciences. 2014. Diss. 
 
607.      MÄKIMATTILA, MARTTI. Organizing for systemic innovations – research on knowledge, interaction 
and organizational interdependencies. 2014. Diss. 
 
608.      HÄMÄLÄINEN, KIMMO. Improving the usability of extruded wood-plastic composites by using 
modification technology. 2014. Diss. 
 
609.      PIRTTILÄ, MIIA. The cycle times of working capital: financial value chain analysis method. 2014. 
Diss. 
 
610.      SUIKKANEN, HEIKKI. Application and development of numerical methods for the modelling of 
innovative gas cooled fission reactors. 2014. Diss. 
 
611.      LI, MING. Stiffness based trajectory planning and feedforward based vibration suppression control of 
parallel robot machines. 2014. Diss.  
 
612.      KOKKONEN, KIRSI. From entrepreneurial opportunities to successful business networks – evidence 
from bioenergy. 2014. Diss.  
 
613.      MAIJANEN-KYLÄHEIKO, PÄIVI. Pursuit of change versus organizational inertia: a study on strategic 
renewal in the Finnish broadcasting company. 2014. Diss.  
 

 

