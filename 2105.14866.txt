Variational Autoencoders: A Harmonic Perspective
Alexander Camuto
acamuto@turing.ac.uk
University of Oxford & Alan Turing Institute
Matthew Willetts
mwilletts@turing.ac.uk
University College London & Alan Turing Institute
Abstract
In this work we study Variational Autoencoders (VAEs) from the perspective of harmonic analysis. By viewing
a VAE’s latent space as a Gaussian Space, a variety of measure space, we derive a series of results that show
that the encoder variance of a VAE controls the frequency content of the functions parameterised by the VAE
encoder and decoder neural networks. In particular we demonstrate that larger encoder variances reduce the
high frequency content of these functions. Our analysis allows us to show that increasing this variance eﬀectively
induces a soft Lipschitz constraint on the decoder network of a VAE, which is a core contributor to the adversarial
robustness of VAEs. We further demonstrate that adding Gaussian noise to the input of a VAE allows us to
more ﬁnely control the frequency content and the Lipschitz constant of the VAE encoder networks. To support
our theoretical analysis we run experiments with VAEs with small fully-connected neural networks and with
larger convolutional networks, demonstrating empirically that our theory holds for a variety of neural network
architectures.
1. Introduction
Variational autoencoders (VAEs) are deep latent variable models that typically use Gaussian priors and Gaussian
posteriors in their latent spaces
Rezende et al. (2014); Kingma and Welling (2014). VAEs have become a
work-horse method in modern machine learning, but still their theoretical properties are not fully understood. In
particular, we do not yet fully understand the regularising eﬀect of latent space sampling during training.
While the eﬀect of the latent space sampling (a.k.a latent noise) on VAEs has been studied from an information-
theoretic standpoint (Shu et al., 2018) and through Taylor analysis (Kumar and Poole, 2020), here we take
a diﬀerent tack and study the impact that latent Gaussian samples have on the harmonic properties of the
underlying neural networks used to implement the model.
Related to our inquiry, is the study of the eﬀects of Gaussian noise injections on neural networks trained on
supervised tasks (Yin et al., 2019; Camuto et al., 2021b,a). Adding (standard) Gaussian noise to the input layer
has long been known to induce regularisation (Webb, 1994; Bishop, 1995; Burger and Neubauer, 2003), and
recently has been used to induce robustness to adversarial attacks (Cohen et al., 2019). Recently, by studying the
eﬀects of these Gaussian noise injections from a functional analysis perspective, (Camuto et al., 2021b) shows
that these injections eﬀectively penalise functions which learn high frequency components in Fourier-space. These
ideas form the starting point for our work.
By considering the latent space of a VAE as a measure space, equipped with a Gaussian measure, we can consider
the decoder of the VAE as being a member of a Gaussian Space, a type of L2 function space equipped with the
Gaussian measure. In our analysis, we view the latent variable’s posterior, a Gaussian, as being the Gaussian
measure with which the latent space is equipped on a per-datapoint basis. This posterior is located around a
particular location, the mean, with a particular scale, the (often-diagonal) standard deviation.
Previous analysis in Gaussian spaces has been done for spaces equipped with the standard Gaussian measure
(zero mean and unit variance). To directly apply the theory of Gaussian spaces to VAEs, we extend the theory of
these spaces for standard Gaussian measures to be applicable to general Gaussian spaces with arbitrary mean
and covariance.
Using this newly developed theory we consider a basis-expansion of the decoder of a VAE in terms of the
1
arXiv:2105.14866v2  [stat.ML]  1 Jun 2021

1.0
0.5
0.0
0.5
1.0
t
0.2
0.0
0.2
0.4
0.6
0.8
1.0
= 0.1, k * = 12
Original
Recon
1.0
0.5
0.0
0.5
1.0
t
0.2
0.0
0.2
0.4
0.6
0.8
1.0
= 0.5, k * = 6
Original
Recon
1.0
0.5
0.0
0.5
1.0
t
0.2
0.0
0.2
0.4
0.6
0.8
1.0
= 1.0, k * = 0
Original
Recon
Figure 1: We plot the reconstructed function of a VAE, with the Sigmoid activation function networks (3 dense
layer each with 256 units), trained on the function sinc(5t), t ∈[ −1, 1]. We use a one dimensional latent space
Z with a ﬁxed encoder variance σφ ∈[0.1, 0.5, 1.0]. As σφ increases we can see qualitatively that the frequency
of the reconstructed function increasingly skews towards lower frequencies. We ﬁt polynomial regression to the
reconstructed function, picking the optimal polynomial degree k∗via cross-validation on 10 randomly chosen
train-test splits. As σφ increases, the lower frequency content of the decoder function corresponds, as one might
expect, to a lower optimal polynomial degree k∗.
eigenfunctions of a general Gaussian space, the Hermite polynomials. This then enables us to see how higher
degrees of these basis functions, these polynomials, are implicitly more-strongly penalised as the variance of
the underlying Gaussian space increases, leading the decoder of VAE to preferentially learn functions that can
be represented as lower degree polynomials. As higher-order polynomials are naturally associated with higher
frequency content in the Fourier domain, this means that a higher variance Gaussian measure in the latent space
necessarily leads to decoders with lower-frequency functional representations.
Further, the encoder, which in our analysis parameterises the Gaussian measure of the latent space, is also aﬀected
by changes in the latent measure’s variance. By studying the Fourier transform of Gaussian measures we can
show very simply that larger-variance measures have less high frequency content in the Fourier domain. This
implies that encoders that learn high-variance posteriors have lower-frequency functional representations.
To modulate the frequency content of the individual networks that constitute the VAE encoder, we study the
eﬀect of adding Gaussian noise to VAE inputs during training. This noising operation eﬀectively turns the
input space into a Gaussian space on a per-datapoint basis. In the context of generative models, this process is
reminiscent both of Spread Divergences (Zhang et al., 2020) and dequantisation (Dinh et al., 2017; Salimans
et al., 2017; Ho et al., 2019). As we demonstrate both theoretically and empirically, these noise additions enable
ﬁner-grained control over the harmonic content of the encoder mean network.
Finally, using this method of harmonic analysis, we extend Nash’s Poincar´e inequality to general Gaussian spaces.
This shows that Gaussian spaces implicitly induce a soft constraint on the Lipschitz constant of their constituent
function. In the VAE setting, we use these results to show that the Lipschitz constant of the VAE decoder
decreases as the posterior variance increases. Similarly, for the encoder we show that as the variance of Gaussian
noise on the VAE inputs increases, the Lipschitz constant of the encoder mean network decreases.
This result links our work both to recent advances in the Lipschitz penalisation and regularisation of deep
generative models (Adler and Lunz, 2018; Terj´ek, 2020) and more generally to the adversarial robustness literature
where control of the Lipschitz constant improves the robustness of models (Gouk et al., 2018; Yang et al., 2020;
Hein and Andriushchenko, 2017; Tsuzuku et al., 2018). Our novel theoretical viewpoint on VAEs uniﬁes the two
emerging strands in the empirical and theoretical study of the robustness of VAEs to adversarial attack. The ﬁrst
strand aims to tune the noise of the VAE latent space by up-weighting various regularisation terms (Willetts
et al., 2021; Camuto et al., 2021c). The second aims to directly control the Lipschitz constants of the underlying
networks (Barrett et al., 2021). In our framework we link these two strands: the Gaussian noise of a VAE’s latent
2

space aﬀects the Lipschitz constants of the VAE’s constituent networks.
2. Background
Variational Autoencoders:
VAEs (Kingma et al., 2014; Kingma and Welling, 2014), and the models they
have inspired (Alemi et al., 2017; Kumar and Poole, 2020; Chen et al., 2018; Willetts et al., 2021), are deep
latent variable models. Using x ∈X to denote data and z ∈Z to denote the latents with associated prior p(z), a
VAE simultaneously learns both a forward generative model, pθ(x|z), and an amortised approximate posterior
distribution, qφ(z|x) (where θ and φ correspond to their respective parameters) which are typically implemented
using neural networks 1. These models are referred to as the decoder and encoder respectively, and a VAE
can be thought of as a deep stochastic autoencoder. Under this autoencoder framework, one typically takes
the reconstructions as deterministic, corresponding to the mean of the decoder, namely gθ(z) := Epθ(x|z) [x], a
convention we adopt.
A VAE is trained by maximizing the evidence lower bound (ELBO) L=EpD(x) [L(x)], where
L(x) = Eqφ(z|x) [log pθ(x|z)] −KL(qφ(z|x)||p(z))
(2.1)
and pD(x) is the empirical data distribution. The optimisation is carried out using stochastic gradient ascent with
Monte Carlo samples to evaluate expectations over z, typically employing the reparameterisation trick (Kingma
and Welling, 2014). For example, for a Gaussian qφ(z|x), we draw samples as z = µφ(x) + ϵ ◦σφ(x), ϵ ∼N(0, I),
where ◦is the element-wise product.
Gaussian Spaces:
A Gaussian Space, denoted L2(Rn, γ) is an L2 space, the space of square-integrable functions
f : Rn →R, equipped with the Gaussian measure γ(x) = Q
i N(xi|0, 1). This space has an inner product between
two functions f and g: ⟨f, g⟩= Eγ(x) [f(x)g(x)].
On the real line, the Hermite polynomials form an orthogonal basis for the space L2(R, γ). These are the
polynomials of degree k that satisfy (Janson, 1997):
Eγ(x) [Hk(x)Hm(x)] = k!1{m = k}
(2.2)
These polynomials can also be deﬁned recursively, in a manner that allows a more intuitive understanding of
their polynomial nature:
Hα+1(x) = xHα(x) −nHα−1(x)
(2.3)
H0(x) = 1, H1(x) = x
We have for instance
H2(x) = x2 −1, H3(x) = x3 −3x, H4(x) = x4 −6x2 + 3
A function f ∈L2(R, γ) in this space can be expressed as a weighted sum of these polynomial functions, where
ˆf(k) are the Hermite coeﬃcients:
f =
X
k∈N
1
α!
ˆf(k)Hk,
ˆf(k) = ⟨f, hk⟩= Eγ(x) [f(x)Hk(x)] .
(2.4)
3. Gaussian Spaces and VAEs
We want to bring the tools of Gaussian space analysis to bear on VAEs, by viewing the latent space of the VAE as
being equipped with a Gaussian measure. The VAE encoder parameterises an amortised (per-datapoint) posterior
Gaussian distribution with mean µφ(x) and diagonal variance diag(σ2
φ(x)). Thus we can view the latent space
of a VAE as being equipped with a Gaussian measure that varies on a per-datapoint basis. This measure is a
1. Notation: We use bold letters to denote vectors or matrices and non-bolded letters to denote scalars. Matrices are capitalised.
3

multivariate general Gaussian measure with mean µφ(x) and diagonal variance diag(σ2
φ(x)). The encoder in
some sense ‘indexes’ over the range of possible Gaussian measures in a data-dependent manner. The decoder,
which acts on this latent space, would then be a member of some Gaussian space (the measure for which depends
on x).
As we were unable to ﬁnd a comprehensive resource for spaces equipped with non-standard Gaussian measures
we must ﬁrst derive results for Gaussian spaces equipped with a general Gaussian measure (with mean µ and
variance σ2) before we apply Gaussian space analysis to VAEs. We start with results for functions acting on a
univariate space, f : R →R.
Proposition 1. If x ∼N(µ, σ2), then we have that y = (x −µ)/σ ∼N(0, 1) and the Hermite polynomials are
the polynomials of degree k that satisfy:
Eγ(y) [Hk(y)Hm(y)] = k!1{m = k},
where γ is the standard Gaussian measure. The family
n
1
√
k!Hk(y) : k ≥0
o
is then an orthonormal basis for
L2(R, N(µ, σ2)).
For the proof see Appendix A.12. As such the polynomials Hn(y) are the orthogonal polynomials for the N(µ, σ2)
measure. We can calculate each of these polynomials by substituting x for y = (x −µ)/σ in Equation (2.3). In
higher dimensions (Rn), for a function f : Rn →R, we assume that we have a diagonal standard deviation S, as
this form of the standard deviation is most directly applicable to how VAEs are used in practice.
We denote the multivariate Gaussian measure as N(µ, S2) with covariance matrix populated by the element-wise
square of S. In this case the basis can be expressed using a multi-index α ∈Nn (the sets of size n of non-negative
integers), where
|α| =
d
X
i=1
αi,
vα =
n
Y
i=1
vαi
i ,
α! =
n
Y
i=1
αi!,
g(α)(x) =
∂|α|g
∂xα1
1 . . . ∂xαn
n
.
The αth multivariate Hermite polynomial (denoted Hα) for the measure N(µ, S2) can then be expressed as the
product of univariate polynomials indexed by α:
Hα((x −µ)S−1) = Hα(y) =
Y
i
Hαi((xi −µi)/σi),
This stems from the fact that each Hαi forms a basis in the univariate case, and that we assume a diagonal
covariance, meaning that we can obtain the basis for Rn simply by tensorisation Hα1 ⊗· · · ⊗Hαn. For the sake
of completeness, in Appendix B we derive the Hermite polynomials for a Gaussian space with a full covariance
matrix.
Because the set of Hα form an orthogonal basis, we can express functions f ∈L2(Rn, N(µ, S2)) as a weighted
sum of these functions:
f =
X
α∈Nn
1
α!
ˆf(α)Hα,
ˆf(α) = Eγ(y) [f(x)Hα(y)] ,
γ(y) = N(x|µ, S2).
(3.1)
Using these results, we can express the Hermite coﬃcients for a general Gaussian space in terms of the underlying
measure’s standard deviation S and the derivatives of a function f in the space.
Proposition 2. Assume we have a function f in Gaussian space with diagonal covariance, f ∈L2(Rn, N(µ, S2)).
Further assume that f is in C∞, the class of inﬁnitely diﬀerentiable functions. For x ∼N(µ, S2), then we have
that y = (x −µ)S−1 ∼N(0, I) and the Hermite coeﬃcients can be expressed as:
ˆf(α) = (diag(S))α Eγ(y)
h
f (α)(x)
i
, γ(y) = N(x|µ, S2)
2.
All proofs are presented in Appendix A
4

We can now express the variance of a function L2(Rn, N(µ, S2)) as a sum of function derivatives, which in turns
allows us to connect this variance to the Fourier domain.
Theorem 3. Assume we have a function f in Gaussian space with diagonal covariance, f ∈L2(Rn, N(µ, S2)).
Further assume that f is in C∞, the class of inﬁnitely diﬀerentiable functions and is L2 integrable with respect to the
Lebesgue measure. For x ∼N(µ, S2), then we have that y = (x −µ)S−1 ∼N(0, I) such that γ(y) = N(x|µ, S2).
The variance of f can be expressed as
Var(f) =
X
|α|≥1
(diag(S))2α
α!
Eγ(y)
h
f (α)(x)
i
2
=
X
|α|≥1
(diag(S))2α
α!

Z
Rn(ω)αF(ω)P(ω)dω

2
,
where P is the Fourier transform of the Gaussian measure N(µ, S2) given by P(ω) = det(S)G(ωS)e−iωµS−1
(where G is the Fourier transform of the standard Gaussian measure γ) and F is the Fourier transform of f, and
α ∈Nn.
We also note that even if a function is not inﬁnitely diﬀerentiable, we can also express Var(f) as a weighted sum
of the Hermite coeﬃcients (see the Proof of Theorem 3):
Var(f) =
X
|α|≥1
1
α!
 ˆf(α)

2
.
(3.2)
If we now assume that f is inﬁnitely diﬀerentiable, then we obtain the intuitive result that high-frequency
functions correspond to larger Hermite coeﬃcients associated with higher degree Hermite polynomials. This can
be deduced by combining Proposition 2 and Theorem 3:
 ˆf(α)

2
=

Z
Rn(diag(S))α(ω)αF(ω)P(ω)dω

2
.
(3.3)
As the variance of the underlying Gaussian measure increases, larger degree polynomials contribute more heavily
to Var(f). This is captured in the terms (diag(S))2α of Equation (3.3), meaning that increases in |(diag(S))|
disproportionately increase large α terms, i.e the coeﬃcients associated with higher degree polynomial terms.
More succinctly, Gaussian spaces with large variances naturally induce larger Hermite coeﬃcients associated with
higher degree polynomials, which themselves are associated with higher frequency components in the Fourier
domain.
Lipschitzness
Finally we redevelop Nash’s Poincar´e inequality for general Gaussian spaces to show that
the variance of a function f ∈L2(Rn, N(µ, S2)), assuming that this function is Lipschitz continuous, can be
upper-bounded by using its Lipschitz constant.
Proposition 4. Let f ∈L2(Rn, N(µ, S2)) be a function that is Lipschitz continuous with Lipschitz constant L.
Further assume that f (|α|=1) ∈L2(Rn, N(µ, S2)), we have:
Var(f) ≤L2∥S∥2
2
This bound is relatively tight in that for a measure N(0, I) and a function f(x) = 1
n
P xi the bound becomes an
equality: L = n−1 and Var(f) = n−2. We can now use these results for a general Gaussian space to study VAE
decoders.
3.1
Gaussian Spaces and VAE decoders
The amortised posterior distributions of a VAE are typically chosen as Gaussians, where, to allow for the
backpropagation of gradients, samples are reparameterised as:
z = µφ(x) + σφ(x) ◦ϵ,
ϵ ∼N(0, I)
(3.4)
This sample is then fed into the decoder to calculate the likelihood term of the ELBO (see Eq (2.1)). Implicit in
this arrangement is that the VAE decoder operates on samples z in latent space Z equipped with a Gaussian
measure, captured by the ϵ term in Eq (3.4).
5

Assuming the function parameterised by the decoder g, for a given point x, is L2 integrable with respect to
the general Gaussian measure, i.e g ∈L2(Rn, N(µφ(x), σφ(x))) (this is reasonable as we are eﬀectively training
the decoder to be in this space) then we can directly apply the theory of Gaussian spaces to the VAE decoder.
Assuming we have a Gaussian likelihood with a ﬁxed standard deviation σθ across dimensions, we express the
likelihood for a single point using a bias-variance decomposition, which gives the sum of an error term captured
by the bias, and a regularisation term captured by the variance:
1
σ2
θ
Eqφ(z|x)

(gθ(z) −x)2
= 1
σ2
θ
 Biasqφ(z|x) (gθ(z)) + Varqφ(z|x)(gθ(z))

(3.5)
Biasqφ(z|x) (gθ(z)) = Eqφ(z|x) [gθ(z)] −x
Varqφ(z|x) (gθ(z)) = Eqφ(z|x)
h gθ(z) −Eqφ(z|x) [gθ(z)]
2i
.
We can directly apply Theorem 3 to the decoder function if we assume that the decoder function is inﬁnitely
diﬀerentiable. This holds for networks with the Sigmoid activation function for instance Hornik (1991). We can
then swap out diag(S) for the encoder standard deviation σφ(x) and repeatedly applying the Theorem to each
output gθ,i(z), i ∈1, . . . , d of the decoder. For an output i of the decoder and α ∈Nn, we have that:
Varqφ(z|x)(gθ,i) =
X
|α|≥1
(σφ(x))2α
α!
g(α)
θ,i (z)

2
=
X
|α|≥1
(σφ(x))2α
α!

Z
Rn(ω)αFθ,i(ω)Pφ(ω)dω

2
0.2
0.1
0.0
0.1
0.2
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
(
)
Original
= 0.1
= 0.5
= 1.0
Figure 2: FFT of the VAE recon-
structions in Figure 1. For larger σφ,
the spectrum loses high frequencies.
Pφ is the Fourier transform of the Gaussian measure N(µφ(x), σ2
φ(x))
and Fθ,i is the Fourier transform of gθ,i.
As σφ(x) increases, particularly for σφ(x) ≥1, larger α terms in the sum,
and large frequencies to the power α will begin to be disproportionately
penalised in the Var term of the bias variance decomposition of the VAE
likelihood. Thus larger σφ(x) will result in a larger penalisation of the
high-frequency components of the decoder function. We note that this
penalisation is modulated on a per-datapoint basis by σφ(x), but that
VAEs which on average have larger encoder variances over training inputs
will learn lower frequency decoders. Because of the link between the
decoder variance, the Hermite polynomials, and the frequency domain
(see Equation (3.3)), the lower frequency function learned by the decoder,
for each z|x, also corresponds to a function that can be described as a
lower degree polynomial.
To demonstrate these ﬁndings, in Figure 1 we use a ﬁxed encoder variance
σ2
φ, uniform across dimensions, on VAEs, with Sigmoid activation function
in their networks, trained on data from the sinc(5t), t ∈[ −1, 1] function.
As σφ increases, we can qualitatively ascertain that the decoder function
loses higher frequency components and that the optimal polynomial to
describe the decoder function (approximated by polynomial regression)
decreases in its degree. Quantitatively measuring the Fourier transform of the decoder here is simple, we collect
the reconstruction of all inputs, ordering by increasing t. We then take the fast Fourier transform (FFT) of this
aggregate reconstructed function. We show this in Figure 2 where as σφ increases, the decoder learns a lower
frequency Fourier representation.
Though our theoretical results hold for classes of inﬁnitely diﬀerentiable neural networks, we empirically conﬁrm
they still hold for more complex neural network architectures with ReLU activation functions, trained on large
multivariate datasets. In Figure 3 we show that for a ﬁxed encoder variance σ2
φ, as σφ increases reconstructed
CelebA images from a convolutional VAE become more similar and start to lose diversity – the images lose
mid to high level frequencies as measured by a 2D-FFT – suggesting that the decoder learns a lower frequency
function. To support this, we plot the mean 1D-Non-uniform discrete Fourier transform (NUDFT) (Bagchi
and Mitra, 1999) across the d dimensions of the output, which shows that models trained with larger σφ learn
6

0
25
50
0
20
40
60
Fourier transform
10
2
10
4
(a) Inputs
0
25
50
0
20
40
60
Fourier transform
10
2
10
4
(b) σφ = 1
0
25
50
0
20
40
60
Fourier transform
10
2
10
4
(c) σφ = 2
0
50
100
150
200
250
300
350
400
25
50
75
100
125
(w)
Mean NUDFT across d dimensions
original
= 1
= 2
Figure 3: We train convolutional VAEs (see Appendix C for network details), with ReLU activation functions in
their networks, on the CelebA dataset, with a 64 dimension latent space Z. As before we ﬁx the encoder σφ.
[ﬁrst + second rows] We show 4 images and the 2D-FFT of the image in the upper right quadrant for a) the
training images used b) VAE reconstructions of the 4 images when σφ = 1 c) VAE reconstructions for σφ = 2. As
σφ increases mid-high level frequencies are increasingly dampened relative to the original image. [bottom row]
Positive components of the mean 1D-NUDFT of the d dimensions of the output of these models (calculated across
2000 images for each of the d = 12288 dimensions). Shading corresponds to the std. dev. over d dimensions.
functions that on average are of much lower frequency content than smaller σφ models. This demonstrates that
the multi-dimensional output decreases in its high-frequency content over the d output dimensions, as predicted
by our theory. In Figure D.7 of the Appendix we show that this also holds for fully-connected VAEs trained on
multivariate sinusoids.
3.2
Gaussian Spaces and VAE encoders
Here we use the analysis developed in previous section to analyze the Harmonic content of the VAE encoder. In
the typical VAE setup, we don’t usually consider the case of Gaussian noise on inputs and we make no assumptions
as to the underlying measure of the input space; such that we cannot directly apply the previous Gaussian space
7

analysis to the VAE encoder.
The perspective we’ve established, however, of the latent space as a Euclidean space equipped with a general
Gaussian measure, and the perspective of the decoder as belonging to Gaussian space equipped with this same
measure, can inform our analysis. The encoder of the VAE eﬀectively parameterises the latent space measure for
each input x such that the function the encoder is learning is the measure N(µφ(x), σφ(x)). In Theorem 3 we
have already established that the Fourier transform of this measure is given by
P(ω) = det(Σφ(x))G(ωΣφ(x))e−iωµφ(x)Σ−1
φ (x)
where Σφ(x) is a diagonal matrix with the elements of σφ(x) on its diagonal. Much like the decoder, as the
encoder variance decreases, the function parameterised by the encoder increases in amplitude in its high-frequency
components. However, here the frequency content of the encoder function is intrinsic to the Gaussian measure,
whereas the frequency content of the decoder is a result of the relative weighting of the decoder variance in the
VAE likelihood, and as such is enforced by optimisation. Note that e−iωµφ(x)Σ−1
φ (x) only modulates the phase
of the Fourier transform and does not alter the amplitude of its frequency content. As such the mean does not
aﬀect the frequency spectrum of the posterior Gaussian. In Figure D.6 of the Appendix we demonstrate that
for a univariate Gaussian, decreasing the variance increases the magnitude of high-frequency components in the
Fourier domain, whereas altering the mean has no such eﬀect.
This perspective gives us an idea of how to alter the harmonic content of the measure parameterised by the
encoder overall. If instead we want to modulate the frequency content of the individual networks that are used to
parameterise the Gaussian measure we need a diﬀerent perspective. In the next section we show that using the
theory developed previously, adding Gaussian noise to the input data oﬀers a simple and eﬀective method for
modulating the frequency content of the encoder mean.
3.2.1
Noisy inputs
Let us now assume that we add Gaussian noise to the data x such that our input for each point x is a Gaussian
space with a Gaussian measure with mean x, variance σ2:
˜x = x + σ2ν, ν ∼N(0, I).
Under this noisy input, the expectation of the KL between the amortised posterior distribution and the unit
Gaussian prior can be written as:
Eγ(ν) [KL(qφ(z|˜x)∥p(z))] = 1
2 Eγ(ν)
"
log

1
Qn
i=1 σφ,i(˜x)

−n +
n
X
i=1
σφ,i(˜x) +
µφ(˜x)
2
2
#
0.10
0.05
0.00
0.05
0.10
0.0
0.5
1.0
1.5
(
)
= 0.1
= 0.5
= 1.0
Figure 4: FFT of µφ of VAEs trained
on sinc(5t) + N(0, σ2I). For larger
σ the spectrum loses high frequency
content.
We now have a Gaussian space variance Var(µφ) = Eγ(ν)
hµφ(˜x)
2
2
i
penalised at each iteration. As before for the decoder we can directly
apply Theorem 3 to the encoder mean function, by swapping out diag(S)
for the data standard deviation σI and repeatedly applying the Theorem
to each output µi(˜x), i ∈1, . . . , n of the encoder. We can expect two
things to happen as σ increases. The frequency content of the encoder
mean should decrease, and generally the function for each x can be
described using an increasingly lower degree polynomial. Thus we can
add Gaussian noise to a VAE’s inputs to modulate the frequency content
of the encoder mean function. We demonstrate these ﬁndings in Figure 4.
Note that though our theory is not directly applicable to the encoder
variance network, if we ﬁx σφ so as to modulate the variance of the
decoder, then we have full control over the variance of the encoder by
adding noise to the VAE inputs.
8

3.3
Gaussian Spaces and VAE Lipschitzness
We know that by Proposition 4 the variance of a function (that is once diﬀerentiable) provides a relatively tight
lower bound on the Lipschitz constant of a neural network. This lower bound also increases as the variance of the
Gaussian measure decreases.
In previous sections we demonstrated that larger encoder variances induce smaller variance decoder functions,
with lower frequency components. As the encoder variance σ2
φ(x) increases, the variance of the decoder function
Varqφ(z|x) (gθ(z)) decreases, which by Proposition 4 will lead to a smaller lower bound on the Lipschitz constant
of the decoder. Due to the computational costs of estimating the Lipschitz constants of networks, we restrict
our empirical analysis here to fully-connected VAEs and use layer-wise LipSDP (Fazlyab et al., 2019) to obtain
estimates. In Table 1 we conﬁrm that regulating the encoder variance allows for us to impose a soft constraint of
the Lipschitzness of the VAE decoder. We also show that adding Gaussian noise on data, as motivated in the
previous sections by its eﬀect on Var(µφ), gives us ﬁner control on the Lipschitz constant of the encoder.
Table 1: The Lipschitz constants (L) of VAEs’ [left] decoder networks (gθ) when trained with ﬁxed encoder scale (σφ)
and [right] encoder networks (µφ) when trained with σ-scale Gaussian noise injection on inputs and ﬁxed encoder variance
σφ=0.5. We train fully-connected VAEs with the Sigmoid activation in their networks; on sinc(5t) (dim(Z) = 1), on
CelebA (dim(Z) = 64), and CIFAR10 (dim(Z) = 64). Each network has 3 dense layers each with 256 units. As σφ
decreases the Lipschitz constant (L) increases, as predicted by Proposition 4. As σ decreases, the Lipschitz constant (L) of
the encoder mean (µφ) increases. (±) is the std. dev. over 3 random seeds.
Data
L(gθ; σφ)
L(µφ; σ)
σφ = 1.0
σφ = 0.5
σφ = 0.1
σ = 1.0
σ = 0.5
σ = 0.0
sinc
2.2 ± 0.2
5.2 ± 0.3
17.9 ± 3.2
13.9 ± 2.7
24.6 ± 1.7
29.8 ± 2.2
CelebA(104)
7.5 ± 1.1
12.0 ± 0.5
13.7 ± 1.2
1.4 ± 0.1
1.6 ± 0.1
1.8 ± 0.1
CIFAR10(102)
17.9 ± 1.2
19.1 ± 1.2
27.3 ± 0.3
4.7 ± 0.2
5.6 ± 0.6
8.5 ± 0.8
10
20
C
10
8
6
4
2
0
Likelihood Degradation
Varying 
= 0.1
= 0.2
= 0.5
= 1.0
10
20
C
8
6
4
2
0
Likelihood Degradation
Varying 
= 0.1
= 0.2
= 0.5
= 1.0
Figure 5: Both plots show the relative log likelihood degradation resulting from a ‘maximum-damage’ (Camuto
et al., 2020) adversarial attack against the maximum attack norm C. As σ, the variance of isotropic Gaussian noise
added to data, and σφ, the ﬁxed posterior variance, increase, the degradation lessens as C increases, highlighting
that both these parameters improve robustness. Shading corresponds to the variance of the likelihood degradation
over 25 points from CIFAR10.
Lipschitzness and Robustness
Recent work shows that larger encoder variances and smaller encoder and
decoder network Lipschitz constants are core contributors to the robustness of VAEs to adversarial attack (Camuto
et al., 2021c; Barrett et al., 2021). Whereas this theory has viewed the network Lipschitz constants and the
encoder variance as distinct parameters to control to attain robustness; our harmonic analysis shows that they
are in fact intrinsically linked. This means that the encoder variance can serve as a single parameter on which we
9

can act to improve the robustness of VAEs to adversarial attack, in that it also aﬀects the Lipschitz constant of
the decoder. Further, adding noise to the VAE input data gives us similar control on the Lipschitz constant of
the encoder mean µφ. As such, ﬁxing and modulating both the encoder variance and the variance of the noise
on data allows for the imposition of soft Lipschitz constraints on the VAE networks that improve adversarial
robustness.
We consider an adversary trying to distort the input data to maximally disrupt a VAE’s output, as in maximum
damage attacks (Camuto et al., 2020; Barrett et al., 2021). The adversary maximises, with respect to a perturbation
on data δx, constrained in norm by a constant C, the distance between the VAE reconstruction and data x.
δ∗
x = arg max
∥δx∥2≤C
 ||gθ(µφ(x + δx) + ησφ(x + δx)) −gθ(µφ(x))||2

.
(3.6)
Given an embedding z∗formed from the mean encoding of x + δx, we measure the likelihood of the original point
x and quantify the degradation in model performance as the relative log likelihood degradation (| log pθ(x|z∗) −
log pθ(x|z)|/ log pθ(x|z)), where z is the embedding of x.
Figure 5 shows that as the variance of noise on data (σ) and the ﬁxed encoder variance (σφ) increase, this
degradation lessens for the VAEs trained on the CIFAR10 dataset in Table 1, indicating less damaging attacks.
In tandem, Table 1 also shows that the increase of both these parameters decreases the Lipschitz constants of the
encoder and decoder network for these VAEs. Thus we conﬁrm the analysis above (and the recent theory on the
adversarial robustness of VAEs (Camuto et al., 2021c; Barrett et al., 2021)), empirically demonstrating that both
σφ and σ reduce the Lipschitz constant of both the encoder and decoder networks and simultaneously improve
the robustness of VAEs trained on CIFAR10.
3.4
Potential Limitations
Our results on Lipschitzness would be stronger if Proposition 4 oﬀered an upper bound on the Lipschitz constant,
in that we could certify a maximum Lipschitz constant. Also, the encoder variance does not directly aﬀect the
frequency content of the encoder, only secondarily. Noise injections on data are needed to provide full control over
the frequency content and Lipschitzness of the VAE networks, whereas a single parameter to control would have
been a stronger result. We also recognise that the results in Figure 5 would have been stronger if the variance of
the likelihood degradation results was such that the shading of the curves did not overlap for the diﬀerent values
of σ and σφ. Our study does not have a direct societal impact as it is largely theoretical.
4. Conclusion
Using the lens of Gaussian spaces, we demonstrated that the variance of the latent encodings features as an
important parameter for regularising VAE decoders, controlling their Lipschitz constants, and improving their
adversarial robustness. By applying the same framework to the VAE encoder we show that simply adding
Gaussian noise to VAE inputs oﬀers the same control over the VAE encoder. This work lays the foundation for
analysing the eﬀect of Gaussian priors on VAEs and oﬀers a novel framework from which to understand the
functions learned by a VAE’s networks. It also paves the way for the study of the eﬀect of non-Gaussian priors
on VAE networks.
5. Acknowledgements
We thank Professor Svante Janson, Professor Wilfreda Urbina-Romero, and Professor Tom Alberts for their
invaluable guidance on Gaussian Spaces. This research was directly funded by the Alan Turing Institute under
Engineering and Physical Sciences Research Council (EPSRC) grant EP/N510129/1. Alexander Camuto was
supported by an EPSRC Studentship.
10

6. References
Jonas Adler and Sebastian Lunz. Banach Wasserstein GAN. In NeurIPS, 2018.
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck.
ICLR, 2017.
Sonali Bagchi and Sanjit K. Mitra. The Nonuniform Discrete Fourier Transform and Its Applications in Signal
Processing. Kluwer Academic Publishers, USA, 1999.
Ben Barrett, Alexander Camuto, Matthew Willetts, and Tom Rainforth. Certiﬁably robust variational autoen-
coders. arXiv, 2021.
Chris M. Bishop. Training with Noise is Equivalent to Tikhonov Regularization. Neural Computation, 1995.
Martin Burger and Andreas Neubauer. Analysis of Tikhonov regularization for function approximation by neural
networks. Neural Networks, 2003.
Alexander Camuto, Matthew Willetts, Umut S¸im¸sekli, Stephen Roberts, and Chris Holmes. Explicit Regularisation
in Gaussian Noise Injections. In NeurIPS, 2020.
Alexander Camuto, Xiaoyu Wang, Lingjiong Zhu, Chris Holmes, Mert G¨urb¨uzbalaban, and Umut S¸im¸sekli.
Asymmetric heavy tails and implicit bias in gaussian noise injections, 2021a.
Alexander Camuto, Matthew Willetts, Brooks Paige, Chris Holmes, and Stephen Roberts. Learning Bijective
Feature Maps for Linear ICA. AISTATS, 2021b.
Alexander Camuto, Matthew Willetts, Stephen Roberts, Chris Holmes, and Tom Rainforth. Towards a theoretical
understanding of the robustness of variational autoencoders. AISTATS, 2021c.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational
autoencoders. NeurIPS, 2018.
Jeremy Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial robustness via randomized smoothing.
ICML, 2019.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In ICLR, 2017.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas. Eﬃcient and
accurate estimation of lipschitz constants for deep neural networks, 2019.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by enforcing
lipschitz continuity. arXiv, 2018.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classiﬁer against adversarial
manipulation. In Advances in Neural Information Processing Systems, pages 2266–2276, 2017.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving Flow-Based
Generative Models with Variational Dequantization and Architecture Design. ICML, 2019.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 1991.
Svante Janson. Gaussian Hilbert Spaces. Cambridge Tracts in Mathematics. Cambridge University Press, 1997.
Diederik P Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimisation. In ICLR, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Diederik P. Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling. Semi-supervised learning
with deep generative models. CoRR, 2014.
Abhishek Kumar and Ben Poole. On Implicit Regularization in β-VAEs. ICML, 2020.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approximate
Inference in Deep Generative Models. In ICML, 2014.
11

Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving the PixelCnn with
discretized logistic mixture likelihood and other modiﬁcations. In ICLR, 2017.
Rui Shu, Hung H. Bui, Shengjia Zhao, Mykel J. Kochenderfer, and Stefano Ermon.
Amortized inference
regularization. In NeurIPS, 2018.
D´avid Terj´ek. Adversarial lipschitz regularization, 2020.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certiﬁcation of perturba-
tion invariance for deep neural networks. In NeuRIPS, 2018.
Andrew R. Webb. Functional Approximation by Feed-Forward Networks: A Least-Squares Approach to General-
ization. IEEE Transactions on Neural Networks, 1994.
Matthew Willetts, Alexander Camuto, Tom Rainforth, Stephen Roberts, and Chris Holmes. Improving VAEs’
Robustness to Adversarial Attack. ICLR, 2021.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaudhuri. Adversarial
robustness through local lipschitzness. arXiv, 2020.
Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier perspective on
model robustness in computer vision. In NeurIPS, 2019.
Mingtian Zhang, Peter Hayes, Thomas Bird, Raza Habib, and David Barber. Spread Divergence. ICML, 2020.
12

Appendix for Variational Autoencoders: a Harmonic Perspective
Appendix A. Technical Proofs
Before we begin the proofs we give an alternative deﬁnition of the Hermite polynomials which uses the kth
divergence operator (δk, (Janson, 1997)). This deﬁnition greatly simpliﬁes some of our later proofs.
Hk = δk1(x),
(A.1)
where 1 is shorthand for the function that is identically 1. The divergence operator is deﬁned using Dom(δk),
which is the subset of functions g ∈L2(R, γ) for which there exists a c > 0 such that for all functions for which
the derivative up to degree k obeys f (k) ∈L2(R, γ):

Z
R
f (k)(x)g(x)dγ(x)
 ≤c
s
Z
R
f (k)(x)dγ(x)

1(x) for example, as deﬁned in Equation (A.1) is in Dom(δk). The kth divergence can then be deﬁned as follows.
If g ∈Dom(δk) then δkg is the unique element of L2(R, γ) such that for all functions for which the derivative up
to degree α obeys f (k) ∈L2(R, γ) we have:
Z
R
f(x)δkg(x)dγ(x) =
Z
R
f (k)(x)dγ(x).
(A.2)
For the general Gaussian space, this becomes by substitution of variables:
Hk = δk1(y),
y = (x −µ)/σ
(A.3)
In the multivariate case, for the general Gaussian space with diagonal standard deviation, we have that Hα =
δα1(y), y = (x −µ)S−1, where δ now is the vector-valued divergence operator indexed by the multi-index
α.
A.1
Proof of Proposition 1
Proof. Due to the scaling and centering relation for Gaussians a simple change of variables y = (x −µ)/σ in
Equation (2.2) gives us the ﬁrst part of the proof. Thus
Eγ(y) [Hk(y)Hm(y)] = k!⊮{m = k},
and thus
 1
k!Hk : k ≥0
	
is orthonormal in L2(R, N(µ, σ2)) because γ(y) = N(µ, σ2).
We know that for k ≥0 the polynomial Hk has degree k, hence it suﬃces to demonstrate that the mononomials
of degree k, {yk : k ∈N, y = (x −µ)/σ} are dense in L2(R, γ(y)) to show that
 1
α!Hk : k ≥0
	
is a basis for
L2(R, γ(y)).
The Elementary Hahn-Banach theorem shows that if g ∈L2(R, γ) is such that
R
R g(y)ykdγ(y) = 0, ∀k ∈N+ then
it suﬃces to show that g = 0 almost everywhere to demonstrate that the mononomials yk are a dense subspace of
L2(R, γ(y)). This proof generally follows that used for the standard Gaussian measure. Assume we have a g that
satisﬁes
R
R g(y)ykdγ(y) = 0. By the series expansion of the exponential function we know that for all t ∈R
Z
R
g(y)eitydγ(y) = lim
m→∞
m
X
k=0
(it)k
k!
Z
R
g(y)ykdγ(y) = 0
Thus we have that
R
R g(y)eitydγ(y) = 0, ∀t. Because eity is injective we have that g(y) = 0, ∀y ∈R.
13

A.2
Proof of Proposition 2
Proof. We begin with the univariate case, from which we build up to the multivariate case. Here we use the
divergence operator deﬁnition of the Hermite polynomials, seen in Equation (A.1). In our case by substitution of
variables Hk(y) = δk1(y) and we can express x as x = yσ + µ, thus:
Z
R
f(yσ + µ)Hk(y)dγ(y) =
Z
R
f(yσ + µ)δk1(y)dγ(y) =
Z
R
σkf k(yσ + µ)dγ(y)
As mentioned in the main paper, in the multivariate case, the divergence operator is an almost exact analogue to
that of the univariate case. Due to the tensorisation of bases to form H the indices α simply become a multi-index
and we recover that ˆf(α) = Eγ(y)

Sαf (α)(x)

.
A.3
Proof of Theorem 3
Proof. By Proposition 2 we know that we can express a function f ∈L2(Rn, N(µ, S2)) that is inﬁnitely
diﬀerentiable as:
f =
X
α∈Nn
1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα,
γ(y) = N(x|µ, S2)
Now note that the expectation of f is simply the ﬁrst Hermite coeﬃcient with degree 0:
Eγ(y) [f] =
X
|α|=0
1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα,
γ(y) = N(x|µ, S2)
Now taking the variance of f:
Var(f) = Eγ(y)
h f −Eγ(y) [f]
2i
= Eγ(y)



X
α∈Nn
1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα −
X
|α|=0
1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα


2

= Eγ(y)



X
|α|≥1
1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα


2

Recall that the bases Hα are orthogonal such that
Eγ(y) [Hα(y)Hm(y)] = α!{m = α}
As such we have that:
Var(f) = Eγ(y)



X
|α|≥1
1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα


2

= Eγ(y)

X
|α|≥1
 1
α!(diag(S))α Eγ(y)
h
f (α)(x)
i
Hα
2


=
X
|α|≥1
(diag(S))2α
α!
Eγ(y)
h
f (α)(x)
i
2
,
This completes the ﬁrst part of the proof.
14

For the second part we focus on terms of the form
Eγ(y)
h
f (α)(x)
i
=
Z
Rn f (α)(yS + µ)dγ(y) =
Z
Rn f (α)(yS + µ)γ(y)dy
Both f and γ are L2 integrable with respect to the Lebesgue measure. As such we can directly apply Plancherel’s
theorem to demonstrate that
Eγ(y)
h
f (α)(x)
i
=
Z
Rn f (α)(yS + µ)γ(y)dy
=
Z
Rn f (α)(x)γ((x −µ)S−1)dx
=
Z
Rn(iω)αF(ω)det(S)G(ωS)e−iωµS−1dω
=
Z
Rn(iω)αF(ω)P(ω)dω
where P is the Fourier transform of the Gaussian measure N(µ, σ2) given by P(ω) = det(S)G(ω)e−iωµS−1 (where
G is the Fourier transform of the standard Gaussian measure γ) and F is the Fourier transform of f. This stems
from the fact that the Fourier transform of the αth derivative of a function is simply (iω)αF(ω)
From this we obtain that
Var(f) =
X
|α|≥1
(diag(S))2α
α!

Z
Rn(iω)αF(ω)P(ω)dω

2
=
X
|α|≥1
(diag(S))2α
α!

Z
Rn(ω)αF(ω)P(ω)dω

2
, α ∈Nn
A.4
Proof of Proposition 4
Proof. We begin with a proof for a univariate Gaussian space. We once again use the deﬁnition of the Hermite
polynomials in terms of the divergence operator. Note that Hk = δHk−1. The variance of a function f ∈
L2(R, N(µ, σ2)) can be expressed as the sum of Hermite coeﬃcients squared:
Var(f) =
∞
X
α=1
1
k!| ˆf(α)|2 =
∞
X
α=1
1
k!
Eγ(y) [f(σy + µ)Hk(y)]
2
(A.4)
Using the deﬁnition of the divergence operator we have that:
Var(f) =
∞
X
k=1
1
k!
Eγ(y)
h
σf (1)(σy + µ)Hα−1(y)
i
2
(A.5)
≤
∞
X
k=1
1
(k −1)!
Eγ(y)
h
σf (1)(σy + µ)Hk−1(y)
i
2
(A.6)
=
∞
X
k=0
σ2
k!
Eγ(y)
h
f (1)(σy + µ)Hk(y)
i
2
(A.7)
As f (1) is a member of the Gaussian space, the last equation is simply the dot product of f (1) with itself, weighted
by σ2. Thus:
Var(f) = σ2⟨f (1), f (1)⟩.
(A.8)
15

By deﬁnition of the Lipschitz constant:
L ≤
sup
x,x′∈R⋉,x̸=x′
|f(x) −f(x′)|
∥x −x′∥
We then have
Var(f) ≤σ2L2
(A.9)
For the multivariate case, we work in reverse. We know that L2 ≥∥Df∥2 where Df is the Jacbobian of the
function with respect to the input such that
Eγ(y)

∥Df∥2
=
X
|α|=1
< f (α), f (α) >
As by assumption each f (α), |α| = 1 is a member of the Gaussian space we have that
∥Df∥2 =
X
|α|=1
X
β∈Nn
1
β!
Eγ(y)
h
f (α)(Sy + µ)Hβ(y)
i
2
(A.10)
=
X
|α|=1
X
β∈Nn
1
β!
Eγ(y)

S−1f(Sy + µ)δαHβ(y)
2
(A.11)
=
X
|α|=1
X
β∈Nn
1
β!
Eγ(y)

S−1f(Sy + µ)Hβ+α(y)
2
(A.12)
≥
X
|β|≥1
1
β!
Eγ(y)

S−1f(Sy + µ)Hβ(y)
2 , β ∈Nn
(A.13)
This last term is simply Var(f)/∥S∥2 (see the proof of Theorem 3 for why this is).
Thus we have that
L2∥S∥2 ≥Var(f). This concludes the proof.
Appendix B. Multivariate Hermite Coeﬃcients for a Gaussian measure with full
Covariance
Let our Gaussian space be equipped with Gaussian measure N(µ, C), where C is a full covariance matrix. By
deﬁnition C is symmetric and positive deﬁnite, thus there exists an orthogonal matrix O such that:
OCOT = D,
where D is some diagonal matrix. Let x ∼N(µ, C), we know that:
y = (x −µ)OT D−1
2 ∼N(0, I)
By substitution of variables for the Hermite polynomials in the case of a diagonal covariance, we have that the
Hermite polynomials for the measure N(µ, C) are given by:
Hα((x −µ)OT D−1
2 ) = Hα(y) =
Y
i
Hαi(yi),
Appendix C. Implementation Details
The architecture of VAEs with dense layers is presented in text. For convolutional networks the encoder layers
had the following number of ﬁlters in order: {64, 64, 128, 128, 512}.
16

The mean and variance of the amortised posteriors are the output of dense layers acting on the output of the
purely convolutional network, where the number of neurons in these layers is equal to the dimensionality of the
latent space Z.
Similarly, for the decoders (p(x|z)) of all our models we also used purely convolutional networks with 6 deconvo-
lutional layers. The layers had the following number of ﬁlters in order: {512, 128, 128, 64, 64, 3}. The mean of the
likelihood p(x|·) was directly encoded by the ﬁnal de-convolutional layer. The variance of the decoder was ﬁxed
to 0.1.
To train models we used ADAM (Kingma and Lei Ba, 2015) with default parameters, a learning rate of 0.001,
and a batch size of 1024. All data was preprocessed to fall on the interval -1 to 1.
All models were trained with an Azure Cloud Standard NC6 machine with a single NVIDIA Tesla K80.
Appendix D. Additional Results
D.1
Fourier Transform of Gaussian Measure
1.0
0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
2.5
(
)
= 1
= 0.5
= 0.2
1.0
0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
2.5
(
)
= 0
= 1
Figure D.6: [left] Fourier transform of the Gaussian measure N(0, σ2) for varying σ. [right] Fourier transform of
the Gaussian measure N(µ, 1) for varying µ. Clearly as σ decreases the spectrum gains high frequencies, whereas
altering µ has no eﬀect.
D.2
Fourier Spectrum of Multivariate VAEs
17

0.4
0.2
0.0
0.2
0.4
10
1
10
2
(
)
= 0.1
r=0.5
r=1.0
r=2.0
r=3.0
0.4
0.2
0.0
0.2
0.4
10
1
10
2
(
)
= 1.0
r=0.5
r=1.0
r=2.0
r=3.0
0.4
0.2
0.0
0.2
0.4
10
1
10
2
(
)
= 1.5
r=0.5
r=1.0
r=2.0
r=3.0
Figure D.7: We train VAEs with 3-dense layers (each with 256 units), with ReLU activation functions in their
networks, on multivariate data consisting of 5 sinusoids y = sin(2πrt), r ∈{0.5, 1.0, 2.0, 3.0, 5.0}, t ∈[ −1, 1],
with a 1 dimension latent space Z. As before we ﬁx the encoder σφ. [top row] We show plots of the regressed
sinusoids for diﬀerent values of σφ. [bottom row] Below we show an FFT for each regressed sinusoid. As σφ
increases we clearly lose some of the mid to high level frequencies.
18

