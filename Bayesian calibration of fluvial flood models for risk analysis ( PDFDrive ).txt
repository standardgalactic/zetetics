Bayesian calibration of ﬂuvial
ﬂood models for risk analysis
Lucy J. Manning
Thesis submitted for the degree of Doctor of Philosophy
School of Civil Engineering and Geosciences
Newcastle University
January 2011

Abstract
Flood risk analysis is now fundamental to ﬂood management decision making. It relies
on the use of computer models to estimate ﬂood depths for given hydrological conditions.
The correct calculation of risks associated with diﬀerent management options requires that
the uncertainty in the computer model output is carefully estimated. There are several
sources of uncertainty in ﬂood models, including structural uncertainties in the model
representation of reality, uncertainty in model parameters, and observation errors. We
refer to the ﬁrst of these as “model inadequacy”. The work described in this thesis con-
cerns the calibration of computer models to describe ﬂuvial ﬂooding, taking into account
model inadequacy and paying particular attention to the requirements of risk analysis
calculations.
A methodology which has had some success in other application areas is Bayesian model
calibration, using Gaussian process representation both for the error arising from model
inadequacy, and to emulate the computer model output. The eﬀectiveness of this method-
ology is demonstrated for steady state ﬂood models, both of a series of laboratory exper-
iments, and of a historical ﬂood using a satellite image of ﬂood outline for calibration.
Extension of the methodology to calibration of dynamic models using gauged data is not
straightforward, but is achieved for ﬂood models by means of an emulator, which replaces
the computationally expensive hydrodynamic model with a time-dependent transfer func-
tion. This permits calibrated prediction of ﬂoods using historical gauged data, both in the
existing channel and after modelling potential modiﬁcations to the channel. It is shown
that calibration without inclusion of a model inadequacy function cannot match measured
data. Finally, application of the methodology is demonstrated in the context of a calcula-
tion of probability of inundation in the channel, both with and without modiﬁcation.

Acknowledgements
Few endeavours can be truly described as solitary, and this has been no exception. I am
grateful for help from the following:
Jim Hall shared his interest with me in the area of Bayesian model calibration, helped me
to shape this project, and provided endless patient encouragement.
Dave Walshaw provided encouragement and practical suggestions, particularly during the
ﬁrst half of the project.
Peter Challenor through his project Probability, Uncertainty, Climate and Modelling
hosted a workshop and a six-week research playground in Durham; attendance at these
helped me to better understand the statistical context of the project I was undertaking
and the use of Gaussian process emulators for computer codes.
Leanna House helped me to get my ﬁrst Markov chain to converge.
Nigel Wright helped me with modelling the data from the Flood Channel Facility, described
in Chapter 4.
Keith Beven provided the data and hydraulic model for the Severn case study used in
Chapters 6 and 7.
Peter Young and his research group spent time to discuss with me the use of their software
package CAPTAIN to emulate the hydraulic model of the Severn, used in Chapter 6.

Contents
1
Introduction
1
1.1
Risk analysis for decision making in ﬂood defence planning
. . . . . . . . .
2
1.2
Flood model calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Bayesian statistics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.4
Thesis structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2
Hydrological background
9
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2
Rainfall-runoﬀmodels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.3
Hydraulic models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.3.1
Description of mechanisms in ﬂoodplain ﬂow
. . . . . . . . . . . . .
12
2.3.2
Equations representing ﬂuvial ﬂow . . . . . . . . . . . . . . . . . . .
12
2.3.2.1
LISFLOOD-FP
. . . . . . . . . . . . . . . . . . . . . . . .
14
2.3.2.2
Hec-Ras . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.3.3
Data requirements of hydraulic models . . . . . . . . . . . . . . . . .
15
2.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.5
Characterisation of uncertainty sources for ﬂood models . . . . . . . . . . .
18
2.5.1
Parametric variability . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.5.2
Structural uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.5.3
Input errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.6
Requirements of current research . . . . . . . . . . . . . . . . . . . . . . . .
22
3
Bayesian analysis of computer code output
23
3.1
The use of Gaussian processes for interpolation and emulation . . . . . . . .
24
3.2
The use of Gaussian processes for calibration
. . . . . . . . . . . . . . . . .
26
3.2.1
Model speciﬁcation when the computer model is expensive to evaluate 27
3.2.2
Integrating out the linear regression
. . . . . . . . . . . . . . . . . .
29
3.2.3
Speciﬁcation of priors
. . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.2.4
Solution of equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.2.5
Calibrated prediction
. . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.3
Markov chain Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.3.1
Metropolis Hastings algorithm
. . . . . . . . . . . . . . . . . . . . .
32
3.3.2
MCMC diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
i

3.3.3
Reﬁnements of the Metropolis-Hastings algorithm
. . . . . . . . . .
35
3.4
Application: simple algebraic example . . . . . . . . . . . . . . . . . . . . .
37
3.4.1
Identiﬁability of the solution with sparse data . . . . . . . . . . . . .
39
3.4.2
Use of an emulator to represent the computer program . . . . . . . .
42
3.4.3
Experimental design . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.4.4
Eﬀect of the choice of regression basis on solution behaviour . . . . .
44
3.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.6
Appendix: Use of BACCO to estimate a Gaussian process emulator
. . . .
46
4
Calibration of steady state laboratory experiments
48
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
4.2
Experimental data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
4.3
Hydraulic model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
4.4
Stage discharge relationship with varying ﬂoodplain width . . . . . . . . . .
50
4.4.1
Variation of ﬂow with depth . . . . . . . . . . . . . . . . . . . . . . .
51
4.4.2
Variation of ﬂow with depth and ﬂoodplain half-width . . . . . . . .
53
4.4.3
Variation of ﬂow with depth, ﬂoodplain half-width, and channel side
slope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.4.4
Comment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.5
Taking into account experimental errors . . . . . . . . . . . . . . . . . . . .
57
4.5.1
Observation errors in the experimental programme . . . . . . . . . .
57
4.5.2
Including input errors in the calibration . . . . . . . . . . . . . . . .
58
4.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
5
Calibration of a steady state ﬂood model
60
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
5.2
Flood model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
5.3
Calibration data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
5.4
Calibration
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
5.4.1
Emulator construction . . . . . . . . . . . . . . . . . . . . . . . . . .
63
5.4.2
Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
5.5
Comparison of results with previous work . . . . . . . . . . . . . . . . . . .
70
5.6
Alternative emulator formulations for multivariate model output
. . . . . .
71
5.7
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
6
Calibration of a dynamic ﬂood model
74
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
6.2
Calibration of dynamic models
. . . . . . . . . . . . . . . . . . . . . . . . .
75
6.3
Extensions of the Kennedy and O’Hagan methodology to time-varying prob-
lems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
6.4
Flood model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
6.5
Calibration
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
6.5.1
Emulator construction . . . . . . . . . . . . . . . . . . . . . . . . . .
79
ii

6.5.1.1
Gaussian process emulator, with calibration in the time
domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
6.5.1.2
Gaussian process emulator for the spline knot values, with
calibration in the transformed domain . . . . . . . . . . . .
85
6.5.1.3
Spline emulator, with calibration in the transformed domain 89
6.5.2
Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
6.5.2.1
Calibration with Gaussian process emulator . . . . . . . . .
91
6.5.2.2
Spline emulator
. . . . . . . . . . . . . . . . . . . . . . . .
93
6.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.6.1
Stability of the emulator formulation to spline knot position and to
lag value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
6.6.2
Comparison with Bayesian calibration, without model inadequacy
.
97
6.7
Channel modiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
6.8
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7
Use of calibrated prediction in calculating probability of inundation
103
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.2
Flood frequency curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
7.3
Synthetic upstream data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7.3.1
Selection of appropriate window length for ﬂood peak
. . . . . . . . 106
7.3.2
Parameterisation of peak shape . . . . . . . . . . . . . . . . . . . . . 109
7.3.3
Joint distributions for peak characteristic parameters . . . . . . . . . 110
7.4
Probability of inundation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.5
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
8
Conclusions and recommendations for further work
120
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
8.1.1
Analytical model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
8.1.2
Steady-state ﬂood model
. . . . . . . . . . . . . . . . . . . . . . . . 121
8.1.3
Dynamic ﬂood model
. . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.1.4
Risk calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
8.1.5
General comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
8.2
Recommendations for further work . . . . . . . . . . . . . . . . . . . . . . . 124
References
126
iii

List of Figures
1.1
Data dependency model (after Huard and Mailhot, 2006)
. . . . . . . . . .
7
3.1
Illustration of a Gaussian process, with given values of ω and σ2, and known
mean function, conditional on given data points.
. . . . . . . . . . . . . . .
25
3.2
Estimated Gaussian approximation to given data points . . . . . . . . . . .
26
3.3
Example chains to show MCMC problems . . . . . . . . . . . . . . . . . . .
35
3.4
Example correlations corresponding to the chains in Figure 3.3: Upper
right, pairwise correlation; Diagonal, individual variable density; Lower left,
2-dimensional density plots
. . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.5
Reference Gaussian processes for use in determining prior distributions for
“roughness” values: three realisations of each. . . . . . . . . . . . . . . . . .
37
3.6
Chains for toy example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.7
Correlations corresponding to the chains in Figure 3.6: Upper right, pair-
wise correlation; Diagonal, individual variable density; Lower left, 2-dimensional
density plots
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.8
Prior and posterior distributions for parameter θ . . . . . . . . . . . . . . .
40
3.9
Calibrated prediction of algebraic example, for a) 10 data points and b) 20
data points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.10 Greyscale plot of the likelihood over a 3-dimensional grid of hyperparameter
values
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.11 Testing the code emulator: a) contour map of the emulator mean response
surface; solid points are where the code has been run, circles are points
where the emulator accuracy has been tested, b) 95% prediction interval
for the emulator at locations labelled in a (central points are true code
output values)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.12 Eﬀect of diﬀerent emulator and model inadequacy regression bases on the
calibrated prediction in extrapolation: a)calibration only, model inadequacy
basis (x); b) emulator basis (1, x, θ), model inadequacy basis (x); c) emula-
tor basis (1), model inadequacy basis (x); d) emulator basis (1, x, θ), model
inadequacy basis (x3)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4.1
View of the Flood Channel Facility experimental setup.
. . . . . . . . . . .
49
4.2
Geometry of the Flood Channel Facility experimental setup for half channel
width. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
iv

4.3
Comparison of measured data with model predictions: a) least-squares best
ﬁt of Manning’s equation for diﬀerent ﬂoodplain widths b) error in measured
data, by comparison with least squares ﬁt to inbank data
. . . . . . . . . .
51
4.4
Bayesian calibrated prediction using data for a single ﬂoodplain width us-
ing a model inadequacy: a) calibrated prediction; b) diﬀerence between
calibrated prediction and Manning’s equation, ﬁtted to the in-bank data.
.
53
4.5
Calibrated prediction: calibration undertaken using data from two ﬂood-
plain widths, and comparing the calibrated prediction with the third. Val-
idation dataset a) bw=3.15m b) bw=5m. . . . . . . . . . . . . . . . . . . . .
55
4.6
FCF straight channel experimental design . . . . . . . . . . . . . . . . . . .
55
4.7
Calibrated prediction for ﬂoodplain half-width bw=3m., comparing the ef-
fect of in-bank covariance structures in Equations: ai) and aii) (4.3a) and
b) (4.3b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
5.1
Digital elevation map of the study region, with river course superimposed;
direction of ﬂow, left to right
. . . . . . . . . . . . . . . . . . . . . . . . . .
61
5.2
Flood extent, obtained from processed SAR data . . . . . . . . . . . . . . .
62
5.3
Modelled ﬂood extent for diﬀerent values of Manning’s n . . . . . . . . . . .
63
5.4
LISFLOOD calibration and emulation data a) Flood water elevation in-
ferred from the SAR ﬂood outline image b) Points where ﬂood elevation
values were extracted from SAR observations c) Typical LISFLOOD wa-
ter surface proﬁles, at diﬀerent values of Manning’s n d) Points used to
construct the LISFLOOD emulator . . . . . . . . . . . . . . . . . . . . . . .
64
5.5
Absolute error in mean emulator prediction, and standard deviation of the
predicted emulator uncertainty for three diﬀerent values of Manning’s n:
close to, in between, and away from the training runs. Values of test runs
and training runs are illustrated . . . . . . . . . . . . . . . . . . . . . . . . .
65
5.6
Deviation of model output from linear slope . . . . . . . . . . . . . . . . . .
65
5.7
Emulator error distribution; with superimposed t distribution with 5 de-
grees of freedom. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
5.8
Prediction of water levels along the measured shorelines, with diﬀerent prior
assumptions about measurement error, and numbers of data points: a)
mean error 35cm, precise distribution, b) mean error 25cm, precise distri-
bution, c) mean error 10cm, precise distribution, d) mean error 25cm, vague
distribution, i) 26 data points, ii) 61 data points
. . . . . . . . . . . . . . .
69
5.9
Probability of inundation
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5.10 Probability of inundation obtained by Aronica et al. (2002) . . . . . . . . .
71
6.1
Digital elevation map of the Severn catchment in the area around Shrews-
bury, with the river course superimposed, and gauging stations marked in
red. Modelled ﬂood relief channel in Shrewsbury marked with dotted line. .
79
v

6.2
a) Gauged upstream stage at Montford, 15th Jan - 7th Mar 2002, b) Output
of Hec-Ras model, with input of gauged upstream stage, for diﬀerent values
of Manning’s n, compared with gauged downstream stage at Welsh Bridge
in Shrewsbury, showing that no value of the parameter will allow the model
output to correspond with the data . . . . . . . . . . . . . . . . . . . . . . .
80
6.3
Heuristic motivation for emulator strategy: a) input-output plot, when out-
put is determined by Equation (6.2); b) input-output plot, when output is
determined by Equation (6.3) . . . . . . . . . . . . . . . . . . . . . . . . . .
81
6.4
Input-output plots: a) downstream v. upstream gauged stage (relative to
local data- 47mAOD and 52mAOD respectively); b) downstream stage v.
transformed upstream gauged stage . . . . . . . . . . . . . . . . . . . . . . .
81
6.5
Schematic of identiﬁcation procedure for nonlinear transfer function
. . . .
82
6.6
Components of nonlinear transfer function for computer output with dif-
ferent values of Manning’s n and for data: a) nonlinear function values,
b(·)
(1 −a), and b) autoregressive coeﬃcient, a. . . . . . . . . . . . . . . . . . .
83
6.7
Comparison with hydraulic model output of time series recovered from non-
linear functions and autoregressive coeﬃcients applied to the upstream hy-
drographs, for a) time period (15th January to 7th March 2002) used for
estimation and b) validation time period (25th October - 15th December,
2002). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
6.8
Predicted a) nonlinear functions and b) autoregressive coeﬃcients for values
of Manning’s n between those used to construct the emulator, compared
with values obtained directly from the hydraulic model output
. . . . . . .
87
6.9
Predicted time series for values of Manning’s n away from those used to
construct the emulator, compared with hydraulic model output for a) orig-
inal time period (15th January to 7th March 2002) and b) validation time
period (25th October - 15th December, 2002). . . . . . . . . . . . . . . . . .
88
6.10 95% prediction interval for Manning’s n = 0.045, compared with the slope of
the predicted stage for a) original time period (15th January to 7th March
2002) and b) validation time period (25th October - 15th December, 2002).
89
6.11 Projected a) nonlinear functions and b) autoregressive coeﬃcients, for val-
ues of Manning’s n between those used to construct the spline, compared
with values obtained directly from the hydraulic model output
. . . . . . .
90
6.12 Projected time series for values of Manning’s n away from those used to
construct the spline, compared with hydraulic model output for a) original
time period (15th January to 7th March 2002) and b) validation time period
(25th October - 15th December, 2002). . . . . . . . . . . . . . . . . . . . . .
90
6.13 Calibrated prediction, using ﬁrst calibration method, for a) nonlinear func-
tion, b) autoregressive coeﬃcient a and c) predictive distribution for Man-
ning’s n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
vi

6.14 Calibrated prediction for stage at Welsh Bridge in Shrewsbury, using ﬁrst
calibration method, compared with model predictions and observed data:
a) time period used in calibration, b) validation time period . . . . . . . . .
93
6.15 Calibrated prediction, using second calibration method, for a) nonlinear
function, b) autoregressive coeﬃcient a and c) predictive distribution for
Manning’s n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.16 Calibrated prediction for stage at Welsh Bridge in Shrewsbury, using second
calibration method, compared with model predictions and observed data:
a) time period used in calibration, b) validation time period . . . . . . . . .
96
6.17 Posterior distribution of parameter Manning’s n, found with simple cali-
bration method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.18 Calibrated prediction for stage at Welsh Bridge in Shrewsbury, using simple
calibration method, compared with model predictions and observed data:
a) time period used in calibration, b)validation time period . . . . . . . . .
98
6.19 Calibrated prediction for a) original river channel, b) channel modiﬁed with
triangular relief channel and c) channel modiﬁed with rectangular relief
channel: i) nonlinear function and ii) autoregressive coeﬃcient
. . . . . . . 100
6.20 Calibrated prediction for a) original river channel, b) channel modiﬁed with
triangular relief channel and c) channel modiﬁed with rectangular relief
channel
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7.1
Variation of ﬂood height with return period at Montford, as predicted by
ﬁtted Generalised Extreme Value model . . . . . . . . . . . . . . . . . . . . 106
7.2
Flood height at Montford, at annual maximum peaks (scaled by maximum
height), for a window of 10 days before and after the peak . . . . . . . . . . 107
7.3
Scaled data for annual maximum peaks in 1960 and 1995, comapred with
synthetic waveforms used for sensitivity analysis
. . . . . . . . . . . . . . . 108
7.4
Illustration of the truncation of the ﬂood peak waveform used in the sensi-
tivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.5
Scaled ﬂood height at Montford, at annual maximum peaks, for a window of
36 hours before and 24 hours after the peak. Hydrograph model superimposed.110
7.6
Correlation of transformed variables describing the ﬂood peak shape at
Montford. Darker shades correspond to higher peaks. . . . . . . . . . . . . . 112
7.7
Choice of covariance matrix for correlated variables.
. . . . . . . . . . . . . 112
7.8
Correlation of simulated transformed variables describing the ﬂood peak
shape at Montford. Darker shades correspond to higher peaks.
. . . . . . . 113
7.9
Probability of issuing a ﬂood warning at Welsh Bridge in Shrewsbury, for
unmodiﬁed and modiﬁed channels. . . . . . . . . . . . . . . . . . . . . . . . 115
vii

List of Tables
3.1
Prior ranges for MCMC example . . . . . . . . . . . . . . . . . . . . . . . .
34
3.2
Prior and posterior distributions for calibration . . . . . . . . . . . . . . . .
38
3.3
Parameters aﬀecting the posterior credible interval . . . . . . . . . . . . . .
41
4.1
Prior and posterior distributions for calibration with respect to depth
. . .
52
4.2
Prior and posterior distributions for calibration with respect to depth and
ﬂoodplain half-width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4.3
Prior and posterior distributions for calibration with respect to depth, ﬂood-
plain half-width and channel side slope tangent . . . . . . . . . . . . . . . .
56
5.1
Prior and posterior distributions for calibration . . . . . . . . . . . . . . . .
67
5.2
Variance explained by diﬀerent numbers of principal components for a ﬂood
model emulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
6.1
Prior and posterior distributions for emulation
. . . . . . . . . . . . . . . .
86
6.2
Prior and posterior distributions for calibration (ﬁrst method) . . . . . . . .
92
6.3
Prior and posterior distributions for calibration (second method) . . . . . .
94
6.4
Lag found in best ARX(1,0) model applied to Hec-Ras output . . . . . . . .
97
7.1
Parameter estimates for Generalised Extreme Value function ﬁtted to his-
torical annual maxima of ﬂood height at Montford . . . . . . . . . . . . . . 105
7.2
Maximum diﬀerence in quantile peak height (m), for truncated compared
with untruncated upstream waveform
. . . . . . . . . . . . . . . . . . . . . 109
7.3
Nature of marginal distributions for variables describing ﬂood peak shapes . 111
7.4
Annual probability of issuing ﬂood warnings, before and after channel mod-
iﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.5
First order sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.6
First order sensitivity analysis, conditional on upstream peak height . . . . 119
viii

Chapter 1
Introduction
Since 1900, ﬂoods have aﬀected 28.6 million people per year around the globe, caus-
ing an average 63,500 deaths per year, and giving rise to annual economic damage of
US$4bn. However, within the last 30 years, the annual death rate due to ﬂooding has
been 7,000, while the mean number aﬀected annually has been 94 million, with annual
costs of US$14bn. Natural disasters from other causes show a similar pattern when a
comparison is made between these time scales, that is the approximate tripling of lives
aﬀected annually and of annual costs, while the numbers of lives lost has reduced, or
at worst remained constant (International Disaster Database, 2010). While the costs re-
ported do not take inﬂation into account, an increase in cost has been highlighted by the
insurance industry; costs of extreme weather events around the world have doubled each
decade since the 1970s (Coomber, 2006).
Indeed, there is a perception, arising from a number of factors, that ﬂood risk is increas-
ing. There is some concern that ﬂooding may increase as a result of the increased rainfall
intensity associated with a rise in global temperature. In addition, development of ﬂood-
plains not only increases the property vulnerable to ﬂooding, but inhibits drainage within
the ﬂoodplain, increasing the proportion of rainfall contributing to surface ﬂow during
heavy storms. Flood defences for such developments may also increase the risk to prop-
erty damage by restricting the area of the ﬂoodplain available for the river in times of high
ﬂow.
In England, the Environment Agency estimate that 2.4 million properties are at risk of
ﬂooding from rivers and the sea, with annual economic damage estimated at £1billion
(Environment Agency, 2009a). In addition, substantial infrastructure services are in areas
at risk of ﬂooding. Current annual spending on ﬂood defences is £800million. In this
context, there is a need for reliable ﬂood prediction not only for the improvement of ﬂood
warnings, but also for for appropriate allocation of such resources. The execution of both
of these employs computer models.
Floods are routinely predicted using computer models, both for operational warnings, and
for risk analysis for planning, design and asset management. The requirements of a ﬂood
1

Chapter 1. Introduction
2
modelling system depends on the end use. The emphasis for operational warnings is on
speed for real time prediction. Real-time ﬂood forecasting models can be either based on a
physical process description or purely on historical data analysis, will start from a known
state, and can be updated during operation by assimilation of weather and ﬂood severity
data as they become available. Their eﬃcacy is easily assessed.
Conversely, ﬂood models for risk analysis need to be largely physical process based, as
such models take as a starting point an unknown state, and may be run for conditions
substantially diﬀerent to those for which the model has been calibrated. While the repre-
sentation for forecasting involves predicting ﬂood levels from weather conditions, a greater
diversity of modelling processes may be used for risk analysis, depending on the empha-
sis of the study. A risk analysis study may examine ﬂood levels predicted from weather
conditions, or from upstream ﬂow conditions. It may also take into account the eﬀect of
climate change on the weather conditions. Flood severity data for such an application are
historical, and incorporated during calibration, and models do not need to be run in real
time. However, since the results concern an integral over the probability distribution of
an event and its consequences, assessment of the accuracy of such a study could only be
made by observation, under unchanged conditions, over an extended period of time, and
is thus diﬃcult to achieve.
All computer models are limited in the accuracy of their predictions by the extent of
scientiﬁc understanding, by the complexity that it is feasible to implement in a model,
and by the data available to input into the modelling system. Uncertainties in computer
model output are thus inevitable, and it is important to be able to assess accurately the
uncertainty involved in the modelling process, whatever the purpose of a ﬂood modelling
study. These uncertainties should be expressed as a probability distribution, since what
is acted upon is not generally the mean prediction, but one of its quantiles; an evacuation
warning will be issued if the probability of ﬂooding is greater than some threshold, and
the probability distribution of a particular ﬂood level being achieved is incorporated into
a risk analysis.
1.1
Risk analysis for decision making in ﬂood defence plan-
ning
Risk analysis permits a direct comparison between the expected damage and the expen-
diture involved in diﬀerent ﬂood protection strategies, thus providing a rational basis for
ﬂood management decision-making and allocation of resources. Indeed, both the UK gov-
ernment and the European Union require the drawing up of ﬂood risk management plans
for all areas of signiﬁcant ﬂood risk (European Union, 2007, Oﬃce of Public Sector Infor-
mation, 2009, 2010), with a view to reducing the risk of adverse consequences according
to appropriate objectives.
During a period of high river ﬂow, ﬂood defence failure may occur, either through over-

Chapter 1. Introduction
3
topping or through structural failure.
Inundation of the ﬂoodplain will then result in
damage to property and life, depending on the water depth and possibly the ﬂow rate in
the ﬂoodplain. In mathematical terms, if the water ﬂow rate at some point in the river is
denoted Q, the event that the ﬂood defence fails is φ and ﬁnancial estimate of the dam-
age is denoted c(h), dependent on ﬂoodplain water height h, then the expected damage
is
E(c) =
Z
c(h) f(h|φ, Q) p(φ|Q) f(Q) dQ
(1.1)
where f(h|φ, Q) is the probability density of ﬂoodplain water height h, conditional both on
ﬂood defence failure φ and on river ﬂow rate Q, p(φ|Q) is the probability of ﬂood defence
failure, conditional on Q, and f(Q) is the probability density of Q. This distribution is to
be determined from available information, which includes historical ﬁeld observations and
computer models.
The construction of a damage function c(h) is a complex problem, bringing together, as
it must, a wide range of consequences of inundation, including those for human health
and life, the environment, cultural heritage, economic activity and infrastructure. In this
study, the simpliﬁed approach is taken that the cost depends only on the exceedance of a
particular water height h0:
c(h) = 1
h > h0
= 0
h ≤h0
Similarly, the failure of ﬂood defences involves detailed structural complexities and uncer-
tainties, with weaknesses not able to be monitored, and deterioration mechanisms which
are not fully understood. As with the damage function, in this study, the complexities of
ﬂood defence failure are by-passed, assuming that the conditional probability p(φ|Q) = 1,
and consequently that f(h|φ, Q) becomes f(h|Q).
Taking into account these simpliﬁcations, the expected damage is reduced to a probability
of inundation,
P(h > h0) =
Z
I(h > h0) f(h|Q) f(Q) dQ
(1.2)
where I(·) is the indicator function, taking a value of 1 for positive argument, and 0
otherwise.
The distribution f(h|Q)is generally provided by a computer model. At its simplest, if Q
is taken to refer to the same location as h, it is the inverse of the rating curve, which
is usually expressed as a simple deterministic relationship, ﬁtted to historical data. Such
relationships are well known to involve uncertainties, but are nonetheless routinely used for
estimating ﬂow when only water height has been measured, for example. It is more usual to
regard Q as the ﬂow at some upstream location. Since, however, evaluation of the integral
involves a knowledge of the distribution of the independent variable of integration, the
integral could be formed in terms of upstream water height, or of catchment rainfall.
The probability distribution f(h|Q) depends on a number of inﬂuences other than Q, and

Chapter 1. Introduction
4
correct evaluation of the integral may depend on identifying these, characterising their
distributions and the dependence of h on them, and then integrating over their range.
Besides Q, a part is played by the antecedent conditions, the capacity of the channel
and the volume of inﬂow from lateral channels. In addition, it is necessary to estimate
the probability distribution of the upstream condition Q from the available information,
which includes both measured historical data and computer models. Both involve uncer-
tainties, which need to be taken into account. Data may only exist for a small number
of events; thus it is diﬃcult to account for the full range of possible input conditions. In
addition to limited coverage, data may be subject to measurement error. Many parts of
the process are not directly observable, leading to substantial uncertainties in modelling
and prediction.
Uncertainties can be classiﬁed as aleatory or epistemic. Aleatory uncertainty, or natural
variability, is random, and is relatively straightforward to deal with; this can be handled
by increasing the number of points sampled in the evaluation of the integral (1.2). By
contrast, epistemic or knowledge uncertainty is much harder to detect and eliminate, as
this requires identiﬁcation of its source within the model used to formulate the integral
for the probability of inundation. The problem is normally compounded in the calibration
of the model; by simply minimising the errors in model ﬁt to the historical data, model
inadequacies are compensated by parameter bias, potentially weakening the predictive
capacity of the model.
Hall et al. (2003) listed a hierarchy of ﬂood risk assessment approaches, to support a
range of ﬂood risk management decisions. These range from a national ﬂood risk assess-
ment, based on the standard of protection and the ﬂood frequency curve, such as the one
outlined in their paper, to detailed local studies, using continuous hydraulic modelling.
Naturally, the more detailed the study, the more opportunity exists to examine the mod-
elling uncertainties involved. However, of those studies reported in the scientiﬁc literature
(Dawson et al., 2005; Apel et al., 2004, 2008; Di Baldassari et al., 2009), none examine
the uncertainty arising from the ﬂood model itself, nor its calibration. Uncertainties exist
nonetheless, both in the structure of the model itself, and in the values of any parameters
used to tune the model for the river under examination.
1.2
Flood model calibration
Calibration is the process of ﬁnding the appropriate values of parameters for a computer
model. In some cases, these parameters represent physical quantities, and can be found by
independent measurements, but it often happens that the measurements cannot be made
at the appropriate scale, and thus cannot be used, or that parameters do not represent
physical quantities. Parameters must then be inferred by comparison of the model output
with historical data.
Historically, the values of such parameters were found manually by running the model with

Chapter 1. Introduction
5
diﬀerent parameters, and minimising some measure of ﬁt of the model output to the data.
Naturally, diﬀerent measures of ﬁt would lead to diﬀerent parameter values. Automation
of the process involves a numerical optimisation, but such techniques do not give rise to
probability distributions required by Equation (1.2). In addition, it has frequently been
found that a large number of parameterisations may lead to equally good model ﬁt. This
implies that the choice of a single parameterisation provided by optimisation of this ﬁt is
not the most appropriate means of model choice, as there is justiﬁcation for considering a
range of values for each parameter.
These last two issues are addressed by Bayesian calibration, since the parameters them-
selves are treated as variables with their own distributions, thus placing an emphasis on
quantifying uncertainties, rather than maximising model ﬁt. Bayesian analysis combines
in a rigorous way the prior beliefs of the user, the statistical model for the data, and the
data values themselves to lead to a distribution which may be used directly in Equation
(1.2) .
The most widely used method for the parameterisation of ﬂood models is undoubtedly
the Generalised Likelihood Uncertainty Estimation (GLUE) method of Beven and Binley
(1992). This is equivalent to the Approximate Bayesian Computation method which is
currently receiving much interest from statistical and biological modellers (e.g. Beaumont
et al., 2002; Gaggiotti, 2010). Starting from a prior joint parameter distribution, usually
taken to be uniform uncorrelated univariate distributions on a limited range, the model
is run a large number of times, randomly sampling from these parameter ranges. The
model is evaluated by some convenient measure of ﬁt, the generalised likelihood, and pa-
rameterisations leading to “non-behavioural” performance are discarded, according to a
criterion determined by the user. A distribution for the model output is then generated
by taking a weighted mean of the model outputs, the weights consisting of the measure
of agreement found, according to the generalised likelihood, for each parameterisation.
While the appropriate choice of likelihood the GLUE method can give rise to a rigorous
parameterisation, such as that demonstrated by Romanowicz et al. (1996), the relaxation
of the requirement to use such a carefully chosen likelihood, depending rigorously on a
plausible statistical data model, has both ensured the popularity of the method, and at-
tracted criticism from a number of authors (Montanari, 2005; Ewen et al, 2006; Mantovan
and Todini, 2006; Stedinger et al., 2008), on the grounds of arbitrariness and inconsistency
of the result.
It is important to consider the diﬀerent error sources in the data model. Ewen et al.
(2006) showed by comparing the output from a reduced model to that of a more detailed
model, that oversimplifying some mechanisms leads to biassed parameter outputs. Much
of the literature on model calibration does not take into account diﬀerent error sources,
and is thus ﬂawed. This shortcoming also applies to the GLUE method.
Other more rigorous calibration methods have been suggested to overcome these deﬁcien-
cies, in particular for models with time-series input and output, with a view to forecasting.
However, for risk analysis, and for modelling of ﬂood extent using aerial or satellite images,

Chapter 1. Introduction
6
to date no adequate method has been developed.
In this context, a Bayesian calibration is required for ﬂood inundation models, taking into
account all the sources of possible error. The method should be statistically rigorous,
should take into account the various sources of uncertainty in model output, and to be
appropriate for risk analysis. In addition, it should be able to be used with data arising
from river gauging, and from ﬂood extent snapshots. This thesis concerns the development
of such a method.
1.3
Bayesian statistics
Statistical inference is the process of drawing conclusions about a system from a data sam-
ple randomly drawn from a population of some quantity of the system under investigation.
The data are generally analysed with reference to a statistical model of the assumed dis-
tribution of the population. The likelihood is deﬁned as the distribution of the observed
data z, under the assumption of the statistical model, dependent on parameters θ.
L = f(z; θ)
Classically, characterisation of a modelled population is performed by maximisation of the
likelihood with respect to the model parameters θ. The Bayesian method is founded on the
concept that the distribution of a variable representing the quantity under investigation
is characterised not only by the available data, but also by the beliefs of the modeller
about the parameters of the distribution, based on his prior experience. In this case, the
parameters, instead of being assumed to have distinct values, are themselves considered
to have a distribution.
Thus, if the user’s belief is that the parameters θ of the system have a distribution f(θ)
(the prior distribution), and the data z have a distribution f(z|θ), deﬁned by the statistical
model and conditional upon the parameters θ, then the conditional probability is given
by
f(z|θ) = f(z, θ)
f(θ)
Equally, the probability distribution of the parameters, conditional on the data, can be
expressed as
f(θ|z) = f(z, θ)
f(z)
Bayes’ theorem gives the probability of the parameters, conditional on the data, as fol-
lows:
f(θ|z) = f(z|θ)f(θ)
f(z)
=
f(z|θ)f(θ)
R  f(z|θ)f(θ)

dθ
(1.3)

Chapter 1. Introduction
7
The distribution f(θ|z) is known as the posterior distribution, and represents the synthesis
between the user’s prior beliefs f(θ), the statistical model f(z|θ), and the data. The use
of Bayes’ theorem provides a means of updating the user’s beliefs about the system in the
light of the data, and can be used recursively, as more data become available.
The distribution of the data has been described above by a model f(z|θ). However, the
parameters θ may be dependent on secondary parameters, φ, with a conditional model
f(θ|φ). This is known as a hierarchical model. The conditionality could be extended, with
φ dependent on further parameters, ψ. We then say that the distribution of parameters at
a given level is conditional on those at the next lower level, and given that conditionality,
is independent of parameters lower than that. We can thus specify the joint distribution
of the data and parameters as:
f(z, θ, φ) = f(z|θ) f(θ|φ) f(φ)
This construct permits us to consider the dependence of our data on both model output,
and diﬀerent error sources; thus Huard and Mailhot (2006) described the sources of error
in modelled streamﬂow data using a physically-based model M, by the diagram in Figure
1.1, and were able to construct a hierarchical model to represent the measured data. The
data model shown in Figure 1.1 is described by the relationship
z = M(w + e, θ) + δ + ϵ
(1.4)
where the observed data z are dependent on output of the model M(w + e, θ) at input
w + e with parameters θ, subject to model structural error δ and observation error ϵ. The
research described in this thesis has been undertaken in the context of the model described
in Equation (1.4).
Measured 
data
w
z
True input
x
True output
y
Model
)
,
( 
x
M
e
Input error
Output error


Structural error
True 
process
Figure 1.1: Data dependency model (after Huard and Mailhot, 2006)
Identiﬁcation of the three unknown quantities, input error e, structural error δ and output
error ϵ at each time step is a diﬃcult problem, for which no solution has yet been achieved.
It relies on the ability to deﬁne a statistical distinction between them, since without this

Chapter 1. Introduction
8
these unknown quantities cannot be individually identiﬁed.
However, it is possible to
achieve a partial solution to this problem by taking a simpliﬁed data model.
1.4
Thesis structure
This chapter has given an overview of the motivation, aims, and statistical context of the
research undertaken. The research is described in further chapters, as follows.
Background to the research is provided in Chapter 2, which describes the models used
in ﬂood prediction, and the simpliﬁcations that are necessary to make their use prac-
ticable, followed by a review of the relevant literature on calibration of hydrodynamic
models.
The methodology used is described in Chapter 3, followed by an example of its application
to an algebraic example. Chapters 4, 5, and 6 describe the application of the methodology
to examples of increasing complexity. These are a series of one-dimensional steady-state
laboratory experiments (Chapter 4), a steady-state model of a ﬂood extent on the Thames
at Buscot, calibrated with satellite data (Chapter 5), and a dynamic model of the Severn
in the region of Shrewsbury, calibrated with gauged river stage data (Chapter 6).
Chapter 7 illustrates the application of the methodology in the comparison of the impact
of two hypothetical ﬂood risk management schemes for the example on the Severn. The
thesis concludes with Chapter 8, giving an indication of the success of the methodology
with respect to the research aims, and suggesting avenues for extension of the investiga-
tion.

Chapter 2
Hydrological background
2.1
Introduction
The probability of inundation described in the Introduction (Equation (1.2)) is
P(h > h0) =
Z
I(h > h0) f(h|Q) f(Q) dQ .
As described in the Introduction, the probability integral may be formulated and evaluated
in terms of upstream ﬂow, water height or catchment rainfall.
The term f(h|Q), or
its equivalent, according to the input conditions used, gives downstream water height
conditional on upstream conditions, encapsulating the information to be derived both
from hydrological and hydraulic models and from measured data. Its evaluation requires
an understanding of the hydrological mechanisms being modelled, the types of models
being used and their limitations, and the limitations of the data used, as well as a sound
understanding of the statistical principles and techniques required. This chapter addresses
not only the area of hydrological and hydraulic modelling, providing a background to
the processes involved and the models used to describe them, but also the literature on
hydrological model calibration, highlighting the issues to be considered.
Flood production from precipitation involves a number of processes. These are non-linear
and their interaction complex. The dynamics and spatial distribution of runoﬀgeneration
depend on the hydraulic properties and storage capacities of highly heterogeneous natural
materials, including vegetation, soils and rocks.
Runoﬀis driven primarily by gravity
and precipitation, modiﬁed by the mechanisms of evapotranspiration and movement of
water through porous media. Local surface or subsurface ﬂow occurs when the volume of
water exceeds the local storage capacity, and connectivity of areas of local ﬂow can lead
to larger-scale ﬂow. Lower in the catchment, the ﬂow dynamics within the channel may
make a more signiﬁcant contribution to total water transport than the runoﬀgenerating
process.
The models describing the mechanisms of runoﬀproduction and channel ﬂow are very
9

Chapter 2. Hydrological background
10
diﬀerent. Runoﬀgeneration is described by rainfall-runoﬀ, or hydrological models, while
the ﬂow dynamics are described by routing, or hydraulic models. It is often the case that
the ﬂow in the upper part of a catchment is described by a hydrological model, while
the ﬂow lower in the catchment is described by a hydraulic model, although hydrological
models may account for the entire catchment.
While the work in this thesis concerns the calibration of hydraulic models, a large part
of the literature on model calibration concerns hydrological models, so a brief description
follows of the types of hydrological model. Following that, a description is given of the
known behaviour of channel and ﬂoodplain ﬂow, and an overview of the models used to
describe these. The chapter concludes with a review of the relevant literature on calibration
of hydrological and hydraulic models.
2.2
Rainfall-runoﬀmodels
Freeze and Harlan (1969) suggested a blueprint for a physically based model of hydrolog-
ical catchment response to precipitation, including interception and evapotranspiration,
inﬁltration and soil moisture ﬂow, groundwater, overland and channel ﬂow. While such
models have been generated, they are costly to run, and require extensive data inputs. It
is thus often not realistic to represent the movement of water through a catchment in such
a detailed manner.
In spite of this blueprint, there is no consensus about the best way to achieve a simple
and eﬀective description of the ﬂow of subsurface water. This is partly because there
are many mechanisms at work, whose relative signiﬁcance will diﬀer according to the
catchment or part catchment, and according to the ﬂow regime. The other major diﬃculty
in describing runoﬀformation is the lack of detailed information about soil structure and
behaviour.
One of the simplest rainfall-runoﬀmodels is the Nash cascade model (Nash, 1959), which
treats the catchment as a series of equally-sized reservoirs. The unknowns in the model are
the number of reservoirs and their total capacity. Since the model is not really a physical
description of the catchment behaviour, it may be extended to have a non-integer number
of reservoirs.
A more sophisticated type of model is the lumped conceptual catchment model. With
this type of model, the catchment is described as a single, aggregated entity, but diﬀerent
mechanisms of water storage are described as reservoirs; hence there may be reservoirs
representing the water held in the canopy, the shallow soil, and deeper water storage.
Deterministic diﬀerential equations are used to represent the transfer of water between
these stores, as well as evaporation and runoﬀ. While these relationships represent physical
concepts, there is no attempt to relate them to spatial locations within the catchment,
and the parameters used in the model do not represent actual physical quantities; thus
values must be found by calibration.

Chapter 2. Hydrological background
11
Fiering developed a simple linear rainfall-runoﬀmodel for teaching purposes (Fiering,
1967); this has been used to demonstrate a number of calibration schemes, as it is a useful
example of the type of equations used in lumped catchment models. Fiering’s “abc” model
is as follows:
Qt = (1 −a −b)rt + cSt
St+1 = (1 −c)St + art
where r is the input rainfall volume, Q and S represent discharge and storage volume
respectively with discrete time index t, and parameters a, b and c are the proportions
of rainfall entering storage, and lost to evapotranspiration, and the proportion of water
leaving storage. The parameters a, b, and c are to be determined by calibration, subject
to the mass conservation condition a + b ≤1.
There are many diﬀerent conceptual rainfall-runoﬀmodels in use; modelling success has
been achieved by the incorporation of nonlinearities resulting from the saturation of in-
creasing area of the catchment, with this area being described probabilistically (Moore and
Clarke, 1981) or deterministically (Wood et al., 1992; Zhao, 1992; Todini, 1996), treating
the catchment as a collection of subcatchments, and solving simultaneously for the runoﬀ
formation in each subcatchment, as well as the channel ﬂow.
Sivapalan et al. (2003) contrasted the “bottom-up” and “top-down” approaches to rainfall-
runoﬀmodelling. The “bottom-up” approach is that described above, where a concept
of the physical processes, inferred from physical understanding and from observation of
catchment behaviour, is translated into a system of mathematical equations, requiring
ﬁtting of a number of parameters. The “top-down” approach infers the model structure
directly from the data, trying to identify the dominant processes at the catchment scale.
Young (e.g. 2003) suggested that the “bottom-up” approach leads to a model requiring
more parameters to be determined than the data quality can justify, and recommends a
“data-based mechanistic” approach, based on a statistical analysis of the input and output
data.
The most detailed rainfall-runoﬀmodels (Abbott et al., 1986; Ewen et al., 2000; Ciarapica
and Todini, 2002) are those which correspond to the original blueprint of Freeze and Harlan
(1969) to represent the spatial and temporal movement of surface and subsurface water
on a grid covering the entire catchment.
Such models can be considered to be truly
physically based, but require large amounts of data to deﬁne the diﬀerent characteristics
of the catchment properties. While much of this data may be obtained from databases of
material properties such as the HOST classiﬁcation of soil types (Boorman et al., 1995),
some parameters represent sub-grid-scale processes, and thus need to be estimated (e.g.
Bathurst, 1986).

Chapter 2. Hydrological background
12
2.3
Hydraulic models
2.3.1
Description of mechanisms in ﬂoodplain ﬂow
Under normal conditions, a river ﬂows within its channel. When the ﬂow is too great to be
contained within the channel, it spills onto the ﬂoodplain, eﬀectively using a new, broader
and more complex channel.
Even simple steady ﬂow in a uniform, prismatic channel may involve three-dimensional
processes, as diﬀerential resistance leads to diﬀerent velocities across the channel, and
the setting up of secondary ﬂows (Knight and Shiono, 1996). The more complex geom-
etry of the ﬂoodplain, and resistance caused by vegetation and other obstacles leads to
more complex ﬂow processes, involving diﬀerent ﬂow velocities in the channel and the
ﬂoodplain. In an eﬀort to characterise these processes, many laboratory experiments have
been conducted, involving measurements of the shear stresses and secondary circulation
in steady-state ﬂows in straight or meandering channels with uneven beds. Sellin (1964)
described vortices with vertical axes along the edge of the main channel, accounting for
the transfer of momentum between the channel and the slower-ﬂowing ﬂoodplain. Ervine
et al. (1993) described horizontal vortices in meandering channels, initiated immediately
below the bend apex. Knight and Shiono (1996) pointed out that as the ﬂoodplain depth
increases, it eﬀectively becomes a larger channel, and the velocities tend to equalise; the
impact of three-dimensional ﬂow processes on the channel conveyance tends to be at its
greatest for the ratio of ﬂoodplain depth to main channel depth in the range 0.1-0.3.
2.3.2
Equations representing ﬂuvial ﬂow
Treating a ﬂuid as a continuum, and averaging out terms representing small-scale eddies,
ﬂow is described by the Navier Stokes equation of motion. Momentum balance for an
incompressible ﬂuid gives the following:
ρ
∂u
∂t + u · ∇u

= ρF −∇p + ∇· T,
where u is the velocity at a point, ρ the density, p the pressure, F represents the external
forces, and T is the deviatoric stress tensor, representing the internal stresses in the ﬂuid
arising from motion (Batchelor, 1967, p142).
The terms on the left hand side of the
equation describes the change in momentum at a point in space and time.
Conservation of mass gives the continuity equation:
∂ρ
∂t + ∇· (ρu) = 0
where clearly for an incompressible ﬂuid the ﬁrst term is zero.
These equations are diﬃcult and costly to solve, not least because there is a discontinuity

Chapter 2. Hydrological background
13
at the free water surface. For most purposes, acceptable approximations can be made
by assuming that the horizontal scale is much larger than the vertical scale, that the
vertical velocity component is small, and there is no variation of ﬂow with depth, and by
integrating over the water depth. The resulting shallow water equations can be expressed
in terms of the ﬂow below a surface of constant pressure in the ﬂuid, such as the free
surface.
Making a further simpliﬁcation, that the ﬂow is predominantly in the longitudinal direction
of the channel, so that the velocity is uniform and the water surface horizontal across any
cross section perpendicular to the longitudinal axis, we arrive at the one-dimensional Saint
Venant equations (e.g. Chow et al., 1988, p281), which are shown below. The continuity
equation, representing mass conservation over a unit width is:
u∂y
∂x + y∂u
∂x + ∂y
∂t = 0
where u is the longitudinal velocity, and y the channel depth.
The momentum equation is:
∂u
∂t + u∂u
∂x + g ∂y
∂x −g(So −Sf) = 0
where in addition and So is the slope of the channel bottom, Sf the friction slope, g
the gravitational constant. A further relation is needed to describe Sf; this provides a
parameter to be determined.
These equations are known as the dynamic wave model, while ignoring the ﬁrst two terms
of the momentum equation, the acceleration terms, leads to the diﬀusion (or non inertial)
wave model. Further simpliﬁcation, ignoring the third term, the pressure force term, yields
the kinematic wave model. Kinematic wave models can make use of empirical equations
for steady ﬂow, including the Manning, Chezy, and Darcy-Weisbach equations.
In practice, diﬀerent simpliﬁcations of the Navier Stokes equations are needed to ade-
quately describe the ﬂow in diﬀerent environments. The kinematic wave model is only
suitable for rivers with signiﬁcant bottom slope (> 0.1%), and overland ﬂow, while the
diﬀusive wave model can include deceleration eﬀects, but does not include a full descrip-
tion of backwater eﬀects or reverse ﬂows. Dynamic wave models are needed for rivers with
mild bottom slopes, tidal rivers, and those with reservoirs.
The majority of commercial codes for solving hydraulic equations use a one-dimensional
approximation for the channel ﬂow. Two such codes are used in this study, which em-
ploy diﬀerent adaptations of the Saint Venant equations to represent the diﬀerent ﬂow
behaviour in the channel and the ﬂoodplain. These are described below.

Chapter 2. Hydrological background
14
2.3.2.1
LISFLOOD-FP
LISFLOOD-FP (Bates and de Roo, 2000) was developed to use as simple as possible a
representation of ﬂood ﬂow, to enable modelling over an extended ﬂoodplain, or to enable
sensitivity analysis. Flows in the channel and ﬂoodplain are represented by diﬀerent sets
of equations, which are coupled. Channel ﬂow is represented by the one dimensional Saint
Venant equations, this time expressed in terms of volumetric ﬂow Q and cross-sectional
area A,
∂Q
∂x + ∂A
∂t = q
(2.1)
where q is the ﬂow into the channel from the ﬂoodplain or tributary channels, together
with a momentum equation (Manning’s equation),
So = Sf
Sf −n2P 4/3Q2
A10/3
= 0
(2.2)
where as before So is the slope of the channel bottom, Sf the friction slope, and in addition
n is the friction parameter, Manning’s n, and
P is the wetted perimeter of the ﬂow.
A rectangular channel cross-section is assumed. Flow over the ﬂoodplain is represented
by a network of two-dimensional rectangular storage cells, based on the DEM, and where
the ﬂow rates between the cells are calculated by the equations:
dhi,j
dt
= Qi−1,j
x
−Qi,j
x + Qi,j−1
y
−Qi,j
y
∆x∆y
Qi,j
x =
h5/3
flow
n
hi−1,j −hi,j
∆x
1/2
∆y
(2.3)
where
hi,j is the water free surface height at node (i, j)
∆x and ∆y are the cell dimensions,
n is the eﬀective grid scale friction parameter, Manning’s n, for the ﬂoodplain,
Qx and Qy are the volumetric ﬂow rates between ﬂoodplain cells, with Qy deﬁned analo-
gously to Qx, with superﬁces (·)i,j referring to the node (i, j), and
hflow, the ﬂow depth, is the depth through which water can ﬂow between two cells, and
is deﬁned as the diﬀerence between the highest water free surface in the two cells and the
highest bed elevation.
Equations (2.3) embody the assumption that the ﬂood spreading over the ﬂoodplain is a
function of gravity and topography.

Chapter 2. Hydrological background
15
2.3.2.2
Hec-Ras
Hec-Ras is a commercially-used hydraulic modelling package written by the US Army
Corps of Engineers (2002). The one dimensional Saint Venant equations of motion are
adapted to allow for diﬀerent conditions in the channel and ﬂoodplain as follows:
∂A
∂t + ∂(ΦQ)
∂xc
+ ∂[(1 −Φ)Q]
∂xf
= 0
∂Q
∂t + ∂(Φ2Q2/Ac)
∂xc
+ ∂[(1 −Φ)2Q2/Af]
∂xf
+ gAc
 ∂z
∂xc
+ Sfc

+ gAf
 ∂z
∂xf
+ Sff

= 0
(2.4)
where
Q is the total ﬂow, as before
Φ is the proportion of the total ﬂow in the channel, =
Kc
Kc + Kf
Kc is the channel conveyance, (deﬁned as
Q
S1/2
fc
= AcR2/3
n
, incorporating Manning’s equa-
tion)
Kf is the ﬂoodplain conveyance,
z is the water surface height, and
cross-sectional area, A, and friction slope, Sf, are distinguished for the channel and ﬂood-
plain by suﬃces c and f respectively.
The river path is determined by plan geometry and cross-sections speciﬁed by the user
across the channel and ﬂoodplain.
An assumption is made that the water surface is
horizontal across the entire cross-section, and the equations are discretised, calculating
values at the cross-sections. Distance xc is measured along the assumed middle line of the
channel, under the assumption that the cross-sections are perpendicular to its direction,
and xf is the distance speciﬁed between the cross-sections.
Cross-sections need to be
speciﬁed adequately close together to ensure that the river ﬂow is adequately represented
by the one-dimensional model.
2.3.3
Data requirements of hydraulic models
Initial conditions are required for dynamic equations, as are boundary conditions at all
boundaries. These can be in the form of water level or ﬂow time series. A simpliﬁed
relationship may be used for the downstream boundary condition, such as Manning’s
equation:
v = 1
nR2/3S1/2
0
where v is the cross-sectionally averaged velocity, R is the hydraulic radius, normally taken
to be the ratio between ﬂow cross-sectional area and the wetted perimeter, S0 is the channel
slope, and n, Manning’s roughness coeﬃcient, is the constant to be determined. This
boundary condition is an approximation, as Manning’s equation is used as a description

Chapter 2. Hydrological background
16
of steady ﬂow. It should thus be applied well below the reach where the model results are
required.
In addition to the ﬂow conditions, it is necessary to have information both on the river
location and cross-sectional geometry, and on the ﬂoodplain elevation. Models diﬀer in
their precise requirements of geometric data; for example, Hec-Ras requires cross-section
data extending over both the channel and the ﬂoodplain, while LISFLOOD assumes a
rectangular cross-section for the channel, but requires the ﬂoodplain to be speciﬁed by a
rasterised digital elevation map.
In general, the primary measurement of river discharge is stage, or water height, found by
recording the level of a ﬂoat in a stilling well adjacent to the river, by pressure sensing,
or by reﬂection of an ultrasound signal from the river bed. It is much more diﬃcult to
determine ﬂow, as this involves estimating the velocity throughout the river cross section,
and integrating. Systematic measurement of ﬂow is most easily undertaken at a weir,
where the river is constrained, and the bed shape is such that the variation of velocity
is well understood. In either case, measurements may be aﬀected by weed growth, or by
bypassing of the measurement station in high ﬂow conditions.
Since ﬂow is diﬃcult to measure, and can require installation and maintenance of so-
phisticated equipment, it is common for ﬂow records to be determined by rating curve,
particularly at small measurement stations. This is a deterministic relationship between
values of stage and ﬂow. Such relationships are often based on a limited number of mea-
surements, and may require extrapolation, particularly for modelling ﬂood conditions.
In addition, the relationship between ﬂow and stage frequently exhibits hysteresis, which
would be expected from the hydrodynamic equations, but which is not necessarily reﬂected
in the rating curve.
Measurements of the river cross-section are laborious and time consuming. It is frequently
the case that a hydraulic model is limited by the number and spacing of available cross-
sections. These cross-sections should also include information about the ﬂoodplain, partic-
ularly where there are man-made obstacles such as bridges and culverts. It is well known
that such structures signiﬁcantly modify the upstream and downstream water levels, and
often the case that the simpliﬁed ﬂow equations may not be valid in their neighbourhood,
but as their characteristics are often not well known, they are an additional source of
model structural uncertainty.
Although many diﬀerent types of data are used as input into a hydraulic model, in general
for a one dimensional model the only parameter which is varied is the roughness parameter,
n, although this may be allowed to vary spatially. This parameter is used to account for
all the inadequacies of the data and model, as well as roughness of the river bed. It has
been shown in laboratory experiments (Ervine et al., 1993) that a signiﬁcant diﬀerence in
value of Manning’s n is required to account for sinuous channels in a one dimensional ﬂow
model.

Chapter 2. Hydrological background
17
2.4
Discussion
It is evident from the foregoing description of hydrological and hydraulic models that
there are similarities as well as diﬀerences between the physical descriptions of rainfall-
runoﬀand channel routing.
The diﬀerences are most obvious for the greatest spatial
aggregation represented in the rainfall runoﬀmodels, or indeed for the greatest detail in
the representation of the hydraulic models.
One very signiﬁcant diﬀerence is in the way in which the models are used; rainfall-runoﬀ
models tend to be used in continuous simulation over extended periods, whereas hydraulic
models are usually limited to event-based modelling. Even this diﬀerence is not strict;
Feyen et al. (2007) reported using a simple hydraulic model in continuous simulation;
while large distributed models are limited by computing resources in the modelling period
or spatial deﬁnition achievable.
In general the number of parameters used in rainfall-runoﬀmodels is greater than that
used in hydraulic models, partly since the variety of mechanisms being represented is
wider, but also because most of the inputs of a hydraulic model are considered ﬁxed,
being of a geometric nature. However, while it is in principle possible to measure all the
necessary geometric data needed, this may not be realistically practicable, in particular as
such characteristics may be subject to morphological change. For both hydrological and
hydraulic models, the parameters may well not correspond to measurable entities; thus
calibration may be required.
A signiﬁcant source of error in both hydrological and hydraulic models is the essentially
unknown information. In the case of hydrological models, this arises from the accuracy
with which rainfall can be determined. The problem arises in hydraulic models, where
ﬂow measurements are dependent on a rating curve, and where gauging of lateral inﬂows
is insuﬃciently accurate, or non-existent.
While both types of models correspond to variable spatial domains, information may not
be available in suﬃcient spatial detail to properly describe the ﬂow mechanisms. Thus,
for rainfall-runoﬀmodels, it is likely that the availability of spatially varying rainfall
measurements and of detailed understanding of spatial subsurface geometry is inadequate.
In the case of hydraulic models, geometrical descriptions of the river bed may not be
available in adequate detail.
The combination of inadequate data, an inability to model at a suﬃciently broad range of
scales to capture all the relevant processes, and a lack of clarity as to which mechanisms are
dominant, must result in models which are in part inadequate to describe the processes
at work.
Thus a calibration method is needed which takes into account the fact that
the modelling process is likely to be inadequate. In the review of literature on model
calibration which follows, an emphasis has been placed on identiﬁcation of the sources of
model output error, and the implications of this for model calibration methodology.

Chapter 2. Hydrological background
18
2.5
Characterisation of uncertainty sources for ﬂood mod-
els
Ewen et al. (2006) classiﬁed the diﬀerent sources of error in computer code outputs by
the stage in the modelling process at which they are introduced:
• model structural error, including conceptual and implementation errors,
• parameter error, including poor parameterisation, errors in parameterisation to com-
pensate for structural deﬁciency and errors due to inaccurate calibration data, and
• runtime error, from errors in input data and in misuse of the model and misinter-
pretation of its results.
By contrast, Kennedy and O’Hagan (2001a) listed the diﬀerent sources of uncertainty in
computer code outputs in terms of where they may be introduced in a statistical model
of the outputs. Their list is as follows:
• parameter uncertainty,
• model inadequacy, where the model may not perfectly specify the process under
consideration,
• residual variability, the variability of the process starting from apparently identical
conditions,
• parametric variability, where the model is insuﬃciently detailed to describe the pro-
cess with a single parameter value,
• error in the observation of outputs for calibration purposes, and
• code uncertainty, because it may not be feasible to run the code for every possible
combination of parameters and input data.
An additional uncertainty, which was largely ignored by Kennedy and O’Hagan, though
not by Ewen et al., but is signiﬁcant in hydrological modelling, is uncertainty in the
measurement of input data. At the very least, for hydraulic models, input measurement
errors may be expected to be of a similar order to the output measurement errors, having
a similar measurement mechanism. However, for hydrological models, the input is rainfall,
which is much more diﬃcult to establish precisely over the entire catchment.
All parameterisations deal with output errors; while some studies have noted evidence for
heteroscedacity or autocorrelation in the output errors, suggesting transformations or the
incorporation of an autoregressive error model (e.g. Sorooshian and Dracup, 1980), the
autocorrelation at least may in fact be caused by inattention to other types of error. A
number of studies have been undertaken, concentrating on one or other error sources; a
review of these is given below. It should be noted that calibration of hydrological and hy-
draulic models is undertaken for three reasons, to understand the underlying mechanisms
of a process, to improve forecasting lead time and uncertainty, and to identify uncertainty

Chapter 2. Hydrological background
19
for performing risk analysis. While the purpose of this study is the third of these aims,
the vast majority of the available literature reﬂects the ﬁrst two. In undertaking a review
of the literature on calibration of hydrodynamic models, then, it is necessary to examine
studies which, while undertaken with a diﬀerent purpose in mind, may have some relevance
to the current aim.
2.5.1
Parametric variability
Boyle et al. (2000) imitated manual model ﬁtting procedure for transient streamﬂow mod-
els, in ﬁtting separately portions of the hydrograph they describe respectively as “driven”,
“non-driven quick” and “non-driven slow” ﬂows, using a multi-objective optimisation to
achieve a Pareto surface for the parameters. They reconciled the diﬀerent parameterisa-
tions required for the diﬀerent ﬂow regimes, by choosing the parameter set on the Pareto
curve that gave the minimum bias in terms of overall mean ﬂow. Wagener et al. (2003) ex-
tended the concept of parameterising diﬀerent parts of the time series, by taking a moving
window, and distinguishing well-identiﬁed parameter ranges as the window moves, showing
that diﬀerent parameters are better identiﬁed at diﬀerent parts of the time series.
Many studies investigating parametric variability in dynamic models are related to the
Kalman ﬁlter (Kalman, 1960). This will be described later in the thesis, along with other
approaches designed speciﬁcally for time-varying models.
2.5.2
Structural uncertainty
A popular method of treating model structural uncertainty, which could be applied to
temporal or spatial data, involves averaging the output of diﬀerent models, thus increasing
the number of mechanisms that can be treated. Georgakakos et al. (2004) demonstrated
that taking an unweighted mean of the output of a number of models improved model ﬁt
compared with output from the individual models. This comparison was made within the
Distributed Model Intercomparison Project (Smith et al., 2004), using 11 models and 6
catchments.
Bayesian model averaging (BMA) is a technique used to combine models, while taking
into account their individual skills. Thus, if there are K models, Mk, (k = 1, . . . , K) with
parameters θk, each predicting a quantity ∆, subject to data, D, the joint prediction is
(Hoeting et al, 1999):
P(∆|D) =
K
X
k=1
P(∆|Mk, D)P(Mk|D) where
P(Mk|D) =
P(D|Mk)P(Mk))
PK
l=1 P(D|Ml)P(Ml)
and
P(D|Mk) =
Z
P(D|θk, Mk)P(θk|Mk)dθk
(2.5)

Chapter 2. Hydrological background
20
The posterior probability distribution is eﬀectively a mean, weighted by the skill of the
individual models.
Neuman (2003) applied a simpliﬁcation of this technique to a set of three pre-calibrated
models, and showed improved prediction skill. Duan et al. (2007) again used three com-
peting models, each previously calibrated using three diﬀerent objective functions, the
Nash-Sutcliﬀe criterion, which is 1- the ratio of the error variance to the data variance
(Nash and Sutcliﬀe, 1970), a measure of absolute error, and a measure taking into ac-
count the heteroscedacity of the data. Taking note that streamﬂow is heteroscedastic,
they applied the methodology to data normalised using the Box-Cox transformation (Box
and Cox, 1964). Having shown that the BMA algorithm demonstrated the superiority of
diﬀerent parameterisations at diﬀerent parts of the time series, the authors went on to pa-
rameterise each model with diﬀerent parts of the input data for each of three catchments,
again demonstrating improved skill. Rojas et al. (2008) combined Bayesian model averag-
ing with GLUE to undertake a full solution to Equation (2.5) for a groundwater ﬂow model,
again comparing the eﬀect of three diﬀerent objective functions. They demonstrated the
feasibility of this technique, but noted that it is computationally expensive.
It should be noted that a model averaging technique is only as good as the individual mod-
els. If there is a common deﬁciency with all of the input models, no averaging technique
can lead to a good resultant model.
2.5.3
Input errors
Kavetski et al. (2002) and Huard and Maillot (2006) have shown that model parame-
terisation without accounting for input errors leads to biassed parameter estimates. In
a spatially aggregated catchment rainfall-runoﬀmodel, there is substantial uncertainty
in deﬁning the input rainfall. This is because the monitoring of rainfall takes place at
individual locations, while a whole-catchment model requires input representing rainfall
throughout the catchment. Spatially aggregated rainfall is diﬃcult to estimate in the face
of a sparse rainfall gauge network, which may not even overlap the catchment; Linsley et
al. (1988, p60) give an illustration of the estimation errors. For catchment rainfall-runoﬀ
modelling, this is in fact a structural as well as an input error; diﬀerent spatial rainfall
patterns will lead to diﬀerent catchment response, which cannot be captured in a spatially
aggregated model.
The issue of input errors is not conﬁned to rainfall-runoﬀmodels. In hydraulic modelling,
upstream model input is often in the form of ﬂow, which may be subject to rating curve
errors. In addition, lateral inﬂow to the river is often not recorded for smaller tributaries;
when it is, it has to be introduced to the main model in the form of ﬂow, which is again
likely to have been derived using a rating curve.
Kuczera et al.
(2006) pointed out that in calibration of spatially aggregated rainfall-
runoﬀmodels, rainfall spatial distribution may vary from storm to storm, giving the eﬀect
of volumetric errors in input rainfall. They thus proposed a multiplicative error model

Chapter 2. Hydrological background
21
applied to the rainfall input, with diﬀerent multipliers for each storm. The multipliers were
estimated simultaneously with other model parameters in a Bayesian regression analysis
(Kuczera et al., 2006, Kavetski et al., 2006).
Thyer et al. (2009) examined the inﬂuence of the time-scale used for the rainfall multi-
pliers, comparing the use of daily and storm-length multipliers. The use of daily rainfall
multipliers represents a signiﬁcant increase in the number of parameters to be determined.
The authors noted that the output was insensitive to a signiﬁcant number of the daily
rainfall multipliers, associated with days where there was low rainfall, and discarded these
in a preliminary analysis stage. In spite of this, they were only able to use a 2-year calibra-
tion period for the daily input error model, while for the storm-length error model, they
were able to use a 5-year calibration period. They demonstrated that their parameterisa-
tion of input errors represents a signiﬁcant improvement in model ﬁt, runoﬀdistributional
consistency and in parameter consistency over a formulation without input error model.
They suggested that the daily rainfall input error model seemed more appropriate, but
suggested that for the catchment they used, the Horton catchment in New South Wales,
the evidence was not conclusive. However, it is possible, that for wetter catchments the
division of the rainfall record into storms, which was done in advance of the regression,
would be less obvious, and therefore more subjective.
Ajami et al.
(2007) tried to combine a formulation of the input error problem with
Bayesian model averaging to account for structural errors; however, their statistical rea-
soning was ﬂawed, as pointed out by Renard et al. (2009), as they had tried to analyse the
problem by solving only for the distributional parameters of the rainfall multipliers rather
than their actual values, leading to an ill-posed problem. Indeed, Renard et al. (2010)
examined the possibility of simultaneously modelling input and structural errors using the
regression formulation of Kuczera et al. (2006) but concluded that without adequate prior
information to distinguish the error sources, the problem is ill-posed.
The Bayesian Forecasting System of Krzysztofowicz is an alternative approach to the in-
corporation of both input errors and structural errors, in this case in a real-time ﬂood
forecasting system, described in a series of papers (Krzysztofowicz, 1999, 2002; Krzyszto-
fowicz and Kelly, 2000; Krzysztofowicz and Herr, 2001). This is a Bayesian time series
river height prediction system, involving a hierarchy of precipitation and rainfall-runoﬀ
models, and was formulated in a general manner, allowing for any plug-in precipitation
input and single stage rainfall-runoﬀmodel, and was implemented as a short-range real-
time forecasting system. The model structure allows for the hydrological model to be
processed under the assumption that the only uncertainties are those of data input, but
the parametric uncertainty of the hydrological model is incorporated further downstream
in the hierarchy. The transformation of the data to a Normal distribution, using an em-
pirical transformation based on historical data, besides ensuring that all assumptions are
supported, permits analytical solution of the model equations at each step, alternating
between data assimilation and stage prediction.
Price (2006) approached the problem of estimation of input errors in a hydraulic model.

Chapter 2. Hydrological background
22
Reformulating the Saint Venant equations as functions of the lateral inﬂow q, a kine-
matic wavespeed, c0(Q) and an attenuation parameter a0(Q), the latter two described
in terms of the instantaneous channel inﬂow Q, the study found sub-optimal values for
these quantities as follows. Taking approximate values for c0(Q) and a0(Q), based on a
uniform cross-section river at uniform slope, the model was calibrated for a variable lateral
inﬂow. After smoothing the calculated values of the lateral inﬂow with a low-pass ﬁlter
for greater realism, the equations were then re-solved for the functions c0(Q) and a0(Q).
The methodology used highlights the identiﬁability problems between the functions of the
main channel ﬂow and the lateral inﬂow.
2.6
Requirements of current research
It is clear from the foregoing, that although there has been much activity in the area of
calibration of hydrological and hydraulic models, the problem is far from solved.
A parameterisation method is required which is statistically coherent, treats diﬀerent error
sources separately, is suitable for calibration with time series data, or with satellite images,
or both, and is also suitable for risk analysis; in other words produces a calibration which
is not conditional on the input time series.
A method which has been applied with some success in other ﬁelds is that proposed,
separately, by Craig et al. (2001) and Goldstein and Rougier (2004, 2006, 2009), and
by Kennedy and O’Hagan (2001a, 2001b). This method has been developed for computer
models with stationary output which varies over a spatial domain, and takes as its starting
point a description of model bias as a stochastic distribution, distributed as correlated
Gaussian, conditional on the locations where data measurements are made.
The two
approaches diﬀer in that that the method of Kennedy and O’Hagan uses a fully Bayesian
representation, fully specifying prior and posterior distributions, while the method of Craig
et al. (2001) and Goldstein and Rougier (2004, 2006) uses a Bayes Linear representation
(Goldstein and Wooﬀ, 2007), which speciﬁes only distribution means and variances, but
is quicker and more stable in computation.
The additional feature of this method is that it can easily incorporate an emulator for the
computer model being calibrated; this permits the undertaking of the model calibration,
and calibrated prediction, using a limited number of calls to the model, which makes a
signiﬁcant saving in computational time for all but the simplest of models.
In the work which follows, the method of Kennedy and O’Hagan is applied to the cali-
bration of hydraulic models for ﬂuvial ﬂooding, using both satellite image and time series
measurements.

Chapter 3
Bayesian analysis of computer
code output
The methodology under investigation is one to calibrate deterministic computer models
in the presence of model structural inadequacy, and has been developed by Kennedy and
O’Hagan (2001a, 2001b), drawing on previous work by Sacks et al. (1989).
The data model considered is
z = M(x, θ) + δ(x) + ϵ
where the observed data z are related to the output of the model M(x, θ) at input x with
parameters θ, subject to errors caused by model structural inadequacy δ, and observation
ϵ.
Distinction is made between the contributions of model inadequacy and observation error
to observed data by considering that the observation errors are uncorrelated, while the
contribution of model structural error is not. Then, the parallel is drawn between obser-
vation errors, which can be described by a Normal distribution, and model inadequacy,
which is described by a Gaussian process, which assumes that the variation at a point
in physical space is Normal, but incorporates a covariance function to model the joint
variation through space.
This chapter shows how Gaussian process models can be employed in Bayesian model cal-
ibration to allow for model inadequacy, in the following steps. First, Gaussian processes
are described, and it is shown how they can be used for function interpolation and emula-
tion. The equations are then developed for Bayesian model calibration. It is shown how a
computationally expensive model can be replaced by a Gaussian process emulator, and the
equations are developed for calibration and for calibrated prediction. The Markov chain
Monte Carlo method is introduced for simulation from a probability distribution. Finally,
the methodology is demonstrated with respect to a simple algebraic example, identifying
potential pitfalls in its use for real problems.
23

Chapter 3. Bayesian analysis of computer code output
24
3.1
The use of Gaussian processes for interpolation and em-
ulation
A Gaussian process is a multivariate Normal distribution. Deﬁned over a d-dimensional
space, a Gaussian process can be described in a hierarchical fashion as
y ∼N(m(x), V (x, x′))
(3.1)
where the mean, m(x), and the covariance function, V (x, x′) are deﬁned separately. For
a stationary process, it may be assumed that the covariance between the two points
x and x′ is dependent on their separation; thus, the covariance could be described as
σ2r(x −x′), where r is a correlation function, and r(0) = 1. As an example, one possible
function to describe the covariance between points evaluated at two locations x and x′ is
an autoregressive relationship,
V (x, x′) = σ2|x −x′|ρ.
(3.2)
Another is the negative squared exponential function,
V (x, x′) = σ2exp
 −(x −x′)T Ω(x −x′)

(3.3)
where Ωis a positive semideﬁnite matrix, often taken to be diagonal, under the assumption
that the diﬀerent dimensions of the data are separable. In the above, the hyperparame-
ters σ2 representing variance, and the autoregressive coeﬃcient ρ, in the case of Equation
(3.2), or in the case of Equation (3.3), the coeﬃcients ω of Ω, representing rate of variation
(“roughness”, as described by Kennedy and O’Hagan) of the Gaussian process correction
term, are to be determined. This last form (Equation (3.3)) is useful to describe smoothly
varying functions, and has the advantage that all orders of derivatives exist and are con-
tinuous.
The properties of conditional Gaussian distributions are then used to describe the distri-
bution at other locations, dependent on the measured or modelled values, since if x, y are
partitioned into (x1, x2) and (y1, y2),
 
y1
y2
!
∼N
  
m(x1)
m(x2)
!
,
 
V11
V12
V21
V22
!!
where
 
V11
V12
V21
V22
!
=
 
V (x1, x′
1)
V (x1, x′
2)
V (x2, x′
1)
V (x2, x′
2)
!
then if y2 are known to have a value ˜y, the conditional distribution is given by (e.g.
Anderson, 1958, p36)
(y1|y2 = ˜y) ∼N
 m(x1) + V12V −1
22 (˜y −m(x2)), V11 −V12V −1
22 V21


Chapter 3. Bayesian analysis of computer code output
25
Figure 3.1 illustrates a Gaussian process, with mean sin(2πx), and covariance a one-
dimensional version of Equation (3.3), V (x, x′) = σ2 exp(−ω(x −x′)2), conditonal on
speciﬁc values of the mean function, under diﬀerent assumptions for known values of the
“roughness” coeﬃcient, ω. The values have been chosen for illustration; it can be seen that
as ω increases, the rate of variation of the individual Gaussian process draws increases;
while for large ω, the width of the 95% probability intervals away from known data points
is dependent on the standard deviation σ, for small ω the width is limited by the high
correlation between points in the x-direction.
G
G
G
G
G
0.0
0.2
0.4
0.6
0.8
1.0
−1
0
1
2
omega= 15
G
G
G
G
G
0.0
0.2
0.4
0.6
0.8
1.0
−1
0
1
2
omega= 50
G
G
G
G
G
0.0
0.2
0.4
0.6
0.8
1.0
−1
0
1
2
omega= 150
G
unknown function
known values
Gaussian Process instances
95% probability intervals
Figure 3.1: Illustration of a Gaussian process, with given values of ω and σ2, and known
mean function, conditional on given data points.
The mean function in Figure 3.1 was taken to be the known generating function for the
data points, to better illustrate the inﬂuence of the parameters ω and σ2. In practice,
the generating function is not known, so has to be estimated.
This can be achieved
by describing it as a regression, m(·) = h(·)T β, where h(·) = (h1(·), h2(·), . . . , hp(·))T is
some suitable basis, and β = (β1, β2, . . . , βp)T are regression coeﬃcients to be determined.
Figure 3.2 shows a more realistic illustration of the Gaussian process, based on an analytic
solution for the conditional distribution subject to a maximum likelihood estimate for
the “roughness” coeﬃcient.
The calculations have been performed using the BACCO

Chapter 3. Bayesian analysis of computer code output
26
computer package (Hankin, 2005), and the equations are detailed in the Appendix at the
end of this chapter. In this case, the mean function of the Gaussian process is a regression
on the basis (1, x). The ﬁrst frame in Figure 3.2 shows that the Gaussian process is not
well able to estimate the region of higher curvature away from the data points, but the
addition of a single extra point makes it possible to approximate the original data curve
suﬃciently closely that the range of the Gaussian process is not visually evident.
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
0.0
1.0
G
G
G
G
G
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
0.0
1.0
G
G
G
G
G
G
G
original function
data used
Gaussian process mean
95% probability interval
Figure 3.2: Estimated Gaussian approximation to given data points
3.2
The use of Gaussian processes for calibration
Suppose that a physical process ζ(x) is to be described, dependent on inputs x comprising
locations x1, · · · , xn, and using a model M(x, θ), which invokes parameters θ requiring
determination. It should be noted that θ does not necessarily include all the parameters
of model M; some may be suﬃciently well determined by other means that they are
included for the purposes of this analysis in x or even in M. The relationship between
the model and the physical process it represents can then be described at a location xi
by
ζ(xi) = M(xi, θ) + δ(xi)
where δ(x) represents the model inadequacy, dependent on the inputs x.
Ignoring for the time being any observation error in the inputs xi, the observed output zi
of the process ζ at xi can be described by
zi = M(xi, ˜θ) + δ(xi) + ϵi
(3.4)
where ˜θ is the “best estimate” value of the parameters θ, and ϵ represents observation

Chapter 3. Bayesian analysis of computer code output
27
error, assumed to be Gaussian, with zero mean and variance σ2
e.
Kennedy and O’Hagan suggest that δ(x) and its prior distribution are conveniently de-
scribed as Gaussian processes. As in expression (3.1), we can say
δ(·) ∼N(hδ(·)T βδ, Vδ(·, ·))
where as before, the mean model inadequacy is described as a regression hδ(x)T βδ on
some suitable basis functions hδ(·), and Vδ(·, ·) is a covariance function, described by
hyperparameters ψδ.
Let x = (xT
1 , xT
2 , . . . , xT
n)T be the vector of locations where observations have been made.
Then the likelihood of the observed data is
L

(zi −M(xi, θ)) |θ, βδ, ψδ, σ2
ϵ

∼N (Hδ(xi, θ)βδ, Σ)
where
Hδ(x) is a matrix representing the regression basis describing the Gaussian process mean,
whose ith row is hδ(xi)T
βδ are the regression parameters describing the Gaussian process mean
ψδ represents the parameters describing the Gaussian process covariance
σ2
ϵ is the observation noise variance, and
Σ = Vδ ((x, θ), (x′, θ′))+σ2
ϵ I is the covariance matrix, comprising the sum of the covariance
representing the model inadequacy function and that of the noise
Thus, subject to prior distributions f(θ, βδ, ψδ, σ2
ϵ ) on θ, the regression parameters βδ,
the hyperparameters ψδ and the observation noise σ2
ϵ , the posterior distribution is given
by
f
 zi|θ, βδ, ψδ, σ2
ϵ

∝
f(θ, βδ, ψδ, σ2
ϵ )Σ−1/2 exp
1
2 (zi −M(xi, θ) −Hδ(xi)βδ)T Σ−1 (zi −M(xi, θ) −Hδ(xi)βδ)

(3.5)
3.2.1
Model speciﬁcation when the computer model is expensive to eval-
uate
The solution of this equation requires repeated calls to the computer model, to evaluate it
with diﬀerent parameter estimates during an iterative numerical solution process. If the
model is of any computational complexity, solution rapidly becomes prohibitive. Kennedy
and O’Hagan suggest that the answer to this is to replace the model in equation (3.4) by a
further Gaussian process, deﬁned conditionally on the computer model output at a ﬁnite

Chapter 3. Bayesian analysis of computer code output
28
set of locations in the (input, parameter) space where it has been run; thus
η(x, θ) ∼N(m(x, θ), V (x, x′)) and
η(x∗, θ) = M(x∗, θ)
at locations x∗where the computer model has been run. The suggestion that the output
of a deterministic computer program can be described as a stochastic process is due to
McKay et al. (1979). Clearly, this is not the case, as repeated runs of a deterministic
program will yield the same output. However, before the program is run for a speciﬁc
input conﬁguration the output is not known, but may be approximated given the output
of previous runs, under the assumption of smoothly varying output at locations x.
If the vector of computer program output is denoted y, and the observed data z, then
these can be described by
y = η(x∗, θ)
z = η(x, ˜θ) + δ(x) + ϵ
(3.6)
where ˜θ is the unknown true value of θ.
Distinguishing between the Gaussian pro-
cesses
η(x∗, θ) ∼N

H1(x∗, θ)β1, V1
 (x∗, θ), (x∗′, θ′)

and
δ(x) ∼N

H2(x)β2, V2(x, x′)

where H1 and H2 are deﬁned as the matrices whose ith rows are the regression bases
h1(xi, θi)T and h2(xi)T respectively, then Equation (3.6) becomes
y ∼N

H1(x∗, θ)β1, V1
 (x∗, θ), (x∗′, θ′)

z ∼N

H1(x, ˜θ)β1, V1
 (x, ˜θ), (x′, ˜θ)

+ N

H2(x)β2, V2(x, x′)

+ N

0, σ2
ϵ

Combining these two, deﬁne
d =
 
y
z
!
then
d ∼N

Hβ, V

where
H =
 
H1(x∗, θ)
0
H1(x, ˜θ)
H2(x)
!
β =
 
β1
β2
!

Chapter 3. Bayesian analysis of computer code output
29
and
V =
 
V1
 (x∗, θ), (x∗′, θ′)

V1
 (x∗, θ), (x′, ˜θ)

V1
 (x, ˜θ), (x∗′, θ′)

V1
 (x, ˜θ), (x′, ˜θ′)

+ V2
 x, x′
+ σ2
ϵ I
!
and H1 and H2 are deﬁned above, and V1 and V2 are characterised by hyperparameter
sets ψ1 and ψ2 respectively.
Thus the likelihood is
L
 d|θ, β, ψ1, ψ2, σ2
ϵ

∝V −1/2 exp
1
2
 d −H(x)β
T V −1 d −H(x)β

(3.7)
3.2.2
Integrating out the linear regression
Solution of Equation (3.5) or Equation (3.7) requires integrating out the parameters βδ
and the hyperparameters ψδ, and σ2
ϵ . Solution is facilitated by removal of the regression
coeﬃcients βδ since these are often highly correlated with each other (if more than one) and
with the θ parameters. Higdon et al. (2004) scale the problem to remove the β parameters,
eﬀectively taking point estimates from pre-analysis, and allowing the Gaussian processes
to absorb the distributional uncertainty. The approach taken by Kennedy and O’Hagan
(2001a, 2001b) is to integrate out the β parameters under the assumption of an improper
uniform prior distribution, and to solve the problem conditionally on this assumption.
Taking expression (3.5) and recognising that
 z −H(x)β
T Σ−1 z −H(x)β

= (β −ˆβ)T H(x)T Σ−1H(x)(β −ˆβ) + constant terms
where ˆβ = (H(x)T Σ−1H(x))−1H(x)T Σ−1z is the classical least squares solution to the
linear equation z −H(x)β ∼N(0, Σ) (e.g. O’Hagan and Forster, 2004), expression (3.5)
can then be integrated with respect to βδ leading to
L
 z|θ, ψδ, σ2
ϵ

∝W1/2
Σ1/2 exp
1
2(z −M(x, θ) −Hδ(x) ˆβδ)T W−1(z −M(x, θ) −Hδ(x) ˆβδ)

(3.8)
where
βδ ∝N( ˆβδ, W),
with
ˆβδ = WHT
δ Σ−1z,
and
W =
 HT
δ Σ−1Hδ
−1
A similar treatment of expression (3.7) leads to
L
 d|θ, ψ1, ψ2, σ2
ϵ

∝W 1/2
Σ1/2 exp
1
2(d −H(x)ˆβ)T W −1(d −H(x)ˆβ)

(3.9)

Chapter 3. Bayesian analysis of computer code output
30
where
β ∝N(ˆβ, W),
with
ˆβ = WHT V −1d,
and
W =
 HT V −1H
−1
3.2.3
Speciﬁcation of priors
Other than the prior on the regression parameters β, the remaining priors needed to specify
the problem are the priors on the computer model parameters θ, the hyperparameters
ψ of the model inadequacy and the emulator if included, and of the observation error
variance, σ2
ϵ . Under the assumption that the observation error is a function only of the
data recording mechanism, and that error values are uncorrelated and independent of the
data values, its variance should be fairly well known from the observation mechanism. It
is to be expected that the prior on the parameters are independent of the priors on the
model inadequacy, and both of these are expected to be independent of the priors on the
emulator.
3.2.4
Solution of equations
Kennedy and O’Hagan point out that analytical solution of Equation (3.9) is not feasible,
and instead ﬁnd the hyperparameters by optimisation. Although they derive an expression
for the distribution of the computer model parameters θ conditional on these optimal
values, they do not evaluate it, concentrating instead on the calibrated prediction of the
output. This solution method has been implemented in R (R Development Core Team,
2009) by Hankin (2005) for a smooth covariance function (Equation (3.3)), using Markov
chain Monte Carlo (described in Section 3.3 below) to evaluate the posterior distribution
of the computer model parameters θ.
The alternative is to simulate from the complete posterior distribution (Equation (3.8)
or Equation (3.9)) by MCMC. This is the approach primarily taken in this study. The
Markov chain is constructed with reference to the logs of the hyperparameters, ﬁrstly
because the transformation ensures positive values of the hyperparameters, and secondly
because the hyperparameter values can vary by several orders of magnitude.
3.2.5
Calibrated prediction
Once the conditional posterior distribution of the data has been found by simulation,
output of the process can be estimated at any input location x†, using the form of the
conditional Gaussian distribution. Thus, for the formulation with direct calls to the com-

Chapter 3. Bayesian analysis of computer code output
31
puter model (Equations (3.8)),
E

ζ(x†)|z, θ, ψδ, σ2
ϵ

= M(x†, θ) + hδ(x†, θ)T ˆβδ + τ(x†, θ)T Σ−1 
z −M(x, θ) −Hδ(x, θ)ˆβδ

Var

ζ(x†)|z, θ, ψδ, σ2
ϵ

= Vδ(x†, x†) −τ(x†, θ)T Σ−1τ(x†, θ) + ΛT WΛ
(3.10)
where in addition to variables previously deﬁned
Λ =
 hδ(x†, θ) −Hδ(x, θ)T Σ−1τ(x†, θ)

, and
τ(x†, θ) = Vδ
 (x†, θ), (x, θ)

The formulation where the computer model is replaced by an emulator (Equations (3.9))
proceeds similarly:
E

ζ(x†)|d, θ, ψ, σ2
ϵ

= h(x†, θ)T ˆβ + t(x†, θ)T V −1 
d −H(x, θ)ˆβ

Var

ζ(x†)|d, θ, ψ, σ2
ϵ

= V1
 (x†, θ), (x†, θ)

+ V2(x†, x†) −t(x†, θ)T V −1t(x†, θ) + LT WL
(3.11)
where
L =
 h(x†, θ) −H(x, θ)T V −1t(x†, θ)

,
h(x†, θ) =
 
h1(x†, θ)
h2(x†)
!
, and
t(x†, θ) =
 
V1
 (x†, θ), (x, θ)

V1
 (x†, θ), (x∗, θ)

+ V2
 (x†, x∗)

!
,
and the other variables are as deﬁned previously.
The conditional posterior expectation and variance are combined to give the unconditional
expectation and variance of the required quantity. The computation is implemented using
Markov chain Monte Carlo, which is described below.
3.3
Markov chain Monte Carlo
Many problems formulated in the Bayesian paradigm, as this one, are not amenable to
analytical description of the posterior distribution. Under these circumstances, the poste-
rior distribution must be characterised by simulation. This can be done by Monte Carlo
methods, by random draws from the prior distribution, accepting these with probabil-
ity proportional to the likelihood. However, the most eﬃcient way of simulating from
a distribution is by taking a Markov process, whose equilibrium distribution is that of
the posterior distribution we wish to simulate, and drawing from this process. A Markov
process is a sequence, with the property that any member is dependent only on the im-

Chapter 3. Bayesian analysis of computer code output
32
mediately preceeding member of the sequence:
P(Xi|X1, X2, · · · , Xi−1) = P(Xi|Xi−1)
Transition from one member of the chain to the next is characterised by a transition kernel,
thus:
P(Xi+1|Xi) = P(Xi|Xi−1)
If a Markov chain is both irreducible, and aperiodic, that is, if it is possible for the chain
to reach every part of the parameter space in a ﬁnite number of transition steps from any
starting point, then it is ergodic; in other words, it has a limiting stationary distribution.
This property enables a Markov chain to be used to simulate from a target distribution;
once the chain has achieved the stationary distribution, all further members will belong
to the distribution, permitting simulation from it. A suﬃcient condition for the chain
to have a speciﬁed invariant distribution f(·) is for it to possess the property of detailed
balance; that is that
f(θ)P(φ|θ) = f(φ)P(θ, |φ) for all θ, φ in the parameter space Θ
(3.12)
3.3.1
Metropolis Hastings algorithm
The Metropolis Hastings algorithm (Metropolis et al., 1953; Hastings 1970) provides a
simple way to ensure that a Markov chain possesses the detailed balance property. Given
the current state of the chain, Xi, a proposal X∗is generated from a proposal distribution
q(Xi, X∗). This proposal is accepted with probability
α{Xi, X∗} = min

1, f(X∗)q(Xi|X∗)
f(Xi)q(X∗|Xi)

,
(3.13)
and becomes the next member of the chain, Xi+1; otherwise, Xi+1 is set to Xi.
The
transition kernel density is K(Xi, X∗) = q(Xi, X∗)α(Xi, X∗);. It can easily be seen that
this transition kernel satisﬁes the detailed balance equation (3.12), with f as an invariant
distribution; thus convergence is guaranteed. In addition, it is not necessary to compute
the denominator in Bayes equation (1.3), as if the detailed balance property holds, this
cancels out from the acceptance relationship, Equation (3.13).
Diﬀerent proposal distributions q(·|·) can be used. A common choice is to take X∗=
Xi + ϵ, where ϵ is a random increment independent of Xi. This is known as random walk
Metropolis-Hastings algorithm, and the distribution of ϵ is commonly taken as uncorrelated
multivariate Normal.
Although convergence to the target distribution is guaranteed by the Metropolis Hastings
scheme, this does not imply that convergence will occur in a reasonable number of iter-
ations. The variance of the proposal increment ϵ needs to be prespeciﬁed, and too small
a step leads to inadequate exploration of the parameter space, while too large a stepsize
results in a small acceptance rate at Equation (3.13). Common practice is to run short

Chapter 3. Bayesian analysis of computer code output
33
pilot chains, to ensure that the stepsize is appropriate. Although Roberts et al. (1997)
suggested that for multivariate Normal distributions, the optimal acceptance rate to per-
mit adequate exploration of the parameter space and reasonable convergence rates, should
be approximately 23%, in reality, for more complex distributions, a lower rate has to be
accepted. For a multivariate distribution, diﬀerent step sizes may well be required in each
dimension. For highly correlated or skewed variable distributions, it is best to apply the
algorithm to transformed variables.
3.3.2
MCMC diagnostics
One diﬃculty with using MCMC methods, is to know how long the chain has to be. Al-
though convergence to a stationary distribution is guaranteed by the Metropolis Hastings
scheme, there is no theoretical indication as to how long this will take. In addition, when
proposal distributions are based on the current state of the chain, it can be expected that
there is signiﬁcant autocorrelation in the chain, aﬀecting the length required for an eﬀec-
tive estimate of the parameter distributions. Apart from visual inspection of the chains,
their cumulative sums and their correlations, a number of methods have been suggested
to identify the length of the “burn-in” period, and to estimate the sample size required
for eﬀective estimation of distributional properties (e.g. Cowles and Carlin, 1996). The
CODA package (Best, Cowles and Vines, 1995) incorporates several of these for analysis
of the output chain.
A test for stationarity of the chain was proposed by Geweke (1992), who used spectral
methods to compare the means of the ﬁrst 10% and the last 50% of the chain under the
assumption that the two parts of the chain are asymptotically independent. This method
is applied to a single variable at a time.
Gelman and Rubin (1992) pointed out that it may not be possible to detect very slow
convergence from a single chain, while by taking independent chains starting at a sample
of points, convergence to a single distribution gives greater conﬁdence. They proposed
a diagnostic based on the comparison of the covariances of individual chains and the
covariances between chains.
Raftery and Lewis (1996) proposed an estimator for the length of chain necessary to
estimate a quantile of the posterior distribution with the required conﬁdence. Taking the
qth quantile u of a distribution U of a function of the parameters in the chain, they form
a new chain Z : zt = I(U ≤u), where I(·) is the indicator function. While Zt is not a
Markov chain, it can be expected to behave like one if thinned to every kth value, for large
enough k; the Raftery and Lewis algorithm gibbsit identiﬁes the lowest value of k required,
and uses this to estimate both the length of the burn-in period, and the chain length for
estimation of the required quantile. It should be noted that the estimate may change if
the chain is extended, so new estimates must be calculated, until they are shorter than
the actual chain length.
R-CODA (Plummer et al., 2006) implements a modiﬁed version of the method of Heidel-

Chapter 3. Bayesian analysis of computer code output
34
berger and Welch (1981) to examine the autocorrelation behaviour of the chain, ﬁtting
a generalised linear model to the lower part of the spectrum, and making it possible to
estimate the eﬀective length of the chain.
It is generally considered that no method for analysing MCMC performance is foolproof,
and it is considered prudent to use a number of methods in parallel, to ensure the validity
of the MCMC output.
An example of the problems which can occur in MCMC is demonstrated in Figures 3.3,
showing the chains and 3.4, showing their correlations. This is in fact a solution to the toy
example described in the next section, but with deliberately poor step sizes for the proposal
distributions, and without integrating out the regression parameter. Thus, the variables
are the parameter θ, the regression parameter β, the log model inadequacy “roughness”
parameter ωδ, and the log variances for the model inadequacy σ2
δ and the observation noise
σ2
ϵ , with priors taken as uniform on a limited range (see Table 3.1), to be able to see the
posterior distributions. The fact that this is a comparatively straightforward distribution
can be seen in that the variable values settle down almost immediately into distributions
seen thoughout the rest of the chain.
Table 3.1: Prior ranges for MCMC example
variable
range
θ
[ -1, 5 ]
βδ
[ -6, 6 ]
log(ωδ)
[ -2, 6 ]
log(σ2
δ)
[-18, 5 ]
log(σ2
ϵ )
[-18, -2]
The ﬁrst two variables, θ and β can be seen to be highly correlated, both from the corre-
lation plots, and also the symmetry between their chains. This results in a low acceptance
rate of the proposal, giving rise to the step-like behaviour seen in the ﬁrst two chains,
and the irregular appearance of the corresponding distributions shown in the the diagonal
plots in the correlation ﬁgure. The third and fourth variables log(ωδ) and log(σ2
δ) are
poorly identiﬁed; the posterior distributions can be seen to be strongly inﬂuenced by the
priors. The proposal step size given to log(ωδ) is larger than that given to log(σ2
δ); this
is responsible for the very diﬀerent appearance of the chains. By contrast with the other
variables, the chain for log(σ2
ϵ ) is fairly well behaved. Lastly, there are positions in the
chain where all of the variables stop moving together, resulting in local isolated peaks in
the correlation plots, and while at about 8500 this appears to be associated with extreme
values of θ and β, this is not always the case. This last behaviour occurred for calibration
problems in a number of applications of the methodology, and while the length of these
ﬂat patches in the current example is not large, for more diﬃcult problems it can become
extended.

Chapter 3. Bayesian analysis of computer code output
35
0
3
−3
0
3
−2
2
6
−15
−5
0
2000
4000
6000
8000
10000
−5
−3
−1
Figure 3.3: Example chains to show MCMC problems
−3
−1
1
3
0.95
0.047
−15
−5
0
0.071
0
2
4
0.016
−3
0
2
0.064
0.084
0.012
0.069
−2
2
4
6
0.039
−15
−5
0
0.062
0
2
4
−2
2
4
6
−5
−3
−1
−5
−3
−1
Figure 3.4: Example correlations corresponding to the chains in Figure 3.3: Upper right,
pairwise correlation; Diagonal, individual variable density; Lower left, 2-dimensional den-
sity plots
3.3.3
Reﬁnements of the Metropolis-Hastings algorithm
A reﬁnement to the simple random walk Metropolis Hastings algorithm described above
was suggested by Haario et al. (2001). This algorithm uses the empirical covariance of the
entire chain, up to the current point, to inform the (still multivariate Normal) distribution
of the increment ϵ for the proposal. Thus the increment for each variable is appropriate to
the scale of variation of that variable. It is necessary to use the entire chain to generate the
proposal distribution to maintain ergodicity; Andrieu and Thoms (2008) pointed out that

Chapter 3. Bayesian analysis of computer code output
36
a necessary condition for this is that the adaptations to the proposal distribution vanish
as the step number i →∞. Marshall et al. (2004) observed that the adaptive algorithm
of Haario et al. (2001) provides substantial improvement in convergence properties over a
simple random walk Metropolis Hastings algorithm in the calibration of a rainfall-runoﬀ
model.
Vrugt and co-workers have implemented two MCMC methods, designed to simulate from
diﬃcult distributions, both employing parallel and interacting chains; each of these in-
corporates a number of performance-improving characteristics. The ﬁrst of these (Vrugt
et al., 2003) draws inspiration both from the algorithm of Haario et al. (1999, 2001),
and from the successful shuﬄed complex evolution optimisation method of Duan et al.
(1992), running a number of parallel chains. In a reﬂection of the work of Duan et al.,
the algorithm maintains a “complex” of candidate points for each chain, which are used
to choose the next point in each of the parallel chains. The method of choosing a new
point reﬂects the early algorithm of Haario et al. (1999) in using the mean and covariance
last m members of the complex to base the multivariate Normal candidate distribution
for the new point. As in the work of Duan et al., the complexes are periodically shuﬄed,
so that the chains are not independent. The algorithm appears fast and eﬃcient, but is
ﬂawed, since it uses the covariance of the last m members of the chain to calculate the new
candidate point. Haario et al. (2001) pointed out this ﬂaw in their earlier work, proving
that it contravenes the conditions necessary to ergodicity of the chain.
A second algorithm, DREAM (Vrugt et al., 2008), was introduced to tackle larger param-
eter sets encountered when solving explicitly for input errors in hydrological models. It
incorporates a number of separate methods to improve the robustness and convergence
speed of the Metropolis-Hastings algorithm. As with the previous method, the inspira-
tion for the proposal distribution came from the optimisation literature. The diﬀerential
evolution method (ter Braak, 2006) involves running parallel chains, but updating each
one using the diﬀerence between a randomly chosen pair from the others. DREAM goes
one step further, using the mean of the diﬀerences between a number of randomly chosen
pairs of separate chains. In the case where the proposal is rejected, DREAM employs the
delayed rejection algorithm of Haario et al. (2006), rejecting the update only if a second,
less stringent acceptance test is failed. In addition, Vrugt et al. note that this method
admits the possibility of a single chain which is substantially diﬀerent from the rest of
the population, and replace such chains with others which are more similar. The method
appears fast and eﬃcient.
In the work described in this thesis, both of the algorithms described above by Vrugt
have been used at diﬀerent times, in model development. It proved helpful to have access
to more robust and eﬃcient codes at the stage where the models were not completely
speciﬁed. However, all the results in shown the thesis have been achieved with a simple
random walk Metropolis-Hastings code, with ﬁxed time step, since, correctly speciﬁed,
the problems here do not present such a diﬃcult task for a Markov chain Monte Carlo
code.

Chapter 3. Bayesian analysis of computer code output
37
3.4
Application: simple algebraic example
The methodology is illustrated with respect to a simple algebraic example. Data were
generated by the process
y = ex −1
in the range x ∈[0, 1].
The computer program used to describe the data follows the
relationship
y = θx2
where θ is the parameter to be determined. Observations were measured at each of 5
points, (0.1, 0.3, 0.5, 0.7, 0.9), subject to additive Gaussian noise. Initially, the simulator
was used directly in the calculations, solving the model described in Equations (3.5), (3.8)
and (3.10). The regression basis for the model inadequacy function was taken to be (x),
and a smooth covariance was chosen, as in Equation (3.3). The equations were solved,
using MCMC, for the the parameter θ and the logs of the variables ω, σ2
δ and σ2
ϵ , and to
estimate the calibrated prediction of the generating process.
Prior distributions are summarised in Table 3.2, and were chosen as follows: for the
parameter θ, it was assumed that the ﬁnal value would be approximated by the value
found in a classical least squares solution to the equation y = θx2, using the data; thus
θ ∼N(1.94, 1). It was assumed that the data standard deviation was in the region of 0.05,
and the model inadequacy standard deviation was assumed to be approximately twice this;
thus log(σ2
δ) ∼N(−4.6, 2), and log(σ2
ϵ ) ∼N(−6, 2). In order to make an initial judgement
of the “roughness” parameter ωδ, a range was taken by comparison with representations
of zero-mean Gaussian processes (Figure 3.5), following the suggestion of Oakley (2002);
thus log(ωδ) ∼N(2.3, 2).
1.60
1.80
omega= 0.1
y
−0.1
0.1
0.3
omega= 0.2
−1.0
−0.8
omega= 0.5
−0.3
0.0
omega= 1
−2.6
−1.8
omega= 2
−0.5
1.0
2.0
omega= 5
−1.5
−0.5
omega= 10
−1
1
2
omega= 20
−1.5
−0.5
0.5
omega= 50
1.5
1.7
1.9
y
1.30
1.50
−1.0
−0.6
−0.2
−0.35
−0.20
−1.5
−0.5
−0.5
0.5
−0.5
0.5
−0.5
1.0
2.0
−1.0
0.5
0.0
0.4
0.8
1.0
1.4
1.8
x
y
0.0
0.4
0.8
−0.7
−0.4
−0.1
x
0.0
0.4
0.8
0.0
0.3
0.6
x
0.0
0.4
0.8
−1.0
0.0
x
0.0
0.4
0.8
0.0
1.0
2.0
x
0.0
0.4
0.8
−1.0
0.0
x
0.0
0.4
0.8
−0.5
0.5
x
0.0
0.4
0.8
0.0
1.0
x
0.0
0.4
0.8
−4
−2
0
2
x
Figure 3.5: Reference Gaussian processes for use in determining prior distributions for
“roughness” values: three realisations of each.
Calibration was undertaken with two measurements at each data point, subject to noise
of standard deviation 0.05. Posterior distributions were found by simulating from dis-

Chapter 3. Bayesian analysis of computer code output
38
Table 3.2: Prior and posterior distributions for calibration
variable
prior distribution
posterior mean
posterior
standard deviation
θ
N(1.94, 1)
1.31
0.48
βδ
0.50
0.17
log(σ2
δ)
N(−4.6, 2)
-5.43
1.26
corresponds to
ˆσδ = 0.066
log(ωδ)
N(2.3, 2)
2.00
2.58
log(σ2
ϵ )
N(−6, 2)
-4.81
0.65
corresponds to
ˆσϵ = 0.090
tribution given by Equation (3.8), using a chain of length 20000. None of the problems
demonstrated in Figures 3.3 and 3.4 were experienced to any signiﬁcant extent; chains
and correlations in this case are shown in Figures 3.6 and 3.7. Note that there are here
only four variables represented, since the regression coeﬃcient βδ has been integrated out
of Equation (3.8).
0
2
−2
2
6
−10
−4
0
0
5000
10000
15000
20000
−6
−4
−2
Figure 3.6: Chains for toy example
Posterior distributions are summarised in Table 3.2. The posterior distribution of the
parameter θ has mean 1.31 and standard deviation 0.48 (Figure 3.8), and is very diﬀerent
from the estimate by classical methods, which corresponds with the prior mean.
Calibrated prediction was performed by conservatively discarding the ﬁrst quarter of each
chain, and for each remaining member, calculating its contribution to the calibrated pre-
diction, using Equation (3.10). It is clear that the calibrated prediction follows the data
closely (Figure 3.9a), with the mean width of the 95% prediction interval 0.065.
For comparison, the analysis was repeated with 4 measurements at each data point (Figure
3.9b). In this case, the posterior mean of θ was lower, at 1.11, and standard deviation was
0.37. The estimated noise standard deviation was somewhat reduced, at ˆσϵ = 0.068, with
the mean width of the 95% prediction interval 0.041.
It should be noted that the interpretation of the prediction interval in Bayesian statistics

Chapter 3. Bayesian analysis of computer code output
39
theta
−2
2
6
0.017
0.17
−6
−4
−2
0
1
2
3
0.012
−2
2
6
log(omegad)
0.18
0.038
log(sigsqd)
−10
−4
0
0.09
0
1
2
3
−6
−4
−2
−10
−6
−2
log(sigsqe)
Figure 3.7: Correlations corresponding to the chains in Figure 3.6: Upper right, pairwise
correlation; Diagonal, individual variable density; Lower left, 2-dimensional density plots
is diﬀerent to that used in classical statistics. Since the solution of the Bayes’ scheme leads
to a probability distribution for the required variables, this is most easily comprehended
in relation to the original research question by specifying the mean of the distribution, and
an interval within which the variable lies, with a speciﬁed probability p. By contrast, the
conﬁdence interval of classical statistics is the interval within which the mean lies, with
probability p.
3.4.1
Identiﬁability of the solution with sparse data
The above solutions to the calibration problem were undertaken with replicated data,
and demonstrated that replication provides suﬃcient data to identify adequately the noise
variance.
A more realistic test of the type of situation which may be encountered in
practice in calibration of a hydraulic model, is an example where only one measurement is
able to be made at each data point. Solving the calibration problem in this case is much
more diﬃcult, illustrating identiﬁability issues in the formulation.
Poor identiﬁability occurs when the data do not support full identiﬁcation of the param-
eters. In the case of the Kennedy and O’Hagan formulation, there may be diﬃculties in
distinguishing between the model inadequacy function and the observation error on the
measured output data, as originally suggested by Wynn (2001). Even when a parameter
is poorly identiﬁed by the data, the problem is not necessarily ill-posed; provided that
the prior distributions are adequately precisely deﬁned, it is still possible to achieve a
solution.

Chapter 3. Bayesian analysis of computer code output
40
θ
Probability density
0
1
2
3
4
0.0
0.2
0.4
0.6
0.8
Figure 3.8: Prior and posterior distributions for parameter θ
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x
y
G
G
G
G
G
G
G
G
G
G
a)
10 data points
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x
y
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
b)
20 data points
G
reality
data
least squares best fit model
calibrated prediction: mean
95% prediction interval
Figure 3.9: Calibrated prediction of algebraic example, for a) 10 data points and b) 20
data points
In order to try and ﬁnd out what inﬂuences the identiﬁability of the hyperparameters,
the likelihood (Equation (3.8)) was calculated on a grid of values of log(ωδ), log(σ2
δ) and
log(σ2
ϵ ), taking values of β and θ corresponding to the classical least squares solution.
Figure 3.10 represents two series of slices through the (log(ωδ), log(σ2
δ), log(σ2
ϵ ))-space, the
ﬁrst row parallel to the log(σ2
ϵ )-plane, and the second to the log(σ2
δ)-plane for a given
realisation. It can be seen that there appear to be two competing solutions, for log(σ2
δ) ≈
−6, log(ωδ) ≈2.5, and log(σ2
ϵ ) < −10, and the second for log(σ2
δ) ≈−6, log(ωδ) ≈1.5, and
log(σ2
ϵ ) ≈−7.5, interpreting the residual with diﬀerent proportions of model inadequacy
and noise.
Further investigation of the likelihoods in this way revealed that for a number of realisa-
tions, one or other of the hyperparameters appeared to be unidentiﬁable. This appears
to happen when one of the σ2 variables is at least 2 orders of magnitude greater than the
other. In this case, the smaller σ2 value makes little diﬀerence to the total variance. If

Chapter 3. Bayesian analysis of computer code output
41
−7.0
−6.0
−5.0
1.0
2.0
3.0
log(σd
2)
log(ωd)
log(σe
2)
           = −25
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −22.5
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −20
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −17.5
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −15
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −12.5
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −10
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −7.5
−7.0
−6.0
−5.0
log(σd
2)
log(σe
2)
           = −5
−25
−15
−5
1.0
2.0
3.0
log(σe
2)
log(ωd)
log(σd
2)
           = −7
−25
−15
−5
log(σe
2)
log(σd
2)
           = −6.75
−25
−15
−5
log(σe
2)
log(σd
2)
           = −6.5
−25
−15
−5
log(σe
2)
log(σd
2)
           = −6.25
−25
−15
−5
log(σe
2)
log(σd
2)
           = −6
−25
−15
−5
log(σe
2)
log(σd
2)
           = −5.75
−25
−15
−5
log(σe
2)
log(σd
2)
           = −5.5
−25
−15
−5
log(σe
2)
log(σd
2)
           = −5.25
−25
−15
−5
log(σe
2)
log(σd
2)
           = −5
Figure 3.10: Greyscale plot of the likelihood over a 3-dimensional grid of hyperparameter
values
the smaller value is σ2
δ, then ωδ is also unidentiﬁable.
There are two other causes for the non-identiﬁability of ωδ. The ﬁrst is that with a large
enough value of ωδ, a one dimensional inadequacy function is indistinguishable from the
noise. Thus, for a spacing of 0.1 between data points, there is little eﬀective diﬀerence be-
tween uncorrelated noise, and correlated noise with a “roughness” of > O
 1
0.12

= O(100);
that is (log(ωδ) > 5). In addition, it should be noted that using a log parameterisation for
the hyperparameters may have the consequence that log(ωδ) is not well deﬁned for small
ωδ, for example.
It has already been noted that where there are identiﬁability problems in the speciﬁcation
of a Bayesian problem, it can still be well-posed if the priors are adequately speciﬁed.
However, the solution will follow the information in the prior, more or less closely, accord-
ing to how much information is in the likelihood. This was demonstrated in the third and
fourth variables of the MCMC example in Figures 3.3 and 3.4.
In the full solution of the problem, with a single data point at each of 5 equally spaced
locations (noise variance 0.05), Normal priors, variance 100 for the parameters and 10 for
the log hyperparameters, the posterior distributions of the parameter θ, the discrepancy
β, and the noise variance σ2
ϵ were reasonably well determined, and not much inﬂuenced by
the prior. However, the hyperparameters of the inadequacy function, σ2
δ and ωδ were quite
strongly inﬂuenced by the prior means. In addition, the priors had considerable inﬂuence
on the posterior prediction intervals, as shown in Table 3.3 below:
Table 3.3: Parameters aﬀecting the posterior credible interval
increase in
no increase in
posterior credible interval
posterior credible interval
θ variance
σ2
δ mean
β variance
σ2
δ variance
ωδ mean
σ2
ϵ mean
ωδ variance
σ2
ϵ variance

Chapter 3. Bayesian analysis of computer code output
42
3.4.2
Use of an emulator to represent the computer program
For the purposes of comparison, an emulator was used to represent the “computer pro-
gram” - in this case, the algebraic example, y = θx2. The “computer program” was run for
selected values of x and θ, and a regression was performed on the output values. Kennedy
and O’Hagan recommend using the simplest possible regression basis that is supported by
the data, suggesting that in many cases it is suﬃcient to use the mean computer output
value; that is, the regression basis is simply (1). This is in contrast to the approach of
Rougier et al. (2009), who point out that the better the regression basis, the less the eﬀect
of the choice of an inappropriate covariance function. In this case, the basis (1, x, θ) has
been taken.
It is advisable to test the accuracy of the emulator away from the points at which the
computer program has been run. In this case, since there are two independent variables, it
is not so simple to visualise the emulator error over the entire domain. Instead, a selection
of points has been chosen in the domain, and the emulator error has been calculated
for those. As for Figure 3.2, the calculations have been performed using the BACCO
package.
The ﬁrst frame of Figure 3.11 shows a contour map of the emulator mean response surface,
identifying points where the computer program has been run, and where the emulator error
has been tested. The second frame shows the 95% error range compared with the exact
function at a number of validation points that were not used for training, marked in
Figure 3.11a by circles. It can be seen that both the mean error and the uncertainty in
the emulator are low, but are higher further away from data points, particularly close to
the edge of the domain where the prediction intervals expand rapidly.
3.4.3
Experimental design
When characterising computer code output to construct an emulator, it is necessary to
decide at which input values the the code should be run in order to ensure that the
emulator faithfully represents the computer code output throughout the input domain.
This question was discussed by Sacks et al (1989), although no solution was oﬀered in the
context of emulator formulation.
The issue of deriving statistics from output of computer code sample runs was discussed by
McKay et al. (1979), who devised the Latin hypercube sampling strategy. This concerns
the choice of a sample of size N from a K dimensional input domain [0, 1]K. For each
dimension k ∈{1, . . . , K}, the value of the ith sample (i ∈{1, . . . , N}) is taken to be
at
πk(i) −U(0, 1)
N
where U(·, ·) refers to the uniform distribution, and πk(i) refers to the ith member of a
permutation of the N intervals, the suﬃx emphasizing that a diﬀerent permutation is
taken for each dimension k. McKay et al. showed that a Latin hypercube sample provides

Chapter 3. Bayesian analysis of computer code output
43
x
theta
 0 
 0 
 0.5 
 1 
 1.5 
 2 
 2.5 
0.0
0.2
0.4
0.6
0.8
1.0
1.0
1.5
2.0
2.5
3.0
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.01
0.05
0.09
0.02
0.89
1.88
0
2.2
0.59
0.61
0.22
0.87
0.19
1.8
1.11
0.87
0.22
0.67
2.35
0.11
G
G
G
G
G
G
G
G
G
a
b
c
d
e
f
g
h
k
a)
a
b
c
d
e
f
g
h
k
−0.005
0.000
0.005
0.010
0.015
95% prediction intervals for emulator
b)
Figure 3.11: Testing the code emulator: a) contour map of the emulator mean response
surface; solid points are where the code has been run, circles are points where the emulator
accuracy has been tested, b) 95% prediction interval for the emulator at locations labelled
in a (central points are true code output values)
a more eﬃcient estimator of the mean and variance of the computer code output than a
random sample of the same size. Owen (1994) showed that the variance of estimates is
reduced when the covariance of the samples is controlled. In practice, what is often done
is to generate a large number of Latin hypercube samples, and to choose the one where
the arrangement of the samples is optimal in some sense, such as the minimum Euclidean
distance between samples. The Latin hypercube sampling scheme is extremely successful,
and widely used (indeed to generate the sample shown in Figure 3.11a). However, in the
choice of samples for emulator generation, other issues may be relevant.
One such issue is the decision at what scale sampling is to be undertaken. For example,
where the sample is to be used to generate a Gaussian process emulator, if samples are
taken too close together the resulting covariance matrix becomes ill-conditioned. However,
the samples must be taken suﬃciently closely to ensure that the emulator can represent
the variability of the computer code output.
An exploratory approach proposed by Morris (1991) involves taking a sample, which could
be a Latin hypercube sample, and taking a secondary sample, by making small perturba-
tions to the locations of the original sample members, allowing an assessment of the local
sensitivity to the diﬀerent parameters. Campolongo et al. (2007) suggested that Morris’
sample scheme should be optimised in the knowledge of the secondary sampling criteria
such as spread of the points, and suggested that this optimisation renders unnecessary the
use of a Latin hypercube scheme.
It is sometimes useful to be able to choose sample points sequentially, for example, in the
case where it is decided that another set of model runs can be made to increase emulator

Chapter 3. Bayesian analysis of computer code output
44
accuracy. The concept of space-ﬁlling sequences was introduced by Halton (1960), while
Sobol (1998) showed that use of such non-random sequences is a more eﬃcient base for
Monte Carlo simulations than random draws, but noted that the number of sampling
points should be increased by doubling. An alternative approach to increasing the size of
a sample was suggested by Sallaberry et al. (2008), who proposed a method of increasing
the size of a Latin hypercube sample to a multiple of the original sample size by generating
a new Latin hypercube sample, and subdividing the interval size so that the samples do
not interfere.
3.4.4
Eﬀect of the choice of regression basis on solution behaviour
An investigation has been made of the dependence of the calibrated prediction on the
choice of regression bases for the emulator and the model inadequacy function. In inter-
polation, provided that there are no excessively large gaps between the data points (either
code outputs in emulator generation, or measured data points), the Gaussian process fol-
lows the data, and its mean is unaﬀected by the regression basis, while the variance can be
reduced by increasing the data. However, in extrapolation, the Gaussian process follows
the regression, deviating from the trend of the data at a rate depending on the “rough-
ness” coeﬃcient, ω, and the solution adequacy thus depends on the regression basis, for
both emulation and calibration.
Figure 3.12 shows calibrated prediction for four cases, using the same data and priors (for
the calibration) as the example in Section 3.4. The ﬁrst frame shows the same solution
as the ﬁrst frame of Figure 3.9, extending the extrapolation further for illustration. In
this case, the calibrated mean eventually follows the path y = E(θ)x2 + E(β)x where
E(θ) = 1.31 and E(β) = 0.50. The rate at which the calibrated mean approaches this
asymptote is determined by E( 1
√ω), the length scale of the model inadequacy Gaussian
process, in this case, 0.50.
The second frame shows the equivalent solution when an
emulator is used to represent the computer program, solving equations (3.9), taking a
regression basis (1, x, θ) for the model inadequacy, and priors ω1 ∼N(, ), σ2
1 ∼N(, ). It
can be seen that the asymptote for the calibrated mean is linear, following y = E(β1,1) +
E(β1,θ)θ + (E(β1,x) + E(β2))x,
where
E(β1,1) = −0.68
E(β1,θ) =
0.69
E(β1,x) =
2.28
E(β2)
=
0.60
This behaviour is even more marked in the third frame, where the emulator regression
basis is taken as 1 instead of (1, x, θ). The asymptote for the calibrated mean is y =
E(β1,1) + E(β2)x,
where
E(β1,1) = 1.59

Chapter 3. Bayesian analysis of computer code output
45
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
2
4
6
8
10
12
x
y
G
G
G
G
G
G
G
G
G
G
a)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
2
4
6
8
10
12
x
y
G
G
G
G
G
G
G
G
G
G
b)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
2
4
6
8
10
12
x
y
G
G
G
G
G
G
G
G
G
G
c)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
−40
−20
0
20
x
y
G
G
G
G
G
G
G
G
G
G
d)
G
reality
data
least squares best fit model
calibrated prediction: mean
95% prediction interval
Figure 3.12: Eﬀect of diﬀerent emulator and model inadequacy regression bases on the
calibrated prediction in extrapolation: a)calibration only, model inadequacy basis (x); b)
emulator basis (1, x, θ), model inadequacy basis (x); c) emulator basis (1), model inade-
quacy basis (x); d) emulator basis (1, x, θ), model inadequacy basis (x3)
E(β2)
= 0.58.
It should be noted that the calibrated mean is just starting to approach the direction
of the asymptote at the right hand side of the frame.
The rate of approach depends
on the size of the hyperparameters ω1 and ω2; the smaller these are, the more slowly
the calibrated prediction will approach its asymptote.
In this case E(ω1) = 1.83 and
E(ω2) = 12.61; Figure 3.5 indicates that the corresponding variability of the Gaussian
process is not large.

Chapter 3. Bayesian analysis of computer code output
46
The fourth frame in Figure 3.12 shows the eﬀect of an inappropriate regression basis for
the model inadequacy. Although the calibrated prediction is almost identical to that of
the other three cases presented here within the range of the data, in extrapolation the
calibrated mean departs from reality, while the prediction limits increase rapidly.
3.5
Summary
The methodology of Kennedy and O’Hagan (2001a, 2001b) has been presented, for the
emulation and calibration of computer models using Gaussian process description of the
emulator and model inadequacy. The equations have been set out for computer model cal-
ibration both with and without use of an emulator. The method has been demonstrated
in terms of a simple algebraic example, ﬁrstly without emulator, and some practical issues
have been raised over the identiﬁability of the model inadequacy function and observation
errors. Finally, an indication has been given of the practical issues involved in the mod-
elling choices involved in the additional use of an emulator. In the next three chapters, the
methodology will be applied to three practical examples of ﬂood models, demonstrating
further the capabilities and diﬃculties of the method.
3.6
Appendix: Use of BACCO to estimate a Gaussian pro-
cess emulator
The method used in the BACCO code (Hankin, 2005) to estimate a Gaussian process
emulator is due to Oakley and O’Hagan (2002). They take as a model for the computer
output d, observed at n input values x = (xT
1 , . . . , xT
n)T , a Gaussian process, where
η(x) ∼N(m(x), V (x, x′)) with
m(x) = HT β,
H =




h(x1)
...
h(xn)




V (x, x′) = σ2c(x, x′), and
c(x, x′) = exp
 −(x −x′)T Ω(x −x′)

,
where Ωis a positive semideﬁnite matrix, and other variables were deﬁned earlier in the
chapter. Further, taking a prior on β and σ2 as
p(β, σ2) ∝σ−1
2 (r+q+2) exp

−(β −b)T B−1(β −b) + a
2σ2

,
where q is the dimensionality of β and a, r, b and B are to be determined by expert
elicitation, they show that at an unobserved input value x, the computer model output

Chapter 3. Bayesian analysis of computer code output
47
has distribution
η(x) −m∗(x)
ˆσ
p
c∗(x, x′)
|d, Ω∼tr+n,
a multivariate t-distribution where
m∗(x) = h(x)T ˆβ + t(x)T A−1(d −H ˆβ)
c∗(x, x′) = c(x, x′) −t(x)T A−1t(x′) +

h(x)T −t(x)T A−1H
	
B∗
h(x)T −t(x)T A−1H
	T
t(x)T = (c(x, x1) . . . , c(x, xn)) ,
HT =
 hT (x1), . . . , hT (xn)

,
A =







1
c(x1, x2)
· · ·
c(x1, xn)
c(x2, x1)
1
...
...
...
c(xn, x1)
· · ·
1







,
ˆβ = B∗(B−1z + HT A−1d),
ˆσ2 = a + bT B−1b + dT A−1d −ˆβ(B∗)−1 ˆβ
n + r −2
B∗= (B−1z + HT A−1H)−1,
dT = (η(x1), · · · , η(xn))
The speciﬁcation then depends on an estimate for the matrix of “roughness” coeﬃcients,
Ω. Assuming a single scale, ω, so that Ω= ωIq, this can be found by maximising the
likelihood,
L(ω|y) = ˆσ−(n−q)
2
|A|−1
2 |HT A−1H|−1
2
The standard weak prior p(β, σ2) ∝σ−2 (Jeﬀreys, 1961), is a special case of the above,
taking B = 0, a = 0 and r = −q. This is used in BACCO. However, in the case where the
above prior on β and σ2 is not appropriate, or the problem cannot be speciﬁed so that the
variance is described by a single scale over diﬀerent dimensions, the above semi-analytical
description is not applicable, and the parameters of the emulator are found by Markov
chain Monte Carlo.

Chapter 4
Calibration of steady state
laboratory experiments
4.1
Introduction
The previous chapter outlined a methodology for Bayesian calibration of computer models,
and demonstrated this in the context of a simple algebraic example. This chapter and
the following two demonstrate the application of the methodology to hydraulic models.
The work described in this chapter concerns the calibration of a steady state ﬂow model
using data obtained in large scale laboratory experiments. The model used is Manning’s
equation, which is a simple algebraic relationship, so that the use of an emulator is not
necessary for calibration. In tightly controlled laboratory conditions, measurement error
is small, but since the model does not adequately describe the data, a model inadequacy
function is required.
The experiments examined the eﬀect of a number of variables on the relationship between
ﬂow and depth.
These have been introduced successively into the formulation of the
model inadequacy, requiring the development of increasingly complex regression basis
functions.
4.2
Experimental data
The Flood Channel Facility (Knight and Sellin, 1987) was established during the 1980s
and 1990s at Wallingford as a cooperative venture between a number of University de-
partments, jointly funded by SERC and Hydraulics Research Ltd at Wallingford, with a
view to improving the understanding of the hydraulic processes involved in out-of-bank
river ﬂows. Three series of experiments were undertaken; on rigid straight channels, on
rigid meandering channels, and on straight channels with mobile boundaries. This study
is concerned with modelling the ﬁrst of these experimental series.
48

Chapter 4. Calibration of steady state laboratory experiments
49
The experimental ﬂume was constructed in concrete, 56m long, and with total width
10m (Figure 4.1, downloaded from http://www.ﬂowdata.bham.ac.uk/fcfa-photos.shtml).
The central channel was trapezoidal, with a depth of 0.15m, and a ﬁxed gradient of
nominally 1 in 1000. Horizontal ﬂoodplains were allowed on both sides of the channel,
and ﬂoodplain widths, and channel and ﬂoodplain sideslopes, were able to be varied.
In addition, it was possible to vary the roughness of the ﬂoodplain by the insertion of
obstacles. The facility was instrumented to measure water levels, discharge, boundary
shear stress, velocity proﬁles and turbulence.
A single test series was undertaken of in-bank ﬂow, consisting of measurements for ﬂows
at a number of diﬀerent levels. For out-of-bank ﬂow, several test series were measured. In
this context, the most convenient to use were series involving variation of the ﬂoodplain
width, and the slope of the channel sides.
Figure 4.1: View of the Flood Channel Facility experimental setup.
4.3
Hydraulic model
The model used in this analysis is Manning’s equation:
v = 1
nR2/3S1/2
(4.1)
where
v is the cross-sectionally averaged velocity
R is the hydraulic radius, which is normally taken to be the ratio between ﬂow cross-
sectional area and the wetted perimeter

Chapter 4. Calibration of steady state laboratory experiments
50
S is the channel slope, and
n, Manning’s roughness coeﬃcient, is the constant to be determined.
This relationship is in frequent use, and tables of Manning’s n have been compiled for
diﬀerent channel and ﬂoodplain characteristics (e.g. Chow et al., 1988).
The geometry of the experimental setup is shown in Figure 4.2; thus for water level y, the
cross-sectional area is given by:
A = (2b + y s)y
y ≤h
= (2b + h s)h + (2bw + (y −h) s)(y −h)
y > h
and the wetted perimiter by
P = 2(b + y
p
1 + s2)
y ≤h
= 2(y
p
1 + s2 + (bw −h s))
y > h
where the variables are deﬁned in Figure 4.2.
This makes it possible to reformulate
Equation (4.1), describing the velocity as a function of the stage and the experimental
geometry. Since the required relationship is normally that between discharge and stage, a
mean velocity can be deﬁned as v = q
A, where q is the discharge and A is the cross-sectional
area, as above.
Figure 4.2: Geometry of the Flood Channel Facility experimental setup for half channel
width.
4.4
Stage discharge relationship with varying ﬂoodplain width
The dataset used here is drawn from four series of experiments; one in-bank, and three out-
of-bank, with varying ﬂoodplain half-widths of 1.65m, 3.15m, and 5m, and are made avail-
able on the website http://www.ﬂowdata.bham.ac.uk. Channel half-width (b) is 0.75m,
bankfull depth (h) is 0.15m and the channel slope (s) is 1. The model assumption is that
of one-dimensional ﬂow, with the consequent neglect of the possibility that the longitudi-
nal velocity varies over the cross-section. When the model is used to plot the discharge

Chapter 4. Calibration of steady state laboratory experiments
51
as a function of water height, or stage, there is a discontinuity at bankfull height, y = h,
where a small increase ∆y in y beyond y = h gives an instantaneous increase in ﬂow
cross-sectional area of ∆y(bw −b −h s). This corresponds to the discontinuity in channel
width, in addition to the unrealistic assumption that the depth over the ﬂoodplain can
be inﬁnitesimally small. However, while results of experiments (Figure 4.3a) do show a
discontinuity, it is considerably smaller than that predicted by the model, indicating that
the model is inadequate to describe this behaviour. The error between the measured data
and the model output, ﬁtted by least squares ﬁt to the in-bank data, is shown in Figure
4.3b, illustrating that the greatest discrepancy between the model and data occurs when
the ﬂow is just out of bank, as might be expected since at low depths, ﬂoodplain resistance
has the greatest impact.
0.05
0.15
0.25
0.35
0.0
0.5
1.0
1.5
y:depth(m)
q:flowrate(cumecs)
G
G
G
GGGGGGG
G
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
GGG GGG
G
G
G
G
a)
0.05
0.15
0.25
0.35
−0.05
0.00
0.05
0.10
y:depth(m)
flowrate error(cumecs)
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
data
best fit model
  
bw=5m
bw=3.15m
bw=1.65m
Figure 4.3: Comparison of measured data with model predictions: a) least-squares best
ﬁt of Manning’s equation for diﬀerent ﬂoodplain widths b) error in measured data, by
comparison with least squares ﬁt to inbank data
4.4.1
Variation of ﬂow with depth
Recall that the data model being used is
z = M(x, θ) + δ(x) + ϵ
where the observed data z are related to the output of the model M(x, θ) at input x with
parameters θ, subject to errors caused by model structural inadequacy δ, and observation
ϵ.
Recall that the model inadequacy δ(x) is described as a Gaussian process,
N(hδ(x)T βδ, Vδ(x, x′))

Chapter 4. Calibration of steady state laboratory experiments
52
Referring to Figure 4.3b, it seems reasonable to describe a model inadequacy function
separately for the in-bank and out-of-bank ﬂows. The data were initially examined for
a single ﬂoodplain width (bw=3.15m). Apart from the data points close to depth 0.25m
(for which no explanation is available from the descriptions of the experimental results), it
seems reasonable to take a linear regression model for the out-of-bank ﬂows; thus (1, y−h)
was taken for the regression basis, deﬁned for y > h only, eﬀectively making the assumption
that the model is unbiassed for in-bank ﬂow. In addition, a smooth covariance was assumed
for the ﬂow values as a function of depth; Vδ(y, y′) = σ2
δ exp(−ωδy|y −y′|2).
It was
assumed that the in-bank and out-of-bank ﬂows were uncorrelated; however, σ2
δ and ωδy
were assumed to have the same value for both in-bank and out-of-bank ﬂows.
Calibration was performed for a single ﬂoodplain width (bw=3.15m), using Equations
(3.8), that is using the model directly without an emulator. Calculation was performed
using the logs of all variables, including the parameter θ (Manning’s n), and distributional
priors used are given in Table 4.1. Note that the regression parameters β1 and β2 have
improper uniform priors. The table also shows the moments of the posterior distributions.
It should be noted that in this analysis, the depth range was not scaled to [0, 1]. Thus,
the mean of the hyperparameter ωδy should be scaled by a factor of approximately 0.06
to be compatible with the reference curves in Figure 3.5.
Table 4.1: Prior and posterior distributions for calibration with respect to depth
prior
posterior
posterior
variable
distribution
mean
standard deviation
log(θ)
N(−4.6, 1.52)
-4.7
0.12
β1
U(−∞, ∞)
0.12
0.04
β2
U(−∞, ∞)
-1.6
0.89
log(ωδy)
N(5, 22)
5.6
0.66
log(σ2
δ)
N(−7, 22)
-7.3
1.0
log(σ2
ϵ )
N(−12, 32)
-13.2
1.0
The posterior distribution of the parameter Manning’s n is symmetric, with mean 0.0094,
and standard deviation 0.0011. Calibrated prediction of the input data is shown in Figure
4.4, where frame (a) shows the calibrated prediction, and frame (b) shows the diﬀerence
between the calibrated prediction and the output of Manning’s equation, using the value
of Manning’s n found by least squares ﬁt to the in-bank data, permitting a more detailed
scrutiny of the prediction. Like the data, the calibrated mean shows a small apparent
discontinuity at the bankfull level; however, the prediction interval, which is elsewhere
except in extrapolation extremely narrow, reﬂecting low errors in laboratory experiment
output, shows a local increase at the bankfull level, indicating some uncertainty in the
correction for the step model. This is clearly related to the large posterior “roughness”
hyperparameter. The calibrated mean in extrapolation continues the trend of the data,
but the prediction interval increases rapidly, reﬂecting similar behaviour seen in the toy
example in Figure 3.12.

Chapter 4. Calibration of steady state laboratory experiments
53
G
G
G
GGG GGG
G
G
G
G
0.05
0.15
0.25
0.35
0.0
0.5
1.0
1.5
y: depth (m)
q: flowrate (cumecs)
a)
G
G
GG
G
G
G
G
GG
G
G
G
G
0.05
0.15
0.25
0.35
−0.2
−0.1
0.0
0.1
0.2
y: depth (m)
q: flowrate (cumecs)
b)
G
data
calibrated prediction
95% prediction interval   
Figure 4.4: Bayesian calibrated prediction using data for a single ﬂoodplain width using
a model inadequacy: a) calibrated prediction; b) diﬀerence between calibrated prediction
and Manning’s equation, ﬁtted to the in-bank data.
4.4.2
Variation of ﬂow with depth and ﬂoodplain half-width
If the discharge q is taken as a function of both y (height) and bw (full channel half width),
it is possible to use all of the data in Figure 4.3a. Calibration was performed using two
data sets, in order to predict the third. This is a potentially problematic formulation, as
the bw-dependence is estimated on only two values.
As in the case for a single ﬂoodplain width, the model inadequacy regression basis was
deﬁned only for out-of-bank depths. Three basis functions were taken, (1, (y −h), bw),
deﬁned for y > h. It is not clear whether this is an appropriate regression basis, since
Figure 4.3b does not give the impression that the dependence on bw is linear, and regression
analysis bears this out.
However, with so few data series, it is diﬃcult to choose an
appropriate measure for the dependency.
As in the previous example, the covariance was deﬁned separately for in-bank and out-of-
bank ﬂows. Two covariance relationships were needed;
Vδ((y, bw), (y′, b′
w)) = σ2
δ exp(−ωδy(y −y′)2),
for in-bank ﬂows, and
(4.2a)
Vδ((y, bw), (y′, b′
w)) = σ2
δ exp(−ωδy(y −y′)2 −ωδbw(bw −b′
w)2),
for out-of-bank ﬂows.
(4.2b)
As in the previous example, the covariance between in-bank and out-of-bank ﬂows was
assumed to be 0.
Two calibrations were performed, in each case using two of the out-of-bank datasets, to
predict the results of the third dataset. In the ﬁrst case, calibration data corresponded to

Chapter 4. Calibration of steady state laboratory experiments
54
ﬂood plain half-width of 1.65m and 5m, and calibrated prediction was made for bw=3.15m.
In the second case, calibration undertaken with data measured at bw=1.65m and bw=3.15m
was used to predict the relationship q(h) at bw=5m.
Prior and posterior distributions are given in Table 4.2. The same prior distributions were
taken as in the single dimensional case, with the addition of a prior on log(ωδbw); however,
it was found that while a reasonably vague prior log(ωδbw) ∼N(5, 22) was adequate in
the case where the validation data corresponded to bw=5m (case (b) in Table 4.2), a
tighter prior was required where there was a larger interval between the ﬂoodplain width
calibration data (case(a)), to avoid a bimodal posterior distribution, a possible outcome
for a large enough posterior “roughness” in the bw-dimension.
Table 4.2: Prior and posterior distributions for calibration with respect to depth and
ﬂoodplain half-width
prior
posterior
posterior
posterior
posterior
variable
distribution
mean
st. deviation
mean
st. deviation
case(a)/case(b)
case (a)
case (a)
case (b)
case (b)
log(θ)
N(−4.6, 1.52)
-4.6
0.07
-4.5
0.04
β1
U(−∞, ∞)
0.06
0.04
0.04
0.02
β2
U(−∞, ∞)
0.02
0.009
0.02
0.006
β3
U(−∞, ∞)
-0.72
0.53
-0.22
0.27
log(ωδy)
N(5, 12)/N(5, 22)
5.5
0.64
6.1
0.68
log(ωδbw)
N(−3, 22)
-5.8
0.84
-3.0
0.80
log(σ2
δ)
N(−7, 22)
-7.1
0.83
-8.0
0.75
log(σ2
ϵ )
N(−12, 32)
-10.8
0.55
-112.9
0.55
In both cases the posterior distribution of θ is fairly symmetric, with a mean of 0.01, and
small standard deviation. Calibrated prediction is shown in Figure 4.5. In both cases,
there is good agreement between the calibrated prediction for the unseen ﬂoodplain width
and the validation data (coloured points). The results of the calibrated prediction show
a slightly wider prediction interval than in the single-dimensional case, and a bias in the
extrapolation case (Figure 4.5b) of the mean prediction, just above the bankfull level.
Interestingly, the spike in the prediction uncertainty at bankfull is smaller than the one-
dimensional example, while the uncertainty in in-bank prediction is slightly larger.

Chapter 4. Calibration of steady state laboratory experiments
55
G
G
G
GGGG
G
G
G
G
G
G
G
G
GGGGGGG
G
G
G
0.05
0.15
0.25
0.35
0.0
0.5
1.0
1.5
y: depth (m)
q: flowrate (cumecs)
G
G
G
GGGG
G
G
G
G
G
G
G
G
GGGGGGG
G
G
G
GG GGG
G
G
G
G
a)
G
G
G
GGG GGG
G
G
G
G
G
G
G
GGGGGGG
G
G
G
0.05
0.15
0.25
0.35
0.0
0.5
1.0
1.5
y: depth (m)
q: flowrate (cumecs)
G
G
G
GGG GGG
G
G
G
G
G
G
G
GGGGGGG
G
G
G
GGG
G
G
G
G
G
b)
G
black points: calibration data    coloured points: validation data
data
calibrated prediction
95% prediction interval   
bw=5m
bw=3.15m
bw=1.65m
Figure 4.5: Calibrated prediction: calibration undertaken using data from two ﬂoodplain
widths, and comparing the calibrated prediction with the third. Validation dataset a)
bw=3.15m b) bw=5m.
4.4.3
Variation of ﬂow with depth, ﬂoodplain half-width, and channel
side slope
G
G
G
G
G
2.0
3.0
4.0
5.0
0.0
0.5
1.0
1.5
2.0
floodplain half−width (m)
bank slope tangent
Figure 4.6: FCF straight channel exper-
imental design
A further extension to the calibration can be
made with inclusion of experiments undertaken
varying channel sideslopes. There is a problem
in doing this, caused by the design of the original
experiments. The design is shown in Figure 4.6.
While the design permits assessment of varia-
tion of ﬂow with respect to changes in ﬂoodplain
width, or of channel bank slope, it is not possible
to assess their eﬀect simultaneously. Thus, while
it is possible to undertake the calibration with
respect to both variables, nothing can be said
about any interaction which may exist between
these variables.
In addition, while it may be expected that variation of the river channel sideslope may
alter the inbank ﬂow as well as the out-of-bank ﬂow, measurements were not taken of the
inbank ﬂow for diﬀerent sideslope values. The solution for this exercise, was to take the
inbank measurements for the middle sideslope value, and attribute them to all sideslope
values.
The model inadequacy regression basis was again deﬁned for out-of-bank ﬂow only, as

Chapter 4. Calibration of steady state laboratory experiments
56
(1, y−h, bw, s), where s is the tangent to the sideslope. As before, the covariance structure
for the model inadequacy was considered to have two parts, relating to in-bank and out-
of-bank ﬂow; these parts of the model inadequacy were assumed to be uncorrelated. The
covariance structure for out of bank ﬂow was taken to be the smooth structure
Vδ((y, bw, s), (y′, b′
w, s′)) = σ2
δ exp(−ωδy(y −y′)2 −ωδbw(bw −b′
w)2 −ωδs(s −s′)2)
Two possible covariance structures were considered for inbank ﬂow:
Vδ(y, bw, s), (y′, b′
w, s′)) = σ2
δ exp(−ωδy(y −y′)2),
and
(4.3a)
V (δ(y, bw, s), (y′, b′
w, s′)) = σ2
δ exp(−ωδy(y −y′)2 −ωδs(s −s′)2)
(4.3b)
Calibration of the Manning equation was performed for both of these covariance structures,
in order to assess their eﬀect on the results. The prior distributions (Table 4.3) were the
same as those used in the previous case, with the tighter prior distribution for ωδbw and a
similar prior distribution for ωδs.
Table 4.3: Prior and posterior distributions for calibration with respect to depth, ﬂoodplain
half-width and channel side slope tangent
prior
posterior
posterior
posterior
posterior
variable
distribution
mean
st. deviation
mean
st. deviation
case (a)
case (a)
case (b)
case (b)
log(θ)
N(−4.6, 1.52)
-4.6
0.08
-4.6
0.08
β1
U(−∞, ∞)
0.011
0.058
0.012
0.056
β2
U(−∞, ∞)
0.021
0.014
0.020
0.013
β3
U(−∞, ∞)
-0.15
0.65
-0.19
0.64
β4
U(−∞, ∞)
0.015
0.016
0.015
0.015
log(ωδy)
N(6, 22)
5.5
0.71
5.5
0.69
log(ωδbw)
N(−3, 12)
-2.8
0.55
-2.8
0.57
log(ωδs)
N(−3, 12)
-2.2
0.59
-2.3
0.72
log(σ2
δ)
N(−7, 22)
-6.1
0.84
-6.2
0.78
log(σ2
ϵ )
N(−12, 32)
-11.6
0.56
-11.6
0.58
It can be seen from Table 4.3 that the calibration of these two models appears identical.
The posterior distributions of θ are similarly almost identical, with mean 0.01 and standard
deviation 0.0009. It is not surprising, then, that in general the calibrated prediction of the
input data for both models is at ﬁrst sight identical. Indeed, the calibrated prediction for
the entire range has only been shown for the ﬁrst case, for ﬂoodplain halfwidth bw=3m,
in Figure 4.7ai). The eﬀect of the diﬀerent ﬂoodplain channel slopes on the out-of-bank
ﬂow is merely a translational eﬀect of the relationship.
However, close inspection of the calibrated prediction of the input data for the two models
shows a substantial diﬀerence in the prediction interval in the inbank ﬂow (Figure 4.7aii
and b), with the larger prediction interval coinciding with a dependency in the covariance
structure on the additional variable, s. The diﬀerence in prediction interval cannot be
resolved without additional data.

Chapter 4. Calibration of steady state laboratory experiments
57
G
G
G
GGG GGG
G
G
G
G
GGGG
G
G
G
G
GGG
G
G
G
G
G
0.05
0.15
0.25
0.35
0.0
0.5
1.0
1.5
y: depth (m)
q: flowrate (cumecs)
G
G
G
GGG GGG
G
G
G
G
GGGG
G
G
G
G
GGG
G
G
G
G
G
ai
G
G
G
GG G
G
G
G
G
G
G
G
G
G
0.00
0.15
0.30
G
G
G
GG G
G
G
G
G
G
G
G
G
G
aii
G
G
G
GG G
G
G
G
G
G
G
G
G
G
0.05
0.10
0.15
0.20
0.00
0.15
0.30
y: depth (m)
G
G
G
GG G
G
G
G
G
G
G
G
G
G
b
G
black points: calibration data    coloured points: validation data
data
calibrated prediction
95% prediction interval   
s=2
s=1
s=0
q: flowrate (cumecs)
Figure 4.7: Calibrated prediction for ﬂoodplain half-width bw=3m., comparing the eﬀect
of in-bank covariance structures in Equations: ai) and aii) (4.3a) and b) (4.3b)
4.4.4
Comment
During the development of these models, the β parameters were not integrated out of the
equations, and the solution was found by integrating the full equations (3.5) including the
regression coeﬃcients explicitly. The posterior distributions were much more diﬃcult to
simulate from, and in particular were sensitive to redundancy in the regression basis for
the model inadequacy mean, and errors in the speciﬁcation of the covariance structure.
For example, in the case where the ﬂow q was modelled as a function of depth y and ﬂood-
plain half-width bw, regression analysis of the out-of-bank measurements also indicated
dependence on the product, (y −h)bw, as well as on the single variates (1, (y −h), bw).
However, inclusion of this term in the basis led to non-identiﬁability of the parameter
Manning’s n, whose posterior distribution then became dependent entirely on the prior
distribution. Further investigation of the input data locations in the (y, bw) plane showed
that the product bw(y −h) is highly correlated with ﬂow depth (y −h) (correlation 0.92),
indicating that this fourth term in the regression basis is redundant.
4.5
Taking into account experimental errors
4.5.1
Observation errors in the experimental programme
The foregoing analysis has been undertaken without reference to the experimental errors
reported in the measurement of the data. Although the experimental programme was more
complex, the two quantities used in this analysis are water height above the channel base,

Chapter 4. Calibration of steady state laboratory experiments
58
and total discharge. Other measurements implicit in this analysis are those of geometry
of the experimental rig, and their errors are assumed to be small.
Myers and Brennan (1990) reported that the water depth was measured by digital gauges
over stilling wells connected to tapping points in the channel bed, and reading to the
nearest 0.01mm. In addition, they reported that discharge was measured by oriﬁce plate
meters installed in the mains supplying the upstream end of the model, although they
did not give a value for the accuracy of this measurement.
It is however clear from
the reports of these experiments, both by Myers and Brennan, and by other authors,
that the primary adjustable quantity in the experiments was the ﬂowrate, and that other
quantities were measured and validated with respect to this quantity. This understanding
led to the deﬁnition of a very small prior for the variance of the ﬂowrate data in the above
analysis.
4.5.2
Including input errors in the calibration
While in practice the input errors in the experimental program analysed in this chapter
are small, the theory is discussed below for the inclusion of these errors in the Bayesian
formulation.
The formulation used in this analysis is to take discharge as a function of water depth,
where the main measurement errors are in the independent variable, water depth. The
model of Kennedy and O’Hagan (2001a), and indeed of Craig et al. (2001) and Goldstein
and Rougier (2004, 2006) do not allow for errors in the independent variable in the calibra-
tion. This is a simpliﬁcation of reality. Clearly, no input errors are required in emulator
formulation. However, in calibration both the input and output data are measured, and
are thus subject to measurement errors. Given calibration under the assumption of no
input errors, Kennedy and O’Hagan (2001b) allow for uncertainty in the input data on
the calibrated prediction, but focus on the ability to achieve an analytical solution for the
mean and variance of properties of the distribution of z(x).
However, for the purposes of ﬁnding a probability of inundation, the full distribution of
the process is required. Expressions were given in Equations (3.10) for the expectation
E
 ζ(x†)|z, θ, ψδ, σ2
ϵ

and variance Var
 ζ(x†)|z, θ, ψδ, σ2
ϵ

of the process, which are Normal,
conditional on the parameter value. If however further calibrated output is required, for
input whose measured value is x†, but whose actual value is uncertain, then the required
distribution, rather than being ζ(x†), is in fact ζ(X|x†), where X is a random variable, with
distribution ζ(X|x†). Then, assuming conditional Normality of the calibrated prediction
so that the distribution f
 ζ(x†)|z, θ, ψδ, σ2
ϵ

is determined by Equation (3.10), the output
value is found by integrating this conditional distribution with respect to f(X|x†), before
summing over the Markov chain to arrive at a posterior distribution.
E

ζ(X|x†, z)

=
ZZZ
θ,ψδ,σ2ϵ
Z
X
f
 ζ(X|z, θ, ψδ, σ2
ϵ

f(X|x†) dX

f(θ, ψδ, σ2
ϵ ) dθ dψδ dσ2
ϵ

Chapter 4. Calibration of steady state laboratory experiments
59
It should be noted, however, that if the input errors are signiﬁcant, they should be taken
into account in the calibration. This is the situation which has been addressed in the
BATEA project (Kuczera et al., 2006, Kavetski et al., 2006, Thyer et al., 2009, Renard
et al., 2010), giving rise to a large numbers of unknowns to be solved for, as this involves
an unknown variable for each input measurement used. As has been noted previously,
Renard et al. (2010) have found that the input and structural errors are unidentiﬁable
when both are speciﬁed in a rainfall-runoﬀmodel.
Calibration in presence of input errors is more complicated than under the assumption of
no input errors. Starting from Equation (1.4),
z = M(w + e, θ) + δ(w + e) + ϵ
the data model now contains additional unknowns to be estimated. Besides the parameters
θ, the hyperparameters for the model inadequacy function δ and the output observation
noise ϵ, there are also not only hyperparameters for the input error distribution, e, but
the input error values themselves at each measurement location w. The expression for the
Gaussian process for the model inadequacy becomes more complicated;
δ(w + e) ∼N

h(w + e)β, V
 (w + e), (w′ + e′)

If it is assumed that the errors on the input are small relative to the diﬀerences between
the input values, then the covariance could be approximated by V
 w, w′
. It would not
be feasible to integrate out the regression parameters β, necessitating their estimation by
MCMC simultaneously with the other variables.
4.6
Summary
The work described in this chapter concerns the calibration of simple steady state ﬂow
models using laboratory data, as a simple application of the Kennedy and O’Hagan calibra-
tion methodology. The nature of the model inadequacy necessitated a correction function
deﬁned over part of the input domain. By including the output of successively larger
numbers of experiments, it has been possible to develop increasingly complex regression
bases.
The treatment of input errors has been discussed, although not demonstrated.
The following two chapters concern the calibration of ﬂood models, using historical ﬂood
data, where it can be expected that the data contain greater errors, and the inadequacies
of the model structure are larger and more complex.

Chapter 5
Calibration of a steady state ﬂood
model
5.1
Introduction
The work in this and the following chapter concerns the calibration of ﬂuvial ﬂood models
using historical ﬂood data. The models used are the hydraulic models described in Chap-
ter 2, which solve the partial diﬀerential equations representing ﬂow in the channel and
ﬂoodplain. Although the models used in this thesis are simple, and thus inadequate to
describe the ﬂow in geometrically complex channels, they take suﬃciently long to execute
that emulators are required to make the calibration manageable. The data too, measured
under ﬂood conditions, can be expected to contain signiﬁcant errors.
This chapter concerns the calibration of a steady state model of a ﬂood on the river Thames
at Buscot, in December 1992. The ﬂood coincided with an overpass of the ERS-1 remote
sensing satellite, so calibration can be undertaken with reference to the resulting SAR
image, which gives a map of the ﬂood extent. The ﬂood has been modelled, using steady
state simulations, with the LISFLOOD-FP package, giving depths across the ﬂoodplain
which can be compared with the satellite image.
The results obtained in this chapter were found using the MCMC formulation described
before.
However, as in Chapter 4, demonstration of the emulator has been obtained
using the BACCO computer package (Hankin, 2005), which has implemented the emulator
formulation of Oakley and O’Hagan (2002) in R (R Development Core Team, 2009); these
equations were detailed in Section 3.6. Much of the work in this chapter has been reported
in Hall et al. (2011), although the calibration results there were achieved using BACCO.
It is thus possible to compare the results of the full calibration using MCMC, with those
achieved using optimisation to ﬁnd the hyperparameters, as described by Kennedy and
O’Hagan (2001b) and implemented in BACCO.
60

Chapter 5. Calibration of a steady state ﬂood model
61
500
1000
1500
2000
2500
3000
3500
500
1000
1500
2000
x dist,  m
y dist,  m
Direction of flow
70
75
80
scale: elevation, m
Figure 5.1: Digital elevation map of the study region, with river course superimposed;
direction of ﬂow, left to right
5.2
Flood model
The LISFLOOD-FP package, used to model the ﬂood, was described in Chapter 2. The
model has been applied to a region of 3.8 x 2.4 km, with a low-lying ﬂoodplain on the
left bank. The river reach is bounded upstream by a gauged weir, and reasonably well-
contained at the downstream end.
Thus, upstream discharge was taken as the value
corresponding to the gauged ﬂow at the time of the satellite overpass. Downstream outﬂow
is given by the weir equation, based on the slope of the given bed dimensions.
A 50km resolution DEM was used (Figure 5.1) to describe the ﬂoodplain topography, with
a vertical accuracy of 0.25m. The channel depth and width are assumed uniform in the
model; these and other geometrical values were provided by the Environment Agency. The
model was set up by Aronica et al. (2002), who point out that a dynamic simulation was
unnecessary, in view of the short reach and broad hydrograph involved.
The parameters to be determined through calibration are the roughness coeﬃcients of
the channel and ﬂoodplain. Werner et al. (2005), using GLUE, demonstrated a lack of
sensitivity to using spatially varying roughness parameters in a ﬂoodplain with mixed
vegetation, while Hall et al.
(2005) used global sensitivity analysis to show that this
model is not sensitive to ﬂoodplain roughness coeﬃcient. Consequently, calibration has
been undertaken with regard to a uniform channel roughness coeﬃcient.

Chapter 5. Calibration of a steady state ﬂood model
62
500
1000
1500
2000
2500
3000
3500
500
1000
1500
2000
x dist (m)
y dist (m)
Figure 5.2: Flood extent, obtained from processed SAR data
5.3
Calibration data
The calibration data were provided from radar imagery, and processed by Horritt et al.
(2001) to form a binary image of inundated and non-inundated areas (Figure 5.2). Mis-
classiﬁcation is possible in some green ﬁelds which give a similar low backscatter to the free
water surface. Equally, it is possible to misclassify ﬂooded area as non-inundated, where
the water surface is wind-roughened, although this was thought to be a lesser problem in
this instance.
Given that the output of LISFLOOD-FP is a map of water surface height, there are two
ways in which this can be related to the processed SAR data. Aronica et al. (2002) used the
binary radar image to calibrate the LISFLOOD-FP model using GLUE, by classifying the
model output array as inundated and non-inundated, and devising a measure of goodness
of ﬁt of these binary images. This measure is
F =
P
i,j{(di,j = 1) ∧(mi,j = 1)}
P
i,j{(di,j = 1) ∨(mi,j = 1)}
where (di,j = 1) and (mi,j = 1) are the events that pixel (i, j) is inundated, according
to the data and model output respectively. Woodhead (2007) undertook a statistically
coherent calibration of the model, using Bayesian techniques and the same binary data
image, investigating a number of binary data models, but without considering model
structural uncertainty.
An alternative is to identify from the data the water surface height at the ﬂood shoreline,
and compare this with the water surface heights predicted by LISFLOOD-FP.
The accuracy with which water heights can be recovered from the superposition of a
ﬂood outline onto a DEM depends not only on the resolution and accuracy, but also on

Chapter 5. Calibration of a steady state ﬂood model
63
500
1500
2500
3500
500
1500
x distance (m)
y distance (m)
500
1500
2500
3500
x distance (m)
500
1500
2500
3500
x distance (m)
69
70
71
72
73
74
scale: elevation, m
run 1 : n = 0.035
run 2 : n = 0.05
run 3 : n = 0.07
Figure 5.3: Modelled ﬂood extent for diﬀerent values of Manning’s n
the slope of the DEM; in the limit, the bounds to the water height at the shoreline are
the height of the DEM at the pixel at the water’s edge, and the height of the lowest
adjacent non-inundated pixel. The uncertainty on the water height at the shoreline has
been reduced (Hall et al., 2011) by incorporating information from an ensemble of 638
model runs with randomly chosen values of Manning’s n in the interval [0.01, 0.1]. For
each shoreline location of the data, those runs in the ensemble were taken whose output
wet/dry classiﬁcation in the nine immediately adjacent pixels matched those of the data.
The water height taken for that shoreline point was the mean value from those runs.
This technique allows the model to interpolate between regions where the shoreline water
height is more and less well-deﬁned. The resulting ﬂood elevations, deﬁned at locations
illustrated in Figure 5.4b, are shown in Figure 5.4a.
5.4
Calibration
5.4.1
Emulator construction
Running the model with diﬀerent values of the roughness parameter gives ﬂood extents
both greater and less than those found from the satellite data (Figure 5.3).
A small
number of the ensemble of 638 model runs was taken, ensuring that the range of their
predicted ﬂood extent spans the inundation range found from the satellite data. Figure
5.4c illustrates the water surface proﬁles at the left bank obtained from these selected
runs, which span reasonably well the observations shown in Figure 5.4a.
In order to construct an emulator in (x, y, n) space, a random sample of 40 water surface
elevations was taken from these selected runs, at points on the boundaries of the model
output ﬂood outline. The projection of these points onto the spatial plane is shown in
Figure 5.4d. Note that these points are concentrated near the boundaries of the ﬂooded
area rather than being uniformly distributed over the plane. This has been done, recog-
nising that emulator errors increase away from the design points, as it is here that the
emulator is required to be most accurate in order to make best use of the observation data
and also to generate accurate ﬂood predictions. Note too, that the right bank is steeper
than the left bank, so the right shoreline changes little with varying roughness parameter,

Chapter 5. Calibration of a steady state ﬂood model
64
0
20
40
60
69
70
71
72
73
74
Cell number in x−direction
Flood elevation(m)
right bank
left bank
a)
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
0
20
40
60
0
10
20
30
40
Cell number in x−direction
Cell number in y−direction
GGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGG
G
G
right bank
left bank
b)
0
20
40
60
69
70
71
72
73
74
Cell number in x−direction
Flood elevation(m)
n = 0.065
n = 0.055
n = 0.046
n = 0.034
n = 0.025
n = 0.015
c)
Manning's n in SI units:m(−1 3)s
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
0
20
40
60
0
10
20
30
40
Cell number in x−direction
Cell number in y−direction
d)
Figure 5.4: LISFLOOD calibration and emulation data a) Flood water elevation inferred
from the SAR ﬂood outline image b) Points where ﬂood elevation values were extracted
from SAR observations c) Typical LISFLOOD water surface proﬁles, at diﬀerent values
of Manning’s n d) Points used to construct the LISFLOOD emulator
and thus emulator design points appear to be spatially closer together on the right bank.
An alternative method for emulator design would have been to have taken a Latin hy-
percube of training points over the (x, y, n)-plane, extrapolating the water surface, using
distance-weighted averaging, from the shoreline to the edge of the domain. This has been
tried, but since the water surface is very smooth, has made little diﬀerence. The number
of design points is limited by the stability of the covariance matrix. If the design points
are taken too close together, then the covariance matrix becomes singular. The use of
pivoted Choleski decomposition or singular value decomposition permits the number of
design points to be increased, ﬁrstly since these are robust decomposition methods, and
secondly, as an approximation can be made by discarding the smallest eigenvalues. Care
should be taken however, as the exclusion of small eigenvalues in the covariance matrix
smoothes the solution, which may be undesirable.
Regression analysis of the model output at the emulator design points suggests linear
dependence on x, and on the parameter, Manning’s n. Dependence on y is not signiﬁcant.
A smooth covariance function has been assumed, reﬂecting our belief about the water
surface.
For illustration, the emulator has been produced using BACCO. Improper vague priors

Chapter 5. Calibration of a steady state ﬂood model
65
were used for all variables, and the calculated regression coeﬃcients were (0.58, -0.78,
0.46) in (1, x, n), the optimised emulator variance was 0.011, while the log(“roughness”)
coeﬃcients were (-5.62, 19.2, -3.94) in (x, y, n)-space. Emulator output has been compared
with LISFLOOD runs not used in the emulator construction. This is diﬃcult to visualise,
as the emulator error is deﬁned as a distribution for every point in the three dimensional
(x, y, n)-space. Figure 5.5 shows the absolute error in mean emulator prediction, and the
standard deviation of the predicted emulator uncertainty plotted on the spatial plane,
compared with output from three LISFLOOD runs not used to train the emulator, whose
values of Manning’s n are close to, midway between, and outside the range of the training
runs. The scale of the mean error is large, but investigation has shown that this is not
due to the choice of training points, but rather to the nature of the model output. While
it might be expected that the water surface varies gradually and smoothly along the
reach, Figure 5.6 shows the deviation of the model output from this assumption, which
is considerable at some locations close to the shore line, or to the edge of the domain.
Figure 5.5 also shows that the uncertainty in the emulator prediction increases away from
the training runs.
500
1500
y distance (m)
500
1500
2500
3500
500
1500
x distance (m)
y distance (m)
500
1500
2500
3500
x distance (m)
500
1500
2500
3500
x distance (m)
G
training values
test values
0.00
0.02
0.04
0.06
0.08
Manning's n
(SI units)
G
G
G
G
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Error (m)
test run 1 : n = 0.035
test run 2 : n = 0.05
test run 3 : n = 0.07
emulator mean error
emulator st. deviation
Figure 5.5: Absolute error in mean emulator prediction, and standard deviation of the
predicted emulator uncertainty for three diﬀerent values of Manning’s n: close to, in
between, and away from the training runs.
Values of test runs and training runs are
illustrated
500
1500
2500
3500
500
1500
x distance (m)
y distance (m)
500
1500
2500
3500
x distance (m)
500
1500
2500
3500
x distance (m)
−1.0
−0.5
0.0
0.5
1.0
scale: elevation anomaly, m
run 1 : n = 0.035
run 2 : n = 0.05
run 3 : n = 0.07
Figure 5.6: Deviation of model output from linear slope

Chapter 5. Calibration of a steady state ﬂood model
66
Bastos and O’Hagan (2009) discuss the validation of Gaussian process emulators. Given
the correlated nature of the emulator output, they suggest calculating the Mahablanobis
distance between the emulator output with the computer model output.
Oakley and
O’Hagan (2002) note that the emulator errors are distributed as a multivariate t-distribution,
with mean m∗(x) and variance ˆσ2c∗(x, x′) given in Section 3.6. For a multivariate emula-
tor,
√q −p
ˆσ1
√q −p −2Q−T  m∗(x, y, n) −M(x, y, n)

∼tq−p
(5.1)
where q is the number of points, p is the number of regression parameters, and Q is a ma-
trix such that QQT = C∗, the correlation matrix deﬁned by C∗
i,j = c∗(xi, x′
j), with c∗(x, x′)
deﬁned in Section 3.6. The simplest formulation for Q is generally the Choleski decompo-
sition of C∗. However, if the points represented in the correlation matrix C∗are too close
together, this matrix becomes numerically unstable, so an alternative decomposition was
used, determining the eigendecomposition of the matrix C∗using singular value decomposi-
tion.
Standardised mean error
Probability density
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
Figure 5.7: Emulator error distribution; with super-
imposed t distribution with 5 degrees of freedom.
The distribution was found as fol-
lows. Taking the ensemble of 638
model runs, a number of draws
were made in the (x, y, n) space, in
a Latin hypercube, and spaced as
far as possible from each other and
from the training locations. Those
points where the water surface was
not above the DEM were then dis-
carded, and the expression on the
left of Equation (5.1) was calcu-
lated. In view of the numerical dif-
ﬁculties found in determining Q, it was infeasible to apply this process for more than
approximately 100 points at a time, so in order to have a reliable estimate of the distribu-
tion, the calculation was repeated until the total sample was 2037 points. The degrees of
freedom should, however be related to the number of runs used to generate the emulator,
giving a theoretical t distribution with 5 degrees of freedom. The transformed diﬀerences
have mean 0.051, standard deviation 1.877 corresponding to a t distribution with 3 degrees
of freedom (Figure 5.7).
5.4.2
Calibration
In the absence of any evidence pointing towards greater complexity, the chosen regression
basis for the model inadequacy, as for the emulator, is a linear spatial and parameter
dependence, this time on (x, y, n), and the covariance is assumed smooth. Note that while
the computer model output is that of a water level, when this is above the land surface,

Chapter 5. Calibration of a steady state ﬂood model
67
both the emulator and the calibration spread across the entire spatial domain.
Calibration has been undertaken using a subset of the data; initially 26 data points, spread
along the shoreline at both sides of the ﬂooded river, and for comparison, with 61 data
points. The prior distributions were as shown in Table 5.1. Speciﬁcation of prior distri-
butions for the noise variance is not straightforward. The Bayesian methodology requires
that the prior distributions represent the best knowledge of the technical expert. How-
ever, in this case the observation data have been derived by a complex process; ﬁrst the
interpretation of satellite images into binary images, followed by translation into water
heights. It was felt that the standard deviation of the error could be considered to be
somewhere between 10cm and 25cm. If, however, the assumption were taken, that the
water level should vary linearly with distance along the x-direction, the residual variance
of the data would imply an error standard deviation of approximately 35cm. Accordingly,
four diﬀerent prior distributions were taken for the observation error, Normal distributions
with means corresponding to each of these values, but small standard deviations, and a
more vague distribution, which encompassed all possibilities. Bearing in mind the sensi-
tivity found for the posterior prediction interval of the toy example to prior distribution
on model inadequacy variance (Table 3.3), this was also investigated, but had little eﬀect
on the calibration and calibrated prediction.
Table 5.1: Prior and posterior distributions for calibration
prior
posterior mean and (st. deviation)
variable
distribution
comment
26 data points
61 data points
n
N(.0265, .02242)
0.027 (0.012)
0.028 (0.013)
β1
emulator (const)
0.62 (0.07)
0.61 (0.07)
β2
emulator (x)
-0.77 (0.09)
-0.78 (0.10)
β3
emulator (n)
0.44 (0.07)
0.44 (0.07)
β4
inadequacy (x)
0.33 (0.18)
0.29 (0.23)
β5
inadequacy (y)
-0.68 (0.32)
-0.63 (0.42)
log(ω1x)
N(3.5, 22)
2.57 (0.35)
2.44 (0.35)
log(ω1y)
N(2.5, 22)
2.84 (0.58)
2.88 (0.58)
log(ω1θ)
N(2, 22)
1.59 (0.47)
1.46 (0.46)
log(ω2x)
N(2, 1.42)
2.07 (1.19)
4.13 (0.66)
log(ω2y)
N(2, 1.42)
2.52 (1.38)
5.11 (0.76)
log(σ2
1)
N(−4.5, 22)
-4.71 (0.37)
-4.62 (0.40)
log(σ2
2)
N(−4.5, 1.42)
-5.32 (1.10)
-4.05 (0.49)
log(σ2
ϵ )
N(−4.5, 0.12)
mean ↔σ=0.35m
N(−5.23, 0.12)
mean ↔σ=0.25m
N(−7.06, 0.12)
mean ↔σ=0.10m
N(−5.23, 22)
mean ↔σ=0.25m
-6.15 (0.44)
-5.72 (0.42)
Distributional means and standard deviations for posterior distributions are given in
Table 5.1 for the cases with vague prior distributions.
Emulation and calibration for
both datasets leads to posterior emulator parameterisation very similar to that found by
BACCO. The diﬀerences in posterior parameterisation of the model inadequacy function

Chapter 5. Calibration of a steady state ﬂood model
68
are largest when looking at the “roughness” coeﬃcients, log(ω2x) and log(ω2y); these are
somewhat larger for the cases with 61 data points than those with 26 data points. In ad-
dition, the model inadequacy variance is slightly larger for the cases with 61 data points.
It is also noticeable that the standard deviations for the model inadequacy regression
parameters are large, indicating that it may have been better to have calibrated with a
zero model inadequacy mean in this case. The posterior mean observation error variance
corresponds to an observation error standard deviation of 16cm in the case with 26 data
points, and 20cm in the case with 61 data points.
As regards the eﬀect of prior observation error distribution, the distribution of the posterior
observation error variance follows the prior closely when the prior is speciﬁed with a small
variance.
However, the model inadequacy variance and “roughness” in both x and y
directions increase with decreasing prior observation error variance (although the range is
rather less than the diﬀerence between the cases with diﬀerent data sets), indicating that
the variability is interpreted as model inadequacy when the solution is constrained.
The calibrated prediction of the input data is shown, for each of the eight cases, in Figure
5.8. From the ﬁrst column of the output water proﬁles, it can be seen that the mean proﬁles
appear somewhat uneven, reﬂecting the uneven path of the shorelines.
In particular,
there is an apparent step in elevation at approximately 800m along the left bank. This
corresponds to the end of the branching area of inundation seen in Figure 5.2, at a low-
lying area on the DEM; the shoreline has been deﬁned (Figure 5.4a) to be at each position
on the x-axis, the extrema of the inundated area on the y-axis, thus giving rise to the step
seen in Figure 5.4a and Figure 5.8. With hindsight, it may have been more appropriate to
deﬁne the axis along the centre-line of the river, as would be natural for a one-dimensional
model. This would certainly have been the case for a reach which did not lie conveniently
along the x-axis, but the beneﬁts of the additional notational complexity were less obvious
in this case.
The second issue to appear from the ﬁrst column of output, is that the width of the
prediction interval reduces with reducing prior mean observation error; indeed, the third
and fourth results (Figure 5.8ci and di), appear to be plausible in predicting a proﬁle
which could have given rise to the observed data.
Looking at the second column of results, however, the picture is not so clear. These runs
have been done with more data points, showing greater scatter, and in particular there are
two apparently low values on the left bank at approximately 700m along the reach, and a
series of three apparently high values on the right bank,at approximately 2500m. It can
be seen in the ﬁrst three frames in the second column (Figure 5.8aii, bii and cii) that the
predicted mean water height proﬁle follows the data increasingly closely with decreasing
prior observarion error variance. The fourth case (dii) looks fairly close to the second (bii),
unsurprising since the posterior observation error estimates are similar.
What is not clear, is whether the fourth proﬁle (Figure 5.8dii) follows the data implausibly
closely, indicating that for this problem there may be an issue of identiﬁability between

Chapter 5. Calibration of a steady state ﬂood model
69
G
G
G
GG
G
G
G
G
G
G
G
0
500
1500
2500
3500
69
70
71
72
x distance, m
Elevation, m
G G
G
G
G
G
G
G
G
G
G
G
G
G
di
G
G G
GG G G
GG
G
GG
GGG G
GG
G
G G
G
G GG
GG
G
G G
0
500
1500
2500
3500
x distance, m
G GGG
G
G
G
G G G
G
GGG
GGGG
GG
GG
G G
GG
GG
G
G
G
dii
G
G
G
GG
G
G
G
G
G
G
G
69
70
71
72
Elevation, m
G G
G
G
G
G
G
G
G
G
G
G
G
G
ci
G
G G
GG G G
GG
G
GG
GGG G
GG
G
G G
G
G GG
GG
G
G G
G GGG
G
G
G
G G G
G
GGG
GGGG
GG
GG
G G
GG
GG
G
G
G
cii
G
G
G
GG
G
G
G
G
G
G
G
69
70
71
72
Elevation, m
G G
G
G
G
G
G
G
G
G
G
G
G
G
bi
G
G G
GG G G
GG
G
GG
GGG G
GG
G
G G
G
G GG
GG
G
G G
G GGG
G
G
G
G G G
G
GGG
GGGG
GG
GG
G G
GG
GG
G
G
G
bii
G
G
G
GG
G
G
G
G
G
G
G
69
70
71
72
Elevation, m
G G
G
G
G
G
G
G
G
G
G
G
G
G
ai
G
G G
GG G G
GG
G
GG
GGG G
GG
G
G G
G
G GG
GG
G
G G
G GGG
G
G
G
G G G
G
GGG
GGGG
GG
GG
G G
GG
GG
G
G
G
aii
G
G
right bank, data
right bank, mean
right bank, 95% prediction int.  
left bank, data
left bank, mean
left bank, 95% prediction int.  
Figure 5.8: Prediction of water levels along the measured shorelines, with diﬀerent prior
assumptions about measurement error, and numbers of data points: a) mean error 35cm,
precise distribution, b) mean error 25cm, precise distribution, c) mean error 10cm, precise
distribution, d) mean error 25cm, vague distribution, i) 26 data points, ii) 61 data points
the observation error and the model inadequacy function (Wynn, 2001), which can only
be resolved by a tight speciﬁcation of the observation error prior mean. An alternative

Chapter 5. Calibration of a steady state ﬂood model
70
interpretation would be that the excursions from a smooth proﬁle are real; the low values
on the left bank near 700m correspond with the jump in the shoreline location, while the
high values on the right bank occur close to the steep bank section (Figure 5.1, where
the data processing may be susceptible to greater errors in the translation from binary to
height data. In any case, since this methodology distinguishes between model inadequacy
and observation error on the grounds of correlation, it may be unreasonable to expect a
strict separation between the two where there are sequences of points deviating from the
mean line.
500
1000
1500
2000
2500
3000
3500
500
1000
1500
x distance, m
y distance, m
0.0
0.2
0.4
0.6
0.8
1.0
Figure 5.9: Probability of inundation
5.5
Comparison of results with previous work
By combining the predicted water proﬁle from the case ci with the DEM, it is possible to
produce a map of probability of inundation across the entire domain (Figure 5.9). This can
be compared with the ﬂood likelihood map for the same event generated by Aronica et al.
(2002) who used GLUE (Figure 5.10). The images are similar, although some diﬀerence
may be expected from the diﬀerence in preparation of the data: the Aronica et al. (2002)
analysis predicted binary ﬂood inundation on a 50m grid, whereas the current analysis has
been performed in terms of depths, making it more likely to show small areas of isolated
inundation.
Aronica et al. (2002) noted the sensitivity of their prediction to the choice of threshold
for discarding “non-behavioural” runs. This corresponds with the analysis of GLUE by
Montanari (2005), who undertook a systematic examination of the dependency of GLUE
model parameterisation on the assumptions commonly taken in its application. The re-

Chapter 5. Calibration of a steady state ﬂood model
71
sults in the previous section illustrate that the Bayesian predictions are equally sensitive
to modelling assumptions, in this case the prior distribution of observation error. How-
ever, it is more natural to express a prior belief in the distribution of observation error,
which is a physical consequence of the quality of the observation process, than in the
behavioural threshold used in GLUE, which does not correspond to a physical quantity.
Moreover, the results generated here can strictly be referred to as predictive probabilities
of ﬂooding, whilst the results presented by Aronica et al. (2002) are relative measures of
agreement which do not correspond to a carefully constructed statistical model. Clearly,
the comparison between the two methods is based on a single example, moreover one
where the existence of model inadequacy is not obvious from the hydraulic model output,
but the current Bayesian method has the advantage of a coherent statistical analysis, and
the potential of handling more substantial model bias.
Figure 5.10: Probability of inundation obtained by Aronica et al. (2002)
5.6
Alternative emulator formulations for multivariate model
output
The problem of the ﬂood extent model could be classed as one of multivariate output. A
number of methods have been explored to deal with multivariate output in the context of
the formulation of Kennedy and O’Hagan; the chief issue being that the dimensionality
of the calibration problem can easily become unwieldy. Rougier (2008) has classiﬁed a
number of methods for dealing with emulators for multivariate output. Of these two are
particularly suitable for the ﬂood extent model. One is the approach used here. The
other is the use of principal components analysis, suggested independently by Higdon et
al. (2008a), and by McNeall (2008).
Principal components analysis is a technique involving linear transformation, which can
be applied to a correlated set of zero mean vectors, to yield an orthogonal vector set.
The transformation is unique (to scaling) and invertible.
The orthogonal set, known

Chapter 5. Calibration of a steady state ﬂood model
72
as eigenvectors, can be ordered by the eigenvalues, each of which contains the variance
of the set in the direction of the related eigenvector. This provides the key to the use
of the technique for dimensionality reduction, when the variance in some directions can
be considered small enough to be ignored. The technique eﬀectively involves projection
of the data matrices onto the hyperplane which will explain the signiﬁcant part of the
variation.
Higdon et al. (2008a) and McNeall (2008) have used this technique to formulate empirical
emulators for their high-dimensional model output. It would be possible to transform
the data in a similar way, and conduct the entire analysis in the transformed space, but
instead the data is introduced untransformed.
This ensures straightforward treatment
of uncorrelated noise, but raises the issue of the cross-covariance between transformed
emulator runs and untransformed data. Higdon et al. sidestepped this issue by assuming
zero cross-covariance without comment.
The use of principal components analysis has been explored for the generation of an
emulator for the Buscot ﬂood model, although not for calibration. To simplify the problem,
computer model output was preprocessed to extend the speciﬁed water surface beyond the
point at which it intersects the DEM to the edge of the domain. A stratiﬁed sample of runs
was then drawn from the range of Manning’s n, and the entire extended water surface was
used for that parameter value, comprising 48 × 76 = 3648 data points for each parameter
value. A complete transformation reduces the dimensionality of the problem from 3648 to
the number of parameter values chosen. Discarding dimensions with insigniﬁcant variance
will reduce the size of the problem still further. Thus, taking 6 runs with a spread of input
parameter values, and applying principal components analysis, Table 5.6 shows that 98.7%
of the variance is explained by the ﬁrst principalcomponent. An emulator was formulated
with a single principal component, yielding results similar to those already shown.
The potential advantage of this emulator formulation is in condensing information from
an extensive ﬂood domain to a few principal components, easing the eventual calibration
study. While the method is particularly useful in ﬁelds where the model output values
contain underlying structure, such as the global spatial distribution of climate variables of
McNeall’s study (2008), it could equally be used in the case of an extensive ﬂood domain,
where it might not be appropriate to describe the model output by a few individual points,
as has been done in this comparatively small calibration study.
Table 5.2: Variance explained by diﬀerent numbers of principal components for a ﬂood
model emulator
Number of principal components
1
2
3
4
5
6
Percentage of total variance
98.69
0.70
0.55
0.06
0.00
0.00
Cumulative percentage
98.69
99.38
99.94
100.00
100.00
100.00

Chapter 5. Calibration of a steady state ﬂood model
73
5.7
Summary
The application has been presented of the calibration methodology of Kennedy and O’Hagan
to a steady state ﬂood model, using satellite data. This has been demonstrated in the
case of a model, using LISFLOOD-FP, of a reach of the river Thames, near Buscot. The
application necessitated the construction of an emulator for the model, to speed up the
calibration. A number of methodological choices have been discussed and demonstrated,
and sensitivity of the calibration to prior distributions was examined. It was particularly
noted that attention must be paid to the prior estimate of observation error variance, as
this does aﬀect the prediction interval, and hence, any inundation probability based on
it.
In spite of this weakness, this calibration of a steady-state ﬂood model on ﬂood extent
data illustrates a signiﬁcant step forward, since the only other studies reported in the
literature of ﬂood model calibration based on ﬂood extent data use a rather unsatisfactory
method, leaving the uncertainties embodied in a poorly justiﬁed probability of inundation.
With the increasing availability of satellite images, the calibration of ﬂood models using
such information may be expected to become more prevalent, and the introduction of an
eﬀective calibration method is thus important.

Chapter 6
Calibration of a dynamic ﬂood
model
6.1
Introduction
This chapter concerns the calibration of a dynamic ﬂood model using historical data. Most
ﬂood models are dynamic, reﬂecting the evolution of the ﬂood wave. Time series of gauged
river stage or ﬂow data are far more freely available than satellite images, such as that
used for calibration in the previous chapter. Aerial images, similarly, are rarely available
until well after the peak of the ﬂood. It is thus important to be able to calibrate dynamic
models.
The model calibration methodology based on Gaussian process representation of emulator
and model inadequacy function was conceived for static computer models, or computer
models where the output can be captured as a snapshot. A number of extensions to the
Gaussian process emulator have been proposed for time-varying output, but none are so
far suﬃciently promising to be attractive as a starting point for calibration of output from
a dynamic model, using dynamic data. The chapter begins with a brief review of these
extensions in the context of time series methods used in the calibration of dynamic models,
followed by a description of an emulator approach transforming the time-varying problem
to one which can be treated in an analogous manner to the static model.
The approach to dynamic emulation is applied to a dynamic model of a reach of the
river Severn, near Shrewsbury. Flows on this river reach have been previously modelled,
with both gauged records (Romanowicz et al., 2008) and satellite images of ﬂood extent
(Bates et al., 2004) used for calibration. The work presented here is the ﬁrst development
of a Bayesian calibration method for dynamic ﬂood models that explicitly incorporates
model structural inadequacy. Part of this work has been reported in Manning and Hall
(2010).
74

Chapter 6. Calibration of a dynamic ﬂood model
75
6.2
Calibration of dynamic models
Most calibration methods for spatially varying models can be adapted for calibration of
dynamic models, by treating time as another spatial dimension. However, this leads to a
high-dimensional problem. Recursive methods, where the estimates are updated one step
at a time, are often more suitable for forecasting, permitting assimilation of new data as it
becomes available. These methods lend themselves to the identiﬁcation of models whose
parameter values vary with time, although this is not a necessity. Moreover, recursive
methods can be particularly eﬃcient, using the current time series value as a starting
point for estimation of the value at the next time point.
A popular and eﬃcient method for recursive time-series estimation is the Kalman ﬁlter,
which has also inspired related methods. The Kalman ﬁlter (Kalman, 1960) is a recursive
algorithm designed to solve the linear time series problem:
xt = Fxt−1 + Dut−1 + wt−1
yt = Mxt + ϵt
(6.1)
where ut is the input, xt the state variable vector, and yt the observed value at time t,
with system noise wt and observation noise ϵt. The assumed system model is represented
by matrices F and D, while M represents the relationship between the system variables xt
and the observed variables yt. Given the covariance of the observation errors, the Kalman
ﬁlter alternates between predicting the state at the next time step, and its covariance,
and updating the prediction, using the deviation of the data from that forecast, under the
assumption of multivariate Normal errors. The Kalman ﬁlter is most commonly thought of
as a technique for data assimilation in forecasting, but can also be applied as a smoother,
to provide the optimal estimate of the state xt at all values of t in the observed series
t ∈[0, T], under the assumption of uncorrelated Gaussian observation noise.
An alternative derivation is the Dynamic Linear Model of Harrison and Stevens (1976),
who started from a Bayesian standpoint, but eﬀectively used the Kalman ﬁlter to solve
their models. The context of their work (West and Harrison, 1989), is in data models
which do not necessarily have exogenous time series input, while the Kalman ﬁlter was
derived expressly to interpret systems with time series input and output.
The Kalman ﬁlter represents a linear model with known parameters, but an extension,
the ensemble Kalman ﬁlter (Evensen, 2003), and its associated smoother (Evensen and
van Leeuwen, 2000), allow for model nonlinearity by using an ensemble of model states to
estimate the state covariance. This permits simultaneous estimation of the system states
and the model parameters, which need not be constant, and has been successfully applied
in the assimilation of river ﬂow data (Moradkhani et al., 2005a, Todini, 2008).
Dr´ecourt et al.
(2006) described a version of the ensemble Kalman ﬁlter, designed to
allow for bias in model predictions, while Kollat et al.
(2008) combined such a bias-
aware ensemble Kalman ﬁlter with sequential Gaussian simulation (Deutsch and Journel,

Chapter 6. Calibration of a dynamic ﬂood model
76
1992), which assumes a spatial Gaussian ﬁeld, forming a spatio-temporal model, and
demonstrated the spatio-temporal modelling method with respect to the assimilation of
monitoring data in a three-dimensional groundwater experiment.
The ensemble Kalman ﬁlter still makes an approximation to a nonlinear system model, only
taking into account the ﬁrst two moments of the distribution. The particle ﬁlter provides
a closer approximation to such a system model, enabling greater accuracy and stability
(Moradkhani et al., 2005b). The model solved by the particle ﬁlter can be generalised
from Equations (6.1) to
xt = f(xt−1) + d(ut−1) + wt−1
yt = m(xt) + ϵt ,
where the system and observation models are now allowed to be nonlinear, and it is not
necessary to make the assumption that the noise series wt and ϵt are Gaussian. Bayesian
solution of the model equations results in an intractable posterior distribution; simulation
from this distribution is achieved by tracking the behaviour of particles, or points sampled
from the state space.
It could be said that the identiﬁcation of model parameters which vary with time, often the
outcome of recursive time series estimation, points to a structural inadequacy of the spec-
iﬁed model to describe the system under investigation. A number of studies have similar
aims in investigating the structural uncertainty in dynamic models; by identifying parallel
time-dependent structures, and relating them to the original constant parameter model,
it is possible to detect structural weaknesses in the original physics-based model.
The data-based mechanistic (DBM) methodology of Young (2003) has Kalman ﬁlters
at its core.
Many of Young’s transfer function models involve the use of a nonlinear
state-dependent transformation of the input data, followed by a linear transfer function
model. This is a ﬂexible and eﬃcient model, which has been demonstrated in a number
of ﬂood catchments (e.g. Young, 2003, Romanowicz et al., 2008). It is also possible to use
Young’s nonlinear transfer function to replace the original, physics-based model, eﬀectively
providing an emulator for it (Beven et al., 2009).
Stigter and Beck (2004) developed a recursive prediction error algorithm, as an alternative
to the extended Kalman ﬁlter, with the additional ﬂexibility that it can be used for
nonlinear as well as linear models, and used this to estimate parameters for a water-quality
model for the river Calder. Lin and Beck (2007a, 2007b) synthesized the algorithm with
the data-driven techniques of Young, using the parallels between the approaches to identify
model structures for enviromental processes.
Reichert and Mieleitner (2009) proposed a method to identify which model parameters
should be considered variable, suggesting that single variables, or groups of variables be
replaced by a time-dependent, stochastic parameter. The stochasticity is described by an
Ornstein-Uhlenbeck process, which permits short-term variation from the parameter mean
value, depending on a characteristic correlation time and an asymptotic variance. Setting

Chapter 6. Calibration of a dynamic ﬂood model
77
the parameters for the Ornstein-Uhlenbeck process to appropriate values, the model pa-
rameters are estimated by Bayesian regression, with a signiﬁcant reduction in errors taken
to imply that the data justify the replacement of the parameter under examination with a
stochastic parameter in the original model. The analysis can be repeated for all variables
in the model, to ﬁnd out which, if any, should be considered time-dependent.
6.3
Extensions of the Kennedy and O’Hagan methodology
to time-varying problems
A number of diﬀerent approaches have been suggested to extend the Kennedy and O’Hagan
approach to time-varying problems. Not all of these have been demonstrated in the context
of calibration, as some emulation formulations are quite cumbersome.
The ﬁrst approach applies to those problems where the input can be characterised by a set
of scalar parameters. In this case it is possible to treat the time domain as an additional
spatial dimension, and to emulate and calibrate in the same way as the foregoing examples.
It can be seen that this approach may limit the length of the output time series which
can be treated. An extension of this approach is thus to transform the model output
into a more parsimonious domain. Higdon et al. (2008b) used Principal Components
Analysis to reduce the dimensionality of a heat conduction problem in the time domain,
in a similar approach to that of Paragraph 5.6 in the previous chapter. McNeall (2008) and
Wilkinson (2010) both used Principal Components Analysis to reduce the dimensionality
of time varying output from earth system models of intermediate complexity. Bayarri et
al. (2007) used wavelet transformations to describe the complex time-series output from
a model for vehicle crashworthiness. In both cases, the transformation was undertaken
oﬀ-line, and the analysis proceeded on the basis of this transformation. In the case of the
study of Bayarri et al., the analysis was done in the domain of the transformed output. In
neither case, however, was the input also high-dimensional, such as a time series.
A second approach to emulation of time-varying models, which has not been demonstrated
in the context of calibration, is to train an emulator to describe a single step of the
computer model, and to emulate a time-varying output by simulation through repeated
application of the single-step emulator. This approach has been used independently by
Bhattacharya (2007), and by Conti et al. (2009), with slightly diﬀerent implementations
concerning the locations where the emulator is to be evaluated - Bhattacharya deﬁned a
grid, and Conti et al. chose points as they arose - and which simulated emulator values are
used to condition subsequent evaluations. Both implementations are rather cumbersome.
Conti et al. claim that their emulator is more eﬃcient than that of Bhattacharya, but
demonstrated it for only 25 time-steps of a simple rainfall-runoﬀmodel.
Little et al. (2004) used Bayes Linear modelling, while Liu and West (2009) used fully
Bayes analysis, to combine the Gaussian process methodology with a time-varying au-
toregressive Dynamic Linear Model (West and Harrison, 1989), to form a temporo-spatial

Chapter 6. Calibration of a dynamic ﬂood model
78
calibration. These approaches oﬀer greater speed than that of Bhattacharya and of Conti
et al., but replaces the Gaussian process representation in the time domain with one which
is expressly designed for time series analysis. However, as with the models of Higdon et
al. (2008b), McNeall (2008), Wilkinson (2010) and Bayarri et al. (2007), neither formula-
tion allowed for time series input. For hydrological applications, the autoregressive model
would need to be replaced by an autoregressive model with exogenous inputs, to account
for forcing. In this case, this approach would resemble that of Kollat et al. (2008), who
combined a bias-aware ensemble Kalman ﬁlter with a spatial model based on a Gaussian
process.
The approach taken in this study is determined by the application, involving the evaluation
of Equation (1.2) in the Introduction, where the probability of downstream inundation is
integrated over all possible upstream input conditions. In particular, having calibrated
the hydraulic model for a given input hydrograph, the calibrated prediction is needed for
other input hydrographs. The most parsimonious method of evaluation of this integral
would be to couch the eﬀect of the catchment on the upstream hydrograph as a transfer
function. This is the approach taken in this study, and calibration has been performed in
the transformed space, in a similar way to the study of Bayarri et al. (2007).
6.4
Flood model
The dynamic calibration methodology is developed here in the context of a dynamic ﬂood
model of the river Severn, in the region around Shrewsbury. The Severn and its chief
tributary in its upper reaches, the Vyrnwy, rise in the Welsh mountains in an area of
high rainfall and steep slopes. Below their conﬂuence, the proﬁle is much shallower, and
ﬂooding is a frequent issue in the centres of population along its course.
The area under study concerns a 22 km reach of the river Severn below the conﬂuence
with the Vyrnwy and above Shrewsbury (Figure 6.11). At its upstream point, Montford,
long-term records exist of 15-minute measurements of both stage and discharge, although
the Environment Agency (2010) note that the discharge measurements are not reliable at
high ﬂows, and recommend use of discharge values obtained from the stage using a rating
curve. Similarly, records of river stage have been kept at Welsh Bridge in the centre of
Shrewsbury.
The model of the river Severn used in this study was originally set up by the Environment
Agency, using Hec-Ras, a commercially-used hydraulic modelling package developed by the
US Army Corps of Engineers (2002), and described in Section 2.3.2.2. This was applied to
a 60km reach from Montford to well below Shrewsbury. Upstream boundary conditions are
provided by hourly gauged stage at Montford, while initial ﬂow conditions along the reach
were speciﬁed as the upstream gauged ﬂow at the beginning of the modelled time frame.
Downstream boundary condition was provided by the normal depth equation (Manning’s
1Data obtained from EDINA (Edinburgh University Data Library).

Chapter 6. Calibration of a dynamic ﬂood model
79
Figure 6.1: Digital elevation map of the Severn catchment in the area around Shrewsbury,
with the river course superimposed, and gauging stations marked in red. Modelled ﬂood
relief channel in Shrewsbury marked with dotted line.
equation), relating ﬂow to slope.
Taking the historical record at Montford for the period of high ﬂow from 15th January
to 7th March 2002, Hec-Ras has been run for a number of (constant) values of Manning’s
n for the channel. As in the previous study the value for the ﬂoodplain roughness was
not varied, since the output was found to be insensitive to ﬂoodplain roughness. Figure
6.2 shows both the gauged stage at Montford, and the model output at Welsh Bridge
in Shrewsbury, together with gauged stage. It can be seen that no parameter value will
enable the model to reproduce the output data.
6.5
Calibration
6.5.1
Emulator construction
The emulator chosen for this river reach is due to Romanowicz et al. (2008). This is
a nonlinear transfer function, consisting of an ARX model (autoregressive model with
exogenous input), applied to the output of a nonlinear transformation of the upstream
stage measurement. An ARX model can be described as a relationship between an input
time series xt, t = 1, · · · , n and output time series yt, t = 1, · · · , n as follows:
yt = a1yt−1 + a2yt−2 + · · · + apyt−p + b0xt−l + b1xt−l−1 + · · · bqxt−l−q + ϵt
where l refers to the lag between input and output, and ϵt is an uncorrelated noise series.
The model to be used to describe the relationship between input and output is the most
parsimonious model justiﬁed by the data. In this case, since a nonlinear transformation is
applied to the input data before formulating the ARX model, the model order is described

Chapter 6. Calibration of a dynamic ﬂood model
80
0
200
400
600
800
1000
1200
53
54
55
56
57
58
time, h
Upstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
53
time, h
Downstream stage, mAOD
b)
Model output, Manning's n = 0.02
Model output, Manning's n = 0.03
Model output, Manning's n = 0.04
Model output, Manning's n = 0.05
Measured stage
Figure 6.2: a) Gauged upstream stage at Montford, 15th Jan - 7th Mar 2002, b) Output
of Hec-Ras model, with input of gauged upstream stage, for diﬀerent values of Manning’s
n, compared with gauged downstream stage at Welsh Bridge in Shrewsbury, showing that
no value of the parameter will allow the model output to correspond with the data
by p = 1, q = 0 in the above, giving
yt = ayt−1 + bxt−l + ϵt
(6.2)
Heuristic justiﬁcation for the choice of such a nonlinear transfer function can be demon-
strated with respect to Figure 6.3. If the linear ARX model, Equation 6.2, is applied
to the scaled upstream stage (Figure 6.2a), and taking the lag to be 0, the output yt is
approximately linearly related to the exogenous input xt (Figure 6.3a). If, however, the
input is, for example, squared before applying the ARX model, so that the relationship
is
yt = ayt−1 + bx2
t + ϵt
(6.3)
then the relationship between the input and output (Figure 6.3b) is clearly nonlinear.
A plot of the downstream measured hydrograph against the upstream hydrograph (Fig-

Chapter 6. Calibration of a dynamic ﬂood model
81
0.4
0.6
0.8
1.0
1.0
1.5
2.0
x
y
a)
0.4
0.6
0.8
1.0
0.5
1.5
2.5
x
y
b)
Figure 6.3: Heuristic motivation for emulator strategy: a) input-output plot, when output
is determined by Equation (6.2); b) input-output plot, when output is determined by
Equation (6.3)
1
2
3
4
5
6
1
2
3
4
upstream stage x
downstream stage
a)
1
2
3
4
1
2
3
4
transformed upstream stage b(x)
downstream stage
b)
Figure 6.4: Input-output plots: a) downstream v. upstream gauged stage (relative to
local data- 47mAOD and 52mAOD respectively); b) downstream stage v. transformed
upstream gauged stage
ure 6.4a), demonstrates a nonlinear relationship. The modelling method suggested by
Romanowicz et al. is to ﬁnd a nonlinear function b(·) so that yt and b(xt−l) are related
by the ﬁrst order ARX model yt = ayt−1 + xt−lb(xt−l). Figure 6.4b shows the eﬀect of
applying an appropriate transformation b(xt−l) on the linearity of the relationship with yt.
It should be noted that the hysteretic eﬀect seen in Figure 6.4a is due to the lag between
the upstream and downstream signal. Choice of an appropriate lag reduces the ampli-
tude of the “loops” in the relationship, while straightening is eﬀected by the nonlinear
transformation.
Romanowicz et al. (2008) used the function SDP of the CAPTAIN time series analysis
toolbox (Taylor et al., 2007) to determine the nonlinear transformation. This method
involves reordering the input and output data in terms of increasing output stage, and
solving for a variable-parameter ARX model with respect to the stage rather than with
respect to time.
The reordering yields a smooth parameter variation with respect to
the input stage. The resulting parameter variation is in eﬀect a non-parametric function

Chapter 6. Calibration of a dynamic ﬂood model
82
of input data values; in order to use it further, a parametric model has to be ﬁtted to
this.
In this study, instead of using the SDP function, the nonlinearity is described as a de-
terministic spline function of the upstream stage, so that b(xt) = b(xt|xτ, τ = 1, . . . , nτ).
The function values b(xτ) at the spline knots are found using the transfer function iden-
tiﬁcation routine RIV from the CAPTAIN toolbox as an inner loop of an optimisation,
as illustrated in Figure 6.5. The lag used is that which minimises the ﬁtting error for the
optimal model.
Figure 6.5: Schematic of identiﬁcation procedure for nonlinear transfer function
Applying this procedure to the hydraulic model input and output for diﬀerent values of
the parameter Manning’s n, and also for the upstream and downstream gauged stage,
leads to such a representation for each case. These are shown in Figure 6.6. It should be
noted that the stage is deﬁned for this transformation relative to a local datum, and the
choice of data at the two gauges is signiﬁcant in this transformation. Data were chosen
to ensure that the scaled nonlinear function b(xτ)
1 −a was nearly constant through the range
of xτ, as this reduced the numerical errors at later stages of the analysis, caused by trend
in the scaled nonlinear function swamping the eﬀects of a change in shape. Local data
were chosen as large as possible, while ensuring that the relative stages were positive at
all times.
Once parameters have been identiﬁed for the autoregressive function a and the spline
knot values b(xτ), time series can be recovered by application of the model yt = ayt−1 +
xt−lb(xt−l) to the upstream hydrograph, x1, · · · , xn. This has been done, both for the
calibration period, and for a validation period, 25th October - 15th December, 2002 (Figure
6.7). Agreement between the recovered time series and the output of the hydraulic model
is good for both time periods, and all values of Manning’s n. Root-mean-squared error
is 0.026m, 0.046m, 0.069m and 0.080m respectively for the four cases illustrated for the
calibration period, and 0.027m, 0.043m, 0.080m, and 0.105m respectively for the cases
for the validation period.
This error represents a baseline error in calibration, as the
calibration exercise and further analysis is based on the emulator.

Chapter 6. Calibration of a dynamic ﬂood model
83
0
1
2
3
4
5
6
7
0.5
0.6
0.7
0.8
0.9
Upstream stage, m above local datum
Scaled nonlinear function b/(1+a)
a)
0.01
0.02
0.03
0.04
0.05
0.06
−0.6
−0.4
−0.2
0.0
Manning's n
autoregressive coefficient, a
G
G
G
G
G
b)
G
G
G
G
G
model, n = .02
model, n = .022
model, n = .03
model, n = .04
model, n = .05
data
Figure 6.6: Components of nonlinear transfer function for computer output with diﬀer-
ent values of Manning’s n and for data: a) nonlinear function values,
b(·)
(1 −a), and b)
autoregressive coeﬃcient, a.
0
200
400
600
800
1000
1200
48
49
50
51
52
time, h
Downstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
time, h
Downstream stage, mAOD
b)
Recovered time series, n=0.02
Recovered time series, n=0.03
Recovered time series, n=0.04
Recovered time series, n=0.05
Hydraulic model output
Figure 6.7: Comparison with hydraulic model output of time series recovered from non-
linear functions and autoregressive coeﬃcients applied to the upstream hydrographs, for
a) time period (15th January to 7th March 2002) used for estimation and b) validation
time period (25th October - 15th December, 2002).
Three formulations have been investigated, to use the above transformation as an emulator.
They are described below.

Chapter 6. Calibration of a dynamic ﬂood model
84
6.5.1.1
Gaussian process emulator, with calibration in the time domain
In what follows in this section, the spline described above and the CAPTAIN package
are not used. Instead, a Gaussian process is invoked to serve the purpose of the spline,
deﬁning the time-domain nonlinearity in a stochastic manner. In this formulation, the
conditioning points of the Gaussian process serve a similar purpose to spline knots.
If xt and ηt(θ) are time series, representing measured upstream river level, and calculated
downstream river level from the hydraulic model with parameter θ, assume that the model
runs can be represented by an ARX model with input nonlinearity, as follows:
ηt(θ) = a(θ)ηt−1(θ) + xt−lb(xt−l, θ)
(6.4)
where xt−lb(xt−l, θ) is represented by a Gaussian process.
Simultaneous estimation of ηt(θ) and b(xt−l, θ) for all time values requires the covariance
matrix Σi,j =
X
t
ηt−iηt−j. This covariance matrix is not easy to formulate in the context of
model (6.4); for the simpliﬁed model where ηt(θ) = a(θ)ηt−1(θ)+ϵt, where ϵt is uncorrelated
Gaussian noise, the covariance matrix would be Σi,j = σ2
a (a(θ))|i−j| (cf. Equation 3.2).
Equally, for the model ηt(θ) = xt−lb(xt−l, θ), the covariance could be reasonably taken as
Σi,j = σ2
b exp
 −ω(xi −xj)2
(cf. Equation 3.3). However, the formulation of a covariance
expression is more complicated for the model (6.4) where both of the terms a(θ)ηt−1(θ)
and xt−lb(xt−l, θ) are present.
Dropping the function arguments and invoking the Gaussian process b′
t = xt−lb(xt−l, θ)
for clarity, the value ηt can be rewritten as follows:
ηt = aηt−1 + b′
t
= a2ηt−2 + ab′
t−1 + b′
t
= arηt−r +
r−1
X
s=0
asb′
t−s
The covariance term is then
X
t
ηtηt−r = ar X
t
η2
t−r +
X
t
r−1
X
s=0
asb′
t−sηt−r
It is not obvious how to evaluate the ﬁnal cross term P asb′
t−sηt−r, since b′
t−s and ηt−r
are correlated, so it must be eliminated. This is only feasible if b′ represents a zero mean
Gaussian process; however since in reality this is not the case, the cross terms cannot
be eliminated, and the covariance cannot be evaluated.
This formulation is thus not
practicable.
It is possible that the analysis could have been reformulated to estimate
the calibration recursively thriough time. Young (1984) pointed out that the inability
to formulate the covariance relationship correctly is one reason why recursive time-series
estimation is to be preferred.

Chapter 6. Calibration of a dynamic ﬂood model
85
The inability to formulate the emulator in the time domain has consequences for the
estimation of noise error in calibration. Under these circumstances, calibration has to be
undertaken in the transformed domain of the autoregressive coeﬃcient and the spline knot
values.
If the autoregressive coeﬃcient a and the spline knot values b(xτ) are found by optimisation
as described in Section 6.5.1 with reference to Figure 6.5, then the estimation errors are
minimised during the optimisation, without taking into account any prior knowledge.
Since the same procedure is used for both the hydraulic model output and the measured
data, the process of transforming from the time domain to the domain of the autoregressive
coeﬃcient and the spline knot values does not in itself allow for measurement error. If now
the calibration is performed in the transformed domain, error estimation is undertaken
in this domain. However, the structure of the estimated error, transformed back into the
time domain, is no longer that of Gaussian noise.
6.5.1.2
Gaussian process emulator for the spline knot values, with calibration
in the transformed domain
An alternative representation of the transformation described in Section 6.5.1.1 above,
is to take two Gaussian processes in the domain of the autoregressive coeﬃcients a(θ)
and the scaled nonlinear function values at the spline knots b(xτ, θ)
1 −a(θ) respectively, deﬁning
suitable mean functions as in previous examples. Examination of the variation represented
in Figure 6.6 indicates that the scaled function values do not vary independently with
parameter θ. Thus, a two-dimensional Gaussian process is invoked for the scaled nonlinear
function values:
ηb(xτ, θ) ∼N
 Hb(xτ, θ)βb, Vb

where
Vb
 (xτ, θ), (x′
τ, θ′)

= σ2
bexp
 −ωbx(xτ −x′
τ)2 −ωbθ(θ −θ′)2
The autoregressive coeﬃcient depends on θ alone:
ηa(θ) ∼N
 Ha(θ)βa, Va

where
Va
 θ, θ′
= σ2
aexp
 −ωa(θ −θ′)2
Emulation is undertaken in the domain of these Gaussian processes, using for data out-
put from an optimisation based on the CAPTAIN package. However, to avoid stability
problems in subsequent recovery of the time series, ηa is deﬁned in terms of an arctanh
transformation of the data, a′ = arctanh(a); this transformation is also implicit in ηb,
which is deﬁned in terms of the transformed data b′′ =
b
1 −a′ . As in previous examples,
the parameters βb, ωbx, ωbθ, σ2
b, βa, ωa and σ2
a are to be estimated by MCMC. An assump-

Chapter 6. Calibration of a dynamic ﬂood model
86
tion is made that the scaled nonlinear functions are uncorrelated with the autoregressive
coeﬃcient, so that there are in eﬀect two independent emulators. This last assumption is
an over-simpliﬁcation of the reality; it has been seen empirically that the values at diﬀerent
knot points have diﬀerent correlation with a.
Referring to Figure 6.6, regression bases are taken as (1, xτ, x2
τ, θ) for the nonlinear func-
tions, and (1, (θ −1.5)−2) for the autoregressive coeﬃcients. Deﬁning the parameter θ as
100 * Manning’s n, the emulator equations were solved for the logs of the hyperparame-
ters. Prior and posterior distributions are given in Table 6.1; note that the distributions
for the ω values are tighter than for the other variables. This reﬂects sensitivity of the
solution to these values, in particular to the value of ωbx.
Table 6.1: Prior and posterior distributions for emulation
Variable
prior
posterior
posterior
distribution
mean
standard deviation
βb1
0.70
0.01
βbx
-0.13
0.00
βbx2
0.06
0.00
βbθ
0.01
0.04
βa1
-0.42
0.04
βa(θ−1.5)−2
0.05
0.02
log(ωbx)
N(0, 12)
-0.25
0.44
log(ωbθ)
N(−1, 12)
-1.83
0.39
log(ωaθ)
N(0, 0.72)
-0.12
0.79
log(σ2
b)
N(−4.6, 22)
-6.31
0.46
log(σ2
a)
N(−4.6, 22)
-2.76
0.87
As in previous examples, the emulator errors and variance are calculated for particular
points in (x, θ). Figure 6.8 shows the 95% prediction intervals for three values of Manning’s
n, midway between pairs of values where the hydraulic model has been run, compared
with validation output from the hydraulic model. It can be seen, as in calibration output
in previous chapters, that the uncertainty increases with distance from the spline knot
values. In this case, the variation of the error variance appears more pronounced, since
the emulator model does not include observation noise.
Note that the widths of the
prediction intervals never quite shrink to zero in Figure 6.8, as the Manning’s n values are
between the spline knots. The root mean variances for the nonlinear functions in Figure
6.8 are all 0.0094, with standard deviations on the autoregressive coeﬃcients of 0.060,
0.101 and 0.109 respectively.
It should be noted that unlike the toy example described in Chapter 3, the variance of
the posterior distribution depends heavily on the prior distributions for the “roughness”
coeﬃcients ω. These are also not easy to determine using the reference curves in Figure
3.5 (and allowing for scaling) for comparatively small values of ω. The prior distributions
here were chosen, since they gave comparatively small posterior variances for the nonlinear
function and autoregressive coeﬃcients. Needless to say, a small variance in the transfer

Chapter 6. Calibration of a dynamic ﬂood model
87
function coeﬃcients can be expected to translate into a small variance in the output time
series.
1
2
3
4
5
6
7
0.5
0.6
0.7
0.8
0.9
Upstream stage, m above local datum
Scaled nonlinear function b/(1−a)
a)
0.01
0.03
0.05
0.0
0.2
0.4
0.6
0.8
1.0
Manning's n
autoregressive coefficient, a
G
G
G
b)
G
95% prediction interval, n=0.025
95% prediction interval, n=0.035
95% prediction interval, n=0.045
Nonlinear functions for validation
Predicted AR coefficient
AR coefficients for validation
Figure 6.8: Predicted a) nonlinear functions and b) autoregressive coeﬃcients for values of
Manning’s n between those used to construct the emulator, compared with values obtained
directly from the hydraulic model output
Transformation to the time domain has been achieved by simulation, as follows.
For
each retained member of the Markov chain, estimates are calculated of the conditional
means
E

ηb(x†
τ, θ†)|b′′(xτ, θ), βb, ωbx, ωbθ, σ2
b

and E

ηa(θ†)|a′(θ), βa, ωa, σ2
a

and variances
Var

ηb(x†
τ, θ†)|b′′(xτ, θ), βb, ωbx, ωbθ, σ2
b

and Var

ηa(θ†)|a′(θ), βa, ωa, σ2
a

of the nonlinear function and (arctanh) autoregressive coeﬃcient values, where we recall
that a′(θ) = arctanh (a(θ)) and b′′(xτ, θ) =
b(xτ, θ)
1 −a′(θ).
Values of ηb(x†
τ, θ†) and ηa(θ†)
can be drawn from these distributions, and a number of such draws used to evaluate a
conditional time series, using the time series model, ηt(θ) = tanh(ηa(θ))ηt−1(θ) + xt−l(1 −
tanh(ηa(θ)))ηb(xt−l, |xτ, θ). A full conditional distribution is achieved by repeating the
process for each member of the Markov chain.
In practice, only one draw is needed
from each conditional distribution, as further draws do not add to the accuracy of the
unconditional distribution. Statistics of the output time series are then calculated from the
ensemble of the output distributions at each time value. Use of the arctanh transformation
for a ensures that the simulated conditional time series remain stable, even when the mean
and variance of ηa are not small.
On transformation to the time domain, use of the scaling in the calculation of b′′ =
b(·)
(1 −a′) ensures that the errors are comparatively small, by minimising the impact of the
uncertainty in a. However, Figure 6.9 shows that the emulation errors are not negligible,
in particular for the curve with the higher Manning’s n value; the mean of the 95% inter-
quantile range of the three curves are 0.107m, 0.154m and 0.267m respectively, although

Chapter 6. Calibration of a dynamic ﬂood model
88
0
200
400
600
800
1000
1200
48
49
50
51
52
time, h
Downstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
time, h
Downstream stage, mAOD
b)
Hydraulic model output
95% prediction interval, n=.025
95% prediction interval, n=.035
95% prediction interval, n=.045
Figure 6.9: Predicted time series for values of Manning’s n away from those used to
construct the emulator, compared with hydraulic model output for a) original time period
(15th January to 7th March 2002) and b) validation time period (25th October - 15th
December, 2002).
the root mean squared error of the median time series with regard to the output of the
hydraulic model are 0.031m, 0.052m and 0.063m respectively. For the validation period,
the equivalent values for the 95% inter-quantile ranges are 0.086m, 0.122m and 0.213m,
while the root mean squared errors of the median time series are 0.025m, 0.052m and
0.070m.
The prediction range increases, not with mean predicted stage, but with the
slope of the predicted stage. Figure 6.10 shows the relationship between the prediction
range and the slope of the predicted stage, with a 2-hour lag, for Manning’s n = 0.045,
for both time periods considered, indicating a strong linear dependence. This is consistent
with error caused by uncertainty in the autoregressive coeﬃcient, which will aﬀect the
timing of the predicted peak.
The errors in extrapolation have not been estimated. This is because the hydraulic model
failed to compute outside the range which had been used for emulator construction.
There is some potential confusion in this formulation between the representation of the
nonlinear functions as splines b(xt|xτ, τ = 1, . . . , nτ), and the variation of the knot values
b(xτ) as Gaussian process functions of (x, θ). This would have been avoided if it had been
possible to represent the nonlinear functions as Gaussian processes, as was proposed in
Section 6.5.1.1.

Chapter 6. Calibration of a dynamic ﬂood model
89
G
G
GG
G
G
GGG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GGG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
GGG
G GG
GG
G
G GG GGG
GGGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G G
G
G
G
G
G
G
G
GGG
G
G G
G
GG
GGG
G G
GG
G
G
G
GGGG G GG G
G
G
G
G
G
G
G
−0.1
0.0
0.1
0.2
0.2
0.4
0.6
0.8
rate of increase of stage, m/s
95% prediction interval, m
a)
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
GGG G G
G
G
G
G
G
G
GG
G
G GG
G
G
G
G
G
G
G
G
G G GGGGGG
G
G
G
G
G
G
G
G
GG
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G G
G
GGG
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GGGG G GG
GG
G GGG G
G
G
GGGGG G
G
G G
G
G
G
G
GGGG
G
G
G
GG G GGGGG
G
G
G
G
G
G
GG
GGGG
G
G
GG
GGGGGG
G
G
GG
G
G
G
G
G
G
G
G
GG
G
GGG G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GGG
GG
G
GG G GG G GG
GG GGGGG
GGGG
GG
GG
GG
GGG
GG
GGGGGG
GGGGG
G
G
G
G
GGGGGG
−0.2
−0.1
0.0
0.1
0.2
0.0
0.2
0.4
0.6
0.8
1.0
rate of increase of stage, m/s
95% prediction interval, m
b)
Figure 6.10: 95% prediction interval for Manning’s n = 0.045, compared with the slope of
the predicted stage for a) original time period (15th January to 7th March 2002) and b)
validation time period (25th October - 15th December, 2002).
6.5.1.3
Spline emulator, with calibration in the transformed domain
Avoiding the diﬃculty in speciﬁcation of prior distributions for the “roughness” coeﬃ-
cients, and confusion in switching between spline and Gaussian process representation of
the nonlinear functions, the transformation can simply be represented by a pair of spline
functions, the arctanh of the autoregressive coeﬃcient depending simply on θ and the
scaled nonlinear function depending on both x and θ, again using for data output from
optimisations based on the CAPTAIN package. This is a deterministic representation,
and makes no allowance for the errors in determination of the original values, or of the in-
terpolation errors for diﬀerent θ values. For comparison with Figures 6.8 and 6.9, Figures
6.11 and 6.12 show the equivalent projection of the spline functions for parameter values
away from those used to construct the splines. While there are no estimates involved in
these projections, a very slight disagreement between the spline projections for high input
stage in Figure 6.11 is reﬂected in a slight underestimate in the peaks of the output time
series in Figure 6.12a.
6.5.2
Calibration
Calibration has been undertaken separately using the last two of the emulator formulations
described above. As with these emulator formulations, calibration is undertaken in the

Chapter 6. Calibration of a dynamic ﬂood model
90
1
2
3
4
5
6
7
0.5
0.6
0.7
0.8
0.9
Upstream stage, m above local datum
Scaled nonlinear function b/(1−a)
a)
0.01
0.03
0.05
0.0
0.2
0.4
0.6
0.8
1.0
Manning's n
autoregressive coefficient, a
G
G
G
b)
G
Spline prediction, n=0.025
Spline prediction, n=0.035
Spline prediction, n=0.045
Nonlinear functions for validation
Predicted AR coefficient
AR coefficients for validation
Figure 6.11: Projected a) nonlinear functions and b) autoregressive coeﬃcients, for values
of Manning’s n between those used to construct the spline, compared with values obtained
directly from the hydraulic model output
0
200
400
600
800
1000
1200
48
49
50
51
52
time, h
Downstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
time, h
Downstream stage, mAOD
b)
Hydraulic model output
Predicted output, n=.025
Predicted output, n=.035
Predicted output, n=.045
Figure 6.12: Projected time series for values of Manning’s n away from those used to
construct the spline, compared with hydraulic model output for a) original time period
(15th January to 7th March 2002) and b) validation time period (25th October - 15th
December, 2002).
transformed domain of the scaled nonlinear function and autoregressive coeﬃcient, shown
in Figure 6.6.

Chapter 6. Calibration of a dynamic ﬂood model
91
6.5.2.1
Calibration with Gaussian process emulator
As in the steady state ﬂooding example, the emulator and model inadequacy are jointly
estimated. However, in formulating the calibration no model inadequacy has been allowed
for in the autoregressive coeﬃcient, since a one-dimensional variable does not provide
suﬃcient information to estimate such a quantity. Joint calibration of two quantities, one
of which is estimated without model inadequacy, implies that there is a direct trade-oﬀ
between the estimated parameter value and the posterior variance of that quantity. It is
noted that the emulator analysis already resulted in a large uncertainty on the estimate
of the autoregressive coeﬃcient.
The model used is as follows:
d =






yb
ya
zb
za






∼N
 Hβ, V

(6.5)
where
H =






H1b
0
0
0
H1a
0
H1b
0
H2b
0
H1a
0






β =



β1
β2
β3



and
V
 (x, θ), (x′, θ′)

=






Σ1b
0
Σ1b
0
0
Σ1a
0
Σ1a
Σ1b
0
Σ1b + Σ2b + σ2
ϵbI
0
0
Σ1a
0
σ2
ϵa






where deﬁnitions are analogous to those in Chapter 5. Note that there are three regres-
sion basis submatrices, H1a, H1b and H2b, two for the emulators for the autoregressive
coeﬃcient and the nonlinear function, and one for the model inadequacy for the nonlinear
function. Similarly, the covariance matrices Σ1b and Σ2b refer to the emulator and model
inadequacy for the nonlinear function. Σ1a refers to the variance of the autoregressive
model, while σ2
ϵb and σ2
ϵa are scalar quantities arising from ARX model representation
errors in the nonlinear function and autoregressive coeﬃcient respectively. The interpre-
tation of σ2
ϵb and σ2
ϵa as physical quantities are diﬃcult, as they refer to a transformed
variables, and are diﬃcult to understand in the transformed space.
Once again, the input data are transformed to a′ = arctanh(a) and b′′ =
b
1 −a′ . The
regression bases for the emulator are h1b = (1, x, x2, θ) for the nonlinear function, and

Chapter 6. Calibration of a dynamic ﬂood model
92
h1a = (1, (θ −1.5)−2) for the autoregressive coeﬃcient, while for the model inadequacy
h2b = (1, x) has been used for the nonlinear function. The covariance functions are taken
to be smooth as in Equation (3.3), with diﬀerent hyperparameters for each of the three
covariance functions. These hyperparameters are to be estimated, together with the pa-
rameter θ and the variance of two sources of error, σ2
ϵb for inaccuracies in ﬁnding the
nonlinear functions, and σ2
ϵa for the autoregressive coeﬃcient. Prior and posterior distri-
butions for the parameter θ (=100 * Manning’s n) and the log hyperparameters are shown
in Table 6.2.
Table 6.2: Prior and posterior distributions for calibration (ﬁrst method)
Variable
prior
posterior
posterior
distribution
mean
standard deviation
β1b1
0.60
0.09
β1bx
-0.02
0.01
β1bx2
0.02
0.05
β1bθ
0.01
0.01
β1a1
-0.42
0.21
β1a(θ−1.5)−2
0.05
0.04
β2b1
-0.13
0.08
β2bx
0.03
0.02
θ
N(3.5, 0.52)
3.23
0.39
log(ω1bx)
N(0, 12)
-0.32
0.42
log(ω1bθ)
N(−1, 12)
-2.02
0.39
log(ω1aθ)
N(0, 0.72)
-0.12
0.69
log(ω2bx)
N(0, 1.42)
-1.06
1.14
log(σ2
1b)
N(−4.6, 22)
-5.70
0.41
log(σ2
1a)
N(−4.6, 22)
-2.79
0.83
log(σ2
2b)
N(−4.6, 22)
-6.09
1.55
log(σ2
ϵb)
N(−4.6, 22)
-8.85
1.05
log(σ2
ϵa)
N(−4.6, 22)
-5.12
1.82
Calibrated prediction of the input data yields the posterior distributions shown in Figure
6.13 for the parameter, Manning’s n, for the nonlinear functions and the autoregressive
coeﬃcient. The root mean variance of the estimate for the nonlinear function is 0.014, with
a mean error with respect to the transformed output data of 0.0012, while the standard
deviation of the estimate for the autoregressive function is 0.094 with a mean error with
respect to the autoregressive coeﬃcient associated with the data of 0.013. Transformation
to the time domain is done in much the same way as the transformation of the emulator
estimate, and has again been done for both the calibration and validation time series
(Figure 6.14). For the calibration period, the mean of the 95% inter-quantile range is
0.2m, although the root mean squared error of the median time series with regard to the
output of the hydraulic model is 0.004m. For the validation period, the equivalent value
for the 95% inter-quantile range is 0.15m, while the root mean squared error of the median
time series is 0.017m.

Chapter 6. Calibration of a dynamic ﬂood model
93
1
2
3
4
5
6
7
0.5
0.6
0.7
0.8
0.9
Upstream stage, m above local datum
Scaled nonlinear function b/(1−a)
a)
G
G
G
G
G
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
autoregressive coefficient, a
G
G
G
G
G
G
model output, different n values
data
95% prediction interval
b)
Manning's n
probability
density
0.020
0.030
0.040
0.050
c)
Figure 6.13: Calibrated prediction, using ﬁrst calibration method, for a) nonlinear func-
tion, b) autoregressive coeﬃcient a and c) predictive distribution for Manning’s n
0
200
400
600
800
1000
1200
48
49
50
51
52
time,h
Downstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
time,h
Downstream stage, mAOD
b)
data
model output
95% prediction interval
Figure 6.14: Calibrated prediction for stage at Welsh Bridge in Shrewsbury, using ﬁrst
calibration method, compared with model predictions and observed data: a) time period
used in calibration, b) validation time period
6.5.2.2
Spline emulator
Use of a deterministic spline to encode the model output makes the problem much easier, as
there is no uncertainty associated with the emulator. Once more, the calibration problem
may be formulated in the domain of the autoregressive coeﬃcients and nonlinear functions,
this time without emulator uncertainty, referencing the spline functions directly. As with

Chapter 6. Calibration of a dynamic ﬂood model
94
the formulation in section 6.5.2.1 above, calibration is performed simultaneously for the
autoregressive coeﬃcient and nonlinear function, again, with a model inadequacy for the
nonlinear function, but not for the autoregressive coeﬃcient.
Thus, the model used is:
d =
 
zb
za
!
−
 
b′′(xτ, θ)
a′(θ)
!
∼N
 Hβ, V

(6.6)
where
H =
 
Hδb
0
!
β =
 
βb
0
!
and
V ((x, θ), (x∗, θ∗)) =
 
Σδb + σ2
ϵbI
0
0
σ2
ϵa
!
and b′′(xτ, θ) and a′(θ) are the spline functions embodying the transfer functions, respec-
tively the scaled nonlinear functions and the transformed autoregressive coeﬃcients. Other
deﬁnitions are analogous to those in Chapter 5.
As before, the regression basis for the model inadequacy hδb(x) = (1, x) has been used for
the nonlinear function. The covariance functions are taken to be smooth, as in Equation
(3.3). These hyperparameters are to be estimated, together with the parameter θ, and the
variance of two sources of error, σ2
ϵb for inaccuracies in ﬁnding the nonlinear functions, and
σ2
ϵa for the autoregressive coeﬃcient. Prior and posterior distributions for the parameter
θ and the log hyperparameters are given in Table 6.3.
Table 6.3: Prior and posterior distributions for calibration (second method)
Variable
prior
posterior
posterior
distribution
mean
standard deviation
βδb1
-0.14
0.02
βδbx
0.09
0.02
θ
N(3.5, 0.52)
3.59
0.35
log(ωδbx)
N(0, 1.42)
-0.81
1.22
log(σ2
δb)
N(−4.6, 22)
-5.23
1.69
log(σ2
ϵb)
N(−4.6, 22)
-6.30
1.38
log(σ2
ϵa)
N(−4.6, 22)
-8.53
0.93
Calibrated prediction of the input data yields the posterior distributions shown in Figure
6.15 for the parameter, Manning’s n, for the nonlinear functions and the autoregressive
coeﬃcient. The root mean variance of the estimate for the nonlinear function is 0.014,
with a root mean squared error with respect to the transformed output data of 0.0006,
while the standard deviation of the estimate for the autoregressive function is 0.109 with a
mean error with respect to the autoregressive coeﬃcient associated with the data of 0.028.

Chapter 6. Calibration of a dynamic ﬂood model
95
Transformation to the time domain is done in much the same way as the transformation
of the emulator estimate, and has again been done for both the calibration and validation
time series (Figure 6.16). For the calibration period, the mean of the 95% inter-quantile
range is 0.2m, although the root mean squared error of the median time series with regard
to the output of the hydraulic model is 0.006m. For the validation period, the equivalent
value for the 95% inter-quantile range is 0.15m, while the root mean squared error of the
median time series is 0.018m.
1
2
3
4
5
6
7
0.5
0.6
0.7
0.8
0.9
Upstream stage, m above local datum
Scaled nonlinear function b/(1−a)
a)
G
G
G
G
G
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
autoregressive coefficient, a
G
G
G
G
G
G
model output, different n values
data
95% prediction interval
b)
Manning's n
probability
density
0.020
0.030
0.040
c)
Figure 6.15: Calibrated prediction, using second calibration method, for a) nonlinear
function, b) autoregressive coeﬃcient a and c) predictive distribution for Manning’s n
A comparison between the predictive distributions for Manning’s n in Tables 6.3 and 6.3
indicates at ﬁrst sight that the distributions are diﬀerent. In addition, the distribution in
Figure 6.13c is positively skewed, while that in Figure 6.15c is negatively skewed. However,
further analysis shows that the two posterior distributions are not signiﬁcantly diﬀerent.
The width of the posterior intervals is in addition inﬂuenced by the vague prior distribution
on the parameter. With more information about the river reach, it would be possible to
specify a tighter prior distribution. There thus appears to be little to choose between the
output of the two calibration methods detailed above. However, the second calibration
method has been chosen for further use, for simplicity.
6.6
Discussion
In applying the foregoing analysis to further examples, it is necessary to consider the
resilience of the uncertainty estimates to a number of implementational and methodological
choices. These include the choice of time interval for the calibration data and the choice of
spline knot position, the stability of the calibration to input noise, and to the determination
of lag in the ARX model.

Chapter 6. Calibration of a dynamic ﬂood model
96
0
200
400
600
800
1000
1200
48
49
50
51
52
time,h
Downstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
time,h
Downstream stage, mAOD
b)
data
model output
95% prediction interval
Figure 6.16: Calibrated prediction for stage at Welsh Bridge in Shrewsbury, using second
calibration method, compared with model predictions and observed data: a) time period
used in calibration, b) validation time period
6.6.1
Stability of the emulator formulation to spline knot position and
to lag value
In order to check the stability of the optimisation calculation, it was repeated for 5, 7,
9, and 11 equally spaced spline knots, and at diﬀerent lag values. It was found that the
scaled spline values were stable to the number of knots, although 5 knots appeared to be
somewhat coarse for the shape of the curve. The autoregressive coeﬃcient was also stable,
provided that the calculation was done with the correct lag.
However, if a lag was used other than the optimal lag identiﬁed for the transfer function, it
was found that the autoregressive coeﬃcient could change as the number of knot locations
were increased. In addition, with a suboptimal lag, the autoregressive coeﬃcient was not
stable to uncorrelated noise, added to the input hydrograph, before running the hydraulic
model. In this case, if the test was repeated using a number of diﬀerent noise series, there
appeared to be two groups of a values; however, the spline values, scaled by (1 −a), were
always stable.
This instability of the calculation when undertaken with the wrong lag, highlights a prob-
lem with the assumption of uniform lag. Since the emulator is constructed by running an
optimisation to ﬁnd the nonlinear spline function and autoregressive coeﬃcient for each

Chapter 6. Calibration of a dynamic ﬂood model
97
Manning’s n value, repeating the calculation at diﬀerent lags to ﬁnd the best one, the
optimal lag is found to increase with increasing Manning’s n. The emulator, however,
has not been constructed to allow for diﬀerent lags at diﬀerent Manning’s n values. In
the construction of the emulator, therefore, a compromise lag is chosen, giving a subop-
timal solution for some Manning’s n values, but which leads to consistent autoregressive
coeﬃcients.
Application of this emulator to a longer reach may give rise to diﬃculties in the choice of a
compromise lag. While the diﬀerence between lags for diﬀerent Manning’s n values is not
great for the reach between Montford and Welsh Bridge, it can be expected to increase for
a longer reach. This is illustrated in Table 6.4, which shows the optimal lags found from
linear ARX(1,0) models applied to Hec-Ras input and output for this reach, and for the
reach between Montford and Buildwas, a further 35km downstream.
Table 6.4: Lag found in best ARX(1,0) model applied to Hec-Ras output
Manning’s n
lag at Welsh Bridge
lag at Buildwas
(hours)
(hours)
.02
0
7
.03
2
10
.04
4
12
.05
5
15
6.6.2
Comparison with Bayesian calibration, without model inadequacy
In order to understand the impact of a model inadequacy representation, a simple Bayesian
calibration has been performed in the time domain, using the spline emulator instead of
embedding the hydraulic model in the calibration. The data model used was:
et = zt −
 a(θ)zt−1 + xt−lb(xt−l, θ)

∼N(0, σ2)f(e|θ, σ2, a, b)
L(et|θ, σ2) ∝1
σn exp
P e2
t
2σ2

Manning's n
probability
density
0.0260
0.0270
0.0280
0.0290
Figure 6.17: Posterior distribution of param-
eter Manning’s n, found with simple calibra-
tion method
Priors for θ and σ2 were the same as those
in the other calibrations in this chapter.
The posterior distribution for θ is very
narrow (Figure 6.17), with mean 0.0274,
and standard deviation 0.0003. The com-
parative narrowness of the distribution re-
ﬂects the lack of a model inadequacy func-
tion, since in the other formulations exam-
ined in this thesis, it could be said that the
model inadequacy function has weakened

Chapter 6. Calibration of a dynamic ﬂood model
98
the model parameter identiﬁability. Bearing in mind this narrow distribution, it is not
surprising that the calibrated prediction is suﬃciently narrow, that the 95% prediction
interval cannot be seen at the scale of Figure 6.18; the mean standard deviation is 0.12m
for the calibration period and 0.08m for the validation period. In spite of this narrow
prediction interval, it is plain that the model does not ﬁt the data, as was already evident
from Figure 6.2. This is thus a case of an over-ﬁtted model; the eﬀect of the calibration
with model inadequacy is eﬀectively to prevent over-ﬁtting.
0
200
400
600
800
1000
1200
48
49
50
51
52
time,h
Downstream stage, mAOD
a)
0
200
400
600
800
1000
1200
48
49
50
51
52
time,h
Downstream stage, mAOD
b)
data
model output
mean predicted output
Figure 6.18: Calibrated prediction for stage at Welsh Bridge in Shrewsbury, using simple
calibration method, compared with model predictions and observed data: a) time period
used in calibration, b)validation time period
6.7
Channel modiﬁcation
The object of using a physically based ﬂow model is to be able to analyse the eﬀect of
future physical changes in the channel, including options for ﬂood risk management. Thus
it is necessary to consider whether the calibration method is applicable to situations where
there has been a modiﬁcation to the river channel. It is possible to use the hydraulic model
to simulate ﬂow in the modiﬁed channel, but the new channel model cannot be separately
calibrated, because the modiﬁed channel has not been observed. Instead, it is necessary
to use the existing calibration. The diﬀerence in the application comes in the calibrated
prediction. In Chapter 5, the expected value for the calibrated prediction ζ(x†) at input
location x†, in the case where an emulator is not used, conditional on data z, parameters

Chapter 6. Calibration of a dynamic ﬂood model
99
θ, model inadequacy hyperparameters ψδ and observation noise variance σ2
ϵ , is given by
Equation (3.10), repeated below:
E

ζ(x†)|z, θ, ψδ, σ2
ϵ

= M(x†, θ)+hδ(x†, θ)T ˆβδ+τ(x†, θ)T Σ−1 
z −M(x, θ) −Hδ(x, θ)ˆβδ

where
τ(x†, θ) = Vδ
 x†, θ), (x, θ)

, with (x, θ) referring to the locations where the data z have
been collected,
Σ = Vδ (x, θ), (x′, θ′)) + σ2
ϵ I is the covariance matrix, and
Vδ ((x, θ), (x′, θ′)) is the covariance matrix representing the model inadequacy alone.
Now suppose that the modiﬁed model is denoted M. Then the expected value for the
calibrated prediction is given by
E

ζ(x†)|z, θ, ψδ, σ2
ϵ

= M(x†, θ)+hδ(x†, θ)T ˆβδ+τ(x†, θ)T Σ−1 
z −M(x, θ) −Hδ(x, θ)ˆβδ

The variance of the conditional calibrated prediction remains unchanged, being given
by
Var

ζ(x†)|z, θ, ψδ, σ2
ϵ

= Σ −τ(x†, θ)T Σ−1τ(x†, θ) + ΛT WΛ
(6.7)
where the elements of the second correction term are
W, the covariance matrix of the regression coeﬃcients βδ, and
Λ, deﬁned by
 hδ(x†, θ) −τ(x†, θ)T Σ−1Hδ(x, θ)

.
In order to ﬁnd the calibrated prediction in practice, the conditional distribution for ζ(x†)
must be found for each member of the Markov chain; the mean of the unconditional
calibration is given by
E

E

ζ(x†)|z, θ, ψδ, σ2
ϵ

while the variance is
Var

E

ζ(x†)|z, θ, ψδ, σ2
ϵ

+ E

Var

ζ(x†)|z, θ, ψδ, σ2
ϵ

(6.8)
Predicted time series are recovered as before, by simulation of the output, drawing from
the conditional posterior distributions of the autoregressive coeﬃcient and spline knot
values, and ﬁltering the input waveform for each member of the Markov chain.
As an illustration of the application of the calibration to a modiﬁed channel, consider
a relief channel in Shrewsbury itself, crossing the neck of the meander, illustrated by
a dotted line in Figure 6.1. Note that this is not a realistic option for ﬂood relief; the
Environment Agency recommended increasing upstream storage in the Welsh mountains to
protect the entire catchment (Environment Agency, 2009b). However, the use of this relief
channel provides an illustration of the methodology. The relief channel has been modelled
in Hec-Ras, using two diﬀerent cross sections: a triangular cross-section, of depth 5m
and ground-level width 6m, and a rectangular cross section of depth 5m and ground-level
width 10m. Figure 6.19 shows the calibrated prediction for (a) the original channel, (b) the

Chapter 6. Calibration of a dynamic ﬂood model
100
triangular cross-section relief channel, and (c) the rectangular cross-section relief channel.
The equivalent predictions for the time series are shown in Figure 6.20.
Note that although the expression (6.7) for the conditional variance remains the same,
the total unconditional variance (6.8) is the sum of the mean of the (unaltered) variance
and the variance of the (altered) mean; thus the variance of the unconditional estimates
in Figure 6.20 are not the same.
1
2
3
4
5
6
7
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Upstream stage, m
Scaled nonlinear function b/(1−a)
ai)
1
2
3
4
5
6
7
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Upstream stage, m
Scaled nonlinear function b/(1−a)
bi)
1
2
3
4
5
6
7
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Upstream stage, m
Scaled nonlinear function b/(1−a)
ci)
G
G
G
G
G
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
Autoregressive coefficient, a
G
G
G
G
G
G
G
G
G
G
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
Autoregressive coefficient, a
G
G
G
G
G
G
G
G
G
G
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
Autoregressive coefficient, a
G
G
G
G
G
aii)
bii)
cii)
G
Data
Model output
95% prediction interval
Figure 6.19: Calibrated prediction for a) original river channel, b) channel modiﬁed with
triangular relief channel and c) channel modiﬁed with rectangular relief channel: i) non-
linear function and ii) autoregressive coeﬃcient
It is questionable whether the prediction intervals for these modiﬁcations are plausible.
In Figure 6.19c the transformed model output for low stage varies much less than those
for the unmodiﬁed channel, calling into question the large translation at low stage for the
calibrated prediction in the context of more major modiﬁcation. Similarly, in the time
domain (Figure 6.20), where the predicted outputs are compared with the outputs for the
modiﬁed models, the predicted output in case (c) is for most of the time period below
the lowest model output. However, without other information, from another model for
example, since there are clearly no observations of the modiﬁed channel, it is impossible
to say how large a perturbation to the original model is justiﬁable.
Other authors have considered the translation of the outcome of a calibration to a diﬀerent
model. Bayarri et al. (2007), in their calibration of vehicle crashworthiness, translated
the results of the calibration from one vehicle to another, and demonstrated success in
their calibration by comparison with subsequent measurements. However, it is clear that
the practicality of extrapolation of a model is problem-dependent. Goldstein and Rougier
(2009) suggested the principle of “reiﬁcation”, where the outcome of a model could be
compared in a thought experiment to a more complex model, which may or may not
physically exist. The two models are compared by using an emulator for the original model,

Chapter 6. Calibration of a dynamic ﬂood model
101
0
200
400
600
800
1000
1200
48
50
52
Time, h
Downstream stage, 
mAOD
a)
0
200
400
600
800
1000
1200
48
50
52
Time, h
Downstream stage, 
mAOD
b)
0
200
400
600
800
1000
1200
48
50
52
Time, h
Downstream stage, 
mAOD
c)
data
model output
95% prediction interval
Figure 6.20: Calibrated prediction for a) original river channel, b) channel modiﬁed with
triangular relief channel and c) channel modiﬁed with rectangular relief channel
which can be extended to account for the additional complexity of the second model, using
Bayes Linear analysis to relate the models to the data. House (2009) applied the reiﬁcation
principle to a rainfall-runoﬀmodel, comparing the output of a reduced complexity rainfall-
runoﬀmodel to the full model at a particular time. 67% of the outputs from complete
model runs were contained within the 95% prediction interval from the reiﬁed model,
indicating some success for the reiﬁcation method.
However, the choices made in the
extension of the emulator are based on expert judgement about the expected alterations
to the model output. Under some circumstances, it may be diﬃcult to anticipate the
diﬀerence in output arising from an extended model.
6.8
Summary
A methodology has been presented in this chapter for the calibration of dynamic ﬂood
models using an emulator suggested by Romanowicz et al. (2008), in the form of a transfer
function. This has been demonstrated with respect to a hydraulic model for a reach of the
river Severn above Shrewsbury, with gauged river stage data from a historical ﬂood. A

Chapter 6. Calibration of a dynamic ﬂood model
102
number of algorithmic choices have been examined, and the robustness of the calibration
methodology has been explored in terms of a number of its variables.
The methodology used involves performing the calibration in the domain of the transfer
function.
This choice suﬀers from the problem that observation noise is estimated in
the domain of the transfer function parameters, and will no longer have the structure of
uncorrelated Gaussian noise on retransformation to the time domain. In addition, the
uncertainty accrued in translating from the time domain to the domain of the transfer
function is ignored in the analysis.
Bayesian calibration of the hydraulic model, using the emulator but not the model inad-
equacy function, results in a calibrated prediction which underestimates the peak events,
demonstrating the eﬃcacy of the model inadequacy representation in counteracting the
model bias towards lower peaks.
The emulator has been shown to form an eﬀective transfer function for this river reach, by
applying it to a diﬀerent ﬂood event. However, application of the calibration to a model
representing modiﬁcations to the river reach is more problematical, as it is not clear under
what circumstances the model calibration and model inadequacy can be transferred to
unobserved situations.
The transfer function emulator and calibration approach have been chosen for ease of in-
corporation of calibrated prediction into a ﬂood risk calculation; this will be demonstrated
in the next chapter.

Chapter 7
Use of calibrated prediction in
calculating probability of
inundation
7.1
Introduction
The previous three chapters have demonstrated, for diﬀerent types of models, how the
Bayesian calibration methodology can be used to provide a calibrated prediction for ﬂood
model output, taking into account model inadequacy. However, the motivation of this
study is to provide a means of incorporating the uncertainties in ﬂood model calibration
into a risk analysis study. The work in this chapter outlines, for the dynamic model of the
Severn presented in the previous chapter, how the calibrated predictions from that model
may be used to provide a probability of inundation.
Reformulating the statement of expected damage in expression (1.1) in the introduction
to this thesis in terms of water height, the expected damage E(c) is
E(c) =
Z
h
c(h)φ(h)f(h)dh
where the height of water at some point in a river or ﬂoodplain is denoted h, and the
damage function is denoted c(h), φ(h) is the probability of a failure of the ﬂood defence
system at water height h, and f(h) is the probability density of h. The distribution f(h)
is to be determined from available information, which includes historical ﬁeld observations
and computer models.
As before, taking the approach that failure of ﬂood defence is inevitable when a threshold
water height is exceeded, and that damage is a ﬁxed cost once the ﬂood defence has failed,
103

Chapter 7. Use of calibrated prediction in calculating probability of inundation
104
then the expected damage is proportional to the probability of inundation:
P(h > h0) =
Z
h
I(h > h0)f(h)dh
=
Z ∞
h=h0
f(h)dh
In this case, where the model used to predict water height is conditional on the upstream
water height the integral can be further developed:
P(h > h0) = P(h > h0|h†)P(h†)
where h† is the upstream water height. Thus,
P(h > h0) =
Z
h† I(h > h0)f(h|h†)f(h†)dh†
(7.1)
Evaluation of the integrand is then reduced to two problems; evaluation of the two distri-
butions f(h|h†) and f(h†). The second of these two is known as the ﬂood frequency curve,
and an approach to this is described in section 7.2. The ﬁrst of these distributions is to
be evaluated with reference to synthetic upstream waveforms, for which downstream cali-
brated prediction can be evaluated. The treatment of the synthetic upstream waveforms is
dealt with in section 7.3. Subsequently, the probability of inundation can be evaluated for
both the existing channel, and a proposed alteration to the channel, as described in section
6.7. A sensitivity analysis is then performed, to identify which parts of the model are most
inﬂuential, and a comparison is made with the simple calibration of section 6.6.2.
7.2
Flood frequency curve
The Flood Estimation Handbook (Robson and Reed, 1999) gives a set of statistical proce-
dures for the estimation of ﬂood frequency in a catchment. Robson and Reed recommend
estimating separately the median annual maximum daily mean ﬂow, and ﬁtting a “growth
curve” or extreme value distribution to the scaled data. Although this separation is un-
necessary when dealing with data from a single catchment, the reason for it becomes more
obvious when it is recognised that many catchments do not have a long record of annual
maximum ﬂows. Robson and Reed recommend that when the ﬂood frequency of interest
is smaller than the reciprocal of half of the length of reliable record, data are combined
from diﬀerent catchments to extend the record. The recommended threshold frequency
in this procedure is a rule of thumb, resulting from the increasing prediction interval with
decreasing frequency. While care must be taken to pool data from catchments with similar
characteristics, errors are reduced when the data are ﬁrst scaled by the median annual
maximum ﬂow.
Using the methodology set out in the Flood Estimation Handbook gives a frequency curve
for upstream daily mean ﬂow; this then has to be converted to upstream daily peak

Chapter 7. Use of calibrated prediction in calculating probability of inundation
105
ﬂow using a methodology such as that of Fill and Steiner (2003), and for this study a
further conversion would be required for peak height, with the aid of a rating curve. Both
procedures add extra layers of uncertainty. However, since the ﬁtting of a ﬂood frequency
curve is not the main thrust of this study, a simpler methodology is used here, ﬁtting an
extreme value distribuition to the annual maxima of instantaneous upstream river peak
heights.
A Generalised Extreme Value distribution (Coles, 2001, p 48) gives the probability that
the level z will be exceeded in any given year, and is described by:
G(z) = exp





−

1 + ξ
z −µ
σ
−1
ξ





(7.2)
where the parameters µ represents the location, σ the spread, and ξ the shape of the
curve. This distribution has been ﬁtted to the available records of instantaneous stage
at Montford, which stretch from 1952 to 2008, comprising 55 complete years of data.
Parameter estimates are given in Table 7.1. The annual probability of exceedance p is
usually described by its reciprocal, the return period, so the ﬂood height dependence on
return period is found by substituting G(zp) = 1 −p into Equation (7.2), giving
zp = µ −σ
ξ [1 −{−log(1 −p)}−ξ]
zp is called the return level associated with the return period 1
p, and is exceeded by the
annual maximum in any particular year with probability p. A return period plot is shown
in Figure 7.1.
Table 7.1: Parameter estimates for Generalised Extreme Value function ﬁtted to historical
annual maxima of ﬂood height at Montford
Variable
Mean
Standard
error
µ, location
5.5332
0.08761
σ, scale
0.5741
0.06404
ξ, shape
-0.2557
0.11471
7.3
Synthetic upstream data
The integral in expression (7.1) needs to be evaluated over all possible input conditions.
The section above has dealt with the distribution of peak height, but it is important
to investigate to what extent the shape of the hydrograph around the peak inﬂuences
the probability of inundation. If variables χ describing peak shape are inﬂuential on the
probability of inundation, the integral (7.1) should be rewritten incorporating the variables

Chapter 7. Use of calibrated prediction in calculating probability of inundation
106
0.2
0.5
2.0
5.0
20.0
100.0
4.0
4.5
5.0
5.5
6.0
6.5
7.0
7.5
Return period (years)
Height above local datum (m)
data
model prediction
95%confidence interval
Figure 7.1: Variation of ﬂood height with return period at Montford, as predicted by ﬁtted
Generalised Extreme Value model
χ, thus:
P(h > h0) =
Z
h†
Z
χ
I(h > h0)f(h|h†, χ)f(h†, χ)dχ dh†
(7.3)
A sensitivity analysis has been undertaken with regard to characteristics of peak shape.
This involves identifying the extent of upstream time series required before and after
the peak to capture all dependencies, and a parameterisation of relevant shape indica-
tors.
7.3.1
Selection of appropriate window length for ﬂood peak
Figure 7.2 shows the annual peaks of river height at Montford, from 1952 to 2007, inclusive,
scaled for comparison, and centred in a 20-day window. It is clear that while some of the
peaks are isolated, such as 1971 and 1972, most of the others have nearby subsidiary
peaks.
In order to examine the proximity at which a subsidiary peak does not aﬀect
the output, synthetic curves have been constructed to represent the peak shape, and
truncated at varying distances before and after the peak, substituting diﬀerent constant
water heights for the truncated sections. These synthetic waveforms were then introduced
to the calibrated model for the transfer function representing the river section, and the
sensitivity was examined of the quantiles of the predicted output peak height to the
truncation window of the input peak.
The synthetic input peaks were constructed with reference to the annual maximum peaks
of 1960 and 1995, both of which appear to be fairly isolated (Figure 7.3). A power curve
is used for the rising limb, while the falling limb is represented by the diﬀerence between

Chapter 7. Use of calibrated prediction in calculating probability of inundation
107
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
1965
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
Figure 7.2: Flood height at Montford, at annual maximum peaks (scaled by maximum
height), for a window of 10 days before and after the peak
two exponentials, thus:
xrising = 1 −a (tp −t)b
xfalling = c exp(−d(t −t0)) −e exp(−f(t −t0))
where
tp is the location of the peak, and
a, b, c, d, e, f and t0 are constants to be chosen.
Synthetic curves are used to avoid the interference of irregularities found in real data.
These synthetic curves are scaled to have peak height 7m above local datum (a peak
height with annual probability of 0.016), have then been truncated at (72, 48, 36, 24, 12)
hours before the peak, and (60, 36, 24, 12, 8, 4) hours after the peak, with the remaining
parts of the curve replaced by values of 2,4, or 6. One example of this is shown in Figure
7.4.
Given a truncated input waveform, a distribution of predicted output waveforms can be
found from the calibrated model as follows. For each member of the Markov chain, a draw
is made from the posterior conditional distribution of the autoregressive coeﬃcient and
nonlinear function spline values. These are then used to form a transfer function which is
applied to the truncated synthetic waveforms, chosen using Monte Carlo sampling. Taking

Chapter 7. Use of calibrated prediction in calculating probability of inundation
108
0
100
200
300
400
500
0
1
2
3
4
5
6
7
time (h)
water level (m)
data, 1960 peak
data, 1995 peak
synthetic waveforms
Figure 7.3: Scaled data for annual maximum peaks in 1960 and 1995, comapred with
synthetic waveforms used for sensitivity analysis
0
100
200
300
400
0
1
2
3
4
5
6
7
time (h)
water level (m)
original waveform
truncated waveform
Figure 7.4: Illustration of the truncation of the ﬂood peak waveform used in the sensitivity
analysis
together the elements of the Markov chain to represent the posterior distribution, and
Monte Carlo samples from the distributions of the predicted transfer function coeﬃcients,
a distribution of output peaks results for each truncated waveform. 5%, 50% and 95%
quantiles of this distribution were taken, and compared with similar distributions for the
untruncated input waveform. The diﬀerences at each of these quantiles are shown in Table
7.2. From this it was determined that the input waveform could be truncated 36 hours
before and 24 hours after the peak.

Chapter 7. Use of calibrated prediction in calculating probability of inundation
109
Table 7.2: Maximum diﬀerence in quantile peak height (m), for truncated compared with
untruncated upstream waveform
Quantile
Length
Length after peak
before peak
60 hours
36 hours
24 hours
12 hours
8 hours
4 hours
0.025
72 bours
0.000
0.000
0.000
0.000
0.001
0.064
48 hours
0.009
0.009
0.009
0.009
0.009
0.060
36 hours
0.022
0.022
0.022
0.022
0.022
0.058
24 hours
0.070
0.070
0.070
0.070
0.070
0.070
12 hours
0.187
0.187
0.187
0.187
0.187
0.215
0.5
72 bours
0.000
0.000
0.000
0.000
0.000
0.013
48 hours
0.001
0.001
0.001
0.001
0.001
0.013
36 hours
0.004
0.004
0.004
0.004
0.004
0.012
24 hours
0.017
0.017
0.017
0.017
0.017
0.017
12 hours
0.166
0.166
0.166
0.166
0.166
0.166
0.975
72 bours
0.000
0.000
0.000
0.000
0.000
0.000
48 hours
0.000
0.000
0.000
0.000
0.000
0.000
36 hours
0.001
0.001
0.001
0.001
0.001
0.001
24 hours
0.006
0.006
0.006
0.006
0.006
0.006
12 hours
0.059
0.059
0.059
0.059
0.059
0.059
7.3.2
Parameterisation of peak shape
The rising and falling limbs of the annual maximum peaks, truncated to 36 hours before
and 24 hours after the peak, have been modelled separately. Recognising that the rising
slope can be steep, and can occur in a short space of time, at any time in the 36 hours
before the peak, the rising limb has been modelled with a logistic curve (Equation 7.4),
while the falling limb is modelled by a power curve (Equation 7.5). Expressions for the
water heights, as a function of time are thus:
x = x−+
(xp −x−)
1 + exp
 
−γ
 t −t0
tp −t−
!
(7.4)
x = xp −(xp −x+)
 t −tp
t+ −tp
δ
(7.5)
where
t represents time and x represents upstream peak height, with suﬃces (·)p referring to
peak time, and (·)−and (·)+ to 36 hours before and 24 hours after the peak respectively,
and
t0, γ and δ are parameters to be determined.
The eﬀectiveness of these functions to represent the rising and falling limbs of the histor-
ical annual peaks is shown in Figure 7.5. While it can be seen that the falling limb is
well represented, the rising limb is approximated in most, but not all cases, if preceding

Chapter 7. Use of calibrated prediction in calculating probability of inundation
110
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
1965
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
data
model
Figure 7.5: Scaled ﬂood height at Montford, at annual maximum peaks, for a window of
36 hours before and 24 hours after the peak. Hydrograph model superimposed.
secondary peaks are ignored.
The sensitivity of the downstream peak height has been assessed to the peak shape param-
eters x−and x+ (given peak height xp), and t0, γ and δ. It was found that downstream
peak height was sensitive to all of these parameters; however, it was most sensitive to the
parameters x−and x+ representing height before and after the peak.
7.3.3
Joint distributions for peak characteristic parameters
The evaluation of integral (7.1) requires a joint distribution to be found of all the vari-
ables describing distribution of peak height and upstream shape, namely, xp, ∆−=
1 −x−
xp
, ∆+ = 1 −x+
xp
, t0, γ and δ.
In order to evaluate the integral, it is necessary
to characterise both the marginal distributions of the independent variates, and their
correlations.
Since the number of annual maximum peaks is limited, marginal distributions and cor-

Chapter 7. Use of calibrated prediction in calculating probability of inundation
111
relations of the parameters describing peak shape have been estimated from the monthly
maximum peaks. Discarding those months where the maximum peak is overshadowed by
the adjacent month’s peak, and those where the models were not able to well estimate the
peak shape, there remained a sample of 540 peaks.
The marginal distributions were characterised empirically, by interpolation between the
percentiles of the data. However, it was considered that while some of the variables were
bounded at the upper end, others were not; the nature of the tails is listed in Table 7.3. For
each of those distributions whose tails were unbounded, a Generalised Pareto distribution
was ﬁtted to the highest 5% of the data. This distribution
G(u) = 1 −ζ

1 + ξ(u −uo)
σ
−1/ξ
, u > uo
where the model is deﬁned above threshold uo by parameters ζ, σ and ξ. The threshold
is taken here to be the 95th centile of the observed data.
Table 7.3: Nature of marginal distributions for variables describing ﬂood peak shapes
Variable
Lower tail
Upper tail
yp
bounded
unbounded
y−
bounded
bounded
y+
bounded
bounded
t0
bounded
bounded
γ
bounded
unbounded
δ
bounded
unbounded
Simulating from the joint distribution for the ﬂood peak heights and the other variables
requires knowledge not only of their marginal distributions, but also of their correlation.
Correlated covariates are to be generated by postmultiplying Normal random variates with
the Choleski decomposition of the covariance matrix of appropriately transformed data.
Accordingly, the parameter distributions were transformed to Normal, to investigate their
correlation. Figure 7.6 is a scatterplot matrix of the transformed variables, showing the
correlations between each pair. The ﬁgure has been greyscale coded for the height of the
ﬂood peak, with darker points referring to the characteristics of higher ﬂood peaks.
It can be seen that the correlation is not constant for all peak heights; indeed it changes
sign for correlation of the scaled heights before and after the peak with the peak height.
There are three ways in which this could be addressed; the ﬁrst would be to transform the
data once more, to a set of variables with more consistent correlations, bearing in mind
that the parameters are required to simulate realistic ﬂood peaks. The second approach
would be to identify copula functions to model the changing covariance directly.
The
third approach, which has been used here, is to partition the data into subsets with stable
correlations. After investigation, correlations have been performed for peak height centile
intervals of {(0,0.2), (0,0.3), (0.1,0.4), (0.2,0.5), (0.3,0.6), (0.4,0.7), (0.5,0.8), (0.6,0.9),
(0.7,1), (0.8,1)}.
These overlapping intervals provided as large as possible a data set
from which to model the correlations, while signiﬁcance tests showed that for the most

Chapter 7. Use of calibrated prediction in calculating probability of inundation
112
peak height
rel. peak height 
−36 hrs
rel. peak height 
+24 hrs
point of 
inflection
gamma
rel. peak height 
−36 hrs
rel. peak height 
+24 hrs
point of 
inflection
gamma
delta
Figure 7.6: Correlation of transformed variables describing the ﬂood peak shape at Mont-
ford. Darker shades correspond to higher peaks.
part there was not a signiﬁcant diﬀerence between the correlation coeﬃcients of adjacent
overlapping intervals, ensuring that the change was gradual. The correlations were used
for all variables, with the peak height in the intervals {(0,0.1), (0.1,0.2), (0.2,0.3), (0.3,0.4),
(0.4,0.5), (0.5,0.6), (0.6,0.7), (0.7,0.98, (0.8,0.9), (0.9,1)} respectively (Figure 7.7).
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
quantile of drawn variate 
for peak height
quantiles included in 
calculation of covariance matrix
Figure 7.7: Choice of covariance matrix for correlated variables.
Use of the diﬀerent correlation matrices alone, for diﬀerent parts of the distribution is not
adequate to ensure the correct correlation between the variates, and marginal distributions
have to be used which correspond to the correlation matrices. On generating a random

Chapter 7. Use of calibrated prediction in calculating probability of inundation
113
sample, transforming, and transforming the correlated sample back to physical units using
the appropriate marginal distribution, an appropriately correlated sample can be drawn.
An example is shown in Figure 7.3.3, indicating that the method is well able to represent
the covariance structure.
peak height
rel. peak height 
−36 hrs
rel. peak height 
+24 hrs
point of 
inflection
gamma
rel. peak height 
−36 hrs
rel. peak height 
+24 hrs
point of 
inflection
gamma
delta
Figure 7.8: Correlation of simulated transformed variables describing the ﬂood peak shape
at Montford. Darker shades correspond to higher peaks.
7.4
Probability of inundation
Given these model parameters, the integral in Equation (7.3) is repeated here:
P(h > h0) =
Z
h†
Z
χ
I(h > h0)f(h|h†, χ)f(h†, χ)dχ dh†
However, the estimate of the downstream water height, h, is dependent not only on the
upstream peak height, h† and peak shape parameters χ, but also on the calibration, and
in particular on the parameters and hyperparameters ψ of the statistical model used for
the calibration; thus
P(h > h0) =
Z
h†
Z
χ
Z
ψ
I(h > h0)f(h|h†, χ, ψ)f(h†, χ)f(ψ)dψ dχ dh†
(7.6)
This integral is evaluated as follows. For each iteration, six random numbers are drawn
from the Normal distribution. The ﬁrst of these represents ﬂood peak height, and its
level determines the covariance matrix for the peak shape parameters, as described in

Chapter 7. Use of calibrated prediction in calculating probability of inundation
114
the previous section. The vector of random variates is transformed to a correlated set by
postmultiplying the vector by the Choleski decomposition of the covariance matrix. The
correlated variates are then transformed from Normal to their empirical marginal distri-
butions. The distribution for peak height is the Generalised Extreme Value distribution
whose ﬁtted parameters are listed in Table 7.1, while the distributions of the other vari-
ables are the marginal distributions found from the historical monthly ﬂood peaks. With
these variables, an input waveform is generated. A downstream waveform is the result of
ﬁltering this input waveform using a transfer function derived from the calibration process,
in the same way that a calibrated prediction was found, using the procedure described in
Section 6.5.1.2, and repeated below.
Recall that during the calibration process, a Markov chain is generated embodying the
distributions of the parameters and hyperparameters of the statistical data model. For
each member of this Markov chain, estimates are calculated of the conditional means
E

ηb(x†
τ, θ†)|b′′(xτ, θ), βb, ωbx, ωbθ, σ2
b

and E

ηa(θ†)|a′(θ), βa, ωa, σ2
a

and variances
Var

ηb(x†
τ, θ†)|b′′(xτ, θ), βb, ωbx, ωbθ, σ2
b

and Var

ηa(θ†)|a′(θ), βa, ωa, σ2
a

of the nonlinear function and (arctanh) autoregressive coeﬃcient values describing the
calibrated transfer function. Values of ηb(x†
τ, θ†) and ηa(θ†) are drawn from these dis-
tributions, which are used to ﬁlter the input waveform, using the time series model,
ηt(θ) = tanh(ηa(θ))ηt−1(θ) + xt−l(1 −tanh(ηa(θ)))ηb(xt−l, |xτ, θ).
A positive contribu-
tion is made to the integral from a draw of the variates representing the shape of the
input waveform and a member drawn from the Markov chain, if the output waveform,
ηt(θ) at any point exceeds the threshold level h0.
The integral has been evaluated with regard to the indicator water levels set by the
Environment Agency. A ﬂood warning is issued to at risk properties when the recorded
water level at Welsh Bridge is 3.15m above datum; this is upgraded to a severe ﬂood
warning when the water level reaches 4.5m above datum (Environment Agency, 2007).
The integral was evaluated both for the existing channel, and using the smaller of the two
modiﬁcations described in Section 6.7, for both ﬂood warning levels.
Figure 7.9 illustrates the convergence of the probability of inundation to a ﬁnal value, with
an increasing number of draws from the distribution of input waveform shape parameters.
Means and standard deviations are given in Table 7.4 of the annual probability of issuing
the two levels of ﬂood warning, both before and after modiﬁcation. At both levels, the
modiﬁcation would result in a reduction of the annual probability.

Chapter 7. Use of calibrated prediction in calculating probability of inundation
115
0
50
100
150
200
250
0.0
0.2
0.4
0.6
0.8
1.0
Iteration number
Probablility of inundation
a)
flood warning
severe flood warning
current channel
altered channel
Figure 7.9: Probability of issuing a ﬂood warning at Welsh Bridge in Shrewsbury, for
unmodiﬁed and modiﬁed channels.
Table 7.4: Annual probability of issuing ﬂood warnings, before and after channel modiﬁ-
cation
Mean
Flood warning, unmodiﬁed channel
0.72
Flood warning, modiﬁed channel
0.56
Severe ﬂood warning, unmodiﬁed channel
0.11
Severe ﬂood warning, modiﬁed channel
0.05
7.5
Sensitivity analysis
In order to ﬁnd which are the most important factors in the probability of inundation
calculation, a sensitivity analysis was conducted. The sensitivity measure used is a variance
ratio method for determining the most important factors in the evaluation of a function.
If a function y is dependent on inputs x = x1, · · · , xn, then a ﬁrst order measure of the
sensitivity of y to input xi is (McKay,1995)
Si = V (E(y|xi = ˜xi))
V (y)
(7.7)
V (E(y|xi = ˜xi)) is known as the variance of conditional expectation.
Evaluation of the sensitivity of the probability of inundation integral (7.6) to the in-
put variables is not a practicable option, as these are integrated out of the expression.
The sensitivity measure is thus required of the predicted downstream peak ﬂood height
f(h|h†, χ, ψ) to upstream peak height and shape, Manning’s n, the regression parameters
β for the model inadequacy function, which were introduced explicitly into the calibration

Chapter 7. Use of calibrated prediction in calculating probability of inundation
116
for the purpose of this sensitivity analysis, variances and the other hyperparameters for
the model. These are the same variates as the probability of inundation calculation, with
the addition of the model inadequacy regression parameters.
It should be noted that the sensitivity analysis used here has been formulated under the
implicit assumption that the function y is a deterministic one, while the conditional pre-
dicted peak downstream ﬂood height is a stochastic function, being derived from posterior
Gaussian processes. Oakley and O’Hagan (2004) used a Gaussian process emulator to
increase the speed of sensitivity analysis calculations for expensive deterministic computer
simulators. This analysis could be adapted to the predicted Gaussian processes arising
from a calibration exercise, such as those undertaken in Chapters 4 and 5. The conditional
predicted peak downstream ﬂood height derived in Chapter 6 arises from ﬁltering the input
data with the posterior transfer function, so its distribution is a ﬁnite sum of Gaussian
processes, and is thus a Gaussian process. The analysis of Oakley and O’Hagan could
thus be used to examine the sensitivity of the predicted downstream peak ﬂood height to
upstream peak height and shape and Manning’s n. However, in this instance a sensitivity
analysis is required which includes not only the input conditions and the model parameter
but also the additonal parameters introduced in the calibration. The method of Oakley
and O’Hagan does not allow for this. An alternative approach was taken by Degasperi and
Gilmore (2008), who reported the sensitivity analysis of a stochastic biochemical model,
by examining the changes in histogram distance between diﬀerent realisations. Marrel et
al. (2010) undertook a sensitivity analysis of stochastic models by jointly modelling the
mean and variance, comparing the performance of Gaussian processes with Generalised
Linear Models and Generalised Additive Models.
To evaluate the ﬁrst order sensitivity measure, correlated random variates are drawn
on a replicated modiﬁed Latin hypercube scheme. The modiﬁcation to the usual Latin
hypercube scheme is as follows. Recall that the choice of a sample of size N from a K
dimensional input domain [0, 1]K. For each dimension k ∈{1, . . . , K}, the value of the ith
sample (i ∈{1, . . . , N}) is taken to be at
πk(i) −U(0, 1)
N
where U(·, ·) refers to the uniform distribution, and πk(i) refers to the ith member of a
permutation of the N intervals, the suﬃx emphasizing that a diﬀerent permutation is taken
for each dimension k. In this case, the sample values are given by:
πk(i) −0.5
N
A replicated Latin hypercube scheme involves repeating this scheme R times, so that there
are NR samples of the variates.
Iman and Conover (1982) suggested a method for generating correlated variables under a
Latin hypercube scheme. The method hinges on the rank correlation, and the fact that a
random draw of the variates can be transformed to have a correlation C by postmultiplying

Chapter 7. Use of calibrated prediction in calculating probability of inundation
117
by C, the Choleski decomposition of C. Iman and Conover suggest that a Latin hypercube
L is drawn for the set of variates, using the appropriate marginal distributions. For each
column, the rank of each member is found. Let R be the matrix of Normal quantiles
corresponding to the ranks. If R were uncorrelated, then the matrix RC would have the
appropriate correlation between the variates. However, R will not be uncorrelated, so its
empirical rank correlation D must be found, and D, the Choleski decomposition; D−1RC
will have the required rank correlation. The ﬁnal step is to reorder the individual columns
of the original matrix L to have the same rank order as this new matrix.
It is then
a Latin hypercube with approximately the appropriate rank correlation. Note that the
correlation scheme described in Section 7.3.3 was eﬀected using variates transformed to
Normal distributions. A minor modiﬁcation is made here, transforming the variates to
a Uniform distribution to deﬁne the correlation, thus ensuring the distribution tails are
better sampled.
Evaluation of these ﬁrst order sensitivity indices for a replicated correlated Latin hypercube
of m = 100 levels and r = 5000 replications (Table 7.5, ﬁrst column), shows that the
output of the conditional posterior is indeed sensitive to the upstream peak height, and
to a far lesser extent, to the peak shape parameters.
However, the ﬁrst order indices
indicate no sensitivity, either to the model parameter, or to the parameters characterising
the model inadequacy. A similar evaluation (Table 7.5, second column), using the simple
calibration of Section 6.6.2 without model inadequacy, does not show sensitivity to the
model parameter, either. It appears that the contribution of the upstream peak height
swamps all other relationships, as one might expect.
Table 7.5: First order sensitivity analysis
Variable
Full model
Emulator only
yp
0.96
1.01
∆−
0.11
0.1
∆+
0.15
0.12
t0
0.09
0.1
γ
0.16
0.15
δ
0.33
0.32
θ
0
0
β1
0
-
β2
0
-
ωb
0
-
σ2
a
0
-
σ2
b
0
-
σ2
ϵ
0
0
In an eﬀort to overcome the swamping eﬀect of peak height on the sensitivity analysis, ﬁrst
order sensitivity was calculated for speciﬁc upstream peak height, for heights at diﬀerent
quantiles of the distribution. However, in this case, the sensitivity to all variables was ex-
tremely low. It should be noted that the sensitivity analysis is designed for functions with
deterministic output, while the predicted downstream height in this case is a stochastic

Chapter 7. Use of calibrated prediction in calculating probability of inundation
118
quantity, resulting in lower sensitivities being found.
A more realistic test is to investigate the sensitivity of the downstream peak height to the
upstream ﬂood peak shape parameters and to the parameters of the nonlinear transfer
function; that is the autoregressive coeﬃcient and the spline values at diﬀerent water
heights, representing the calibrated model. The output of this sensitivity analysis is shown
in Table 7.6, where it can be seen that the downstream peak height sensitivity changes
with upstream peak height, and is greatest at that part of the spline corresponding to the
upstream peak height.
7.6
Summary
This chapter has shown how the output from the calibrated prediction may be used to
formulate a probability of inundation for a risk calculation. In addition to the posterior
distribution of the parameters and hyperparameters from the calibration exercise, the
probability of inundation depends on the distribution of upstream peak heights and peak
shapes. By combining all of these uncertain factors it is possible to generate a probability
of ﬂooding (in this case the probability of exceeding a ﬂood warning trigger level) that
takes into account all of the sources of uncertainty in the ﬂooding prediction.
The sensitivity has been explored, of the prediction of downstream peak height, condi-
tional on the upstream peak height and shape, and on the statistical model. It has been
demonstrated that the downstream peak height is sensitive to diﬀerent parts of the sta-
tistical model at diﬀerent upstream peak heights, a sensitivity not available for a more
simple calibration.

Chapter 7. Use of calibrated prediction in calculating probability of inundation
119
Table 7.6: First order sensitivity analysis, conditional on upstream peak height
Peak height
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.95
quantile
Peak height (m)
5.24
5.42
5.58
5.73
5.89
6.05
6.25
6.52
6.73
∆−
0
0
0
0
0
0
0
0
0
∆+
0
0
0
0
0
0
0
0
0
t0
0
0
0
0
0
0
0
0
0
γ
0
0.01
0.01
0.01
0
0
0
0
0
δ
0
0
0
0
0
0
0
0
0
a
0.09
0.12
0.12
0.14
0.13
0.11
0.08
0.06
0.05
b(1m)
0
0
0
0
0
0
0
0
0.01
b(1.2m)
0.01
0.01
0.01
0.01
0
0
0
0.01
0.01
b(1.4m)
0.02
0.02
0.02
0.01
0
0
0.01
0.01
0.01
b(1.6m)
0.01
0.01
0.01
0.01
0
0
0.01
0.01
0
b(1.8m)
0
0
0
0
0
0
0
0
0
b(2m)
0.01
0.01
0.01
0.01
0
0
0
0.01
0.01
b(2.2m)
0.03
0.03
0.03
0.02
0.01
0
0.01
0.02
0.02
b(2.4m)
0.04
0.04
0.04
0.03
0.01
0
0.01
0.03
0.03
b(2.6m)
0.04
0.03
0.03
0.02
0.01
0
0.01
0.03
0.04
b(2.8m)
0.03
0.02
0.02
0.02
0.01
0
0
0.02
0.04
b(3m)
0.03
0.02
0.02
0.02
0.01
0
0
0.02
0.04
b(3.2m)
0.03
0.02
0.02
0.02
0.01
0
0
0.02
0.04
b(3.4m)
0.03
0.03
0.03
0.02
0.01
0
0
0.02
0.04
b(3.6m)
0.04
0.04
0.03
0.03
0.02
0
0.01
0.03
0.04
b(3.8m)
0.06
0.05
0.05
0.04
0.02
0
0.01
0.04
0.04
b(4m)
0.08
0.08
0.08
0.06
0.02
0
0.03
0.05
0.04
b(4.2m)
0.07
0.08
0.07
0.05
0.02
0
0.04
0.05
0.02
b(4.4m)
0.03
0.04
0.03
0.02
0
0.01
0.03
0.02
0
b(4.6m)
0.01
0
0
0
0.01
0.01
0.01
0
0.01
b(4.8m)
0.17
0.12
0.1
0.09
0.05
0.01
0.01
0.06
0.08
b(5m)
0.36
0.33
0.27
0.21
0.1
0.01
0.05
0.15
0.14
b(5.2m)
0.82
0.42
0.35
0.28
0.12
0.01
0.08
0.18
0.15
b(5.4m)
0.44
0.84
0.46
0.29
0.15
0.02
0.09
0.19
0.16
b(5.6m)
0.29
0.33
0.77
0.48
0.14
0.03
0.07
0.16
0.13
b(5.8m)
0.13
0.12
0.1
0.54
0.57
0.06
0.03
0.08
0.07
b(6m)
0
0
0
0.02
0.18
0.7
0.12
0.02
0
b(6.2m)
0.12
0.12
0.11
0.06
0.05
0.04
0.67
0.12
0.06
b(6.4m)
0.19
0.19
0.17
0.13
0.04
0
0.12
0.55
0.14
b(6.6m)
0.16
0.15
0.14
0.11
0.05
0
0.03
0.48
0.61
b(6.8m)
0.09
0.08
0.07
0.06
0.03
0
0.01
0.06
0.55
b(7m)
0.05
0.04
0.03
0.03
0.02
0
0
0.06
0.11

Chapter 8
Conclusions and recommendations
for further work
8.1
Introduction
The study described in this thesis has demonstrated a methodology for calibration of ﬂood
models which includes known information about measurement accuracy, allows for model
bias, is statistically coherent and can be used with models and data of diﬀerent types.
While the literature abounds with calibration methods, this one is particularly suitable
for use in risk analysis, which is one of the most important purposes of ﬂood modelling.
Risk analysis has particular requirements in terms of model calibration; the calibration
method must produce a probability distribution, and this must have statistical credibility.
It is also important that the method should allow the input of upstream conditions other
than those used for calibration.
The formulation of the current method allows not only an emulator to increase the speed
of the calculation, but also a model inadequacy function to allow for bias. In the spa-
tial domain and in the transformed time domain, both the emulator for the ﬂood model
output, and the model inadequacy are described as Gaussian processes. These are corre-
lated Gaussian distributions, deﬁned at any point in space, time and parameter space, but
conditioned on the known model output at locations in this domain where model output
has been obtained, and on measurements, where it is assumed that there is some under-
standing of model error. The method has been used with some success in other ﬁelds of
application.
The study has been conducted through calibration of three hydraulic models of increasing
complexity.
Calibration of all of these involved estimation of a single parameter, the
channel roughness, Manning’s n.
120

Chapter 8. Conclusions and recommendations for further work
121
8.1.1
Analytical model
The ﬁrst application of the methodology concerned calibration of steady state laboratory
experiments, using an analytical hydraulic model which was clearly inadequate to describe
the physical processes. In this simple case, where there was no need for an emulator, it was
possible to concentrate on the formulation of the model inadequacy representation, whose
ﬂexibility arises from the formulation as the sum of a regression relationship and a corre-
lated Gaussian distribution. The analytical model exhibited a step behaviour which was
not evident in experimental measurements, and it was shown that this behaviour needed to
be represented both in the regression relationship and in the correlation structure. As more
experimental series were included, a greater sophistication was required for the regression
basis for the model inadequacy function to account for increasing dimensionality, while
the complexity of the correlation structure increased more slowly. Eventually, however,
lack of data made it impossible to specify the correlation structure completely.
The ability to describe ﬂow as a function of stage naturally leads to the suggestion that
this method could be used for the determination of rating curves, an area where there has
been much activity, and where the results are of everyday use in practice.
8.1.2
Steady-state ﬂood model
The demonstration of calibration of a two dimensional steady-state ﬂood model using in-
formation from a satellite image represents a straightforward application of the Bayesian
calibration methodology, incorporating a simple emulator for the level of the water surface.
The resulting map of inundation probability is comparable with previous results in the
literature. This example demonstrated the issue of identiﬁability of the observation error
and model inadequacy, illustrating the need to have prior knowledge of the observation
error variance, an issue which was noted by Wynn (2001). However, the proposed method
is superior to previous work on this topic which has been based upon the GLUE method-
ology, and requires subjective judgment with regard to an arbitrary threshold which does
not have any physical signiﬁcance.
The calibration of the steady state ﬂood model could have been made more general by
including not only the roughness parameter but also the upstream ﬂow in the calibration.
As formulated, ﬂow was taken as given, being the quantity measured at the time of
the satellite overpass. However, a more thorough investigation would have couched the
upstream ﬂow as a parameter to be determined, and the analysis could thus have furnished
an estimate of the accuracy of the measurement. While to do so would have exacerbated
the identiﬁability problem, it is commonplace in ﬂood modelling practice to update the
ﬂow estimate as part of the calibration process, so the Bayesian procedure provides a
statistical framework within which to formalise this practical necessity.
One issue which was raised in the calibration of the two dimensional steady-state ﬂood
model is as follows. Computation of the Gaussian processes describing the posterior dis-

Chapter 8. Conclusions and recommendations for further work
122
tribution of the water surface involves inversion of the covariance matrix, conditioned on
the locations of the output from the ﬂood model, and measured data. In practice, this
matrix is unstable if computer output or data are used for locations which are too close
together, so it may well not be possible to use all of the available data in calibration.
A second issue which appeared in this example, is that the covariance structure inves-
tigated was not able to accommodate model variability over a number of spatial scales.
Kennedy and O’Hagan (2001a) did attempt to investigate other structures, such as a
Mat`ern correlation function, noting that the Gaussian correlation function did show a
tendency to smooth the model output, but found no signiﬁcant diﬀerence between the
structures they investigated.
8.1.3
Dynamic ﬂood model
Using the same methodology for calibration of a dynamic ﬂood model presents a con-
siderably larger challenge than the previous examples, since the Gaussian process rep-
resentation is essentially spatially inspired, being derived from geostatistical techniques.
Two signiﬁcant diﬀerences arise when dealing with the output of time-varying rather than
spatially-varying models; one is that rate of change of the output may be more rapid by
comparison with the time scale of interest, and the second is that the dependency in the
output is on historical information alone, whereas spatial models involve dependency in
any spatial direction. These complications are compounded when an emulator or a model
inadequacy function are to be identiﬁed. Thus, the solutions proposed in the literature
involve treating the time domain as another spatial dimension for models with simple
variation, or creating emulators and model inadequacy function for a single step of the
computer model. An alternative is to use time series methods for the time domain, and
to link the time series with Gaussian process techniques. However, care is required in this
last approach, since the end goal is not forecasting, but risk analysis, where calibrated
prediction is required for the full distribution of possible input time series.
The method chosen here is to parameterise the transfer function from input to output time
series, and to deﬁne the model inadequacy in terms of this parameterisation. It did not
prove feasible to apply the calibration directly in the time domain, so the parameterisation
of the transfer function was done oﬀ-line, and calibration was performed directly in the
domain of the parameterisation. The resulting calibration problem reduces to the same
class of spatial problems as the previous examples. By separating the transfer function pa-
rameterisation from the calibration, and in particular by using optimisation to perform the
parameterisation, the errors were minimised during the process, thus overriding any prior
knowledge which may exist about observation error variance, and potentially introducing
bias in the model parameterisation.
The calibration of this model was found to be inﬂuenced by the prior distribution of a
parameter in the Gaussian process covariance matrix representing the smoothness of the
parameterisation of the transfer function. While this is easy to estimate when the transfer

Chapter 8. Conclusions and recommendations for further work
123
function parameterisation is fairly irregular, it is less so for a smooth transfer function.
This represents a potential weakness of the method.
Two features are required for a calibration method to be suitable for risk analysis. The
ﬁrst is that the calibrated model should be capable of being applied to input time series
other than the time series used in calibration. This was ensured by deﬁning the model
inadequacy in terms of a transfer function, and was demonstrated for a second, validation
time period. The second requirement is that the calibration can be extended to a per-
turbed model to be able to compare diﬀerent ﬂood management options, or other future
changes. The demonstration of this capability was less convincing, as it would require
a more complex model to validate the perturbation, in order to determine what size of
perturbation it is feasible to use. A coherent comparison would employ a formal combi-
nation of the output from the diﬀerent computer models. Kennedy and O’Hagan (2000)
suggested a method of combining the output from computer programs at diﬀerent levels
of complexity, while Rougier et al. (2009) demonstrated the combination of ensembles of
model output from diﬀerent climate models, using Gaussian process emulators.
8.1.4
Risk calculation
Once it has been demonstrated that calibrated prediction can be produced for other inputs
than that used in calibration, it becomes feasible to undertake a risk analysis. In the case
of this thesis, the probability of inundation for an existing channel was compared with a
modiﬁed channel.
Some eﬀort was required to characterise the possible input states, in other words the
historical upstream ﬂow patterns, and to determine appropriate statistical distributions
for the parameters that described these ﬂow patterns. A sensitivity analysis showed that
the output peak ﬂood height is dependent on the entire upstream ﬂood peak. For a given
ﬂood peak, dependency on other uncertain quantities that determine ﬂood depth is more
complex. Where no model inadequacy has been included, predicted output peak height
depends directly on the model calibration parameter, in this case Manning’s n. However,
where there is a model inadequacy function, dependence of the predicted output peak
height is sensitive to several input factors, including the parameters deﬁning the model
inadequacy function, indicating that its contribution to the calibrated prediction, and
therefore to the risk analysis, is signiﬁcant.
8.1.5
General comments
Other than optimisation, the automatic calibration method currently in most widespread
use in hydrology is the Generalised Likelihood Uncertainty Estimation method of Beven
and Binley (1992). The relaxation of the necessity of a formal likelihood in this method,
and the possibility of discarding model runs deemed non-behavioural, means that the
method is easy to use, but also that its results are somewhat arbitrary. In addition, the

Chapter 8. Conclusions and recommendations for further work
124
failure of the method to distinguish between diﬀerent error sources potentially leads to
biassed parameterisations.
The diﬃculty of the calibration problem is indicated by the fact that no calibration
methodology has been adopted that addresses the well-known deﬁciences of GLUE. While
the Bayesian methodology proposed and demonstrated in this thesis appears to oﬀer a clear
improvement over GLUE for the calibration of steady state models using satellite image
information, comparison of the steady state calibration using the current methodology
with one undertaken using GLUE does not show a large diﬀerence in the ﬂood probability
map. This similarity could be used as an argument that the diﬃculties in applying the
current method are not justiﬁed by an obvious diﬀerence in the results. However, the
example of calibration of a dynamic model of the river Severn shows a clear advantage for
a method which can incorporate bias correction.
It cannot be denied that there are diﬃculties in using the current method. The statistical
model equations take some eﬀort to understand, and are not straightforward to program.
The use of Markov chain Monte Carlo is undeniably not straightforward, and cannot be
entirely automated, although the ﬂood extent model does not pose a diﬃcult problem for
the calculation method. Covariance matrix stability is a signiﬁcant problem for adjacent
input or output locations, and necessitates the discarding of model output or, more seri-
ously, of data. The choice of prior distributions did not pose a diﬃcult problem for the
most part, although it did require some thought.
The greatest need for model calibration in hydrology is for calibration of dynamic models,
using gauged data. The work in this thesis represents a partial solution of this problem.
No other calibration study of a physically based hydrological model has demonstrated bias
correction, and at the same time produced output suitable for risk analysis.
Integral to the current dynamic calibration methodology is the deﬁnition of an emula-
tor. While an emulator is not an absolute necessity for model calibration, it is not really
practicable to calibrate a complex physically-based spatio-temporal model without one.
However, the emulator used here, while both parsimonious and eﬀective, has posed diﬃcul-
ties in the formulation of the error model in the time domain, undermining the statistical
credibility of the calibration method.
8.2
Recommendations for further work
The study in this thesis has demonstrated a feasible method for calibration of hydraulic
models in the presence of model inadequacy, and has indicated how this calibration method
can be incorporated in a risk analysis calculation. However, in order to be of signiﬁcant
practical use in the improvement of risk analysis, a number of issues need to be ad-
dressed:
Calibration has been demonstrated separately using spatial data, and using temporally
varying gauged data. Pappenberger et al. (2005) noted an improvement in parameter

Chapter 8. Conclusions and recommendations for further work
125
determination where both spatial and temporal data are used for calibration.
Model inadequacy represents a major potential source of bias in the output of a calibration.
However, there are other sources of bias which have not been addressed in this study. In
particular, the calibration and output of a model is aﬀected by errors in the determination
of input forcing. This error source is most signiﬁcant where the model input is rainfall,
which varies spatially to an extent that it cannot be precisely determined by a typical
raingauge network, or ﬂow, which may be determined by rating curve thus introducing
bias, or where there is signiﬁcant unmeasured lateral inﬂow. Naturally, there may well
be an identiﬁability issue here, because of the number of uncertain quantities relative to
the information contained in the observations (Renard et al., 2010). Problems of identi-
ﬁability represent a fundamental limitation in the absence of more, and more accurate,
observations.
The proposed methodology should be extended to diﬀerent types of models; speciﬁcally
rainfall-runoﬀmodels, where the input is rainfall, and the model represents the hydro-
logical behaviour of the entire catchment, thus allowing a broader range of ﬂood defence
measures to be considered. Besides the issue of input errors, another potential problem
with the calibration of rainfall-runoﬀmodels is the number of parameters used in these
models. It is likely that the distributions of some of these parameters may be correlated,
causing further diﬃculties in solution.
A full risk analysis requires consideration of long term consequences of processes of change;
this necessitates analysis of the impacts of climate change. To address these processes,
analysis will entail a sequence of models; climate change models, a rainfall model, a
rainfall-runoﬀmodel, a hydraulic model, a model of ﬂood defence failure, and a model
of propagation of a ﬂood wave through the ﬂoodplain. Each of these modelling stages will
introduce uncertainties and errors into the ﬁnal risk calculation. The errors need to be
propagated through the modelling cascade.
It was demonstrated that in the extension of the calibration to a modiﬁed channel, it is
unclear at what point a modiﬁcation is too large for the calibration to remain credible.
Under these circumstances, it would be helpful to use a second model, possibly a more
complex one, as arbitrator. Kennedy and O’Hagan (2000) considered the use of a simpler
model to serve as an emulator for a more complex model. An alternative might be to
calibrate two models in parallel, using the more complex model to inform the emulation
and calibration of the simpler model.
Finally, the work in this thesis has concentrated on the contribution of ﬂuvial ﬂood models
to ﬂood risk analysis. Similar techniques can be used for calibration of other models, both
for risk analysis, and for other applications.

References
Abbott, M.B., Bathurst, J.C., Cunge, J.A., O’Connell, P.E. and Rasmussen, J., 1986. An
introduction to the European hydrological system - Systeme Hydrologique Europeen,“SHE”,
2: Structure of a physically-based, distributed modelling system. Journal of Hydrology,
87, 61-77.
Ajami, N.K., Duan, Q. and Sorooshian, S., 2007.
An integrated hydrologic Bayesian
multimodel combination framework: Confronting input, parameter, and model structural
uncertainty in hydrologic prediction. Water Resources Research, 43, W01403.
Anderson, T.W., 1958. An introduction to multivariate statistical analysis. Wiley, New
York.
Andrieu, C. and Thoms, J., 2008. A tutorial on adaptive MCMC. Statistics and Comput-
ing, 18, 343-373.
Apel, H., Thieken, A.H., Merz, B. and Bl¨oschl, G., 2004.
Flood risk assessment and
associated uncertainty. Natural Hazards and Earth System Sciences, 4, 295-308.
Apel, H., Thieken, A.H., Merz, B. and Bl¨oschl, G., 2006. A probabilistic modeling system
for assessing ﬂood risks. Natural Hazards, 38, 79-100
Apel, H., Merz, B. and Thieken, A.H., 2008. Quantiﬁcation of uncertainties in ﬂood risk
assessments. International Journal of River Basin Management, 6(2), 149-162.
Aronica G., Bates, P.D. and Horritt, M.S., 2002. Assessing the uncertainty in distributed
model predictions using observed binary pattern information within GLUE. Hydrological
Processes, 16, 2001-2016.
Bastos, L.S. and O’Hagan A., 2009. Diagnostics for Gaussian process emulators. Techno-
metrics, 51(4), 425-438.
Batchelor, G.K., 1967. An introduction to ﬂuid dynamics. Cambridge.
Bates, P.D. and de Roo, A.P.J., 2000. A simple raster-based model for ﬂood inundation
simulation. Journal of Hydrology 236, 54-77.
Bates, P.D., Horritt, M.S., Aronica, G. and Beven, K., 2004. Bayesian updating of ﬂood
inundation likelihoods cobnditioned on ﬂood extent data.
Hydrological Processes, 18,
3347-3370.
Bathurst, J.C., 1986. Physically-based distributed modelling of an upland catchment using
the Systeme Hydraulique Europeen. Journal of Hydrology, 87, 79-102.
Bayarri, M.J., Berger, J.O., Cafeo, J., Garcia-Donato, G., Liu, F., Palomo, J., Parthasarathy,
R.J., Paulo, R., Sacks, J., Walsh, D., 2007. Computer model validation with functional
output. Annals of Statistics, 35(5), 1874-1906.
126

Beaumont, M.A., Zhang, W. and Balding, D.J., 2002. Approximate Bayesian Computa-
tion in Population Genetics. Genetics, 162, 2025-2035.
Best, N.G., Cowles, M.K. and Vines, K., 1995. CODA: Convergence diagnostics and out-
put analysis software for Gibbs sampling output, version 0.30, Technical report, University
of Cambridge, MRC Biostatistics unit.
Beven, K. and Binley, A., 1992. The future of distributed models: model calibration and
uncertainty prediction. Hydrological Processes, 6, 279-298.
Beven, K.J., Young, P.C., Leedal, D.T. and Romanowicz, R., 2009.
Computationally
eﬃcient ﬂood water level prediction (with uncertainty). In: P. Samuels, S. Huntingdon,
W. Allsop and J. Harrop eds., Flood risk management : research and practice. CRC Press,
London.
Bhattacharya, S., 2007. A simulation approach to Bayesian emulation of complex dynamic
computer models. Bayesian Analysis, 2(4) 783-816.
Boorman, D.B., Hollis, J.M. and Lilly, A., 1995. Hydrology of Soil Types: a hydrologically
based classiﬁcation of the soils of the UK Institute of Hydrology Report 126.
Box, G.E.P. and Cox, D.R., 1964. An analysis of transformations. Journal of the Royal
Statistical Society, series B, 26(2), 211-252.
Boyle, D.P., Gupta, H.V. and Sorooshian S., 2000. Toward improved calibration of hy-
drologic models: combining the strengths of manual and automatic methods.
Water
Resources Research, 36(12), 3663-3674.
Campolongo F., Cariboni, J. and Saltelli, A., 2007. An eﬀective screening design for sensi-
tivity analysis of large models Environmental modelling and Software, 22, 1509-1518.
Chow, V.T., Maidment, D.R. and Mays, L.W., 1988.
Applied Hydrology.
McGraw-
Hill.
Ciarapica, L. and Todini, E., 2002. TOPKAPI: a model for the representation of the
raintfall-runoﬀprocess at diﬀerent scales. Hydrological Processes, 16, 207-229.
Coles, S., 2001. An introduction to statistical modelling of extreme values. Springer.
Conti, S. Gosling, J.P., Oakley, J. and O’Hagan, A., 2009. Gaussian process emulation of
dynamic computer codes. Biometrika, 96, 663-676.
Coomber, J.R., 2006. Natural and large catastrophes - changing risk characteristics and
challenges for the insurance industry. The Geneva Papers, 31, 88-95.
Cowles, M.K. and Carlin, B.P., 1996. Markov chain Monte Carlo convergence diagnostics:
A comparative review. Journal of the American Statistical Association, 91, 883-904.
Craig, P.S., Goldstein, M., Rougier, J.C. and Seheult, A.H., 2001.
Bayesian forecast-
ing for complex systems using computer simulators. Journal of the American Statistical
Association, 961(454), 717-729.
Dawson, R., Hall, J., Sayers, P., Bates, P. and Rosu, C., 2005. Sampling-based ﬂood risk
analysis for ﬂuvial dike systems. Stochastic Environmental Research and Risk Assessment,
19: 388-402.
Degasperi, A. and Gilmore, S., 2008. Sensitivity Analysis of Stochastic Models of Bistable
Biochemical Reactions. In: M. Bernardo, P. Degano and G. Zavattaro eds., Formal Meth-
ods in Computational Systems Biology. Springer.
127

Deutsch, C.V. and Journel, A.G., 1998. GSLIB: Geostatistical software library and user’s
guide. O.U.P.
Di Baldassarre, G., Castellarin, A., Montanari, A. and Brath, A., 2009.
Probability-
weighted hazard maps for comparing diﬀerent ﬂood risk management strategies: a case
study. Natural Hazards, 50, 479-496.
Dr´ecourt, J.-P., Madsen, H. and Rosbjerg, D., 2006. Bias aware Kalman ﬁlters: compari-
son and improvements. Advances in Water Resources, 29, 707-718.
Duan, Q., Ajami, N.K., Gao, X. and Sorooshian, S., 2007. Multi-model ensemble hy-
drologic prediction using Bayesian model averaging Advances in Water Resources, 30,
1371-1386.
Duan, Q., Sorooshian, S. and Gupta, V.K., 1992. Eﬀective and eﬃcient global optimiza-
tion for conceptual rainfall-runoﬀmodels. Water Resources Research, 28, 1015-1031.
Environment Agency, 2007. Flooding in Shrewsbury: joint ﬂood action plan. Environment
Agency.
Environment Agency, 2009a. Flood risk and coastal management in England. Environment
Agency.
Environment Agency, 2009b. River Severn catchment ﬂood management plan - ﬁnal report.
Environment Agency.
Environment Agency, 2010. Station summary: Severn at Montford (54005).
http://www.environment-agency.gov.uk/hiﬂows/station.aspx?54005 accessed 9 Aug, 2010
Ervine, D.A., Willets, B.B., Sellin R.H.J. and Lorena, M., 1993. Factors aﬀecting con-
veyance in Meandering Compound Flows. Journal of Hydraulic Engineering, 119, 1383-
1399.
European Union, 2007.
Directive 2007/60/EC of the European Parliament and of the
Council, of 23 October 2007 on the assessment and management of ﬂoods. Available from:
http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32007L0060:EN:NOT
Evensen, G., 2003. The ensemble Kalman ﬁlter: theoretical formulation and practical
implementation. Ocean Dynamics, 53(4), 343-367.
Evensen, G. and van Leeuwen, P.J., 2000. An ensemble Kalman smoother for nonlinear
dynamics. Monthly Weather Review, 128, 1852-1867.
Ewen, J., Parkin, G. and O’Connell, P.E., 2000 SHETRAN: Distributed River Basin
Flow and Transport Modelling System. ASCE Journal of Hydrologic Engineering 5, 250-
258.
Ewen, J., O’Donnell, G.M., Burton, A. and O’Connell P.E., 2006.
Errors and uncer-
tainty in physically-based rainfall-runoﬀmodelling of catchment change eﬀects. Journal
of Hydrology, 330(3-4), 641-650.
Feyen, L., Vrugt, J.A., ´O Nuall´ain, B., van der Knijﬀ, J. and De Roo, A., 2007. Parameter
optimisation and uncertainty assessment for large-scale streamﬂow simulation with the
LISFLOOD model. Journal of Hydrology, 332(3-4), 276-289.
Fiering, M.B., 1967. Streamﬂow synthesis. Harvard Univ. Press, Cambridge, Mass.
Fill, H.D. and Steiner, A.A., 2003. Estimating instantaneous peak ﬂow from mean daily
ﬂow data. ASCE Journal of Hydraulic Engineering, 109, 549-563.
128

Freeze, R.A. and Harlan, R.L., 1969. Blueprint for a physically-based, digitally-simulated
hydrologic response model. Journal of Hydrology, 9, 237-258.
Gaggiotti O.E., 2010. Preface to the special issue: advances in the analysis of spatial
genetic data. Molecular Ecology Resources, 10, 757-759.
Gelman, A. and Rubin, D.B., 1992. Inference from iterative simulation using multiple
sequences (with discussion). Statistical Science, 7, 457-511.
Georgakakos, K.P., D.-J. Seo, H. Gupta, J. Schaake and M.B. Butts, 2004.
Towards
the characterization of streamﬂow simulation uncertainty through multimodel ensembles.
Journal of Hydrology, 298, 222-241.
Geweke, J., 1992. Evaluating the accuracy of sampling-based approaches to the calculation
of posterior moments. In J.M. Bernardo, J.O. Berger, A.P. Dawid and A.F.M. Smith, eds.
Baysian Statistics 4, O.U.P.
Goldstein, M. and Rougier, J.C., 2004. Probabilistic formulations for transferring infer-
ences from mathematical models to physical systems. SIAM Journal on Scientiﬁc Com-
puting, 26(2), 467-487.
Goldstein, M. and Rougier, J.C., 2006. Bayes linear calibrated prediction for complex
systems. Journal of the American Statistical Association, 101(475), 1132-1143.
Goldstein, M. and Rougier, J.C., 2009.
Reiﬁed Bayesian Modelling and Inference for
Physical Systems (with discussion and rejoinder).
Journal of Statistical Planning and
Inference, 139(3), 1221-1239.
Goldstein, M. and Wooﬀ, D., 2007. Bayes Linear Statistics, Theory and Methods. Wiley.
ISBN 978-0-470-01562-9
Haario, H., Laine, M., Mira, A. and Saksman, E., 2006. DRAM: Eﬃcient adaptive MCMC.
Statistics and Computing, 16, 339:354.
Haario, H., Saksman, E. and Tamminen, J., 1999.
Adaptive proposal distribution for
random walk Metropolis algorithm. Computational Statistics, 14, 375-395.
Haario, H., Saksman, E., Tamminen, J., 2001. An adaptive Metropolis algorithm. Bernoulli,
70(2), 223-242.
Hall, J.W., Dawson, R. J., Sayers, P. B., Rosu, C., Chatterton, J. B. and Deakin, R.,
2003. A methodology for national-scale ﬂood risk assessment. Proceedings of the ICE -
Maritime Engineering, 156(3), 235-247.
Hall, J.W., Manning, L.J. and Hankin, R.K.S., 2011.
Bayesian calibration of a ﬂood
inundation model using spatial data. Water Resources Research, in press.
Hall, J.W., Tarantola, S., Bates, P.D. and Horritt, M.S., 2005. Distributed sensitivity
analysis of ﬂood inundation calibration. Journal of Hydraulic Engineering, 131(2), 117-
126.
Halton, J.H., 1960.
On the eﬃciency of certain quasi-random sequences of points in
evaluating multi-dimensional integrals. Numerische Mathematik, 2, 84-90.
Hankin, R.K.S., 2005. Introducing BACCO, an R bundle for Bayesian analysis of computer
code output. Journal of Statistical Software, 14(16).
Harrison, P.J. and Stevens, C.F., 1976. Bayesian Forecasting (with discussion). Journal
of the Royal Statistical Society, series B, 38, 205-247.
129

Hastings, W.K., 1970. Monte Carlo sampling methods using Markov chains and their
applications, Biometrika, 57, 97-109.
Heidelberger, P. and Welch, P.D., 1981. A spectral method for conﬁdence interval gen-
eration and run length control in simulations. Communications of the ACM, 24, 233-
245.
Higdon, D., Gattiker, J., Williams, B. and Rightley, M., 2008a. Computer Model Cali-
bration Using High-Dimensional Output. Journal of the American Statistical Association,
103(482), 570-583.
Higdon, D., Kennedy, M.C., Cavendish, J., Cafeo, J. and Ryne, R.D., 2004. Combining
ﬁeld data and computer simulations for calibration and prediction.
SIAM Journal on
Scientiﬁc Computing, 26(2), 448-466.
Higdon, D., Nakhleh, C., Gattiker, J. and Williams, B., 2008b. A Bayesian calibration
approach to the thermal problem. Computer Methods in Applied Mechechanics and En-
gineering, 197, 2431-2441.
Hoeting, J.A., Madigan, D., Raftery, A.E. and Volinsky, C.T., 1999.
Bayesian model
averaging: a tutorial. Statistical Science, 14(4), 382-417.
Horritt, M.S., Mason, D.C. and Luckman, A.J., 2001. Flood boundary delineation from
Synthetic Aperture Radar imagery using a statistical active contour model. International
Journal of Remote Sensing, 22(13), 2489-2507.
House, L., 2009. An application of reiﬁcation to a rainfall-runoﬀcomputer model Unpub-
lished internal report 2.1.4, Managing Uncertainty in Computer Models research project.
Huard, D.and Maillot, A., 2006. A Bayesian perspective on input uncertainty in model
calibration:
application to hydrological model ‘abc’.
Water Resources Research, 42,
W07416.
Iman, R.L. and Conover, W.J., 1982. A distribution-free approach to inducing rank corre-
lation among input variables. Communications in statistics - simulation and computation,
B11, 311-334.
International Disaster Database, 2010. www.emdat.net, accessed 6-05-2010.
Jacques, J., Lavergne, C. and Devictor, N., (2006). Sensitivity analysis in presence of
model uncertainty and correlated inputs. Reliability Engineering and System Safety, 91,
1126-1134.
Jeﬀreys, H., 1961. Theory of probability Oxford: Clarendon Press
Kalman, R.E., 1960. A new approach to linear ﬁltering and prediction problems. Trans-
actions of the ASME Journal of Basic Engineering, (Series D), 82, 35-45.
Kavetski, D., Kuczera, G. and Franks, S.W., 2002.
Confronting input uncertainty in
environmental modelling. In Q. Duan, H.V. Gupta, S. Sorooshian, A.N. Rousseau and R.
Turcotte (eds) Calibration of watershed models. AGU, Washington.
Kavetski, D., Kuczera, G. and Franks, S.W., 2006. Bayesian analysis of input uncertainty
in hydrological modelling 2: Application. Water Resources Research, 42, W03408.
Kennedy, M.C. and O’Hagan, A., 2000. Predicting the output from a complex computer
code when fast approximations are available. Biometrika, 87, 1-13.
Kennedy, M.C. and O’Hagan, A., 2001a. Bayesian calibration of computer models. Jour-
nal of the Royal Statistical Society, series B, 63(3), 425-464.
130

Kennedy, M.C. and O’Hagan, A., 2001b. Supplementary details on Bayesian calibration
of computer models. Available at www.shef.ac.uk/∼st1ao/ps/calsup.ps
Knight, D.W. and Sellin, R.H.J., 1987. The SERC Flood Channel Facility. Journal of the
Institution of Water and Environmental Management, 1(2), 198-204.
Knight, D.W., Shiono, K., 1996.
River Channel and Floodplain Hydraulics.
In M.G.
Anderson, D.E. Walling and P.D. Bates (eds), Floodplain Processes. Wiley.
Kollat J.B., Reed, P.M. and Rizzo, D.M., 2008. Assessing model bias and uncertainty
in three-dimensional groundwater transport forecasts for a physical aquifer experiment.
Geophysical Research Letters, 35, L17402.
Krzysztofowicz, R., 1999. Bayesian theory of probabilistic forecasting via deterministic
hydrologic model. Water Resources Research, 35(9), 2739-2750.
Krzysztofowicz, R., 2002. Bayesian system for probabilistic river stage forecasting. Jour-
nal of Hydrology, 268, 16-40.
Krzysztofowicz, R. and Herr, H.D., 2001. Hydrologic uncertainty processor for proba-
bilistic river stage forecasting: precipitation-dependent model. Journal of Hydrology, 249,
46-68.
Krzysztofowicz, R. and Kelly, K.S., 2000. Hydrologic uncertainty processor for probabilis-
tic river stage forecasting. Water Resources Research, 36(11), 3265-3277.
Kuczera, G., Kavetski, D., Franks, S.W. and Thyer, M., 2006. Towards a Bayesian total
error analysis of conceptual rainfall-runoﬀmodels: characterising model error using storm-
dependent parameters. Journal of Hydrology, 331, 161-77.
Lin, Z. and Beck, M.B., 2007a. On the identiﬁcation of model structure in hydrological
and environmental systems. Water Resources Research, 43, W02402.
Lin, Z. and Beck, M.B., 2007b. Understanding dual complex environmental systems: a
dual approach. Environmetrics, 18, 11-26.
Linsley, R.K.Jr., Kohler, M.A. and Paulus, J.L.H., 1988. Hydrology for engineers. SI
edition, McGraw-Hill
Little, J., Goldstein, M. and Jonathan P., 2004. Eﬃcient Bayesian sampling inspection
for industrial processes based on transformed spatio-temporal data. Statistical Modelling,
4, 299-313.
Liu, F. and West, M., 2009. A dynamic modelling strategy for Bayesian computer model
emulation. Bayesian Analysis, 4(2), 393-412.
Manning, L.J. and Hall, J.W., 2010. A Bayesian approach to analysing structural un-
certainties in ﬂood inundation models. Proceedings of the 1st European IAHR Congress,
Edinburgh 2010, 4th-6th May 2010.
Mantovan, P. and Todini, E., 2006. Hydrological forecasting uncertainty assessment: In-
coherence of the GLUE methodology. Journal of Hydrology, 330(1-2) 368-381.
Marrel, A., Iooss, B., da Viega, S. and Ribatet, M., 2011. Global sensitivity analysis of
stochastic computer models with joint metamodels. Statistics and Computing, in review.
http://fr.arxiv.org/abs/0911.1189
Marshall, L., Nott, D. and Sharma, A., 2004. A comparative study of Markov chain Monte
Carlo methods for conceptual rainfall-runoﬀmodeling. Water Resources Research, 40(2),
W02501.
131

McKay, M.D., 1995. Evaluating prediction uncertainty. Internal report NUREG/GR-6311
Los Alamos National Laboratory.
McKay, M.D., Conover, W.J. and Beckman, R.J., 1979. A comparison of three methods
for selecting values of input variables in the analysis of output from a computer code.
Technometrics, 21, 239-245.
McNeall, D.J., 2008. Dimension reduction in the Bayesian analysis of a numerical climate
model. Doctoral Thesis, University of Southampton.
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller A.H. and Teller, E., 1953.
Equations of State calculations by fast computing machine. Journal of Chemical Physics,
21, 1087-1091.
Moradkhani, H., Sorooshian S., Gupta, H.V., Houser, P., 2005a. Dual State-Parameter
Estimation of Hydrological Models using Ensemble Kalman Filter. Advances in Water
Resources, 28(2),135-147.
Moradkhani, H., Hsu, K., Gupta, H. V. and Sorooshian, S., 2005b. Uncertainty Assess-
ment of Hydrologic Model States and Parameters: Sequential Data Assimilation Using
Particle Filter. Water Resources Research, 41, W05012.
Morris, M.D., 1991. Factorial sampling plans for preliminary computational experiments.
Technometrics, 33(2), 161-174.
Montanari, A., 2005. Large sample behaviors of GLUE in assessing the uncertainty of
rainfall-runoﬀsimulations. Water Resources Research, 41, W08406.
Moore, R.J. and Clark, R.T., 1981. A distribution function approach to rainfall runoﬀ
modeling Water Resources Research, 17(5), 1367-1382.
Myers W.R.C. and Brennan E.K., 1990. Flow resistance in compound channels, Journal
of Hydraulic Research, 28(2),141-155.
Nash, J.E., 1959. Systematic determination of unit hydrograph parameters. Journal of
Geophysical Research, 64, 111-115.
Nash, J.E. and Sutcliﬀe, J.V., 1970. River ﬂow forecasting through conceptual models.
Part 1 - A discussion of principles. Journal of Hydrology, 10, 282-290.
Neuman, S.P., 2003. Maximum likelihood Bayesian averaging of uncertain model predic-
tions. Stochastic Environmental Research and Risk Assessment, 17,(5), 291-305.
Oakley, J., 2002.
Eliciting Gaussian process priors for complex computer codes.
The
Statistician, 51, 81-97.
Oakley, J. and O’Hagan, A., 2002. Bayesian inference for the uncertainty distribution of
computer model outputs. Biometrika, 89(4), 769-784.
Oakley, J. and O’Hagan, A., 2004. Probabilistic sensitivity analysis of complex models: a
Bayesian approach. Journal of the Royal Statistical Society, series B, 66(3), 751-769
O’Hagan, A. and Forster, J., 2004.
Kendall’s advanced theory of statistics:
Vol 2B
Bayesian inference (2nd edition). Arnold.
Oﬃce of Public Sector Information, 2009. Flood Risk Regulations 2009.
http://www.opsi.gov.uk/si/si2009/uksi 20093042 en 1 Accessed 13-06-2010
132

Oﬃce of Public Sector Information, 2010.
Flood and Water Management Act, 2010,
Chapter 29. http://www.opsi.gov.uk/acts/acts2010/pdf/ukpga 20100029 en.pdf Accessed
13-06-2010
Owen, A.B., 1994. Controlling Correlations in Latin Hypercube Samples. Journal of the
American Statistical Society 89, 1517-1522.
Pappenberger, F., Beven, K., Horritt, M. ans Blazkova, S., 2005. Uncertainty in the cali-
bration of eﬀective roughness parameters in HEC-RAS using inundation and downstream
level observations Journal of Hydrology, 302, 46-69.
Parkin, G., O’Donnell, G.M., Ewen, J., Bathurst, J.C., O’Connell, P.E. and Lavabre, J.,
1996. Validation of catchment models for predicting land-use and climate change impacts.
2: Case study for a Mediterranean catchment. Journal of Hydrology, 175, 595-613.
Plummer, M., Best, N., Cowles, M. and Vines, K., 2006. CODA: Convergence Diagnosis
and Output Analysis for MCMC. R-news, 6(1) 7-11.
http://CRAN.R-project.org/doc/Rnews/Rnews 2006-1.pdf (accessed 27-06-2010)
Price, R.K., Fernando, W.A.Y.S and Solomantine, D.P., 2006. Inverse modelling for ﬂood
propagation. In: P. Goubesville, J. Cunge, V. Guinot and S. Y. Liong, eds, Proceedings
of the 7th International Conference in Hydroinformatics, Research Publishing Services,
Chennai, India.
R Development Core Team, 2009. R: A language and environment for statistical comput-
ing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL
http://www.R-project.org.
Raftery, A.E. and Lewis, S.M., 1996. Implementing MCMC. In W.R. Gilks, S. Richardson
and D.J. Spiegelhalter, Markov Chain Monte Carlo in Practice Chapman and Hall.
Reichert P. and Mieleitner, J., 2009. Analyzing input and structural uncertainty of non-
linear dynamic models with stochastic, time-dependent parameters.
Water Resources
Research, 45, W10402.
Renard, B., Kavetski, D. and Kuczera, G., 2009. Comment on “An integrated hydro-
logic Bayesian multimodel combination framework: Confronting input, parameter, and
model structural uncertainty in hydrologic prediction”. Water Resources Research, 45,
W03603.
Renard, B., Kavetski, D., Kuczera, G., Thyer, M. and Franks, S.W., 2010. Understanding
predictive uncertainty in hydrologic modelling: the challenge of identifying input and
structural errors. Water Resources Research, 46, W05521.
Roberts, G.O., Gelman, A., Gilks, W.R., 1997. Weak convergence and optimal scaling of
random walk Metropolis algorithms. Annals of Applied Probability, 7(1), 110-120.
Robson, A.J. and Reed, D.W., 1999. Statistical procedures for ﬂood frequency estimation.
Volume 3 of the Flood Estimation Handbook. Centre for Ecology and Hydrology
Rojas, R., Feyen, L. and Dassargues A., 2008. Conceptual model uncertainty in ground-
water modeling: combining generalized likelihood uncertainty estimation and Bayesian
model averaging. Water Resources Research, 44, W12418.
133

Romanowicz, R., Beven, K.J. and Tawn J., 1996. Bayesian calibration of ﬂood inundation
models.
In M.G. Anderson, D.E. Walling and P.D. Bates (eds), Floodplain Processes.
Wiley.
Romanowicz, R.J., Young, P.C., Beven, K.J. and Pappenberger, F., 2008. A data based
mechanistic approach to nonlinear ﬂood routing and adaptive ﬂood level forecasting. Ad-
vances in Water Resources, 31, 1048-1056.
Rougier, J.C., 2008. Eﬃcient emulators for multivariate deterministic functions. Journal
of Computational and Graphical Statistics, 17(4), 827-843.
Rougier J.C., Sexton, D.M.H., Murphy, J.M. and Stainforth, D., 2009. Analyzing the
climate sensitivity of the HadSM3 climate model using ensembles from diﬀerent but related
experiments. Journal of Climate, 22, 3540-3557.
Sacks J., Welch W.J. Mitchell T.J. and Wynn, H.P., 1989. Design and analysis of computer
experiments. Statistical Science, 4(4), 409-435.
Sallaberry, C.J., Helton, J.C. and Hora, S.C., 2008. Extension of Latin hypercube samples
with correlated variables. Reliability Engineering and System Safety, 93, 1047-1059.
Sellin R.H.J., 1964. A laboratory investigation into the interaction between the ﬂow in
the channel of a river, and that over its ﬂoodplain. La Houille Blanche, 7, 793-801.
Sivapalan, M., Bl¨oschl, G., Zhang, L. and Vertessy, R., 2003. Downward approach to
hydrological prediction Hydrological Processes, 17(11), 2101-2111.
Smith, M.B., Seo, D.-J., Koren, V.I., Reed, S.M., Zhang, Z., Duan, Q., Moreda, F. and
Cong, S., 2004. The distributed model intercomparison project (DMIP): motivation and
experiment design. Journal of Hydrology,298(1-4), 4-26.
Sobol’, I.M., 1998.
On quasi-Monte Carlo algorithms Mathematics and computers in
simulation, 47, 103-112.
Sorooshian, S. and Dracup, J.A., 1980. Stochastic parameter estimation procedures for
hydrologic rainfall-runoﬀmodels: correlated and heteroscedastic error cases. Water Re-
sources Research, 16, 430-442.
Stedinger, J.R., Vogel, R.M., Lee, S.U. and Batchelder, R., 2008. Appraisal of Generalized
Likelihood Uncertainty Estimation (GLUE) methodology. Water Resources Research, 44,
WOOB06.
Stigter, J.D. and Beck, M.B., 2004. On the development and application of a continuous-
discrete recursive prediction error algorithm. Mathematical Biosciences, 191, 141-158.
Taylor, C.J., Pedregal, D.J., Young, P.C. and Tych, W., 2007. Environmental time series
analysis and forecasting with the Captain toolbox. Environmental Modelling and Software,
22, 797-814.
ter Braak, C.J.F., 2006. A Markov chain Monte Carlo version of the genetic algorithm
Diﬀerential Evolution: easy Bayesian computing for real parameter spaces. Statistics and
Computing, 16, 239-249.
Thyer, M.A., Renard, B., Kavetski, D., Kuczera, G., Franks, S.W. and Srikanthan, S.,
2009.
Critical evaluatrion of parameter consistency and predictive uncertiainty in hy-
drological modelling: a case study using Bayesian total error analysis. Water Resources
Research, 45, W00B14.
134

Todini, E., 1996. The ARNO rainfall-runoﬀmodel. Journal of Hydrology, 175(1-4), 339-
382.
Todini, E., 2008. A model conditional processor to assess predictive uncertainty in ﬂood
forecasting. Journal of River Basin Management, 6(2), 123-137.
U.S. Army Corps of Engineers, 2002. Hec Ras River Analysis System.
http://www.hec.usace.army.mil/software/hec-ras/index.html
Vrugt, J.A., Gupta, H.V., Bouten, W. and Sorooshian, S., 2003.
A shuﬄed complex
evolution metropolis algorithm for optimisation and uncertainty assessment of hydrologic
model parameters. Water Resources Research, 39(8), 1201.
Vrugt, J.A., ter Braak, C.J.F., Clark, M.P., Hyman, J.M. and Robinson, B.A., 2008.
Treatment of input uncertainty in hydrologic modeling: doing hydrology backward with
Markov chain Monte Carlo simulation. Water Resources Research, 44, W00B09.
Wagener, T., McIntyre, N., Lees, M.J., Wheater, H.S. and Gupta, H.V., 2003. Towards
reduced uncertainty in conceptual rainfall-runoﬀmodelling: Dynamic identiﬁability anal-
ysis. Hydrological Processes, 17(2), 455-476.
Werner, M.G.F., Hunter, N.M. and Bates, P.D., 2005.
Identiﬁability of distributed
ﬂoodplain roughness values in ﬂood extent estimation. Journal of Hydrology, 314, 139-
157.
West, M. and Harrison, J. 1989. Bayesian forecasting and dynamic models. Springer, New
York.
Wilkinson, R.D., 2010. Bayesian calibration of expensive computer experiments. In L.T.
Biegler, G. Biros, O. Ghattas, M. Heinkenschloss, D. Keyes, B.K. Mallick, L. Tenorio, B.
Van Bloemen Waanders and K. Wilcox eds. Large scale inverse problems and quantiﬁcation
of uncertainty, Wiley.
Wood, E.F., Lettenmaier, D.P. and Zartarian, V.G., 1992. A land-surface hydrology pa-
rameterization with subgrid variability for General Circulation Models. Journal of Geo-
physical Research, 97, 2717-2728.
Woodhead, S.P.B., 2007. Bayesian calibration of ﬂood inundation simulators using an
observation of ﬂood extent. Doctoral thesis, University of Bristol.
Wynn, H.J., 2001. Contribution to the discussion on the paper by Kennedy and O’Hagan.
Journal of the Royal Statistical Society, series B, 63(3), 425-464.
Young, P.C., 1984. Recursive estimation and time series analysis: an introduction. Springer-
Verlag.
Young, P.C., 2003. Top-down and data-based mechanistic modelling of rainfall-ﬂow dy-
namics at catchment scale. Hydrological Processes, 17(11), 2195-2217.
Zhao, R.J., 1992. The Xinanjiang model applied in China, Journal of Hydrology, 135,
371-381.
135

