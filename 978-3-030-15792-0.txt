Emergence, Complexity and Computation ECC
Andrew Adamatzky
Vivien Kendon    Editors 
From 
Astrophysics to 
Unconventional 
Computation
Essays Presented to Susan Stepney on 
the Occasion of her 60th Birthday

Emergence, Complexity and Computation
Volume 35
Series Editors
Ivan Zelinka, Technical University of Ostrava, Ostrava, Czech Republic
Andrew Adamatzky, University of the West of England, Bristol, UK
Guanrong Chen, City University of Hong Kong, Hong Kong, China
Editorial Board Members
Ajith Abraham, MirLabs, USA
Ana Lucia, Universidade Federal do Rio Grande do Sul, Porto Alegre, Rio Grande
do Sul, Brazil
Juan C. Burguillo, University of Vigo, Spain
Sergej Čelikovský, Academy of Sciences of the Czech Republic, Czech Republic
Mohammed Chadli, University of Jules Verne, France
Emilio Corchado, University of Salamanca, Spain
Donald Davendra, Technical University of Ostrava, Czech Republic
Andrew Ilachinski, Center for Naval Analyses, USA
Jouni Lampinen, University of Vaasa, Finland
Martin Middendorf, University of Leipzig, Germany
Edward Ott, University of Maryland, USA
Linqiang Pan, Huazhong University of Science and Technology, Wuhan, China
Gheorghe Păun, Romanian Academy, Bucharest, Romania
Hendrik Richter, HTWK Leipzig University of Applied Sciences, Germany
Juan A. Rodriguez-Aguilar, IIIA-CSIC, Spain
Otto Rössler, Institute of Physical and Theoretical Chemistry, Tübingen, Germany
Vaclav Snasel, Technical University of Ostrava, Czech Republic
Ivo Vondrák, Technical University of Ostrava, Czech Republic
Hector Zenil, Karolinska Institute, Sweden

The Emergence, Complexity and Computation (ECC) series publishes new
developments, advancements and selected topics in the ﬁelds of complexity,
computation and emergence. The series focuses on all aspects of reality-based
computation approaches from an interdisciplinary point of view especially from
applied sciences, biology, physics, or chemistry. It presents new ideas and inter-
disciplinary insight on the mutual intersection of subareas of computation, com-
plexity and emergence and its impact and limits to any computing based on
physical limits (thermodynamic and quantum limits, Bremermann’s limit, Seth
Lloyd limits…) as well as algorithmic limits (Gödel’s proof and its impact on
calculation, algorithmic complexity, the Chaitin’s Omega number and Kolmogorov
complexity, non-traditional calculations like Turing machine process and its con-
sequences,…) and limitations arising in artiﬁcial intelligence. The topics are
(but not limited to) membrane computing, DNA computing, immune computing,
quantum computing, swarm computing, analogic computing, chaos computing and
computing on the edge of chaos, computational aspects of dynamics of complex
systems (systems with self-organization, multiagent systems, cellular automata,
artiﬁcial life,…), emergence of complex systems and its computational aspects, and
agent based computation. The main aim of this series is to discuss the above
mentioned topics from an interdisciplinary point of view and present new ideas
coming from mutual intersection of classical as well as modern methods of com-
putation. Within the scope of the series are monographs, lecture notes, selected
contributions from specialized conferences and workshops, special contribution
from international experts.
More information about this series at http://www.springer.com/series/10624

Andrew Adamatzky
• Vivien Kendon
Editors
From Astrophysics
to Unconventional
Computation
Essays Presented to Susan Stepney
on the Occasion of her 60th Birthday
123

Editors
Andrew Adamatzky
Unconventional Computing Laboratory
University of the West of England
Bristol, UK
Vivien Kendon
Joint Quantum Centre
Durham University
Durham, UK
ISSN 2194-7287
ISSN 2194-7295
(electronic)
Emergence, Complexity and Computation
ISBN 978-3-030-15791-3
ISBN 978-3-030-15792-0
(eBook)
https://doi.org/10.1007/978-3-030-15792-0
Library of Congress Control Number: 2019934524
© Springer Nature Switzerland AG 2020
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Susan Stepney
Photo by S. Barry Cooper, taken at the 8th International Conference on Unconventional
Computation (UC 2009), Azores

Preface
The unconventional computing ﬁeld is populated by experts from all ﬁelds of
science and engineering. Susan Stepney is a classical unconventional computist.
She started as an astrophysicist, developed as a programmer and ﬂourished as a
computer scientist.
In the early 1980s, while at the Institute of Astronomy, University of Cambridge,
Susan studied relativistic thermal astrophysical plasmas. She derived exact
expressions for the fully relativistic energy exchange rate, evaluated it for the cases
of electron-proton, electron-electron and proton-proton relaxation, and developed a
computer model of thermal plasmas at mildly relativistic temperatures [8, 7, 20, 30].
Susan’s 1983 paper on simple but accurate numerical ﬁts to various two-body rates
in relativistic thermal plasmas [30] remains among her three most cited
publications.
In the mid-1980s, Susan left astrophysics and academia to join the GEC–
Marconi Research Centre in Chelmsford. Her ﬁrst computer science paper was
about modelling access control of networks with different administrations linked
together [15], and she later wrote a formal model of access control in the Z formal
speciﬁcation language [31]. She moved to Logica Cambridge in 1989, where she
developed the formal speciﬁcation and correctness proofs of a high-integrity
compiler [21, 22, 36] and the Mondex electronic purse [2, 29]. Susan’s contribu-
tions to the Z language span over two decades. These include a highly cited text-
book for advanced users of Z [4]; an edited collection of various approaches for
adding object orientation to the Z language [25]; a widely applicable set of Z data
reﬁnement proof obligations allowing for non-trivial initialisation, ﬁnalisation and
input/output reﬁnement [28]; and refactoring guided by the target patterns and
sub-patterns [33].
Susan moved back into academia in 2002, joining the Department of Computer
Science, University of York, where she established a fruitful space for natural
computation and unconventional computing ideas. Susan’s contributions to
vii

unconventional computing are so diverse and numerous that it would a separate
book to describe these. Most cited and/or most close to the editors’ hearts include
artiﬁcial immune systems [35], grand challenges in unconventional computation
[26, 27], evolving quantum circuits via genetic programming [16], controlling
complex dynamics with artiﬁcial chemical networks [13, 14], simulating complex
systems [3, 32], speed limits of quantum information processing [19], cellular
automata on Penrose tilings [17], philosophy of unconventional computing [1],
evolvable molecular microprograms [5, 9, 10], computing with nuclear magnetic
resonance [18], heterotic computing [12], material computation [11, 23, 34], theory
of programming of unconventional computers [24], and reservoir computing [6].
This book commemorates Susan Stepney’s achievements in science and engi-
neering. The chapters are authored by world leaders in computer science, physics,
mathematics and engineering.
Susan’s work in formal speciﬁcation of programming languages and semantics is
echoed in Chapters “Playing with Patterns” (Fiona A. C. Polack), “Compositional
Assume-Guarantee Reasoning of Control Law Diagrams Using UTP” (Kangfeng
Ye, Simon Foster, and Jim Woodcock), “Understanding, Explaining, and Deriving
Reﬁnement” (Eerke Boiten and John Derrick), and “Sound and Relaxed
Behavioural Inheritance” (Nuno Amálio). Susan’s life-long contributions to com-
plex systems analysis, modelling and integration is reﬂected in Chapters “A Simple
Hybrid Event-B Model of an Active Control System for Earthquake Protection”
(Richard Banach and John Baugh), “Complex Systems of Knowledge Integration:
A Pragmatic Proposal for Coordinating and Enhancing Inter/Transdisciplinarity”
(Ana Teixeira de Melo and Leo Simon Dominic Caves), “Growing Smart Cities”
(Philip Garnett), “On the Emergence of Interdisciplinary Culture: The York Centre
for Complex Systems Analysis (YCCSA) and the TRANSIT Project” (Leo Simon
Dominic Caves), and “On the Simulation (and Energy Costs) of Human
Intelligence, the Singularity and Simulationism” (Alan F. T. Winﬁeld). Chapters
“Putting Natural Time into Science” (Roger White and Wolfgang Banzhaf) and
“Anti-heterotic Computing” (Viv Kendon) address topics on programmability and
unconventional computing substrates raised by Susan in her previous papers. The
ﬁeld of unconventional computing is represented in Chapters “From Parallelism to
Nonuniversality: An Unconventional Trajectory” (Selim G. Akl), “Evolving
Programs to Build Artiﬁcial Neural Networks” (Julian F. Miller, Dennis
G. Wilson and Sylvain Cussat-Blanc), “Oblique Strategies for Artiﬁcial Life”
(Simon Hickinbotham), and “On Buildings that Compute. A Proposal” (Andrew
Adamatzky et al.).
viii
Preface

The book will be a pleasure to explore for readers from all walks of life, from
undergraduate students to university professors, from mathematicians, computers
scientists and engineers to chemists and biologists.
Bristol, UK
Andrew Adamatzky
Durham, UK
Vivien Kendon
March 2019
References
1. Adamatzky, A., Akl, S., Burgin, M., Calude, C.S., Costa, J.F., Dehshibi, M.M., Gunji, Y.-P.,
Konkoli, Z., MacLennan, B., Marchal, B., et al.: East-west paths to unconventional com-
puting. Prog. Biophys. Mol. Biol. 131, 469–493 (2017)
2. Banach, R., Jeske, C., Hall, A., Stepney, S.: Atomicity failure and the retrenchment atomicity
pattern. Formal Aspects Comput. 25, 439–464 (2013)
3. Banzhaf, W., Baumgaertner, B., Beslon, G., Doursat, R., Foster, J.A., McMullin, B., De
Melo, V.V., Miconi, T., Spector, L., Stepney, S., White, R.: Deﬁning and simulating
open-ended novelty: requirements, guidelines, and challenges. Theory Biosci. 135(3), 131–
161 (2016)
4. Barden, R., Stepney, S., Cooper, D.: Z in Practice. Prentice-Hall (1994)
5. Clark, E.B., Hickinbotham, S.J., Stepney, S.: Semantic closure demonstrated by the evolution
of a universal constructor architecture in an artiﬁcial chemistry. J. R. Soc. Interface 14(130),
20161033 (2017)
6. Dale, M., Miller, J.F., Stepney, S. Trefzer, M.A.: Evolving carbon nanotube reservoir com-
puters.
In:
International
Conference
on
Unconventional
Computation
and
Natural
Computation, pp. 49–61. Springer (2016)
7. Guilbert, P.W., Fabian, A.C., Stepney, S.: Electron-ion coupling in rapidly varying sources.
Mon. Not. R. Astron. Soc. 199(1), 19–21 (1982)
8. Guilbert, P.W., Stepney, S.: Pair production, comptonization and dynamics in astrophysical
plasmas. Mon. Not. R. Astron. Soc. 212(3), 523–544 (1985)
9. Hickinbotham, S., Clark, E., Nellis, A., Stepney, S., Clarke, T., Young, P.: Maximizing the
adjacent possible in automata chemistries. Artif. Life 22(1), 49–75 (2016)
10. Hickinbotham, S., Clark, E., Stepney, S., Clarke, T., Nellis, A., Pay, M., Young, P.:
Molecular microprograms. In: European Conference on Artiﬁcial Life, pp. 297–304. Springer
(2009)
11. Horsman, C., Stepney, S., Wagner, R.C., Kendon, V.: When does a physical system compute?
Proc. R. Soc. A. 470(2169), 20140182 (2014)
12. Kendon, V., Sebald, A., Stepney, S.: Heterotic computing: past, present and future. Philos.
Trans. R. Soc. A. 373(2046), 20140225 (2015)
13. Lones, M.A., Tyrrell, A.M., Stepney, S., Caves, L.S.: Controlling complex dynamics with
artiﬁcial biochemical networks. In: European Conference on Genetic Programming, pp. 159–
170. Springer (2010)
14. Lones, M.A., Tyrrell, A.M., Stepney, S., Caves, L.S.D.: Controlling legged robots with
coupled artiﬁcial biochemical networks. In: ECAL 2011, pp. 465–472. MIT Press (2011)
15. Lord, S.P., Pope, N.H., Stepney, S.: Access management in multi-administration networks.
IEE Secure Communication Systems (1986)
16. Massey, P., Clark, J.A., Stepney, S.: Evolving quantum circuits and programs through genetic
programming. In: Genetic and Evolutionary Computation Conference, pp. 569–580. Springer
(2004)
Preface
ix

17. Owens, N., Stepney, S.: The game of life rules on penrose tilings: still life and oscillators.
In: Adamatzky, A. (ed.), Game of Life Cellular Automata, pp. 331–378. Springer (2010)
18. Roselló-Merino, M., Bechmann, M., Sebald, A., Stepney, S.: Classical computing in nuclear
magnetic resonance. Int. J. Unconv. Comput. 6, 163–195 (2010)
19. Russell, B., Stepney, S.: Zermelo navigation in the quantum brachistochrone. J. Phys. A:
Math. Theor. 48(11), 115303 (2015)
20. Stepney, S.: Two-body relaxation in relativistic thermal plasmas. Mon. Not. R. Astron. Soc.
202(2), 467–481 (1983)
21. Stepney, S.: High Integrity Compilation. Prentice Hall (1993)
22. Stepney, S.: Incremental development of a high integrity compiler: experience from an
industrial development. In: Third IEEE High-Assurance Systems Engineering Symposium
(HASE(1998)), pp. 142–149. IEEE (1998)
23. Stepney, S.: The neglected pillar of material computation. Physica D. 237(9), 1157–1164
(2008)
24. Stepney, S.: Programming unconventional computers: dynamics, development, self-reference.
Entropy 14(10), 1939–1952 (2012)
25. Stepney, S., Barden, R., Cooper, D.: Object Orientation in Z. Springer (2013)
26. Stepney, S., Braunstein, S.L., Clark, J.A., Tyrrell, A., Adamatzky, A., Smith, R.E., Addis, T.,
Johnson, C., Timmis, J., Welch, P., Milner, R., Partridge, D.: Journeys in non-classical
computation I: A grand challenge for computing research. Int. J. Parallel Emergent Distrib.
Syst. 20(1), 5–19 (2005)
27. Stepney, S., Braunstein, S.L., Clark, J.A., Tyrrell, A., Adamatzky, A., Smith, R.E., Addis, T.,
Johnson, C., Timmis, J., Welch, P., Milner, R., Partridge, D.: Journeys in non-classical
computation II: initial journeys and waypoints. Int. J. Parallel Emergent Distrib. Syst. 21(2),
97–125 (2006)
28. Stepney, S., Cooper, D., Woodcock, J.: More powerful Z data reﬁnement: pushing the state
of the art in industrial reﬁnement. In: International Conference of Z Users, pp. 284–307.
Springer (1998)
29. Stepney, S., Cooper, D., Woodcock, J.: An electronic purse: speciﬁcation, reﬁnement, and
proof. Technical monograph PRG-126. Oxford University Computing Laboratory (2000)
30. Stepney, S., Guilbert, P.W.: Numerical ﬁts to important rates in high temperature astro-
physical plasmas. Mon. Not. R. Astron. Soc. 204(4), 1269–1277 (1983)
31. Stepney, S., Lord, S.P.: Formal speciﬁcation of an access control system. Soft.: Pract. Experie.
17(9), 575–593 (1987)
32. Stepney, S., Polack, F., Alden, K., Andrews, P., Bown, J., Droop, A., Greaves, R., Read, M.,
Sampson, A., Timmis, J., Winﬁeld, A. (eds.): Engineering Simulations as Scientiﬁc
Instruments: A Pattern Language. Springer (2018)
33. Stepney, S., Polack, F., Toyn, I.: Patterns to guide practical refactoring: examples targetting
promotion in Z. In: International Conference of B and Z Users, pp. 20–39. Springer (2003)
34. Stepney, S., Rasmussen, S., Amos, M., (eds.): Computational Matter. Springer (2018)
35. Stepney, S., Smith, R.E., Timmis, J., Tyrrell, A.M., Neal, M.J., Hone, A.N.W.: Conceptual
frameworks for artiﬁcial immune systems. Int. J. Unconv. Comput. 1(3), 315–338 (2005)
36. Stepney, S., Whitley, D., Cooper, D., Grant, C.: A demonstrably correct compiler. Formal
Aspects Comput. 3(1), 58–101 (1991)
x
Preface

By Julianne Halley

Contents
Putting Natural Time into Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Roger White and Wolfgang Banzhaf
Evolving Programs to Build Artiﬁcial Neural Networks . . . . . . . . . . . . .
23
Julian F. Miller, Dennis G. Wilson and Sylvain Cussat-Blanc
Anti-heterotic Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
Viv Kendon
Visual Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
Ian T. Nabney
Playing with Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
Fiona A. C. Polack
From Parallelism to Nonuniversality: An Unconventional
Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
Selim G. Akl
A Simple Hybrid Event-B Model of an Active Control System
for Earthquake Protection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
Richard Banach and John Baugh
Understanding, Explaining, and Deriving Reﬁnement . . . . . . . . . . . . . .
195
Eerke Boiten and John Derrick
Oblique Strategies for Artiﬁcial Life . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Simon Hickinbotham
Compositional Assume-Guarantee Reasoning of Control Law
Diagrams Using UTP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
Kangfeng Ye, Simon Foster and Jim Woodcock
Sound and Relaxed Behavioural Inheritance . . . . . . . . . . . . . . . . . . . . .
255
Nuno Amálio
xiii

Growing Smart Cities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
Philip Garnett
On Buildings that Compute. A Proposal . . . . . . . . . . . . . . . . . . . . . . . .
311
Andrew Adamatzky, Konrad Szaciłowski, Zoran Konkoli, Liss C. Werner,
Dawid Przyczyna and Georgios Ch. Sirakoulis
Complex Systems of Knowledge Integration: A Pragmatic Proposal
for Coordinating and Enhancing Inter/Transdisciplinarity . . . . . . . . . .
337
Ana Teixeira de Melo and Leo Simon Dominic Caves
On the Emergence of Interdisciplinary Culture: The York Centre
for Complex Systems Analysis (YCCSA) and the TRANSIT
Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
Leo Simon Dominic Caves
On the Simulation (and Energy Costs) of Human Intelligence,
the Singularity and Simulationism . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
Alan F. T. Winﬁeld
xiv
Contents

Putting Natural Time into Science
Roger White and Wolfgang Banzhaf
Abstract This contribution argues that the notion of time used in the scientiﬁc
modeling of reality deprives time of its real nature. Difﬁculties from logic para-
doxes to mathematical incompleteness and numerical uncertainty ensue. How can
the emergence of novelty in the Universe be explained? How can the creativity of the
evolutionary process leading to ever more complex forms of life be captured in our
models of reality? These questions are deeply related to our understanding of time.
We argue here for a computational framework of modeling that seems to us the only
currently known type of modeling available in Science able to capture aspects of the
nature of time required to better model and understand real phenomena.
1
Introduction
Since its origin more than two millennia ago, epistemological thinking in the West has
been driven by a desire to ﬁnd a way to certify knowledge as certain. Mathematics and
logic seemed to offer a template for certainty, and as a consequence, modern science
as it emerged from the work of scholars like Galileo and Newton aimed as much
as possible for mathematical formalization. But by the beginning of the twentieth
century the certainty of logic and mathematics was no longer an unquestioned truth:
it had become a research question. From paradoxes in the foundations of logic and
mathematics to equations with solutions that are in a sense unknowable, certainty
had begun to seem rather uncertain. By the 1930s, as mathematicians and logicians
grappled with the problem of formalisation, they found themselves being forced to
look beyond formal systems, and to contemplate the possibility that the solution was
R. White
Department of Geography, Memorial University of Newfoundland, St. John’s,
NL A1B 3X9, Canada
e-mail: roger@mun.ca
W. Banzhaf (B)
BEACON Center for the Study of Evolution in Action and Department of Computer Science
and Engineering, Michigan State University, East Lansing, MI 48824, USA
e-mail: banzhafw@msu.edu
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_1
1

2
R. White and W. Banzhaf
of a different nature than they had imagined, that the answers lay in the realm of the
living,creativeworld—theworldofuncertainty.Althoughtheydidnotsystematically
pursue this line of thought, we believe that their intuition was essentially correct, and
it serves as our starting point.
The basic reason for the failure of the formalisation programme, we contend,
has to do with the inherent nature of logic and mathematics, the very quality that
makes these ﬁelds so attractive as the source of certainty: their timelessness. Logic
and mathematics, as formal systems, exist outside of time; hence their truths are
timeless—they are eternal and certain. But because they exclude time, they are unable
to represent one of the most fundamental characteristics of the world: its creativity,
its ability to generate novel structures, processes, and entities.
Bergson [4], by the beginning of the twentieth century, was already deeply both-
ered by the inability of mathematics and formal science to handle the creativity
that is ubiquitous in the living world, and he believed that this was due to the use
of “abstract” time—a representation of time—rather than “concrete” or real time.
In spite of his perceptive analysis of the problem, however, he saw no real solu-
tion. In effect mathematics and the hard sciences would be unable to address the
phenomenon of life because there was no way they could embody real or natural
time1 in their formal and theoretical structures. Real time could only be intuited,
and intuition fell outside the realm of hard science. This view had been anticipated
by Goethe [1, 12], who took metamorphosis as the fundamental feature of nature.
Implicitly recognising that formal systems could not encompass metamorphosis, he
proposed a scientiﬁc methodology based on the development of an intuition of the
phenomena by means of immersion in them. A recent echo of Bergson is found in
the work of Latour [10], who argues that existence involves continuous re-creation
in real (natural) time, while the representations of science short-circuit this process
of constructive transformation by enabling direct jumps to conclusions. He cleverly
refers to this time-eliminating aspect of science as “double clic”.
The solution to the problem of time, we believe, is to re-introduce real, natural
time into formal systems. Bergson did not believe there was any way of doing this,
because science was essentially tied to mathematics or other systems of abstraction
which seemed always to eliminate real time. Today, however, we can introduce real
or natural time into our formal systems by representing the systems as algorithms and
executing them on a computer, which because it operates in natural time, introduces
natural time into the algorithm. The answer, in other words, lies in computing (In
this chapter we will normally use the expression natural time rather than real time
in order to avoid confusion with the common use of the latter expression to refer to
something happening immediately, as in “a real time solution”.).
This contribution will develop the argument that various difﬁculties that arise in
logic and formal scientiﬁc modelling point to the necessity of introducing natural
time itself into our formal treatment of science, mathematics, and logic. We ﬁrst
discuss some of the difﬁculties in logic and scientiﬁc theory that arise either from
1While time as a parameter has been used in mathematical tools, this amounts to merely a repre-
sentation of time.

Putting Natural Time into Science
3
a failure to include time in the formal system, or if it is included, from the way it
is represented. We then provide examples of the explanatory power that becomes
available with the introduction of natural time into formal systems. Finally, the last
part of the contribution offers an outline of the way forward.
2
The Role of Time in Mathematics and Science
By the end of the nineteenth century the question of the certainty of mathematics
itself was being raised. This led initially to efforts to show that mathematics could be
formalised as a logical system. Problems in the form of paradox soon emerged in this
programme, and those difﬁculties in turn led to attempts to demonstrate that the pro-
gramme was at least feasible in principle. However those attempts also failed when it
was proven that mathematics must contain unprovable or undecidable propositions.
The result was that mathematics came to resemble an archipelago of certainties
surrounded by a sea of logically necessary uncertainty. Moreover, with the discov-
ery of the phenomenon of deterministic chaos emerging in the classic three body
problem, uncertainties emerged even within the islands of established mathemat-
ics. Meanwhile, in physics, while thermodynamics had long seemed to be somehow
problematic because of the nature of entropy, at least it produced deterministic laws.
During the second half of the 20th century, however, it was shown by Prigogine [15,
17] and others [13] that these laws are special cases, and that there are no laws gov-
erning most phenomena arising in such systems, because the phenomena of interest
arise when the systems are far from thermodynamic equilibrium, whereas the laws
describe the equilibrium state. At the same time, in biology, there was a growing
realisation that living systems exist in the energetic realm in which the traditional
laws of thermodynamics are of only limited use. And with the discovery that DNA
is the genetic material of life, much of biology was transformed into what might be
termed the informatics of life.
All of these developments have in common that they introduced, irrevocably and
in a radical way, uncertainty and unpredictability into mathematics and science.
They also have in common that they arose from attempts to ensure certainty and
predictability by keeping time, real time, out of the formal explanatory systems. To
a large extent, in the practice of everyday science, these developments have been
ignored. Science continues to focus on those areas where certainty seems to be
attainable. However, many of the most interesting and important problems are ones
that grow out of the uncertainties that have been uncovered over the past century:
problems like the origin and nature of life, the nature of creative processes, and the
origin of novel processes and entities. These tend to be treated discursively rather than
scientiﬁcally.However,webelievethatitisnowpossibletodevelopformaltreatments
of problems like these by including time—real, natural time rather than any formal
representation of it—in the explanatory mechanism. This will also mean recognising
that a certain degree of uncertainty and unpredictability is inherent in any scientiﬁc
treatment of these problems. But that would be a strength rather than a failure,

4
R. White and W. Banzhaf
because uncertainty and indeterminacy are inherent characteristics of these systems,
and so any explanatory mechanism that doesn’t generate an appropriate degree of
unpredictability is a misrepresentation. Creativity itself cannot be timeless, but it
relies on the timeless laws of physics and chemistry to produce new phenomena that
transcend those laws without violating them. In the next four sections we discuss in
more detail the role of time in the treatment of paradox, incompleteness, uncertainty,
and emergence in order to justify the necessity of including natural time itself rather
than formal time in our explanatory systems.
2.1
Paradox
By the end of the 19th century the desire to show that mathematics is certain had
led to Hilbert’s programme to show that mathematics could be recast as a syntactical
system, one in which all operations were strictly “mechanical” and semantics played
a minimal role. In this spirit, Frege, Russell and Whitehead reduced several branches
of mathematics to logic, thus apparently justifying the belief that the programme was
feasible. However, Russell’s work produced a paradox in set theory that ultimately
raised doubts about it. Russell’s paradox asks if the set of all sets that do not contain
themselves contains itself. Paradoxically, if it doesn’t, then it does; if it does, then it
doesn’t. Several solutions have been proposed to rid logic of this paradox, but there
is still some debate as to whether any of them is satisfactory [8].
The paradox seems to emerge because of the time-free nature of logic. If we
treat the process described in the analysis as an algorithm and execute it, then the
output is an endless oscillation. Does the set contain itself? First it doesn’t, then it
does, then it doesn’t, …. There is no contradiction. This oscillation depends on our
working in discontinuous time—in this case the time of the computer’s clock. We
can in principle make the clock speed arbitrarily fast, but if we could go to the limit
of continuous time in operating the computer we would see the paradox re-emerge
as something analogous to a quantum superposition: since the superposition endures
it doesn’t depend on time, and independent of time, the value is always both ‘does’
and ‘doesn’t’. However, if we were to observe the state of the system at a particular
instant, the superposition would collapse to a deﬁnite but arbitrary value—“does” or
“doesn’t”—just as Schrodinger’s famous cat is only deﬁnitively alive or dead when
the box is opened.
Of course a computer must work with a ﬁnite clock speed and so the paradox
cannot appear. This resolution of the paradox by casting it as an algorithm to be
executed in natural time emphasizes the difference between the existence of a set
and the process of constructing the set, a difference that echoes Prigogine’s [15, 17,
p. 320] distinction between being and becoming. In the algorithmic construction of
the set, the size of the set oscillates between n and n + 1, sizes that correspond to
“doesn’t” and “does”. Most well known paradoxes are similar to Russell’s in that
they involve self-reference. However, Yanofsky [29, pp. 24–25] has demonstrated
one that is non-self-referential; but it, too, yields an oscillation (true, false, true, false
…) if executed as an algorithm.

Putting Natural Time into Science
5
2.2
Incompleteness
Whereas Russell discovered a paradox that cast doubt on the possibility of demon-
strating that mathematics is a purely syntactical system, both Gödel and Turing came
up with proofs that even to the extent that it is syntactical, it is impossible to demon-
strate that it is. Gödel showed that even for a relatively limited deductive system there
were true statements that couldn’t be proven true within the system. In order to prove
those statements, the system would have to be enlarged in some way, for example with
additional axioms. But in this enlarged system there would again appear true state-
ments that were not provable within it. This result highlighted the degree to which
mathematics as it existed depended not only on axioms and logical deduction—that
is, syntax—but also on the products of what was called mathematical intuition—the
source of new concepts and procedures introduced to deal with new problems or
problems that were otherwise intractable. In other words, mathematics was progres-
sively extended by constructions based on these intuitions. The intuitions came with
semantic content, which Gödel’s result implicitly suggested could not be eliminated,
even in principle.
But there was another problem. Gödel, like Hilbert, thought of deduction as a
mechanical procedure; thus the idea of certainty was closely linked to the idea of
a machine operating in a completely predictable manner. Of course at the time the
idea was metaphorical; the deductions would actually be carried out by a person (the
person was invariably referred to as a computer), but for the results to be reliable,
the person would have to follow without error a precise sequence of operations that
would yield the desired result; in other words, for each deduction the person would
have to execute an algorithm. Thus the algorithm was implicitly part of the logical
apparatus used to generate mathematics. It was clear to Turing and Church that a
formal understanding of the nature of these algorithms was necessary. To that end
Turing formulated what is now known as the Turing machine (at the time, of course—
1937—it was purely conceptual), and at about the same time Church developed an
equivalent approach, the lambda calculus. An algorithm was considered legitimate
as a procedure if it could be described as a Turing machine or formulated in the
lambda calculus. Each algorithm corresponded to a particular Turing machine. A
Turing machine was required to have a ﬁnite number of states, and a proof calculated
on the machine would need to ﬁnish in a ﬁnite number of steps. By proving that it
could not in general be shown whether or not algorithms would execute in a ﬁnite
number of steps (the halting problem), Turing demonstrated, in results analogous to
Gödel’s, that some propositions were not provable. Church arrived at the same result
using his lambda calculus. The results of Gödel, Turing, and Church showing that
it is impossible to prove that a consistent mathematical system is also complete are
complementary to Russell’s discovery of the paradox in set theory, which suggests
that an attempt to make a formal system complete will introduce an inconsistency.
Turing presumably imposed the restrictions of ﬁnite machine states and ﬁnite
steps in order to ensure that Turing machines would be close analogues of the recur-
sive equations which were the standard at that time for computing proofs. These

6
R. White and W. Banzhaf
restrictions on the Turing machine were necessary conditions for proof, but as his
results showed, they did not amount to sufﬁcient conditions. In making these assump-
tions he effectively restricted the machines to producing time-free results. This was
entirely reasonable given that his goal was to formalize a procedure for producing
mathematical results which were themselves timeless. Nevertheless he found the
restrictions to be somewhat problematic, as did Church and Gödel, because they
meant that a Turing machine could not capture the generation of the new concepts
and procedures that ﬂowed from the exercise of mathematical intuition by mathe-
maticians. There was much informal discussion of this issue, including possibilities
for circumventing the limitations. Turing suggested that no single system of logic
could include all methods of proof, and so a number of systems would be required
for comprehensive results. Stating this thought in terms of Turing machines, he sug-
gested a multiple machine theory of mind—mind because he was still thinking of
the computer as a person, and the machine as the algorithm that the person would
follow mechanically. The multiple machine idea was that Turing machines would be
chained, so that different algorithms would be executed sequentially, thus overcom-
ing some of the limitations of the simple Turing machine. How would the sequence
be determined? Initially the idea was that it would be speciﬁed by the mathematician,
but subsequently other methods were proposed, ranging from stochastic choice to a
situation in which each machine would choose the subsequent machine. Eventually
Turing proposed that learning could provide the basis of choice. He observed that
mathematics advances by means of mathematicians exercising mathematical intu-
ition, which they then use to create new mathematics, a process he thought of as
learning. He then imagined a machine that could learn by experience, by means of
altering its own algorithms.
Time had been kept out of mathematics by deﬁning it as consisting only of the
achieved corpus of results and ignoring the process by which those results were
generated. Turing and Church took a ﬁrst step toward including the process by for-
malizing the treatment of the algorithms by which proofs were derived. But this did
not seem to them (or others) to be sufﬁcient, because it did not capture the deeply
creative nature of mathematics as seen in the continual introduction of new con-
cepts and procedures that established whole new areas of mathematics. We might
interpret the journey from Turing algorithm to learning as an implicit recognition of
the necessity of natural time in mathematics. On the other hand, while Turing had
formalized the proof process in terms of a machine which would have to operate in
natural time, the machine was deﬁned in such a way that the results that it produced
would be time free. Thus in postulating strategies like chained Turing machines to
represent learning, he probably assumed that the results would also be time free.
However, if mathematics is understood to include the process by which it is created,
it will have to involve natural time, even if the result of that creative process is time
free. While Turing spoke of learning, a more appropriate term might be creativity,
and creativity—the emergence of something new—necessarily involves time.

Putting Natural Time into Science
7
2.3
Uncertainty
But is mathematics, in the narrower sense of established results, really entirely time-
less? Perhaps. But there is at least one small part of it—deterministic chaos—that
seems to be trying to break free and take up residence in natural time. The phe-
nomenon was discovered by Henri Poincaré at the beginning of the twentieth cen-
tury as he attempted to solve the Newtonian three-body problem. An equation that
exhibits deterministic chaotic dynamics can, up to a point, be treated as a time-
less structure and its properties investigated using standard mathematical techniques
(some invented by Poincaré for this purpose). It has been shown, for example, that the
attractor is a fractal object, and therefore inﬁnitely complicated. As a consequence
the solution trajectory cannot be written explicitly. It can, however, be calculated
numerically—but only up to a point: since the attractor, as a fractal, is inﬁnitely
complex, we can never know exactly where the system is on it, and hence cannot
predict the future states of the system.
Take one of the simplest possible cases, the difference equation version of the
logistic function:
Xt+1 = r Xt(1 −Xt) with 0 < Xt < 1 and 0 < r < 4.
Solving the equation for X as a function of t we have
X∗= 1 −1/r
This solution is stable for r ≤3; otherwise it is oscillatory. For r > 3.57 approxi-
mately, the oscillations are inﬁnitely complex; i.e. the dynamics are chaotic. Since the
solution cannot be written explicitly, to see what it looks like we calculate successive
values of X, while recognizing that these values become increasingly approximate,
and soon become entirely arbitrary—that is, they become unpredictable even though
the equation generating them is deterministic. This can be dismissed as simply due
to the rounding errors that result from the ﬁnite precision of the computer, but it
is actually a consequence of the interaction of the rounding errors with the fractal
nature of the attractor. The rate at which the values evolve from precise to arbitrary
is described by the Lyapunov exponent. So while the system may look well deﬁned
and timeless from an analytical point of view because the attractor determines the
complete behaviour of the system, in fact the attractor is unknowable analytically,
and can only be known (and only in a very limited way) by iterating the equation,
or by other iterative techniques such as the one developed by Poincaré. The iter-
ations take place in time, and so it seems that at least some of our knowledge of
the behaviour of the equation necessarily involves natural time. Note that a physical
process characterised by chaotic dynamics must also be unpredictable in the long
run, because as the process “executes” it will be following a fractal attractor, and
the physical system that is executing the process, being ﬁnite, will be subject to
“rounding errors”, which in effect act as a stochastic perturbation. In other words,
the resolution that can be achieved by the physical system is less than that of the

8
R. White and W. Banzhaf
attractor. This is the case with the three body problem that Poincaré was working on,
and the reason that the planetary trajectories of the solar system are unpredictable
at timescales beyond a hundred million years or so. It is also, in Prigogine’s (1997)
view, the fundamental reason for the unpredictability of thermodynamic systems at
the microscopic level (Laplace’s demon cannot do the math well enough), and also a
necessary factor in macroscopic self-organization in that the unpredictability permits
symmetry breaking—Prigogine calls this order by ﬂuctuations.
With chaotic systems, then, we lose the promise that mathematics has tradition-
ally provided of a precise, God’s eye view over all time, and thus of certainty and
predictability. We are left only with calculations in natural time that give us rapidly
decreasing accuracy and hence increasing unpredictability. But from some points
of view this is not necessarily a problem. As Turing speculated in his discussion of
learning, learning requires trial and error, which would be pointless in a perfectly
predictable world, and speciﬁcally it requires an element of stochasticity. Many oth-
ers have made the same observation—that stochasticity seems to be a necessary
element in any creative process. In physical systems stochastic perturbations are the
basis of the symmetry breaking by which systems become increasingly complex and
organized. Chaotic dynamics may thus play a positive role by providing a necessary
source of stochasticity in physical, biological, and human systems.
Physics, with the major exception of thermodynamics (together with two very
speciﬁc cases in particle physics: CPT and a case of a heavy-fermion superconductor
[20]), is characterized by laws that are “time reversible” in the sense that they remain
valid if time runs backward. In other words, time can be treated as a variable, t, and
the laws remain valid when −t is substituted for t. This is referred to by some (e.g.
[22]) as spatialized time, because we can travel in both directions in it, as we can
in space. Spatialized time is a conceptualization and representation of natural time,
whereas natural time is the time in which we and the world exist, independently of
any representation of it. Spatializing time is thus a way of eliminating natural time by
substituting a model for the real thing. The physics of spatialized time is essentially
a timeless physics, since we have access to the entire corpus of physical laws in the
same sense that we have access to the entire body of timeless mathematics.
The fact that time can be treated as a variable permits the spectacularly accurate
predictions that ﬂow from physical theory: the equations can be solved to show
the state of the system as a function of time, and thus the state at any particular
time, whether past, present or future. This physics is in a deep sense deterministic.
This is true even of quantum physics, where, as Prigogine [16] points out, the wave
function evolves deterministically; uncertainty appears only when the wave function
collapses as a result of observation. The determinism of spatialized time is the basis of
Einstein’s famous remark that “for us convinced physicists, the distinction between
past, present and future is an illusion, although a persistent one” (as quoted in [16, p.
165]). His point was that time as we experience it ﬂowing inexorably and irreversibly
is an illusion; in relativistic space-time, the reality that underlies our daily illusory
existence, we have access to all times. However, Prigogine [16] points out that the
space-time of relativity is not necessarily spatialized; that is just the conventional
interpretation. In any case, because it is apparently timeless, the physics of quantum

Putting Natural Time into Science
9
theory and relativity is understood to represent our closest approximation to certain
knowledge of the world.
Thermodynamics represents a rude exception to this timelessly serene picture.
Here time has a direction, and when it is reversed the physics doesn’t work quite the
same way. In the forward direction of time, the entropy of an isolated system increases
until it reaches the maximum possible value given local constraints. In this sense the
system is predictable. But when time is reversed, so that entropy is progressively
lowered, the system becomes unpredictable, because, as Prigogine showed, when
the entropy of a system is lowered, an increasing number of possible states appears,
states that are macroscopically quite distinct but have similar entropy levels. But only
one of these can actually exist, and in general we have no certain way of knowing
which one that will be. The same phenomenon appears in reversed computation.
In other words the reversed time future is characterized by a bifurcation tree of
possibilities. Its future is open; it is no longer deterministic or fully predictable, but
rather path dependent. This discussion of reversed time futures applies to isolated
systems, the ones for which thermodynamic theory was developed. However, we do
not live in an isolated system. Our planet is bathed in solar energy, which keeps it far
from equilibrium, and we supplement this with increasing amounts of energy from
other sources. Thus our open-system world is equivalent to a reversed time, isolated-
system world. It is a world of path dependency and open futures of a self-organizing
system.
The open futures of these systems is a source of unpredictability or uncertainty,
just as is the uncertainty arising from chaotic dynamics, and the two work together.
In both of these situations in which unpredictability appears, time continues to be
treated as a variable, but in the case of far from equilibrium systems the behaviour is
time asymmetric: if we treat decreasing entropy as equivalent to time reversal, phys-
ical systems are deterministic in +t but undetermined in −t. In the case of chaotic
systems, while the process may be mathematically deterministic in both +t and −t,
the outcome is undetermined for both directions of time. In both cases, since a math-
ematical treatment of the phenomenon is of limited use, the preferred approach is
computational. This is not just a pragmatic choice. It reﬂects the poverty of spatial-
ized time compared to the possibilities offered by real time. Whereas physics, with
the major exception of thermodynamics, is based on the assumption that spatialized
time captures all the characteristics of time that are essential for a scientiﬁc under-
standing of the world, natural time involves no assumptions. It is simply itself. A
computer can only implement an algorithm step by step, in natural time. As a con-
sequence, algorithms as they are executed do not depend on any conceptualization
or representation of time beyond a working assumption that time is discontinuous or
quantized, rather than continuous, an assumption imposed by the computer’s clock.

10
R. White and W. Banzhaf
2.4
Emergence
Far from equilibrium, self-organizing systems are the ones that we live in; our planet
is essentially a spherical bundle of such systems. The dynamics of plate tectonics
is driven by energy generated by radioactive decay in the earth’s core; the complex
behaviour of the oceans and atmosphere is driven by the ﬂux of solar energy; and
life itself, including human societies, also depends on the continuous input of energy
from the sun. Self-organization is a kind of emergence—it is the process by which
an organized structure or pattern emerges from the collective dynamics of the indi-
vidual objects making up the system, whether these are molecules of nitrogen and
oxygen organizing themselves into a cyclonic storm or individual people moving
together to form an urban settlement. However, as the phrase self-organization sug-
gests, there is no prior speciﬁcation of the form that is to emerge, and because of the
inherent indeterminacy of far-from equilibrium systems, there is always a degree of
uncertainty as to exactly what form will appear, as well as where and when it will
emerge. These forms are essentially just patterns in the collection of their constituent
particles or objects. Unlike their constituent objects, they have no existence as inde-
pendent entities, and they cannot, simply as patterns, act on their environment—in
other words, they have no agency. For this reason the emergence of self-organized
systems is called soft or weak emergence.
Strong emergence, on the other hand, refers to the appearance of new objects, or
new types of objects, in the system. We can identify three levels of strong emergence:
1. In high energy physics, forces and particles emerge through symmetry breaking.
Unlike the increasing energy input required to drive self-organization, this process
occurs as free energy in the system decreases and entropy increases.
2. At relatively moderate energy levels, physical systems produce an increasing
variety of chemical compounds. These molecules have an independent existence
and distinctive properties, like a characteristic colour of solubility in water, that
are not simply the sum of the characteristics of their constituent atoms. They also
have a kind of passive agency: for example, they can interact with each other
chemically to produce new molecules with new properties, like a new colour. Of
course they can also interact physically, by means of collisions, to produce weak
emergence, for example in the form of a convection cell or a cyclonic storm.
But chemical reactions can result in the simultaneous occurrence of both strong
and weak emergence, as when reacting molecules and their products generate the
macroscopic self-organized spiral patterns of the Belosov-Zhabotinsky reaction.
The production of a particular molecule may either use or produce free energy,
i.e. it may be either entropy increasing or entropy decreasing.
3. Also at relatively moderate energy levels, living systems emerge through chem-
ical processes, but also through self-assembly of larger structures (cells, organs,
organisms). The key characteristic of this level of strong emergence is that the
process is initiated and guided by an endogenous model of the system and its
relationship with its environment. While in (1) and (2) emergence is determined
by the laws of physics, in this case it is determined by the relevant models work-

Putting Natural Time into Science
11
ing together with the laws of physics and chemistry. We include in living systems
the meta-systems of life such as ecological, social, political, technological, and
economic systems.
It is this third kind of strong emergence, the kind that depends on and is guided
by models, that is the focus of our interest. Nevertheless the weak emergence of self-
organizing systems remains important in the context of strong emergence, because a
process of strongemergence, as inthecaseof thedevelopment of afertilisedeggintoa
mature multi-cellular individual, often makes use of local self-organization. Further-
more, self-organized structures are often the precursors of individuals with agency,
making the transition by means of a process of reiﬁcation, as when a self-organized
settlement is incorporated as a city, a process that endows it with independent agency.
In general, while self-organized systems are forced to a state of lower entropy by
an exogenously determined ﬂux of energy, living systems create and maintain their
organized structures in order to proactively import energy and thus maintain a state
of lower entropy. The causal circularity is a characteristic of such systems.
Model based systems are a qualitatively new type. The models provide context
dependent rules of behaviour that supplement the effects of the laws of physics and
chemistry. Of course we can always reduce the structures that act as the models to
their basic chemical components in order to understand, for example, the chemical
structure of DNA or the chemistry of the synapses in a network of neurons, and there
are good reasons for doing this: it allows us to understand the underlying physical
mechanisms by which the model—and by extension the system of which it is a part—
functions. But this reduction to chemistry and physics tells us nothing about how or
why the system as a whole exists. These questions can only be answered at the level
of the model considered as a model, because it is the model that guides the creation
and functioning of the system of which it is a part. In other words, the reductionist
programme reveals the syntax of the system, but tells us nothing of the semantics.
It is the rules of behaviour of the system as a whole, rules provided by the model,
that determine the actions of the system in its environment, and thus, ultimately,
its success in terms of reproduction or survival. Part of the semantic content of the
model is therefore the teleonomic goal of survival. The teleonomy is the result of the
evolutionary process that produced the system. In this sense evolution is the ultimate
source of semantics: as Dobzhansky said in the famous title of his paper, “Nothing in
biology makes sense except in the light of evolution” [5]. The mathematical biologist
Rosen [18, 19] speculated that life, rather than being a special case of physics and
chemistry, in fact represents a generalization of those ﬁelds, in the sense that a
scientiﬁc explanation of life would reveal new physics and chemistry. In other words
the models inherent in living systems could be seen as new physics and chemistry:
they introduce semantics as an emergent property of physico-chemical systems.

12
R. White and W. Banzhaf
2.4.1
Models
An interesting and useful deﬁnition of life, due to Rosen [18], is that life consists of
entities that contain models of themselves, that is, entities that exist and function by
virtue of the models they contain. The most basic model is that coded in DNA. But
neural systems also contain models, some of them, as we know, very elaborate. And
of course some models are stored in external media such as books and computers.
These three loci of models correspond to the three worlds of Karl Popper: World 1 is
the world of physical existence, World 2 corresponds to mental phenomena or ideas;
and World 3 consists of the externally stored and manipulated representations of the
ideas. Worlds 2 and 3 are not generally considered by scientists to be constituents
of the world that science seeks to explain. However, as Popper points out, they are
in fact part of it, and exert causal powers on World 1 [14]. The implication is that
a scientiﬁc understanding of biological and social phenomena requires not just an
analysis at the level of physical and chemical causation, but also consideration of the
causal role of meaning, or more speciﬁcally, meaning as embodied in models. Thus
semantics re-enters the picture in a fundamental way.
A model that is a part of a living system must be a formal structure with semantics,
not just syntax. It can function as a model only by virtue of its semantic content, since
in order to be a model it must represent another system, a system of which, in the case
of living organisms, it is itself usually a part. The modelled system thus provides the
model with its semantic content. As Rosen points out, this contradicts the orthodox
position of reductionist science (and in particular of Newtonian particle physics) that
“every material behaviour [can] be …reduced to purely syntactical sequences of
conﬁgurations in an underlying system of particles” [19, p. 68; see also p. 46ff].
Non-living systems lack semantics; they might thus be characterised as identity
modelsofthemselves,orzero-ordermodels.Modelsassociatedwithlivingorganisms
(e.g. DNA or an idea of self) would then be ﬁrst order models. And some scientiﬁc
models, those that are models of models (e.g. a mathematical model of DNA), would
be second order models. This chapter is concerned with ﬁrst order models.
We propose the following deﬁnition:
a is a ﬁrst order model of A, i.e. a is a functional representation of A (a r A), if
1. a is a structure (not just a collection) of appropriate elements in some medium,
whether chemical (e.g. a DNA molecule composed of amino acids), cellular (e.g. a
synaptic structure in a network of neurons), or symbolic (e.g. a program composed
of legitimate statements in some programming language).
2. The structure a can act as an algorithm when executed on some suitable machine
M, where M may be either separate from A (e.g. a computer running a model of an
economic system), or some part of A (e.g. a bacterial cell running the behavioural
model coded in its DNA);
3. The output of the algorithm corresponds to or consists of some characteristics of
A. Speciﬁcally:

Putting Natural Time into Science
13
(a) Given a suitable environment, a running on M can create a new instance of A
(e.g. in the environment provided by a warm egg, the DNA being run by the egg
cell containing the DNA can create a new instance of the kind of organism that
produced the egg).
(b) a can guide the behaviour of A in response to certain changes in the state of
the environment (e.g. on the arrival of night, go to your nest; if inﬂation is greater
than 3 percent, raise the interest rate).
4. 3(a) and 3(b) are evolved (or in human World 3 systems, designed) capabilities
or functions that in general serve to maximise the chance of survival of A.
5. If A is a living organism, r is an emergent property of the underlying physical
and chemical systems.
First (and higher) order models are essentially predictive. Although the output of
a when executed on M is literally a response to a current condition c which represents
input to a, because of the evolutionary history of a that brought it into existence,
the behaviour of A in response to a is actually a response to a future, potentially
detrimental, condition c′ predicted by a; in other words, on the basis of the current
condition c, a predicts that c′ will occur, and as a consequence produces a response
in A intended to prevent the occurrence of c′ or mitigate its impact. Thus a acts as a
predictive algorithm, and guides the behaviour of A on the basis of its predictions.
The model a is thus rich in time. It involves both past time (the time in which it
evolved) and future time (the time of its prediction), as well as, during execution,
the natural time of the present. This reminds us of Bergson’s [4, p. 20] observation
regarding natural (“concrete”) time: “the whole of the past goes into the making of
the living being’s present moment.” Only if we know already about evolution as a
process can we see the three times present in a. Only by virtue of being such a system
ourselves do we have the ability to perceive its purpose.
In this three-time aspect, a as it represents A is fundamentally different from a
purely chemical or physical phenomenon in the conventional sense. It has semantic
content which would be eliminated by any possible reduction to the purely mechan-
ical causation of chemical and physical events. From a reductionist standpoint we
would see only chemical reactions, nothing of representation, purpose, past, or antic-
ipation. On the other hand, since a does actually have this semantic content, that con-
tent must emerge in the chemical system itself. It does so by virtue of the relationship
between the part of the system that constitutes a and the larger system that is being
modelled, just as a molecular property like solubility in water emerges from inter-
actions among the atoms making up the molecule. In this sense Rosen was correct
that life represents a radical extension of chemistry and physics: at no point do we
require a vital principle or a soul to breathe semantics, or even life, into chemistry.
In the speciﬁc case of DNA, as a molecule it is essentially ﬁxed from the point of
view of the organism: over that timescale, as a molecule, it is timeless. But natural
time appears as the organism develops following conception, when various genes
are turned on or off, and this behaviour continues in the fully developed organism
as its interactions with the environment are guided by various contingently activated
combinations of genes. In that sense DNA acts as a model that changes as a result of

14
R. White and W. Banzhaf
its interactions with the modelled system of which it is a part. This is reminiscent of
the chained Turing machines proposed by Turing to permit creativity. Neural models,
in contrast, lack a comprehensive ﬁxed structure analogous to that of DNA; they are
open ended and develop or change continually as a result of interactions with their
host organism and the environment. But in both cases, as a computational system,
life is essentially a case of open ended computation.
2.4.2
Information
The model of a system represents information, and its role in the functioning of
the system depends on its being treated as information by the system. Note that
this is information in the sense of semantics, or meaningful information, rather than
Shannon information, which is semantics-free and represents information capacity or
potential information. In other words, we could say that while semantics represents
the content or meaning of information, Shannon information represents its quantity,
and syntax represents its structure. Shannon information is maximised when a system
is in its maximum entropy state. In the case of a self-organized system, the macro-
scale pattern constrains the behaviour of the constituent particles so that the system’s
entropy and hence its Shannon information is less than it would be if its particles
were unconstrained by the self-organized structures.
We do not know of a measure of semantic information; it seems unlikely that such
a measure could even be deﬁned. Nevertheless, it seems that the model is the means
by which semantic content emerges from syntax. We speculate that it is ultimately
the teleonomic nature of living systems that populates the vacant lands of Shannon
information with the semantics of meaning-laden information. A model embedded
in a living system does not simply represent some aspect of another system; it does
so purposefully. In living systems, the function of the model is to guide the behaviour
of the system of which it is a part, and it does this by predicting future states of both
the system and its environment. System behaviour thus depends to some degree on
the anticipated future state of the system and its environment—i.e. the behaviour
is goal directed. In contrast, in the case of traditional feedback systems, behaviour
depends on the current state of the system and its environment. We note the apparent
irony that life, a system that depends for its origin and evolution on uncertainty,
nevertheless depends for its survival on an ability to predict future states. In fact, it
requires a balance of predictability and unpredictability. In Langton’s [9] terms, it
exists on the boundary between order and chaos.
2.4.3
Agency
First order models emerged with life in an evolutionary process, one in which the
model both depends on and facilitates the persistence of the system of which it is a
part.Themodelthusnecessarilyhasateleonomicquality—itspurposeisultimatelyto
enhance the likelihood of its own survival and that of the host system that implements

Putting Natural Time into Science
15
it. To this end, the model endows its host system with agency—i.e. it transforms the
system into an agent that can act independently. The relationship between model
and evolutionary process, the basis of strong emergence, seems fundamental: each
seems to be necessary for the other. This is in a sense the basic assumption of the
theory of biological evolution. In contrast, a self-organized system, the result of weak
emergence, does not act independently to ensure its own persistence. Living systems,
by virtue of their agency, act to maintain themselves in a state of low entropy.
2.5
Creative Algorithms
The models that guide the generation and behaviour of living systems are necessar-
ily self-referential, since they are models of a system of which they are an essential
part. This means that they cannot be represented purely as mathematical structures.
However, if the mathematical structures are appropriately embedded in algorithms
being executed in natural time, the problem disappears. Nevertheless, the deﬁnition
of algorithm remains crucial. Rosen, with deep roots in mathematics, was never quite
able to resolve the problems arising from self-reference because he worked with Tur-
ing’s deﬁnition of algorithm; this is clear when he claims, repeatedly, that life is not
algorithmic. But as we have noted, the Turing machine was deﬁned in such a way as to
produce only results that are consistent with time-free mathematics. To generate that
mathematics, the Turing machine must be supplemented by a source of learning or
creativity. Learning and creativity are essential characteristics of living systems, as is
the appearance of new entities with agency, which learning and creativity make pos-
sible. Consequently, a formal understanding of life must include a formal treatment
of learning, creativity and strong emergence. That requires algorithms that transcend
Turing’s deﬁnition. It requires algorithms that are able to model their own behaviour
and alter themselves on the basis of their models of themselves. Using a computer
operating in natural time to execute only Turing algorithms is like insisting on using
three dimensional space to do only two dimensional geometry: it is a colossal waste
of capacity as well as a refusal to consider the unfolding world of possibilities that
emerge in natural time.
3
Further Explorations on the Role of Time in Science
We have gotten so used to the concept of creativity and completely new solutions to
problems, or to inventions that make our life easier and are introduced the ﬁrst time,
that we tend to overlook the principle aspect of creating new things.
In the daily processes of synthetic chemistry, for example, new molecules are
generated every day by combining existing molecules into new combinations. Given
the enormous extent of the combinatorial space of chemistry, we have to presume
that some of those are created the very ﬁrst time.

16
R. White and W. Banzhaf
If some of these compounds are stable and created today in the Universe for the
ﬁrst time—note we speak of actual realization of material compounds, as opposed
to the mere possibility of their existence being “discovered”—they come with a time
stamp of today. Thus, every material substance or object has in some way attached
to it a time stamp of when it or its earlier copies ﬁrst appeared in the Universe.
Time, therefore, is of absolute importance to everything that exists and is able to
characterize it in some way.
Can we make use of that in the Sciences? Here, we want to look at the two
sciences that provide modeling tools for others to use in their effort to model the
material universe, mathematics and computer science.
3.1
Mathematics
We have already mentioned that mathematics uses the concept of time (if at all) in
a spatial sense. This means, time can be considered as part of a space that can be
traversed in all directions. Notably, it can be traversed backward in time! But mathe-
matics is actually mostly concerned about the unchanging features of the objects and
transformations it has conceptualized. Thus, it glosses over, or even ignores changes
infeatures, as theycouldprevent truthfromgettingestablished. For instance, amathe-
maticalproofisasetoftransformationsofastatementintothevalues“true”or“false”,
values that are unchanging and not dynamic. This reliability is its strength. Once a
statement is established to be true, it is accepted into the canon of mathematically
proven statements, and can serve as an intermediate for other proof transformations.
But what about a mathematics of time? How would such a mathematics look like?
We don’t know yet, perhaps because the notion of time is something many math-
ematicians look at with suspicion, and rather than asking how such a mathematics
would look like, they ask themselves whether time exists at all and how they can
prove that it does not exist—except as an illusion in our conciousness [3]. Although
Science has always worked like that—ignoring what it cannot explain and focusing
on phenomena it can model and explain—we have reached a point now where we
simply cannot ignore the nature of time any more as a concept that is key to our
modeling of natural systems.
So let’s offer another speculation here. We said before that every object in the
universe carries a property with it we can characterize as a time stamp, stating when it
ﬁrst appeared. This is one of its unalienable properties, whether we want to consider it
or not. So how about imagining that every mathematical object and all the statements
and transformations in mathematics would come with the feature of a time stamp?
In other words, besides its other properties, an object, statement or transformation
would carry a new property, the time when it was ﬁrst created. This would help sort
out some of the problems when trying to include the creation of mathematics into
mathematics itself. It wouldactuallygiveus awayof characterizinghowmathematics
is created by mathematicians. The rule would be that new objects, statements and

Putting Natural Time into Science
17
transformations can only make use of what is already in existence at the time of their
own creation.
Once we have achieved such a description, can we make a model of the pro-
cess? Perhaps one of the natural things to ask is whether it would be possible to
at least guess which objects, statements or transformations could be created next?
The situation is a reminder of the “adjacent possible” of Kaufmann who proposed
that ecological systems inhabit a state space that is constantly expanding through
accessing “adjacent” states that increase its dimensionality. What this includes is a
notion that only what interacts with the existing (which we can call “the adjacent”)
could be realized next. Everything else would be a creatio ex nihilo and likely never
be realized.
Here is an example: Suppose we have a set of differential rate equations that
describe a system at the current state. For simplicity, let’s assume that all the variables
of the system carry a time stamp of this moment. Suppose now that we want to
introduce a new variable, for another quantity that develops according to a new
differential rate equation. Would it make sense to do that even without any coupling
of this new variable to the existing system? We don’t think it would. In fact, the very
nature of our wish to introduce this variable has to do with its interaction with the
system as it is currently described. Thus, introducing a variable that can describe the
adjacent possible has at least to have some interaction with the current system.
Dynamic set theory [11] is an example of how this could work. Dynamic set
theory was inspired by the need to deal with sets of changing elements in a software
simulation. Mathematically, normal sets are static, in that membership in a set does
not change over time. But dynamic sets allow just that: Sets can be deﬁned over time
intervals T , and might contain certain elements at certain times only. For example,
if you have a set of elements
X = {a1, a2, a3, b1, b2, b3, c1, c2, c3}
we can assign speciﬁc dynamic sets to a time interval T as follows
AT = {(t1, {a1, b1, c1}), (t2, {a2, b1}), (tT , ∅)}
and
BT = {(t1, {a1, a2}), (t3, {a3, c1, c3}), (tT , ∅)}
We can then manipulate these sets using set operations, for instance:
AT ∩BT = {(t1, {a1}), (tT , ∅)}
or
AT ∪BT = {(t1, {a1, a2, b1, c1}), (t2, {a2, b1}), (t3, {a3, c1, c3}), (tT , ∅)}

18
R. White and W. Banzhaf
We can see here that each of these elements is tagged with a particular time at which
they are part of the dynamic set, and can take part in set operations for that particular
moment.2 A generalization of set theory is possible to this case. Our hope is that—
ultimately—mathematics will be able to access the constructive, intuitional aspects
of its own creation. Once we have assigned the additional property of time/age to
mathematical objects, perhaps its generative process can be modeled.
Another example of mathematical attempts at capturing time in mathematics is
real-time process algebra [25]. The idea of this approach is to try to describe for-
mally what a computational system is able to do, in particular its dynamic behavior.
This project of formalization was generalized under the heading of “denotational
mathematics” [26, 27].
Theseareallinterestingattemptstocapturetheeffectoftimewithintheframework
of Mathematics, but they fall short of the goal, because they are descriptive in nature,
i.e. they are not generative and able to create novel structures, processes or variables
themselves.
3.2
Computer Science
Computers allow the execution of mathematical models operationalized as algo-
rithms. But as we have seen from the discussion in this chapter, mathematics cur-
rently deals with spatialized time, not real, natural time. Thus, if we were to only aim
at simulating mathematical models, we do not need natural time. This is indeed Tur-
ing’s deﬁnition of an algorithm, restricted exactly in the way required to make sure
that it cannot do anything that requires natural time, so that the computer executing
a Turing algorithm is only doing what, in principle, timeless mathematics can do.
Here, instead, we aim for algorithms to execute on machines that need to go beyond
traditional mathematical models.
We need to provide operations within our algorithms that allow for modiﬁcation of
models. Let us brieﬂy consider how variables (potential observables of the behavior
of a [simulated] model) are realized in a computer: They are handled using the
address of their memory location. Thus if we allow memory address manipulations
in our algorithms, like allocating memory for new variables, or garbage collection
(for variables/memory locations that have fallen out of use), we should be able to
modify at least certain aspects of a model (the variables). Since the address space of
a computer is limited, memory locations can be described by integer numbers in a
certain range, so we are able to modify them during execution.
Of course, variables are but one class of entities that need to be modiﬁed from
within the code. Reﬂective computer languages allow precisely this type of manipu-
lation [21]. Reﬂection describes the ability of a computer language to modify its own
2Note that we have skirted the issue of how to measure time, and how to precisely determine a
particular moment and its synchronous counterparts in other regions of the Universe. For now, we’d
stick to classical time and assume a naive ability to measure it precisely.

Putting Natural Time into Science
19
structure and behavior. Mostly interpreted languages have been used for reﬂection,
yet more modern approaches like SELF offer compiling capabilities, based on an
object-oriented model. As Sobel and Friedman write: “Intuitively, reﬂective compu-
tational systems allow computations to observe and modify properties of their own
behavior, especially properties that are typically observed only from some external,
meta-level viewpoint” [23]. What seems to make SELF particularly suitable is its
ability to manipulate methods and variables in the same framework. In fact, there is
no difference in SELF between them. Object classes are not based on an abstract col-
lection of properties and their inheritance in instantiation, but on prototype objects,
object copy and variation. We believe that SELF allows an easier implementation of
an evolutionary system than other object-oriented languages.
Susan Stepney’s work [24] in the context of the CoSMoS project provides a
good discussion of the potential of reﬂective languages to allow to capture emergent
phenomena through self-modiﬁcation. In order for a self-modifying system not to
sink into a chaotic mess, though, we probably shall need again to time stamp the
generation of objects.
However, the open-ended power of those systems might only come into its own
when one of the key aspects of natural time is respected as well—the fact that one
cannot exit natural time. This calls for systems that are not terminated. Natural open-
ended processes like scientiﬁc inquiry or economic activity or biological evolution
do not allow termination and restart. While objects in those systems might have a
limited lifetime, entire systems are not “rebooted”. Instead, new objects have to be
created and integrated into the dynamics of the existing systems.
We return here to a theme already mentioned with Turing machines: The tra-
ditional idea of an algorithm, while having to make use of natural time during its
execution as a step-by-step process, attempts to ignore time by requiring the algo-
rithm to halt. Traditional algorithms are thus constructed to halt for their answer to be
considered deﬁnitive. This, in fact, makes them closed system approaches to compu-
tation, as opposed to streaming processes, that analyze data continuously and provide
transient answers at any time [6]. We might want to ask: What are the requirements
for systems that do not end, i.e. do not exit natural time? [2].
3.3
Other Sciences
In this contribution we do not have enough space to discuss in detail how natural
phenomena as encountered in simple and complex systems can inform the corre-
sponding sciences—which attempt to model those phenomena (physics, chemistry,
biology, ecology and economy)—about natural time. But we believe it is important
to emphasize that a clear distinction should be made between our modeling attempts
and the actual phenomena underlying them. In the past, there were times when the
model and the reality were conceptually not separated. For instance, the universe
was considered like clockwork, or later as a steam engine, and even later as a giant

20
R. White and W. Banzhaf
computer. All of these attempts to understand the universe mistook the underlying
system for its metaphor.
4
Conclusion
Our argument here is not that the Universe is a giant computer [7], preferably running
an irreducible computation [28]. This would interchange the actual system with the
model of it. Rather, our argument is that time is so fundamental to the Universe that
we need tools (computers) and formalisms (algorithms) that rely on natural time to be
able to faithfully model its phenomena. We believe that there are many phenomena
in the natural and artiﬁcially made world making use of novelty, innovation, emer-
gence, or creativity, which have resisted modeling attempts with current techniques.
We think those phenomena are worth the effort to change our concepts in order to
accommodate them into our world view and allow us to develop models. As hard as
it might be to do that, what would Science be without taking stock of what is out
there in the world and attempting to incorporate it in our modelling efforts?
Acknowledgements This essay was written on the occasion of the Festschrift for Susan Stepney’s
60th birthday. It is dedicated to Susan, whose work has been so inspiring and deep.
References
1. Amrine, F.: The Metamorphosis of the Scientist, vol. 5, pp. 187–212. North American Goethe
Society (1990)
2. Banzhaf, W., Baumgaertner, B., Beslon, G., Doursat, R., Foster, J., McMullin, B., de Melo, V.,
Miconi, T., Spector, L., Stepney, S., White, R.: Deﬁning and simulating open-ended novelty:
requirements, guidelines, and challenges. Biosciences 135, 131–161 (2016)
3. Barbour, J.: The End of Time: The Next Revolution in Our Understanding of the Universe.
Oxford University Press (2001)
4. Bergson, H.: Creative Evolution, vol. 231. University Press of America (1911)
5. Dobzhansky, T.: Nothing in biology makes sense except in the light of evolution. Am. Biol.
Teach. 35, 125–129 (1973)
6. Dodig-Crnkovic, G.: Signiﬁcance of models of computation, from turing model to natural
computation. Minds Mach. 21, 301–322 (2011)
7. Fredkin, E.: An introduction to digital philosophy. Int. J. Theor. Phys. 42, 189–247 (2003)
8. Irvine, A., Deutsch, H.: Russell’s paradox. In: Zalta, E. (ed.) The Stanford Encyclopedia of
Philosophy. https://plato.stanford.edu/archives/win2016/entries/russell-paradox (2016). Win-
ter
9. Langton, C.: Computation at the edge of chaos: phase transitions and emergent computation.
Phys. D 42, 12–37 (1990)
10. Latour, B.: Enquête sur les modes d’existence. Une anthropologie des Modernes, Découverte
(La) (2012)
11. Liu, S., McDermid, J.A.: Dynamic sets and their application in VDM. In: Proceedings of the
1993 ACM/SIGAPP Symposium on Applied Computing: States of the Art and Practice, pp.
187–192. ACM (1993)

Putting Natural Time into Science
21
12. Miller, E.: The Vegetative Soul. From Philosophy of Nature to Subjectivity in the Feminine.
Suny Press (2012)
13. Nicolis, G., Prigogine, I.: Self-organization in Nonequilibrium Systems: From Dissipative
Structures to Order Through Fluctuations. Wiley, New York (1977)
14. Popper, K.: The Open Universe. Rowman and Littleﬁeld, Totowa, NJ (1982)
15. Prigogine, I.: From Being to Becoming: Time and Complexity in the Physical Sciences. W.H.
Freeman and Co., New York (1981)
16. Prigogine, I.: The End of Certainty. Simon and Schuster (1997)
17. Prigogine, I., Stengers, I.: Order Out of Chaos: Man’s New Dialogue with Nature. Bantam
Books (1984)
18. Rosen, R.: Life Itself: A Comprehensive Inquiry into the Nature, Origin, and Fabrication of
Life. Columbia University Press (1991)
19. Rosen, R.: Essays on Life Itself. Columbia University Press (2000)
20. Schemm, E., Gannon, W., Wishne, C., Halperin, W., Kapitulnik, A.: Observation of broken
time-reversal symmetry in the heavy-fermion superconductor UPt3. Science 345(6193), 190–
193 (2014)
21. Smith, B.: Behavioral reﬂection in programming languages. Ph.D. thesis, Department of Elec-
trical Engineering and Computer Science, MIT (1982)
22. Smolin, L.: Time Reborn: From the Crisis in Physics to the Future of the Universe. Houghton
Mifﬂin Harcourt (2013)
23. Sobel, J.M., Friedman, D.P.: An introduction to reﬂection-oriented programming. In: Proceed-
ings of Reﬂection, vol. 96 (1996)
24. Stepney, S., Hoverd, T.: Reﬂecting on open-ended evolution. In: Proceedings of the 11th Euro-
pean Conference on Artiﬁcial Life (ECAL-2001), pp. 781–788. MIT Press, Cambridge, MA
(2011)
25. Wang, Y.: The real-time process algebra (RTPA). Ann. Softw. Eng. 14, 235–274 (2002)
26. Wang, Y.: Software science: on the general mathematical models and formal properties of
software. J. Adv. Math. Appl. 3, 130–147 (2014)
27. Wang, Y.: A denotational mathematical theory of system science: system algebra for formal
system modeling and manipulations. J. Adv. Math. Appl. 4, 132–157 (2015)
28. Wolfram, S.: A New Kind of Science. Wolfram Science Inc. (2002)
29. Yanofsky, N.S.: The Outer Limits of Reason: What Science, Mathematics, and Logic Cannot
Tell Us. MIT Press (2013)

Evolving Programs to Build Artiﬁcial
Neural Networks
Julian F. Miller, Dennis G. Wilson and Sylvain Cussat-Blanc
Abstract In general, the topology of Artiﬁcial Neural Networks (ANNs) is human-
engineered and learning is merely the process of weight adjustment. However, it is
wellknownthatthiscanleadtosub-optimalsolutions.TopologyandWeightEvolving
Artiﬁcial Neural Networks (TWEANNs) can lead to better topologies however, once
obtained they remain ﬁxed and cannot adapt to new problems. In this chapter, rather
than evolving a ﬁxed structure artiﬁcial neural network as in neuroevolution, we
evolve a pair of programs that build the network. One program runs inside neurons
and allows them to move, change, die or replicate. The other is executed inside
dendrites and allows them to change length and weight, be removed, or replicate.
The programs are represented and evolved using Cartesian Genetic Programming.
From the developed networks multiple traditional ANNs can be extracted, each of
which solves a different problem. The proposed approach has been evaluated on
multiple classiﬁcation problems.
1
Introduction
Artiﬁcial neural networks (ANNs) were ﬁrst proposed seventy-ﬁve years ago [45].
Yet, they remain poor caricatures of biological neurons. ANNs are almost always
static arrangements of artiﬁcial neurons with simple activation functions. Learning
Julian F. Miller—It is with great pleasure that I offer this article in honour of Susan Stepney’s 60th
birthday. Susan has been a very stimulating and close colleague for many years.
J. F. Miller (B)
University of York, Heslington, York YO10 5DD, UK
e-mail: julian.miller@york.ac.uk
D. G. Wilson · S. Cussat-Blanc
University of Toulouse, IRIT - CNRS - UMR5505, 21 allee de Brienne,
31015 Toulouse, France
e-mail: dennis.wilson@irit.fr
S. Cussat-Blanc
e-mail: sylvain.cussat-blanc@irit.fr
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_2
23

24
J. F. Miller et al.
is largely considered to be the process of adjusting weights to make the behaviour of
the network conform as closely as possible to a desired behaviour. We refer to this
as the “synaptic dogma”. Indeed, there is abundant evidence that biological brains
learn through many mechanisms [32] and in particular, changes in synaptic strengths
are at best a minor factor in learning since synapses are constantly pruned away and
replaced by new synapses [66]. In addition, restricting learning to weight adjustment
leads immediately to the problem of so-called catastrophic forgetting. This is where
retraining an ANN on a new problem causes the previously learned behaviour to be
disrupted if not forgotten [19, 44, 55]. One approach to overcoming catastrophic
forgetting is to create developmental ANNs which, in response to environmental
stimulus (i.e. being trained on a new problems), grow a new sub-network which
integrates with the existing network. Such new networks could even share some
neurons with pre-existing networks.
In contrast to the synaptic dogma in ANNs, in neuroscience it is well-known that
learning is strongly related to structural changes in neurons. Examples of this are
commonplace. Mice reared in the dark and then placed in the light develop new
dendrites in the visual cortex within days [78]. Animals reared in complex environ-
ments where active learning is taking place have an increased density of dendrites
and synapses [37, 38]. Songbirds in the breeding season increase the number, size
and spacing of neurons, where the latter is caused by increases in dendritic arboriza-
tion [74]. It is also well-known that the hippocampi of London taxi drivers are signif-
icantly larger relative to those of control subjects [43]. Rose even argues that after a
few hours of learning the brain is permanently altered [60]. Another aspect support-
ing the view that structural changes in the brain are strongly associated with learning,
is simply that the most signiﬁcant period of learning in animals happens in infancy,
when the brain is developing [8].
There have been various attempts to create developmental ANNs and we review
past work in Sect.2. Although many interesting past approaches have been presented,
generally the work has not continued. When one considers the enormous potential of
developmental neural networks there needs to be sustained and long-term research
on a diversity of approaches. In addition there is a need for such approaches to be
applied in a variety of real-world applications.
2
Related Work on the Development of ANNs
Although non-developmental in nature, a number of methods have been devised
which under supervision gradually augment ANNs by adding additional neurons or
join trained ANNs together via extra connections. ‘Constructive neural networks’ are
traditional ANNs which start with a small network and add neurons incrementally
while training error is reduced [13, 18]. Modular ANNs use multiple ANNs each
of which has been trained on a sub-problem and these are combined by a human
expert [64]. Both of these approaches could be seen as a form of human engineered
development. More recent approaches adjust weighted connections between trained

Evolving Programs to Build Artiﬁcial Neural Networks
25
networks on sub-problems [62, 72]. Aljundi et al. have a set of trained ANNs for
each task (experts) and use an additional ANN as a recommender as to which expert
to use for a particular data instance [1].
A number of authors have investigated ways of incorporating development to
help construct ANNs [41, 70]. Researchers have investigated a variety of genotype
representations at different levels of abstraction.
Cangelosi et al. deﬁned genotypes which were a mixture of variables, parameters,
and rules (e.g. cell type, axon length and cell division instructions) [6]. The task was
to control a simple artiﬁcial organism. Rust et al. constructed a binary genotype which
encoded developmental parameters that controlled the times at which dendrites could
branch and how the growing tips would interact with patterns of attractants placed
in an environment [61]. Balaam investigated controlling simulated agents using a
two-dimensional area with chemical gradients in which neurons were either sensors,
affectors, or processing neurons according to location [3]. The neurons were deﬁned
as standard continuous time recurrent neural networks (CTRNNS). The genotype was
effectively divided into seven chromosomes each of which read the concentrations
of the two chemicals and the cell potential. Each chromosome provided respectively
the neuron bias, time constant, energy, growth increment, growth direction, distance
to grow and new connection weight.
A variety of grammar-based developmental methods for building ANNs have
been proposed in the literature. Kitano evolved matrix re-writing rules to develop an
adjacency matrix deﬁning the structure of a neural network [36]. He used backpropa-
gation to adjust the connection weights. He applied the technique to encoder-decoder
problems of various sizes. Kitano claimed that his method produced superior results
to direct methods. However, it must be said, it was later shown in a more careful study
by Siddiqi and Lucas, that the two approaches were of equal quality [65]. Belew [4]
evolved a two-dimensional context sensitive grammar that constructed polynomial
networks (rather than ANNs) for time-series prediction. Gradient descent was used
to reﬁne weights. Genotypes were variable length with a variety of mutation opera-
tors. He found a gene-doubling mutation to be critically important. Gruau devised a
grammar-based approach called cellular encoding in which ANNs were developed
using graph grammars [22, 23]. He evaluated this approach on hexapod robot loco-
motion and pole-balancing. Kodjabachian and Meyer used a “geometry-orientated”
variant of cellular encoding to develop recurrent neural networks to control the
behaviour of simulated insects [39]. Drchal and Šnorek [10] replaced the tree-based
cellular encoding of Gruau with alternative genetic programming methods, namely
grammatical evolution (GE) [63] and gene expression programming (GEP) [16].
They also investigated the use of GE and GEP to evolve edge encoded [42] devel-
opment trees. They evaluated the developed ANNs on the two-input XOR problem.
Jung used a context-free grammar to interpret an evolved gene sequence [31]. When
decoded it generates 2D spatially modular neural networks. The approach was eval-
uated on predator-prey agents using a coevolution. Floreano and Urzelai [17] crit-
icised developmental methods that developed ﬁxed weights. They used a matrix
re-writing method inspired by Kitano, but each connection either had a ﬁxed weight
or the neuron could choose one of four synaptic adjustment rules (i.e. plasticity).

26
J. F. Miller et al.
After development stopped synaptic adjustment rules were applied to all neuron
connections. They applied their method to robot controllers and found that synaptic
plasticity produced ANNs with better performance than ﬁxed weight networks. In
addition they were more robust in changed environments.
Jacobi presented a low-level approach in which cells used artiﬁcial genetic regu-
latory networks (GRNs). The GRN produced and consumed simulated proteins that
deﬁned various cell actions (protein diffusion movement, differentiation, division,
threshold). After a cellular network had developed it was interpreted as a neural net-
work [30]. Eggenberger also used an evolved GRN [12]. A neural network phenotype
was obtained by comparing simulated chemicals in pairs of neurons to determine if
the neurons are connected and whether the connection is excitatory or inhibitory.
Weights of connections were initially randomly assigned and Hebbian learning used
to adjust them subsequently. Astor and Adami devised a developmental ANN model
known as Norgev (Neuronal Organism Evolution) which encoded a form of GRN
together with an artiﬁcial chemistry (AC), in which cells were predeﬁned to exist
in a hexagonal grid. Genes encoded conditions involving concentrations of simu-
lated chemicals which determine the level of activation of cellular actions (e.g. grow
axon or dendrite, increase or decrease weight, produce chemical) [2]. They evalu-
ated the approach on a simple artiﬁcial organism. In later work, Hampton and Adami
showed that neurons could be removed and the developmental programs would grow
new neurons and recover the original functionality of the network (it computed the
NAND function) despite having a non-deterministic developmental process [24].
Yerushalmi and Teicher [80] presented an evolutionary cellular development model
of spiking neurons. Inside the cells was a bio-plausible genetic regulatory network
which controls neurons and their dendrites and axons in 2D space. In two separate
experiments the GRN in cells was evolved to produce: (a) speciﬁc synaptic plasticity
types (Hebbian, AntiHebbian, Non-Hebbian and Spike-Time Dependent Plasticity)
and (b) simple organisms that had to mate. In the latter, they examined the types of
synaptic plasticity that arose.
Federici used a simple recursive neural network as a developmental cellular pro-
gram [14]. In his model, cells could change type, replicate, release chemicals or
die. The type and metabolic concentrations of simulated chemicals in a cell were
used to specify the internal dynamics and synaptic properties of its corresponding
neuron. The weight of a connection between cells was determined by the difference
in external chemical concentrations produced by the two cells. The position of the
cell within the organism is used to produce the topological properties of a neuron:
its connections to inputs, outputs and other neurons. From the cellular phenotype,
Federici interpreted a network of spiking neurons to control a Khepera robot.
Roggenetal.devisedahighlysimpliﬁedmodelofdevelopmentthatwastargetedat
electronic hardware [59]. Circuits were developed in two phases. Diffusers are placed
in a cellular grid and diffuse signals (analogous to chemicals) to local neighbours.
There can be multiple signal types and they do not interact with each other. At the
same time as the diffusion phase an expression phase determines the function of
the cells by matching signal intensities with a signal-function expression table. A
genetic algorithms is used to evolve the positions of diffusing cells (which are given

Evolving Programs to Build Artiﬁcial Neural Networks
27
a maximum signal) and the expression table. By interpreting the functionalities as
various kinds of spiking neurons with particular dendritic branches, the authors were
able develop spiking neural networks. Connection weights were ﬁxed constants with
a sign depending on whether the pre-synaptic neuron is excitatory or inhibitory. They
evaluated the approach on character recognition and robot control.
Some researchers have studied the potential of Lindenmayer systems for devel-
oping artiﬁcial neural networks. Boers and Kuiper adapted L-systems to develop
artiﬁcial feed-forward neural networks [5]. They found that this method produced
more modular neural networks that performed better than networks with a predeﬁned
structure. They showed that their method could produce ANNs for solving problems
such as the XOR function. Hornby and Pollack evolved L-systems to construct com-
plex robot morphologies and neural controllers [26, 27].
Downing developed a higher-level, neuroscience-informed approach which
avoided having to handle axonal and dendritic growth, and maintained important
aspects of cell signaling, competition and cooperation of neural topologies [9]. He
adopted ideas from Deacon’s Displacement Theory [7] which is built on Edelman’s
Darwinistic view of neurogenesis, known as “The Theory of Neural Group Selec-
tion” [11]. In the latter, neurons will only reach maturity if they grow axons to, and
receive axons from, other neurons. Downing’s method has three phases. The ﬁrst
(translation) decodes the genotype which deﬁnes one or more neuron groups. In the
second phase (displacement) the sizes and connectivity of neuron groups undergo
modiﬁcation. In the ﬁnal stage (instantiation) populations of neurons and their con-
nections are established. He applied this technique to the control of a multi-limbed
starﬁsh-like animat.
Khan and Miller created a complex developmental neural network model that
evolved seven programs each representing various aspects of idealised biological
neurons [33]. The programs were represented and evolved using Cartesian Genetic
Programming (CGP) [50]. The programs were divided into two categories. Three
of the encoded chromosomes were responsible for ‘electrical’ processing of the
‘potentials’. These were the dendrite, soma and axo-synapse chromosomes. One
chromosome was devoted to updating the weights of dendrites and axo-synapses.
The remaining three chromosomes were responsible for updating the neural variables
for the soma (health and weight), dendrites (health, weight and length) and axo-
synapse (health, length). The evolved developmental programs were responsible for
the removal or replication of neural components. The model was used in various
applications: intelligent agent behaviour (wumpus world), checkers playing, and
maze navigation [34, 35].
Although not strictly developmental, Koutník et al. [40] investigated evolving
a compression of the ANN weight matrix by mapping it to a real-valued vector of
Fourier coefﬁcients in the frequency domain. This idea reduces the dimensionality of
the weight space by ignoring high-frequency coefﬁcients, as in lossy image compres-
sion. They evaluated the merits of the approach on ANNs solving pole-balancing,
ball throwing and octopus arm control. They showed that approach found solutions
in signiﬁcantly fewer evaluations than evolving weights directly.

28
J. F. Miller et al.
Stanley introduced the idea of using evolutionary algorithms to build neural net-
works constructively (called NEAT). The network is initialised as a simple structure,
with no hidden neurons consisting of a feed-forward network of input and output
neurons. An evolutionary algorithm controls the gradual complexiﬁcation of the net-
work by adding a neuron along an existing connection, or by adding a new connection
between previously unconnected neurons [67]. However, using random processes to
produce more complex networks is potentially very slow. It also lacks biological
plausibility since natural evolution does not operate on aspects of the brain directly.
Later Stanley introduced an interesting and popular extension to the NEAT approach
called HyperNEAT [69] which uses an evolved generative encoding called a Com-
positional Pattern Producing Network (CPPN) [68]. The CPPN takes coordinates
of pairs of neurons and outputs a number which is interpreted as the weight of that
connection. The advantage this brings is that ANNs can be evolved with complex
patterns where collections of neurons have similar behaviour depending on their spa-
tial location. It also means that one evolved function (the CPPN) can determine the
strengths of connections of many neurons. It is a form of non-temporal development,
where geometrical relationships are translated into weights.
Developmental Symbolic Encoding (DSE) [71] combines concepts from two ear-
lier developmental encodings, Gruau’s cellular encoding and L-systems. Like Hyper-
NEAT it can specify connectivity of neurons via evolved geometric patterns. It was
shown to outperform HyperNEAT on a shape recognition problem deﬁned over small
pixel arrays. It could also produce partly general solutions to a series of even-parity
problems of various sizes. Huizinga et al. added an additional output to the CPP
program in HyperNEAT that controlled whether or not a connection between a pair
of neurons was expressed or not [28]. They showed that the new approach produced
more modular solutions and superior performance to HyperNEAT on three specially
devised modular problems.
Evolvable-substrate HyperNEAT (ES-HyperNEAT) implicitly deﬁned the posi-
tions of the neurons [56], however it proved to be computationally expensive. Iterated
ES-HyperNEAT proposed a more efﬁcient way to discover suitable positioning of
neurons [58]. This idea was taken further leading to Adaptive HyperNEAT which
demonstrated that not only could patterns of weights be evolved but also patterns
of local neural learning rules [57]. Like [28] in Adaptive HyperNEAT Risi et al.
increased the number of outputs from the CPPN program to encode learning rate and
other neural parameters.
3
Abstracting Aspects of Biological Brains
Once we accept that developmental ANNs are desirable, it becomes necessary to
abstract and simplify important mechanisms from neuroscience. Which aspects of
biological neurons are most relevant depends on the nature of the abstracted neuron
model. For instance, if the abstracted developmental neural programs include genetic
regulatory networks one could consider epigenetic processes inside neurons since

Evolving Programs to Build Artiﬁcial Neural Networks
29
recent evidence from neuroscience suggests that these may be important in neuron
response to past environmental conditions [29, 75]. Here we compare and contrast
the proposed abstract model with various aspects of biological neurons in Sect.3.
Brain development: All multicellular organisms begin as a single cell which
undergoes development. Some of the cells become neural stem cells which gradu-
ally differentiate into mature neurons. In the proposed model we allow the user to
choose how many non-output neurons to start with prior to development. However,
the model assumes that there is a dedicated output neuron corresponding to each
output in the suite of computational problems being solved. Further discussion on
the topic of how to handle outputs can be found in Sect.13.
Arrangement of neurons: The overall architecture of both ANNs and many neu-
ral developmental systems is ﬁxed once developed, whereas biological neurons move
themselves (in early development) and their branches change over time. Thus the
morphology of the network is time dependent and can change during problem solv-
ing. In our model, we evolve a developmental process which means that the network
of neurons and branches are time dependent.
Neuron structure: Biological neurons have dendrites and axons with branches.
Each neuron has a single axon with a variable number of axon branches. In addi-
tion, it has a number of dendrites and dendrite branches. There are many types of
neurons with different morphologies, some with few dendrites and others with huge
numbers of highly branched dendritic trees. In addition neurons have a membrane,
a nucleus and a cytoplasm. Since our model of the neuron has zero volume, these
aspects are also not included. In our model, the user can choose the minimum and
the maximum number of dendrites neurons can have. The evolved developmental
programs determine how many dendrites individual neurons can have and indeed,
different neurons can have different numbers of dendrites. We have not modelled
the axon in our approach. We decided this to keep the proposed model as simple as
possible.
Neuron volume: Neurons have many physical properties (volume, temperature,
pressure, elasticity…). We have not modelled any of these and like conventional
ANNs the neurons in our model are mathematical points. Dendrites are equally
unphysical and are merely lines that emanate from neurons and are positioned on the
left of the neuron position. They can pass through each other.
Neuron movement: In brains undifferentiated neurons migrate to speciﬁc loca-
tions in the early stages of development. When they reach their destinations they
either develop into mature neurons complete with dendrites and an axon, or they
undergo cell death [73]. Moreover, this ability of neurons to migrate to their appropri-
ate positions in the developing brain is critical to brain architecture and function [46,
54]. We have allowed neuron movement in our model. Neuron movement in real
brains is closely related to the physicality of neurons and this could have important

30
J. F. Miller et al.
consequences. Clearly, mature biological neurons that are entangled with many other
neurons will have restricted movement.
Synapses: The connections between biological neurons are called synapses. Sig-
nal propagation is via the release of neurotransmitters from the pre-synaptic neuron
to the post-synaptic. Like traditional ANNs, we have no notion of neurotransmitters
and signals are propagated as if by wires. However, unlike traditional ANNs, den-
drites connect with their nearest neuron (on the left). These connections can change
when dendrites grow or shrink and when neurons move. Thus, the number of con-
nections between neurons is time-dependent.
Activity Dependent Morphology: There are few proposed models in which
changes in levels of activity (in potentials or signals) between neurons leads to
changes in neural morphology. This is an extremely important aspect of real brains
[53]. This has not been included in the current model. Possibly a measure of signals
could be used as an input to the developmental programs. We return to this issue in
Sect.13.
Neuron State: Biological neurons have dendritic trees which change over time.
New dendrites can emerge and dendrite branches can develop or be pruned away.
Indeed, we discussed in the introduction how important this aspect is for learning. We
have abstracted this process by allowing neurons to have multiple dendrites which
can replicate (an abstraction of branching) or be removed (an abstraction of prun-
ing). In the model, this process is dependent on a variable called ‘health’ which is
an abstraction of neuron and dendrite state. Khan et al. [35] ﬁrst suggested the use
of this variable.
4
The Neuron Model
The model we propose is new and conceptually simple. Two evolved neural programs
are required to construct neural networks. One represents the neuron soma and the
other the dendrite. The role of the soma program is to allow neurons to move, change,
die or replicate. For the dendrite, the program needs to be able to grow and change
dendrites, cause them to be removed and also to replicate.
The approach is simple in two ways. Firstly, because only two evolved programs
are required to build an entire neural network. Secondly, because a snapshot of the
neural network at a particular time would show merely a conventional graphs of
neurons, weighted connections and standard activation functions. To construct such
a developmental model of an artiﬁcial neural network we need neural programs
that not only apply a weighted sum of inputs to an activation function to determine
the output from the neuron, but a program that can adjust weights, create or prune
connections, and create or delete neurons.

Evolving Programs to Build Artiﬁcial Neural Networks
31
Since developmental programs build networks that change over time it is neces-
sary to deﬁne new problem classes that are suitable for evaluating such approaches.
We argue that trying to solve multiple computational problems (potentially even of
different types) is an appropriate class of problems.
The pair of evolved programs can be used to build a network from which multiple
conventional ANNs can be extracted each of which can solve a different classiﬁca-
tion problem. We investigate many parameters and algorithmic variants and assess
experimentally which aspects are most associated with good performance. Although
we have concentrated in this paper on classiﬁcation problems, our approach is quite
general and it could be applied to a much wider variety of problems.
(a)
(b)
Fig. 1 The model of a developmental neuron. Each neuron has a position, health and bias and
a variable number of dendrites. Each dendrite has a position, health and weight. The behaviour
of a neuron soma is governed by a single evolved program. In addition each dendrite is governed
by another single evolved program. The soma program decides the values of new soma variables
position, health and bias based on previous values, the average over all dendrites belonging to the
neuron of dendrite health, position and weight and an external input called problem type. The latter
is a ﬂoating point value that indicates the neuron type. The dendrite program updates dendrite
health, position and weight based on previous values, the health, position and bias of the neuron the
dendrite belongs to, and the problem type. When the evolved programs are executed, neurons can
change, die replicate and grow more dendrites and their dendrites can also change or be removed

32
J. F. Miller et al.
The model is illustrated in Fig.1. The neural programs are represented using
Cartesian Genetic Programming (CGP) (see Sect.5). The programs are actually sets
of mathematical equations that read variables associated with neurons and dendrites
to output updates of those variables. This approach was inspired by some aspects
of a developmental method for evolving graphs and circuits proposed by Miller and
Thomson [51] and is also strongly inﬂuenced by some of the ideas described in [35].
In the proposed model, weights are determined from a program that is a function
of neuron position, together with the health, weight and length of dendrites. It is
neuro-centric and temporal in nature.
As shown in Fig.1 the inputs to the soma program are: the health, bias and position
of the neuron and the average health, length and weight of all dendrites connected
to the neuron and problem type. The problem type is a constant (in range [−1, 1])
which indicates whether a neuron is not an output or in the case of an output neuron
what computational problem the output neuron belongs to. Let Pt denote the compu-
tational problem. Deﬁne Pt = 0 to denote a non-output neuron, and Pt = 1, 2 or Np to
respectively denote output neurons belonging to different computational problems,
where, Np denotes the number of computational problems. We deﬁne the problem
type input to be given by −1 + 2Pt/Np. For example, if the neuron is not an output
neuron the problem type input is −1.0. If it is an output neuron belonging to the last
problem its value is 1.0. For all other computational problems its value is a value
greater than −1.0 and less than 1.0. The thinking behind the problem type input is
that since output neurons are dedicated to a particular computational problem, they
should be given information that relates to this, so that the identical neural programs
can behave differently according to the computational problem they are associated
with. Later experiments were conducted to investigate the utility of problem type
(see Sect.7).
Bias refers to an input to the neuron activation function which is added to the
weighted sum of inputs (i.e. it is unweighted). The soma program updates its own
health,biasandpositionbasedontheseinputs.Theseareindicatedbyprimedsymbols
in Fig.1). The user can decide between three different ways of using the program
outputs to update the neural variables. The update method is decided by a user
deﬁned parameter called Incropt (see Sect.4.5) which deﬁnes how neuron variables
are adjusted by the evolved programs (using user-deﬁned incremental constants or
otherwise).
Every dendrite belonging to each neuron is controlled by an evolved dendrite
program. As shown in Fig.1 the inputs to this program are the health, weight and
position of the dendrite and also the health, bias and position of the parent neuron. In
addition as mentioned earlier, dendrite programs can also receive the problem type
of the parent neuron The evolved dendrite program decides how the health, weight
and position of the dendrite are to be updated.
Inthemodel,alltheneuronanddendriteparameters(weights,bias,health,position
and problem type) are deﬁned by numbers in the range [−1, 1].

Evolving Programs to Build Artiﬁcial Neural Networks
33
(a)
(b)
(c)
0
0
1
A
B
A
B
0
1
2
A
B
Fig. 2 Example showing a developing ﬁctitious example brain. The squares on the left represent the
inputs. The solid circles indicate non-output neurons. Non-output neurons have solid dendrites. The
dotted circles represent output neurons. Output neuron’s dendrites are also dotted. In this example,
we assume that only output neurons are allowed to move. The neurons, inputs and dendrites are
all bound to the interval [−1, 1]. Dendrites connect to nearest neurons or inputs on the left of their
position (snapping). a Shows the initial state of the example brain. b Shows the example brain after
one developmental step and c shows it after two developmental steps
4.1
Fictitious Example of Early Brain Development
A ﬁctitious developmental example is shown in Fig.2. The initial state of the example
brain is represented in (a). Initially there is one non-output neuron with a single
dendrite. The curved nature of the dendrites is purely for visualisation. In reality the
dendrites are horizontal lines emanating from the centre of neurons and of various
lengths. When extracting ANNs the dendrites are assumed to connect to their nearest
neuron on the left (referred to as ‘snapping’). Output neurons are only allowed to
connect to non-output neurons or the ﬁrst input (by default, if their dendrites lie on
the left of the leftmost non-output neuron). Thus the ANN that can be extracted from
the initial example brain, has three neurons. The non-output neuron is connected to
the second input and both output neurons are connected via their single dendrite to
the non-output neuron.
Figure2b shows the example brain after a single developmental step. In this step,
thesomaprogramanddendriteprogramsareexecutedineachneuron.Thenon-output
neuron (labeled 0) has replicated to produce another non-output neuron (labeled 1) it
has also grown a new dendrite. Its dendrites connect to both inputs. The newly created
non-output neuron is identical to its parent except that its position is a user-deﬁned
amount, MNinc, to the right of the parent and its health is set to 1 (an assumption
of the model). Both its dendrites connect to the second input. It is assumed that the
soma programs running in the two output neurons A and B have resulted in both

34
J. F. Miller et al.
output neurons having moved to the right. Their dendrites have also grown in length.
Neuron A’s ﬁrst dendrite is now connected to neuron one. In addition, neuron A has
high health so that it has grown a new dendrite. Every time a new dendrite grows it
is given a weight and health equal to 1.0. Also its new dendrite is given a position
equal to half the parent neuron’s position. These are assumptions of the model. Thus
its new dendrite is connected to neuron zero. Neuron B’s only dendrite is connected
to neuron one.
Figure2c shows the example brain after a two developmental steps. The dendrites
of neuron zero have changed little and it is still connected in the same way as the
previous step. The dendrites of neuron one have both changed. The ﬁrst one has
become longer but remains connected to the ﬁrst input. The second dendrite has
become shorter but it still snaps to the second input. Neuron one has also replicated
as a result of its health being above the replication threshold. It gets dendrites identical
to its parent, its position is again incremented to the right of its parent and its health
is set to 1.0. Its ﬁrst dendrite connects to input one and its second dendrite to neuron
zero. Output neuron A has gained a dendrite, due to its health being above the dendrite
birth threshold. The new dendrite stretches to a position equal to half of its parent
neuron. So it connects to neuron zero. The other two dendrites remain the same and
they connect to neuron one and zero respectively. Finally, output neuron B’s only
dendrite has extended a little but still snaps to neuron one. Note, that at this stage
neuron two is not connected to by another neuron and is redundant. It will be stripped
out of the ANN that is extracted from the example brain.
4.2
Model Parameters
The model necessarily has a large number of user-deﬁned parameters these are shown
in Table1.
The total number of neurons allowed in the network is bounded between a user-
deﬁned lower (upper) bound NNmin (NNmax). The number of dendrites each neuron
canhaveisboundedbyuser-deﬁnedlower(upper)boundsdenotedbyDNmin (DNmax).
These parameters ensure that the number of neurons and connections per neuron
remain in well-deﬁned bounds, so that a network can not eliminate itself or grow
too large. The initial number of neurons is deﬁned by Ninit and the initial number of
dendrites per neuron is given by NDinit.
If the health of a neuron falls below (exceeds) a user-deﬁned threshold, NHdeath
(NHbirth) the neuron will be deleted (replicated). Likewise, dendrites are subject
to user deﬁned health thresholds, DHdeath (DHbirth) which determine whether the
dendrite will be deleted or a new one will be created. Actually, to determine dendrite
birth the parent neuron health is compared with DHbirth rather than dendrite health.
This choice was made to prevent the potential very rapid growth of dendrite numbers.
When the soma or dendrite programs are run the outputs are used to decide how to
adjust the neural and dendrite variables. The amount of the adjustments are decided
by the six user-deﬁned δ parameters.

Evolving Programs to Build Artiﬁcial Neural Networks
35
Table 1 Table of neural model constants and their meanings
Symbol
Meaning
NNmin(NNmax)
Min. (Max.) allowed number of neurons
Ninit
Initial number of non-output neurons
DNmin(DNmax)
Min. (Max.) number of dendrites per neuron
NDinit
Initial number of dendrites per neuron
NHdeath(NHbirth)
Neuron health thresholds for death (birth)
DHdeath(DHbirth)
Dendrite health thresholds for death (birth)
δsh
Soma health increment (pre, while)
δsp
Soma position increment (pre, while)
δsb
Soma bias increment (pre, while)
δdh
Dendrite health increment (pre, while)
δdp
Dendrite position increment (pre, while)
δdw
Dendrite weight increment (pre, while)
NDSpre
Number of developmental steps before epoch
NDSwhi
Number of ‘while’ developmental steps during epoch
Nep
Number of learning epochs
MNinc
Move neuron increment if collision
Iu
Max. program input position
Ol
Min. program output position
α
Sigmoid/Hyperbolic tangent exponent constant
The number of developmental steps in the two developmental phases (‘pre’ learn-
ing and ‘while’ learning) are deﬁned by the parameters, NDSpre and NDSwhi. The
number of learning epochs is deﬁned by Nep. Note that the pre-learning phase of
development, ‘pre’, can have different incremental constants (i.e. δs) to the learning
epoch phase, ‘while’.
In some cases, neurons will collide with other neurons (by occupying a small
interval around an existing neuron) and the neuron has to be moved by a certain
increment until no more collisions take place. This increment is given by MNinc.
The places where external inputs are provided is predetermined uniformly within
the region between −1 and Iu. The parameter Iu deﬁnes the upper bound of their posi-
tion. Also output neurons are initially uniformly distributed between the parameter
Ol and 1. However, depending on a user-deﬁned option the output neurons as with
other neurons can move according to the neuron program. All neurons are marked
as to whether they provide an external output or not. In the initial network there are
Ninit non-output neurons and No output neurons, where No denotes the number of
outputs required by the computational problem being solved.
Finally, the neural activation function (hyperbolic tangent) and the sigmoid func-
tion (which is used in nonlinear incremental adjustment of neural variables) have a
slope constant given by α.

36
J. F. Miller et al.
4.3
Developing the Brain and Evaluating the Fitness
An overview of the algorithm used for training and developing the ANNs is given in
Overview 1.
Overview 1 Overview of ﬁtness algorithm
1: function Fitness
2:
Initialise brain
3:
Load ‘pre’ development parameters
4:
Update brain NDSpre times by running soma and dendrite programs
5:
Load ‘while’ developmental parameters
6:
repeat
7:
Update brain NDSwhi times by running soma and dendrite programs
8:
Extract ANN for each benchmark problem
9:
Apply training inputs and calculate accuracy for each problem
10:
Fitness is the normalised average accuracy over problems
11:
If ﬁtness reduces terminate learning loop and return previous ﬁtness
12:
until Nep epochs complete
13:
return ﬁtness
14: end function
The brain is always initialised with at least as many neurons as the maximum
number of outputs over all computational problems. Note, all problem outputs are
represented by a unique neuron dedicated to the particular output. However, the
maximum and initial number of non-output neurons can be chosen by the user. Non-
output neurons can grow change or give birth to new dendrites. Output neurons can
change but not die or replicate as the number of output neurons is ﬁxed by the choice
of computational problems. The detailed algorithm for training and developing the
ANN is given in Algorithm 1.
Development of the brain can happen in two phases, ‘pre’ and ‘while’. The ‘pre’
phase runs for NDSpre developmental steps and is outside the learning loop. This
is an early phase of development. It has its own set of developmental parameters.
The ‘while’ phase happens inside the learning loop which has Nep epochs. It too
has its own set of developmental parameters. The idea of two phases is inspired
by the phases of biological development. In early brain development, neurons are
stem cells that move to particular locations and have no dendrites and axons. This
can effectively be mimicked since in the ‘pre’ phase the parameters controlling
dendrites can be disabled by setting DHdeath = −1.0 and DHbirth = 1.0. This means
that dendrites can not be removed or be born. In addition, setting δdh = 0.0, δdp = 0.0
and δdw = 0.0 would mean that any existing dendrites could not change. In a similar
way, ‘while’ parameters could be chosen to disallow somas to move, die, replicate or
change during the learning loop and also to allow dendrites to grow/shrink, change,
be removed, or replicate. Thus it can be seen that the collection of parameters gives
the user a lot of control of the developmental process.

Evolving Programs to Build Artiﬁcial Neural Networks
37
The learning loop evaluates the brain by extracting conventional ANNs for each
problem and calculating a ﬁtness (based on accuracy of classiﬁcation) it checks to
see if the new ﬁtness value is greater than or equal to the previous value at the last
epoch. If the ﬁtness has reduced the learning loop terminates and the previous value
of ﬁtness is returned. The purpose of the learning loop is to enable the evolution of a
development process that progressively improves the brain’s performance. The aim is
to ﬁnd programs for the soma and dendrite which allow this improvement to continue
with epoch beyond the limit chosen (Nep). Later in this chapter, an experiment is
conducted to test whether this general learning behaviour has been achieved (see
Sect.12).
4.4
Updating the Brain
Updating the brain is the process of running the soma and dendrite programs once in
all neurons and dendrites (i.e. it is a single developmental step). Doing this will cause
the brain to change and after all changes have been carried out a new updated brain
will be produced. This replaces the previous brain. Overview Algorithm 2 gives a
high-level overview of the update brain process.
Overview 2 Update brain overview
1: function UpdateBrain
2:
Run soma program in non-output neurons to update soma
3:
Ensure neuron does not collide with neuron in updated brain
4:
Run dendrite program in all non-output neurons
5:
If neuron survives add it to updated brain
6:
If neuron replicates ensure new neuron does not collide
7:
Add new neuron to updated brain
8:
Run soma program in output neurons to update soma
9:
Ensure neuron does not collide
10:
Run dendrite program in all output neurons
11:
If neuron survives add it to updated brain
12:
Replace old brain with updated brain
13: end function
Section15.1 presents a more detailed version of how the brain is updated at each
developmental step (see Algorithm 2) and gives details of the neuron collision avoid-
ance algorithm.

38
J. F. Miller et al.
4.5
Running and Updating the Soma
The UpdateBrain program calls the RunSoma program to determine how the soma
changes in each developmental step. As we saw in Fig.1a the seven soma program
inputs are: neuron health, position and bias, the averaged position, weight and health
of the neuron’s dendrites and the problem type. Once the evolved CGP soma program
is run the soma outputs are returned to the brain update program. These steps are
shown in Overview 2.
Overview 2 Running the soma: algorithm overview
1: function RunSoma
2:
Calculate average dendrite health, position and weight
3:
Gather soma program inputs
4:
Run soma program
5:
Return updated soma heath, bias and position
6: end function
The detailed version of the RunSoma function can be found in Sect.15.3. The
RunSoma function uses the soma program outputs to adjust the health, position and
bias of the soma according to three user-chosen options deﬁned by a variable Incropt.
This is carried out by the UpdateNeuron Overview Algorithm 3.
Overview 3 Update neuron algorithm overview
1: function UpdateNeuron
2:
Assign original neuron variables to parent variables
3:
Assign outputs of soma program to health, position and bias
4:
Depending on Incropt get increments
5:
If soma program outputs > 0 (≤0) then incr. (decr.) parent variables
6:
Assign parent variables to neuron
7:
Bound health, position and bias
8: end function
4.6
Updating the Dendrites and Building the New Neuron
This section is concerned with running the evolved dendrite programs. In every
dendrite, the inputs to the dendrite program have to be gathered. The dendrite pro-
gram is executed and the outputs are used to update the dendrite. This is carried out
by a function called RunDendrite. Note, in RunAllDendrites we build the com-
pletely updated neuron from the updated soma and dendrite variables. The simpliﬁed
algorithm for doing this is shown in Overview Algorithm 4. The more detailed version
is available in Sect.15.5.

Evolving Programs to Build Artiﬁcial Neural Networks
39
Overview 4 An overview of the RunAllDendrites algorithm which runs all den-
drite programs and uses all updated variables to build a new neuron.
1: function RunAllDendrites
2:
Write updated soma variables to new neuron
3:
if Old soma health > DHbirth then
4:
Generate a dendrite for new neuron
5:
end if
6:
for all Dendrites do
7:
Gather dendrite program inputs
8:
Run dendrite program to get updated dendrite variables
9:
Run dendrite to get updated dendrite
10:
if Updated dendrite is alive then
11:
Add updated dendrite to new neuron
12:
if Maximum number of dendrites reached then
13:
Stop processing any more dendrites
14:
end if
15:
end if
16:
end for
17:
if All dendrites have died then
18:
Give new neuron the ﬁrst dendrite of the old neuron
19:
end if
20: end function
Overview Algorithm 4 (in line 9) uses the updated dendrite variables obtained
from running the evolved dendrite program to adjust the dendrite variables (accord-
ing to the incrementation option chosen). This function is shown in the Overview
Algorithm 5. The more detailed version is available in Sect.15.5.
The RunDendrite function begins by assigning the dendrite’s health, position
and weight to the parent dendrite variables. It writes the dendrite program outputs
to the internal variables health, weight and position. It respectively carries out the
increments or decrements of the parent dendrite variables according whether the
corresponding dendrite program outputs are greater than or less than or equal to zero.
After this it bounds those variables. Finally, it updates the dendrites health, weight
and position provided the adjusted health is above the dendrite death threshold.
We saw in the ﬁtness function that we extract conventional ANNs from the evolved
brain. The way this is accomplished is as follows.
Since we share inputs across problems we set the number of inputs to be the
maximum number of inputs that occur in the computational problem suite. If any
problem has less inputs the extra inputs are set to zero.
The next phase is to go through all dendrites of the neurons to determine which
inputs or neurons they connect to. To generate a valid neural network we assume that
dendrites are automatically connected to the nearest neuron or input on the left. We
refer to this as snapping. The dendrites of non-output neurons are allowed to connect
to either inputs or other non-output neurons on their left. However, output neurons
are only allowed to connect to non-output neurons on their left. It is not desirable for

40
J. F. Miller et al.
Overview 5 Change dendrites according to the evolved dendrite program.
1: function RunDendrite
2:
Assign original dendrite variables to parent variables
3:
Assign outputs of dendrite program to health, position and weight
4:
Depending on Incropt get increments
5:
If dendrite program outputs > 0 (≤0) then incr(decr.) parent variables
6:
Assign parent variables to neuron
7:
Bound health, position and weight
8:
if (health > DHdeath) then
9:
Update dendrite variables
10:
Dendrite is alive
11:
else
12:
Dendrite is dead
13:
end if
14:
Return updated dendrite variables and whether dendrite is alive
15: end function
the dendrites of output neurons to be connected directly to inputs, however, when
output neurons are allowed to move, they may only have inputs on their left. In this
case the output neuron’s dendrite will be connected to the ﬁrst external input to the
ANN network (by default).
The detailed version of the ANN extraction process is given in Sect.15.6.
5
Cartesian GP
The two neural programs are represented and evolved using a form of Genetic Pro-
gramming (GP) known as Cartesian Genetic Programming (CGP). CGP [48, 50] is
a form of GP in which computational structures are represented as directed, often
acyclic graphs indexed by their Cartesian coordinates. Each node may take its inputs
from any previous node or program input (although recurrent graphs can also be
implemented see [77]). The program outputs are taken from the output of any inter-
nal node or program input. In practice, many of the nodes described by the CGP
chromosome are not involved in the chain of connections from program input to
program output. Thus, they do not contribute to the ﬁnal operation of the encoded
program, these inactive, or “junk”, nodes have been shown to greatly aid the evolu-
tionary search [49, 79, 81]. The representational feature of inactive genes in CGP is
also closely related to the fact that it does not suffer from bloat [47].
Ingeneral,thenodesdescribedbyCGPchromosomesarearrangedinarectangular
r × c grid of nodes, where r and c respectively denote the user-deﬁned number
of rows and columns. In CGP, nodes in the same column are not allowed to be
connected together. CGP also has a connectivity parameter l called “levels-back”
which determines whether a node in a particular column can connect to a node in a
previous column. For instanceif l = 1all nodes inacolumncanonlyconnect tonodes

Evolving Programs to Build Artiﬁcial Neural Networks
41
in the previous column. Note that levels-back only restricts the connectivity of nodes;
it does not restrict whether nodes can be connected to program inputs (terminals).
However, it is quite common to adopt a linear CGP conﬁguration in which r = 1
and l = c. This was done in our investigations here. CGP chromosomes can describe
multiple input multiple output (MIMO) programs with a range of node functions
and arities. For a detailed description of CGP, including its current developments
and applications, see [48]. Both the soma and dendrite program have 7 inputs and
3 outputs. (see Fig.1). The function set chosen for this study are deﬁned over the
real-valued interval [−1.0, 1.0]. Each primitive function takes up to three inputs,
denoted z0, z1 and z2. The functions are deﬁned in Table2.
Table 2 Node function gene values, mnemonic and function deﬁnition
Value
mnemonic
Deﬁnition
0
abs
|z0|
1
sqrt
√|z0|
2
sqr
z02
3
cube
z03
4
exp
(2exp(z0 + 1) −e2 −1)/(e2 −1)
5
sin
sin(z0)
6
cos
cos(z0)
7
tanh
tanh(z0)
7
inv
−z0
9
step
if z0 < 0.0 then 0 else 1.0
10
hyp

(z02 + z12)/2
11
add
(z0 + z1)/2
12
sub
(z0 −z1)/2
13
mult
z0z1
14
max
if z0 >= z1 then z0 else z1
15
min
if z0 <= z1 then z0 else z1
16
and
if (z0 > 0.0 and z1 > 0.0) then 1.0 else −1.0
17
or
if (z0 > 0.0 or z1 > 0.0) then 1.0 else −1.0
18
rmux
if z2 > 0.0 then z0 else z1
19
imult
−z0z1
20
xor
if (z0 > 0.0 and z1 > 0.0) then −1.0
else if (z0 < 0.0 and z1 < 0.0) then −1.0
else 1.0
21
istep
if z0 < 1.0 then 0 else −1.0
22
tand
if (z0 > 0.0 and z1 > 0.0) then 1.0
else if (z0 < 0.0 and z1 < 0.0) then −1.0
else 0.0
23
tor
if (z0 > 0.0 or z1 > 0.0) then 1.0
else if (z0 < 0.0 or z1 < 0.0) then −1.0
else 0.0

42
J. F. Miller et al.
6
Benchmark Problems
In this study, we evolve neural programs that build ANNs for solving three standard
classiﬁcation problems. The problems are cancer, diabetes and glass. The deﬁnitions
of these problems are available in the well-known UCI repository of machine learning
problems.1 These three problems were chosen because they are well-studied and
also have similar numbers of inputs and a small number of classes. Cancer has
9 real attributes and two Boolean classes. Diabetes has 8 real attributes and two
Boolean classes. Glass has 9 real attributes and six Boolean classes. The speciﬁc
datasets chosen were cancer1.dt, diabetes1.dt and glass1.dt which are described in
the PROBEN suite of problems.2
7
Experiments
The long-term aim of this research is to explore effective ways to develop ANNs.
The work presented here is just a beginning and there are many aspects that need to
be investigated in the future (see Sect.13). The speciﬁc research questions we have
focused on in this chapter are:
• What types of neuron activation function is most effective?
• How many neurons and dendrites should we allow?
• Should neuron and dendrite programs be allowed to read problem type?
These questions complement the questions asked and investigated using the same
neural model in recent previous work [52]. There, the utility of neuron movement in
both non-output and output neurons was investigated. It was found that statistically
signiﬁcantly better results were obtained when only output-neurons were allowed to
move. In addition, the work examined three ways of incrementing or decrementing
neural variables. In the ﬁrst the outputs of evolved programs determines directly the
new values of neural variables (position, health, bias, weight), that is to say there
is no incremental adjustment of neural variables. In the second, the variables are
incremented or decremented in user-deﬁned amounts (the deltas in Table1). In the
third, the adjustments to the neural variables are nonlinear (they are adjusted using
a sigmoid function). Linear adjustment of variables (increment or decrement) was
found to be statistically superior to the alternatives.
To answer the questions above, a series of experiments were carried out to inves-
tigate the impact of various aspects of the neural model on classiﬁcation accuracy.
Twenty evolutionary runs of 20,000 generations of a 1+5-ES were used. Genotype
lengths for soma and dendrite programs were chosen to be 800 nodes. Goldman
mutation [20, 21] was used which carries out random point mutation until an active
1https://archive.ics.uci.edu/ml/datasets.html.
2https://publikationen.bibliothek.kit.edu.

Evolving Programs to Build Artiﬁcial Neural Networks
43
gene is changed. For these experiments a subset of allowed node functions were cho-
sen as they appeared to give better performance. These were: step, add, sub, mult,
xor, istep. The remaining experimental parameters are shown in Table3.
Some of the parameter values are very precise (deﬁned to the fourth decimal
place). The process of discovering these values consisted of an informal but greedy
search for good parameters that produced high ﬁtness in the ﬁrst evolutionary run.
It was fortuitous as it turned out that this was a reasonable way of obtaining good
parameters on average.
Table 3 Table of neural
model parameters
Parameter
Value
NNmin(NNmax)
0 (20–100)
Ninit
5
DNmin(DNmax)
1 (5–50)
NDinit
5
NDSpre
4
NDSwhi
8
Nep
1
MNinc
0.03
Iu
−0.6
Ol
0.8
α
1.5
‘Pre’ development parameters
NHdeath(NHbirth)
−0.405 (0.406)
DHdeath(DHbirth)
−0.39 (−0.197)
δsh
0.1
δsp
0.1487
δsb
0.07
δdh
0.1
δdp
0.1
δdw
0.101
‘While’ development parameters
NHdeath(NHbirth)
0.435 (0.7656)
DHdeath(DHbirth)
0.348 (0.41)
δsh
0.009968
δsp
0.01969
δsb
0.01048
δdh
0.0107
δdp
0.0097
δdw
0.0097

44
J. F. Miller et al.
Table 4 Training and testing accuracy for three neural activation functions
Acc.
Hyperbolic tangent
Rectilinear
Sigmoid
Train (Test)
Train (Test)
Train (Test)
Mean
0.7401 (0.7075)
0.7150 (0.6698)
0.6980 (0.6760)
Median
0.7392 (0.7266)
0.7101 (0.6717)
0.7036 (0.6851)
Maximum
0.7988 (0.7669)
0.7654 (0.7398)
0.7315 (0.7237)
Minimum
0.6840 (0.6200)
0.6815 (0.6217)
0.6251 (0.5894)
Table 5 Training and testing accuracy on individual problems when using tanh activation
Acc.
Cancer
Diabetes
Glass
Train (Test)
Train (Test)
Train (Test)
Mean
0.9086 (0.9215)
0.7233 (0.6594)
0.5883 (0.5415)
Median
0.9129 (0.9281)
0.7266 (0.6615)
0.5841 (0.5849)
Maximum
0.9657 (0.9770)
0.7630 (0.6823)
0.6729 (0.6981)
Minimum
0.8571 (0.8621)
0.6693 (0.6198)
0.4673 (0.3396)
Table6 Comparisonoftestaccuraciesonthreeclassiﬁcationproblems.Modelusingtanhactivation
compared with huge suite of classiﬁcation methods as described in [15]
Acc.
Cancer
Diabetes
Glass
ML (model)
ML (model)
ML (model)
Mean
0.935 (0.9215)
0.743 (0.6594)
0.610 (0.5415)
Maximum
0.974 (0.9770)
0.790 (0.6822)
0.785 (0.6981)
Minimum
0.655 (0.8621)
0.582 (0.6198)
0.319 (0.3340)
8
Results
The mean, median, maximum and minimum accuracies achieved over 20 evolu-
tionary runs for three different neuron activation functions neurons are shown in
Table4. We can see that the best values of mean, median, maximum and minimum
are all obtained when the hyperbolic tangent function is used. The rectilinear neu-
ral functions is zero for negative arguments and equal to its argument for positive
arguments. Both the sigmoid and hyperbolic tangent activation functions have α as
exponent multipliers. The training and testing accuracies on individual classiﬁcation
problems using the hyberbolic tangent activation functions are shown in Table5.
Table6 shows how the results for the model (using tanh activation) compare
with the performance of 179 classiﬁers (covering 17 families) [15].3 The ﬁgures
are given just to show that the results for the developmental ANNs are respectable.
3The paper gives a link to the detailed performance of the 179 classiﬁers which contain the ﬁgures
given in the table.

Evolving Programs to Build Artiﬁcial Neural Networks
45
Table 7 Training and testing accuracy for different upper bounds on the number of neurons
(NNmax). The number of dendrites was 50 in all cases
Acc.
NNmax = 20
NNmax = 40
NNmax = 60
NNmax = 80
NNmax = 100
Train (Test)
Train (Test)
Train (Test)
Train (Test)
Train (Test)
Mean
0.7314 (0.7026)
0.7401 (0.7075)
0.7161 (0.6927)
0.7179 (0.6969)
0.7222 (0.7072)
Median
0.7327 (0.7176)
0.7392 (0.7266)
0.7155 (0.6919)
0.7252 (0.7045)
0.7219 (0.7081)
Maximum 0.7605 (0.7468)
0.7988 (0.7669)
0.7355 (0.7461)
0.7757 (0.7483)
0.7493 (0.7480)
Minimum
0.7002 (0.6563)
0.6840 (0.6200)
0.6654 (0.6408)
0.6284 (0.5998)
0.6691 (0.6492)
Table 8 Training and testing accuracy for different upper bounds on the number of dendrites
(DNmax). The upper bound on the number of neurons was 40 in all cases
Acc.
DNmax = 5
DNmax = 10
DNmax = 15
DNmax = 50
Train (Test)
Train (Test)
Train (Test)
Train (Test)
Mean
0.7408 (0.7125)
0.7347 (0.7053)
0.7338 (0.6966)
0.7401 (0.7075)
Median
0.7463 (0.7208)
0.7335 (0.7152)
0.7325 (0.6857)
0.7392 (0.7266)
Maximum
0.7926 (0.7701)
0.7924 (0.7640)
0.7988 (0.7669)
0.7988 (0.7669)
Minimum
0.6789 (0.6047)
0.6850 (0.6395)
0.6797 (0.6281)
0.6840 (0.6200)
The results are particularly encouraging considering that the evolved developmental
programs build classiﬁers for three different classiﬁcation problems simultaneously,
so the comparison is unfairly biased against the proposed model. The cancer results
producedbythemodelareveryclosetothosecompiledfromthesuiteofMLmethods,
however it can be seen that the models’s results for diabetes and glass are not as close.
It is unclear why this is the case.
The second series of experiments examined how the classiﬁer performance varied
with the upper bound on the number of allowed neurons (Table7).
The third series of experiments was concerned with varying the upper bound on
the number of dendrites allowed for each neuron (DNmax). It was found that the
results when the dendrite upper bound is 20 produced exactly the same results as
an upper bound of 50. All dendrites must snap to a neuron or input and therefore
contribute to the output of the network. The fact that increasing the upper bound made
no difference implies that the number of dendrites the neurons used never exceeded
20 anyway (Table8).
Statistical signiﬁcance testing with the test data revealed that the better scenarios
(NNmax = 20, 40 or DNmax = 5, 50) are only weakly statistically different from other
scenarios. So it appears that performance is not particularly sensitive to the maximum
number of neurons and dendrites (within reasonable bounds).
Thefourthseriesofexperimentsconcernedtheissueofproblemtype.Asdiscussed
in Sect.4, problem type is a real-valued quantity in the interval [−1, 1]. It is a
quantity that indicates what computational problem a neuron belongs to. Non-output
neurons are not committed to a problem type so the problem type is assumed to be
−1.0. However, output neurons are all dedicated to a particular problem. The cancer
problem has two output neurons, the diabetes has two and the glass problem has

46
J. F. Miller et al.
Table 9 Training and testing
accuracy with and without
problem type inputs to
evolved programs. The upper
bounds on the number of
neurons and dendrites was 40
and 50 respectively
Acc.
Without problem type
With problem type
Train (Test)
Train (Test)
Mean
0.7314 (0.7026)
0.7401 (0.7075)
Median
0.7327 (0.7176)
0.7392 (0.7266)
Maximum
0.7605 (0.7468)
0.7988 (0.7669)
Minimum
0.7002 (0.6563)
0.6840 (0.6200)
six. The extraction process that gets the ANNs associated with each problem begins
from the output neurons corresponding to each problem. So the question arises as to
whether it is useful or not to allow the neural programs to read the problem type?
The results are shown in Table9.
Using the problem type as an input to neural programs appears to be useful as
it improves the mean, median and maximum compared with not using the problem
type. However, tests of statistical signiﬁcance discussed in the next section show that
the difference is not signiﬁcant.
9
Comparisons and Statistical Signiﬁcance
The Wilcoxon Ranked-Sum test (WRS) was used to assess the statistical difference
between pairs of experiments. In this test, the null hypothesis is that the results (best
accuracy) over the multiple runs for the two different experimental conditions are
drawn from the same distribution and have the same median. If there is a statistically
signiﬁcant difference between the two then null hypothesis is false with a degree of
certainty which depends on the smallness of a calculated statistic called a p-value.
However, in the WRS before interpreting the p-value one needs to calculate another
statistic called Wilcoxon’s W value. This value needs to be compared with calculated
values which depend on the number of samples in each experiment. Results are
statistically signiﬁcant when the calculated W-value is less than or equal to certain
critical values for W [82]. The critical values depend on the sample sizes and the p-
value. We used a publicly available Excel spreadsheet for doing these calculations.4
The critical W-values can be calculated in two ways: one-tailed or two-tailed. The
two-tailed test is appropriate here as we are interested in whether one experiment is
better than another (and vice versa).
For example, in Table10 the calculated W-value is 34 and the critical W-value
for for the paired sample sizes of 20 (number of runs) with p-value less than 0.01 is
38 (assuming a two-tailed test).5 The p-value gives a measure of the certainty with
which the null hypothesis can be accepted. Thus the lower the value the more likely
4http://www.biostathandbook.com/wilcoxonsignedrank.html.
5http://www.real-statistics.com/statistics-tables/wilcoxon-signed-ranks-table/.

Evolving Programs to Build Artiﬁcial Neural Networks
47
Table 10 Statistical comparison of testing results from experiments (Wilcoxon Rank-Sum two-
tailed)
Question
Expt. A
Expt. B
W
W critical
P-value
Signiﬁcant?
Activation
tanh
Rectilinear
34
38
0.005 < p < 0.01
Yes
Activation
Rectilinear
Sigmoid
90
69
0.2 < p
No
Problem type input
Yes
No
92
69
0.2 < p
No
that the two samples come from different distributions (i.e. are statistically different).
Thus in this case, the probability that the null hypothesis can be rejected is 0.99.
The conclusions we can draw from Table10 are that a hyperbolic tangent activa-
tion function is statistically signiﬁcantly superior to either a rectilinear or sigmoid
function. In addition, the use of problem type as an evolved program input is not
statistically distinguishable from not using a problem type input. This is surprising
as output neurons are dedicated to problem types and one might expect by a problem
type input would allow neural programs to behave differently according to problem
type.
10
Evolved Developmental Programs
The average number of active nodes in the soma and dendrite programs for the
hyperbolic tangent activation function is 54.7 in both cases. Thus the programs are
relatively simple. It is also possible that the graphs can be logically reduced to
even simpler forms. The graphs of the active nodes in the CGP graphs for the best
evolutionary run (0) are shown in Figs.3 and 4. The red input connections between
Fig. 3 Best evolved soma program. The input nodes are: soma heath (sh), soma bias (sb), soma
position(sp),averagedendritehealth(adh),averagedendriteweight(adw),averagedendriteposition
(adp) and problem type (pt). The output nodes are: soma health (SH), soma bias (SB) and soma
position (SP)

48
J. F. Miller et al.
Fig. 4 Best evolved dendrite program. The input nodes are: soma heath (sh), soma bias (sb), soma
position (sp), dendrite health (dh), dendrite weight (dw), dendrite position (dp) and problem type
(pt). The output nodes are: dendrite health (DH), dendrite weight (DW) and dendrite position (DP)
nodes indicate the ﬁrst input in the subtraction operation. This is the only node
operation where node input order is important.
11
Developed ANNs for Each Classiﬁcation Problem
The ANNs for the best evolutionary run (0) were extracted (using Algorithm 9) and
can be seen in Figs.5 and 6. The plots ignore connections with weight equal to zero.
In the ﬁgures, a colour scheme is adopted to show which neurons belong to which
problems. Red indicates a neuron belonging to the ANN predicting cancer, green
indicates a neuron belonging to the ANN predicting diabetes and blue indicates a
neuron belonging to an ANN predicting the type of glass. If neurons are shared
the colours are a mixture of the primary colours. So white, or an absence of colour,

Evolving Programs to Build Artiﬁcial Neural Networks
49
Fig. 5 Developed ANN for cancer dataset. This dataset has 9 attributes and two outputs. The
numbers inside the circles are the neuron bias. The numbers in larger font near but outside the
neurons are neuron IDs. The connection weights between neurons are shown near to the connections.
If any attributes are not present it means they are unused. The training accuracy is 0.9657 and the
test accuracy is 0.9770
indicates that a neuron is shared over all three problems. Magenta indicates a neurons
shared between cancer and glass. Yellow would indicate neurons shared between

50
J. F. Miller et al.
Fig. 6 Developed ANN for diabetes (a) and glass (b) datasets. The diabetes dataset has 8 attributes
and two outputs. The glass dataset has 9 attributes and six outputs. The numbers inside the circles
are the neuron bias. The numbers in larger font near but outside the neurons are neuron IDs. The
connection weights between neurons are shown near to the connections. Attributes not present are
unused. The diabetes training accuracy is 0.7578 and the test accuracy is 0.6823. The glass training
accuracy is 0.6729 and the test accuracy is 0.6415
cancer and diabetes (in the case shown there are no neurons shared between cancer
and diabetes).
The ANNs use 21 neurons in total of which only four non-output neurons were
unshared (output neurons cannot be shared). It is interesting to note that the number
of neurons used for the diabetes data set was 6 (two of which were output neurons)
even though the diabetes classiﬁcation problem is more difﬁcult than cancer. Also
the brain was allowed to use up to 40 neurons but many of these were redundant and
not used in the extracted ANNs. These redundant neurons were not referenced in the
chain of connections from output neurons to inputs. This redundancy is very much
like the redundancy that occurs in CGP when extracting active computational nodes.
It is interesting that for the cancer prediction ANN, class 0 is decided by a very
small network in which only two attributes are involved (the 5 and 6th attributes).
This network is independent of the network determining class 1. It is also interesting
that the neurons with IDs 17 and 18 are actually identical. Note that attributes 0, 1
and 4 have been ignored. There are three identical neurons connected to attribute 2
(with IDs 9, 10 and 11), these can be reduced to a single neuron!

Evolving Programs to Build Artiﬁcial Neural Networks
51
It is surprising that the diabetes ANN is so small. Indeed the neurons with IDs 18
and 19 can be replaced with a single neuron as the two neurons are identical. The
ANN only reads three attributes of the dataset! In the case of the glass ANN, once
again we see that the ANN consists of distinct networks (3), one predicting classes
2 and 5, one predicting classes 0, 1 and one predicting classes 3 and 4.
When we analyzed ANNs produced in other cases we sometimes observed ANNs
in which neurons occur that only have inputs with weight zero (i.e. effectively no
inputs). Such neurons can still be involved in prediction as provided the bias is
non-zero the neuron will output a constant value.
Another interesting feature is that pairs of neurons often have multiple connec-
tions. This is equivalent to a single connection where the weighted value is the sum
of the individual connections weights. This phenomenon was also observed in CGP
encoded and evolved ANNs [76].
12
Evolving Neural Learning Programs
The ﬁtness function (see Overview Algorithm 1) included the possibility of learning
epochs. In this section we present and discuss results when a number of learning
epochs have been chosen. The task for evolution is then to construct two neural
programs that develop ANNs that improve with each learning epoch. The aim is
to ﬁnd a general learning algorithm in which the ANNs change and improve with
each learning epoch beyond the limited number of epochs used in training. The
‘while’ experimental parameters required to investigate were changed from those
used previously when there were no learning epochs. The new parameters are shown
in Table11.
Twenty evolutionary runs were carried out using these parameters and the results
are shown in Table12. Once again, we see that parameter values are very precise.
They were obtained using the same greedy search discussed earlier.
In Table12 shows the results with multiple learning epochs versus those with no
learning epochs. Table13 shows the results on each problem using multiple learning
epochs. It is clear that using no learning epochs gives better results. However, the
resultswithmultiplelearningepochsarestillreasonabledespitethefactthatthetaskis
much more difﬁcult, since one is effectively trying to evolve a learning algorithm. It is
possible that further experimentation with developmental parameters could produce
better results with multiple epochs.
In Fig.7 we examine how the accuracy of the classiﬁcations varies with learning
epochs (run 1 of 20). We set the maximum number of epochs to 100 now to see if
learning continues beyond the upper limit used during evolution (10). We can see
that classiﬁcation accuracy increases with each epoch up to 10 epochs and starts to
gradually decline after that. However, at epoch 29 the accuracy suddenly drops to
0.544 and at epoch 39 the accuracy increases again to 0.647. After this the accu-
racy shows a slow decline. We obtained several evolved solutions in which training

52
J. F. Miller et al.
Table 11 Table of neural
model parameters
Parameter
Value
NNmin(NNmax)
0 (40)
Ninit
5
DNmin(DNmax)
1 (50)
NDinit
5
NDSpre
4
NDSwhi
1
Nep
10
MNinc
0.03
Iu
−0.6
Ol
0.8
α
1.5
‘Pre’ development parameters
NHdeath(NHbirth)
−0.405 (0.406)
DHdeath(DHbirth)
−0.39 (−0.197)
δsh
0.1
δsp
0.1487
δsb
0.07
δdh
0.1
δdp
0.1
δdw
0.101
‘While’ development parameters
NHdeath(NHbirth)
−0.8 (0.8)
DHdeath(DHbirth)
−0.7 (0.7)
δsh
0.0011
δsp
0.0011
δsb
0.0011
δdh
0.0011
δdp
0.0011
δdw
0.0011
Table 12 Test accuracy for
ten learning epochs versus no
learning epochs
Acc.
Learning epochs
No learning epochs
Mean
0.6974
0.7075
Median
0.6985
0.7266
Maximum
0.7355
0.7669
Minimum
0.6685
0.6200

Evolving Programs to Build Artiﬁcial Neural Networks
53
Table 13 Test accuracies
over problems using ten
learning epochs
Acc.
Cancer
Diabetes
Glass
Mean
0.8799
0.6250
0.3774
Median
0.9425
0.6354
0.3962
Maximum
0.9885
0.7031
0.5660
Minimum
0.6092
0.4740
0.0000
Fig. 7 Variation of test classiﬁcation accuracy with learning epoch for run 1 of 20
accuracy increased at each epoch until the imposed maximum number of epochs,
however, as yet none of these were able to improve beyond the limit.
13
Open Questions
There are many issues and questions that remain to be investigated.
Firstly, it is unclear why better results can not at present be obtained when evolv-
ing developmental programs with multiple epochs. Neither is it clear why programs
can be evolved that continuously improve the developed ANNs over a number of
epochs (i.e. 10) yet do not improve subsequently. It is worth contrasting the model
discussed in this chapter with previous work on Self-Modifying CGP (SMCGP) [25].
In SMCGP phenotypes can be iterated to produce a sequence of programs or phe-
notypes. In some cases genotypes were found that produced general solutions and
always improved at each iteration. The ﬁtness was accumulated over all the correct
test cases summed over all the iterations. In the problems studied (i.e. even-n par-
ity, producing π) there was also a notion of perfection. For instance in the parity
case perfection meant that at each iteration it produced the next parity case (with
more inputs) perfectly. If at the next iteration, the appropriate parity function was
not produced, then the iteration stopped. In the work discussed here, the ﬁtness is
not cumulative. At each epoch, the ﬁtness is the average accuracy of the classiﬁers

54
J. F. Miller et al.
over the three classiﬁcation problems. If the ﬁtness reduces at the next epoch, then
the epoch loop is terminated. However, in principle, we could sum the accuracies at
each epoch and if an accuracy at a particular epoch is reduced, terminate the epoch
loop. Summing the accuracies would give reward to developmental programs that
produced the best history of developmental changes.
At present, the developmental programs do not receive a reward signal during
multiple epochs. This means that the task for evolution is to continuously improve
developed ANNs without being supplied with a reward signal. However, one would
expect that as the ﬁtness increases at each epoch the number of changes that need to
be made to the developed ANNs should decrease. This suggests that supplying the
ﬁtness at the previous epoch to the developmental programs might be useful. In fact
this option has already been implemented but as yet evidence is inconclusive that
this produces improved results.
While learning over multiple epochs, we have assumed that the developmental
parameters should be ﬁxed (i.e. they are chosen before the development loop—see
line 5 of Overview Algorithm 1). However, it is not clear that this should be so.
One could argue that during early learning topological changes in the brain network
are more important and weight changes more important in later phases of learning.
This suggests that at each step of the learning loop one could load developmental
parameters, this would allow control of each epoch of learning. However, this has
the drawback of increasing the number of possible parameter settings.
The neural variables that are given as inputs to the CGP developmental programs
are an assumption of the model. For the soma these are: health, bias, position, problem
type and average dendrite health, position and weight. For the dendrite they are:
dendrite health, weight, position, problem type and soma health, bias and position.
Further experimental work needs to be undertaken to determine whether they all are
useful. The program written already has the inclusion of any of these variables as an
option.
There are also many assumptions made in quite small aspects of the whole model.
When new neurons or dendrites are born what should the initial values of the neural
variables be?What arethebest upper bounds onthenumber of neurons anddendrites?
Currently, dendrite replication is decided by comparing the parent neuron health
with DHbirth rather than comparing dendrite health with this threshold. If dendrite
health was compared with a threshold it could rapidly lead to very large numbers of
dendrites. Many choices have been made that need to be investigated in more detail.
In real biological brains, early in the developmental process neurons move (and have
no or few dendrites) and later neurons do not move and have many or at least a number
of dendrites. This is understandable as moving when you have dendrites is difﬁcult
(if not impossible) as they provided resistance and would get obstructed by the
dendrites of other neurons. However, in the proposed model movement can happen
irrespective of such matters. It would be possible to restrict movement of whole
neurons by making the movement increments depend on the number of dendrites a
neuron has.

Evolving Programs to Build Artiﬁcial Neural Networks
55
There are also very many parameters in the model and experiment has shown
that results can be very sensitive to some of these. Thus further experimentation is
required to identify good choices for these parameters.
A fundamental issue is how to handle inputs and outputs. In the classiﬁcation
problems the number of inputs is given by the problem with the most attributes,
problems with less are given the value zero for those inputs. This could be awkward
if the problems have hugely varying numbers of inputs. Is there another way of
handling this? Perhaps one could borrow more ideas from SMCGP and make all
input connections access inputs using pointer to a circular register of inputs. Every
time a neuron connected to an input, a global pointer to the register of inputs would
be incremented. Another possible idea is to assign all inputs a unique position (across
all problems) and introduce the appropriate inputs at the ANN extraction stage, this
would remove the need to assign zero to non-existent inputs (as mentioned above).
This would mean no inputs are shared. Equally fundamental is the issue of handling
outputs. Currently, we have dedicated output neurons for each output, however, this
means that development can not start with a single neuron. Perhaps, neurons could
decide to be an output neuron for a particular problem and some scheme would need
to be devised to allocate the appropriate number of outputs for each computational
problem (rather like was done in SMCGP). Alternatively, extra genes could be added
to the genome like output genes in standard CGP. These output connection genes
would be real-valued between −1 and 1 and snap to nearest neurons. Essentially, this
would mean that the model would have three chromosomes, one each for the soma,
dendrites and outputs. This would have the advantage that only non-output neurons
would be necessary.
So far, we have examined the utility of the developmental model on three classiﬁ-
cation problems. However, the aim of the work is to produce general problem solving
on many different kinds of computational problems. Clearly, a favourable direction
to go is to expand the list of problems and problem types. How much neuron sharing
would take place across problems of different types (e.g. classiﬁcation and real-time
control)? Would different kinds of problems cause whole new sub-networks to grow?
These questions relate to a more fundamental issue which is the assessment of devel-
opmental ANNs. Should we have a training set of problems (rather than data) and
evaluate on an unseen (but related) set of problems?
Currently the neurons exist in a one-dimensional space however it would be rel-
atively straightforward to extend it to two or even three spatial dimensions.
In brains the morphology of neurons is activity dependent [53]. A simple way to
introduce this would be to examine whether a neuron is actually involved in the prop-
agation of signal from inputs to outputs (i.e. whether it is redundant or not). This
activity could be input to developmental programs. Alternatively, a signal related
input could be provided to developmental programs. This would mean that signals
from other neurons in the model could inﬂuence decisions made by neural and den-
drite programs. However, running developmental programs during the process of
passing signals through the ANN would mean that conventional ANNs could not be
extracted and also it would slow down assessment of network response to applied
signals. Perhaps some statistical measures of signals could be computed which are

56
J. F. Miller et al.
supplied to neural programs. They could be calculated during each developmen-
tal step and then supplied to neuron and dendrite programs at the start of the next
developmental step. However, it should be noted that activity dependent morphology
implies that networks would change during training (and testing) and network mor-
phology and behaviour would depend on past training history. This would complicate
ﬁtness assessment!
Eventually, the aim is to create developmental networks of spiking neurons. This
would allow models of activity dependent development based on biological neurons
to be abstracted and included in artiﬁcial models.
14
Conclusions
We have presented a conceptually simple model of a developmental neuron in which
neural networks develop over time. Conventional ANNs can be extracted from these
networks. We have shown that an evolved pair of programs can produce networks
that can solve multiple classiﬁcation problems reasonably well. Multiple-problem
solving is a new domain for investigating more general developmental neural models.
15
Appendix: Detailed Algorithms
15.1
Developing the Brain and Evaluating the Fitness
The detailed algorithm for developing the brain and assessing its ﬁtness is shown
in Algorithm 1 There are two stages to development. The ﬁrst (which we refer to
as ‘pre’) occurs prior to a learning epoch loop (lines 3–6). While the second phase
(referred to as ‘while’) occurs inside a learning epoch loop (lines 9–12).
Lines 13–22 are concerned with calculating ﬁtness. For each computational prob-
lem an ANN is extracted from the underlying brain. This is carried by a function
ExtractANN(problem, OutputAddress) which is detailed in Algorithm 9. This func-
tion extracts a feedforward ANN corresponding to each computational problem (this
is stored in a phenotype which we do not detail here). The array OutputAddress stores
the addresses of the output neurons associated with the computational problem. It is
used together with the phenotype to extract the network of neurons that are required
for the computational problem. Then the input data is supplied and the outputs of
the ANN calculated. The class of a data instance is determined by the largest out-
put. The learning loop (lines 8–29) develops the brain and exits if the ﬁtness value
(in this case classiﬁcation accuracy) reduces (lines 23–27 in Algorithm 1). One can
think of the ‘pre’ development phase as growing a neural network prior to training.
The ‘while’ phase is a period of development within the learning phase. Nep denotes
the user-deﬁned number of learning epochs. Np represents the number of problems

Evolving Programs to Build Artiﬁcial Neural Networks
57
in the suite of problems being solved. Nex(p) denotes the number of examples for
each problem. A is the accuracy of prediction for a single training instance. F is the
ﬁtness over all examples. TF is the accumulated ﬁtness over all problems. Fitness is
normalised (lines 20 and 22).
Algorithm 1 Develop network and evaluate ﬁtness
1: function Fitness
2:
Initialise brain
3:
Use ‘pre’ parameters
4:
for s = 0 to s < NDSpre do
# develop prior to learning
5:
UpdateBrain
6:
end for
7:
TFprev = 0
8:
for e = 0 to e < Nep do
# learning loop
9:
Use ‘while’ parameters
# learning phase
10:
for s = 0 to s < NDSwhi do
11:
UpdateBrain
12:
end for
13:
TF = 0
# initialise total ﬁt
14:
for p = 0 to p < Np do
15:
ExtractANN(p, OutputAddress)
# Get ANN for problem p
16:
F = 0
# initialise ﬁt
17:
for t = 0 to t < Nex(p) do
18:
F = F + Acc
# sum acc. over instances
19:
end for
20:
TF = TF + F/Nex(p)
# sum normalised acc. over problems
21:
end for
22:
TF = TF/Np
# normalise total ﬁtness
23:
if TF < TFprev then
# has ﬁtness reduced?
24:
TF = TFprev
# return previous ﬁtness
25:
Break
# terminate learning loop
26:
else
27:
TFprev = TF
# update previous ﬁtness
28:
end if
29:
end for
30:
return TF
31: end function
15.2
Developing the Brain and Evaluating the Fitness
Algorithm 2 shows the update brain process. This algorithm is run at each devel-
opmental step. It runs the soma and dendrite programs for each neuron and from
the previously existing brain creates a new version (NewBrain) which eventually
overwrites the previous brain at the last step (lines 52–53).

58
J. F. Miller et al.
Algorithm 2 Update brain
1: function UpdateBrain
2:
NewNumNeurons = 0
3:
for i = 0 to i < NumNeurons do
# get number and addresses of neurons
4:
if (Brain[i].out = 0) then
5:
NonOutputNeuronAddress[NumNonOutputNeurons] = i
6:
increment NumNonOutputNeurons
7:
else
8:
OutputNeuronAddress[NumOutputNeurons] = i
9:
increment NumOutputNeurons
10:
end if
11:
end for
12:
for i = 0 to i < NumNonOutputNeurons do
# process non-output neurons
13:
NeuronAddress = NonOutputNeuronAddress[i]
14:
Neuron = Brain[NeuronAddress]
15:
UpdatedNeurVars = RunSoma(Neuron)
# get new position, health and bias
16:
if (DisallowNonOutputsToMove) then
17:
UpdatedNeurVars.x = Neuron.x
18:
else
19:
UpdatedNeurVars.x = IfCollision(NewNumNeurons,NewBrain,UpdatedNeurVars.x)
20:
end if
21:
UpdatedNeuron = RunAllDendrites(Neuron, UpdatedNeurVars)
22:
if (UpdatedNeuron.health > NHdeath) then
# if neuron survives
23:
NewBrain[NewNumNeurons] = UpdatedNeuron
24:
Increment NewNumNeurons
25:
if (NewNumNeurons = NNmax-NumOutputNeurons) then
26:
Break
# exit non-output neuron loop
27:
end if
28:
end if
29:
if (UpdatedNeuron.health > NHhealth) then
# neuron replicates
30:
UpdatedNeuron.x = UpdatedNeuron.x+MNinc
31:
UpdatedNeuron.x = IfCollision(NewNumNeurons, NewBrain, UpdatedNeuron.x)
32:
NewBrain[NewNumNeurons] = CreateNewNeuron(UpdatedNeuron)
33:
Increment NewNumNeurons
34:
if (NewNumNeurons = NNmax - NumOutputNeurons) then
35:
Break
# exit non-output neuron loop
36:
end if
37:
end if
38:
end for
39:
for i = 0 to i < NumOutputNeurons do
# process output neurons
40:
NeuronAddress = OutputNeuronAddress[i]
41:
Neuron = Brain[NeuronAddress]
42:
UpdatedNeurVars = RunSoma(Neuron)
# get new position, health and bias
43:
if (DisallowOutputsToMove) then
44:
UpdatedNeurVars.x = Neuron.x
45:
else
46:
UpdatedNeurVars.x = IfCollision(NewNumNeurons,NewBrain,UpdatedNeurVars.x)
47:
end if
48:
UpdatedNeuron = RunAllDendrites(UpdatedNeuron)
49:
NewBrain[NewNumNeurons] = UpdatedNeuron
50:
Increment NewNumNeurons
51:
end for
52:
NumNeurons = NewNumNeurons
53:
Brain = NewBrain
54: end function

Evolving Programs to Build Artiﬁcial Neural Networks
59
Algorithm 2 starts by analyzing the brain to determine the addresses and numbers
of non-output and output neurons (lines 3–11). Then the non-output neurons are pro-
cessed. The evolved soma program is executed and it returns a neuron with updated
values for the neuron position, health and bias. These are stored in the variable
UpdatedNeurV ars.
If the user-deﬁned option to disallow non-output neuron movement is chosen
then the updated neuron position is reset to that before the soma program is run
(lines 16–17). Next the evolved dendrite programs are executed in all dendrites. The
algorithmic details are given in Algorithm 6 (See Sect.4.6).
The neuron health is compared with the user-deﬁned neuron death threshold
NHdeath and if the health exceeds the threshold the neuron survives (see lines 22–28).
At this stage it is possible that the neuron has been given a position that is identical
to one of the neurons in the developing brain (NewBrain) so one needs a mechanism
for preventing this. This is accomplished by Algorithm 3 (Lines 19 and 46). It checks
whether a collision has occurred and if so an increment MNinc is added to the position
and then it is bound to the interval [−1, 1]. In line 23 the updated neuron is written
into NewBrain. A check is made in line 25 to see if the allowed number of neurons
has been reached, if so the non output neuron update loop (lines 12–38) is exited and
the output neuron section starts (lines 39–51). If the limit on numbers of neurons
has not been reached, the updated neuron may replicate depending on whether its
health is above the user-deﬁned threshold, NHhealth (line 29). The position of the new
born neuron is immediately incremented by MNinc so that it does not collide with its
parent (line 30). However, its position needs to be checked also to see if it collides
with any other neuron, in which case its position is incremented again until a position
is found that causes no collision. This is done in the function IfCollision.
In CreateNewNeuron (see line 32) the bias, the incremented position and den-
drites of the parent neuron are copied into the child neuron. However, the new neuron
is given a health of 1.0 (the maximum value). The algorithm examines the non-output
neurons (lines 39–51) and again is terminated if the allowed number of neurons is
exceeded. The steps are similar to those carried out with non-output neurons, except
that output neurons can not either die or replicate as their number is ﬁxed by the
number of outputs required by the computational problem being solved.
The details of the neuron collision avoidance mechanism is shown in Algorithm 3.
15.3
Running the Soma
The UpdateBrain program calls the RunSoma program (Algorithm 4) to determine
how the soma changes in each developmental step. The seven soma program inputs
comprising the neuron health, position and bias, the averaged position, weight and
healthoftheneuron’sdendritesandtheproblemtypearesuppliedtotheCGPencoded
soma program (line 12). The array ProblemTypeInputs stores NumProblems+1

60
J. F. Miller et al.
Algorithm 3 Move neuron if it collides with another.
1: function IfCollision(NumNeurons, Brain, NeuronPosition)
2:
NewPosition = NeuronPosition
3:
collision = 1
4:
while collision do
5:
collision = 0
6:
for i = 0 to j < NumNeurons do
7:
if (| NeuronPosition - Brain[i].x | < 1.e-8) then
8:
collision = 1
9:
end if
10:
if collision then
11:
break
12:
end if
13:
end for
14:
if collision then
15:
NewPosition = NewPosition+MNinc
16:
end if
17:
end while
18:
if collision then
19:
NewPosition = Bound(NewPosition)
20:
end if
21:
return NewPosition
22: end function
constants equally spaced between −1 and 1. These are used to allow output neu-
rons to know what computational problem they belong to.
The soma program has three outputs relating to the position, health and bias of
the neuron. These are used to update the neuron (line 13).
Algorithm 4 RunSoma(Neuron)
1: function RunSoma(Neuron)
2:
AvDendritePosition = GetAvDendritePosition(Neuron)
3:
AvDendriteWeight = GetAvDendriteWeight(Neuron)
4:
AvDendriteHealth = GetAvDendriteHealth(Neuron)
5:
SomaProgramInputs[0] = Neuron.health
6:
SomaProgramInputs[1] = Neuron.x
7:
SomaProgramInputs[2] = Neuron.bias
8:
SomaProgramInputs[3] = AvDendritePosition
9:
SomaProgramInputs[4] = AvDendriteWeight
10:
SomaProgramInputs[5] = AvDendriteHealth
11:
SomaProgramInputs[6] = ProblemTypeInputs[WhichProblem]
12:
SomaProgramOutputs = SomaProgram(SomaProgramInputs)
13:
UpdatedNeuron = UpdateNeuron(Neuron, SomaProgramOutputs)
14:
return UpdatedNeuron.x, UpdatedNeuron.health, UpdatedNeuron.bias
15: end function

Evolving Programs to Build Artiﬁcial Neural Networks
61
15.4
Changing the Neuron Variables
The UpdateNeuron Algorithm (5) updates the neuron properties of health, position
and bias according to three user-chosen options deﬁned by a variable Incropt. If this
is zero, then the soma program outputs determine directly the updated values of the
soma’s health, position and bias. If Incropt is one or two, the updated values of the
soma are changed from the parent neuron’s values in an incremental way. This is
either a linear or nonlinear increment or decrement depending on whether the soma
program’s outputs are greater than or less than or equal to zero (lines 8–16). The
magnitudes of the increments is deﬁned by the user-deﬁned constants: δsh, δsp, δsb
and sigmoid slope parameter, α (see Table1).
TheincrementmethodsdescribedinAlgorithm5changeneuralvariables,soaction
needstobetakentoforcethevariablestostrictlylieintheinterval [−1, 1].Wecallthis
‘bounding’ (lines 34–36). This is accomplished using a hyperbolic tangent function.
15.5
Running All Dendrite Programs and Building a New
Neuron
Algorithm 6 takes an existing neuron and creates a new neuron using the updated
soma variables, position, health and bias which are stored in UpdateNeurV ars (from
Algorithm 4) and the updated dendrites which result from running the dendrite pro-
gram in all the dendrites. Initially (line 3–5), the updated soma variables are written
into the updated neuron. The number of dendrites in the updated neuron is set to zero.
In lines 8–11, the health of the non-updated neuron is examined and if it is above
the dendrite health threshold for birth, a new dendrite is generated and the updated
neuron gains a dendrite. If so, the neuron gains a dendrite created by a function
GenerateDendrite(). This assigns a weight, health and position to the new dendrite.
The weight and health is set to one and the position set to half the parent neuron
position. These choices appeared to give good results.
Lines 12–33 are concerned with processing the dendrite program in all the den-
drites of the non-updated neuron and updating the dendrites. If the updated dendrite
has a health above its death threshold then it survives and gets written into the updated
neuron (lines 22–28). Updated dendrites do not get written into the updated neuron
if it already has the maximum allowed number of dendrites (line 25–27). In lines
30–33 a check is made as to whether the updated neuron has no dendrites. If this is
so, it is given one of the dendrites of the non-updated neuron. Finally, the updated
neuron is returned to the calling function.
Algorithm 6 calls the function RunDendrite (line 21). This function is detailed
in Algorithm 7. It changes the dendrites of a neuron according to the evolved dendrite
program. It begins by assigning the dendrites health, position and weight to the parent
dendrite variables. It writes the dendrite program outputs to the internal variables
health, weight and position. Then in lines 8–16 it deﬁnes the possible increments in
health, weight and position that will be used to increment or decrement the parent

62
J. F. Miller et al.
Algorithm 5 Neuron update function
1: function UpdateNeuron(Neuron, SomaProgramOutputs)
2:
ParentHealth = Neuron.health
3:
ParentPosition = Neuron.x
4:
ParentBias = Neuron.bias
5:
health = SomaProgramOutputs[0]
6:
position = SomaProgramOutputs[1]
7:
bias = SomaProgramOutputs[2]
8:
if (Incropt = 1) then
# calculate increment
9:
HealthIncrement = δsh
10:
PositionIncrement = δsp
11:
BiasIncrement = δsb
12:
else if (Incropt = 2) then
13:
HealthIncrement = δsh*sigmoid(health, α)
14:
PositionIncrement = δsp*sigmoid(position, α)
15:
BiasIncrement = δsb*sigmoid(bias, α)
16:
end if
17:
if (Incropt > 0) then
# apply increment
18:
if (health > 0.0) then
19:
health = ParentHealth + HealthIncrement
20:
else
21:
health = ParentHealth - HealthIncrement
22:
end if
23:
if (position > 0.0) then
24:
position = ParentPosition + PositionIncrement
25:
else
26:
health = ParentPosition - PositionIncrement
27:
end if
28:
if (bias > 0.0) then
29:
bias = ParentBias + BiasIncrement
30:
else
31:
bias = ParentBias - BiasIncrement
32:
end if
33:
end if
34:
health = Bound(health)
35:
position = Bound(position)
36:
bias = Bound(bias)
37:
return health, position and bias
38: end function
variables according to the user deﬁned incremental options (linear or non-linear).
In lines 17–33 it respectively carries out the increments or decrements of the parent
dendrite variables according whether the corresponding dendrite program outputs
are greater than or less than or equal to zero. After this it bounds those variables.
Finally, in lines 37–44 it updates the dendrites health, weight and position provided
the adjusted health is above the dendrite death threshold (in other words it survives).
Note that if Incropt = 0 then there is no incremental adjustment and the health, weight
and position of the dendrites are just bounded (lines 34–36).

Evolving Programs to Build Artiﬁcial Neural Networks
63
Algorithm 6 Run the evolved dendrite program in all dendrites
1: function RunAllDendrites(Neuron, DendriteProgram, NewSomaPosition, NewSomaHealth,
NewSomaBias)
2:
WhichProblem = Neuron.isout
3:
OutNeuron.x = NewSomaPosition
4:
OutNeuron.health = NewSomaHealth
5:
OutNeuron.bias = NewSomaBias
6:
OutNeuron.isout = WhichProblem
7:
OutNeuron.NumDendrites = 0
8:
if (Neuron.health > DHbirth ) then
9:
OutNeuron.dendrites[NumDendrites] = GenerateDendrite()
10:
Increment OutNeuron.NumDendrites
11:
end if
12:
for i = 0 to i < OutNeuron.NumDendrites do
13:
DendriteProgramInputs[0] = Neuron.health
14:
DendriteProgramInputs[1] = Neuron.x
15:
DendriteProgramInputs[2] = Neuron.bias
16:
DendriteProgramInputs[3] = Neuron.dendrites[i].health
17:
DendriteProgramInputs[4] = Neuron.dendrites[i].weight
18:
DendriteProgramInputs[5] = Neuron.dendrites[i].position
19:
DendriteProgramInputs[6] = ProblemTypeInputs[WhichProblem]
20:
DendriteProgramOutputs = DendriteProgram(DendriteProgramInputs)
21:
UpdatedDendrite = RunDendrite(Neuron, DendriteProgramOutputs)
22:
if (UpdatedDendrite.isAlive) then
23:
OutNeuron.dendrites[NumDendrites] = UpdatedDendrite
24:
increment OutNeuron.NumDendrites
25:
if (OutNeuron.NumDendrites > MaxNumDendrites) then
26:
break
27:
end if
28:
end if
29:
end for
30:
if (OutNeuron.NumDendrites = 0) then
# if all dendrites die
31:
OutNeuron.dendrites[0] = Neuron.dendrites[0]
32:
OutNeuron.NumDendrites = 1
33:
end if
34:
return OutNeuron
35: end function
Algorithm 2 uses a function CreateNewNeuron to create a new neuron if the
neuron health is above a threshold. This function is described in Algorithm 8. It makes
the new born neuron the same as the parent (note, its position will be adjusted by the
collision avoidance algorithm) except that it is given a health of one. Experiments
suggested that this gave better results.

64
J. F. Miller et al.
Algorithm 7 Change dendrites according to the evolved dendrite program
1: function RunDendrite(Neuron, WhichDendrite, DendriteProgramOutputs)
2:
ParentHealth = Neuron.dendrites[WhichDendrite].health
3:
ParentPosition = Neuron.dendrites[WhichDendrite].x
4:
ParentWeight = Neuron.dendrites[WhichDendrite].weight
5:
health = DendriteProgramOutputs[0]
6:
weight = DendriteProgramOutputs[1]
7:
position = DendriteProgramOutputs[2]
8:
if (Incropt = 1) then
9:
HealthIncrement = δdh
10:
WeightIncrement = δdw
11:
PositionIncrement = δdp
12:
else if (Incropt = 2) then
13:
HealthIncrement = δdh*sigmoid(health, α)
14:
WeightIncrement = δdw*sigmoid(weight, α)
15:
PositionIncrement = δdp*sigmoid(position, α)
16:
end if
17:
if (Incropt > 0) then
18:
if (health > 0.0) then
19:
health = ParentHealth + HealthIncrement
20:
else
21:
health = ParentHealth - HealthIncrement
22:
end if
23:
if (position > 0.0) then
24:
position = ParentPosition + PositionIncrement
25:
else
26:
health = ParentPosition - PositionIncrement
27:
end if
28:
if (weight > 0.0) then
29:
weight = ParentWeight + BiasIncrement
30:
else
31:
weight = ParentWeight - BiasIncrement
32:
end if
33:
end if
34:
health = Bound(health)
35:
position = Bound(position)
36:
weight = Bound(weight)
37:
if (health > DHdeath) then
38:
UpdatedDendrite.weight = weight
39:
UpdatedDendrite.health = health
40:
UpdatedDendrite.x = position
41:
UpdatedDendriteisAlive = 1
42:
else
43:
UpdatedDendriteisAlive = 0
44:
end if
45:
return UpdatedDendrite and UpdatedDendriteisAlive
46: end function

Evolving Programs to Build Artiﬁcial Neural Networks
65
Algorithm 8 Create new neuron from parent neuron
1: function CreateNewNeuron(ParentNeuron)
2:
ChildNeuron.NumDendrites = ParentNeuron.NumDendrites
3:
ChildNeuron.isout = 0
4:
ChildNeuron.health = 1
5:
ChildNeuron.bias = ParentNeuron.bias
6:
ChildNeuron.x = ParentNeuron.x
7:
for i = 0 to i < ChildNeuron.NumDendrites do
8:
ChildNeuron.dendrites[i] = ParentNeuron.dendrites[i]
9:
end for
10: end function
15.6
Extracting Conventional ANNs from the Evolved Brain
In Algorithm 1, a conventional feed-forward ANN is extracted from the underlying
collection of neurons (line 15). The algorithm for doing this is shown in Algorithm 9.
Firstly, this algorithm determines the number of inputs to the ANN (line 5). Since
inputs are shared across problems the number of inputs is set to be the maximum
number of inputs that occur in the computational problem suite. If an individual
problem has less inputs than this maximum, the extra inputs are set to 0.0. The
brain array is sorted by position. The algorithm then examines all neurons (line 7)
and calculates the number of non-output neurons and output neurons and stores the
neuron data in arrays NonOutputNeurons and OutputNeurons. It also calculates their
addresses in the brain array.
The next phase is to go through all dendrites of the non-output neurons to deter-
mine which inputs or neurons they connect to (lines 19–33). The evolved neuron
programs generate dendrites with end positions anywhere in the interval [−1, 1].
The end positions are converted to lengths (line 25). In this step the dendrite position
is linearly mapped into the interval [0, 1]. To generate a valid neural network we
assume that dendrites are automatically connected to the nearest neuron or input on
the left. We refer to this as “snapping” (lines 28 and 44). The dendrites of non-output
neurons are allowed to connect to either inputs or other non-output neurons on their
left. However, output neurons are only allowed to connect to non-output neurons
on their left. Algorithm 10 returns the address of the neuron or input that the den-
drite snaps to. The dendrites of output neurons are not allowed to connect directly to
inputs (see Line 4 of the GetClosest function), however, when neurons are allowed
to move, there can occur a situation where an output neuron is positioned so that it
is the ﬁrst neuron on the right of the outputs. In that situation it can only connect to
inputs. If this situation occurs then the initialisation of the variable AddressOfClosest
to zero in the GetClosest function (line 2) means that all the dendrites of the output
neuron will be connected to the ﬁrst external input to the ANN network. Thus a
valid network will still be extracted albeit with a rather useless output neuron. It is
expected that evolution will avoid using programs that allow this to happen.

66
J. F. Miller et al.
Algorithm 9 The extraction of neural networks from the underlying brain.
1: function ExtractANN(problem, OutputAddress)
2:
NumNonOutputNeurons = 0
3:
NumOutputNeurons = 0
4:
OutputCount=0
5:
Ni = max(Ni, p)
6:
sort(Brain, 0, NumNeurons-1)
# sort neurons by position
7:
for i = 0 to i < NumNeurons do
8:
Address = i + Ni
9:
if (Brain[i].isout > 0) then
# non-output neuron
10:
NonOutputNeur[NumNonOutputNeur] = Brain[i]
11:
NonOutputNeuronAddress[NumNonOutputNeur]= Address
12:
Increment NumNonOutputNeur
13:
else
# output neuron
14:
OutputNeurons[NumOutputNeurons]= Brain[i]
15:
OutputNeuronAddress[NumOutputNeurons]= Address
16:
Increment NumOutputNeurons
17:
end if
18:
end for
19:
for i = 0 to i < NumNonOutputNeur do
# do non-output neurons
20:
Phenotype[i].isout = 0
21:
Phenotype[i].bias = NonOutputNeur[i].bias
22:
Phenotype[i].address = NonOutputNeuronAddress[i]
23:
NeuronPosition = NonOutputNeur[i].x
24:
for j = 0 to j < NonOutputNeur[i].NumDendrites do
25:
Convert DendritePosition to DendriteLength
26:
DendPos = NeuronPosition - DendriteLength
27:
DendPos = Bound(DendPos)
28:
AddressClosest = GetClosest(NumNonOutputNeur, NonOutputNeur, 0, DendPos)
29:
Phenotype[i].ConnectionAddresses[j] = AddressClosest
30:
Phenotype[i].weights[j] = NonOutputNeur[i].weight[j]
31:
end for
32:
Phenotype[i].NumConnectionAddress = NonOutputNeur[i].NumDendrites
33:
end for
34:
for i = 0 to i < NumOutputNeurons do
# do output neurons
35:
i1 = i+NumOutputNeurons
36:
Phenotype[i1].isout = OutputNeurons[i].isout
37:
Phenotype[i1].bias = OutputNeurons[i].bias
38:
Phenotype[i1].address = OutputNeuronAddress[i]
39:
NeuronPosition = OutputNeurons[i].x
40:
for j = 0 to j < OutputNeurons[i].NumDendrites do
41:
Convert DendritePosition to DendriteLength
42:
DendPos = NeuronPosition - DendriteLength
43:
DendPos = Bound(DendPos)
44:
AddressClosest = GetClosest(NumNonOutputNeur, NonOutputNeur, 1, DendPos)
45:
Phenotype[i1].ConnectionAddresses[j] = AddressClosest
46:
Phenotype[i1].weights[j] = OutputNeuron[i].weight[j]
47:
end for
48:
Phenotype[i1].NumConnectionAddress = OutputNeurons[i].NumDendrites
49:
if (OutputNeurons[i].isout == problem+1) then
50:
OutputAddress[OutputCount] = OutputNeuronAddress[i]
51:
Increment OutputCount
52:
end if
53:
end for
54: end function

Evolving Programs to Build Artiﬁcial Neural Networks
67
Algorithm 10 Find which input or neuron a dendrite is closest to
1: function GetClosest(NumNonOutNeur, NonOutNeur, IsOut, DendPos)
2:
AddressOfClosest = 0
3:
min = 3.0
4:
if (IsOut = 0) then
# only non-out neurons connect to inputs
5:
for (i = 0 to i < MaxNumInputs) do
6:
distance = DendPos - InputLocations[i]
7:
if distance > 0 then
8:
if (distance < min) then
9:
min = distance
10:
AddressOfClosest = i
11:
end if
12:
end if
13:
end for
14:
end if
15:
for j = 0 to j <NumNonOutputNeur do
16:
distance = DendPos - NonOutNeur[j].x
17:
if distance > 0 then
# feed-forward connections
18:
if (distance < min) then
19:
min = distance
20:
AddressOfClosest = j + MaxNumInputs
21:
end if
22:
end if
23:
end for
24:
return AddressOfClosest
25: end function
Algorithm 9 stores the information required to extract the ANN in an array called
Phenotype. It contains the connection addresses of all neurons and their weights
(lines 29–30 and 45–46). Finally it stores the addresses of the output neurons
(OutputAddress) corresponding to the computational problem whose ANN is being
extracted (lines 49–52). These deﬁne the outputs of the extracted ANNs when they
are supplied with inputs (i.e. in the ﬁtness function when the Accuracy is assessed
(see Algorithm 1). The Phenotype is stored in the same format as Cartesian Genetic
Programming (see Sect.5) and decoded in a similar way to genotypes.
References
1. Aljundi, R., Chakravarty, P., Tuytelaars, T.: Expert gate: lifelong learning with a network of
experts 2 (2016). CoRR. arXiv:1611.06194
2. Astor, J.C., Adami, C.: A development model for the evolution of artiﬁcial neural networks.
Artiﬁcial Life 6, 189–218 (2000)
3. Balaam,A.:Developmentalneuralnetworksforagents.In:AdvancesinArtiﬁcialLife,Proceed-
ings of the 7th European Conference on Artiﬁcial Life (ECAL 2003), pp. 154–163. Springer
(2003)

68
J. F. Miller et al.
4. Belew, R.K.: Interposing an ontogenic model between genetic algorithms and neural networks.
In: S.J. Hanson, J.D. Cowan, C.L. Giles (eds.) Advances in neural information processing
systems NIPS5, pp. 99–106. Morgan Kaufmann (1993)
5. Boers, E.J.W., Kuiper, H.: Biological metaphors and the design of modular neural networks.
Master’s thesis, Dept. of Computer Science and Dept. of Experimental and Theoretical Psy-
chology, Leiden University (1992)
6. Cangelosi, A., Nolﬁ, S., Parisi, D.: Cell division and migration in a ‘genotype’ for neural
networks. Netw.-Comput. Neural Syst. 5, 497–515 (1994)
7. Deacon, T.: The Symbolic Species: The Co-evolution of Language and the Brain. W.W. Norton
and Company, New York (1998)
8. Dekaban, A.S., Sadowsky, D.: Changes in brain weights during the span of human life. Ann.
Neurol. 4, 345–356 (1978)
9. Downing, K.L.: Supplementing evolutionary developmental systems with abstract models of
neurogenesis. In: Proceedings of the Conference on Genetic and Evolutionary Computation,
pp. 990–996 (2007)
10. Drchal, J., Šnorek, M.: Tree-based indirect encodings for evolutionary development of neural
networks. In: K˚urková, V., Neruda, R., Koutník, J. (eds.) Artiﬁcial Neural Networks - ICANN,
pp. 839–848. Springer, Berlin (2008)
11. Edelman, G., Tononi, G.: A Universe of Consciousness. Basic Books, New York (2000)
12. Eggenberger, P.: Creation of neural networks based on developmental and evolutionary princi-
ples. In: Gerstner, W., Germond, A., Hasler, M., Nicoud J.D. (eds.) Artiﬁcial Neural Networks
— ICANN’97, pp. 337–342 (1997)
13. Fahlman, S.E., Lebiere, C.: The cascade-correlation learning architecture. In: Advances in
Neural Information Processing Systems, pp. 524–532 (1990)
14. Federici, D.: A regenerating spiking neural network. Neural Netw. 18(5–6), 746–754 (2005)
15. Fernández-Delgado, M., Cernadas, E., Barro, S., Amorim, D.: Do we need hundreds of clas-
siﬁers to solve real world classiﬁcation problems? J. Mach. Learn. Res. 15(1), 3133–3181
(2014)
16. Ferreira, C.: Gene Expression Programming: Mathematical Modeling by an Artiﬁcial Intelli-
gence, 2nd edn. Springer, New York (2006)
17. Floreano, D., Urzelai, J.: Neural morphogenesis, synaptic plasticity, and evolution. Theory
Biosci. 120(3), 225–240 (2001)
18. Franco, L., Jerez, J.M.: Constructive Neural Networks, vol. 258. Springer, Berlin (2009)
19. French, R.M.: Catastrophic forgetting in connectionist networks: causes, consequences and
solutions. Trends Cogn. Sci. 3(4), 128–135 (1999)
20. Goldman, B.W., Punch, W.F.: Reducing wasted evaluations in cartesian genetic program-
ming. In: Proceedings of the Genetic Programming: 16th European Conference, EuroGP 2013,
Vienna, Austria, April 3–5, 2013, pp. 61–72. Springer, Berlin (2013)
21. Goldman, B.W., Punch, W.F.: Analysis of cartesian genetic programmings evolutionary mech-
anisms. IEEE Trans. Evol. Comput. 19, 359–373 (2015)
22. Gruau, F.: Automatic deﬁnition of modular neural networks. Adapt. Behav. 3, 151–183 (1994)
23. Gruau, F., Whitley, D., Pyeatt, L.: A comparison between cellular encoding and direct encoding
for genetic neural networks. In: Proceedings of Conference on Genetic Programming, pp. 81–89
(1996)
24. Hampton, A.N., Adami, C.: Evolution of robust developmental neural networks. In: Pollack,
J., Bedau, M.A., Husbands, P., Ikegami, T., Watson R. (eds.) Proceedings of Artiﬁcial Life IX,
pp. 438–443 (2004)
25. Harding, S., Miller, J.F., Banzhaf, W.: Developments in cartesian genetic programming: self-
modifying cgp. Genet. Program. Evolvable Mach. 11(3–4), 397–439 (2010)
26. Hornby, G., Lipson, H., Pollack, J.B.: Generative representations for the automated design of
modular physical robots. IEEE Trans. Robot. Autom. 19, 703–719 (2003)
27. Hornby, G.S., Pollack, J.B.: Creating high-level components with a generative representation
for body-brain evolution. Artif. Life 8(3) (2002)

Evolving Programs to Build Artiﬁcial Neural Networks
69
28. Huizinga, J., Clune, J., Mouret, J.B.: Evolving neural networks that are both modular and
regular: HyperNEAT plus the connection cost technique. In: Proceedings of the Conference on
Genetic and Evolutionary Computation, pp. 697–704 (2014)
29. Isles, A.: Neural and behavioral epigenetics; what it Is, and what is hype. Wiley (2015)
30. Jakobi, N.: Harnessing morphogenesis, COGS Research Paper 423. University of Sussex,
Technical report (1995)
31. Jung, S.Y.: A topographical method for the development of neural networks for artiﬁcial brain
evolution. Artif. Life 11, 293–316 (2005)
32. Kandel, E.R., Schwartz, J.H., Jessell, T.M.: Principles of Neural Science, 4th edn. McGraw-
Hill, New York (2000)
33. Khan, G.M.: Evolution of Artiﬁcial Neural Development - In Search of Learning Genes. Studies
in Computational Intelligence, vol. 725. Springer, Berlin (2018)
34. Khan, G.M., Miller, J.F.: In search of intelligence: evolving a developmental neuron capable
of learning. Connect. Sci. 26(4), 297–333 (2014)
35. Khan, G.M., Miller, J.F., Halliday, D.M.: Evolution of cartesian genetic programs for develop-
ment of learning neural architecture. Evol. Comput. 19(3), 469–523 (2011)
36. Kitano, H.: Designing neural networks using genetic algorithms with graph generation system.
Complex Syst. 4, 461–476 (1990)
37. Kleim, J.A., Lussnig, E., Schwartz, E.R., Comery, T.A., Greenough, W.T.: Synaptogenesis and
fos expression in the motor cortex of the adult rat after motor skill learning. J. Neurosci. 16,
4529–4535 (1996)
38. Kleim, J.A., Vij, K., Ballard, D.H., Greenough, W.T.: Learning-dependent synaptic modiﬁca-
tions in the cerebellar cortex of the adult rat persist for at least four weeks. J. Neurosci. 17,
717–721 (1997)
39. Kodjabachian, J., Meyer, J.A.: Evolution and development of neural controllers for locomotion,
gradient-following, and obstacle-avoidance in artiﬁcial insects. IEEE Trans. Neural Netw. 9,
796–812 (1998)
40. Koutník, J., Gomez, F., Schmidhuber, J.: Evolving neural networks in compressed weight space.
In: Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO-10)
(2010)
41. Kumar, S., Bentley, P. (eds.): On Growth, Form and Computers. Academic Press (2003)
42. Luke, S., Spector, L.: Evolving graphs and networks with edge encoding: preliminary report.
In: Late Breaking Papers at the Genetic Programming Conference, pp. 117–124 (1996)
43. Maguire, E.A., Gadian, D.G., Johnsrude, I.S., Good, C.D., Ashburner, J., Frackowiak, R.S.J.,
Frith, C.D.: Navigation-related structural change in the hippocampi of taxi drivers. PNAS 97,
4398–4403 (2000)
44. McCloskey, M., Cohen, N.: Catastrophic interference in connectionist networks: the sequential
learning problem. Psychol. Learn. Motiv. 24, 109–165 (1989)
45. McCulloch, W., Pitts, W.: A logical calculus of the ideas immanent in nervous activity. Bull.
Math. Biophys. 5, 115–133 (1943)
46. Métin, C., Vallee, R., Rakic, P., Bhide, P.: Modes and mishaps of neuronal migration in the
mammalian brain. Neuroscience 28, 11746–11752 (2008)
47. Miller, J.F.: What bloat? cartesian genetic programming on boolean problems. In: Proceedings
of the Conference on Genetic and Evolutionary Computation, Late Breaking Papers, pp. 295–
302 (2001)
48. Miller, J.F. (ed.): Cartesian Genetic Programming. Springer, Berlin (2011)
49. Miller, J.F., Smith, S.L.: Redundancy and computational efﬁciency in cartesian genetic pro-
gramming. IEEE Trans. Evol. Comput. 10(2), 167–174 (2006)
50. Miller, J.F., Thomson, P.: Cartesian genetic programming. In: Proceedings of European Con-
ference on Genetic Programming. LNCS, vol. 10802, pp. 121–132 (2000)
51. Miller, J.F., Thomson, P.: A developmental method for growing graphs and circuits. In: Pro-
ceedings of the International Conference on Evolvable Systems. LNCS, vol. 2606, pp. 93–104
(2003)

70
J. F. Miller et al.
52. Miller, J.F., Wilson, D.G., Cussat-Blanc, S.: Evolving developmental programs that build neural
networks for solving multiple problems. In: Banzhaf, W., Spector, L., Sheneman L. (eds.)
Genetic Programming Theory and Practice XVI, Chap. TBC. Springer (2019)
53. Ooyen, A.V. (ed.): Modeling Neural Development. MIT Press, Cambridge (2003)
54. Rakic, P.: Principles of neural cell migration. Experientia 46, 882–891 (1990)
55. Ratcliff, R.: Connectionist models of recognition and memory: constraints imposed by learning
and forgetting functions. Psychol. Rev. 97, 205–308 (1990)
56. Risi, S., Lehman, J., Stanley, K.O.: Evolving the placement and density of neurons in the Hyper-
NEAT substrate. In: Proceedings of the Conference on Genetic and Evolutionary Computation,
pp. 563–570 (2010)
57. Risi, S., Stanley, K.O.: Indirectly encoding neural plasticity as a pattern of local rules. In: From
Animals to Animats 11: Conference on Simulation of Adaptive Behavior (2010)
58. Risi, S., Stanley, K.O.: Enhancing ES-HyperNEAT to evolve more complex regular neural
networks. In: Proceedings of the Conference on Genetic and Evolutionary Computation, pp.
1539–1546 (2011)
59. Roggen, D., Federici, D., Floreano, D.: Evolutionary morphogenesis for multi-cellular systems.
Genet. Program. Evolvable Mach. 8(1), 61–96 (2007)
60. Rose, S.: The Making of Memory: From Molecules to Mind. Vintage (2003)
61. Rust, A., Adams, R., Bolouri, H.: Evolutionary neural topiary: growing and sculpting artiﬁcial
neurons to order. In: Proceedings of the Conference on the Simulation and synthesis of Living
Systems, pp. 146–150 (2000)
62. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K.,
Pascanu, R., Hadsell, R.: Progressive neural networks (2016). arXiv:1606.04671
63. Ryan, C., Collins, J.J., Neill, M.O.: Grammatical evolution: evolving programs for an arbitrary
language. In: Banzhaf, W., Poli, R., Schoenauer, M., Fogarty, T.C. (eds.) Genetic Programming,
pp. 83–96. Springer, Berlin (1998)
64. Sharkey, A.J.: Combining artiﬁcial neural nets: ensemble and modular multi-net systems.
Springer Science & Business Media (2012)
65. Siddiqi, A.A., Lucas, S.M.: A comparison of matrix rewriting versus direct encoding for evolv-
ing neural networks. In: Proceedings IEEE International Conference on Evolutionary Compu-
tation Proceedings, pp. 392–397 (1998)
66. Smythies, J.: The Dynamic Neuron. MIT Press, Cambridge (2002)
67. Stanley, K., Miikkulainen, R.: Efﬁcient evolution of neural network topologies. Proc. Congr.
Evol. Comput. 2, 1757–1762 (2002)
68. Stanley, K.O.: Compositional pattern producing networks: a novel abstraction of development.
Genet. Program. Evolvable Mach. 8, 131–162 (2007)
69. Stanley, K.O., D’Ambrosio, D.B., Gauci, J.: A hypercube-based encoding for evolving large-
scale neural networks. Artif. Life 15, 185–212 (2009)
70. Stanley, K.O., Miikkulainen, R.: A taxonomy for artiﬁcial embryogeny. Artif. Life 9(2), 93–130
(2003)
71. Suchorzewski, M., Clune, J.: A novel generative encoding for evolving modular, regular and
scalable networks. In: Proceedings of the Conference on Genetic and Evolutionary Computa-
tion, pp. 1523–1530 (2011)
72. Terekhov, A.V., Montone, G., ORegan, J.K.: Knowledge transfer in deep block-modular neural
networks. In: Conference on Biomimetic and Biohybrid Systems, pp. 268–279. Springer (2015)
73. Tierney, A., Nelson III, C.: Brain development and the role of experience in the early years.
Zero Three 30, 9–13 (2009)
74. Tramontin, A.D., Brenowitz, E.: Seasonal plasticity in the adult brain. Trends Neurosci. 23,
251–258 (2000)
75. Tsankova, N., Renthal, W., Kumar, A., Nestler, E.: Epigenetic regulation in psychiatric disor-
ders. Nat. Rev. Neurosci. 8(5), 33–367 (2007)
76. Turner, A.J., Miller, J.F.: Cartesian genetic programming encoded artiﬁcial neural networks:
a comparison using three benchmarks. In: Proceedings of the Conference on Genetic and
Evolutionary Computation (GECCO), pp. 1005–1012 (2013)

Evolving Programs to Build Artiﬁcial Neural Networks
71
77. Turner, A.J., Miller, J.F.: Recurrent cartesian genetic programming. In: Proceedings of the
Parallel Problem Solving from Nature, pp. 476–486 (2014)
78. Valverde, F.: Rate and extent of recovery from dark rearing in the visual cortex of the mouse.
Brain Res. 33, 1–11 (1971)
79. Vassilev, V.K., Miller, J.F.: The advantages of landscape neutrality in digital circuit evolution.
In: Proceedings of the International Conference on Evolvable Systems. LNCS, vol. 1801, pp.
252–263. Springer (2000)
80. Yerushalmi, U., Teicher, M.: Evolving synaptic plasticity with an evolutionary cellular devel-
opment model. PLOS One 3(11), e3697 (2008)
81. Yu, T., Miller, J.F.: Neutrality and the evolvability of Boolean function landscape. In: Proceed-
ings of the European Conference on Genetic Programming. LNCS, vol. 2038, pp. 204–217
(2001)
82. Zar, J.H.: Biostatistical Analysis, 2nd edn. Prentice Hall, Upper Saddle River (1984)

Anti-heterotic Computing
Viv Kendon
Abstract When two or more different computational components are combined to
produce computational power greater than the sum of the parts, this has been called
heterotic computing (Stepney et al in 8th workshop on quantum physics and logic
(QPL 2011), vol 95, pp 263–273, 2012 [46, EPTCS 95 263]). An example is mea-
surement based quantum computing, in which a set of entangled qubits are measured
in turn, with the measurement outcomes fed forward by a simple classical computer,
to keep track of the parity of the measurement outcomes on each qubit. The parts
are no more powerful than a classical computer, while the combination provides
universal quantum computation. Most practical physical computers are hybrids of
several different types of computational components, but not all are heterotic. In
fact, anti-heterotic, in which the whole is less than the sum of the parts, is the most
awkward case to deal with. This occurs commonly in experiments on new uncon-
ventional computational substrates. The classical controls in such experiments are
almost always conventional classical computers with computational power far out-
stripping the samples of materials being tested. Care must be exercised to avoid
accidentally carrying out all of the computation in the controlling classical com-
puter. In this overview, existing tools to analyse hybrid computational systems are
summarised, and directions for future development identiﬁed.
1
Background
Nowadays, almost all practical physical computers are composed of more than one
distinct computational component. For example, a desktop computer will usually
have a CPU and a graphics co-processor, and a chip dedicated to ethernet or wiﬁ
protocols for fast communications. A more powerful work station will also have
space for dedicated processors, like FPGAs (ﬁeld-programmable gate array). Silicon-
based chips are no longer becoming faster through shrinking the feature size. Instead,
V. Kendon (B)
Department of Physics, Durham University, Durham DH1 3LE, UK
e-mail: viv.kendon@durham.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_3
73

74
V. Kendon
increased computational power is achieved through optimising the chip design for
speciﬁc applications, resulting in hundreds of different chips, several of which are
combined in a general purpose digital computer. Although they are all treated as
being the same type of computational device by theory, the practical difﬁculties in
programming multi-CPU or GPU devices are considerable, and there are many open
questions in current research on parallel and multi-threaded computation. Composing
computational devices to gain computational power is highly non-trivial even in a
conventional setting. Indeed, the theory of computation usually ignores the physical
layer entirely, so it has no way to model the effects of combining different types
of computational components, let alone tell us when this might be an advantageous
thing to do.
When unconventional computational devices are considered, the challenges
around composing them into hybrid computers increase further. Their computational
capabilities are determined by the properties of the physical system [44]. Any physi-
cal system with sufﬁciently complex structure and dynamics can be used to compute.
Examples under active current study include chemical [36, 39, 47], biological [1, 2],
quantum [43], optical [48, 49], and various analog [26, 37, 38] computational sub-
strates. Such devices may be best described by models with intrinsically different
logic, e.g., quantum computers and neural nets both use different logic, and are cor-
respondingly challenging to program. There may be additional challenges in data
encoding. Given a quantum register, it is in general impossible to efﬁciently repre-
sent the data it encodes in a classical register of equivalent size. Passing data between
quantum and classical processors therefore has to be planned carefully, to avoid loss
of information.
Passing data between processors is one way to combine different computational
devices. A dedicated fast co-processor can perform a commonly used subroutine
(e.g., a Fourier transform) on data passed to it and return the result for further pro-
cessing. But most co-processors are not only passed data, they are also passed control
instructions that tell them what processing to perform. This is potentially more pow-
erful, since repeated cycles of data passing and control can act as a feedback loop
control system. Control systems include computation as part of their basic function-
ality [31], so this kind of interconnection potentially adds computation in addition
to that performed by the constituent components. Examples where the combination
is more powerful than the sum of the parts are known, and designated heterotic
computers—examples are described in Sect.2.
When the computational device is an experimental test of a new material, there is
almost always a control computer that is powerful enough to do far more computation
than is being tested in the material. This is the anti-heterotic case, where we are
interested in the computational power of the substrate with only minimal i/o or
controls, but for practical reasons, the controls are actually much more powerful.
The presence of a powerful classical computer makes it difﬁcult to tell where the
observed computation is taking place.
Exploiting the potential of diverse computational devices in combination needs
new theoretical tools, as argued in Kendon et al. [34], Stepney et al. [45, 46].

Anti-heterotic Computing
75
Such tools will enable us to compose the models of different computational sub-
strates and determine the power of the resulting hybrid computers.
This chapter is organized as follows: Sect.2 summarises prior work on identify-
ing heterotic systems in quantum computing. Section3 considers cases in classical
unconventional computing where the combination has less computational power,
rather than more. Sections4 and 5 provide an overview of tools available and desir-
able for effectively analysing hybrid computational systems. Section6 summarises
and outlines future directions.
2
Computational Enhancement in Hybrid Systems
Heterotic computing was ﬁrst identiﬁed in a quantum computational setting where
speciﬁc classical controls are required as part of the operation of the computer. The
importance of the classical control system in quantum computers was ﬁrst noted
by Jozsa [33], without identifying the contribution to the computational power. A
speciﬁc type of quantum computing in which the classical controls play an essential
role is measurement-based quantum computing (MBQC), also known as cluster state,
and as one-way, quantum computing [40]. In MBQC, many qubits are ﬁrst prepared
in an entangled state. The computation proceeds by measuring the qubits in turn.
The outcomes from the measurements feed forward to determine the settings for the
measurements performed on the next qubits, see Fig.1.
Anders and Browne [3] realised that the classical computation required to control
and feed forward information in MBQC provides a crucial part of the computa-
tional power. Applying measurements without feed-forward is efﬁciently classically
simulatable. The classical part of the computation that keeps track of the parity
of measurements on each qubit is obviously also efﬁciently classical simulatable.
However, the combination of the two is equivalent to the quantum circuit model
with unbounded fan-out [17], which is not (efﬁciently) classically simulatable. This
Fig. 1 Schematic diagram of a measurement-based quantum computer. The qubits (circles) are pre-
pared in an entangled state, indicated by grey ﬁll and connecting lines. Measurements are performed
on the qubits in turn, removing them from the entangled state (unﬁlled) and producing a measure-
ment output. The value of the output determines (decision) the settings for the next measurements.
The measured qubits can be recycled to form new cluster state (dashed line), see Horsman et al. [27]

76
V. Kendon
Fig. 2 A qubus sequence using six operations instead of eight to perform two gates. The boxes
show displacements performed on the continuous variable ﬁeld controlled by the qubits. Shaded
boxes represent an operation acting on the position quadrature of the bus, while unshaded boxes
represent operations on the momentum quadrature, see Brown et al. [15]
shows that the combination of two or more systems to form a new computational
system, can be in a more powerful computational class than the constituent systems
acting separately.
The theory of ancilla-based quantum computation [4] has been abstracted and
developed from MBQC, into a framework where now a quantum system (ancilla)
controls another quantum system (the qubits), with or without measurement of the
ancilla system during the computation. This framework is capable of modelling
many types of hybrid quantum computing architectures in which two or more dif-
ferent quantum systems are combined. When the role of the ancilla system is played
by a continuous variable quantum system instead of a qubit or qudit (d-dimensional
quantum system), further efﬁciencies become available. The qubus quantum com-
puter uses a coherent state as a bus connecting the individual qubits. A quantum
coherent state has two quadratures, which act as two coupled continuous variable
quantum systems. A practical example is a single-mode laser, such as can be found
in many state-of-the-art quantum optics experimental labs. This type of ancilla can
interact with many qubits in turn, allowing savings in the number of basic operations
required for gate operations [15] and for building cluster states [16, 27]. Figure2
shows a sequence of six operations that performs four gates, one between each pos-
sible pair of the three qubits. Each gate performed separately would require two
operations, thus this sequence saves at least two operations over standard methods,
more if the qubits have to be swapped to adjacent positions for direct gates. Typically,
this provides polynomial reductions in the number of elementary operations required
for a computation, when compared with interacting the qubits directly.
The natural computational power of the coherent state is only classical without
some type of nonlinear interaction [9]. A nonlinear interaction can be with another
type of qubit, as in the qubus case, or via measurements of entangled coherent states
[35], which generates quantum computing probabilistically, with a high overhead in
failed operations which have to be repeated. The theoretical heterotic advantage of
hybrid quantum computers is less dramatic than in the classical control case, but the
practical advantages are clear from many points of view. Using the quantum system
best suited to each role (memory, processing, data transfer, read out) produces a more
efﬁcient computer overall.
Our examples in this section consist of a substrate system with a control computer.
In MBQC, which has classical controls, the system clearly has two components and

Anti-heterotic Computing
77
can be precisely analysed. For the hybrid quantum systems such as the qubus, the
controls are quantum, but there are always further classical controls in any actual
experiment. This thus forms a multi-layer system. In fact, it is not clear that meaning-
ful quantum computing without a classical component is possible if “whole system”
analysis is performed.
3
Identifying the Computational Power of the Substrate
The ﬁrst clearly identiﬁed classical example of heterotic computing uses a liquid
state NMR experiment to perform classical gates [41]. The use of NMR to study
computation was ﬁrst established for quantum computing (e.g., Jones [32]). Both
liquid state [22, 25] and solid state [21] NMR quantum computing architectures have
been implemented. However, the quantum nature of the liquid state NMR ensemble
hasbeenquestioned[14],leavingthetruepowerofNMRcomputersanopenquestion.
Roselló-Merino et al. [41] used a liquid state NMR system to study classical
computing. In experiments to perform a sequence of simple classical logic gates,
such as NAND, the instruments controlling the NMR experiment pass the outputs of
one gate through to the inputs of the next. The output signals and input controls are in
completely different formats, so the controlling instruments do signal transduction,
which can require signiﬁcant computational power. In this case, a Fourier transform
is applied, which requires vastly more computational power than a single logic gate.
It is thus rather stretching the deﬁnition of heterotic computing to claim these NMR
experiments as an example.
Nonetheless, such experiments are very insightful. Using NMR to do classical
computing involves choosing a subset of the available parameters that are suitable
for representing classical bits, and restricting the operations so as to keep the spin
ensemble in fully determined classical states. In this way, more robust operations are
obtained at the expense of not exploiting the full capabilities of the physical system.
As with MBQC, the control computer plays an essential role in the computation, but
by itself does not perform the gate logic. Although, unlike MBQC, in this case the
control computer very well could perform the gate logic. This is thus an example of
anti-heterotic computing, in which the whole is constrained to be less than the sum
of the parts.
As a step towards fully characterizing the classical computational power of the
NMR system, Bechmann et al. [11] have produced a preliminary classiﬁcation of
the experimental NMR parameters for implementing classical logic gates. As part
of this, they determined that a trinary logic (rather than binary) is natural for NMR.
Using the natural logic of the model of the physical system is an important element in
fully exploiting the computational capabilities. Their work has been extended to take
advantage of the inherently continuous nature of the NMR parameter space of non-
coupled spin species [10] by implementing continuous gates, so the combined system
performs an analog computation. However, a full analysis of the computational power
of the components and combinations of these NMR systems has not been done.

78
V. Kendon
Analysing the roles of the NMR substrate and the controlling computer in the overall
computationisnotstraightforward,andprovidespartofthemotivationfordeveloping
a set of tools for this purpose. Tools for more speciﬁc settings have grown out of these
early experiments. Dale et al. [23] provide a framework for analysing substrates used
in reservoir computing, and Russell and Stepney [42] provide general methods for
obtaining speed limits on computing using ﬁnite resources.
4
Abstraction-Representation Theory
The most basic question, that needs to be answered ﬁrst, is whether a computational
system is actually computing at all. To facilitate answering this question for diverse
physical systems, Horsman et al. [28] developed a framework, known as abstraction-
representation theory (ART), in which science, engineering, computing, and use
of technology are all modeled as variations on the same diagrams. This highlights
the relationships between the different activities, and allows clear distinctions to be
made between experiments on potential new computational substrates, engineering a
computer using a new substrate, and performing a computation on a computer made
with the substrate. The ART framework facilitated the identiﬁcation of some basic
pre-requisites for computing to be taking place, including the need for a computa-
tional entity for which the computation is meaningful or purposeful in some way.
The computational entity doesn’t have to be human, nor distinct from the computer:
Horsman et al. [29] explain how bacteria carry out simple computations in the course
of their search for food. The ART framework was further developed in Horsman et
al. [30], and applied to a variety of computational edge cases, such as slime mould
and black holes.
Figure3 illustrates the basic scientiﬁc process as an AR theory commuting dia-
gram. An experimental procedure H(p) is performed to test a model CT (mp) in
theory T . The results of the experiment are compared with the theory and any dif-
ference ϵ considered. If ϵ is too large to be explained by experimental precision or
other known factors, then the theory needs to be improved. Science is about creating
theories that describe the physical world well.
Fig. 3 Science in the AR theory framework

Anti-heterotic Computing
79
Fig. 4 Engineering in the AR theory framework
Fig. 5 Computing in the AR theory framework
Figure4 illustrates the engineering process as an AR diagram. Once we have a
good enough theory (small enough ϵ), we can build something useful with properties
we trust because of our well-tested scientiﬁc theory. If it turns out that our product
doesn’t perform well enough, we need to change the physical system so that it ﬁts
our design speciﬁcation more closely. Engineering is about making physical devices
that match our design speciﬁcations.
Among the many things we build are computers, which we trust to perform our
calculations. This trust is based on the prior scientiﬁc experiments to develop and test
the theories, and engineering processes that meet the design speciﬁcations. These
are, of course, not a guarantee that the computer will perform correct computations,
but that is a topic well beyond the scope of this paper. Figure5 illustrates the process
of using a computer to calculate a computation c. The main extra step is the encoding
of the problem c into the model of the computer mp, and subsequent decoding to
obtain the result c′. Note that the commuting nature of the diagram is now assumed
in order to carry out the computation, the abstract link between c and c′ is shown
dotted, but not carried out. Computing is the use of an engineered physical system
to solve an abstract problem. Engineered is used in a broad sense that includes the
evolutionary development of brains and bacteria.
The AR theory framework relates science, engineering, technology, and comput-
ing by providing a framework for reasoning about these processes without needing

80
V. Kendon
to ﬁrst solve all the philosophical questions about exactly how the scientiﬁc method
works. It also is not a “dualist” theory, despite the physical and abstract realms
labeled in the diagrams. Everything in the models is physically instantiated, it is
just the roles the models are playing in relation to other physical objects that differ.
The model mp of a physical particle p is instantiated in the pages of a text book, in
the brain states of the students who learn about it, on the blackboard as the lecturer
describes it. What makes it abstract is that these instantiations have a degree of
arbitrariness about them, and can change, while still representing the same model.
The AR theory diagrams focus on the key relationships for understanding science
and computing, and avoid having to deal with too many layers of recursion of mod-
els of models. When necessary, the computational entity can be explicitly included
in the AR diagrams [31], making the relationships three-dimensional (cubic) rather
than the two-dimensional squares in Figs.3, 4 and 5. The AR diagrams also extend
smoothly to reﬁnement layers [28], thus providing a link to practical programming
tools and their role in the computing process.
5
Theoretical Tools for Heterotic Computing
As outlined in Sect.4, AR theory analyses whole systems that compute. More tools
are needed to analyse compositions of different systems, to determine if, and how,
the combinations provide different computational power than the parts. There are
many ways in which a combination of two or more different systems can provide an
advantage, besides increased computational power. It can also be better optimised
for a particular task, by matching the natural system dynamics to the problem. Or
it can be more robust against errors, or have a lower power consumption. It may
even provide privacy or security guarantees for distributed computing [12]. Of great
practical importance is decreasing the power consumed by computation: this can be
achieved by extracting more computational power from the same systems.
A more efﬁcient representation of the data can also provide advantages for well-
matched problems and computers. This was the inspiration for the ﬁrst proposed
application of quantum computers: simulation of one quantum system by other quan-
tum system [24]. The state space of a quantum system is exponentially large, and
best represented by another quantum system of similar dimensions. Complex prob-
lems may require several different types of variables (discrete; continuous), making
a computer with several types of registers and processors a natural approach.
A framework for analysing heterotic computers needs to be capable of quantifying
the computational and representational power of each component, so that meaning-
ful comparisons can be made across data types. The standard method of mapping
everything to a Turing machine is not suitable, because it fails to account for the
efﬁciencies gained through using a natural representation for different data types
in different computational substrates. The distinctions between different data types
need to be retained in order to fully exploit the matching computational substrates.

Anti-heterotic Computing
81
Fig. 6 The simplest non-trivial composition of two different computational systems B and C. Note
that both are in the physical part of an AR theory diagram, the corresponding abstract models are
not shown. The transduction is required if the input and output formats differ between B and C
The nature of the connections between the components is also a core feature of any
framework for modelling heterotic computers. If two components use different data
encodings, then there will need to be signal transduction to convert the data formats.
At a high level of abstraction, these connections look like communication channels
(identity operations on the logical data). However, at a lower level, any non-trivial
dataencodingtransductionwillinvolvenon-trivialcomputation,andhencecontribute
to the overall resource requirements and computational power of the system. A full
accounting of all the computation in a hybrid device is crucial for fair comparisons
to be made between disparate computational systems.
Figure6 shows the simplest non-trivial composition of two different computa-
tional systems, with the output of one becoming the input to the other, and vice versa
after internal operations COp and BOp update the internal states of C and B to C′
and B′ respectively. If the input and output formats don’t match, additional signal
transduction steps are required. These in general involve computation that must be
accounted for. Even if the two components B and C are identical, and the transduc-
tion is not required, we still gain computational power in the sense that we now have
a larger computer capable of solving larger problems than if we have two identical
computers that are not connected.
From a modelling point of view, we can require that all connections match in
terms of data format, and insert extra computational units between non-matching data
connections to convert the format. This ensures all transduction steps are recorded
as computation in the process. Even when transduction is accounted for, there are
furtherissues,suchastimescales,inconnectingdifferentcomputationalcomponents.
Stepney et al. [45] have proposed a heterotic system combining optical, bacterial and
chemical computing as a proof-of-concept exploration of the very different time
scales in these systems, but with direct interfaces that do not require a classical
computer to mediate.
A “matching plug” model like this is crying out for a graphical applied category
theory realisation. For examples of such systems in a quantum context, see Coecke
and Kissinger [19]—this demonstrates that such models are powerful enough to
represent quantum computers. Hence, a category theory approach should be both
powerful and ﬂexible enough for current and near future hybrid computer applica-
tions.

82
V. Kendon
Fig. 7 The step by step interactions between a computational substrate (state B, state change BOp)
and a controller (state C, state change COp): the input to one is the output from the other (assuming
no signal transduction needed)
As already noted, connections between components usually go beyond data
exchange to include control instructions. The MBQC example in Sect.2 has one
component controlling another. The basic control system is a feedback loop in which
the output of the substrate is processed by the control to determine the instruction
passed to the substrate for the next round of the loop. This is illustrated in Fig.7.
The dependence of each instruction on the output from the previous operation means
that the control sequence cannot be compressed into parallel operations by duplicat-
ing the substrate units. Control systems also have well-developed category theory
models that can be harnessed for heterotic computing analysis. The difference in a
computational setting is that the goal of the process is an abstract computation, rather
than a physical system or process.
As well as one component controlling a substrate, two components can also inter-
act such that the states of both components are updated. This is the norm for quantum
systems, where interaction always has a “back action” associated with it. This back
action can be harnessed for computation, and forms the basis of operation of the
ancilla-mediated quantum gates described in Sect.2.
A category theory framework provides a tool for analysing and understanding
hybrid computational devices and the computational capabilities they provide. From
this, it is then necessary to create tools for using such devices, that handle the different
types of computation they support, and their interactions. As already advocated [46],
building on the existing tools of reﬁnement, retrenchment [5–8], initialisation and
ﬁnalisation [18, 20] are a practical route to develop tools for programmers. This
would include analysis of propagation of errors [13] e.g., due to noise, and to drift,
and techniques for correction and control of these errors.
6
Summary and Future Directions
Use of hybrid and unconventional computational devices is expanding rapidly as
computers diversify to gain performance through other routes besides smaller com-
ponents. While pioneers like Stepney have been studying unconventional hybrid
systems for many years, developing a theoretical framework to analyse and under-
stand diverse combinations of computational devices is now becoming more urgent

Anti-heterotic Computing
83
and important. There are many subtleties that arise when composing devices with
the goal of obtaining a heterotic advantage: this chapter has outlined a few of them,
and proposed some directions for modelling them effectively.
The AR theory framework reviewed in Sect.4 provides the tools for the ﬁrst step,
understanding the development process from a new material to using a computer
made from it. To incorporate unconventional physical substrates as computational
devices, we need to clearly identify the computation the device is capable of perform-
ing for the purpose we want to use it for. Experiments to determine the computational
capabilities of the substrate are an essential part of the development process, in which
the anti-heterotic system has to be dissected to separate the contributions of the clas-
sical control computers from the substrate. While some progress has been made in
this direction (e.g., Dale et al. [23]), there is much more to be done before we can
conﬁdently hand a hybrid unconventional computer to a programmer and expect
efﬁcient computing to happen with it. Nonetheless, the heterotic approach is crucial
to ensure that the many forms of unconventional computation can be exploited fully.
Different components should be combined with each doing what it does naturally,
and best, for the whole to be a more powerful and efﬁcient computer.
Acknowledgements VK funded by the UK Engineering and Physical Sciences Research Council
Grant EP/L022303/1. VK thanks Susan Stepney for many stimulating and productive hours of
discussions on these subjects and diverse other topics. And for the accompanying stilton scones.
References
1. Adamatzky, A.: Physarum machines: encapsulating reaction-diffusion to compute spanning
tree. Naturwissenschaften 94(12), 975–980 (2007)
2. Amos, M.: Theoretical and Experimental DNA Computation. Springer, Berlin (2005)
3. Anders, J., Browne, D.: Computational power of correlations. Phys. Rev. Lett. 102, 050502
(2009)
4. Anders, J., Oi, D.K.L., Kasheﬁ, E., Browne, D.E., Andersson, E.: Ancilla-driven universal
quantum computation. Phys. Rev. A 82(2), 020301 (2010)
5. Banach, R., Poppleton, M.: Retrenchment: an engineering variation on reﬁnement. In: 2nd
International B Conference. LNCS, vol. 1393, pp. 129–147. Springer (1998)
6. Banach, R., Jeske, C., Fraser, S., Cross, R., Poppleton, M., Stepney, S., King, S.: Approaching
the formal design and development of complex systems: the retrenchment position. In: WSCS,
IEEE ICECCS’04 (2004)
7. Banach, R., Jeske, C., Poppleton, M., Stepney, S.: Retrenching the purse. Fundam. Informaticae
77, 29–69 (2007)
8. Banach, R., Poppleton, M., Jeske, C., Stepney, S.: Engineering and theoretical underpinnings
of retrenchment. Sci. Comput. Program. 67(2–3), 301–329 (2007)
9. Bartlett, S., Sanders, B., Braunstein, S.L., Nemoto, K.: Efﬁcient classical simulation of con-
tinuous variable quantum information processes. Phys. Rev. Lett. 88, 097904 (2002)
10. Bechmann, M., Sebald, A., Stepney, S.: From binary to continuous gates—and back again. In:
ICES 2010, 335–347 (2010)
11. Bechmann, M., Sebald, A., Stepney, S.: Boolean logic-gate design principles in unconventional
computers: an NMR case study. Int. J. Unconv. Comput. 8(2), 139–159 (2013)
12. Ben-Or, M., Crepeau, C., Gottesman, D., Hassidim, A., Smith, A.: Secure multiparty quantum
computation with (only) a strict honest majority. In: 2006 47th Annual IEEE Symposium on

84
V. Kendon
Foundations of Computer Science (FOCS’06), pp. 249–260 (2006). https://doi.org/10.1109/
FOCS.2006.68
13. Blakey, E.: Unconventional complexity measures for unconventional computers. Nat. Comput.
10, 1245–1259 (2010). https://doi.org/10.1007/s11047-010-9226-9
14. Braunstein, S.L., Caves, C.M., Jozsa, R., Linden, N., Popescu, S., Schack, R.: Separability of
very noisy mixed states and implications for nmr quantum computing. Phys. Rev. Lett. 83,
1054–1057 (1999). https://doi.org/10.1103/PhysRevLett.83.1054
15. Brown, K.L., De, S., Kendon, V., Munro, W.J.: Ancilla-based quantum simulation. New J.
Phys. 13, 095007 (2011)
16. Brown, K.L., Horsman, C., Kendon, V.M., Munro, W.J.: Layer by layer generation of cluster
states. Phys. Rev. A 85, 052305 (2012)
17. Browne, D., Kasheﬁ, E., Perdrix, S.: Computational depth complexity of measurement-based
quantum computa tion. In: van Dam, W., Kendon, V.M., Severini S. (eds.) TQC 2010. LNCS,
vol 6519, pp. 35–46. Springer (2010)
18. Clark, J.A., Stepney, S., Chivers, H.: Breaking the model: ﬁnalisation and a taxonomy of
security attacks. ENTCS 137(2), 225–242 (2005)
19. Coecke, B., Kissinger, A.: Picturing Quantum Processes—A First Course in Quantum Theory
and Diagrammatic Reasoning. Cambridge University Press, UK (2017). ISBN 9781107104228
20. Cooper, D., Stepney, S., Woodcock, J.: Derivation of Z reﬁnement proof rules: forwards
and backwards rules incorporating input/output reﬁnement. Technical Report YCS-2002-347,
Department of Computer Science, University of York (2002)
21. Cory, D.G., Laﬂamme, R., Knill, E., Viola, L., Havel, T.F., Boulant, N., Boutis, G., Fortunato,
E., Lloyd, S., Martinez, R., Negrevergne, C., Pravia, M., Sharf, Y., Teklemariam, G., Weinstein,
Y.S., Zurek, W.H.: NMR based quantum information processing: achievements and prospects.
Fortschritte der Phys. 48(9–11), 875–907 (2000)
22. Cory, D.G., Fahmy, A.F., Havel, T.F.: Ensemble quantum computing by NMR spectroscopy.
Proc. Natl. Acad. Sci. 94(5), 1634–1639 (1997). https://doi.org/10.1073/pnas.94.5.1634
23. Dale, M., Miller, J.F., Stepney, S., Trefzer, M.A.: A substrate-independent framework to char-
acterise reservoir computers (2018). CoRR arXiv:1810.07135
24. Feynman, R.P.: Simulating physics with computers. Intern. J. Theoret. Phys. 21(6/7), 467–488
(1982)
25. Gershenfeld, N.A., Chuang, I.L.: Bulk spin-resonance quantum computation. Science
275(5298), 350–356 (1997). https://doi.org/10.1126/science.275.5298.350
26. Graca, D.S.: Some recent developments on Shannon’s GPAC. Math. Log. Q. 50(4–5), 473–485
(2004)
27. Horsman, C., Brown, K.L., Munro, W.J., Kendon, V.M.: Reduce, reuse, recycle for
robust cluster-state generation. Phys. Rev. A 83(4), 042327 (2011). https://doi.org/10.1103/
PhysRevA.83.042327
28. Horsman, C., Stepney, S., Wagner, R.C., Kendon, V.: When does a physical system compute?
Proc. R. Soc. A 470(2169), 20140182 (2014)
29. Horsman, D., Kendon, V., Stepney, S., Young, P.: Abstraction and representation in living
organisms: when does a biological system compute? In: Dodig-Crnkovic, G., Giovagnoli R.
(eds.) Representation and Reality: Humans, Animals, and Machines, vol. 28, pp. 91–116.
Springer (2017)
30. Horsman, D., Kendon, V., Stepney, S.: Abstraction/representation theory and the natural science
of computation. In: Cuffaro, M.E., Fletcher, S.C. (eds.) Physical Perspectives on Computation,
Computational Perspectives on Physics, pp. 127–149. Cambridge University Press, Cambridge
(2018)
31. Horsman, D., Clarke, T., Stepney, S., Kendon, V.: When does a control system compute? The
case of the centrifugal governor (2019). In preparation
32. Jones, J.A.: Quantum computing with NMR. Prog. Nucl. Magn. Reson. Spectrosc. 59, 91–120
(2011)
33. Jozsa,
R.:
An
introduction
to
measurement
based
quantum
computation
(2005).
arXiv:quant-ph/0508124

Anti-heterotic Computing
85
34. Kendon, V., Sebald, A., Stepney, S., Bechmann, M., Hines, P., Wagner, R.C.: Heterotic com-
puting. In: Unconventional Computation. LNCS, vol. 6714, pp. 113–124. Springer (2011)
35. Knill, E., Laﬂamme, R., Milburn, G.J.: A scheme for efﬁcient quantum computation with linear
optics. Nature 409(6816), 46–52 (2001). https://doi.org/10.1038/35051009
36. Kuhnert, L., Agladze, K., Krinsky, V.: Image processing using light-sensitive chemical waves.
Nature 337, 244–247 (1989)
37. Lloyd, S., Braunstein, S.L.: Quantum computation over continuous variables. Phys. Rev. Lett.
82, 1784 (1999)
38. Mills, J.W.: The nature of the extended analog computer. Phys. D Nonlinear Phenom. 237(9),
1235–1256 (2008). https://doi.org/10.1016/j.physd.2008.03.041
39. Motoike, I.N., Adamatzky, A.: Three-valued logic gates in reaction-diffusion excitable media.
Chaos Solitons Fractals 24(1), 107–114 (2005)
40. Raussendorf, R., Briegel, H.J.: A one-way quantum computer. Phys. Rev. Lett. 86, 5188–5191
(2001)
41. Roselló-Merino, M., Bechmann, M., Sebald, A., Stepney, S.: Classical computing in nuclear
magnetic resonance. Int. J. Unconv. Comput. 6(3–4), 163–195 (2010)
42. Russell, B., Stepney, S.: The geometry of speed limiting resources in physical models of
computation. Int. J. Found. Comput. Sci. 28(04), 321–333 (2017). https://doi.org/10.1142/
S0129054117500204
43. Spiller, T.P., Munro, W.J., Barrett, S.D., Kok, P.: An introduction to quantum information
processing: applications and realisations. Contemp. Phys. 46, 407 (2005)
44. Stepney, S.: The neglected pillar of material computation. Phys. D Nonlinear Phenom. 237(9),
1157–1164 (2008)
45. Stepney, S., Abramsky, S., Bechmann, M., Gorecki, J., Kendon, V., Naughton, T.J., Perez-
Jimenez, M.J., Romero-Campero, F.J., Sebald, A.: Heterotic computing examples with optics,
bacteria, and chemicals. In: 11th International Conference on Unconventional Computation
and Natural Computation 2012 (UCNC 2012), Oréans, France. Lecture Notes in Computer
Science, vol. 7445, pp. 198–209. Springer, Heidelberg (2012)
46. Stepney, S., Kendon, V., Hines, P., Sebald, A.: A framework for heterotic computing. In: 8th
Workshop on Quantum Physics and Logic (QPL 2011) Nijmegen, Netherlands, EPTCS, vol. 95,
pp. 263–273 (2012)
47. Tóth, Á., Showalter, K.: Logic gates in excitable media. J. Chem. Phys. 103, 2058–2066 (1995)
48. Tucker, R.S.: The role of optics in computing. Nat. Photonics 4, 405 (2010). https://doi.org/
10.1038/nphoton.2010.162
49. Woods, D., Naughton, T.J.: Parallel and sequential optical computing. In: Optical SuperCom-
puting. LNCS, vol. 5172, pp. 70–86. Springer (2008)

Visual Analytics
Ian T. Nabney
Abstract We are in a data-driven era. Making sense of data is becoming increas-
ingly important, and this is driving the need for systems that enable people to analyze
and understand data. Visual analytics presents users with a visual representation of
their data so that they make sense of it: reason about it, ask important questions,
and interpret the answers to those questions. This paper shows how effective visual
analytics systems combine information visualisation (to visually represent data) with
machine learning (to project data to a low-dimensional space). The methods are illus-
trated with two real-world case studies in drug discovery and condition monitoring
of helicopter airframes. This paper is not the ﬁnal word on the subject, but it also
points the way to future research directions.
1
Introduction
In a volume of this sort, it seems ﬁtting to start this paper with my memories of
working with Susan. We were both at Logica Cambridge, which was the research lab
of the software consulting company Logica, absorbed into CGI a number of years
ago. My work there focused on neural networks and other forms of rule induction, but
my background was in pure mathematics. Susan recruited me to work with a small
team developing a veriﬁably correct compiler for a subset of Pascal for a client. The
results of that work are contained in a series of technical reports [12–14].
The heavy lifting on the key ideas had already been developed by Susan in an
earlier project for a much smaller language: my role was to take those ideas and
specify mathematically the high-level language, the low level language, and all the
checks and compilation steps, and then to prove that the transformations were cor-
rect. This was no small task: the processor was quite limited (being 8-bit), so even
16-bit integer division required a signiﬁcant body of code (more than 100 lines
from memory) to implement. Any thoughts I had that I might be the ‘rigorous’
I. T. Nabney (B)
School of Engineering, University of Bristol, Clifton, Bristol BS8 1UB, UK
e-mail: ian.nabney@bristol.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_4
87

88
I. T. Nabney
mathematician compared to Susan’s ‘empirical’ physicist were soon put aside given
the immense care over the detail that Susan took in checking my work.
Those hopes (of impressing her) were also dashed by a story that Susan, character-
istically, told against herself. Before joining Logica, she had worked for GEC, where
her colleagues had designed a new unit of pedantry: the Stepney. Unfortunately, it
was useless for practical purposes, because the pedantry of anyone other than Susan
could only be measured in micro-Stepneys. I consoled myself with the thought that
while my precision was not measurable in a Stepney order-of-magnitude, I at least
rated at one milli-Stepney, or possibly two on a good day.
Since leaving Logica myself and joining Aston University and more recently the
University of Bristol, I haven’t revisited my formal methods work, but have concen-
trated on machine learning instead. Searching for a topic that would be appropriate
for this collection, I decided on writing about ‘Visual Analytics’ which if not ‘non-
standard computation’ does at least provide users with the possibility of reasoning
about the results of computation in a different way.
The structure of this paper is as follows. The second section deﬁnes visual ana-
lytics and explains why it is of great relevance today. The third section describes the
relevance of machine learning to visual analytics both in core data projection models
and further interaction and analysis that they support. These methods are illustrated
by two real-world case studies in the fourth section, while the ﬁnal section presents
conclusions and future work.
2
Visual Analytics
We are in a data-driven era. Increasingly many domains of our everyday life generate
and consume data. People have the potential to understand phenomena in more depth
using new data analysis techniques. Additionally, new phenomena can be uncovered
in domains where data is becoming available. Thus, making sense of data is becoming
increasingly important, and this is driving the need for systems that enable people to
analyze and understand data.
However, this opportunity to discover also presents challenges. Reasoning about
data is becoming more complicated and difﬁcult as data types and complexities
increase. People require powerful tools to draw valid conclusions from data, while
maintaining trustworthy and interpretable results.
Visual analytics (VA) is a multi-disciplinary domain that combines information
visualization with machine learning (ML) and other automated techniques to create
systems that help people make sense of data [5]. Visual analytics presents people
with a visual representation of their data so that they make sense of it: reason about it,
ask important questions, and interpret the answers to those questions. Visualisation
is an important tool for developing a better understanding of large complex datasets.
It is particularly helpful for users who are not specialists in data modelling. Typical
tasks that users can carry out include:

Visual Analytics
89
Fig. 1 A word cloud for text drawn from Primo Levi’s book ‘If this is a Man’. ‘One must understand
Man’ could serve as a motto for the book. Figure produced using wordle.net
• detection of outliers (ﬁnding data points that are different from the norm);
• clustering and segmentation (identifying important groups within the data);
• aid to feature selection (identifying a subset of the features that provides the most
information);
• feedback on results of analysis (improving the fundamental task performance by
seeing what you are doing).
There are two important components to visual analytics: information visualisation
and data projection.
Information visualisation refers to a wide range of techniques for visually rep-
resenting data. This data can be of many different types: numeric, discrete, spatial,
temporal, textual, image, etc. A word cloud (see Fig. 1) is used to show the relative
frequencies of words in a text document: the size of the word in the cloud is pro-
portional to its frequency in the document. It is usual to set a lower bound on the
frequency, to remove common stop words (such as ‘the’), and to stem the words (i.e.
map all variants of a word to a single term, such as ‘move’ for ‘moving’, ‘moved’,
etc.).
The key limitation of information visualisation is that it is limited to data with
a small number of variables. For example, word cloud visualises a single variable
(word frequency). Even with a great deal of ingenuity using symbols, colours and
three-dimensional plots, it is very hard to provide a really interpretable display of
data above, say, ﬁve dimensions. And yet, many of the most interesting and difﬁcult
challenges we face are inherently multi-dimensional or are described by very high-
dimensional data. Clearly, information visualisation alone is not enough to tackle
these challenges.
In data projection, the goal is to project higher-dimensional data to a lower-
dimensional space (usually 2d or 3d) while preserving as much information or
structure as possible. Once the projection is done standard information visualisa-
tion methods can be used to support user interaction. These may need to be modiﬁed
for particularly large datasets. The quantity and complexity of many datasets means

90
I. T. Nabney
that simple or classical visualisation methods, such as Principal Component Analysis,
are not very effective. The typical result of applying them is that the data is conﬁned
to a very dense circular blob in the middle of the plot, which leaves little scope for
discovering the deeper relationships between individual points and groupings within
the data.
3
Machine Learning and Data Projection
3.1
Uncertainty
Doubt is not a pleasant condition, but certainty is absurd.
Voltaire1
Real data is uncertain: there is measurement error, missing data (i.e. unrecorded),
censored data (values that are clipped when they pass a threshold), etc. We are forced
to deal with uncertainty, yet we need to be quantitative: typically this means providing
our best prediction with some measure of our uncertainty. The optimal formalism for
inference in the presence of uncertainty is probability theory [4] and we assume the
presence of an underlying regularity to make predictions. Bayesian inference allows
us to reason probabilistically about the model as well as the data [2].
3.2
Linear Models
The simplest way to project data is a linear map from the dataspace to a lower-
dimensional space. To choose between all the different possible linear maps, we
need to determine an appropriate criterion to optimise. In this application we want to
preserve as much information as possible. If we assume that information is measured
by variance this implies choosing new coordinate axes along directions of maximal
data variance; these can be found by analysing the covariance matrix of the data.
This method is called Principal Component Analysis (PCA): see [7].
Let S be the covariance matrix of the data, so that
Si j = 1
N

n
(xn
i −xi)(xn
j −x j)
The ﬁrst q principal components are the ﬁrst q eigenvectors w j of S, ordered by the
size of the eigenvalues λ j. The percentage of the variance explained by the ﬁrst q
principal components is
1In the original French: Le doute n’est pas une état bien agréable, mais l’assurance est un état
ridicule.

Visual Analytics
91
q
j=1 λ j
d
j=1 λ j
where the data dimension is d. These vectors are orthonormal (perpendicular and
unit length). The variance when the data is projected onto them is maximal.
For large datasets, the end result is usually a circular blob in the middle of the
screen. The reason is that we can also view PCA as a generative model. Classical
PCA is made into a density model by using a latent variable approach, derived from
standard factor analysis, in which the data x is generated by a linear combination of
a number of hidden variables z
x = Wz + μ + ϵ,
(1)
where z has a zero mean, unit isotropic variance, Gaussian distribution N(0, I), μ
is a constant (whose maximum likelihood estimator is the data mean), and ϵ is an
x-independent noise process. The fact that the probabilistic component of the model
is a single spherical Gaussian explains the uninformative nature of the visualised
data. To do better than this, we need to use a more complex generative model.
3.3
Generative Models
The projection approach to visualisation is a way of reducing the data complexity in
a direct fashion. An alternative view is to hypothesise how the data might have been
generated from a lower-dimensional space.
A hidden connection is stronger than an obvious one.
Heraclitus
We consider separately the observed variables (i.e. the data) and latent variables
which generate observations. We then use (probabilistic) inference to deduce what
is happening in latent variable space from the observations. In such latent models,
we often use Bayes’ Theorem:
P(L|O) = P(O|L) P(L)
P(O)
,
where L refers to latent variables and O refers to observed variables. A large number
of interestingdatamodels canbeviewedinthis framework, includingHiddenMarkov
Models and State-Space Models for time series data [2]. Here we will focus on the
case of static data and the Generative Topographic Mapping [3].
The Generative Topographic Mapping (GTM) is a non-linear probabilistic data
visualisation method that is based on a constrained mixture of Gaussians, in which
the centres of the Gaussians are constrained to lie on a two-dimensional space.

92
I. T. Nabney
Latent space
Data space
z2
z1
x1
x
x3
2
y(z;W)
Fig. 2 The Generative Topographic Mapping
In the GTM, a D-dimensional data point (x1, . . . , xD) is represented by a point
in a lower-dimensional latent-variable space t ∈Rq (with q < D). This is achieved
using a forward mapping function x = y(t; W) which is then inverted using Bayes’
theorem. This function (which is usually chosen to be a radial-basis function (RBF)
network) is parameterised by a network weight matrix W. The image of the latent
space under this function deﬁnes a q-dimensional manifold in the data space (Fig.2).
To induce a density p(y|W) in the data space, a probability density p(t) is deﬁned
on the latent space. Since the data is not expected to lie exactly on the q-dimensional
manifold, a spherical Gaussian model with inverse variance β2 is added in the data
space so that the conditional density of the data is given by
p(x|t, W, β) =

β
√(2π)
D
exp

−(β||y(t; W) −x||)2
2

.
(2)
To get the density of the data space, the hidden space variables must be integrated
out
p(x|W, β) =

p(x|t, W, β)p(t) dt.
(3)
In general, this integral would be intractable for a non-linear model y(t; W). Hence
p(t) is deﬁned to be a sum of delta functions with centres on nodes t1, . . . , tK in the
latent space
p(t) = 1
M
M

i=1
δ(t −ti).
(4)
This can be viewed as an approximation to a uniform distribution if the nodes are
uniformly spread. Now Eq. (3) can be written as
p(x|W, β) = 1
K
K

i=1
p(x|ti, W, β).
(5)

Visual Analytics
93
Fig. 3 Schematic of GTM-FS (GTM with feature saliency). d1 and d2 have high saliency, d3 has
low saliency
This is a mixture of K Gaussians with each kernel having a constant mixing coef-
ﬁcient 1/K and inverse variance β2. The ith centre is given by y(ti; W). As these
centres are dependent and related by the mapping, it can be viewed as a constrained
Gaussian Mixture Model (GMM): see Fig. 3. The model is trained in a maximum
likelihood framework using an iterative algorithm (EM). Provided y(t; W) deﬁnes a
smooth mapping, two points t1 and t2 which are close in the latent space are mapped
to points y(t1; W) and y(t2; W) which are close in the data space.
We can provide more insight into the visualisation in a number of ways. The
mapping of the latent space stretches and twists the manifold. This can be measured
using methods from differential geometry and plotted as a background intensity:
magniﬁcation factors measure the stretch of the manifold, and directional curvatures
show the magnitude and direction of the main curvature [16].
Other mechanisms are more interactive, and use methods drawn from the ﬁeld of
information visualisation. Parallel coordinates [6] maps d-dimensional data space
onto two display dimensions by using d equidistant axes parallel to the y-axis. Each
datapointisdisplayedasapiecewiselineargraphintersectingeachaxisattheposition
corresponding to the data value for that dimension. It is impractical to display this
for all the data points, so allow the user to select a region of interest. The user can
also interact with the local parallel coordinates plot to obtain detailed information.
There are a number of extensions to the basic GTM that allow users to visualise
a wider range of datasets:
• Temporal dependencies in data handled by GTM through Time.
• Discrete data handled by Latent Trait Model (LTM): in fact, heterogeneous datasets
containing both discrete and continuous data can be modelled.
• Missing data can be handled in training and visualisation.
Here we will consider just two extensions: hierarchies and feature selection.

94
I. T. Nabney
3.3.1
Hierarchical Models
The GTM assumes that the data lies ‘close’ to a two dimensional manifold; however,
this is likely to be too simple a model for many datasets. Hierarchical GTM [15]
allows the user to drill down into data with either user-deﬁned or automated (using
a minimum message-length MML criterion) selection of sub-model positions.
Bishop and Tipping [17] introduced the idea of hierarchical visualisation for
probabilistic PCA. A general framework for arbitrary latent variable models has been
developed from this. Because GTM is a generative latent variable model, it is possible
to train hierarchical mixtures of GTMs. We model and visualise the whole data set
with a GTM at the top level, which is broken down into clusters at deeper levels of
the hierarchy. Because the data can be visualised at each level of the hierarchy, the
selection of clusters, which are used to train GTMs at the next level down, can be
carried out interactively by the user.
3.3.2
Feature Selection
Feature selection is harder for unsupervised learning than for supervised learning
as there is no obvious criterion to guide the search. Instead of selecting a subset of
features, we estimate a set of real-valued (in [0, 1]) variables (one for each feature);
feature saliencies. We adopted a minimum message length (MML) penalty for model
selection (based on a similar approach for GMMs: [8]).
Instead of a mixture of spherical Gaussians, as in standard GTM, we use a mixture
of diagonal Gaussians. For each feature d = {1, . . . , D}, we ﬂip a biased coin whose
probability of a head is ρd; if we get a head, we use the mixture component p(· · · |θmd)
to generate the dth feature; otherwise, the common density q(· · · |λd) is used.
GTM-FS associates a variation measure with each feature by using a mixture
of diagonal-covariance Gaussians. This assumes that the features are conditionally
independent. The dth feature is irrelevant if its distribution is independent of the
component labels, i.e. if it follows a common density, denoted by q(xnd|λd) which
is deﬁned to be a diagonal Gaussian with parameters λd. Let { = (ψ1, . . . , ψD)}
be an ordered set of binary parameters such that ψd = 1 if the dth feature is relevant
and ψd = 0 otherwise. Now the mixture density is
p(xn|
) =
K

k=1
1
K
D

d=1
[p(xnd | βkd)]ψd[q(xnd | λd)](1−ψd).
(6)
The value of the feature saliencies is obtained by ﬁrstly treating the binary values
in the set  as missing variables in the EM algorithm and then deﬁning it by a
probability pd that a particular feature is relevant (ψd = 1). Cheminformatics data
from was analysed in [9] using GTM, GTM-FS and SOM. In GTM and GTM-FS, the
separation of data clusters was better while GTM-FS showed more compact results
because the irrelevant features were projected using a different distribution.

Visual Analytics
95
3.4
Metric-Based Models
The basic aim of metric-based models is to deﬁne a mapping from the data space
to a visualisation space that preserves inter-point distances, i.e. that distances in the
visualisation space are as close as possible to those in original data space. Given a
dissimilarity matrix di j (which is usually deﬁned by Euclidean distance in the original
data space), the aim is to map data points xi to points yi in a feature space such that
their dissimilarities in feature space, ˜di j, are as close as possible to the di j. We say
that the map preserves similarities.
The stress measure is used as objective function
E =
1

i j di j

i< j

di j −˜di j
	2
di j
.
In classical scaling, the distance between the objects is assumed to be Euclidean.
A linear projection then corresponds to PCA.
The Sammon mapping [11] does not actually deﬁne a map: instead it ﬁnds a point
yi for each point xi to minimise stress. Essentially, this is a lookup table: if there is
any change to the dataset the entire lookup table must be learned again from scratch.
Neuroscale [18] is a neural network-based scaling technique that has the advantage
of actually giving a map (Fig.4).
Fig. 4 Neuroscale network architecture

96
I. T. Nabney
3.5
Evaluation
There is a need to compare the ‘quality’ of different visualisations. This matters to
algorithm developers to demonstrate the value (or lack of it) of new techniques. It
also matters to visual analytics practitioners since they are likely to generate multiple
visualisations (parameter settings, different visualisation methods), sometimes in the
thousands, and need to choose between them or guide the process of visualisation.
This is a challenging task: see [1]. Dimensionality reduction is an inherently
unsupervised task: there is no ground truth or gold standard and so there is no
single globally correct deﬁnition of what quality means. Instead, there is a wide vari-
ety of dimensionality reduction methods with different assumptions and modelling
approaches.
Inpractice, visualisationis usedbothfor speciﬁctasks but alsodata-drivenhypoth-
esis formation. Any quality metrics should measure what the user requires the visu-
alization for:
• accurate representation of data point relationships (local/global);
• good class separation;
• identiﬁcation of outliers;
• reduction of noise
• ‘understanding’ data—perception of data characteristics;
• choosing how to represent data.
One way to approach this task is to use user-based evaluation, either evaluating user
performance at a task based on visualisation or evaluating the user experience of the
visualisation process.
The key challenge is to gather enough meaningful data to make sound judgements.
Humans are not good at quantifying what they see (e.g. is one plot more structured
than another?). Thus, given this inherent unreliability of user judgement, arriving at
a reliable metric may require a very large number of users. As a result, most studies
are very under-powered (in the statistical sense). For example, a user study in [10]
that compared two information visualisation methods on three different tasks had just
ﬁve subjects, all of whom were graduate students. Where user trials can be valuable
is in providing richer qualitative data (e.g. reaction cards, choosing cards/words to
reﬂect UX).
A more fruitful approach is to use metric-based evaluation. This can be of three
types: model-based; unsupervised learning metrics; and task-based metrics.
Most models have an associated cost function. In the case of Neuroscale and multi-
dimensional scaling, this is stress; for PCA it is variance; for GTM and other latent-
variable models, it is the log likelihood of a dataset. But some models don’t have a cost
function (e.g. the self-organising map, or SOM); cost functions for different models
are incompatible; and we need some form of regularisation to compare different
architectures and parameter spaces. So, model-based metrics are only useful when
comparing a relatively narrow set of possible models.
Stress can always be calculated for a data projection, but it is questionable if it is
always relevant. Instead, we can argue that what is of most relevance to users is that

Visual Analytics
97
local neighbourhoods are preserved in the project and that larger inter-point distances
do not have to be preserved so exactly. This leads to the deﬁnition of metrics that
take account of local neighbourhood preservation.
Two exemplar visualisation quality measures based on comparing neighbour-
hoods in the data space X and projection space Z are trustworthiness and continu-
ity [19]. A mapping is said to be trustworthy if the k-neighbourhood in the visualised
space matches that in the data space but if the k-neighbourhood in the data space
matches that in the visualised space it maintains continuity.
For measuring the trustworthiness, we consider that the rX
i, j is the rank of the jth
data point from the corresponding ith data point with respect to the distance measure
in the high-dimensional data space X, and Pk(i) represents the data points in the
k-nearest neighbourhood of the ith data point in the latent space Z but not in the data
space X. Trustworthiness with k neighbours can be calculated as
1 −2
γk
N

i=1

j∈Pk(i)
(r X
i, j −k).
(7)
For measuring the continuity, we consider that the rZ
i, j is the rank of the jth data
point from the ith data point with respect to the distance measure in the visualisation
space Z and Qk(i) to be the set of data points in the k-nearest neighbourhood of the
ith data point in the data space X but not in the visualisation space Z. The continuity
with k neighbours can be calculated as
1 −2
γk
N

i=1

j∈Qk(i)
(r Z
i, j −k).
(8)
Both for trustworthiness and continuity, we take the normalising factor (γk) as
γk =

Nk(2N −3k −1)
if k < N/2,
N(N −k)(N −k −1) if k ⩾N/2,
(9)
where the deﬁnition of γk ensures that the value of trustworthiness and continuity lie
between 0 and 1. The higher the measure the better the visualisation as this implies
that local neighbourhoods are better preserved by the projection.
The third approach to quality measurement is to use objective metrics based on a
user task, but without the use of use trials or subjective experiments. These metrics
often work best in a semi-supervised way: with additional class information that is
not included in the visualisation but can be used to deﬁne a meaningful task. For
example, if the user wants the visualization to preserve class information, we can
use the nearest-neighbour classiﬁcation error (in visualization space) normalised by
its value in data space so that we are comparing performance in the original and
projection spaces.
If the user wants the visualization to be informative about class separation, we
need a measure of class separation in the visualisation space. To measure class

98
I. T. Nabney
separation, we can ﬁt a Gaussian Mixture Model to each class in visualisation space.
A variational Bayesian GMM can be used to automate the optimisation of model
complexity (i.e. number of Gaussian components). We then compute the Kullback–
Leibler divergence between all possible class pairs GMMs as a measure of overall
class separation.
DK L(P ∥Q) =

i
P(i) log P(i)
Q(i).
(10)
This divergence is not symmetric, so it is usual to compute a symmetric metric
DK L(P ∥Q) + DK L(Q ∥P). The larger the value of this metric, the better the class
separation in the visualisation, and hence, the better the visualisation is for this task.
4
Case Studies
4.1
Chemometrics
In this case study, we show how an interactive visualisation tool helped scientists at
Pﬁzer to evaluate the results of large assays in the search for new targets and drugs.
With the introduction of high-throughput screening techniques based on robotics,
it is possible to screen millions of compounds against biological targets in a short
period of time. It is thus important for scientists to gain a better understanding of the
results of multiple screens through the use of novel data visualisation and modelling
techniques. The initial task is to ﬁnd clusters of similar compounds (measured in
terms of biological activity) and using a representative subset to reduce the number
of compounds in a screen. Once these clusters have been identiﬁed, the goal is to
build local in silico prediction models.
Fig. 5 GTM plot of high-throughput screening dataset. Parallel coordinate plots show how the
characteristics of compounds vary in different parts of the dataset

Visual Analytics
99
We have taken data from Pﬁzer which consists of 6912 14-dimensional vectors
representing chemical compounds using topological indices developed at Pﬁzer. The
task is to predict lipophilicity. This is a component of Lipinski’s ‘Rule of 5’, a rule of
thumb to predict drug-likeness. The most commonly used measure of lipophilicity
is LogP, which is the partition coefﬁcient of a molecule between an aqueous and
lipophilic phases, usually octanol and water.
Plots (see Figs. 5, 6, and 7) segment the data which can be used to build local
predictive models which are often more accurate than global models. Note that we
used only 14 inputs, compared with c. 1000 for other methods of predicting logP,
while the accuracy of our results was comparable to that of global models.
Fig. 6 Hierarchical GTM plot of high-throughput screening dataset. Plots at lower levels show
structure that is obscured in the very dense top-level plot
Fig. 7 a GTM visualisation of chemometric data. b GTM-FS visualisation of chemometric data

100
I. T. Nabney
4.2
Condition Monitoring
The main objective of this project for Agusta Westland (now Leonardo) is to enhance
the HUMS for helicopter airframes by analysing signals related to structural vibra-
tion.
Vibration information during ﬂight is provided by sensors located at different
parts of the aircraft. Before structural health can be inferred, features (i.e. sensors
and frequency bands) must be chosen which provide the best information on the
state of the aircraft. These selected features are then used to infer the ﬂight modes
and eventually the health and deviations from the normal state of the aircraft. The
purpose of this case study is to show how ﬂight condition can be inferred accurately
from the vibration data and this information can be used to detect abnormalities
in the vibration signature. The data provided by AgustaWestland Ltd. is continu-
ously recorded vibration signals from 8 different sensors during ﬂight. Each sensor
measures the vibration in a particular direction at a chosen location on the aircraft.
During test ﬂights, the aircraft carries out certain planned manoeuvres and we use
the knowledge of these manoeuvres to help build the models on the labelled data. As
opposed to ﬁxed wing aircrafts, rotorcraft undergo more distinct ﬂight states such
as: steep approach, normal approach, hover, forward ﬂight etc.
Our approach is to build models using features that capture (non-stationary) fre-
quency information by applying a short-time Fourier transform. In this way, it is
possible to detect certain signatures or intensities at fundamental frequencies and
their higher harmonics. Many of the key frequencies are related to the period of
either the main or tail rotor. The intensity at these frequencies is greater during cer-
tain periods of time and these periods can be associated with ﬂight conditions and
transition periods. The frequency resolution we selected yields around 100 features
(frequency bands) for each signal. If we were to use all the features from all the sen-
sors together this would give a total of 800 features, which is too high-dimensional
for practical modelling and inference. To reduce the dimensionality, we have used
a data-driven procedure to select the sensors which provide the most relevant infor-
mation about the ﬂight conditions and select frequencies which are most relevant to
our analysis of airframe condition. This process is underpinned by data visualisation
in order to explore the dataset and understand the feature selection process better
(particularly since that process is of necessity unsupervised).
To understand the ﬂight data better, we used GTM and GTM-FS to visualise data
from individual sensors and sensor-pairs. To conﬁrm which data set has a better sep-
aration between classes, a Kullback–Leibler (KL) divergence (Eq. (10)) is calculated
for the visualisation plots. The calculated value from the KL function indicates how
much the classes in each visualisation plots are separated. The higher the resulting
number, the more seperated the individual classes are from each other. To compute
the KL-divergence, the probability density of each class is estimated by ﬁtting a
Gaussian Mixture Model in the 2D visualisation space as shown in Fig. 8.
After analysing the data with different visualisation methods, it can be concluded
that the GTM visualisation showed clear transitions between classes. However, the

Visual Analytics
101
Fig. 8 Helicopter airframe vibration data: GTM-FS visualisation of sensors 1 and 6. Note the
ellipses representing the Gaussian components of the Gaussian Mixture Model ﬁtted to each class
data points in a class were not compactly clustered. Multiple signals with multiple
ﬂight condition transitions were intended to use for GTM-FS and its log version
and ﬁnd the relevant features. It was found that the data with least irrelevant features
(noise)showedbetterseparationﬁrstvisuallyandthenevaluatedwithKL-divergence.
So, to obtain better results, the feature set should include the maximum number of
relevant features with high feature saliency.
5
Conclusions
We need to understand the vast quantities of data that surround us; visualisation and
machine learning can help us in that task. In this way, models can be used to uncover
the hidden meanings of data. Visual analytics is a powerful tool that provides insight
to non-specialists: the three case studies demonstrated how complex datasets could
be interpreted better by domain experts with the use of visualisation. It is clear that
visual analytics enables human users to understand complex multivariate data, but
that this is a multi-skilled, collaborative effort.
Future challenges:
• Visualisation is currently not as widely used as it might be because it requires
signiﬁcant expertise to set up and train models of the type discussed in this paper.
One way to mitigate this is to use Bayesian methods to (semi-)automate model
building and metrics to provide feedback on visualisation ﬁdelity.
• Visualisation will provide richer insights when data projection and expert domain
knowledge are better integrated.

102
I. T. Nabney
• On a more practical level, it is important to treat data visualisation as a true com-
ponent in data analysis. To that end, we need to develop systems that record users
analytical pathways for sharing, reproduction and audit.
• Visualisation is often an early stage in the data analysis pipeline. As such, auto-
mated and intelligent data cleansing and semantic annotation need to be combined
with data projection.
References
1. Bertini, E., Tatu, A., Keim, D.A.: Quality metrics in high-dimensional data visualization: an
overview and systematization. IEEE Trans. Vis. Comput. Graph. 17, 2203–2212 (2011)
2. Bishop, C.M.: Pattern Recognition and Machine Learning. Springer, Berlin (2006)
3. Bishop, C.M., Svensén, M., Williams, C.K.I.: GTM: the generative topographic mapping.
Neural Comput. 10(1), 215–235 (1996)
4. Cox, R.T.: Probability, frequency and reasonable expectation. Am. J. Phys. 14(1), 1–13 (1946)
5. Endert, A., Ribarsky, W., Turkay, C., Wong, B.L.W., Nabney, I.T., Blanco, I.D., Rossi, F.: The
state of the art in integrating machine learning into visual analytics. Comput. Graph. Forum
36(8), 458–486 (2017)
6. Inselberg, A.: The plane with parallel coordinates. Vis. Comput. 1(2), 69–91 (1985)
7. Jollife, I.T.: Principal Component Analysis. Springer, New York (1986)
8. Law, Martin H.C., Figueiredo, Mario A.T., Jain, Anil K.: Simultaneous feature selection and
clustering using mixture models. IEEE Trans. Pattern Anal. Mach. Intell. 26(9), 1154–1166
(2004)
9. Maniyar, D.M., Nabney, I.T.: Data visualization with simultaneous feature selection. In: IEEE
Symposium on Computational Intelligence in Bioinformatics and Computational Biology, pp.
1–8 (2006)
10. Pillat, R.M., Valiati, E.R.A., Freitas, C.M.D.S.: Experimental study on evaluation of multidi-
mensional information visualization techniques. In: Proceedings of the 2005 Latin American
conference on Human-Computer Interaction, pp. 20–30. ACM (2005)
11. Sammon, J.W.: A nonlinear mapping for data structure analysis. IEEE Trans. Comput. 18(5),
401–409 (1969)
12. Stepney, S., Nabney, I.T.: The DeCCo project papers I: Z speciﬁcation of Pasp. Technical report
YCS-358, University of York (2003)
13. Stepney, S., Nabney, I.T.: The DeCCo project papers II: Z speciﬁcation of Asp. Technical report
YCS-359, University of York (2003)
14. Stepney, S., Nabney, I.T.: The DeCCo project papers III: Z speciﬁcation of compiler templates.
Technical report YCS-360, University of York (2003)
15. Tino, P., Nabney, I.T.: Hierarchical gtm: constructing localized nonlinear projection manifolds
in a principled way. IEEE Trans. Pattern Anal. Mach. Intell. 24(5), 639–656 (2002)
16. Tino, P., Nabney, I.T., Sun, Y.: Using directional curvatures to visualize folding patterns of
the GTM projection manifolds. In: International conference on artiﬁcial neural networks, pp.
421–428. Springer (2001)
17. Tipping, M.E., Bishop, C.M.: Mixtures of principal component analysers. In: Proceedings of
the International Conference on Artiﬁcial Neural Networks, vol. 440, pp. 13–18. IEE (1997)
18. Tipping, M.E., Lowe, D: Shadow targets: a novel algorithm for topographic projections by
radial basis functions. In: Proceedings of the International Conference on Artiﬁcial Neural
Networks, vol. 440, pp. 7–12. IEE (1997)
19. Venna, J., Kaski, S.: Neighborhood preservation in nonlinear projection methods: an experi-
mental study. In: Proceedings of the International Conference on Artiﬁcial Neural Networks,
pp. 485–491 (2001)

Playing with Patterns
Fiona A. C. Polack
Abstract Susan Stepney has created novel research in areas as diverse as formal
software modelling and evolutionary computing. One theme that spans almost her
whole career is the use of patterns to capture and express solutions to software engi-
neering problems. This paper considers two extremes, both in time and topic: patterns
for formal modelling languages, and patterns related to the principled modelling and
simulation of complex systems.
1
Introduction
Software engineering uses patterns to express possible solutions to common prob-
lems. Patterns have been widely used both to capture expertise and to explore generic
solutions to problems: in engineered emergence, for instance, it is often noted that if
we could capture the patterns of multi-scale behaviour that results in desired emer-
gent behaviours, we might be able to develop patterns that could be instantiated to
efﬁciently engineering reliable emergent systems [57].
This paper summarises the origins of patterns (Sect.2), and explores two areas of
pattern research representing different approaches to pattern creation. Stepney’s work
on formal modelling, and subsequent research on formal language patterns, aimed
to support practitioners and those tasked with making formal models accessible; this
work is reviewed in Sect.3.
Subsequently, the focus of Prof. Stepney’s work shifted to complex systems. An
engineering interest in the simple algorithms that give rise to complex emergent
behaviours (ﬂocking, cellular automata, L-systems and the like) ultimately led to
the CoSMoS project and patterns for simulation and modelling of complex sys-
tems, introduced in Sect.4. The ﬁnal sections focus on common features of patterns
and potential directions for research, as well as reﬂecting speciﬁcally on Stepney’s
contribution.
F. A. C. Polack (B)
School of Computing and Mathematics, Keele University, Keele, Newcastle ST5 5BG, UK
e-mail: f.a.c.polack@keele.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_5
103

104
F. A. C. Polack
2
Patterns
Patterns, like methods, capture expertise in a way that is accessible to less-expert
or less-experienced practitioners. Patterns were developed by the architect, Christo-
pher Alexander, to demonstrate his approach to creating buildings and townscapes
[11]. Many other disciplines have subsequently adopted the concept of patterns, and
successful patterns have entered the language of discourse of these disciplines.
Computing-related pattern research (e.g. human computer interaction, software
design, safety-critical software engineering, systems of systems and other work on
large or complex computer systems) is responsible for more direct citation of Alexan-
der’s original pattern works than any other discipline [61]. At the time that his ideas
were gaining popularity in software engineering, Prof. Alexander found himself
being invited to address high-proﬁle computing conferences. Coplien [10], in his
introduction to a journal reprint of Alexander’s address to the 1996 OOPSLA con-
ference, states:
Focusing on objects had caused us to lose the system perspective. Preoccupation with design
method had caused us to lose the human perspective. The curious parallels between Alexan-
der’s world of buildings and our world of software construction helped the ideas to take root
and thrive in grassroots programming communities worldwide. The pattern discipline has
become one of the most widely applied and important ideas of the past decade in software
architecture and design.1
Coplien [10] goes on to identify Alexander’s inﬂuence as one of the three under-
pinnings of patterns in computing—the others being the PloP (Pattern Languages
of Programming) conference series2 and the pattern book of Gamma et al. [26].
Alexander’s work is, of course, a key inﬂuence on both PLoP and the Gamma et al.
book. However, Alexander was always sceptical about computing patterns, and of the
implicit view that patterns “can make a program better” [10]. Alexander stresses the
process of design [9, 11], whereas most computing-related patterns aim to provide
an instantiatable solution to a problem—not a process.
Alexander made his scepticism about computing’s use of patterns abundantly clear
when he spent a year as an honorary associate of the EPSRC CoSMoS project, 2007–
2011(seeSect.4).Adecadeafterhis1996addresstoOOPSLA,hewasstillconcerned
that patterns—even computing-related patterns—needed to express things that made
people’s lives better. Alexander had a lifelong, deep but intuitive, understanding
of the complexity of social and architectural environments, and was focused on
the search for an objective way to express and understand the essential properties
of positive spaces—that is, process patterns for making the built environment a
more positive space and a reﬂection of a natural environment. It is unfortunate that,
although patterns in computing contexts owe so much to Alexander, there is little to
offer Alexander in return.
1Note
that
the
youTube
recording
of
the
1996
talk,
https://www.youtube.com/watch?
v=98LdFA-_zfA is slightly different from the journal reprint [10].
2http://hillside.net/plop/archive.html.

Playing with Patterns
105
2.1
Developing and Evaluating Computing-Related Patterns
Alexander identiﬁes two ways in which patterns can be discovered [9]. The ﬁrst
starts from a speciﬁc problem or class of problems, and proposes a solution that can
be generalised as a pattern. The second, which depends on established practice and
experience, considers the commonalities in the ways that a problem is addressed in
practice, and extracts the characteristics of best-practice into patterns.
The earliest published computer patterns are of the second type: for example,
Gamma et al’s 1994 book [26] presents software engineering patterns derived from
observation of good solutions to common problems. Software engineering also intro-
duced antipatterns (also called bad smells) that capture the common features of poor
solutionstocommonproblems,againbasedonexperienceandobservationofpractice
[19, 25]. The latter, in particular, are widely discussed in the programming literature
and even in educating programmers, and have motivated work on refactoring and
code evolution.
The approach of patterns-from-practice can be applied as a systematic process
(rather than an intuitive retrospective), to derive patterns for a new domain, as demon-
strated by Wania and Atwood [62]. From a systematic review of the ways in which
30 existing information retrieval (IR) systems solved six aspects of an interaction
problem, Wania and Atwood derive a pattern language of 39 novel IR patterns [62].
Interestingly, further systematic research by Wania and Atwood [62] ﬁnds that
many software products that are generally considered to have good quality attributes
share common solutions—things that might be expressed as patterns—whereas
poorer-quality software development tended not to share these quality-enhancing
characteristics. It seems that the pattern-from-practice approach is embedded in
good-quality software engineering, even where authors have not captured a soft-
ware engineering pattern. This is good, since other work (some by the same authors)
has shown that computer pattern work focuses on the development of pattern lan-
guages and libraries but pays very little attention to providing objective evidence that
the identiﬁed patterns make any difference to quality or productivity [21, 23, 61].
We can speculate what an objective evaluation of a pattern catalogue—or even a
single pattern—might look like. It might, for instance, address and measure usage
criteria (is it commonly used to solve the associated problem, do users ﬁnd it easy
to understand and instantiate, is it ﬂexible to the contexts in which the problem
arises). However, a subjective measure of the success of a pattern is the extent to
which the pattern’s name has entered the language of the domain. For example,
even undergraduate students discuss their programming projects in terms of model,
view, controller (MVC) and factory patterns; these are now so fundamental in OO
and user-facing programming contexts that they are inherent rather than being seen
as patterns to be applied. Interestingly, MVC pre-dates the formal introduction of
patterns in computing: it appears to have been devised in the 1970s, as a solution
architecture for systems with a user interface, and is described as a “user interface
paradigm” in 1988 [33]. The factory pattern is a creational pattern from Gamma et
al. [26] that addresses a very common problem in OO programming; its description

106
F. A. C. Polack
is: “Deﬁne an interface for creating an object, but let subclasses decide which class
to instantiate” [26].
Pattern discovery can cheat the popularity metric, by adopting a name from the
language of the domain—this is true of many of the formal patterns in Sect.3. How-
ever, with these exceptions, most of the patterns considered here have not made it into
the language of discourse, but they do capture an aspect of expertise or experience
that the pattern’s discoverers felt to be worth preserving. The pattern languages dis-
cussed in Sects.3 and 4 include patterns-from-practice and patterns that are created
as proposed solutions to a speciﬁc problem (Alexander’s ﬁrst approach to pattern
discovery, above).
3
Patterns for Formal Modelling
In the late twentieth century, formal methods such as Z [31], B [1] and VDM [16] were
seen as a major factor in future software reliability. Notations underpinned by sound
mathematical theory allowed precise software speciﬁcation and proof of properties
at the speciﬁcation stage. Formal reﬁnement [36] allowed the systematic formal
derivation of code from formal speciﬁcations, such that proved properties could be
shown to be preserved—though it had long been realised that reﬁnement could only
preserve functional properties; properties such as safety and security (and, indeed
emergent properties [40]) are not guaranteed to be preserved by systematic formal
reﬁnement, not least because the reﬁnement says nothing about the ability of the
implementation platform to preserve non-functional properties in the design. There
are some programming languages that correspond so closely to formal modelling
languages that programs can be correct-by-construction if appropriate coding memes
(essentially patterns) are used correctly—for instance, liveness properties proved
using the formal language, CSP [29] can be implemented directly in the occam-π
language [64].
Formal methods are often stated to be “difﬁcult”. In reality, once a practitioner has
grasped the basic principles of deﬁning updates on mathematical sets and relations,
and can read the notations, the act of writing a speciﬁcation is not hard (it is certainly
easier than the process of working out what the speciﬁcation should have in it in the
ﬁrst place). It is not so much their inherent difﬁculty which affects use, but the lack
of alignment with industrial software engineering methods [17, 28, 34]. However,
like coding, there are good ways and less good ways to construct and document a
formal model. Again, like coding, there are ways to achieve particular goals that are
speciﬁc to one formalism. Crucially, some ways of writing a proposal suit particular
purposes better than others—for instance, a speciﬁcation that is written for readability
is unlikely to be in a format that facilitates (semi-)automated proof of properties or
formal reﬁnement.
For Stepney et al. [55], the purpose and motivation for writing a catalogue of
Z patterns was to capture good practice and presentation, and to support use of

Playing with Patterns
107
Z-speciﬁc processes. Later, some of the Z patterns and the conceptual-level ideas
introduced via Z patterns were generalised to other formal languages [56].
The Z patterns build on best-practice developed by Logica UK’s Formal Meth-
ods Team (LFM), of which Prof. Stepney had been a member. In the 1990s, LFM
was a key player in the industrial use of formal speciﬁcation and proof, and pub-
lished the book, Z in Practice [15]. In LFM, Prof. Stepney worked extensively on
large-scale industrial speciﬁcation and proof. Published examples include the DeCCo
compiler for a high-integrity system [52], and the Mondex electronic purse [51, 69].
These are signiﬁcant industrial achievements, which required not a little formal meth-
ods research. The formal modelling and proof underpinning Mondex resulted in its
being awarded ITSEC level E6 security clearance [49, 51, 69]; until this point, the
formalism requirements of level E6 had been widely considered unattainable for
software-based systems.
Although not explicitly presenting Z patterns, the LFM book [15] presents good
practice in a clear and applicable manner. A decade later, our Z patterns use the
LFM house style throughout. We also capture the LFM house style in patterns as,
“some simple Patterns . . . to aid the original speciﬁcation process, and to help with
maintenance” [55]:
• Comment the intention
• Format to expose structure
• Provide navigation
• Name consistently.
We also introduce anti-patterns that capture our experience of bad presentation,
for instance:
• Overmeaningful name
• Overlong name.3
Like all good (i.e. clear, usable) patterns, the names of these patterns are almost
entirely self-explanatory—even Provide navigation is an obvious name to someone
using Z (it proposes that, where a Z schema includes—references and incorporates—
other schemas, the accompanying text should make clear what is used, rather than
leaving the reader to search a potentially large formal text for the speciﬁcation of the
included schemas). As noted above, many of the patterns are named with the familiar
Z names of structures or activities (Delta/Xi, promotion, etc.).
In writing the Z patterns [53–55], we identiﬁed a range of possible uses for pat-
terns. The LFM house style patterns are examples of presentation patterns. Four other
uses of pattern are documented: idioms, structure patterns, architecture patterns and
development patterns.
• Idioms are stylistic patterns and anti-patterns that relate to common challenges
encountered in expressing a model in Z such as Represent a 1:many mapping,
Use free types for unions, and Overloaded numbers.
3In line with usage in the Z Patterns report [55], we use a sans-serif font for pattern names and an
italicised font for anti-pattern names.

108
F. A. C. Polack
• Structure patterns aim to address the problem of structuring a long or complicated Z
speciﬁcation, and encapsulate good practice in modularisation. This includes pat-
terns to address readability, such as Name meaningful chunks and Name pred-
icates; ways to model speciﬁc structural elements such as Modelling optional
elements or Boolean ﬂag; and stylistic patterns such as Use generics to con-
trol detail. The anti-pattern Fortran warns against writing code in Z rather than
exploiting the abstraction and power of the language.
• Architectural patterns express paradigmatic ways in which Z notation can be used,
aiming to facilitate ways of writing Z that are often unfamiliar to mainstream Z
users. The patterns include Object orientation, and Algebraic style, whilst the
anti-pattern Unsuitable Delta/Xi pattern captures the fact that the mainstream Z
approach, of specifying a state and then specifying predicates that specify formal
update and read operations on that state (see e.g. [15, 46]), is unsuitable for some
uses of Z. Examples of (appropriate use of) the algebraic style can be found in
the deﬁnitions that underpin Z: for instance, Stepney et al’s Z deﬁnitions and laws
[59], or Woodcock and Loomes’ theory of natural numbers [68, Chap. 11].
• Developmentpatternsarestylisticandusagepatternsthatcanbeappliedinworking
with Z, such as Use integrated methods. They include patterns such as Do a
reﬁnement, Do sanity checks and Express implicit properties.
The architectural and development patterns come close to Alexander’s conception
of process patterns, in that they guide a user in creating or working with an appropriate
(in aesthetic and architectural terms) Z model. The Z catalogue takes the support
for the development process further in generative patterns: collections of patterns
that capture what needs to be done to achieve or manage a particular form of Z
model. Examples include collections of patterns relating to creation of a good quality
Delta/Xi (state-and-operations) speciﬁcation, and a collection which can be applied
to create a Z Promotion, in which local operations are deﬁned on a local state and
then “promoted” to global operations on a global state. A set of patterns to support
a full formal reﬁnement was also presented [54].
In presenting the Z Pattern Catalogue, the original intention had been to extend
the catalogue over time as formality became embedded in software engineering.
To support this aim, the catalogue includes a pattern template. To make our Z pat-
terns accessible to a wider audience, we also deﬁned a diagrammatic language—a
domain-speciﬁc language (DSL) in modern parlance. Diagrams with clearly-deﬁned
meanings were used to illustrate the structure and behaviour of pattern components.
Later, the DSL was elaborated and applied to other formal languages [56].
The Z Pattern Catalogue and associated papers were well received at the time
of writing, but Z patterns did not enter the language of discourse (except where
we named patterns with terms from the existing language of discourse) and did not
become the major resource for formalists that we had hoped.4 In retrospect, we see
that our work on making Z more accessible came at the start of the decline in the belief
4In 2018, the various papers had only some 30 citations. However, to put that in context, the LFM
book [15], has only 142 citation.

Playing with Patterns
109
that formal methods would be the underpinning of all software engineering; indeed,
in the twenty-ﬁrst century, the predominant approach to software speciﬁcation and
design uses model-driven engineering, and metamodelling approaches to language
deﬁnition (e.g. [18]).
4
Patterns for Modelling Complex Systems: CoSMoS
The Z Pattern Catalogue comprises patterns-from-practice, building on many years
of experience using the Z language in practice and as part of a software development
process. Even the seemingly-esoteric deﬁnitions and laws of Z [59] are motivated by
practical requirements for ﬁt-for-purpose formal speciﬁcation tool-support, and the
need to demonstrate the internal consistency of ISO standard Z [31].
In the last decade, Prof. Stepney’s focus has shifted to complex systems, combin-
ing interests in complexity and software engineering. Stepney led development of
CoSMoS, a principled approach to the modelling and simulation of complex systems,
targetted to research and engineering uses of simulation.5 Simulations developed fol-
lowing the CoSMoS approach and principles include, for instance [2, 27, 37, 47,
48, 66].
The CoSMoS process is summarised as a life-cycle, phases and products (see
Fig.1); the high-level problem is thus how to support the the activities that are
needed in each phase. CoSMoS patterns represent ways to address the sub-problems
identiﬁed in decomposing the top-level problem of engineering a demonstrably
ﬁt-for-purpose simulation [58] (an example of Alexander’s ﬁrst pattern-discovery
approach).
The top-level CoSMoS process can be summarised in three patterns [50, 58],
• carry out the Discovery Phase,
• carry out the Development Phase,
• carry out the Exploration Phase.
Further patterns are identiﬁed as the phases are decomposed. For the Discovery
Phase, for instance, the next level of patterns are [50, 58]:
• identify the Research Context,
• deﬁne the Domain,
• build a Domain Model,
• Argue Appropriate Instrument Designed.
Focusing on the pattern, Identify the research context, Stepney [50] uses the
Intent, Context and Discussion sections of the pattern to convey the role that the
research context plays in a CoSMoS development (reproduced in Table1). Notice
5The CoSMoS project, 2007–11, was led by Susan Stepney (York: EPSRC EP/E053505) and Peter
Welch (Kent: EPSRC EP/E049419), along with researchers from York, Kent, University of the
West of England and Abertay. The outputs of CoSMoS can be found online at https://www.cosmos-
research.org/about.html. Background and motivation to CoSMoS can be found in [13, 42–45, 58].

110
F. A. C. Polack
Fig. 1 Phases (ellipses) and products (boxes) of the CoSMoS process [13, 50, 58]. The CoSMoS
lifecycle starts with identiﬁcation of the domain (not shown), and proceeds through development
of the domain and platform models, the simulation platform and the results model. Information
collected en-route is summarised in the research context, which is used to map between simulation
results and domain observables
that further patterns are identiﬁed in the pattern’s Discussion section: whenever a
variant of a problem is identiﬁed, and a potential solution identiﬁed, whether from
scratch or from the wider software engineering experience of the CoSMoS team, a
pattern captures the solution.
Again, antipatterns identify some of the things to avoid in deﬁning the research
context [50, 58]. For example, avoiding an ever-expanding scope or scale, which is
referred to as Everything but the kitchen sink.
To further develop a research context, we decompose the problem again, and seek
to capture more potential solutions as patterns. The following list of problems to be
solved shows where patterns have been documented [50, 58]:
• Document the research goals
• Document Assumptions relevant to the research context
• Identify the team members, including the Domain Expert, the Domain Modeller,
and the Simulation Implementor, their roles, and experience
• Agree the Simulation Purpose, including criticality and impact
• Note the available resources, timescales, and other constraints
• Determine success criteria
• Revisit between phases, and at discovery points; if necessary, change the context,
and Propagate Changes.

Playing with Patterns
111
Table 1 The intent, context and discussion sections of stepney’s pattern, Identify the Research
Context [50, 58]
Intent
Identify the overall scientiﬁc context and scope of the simulation-based research being
conducted
Context
A component of the Discovery Phase, Development Phase, and Exploration Phase patterns.
Setting (and resetting) the scene for the whole simulation project
Discussion
The role of the research context is to collate and track any contextual underpinnings of the
simulation-based research, and the technical and human limitations (resources) of the work
The research context comprises the high-level motivations or goals for the research use, the
research questions to be addressed, hypotheses, general deﬁnitions, requirements for validation
and evaluation, and success criteria (how will you know the simulation has been successful)
The scope of the research determines how the simulation results can be interpreted and applied.
Importantly, it captures any requirements for validation and evaluation of simulation outputs. It
inﬂuences the scale and scope of the simulation itself
Consideration should be made of the intended criticality and impact of the simulation-based
research. If these are judged to be high, then an exploration of how the work can be validated
and evaluated should be carried out
Determine any constraints or requirements that apply to the project. These include the resources
available (personnel and equipment), and the time scale for completion of each phase of the
project. Any other constraints, such as necessity to publish results in a particular format (for
example, using the ODD Protocol), should be noted at this stage. This helps ensure that later
design decisions do not violate the project constraints. Ensure that the research goals are
achievable, given the constraints. As information is gathered during the project, more
understanding of the domain and the research questions will be uncovered. For example, a
Prototype might indicate that a simulation of the originally required detail is computationally
infeasible. The research context should be revisited between the various phases, and also at any
point where major discoveries are made, in order to check whether the context needs to change
in light of these discoveries
Whilst some of these patterns are speciﬁc to a CoSMoS-style simulation devel-
opment, all the patterns draw on general software engineering or simulation best
practice. For example the patterns that decompose the problem of identifying roles
draw on software engineering roles (cf. Scrum roles6). A CoSMoS role implies par-
ticular responsibilities, but it does not imply a single person—typically, the domain
expert role is taken by a lab or research team; it is also possible for one person to
play several, or even all, the roles.
As for the Z generative patterns (Sect.3), lists of patterns that capture what could
be done to address a particular problem are not ordered steps, but a collection of
activities that should be considered in order to address the sub-problem of a higher
goal [50, 58]. As in other pattern catalogues, patterns may be optional, alternatives,
or might be instantiated in spirit only in smaller or less critical projects.
6https://www.scrumalliance.org/.

112
F. A. C. Polack
4.1
Fitness for Purpose and Patterns
Documenting a whole development process by successive problem decomposition is
not a task to take lightly: a full set of CoSMoS patterns has recently been published
[58]. However, some of the lower-level patterns have been adopted in practice.7 In this
section, I present some patterns that we have developed to support documentation of
the ﬁtness-for-purpose of a simulation using arguments [2, 3, 42, 66]; ﬁrst, however,
I explain how and why CoSMoS addresses ﬁtness for purpose.
CoSMoS is a process to support principled modelling and simulation of complex
systems. Principled modelling and simulation implies that we can show that our
modelling and simulation match the purpose of our simulation—in most cases, that
the models are appropriate models of the domain, and that the simulation faithfully
implements the models.
Validation of simulations has often been limited to checking that the simulation
produces the expected results by a process that looks a bit like reality; there is little
concern for the quality of the underlying simulation [24]. The lack of overt emphasis
on ﬁtness for purpose in simulation has led to intellectual debate over whether it is
possible to do science through simulation [20, 22, 35, 38, 45, 65]. Similar issues
with the validity of simulation and use of simulation evidence arise in safety critical
systemsengineering[12],andinsocialsimulation[65].Wheeleretal.[65]summarise
the concerns about simulation in noting that to assess the role and value of complex
systems simulation, we need to address deep questions of comparability: we need a
record of experience, of how good solutions are designed, of how to chose parameters
and calibrate agents, and, above all, how to validate a complex system simulation.
Many of these issues were subsequently considered in the CoSMoS project [13].
Of particular interest here are simulations created as tools for scientiﬁc exploration.
A scientiﬁc tool enhances the ability of a human to observe or understand a sci-
entiﬁc subject. Humphreys [30] identiﬁes three ways in which scientiﬁc instruments
enhance the range of natural human abilities.
• Extrapolation describes the extension of an existing modality. For example, vision
is extended using a microscope.
• Conversion changes the mode through which something is accessed. For exam-
ple, a sonar device has a visual display that converts sonar echoes into a three-
dimensional image of a surface.
• Augmentation is used to access features that people cannot normally detect in their
original form. Examples include the detection of magnetism or elementary particle
spin.
Tools that use extrapolation, conversion or augmentation exhibit an increasingly
tenuous or abstract link to reality (the domain of study). Conversely, the implicit or
explicit model of reality plays an increasingly important role in understanding the
outputs of the scientiﬁc instrument.
7see e.g. the simulation projects of the York Computational Immunology Lab, https://www.york.
ac.uk/computational-immunology/.

Playing with Patterns
113
For simulation, the properties of the real domain are often estimated and always
abstracted: the environment, the input parameters, and the layers and the forms of
interaction are necessarily simpliﬁed. However, just as with conventional scientiﬁc
instruments, the simulation needs to be constructed, calibrated and documented in a
way that allows it to be used as a robust tool.
This puts a signiﬁcant responsibility on the modelling. Without a principled
approach to development, the results of a simulation cannot be interpreted in any
meaningful way.
The CoSMoS process has mostly been used for research simulation, aiming to
create a simulation (usually an agent or individual based simulation) that is a credible
tool for testing or developing speciﬁc scientiﬁc hypotheses in a speciﬁc laboratory
(domain) context. The intended purpose and the scope of simulation are determined
by the domain expert, in conjunction with the developer, (who usually has the
more realistic idea of what is possible in a simulation). Purpose is deﬁned by, and is
that of, the domain expert laboratory. It is not the whole of that scientiﬁc domain.
In this respect, CoSMoS-style simulation differs from much scientiﬁc simulation,
which seeks to create a simulation of a whole concept using information from many
domain experts and related sources—see for instance the Chaste project, which is
building an in silico heart model.8
A CoSMoS simulation purpose typically relates to the investigation of some
dynamic process that underpins an observable behaviour of a real system—for exam-
ple, the development of cell clusters or the evolution of a pathology that depends on
cell and molecular interactions over time and/or space. The simulation is needed
either because the dynamic systems behaviour cannot be observed, or because it is
useful to be able to perturb the dynamics in ways that cannot easily be attempted in
vivo, in vitro or in dead tissue. The ﬁtness-for-purpose of the simulation is assessed
against the purpose, and must be re-assessed if the simulation purpose is changed.
As a CoSMoS-style development proceeds, many discussions and some research
take place, to establish a model of the domain. In engineering terms, the team aims
to establish the scope and scale of a simulation that is computationally feasible, and
can meet the purpose of the simulation. Many design decisions are made, both in
terms of the domain and in terms of the modelling and potential realisation of the
simulator. Many assumptions are also identiﬁed and recorded. (Of course, there are
many un-noticed assumptions and instinctive design decisions that are not recorded,
but that is another story.)
Most CoSMoS-based simulation projects use argumentation to capture the belief
of the team that speciﬁc aspects of the development are ﬁt for purpose. Arguments
lend themselves well to patterning. CoSMoS argumentation is derived from safety
case argumentation, which, from its earliest days, has used patterns to capture the
structure of common safety cases such as Component contributions to system
hazards and Effects of other components [32, 63]. In safety case argumentation
patterns, the top-level goal or claim often presents a generalisation of a common
hazard or safety concern, and further patterns present potential decompositions of
8www.cs.ox.ac.uk/chaste.

114
F. A. C. Polack
the main claims (referred to as goals) of the argument. The solutions represented by
safety argumentation patterns capture best practice in safety critical engineering as
well as ultimately presenting evidence that a goal has been met.
Safety case arguments are typically presented using the Goal Structuring Notation
(GSN) [39, 67]. CoSMoS adopts, and slightly adapts, GSN for the visual represen-
tation of ﬁtness-for-purpose arguments [4, 41, 42].
Whereas a safety case argument must be complete, from goal to evidence, the
primary purpose of a ﬁtness-for-purpose argument is to expose and express the rea-
soning behind a particular aspect of the simulation design. For example, Alden [2]
capturesadetailedrationalefortheﬁtness-for-purposeofthedomainmodelcreatedto
model Peyers’ patch development in an embryonic mouse gut. The top claim, that the
[m]odel is an adequate representation of the biology, is addressed using the decom-
position strategy, [a]rgue over scientiﬁc content, the adequacy of the abstraction
and the experimental results. Each element of the decomposition can be addressed
in turn, broken down into sub-goals, and, in Alden’s case, used to point to the even-
tual evidence (data, statistical analysis) that the simulator is capable of generating
results that are comparable to observations of the domain.
Fig. 2 An argument pattern for the top-level ﬁtness-for-purpose of a domain model for a simulation.
Claims are represented as rectangles, strategies as parallelograms, contextual information (which
could also include assumptions, justiﬁcations, etc.), as soft-cornered rectangles. Instantiation of the
pattern replaces names in angle-brackets and elaborates the sub-claims [14, 42]

Playing with Patterns
115
Fig. 3 Representation of an argument pattern for the appropriateness of statistical analysis. Claims
are represented as rectangles, strategies as parallelograms, contextual information (which could also
include assumptions, justiﬁcations, etc.), as soft-cornered rectangles. Instantiation of the pattern
replaces names in angle-brackets
Alden’s top-level argument [2] is based on an instantiation of one of the sub-claims
of the top-level ﬁtness-for-purpose argument pattern shown in Fig.2.9
The argument in Fig.2 presents a possible solution to the problem of demonstrat-
ing ﬁtness for purpose of a whole simulation (the top claim, Claim 1). The argument
points to sources of information about the claim in two “Context” statements—one
is a pointer to the deﬁnition of the system that we want to show to be ﬁt, and the
other, here, points to the statement of purpose against which ﬁtness is to be judged.
These contextual elements are essential in documenting an argument that something
is ﬁt for purpose. Other contextual items, assumptions and justiﬁcations could be
added.
The top-level claim in the argument (Fig.2) is addressed using a decomposition
strategy: arguing over the ﬁtness for purpose of the modelling, the software engi-
neering and the results of the simulation. Each decomposed element results in further
claims, and each of these can be broken down and documented in the same way. To
instantiate this argument, the terms in angle-brackets must be replaced by appropri-
ate terms from the project, and thought must be given both to how to elaborate each
sub-claim, and to what assumptions, justiﬁcations and context statements are needed
to substantiate the claims.
9Argument representations in this paper have been produced using YCIL’s open-source Artoo argu-
mentation tool https://www.york.ac.uk/computational-immunology/software/artoo/, which runs in
a browser, loads and stores argument structures as XML, and generates .png images. The tool
underpins Simomics Ltd’s Reason tool, www.simomics.com.

116
F. A. C. Polack
To illustrate a pattern and its instantiation, it is easiest to take a more focused
argument. A common problem in demonstrating ﬁtness for purpose in a simulation
(or real-life) context is showing that the statistical analysis undertaken has used an
appropriate statistic. Most common statistics make some assumptions about whether
data is continuous or discrete and how population and sample data and data errors
are distributed. Figures3 and 4 illustrate a general pattern for an argument about
Fig. 4 An instantiation of the argument pattern in Fig. 3 for the use of Vargha and Delaney’s A test
[60]. A fuller version of the argument, showing how it has been applied to speciﬁc experimental
results, can be found in [42]

Playing with Patterns
117
the appropriateness of a statistic, and an instantiation of that argument for the A
test [60], which can be used to compare medians (an example use of the argument
can be found in [42]). A key beneﬁt of argumentation is that anyone can review the
argument: for example, in Fig.4, someone might spot an error in our assumptions,
or might challenge the validity of the strategy used.
As an aside, we show the argument of ﬁtness for the A test here because it is a test
that many CoSMoS-style simulations have used, in situations where the distribution
characteristics of the data cannot be known. It is one of a number of statistical
tests built into the Spartan10 tool [5–8] which supports analysis of CoSMoS-style
simulation and domain results.
5
Discussion
The concept of a pattern is appealing to engineers because it encapsulates a solution to
a typical problem. In this paper, I have given examples of patterns that are synthesised
from practice and experience, exempliﬁed by the Z and formal methods patterns;
and patterns that are proposed as potential solutions to problems, exempliﬁed by the
CoSMoS development and argumentation patterns.
Both approaches to pattern discovery illustrate some typical issues with patterns.
The patterns-from-practice approach relies on there being established usage and
expertise; the risk (demonstrated by the Z patterns work) is that the patterns are
distilled too late to be of value: in a fast-changing area such as software engineering,
this is not uncommon. However, the capturing of solutions in patterns can highlight
aspects of solutions or best practice that are of relevance beyond the speciﬁc pattern
domain. For instance, in the Z patterns work, we used generative patterns to identify
sets of speciﬁc development (etc.) patterns that can be used to develop different styles
of formal solution: the same meme is used in the CoSMoS patterns, and we suggest
that the principle of providing a set of patterns relating to a particular problem takes
patterns closer to Alexander’s intent of process patterns.
In work on formal patterns, our use of “meta-patterns” shows how we adapt
Z patterns to other formal and less formal modelling approaches, to capture design
processes. If we replace the Z-speciﬁc patterns with either generic or other language-
speciﬁc patterns, we might be able to improve the quality of development more
widely. To illustrate, consider Table2, which takes three of the component patterns
from the generative pattern for a conventional Z Delta/Xi speciﬁcation [55], and
suggests how these might relate to, and illuminate, a more general development
process.
It is interesting to observe in Table2 how things that are implicit in one context
(e.g.type-checkinginaformalcontext;drawingdesigndiagramsinageneralcontext)
need to be made explicit in another context. It is tempting to suggest that there is
10https://www.york.ac.uk/computational-immunology/software/.

118
F. A. C. Polack
Table 2 Using components of a Z generative pattern to illuminate more general development
patterns and vice versa
Z generative patterns: intent
General development
Delta/Xi: specify a system as a state, with
operations based on that state
Model the state and operations of the proposed
system—note that a formal model can be
proven internally consistent; a non-formal
model needs an additional validation step
Diagram the structure: create a visual
representation of the structure of the solution,
to summarise the structure and allow critical
review
Create and validate diagrammatic models;
validate internal consistency of models
Strict convention: Use the Δ/Ξ Z naming
convention strictly, to avoid surprising the
reader with hidden constraints. Note: Z
schemas whose name starts with Δ deﬁne a
pre and post state, and can be used to formally
deﬁne updates to the state; those whose name
starts with Ξ require that the pre and post
states are equal, so can only deﬁne read
operations
Consider the meaning of notations and state
semantic variations For example, when
modelling in UML, consider whether the MOF
deﬁnitions of class, generalisation, etc., are
used strictly; consider stating the semantics of
generalisation in terms of speciﬁc theory of OO
inheritance, etc.
Change part of the state: deﬁne an operation
that changes only part of the state Note: Z
allows “set difference” to be applied to
schemas, constraining hidden elements to
remain unchanged in operations
Validate operations to ensure that unintended
side-effects are eliminated; ensure that a
requirement not to change part of a state is
explicit and tested
scope for comparison of developmental patterns for different contexts and languages,
to identify a superset of generic quality or validation patterns.
Turning to patterns invented to address existing problems, a similar effect can be
observed. In developing a new pattern catalogue, we use our experience of working in
other problem-solving domains and with other approaches or languages. Thus, one of
the underpinnings of our work on the CoSMoS pattern language is our background in
formal methods and software engineering. The CoSMoS team included experienced
academic and industrial software engineers who had experience of a wide range of
methods, techniques and languages. Experience with formal methods (the team had a
good working knowledge and practical experience of Z, B and CSP) focuses attention
on the use and misuse of models more generally; typechecking focuses attention on
the need to cross-validate models. In terms of software engineering modelling, the
team’s expertise covers conventional Harel statecharts, Petri nets, and many of the
software modelling languages that comprise or inﬂuenced UML/MOF, as well as
a number of bespoke modelling languages, and research expertise in model driven
engineering. A strong background in notations and semantics, and a broad experience
of modelling languages, inevitably focuses attention on the difﬁculty of knowing
whetherandtowhatextentamodeladequatelycaptureswhatisintended.Overall,this
led to the CoSMoS Research Context (colloquially, the repository of all the stuff
that we accumulate about the domain and the development) and the focus on ﬁtness-
for-purpose and argumentation. Thus, whilst experience of principled modelling

Playing with Patterns
119
and simulation is still limited, the CoSMoS patterns are well-grounded in extensive
software engineering practice and experience. Again, it would be tempting to suggest
a more systematic review of pattern languages, to establish links to patterns in other
domains which may already have solutions to some of the problems of a CoSMoS-
style development.
6
Summary and Conclusion
Patterns, building on the work of Alexander, have become inﬂuential and mainstream
in software engineering. Prof. Stepney’s work has returned to patterns repeatedly,
both as a means of capturing long-time experience, and as a way to present potential
solutions to problems.
Stepney and her teams have produced novel patterns, with a focus on visual
representations, as well as on practicality and use. This platform of pattern research
has the potential for improving software development both of research simulators and
more generally. Even if none of the patterns presented here and in the referenced work
was never used again, they remain an archive of experience and practice, presented
in an accessible way, to future researchers and software developers. The examples
show the breadth of software engineering research led by Prof. Stepney over the last
three decades.
References
1. Abrial, J.-R.: The B-book: Assigning Programs to Meanings. CUP (1996)
2. Alden, K.: Simulation and statistical techniques to explore lymphoid tissue organogenesis.
Ph.D. thesis, University of York (2012). http://etheses.whiterose.ac.uk/3220/
3. Alden, K., Andrews, P., Timmis, J., Veiga-Fernandes, H., Coles, M.C.: Towards argument-
driven validation of an in-silico model of immune tissue organogenesis. In: Proceedings of
ICARIS, vol. 6825, LNCS, pp. 66–70. Springer (2011)
4. Alden, K., Andrews, P.S., Polack, F.A.C., Veiga-Fernandes, H., Coles, M.C., Timmis, J.: Using
argument notation to engineer biological simulations with increased conﬁdence. J. R. Soc.
Interface 12(104) (2015)
5. Alden, K., Andrews, P.S., Veiga-Fernandes, H., Timmis, J., Coles, M.C.: Utilising a simulation
platform to understand the effect of domain model assumptions. Nat. Comput. 14(1), 99–107
(2014)
6. Alden, K., Read, M., Andrews, P.S., Timmis, J., Coles, M.C.: Applying Spartan to understand
parameter uncertainty in simulations. R J. (2014)
7. Alden, K., Read, M., Timmis, J., Andrews, P., Veiga-Frenandes, H., Coles, M.: Spartan: a
comprehensive tool for understanding uncertainty in simulations of biological systems. PLoS
Comput. Biol. 9(2) (2013)
8. Alden, K., Timmis, J., Andrews, P.S., Veiga-Fernandes, H., Coles, M.C.: Extending and apply-
ing Spartan to perform temporal sensitivity analyses for predicting changes in inﬂuential bio-
logical pathways in computational models. IEEE Trans. Comput. Biol. 14(2), 422–431 (2016)
9. Alexander, C.: The Timeless Way of Building. OUP (1979)

120
F. A. C. Polack
10. Alexander, C.: The origins of pattern theory: the future of the theory, and the generation of a
living world. IEEE Softw. 16(5), 71–82 (1999)
11. Alexander, C., Ishikawa, S., Silverstein, M., Jacobson, M., Fiksdahl-King, I., Angel, S.: A
Pattern Language—Towns, Buildings, Construction. OUP (1977)
12. Alexander, R.: Using simulation for systems of systems hazard analysis. Ph.D. thesis, Depart-
ment of Computer Science, University of York, YCST-2007-21 (2007)
13. Andrews, P.S., Polack, F.A.C., Sampson, A.T., Stepney, S., Timmis, J.: The CoSMoS process,
version 0.1. Technical Report, Computer Science, University of York, YCS-2010-450 (2010)
14. Andrews, P.S., Stepney, S., Hoverd, T., Polack, F.A.C., Sampson, A.T., Timmis, J.: CoSMoS
process, models and metamodels. In: CoSMoS Workshop, pp. 1–14. Luniver Press (2011)
15. Barden, R., Stepney, S., Cooper, D.: Z in Practice. Prentice-Hall (1995)
16. Bjørner, D., Jones, C.B. (eds.): The Vienna Development Method: The Meta-Language, vol.
61, LNCS. Springer (1978)
17. Bowen, J.P., Hinchey, M.G.: Seven more myths of formal methods. IEEE Softw. 12(4), 34–41
(1995)
18. Brambilla, M., Cabot, J., Wimmer, M.: Model-driven Software Engineering (MDSE) in Prac-
tice, 2nd edn. Morgan & Claypool (2017)
19. Brown, W.H., Malveau, R.C., McCormick, H.W., Mowbray, T.J.: AntiPatterns: Refactoring
Software, Architectures, and Projects in Crisis, 1st edn. Wiley (1998)
20. Bryden, J., Noble, J.: Computational modelling, explicit mathematical treatments, and scientiﬁc
explanation. In: Proceedings of Artiﬁcial Life X, pp. 520–526. MIT Press (2006)
21. Dearden, A., Finlay, J.: Pattern languages in HCI: a critical review. Hum. Comput. Interact.
21(1), 49–102 (2006)
22. Di Paolo, E., Noble, J., Bullock, S.: Simulation models as opaque thought experiments. In:
Proceedings of Artiﬁcial Life VII, pp. 497–506. MIT Press (2000)
23. Duncan, I.M.M., de Muijnck-Hughes, J.: Security pattern evaluation. In: Proceedings of SOSE,
pp. 428–429. IEEE (2014)
24. Epstein, J.M.: Agent-based computational models and generative social science. Complexity
4(5), 41–60 (1999)
25. Fowler, M.: Refactoring: Improving the Design of Existing Code. Addison-Wesley (1999)
26. Gamma, E., Helm, R., Johnson, R., Vlissides, J.: Design Patterns: Elements of Reusable Object-
oriented Software. Addison-Wesley (1995)
27. Greaves, R.B., Read, M., Timmis, J., Andrews, P.S., Butler, J.A., Gerckens, B., Kumar, V.:
In silico investigation of novel biological pathways: the role of CD200 in regulation of T cell
priming in experimental autoimmune encephalomyelitis. Biosystems (2013). https://doi.org/
10.1016/j.biosystems.2013.03.007
28. Hall, A.: Seven myths of formal methods. IEEE Softw. 7(5), 11–19 (1990)
29. Hoare, C.A.R.: Communicating Sequential Processes. Prentice-Hall (1985)
30. Humphreys, P.: Extending Ourselves: Computational Science, Empiricism, and Scientiﬁc
Method. OUP (2004)
31. Information Technology—Z formal speciﬁcation notation—syntax, type system and semantics.
ISO Standard 13568 (2002)
32. Kelly, T.P.: Arguing safety—a systematic approach to managing safety cases. Ph.D. thesis,
Department of Computer Science, University of York, YCST 99/05 (1999)
33. Krasner, G.E., Pope, S.T.: A cookbook for using the model-view controller user interface
paradigm in Smalltalk-80. J. Object Oriented Program. 1(3), 26–49 (1988)
34. Le Charlier, B., Flener, P.: Speciﬁcations are necessarily informal or: some more myths of
formal methods. Syst. Softw. 40(3), 275–296 (1998)
35. Miller, G.F.: Artiﬁcial life as theoretical biology: how to do real science with computer simu-
lation. Technical Report Cognitive Science Research Paper 378, University of Sussex (1995)
36. Morgan, C.: Programming from Speciﬁcations, 2nd edn. Prentice Hall (1994)
37. Moyo, D.: Investigating the dynamics of hepatic inﬂammation through simulation. Ph.D. thesis,
University of York (2014)

Playing with Patterns
121
38. Nance, R.E., Sargent, R.G.: Perspectives on the evolution of simulation. Oper. Res. 50(1),
161–172 (2002)
39. Origin Consulting (York): GSN community standard version 1. Technical report, Department
of Computer Science, University of York (2011). http://www.goalstructuringnotation.info
40. Polack, F., Stepney, S.: Emergent properties do not reﬁne. ENTCS 137(2), 163–181 (2005)
41. Polack, F.A.C.: Arguing validation of simulations in science. In: CoSMoS Workshop, pp. 51–
74. Luniver Press (2010)
42. Polack, F.A.C.: Filling gaps in simulation of complex systems: the background and motivation
for CoSMoS. Nat. Comput. 14(1), 49–62 (2015)
43. Polack, F.A.C., Andrews, P.S., Ghetiu, T., Read, M., Stepney, S., Timmis, J., Sampson, A.T.:
Reﬂections on the simulation of complex systems for science. In: Proceedings of ICECCS, pp.
276–285. IEEE Press (2010)
44. Polack, F.A.C., Andrews, P.S., Sampson, A.T.: The engineering of concurrent simulations of
complex systems. In: Proceedings of CEC, pp. 217–224. IEEE Press (2009)
45. Polack, F.A.C., Hoverd, T., Sampson, A.T., Stepney, S., Timmis, J.: Complex systems models:
engineering simulations. In: Proceedings of ALife XI, pp. 482–489. MIT press (2008)
46. Potter, B., Till, D., Sinclair, J.: An Introduction to Formal Speciﬁcation and Z, 2nd edn. Prentice
Hall (1996)
47. Read, M., Andrews, P.S., Timmis, J., Kumar, V.: Techniques for grounding agent-based simu-
lations in the real domain: a case study in experimental autoimmune encephalomyelitis. Math.
Comput. Model. Dyn. Syst. 18(1), 67–86 (2012)
48. Read, M.N.: Statistical and modelling techniques to build conﬁdence in the investigation of
immunology through agent-based simulation. Ph.D. thesis, University of York (2011)
49. Stepney, S.: A tale of two proofs. In: BCS-FACS Northern Formal Methods Workshop. Elec-
tronic Workshops in Computing (1998)
50. Stepney, S.: A pattern language for scientiﬁc simulations. In: CoSMoS Workshop, pp. 77–103.
Luniver Press (2012)
51. Stepney, S., Cooper, D., Woodcock, J.C.P.: An electronic purse: speciﬁcation, reﬁnement, and
proof. Technical Monograph PRG-126, Oxford University Computing Laboratory (2000)
52. Stepney, S., Nabney, I.T.: The DeCCo project papers, I to VI. Technical Report, Computer
Science, University of York, YCS-2002-358 to YCS-2002-363 (2003)
53. Stepney, S., Polack, F., Toyn, I.: An outline pattern language for Z: ﬁve illustrations and two
tables. In: Proceedings of ZB2003, vol. 2651, LNCS, pp. 2–19. Springer (2003)
54. Stepney, S., Polack, F., Toyn, I.: Patterns to guide practical refactoring: examples targetting
promotion in Z. In: Proceedings of ZB2003, vol. 2651, LNCS, pp. 20–39. Springer (2003)
55. Stepney, S., Polack, F., Toyn, I.: A Z patterns catalogue I: speciﬁcation and refactorings, v0.1.
Technical Report, Computer Science, University of York, YCS-2003-349 (2003)
56. Stepney, S., Polack, F., Toyn, I.: Diagram patterns and meta-patterns to support formal mod-
elling. Technical Report, Computer Science, University of York, YCS-2005-394 (2005)
57. Stepney, S., Polack, F., Turner, H.: Engineering emergence. In: ICECCS, pp. 89–97. IEEE
Computer Society (2006)
58. Stepney, S., Polack, F.A.C.: Engineering Simulations as Scientiﬁc Instruments: A Pattern Lan-
guage. Springer (2018)
59. Valentine, S.H., Stepney, S., Toyn, I.: A Z patterns catalogue II: deﬁnitions and laws, v0.1.
Technical Report, Computer Science, University of York, YCS-2003-383 (2004)
60. Vargha, A., Delaney, H.D.: A critique and improvement of the CL common language effect
size statistics of McGraw and Wong. J. Educ. Behav. Stat. 25(2), 101–132 (2000)
61. Wania, C.E.: Investigating an author’s inﬂuence using citation analyses: Christopher Alexander
(1964–2014). Proc. Assoc. Inf. Sci. Technol. 52(1), 1–10 (2015)
62. Wania, C.E., Atwood, M.E.: Pattern languages in the wild: exploring pattern languages in the
laboratory and in the real world. In: Proceedings of DESRIST, pp. 12:1–12:15. ACM (2009)
63. Weaver, R.A.: The safety of software—constructing and assuring arguments. Ph.D. thesis,
Department of Computer Science, University of York, YCST-2004-01 (2003)

122
F. A. C. Polack
64. Welch, P.H., Barnes, F.R.M.: Communicating mobile processes: introducing occam-pi. In:
Proceedings of 25 Years of CSP, vol. 3525, LNCS, pp. 175–210. Springer (2005)
65. Wheeler, M., Bullock, S., Di Paolo, E., Noble, J., Bedau, M., Husbands, P., Kirby, S., Seth, A.:
The view from elsewhere: perspectives on ALife modelling. Artif. Life 8(1), 87–100 (2002)
66. Williams, R.A., Greaves, R., Read, M., Timmis, J., Andrews, P.S., Kumar, V.: In silico inves-
tigation into dendritic cell regulation of CD8Treg mediated killing of Th1 cells in murine
experimental autoimmune encephalomyelitis. BMC Bioinform. 14, S6–S9 (2013)
67. Wilson, S.P., McDermid, J.A.: Integrated analysis of complex safety critical systems. Comput.
J. 38(10), 765–776 (1995)
68. Woodcock, J., Loomes, M.: Software Engineering Mathematics. Addison-Wesley (1990)
69. Woodcock, J., Stepney, S., Cooper, D., Clark, J.A., Jacob, J.L.: The certiﬁcation of the Mondex
electronic purse to ITSEC Level E6. Form. Asp. Comput. 20(1), 5–19 (2008)

From Parallelism to Nonuniversality: An
Unconventional Trajectory
Selim G. Akl
Abstract I had the distinct pleasure of meeting Dr. Susan Stepney in September
2006, on the occasion of the Fifth International Conference on Unconventional Com-
putation (UC’06) held at the University of York in the United Kingdom. I learned a
great deal at that conference co-chaired by Dr. Stepney, enough to motivate me to
organize the sixth edition of that conference series the following year in Kingston,
Ontario, Canada. This chapter relates my adventures in unconventional computation
and natural computation, and offers some recollections on the path that led me to
nonuniversality. It is dedicated to Susan in recognition of her contributions and in
celebration of her 60th birthday.
Keywords Parallelism · Parallel computer · Inherently parallel computations ·
Unconventional computing · Unconventional computational problems · Quantum
computing · Quantum chess · Computational geometry · Quantum cryptography ·
Superposition · Entanglement · Universality · Nonuniversality · Superlinear
performance · Speed · Quality · Natural computing · DNA computer ·
Biomolecular computing · Simulation · Key distribution · Optical computing ·
Sensor network · Cellular automata · Fully homomorphic encryption · Cloud
security.
1
Introduction
My thinking has always been inﬂuenced by unconventional wisdom. Throughout
my life I always tried to see if things could be done differently. I found it incredi-
bly interesting (and tantalizingly mischievous) to question dogmas and established
ideas. Sometimes this worked in my favor, and I discovered something new. Often
I was not so lucky, and I rufﬂed some feathers. Either way, it was a special thrill to
explore the other side of the coin, the home of uncommon sense. Since the present
S. G. Akl (B)
School of Computing, Queen’s University, Kingston, ON K7L 3N6, Canada
e-mail: akl@cs.queensu.ca
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_6
123

124
S. G. Akl
chapter recounts my journey to unconventional computation and nonuniversality, I
will restrict my recollections to this topic. This work is an extended version of a short
section contributed by the author to a recent collaboration [5].
2
The Early Years
I begin with my days as a student. While I followed the conventional trajectory from
undergraduate, to Master’s student, to Doctoral candidate, my university experience
was far from conventional.
2.1
Aérospatiale
It all started in 1969. I remember that year well. It was one of the most exciting years
for me. As a space travel buff, how can I forget witnessing the ﬁrst lunar landing?
Humanity had just realized its dream of walking on another celestial body. But of
more direct relevance to me personally, 1969 was the year I discovered Computer
Science.AsanengineeringstudentIwasonaninternshipatSudAviationinToulouse,
France. Later known as Aérospatiale, this was the company that built (in partnership
with British Aircraft Corporation) the supersonic passenger jet Concorde, which
was at that time doing test ﬂights, while the now popular Airbus airplane was being
designed. As an intern, I was offered a choice to join a number of engineering
departments. Unsurprisingly, not knowing a single thing about computer science, I
selected the programming group! And this is how I came to be part of the team that
wrote the ﬁrst ﬂight simulator for the Airbus. From simulation, I learned that you
could create worlds that did not previously exist, yet worlds as real, as useful, as
effective, and as far reaching as one could ever imagine. I knew there and then that
this is what I wanted to do. I wanted one day to become a computer scientist.
2.2
Exactly One Computer
For my undergraduate capstone project, I decided to choose as a topic computer simu-
lation. What is unconventional about this choice is that I was an electrical engineering
student planning to graduate with an electrical engineering degree (not computer sci-
ence, for there was no such department at my university). The year was 1971. There
was exactly one computer in the entire university, and it was an IBM 1620. The
1620 fully occupied a large room. In order to run my program, I had to book the
room overnight, and hope and pray that things will work out. Lucky for me, the 1620
behaved ﬂawlessly, my program worked, and I graduated.

From Parallelism to Nonuniversality: An Unconventional Trajectory
125
2.3
Who in the World?
Now I was ready and eager to do a Ph.D. in computer science, a subject which,
like I said, did not exist at my university. The reaction from the professors was
negative. “After all”, I was told, “who in the world could possibly need, even a
Master’s in computing, never mind a Ph.D.?” Such was society’s view of computing
in the late 1960s. Computers were regarded merely as number crunching machines,
useful mainly for generating telephone bills (and the like) at the end of the month.
Consequently, I was advised that a much better idea would be to do graduate work
in mathematics. And so I did (but not precisely). What happened is that I ended
up writing an M.Sc. thesis on computer switching circuits, disguised as a thesis in
mathematical logic. Furthermore, at the time, the common approach to tackle the kind
of problem I chose to solve, was software based. Instead of following the trend, my
thesis explored a counterintuitive, yet effective, hardware solution to error detection
in asynchronous switching circuits [116].
2.4
Give Me Something More Useful!
To study for the Ph.D., I decided to move to Canada. The ofﬁcial at the Canadian
Embassy, a very nice and very helpful gentleman, conducted my visa pre-application
interview. There was a form to be ﬁlled out. “What is your profession?” he asked.
I replied (proudly!): “Computer Science.” He looked at me quizzically: “That will
not do. In order to improve your chances of being admitted, you need to give me
something more useful than this, something of beneﬁt to society. What else do you
do?” Hesitantly, I offered: “I work in the evenings as a movie projectionist at a
cultural center.” The kind man beamed: “Excellent! Let us put down Audio Visual
Specialist!”
2.5
Chess and Geometry
Some time later, during my Ph.D. work on combinatorial optimization, I had the good
fortune of meeting two exceptional individuals who allowed me to join them in their
research when I needed a break from my own work. Dr. Monty Newborn introduced
me to computer chess. Together, we wrote a paper that described a new method,
the “killer heuristic”, which allowed chess-playing programs to run much faster by
signiﬁcantly reducing the size of the trees that they searched [60]. Dr. Godfried
Toussaint taught me computational geometry. Our delightful dinner conversations at
the Basha restaurant in Montreal led to many results, most memorably the fastest
known practical algorithm for computing the convex hull of a set of planar points
[67]. Throughout my career I will return often to chess and geometry [45].

126
S. G. Akl
3
Parallel Computation
Shortly after ﬁnishing my Ph.D. and starting an academic career, I went back to
thinking about computer chess, and how to improve the chess-playing programs by
making them run faster, and hence allowing them to explore more positions. The idea
of searching game trees in parallel, while obvious today, seemed like a huge insight
at the time. This approach proved very successful and opened for me a research path
on which I am still to this day. Parallel computation is a ﬁeld that requires a com-
pletely different way of thinking. From shared-memory machines to interconnection
networks, from special-purpose combinational circuits to reconﬁgurable computers,
and everything else in between, parallel algorithms are only limited by the extent
of their designer’s imagination [58]. Indeed, in my opinion, sequential computing
is only a very primitive special case of the much more general, richer, and more
powerful algorithmic possibilities offered by parallelism [10, 14, 17, 52, 54, 61, 64,
65, 108].
3.1
Parallel Algorithms
My very ﬁrst paper on parallel algorithms, which also happened to be the ﬁrst paper
ever on searching game trees in parallel, was co-authored with D. Barnard and R.
Doran [47, 48]. In it, the alpha-beta algorithm for searching decision trees is adapted
to allow parallel activity in different parts of a tree during the search. Our results
indicated that a substantial reduction in the time required by the search occurs because
of the use of parallelism [12].
To test the waters of this new ﬁeld a little more in depth, I embarked on a project
to write a monograph on parallel algorithms, choosing as subject the problem of
sorting, the most-studied problem in computer science. My book Parallel Sorting
Algorithms appeared in 1985 [11], followed by a Japanese edition shortly thereafter.
Topics covered included specialized networks for sorting, as well as algorithms for
sorting on various parallel processor architectures such as linear arrays, perfect shuf-
ﬂes, meshes, trees, cubes, synchronous shared-memory machines, and asynchronous
multiprocessors, algorithms for external sorting, and lower bounds on parallel sort-
ing. It was the ﬁrst book to appear in print that was devoted entirely to parallel
algorithms (and later earned me the privilege of writing an encyclopedia entry on
parallel sorting [33], one of three encyclopedia articles I have authored).
Soon after, I published a second, more comprehensive, book on The Design and
AnalysisofParallelAlgorithms,eachchapterofwhichwasdevotedtoadifferentcom-
putational problem [13], namely, selection, merging, sorting, searching, generating
permutations and combinations, matrix operations, numerical problems, computing
Fourier transforms, graph theoretic problems, computational geometry, traversing
combinatorial spaces, decision and optimization, and analyzing the bit complexity

From Parallelism to Nonuniversality: An Unconventional Trajectory
127
of parallel computations. The book was translated into several languages, including
Spanish, Italian, and Chinese.
A third book entitled Parallel Computation: Models and Methods took the
approach of presenting parallel algorithms from the twin perspectives of models
of computation and algorithmic paradigms [15]. Thus there are chapters on combi-
national circuits, parallel preﬁx computation, divide and conquer, pointer-based data
structures, linear arrays, meshes and related models, hypercubes and stars, models
using buses, broadcasting with selective reduction, and parallel synergy. The book
is listed on amazon.com among the 21 best books on algorithmics–not just parallel
algorithms (Professor Christoph Koegl, the creator of the list and a theoretical com-
puter scientist at the University of Kaiserslautern, described it as “Possibly the best
book on parallel algorithms” [121]).
Using my knowledge of parallel algorithms, I returned to computational geometry,
teaming up with Dr. Kelly Lyons to produce Parallel Computational Geometry [55].
The book describes parallel algorithms for solving a host of computational geomet-
ric problems, such as the convex hull, intersection problems, geometric searching,
visibility, separability, nearest neighbors, voronoi diagrams, geometric optimization,
and triangulations of polygons and point sets.
3.2
Parallel Computer
I should also mention that in my work, theory was never too far from practice. Indeed,
as early as 1982, my students and I built one of the earliest functioning parallel com-
puters. The computer consisted of twelve processors, each a full-ﬂedged computer
with a microprocessor, a memory and four communication lines. Its architecture was
hardware-reconﬁgurable in the sense that the processors could be connected in a
number of different ways, including a linear array, a two dimensional array, a per-
fect shufﬂe, and a tree. It provided great ﬂexibility and speed and allowed several
algorithms to be tested on different architectures [9].
3.3
Superlinear Performance
A great deal of my work in parallelism focused on discovering what surprising results
in speed and quality can be obtained once the power of parallelism is unleashed.
3.3.1
Speed
Of course, the principal reason for using parallelism is to speed up computations
that are normally run on sequential machines. In order to measure the improvement
in processing speed afforded by a particular parallel algorithm, the following ratio,

128
S. G. Akl
known as the speedup is used: The running time of the best sequential algorithm for
the problem at hand, divided by the running time of the parallel algorithm. It was
widely believed that an algorithm with n processors can achieve a speedup at most
equal to n. It was also believed that if only p processors, where p < n, are available,
then the slowdown, that is, the running time of the p-processor algorithm divided by
the running time of the n-processor algorithm, is smaller than n/p. Unconvinced, I set
outtoprovethatthesetwo“folkloretheorems”areinfactfalse.Iexhibitedseveralout-
of-the-ordinary computational problems for which an n-processor parallel algorithm
achieves a speedup superlinear in n (for example, a speedup of 2n), while a p-
processor parallel algorithm, where p < n, causes a slowdown superlinear in n/p
(for example, a slowdown of 2n/p) [19, 43, 53].
One-way functions provide a nice example of superlinear speedup. A function f
is said to be one-way if the function itself takes little time to compute, but (to the
best of our knowledge) its inverse f −1 is computationally prohibitive. For example,
let x1, x2, . . . , xn be a sequence of integers. It is easy to compute the sum of a given
subset of these integers. However, starting from a sum, and given only the sum, no
efﬁcient algorithm is known to determine a subset of the integer sequence that add
up to this sum.
Consider that in order to solve a certain problem, it is required to compute
g(x1, x2, . . . , xn), where g is some function of n variables. The computation of
g requires Ω(n) operations. For example, g(x1, x2, . . . , xn) = x2
1 + x2
2 + · · · + x2
n,
might be such a function. The inputs x1, x2, . . . , xn needed to compute g are received
as n pairs of the form ⟨xi, f (x1, x2, . . . , xn)⟩, for i = 1, 2, . . . , n.
Thefunction f possessesthefollowingproperty:Computing f from x1, x2, . . . , xn
is done in n time units; on the other hand, extracting xi from f (x1, x2, . . . , xn) takes
2n time units.
Because the function g is to be computed in real time, there is a deadline constraint:
If a pair is not processed within one time unit of its arrival, it becomes obsolete (it is
overwritten by other data in the ﬁxed-size buffer in which it was stored).
Sequential Solution. The n pairs arrive simultaneously and are stored in a buffer,
waiting in queue to be processed. In the ﬁrst time unit, the pair ⟨x1, f (x1, x2, . . . , xn)⟩
is read and x2
1 is computed. At this point, the other n −1 pairs are no longer available.
In order to retrieve x2, x3, . . . , xn, the sequential processor p1 needs to invert f . This
requires (n −1) × 2n time units. It then computes g(x1, x2, . . . , xn) = x2
1 + x2
2 +
· · · + x2
n. Consequently, the sequential running time is given by t1 = 1 + (n −1) ×
2n + 2 × (n −1) time units. Clearly, this is optimal considering the time required to
obtain the data.
Parallel Solution. Once the n pairs are received, they are processed immediately
by an n-processor parallel computer, in which the processors p1, p2, . . . , pn share
a common memory for reading and for writing [15]. Processor pi reads the pair
⟨xi, f (x1, x2, . . . , xn)⟩and computes x2
i , for i = 1, 2, . . . , n. The n processors now
compute the sum g(x1, x2, . . . , xn) using a concurrent-write operation to the shared
memory. It follows that the parallel running time is equal to tn = 1.
Speedup and slowdown. The speedup provided by the parallel computer over
the sequential one, namely, S(1, n) = (n −1) × 2n + 2n −1, is superlinear in n

From Parallelism to Nonuniversality: An Unconventional Trajectory
129
and thus contradicts the speedup folklore theorem. What if only p processors are
available on the parallel computer, where 2 ≤p < n? In this case, only p of the n
variables (for example, x1, x2, . . . , x p) are read directly from the input buffer (one
by each processor). Meanwhile, the remaining n −p variables vanish and must be
extracted from f (x1, x2, . . . , xn). It follows that the parallel running time is now
tp = 1 + ⌈(n −p)/p⌉× 2n +
⎛
⎝
logp(n−p)

i=1
⌈(n −p)/pi⌉
⎞
⎠+ 1,
where the ﬁrst term is for computing x2
1 + x2
2 + · · · + x2
p, the second for extracting
x p+1, x p+2, . . . , xn, the third for computing x2
p+1 + x2
p+2 + · · · + x2
n, and the fourth
for producing g. Therefore, tp/tn is asymptotically larger than ⌈n/p⌉by a factor that
grows exponentially with n, and the slowdown folklore theorem is violated.
3.3.2
Quality
As a second manifestation of superlinear performance, I introduced the notion of
quality-up as a measure of the improvement in the quality of a solution obtained
through parallel computation [16]. I proved that a parallel computer can in some
circumstances obtain a solution to a problem that is better than that obtained by
a sequential computer. What constitutes a better solution depends on the problem
under consideration. Thus, for example, ‘better’ means ‘closer to optimal’ for opti-
mization problems, ‘more accurate’ for numerical problems, and ‘more secure’ for
cryptographic problems. A source coding algorithm is ‘better’ if it yields a higher
compression rate. An error correction code is ‘better’ if it provides a superior error
correction capability. I exhibited several classes of problems having the property
that a solution to a problem in the class, when computed in parallel, is far superior
in quality than the best one obtained on a sequential computer [19]. This was a far
more unconventional idea than superlinear speedup, for it is a fundamental belief
in computer science that anything that can be computed on one computer can be
obtained exactly on another computer (through simulation, if necessary).
To illustrate, consider the following computational environment:
1. A computer system receives a stream of inputs in real time.
2. The numerical problem to be solved here is to ﬁnd, for a continuous function
f (x), a zero xapprox that falls between x = a and x = b using, for example, the
bisection algorithm. At the beginning of each time unit, a new 3-tuple ⟨f, a, b⟩is
received by the computer system.
3. It is required that ⟨f, a, b⟩be processed as soon as it is received and that xapprox
be produced as output as soon as it is computed. Furthermore, one output must
be produced at the end of each time unit (with possibly an initial delay before the
ﬁrst output is produced).

130
S. G. Akl
4. The operations of reading ⟨f, a, b⟩, performing one iteration of the algorithm, and
producing xapprox as output once it has been computed, can be performed within
one time unit.
Sequential Solution. Here, there is a single processor whose task is to read each
incoming 3-tuple, to compute xapprox, and to produce the latter as output. Recall
that the computational environment we assumed dictates that a new 3-tuple input
be received at the beginning of each time unit, and that such an input be processed
immediately upon arrival. Therefore, the sequential computer must have ﬁnished
processing a 3-tuple before the next one arrives. It follows that, within the one
time unit available, the algorithm can perform no more than one iteration on each
input ⟨f, a, b⟩. The approximate solution computed by the sequential computer is
xapprox = (a + b)/2. This being the only option available, it is by default the best
solution possible sequentially.
Parallel Solution. When solving the problem on an n-processor computer,
equipped with an array of processors p1, p2, . . . , pn, it is evident that one processor,
for example p1, must be designated to receive the successive input 3-tuples, while it
is the responsibility of another processor, for example pn, to produce xapprox as out-
put. The fact that each 3-tuple needs to be processed as soon as it is received implies
that the processor must be ﬁnished processing a 3-tuple before the next one arrives.
Since a new 3-tuple is received every time unit, processor p1 can perform only one
iteration on each 3-tuple it receives. Unlike the sequential solution, however, the
present algorithm can perform additional iterations. This is done as follows. Once
p1 has executed its single iteration on ⟨f, a1, b1⟩, it sends ⟨f, a2, b2⟩to p2, and turns
its attention to the next 3-tuple arriving as input. Now p2 can execute an additional
iteration before sending ⟨f, a3, b3⟩to p3. This continues until xapprox = (an + bn)/2
is produced as output by pn. Meanwhile, n −1 other 3-tuple inputs co-exist in the
array of processors (one in each of p1, p2, . . . , pn−1), at various stages of processing.
One time unit after pn has produced its ﬁrst xapprox, it produces a second, and so on,
so that an output emerges from the array every time unit. Note that each output xapprox
is the result of applying n iterations to the input 3-tuple, since there are n processors
and each executes one iteration.
Quality-up. In what follows we derive a bound on the size of the error in xapprox
for the sequential and parallel solutions. Let the accuracy of the solution be deﬁned
as the inverse of the maximum error.
Sequentially, one iteration of the bisection algorithm is performed to obtain xapprox.
The maximum error is |b −a|/2.
In parallel, each 3-tuple input is subjected to n iterations of the bisection algorithm,
where each processor performs one iteration. The maximum error is |b −a|/2n.
By deﬁning quality-up as the ratio of the parallel accuracy to the sequential accu-
racy, quality-up(1, n) = 2(n−1). This suggests that increasing the number of proces-
sors by a factor of n leads to an increase in the level of accuracy by a factor on
the order of 2n. In other words, the improvement in quality is exponential in n, the
number of processors on the parallel computer.

From Parallelism to Nonuniversality: An Unconventional Trajectory
131
3.4
From Parallel to Unconventional
Because parallelism is inherent to all computational paradigms that later came to be
knownas“unconventional”,mytransitionfromarchitecture-dependentparallelismto
substrate-dependent parallelism and to inherently-parallel computational problems,
was logical, natural, and easy. This is how I embraced quantum computing [59], opti-
cal computing [114], biomolecular computing [93], cellular automata [119], slime
mold computing [3], unconventional computational problems [38], nonuniversality
in computation [37], and various other unconventional paradigms [18, 105–107,
120]. My ﬁrst speciﬁc contribution in this direction was made in the early 1990s,
when I developed, with Dr. Sandy Pavel, processor arrays with reconﬁgurable opti-
cal networks for such computations as integer sorting, the Hough transform, and
other computations [109–113]. Since most of my work has focused on that part of
unconventional computation that one might call natural computing, the next section
brieﬂy outlines my perspective of this area of research [39].
4
Nature Computes
In our never-ending quest to understand the workings of Nature, we humans began
with the biological cell as a good ﬁrst place to look for clues. Later, we went down to
the molecule, and then further down to the atom, in hopes of unraveling the mysteries
of Nature. It is my belief that the most essential constituent of the Universe is the
bit, the unit of information and computation. Not the cell, not the molecule, not the
atom, but the bit may very well be the ultimate key to reading Nature’s mind.
Does Nature compute? Indeed, we can model all the processes of Nature as infor-
mation processes. For example, cell multiplication and DNA replication are seen as
instances of text processing. A chemical reaction is simply an exchange of electrons,
that is, an exchange of information between two molecules. The spin of an atom,
whether spin up or spin down, is a binary process, the answer to a ‘yes’ or ‘no’
question. Information and computation are present in all natural occurrences, from
the simplest to the most complex. From reproduction in ciliates to quorum sensing
in bacterial colonies, from respiration and photosynthesis in plants to the migration
of birds and butterﬂies, and from morphogenesis to foraging for food, all the way to
human cognition, Nature appears to be continually processing information.
Computer scientists study information and computation in Nature in order to:
1. Better understand natural phenomena. We endeavor to show that the computa-
tional paradigm is capable of modeling Nature’s work with great precision. Thus,
when viewed as computations, the processes of Nature may be better explained
and better understood at their most basic state.
2. Exhibit examples of natural algorithms whose features are sufﬁciently attrac-
tive, so as to inspire effective algorithms for conventional computers. Nature’s

132
S. G. Akl
algorithms may be more efﬁcient than conventional ones and may lead to better
answers in a variety of computational situations.
3. Identifyproblemswherenaturalprocessesthemselvesaretheonlyviableapproach
towards a solution. Such computational problems may occur in environments
where conventional computers are inept, in particular when living organisms,
including the human body itself, are the subject of the computation.
4. Obtain a more general deﬁnition of what it means ‘to compute’. For example, is
there more to computing than arithmetic and logic? Natural phenomena involve
receiving information from, and producing information to, the external physical
environment–are these computations?
The next three sections offer a glimpse into the evolution of my approach to
unconventional computation, in general, and natural computing, in particular.
5
Quantum Computing and Quantum Cryptography
My foray into the quantum realm was one of the most intriguing and most enjoyable
experiences in my research life. Everyone knows the strange and somewhat far-
fetched connections that some seek to establish between quantum physics and various
mystical beliefs (e.g. eastern religions) and everyday observations (e.g. the behavior
of identical twins). In what follows I will stay away from these speculations, and
highlight instead four adventures in the quantum world.
5.1
Quantum Computers can do More than Conventional
Computers
Quantum computers are usually promoted as being able to quickly perform compu-
tations that are otherwise infeasible on classical computers (such as factoring large
numbers). My work with Dr. Marius Nagy, by contrast, has uncovered computations
for which a quantum computer is, in principle, more powerful than any conventional
computer. One example of such a computation is that of distinguishing among the 2n
entangled states of a quantum system of n qubits: This computation can be performed
on a quantum computer but not on any classical computer [88]. Suppose that we have
a quantum system composed of n qubits whose state is not known exactly. What we
know with certainty is that the system can be described by one of the following 2n
entangled states:

From Parallelism to Nonuniversality: An Unconventional Trajectory
133
1
√
2
(|000 · · · 0⟩± |111 · · · 1⟩),
1
√
2
(|000 · · · 1⟩± |111 · · · 0⟩),
...
(1)
1
√
2
(|011 · · · 1⟩± |100 · · · 0⟩).
The challenge for the two candidate computers, conventional and quantum, is to
correctly identify the state of the system by resorting to all of their measurement
and computational abilities. Alternatively, the problem can also be formulated as a
function computation (evaluation), with the unknown quantum state as the input and
the corresponding index (between 0 and 2n −1) as the output.
It is shown in [88] that neither of the two computers can perform this task by using
measurements only. However, if we resort to their processing capabilities, the situ-
ation changes. Unitary operators preserve inner products, so any unitary evolution of
thesystemdescribedby(1)willnecessarilytransformitintoanotherorthonormalbasis
set. Therefore, a unitary transformation must exist that will allow a subsequent mea-
surement in the standard computational basis without any loss of information. Indeed,
we demonstrated that such a transformation not only exists, but that in fact it can be
implemented efﬁciently. Our result is stated as follows: The transformation between
the following two orthonormal basis sets for the state space spanned by n qubits,
1
√
2
(|000 · · · 0⟩+ |111 · · · 1⟩) ←→|000 · · · 0⟩,
1
√
2
(|000 · · · 0⟩−|111 · · · 1⟩) ←→|111 · · · 1⟩,
1
√
2
(|000 · · · 1⟩+ |111 · · · 0⟩) ←→|000 · · · 1⟩,
1
√
2
(|000 · · · 1⟩−|111 · · · 0⟩) ←→|111 · · · 0⟩,
...
(2)
1
√
2
(|011 · · · 1⟩+ |100 · · · 0⟩) ←→|011 · · · 1⟩,
1
√
2
(|011 · · · 1⟩−|100 · · · 0⟩) ←→|100 · · · 0⟩.

134
S. G. Akl
can be realized by a quantum circuit consisting of 2n −2 controlled-NOT gates and
one Hadamard gate. Due to its symmetric nature, the same quantum circuit can also
perform the inverse transformation, from the normal computational basis set to the
entangled basis set.
By applying the transformation realized by this circuit, the quantum computer
can disentangle the qubits composing the system and thus make the act of measuring
each qubit entirely independent of the other qubits. This is possible because the ﬁnal
states (after the transformation) are actually classical states which can be interpreted
as the indices corresponding to the original entangled quantum states. Obtaining the
correct answer to the distinguishability problem amounts to accurately computing
the index associated with the given input state. The procedure detailed above gives us
a reliable way to do this, 100% of the time. In other words, the function is efﬁciently
computable (in quantum linear time) by a quantum computer.
Can the classical computer replicate the operations performed by the quantum
machine? We know that a classical computer can simulate (even if inefﬁciently) the
continuous evolution of a closed quantum system (viewed as a quantum computa-
tion in the case of an ensemble of qubits). So, whatever unitary operation is invoked
by the quantum computer, it can certainly be simulated mathematically on a Turing
Machine. The difference resides in the way the two machines handle the uncertainty
inherent in the input. The quantum computer has the ability to transcend this uncer-
tainty about the quantum state of the input system by acting directly on the input
in a way that is speciﬁc to the physical support employed to encode or describe the
input. The classical computer, on the other hand, lacks the ability to process the
information at its original physical level, thus making any simulation at another level
futile exactly because of the uncertainty in the input.
It is worth noting that had the input state been perfectly determined, then any
transformation applied to it, even though quantum mechanical in nature, could have
been perfectly simulated using the classical means available to a Turing Machine.
However, in our case, the classical computer does not have a description of the input
in classical terms and can only try to obtain one through direct measurement. This
will in turn collapse the superposition characterizing the input state, leaving the
classical computer with only a 50% probability of correctly identifying the original
quantum state. This means that the problem cannot be solved classically, not even
by a Probabilistic Turing Machine. There is no way to improve the 50% error rate of
the classical approach in attempting to distinguish among the 2n states.
This problem taught us that what draws the separation line between a quantum
and a classical computer, in terms of computational power, is not the ability to extract
information from a quantum system through measurements, but the ability to process
information at the physical level used to represent it. For the distinguishability prob-
lem presented here, this is the only way to deal with the non-determinism introduced
by superposition of states.
At this point, it is important to clarify the implications of our result on the Church-
Turing thesis. The deﬁnition of the Turing machine was an extraordinary achievement
in abstracting out computation as an information manipulating process. But although
the model was thought to be free of any physical assumptions, it is clear today that the

From Parallelism to Nonuniversality: An Unconventional Trajectory
135
description of the Turing machine harbors an implicit assumption: the information it
manipulates is classical. Computation is a physical process and the Turing machine
computes in accord with the laws of classical physics.
However, the success of quantum mechanics in explaining the reality of the micro-
cosmos is challenging our traditional views on information processing, forcing us to
redeﬁne what we mean by computation. In the context of quantum computation, the
data in general, and the input in particular, are not restricted to classical, orthogonal
values, but can be arbitrary superpositions of them. Therefore, computational prob-
lems, such as distinguishing among entangled quantum states, are not an attack on
the validity of the Church-Turing thesis, but rather they precisely deﬁne its scope.
It is in these terms that our result [88] has to be understood: The set of functions
computable by a classical Turing machine is a proper subset of those computable
using a quantum Turing machine.
5.2
Key Distribution Using Public Information
Since ancient times, cryptography had been used primarily in military operations,
in diplomatic and intelligence activities, and in the protection of industrial secrets.
By the late 1970s, cryptography emerged as an endeavor of public interest. It is at
that time that I became involved in cryptographic research. In the summer of 1981,
I attended the ﬁrst ever conference on research in cryptology (that is, cryptography
and cryptanalysis) to be organized by, and open to, civilians. The conference was
held on the campus of the University of California, Santa Barbara, August 24–26.
Attendees were a handful of academics and researchers (who wore name tags) and a
couple of spy agency intelligence operatives (without name tags). We slept on bunk
beds in the university dormitory, ate our meals at the campus cafeteria, and held our
meetings in an undergraduate classroom. It was a far cry from today’s gigantic and
expensive security conferences. It was also a shock to my wife, as we happened to
be celebrating our honeymoon at that time! She later forgave me.
My early work in this area spanned the spectrum of cryptologic research, most
particularly cryptosystems [8, 56, 57], digital signatures [6, 7, 83–85], access con-
trol in a hierarchy [66, 80, 81], and database security [50, 51, 76, 79]. This interest
continues to this day, with new results in graph encryption [44], and fully homomor-
phic cryptography (which allows operations on encrypted data that are stored on an
untrusted cloud) [46].
However, most of my recent work in this ﬁeld has been in quantum cryptography.
Dr. Marius Nagy, Dr. Naya Nagy, and I proved that, contrary to established belief,
authentication between two parties, for cryptographic purposes, can be performed
through purely quantum means [99]. Our quantum key distribution protocols produce
secret information using public information only, which was thought to be impossible
for any cryptosystem [91, 92, 94, 100]. We have also provided for the ﬁrst time
quantum cryptographic solutions to the problems of security and identity protection
in wireless sensor networks [101, 102], multilevel security in hierarchical systems

136
S. G. Akl
[95], and coping with decoherence [90], as well as exposing a less well known aspect
of quantum cryptography [104].
The following example illustrates the quantum key distribution protocol described
in [104]. Let A and B be two parties wishing to establish a secret cryptographic key
in order to protect their communications. We further assume that A and B share an
array of ten entangled qubit pairs
(q A
1 , q B
1 ), (q A
2 , q B
2 ), . . . , (q A
10, q B
10),
such that q A
1 , q A
2 , . . . , q A
10 are in A’s possession, and q B
1 , q B
2 , . . . , q B
10 are in B’s
possession. Note that an array of ten entangled qubit pairs is obviously far too short
to be of any practical use; it is, however convenient and sufﬁcient for this exposition.
The type of entanglement used here is phase incompatibility. This means that if A
measures q A
i and obtains a classical bit 1 (0), then it is guaranteed that if B transforms
(the now collapsed) q B
i using a Hadamard gate, the resulting classical bit is 0 (1).
The situation is symmetric if B measures q B
i ﬁrst. In order to obtain a secret key to
be used in their communications, both A and B apply the steps below.
Measuring the entangled qubits. For each qubit, both A and B have the choice of
measuring the qubit directly, or applying a Hadamard gate H to the qubit ﬁrst and
then measuring it. Let A’s random choice be:
q A
1 , Hq A
2 , Hq A
3 , q A
4 , q A
5 , q A
6 , Hq A
7 , Hq A
8 , q A
9 , q A
10,
yielding 1, 1, 1, 0, 0, 0, 0, 1, 1, 1. Similarly, let B’s random choice be:
Hq B
1 , Hq B
2 , q B
3 , Hq B
4 , q B
5 , q B
6 , q B
7 , Hq B
8 , Hq B
9 , q B
10,
yielding 0, 1, 0, 1, 1, 0, 1, 0, 0, 1. Of the four measurement options
(q A
i , Hq B
i ), (Hq A
i , q B
i ), (q A
i , q B
i ), (Hq A
i , Hq B
i ),
only the ﬁrst two are ‘valid’. This means that there are only ﬁve valid qubit pairs in
our example, namely,
(q A
1 , Hq B
1 ), (Hq A
3 , q B
3 ), (q A
4 , Hq B
4 ), (Hq A
7 , q B
7 ), (q A
9 , Hq B
9 ),
with values
(1, 0), (1, 0), (0, 1), (0, 1), (1, 0).
Publishing the measurement strategy. Suppose that the digit 0 is used by A and
B to indicate that a qubit has been measured directly, and the digit 1 to indicate
that a Hadamard gate was used. As well, A and B choose to publish the index and
value of 40% of their qubits (that is, 4 qubits in this case) selected at random. Thus,
the string 0110001100 is disclosed as A’s measurement strategy, and the quadruple

From Parallelism to Nonuniversality: An Unconventional Trajectory
137
(0001)1, (0010)1, (1001)1, (1010)1 is made public as A’s four randomly selected
qubits 1, 2, 9, 10. Similarly, the string 1101000110 is disclosed as B’s measurement
strategy, and the quadruple (0001)0, (0101)1, (0111)1, (1000)0 is made public as
B’s four randomly selected qubits 1, 5, 7, 8.
Checking for eavesdropping. By computing the exclusive-OR of their two mea-
surement strategies, A and B can determine that only qubits 1, 3, 4, 7, 9 have been
measured correctly:
(0110001100) ⊗(1101000110) = 1011001010.
A compares the values of q A
1 and q A
7 with the values of q B
1 and q B
7 published by
B. Similarly, B compares the values of q B
1 and q B
9 with the values of q A
1 and q A
9
published by A. In each case, in the absence of eavesdropping, the values must be
the opposite of one another.
For large qubit arrays and a large number of qubits checked, A and B will sepa-
rately reach the same conclusion as to whether they should continue with the protocol,
or that malevolent interference has disrupted the entanglement and that, therefore,
they must discard everything and start all over.
Constructing the secret key. Assuming no eavesdropping has been detected, the
unpublished qubits form the secret key. In our small example these qubits are qubits
3 and 4, that is, the secret key is 10.
5.3
Carving Secret Messages Out of Public Information
Usingourpreviousresultonone-timepads[96],Dr.NayaNagy,Dr.MariusNagy,and
I showed that secret information can be shared or passed from a sender to a receiver
even if not encoded in a secret message. No parts of the original secret information
ever travel via communication channels between the source and the destination. No
encoding/decoding key is ever used. The two communicating partners, are endowed
with coherent qubits that can be read and set while keeping their quantum values over
time. Also any classical communication channel need not be authenticated. As each
piece of secret information has a distinct public encoding, the protocol is equivalent
to a one-time pad protocol [103].
Brieﬂy described, the idea of this result is as follows. Suppose that the sender and
the receiver each holds one array of a pair of arrays of entangled qubits. Speciﬁcally,
the ith qubit in the array held by the sender is entangled with the ith qubit in the
array held by the receiver. When the sender wishes to send a message M made up of
the bits
m1, m2, . . . , mn,
she looks up the positions in her array of an arbitrary sequence of qubits that (when
measured) would yield the message

138
S. G. Akl
m1, m2, . . . , mn.
Let these positions (that is, the indices) of these (not necessarily contiguous) qubits
be
x1, x2, . . . , xn,
in which, for example, x1 = 24, x2 = 77, x3 = 5, and so on. The sender transmits
these numbers to the receiver who then reconstructs the message M using the indices
x1, x2, . . . , xn,
in his array of qubits. One could say that this is, in some sense, the “book cipher”
approach to cryptography revisited, with a quantum twist that is theoretically
unbreakable.
5.4
Quantum Chess
Games of strategy, and in particular chess, have long been considered as true tests of
machine intelligence, namely, the ability of a computer to compete against a human
in an activity that requires reason. Today, however, most (if not all) human players do
not stand a chance against the best computer chess programs. In an attempt to restore
some balance, I proposed Quantum Chess (QC), a version of chess that includes an
element of unpredictability, putting humans and computers on an equal footing when
faced with the uncertainties of quantum physics [32]. Unlike classical chess, where
each piece has a unique and known identity, each piece in QC is in a superposition
of states, revealing itself to the player as one of its states only when it is selected to
move [32, 36]. It is essential to note that bringing quantum physics into chess should
be understood as being signiﬁcantly different from merely introducing to the game
an element of chance, the latter manifesting itself, for example, in games involving
dice or playing cards, where all the possible outcomes and their odds are known in
advance.
The behavior of the Quantum Chess pieces is deﬁned as follows:
1. Each Quantum Chess piece is in a superposition of states. Since there are 16
pieces for each side, then (ignoring color for simplicity) four qubits sufﬁce to
represent each piece distinctly. Again for simplicity, we assume henceforth that
each Quantum Chess piece is a superposition of two conventional Chess pieces.
2. “Touching” a piece is tantamount to an “observation”; this collapses the super-
position to one of the classical states, and this deﬁnes the move that this piece
makes.
3. A piece that lands on a white square remains in the classical state in which
it arrived. By contrast, a piece that lands on a black square is considered to

From Parallelism to Nonuniversality: An Unconventional Trajectory
139
have traversed a quantum circuit, thereby undergoing a quantum transformation:
Having arrived in a classical state, it recovers its original quantum superposition.
4. Initially, each piece on the board is in a quantum superposition of states. However,
neither of the two players knows the states in superposition for any given piece.
5. At any moment during the game, the locations on the board of all the pieces are
known. Each of the two players can see (observe) the states of all the pieces in
classical state, but not the ones in superposition.
6. If a player likes the current classical state of a piece about to be moved, then
he/she will attempt not to land on a quantum circuit. In the opposite case, the
player may wish to take a chance by landing the piece on a quantum circuit.
A true QC board with true QC pieces is a long way from being constructed,
however. Therefore, my undergraduate summer student Alice Wismath implemented
a simulation of the game, and a competition was held pitting humans against a
computer. The experiment conﬁrmed that QC indeed provides a level playing ﬁeld
and drew an enormous following for QC on the Internet (in many languages). QC
received coverage by the CBC and Wired magazine, among many other outlets,
and a complimentary tip of the hat from the (then) reigning Women’s World Chess
Champion Alexandra Kosteniuk [122]. Most notably, our work was featured on the
Natural Sciences and Engineering Research Council of Canada web site [123].
Alice also created a version of Quantum Chess which includes the principle of
quantum entanglement:
1. Both players’ pieces have the same superposition combinations (the speciﬁc com-
binations are randomly assigned at game start up). Each piece therefore has a
‘twin’ piece of the opposite colour.
2. All pieces, except for the king, start in quantum state, and both of their piece
types are initially unknown. The location of the pieces within the player’s ﬁrst
two rows is random.
3. Each piece is initially entangled with its twin piece. When the piece is touched, its
twin piece (belonging to the opponent) is also touched and collapses to the same
state. Since both pieces are now in classical state, the entanglement is broken and
the pieces behave normally henceforth.
Recently, QC was back in the news when a program inspired by Alice’s imple-
mentations pitted physicist Dr. Stephen Hawking against Hollywood actor Paul Rudd
[124].
It was fun to get back to computer chess after all these years since my graduate
student days and my early work on searching game trees in parallel. Several intrigu-
ing open questions remain. As shown in my original paper, unlike a conventional
computer, a quantum computer may be able to determine the superposition hidden in
a QC piece; does this extra knowledge allow it to develop a winning strategy? What
other games of strategy are amenable to the same modiﬁcations that led to QC? Are
there implications to decision theory?

140
S. G. Akl
6
Biomolecular, Cellular, and Slime Mold Computing
In this section I will brieﬂy cover other aspects of my work in natural computing.
6.1
Biomolecular Computing
With Dr. Virginia Walker, a biologist, I co-supervised three graduate students who
built a DNA computer capable of performing a simple form of cryptanalysis [82].
They also put to the test the idea of double encoding as an approach to resisting error
accumulation in molecular biology techniques such as ligation, gel electrophoresis,
polymerase chain reaction (PCR), and graduated PCR. While the latter question
was not completely settled, several pivotal issues associated with using ligation for
double encoding were identiﬁed for future investigation, such as encoding adaptation
problems, strand generation penalties, strand length increases, and the possibility that
double encoding may not reduce the number of false negatives.
6.2
Cellular Automata
A cellular automaton (CA) is a nature-inspired platform for massively parallel com-
putation with the ability to obtain complex global behavior from simple local rules.
The CA model of computation consists of n cells in a regular arrangement, where
each cell is only aware of itself and of its immediate neighbors. In plant respira-
tion, the stomata on a leaf are an example of a natural CA. With Dr. Sami Torbey I
used the two-dimensional CA model to provide O(n1/2) running-time solutions to
computational problems that had remained open for some time:
1. The density classiﬁcation problem is arguably the most studied problem in cellular
automata theory: Given a two-state cellular automaton, does it contain more black
or more white cells?
2. The planar convex hull problem is a fundamental problem in computational geom-
etry: Given a set of n points, what is the convex polygon with the smallest possible
area containing all of them?
The solution to each of these problems was unconventional. The density classi-
ﬁcation problem was solved using a “gravity automaton”, that is, one where black
cells are programmed to “fall” down towards the bottom of the grid. The solution to
the convex hull problem programmed the cells to simulate a rubber band stretched
around the point set and then released [117, 118].
We also used cellular automata to solve a coverage problem for mobile sensor
networks, thus bringing together for the ﬁrst time two unconventional computational
models, namely, cellular automata and sensor networks [70–74, 119].

From Parallelism to Nonuniversality: An Unconventional Trajectory
141
Most recently, the CA (a highly organized and structured model) provided me
with a novel approach to solving two combinatorial problems, speciﬁcally, com-
puting single-pair as well as all-pair shortest paths in an arbitrary graph (a highly
unstructured collection of data) [42]. Such is the power of abstraction in the science
of computation: An appropriate encoding and a clever algorithm is usually all it takes
to do the job!
6.3
Computing with Slime Mold
Dr. Andrew Adamatzky and I demonstrated that the plasmodium of Physarum poly-
cephalum can compute a map of the Canadian highway system fairly accurately [1,
2, 4]. The result may be interpreted as suggesting that this amoeba, a unicellular
organism with no nervous system, no brain, no eyes, and no limbs, is capable of
performing what humans may call a complex information processing task (network
optimization). Fascinating questions arise. Do simple biologic organisms require a
central brain for cognitive behavior? What are the implications to medicine if slime
mold were to be bio-engineered to forage for diseased cells? Are there possibilities
for green computing using such simple organisms? Our experiment generated con-
siderable interest from the media, including the Discovery Channel, the National
Post, PBS Newshour, PBS NOVA, PhysOrg, Science Codex, Science Daily, Popular
Science, Scientiﬁc Canadian, CKWS TV, and the Whig Standard [125].
7
Nonuniversality in Computation
OneofthedogmasinComputerScienceistheprincipleofcomputationaluniversality,
and the attendant principle of simulation: “Given enough time and space, any general-
purpose computer can, through simulation, perform any computation that is possible
on any other general-purpose computer.” Statements such as this are commonplace in
the computer science literature, and are served as standard fare in undergraduate and
graduate courses alike [21–23]. Sometimes the statement is restricted to the Turing
Machine, and is referred to as the Church-Turing Thesis, as in: “A Turing machine can
do everything that a real computer can do” [115]. Other times the statement is made
more generally about a Universal Computer (which is not to be confused with the
more restricted Universal Turing Machine), as in: “It is possible to build a universal
computer: a machine that can be programmed to perform any computation that any
other physical object can perform” [77]. I consider it one of my most meaningful
contributions to have shown that such a Universal Computer cannot exist. This is the
Principle of Nonuniversality in Computation [20, 25, 26, 28, 31].
I discovered nonuniversality because of a challenge. While giving an invited
talk on parallel algorithms, a member of the audience kept heckling me by repeat-
edly interrupting to say that anything I can do in parallel he can do sequentially

142
S. G. Akl
(speciﬁcally, on the Turing Machine). This got me thinking: Are there computations
that can be performed successfully in parallel, but not sequentially? It was not long
before I found several such computations [49, 68].
The bigger insight came when I realized that I had discovered more than I had set
out to ﬁnd. Each of these computations had the following property: For a problem
of size n the computation could be done by a computer capable of n elementary
operations per time unit (such as a parallel computer with n processors), but could
not be done by a computer capable of fewer than n elementary operations per time
unit [34, 35, 62, 78, 87, 89, 98]. This contradicted the aforementioned principle of
simulation, and as a consequence also contradicted the principle of computational
universality [29, 37, 43]. Thus parallelism was sufﬁcient to establish nonuniversal-
ity in computation. With Dr. Nancy Salay, I later proved that parallelism was also
necessary for any computer that aspires to be universal [63].
7.1
Theoretical Proof of Nonuniversality in Computation
Suppose that time is divided into discrete time units, and let U1 be a computer capable
of V (t) elementary operations at time unit number t, where t is a positive integer and
V (t) is ﬁnite and ﬁxed a priori for all t. Here, an elementary computational operation
may be any one of the following:
1. Obtainingthevalueofaﬁxed-sizevariablefromanexternalmedium(forexample,
reading an input, measuring a physical quantity, and so on),
2. Performing an arithmetic or logical operation on a ﬁxed number of ﬁxed-size
variables (for example, adding two numbers, comparing two numbers, and so
on), and
3. Returning the value of a ﬁxed-size variable to the outside world (for example,
displaying an output, setting a physical quantity, and so on).
Each of these operations can be performed on every conceivable machine that is
referred to as a computer. Together, they are used to deﬁne, in the most general
possible way, what is meant by to compute: the acquisition, the transformation, and
the production of information.
Now all computers today (whether theoretical or practical) have V (t) = c, where
c is a constant (often a very large number, but still a constant). Here, we do not restrict
V (t) to be a constant. Thus, V (t) is allowed to be an increasing function of time,
such as V (t) = t, or V (t) = 22t, and so on, as is the case, for example, with some
hypothetical accelerating machines [29]. (The idea behind these machines goes back
to Bertrand Russell, Ralph Blake, and Hermann Weyl; recent work is surveyed in
[78]). The crucial point is that, once deﬁned, V (t) is never allowed to change, it
remains ﬁnite and ﬁxed once and for all.
Finally, U1 is allowed to have an unlimited memory in which to store its program,
as well as its input data, intermediate results, and outputs. It can interact freely with

From Parallelism to Nonuniversality: An Unconventional Trajectory
143
the outside world. Furthermore, no limit whatsoever is placed on the time taken by
U1 to perform a computation.
Theorem U: U1 cannot be a universal computer.
Proof. Let us deﬁne a computation C1 requiring W(t) operations during time unit
number t. If these operations are not performed by the beginning of time unit t + 1,
the computation C1 is said to have failed. Let W(t) > V (t), for at least one t. Clearly,
U1 cannot perform C1. However, C1 is computable by another computer U2 capable
of W(t) operations during the tth time unit. ■
This result applies to all computers, theoretical or practical, that claim to be
universal. It applies to sequential computers as well as to parallel ones. It applies to
conventional as well as unconventional computers. It applies to existing as well as
contemplated models of computations, so long as the number of operations they can
perform in one time unit is ﬁnite and ﬁxed.
7.2
Practical Proof of Nonuniversality in Computation
In order to establish nonuniversality in a concrete fashion, I exhibited functions of n
variables that are easily evaluated on a computer capable of n elementary operations
per time unit, but cannot be evaluated on a computer capable of fewer than n ele-
mentary operations per time unit, regardless of how much time and space the latter is
given [24, 30, 86, 97]. Examples of such functions are given in what follows. In all
examples, an n-processor parallel computer sufﬁces to solve a problem of size n. In
the examples of Sects.7.2.1, 7.2.3, 7.2.5, and 7.2.7, an n-processor parallel computer
is necessary to solve the problem.
7.2.1
Computations Obeying Mathematical Constraints
There exists a family of computational problems where, given a mathematical object
satisfying a certain property, we are asked to transform this object into another
which also satisﬁes the same property. Furthermore, the property is to be maintained
throughout the transformation, and be satisﬁed by every intermediate object, if any.
More generally, the computations we consider here are such that every step of the
computation must obey a certain predeﬁned mathematical constraint. (Analogies
from popular culture include picking up sticks from a heap one by one without
moving the other sticks, drawing a geometric ﬁgure without lifting the pencil, and
so on).
An example of computations obeying a mathematical constraint is provided by a
variant to the problem of sorting a sequence of numbers stored in the memory of a
computer. For a positive even integer n, where n ≥8, let n distinct integers be stored
in an array A with n locations A[1], A[2], . . ., A[n], one integer per location. Thus

144
S. G. Akl
A[ j], for all 1 ≤j ≤n, represents the integer currently stored in the jth location of
A. It is required to sort the n integers in place into increasing order, such that:
1. After step i of the sorting algorithm, for all i ≥1, no three consecutive integers
satisfy:
A[ j] > A[ j + 1] > A[ j + 2] ,
(3)
for all 1 ≤j ≤n −2.
2. When the sort terminates we have:
A[1] < A[2] < · · · < A[n].
(4)
This is the standard sorting problem in computer science, but with a twist. In
it, the journey is more signiﬁcant than the destination. While it is true that we are
interested in the outcome of the computation (namely, the sorted array, this being the
destination), in this particular variant we are more concerned with how the result is
obtained (namely, there is a condition that must be satisﬁed throughout all steps of
the algorithm, this being the journey). It is worth emphasizing here that the condition
to be satisﬁed is germane to the problem itself; speciﬁcally, there are no restrictions
whatsoever on the model of computation or the algorithm to be used. Our task is
to ﬁnd an algorithm for a chosen model of computation that solves the problem
exactly as posed. One should also observe that computer science is replete with
problems with an inherent condition on how the solution is to be obtained. Examples
of such problems include: inverting a nonsingular matrix without ever dividing by
zero, ﬁnding a shortest path in a graph without examining an edge more than once,
sorting a sequence of numbers without reversing the order of equal inputs (stable
sorting), and so on.
An oblivious (that is, input-independent) algorithm for an n/2-processor parallel
computer solves the aforementioned variant of the sorting problem handily in n steps,
by means of predeﬁned pairwise swaps applied to the input array A, during each of
which A[ j] and A[k] exchange positions (using an additional memory location for
temporary storage) [15]. Thus, for example, the input array
7 6 5 4 3 2 1 0
would be sorted by the following sequence of comparison/swap operations (each
pair of underlined numbers are compared to one another and swapped if necessary
to put the smaller ﬁrst):
7 6 5 4 3 2 1 0
6 7 4 5 2 3 0 1
6 4 7 2 5 0 3 1
4 6 2 7 0 5 1 3

From Parallelism to Nonuniversality: An Unconventional Trajectory
145
4 2 6 0 7 1 5 3
2 4 0 6 1 7 3 5
2 0 4 1 6 3 7 5
0 2 1 4 3 6 5 7
0 1 2 3 4 5 6 7
Aninput-dependentalgorithm succeedsonacomputerwith(n/2) −1processors.
However, a sequential computer, and a parallel computer with fewer than (n/2) −1
processors, both fail to solve the problem consistently, that is, they fail to sort all
possible n! permutations of the input while satisfying, at every step, the condition
that no three consecutive integers are such that A[ j] > A[ j + 1] > A[ j + 2] for all
j. In the particularly nasty case where the input is of the form
A[1] > A[2] > · · · > A[n] ,
(5)
any sequential algorithm and any algorithm for a parallel computer with fewer than
(n/2) −1 processors fail after the ﬁrst swap.
7.2.2
Time-Varying Computational Complexity
Here, the computational complexity of the problems at hand depends on time (rather
than being, as usual, a function of the problem size). Thus, for example, tracking a
moving object (such as a spaceship racing towards Mars) becomes harder as it travels
away from the observer.
Suppose that a certain computation requires that n independent functions, each of
one variable, namely, f1(x1), f2(x2), . . . , fn(xn), be computed. Computing fi(xi)
at time t requires C(t) = 2t algorithmic steps, for t ≥0 and 1 ≤i ≤n. Further,
there is a strict deadline for reporting the results of the computations: All n values
f1(x1), f2(x2), . . . , fn(xn) must be returned by the end of the third time unit, that is,
when t = 3.
It should be easy to verify that no sequential computer, capable of exactly one
algorithmic step per time unit, can perform this computation for n ≥3. Indeed,
f1(x1) takes C(0) = 20 = 1 time unit, f2(x2) takes another C(1) = 21 = 2 time
units, by which time three time units would have elapsed. At this point none of
f3(x3), . . . , fn(xn) would have been computed. By contrast, an n-processor parallel
computer solves the problem handily. With all processors operating simultaneously,
processor i computes fi(xi) at time t = 0, for 1 ≤i ≤n. This consumes one time
unit, and the deadline is met.

146
S. G. Akl
7.2.3
Rank-Varying Computational Complexity
Suppose that a computation consists of n stages. There may be a certain precedence
among these stages, or the n stages may be totally independent, in which case the
order of execution is of no consequence to the correctness of the computation. Let
the rank of a stage be the order of execution of that stage. Thus, stage i is the ith stage
to be executed. Here we focus on computations with the property that the number of
algorithmic steps required to execute stage i is C(i), that is, a function of i only.
When does rank-varying computational complexity arise? Clearly, if the compu-
tational requirements grow with the rank, this type of complexity manifests itself in
those circumstances where it is a disadvantage, whether avoidable or unavoidable,
to being ith, for i ≥2. For example, the precision and/or ease of measurement of
variables involved in the computation in a stage s may decrease with each stage
executed before s.
The same analysis as in Sect.7.2.2 applies by substituting the rank for the time.
7.2.4
Time-Varying Variables
For a positive integer n larger than 1, we are given n functions, each of one variable,
namely, f1, f2, . . . , fn, operating on the n physical variables x1, x2, . . . , xn, respec-
tively. Speciﬁcally, it is required to compute fi(xi), for i = 1, 2, . . ., n. For example,
fi(xi) may be equal to x2
i . What is unconventional about this computation, is the fact
that the xi are themselves (unknown) functions x1(t), x2(t), . . . , xn(t), of the time
variable t. It takes one time unit to evaluate fi(xi(t)). The problem calls for com-
puting fi(xi(t)), 1 ≤i ≤n, at time t = t0. Because the function xi(t) is unknown, it
cannot be inverted, and for k > 0, xi(t0) cannot be recovered from xi(t0 + k). Note
that the value of an input variable xi(t) changes at the same speed as the processor
in charge of evaluating the function fi(xi(t)).
A sequential computer fails to compute all the fi as desired. Indeed, suppose that
x1(t0) is initially operated upon. By the time f1(x1(t0)) is computed, one time unit
would have passed. At this point, the values of the n −1 remaining variables would
have changed. The same problem occurs if the sequential computer attempts to ﬁrst
read all the xi, one by one, and store them before calculating the fi.
By contrast, a parallel computer consisting of n independent processors may
perform all the computations at once: For 1 ≤i ≤n, and all processors working at
the same time, processor i computes fi(xi(t0)), leading to a successful computation.
7.2.5
Interacting Variables
A physical system has n variables, x1, x2, . . ., xn, each of which is to be measured or
set to a given value at regular intervals. One property of this system is that measuring

From Parallelism to Nonuniversality: An Unconventional Trajectory
147
or setting one of its variables modiﬁes the values of any number of the system
variables uncontrollably, unpredictably, and irreversibly.
A sequential computer measures one of the values (x1, for example) and by so
doing it disturbs an unknowable number of the remaining variables, thus losing all
hope of recording the state of the system within the given time interval. Similarly,
the sequential approach cannot update the variables of the system properly: Once x1
has received its new value, setting x2 may disturb x1 in an uncertain way.
A parallel computer with n processors, by contrast, will measure all the variables
x1, x2, . . . , xn simultaneously (one value per processor), and therefore obtain an
accurate reading of the state of the system within the given time frame. Consequently,
new values x1, x2, . . . , xn can be computed in parallel and applied to the system
simultaneously (one value per processor).
7.2.6
Uncertain Time Constraints
In this paradigm, we are given a computation consisting of three distinct phases,
namely, input, calculation, and output, each of which needs to be completed by a
certain deadline. However, unlike the standard situation in conventional computation,
the deadlines here are not known at the outset. In fact, and this is what makes this
paradigm truly uncommon, we do not know at the moment the computation is set to
start, what needs to be done, and when it should be done. Certain physical parameters,
from the external environment surrounding the computation, become spontaneously
available. The values of these parameters, once received from the outside world, are
then used to evaluate two functions, f1 and f2, that tell us precisely what to do and
when to do it, respectively.
The difﬁculty posed by this paradigm is that the evaluation of the two functions f1
and f2 is itself quite demanding computationally. Speciﬁcally, for a positive integer
n, the two functions operate on n variables (the physical parameters). A parallel
computer equipped with n processors succeeds in evaluating the two functions on
time to meet the deadlines.
7.2.7
The Global Variables Paradigm
In a computation C1, we assume the presence of n global variables, namely,
x0, x1, . . . , xn−1, all of which are time critical, and all of which are initialized to
0. There are also n nonzero local variables, namely, y0, y1, . . . , yn−1, belonging,
respectively, to the n processes P0, P1, . . . , Pn−1 that make up C1. The computation
C1 is as follows:

148
S. G. Akl
P0: if x0 = 0 then x1 ←y0 else loop forever end if.
P1: if x1 = 0 then x2 ←y1 else loop forever end if.
P2: if x2 = 0 then x3 ←y2 else loop forever end if.
...
Pn−2: if xn−2 = 0 then xn−1 ←yn−2 else loop forever end if.
Pn−1: if xn−1 = 0 then x0 ←yn−1 else loop forever end if.
Suppose that the computation C1 begins when xi = 0, for i = 0, 1, . . . , n −1.
For every i, 0 ≤i ≤n −1, if Pi is to be completed successfully, it must be executed
while xi is indeed equal to 0, and not at any later time when xi has been modiﬁed by
P(i−1) mod n and is no longer equal to 0.
On a parallel computer equipped with n processors, namely, p0, p1, . . . , pn−1, that
conforms to the Exclusive Read Exclusive Write Parallel Random Access Machine
(EREW PRAM) model of computation [15], it is possible to test all the xi, 0 ≤i ≤
n −1, for equality to 0 in one time unit; this is followed by assigning to all the xi,
0 ≤i ≤n −1, their new values during the next time unit. Thus all the processes Pi,
0 ≤i ≤n −1, and hence the computation C1, terminate successfully.
By contrast, a sequential computer such as the Random Access Machine (RAM)
model of computation [15], has but a single processor p0 and, as a consequence,
fails to meet the time-critical requirements of C1. At best, it can perform no more
than n −1 of the n processes as required (assuming it executes the processes in the
order Pn−1, Pn−2, . . . , P1, then fails at P0 since x0 was modiﬁed by Pn−1), and thus
does not terminate. An EREW PRAM with only n −1 processors, p0, p1, . . . , pn−2,
cannot do any better. At best, it too will attempt to execute at least one of the Pi when
xi ̸= 0 and hence fail to complete at least one of the processes on time.
As mentioned earlier, nonuniversality applies to all models of computation, con-
ventional and unconventional, as long as the computational model is capable only of
a ﬁnite and ﬁxed number of operations per time unit. Accordingly, it is abundantly
clear from the global variables paradigm example that unless it is capable of an inﬁ-
nite number of operations per time unit executed in parallel [62], no computer can be
universal—not the universal Turing machine [25, 37], not an accelerating machine
[26], not even a ‘conventional’ time traveling machine (that is, a machine that allows
a travel through time without the paradoxes of time travel) [31].
Several attempts were made to disprove nonuniversality (see, for example, [69,
75]). They all fell short [35, 37]. Clearly, in order for a challenge to nonuniversality to
bevalid,itmustexhibit,foreachofthecomputationsdescribedinSects.7.2.1–7.2.7,a
concrete algorithm that can perform this computation on a ‘universal’ computational
model. Because each of these computations requires n operations per time unit in
order to be executed successfully, where n is a variable, and because these putative
algorithms would run on a computer, by deﬁnition, capable of only a ﬁnite and
ﬁxed number of operations per time unit, this leads to a contradiction. Indeed, it has

From Parallelism to Nonuniversality: An Unconventional Trajectory
149
been suggested that the Principle of Nonuniversality in Computation is the computer
science equivalent of Gödel’s Incompleteness Theorem in mathematical logic [27].
And thus the loop was closed. My journey had taken me from parallelism to
unconventional computation, and from unconventional computational problems to
nonuniversality. Now, nonuniversality has brought me back to parallel computation.
All said, I trust that unconventional computation provided a perfect research home
for my character and my way of thinking, and uncovered a wondrous world of
opportunities in which to invent and create.
8
Looking to the Future
Unconventional is a movable adjective. What is unconventional today may be con-
ventional tomorrow. What will unconventional computing look like 32 years, 64
years, 128 years from now? It is difﬁcult to say which of the current unconventional
information processing ideas will be conventional wisdom in the future. It is equally
hard to predict what computational paradigms our descendants in 2148 will consider
‘unconventional’.
In this section, I take a third, perhaps safer approach of forecasting what contri-
butions to humanity will be made by today’s efforts in the ﬁeld of unconventional
computing.
8.1
The Meaning of Life
By the eighth decade of this century, humans will receive their ﬁrst signiﬁcant gift
from unconventional computation. It would have all started by achieving a complete
understanding of the biological cell through the dual lenses of information and com-
putation. The behavior of the cell will be modeled entirely as a computer program.
Thus, a cell with a disease is essentially a program with a ﬂaw. In order to cure the
cell, it sufﬁces to ﬁx the program. Once this is done through the help of unconven-
tional computer scientists, healthcare will advance by leaps and bounds. Disease will
be conquered. It is the end of death as a result of sickness [40].
It is also the end of death from old age. When this century closes, aging will
be a thing of the past. It is well known that we grow old because our genetic code
constantly makes copies of itself and these copies inevitably deteriorate. However,
this need not happen if a digital version of an individual’s genetic code is created
and the analog version is refreshed from the digital one on a regular basis. Natural
death is no longer a necessity.
The disappearance of death from sickness and from aging, coupled with the ability
to procreate indeﬁnitely, will present the danger of an overpopulated planet Earth.
Once again, unconventional computation will provide the solution, namely, fast space
travel. Huge numbers of humans will undertake a voyage every year to settle on

150
S. G. Akl
another celestial object, in search of a new life, new possibilities, and new knowledge.
They will also bring with them humanity’s ideas and ideals in every area of endeavor.
8.2
The Arrow of Time
By the middle of next century, humans will ﬁnally achieve control of time, the last
aspect of their lives over which they had always been powerless. Time, which had
constantly dominated and regulated their day to day existence, will no longer be
their master. Human beings will be able to travel backward in time and visit their
ancestors. They will be able to travel forward in time and meet their descendents.
Free from time’s grip, we will be able to do extraordinary things.
The technology will be fairly simple. First, humans will use information and
computation to unify all the forces of nature. This will lead to a theory of everything.
Quantum theory and gravity will be uniﬁed. Reversibility is fundamental to quantum
theory. The curvature of space is inherent to the general theory of relativity. Closed
timelike curves (CTCs) will become a reality. Traveling through time and space will
logically follow.
Travel to the past will allow universality in computation to be restored. All that
will be needed is to equip the universal computer with the ability to time travel to a
past where it meets a younger version of itself (unconventionally and paradoxically).
If a computer can travel to the past and meet a younger version of itself, then there
are two computers in the past and they can work in parallel to solve a problem that
requires that two operations be applied simultaneously. More generally, the computer
can travel to the past repeatedly, as many times as necessary, in order to encounter
additional versions of its younger self, and solve a problem that requires that several
operations be applied simultaneously [41].
9
Conclusion
It is relevant to mention in closing that the motto of my academic department is Sum
ergo computo, which means I am therefore I compute. The motto speaks at different
levels. At one level, it expresses our identity. The motto says that we are computer
scientists. Computing is what we do. Our professional reason for being is the theory
and practice of Computing. It also says that virtually every activity in the world
in which we live is run by a computer, in our homes, our ofﬁces, our factories, our
hospitals, our places of entertainment and education, our means of transportation and
communication, all. Just by the simple fact of living in this society, we are always
computing.
At a deeper level the motto asserts that “Being is computing”. In these three words
is encapsulated our vision, and perhaps more concretely our model of computing in
Nature. To be precise, from our perspective as humans seeking to comprehend the

From Parallelism to Nonuniversality: An Unconventional Trajectory
151
natural world around us, the motto says that computing permeates the Universe and
drives it: Every atom, every molecule, every cell, everything, everywhere, at every
moment, is performing a computation. To be is to compute.
What a magniﬁcent time to be a computer scientist! Computing is the most inﬂu-
ential science of our time. Its applications in every walk of life are making the world a
better place in which to live. Unconventional computation offers a wealth of unchar-
tered territories to be explored. Natural computing may hold the key to the meaning
of life itself. Indeed, unconventional information processing may be the vehicle to
conquer time, the ﬁnal frontier. What more can we hope for?
References
1. Adamatzky, A., Akl, S.G.: Trans-Canada slimeways: slime mould imitates the Canadian
transport network. Int. J. Nat. Comput. Res. 2, 31–46 (2011)
2. Adamatzky, A., Akl, S.G.: Trans-Canada slimeways: from coast to coast to coast. In:
Adamatzky, A. (ed.) Bioevaluation of World Transport Networks, pp. 113–125. World Sci-
entiﬁc Publishing, London (2012)
3. Adamatzky, A., Akl, S.G., Alonso-Sanz, R., Van Dessel, W., Ibrahim, Z., Ilachinski, A., Jones,
J., Kayem, A.V.D.M., Martínez, G.J., De Oliveira, P., Prokopenko, M., Schubert, T., Sloot, P.,
Strano, E., Yang, X.S.: Biorationality of motorways. In: Adamatzky, A. (ed.) Bioevaluation
of World Transport Networks, pp. 309–325. World Scientiﬁc Publishing, London (2012)
4. Adamatzky, A., Akl, S.G., Alonso-Sanz, R., Van Dessel, W., Ibrahim, Z., Ilachinski, A., Jones,
J., Kayem, A.V.D.M., Martínez, G.J., De Oliveira, P., Prokopenko, M., Schubert, T., Sloot,
P., Strano, E., Yang, X.S.: Are motorways rational from slime mould’s point of view? Int. J.
Parallel Emergent Distrib. Syst. 28, 230–248 (2013)
5. Adamatzky, A., Akl, S.G., Burgin, M., Calude, C.S., Costa, J.F., Dehshibi, M.M., Gunji,
Y.P., Konkoli, Z., MacLennan, B., Marchal, B., Margenstern, M., Martinez, G.J., Mayne, R.,
Morita, K., Schumann, A., Sergeyev, Y.D., Sirakoulis, G.C., Stepney, S., Svozil, K., Zenil, H.:
East-west paths to unconventional computing. Prog. Biophys. Mol. Biol. Elsevier, Amsterdam
(2017) (Special issue on Integral Biomathics: The Necessary Conjunction of the Western and
Eastern Thought Traditions for Exploring the Nature of Mind and Life)
6. Akl, S.G.: Digital signatures with blindfolded arbitrators who cannot form alliances. In:
Proceedings of 1982 IEEE Symposium on Security and Privacy, pp. 129–135. IEEE, Oakland
(1982)
7. Akl, S.G.: Digital signatures: a tutorial survey. Computer 16, 15–24 (1983)
8. Akl, S.G.: On the security of compressed encodings. In: Chaum, D. (ed.) Advances in Cryp-
tology, pp. 209–230. Plenum Press, New York (1984)
9. Akl, S.G.: A prototype computer for the year 2000. Queen’s Gaz. 16, 325–332 (1984)
10. Akl, S.G.: Optimal parallel algorithms for selection, sorting and computing convex hulls. In:
Toussaint, G.T. (ed.) Computational Geometry, pp. 1–22. North Holland, Amsterdam (1985)
11. Akl, S.G.: Parallel Sorting Algorithms. Academic Press, Orlando (1985)
12. Akl, S.G.: Checkers playing programs. In: Shapiro, S.C. (ed.) Encyclopedia of Artiﬁcial
Intelligence, pp. 88–93. Wiley, New York (1987)
13. Akl, S.G.: The Design and Analysis of Parallel Algorithms. Prentice-Hall, Englewood Cliffs
(1989)
14. Akl, S.G.: Memory access in models of parallel computation: from folklore to synergy and
beyond. In: Dehne, F., Sack, J.-R., Santoro, N. (eds.) Algorithms and Data Structures, pp.
92–104. Springer, Berlin (1991)
15. Akl, S.G.: Parallel Computation: Models and Methods. Prentice Hall, Upper Saddle River
(1997)

152
S. G. Akl
16. Akl, S.G.: Parallel real-time computation: sometimes quantity means quality. In: Sudborough,
H., Monien, B., Hsu, D.F. (eds.) Proceedings of the International Symposium on Parallel
Architectures, Algorithms and Networks, pp. 2–11. IEEE, Dallas (2000)
17. Akl, S.G.: The design of efﬁcient parallel algorithms. In: Blazewicz, J., Ecker, K., Plateau, B.,
Trystram, D. (eds.) Handbook on Parallel and Distributed Processing, pp. 13–91. Springer,
Berlin (2000)
18. Akl, S.G.: Parallel real-time computation of nonlinear feedback functions. Parallel Process.
Lett. 13, 65–75 (2003)
19. Akl, S.G.: Superlinear performance in real-time parallel computation. J. Supercomput. 29,
89–111 (2004)
20. Akl, S.G.: The myth of universal computation. In: Trobec, R., Zinterhof, P., Vajteršic, M.,
Uhl, A. (eds.) Parallel Numerics, pp. 211–236. University of Salzburg, Salzburg and Jozef
Stefan Institute, Ljubljana (2005)
21. Akl, S.G.: Non-Universality in Computation: The Myth of the Universal Computer. Queen’s
University, School of Computing (2005). http://research.cs.queensu.ca/Parallel/projects.html
22. Akl, S.G.: A computational challenge. Queen’s University, School of Computing (2006).
http://www.cs.queensu.ca/home/akl/CHALLENGE/A-Computational-Challenge.htm
23. Akl, S.G.: Universality in computation: some quotes of interest. Technical Report No. 2006-
511, School of Computing, Queen’s University (2006). http://www.cs.queensu.ca/home/akl/
techreports/quotes.pdf
24. Akl, S.G.: Conventional or unconventional: is any computer universal? In: Adamatzky, A.,
Teuscher, C. (eds.) From Utopian to Genuine Unconventional Computers, pp. 101–136.
Luniver Press, Frome (2006)
25. Akl, S.G.: Three counterexamples to dispel the myth of the universal computer. Parallel
Process. Lett. 16, 381–403 (2006)
26. Akl, S.G.: Even accelerating machines are not universal. Int. J. Unconv. Comput. 3, 105–121
(2007)
27. Akl, S.G.: Gödel’s incompleteness theorem and nonuniversality in computing. In: Nagy, M.,
Nagy, N. (eds.) Proceedings of the Workshop on Unconventional Computational Problems,
pp. 1–23. Sixth International Conference on Unconventional Computation, Kingston (2007)
28. Akl, S.G.: Unconventional computational problems with consequences to universality. Int. J.
Unconv. Comput. 4, 89–98 (2008)
29. Akl, S.G.: Evolving computational systems. In: Rajasekaran, S., Reif, J.H. (eds.) Parallel
Computing: Models, Algorithms, and Applications, pp. 1–22. Taylor and Francis, Boca Raton
(2008)
30. Akl, S.G.: Ubiquity and simultaneity: the science and philosophy of space and time in uncon-
ventionalcomputation.Keynoteaddress,ConferenceontheScienceandPhilosophyofUncon-
ventional Computing, The University of Cambridge, Cambridge (2009)
31. Akl, S.G.: Time travel: A new hypercomputational paradigm. Int. J. Unconv. Comput. 6,
329–351 (2010)
32. Akl, S.G.: On the importance of being quantum. Parallel Process. Lett. 20, 275–286 (2010)
(Special Issue on Advances in Quantum Computation. Qiu, K. (ed.))
33. Akl, S.G.: Bitonic sort. In: Padua, D. (ed.) Encyclopedia of Parallel Computing, pp. 139–146.
Springer, New York (2011)
34. Akl, S.G.: What is computation? Int. J. Parallel Emergent Distrib. Syst. 29, 337–345 (2014)
35. Akl, S.G.: Nonuniversality explained. Int. J. Parallel Emergent Distrib. Syst. 31, 201–219
(2016)
36. Akl, S.G.: The quantum chess story. Int. J. Unconv. Comput. 12, 207–219 (2016)
37. Akl, S.G.: Nonuniversality in computation: ﬁfteen misconceptions rectiﬁed. In: Adamatzky,
A. (ed.) Advances in Unconventional Computing, pp. 1–30. Springer, Cham (2017)
38. Akl, S.G.: Unconventional computational problems. In: Meyers, R.A. (ed.) Encyclopedia of
Complexity and Systems Science. Springer, New York (2017)
39. Akl, S.G.: Natures computes. Queen’s Alumni Rev. (2), 44 (2017)

From Parallelism to Nonuniversality: An Unconventional Trajectory
153
40. Akl, S.G.: Information and computation: the essence of it all. Int. J. Unconv. Comput. 13,
187–194 (2017)
41. Akl, S.G.: Time: the ﬁnal frontier. Int. J. Unconv. Comput. 13, 273–281 (2017)
42. Akl, S.G.: Computing shortest paths with cellular automata. J. Cell. Autom. 13, 33–52 (2018)
43. Akl, S.G.: Unconventional wisdom: superlinear speedup and inherently parallel computations.
Int. J. Unconv. Comput. 13, 283–307 (2018)
44. Akl, S.G.: How to encrypt a graph. Int. J. Parallel Emergent Distrib. Syst
45. Akl, S.G.: A computational journey in the true north. Int. J. Parallel Emergent Distrib. Syst.
(Special Issue on A Half Century of Computing. Adamatzky, A.I., Watson, L.T. (eds.))
46. Akl, S.G. and Assem, I.: Fully homomorphic encryption: a general framework and imple-
mentations. Int. J. Parallel Emergent Distrib. Syst
47. Akl, S.G., Barnard, D.T., Doran, R.J.: Searching game trees in parallel. In: Proceedings of the
Third Biennial Conference of the Canadian Society for Computational Studies of Intelligence,
pp. 224–231. Victoria (1980)
48. Akl, S.G., Barnard, D.T., Doran, R.J.: Design, analysis and implementation of a parallel tree
search algorithm. IEEE Trans. Pattern Anal. Mach. Intell. PAMI-4, 192–203 (1982)
49. Akl, S.G., Cordy, B., Yao, W.: An analysis of the effect of parallelism in the control of
dynamical systems. Int. J. Parallel Emergent Distrib. Syst. 20, 147–168 (2005)
50. Akl, S.G., Denning, D.E.: Checking classiﬁcation constraints for consistency and complete-
ness. In: Proceedings of 1987 IEEE Symposium on Security and Privacy, pp. 196–201. IEEE,
Oakland (1987)
51. Akl, S.G., Denning, D.E.: Checking classiﬁcation constraints for consistency and complete-
ness. In: Turn, R. (ed.) Advances in Computer System Security, vol. 3, pp. 271–276. Artech
House, Norwood (1988)
52. Akl, S.G., Doran, R.J.: A comparison of parallel implementations of the alpha-beta and Scout
tree search algorithms using the game of checkers. In: Bramer, M.A. (ed.) Computer Game
Playing, pp. 290–303. Wiley, Chichester (1983)
53. Akl, S.G., Fava Lindon, L.: Paradigms for superunitary behavior in parallel computations. J.
Parallel Algorithms Appl. 11, 129–153 (1997)
54. Akl, S.G., Lindon, L.: Modèles de calcul parallèle à mémoire partagée. In: Cosnard, M., Nivat,
M., Robert, Y. (eds.) Algorithmique Parallèle, pp. 15–29. Masson, Paris (1992)
55. Akl, S.G., Lyons, K.A.: Parallel Computational Geometry. Prentice Hall, Englewood Cliffs
(1993)
56. Akl, S.G., Meijer, H.: A fast pseudo random permutation generator with applications to
cryptology. In: Blakley, G.R., Chaum, D. (eds.) Advances in Cryptology. Lecture Notes in
Computer Science, vol. 196, pp. 269–275. Springer, Berlin (1985)
57. Akl, S.G., Meijer, H.: Two new secret key cryptosystems. In: Pichler, F. (ed.) Advances in
Cryptology. Lecture Notes in Computer Science, vol. 219, pp. 96–102. Springer, Berlin (1986)
58. Akl, S.G., Nagy, M.: Introduction to parallel computation. In: Trobec, R., Vajteršic, M.,
Zinterhof, P. (eds.) Parallel Computing: Numerics, Applications, and Trends, pp. 43–80.
Springer, London (2009)
59. Akl, S.G., Nagy, M.: The future of parallel computation. In: Trobec, R., Vajteršic, M., Zinter-
hof, P. (eds.) Parallel Computing: Numerics, Applications, and Trends, pp. 471–510. Springer,
London (2009)
60. Akl, S.G., Newborn, M.M.: The principal continuation and the killer heuristic. In: Proceedings
of the ACM Annual Conference, pp. 466–473. ACM, Seattle (1977)
61. Akl, S.G., Qiu, K.: Les réseaux d’interconnexion star et pancake. In: Cosnard, M., Nivat, M.,
Robert, Y. (eds.) Algorithmique Parallèle, pp. 171–181. Masson, Paris (1992)
62. Akl, S.G., Salay, N.: On computable numbers, nonuniversality, and the genuine power of
parallelism. Int. J. Unconv. Comput. 11, 283–297 (2015)
63. Akl, S.G.: On computable numbers, nonuniversality, and the genuine power of parallelism.
In: Adamatzky, A. (ed.) Emergent Computation: A Festschrift for Selim G. Akl, pp. 57–69.
Springer, Cham (2017)

154
S. G. Akl
64. Akl, S.G., Stojmenovi´c, I.: Generating combinatorial objects on a linear array of proces-
sors. In: Zomaya, A.Y. (ed.) Parallel Computing: Paradigms and Applications, pp. 639–670.
International Thomson Computer Press, London (1996)
65. Akl, S.G., Stojmenovi´c, I.: Broadcasting with selective reduction: a powerful model of parallel
computation. In: Zomaya, A.Y. (ed.) Parallel and Distributed Computing Handbook, pp. 192–
222. McGraw-Hill, New York (1996)
66. Akl, S.G., Taylor, P.D.: Cryptographic solution to a problem of access control in a hierarchy.
ACM Trans. Comput. Syst. 1, 239–248 (1983)
67. Akl, S.G., Toussaint, G.T.: A fast convex hull algorithm. Inf. Process. Lett. 7, 219–222 (1978)
68. Akl, S.G., Yao, W.: Parallel computation and measurement uncertainty in nonlinear dynamical
systems. J. Math. Model. Algorithms 4, 5–15 (2005)
69. Bringsjord, S.: Is universal computation a myth? In: Adamatzky, A. (ed.) Emergent Compu-
tation: A Festschrift for Selim G. Akl, pp. 19–37. Springer, Cham (2017)
70. Choudhury, S., Salomaa, K., Akl, S.G.: A cellular automaton model for wireless sensor
networks. J. Cell. Autom. 7, 223–242 (2012)
71. Choudhury, S., Salomaa, K., Akl, S.G.. A cellular automaton model for connectivity preserv-
ing deployment of mobile wireless sensors. In: Proceedings of the Second IEEE International
Workshop on Smart Communication Protocols and Algorithms, pp. 6643–6647. IEEE, Ottawa
(2012)
72. Choudhury, S., Salomaa, K., Akl, S.G.: Energy efﬁcient cellular automaton based algorithms
for mobile sensor networks. In: Proceedings of the 2012 IEEE Wireless Communications and
Networking Conference, pp. 2341–2346. IEEE, Paris (2012)
73. Choudhury, S., Salomaa, K., Akl, S.G.: Cellular automaton based algorithms for the dispersion
of mobile wireless sensor networks. Int. J. Parallel Emergent Distrib. Syst. 29, 147–177 (2014)
74. Choudhury, S., Salomaa, K., Akl, S.G.: Cellular automaton based localized algorithms for
mobile sensor networks. Int. J. Unconv. Comput. 11, 417–447 (2015)
75. Dadizadeh, A.: Two problems believed to exhibit superunitary behaviour turn out to fall within
the church-turing thesis. M.Sc. Thesis, Bishop’s University, Canada (2018)
76. Denning, D.E., Akl, S.G., Heckman, M., Lunt, T.F., Morgenstern, M., Neumann, P.G., Schell,
R.R.: Views for multilevel database security. IEEE Trans. Softw. Eng. SE-13, 129–140 (1987)
77. Deutsch, D.: The Fabric of Reality, p. 134. Penguin Books, London (1997)
78. Fraser, R., Akl, S.G.: Accelerating machines: a review. Int. J. Parallel Emergent Distrib. Syst.
23, 81–104 (2008)
79. Kayem, A., Martin, P., Akl, S.G.: Adaptive Cryptographic Access Control. Springer, New
York (2010)
80. Kayem, A.V.D.M., Martin, P., Akl, S.G.: Self-protecting access control: on mitigating privacy
violations with fault tolerance. In: Yee, G.O.M (ed) Privacy Protection Measures and Tech-
nologies in Business Organizations: Aspects and Standards, pp. 95–128. IGI Global, Hershey
(2012)
81. MacKinnon, S., Taylor, P.D., Meijer, H., Akl, S.G.: An optimal algorithm for assigning cryp-
tographic keys to control access in a hierarchy. IEEE Trans. Comput. C-34, 797–802 (1985)
82. McKay, C.D., Afﬂeck, J.G., Nagy, N., Akl, S.G., Walker, V.K.: Molecular codebreaking and
double encoding - Laboratory experiments. Int. J. Unconv. Comput. 5, 547–564 (2009)
83. Meijer, H., Akl, S.G.: Digital signature schemes. In: Proceedings of Crypto 81: First IEEE
Workshop on Communications Security, pp. 65–70. IEEE, Santa Barbara (1981)
84. Meijer, H., Akl, S.G.: Digital signature schemes. Cryptologia 6, 329–338 (1982)
85. Meijer, H., Akl, S.G.: Remarks on a digital signature scheme. Cryptologia 7, 183–186 (1983)
86. Nagy, M., Akl, S.G.: On the importance of parallelism for quantum computation and the
concept of a universal computer. In: Calude, C.S., Dinneen, M.J., Paun, G., Pérez-Jiménez, M.,
de, J., Rozenberg, G. (eds.) Unconventional Computation, pp. 176–190. Springer, Heildelberg
(2005)
87. Nagy, M., Akl, S.G.: Quantum measurements and universal computation. Int. J. Unconv.
Comput. 2, 73–88 (2006)

From Parallelism to Nonuniversality: An Unconventional Trajectory
155
88. Nagy, M., Akl, S.G.: Quantum computing: beyond the limits of conventional computation.
Int. J. Parallel Emergent Distrib. Syst. 22, 123–135 (2007)
89. Nagy, M., Akl, S.G.: Parallelism in quantum information processing defeats the Universal
Computer. Parallel Process. Lett. 17, 233–262 (2007) (Special Issue on Unconventional Com-
putational Problems)
90. Nagy, M., Akl, S.G.: Coping with decoherence: parallelizing the quantum Fourier transform.
Parallel Process. Lett. 20, 213–226 (2010) (Special Issue on Advances in Quantum Compu-
tation. Qiu, K. (ed.))
91. Nagy,M.,Akl,S.G.:Entanglementveriﬁcationwithanapplicationtoquantumkeydistribution
protocols. Parallel Process. Lett. 20, 227–237 (2010) (Special Issue on Advances in Quantum
Computation. Qiu, K. (ed.))
92. Nagy, M., Akl, S.G., Kershaw, S.: Key distribution based on the quantum Fourier transform.
Int. J. Secur. Appl. 3, 45–67 (2009)
93. Nagy, N., Akl, S.G.: Aspects of biomolecular computing. Parallel Process. Lett 17, 185–211
(2007)
94. Nagy,N.,Akl,S.G.:Authenticatedquantumkeydistributionwithoutclassicalcommunication.
Parallel Process. Lett. 17, 323–335 (2007) (Special Issue on Unconventional Computational
Problems)
95. Nagy, N., Akl, S.G.: A quantum cryptographic solution to the problem of access control in a
hierarchy. Parallel Process. Lett. 20, 251–261 (2010) (Special Issue on Advances in Quantum
Computation. Qiu, K. (ed))
96. Nagy, N., Akl, S.G.: One-time pads without prior encounter. Parallel Process. Lett. 20, 263–
273 (2010) (Special Issue on Advances in Quantum Computation. Qiu, K. (ed))
97. Nagy, N., Akl, S.G.: Computations with uncertain time constraints: effects on parallelism
and universality. In: Calude, C.S., Kari, J., Petre, I., Rozenberg, G. (eds.) Unconventional
Computation, pp. 152–163. Springer, Heidelberg (2011)
98. Nagy, N., Akl, S.G.: Computing with uncertainty and its implications to universality. Int. J.
Parallel Emergent Distrib. Syst. 27, 169–192 (2012)
99. Nagy, N., Akl, S.G., Nagy, M.: Applications of Quantum Cryptography. Lambert Academic
Publishing, Saarbrüken (2016)
100. Nagy, N., Nagy, M., Akl, S.G.: Key distribution versus key enhancement in quantum cryp-
tography. Parallel Process. Lett. 20, 239–250 (2010) (Special Issue on Advances in Quantum
Computation. Qiu, K. (ed.))
101. Nagy, N., Nagy, M., Akl, S.G.: Hypercomputation in a cryptographic setting: solving the
identity theft problem using quantum memories. Int. J. Unconv. Comput. 6, 375–398 (2010)
102. Nagy, N., Nagy, M., Akl, S.G.: Quantum security in wireless sensor networks. Nat. Comput.
9, 819–830 (2010)
103. Nagy, N., Nagy, M., Akl, S.G.: Carving secret messages out of public information. J. Comput.
Sci. 11, 64–70 (2015)
104. Nagy, N., Nagy, M., Akl, S.G.: A less known side of quantum cryptography. In: Adamatzky,
A. (ed.) Emergent Computation: A Festschrift for Selim G. Akl, pp. 121–169. Springer, Cham
(2017)
105. Palioudakis, A., Salomaa, K., Akl, S.G.: Unary NFAs, limited nondeterminism, and Chrobak
normal form. Int. J. Unconv. Comput. 11, 395–416 (2015)
106. Palioudakis, A., Salomaa, K., Akl, S.G.: Operational state complexity of unary NFAs with
ﬁnite nondeterminism. Theor. Comput. Sci. 610, 108–120 (2016)
107. Palioudakis, A., Salomaa, K., Akl, S.G.: Worst case branching and other measures of nonde-
terminism. Int. J. Found. Comput. Sci. 28, 195–210 (2017)
108. Osiakwan,C.N.K.,Akl,S.G.:Aperfectspeedupparallelalgorithmfortheassignmentproblem
on complete weighted bipartite graphs. In: Rishe, N., Navathe, S., Tal, D. (eds.) Parallel
Architectures, pp. 161–180. IEEE Computer Society Press, Los Alamitos (1991)
109. Pavel, S., Akl, S.G.: Matrix operations using arrays with reconﬁgurable optical buses. J.
Parallel Algorithms Appl. 8, 223–242 (1996)

156
S. G. Akl
110. Pavel, S., Akl, S.G.: Area-time trade-offs in arrays with optical pipelined buses. Appl. Opt.
35, 1827–1835 (1996)
111. Pavel, S., Akl, S.G.: On the power of arrays with reconﬁgurable optical buses. In: Proceed-
ings of the International Conference on Parallel and Distributed Processing Techniques and
Applications, pp. 1443–1454. Sunnyvale, (1996)
112. Pavel,S.,Akl,S.G.:EfﬁcientalgorithmsfortheHoughtransformonarrayswithreconﬁgurable
optical buses. In: Proceedings of the International Parallel Processing Symposium, pp. 697–
701. Maui (1996)
113. Pavel, S. and Akl, S.G.: Integer sorting and routing in arrays with reconﬁgurable optical
buses. Int. J. of Found. of Comput. Sci. 9, 99–120 (1998) (Special Issue on Interconnection
Networks)
114. Pavel, S.D., Akl, S.G.: Computing the Hough transform on arrays with reconﬁgurable optical
buses. In: Li, K., Pan, Y., Zheng, S.-Q. (eds.) Parallel Computing Using Optical Interconnec-
tions, pp. 205–226. Kluwer Academic Publishers, Dordrecht (1998)
115. Sipser, M.: Introduction to the Theory of Computation, p. 125. PWS, Boston (1997)
116. Taleb, N., Akl, S.G.: Error detection in asynchronous sequential circuits - the hardware
approach. In: Proceedings of the Tenth Conference on Statistics and Scientiﬁc Computations,
pp. S201–S215. Cairo University, Cairo (1974)
117. Torbey, S., Akl, S.G.: An exact and optimal local solution to the two-dimensional convex hull
of arbitrary points problem. J. Cell. Autom. 4, 137–146 (2009)
118. Torbey, S., Akl, S.G.: An exact solution to the two-dimensional arbitrary-threshold density
classiﬁcation problem. J. Cell. Autom. 4, 225–235 (2009)
119. Torbey, S., Akl, S.G.: Reliable node placement in wireless sensor networks using cellular
automata. In: Durand-Lose, J., Jonoska, N. (eds.) Unconventional Computation and Natural
Computation, pp. 210–221. Springer, Heidelberg (2012)
120. Torbey, S., Akl, S.G., Redfearn, D.: Time-scale analysis of signals without basis functions:
application to sudden cardiac arrest prediction. Int. J. Unconv. Comput. 11, 375–394 (2015)
121. https://www.amazon.com/gp/richpub/listmania/fullview/32A3PMKCJH0Y8
122. http://www.cs.queensu.ca/home/akl/QUANTUMCHESS/QCOnTheWeb.pdf
123. http://www.nserc-crsng.gc.ca/Media-Media/ImpactStory-ArticlesPercutant_eng.asp?
ID=1053
124. https://www.youtube.com/watch?v=Hi0BzqV_b44
125. http://research.cs.queensu.ca/home/akl/SLIMEMOLD/SlimeMoldInTheNews.pdf

A Simple Hybrid Event-B Model
of an Active Control System
for Earthquake Protection
Richard Banach and John Baugh
Abstract In earthquake-prone zones of the world, severe damage to buildings and
life endangering harm to people pose a major risk when severe earthquakes hap-
pen. In recent decades, active and passive measures to prevent building damage have
been designed and deployed. A simple model of an active damage prevention system,
founded on earlier work, is investigated from a model based formal development per-
spective, using Hybrid Event-B. The non-trivial physical behaviour in the model is
readily captured within the formalism. However, when the usual approximation and
discretization techniques from engineering and applied mathematics are used, the
rather brittle reﬁnement techniques used in model based formal development start to
break down. Despite this, the model developed stands up well when compared via
simulation with a standard approach. The requirements of a richer formal develop-
ment framework, better able to cope with applications exhibiting non-trivial physical
elements are discussed.
1
Introduction
In earthquake-prone zones of the world, damage to buildings during an earthquake
is a serious problem, leading to major rebuilding costs if the damage is severe. This
is to say nothing of the harm to people that ensues if they happen to be inside, or
near to, a building that fails structurally. One approach to mitigating the problem is
to make buildings so robust that they can withstand the severest earthquake that may
befall them; but this not only greatly increases cost, but also places limits on the size
R. Banach (B)
School of Computer Science, University of Manchester, Oxford Road,
Manchester M13 9PL, UK
e-mail: richard.banach@manchester.ac.uk
J. Baugh
Department of Civil, Construction, and Environmental Engineering,
North Carolina State University, Raleigh, NC 27695-7908, USA
e-mail: jwb@ncsu.edu
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_7
157

158
R. Banach and J. Baugh
and shape of buildings, so that the desired robustness remains feasible with available
materials.
In recent decades, an alternative approach to earthquake protection has been to
use control techniques to dissipate the forces that reach vulnerable elements of a
building by using control strategies of one kind or another. In truth, the ﬁrst proposal
for intervening in a building’s ability to withstand earthquake dates back to 1870 (a
patent ﬁled at the U.S. Patent Ofﬁce by one Jules Touaillon), but such ideas were not
taken seriously till a century or so later.
One approach is to use passive control. In this approach massive members and/or
damping mechanisms are incorporated into the building in such a way that their
parameters and the coupling between them and the rest of the building are chosen
just so that the destructive forces are preferentially dissipated into these additional
elements, leaving the building itself undamaged.
An alternative, more recent approach, is to use active control. Since it is the
amplitude and frequency of the vibrations that a building is subject to during an
earthquake that determine whether it will sustain damage or not, damping earthquake
vibrations by applying suitably designed counter vibrations to the building reduces
the net forces that the building must withstand, and thus the damage it will sustain
under a given severity of earthquake and given a speciﬁc standard of construction.
In [16, 20, 21, 31] there is a study of such an active control system for earthquake
resistance for buildings. Ultimately, it is targetted at an experimental tall building of
six stories. These papers investigate various aspects of veriﬁcation for a system of
this kind, based largely on timing considerations, which inevitably generate uncer-
tainties due to equipment latencies. The approach to the system is rather bottom up in
[20, 21]. The design is presented at a low level of detail with separate elements for
the start of an action, the end of the action, and a synchronisation point within the
action (as needed), with timings attaches to each element. Even for a simple system,
this results in a huge state space. The focus of [20, 21] then becomes reduction of
the state space size, showing no loss of behaviour via bisimulation. Following this,
useful properties of the system model may be demonstrated using the smaller state
space version.
In the present paper, we take an alternative route, going top down instead of bottom
up, and using Hybrid Event-B (henceforth HEB) [11, 12] as the vehicle for doing the
development. We work top-down, and for simplicity and through lack of space, we do
not get to the low level of detail present in [20, 21]. In particular, we omit replication
of subsystems, timing and fault tolerance (though we comment on these aspects at
the end). As well as providing the contrast to the previous treatment, the present case
study offers some novelty regarding the interaction of physical and digital behaviours
(in particular, regarding continuous vs. impulsive physics, as treated within the HEB
formalism), compared with other case studies, e.g. [5–7, 9, 10, 15].
The rest of this paper is as follows. In Sect.2 we brieﬂy overview control strategies
for seismic protection for buildings, and focus on the active control principles that
underpin this paper’s approach. Section3 has an outline of single machine HEB,
for purposes of orientation. In Sect.4 we present the simple dynamical model we
develop, and its most abstract expression in HEB, and Sect.5 presents an ideal but

A Simple Hybrid Event-B Model of an Active Control System …
159
completely unrealistic solution to the problem posed in the model. The next few
sections develop and reﬁne the original model in a less idealised way, bringing
in more of the detailed requirements of a practical solution. The more detail we
bring in, the greater the challenge to the usual reﬁnement technique found in formal
development frameworks, including HEB.
Thus Sect.6 presents a ﬁrst reﬁnement towards a practical solution, while Sect.7
engages more seriously with an ‘ideal pulse’ strategy for the active control solution.
Section8 pauses to discuss the issues for reﬁnement that this throws up. Section9
incorporates the discretization typically seen in practical engineering solutions, and
also treats decomposition into a family of machines that reﬂect a more convincing
system architecture, one resembling the approach of [20, 21]. Section10 continues
the discussion of issues raised for reﬁnement and retrenchment by these development
steps. Section11 presents numerical simulation work showing that the theoretically
based earlier models give good agreement when compared with solutions derived
using conventional engineering approaches. Section12 recapitulates and concludes.
2
Control Strategies for Earthquake Damage Prevention
Since mechanical prevention of earthquake damage to structures began to be taken
seriously, a number of engineering techniques have been brought to bear on the
problem [40]. These days this is a highly active ﬁeld and the literature is large,
e.g. [1, 2, 19].
In passive control [17], decoupling of the building from the ground, and/or the
incorporation of various additional members, are used to ensure that the forces of an
earthquake do not impinge on the important building structure. Passive approaches
are often used to protect historical buildings in which major re-engineering is imprac-
tical. One disadvantage of the passive approach is the potential transverse displace-
ments relative to the ground that the protected building may undergo. If, with respect
to an inertial frame, the building stays still, and the ground moves by 20cm, then the
relative movement of the building is 20cm. This may not be practical.
In alternative approaches the engineering compensation is more active. Active
approaches (such as the one we will pursue in more detail below) have the compen-
sation mechanism trying to actively counter the forces imparted by the earthquake
in order to limit the amplitude of vibrations at the building’s resonant frequencies
[33, 34]. One problem experienced by active prevention systems is that they may
consume a lot of energy, which is expensive and undesirable. Another is that if one is
unlucky, and the parameters of the earthquake fall in the wrong region (due to impre-
scient design, or error), then because an active system is injecting energy into the
overall structure, it may actually make things worse, driving the overall structure into
instability, perhaps because the injected energy is being introduced in phase rather
than in anti-phase with the earthquake itself. An increasingly popular approach these
days is semi-active control [22], in which the main aim is to intervene actively only in

160
R. Banach and J. Baugh
the dissipative part of the building’s response, decreasing energy costs and avoiding
potential instabilities, for only a small decrease in performance.
In all of these active strategies for prevention of earthquake damage to buildings,
the building contains a set of sensors which constantly monitor vibrations impinging
on the building from the ground. The signals coming from these are analysed to
differentiate between earthquake originated forces, and normal day to day vibrations
caused by everyday activities in the building’s surroundings. The latter are, of course,
ignored.
The building also contains active members which can impart forces to the building
structure. The aim of the active control is to impart forces to the building that counter
the damaging forces coming from the earthquake, so that the net force that the
building must withstand remains within its safe design parameters.
These days, these design aims are achieved using a sophisticated control engineer-
ing approach. Many strategies have been tried, but among the most popular currently
is to use a LQG (Linear Quadratic Gaussian) strategy for designing a nominal con-
troller, which is then modulated by clipping extreme values. This approach is based
on a sophisticated formulation of noisy dynamics and its control via a Bayesian esti-
mation of current and future behaviour. See e.g. [22] (or [28] for a simpler example).
One consequence of this approach is some loss of direct contact between the control
algorithm design and real time values, due to the use of L2 estimates in the derivation
of the controller. This is a disadvantage regarding direct correspondence with typical
formal methods approaches, which are wedded exclusively to variable values in real
time.
Our own study is based on the strategy used in [20, 21]. Figure1, taken from
[20, 21], gives a schematic outline of how active elements are disposed in an exper-
imental building in Tokyo. There are sensors near the ground, and at the top of the
building. The active members, which have to be capable of exerting signiﬁcant force
if they are to move signiﬁcant aspects of the building, are found at the bottom of the
building.1
The technique by which the corrective forces are applied to the building is to have
the active members impart a series of pulses to the core framework of the build-
ing. Of course, for this to be successful on the timescales of earthquake vibrations,
there has to be accurate real time control, and an appropriate balance between the
aggregated effect of the applied pulse series and the sensed vibrations coming from
the earthquake. In contrast to the LQG approach, the technique used in [16, 20, 21,
29, 31] is based on real time monitoring of positions, velocities and accelerations in
the building’s structure, thus greatly facilitating a correspondence with conventional
model based formal methods techniques (a point that emerges, though obviously
quite indirectly, from remarks in [29]).
It is not our aim in this paper to get deeply embroiled in the detailed control
engineering aspects of the problem. We leave that to other work. Instead, our aim
1In sophisticated modern designs, active members are also found higher up the building, to counter
vibration antinodes part way up a tall structure.

A Simple Hybrid Event-B Model of an Active Control System …
161
Fig. 1 A schematic of a
building design, to be
protected by an active
earthquake damage
prevention system.
From [20, 21]
is to take a top down approach to the implementation task, and to see how a HEB
perspective can bring efﬁciencies and a degree of clarity to that. Accordingly, we
next turn to HEB itself.
3
A Brief Outline of Hybrid Event-B
In this section we give an outline of Hybrid Event-B for single machines. In Fig.2 we
see a bare bones HEB machine, HyEvBMch. It starts with declarations of time and
of a clock. In HEB, time is a ﬁrst class citizen in that all variables are functions of
time, whether explicitly or implicitly. However time is special, being read-only, never
being assigned, since time cannot be controlled by any human-designed engineering
process. Clocks allow a bit more ﬂexibility, since they are assumed to increase their
value at the same rate that time does, but may be set during mode events (see below).
Variables are of two kinds. There are mode variables (like u, declared as usual)
which take their values in discrete sets and change their values via discontinuous
assignment in mode events. There are also pliant variables (such as x, y), declared
in the PLIANT clause, which take their values in topologically dense sets (normally
R) and which are allowed to change continuously, such change being speciﬁed via
pliant events (see below).

162
R. Banach and J. Baugh
MACHINE HyEvBMch
TIME t
CLOCK clk
PLIANT x,y
VARIABLES u
INVARIANTS
x ∈R
y ∈R
u ∈N
EVENTS
INITIALISATION
STATUS ordinary
WHEN
t = 0
THEN
clk := 1
x := x0
y := y0
u := u0
END
...
...
...
...
MoEv
STATUS ordinary
ANY i?,l,o!
WHERE grd(x,y,u,i?,l,t,clk)
THEN
x,y,u,clk,o! : |BApred(x,y,u,
i?,l,o!,t,clk,x′,y′,u′,clk′)
END
PliEv
STATUS pliant
INIT iv(x,y,t,clk)
WHERE grd(u)
ANY i?,l,o!
COMPLY BDApred(
x,y,u,i?,l,o!,t,clk)
SOLVE
Dx =
(x,y,u,i?,l,o!,t,clk)
y := E(x,y,u,i?,l,o!,t,clk)
END
END
φ
Fig. 2 A schematic Hybrid Event-B machine
Next are the invariants. These resemble invariants in discrete Event-B, in that the
typesofthevariablesareassertedtobethesetsfromwhichthevariables’valuesatany
given moment of time are drawn. More complex invariants are similarly predicates
that are required to hold at all moments of time during a run.
Then we get to the events. The INITIALISATION has a guard that synchronises
time with the start of any run, while all other variables are assigned their initial values
in the usual way. As hinted above, in HEB, there are two kinds of event: mode events
and pliant events.
Mode events are direct analogues of events in discrete Event-B. They can assign all
machine variables (except time itself). In the schematic MoEv of Fig.2, we see three
parameters i?, l, o!, (an input, a local parameter, and an output respectively), and a
guard grd which can depend on all the machine variables. We also see the generic
after-value assignment speciﬁed by the before-after predicate BApred, which can
specify how the after-values of all variables (except time, inputs and locals) are to
be determined.
Pliant events are new. They specify the continuous evolution of the pliant vari-
ables over an interval of time. The schematic pliant event PliEv of Fig.2 shows
the structure. There are two guards: there is iv, for specifying enabling conditions
on the pliant variables, clocks, and time; and there is grd, for specifying enabling
conditions on the mode variables. The separation between the two is motivated by
considerations connected with reﬁnement.
The body of a pliant event contains three parameters i?, l, o!, (once more an
input, a local parameter, and an output respectively) which are functions of time,
deﬁned over the duration of the pliant event. The behaviour of the event is deﬁned

A Simple Hybrid Event-B Model of an Active Control System …
163
by the COMPLY and SOLVE clauses. The SOLVE clause speciﬁes behaviour fairly
directly. For example the behaviour of pliant variable y is given by a direct assignment
to the (time dependent) value of the expression E(. . .). Alternatively, the behaviour of
pliantvariablex isgivenbythesolutionoftheﬁrstorderordinarydifferentialequation
(ODE) D x = φ(. . .), where D indicates differentiation with respect to time. (In fact
the semantics of the y = E case is given in terms of the ODE D y = D E, so that
both x and y satisfy the same regularity properties.) The COMPLY clause can be
used to express any additional constraints that are required to hold during the pliant
event via its before-during-and-after predicate BDApred. Typically, constraints on
the permitted range of values for the pliant variables, and similar restrictions, can be
placed here.
The COMPLY clause has another purpose. When specifying at an abstract level,
we do not necessarily want to be concerned with all the details of the dynamics—it is
often sufﬁcient to require some global constraints to hold which express the needed
safety properties of the system. The COMPLY clauses of the machine’s pliant events
can house such constraints directly, leaving it to lower level reﬁnements to add the
necessary details of the dynamics.
Brieﬂy, the semantics of a HEB machine is as follows. It consists of a set of system
traces, each of which is a collection of functions of time, expressing the value of
each machine variable over the duration of a system run. (In the case of HyEvBMch,
in a given system trace, there would be functions for clk, x, y, u, each deﬁned over
the duration of the run.)
Time is modeled as an interval T of the reals. A run starts at some initial
moment of time, t0 say, and lasts either for a ﬁnite time, or indeﬁnitely. The dura-
tion of the run T, breaks up into a succession of left-closed right-open subintervals:
T = [t0 . . . t1), [t1 . . . t2), [t2 . . . t3), . . . . The idea is that mode events (with their dis-
continuous updates) take place at the isolated times corresponding to the common
endpoints of these subintervals ti, and in between, the mode variables are constant
and the pliant events stipulate continuous change in the pliant variables.
Although pliant variables change continuously (except perhaps at the ti), conti-
nuity alone still allows for a wide range of mathematically pathological behaviours.
To eliminate these, we make the following restrictions which apply individually to
every subinterval [ti . . . ti+1):
I Zeno: there is a constant δZeno, such that for all i needed, ti+1 −ti ≥δZeno.
II Limits: for every variable x, and for every time t ∈T, and for δ > 0, the left limit
limδ→0 x(t −δ) written −→
x(t) and right limit limδ→0 x(t + δ), written ←−
x(t), exist,
and for every t, x(t) = ←−
x(t). [N.B. At the endpoint(s) of T, any missing limit is
deﬁned to equal its counterpart.]
III Differentiability: The behaviour of every pliant variable x in the interval
[ti . . . ti+1) is given by the solution of a well posed initial value problem
D xs = φ(xs . . .) (where xs is a relevant tuple of pliant variables and D is the time
derivative). ‘Well posed’ means that φ(xs . . .) has Lipschitz constants which are
uniformly bounded over [ti . . . ti+1) bounding its variation with respect to xs, and
that φ(xs . . .) is measurable in t.

164
R. Banach and J. Baugh
Regarding the above, the Zeno condition is certainly a sensible restriction to
demand of any acceptable system, but in general, its truth or falsehood can depend
on the system’s full reachability relation, and is thus very frequently undecidable.
The stipulation on limits, with the left limit value at a time ti being not necessarily
the same as the right limit at ti, makes for an easy interpretation of mode events that
happen at ti. For such mode events, the before-values are interpreted as the left limit
values, and the after-values are interpreted as the right limit values.
The differentiability condition guarantees that from a speciﬁc starting point, ti say,
there is a maximal right open interval, speciﬁed by tMAX say, such that a solution to
the ODE system exists in [ti . . . tMAX). Within this interval, we seek the earliest time
ti+1 at which a mode event becomes enabled, and this time becomes the preemption
point beyond which the solution to the ODE system is abandoned, and the next
solution is sought after the completion of the mode event.
In this manner, assuming that the INITIALISATION event has achieved a suitable
initial assignment to variables, a system run is well formed, and thus belongs to the
semantics of the machine, provided that at runtime:
• Every enabled mode event is feasible, i.e. has an after-state, and on its com-
pletion enables a pliant event (but does not enable any mode event).2
(1)
• Every enabled pliant event is feasible, i.e. has a time-indexed family of after-
states, and EITHER:
(2)
(i) During the run of the pliant event a mode event becomes enabled. It preempts
the pliant event, deﬁning its end. ORELSE
(ii) During the run of the pliant event it becomes infeasible: ﬁnite termination.
ORELSE
(iii) The pliant event continues indeﬁnitely: nontermination.
Thus in a well formed run mode events alternate with pliant events. The last event
(if there is one) is a pliant event (whose duration may be ﬁnite or inﬁnite).
We note that this framework is quite close to the modern formulation of hybrid
systems. (See e.g. [27, 35] for representative formulations, or the large literature in
the Hybrid Systems: Computation and Control series of international conferences,
and the further literature cited therein.)
In reality, there are a number of semantic issues that we have glossed over in the
framework just sketched. We refer to [11] for a more detailed presentation. Also,
the development we undertake requires the multi-machine version of HEB [12].
Since the issues that arise there are largely syntactic, we explain what is needed for
multi-machine HEB in situ, as we go along.
2 If a mode event has an input, the semantics assumes that its value arrives at a time distinct from
the previous mode event, ensuring part of (1) automatically.

A Simple Hybrid Event-B Model of an Active Control System …
165
Fig. 3 The simple
mechanical model that the
HEB development is based
on, after [31]
k
c
m
w(t)
z(t)
p(t)
4
Top Level Abstract Model of the Control System
As stated in Sect.2, we do not get deeply embroiled in the detailed control engineering
aspects of realistic active control in this paper. We base our treatment on the relatively
simple strategy described in detail in [31].
Figure3 shows the simple system investigated in [29, 31]. The model refers to
the dynamics of a mechanical system with a single degree of freedom (SDOF).
The building to be protected is modelled as a concentrated or lumped mass m and
a structural system that resists lateral motion with a spring of stiffness k and a
viscous damper with coefﬁcient c.3 A force p is applied to the mass by the active
control system. The effects of spring and damper depend on the relative position
between a ﬁducial point in the building w, and another ﬁducial point in the earth
z, i.e. on x = w −z. When w = z = 0, the spring is unstretched. Writing D for the
time derivative, and deﬁning e ≡D2z, the dynamics of w, expressed in terms of the
relative displacement x, is thus controlled by:
m D2 x + c D x + k x = p −m e
(3)
Since p is to be chosen by the system, e can be measured, and the other data are
known, (3) yields a yields a law that can be used to keep x within desired bounds.
The code for the top level model of the HEB development is in Fig.4. At this level
the system consists of a single machine, ActConMch_0. There are pliant variables
x, p, which capture the model elements discussed above.
The INVARIANTS are rather basic at this stage. They declare the types of the
variables, and one further non-trivial property. This property actually expresses the
3Idealizing a building by an equivalent SDOF system requires an assumption about its displaced
shape and other details that are beyond the scope of the paper. The interested reader is directed to
the methodology outlined by Kuramoto et al. [24], which is included in the current building design
code used in Japan, as one example.

166
R. Banach and J. Baugh
MACHINE ActConMch 0
PLIANT x, p
INVARIANTS
x, p ∈R,R
|x| ≤XB
EVENTS
INITIALISATION
STATUS ordinary
BEGIN
x, p := 0,0
END
...
...
...
...
MONITOR
STATUS pliant
ANY e?
WHERE
e? ∈R∧|e?| ≤EB
COMPLY INVARIANTS
END
END
Fig. 4 A highly abstract model of the earthquake damage prevention active control system
key system requirement of the whole development, namely that the value of variable
x stays within a range −XB ≤x ≤XB. Imposing it amounts to placing a limit on
the lateral drift4 of a building structure, i.e., the horizontal displacement that upper
stories undergo with respect to the base. This relative motion is resisted by the
building’s structural system, but under extreme events the internal deformations may
be excessive, leading to structural damage and ultimately collapse of the building.
The INITIALISATION event sets all the variables to zero. Then there is the single
actual event of the model: the MONITOR pliant event, which covers the continuous
monitoring of the system when it is in the monitoring mode. Since this is the only
mode in the model, it does not require a speciﬁc variable or value to name it.5
The deﬁnition of the MONITOR pliant event is trivial at this level of abstraction. It
consumes its input e? evidently corresponding to the relevant model element above.
For future calculational tractability, e? is assumed to be bounded by an explicit
constant EB. The event simply demands that the INVARIANTS are to be maintained.
This is to be established in some as yet unspeciﬁed manner, since we postpone the
more demanding details of the calculations involved in the control model we have
introduced. Later on, its more precisely deﬁned job will be to monitor the information
coming from the sensors (i.e. to monitor w and its derivatives, and e), to calculate
what response may be necessary, and to issue pulses to the actuators, as may be
required, these being the embodiment of p. Of course, in reality, the monitoring will
be done via a series of discrete events, resulting in a series of readings from the
sensors, but for the next few models in the development, we will assume that this is
all a continuous process.
4The International Building Code (IBC) requires that drift be limited in typical buildings to 1–2%
of the building’s height for reasons of both safety and functional performance.
5Of course, a more realistic model would contain modes for maintenance, and for other forms of
partial running or of inactivity—to be used, presumably, only when the building is unoccupied.

A Simple Hybrid Event-B Model of an Active Control System …
167
MACHINE ActConMch IDEAL
REFINES ActConMch 0
PLIANT x,y, p
INVARIANTS
x,y, p ∈R,R,R
|x| ≤XB
EVENTS
INITIALISATION
REFINES INITIALISATION
STATUS ordinary
BEGIN
x,y, p := 0,0,0
END
...
...
...
...
MONITOR
REFINES MONITOR
STATUS pliant
ANY e
WHERE
e? ∈R∧|e?| ≤EB
SOLVE
p := me?
Dx = y
Dy = −c
m y−k
m x+ 1
m p−e?
END
END
Fig. 5 Idealised reﬁnement of the system
5
An Idealised Reﬁnement: Miraculous ODE Behaviour
Figure5 presents a somewhat idealised reﬁnement of ActConMch_0. Rather than
assume the desired effect is achieved nondeterministically, we introduce the control
law (3) in the SOLVE clause of MONITOR. In order to do that we introduce a new
variable y so that we can translate the second order (3) into the ﬁrst order form
stipulated by HEB. Having done that, we notice that we need merely to set p to m e?
and the zero initialisation of x, y persists to a global solution satisfying the invariants.
We are done! The building stands still under all admissible earthquake conditions!
If only it were that simple. Unfortunately, it requires that p be chosen to mir-
ror the instantaneous real time behaviour of e? with complete precision, with no
allowance for quantisation effects or for signal propagation delay in equipment. This
is impractical in the real world. Accordingly, we abandon this route in favour of a
more achievable development route.
6
A More Realistic Reﬁnement: Achievable ODE
Behaviour
The problem with the MONITOR of Fig.5 is that it is already so precise that there
is no way to backtrack to a more tolerant engineering model while remaining within
the restrictions of reﬁnement theory. In this section we have a different reﬁnement
of ActConMch_0, which allows some leeway for engineering imprecision.
In Fig.6 we have a transition diagram representation of the system for this reﬁne-
ment. As before there is only one state, the one occupied by the MONITOR event,
and there is now a mode event too, MoSkip, of which more later. The HEB code for
the reﬁnement is in Fig.7.

168
R. Banach and J. Baugh
Fig. 6 A transition diagram
for the ﬁrst reﬁnement of the
HEB model of the
earthquake damage
prevention system
MONITOR
MoSkip
The behaviour of the MONITOR event reﬁnes its previous incarnation by restrict-
ingthebehaviour.Aswellastheinpute?therearenowtwolocallychosenparameters,
pp and e. The former allows values that match e? imprecisely to be fed to the ODE
system in the SOLVE clause (which is almost the same as in Fig.5), while the latter
permits the stipulation that e? differs from a constant value (which may be chosen
conveniently) by not too much during a MONITOR transition.
The COMPLY clause takes advantage of the fact that the solution to such linear
constant coefﬁcient inhomogeneous ODE systems is routine. See [4, 18, 32, 36] as
well as a host of other sources. The ﬁrst term is the homogeneous solution, primed
by the initial values: eAAA(t−tL)[x(tL), y(tL)]T, where AAA is the companion matrix of
the homogeneous part of the ODE system in the SOLVE clause, and tL refers,
generically,tothestarttimeofanyruntimetransitionspeciﬁedbythepliantevent.The
second term is the convolution ∗over the interval [tL . . . t] between the homogeneous
solution eAAA(s) (with bound convolution variable renamed to s) and the inhomogeneous
part [0, 1
mpp −e?]T. If the projection of all this to the x variable (written ...x) achieves
the desired bound, then the ODE system in the SOLVE clause establishes the desired
MACHINE ActConMch 1
REFINES ActConMch 0
CLOCK clk pls
PLIANT x,y, p
INVARIANTS
x,y, p ∈R,R,R
|x| ≤XB
EVENTS
INITIALISATION
REFINES INITIALISATION
STATUS ordinary
BEGIN
clk pls := 0
x,y, p := 0,0,0
END
MoSkip
STATUS anticipating
WHEN
clk pls = TP
THEN
clk pls := 0
END
...
...
...
...
MONITOR
REFINES MONITOR
STATUS pliant
ANY pp,e,e?
WHERE
pp ∈R∧|pp| ≤PPB ∧
e ∈R∧
(e)∧
e? ∈R∧|e?| ≤EB ∧|e−e?| ≤eB
COMPLY
{{{eAAA(t−L)[x( L),y( L)]T+
(eAAAs ∗[ L...t] [0, 1
m pp−e?]T)}}}...x
≤XB
SOLVE
p := pp
Dx = y
Dy = −c
m y−k
m x+ 1
m p−e?
END
END
Fig. 7 First reﬁnement of the system

A Simple Hybrid Event-B Model of an Active Control System …
169
invariant. The permitted imprecision between pp and e? now makes this a practical
proposition.
The mode event MoSkip interrupts the MONITOR event at intervals of TP,
after which MONITOR restarts. This permits the reassignment of the constant e
in MONITOR at each restart. If the interval TP is short enough it permits the choice
of pp during each MONITOR transition to achieve the desired outcome.
The caption of Fig.7 claims that ActConMch_1 is a reﬁnement of ActConMch_0.
Indeed it is, although we do not describe the details of this here; see [11] for a more
thorough account. We note though that: the invariants are not weakened; the new
initialisation is evidently consistent with the old; the new behaviour of MONITOR
evidently satisﬁes the previous deﬁnition; and the new mode event only updates a
newly introduced (clock) variable. All of these are characteristics of HEB reﬁnement.
7
Reﬁning pp
The objective of the next reﬁnement is to address the speciﬁc form of pp, bringing the
design closer to engineering practice, and to [29, 31] in particular. For this we follow
the detailed formulation in [19], which contains a wealth of detailed calculation.
We make a conventional change of parameters: ζ = c/2
√
km, ωn = √k/m, ωD =
ωn

1 −ζ 2. This change reduces the LHS of (3) to (m times):
D2x + 2ζDx + ω2
nx
(4)
In terms of these quantities, the generic solution of the ODE system indicated above
reduces to a Duhamel integral with speciﬁed initial values [19, 39]:
x(t −tL) =
e−ζ ωn (t−tL)

x(tL) cos(ωD(t −tL)) + y(tL) + ζ ωn x(tL)
ωD
sin(ωD(t −tL))

+
1
m ωD
 (t−tL)
0
(pp(s) −m e?(s)) e−ζ ωn((t−tL)−s) sin(ωD((t −tL) −s)) ds
(5)
y(t −tL) = D x(t −tL) =
e−ζ ωn(t−tL)

y(tL) cos(ωD(t −tL)) −ωn
ωD
(ζ y(tL) + ωn x(tL)) sin(ωD(t −tL))

+ 1
m
 (t−tL)
0
(pp(s) −m e?(s)) ×
e−ζ ωn((t−tL)−s) 
cos(ωD((t −tL) −s)) −ζ ωn
ωD
sin(ωD((t −tL) −s))

ds
(6)

170
R. Banach and J. Baugh
The idea now is to tailor the various parameters of the model in such a way that we
can prove that the form we choose for pp lets us derive the desired bound |x| ≤XB.
To conform to engineering practice for this class of systems, the form we choose
for pp will consist of pulses, as suggested earlier. Pulses have a large value for a short
period, and are zero the rest of the time. As clearly explained in [19], if the support of
a pulse is small, its precise shape has little effect on the dynamics, and only its overall
impulse (i.e. integral) matters. Thus the natural temptation is to idealise the pulse
into a ‘delta function’, which has zero duration but nonzero integral. Although no
engineering equipment implements a delta function pulse, the idealisation simpliﬁes
calculations, and so we will pursue it here, since the deviation from a realistic pulse
will be small. Technically, the idealisation also allows us to illustrate how delta
functions can be handled in HEB.6
Tacitly, we can identify the time period TP in Fig.7 with the interval between
pulses. Suppose then that one of these idealised delta pulses has just occurred. In
the ensuing interval, the form of pp will be zero, so the pp terms can be removed
from (5) and (6). Assuming that we know x(tL) and y(tL), we thus calculate the
behaviour of x and y in the ensuing interval. Demanding that this remains within
safe limits imposes constraints on x(tL) and y(tL), which it was the obligation of
the immediately preceding pulse to have ensured. Analogously, it is the obligation
of the next pulse to ensure equally safe conditions for the next interval. And so on.
Thus, we are interested in estimating the behaviour of the x and y variables during
a transition of the MONITOR event. To this end, we argue as follows. Having tacitly
arranged that the pulses occur at the transitions speciﬁed by the MoSkip event, and
thus that pp is zero during a MONITOR transition, we note that the period of the
building’s vibrations during an earthquake, which is typically of the order of a second
or two and is captured in the constants ωn and ζ, will be much longer than the response
time of the active protection system, i.e. will be much longer than TP. Therefore, the
domain of integration in (5) and (6) will always be much shorter than a half cycle of
the trigonometric terms, as a consequence of which the combined exponential and
trigonometric terms will always be positive throughout the domain of integration.
In such a case, the extremal values of the integral will arise when the modulating
factor e? takes its own extremal values. These are just the constant values e ± eB
(the sign to be chosen depending on which one favours the argument we wish to
make). Substituting these (and keeping both signs in case of future need) reduces the
integrals to an analytically solvable form, which is readily evaluated [23, 26]. For a
duration TP we get:
x(TP + tL) =
e−ζ ωn TP

x(tL) cos(ωD TP) +
1

1 −ζ 2
 1
ωn
y(tL) + ζ x(tL)

sin(ωD TP)

6The issue is not a trivial one. HEB semantics is deﬁned in terms of piecewise absolutely continuous
functions [11]. But a delta function is not piecewise absolutely continuous, because, to be precise,
it is not a function at all.

A Simple Hybrid Event-B Model of an Active Control System …
171
−(e ± eB) 1
ω2n

1 −e−ζ ωn TP
cos(ωD TP) +
ζ

1 −ζ 2 sin(ωD TP)

(7)
y(TP + tL) = D x(TP + tL) =
e−ζ ωn TP

y(tL) cos(ωD TP) −
1

1 −ζ 2

ωn x(tL) + ζ y(tL)

sin(ωD TP)

−(e ± eB) 1
ωD
e−ζ ωn TPsin(ωD TP)
(8)
Next we observe that the impulsive force that the active protection system applies
during a pulse will not signiﬁcantly changex but will only have a signiﬁcant impact on
y. Thus, assuming the system only reacts when |x| is close to its permitted maximum
value (speciﬁed by an appropriately chosen threshold value Xth), we infer that the
following statements:
if 0 < Xth ≤x(tL) ≤XB then ensure x(TP + tL) ≤XB fi
and
(9)
if 0 > −Xth ≥x(tL) ≥−XB then ensure x(TP + tL) ≥−XB fi
(10)
express a policy for ensuring that the invariant |x| ≤XB is maintained throughout
the dynamics of the system. These allow us to focus predominantly on Eq.(7), using
(8) only occasionally.
We note that for the typical scenario of interest, ζ ≲0.1, so that

1 ± ζ 2 ∼= 1.
From this we deduce that ωn ∼= ωD, so we call both of them ω henceforth. Bearing
the implications of such system parameters in mind, we embark on a process of
simplifying (7) and (8). The observations just made lead to:
x(TP + tL) =
e−ζ ω TP
x(tL) cos(ω TP) +
 1
ω y(tL) + ζ x(tL)

sin(ω TP)
	
−(e ± eB) 1
ω2

1 −e−ζ ω TP
cos(ω TP) + ζ sin(ω TP)
	
(11)
y(TP + tL) =
e−ζ ω TP
y(tL) cos(ω TP) −

ω x(tL) + ζ y(tL)

sin(ω TP)
	
−(e ± eB) 1
ω e−ζ ω TPsin(ω TP)
(12)
Also, to ensure that the system is responsive enough to adequately dampen large
oscillations coming from an earthquake, it should be prepared to respond at least
20 times per building oscillation, making ωTP ∼= 0.05, and making ζωTP ∼= 0.005.
This allows us to further simplify (11) and (12), keeping low order terms only. We
work to second order in ωTP and regard ζ ≈ωTP. This leads to the discarding
of contributions [(1/2)ζ 2 ω2 T 2
P] (y(tL) TP) to x(TP + tL) and of ζ 2 ω2 T 2
P y(tL) +

172
R. Banach and J. Baugh
ζ ω2 T 2
P (x(tL) ω)−(e ± eB) TP ((1/2) ζ 2 ω2 T 2
P) to y(TP + tL)—these will certainly
be negligible if we consider that real systems are noisy. In this way we get:
x(TP + tL) =

x(tL)

1 −ω2 T 2
P
2

+ y(tL) TP

1 −ζ ω TP
	
−(e ± eB) T 2
P
2
(13)
y(TP + tL) =

y(tL)

1 −2 ζ ω TP

−ω x(tL) ω TP
	
−(e ± eB) TP

1 −ζ ω TP

(14)
These formulae exhibit characteristics that we would expect. Thus, the leading
contribution to x(TP + tL) is x(tL) + y(tL) TP, to which are added smaller correc-
tions, while the leading contribution to y(TP + tL) is y(tL) itself, modiﬁed by smaller
corrections. The relative constancy of the velocity y over an interval TP conﬁrms that
our proposed strategy, of imposing a pulse which discontinuously alters y(tL), will
be the dominant effect on the displacement variable x during the interval. We also see
that the earthquake acceleration, which contributes the (e ± eB) terms, is not very
signiﬁcant unless it is violent enough to be comparable to the time period TP or its
square.
In principle (13) and (14) give us enough to design the protection system. At
the end of each TP interval, we examine x(tL) and y(tL), we calculate x(TP + tL)
according to (13), and if the answer exceeds XB, we apply a pulse to change y(tL) to
a new value y(tL) for which a recalculated x(TP + tL) does not exceed XB. However,
we wish to do a bit better. We would like to identify a safe region, given by a threshold
value Xth, such that if x(tL) ≤Xth, no further action is needed. To identify Xth, we
need an upper bound for y(tL) so that we can estimate how much ‘help’ the velocity
could give to x during an interval. We argue as follows.
We note that starting from a stationary state, neglecting the lower order corrections
(including the contribution from x(tL) whose coefﬁcient is small), and considering
the strongest earthquake the system is designed to cope with, EB, each interval can
add at most EB TP to y. So after N intervals, y is at most N EB TP. Turning to x, an
interval can similarly add at most EB T 2
P/2 from the last term of (13), and after N
intervals, (1 + 2 . . . N) EB TP is added from the velocity term, giving a total, after N
intervals, of EB T 2
P(N 2 + 2N)/2. This must not exceed XB, which leads to7:
N ≈


2XB/EB T 2
P + 1

(15)
The threshold value Xth must be small enough that the largest possible single incre-
ment of x cannot exceed XB −Xth. From (13), using the maximal velocity derived
earlier, we get:
7In deriving this, we dropped a term −1 from the RHS of (15).

A Simple Hybrid Event-B Model of an Active Control System …
173
Fig. 8 A transition diagram
for the second reﬁnement of
the HEB earthquake damage
prevention model
MONITOR
PulseYesY
PulseNo
PulseYesE
PulseMaybe
Xth ≤XB −

EB T 2
P

2XB/EB T 2
P + 1 + EB T 2
P
2

(16)
For (16) to be reasonable, its RHS must be positive, which leads to the consistency
condition 2 XB ≥EB T 2
P. This is sensible, since if not (and referring to (13)), a single
cold start interval could overreach XB, and the threshold idea would not make sense.
(The same condition is also necessary for (15) to yield a positive integer, when the
discarded −1 is reinstated.)
From the account above, it is clear that if |x(tL)| ≤Xth at the start of an interval,
then the system need do nothing. This will be the case most of the time in reality,
since the only vibrations sensed will be from normal everyday activity in the building
and its surroundings. However, if |x(tL)| > Xth, then the more detailed calculation
in (13) will be needed, in case there is a risk of exceeding the bound XB.
These observations underpin the next model in our HEB development, whose
transition diagram is in Fig.8, and the text of which is in Fig.9. In this model, the
MONITOR event no longer has a COMPLY clause stipulating the behaviour of the
system via an implicitly chosen pp function. In accordance with our discussion, the
externally imposed force p is zero during MONITOR. The job of ensuring that the
invariant |x| ≤XB is maintained becomes the responsibility of delta pulses that jolt
the system into acceptable behaviour when necessary.
Here we hit a technical snag, in that delta functions do not exist in the semantics
of HEB (see footnote 6). Rather than express the needed delta functions directly,
we use their time integrals, which are discontinuous functions, which do exist in the
semantics of HEB, and are typically implemented using mode events. The burden
of implementing the pulses thus falls to reﬁnements of the earlier MoSkip event,
which implement the imposition of the needed delta functions onto the acceleration
DDx = Dy, by instead imposing discontinuities on its integral y.
Accordingly, when time is a multiple of TP, if |x| < Xth, then event PulseNo
executes, and just resets the clock. But if |x| ≥Xth then we need a more complex
calculation, analogous to Eq.(13). If this reveals that the projected future |x| value
will nevertheless still be below XB, then the action is the same, expressed in event
PulseMaybe.

174
R. Banach and J. Baugh
Fig. 9 Second reﬁnement of the system

A Simple Hybrid Event-B Model of an Active Control System …
175
However, if the calculation reveals that without intervention XB will be breached,
then the system must intervene to prevent it. This is captured in mode events
PulseYesY and PulseYesE and involves a case analysis as follows.
Let us call Δx the difference between the projected future |x| value and the before-
value of |x| in these events, as in the two events’ guards. Then if Δx turns out positive,
it can only be because either the y term or the −e? term of the projected future |x|
value, or both, is/are driving |x| too high. At least one of these terms has a value
whose sign is the same as that of the before-value of x in the two events, else both
terms would drive |x| smaller, contradicting the breaching of XB. N.B. We assume
that the threshold is big enough that above threshold, a single interval cannot cause x
to change sign, and thus cannot cause |x| to increase even in cases in which the rate
of change of x changes sign.
Suppose then that both terms have values whose sign agrees with that of the value
of x. Then one of them has a value which is at least Δx/2 since they act additively and
their sum is Δx. In this case it is sufﬁcient to invert the sign of the larger contribution
to ensure that their net effect diminishes |x|. So we either ﬂip y, or ﬂip a suitably
rescaled e?. This covers one of the two cases in each of PulseYesY and PulseYesE.
Suppose alternatively that only one of the terms has a value whose sign agrees
with that of the value of x. Then the magnitude of that term must exceed Δx, since
they act subtractively and the difference of their magnitudes is still Δx.8 In this case
it is sufﬁcient to invert the sign of this larger contribution to ensure their net effect
diminishes |x|. This covers the remaining two cases in PulseYesY and PulseYesE.
8
On HEB Reﬁnement
At this point we reﬂect on the reﬁnement just done. A ﬁrst point notes that during nor-
mal Event-B reﬁnement [3], the behaviour of an event is typically restricted, making
it more deterministic. In our case, we have taken this to an extreme, by effectively
abandoning external control of the behaviour of the dynamical variables x and y via
p during MONITOR, and have delegated this duty instead to the PulseXX events. So
the PulseXX events are new in the model of Fig.9, and deﬁne new behaviour for y
(and potentially for x too, if it were needed). This is against the rules of Event-B
reﬁnement, since new behaviour for variables should be introduced at the same time
as the variables themselves (being made more deterministic subsequently)—whereas
we introduced x and y during the previous reﬁnement.
We partly mitigated this by making the PulseXX events reﬁne the earlier MoSkip
events, introducedduringtheprevious reﬁnement stage, andgivingtheMoSkipevents
the status ‘anticipating’. This status allows an event, newly introduced during a
reﬁnement step, and which would normally be required to strictly decrease a relevant
8Mathematically, it is possible for both terms to have magnitude bigger than Δx, unless we take
into account relevant upper bounds etc. and show that it is impossible. We will just assume that
there is no such possibility in our problem space.

176
R. Banach and J. Baugh
variant function (to ensure the convergence of the new behaviour), to not strictly
decrease the variant then, postponing this obligation till later.9 Since we included no
variants in our development, we discharged this duty trivially. The introduction of
MoSkip and variable y at the same time thus not only allowed fresh choice of e in
successive iterations of MONITOR but allowed the manipulation of y in reﬁnements
of MoSkip.
Unfortunately though, by not mentioning y at all, MoSkip by default speciﬁes
that y does not change during MoSkip transitions, while the PulseYes events reﬁning
it specify nontrivial changes to y. It is tempting to think that this is still a reﬁne-
ment, since the only invariant concerning y is y ∈R, the weakest possible invariant.
However, when the same variable exists in a machine and in its reﬁnement, there
is an implicit equality invariant between the abstract and concrete versions of the
variable—otherwise writing more conventional reﬁnements would become intoler-
ably verbose. In this regard, ‘no change’ in MoSkip is incompatible with ‘nontrivial
update’ in PulseYes, and our reﬁnement isn’t quite legal after all. This shows that
‘reﬁning to a delta function’ is not ideally handled in HEB. The only way to make
the development unimpeachable according to the rules of Event-B is to introduce the
variable y and the nontrivial mode event behaviour at the same time. But this is less
desirable from our point of view, as it forces the choice of control strategy without
permitting consideration of alternatives, and ﬂies in the face of the objectives of a
reﬁnement driven development strategy which aims at introducing detail into designs
in stages.
A second point concerns the arguments we employed in the preceding pages. Our
reasoning started out being quite watertight mathematically, but rather quickly, we
started to introduce simpliﬁcations which were perfectly justiﬁable on engineering
grounds, but which would not pass the unblinking scrutiny of formal proof. Two
centuries or more of rigorous mathematical analysis have, in principle, developed
techniques, using which, such a shortcoming could be overcome, but the amount of
work involved would be considerable, and would quickly surpass the small amount of
added assurance that could be gained. The formal development ﬁeld, in its somewhat
strenuous avoidance of engagement with continuous mathematics hitherto, has not
really developed a cost effective approach to dealing with this issue.
A third point concerns the extent to which the model of Fig.9 can actually be
proved correct using a per event proving strategy as embodied in the semantics
and veriﬁcation architecture of Event-B. This is by contrast with the arguments in
preceding pages, which focused on the application structure and employed whatever
observations seemed useful at the time, without regard to how the various points
were to be structured into an overall correctness argument. Here, the news, though
not perfect, is better.
We note that the MONITOR event, as written in Fig.9, cannot by itself be correct
according to the normal ‘preserving the invariant’ notion of event correctness, since
9The formal presentation of HEB [11] does not mention the anticipating status, since that is some-
what outside the main concerns there. But there is no reason to forbid it since it concerns purely
structural matters.

A Simple Hybrid Event-B Model of an Active Control System …
177
it demands no restrictions on x(tL) and y(tL). Without prior knowledge about these,
the ODE system can easily breach the x ≤XB bound during a TP interval. Of course,
we rely on the PulseYes events to ensure appropriate x(tL) and y(tL) values for the
subsequent MONITOR event, but the MONITOR event correctness proof obligations
know nothing of this. However, in HEB, we also have ‘well-formedness’ proof obli-
gations, that police the handover between mode and pliant events. These can check
that after any of the PulseXX events, the values of x and y are appropriate. In par-
ticular, they check that after the PulseXX events the guard for at least one pliant
event is true. Since we have designed the PulseXX events to ensure exactly what
is required here, the trivial guard of the MONITOR event of Fig.9 could, in fact,
be strengthened to demand a suitably stringent constraint on x(tL) and y(tL), from
which, ‘preserving the invariant’ would become possible. So, although we did not
get diverted by this detail earlier, a solution entirely within the rules is available.
9
Sensors, Actuators, Sampling, Quantization,
Decomposition
The next model in our development tackles a number of issues that add low level
complexity. Following the structure of [20, 21], we introduce a sensor and an actu-
ator into the system. Elements like these bring various kinds of imprecision to the
development. Thus, they typically act at discrete moments of time—this brings tem-
poral imprecision. Their inputs and outputs typically have ﬁnite ranges, and are
quantized—this brings imprecision of magnitude. The impact of these sources of
imprecision is similar from a formal point of view, and describing these phenomena
precisely, generates complexity in the textual description of the system.
Moreover, a model close to the architectural structure of [20, 21] would place the
architecturally distinct components of the system in separate constructs. To create
such a model requires the decomposition of a monolithic version into smaller pieces,
a process which, if done with precision, generates both textual complexity and a lot
of repetition of the model text.
To minimise verbosity, our strategy will therefore be as follows. Viewing the
model of Fig.9 as being at level 2, the level 2 model is conceptually developed
into an unstated, but still monolithic model, incorporating the features mentioned
above, at level 3 (with machine ActConMch_3 say). This is then decomposed into
a multimachine project at level 4, exhibiting the desired architectural structure. The
level 4 model is presented in Figs.11, 12, 13 and 14, and described below. We
comment more extensively on the level 4 model later on.
In Fig.10 we have a depiction of the various HEB machines of the distributed
concurrent HEB model that results from the process just sketched. Figures11, 12,
13 and 14 contain the text of the resulting model. We start with the PROJECT
ActCon_4_Prj ﬁle in Fig.11, which describes the overall structure. The DECOM-
POSES ActCon_3_Prj line refers to the ﬁctitious level 3 system, of which more

178
R. Banach and J. Baugh
QUAKE
EarthMch_3
MONITOR
PulseYesY_S
PulseYesE_S
BuildingMch_3
Sample_18_S
PliTrue
PulseYesY_S
PulseNo
PulseYesE_S
PulseMaybe
Sample_19_S
ControllerMch_3
Sample_18_S
PliTrue
Sample_19_S
SensorMch_3
PliTrue
PulseYesY_S
PulseYesE_S
ActuatorMch_3
Fig. 10 A family of transition diagrams for the HEB machines of a distributed concurrent version
of the active earthquake damage prevention system
later. The main job of the PROJECT ﬁle is to name the constituent machines and
interfaces, and to deﬁne needed synchronisations between the mode events of the
different machines. Thus there are machines for the earth, the building, the actua-
tor, the sensor, and the controller. The PROJECT ﬁle also names the INTERFACE
ActCon_4_IF ﬁle. This declares any variables that are shared between more than
one machine, their initialisations, and, most importantly, any invariants that men-
tion any of these variables. (The latter point can place stringent restrictions on how
variables are partitioned into different interfaces and machines.) The ﬁnal respon-
sibility of the PROJECT ﬁle is to declare the mode event synchronisations. Thus
SYNCHronisation Sample18 speciﬁes that mode event Sample_18_S in machine
SensorMch_4 and mode event Sample_18_S in machine ControllerMch_4 must be
executed simultaneously. This means that they can only execute if all the guard con-
ditions in all the events of the synchronisation are true. The same remarks apply to
the other synchronisations declared in the project ﬁle.

A Simple Hybrid Event-B Model of an Active Control System …
179
PROJECT ActCon 4 Pr j
DECOMPOSES ActCon 3 Pr j
MACHINE EarthMch 4
MACHINE BuildingMch 4
MACHINE SensorMch 4
MACHINE ControllerMch 4
MACHINE ActuatorMch 4
INTERFACE ActCon 4 IF
SYNCH(Sample18)
SensorMch 4.Sample 18 S
ControllerMch 4.Sample 18 S
END
SYNCH(Sample19)
SensorMch 4.Sample 19 S
ControllerMch 4.Sample 19 S
END
SYNCH(PulseYesY)
ActuatorMch 4.PulseYesY S
BuildingMch 4.PulseYesY S
ControllerMch 4.PulseYesY S
END
SYNCH(PulseYesE)
ActuatorMch 4.PulseYesE S
BuildingMch 4.PulseYesE S
ControllerMch 4.PulseYesE S
END
END
INTERFACE ActCon 4 IF
PLIANT xx,yy,ee
INVARIANTS
xx,yy,ee ∈R,R,R
|xx| ≤XB
INITIALISATION
xx,yy,ee := 0,0,0
END
Fig. 11 The PROJECT and INTERACE ﬁles of the further developed and decomposed system
We turn to the machines, pictured in Fig.10. In outline, machine EarthMch_4 is
responsible for producing the earthquake acceleration, which comes from the input
e?, as in previous models. This is simply captured during the pliant event QUAKE
and is recorded in the shared pliant variable ee, declared in the interface (which the
EarthMch_4 machine CONNECTS to). We can see that the QUAKE event comes
from decomposing the earlier MONITOR pliant event, and we will see the remnants
of the MONITOR event elsewhere soon. Since any HEB machine must have at least
one pliant event to describe what happens over the course of time, but need not
contain any other event, and since QUAKE addresses that requirement, there are no
other events in EarthMch_4.
The shared variable ee is accessed by machine BuildingMch_4. This contains
the remainder of the earlier MONITOR pliant event, namely the ODE system deﬁn-
ing the building’s response, which uses ee. It also contains the business end of the
PulseYesY_S and PulseYesE_S mode events, which take their inputs (which are
received from the actuator using input ys?) and discontinuously impose the received
values on the velocity variable yy.
We come to the SensorMch_4 and ActuatorMch_4 machines. Their behaviour
is essentially discrete, so to satisfy the requirement for having a pliant event, both

180
R. Banach and J. Baugh
MACHINE EarthMch 4
CONNECTS ActCon 4 IF
QUAKE
STATUS pliant
ANY e,e?
WHERE
e ∈R∧
(e)∧
e? ∈R∧|e?| ≤EB ∧|e−e?| ≤eB
BEGIN
ee := e?
END
END
MACHINE BuildingMch 4
CONNECTS ActCon 4 IF
EVENTS
PulseYesY S
STATUS ordinary
ANY ys?
WHERE
ys? ∈R
THEN
yy := ys?
END
PulseYesE S
STATUS ordinary
ANY ys?
WHERE
ys? ∈R
THEN
yy := ys?
END
MONITOR
STATUS pliant
SOLVE
Dxx = yy
Dyy = −c
m yy−k
m xx−ee
END
END
MACHINE SensorMch 4
CONNECTS ActCon 4 IF
EVENTS
Sample 18 S
ANY sens x!
WHERE
sens x! ∈R
THEN
sens x! := K−1
xsqs ⌊Kxsqs xx⌉
END
Sample 19 S
ANY sens x!,sens e!
WHERE
sens x! ∈R∧sens e! ∈R
THEN
sens x! := K−1
xsqs ⌊Kxsqs xx⌉
sens e! := K−1
esqs ⌊Kesqs ee⌉
END
PliTrue
STATUS pliant
COMPLY INVARIANTS
END
END
MACHINE ActuatorMch 4
EVENTS
PulseYesY S
ANY ys!,act y?
WHERE
ys! ∈R∧act y? ∈R
THEN
ys! := K−1
ysqs ⌊Kysqs act y?⌉
END
PulseYesE S
ANY ys!,act y?
WHERE
ys! ∈R∧act y? ∈R
THEN
ys! := K−1
ysqs ⌊Kysqs act y?⌉
END
PliTrue
STATUS pliant
COMPLY INVARIANTS
END
END
Fig. 12 Machines for earth, building, sensor and actuator
machines have a default COMPLY INVARIANTS pliant event, named, as is typically
the case, PliTrue. In fact, since all the pliant variables are handled by other machines,
there is nothing for these PliTrue events to do, and that is part of the semantics of
‘COMPLY INVARIANTS’ in HEB.

A Simple Hybrid Event-B Model of an Active Control System …
181
MACHINE ControllerMch 4
CONNECTS ActCon 4 IF
CLOCK clk pls
VARIABLES
x18,x19,y19,e19
INVARIANTS
x18,x19,y19,e19 ∈R,R,R,R
VARIANT ⌈(TP −clk pls)×20⌉
EVENTS
INITIALISATION
STATUS ordinary
BEGIN
clk pls := 0
x19,y19,e19 := 0,0,0
END
PliTrue
STATUS pliant
COMPLY INVARIANTS
END
Sample 18 S
ANY sens x?
WHEN
clk pls = 18
20TP
THEN
x18 := sens x?
END
...
...
...
...
Sample 19 S
ANY sens x?,sens e?
WHEN
clk pls = 19
20TP
THEN
x19 := sens x?
y19 := ⌊(sens x?−x18) 20
TP ⌉
e19 := sens e?
END
PulseNo
STATUS ordinary
WHEN
clk pls = TP ∧|x19| < Xthsq
THEN
clk pls := 0
END
PulseMaybe
STATUS ordinary
WHEN
clk pls = TP ∧|x19| ≥Xthsq ∧
 x19 1−ω2 T 2
P /2

+ y19TP 1−ζ ω TP

−e19T 2
P /2
 ≤XBsq
THEN
clk pls := 0
END
...
...
Fig. 13 The controller machine, ﬁrst part
The job of the SensorMch_4 machine is to sample the physical values required by
a idealised implementation of the system. The values are required at pulse issuing
time, but to allow time for computation, as in [20, 21], they are collected a little earlier.
The position and earth acceleration values, from xx and ee, are collected 19/20 of
the way through a TP interval, and are transmitted (to the controller machine) in
output variable sens_x! and sens_e! An extra position value is needed for calculating
a velocity estimate, so another sample of xx is taken 18/20 of the way through TP.
Notice that the xx and ee values are scaled (by Kxsqs and Kesqs), rounded, and then
unscaled (by K−1
xsqs and K−1
esqs) before sending, to model the quantization process.10
The mode events that do these jobs are Sample_18_S and Sample_19_S. The ‘_S’
sufﬁxes on these names indicate, for readability, that these are synchronised with
mode events in one or more other machines, though, as we mentioned earlier, the
formal deﬁnition of a project’s synchronisations are in the project ﬁle.
The same general comments work for the ActuatorMch_4 machine. Only the
velocity variable is modiﬁed in our development, so only this variable is acted on by
10In reality, a sensor would send values in its own units, and scaling would be done as part of the
controller’s job, but we avoid this so as to keep the controller calculation reasonably transparent.

182
R. Banach and J. Baugh
the actuator. The value needed is received (from the controller machine) in the act_y?
input of synchronised events PulseYesY_S and PulseYesE_S, and after quantization
via Kysqs and its inverse, is transmitted (to the building machine) in the ys! output.11
As for the sensor, there is no need for any non-trivial pliant event, so a default PliTrue
sufﬁces.
At the heart of the system is the ControllerMch_4 machine. This houses the
remaining functionality, and the non-trivial computation. The clock clk_pls is
declared here, as are local variables x18, x19, y19, e19, the sampled values of the
dynamical variables, which are not needed in any other machine. We see also that
ControllerMch_4 only requires the PliTrue default pliant event, since its interven-
tions are exclusively at individual moments of time. It also contains the remaining
portions of the various synchronisations we have discussed.
The Sample_18_S event picks up the sampled position at times 18/20 TP of an
interval, recording them in x18. The Sample_19_S event picks up position and accel-
erationsamplesat19/20 TP and,aswellasrecordingthese,itcalculatesanestimateof
velocity from the position samples and records it in y19. The values in x19, y19, e19
are then ready for the pulse calculations, which would consume some time to do, but
which are modelled as taking place instantaneously at the end of the interval in the
various Pulse events.
Comparedwiththeother events, eventsSample_18_S andSample_19_S arenewly
introduced in this development step. In such a case, Event-B practice asks that they
strictly decreases some variant function, which is included in Fig.13 after the invari-
ants. It is clear that at the two occurrences of the Sample events in each interval, the
value of the variant drops, ﬁrstly from limε→0+ ⌈(TP −( 18
20TP + ε)) × 20⌉= 3 to 2,
and then from 2 to 1,12 thus strictly decreasing it, as required.
The PulseNo, PulseMaybe, and now synchronised PulseYesY_S and PulseYesE_S
events, handle the needed responses to building movement, as before, except that the
calculations are now done using the sampled, quantized (SQ) values rather than the
ideal, instantaneous (II) ones. This inevitably leads to disagreement with the ideal
calculations in the border country where different behaviour regimes meet (in our
case, the border country between the do pulse and don’t pulse regimes).13
In our case, we have to cope with the possibility that the SQ values dictate a pulse
in a situation where the II values don’t (which is tolerable, since it will only happen
in the border country, where pulses are probable anyway), or that the SQ values don’t
dictate a pulse in a situation where the II values do (which is intolerable since it may
permit the physical system to overshoot the XB bound without the SQ model being
aware of it). We must prevent the latter.
11On a technical level, the building and actuator machines illustrate the pattern whereby synchro-
nised mode events in different machines can instantaneously share values: one event uses an output
variable and the others use an input variable with a complementary (CSP style) name.
12Note that this critically depends on insisting that intervals of pliant behaviour are left closed and
right open.
13Henceforth, we will use SQ to refer to and to label elements and quantities relevant to the level
4 model of Figs.11, 12, 13 and 14 (and, implicitly to its unstated level 3 precursor), and II for
elements relevant to the level 2 model of Fig.9, as needed.

A Simple Hybrid Event-B Model of an Active Control System …
183
Fig. 14 The controller machine, second part
The approach we take is to conservatively adjust the constants Xth, XB in the
model to new values Xthsq, XBsq that preclude the intolerable omissions at the price
of admitting more superﬂuous pulses.14 For more convenient discussion, we also
renamed the local variables Δx, w in Fig.14 by adding a subscript.
Our remarks indicate that whenever PulseNoSQ or PulseMaybeSQ can run, then we
must be sure that PulseNoII or PulseMaybeII will also run. This implies a condition
on their guards.
We take the events individually, starting with PulseNoSQ and PulseNoII. It is clear
thatthelatterisenabledwhenevertheformeris,providedthat |x19| < Xthsq ⇒|xx| <
Xth holds. Of course, x19 and xx refer to values at different times, but recalling that
Xth was derived by estimating the maximum achievable displacement over a whole
interval in (16), one twentieth of the same argument will cover the difference between
x19 and xx. So our implication will hold, provided:
Xthsq ≤Xth −

EB T 2
P

2XB/EB T 2
P + 1 + EB T 2
P
2

20
(17)
We see that this is a small correction to Xth, which, for typical parameter values,
will be negligible in practice, if not in mathematics, conﬁrming the conjecture in
footnote 14.
14Speaking realistically, in a genuine earthquake scenario, noise and experimental uncertainty are
likely to be such that the differences between the ideal and conservative values of the constants
vanish into insigniﬁcance. But it is worth checking that the mathematics conﬁrms this.

184
R. Banach and J. Baugh
Turning to PulseMaybeSQ and PulseMaybeII a similar argument applies. Looking
at the relevant guards, we see that as well as (17), we will be able to maintain the
invariant |xx| < XB provided we make an analogous correction to XB for the purpose
of the estimates made in the PulseMaybeSQ guard:
XBsq ≤XB −

EB T 2
P

2XB/EB T 2
P + 1 + EB T 2
P
2

20
(18)
With these two cases understood, we see that the ﬁnal two events are covered also.
Both PulseYesY_SSQ and PulseYesE_SSQ ﬂip the sign of the greatest contribution
to the estimated increment in displacement, based on the same estimate made in
PulseMaybeSQ.
10
Reﬁnement, Retrenchment and Other Technical Issues
In Figs.11, 12, 13 and 14 the only structural directives are DECOMPOSES
ActCon_3_Prj in the project ﬁle, and the CONNECTS ActCon_4_IF in the various
machine ﬁles. There are a number of reasons for this. Firstly, we are presuming that
the hard work of reﬁning ActConMch_2 to incorporate the sensor, actuator and dis-
cretization features will have been achieved in the (unstated) ActConMch_3 machine,
the only element of the (unstated) level 3 project ActCon_3_Prj.15 This understood,
the job of decomposing a monolithic ActConMch_3 machine into the components
seen in Figs.11, 12, 13 and 14 is properly covered by the cited directives. Before
continuing, we brieﬂy comment on this by envisaging how Figs.11, 12, 13 and 14
might be reassembled into a single construct.
Let us start with the two Sample_18_S events, shared between SensorMch_4
and ControllerMch_4, and executed synchronously. In a monolithic ActConMch_3
(whichwouldtakeonthedutiesofbothmachines),therewouldbeasingleSample_18
event, with guard clk_pls = 18
20TP and action x18 := K−1
xsqs ⌊Kxsqs xx⌉. There is no
communication, since all the variables are accessible to the one machine. Sample_19
follows a similar pattern, with two variables assigned. Thus is the sensor machine’s
functionality absorbed into one encompassing machine.
The actuator is dealt with similarly, except that the building is involved; the func-
tionality of the building is also absorbed into the single encompassing machine,
rather as was the case in the level 2 and earlier models. The earth machine is similarly
absorbed, eliminating the need for the shared variable ee. This account illustrates, in
reverse, how the distributed model of Figs.11, 12, 13 and 14 is arrived at, presuming
the preexistence of the monolithic version. Note that it is a deliberate design objec-
tive of the multimachine HEB formalism that the monolithic and distributed versions
should be, in all important aspects, semantically indistinguishable; see [12].
15We can also regard all the previous models as each being in its own single machine project.

A Simple Hybrid Event-B Model of an Active Control System …
185
Thus, the ActConMch_3 is relatively easily imagined, avoiding some verbosity.
Less easy is its relationship to the level 2 machine ActConMch_2—the discussion in
Sect.8, on implicit equality invariants, ﬂags up that the introduction of imprecision
via sampling and quantization may not be unproblematic regarding reﬁnement
methodology.
The immediate problem was avoided by renaming variables x, y to xx, yy in the
level 4 model. But this raises the question of what the relationship between x, y and
xx, yy ought to be. It is a truism in formal development that the stronger the invariants
you write, the harder the work to prove that they are maintained, but the stronger
the assurance that is gained thereby. And conversely. We might thus ease our task
by omitting completely any non-trivial relationship between x, y and xx, yy. But this
will not do since we still have the level 2 invariant x ≤XB to establish, which is
rendered impossible in the level 4 model without some coupling between x, y and
xx, yy.
The obvious relationship to consider is some sort of accuracy bound relating
x and xx, and y and yy. Since the damping factor ζ is positive, the dynamics is
asymptotically stable, so we can expect the dynamics to be contracting16 (although
a reﬁnement relationship based on this still often requires appropriate conditions on
the constants of the system [35]). To see the contracting nature of our dynamics we
ﬁrst need to rewrite (13) and (14) in terms of dimensionally comparable quantities,
for example, in terms of ˜x ≡x and ˜y ≡y/ω. When this is done, (13) and (14),
viewed as a matrix operating on differences in pairs of values of ˜x, ˜y, has entries
(δij + (−1)[i≥j]εij), where δij is the identity, and the εij are small and positive, from
which the contracting nature of the transformation can be inferred.
With this, we can claim that a single execution of MONITOR in each of the II and
SQ systems will maintain a joint invariant of the form ||(x, y) −(xx, yy)||˜1 ≤A,17
provided it is true at the start, but it does not tell us what value we would need to
choose for A for this to be true non-trivially.
The latter problem would require a global analysis which could be quite chal-
lenging. The issue is made the more difﬁcult by the possibility mentioned before,
whereby imprecision caused by conservative design in the SQ system causes the SQ
system to express a pulse whereas the II system does not. If this happened, the ||.||˜1
distance between the II and SQ systems would suddenly increase dramatically, even
if it was well behaved previously, and it would consequently cause the ||.||˜1 norm to
function poorly as a joint invariant between II and SQ systems, posing a signiﬁcant
impediment to reﬁnement as an convincing notion for relating the II and SQ systems.
A weakening of the highly demanding reﬁnement concept is the idea of retrench-
ment [13, 14, 30]. In retrenchment the demand to preserve a ‘nearness’ invariant
is relaxed by permitting different conditions to be speciﬁed for the before-state and
after-state of a pair of transitions in the two systems being compared, and allows
constants, such as A, to be declared locally per transition instance, rather than glob-
ally, as in a reﬁnement relation. This formulation also permits the two systems to
16In a contracting dynamics, nearby points are driven closer by the dynamics.
17The ˜1 refers to an L1 norm on the (instantaneous values of the) tilde variables.

186
R. Banach and J. Baugh
part company during exceptional circumstances. It works well enough if the two
systems quickly recover ‘nearness’, or if the models cease to be relevant at all after
the exception.
In our case, the ‘exceptional’ regime, requiring pulses, is precisely the raison
d’etre of the whole protection system, and it is in this regime (rather then the normal,
stable regime when there is no earthquake) in which the behaviours of the II and SQ
systems are the most unruly. And although retrenchment, as described in [13, 14,
30], addresses the onset of unruly behaviour quite well, it does not really engage with
particular properties of extended periods of unruly behaviour, as we would ideally
like in our application.
A further complication of the scenario where the SQ system pulses and the II
system does not, is that different events in the two systems are involved in these
behaviours (PulseNo and PulseMaybe in II and PulseYesY_S and PulseYesE_S in
SQ). Retrenchment and reﬁnement, as usually deﬁned, assume a static (and partial
if needed) bijection between operations/events in the two models being compared.
This does not cope with the scenario just mentioned, in which overlapping pairs of
(names of) events may need to be related at different times.
Thus our reticence in writing down an explicit level 3 system (with its obligation to
make clear its relationship to the level 2 system) is further explained by the absence of
a suitable species of formal relationship that could be used for the purpose. Without
getting embroiled in too many further details, the present case study provides a fertile
stimulus for developing a richer formulation of retrenchment and reﬁnement capable
of coping with the wealth of phenomena it exhibits.
11
Experiments and Simulations
In this section, we compare the expectations raised by the preceding analytical
work, with the outputs of well established conventional earthquake protection design
approaches, based on numerical simulation.
Our simulations were performed over a time interval from 0 to TMAX, using a
control strategy that, as suggested by the analytical work, is deﬁned by a pulse
intervalTP,allowablerelativedisplacementXB,andanadditionalgroundacceleration
variability term eB. At the start of a pulse interval, the simulation chooses whether
to apply a pulse based on Eq.(14), predicting a value of x at the end of the interval
from the expression hx x + hy y + he (e ± eB), where:
hx = 1 −ω2 T 2
P
2
hy = TP

1 −ζ ω TP

he = −T 2
P
2
(19)
using actual values of—or available estimates for—x, y, and e at time t.
Figure15 shows the essence of a Python program that performs simulations
using the numerical and scientiﬁc libraries NumPy and SciPy, as well as the
Matplotlib library for plotting; the complete code is available online [8].

A Simple Hybrid Event-B Model of an Active Control System …
187
Fig. 15 Python program for numerical simulation [8]
Function simulate(TMAX, TP, XB, eB) contains two nested functions, one to
predict future values of x, and another to adjust current values of y, if needed, when
a pulse is called for. In particular, function x_future(x, y, t) estimates x at a
time t + TP in the future, returning the estimate and the sign used for the eB term that
maximizes the absolute value of the estimate—the worse case. The value returned by
function e(t) is the ground acceleration at time t. Function y_new(x, y, t) likewise
uses Eq.(14), but in this case does so to ﬁnd a new value of y that would, one hopes,
cause |x(t + TP)| ≤XB to be satisﬁed; the sign for eB must be supplied (in this case,
by the result from x_future).
As with the HEB model, the simulation (deﬁned by lines 8–13) is broken up into
a succession of subintervals, each with duration TP. Between subintervals, a pulse
may be applied that changes the value of y instantaneously. During a subinterval,
time marches from i TP to (i + 1) TP.18 Function advance(x, y, a, b), not shown,
lets the system evolve from time a to time b, starting from the initial values x(a) and
y(a); it returns their values at time b: x(b), y(b), and b. As a side effect, it builds up
collections of data for plotting time histories of x and y.
With respect to numerical integration, advance solves the system of ﬁrst order
differential equations:
Dx = y
(20)
Dy = −2ζωn y −ω2
n x −e(t)
(21)
18The form for i in range(n) is idiomatic Python for bounded iteration from 0 to n −1 (inclusive).

188
R. Banach and J. Baugh
(a) uncontrolled: peak relative displacement ˜x = 1.51 cm at t = 12.5 s
(b) controlled at XB = 80% of peak relative displacement
Fig. 16 Response to harmonic ground motion (Tn = 2s, ζ = 1%, Z = 1, Ω = 0.37rad/s)
introduced in Eq. (3) and subsequently redeﬁned in LHS (4) in terms of the variables:
ζ, the viscous damping factor (dimensionless fraction of critical damping); and ωn,
the undamped circular natural frequency (in units of radians per second). It does so
using odeint, a SciPy function based on the Fortran LSODA routine from the
ODEPACK library, which uses an Adams predictor-corrector method (when non-stiff
problems like ours are encountered). The routine determines step size automatically
to ensure that error bounds are satisﬁed.
Harmonic ground motion. To illustrate the approach, we begin with a simple
example after Prucz et al. [29] of an SDOF system, like that of Fig.3, with a natural
frequency ωn = π rad/s and viscous damping factor ζ = 1%. It is subjected to har-
monic ground motion z = Z sin Ωt, which has the effect of adding a reversed inertia
force −m D2z to the system, with the ground acceleration given by:
e(t) ≡D2z = −Ω2 Z sin Ωt
(22)

A Simple Hybrid Event-B Model of an Active Control System …
189
where amplitude Z = 1 and frequency Ω = 0.37 rad/s are given. Thus, the case is
one in which the ground motion frequency is lower than the system natural frequency
(i.e., Ω < ωn). The system begins at rest, so x0 = y0 = 0.
Time histories of the uncontrolled response are shown in Fig.16a, where the
dimensionally comparable quantities ˜x ≡x and ˜y ≡y/ωn are plotted. The peak
responses are:
˜x(12.5 s) = −0.0151, ˜y(0.985 s) = 0.00315
The predominant response of ˜x(t) is a harmonic having the same frequency as that of
the ground acceleration; its period is 2 π/Ω, or in this case about 17s. As expected,
when Ω ≪ωn there is little relative motion between the mass and the ground, and
the motions are in phase: they reach their peaks at the same time. Superimposed
‘wiggles’ are (dying) transients induced at the natural frequency of the system, whose
undamped natural period Tn = 2s.
Pulsecontrolcannowbeemployedtolimittheresponseto80%ofthepeakrelative
displacement,whichisdonebysettingXB = 0.0121.Continuingtobeconsistentwith
Prucz et al., we set the pulse interval to be on the order of one fourth the natural period,
so TP = Tn/4 = 0.5s. Speciﬁc to our approach, the additional ground acceleration
variability term eB is set to zero for the moment. Time histories of the controlled
response are shown in Fig.16b, where the pulse trains (in red) have a ‘shape’ that acts
to counterbalance relative displacements, where needed, that would have occurred,
so as to keep them roughly within desired limits.
Though the pulse interval here is about ﬁve times larger than we would anticipate
using in practice, the example motivates the deﬁnition of a metric, the exceedance
level, that can be used to assess the algorithm’s effectiveness as a bounded state
control strategy. To quantify the exceedance level, we consider what happens at the
endpoint of a TP interval where, if |x(t)| > XB, we add |x(t)| −XB to a running sum
S, and deﬁne:
E = 103 S / n XB
(23)
where n is the number of pulse intervals included in sum S. For the Prucz example,
that gives an exceedance level E = 9.07. For pulses at 20 times per natural period
instead (i.e., for TP = 0.1s), we have E = 0.0250, and when in addition eB is raised
to 0.001, the exceedance level E drops to zero, meaning there are no exceedances.
El Centro ground motion. As noted by Prucz et al., the aim of pulse control
is to disrupt, at resonance, the ‘gradual rhythmic build-up’ of the system response.
A more realistic and challenging scenario then is to subject the system to complex
ground accelerations that include resonant frequencies, particularly ones near the
fundamental natural frequency of a building, which typically produce the largest
relative displacements and damage.
Used in the design of earthquake resistant structures, the ground accelerations
recorded in El Centro, California, during the earthquake of May 18, 1940, have a
peak value of 3.13m/s2 (0.319g), the ﬁrst 20s of which are shown in Fig.17. We
now apply them to the system. As before, the natural frequency ωn = π rad/s, so the

190
R. Banach and J. Baugh
undamped natural period Tn = 2s, a value that might correspond to the fundamental
natural period of a 20-story building. We use a viscous damping factor ζ = 5%,
which is representative of a modern ofﬁce building and is a value often used in
design. For control, the pulse interval TP = 0.1s.
Time histories of the uncontrolled response are shown in Fig.18a, where we again
plot ˜x and ˜y. The peak responses are
˜x(6.37 s) = 0.137, ˜y(11.7 s) = 0.199
which occur during a time period from about 6–13s into the event, as the system
begins oscillating near its undamped natural frequency ωn = π rad/s (with a period
Tn = 2s).
To limit the peak displacement, we apply pulse control at 80% of that value
by setting XB = 0.109 (or 10.9cm) and keep the pulse interval as before, TP =
0.1s. Time histories of the controlled response are shown in Fig.18b, where the
pulses, shown in red, effectively counterbalance relative displacements to keep them
approximately within desired bounds. Limiting displacements even further, to 50
and 40% of the peak value, is likewise shown to be effective, as demonstrated by
the time histories in Figs.18c–d, respectively. To achieve the additional level of
control requires that successively more energy be put into the system, with more and
sometimes larger pulses, and earlier into the event.
Looking at exceedance for the three levels of controlled response (80, 50, and
40%), we have E = 0.223, 0.664, and 0.561, respectively, which are reduced to zero
when the additional ground acceleration term, eB, is increased to at least 0.512,
0.358, and 0.469, respectively. Additional analysis, that might lead to ﬁnding good
eB settings a priori for anticipated ground motions, is left for future work.
Fig. 17 North-south component of the ground motion recorded at a site in El Centro, California,
during the Imperial Valley earthquake of May 18, 1940 (showing ﬁrst 20s of the event)

A Simple Hybrid Event-B Model of an Active Control System …
191
(a) uncontrolled: peak relative displacement ˜x = 13.7 cm at t = 6.37 s
(b) controlled at XB = 80% of peak relative displacement
(c) controlled at XB = 50% of peak relative displacement
(d) controlled at XB = 40% of peak relative displacement
Fig. 18 Response to El Centro ground motion (Tn = 2s, ζ = 5%, no time delay)

192
R. Banach and J. Baugh
12
Conclusions
In this paper, we started by reviewing how the initial ideas of earthquake protection
eventually crystallised into a number of distinct approaches, and we focused on the
active control approach. We also reviewed Hybrid Event-B as a suitable vehicle for
attempting a formal development of an SDOF active protection model. We then pur-
sued the development through various levels of detail, culminating in the distributed
sampled and quantized model of Sect.9. Along the way, particularly in Sects.8 and
10, we discussed the obstacles to accomplishing this with full formality.
In Sect.11, we subjected our analytically derived model to simulation using well
established numerical tools typically used in earthquake protection engineering. We
spot-tested our model both on a simple harmonic excitation, and on the El Centro
ground motion data. It was encouraging to see that our model behaved well, despite
the relatively small input from the empirical sphere during its derivation. Enhancing
the latter, can only be expected to improve matters regarding ﬁdelity with conven-
tional approaches.
The present study forms a launchpad for much possible future work. Firstly, there
is the fact that our models’ behaviour was timed with precision—in reality we will
always have stochastic variations in the times of all events. Similar considerations
apply to a better characterisation of the additional ground acceleration variability
term eB. Taking these issues into account would bring us closer to the level of detail
of [16, 20, 21].
Secondly, there is the consideration of the replication of components needed for
adequate fault tolerance. Here, at least, we can see that use of standard approaches
would address the issue, and would again bring us closer to [16, 20, 21].
Thirdly, we note that the SDOF modelling can readily be enriched to capture the
dynamics of a genuine building more accurately. The essentially scalar description
we dealt with here could be enriched to encompass a greater number of linear and
angular degrees of freedom. This again is relatively standard, at least in the linear
dynamics case.
Fourthly, there is the investigation of richer formulations of retrenchment and
reﬁnement capable of coping with the wealth of phenomena discussed in Sect.10. A
generally applicable approach here would yield many dividends for a wide class of
problems of a similar nature.
Fifthly, it is regrettable that there currently is no mechanised support for Hybrid
Event-B. Nevertheless, progress with the issue just discussed would be a prerequisite
for a meaningfully comprehensive coverage of the development route as a whole
by mechanical means, even if individual parts could be treated by conventional
mechanisation of linear and discrete reasoning. Taking all the above together, there
is plenty to pursue in future work.
One ﬁnal comment. In a recent UK terrestrial TV broadcast [25], various aspects
of the construction of Beijing’s Forbidden City were described. Not least among

A Simple Hybrid Event-B Model of an Active Control System …
193
these was the capacity of the Forbidden City’s buildings to withstand earthquakes,19
particularly considering that Beijing lies in a highly seismic region. Fundamental to
this is the use of bulky columns, which are essentially free standing, to support the
weight of the building’s heavy roof, and the use of complex dougong brackets [37, 38]
to couple the columns to the roof. The free standing construction allows the ground
under the building to slip during powerful tremors without breaking the columns,
and the relatively ﬂexible dougong brackets permit relative movement between the
columns and other members without risking structural failure. These building tech-
niques were already ancient by the time the Forbidden City was constructed early
in the 1400s. The cited broadcast showed a scaled structure on a shaking table with-
standing a simulated magnitude 10 quake. So, more recent efforts notwithstanding,
the Chinese had the problem of earthquake protection for buildings licked more than
two thousand years ago!
References
1. J. Earthq. Eng. Eng. Vib.
2. World Conferences on Earthquake Engineering
3. Abrial, J.R.: Modeling in Event-B: System and Software Engineering. Cambridge University
Press, Cambridge (2010)
4. Ahmed, N.: Dynamic Systems and Control With Applications. World Scientiﬁc, Singapore
(2006)
5. Banach, R.: Formal reﬁnement and partitioning of a fuel pump system for small aircraft in
Hybrid Event-B. In: Bonsangue D. (eds.) Proceedings of IEEE TASE-16, pp. 65–72. IEEE
(2016)
6. Banach, R.: Hemodialysis machine in Hybrid Event-B. In: Butler, S., Mashkoor B. (eds.)
Proceedings of ABZ-16. LNCS, vol. 9675, pp. 376–393. Springer (2016)
7. Banach, R.: The landing gear system in multi-machine Hybrid Event-B. Int. J. Softw. Tools
Tech. Transf. 19, 205–228 (2017)
8. Banach, R., Baugh, J.: Active earthquake control case study in Hybrid Event-B web site. http://
www.cs.man.ac.uk/~banach/some.pubs/EarthquakeProtection/
9. Banach, R., Butler, M.: A Hybrid Event-B study of lane centering. In: Aiguier, B., Krob M.
(eds.) Proceedings of CSDM-13, pp. 97–111. Springer (2013)
10. Banach, R., Butler, M.: Cruise control in Hybrid Event-B. In: Woodcock Z.L. (ed.) Proceedings
of ICTAC-13. LNCS, vol. 8049, pp. 76–93. Springer (2013)
11. Banach, R., Butler, M., Qin, S., Verma, N., Zhu, H.: Core Hybrid Event-B I: single Hybrid
Event-B machines. Sci. Comput. Program. 105, 92–123 (2015)
12. Banach, R., Butler, M., Qin, S., Zhu, H.: Core Hybrid Event-B II: multiple cooperating Hybrid
Event-B machines. Sci. Comput. Program. 139, 1–35 (2017)
13. Banach, R., Jeske, C.: Retrenchment and reﬁnement interworking: the tower theorems. Math.
Struct. Comput. Sci. 25, 135–202 (2015)
14. Banach, R., Poppleton, M., Jeske, C., Stepney, S.: Engineering and theoretical underpinnings
of retrenchment. Sci. Comput. Program. 67, 301–329 (2007)
15. Banach, R., Van Schaik, P., Verhulst, E.: Simulation and formal modelling of yaw control in a
drive-by-wire application. In: Proceedings of FedCSIS IWCPS-15, pp. 731–742 (2015)
19Being wooden, the Forbidden City’s buildings were less good at withstanding ﬁre, and several
structures have had to be rebuilt a number of times over the centuries because they had burnt down.

194
R. Banach and J. Baugh
16. Baugh, J., Elseaidy, W.: Real-time software development with formal methods. J. Comput. Civ.
Eng. 9, 73–86 (1995)
17. Buckle, I.: Passive control of structures for seismic loads. In: Proceedings of 12th World
Conference on Earthquake Engineering. Paper No. 2825 (2000)
18. Chicone, C.: Ordinary Differential Equations with Applications, 2nd edn. Springer, New York
(2006)
19. Chopra, A.: Dynamics of Structures: Theory and Applications to Earthquake Engineering, 4th
edn. Pearson, Englewood Cliffs (2015)
20. Elseaidy, W., Baugh, J., Cleaveland, R.: Veriﬁcation of an active control system using temporal
process algebra. Eng. Comput. 12, 46–61 (1996)
21. Elseaidy, W., Cleaveland, R., Baugh, J.: Modeling and verifying active structural control sys-
tems. Sci. Comput. Program. 29, 99–122 (1997)
22. Gattulli, V., Lepidi, M., Potenza, F.: Seismic protection of frame structures via semi-active
control: modelling and implementation issues. Earthq. Eng. Eng. Vib. 8, 627–645 (2009)
23. Gradshteyn, I., Ryzhik, I.: Table of Integrals Series and Products, 7th edn. Academic Press,
New York (2007)
24. Kuramoto, H., Teshigawara, M., Okuzono, T., Koshika, N., Takayama, M., Hori, T.: Predicting
the earthquake response of buildings using equivalent single degree of freedom system. In:
Proceedings of 12th World Conference on Earthquake Engineering. Auckland, New Zealand.
Paper No. 1039 (2000)
25. More4 TV: Secrets of China’s Forbidden City. UK Terrestrial TV Channel: More4. (24 July
2017)
26. Olver, F., Lozier, D., Boisvert, R., Clark, C.: NIST Handbook of Mathematical Functions.
Cambridge University Press, Cambridge (2010)
27. Platzer, A.: Logical Analysis of Hybrid Systems: Proving Theorems for Complex Dynamics.
Springer, Berlin (2010)
28. Popescu, I., Sireteanu, T., Mitu, A.: A comparative study of active and semi-active control of
building seismic response. In: Proceedings of DGDS-09, pp. 172–177. Geometry Balkan Press
(2010)
29. Prucz, Z., Soong, T., Reinhorn, A.: An analysis of pulse control for simple mechanical systems.
J. Dyn. Syst. Meas. Control. 107, 123–131 (1985)
30. Retrenchment Homepage. http://www.cs.man.ac.uk/~banach/retrenchment
31. Rose, B., Baugh, J.: Parametric study of a pulse control algorithm with time delays. Technical
report. CE-302-93, North Carolina State University Department of Civil Engineering (1993)
32. Sontag, E.: Mathematical Control Theory. Springer, New York (1998)
33. Soong, T.: Active Structural Control: Theory and Practice. Longman, Harlow (1990)
34. Soong, T., Chu, S., Reinhorn, A.: Active, Hybrid and Semi-Active Control: A Design and
Implementation Handbook. Wiley, New York (2005)
35. Tabuada, P.: Veriﬁcation and Control of Hybrid Systems: A Symbolic Approach. Springer, US
(2009)
36. Walter, W.: Ordinary Differential Equations. Springer, Berlin (1998)
37. Wikipedia: Chinese architecture
38. Wikipedia: Dougong
39. Wikipedia: Duhamel’s integral
40. Wikipedia: Earthquake engineering

Understanding, Explaining, and Deriving
Reﬁnement
Eerke Boiten and John Derrick
Abstract Much of what drove us in over twenty years of research in reﬁnement,
starting with Z in particular, was the desire to understand where reﬁnement rules
came from. The relational model of reﬁnement provided a solid starting point which
allowed the derivation of Z reﬁnement rules. Not only did this explain and verify the
existing rules—more importantly, it also allowed alternative derivations for different
and generalised notions of reﬁnement. In this chapter, we brieﬂy describe the context
of our early efforts in this area and Susan Stepney’s role in this, before moving on to
the motivation and exploration of a recently developed primitive model of reﬁnement:
concrete state machines with anonymous transitions.
1
Introduction: Z Reﬁnement Theories of the Late 1990s
At the Formal Methods Europe conference at Oxford in 1996 [20], there was a
reception to celebrate the launch of Jim and Jim’s (Woodcock and Davies) book on
Understanding Z [30]. This was a fascinatingly different book on Z for those with a
ﬁrm interest in Z reﬁnement like ourselves, one as aspirational and inspirational as
the slightly earlier “Z in Practice” [5]. It contained a full derivation of the downward
simulation rules for states-and-operations speciﬁcations, with inputs and outputs,
all the way from Hoare, He and Sanders’ relational reﬁnement rules [22], with the
punny “relaxing” and “unwinding” important steps of the derivation process. In
addition, unlike most Z textbooks, it also included upward simulation rules to achieve
completeness—we were told that these had turned out to be necessary in an exciting
but mostly conﬁdential industry project called “Mondex” [29]. There was also a
E. Boiten
De Montfort University, Leicester LE1 9BH, UK
e-mail: eerke.boiten@dmu.ac.uk
J. Derrick (B)
University of Shefﬁeld, Shefﬁeld S1 4DP, UK
e-mail: J.Derrick@shefﬁeld.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_8
195

196
E. Boiten and J. Derrick
strong hint then that the Mondex team couldn’t tell us yet about everything they had
discovered about reﬁnement while doing this research.
At that same conference, we presented the most theoretical Z reﬁnement paper we
had produced so far [8], which constructed a common reﬁnement of two Z speciﬁca-
tions, in support of our work on viewpoint reﬁnement, extending ideas of Ainsworth,
Cruickshank, Wallis and Groves [3] to also cover data reﬁnement. To satisfy our fun-
ders EPSRC that we were being practical and building prototype tools, we had also
implemented [6] this construction in the Generic version of the Z Formaliser tool
[19]. This tool was being developed concurrently by Susan Stepney, who provided
us with advice and debugging, and we all found out more about Smalltalk in the
process.
Our viewpoint uniﬁcation technique, as we called it, gradually relaxed the con-
straints on different speciﬁcations of the same Z operation that we needed a common
reﬁnement of, to constructively show consistency, and continue from there. If the
postconditions were different but the preconditions identical, conjunction of oper-
ations was sufﬁcient. For where the postconditions differed, a form of disjunction
delivered a common reﬁnement. If the state spaces were different, we could use a
“correspondence relation” to still ﬁnd a common data reﬁnement. But that is where
our desire to allow viewpoints to take different perspectives hit the buffers as far as
conventional Z reﬁnement went. In particular, two viewpoint operations with differ-
ent inputs or outputs could never have a common reﬁnement according to the theory
as presented in [30] or Spivey’s earlier Z bible [27]. Which was odd, as both of these
books already contained “reﬁnement” examples that added inputs or outputs—not
least Spivey’s“birthday book” running example.
Based on all this, we set out to reconstruct a sensible theory for how reﬁnement
in Z might also include changes to inputs and outputs. The examples in the noted
textbooksformedastartingpointforconservativegeneralisationofthestandardrules.
We extracted some of the informal and common sense rationales, which can be found
in the paper “IO Reﬁnement in Z” [7]. The ﬁnal version of this paper, inspired by
the derivations in [30], reverted from common sense reasoning to solid mathematics
to establish the rules for IO reﬁnement. The steps in the Woodcock and Davies
derivation of Z reﬁnement from relational reﬁnement where concrete (“global”) input
and output sequences were equated to their abstract (“local”) counterparts were ripe
for generalisation. So, initialisation included potential transformation of inputs, and
ﬁnalisation transformation of outputs—with constraints, such as avoiding the loss of
information in outputs.1
Almost in parallel, the additional output from the Mondex project that had been
hinted at appeared. “More Powerful Data Reﬁnement in Z” by Stepney, Cooper and
Woodcock [28] was presented at the yearly Z conference. Its central observation
was that in the derivation of Z reﬁnement from relational rules, the ﬁnalisation
was a powerful instrument for generalisation. In the standard approach, it would
throw away the abstract state and any remaining inputs, and directly copy the output
1Our little joke was to call this the “every sperm is sacred” principle, in reference to Monty Python.

Understanding, Explaining, and Deriving Reﬁnement
197
sequence across to the global state. Changing the type of outputs was one obvious
generalisation, and had indeed been necessary in the Mondex case study.
In our work on viewpoint speciﬁcation and consistency, it had been clear from
the beginning that we would need to be looking at reconciling behaviour-centred and
state-centred speciﬁcations, as both of these were expected to be used in the Open
Distributed Processing reference model [11]. We explored the cross-over between
the world of Z and the world of process algebras in a variety of ways: translating
LOTOStoZ[14],comparingtherespectivereﬁnementrelations[18],integratingCSP
and Object-Z [26], and adding process algebra features such as internal operations
to Z [17]. However, neither of these felt like the deﬁnitive solution or provided a
comprehensive, let alone complete, reﬁnement basis—until this thread of research
was also infected by the derivation concept. Using relational data types, and their
ﬁnalisations as making the correct observations (often: refusals) was a critical step
forward,representedinaseriesofpapersderivingconcurrentreﬁnementrelationsand
simulation rules to verify them from relational characterisations, under the heading
of “relational concurrent reﬁnement” [12, 15].
2
Concrete State Machines with Anonymous Transitions
Our ﬁrst book on reﬁnement [16] continued from the work on generalising reﬁnement
that we had done to support viewpoint speciﬁcation. It grew almost like a bunch
of ﬂowers, with nearly a new generalisation per chapter, plus an extra chapter of
unopened buds, “Further Generalisations” that we had envisaged but did not develop
in detail or with examples. Relational concurrent reﬁnement gets a brief mention
in the second (2014) edition of the book, as our preferred method of integrating
state-focused and behaviour-focused methods.
We recently completed our second book on reﬁnement [13], in which we take a
rather different approach. We again conclude with relational concurrent reﬁnement,
but this time from a more inclusive perspective. We aimed to provide a comprehensive
story of different reﬁnement relations, mostly not of our own construction, and how
they are related and reﬂected in existing formal methods and languages. In relating
different reﬁnement notions, of course we considered generalisation hierarchies as
established by van Glabbeek [21] and Leduc [24], but also the more conceptual
relationships between them. In that dimension, it almost becomes a genealogy of
reﬁnement relations.
The ﬁrst regular chapter in the new book [13] covers labeled transition systems
as the obvious basic model for behavioural formalisms. There are states (including
initial ones), and transitions between states, labelled with actions from some alpha-
bet. Observations (traces, refusals, etc.) are in terms of these actions, and the states
themselves contain no information beyond the behaviour from that point on.
When later in [13] we get to the basic relational reﬁnement model that has been
central to our work for the last twenty years, there is relevance both to states and to
actions. Observations are deﬁned via ﬁnalisation of the states at the end of a trace;

198
E. Boiten and J. Derrick
reﬁnement is inclusion of such observations, universally qualiﬁed over all traces,
where a trace consists of a sequence of actions. So these are abstract state machines
(as ﬁnalisation modulates the state observations) with visible transitions—labelled
with an action for every transition step.
Clearly that is a few steps away from the labeled transition model. How do we
naturally get to that point? Looking ahead to explaining reﬁnement in formalisms
such as Event-B and ASM, how do we justify that these methods do not seem to
care as much about the labels on transitions as Z (or labeled transition systems, for
that matter) does? Will a deeper understanding of this improve our coverage of what
“stuttering steps” and “reﬁning skip” really mean?
We decided this called for a basic system model that is in some sense dual to
labeled transition systems. Namely, we wanted a model in which the observations
are based on states, and transitions do occur but have no individual meaning or
observability other than through the effect they have on the state. So from a change
of state we can draw the conclusion that “something must have happened” but no
more than that, and in particular we also cannot assume the converse, that nothing
can have happened if the state is unchanged between two observations.
Has such a model been described previously? It comes close to an abstract view
of sequential programs, with possibilities for observation only crystal clear once
the program has terminated. There are some candidates of formal methods in the
literature which take related views, but they are all a bit more concrete than we
would like in terms of the state spaces they assume. Action systems [4] have a
rather concrete view of the state space, as being made up of variables, modiﬁed by
assignments. The reﬁnement theories of Abadi and Lamport [1] also have anonymous
transitions, including stuttering ones, on a state space made up of variables. Hoare and
He’s Unifying Theories of Programming [23] (UTP) in their basic form come close
to what we were looking for, also on state spaces made up of variables, although
the better known variants of UTP are the ones with auxiliary variables encoding
behaviour.
The model we deﬁned has states, initial states, and a transition relation that only
records that some states occur before some other states. We call it CSMAT: Concrete
State Machine with Anonymous Transitions.
Deﬁnition 1 (CSMAT) A CSMAT is a tuple (State, Init, T ) where State is a non-
empty set of states, Init ⊆State is the set of initial states, and T ⊆State × State
is a reﬂexive and transitive transition relation.
We write p −→T q for (p, q) ∈T , leaving out T when it is clear from the context.
This is in close analogy with −→in LTSs, and at the same time also with =⇒because
T is reﬂexive and transitive, and thus equal to T ∗.
In [13] we explain some, but not all, of the “design decisions” of this deﬁnition, and
even then not in great detail. The intended contribution of this article is to highlight
and explore these. Given that this is an artiﬁcial intermediate station in the theory
development, all these decisions are up for discussion. Their best defence is if they
provide some additional insight into reﬁnement, or illuminate and foreshadow issues
cropping up later in the theory development.

Understanding, Explaining, and Deriving Reﬁnement
199
State machine:
This is justiﬁable already as we have states and transitions. A
restriction to ﬁnite states seems unnecessary here. Expressiveness matters, but
will always be secondary in a basic model where anything complex will look
clunky anyway; computability or Turing-completeness of the model is not an
important concern. Some of the more eccentric problems in reﬁnement, around
inﬁnite traces and possible unsoundness of upward simulation, disappear if our
model does not allow for inﬁnite branching.
We have not imagined models so abstract that they do not in some way contain
that-what-is and that-what-happens, especially not when that-what-is is poten-
tially represented by the possible futures, i.e. that-what-may-still-happen. Most
machine models in theoretical computer science (Turing machines, stacks, regis-
ters, evaluation models for lambda calculus) are state machines with a particular
structure of state anyway.
Non-empty set of states:
This is a somewhat arbitrary choice—the trivial CSMAT-
like structure with no states and hence no initial states or transitions is excluded.
However …
Initial states:
We do not insist on the set of initial states being non-empty or even
just a singleton. Allowing the empty set means we have a large collection of trivial
state machines that would behave very interestingly if it wasn’t for the fact they
could never start; however, for a given transition relation, set inclusion on initial
states might induce some lattice-like structure, and retaining an extremal element
in that may be useful.
Wehadinitiallynotbeensureaboutallowingmultipleinitialstatesinthepreceding
chapter on LTSs. It seemed an unnecessary restriction to insist on a single initial
state, but then we found that not doing so meant we needed to talk about internal
versus external choice earlier than we wished to, and in a non-orthogonal way:
LTSs with multiple initial states can be viewed as modelling the possibility of
internal choice in initial states only. For CSMATs, the decision was forced towards
multiple initial states by wanting a non-trivial notion of a state that could or would
(not) lead to termination—effectively introducing external choice at initialisation
only, which makes more sense for CSMATs than for LTSs, as we will explain
below.
Non-determinism:
Aswellascominginviamultipleinitialstates,non-determinism
is implicitly present when transitions are characterised by a relation. Our excuse
is that we want to use this model for reﬁnement—if descriptions are deterministic,
there is nothing left to reﬁne.2
Concrete:
We call this a concrete state machine but after this single deﬁnition
that remains entirely a statement of intent. Comparing the deﬁnition to that of
2One of our most enlightening paper rejections was one for a 1990s ZUM conference, where we had
argued the opposite, namely that data reﬁnement could introduce non-determinism, but a reviewer
explained how this was entirely illusory, as such non-determinism could never be made visible in
external observations. Of course this holds particularly for formal methods like Z where the ﬁnal
reﬁnement outcome is only beholden to the initial speciﬁcation and not to any detail introduced
along the way like it is in for example Event-B [2], where reﬁnement of deterministic systems can
indeed be entirely meaningful.

200
E. Boiten and J. Derrick
labeled transition systems, we have merely removed information that might be
observed: the labels on transitions. That abstraction by itself does not make the
model concrete, of course.
The real contrast is with abstract state machines, as in the standard relational
reﬁnement theory, where the state is not directly observable—we signpost here
that it will be the states themselves that will occur in observations, and deﬁnitions
of observations of CSMATs will be seen to comply with that.
Anonymous:
Transitions are anonymous, omitting the labels that are included in
the LTS transition relation. As a consequence, virtually all of the notions of obser-
vation that LTSs provide and the reﬁnement relations that are based on such
observations become trivial in this model. At a deeper level this means that we
should not look at this as a reactive model: we have removed the handle for the
environment to be interacting with the system. This makes it a model of passive
observation instead.
Transitive:
Our reasoning for making the transition relation transitive is the thought
that if we cannot observe transitions, this implies that we also cannot count tran-
sitions as individual steps. This is deﬁnitely a design choice where we could have
gone the other way. Turing machines, for example, are not normally viewed as
labeling their steps; but the associated theory of time complexity relies on being
able to count them. A bit later in the theory development, the decision on transitiv-
ity will prove to have an adverse effect: it will not be preserved under abstraction
functions.
If we are going to be looking at action reﬁnement later, or at m-to-n simulation
diagrams in ASM [25] where the labels do not matter so much, as we do later
in [13], it is convenient to be able to move between looking at a single step and
multiple steps.
The main justiﬁcation for transitivity is closely tied to reﬂexivity. If we observe a
system in a way that is (unlike Turing machine time complexity) not synchronised
with the system’s internal evolution, or maybe even in a continuous time model,
there may be consecutive observations of the same state value. A transitive and
reﬂexive transition relation between such observations allows us to not distinguish
between the three different cases of this—which has to be the more abstract view.
Reﬂexive:
These three cases are:
• nothing has happened;
• something has happened, but it is not visible at the level of abstraction we are
observing the system at;
• multiple state changes have happened, returning us to the initial state.
Look at this as different versions of someone at a trafﬁc light. The light was red,
they blinked, and when they opened their eyes again it was red. The state of the
trafﬁc lights might be identical; their light might have remained red but the other
ﬂows of trafﬁc might have changed in the meantime; or they might have blinked
long enough for their light to go through an entire cycle.
Implicitly the third case will be noticeable in the transition relation anyway, as it
also records what we would have seen if we had opened our eyes a little earlier.
The ﬁrst two really do not need to be distinguished. “Nothing happens” is often

Understanding, Explaining, and Deriving Reﬁnement
201
an abstraction anyway, for example the empty statement in a busy-waiting loop
in a program is an abstraction of passing time.
The ﬁrst two cases might be called “stuttering steps”. The second case in partic-
ular relates to“reﬁning skip”, which serves a variety of roles in different formal
methods, sometimes causing signiﬁcant problems. We have analysed this previ-
ously [9, 10] and called the second case a “perspicuous” operation. At the more
abstract level, the operation has no visible effect; but a reﬁnement might provide
some behaviour at a greater level of detail.
The book contains a separate chapter on perspicuous operations and whether and
how they relate to internal operations and the consequences this has for reﬁne-
ment. It is also the place where we deal with livelock or divergence: the idea that
nothing visible or externally controllable happens inﬁnitely often.
Reﬂexivity of the transition relation has an important side effect: it means that the
transition relation is total, i.e. from every state there is a possible “transition”, if
only to that state itself. So if we wanted to deﬁne a notion of a computation that
stops for some (positive or negative) reason, we could not do that by ﬁnding states
where T fails to deﬁne a next state.
No ﬁnal states:
Finite state machines (the ones that accept regular languages) have
ﬁnal or accepting states. It would be possible to add those to CSMATs, but then
observations would have to respect that, at some cost of complexity of description.
Looking forward to relational data types in later chapters having ﬁnalisations
which are (typically) applicable in every state, we decided against it here.
Having explained our decisions in deﬁning CSMATs, we now brieﬂy consider the
possible notions of observation that go with it and form the bases for reﬁnement on
CSMATs.
The most elementary of these simply characterises the states that are reachable in
M, starting from a state in Init and following T .
Deﬁnition 2 (CSMAT observations) For a CSMAT M = (State, Init, T ) its obser-
vations are a set of states deﬁned by
O(M) = {s : State|∃init : Init • (init, s) ∈T }
The standard method of deriving a reﬁnement relation when the semantics generates
sets is set inclusion:
Deﬁnition 3 (CSMAT safety reﬁnement) For CSMATs C = (S, C I, CT ) and A =
(S, AI, AT ), C is a safety reﬁnement of A, denoted A ⊑S C, iff O(C) ⊆O(A).
This is called “safety reﬁnement” because the concrete system cannot end up
in states that the abstract system disallows. In common with other safety-oriented
reﬁnement relations, doing nothing is always safe, so if either C I or CT is empty
then so is O(C) and hence (S, C I, CT ) reﬁnes any CSMAT on the same state space
S. Comparing only CSMATs on the same state space is based on a form of “type
correctness”, as the state doubles up (“concrete”!) as the space of observations.

202
E. Boiten and J. Derrick
Given reﬂexivity and totality of T , the best method we have come up with for
characterising “termination” is the absence of non-trivial behaviour, so a terminating
state is one which only allows stuttering, i.e. it is linked by T only to itself.
Deﬁnition 4 (CSMAT terminating states and observations) The terminating states
and terminating observations of a CSMAT M = (S, Init, T ) are deﬁned by
term(M) = {s ∈S|∀t ∈S • (s, t) ∈T ⇒s = t}
OT (M) = O(M) ∩term(M)
Set inclusion on these observations we have called “partial correctness” as it is
very close to that traditional correctness relation for programs: if the computation
terminates, it delivers the correct results; and when it does not, we impose no con-
straints.
Deﬁnition 5 (CSMAT partial correctness reﬁnement)
For CSMATs C = (S, C I, CT ) and A = (S, AI, AT ), C is a partial correctness
reﬁnement of A, denoted A ⊑PC C, iff OT (C) ⊆OT (A).
To get a deﬁnition of “total correctness”, we would normally have to add that the
concrete computation is only allowed to not terminate whenever that is also allowed
by the abstract one.
The concrete computation not (ever) terminating is characterised by OT (C) = ∅,
and the same for the abstract computation is then OT (A) = ∅. The former condition
should then imply the latter. But this is very much an all-or-nothing interpretation of
termination, that does relate closely to ideas of termination and reﬁnement in action
systems and Event-B.
The word “whenever” in the informal description implies a quantiﬁcation of some
kind. If we take that over the entire (shared) state space, i.e. that a state must be a
terminating one in the concrete system whenever the same state is terminating in
the abstract system, this forces equality between the sets of terminating states in the
reﬁnement deﬁnition, so is not very useful. A different way of looking at it is that
“whenever” implies a quantiﬁcation over all initial states—and this invites a different
view of what the different initial states represent.
Our deﬁnition of observations above only considers whether states (or terminating
states) can be reached from some initial state. This implies that, when there are
multiple initial states, we do not know or we do not care in which of these the
computation started. Effectively, the CSMAT starts its operation by a (internal) non-
deterministic choice of one of the possible initial states. We could also makes this
an external choice: so different initial states represent a variable input to the system.
This would make observations a relation between the initial state chosen and the ﬁnal
state observed—in other words, we get relational observations.
Thus, we can deﬁne relational observations that connect an initial state to another
(ﬁnal) state as follows.

Understanding, Explaining, and Deriving Reﬁnement
203
Deﬁnition 6 (Relational observations of a CSMAT) For a CSMAT M = (State,
Init, T ) its relational observations and terminating relational observations are rela-
tions deﬁned as
R(M) = (Init × State) ∩T
RT (M) = (Init × term(M)) ∩T
Analogous reﬁnement relations can be deﬁned using these observations.
Deﬁnition 7 (Relational reﬁnements for CSMATs) For CSMATs C and A,
• C is a (relational) trace reﬁnement of A, denoted A ⊑RT C, iff R(C) ⊆R(A);
• C is a relational partial correctness reﬁnement of A, denoted A ⊑R C, iff RT (C) ⊆
RT (A).
We can now extend partial correctness meaningfully to total correctness, see [13]
for a calculation justifying the additional condition.
Deﬁnition 8 (Total correctness reﬁnement for CSMATs) For CSMATs C and A, C
is a total correctness reﬁnement of A, denoted A ⊑R C, iff RT (C) ⊆RT (A) and
domRT (A) ⊆domRT (C).
Although we have now deﬁned relational observations, we cannot yet use simu-
lations to verify them. This is due to the state values being directly observable, i.e.
we could only ever link fully identical states in a simulation anyway.
At this point we felt we had explored the space of meaningful reﬁnement on
CSMATs. ([13] also contains also a state trace variant of the semantics.) What are
the baby steps that take us towards abstract data types?
First, observations restricted us to considering the same state space between con-
crete and abstract systems. Echoing our earlier work on output reﬁnement, if we
allowed ourselves to transform output types “at the edge of the system”, we could
relax that restriction. So what properties should such a transformation have? It should
certainly apply to every possible “concrete” state, as otherwise we would have con-
crete observations that had no abstract counterparts. For a given concrete observa-
tion, we should also be able to reconstruct the corresponding abstract observation
uniquely—this is the same “no information loss” principle for output transforma-
tions. As our observations are states, this together implies that the transformation is
a total function from concrete to abstract states. Reassuringly, these tend to crop up
in the most elementary deﬁnitions of simulations as well, for example in automata.
Can we deﬁne simulations between CSMATs on this basis? As it turns out, not
quite—more on this below. Instead, we deﬁne the application of such a “state abstrac-
tion” on a CSMAT.
Deﬁnition 9 (State abstraction on CSMATs) Given a (total) function f : S →S′,
the state abstraction of a CSMAT M = (S, Init, T ) under f is the state machine
f (M) = (S′, Init′, T ′) deﬁned by

204
E. Boiten and J. Derrick
Init′ = { f (s)|s ∈Init}
T ′ = {( f (s), f (s′))|(s, s′) ∈T }
Here is where we might regret an earlier design decision, namely transitivity. The
state abstraction image is not necessarily a CSMAT, as its transition relation may
not be transitive when the function is not injective. If states s1 and s2 have the same
image under f , a path ending in s1 may join up with a path beginning in s2, creating
a connection that may not have existed in the original CSMAT.
Injectivity of the abstraction function is not the correct ﬁx for this. Elementarily, it
would not establish an abstraction but merely a renaming, an isomorphism. In looking
at the effect of an abstraction function on reﬂexivity we see why non-injectivity
may actually be required. State abstraction does preserve reﬂexivity of the transition
relation. Moreover, it can introduce stuttering steps in the abstraction that were actual
changes of state in the original: when both the before state and the after state of a step
are abstracted to the same state. This links a non-change in the abstracted machine
to a change in the concrete machine, i.e. it highlights a perspicuous step as discussed
above.
The effect on “termination” is problematic again, though. A system that never
terminates, moving between its two states, can be abstracted to a one state system that
by our deﬁnition of termination (no transitions except to itself) always terminates.
Abstract systems are typically expected to terminate less often, rather than more
often, than concrete ones in reﬁnement relations. Similarly, by collapsing parts of
the state space to a single point, unbounded concrete behaviour (possibly interpreted
as divergence) can be collapsed to stuttering in the abstract model. Again, we would
expect concrete systems to have less rather than more divergence.
A next step from considering abstraction functions in general is to look at the
structure of the state space, and deﬁne speciﬁc abstraction functions from it. For
example, if the state space is made up of the values of a ﬁxed collection of named
variables, projection onto the set of observable (global) variables is a meaningful
abstraction function in the sense described above.
In [13], this particular reﬁnement model has turned out to be illuminating when
thinking about divergence, about internal operations and perspicuous operations—as
well as when looking at reﬁnement in notations (such as ASM, B, and Event-B) that
do not fully conform to the abstract relational datatype model as used in Z.
Success in this undertaking at the most abstract theoretical level was not quite
achieved. Ideally, we would have found a model M such that the abstract relational
model is in some sense the minimal common generalisation of M and labeled transi-
tion systems. Maybe those models are just too subtly different. Time will tell. What
we do know is that the theory of reﬁnement in state-based languages is a lot richer
than ﬁrst appeared in the 1990s. Susan and colleagues’ work in the late 90s initiated
a line of thinking that has developed the theory and practice in a number of ways
that were probably not foreseen when the ﬁrst generalisations appeared.

Understanding, Explaining, and Deriving Reﬁnement
205
References
1. Abadi, M., Lamport, L.: The existence of reﬁnement mappings. Theor. Comput. Sci. 2(82),
253–284 (1991)
2. Abrial, J.R.: Modelling in Event-B. CUP, Cambridge (2010)
3. Ainsworth, M., Cruickshank, A.H., Wallis, P.J.L., Groves, L.J.: Viewpoint speciﬁcation and Z.
Inf. Softw. Technol. 36(1), 43–51 (1994)
4. Back, R.J.R., Kurki-Suonio, R.: Distributed cooperation with action systems. ACM Trans.
Program. Lang. Syst. 10(4), 513–554 (1988)
5. Barden, R., Stepney, S., Cooper, D.: Z in Practice. BCS Practitioner Series. Prentice Hall, New
York (1994)
6. Boiten, E.: Z uniﬁcation tools in generic formaliser. Technical report 10-97, Computing Lab-
oratory, University of Kent at Canterbury (1997)
7. Boiten, E., Derrick, J.: IO-reﬁnement in Z. In: Evans, A., Duke, D., Clark T. (eds.) 3rd BCS-
FACS Northern Formal Methods Workshop. Springer (1998). https://ewic.bcs.org/content/
ConWebDoc/4354
8. Boiten, E., Derrick, J., Bowman, H., Steen, M.: Consistency and reﬁnement for partial speci-
ﬁcation in Z. In: Gaudel and Woodcock [20], pp. 287–306
9. Boiten, E.A.: Perspicuity and granularity in reﬁnement. In: Proceedings 15th International
Reﬁnement Workshop, EPTCS, vol. 55, pp. 155–165 (2011)
10. Boiten, E.A.: Introducing extra operations in reﬁnement. Form. Asp. Comput. 26(2), 305–317
(2014)
11. Boiten, E.A., Derrick, J.: From ODP viewpoint consistency to integrated formal methods.
Comput. Stand. Interfaces 35(3), 269–276 (2013). https://doi.org/10.1016/j.csi.2011.10.015
12. Boiten, E.A., Derrick, J., Schellhorn, G.: Relational concurrent reﬁnement II: internal oper-
ations and outputs. Form. Asp. Comput. 21(1–2), 65–102 (2009). http://www.cs.kent.ac.uk/
pubs/2007/2633
13. Derrick, J., Boiten, E.: Reﬁnement – Semantics, Languages and Applications. Springer, Berlin
(2018)
14. Derrick, J., Boiten, E., Bowman, H., Steen, M.: Viewpoints and consistency: translating LOTOS
to Object-Z. Comput. Stand. Interfaces 21, 251–272 (1999)
15. Derrick, J., Boiten, E.A.: Relational concurrent reﬁnement. Form. Asp. Comput. 15(1), 182–
214 (2003)
16. Derrick, J., Boiten, E.A.: Reﬁnement in Z and Object-Z, 2nd edn. Springer, London (2014).
https://doi.org/10.1007/978-1-4471-0257-1
17. Derrick, J., Boiten, E.A., Bowman, H., Steen, M.W.A.: Specifying and reﬁning internal oper-
ations in Z. Form. Asp. Comput. 10, 125–159 (1998)
18. Derrick, J., Bowman, H., Boiten, E., Steen, M.: Comparing LOTOS and Z reﬁnement relations.
In: FORTE/PSTV’96, pp. 501–516. Chapman & Hall, Kaiserslautern (1996)
19. Flynn, M., Hoverd, T., Brazier, D.: Formaliser – an interactive support tool for Z. In: Nicholls
J.E. (ed.) Z User Workshop, pp. 128–141. Springer, London (1990)
20. Gaudel, M.C., Woodcock, J.C.P. (eds.): FME’96: Industrial Beneﬁt of Formal Methods, Third
International Symposium of Formal Methods Europe. Lecture Notes in Computer Science, vol.
1051. Springer (1996)
21. van Glabbeek, R.J.: The linear time - branching time spectrum I. The semantics of concrete
sequential processes. In: Bergstra, J., Ponse, A., Smolka S. (eds.) Handbook of Process Algebra,
pp. 3–99. North-Holland (2001)
22. He, J., Hoare, C.A.R., Sanders, J.W.: Data reﬁnement reﬁned. In: Robinet, B., Wilhelm R. (eds.)
Proceedings of ESOP 86, Lecture Notes in Computer Science, vol. 213, pp. 187–196. Springer,
Berlin (1986)
23. Hoare, C.A.R., He, J.: Unifying Theories of Programming. Prentice Hall, Englewood Cliffs
(1998)
24. Leduc, G.: On the role of implementation relations in the design of distributed systems using
LOTOS. Ph.D. thesis, University of Liège, Liège, Belgium (1991)

206
E. Boiten and J. Derrick
25. Schellhorn, G.: ASM reﬁnement and generalizations of forward simulation in data reﬁnement:
a comparison. Theor. Comput. Sci. 336(2–3), 403–435 (2005). https://doi.org/10.1016/j.tcs.
2004.11.013
26. Smith, G., Derrick, J.: Speciﬁcation, reﬁnement and veriﬁcation of concurrent systems - an
integration of Object-Z and CSP. Form. Methods Syst. Des. 18, 249–284 (2001)
27. Spivey, J.M.: The Z Notation: A Reference Manual. International Series in Computer Science,
2nd edn. Prentice Hall, Upper Saddle River (1992)
28. Stepney, S., Cooper, D., Woodcock, J.: More powerful data reﬁnement in Z. In: Bowen, J.P.,
Fett, A., Hinchey M.G. (eds.) ZUM’98: The Z Formal Speciﬁcation Notation. Lecture Notes
in Computer Science, vol. 1493, pp. 284–307. Springer, Berlin (1998)
29. Woodcock, J., Stepney, S., Cooper, D., Clark, J., Jacob, J.: The certiﬁcation of the mondex
electronic purse to ITSEC level E6. Form. Asp. Comput. 20(1), 5–19 (2008). https://doi.org/
10.1007/s00165-007-0060-5
30. Woodcock, J.C.P., Davies, J.: Using Z: Speciﬁcation, Reﬁnement, and Proof. Prentice Hall,
New York (1996)

Oblique Strategies for Artiﬁcial Life
Simon Hickinbotham
Abstract This paper applies Eno and Schmidt’s Oblique Strategies to the research
paradigm of fostering major evolutionary transition in Artiﬁcial Life. The Oblique
Strategies are a creative technique for moving projects forward. Each strategy offers
a non-speciﬁc way of forming a new perspective on a project. The practitioner can
try as many strategies as needed until a new way forward is found. Eight randomly-
selected strategies were applied to the problem. Each Strategy was considered for
sufﬁcient time to either sketch out a new research direction or to reject the strategy as
inappropriate. Five of the Eight strategies provoked suggestions for research avenues.
We describe these new ideas, and reﬂect upon the use of creative methodologies in
science.
1
Introduction
Creative projects of any scale can experience ‘blocks’. When a project is blocked, it
seems impossible to move forward without making the piece worse—even starting
again seems to be pointless, since it will probably lead us back to the place we
currently ﬁnd ourselves. The original spirit which inspired the project appears to
have diminished and the remaining prospect of a useful result (any useful result)
appears to be strewn with difﬁculties.
The Oblique Strategies [4] were developed by the artist Peter Schmidt and Brian
Eno, a musician with an enviable reputation for repeated creative innovation. Eno
seems to be able to clear creative blocks quite easily, and has worked with an impres-
sive range of creative artists since the early 1970s. The Oblique Strategies are a set of
short statements, originally printed on cards similar to playing cards, which provoke
the user to thinking about their project in new ways. The user can read as few or as
many of the strategies as they like—the point is to move the viewpoint of the status
of the project to a new perspective, and thus to ﬁnd a way of removing the block.
S. Hickinbotham (B)
YCCSA, University of York, York, UK
e-mail: sjh518@york.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_9
207

208
S. Hickinbotham
OneofthefeaturesoftheObliqueStrategiesisthattheyarepurposelynon-speciﬁc,
with the goal that they can be applied to any creative project. In this essay, we’ll look
at applying the Oblique Strategies to the research ﬁeld of Artiﬁcial Life (ALife)—a
ﬁeld of research with the goal of discovering the principles of “life as it could be” [9].
This raises the question: Is ALife ‘blocked’? ALife researchers (including the author)
would deny this, but it is possible to make the argument that there are currently no
particularly strong research leads that take the ﬁeld beyond the emergence of self-
replicating entities. The initial promise of results [1, 11, 12] regarding the emergence
of self-replicators has stagnated: self-replicators suffer from parasitic attacks in these
systems and do not seem to generate new levels of complexity after an initial period
of innovation.
The (well, a) current goal of ALife concerns the search for artiﬁcial systems that
can generate new levels of complexity beyond these self-replicator systems. In short,
we seek systems that exhibit a recognisable major transition [13] in their evolution.
In addition, another particular ALife maxim needs to be observed: to create the
conditions for life to emerge without enforcing the emergence—von Neumann [14]
was aware of this and famously discusses the issue of “composing away the problem”.
Here we will use the Oblique Strategies to try and foster new thinking about how to
work towards this objective.
At this point we switch from impersonal narrative to the ﬁrst person, where ‘I’ is
the author. This is important because the Oblique Strategies work by triggering new
conﬁgurations of ideas and concepts in the mind of the user, so it is impossible to
separate user from experiment as is desirable in the scientiﬁc method. First, some
background: I am an ALife research professional/hobbyist depending on which year
you read this. My contribution to ALife research has been in the ﬁeld of Automata
Chemistries (AChems), usually under the supervision and always with the support
of Stepney. My main contribution to the ﬁeld has been the Stringmol Automata
chemistry [7], and I will be making reference to this as we go through the exercise.
2
Methods
The premise of Oblique Strategies is simple: a short text statement challenges the
user to think about the project in a new way. Over as many iterations as necessary, the
user draws a random Oblique Strategy card, considers the application of the phrase
to their project, and tries to implement the perceived New Way Forward.
There are many different packs of Oblique Strategies available. The original packs
are collectors item, but thanks to the internet we were able to use the online version
at http://www.oblicard.com/ to generate the draws.
In the context of this essay, and in an effort to evaluate the Oblique Strategies, I’ve
limited the number of cards drawn to 8. I’ve then tried to interpret each statement
in the general context of ALife, trying to use them to guide new angles of research
in ALife and the Automata Chemistries. I then try to assess the usefulness of the
new approach, and where possible to suggest how a research project might be built

Oblique Strategies for Artiﬁcial Life
209
around the new idea. I tried to do this thinking over a short period of time as this
seems to reﬂect the spirit of the approach, but the ‘wall time’ for this covered the ﬁrst
two weeks of February 2018. Where possible, we want to move towards a system
that offers the potential to demonstrate the emergence of a major transition within
the context of open-ended evolution [2].
3
Results
The cards that were drawn contained the following eight strategies in this order:
• Humanize something that is free from error
• Do we need holes?
• Give the Game away
• Is the tuning appropriate?
• Move towards the unimportant
• Use something nearby as a model
• Take away the elements in order of apparent non-importance
• Call your mother and ask her what to do.
I considered each of these phrases in turn and tried to develop a research theme
from each. Below, I detail the responses that each of these strategies provoked.
3.1
Humanize Something that is Free from Error
What does humanize mean in this context? I think the end of the sentence is the clue—
something that is free from error—and this raises the idea of imprecision in ALife
systems. The process of ‘humanizing’ is to introduce what you might call ‘charming
unpredictability’ to a deterministic procedure. This issue is particularly pertinent in
the AChems, where reactions between entities follow the coded program(s) of each
entity. Imprecision would mean that there was more than one possible outcome of a
program execution.
I have an intuition that inexact execution is really important—that information
processing in the presence of noise is the only way to make things robust to noise.
Robustness is difﬁcult to add a posteriori. In addition, I often think back to the
early implementations of Stringmol [7], which generated control ﬂow statements
between sections of the program using a version of the Smith-Waterman algorithm
that didn’t use a traceback feature. Here, the resulting program execution could be
wildly different if a single mutation caused different alignments. Many of the more
complex reactions that were observed in Stringmol were a result of these drastic
changes.
This Oblique Strategy suggests that further work should be done to increase the
stochasticity of program execution. This would mirror the way that enzymes strongly

210
S. Hickinbotham
catalyse one reaction but weakly catalyse many others. In Stringmol, a major chunk
of processing time is dedicated to calculating alignments between program regions.
Could we aim to kill two birds with one stone here—to create a ‘sloppy’ alignment
function that introduces more imprecision, as well as running faster?
3.2
Do We need holes?
ALife systems are arranged in some sort of space—a toroidal grid commonly. There
are no ‘holes’ in these grids—they are completely homogeneous. What might holes
achieve? Well, one thing they do is make it harder to reach one point from another.
This can be a useful component of a system where parasites emerge—it makes it more
difﬁcult for parasites to reach new hosts. We have seen that aspatial systems make it
too easy for parasites to swamp the system, and by introducing a spatial component,
the dominance of parasites is reduced [6]. The landscape is still uniform however—
a replicator can survive as well in one patch as in another, and the only variable
environmental factor is formed from the sequence of the neighbouring replicators or
parasites in the Moore neighbourhood of an individual.
Variability in the landscape of the system might give particular regions more
heterogeneity, and so solve the problem of what landscape size to use. The idea
would be to offer pockets of easy living and pockets of challenge. It may also be
possible reduce the overall size of the grid but preserve interesting properties using
this technique.
3.3
Give the Game Away
This phrase brings to mind fundamental ideas regarding the way ALife systems are
composed. Von Neumann warned: “By axiomatizing automata in this manner one
has thrown half the problem out the window and it may be the more important half.
One does not ask the most intriguing, exciting and important questions of why the
molecules or aggregates that in nature really occur… are the sorts of thing they are…”
[14]. It’s possible that to some extent, ALife systems have already given the game
away and we need to ﬁnd out how to get the game back. Recent work has shown that
this axiomatization of functions into symbols needn’t be a single step process. We
have seen in [3] how the mapping of genetic codons to units of function (e.g. amino
acids) can change through the bio-reﬂective architecture, where the encoding of the
translating machinery on the genome offer parallels to computational reﬂection.
Susan Stepney et al.’s work on sub-symbolic AChems [5] comes into play here.
The idea is to have a core, composable set of very small, efﬁcient operators that can
be composed into units and then manipulated as a language. Bringing bio-reﬂective
evolution into the system yields:

Oblique Strategies for Artiﬁcial Life
211
1. A ‘bare-metal’ set of operators (internal ‘structure’)
2. Compositions of the operators into functional components (‘reaction properties’)
3. A genetic Speciﬁcation-Translation mechanism via which the reaction properties
are arranged into functional machines on a larger scale.
Is it possible that ‘shaking the ﬂask’ of a system like this would allow life to
emerge? And can we build novel models of computation (or novel computers) that
exploit this model? This feels like we are on the right track to a major transition.
3.4
Is the Tuning Appropriate?
Well, no, it isn’t. Parameterisation of these systems is a huge problem. Can we
place these parameters under more direct control of the system—and reduce the
initialisation burden? One day maybe, but this feels like an Oblique Strategy that
doesn’t quite ﬁt into the current situation since we are currently unsure of the relative
merits of the parameters in these systems.
3.5
Move Towards the Unimportant
At ﬁrst I struggled to consider what unimportant feature could possibly move the
ALife paradigm forward. Eventually, I began to focus on an issue that has received
relatively little attention in the ALife community, probably due to the emphasis on
self-replicators. This is the role of translation in living systems. This issue came
to my attention via Lanier and Williams [10] who review the evidence for various
models of origins of life—‘replicator ﬁrst’, ‘metabolism ﬁrst’ and so on. They term
each model as a ‘privileged function’: an operation which, having been observed in
modern life, is supposed to be capable of functioning in isolation at life’s origin.
They argue that these privileged functions lack predictive power because they lack
the indeterminacy and plasticity of biological and chemical processes. They go on
to review the evidence with respect to ancient genes for these functions and what
they code for. They ﬁnd that there is heavy emphasis on translation functionality,
and suggest this is a new avenue for research in the origin of life.
Translation is often a neglected or un-implemented feature of ALife systems, but
we’ve shown in [3] that there are interesting properties of systems that use transla-
tion to decode a genome. Essentially, if the translating entity is itself encoded on the
genome, then mutations on the translator can drastically change the function of every
other machine in the system. Most commonly, these mutations can be disastrous, but
sometimes they change the composition of the system with no corresponding change
in the genetic record. This suggests a dynamic in evolution that is simultaneously
essential (because it tunes the relationship between the coding system and the result-
ing machinery) and difﬁcult to trace (because it leaves little or no trace in the genetic
record).

212
S. Hickinbotham
3.6
Use Something Nearby as a Model
Let’s move quickly on from this strategy as it doesn’t seem to work in the ALife
context where
⌢everything nearby is already a model!
3.7
Take Away the Elements in Order of Apparent
Non-importance
To work on this strategy, I ranked the elements of AChems by importance as follows:
1. The opcode language, which speciﬁes how each entity manages its state and the
state of the entities it contacts.
2. The interaction protocol, ﬁxed rules with specify broadly how an interaction
proceeds.
3. The spatial arrangement, what sits next to what.
4. The initial state, how the system kicks off.
This ordering is open to debate of course. I’ve put the initial state last because
all of the other elements have to be in place before the initial state is formulated.
Also, biology at any scale doesn’t really have an initial state—the initial state arises
from whatever evolves before it (with one notable exception…) The problem is how
to take away this element of course, because there are vastly more initial states that
lead nowhere compared with those that don’t, which seems to make the random
initialisation of an AChem a non-starter.
However, in this modern age of grid computing, perhaps we shouldn’t be afraid
of taking this approach. Over many thousands of runs, a randomly initialised system
might teach us something about the nature of the framework—particularly the nature
of low-probability events. The goal then would be to explore a range of different
frameworks and uncover the conﬁgurations that would best move us nearer to our
goal.
3.8
Call Your Mother and Ask Her What to Do
The important thing about this strategy is the act of explaining/inviting someone
into a community. In the act of doing this, the correct route becomes more apparent.
It’s the idea of being able to explain the problem sufﬁciently that it becomes better
crystallised in the explainer’s head. I remember Susan Stepney talking about a par-
ticularly constructive meeting in which many members were arriving late or had to
leave early. Handing the baton of discussion on to new parties was a way of marking
current progress and allows the building blocks of the project to be assembled.

Oblique Strategies for Artiﬁcial Life
213
So I called my mother.1 I explained that I was writing a paper on trying to ﬁnd a
way to get a major transition in ALife. I then had to explain what a major transition
was: “It’s a massive change in the way life organises itself, usually involving some
sort of new scale of enclosure” I mumbled. “So how are these things enclosed now?”
she asked. Which was a very good point really because there are no enclosures within
the implementations we have. Perhaps we could join this concept with the network-
instead-of-grid-arrangement discussed in Sect.3.2. We could imagine some way in
which the rate or probability of interaction between some regions is hindered—under
control of the entities at the border. A simple way to do this might be for an entity
to refuse interactions in some notional direction in the Moore neighbourhood and to
be able to inﬂuence neighbours to do the same.
4
Discussion
The scientiﬁc method is a wonderful thing when the science gives clear results and
presents an obvious research direction. In complex systems like ALife systems,
interpretation of results is often more difﬁcult, particularly where an artiﬁcial model
needs to be designed, implemented and evaluated. In short, the scientiﬁc method
requires creativity, and in this situation, it makes sense to look at how creative types
like Brian Eno manage to stay productive.
This exercise has provoked several interesting suggestions for ALife research with
the object of fostering major transitions in artiﬁcial systems in the following order:
increase the stochasticity; make the arena heterogeneous; link sub-symbolic AChems
to bio-reﬂective evolution; focus on the encoding of the translation apparatus; foster
the formation of enclosures. I would suggest that the last three research ideas in this
list are particularly strong given that they were initiated by an essentially random
process.
My personal experience of using the Oblique Strategies to explore new research
themes has at times rather felt like having a seance with myself. They seem to provide
a conduit through which one can access and consolidate ideas that have been lurking
in the back of the mind for a long time, re-igniting enthusiasm for these ideas in
the process. It reminds me of Johnson’s [8] concept of the “slow hunch”: although
society’snarrativeofthewaybreakthroughideashappenisthroughso-called“Eureka
moments”, it is clear that these ideas can take years or decades to form, often via
serendipitous coincidences mediated by a few highly-connected individuals. The
process of considering the Oblique Strategies that were drawn for this essay has led
me to think about the research of myself and others in unusual ways, and this process
of stepping back a pace or two has been (for me at least) fruitful.
1No I didn’t. That would be ludicrous.

214
S. Hickinbotham
References
1. Adami, C., Brown, C.T., Kellogg, W.: Evolutionary learning in the 2D artiﬁcial life system
Avida. Artiﬁcial Life IV, vol. 1194, pp. 377–381. The MIT Press, Cambridge (1994)
2. Banzhaf, W., Baumgaertner, B., Beslon, G., Doursat, R., Foster, J.A., McMullin, B., De Melo,
V.V., Miconi, T., Spector, L., Stepney, S., et al.: Deﬁning and simulating open-ended novelty:
requirements, guidelines, and challenges. Theory Biosci. 135(3), 131–161 (2016)
3. Clark, E.B., Hickinbotham, S.J., Stepney, S.: Semantic closure demonstrated by the evolution of
a universal constructor architecture in an artiﬁcial chemistry. J. R. Soc. Interface 14, 20161033
(2017)
4. Eno, B., Schmidt, P.: Oblique strategies. Opal, London (1978)
5. Faulkner, P., Krastev, M., Sebald, A., Stepney, S.: Sub-symbolic artiﬁcial chemistries. In:
Inspired by Nature, pp. 287–322. Springer (2018)
6. Hickinbotham, S., Hogeweg, P.: Evolution towards extinction in replicase models: inevitable
unless. In: 2nd EvoEvo Workshop, Amsterdam (NL), September 2016, 2016
7. Hickinbotham, S.J., Clark, E., Stepney, S., Clarke, T., Nellis, A., Pay, M., Young, P.: Diversity
fromamonoculture-effectsofmutation-on-copyinastring-basedartiﬁcialchemistry.In:ALife,
pp. 24–31 (2010)
8. Kelly, K., Johnson, S.: Where ideas come from. Wired 18(10) (2010)
9. Langton, C.G., et al.: Artiﬁcial Life (1989)
10. Lanier, K.A., Williams, L.D.: The origin of life: models and data. J. Mol. Evol. 84(2–3), 85–92
(2017)
11. Pargellis, A.: Self-organizing genetic codes and the emergence of digital life. Complexity 8(4),
69–78 (2003)
12. Ray, T.S.: An approach to the synthesis of life. In: Langton, C., Taylor, C., Farmer, J.D.,
Rasmussen, S. (eds.) Artiﬁcial Life II. Santa Fe Institute Studies in the Science of Complexity,
vol. XI, pp. 371–408. Addison-Wesley, Redwood City (1991)
13. Smith, J.M., Szathmary, E.: The Major Transitions in Evolution. Oxford University Press,
Oxford (1997)
14. Von Neumann, J., Burks, A.W.: Theory of Self-reproducing Automata. University of Illinois
Press, Urbana (1996)

Compositional Assume-Guarantee
Reasoning of Control Law Diagrams
Using UTP
Kangfeng Ye, Simon Foster and Jim Woodcock
Abstract Simulink is widely accepted in industry for model-based designs. Veriﬁ-
cation of Simulink diagrams against contracts or implementations has attracted the
attention of many researchers. We present a compositional assume-guarantee reason-
ing framework to provide a purely relational mathematical semantics for discrete-
time Simulink diagrams, and then to verify the diagrams against the contracts in the
same semantics in UTP. We deﬁne semantics for individual blocks and composition
operators, and develop a set of calculation laws (based on the equational theory) to
facilitate automated proof. An industrial safety-critical model is veriﬁed using our
approach. Furthermore, all these deﬁnitions, laws, and veriﬁcation of the case study
are mechanised in Isabelle/UTP, an implementation of UTP in Isabelle/HOL.
1
Introduction
Simulink [26] and OpenModelica [30] are widely used industrial languages and tool-
sets for expressing control laws diagrammatically, including support for simulation
and code generation. In particular, Simulink is a de facto standard in many areas
in industry. For example, in the automotive industry, General Motors, Jaguar Land
Rover (JLR), Volkswagen, Daimler, Toyota, and Nissan all use Simulink and State-
Flow for system-level modelling, electronics and software design and implemen-
tation, powertrain calibration and testing, and vehicle analysis and validation [36].
Model-based design, simulation and code generation make it a very efﬁcient and cost-
effective way to develop complex systems. For example, JLR report signiﬁcant time
and cost savings accruing from the use of these tools, with increased ability to test
K. Ye · S. Foster · J. Woodcock (B)
University of York,York, United Kingdom
e-mail: jim.woodcock@york.ac.uk
K. Ye
e-mail: kangfeng.ye@york.ac.uk
S. Foster
e-mail: simon.foster@york.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_10
215

216
K. Ye et al.
moredesignoptionsandfasterdevelopmentofembeddedcontroldesigns[1].Though
empirical analysis through simulation is an important technique to explore and reﬁne
models, only formal veriﬁcation can make speciﬁc mathematical guarantees about
behaviour, which are crucial to ensure safety of associated implementations. Whilst
veriﬁcation facilities for Simulink exist [3, 8, 9, 11, 32, 35], there is still a need for
assertional reasoning techniques that capture the full range of speciﬁable behaviour,
provide nondeterministic speciﬁcation constructs, and support compositional veriﬁ-
cation. Such techniques also need to be sufﬁciently expressive to handle the plethora
of additional languages and modelling notations that are used by industry in con-
cert with Simulink, in order to allow formulation of heterogeneous “multi-models”
that capture the different paradigms and disciplines used in large-scale systems [41].
We analyse these requirements from a variety of aspects: compositional reasoning,
expressiveness, algebraic loops, multi-rate models, semantics unifying and general-
ising, and tool support.
Assume-Guarantee (AG) reasoning is a valuable compositional veriﬁcation tech-
nique for reactive systems [4, 21, 27]. In AG, one demonstrates composite system
level properties by decomposing them into a number of contracts for each component
subsystem. Each contract speciﬁes the guarantees that the subsystem will make about
its behaviour, under certain speciﬁed assumptions of the subsystem’s environment.
Such a decomposition is vital in order to make veriﬁcation of a complex system
tractable, and to allow development of subsystems by separate teams.
AGreasoninghaspreviouslybeenappliedtoveriﬁcationofdiscrete-timeSimulink
control law diagrams through mappings into synchronous languages like Lustre [38]
and Kahn Process Networks [8]. These languages are inherently deterministic and
non-terminating in nature, which are a good ﬁt for discrete-time Simulink. However,
as discussed in [37], in order to be general and rich in expressiveness, contracts should
be relational (the value of outputs depends on the value of inputs), nondeterminis-
tic, and non-input-receptive (also called non-input-enabled or non-input-complete,
which means rejection of some input values). Relations allow systems to be speciﬁed
using input-output properties, such as assumptions and guarantees. Nondeterminism
is useful to give high-level speciﬁcations and abstract low-level details. Nondeter-
minism also plays a vital role in reﬁnement. Reducing nondeterminism results in
reﬁnement. Non-input-receptive contracts exclude illegal inputs for real systems.
Though discrete-time Simulink is input-receptive (accept all input values), there is
still a need to provide some protections and detections of such errors (due to illegal
inputs for real systems), and ﬁnally avoid these errors. For example, divide-by-zero
is one error that is expected to be avoided. Both the compositional theory for syn-
chronous concurrent systems (the interface theory) that is presented in [37] and the
Reﬁnement Calculus for Reactive Systems (RCRS) [32, 33] cater for these general
contract requirements. In addition, RCRS extends relational interfaces with liveness
properties.
Simulink diagrams may also contain algebraic loops (instantaneous feedbacks).
Various approaches [8, 38] rely on the algebraic loop detection mechanism in
Simulink to exclude diagrams with algebraic loops. Both the interface theory and
RCRS identify it as a current restriction and expect a future extension to the

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
217
framework in order to cope with instantaneous feedbacks. ClawZ [3, 22]1 and its
extensions [10, 11] are able to translate Simulink diagrams with algebraic loops to
Z, but they do not explicitly state how to solve instantaneous feedbacks and how to
reason about uniqueness of solutions as a proof obligation.
Simulink is also capable of modelling and analysing multi-rate distributed sys-
tems in which components have different sampling rates or sampling periods. That
different components in a system have different sampling rates is in the nature of
distributed systems and so it is necessary to take veriﬁcation of multi-rate systems
into account. The veriﬁcation of multi-rate Simulink models is widely supported by
[8, 12, 24, 38] but it is not the case for the interface theory and RCRS.
There is another need to unify and generalise semantic domains for Simulink
and other control law block diagrams. Current approaches [3, 8, 12, 38] translate
Simulink diagrams to one language and contracts or implementations into the same
language, and then use existing veriﬁcation methodologies for these languages to
reason about contracts-to-Simulink or Simulink-to-programs. But the interface the-
ory and RCRS are different. They introduce new notions: relational interfaces and
monotonic property transformers (MPT) respectively for contract-based veriﬁcation
of reactive systems. To the best of our knowledge, these approaches verify either
contracts or implementations, but not both. What is needed is a rich unifying and
generalising language capable of AG based compositional reasoning, and providing
a same semantic foundation for contracts, Simulink diagrams, and their implemen-
tations in various paradigms. Eventually, the development of such systems from
contracts, Simulink diagrams, to ﬁnal implementations (such as simulation and code
generation) is supported through reﬁnement with traceability and compositionality,
and systems are able to be veriﬁed from contracts to implementations. One such
example is the capability to compose a Simulink subsystem with another compo-
nent (maybe a contract or a C program) to form another subsystem, and this new
subsystem still shares the semantics on the same foundation.
Applicable tool support with a high degree of automation is also of vital impor-
tance to enable adoption by industry. Since Simulink diagrams are data rich and
usually have an uncountably inﬁnite state space, model checking alone is insufﬁ-
cient and there is a need for theorem proving facilities.
Based on analysis of these aspects, it is necessary to have an approach to support
compositional reasoning, be general in contracts that facilities a ﬂexible contract-
based speciﬁcation mechanism, able to reason about algebraic loops and multi-rate
models, capable of unifying semantics from various paradigms, as well as theo-
rem proving tool support. Our proposed solution in this paper aims to be one such
approach, though the work presented in this paper currently cannot support multi-
rate models (but it would be one of our future extensions). Our approach explores
development of formal AG-based proof support for discrete-time Simulink diagrams
through a semantic embedding of the theory of designs [40] in Unifying Theo-
ries of Programming (UTP) [18] in Isabelle/HOL [28] using our developed tool
1ClawZ: http://www.lemma-one.com/clawz_docs/.

218
K. Ye et al.
Isabelle/UTP [16].2 A design in UTP is a relation between two predicates where the
ﬁrst predicate (precondition) records the assumption and the second one (postcon-
dition) speciﬁes the guarantee. Designs are intrinsically suitable for modelling and
reasoning about control law diagrams because they are relational, nondeterministic,
and non-input-receptive.
Our work presented in this paper has three contributions. The main contribution
is to deﬁne a theoretical reasoning framework for control law block diagrams using
the theory of designs in UTP. Our translation is based on a denotational semantics in
UTP. Since UTP provides unifying theories for programs from different paradigms,
it enables specifying, modelling and verifying in a common theoretical foundation.
The capability to reason about diagrams with algebraic loops is another distinct fea-
ture of this reasoning framework. The second contribution is the mechanisation of
our theories in the theorem prover Isabelle/HOL using our implementation of UTP,
Isabelle/UTP. Both compositional reasoning and theorem proving help to tackle the
state space explosion problem by giving a purely symbolic account of Simulink
diagrams. Finally, the third and practical contribution is our industrial case study to
verify a subsystem in a safety-critical aircraft cabin pressure control system. We iden-
tify a vulnerable block and suggest to replace the block or strengthen its assumption.
This case study gives insight into the veriﬁcation methodology of our approach.
In the next section, we describe the relevant preliminary background about
Simulink and UTP. Then Sect.3 presents the assumptions we made, deﬁnes our
treatment of blocks in UTP, and translations of a number of blocks are illustrated.
Furthermore, in Sect.4 we introduce our composition operators and their correspond-
ing theorems. Afterwards, in Sect.5 we brieﬂy describe our veriﬁcation strategies, the
mechanisation of our approach in Isabelle/HOL, and demonstrate with an industrial
case study. We conclude our work in Sect.6.
2
Preliminaries
2.1
Control Law Diagrams and Simulink
Simulink [26] is a model-based design modelling, analysis and simulation tool for
signal processing systems and control systems. It offers a graphical modelling lan-
guage that is based on hierarchical block diagrams. Its diagrams are composed of
subsystems and blocks as well as connections between these subsystems and blocks.
In addition, subsystems also can consist of others subsystems and blocks. Single
function blocks have inputs and outputs, and some blocks also have internal states.
A simple PID integrator is shown in Fig.1. The integrator is composed of four
blocks: an input port, an output port, a unit block (its initial value x0 is 0), and a
sum block. They are connected by signals (or wires). A unit delay block delays its
2Isabelle/UTP: https://www.cs.york.ac.uk/circus/isabelle-utp/.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
219
Fig. 1 PID integrator
input one sample period. It outputs x0 at initial step and previous input afterwards.
The sum block takes the signal from the input port and the signal from the unit delay
block as inputs, and outputs addition of them. Furthermore, the integrator uses the
unit delay block to feed previous output of the model back to the sum block by the
feedback. Therefore, the output of this model (or the output of the sum block) is
equal to addition of previous output (or x0 at initial step) and current input. Because
x0 is 0, the output at any time is a summation of all inputs up to that time.
A consistent understanding [12, 25] of the simulation in discrete-time Simulink is
based on an idealized time model. All executions and updates of blocks are performed
instantaneously (and inﬁnitely fast) at exact simulation steps. Between the simulation
steps, the system is quiescent and all values held on lines and blocks are constant.
The inputs, states and outputs of a block can only be updated when there is a time
hit (simulation time t at which Simulink executes the output method of a block for a
given sample period Tb and an offset To of the block, that is, (t −To) mod Tb = 0)
for this block. Otherwise, all values held in the block are constant too, though at
exact simulation steps.
SimulationandcodegenerationofSimulinkdiagramsusesequentialsemanticsfor
implementation, but it is not always necessary for reasoning about the simulation.
Based on the idealized time model, a single function block can be regarded as a
relation between its inputs and outputs. For instance, a unit delay block speciﬁes that
its initial output is equal to its initial condition and its subsequent output is equal
to previous input. Then connections of blocks establish further relations between
blocks. A directed connection from one block to another block speciﬁes that the
output of one block is equal to the input of another block. Finally, hierarchical block
diagrams establish a relation network between blocks and subsystems.
2.2
Unifying Theories of Programming (UTP)
Unifying Theories of Programming (UTP) [18] is a unifying framework to provide a
theoretical basis for describing and specifying programs across different paradigms
such as imperative, functional, declarative, nondeterministic, concurrent, reactive
and higher order. A theory in UTP is described using three parts: an alphabet, a
set of variables for the theory to be studied; a signature, the syntax for denoting
members of the theory; and healthiness conditions, a set of conditions characterising
membership of the theory.

220
K. Ye et al.
Our understanding of the simulation in Simulink as a relation network is very sim-
ilar to the concept “programs-as-predicates” [20] in UTP. This similarity makes UTP
[18] intrinsically suitable for reasoning about the semantics of Simulink simulation
because UTP uses an alphabetised predicate calculus to model computations.
2.2.1
Alphabetised Relation Calculus
The alphabetised relational calculus [13] is the most basic theory in UTP. A relation
is deﬁned as a predicate with undecorated variables (v) and decorated variables (v′)
in its alphabet. v denotes an observation made initially and v′ denotes an observation
made at an intermediate or ﬁnal state.
In addition to normal predicate operators such as ∧, ∨, ¬, =⇒, etc., more are
deﬁned to construct relations in UTP.
Deﬁnition 1 (Relations)
P ◁b ▷Q ≜(b ∧P) ∨(¬b ∧Q)
[Conditional]
P; Q ≜

∃v0 • P[v0/v′] ∧Q[v0/v]

[Sequence]
x := e ≜

x′ = e ∧u′ = u

[Assignment (u denotes all other variables in its alphabet)]
P ⊓Q ≜(P ∨Q)
[Nondeterminism]
Conventionally, we use upper and lower case variables for predicates (P and Q)
and conditions (b) respectively. A condition is a relation without decorated variables
in its alphabet, such as x > 3. P[v0/v′] (substitution) denotes that all occurrences of
v′ in P are replaced by v0.
Reﬁnement is an important concept in UTP concerned with program development
to achieve program correctness. In UTP, the notation for program correctness is the
same for every paradigm: in each state, the implementation P implies its speciﬁcation
S, which is denoted by S ⊑P. Prior to the deﬁnition of reﬁnement, we deﬁne the
universal closure of a relation below.
Deﬁnition 2 (Universal Closure) [P] ≜

∀v, v′ · P

providing the alphabet of P is
{v, v′}.
Deﬁnition 3 (Reﬁnement) S ⊑P ≜[P =⇒S]
S ⊑P means that everywhere P implies S. A reﬁnement sequence is shown
in (1).
true ⊑S1 ⊑S2 ⊑P1 ⊑P2 ⊑false
(1)
S1 is a more general and abstract speciﬁcation than S2 and thus easier to implement.
The relation true is the easiest one and can be implemented by anything. P2 is more
speciﬁc and determinate program than P1. Thus P2 is more useful in general because
it is easier to be implemented. The relation false is the strongest predicate and it is
impossible to implement in practice.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
221
For example, for a speciﬁcation

x ≥3 ∧x′ = 4 ∧y′ = y

, the correctness of an
implementation (x := x + 1) is shown below.
Proof (Example)

x ≥3 ∧x′ = 4 ∧y′ = y

⊑(x := x + 1)
[reﬁnement]
=

(x := x + 1) =⇒

x ≥3 ∧x′ = 4 ∧y′ = y

[assignment]
=

x′ = x + 1 ∧y′ = y

=⇒

x ≥3 ∧x′ = 4 ∧y′ = y

[universal one-point rule]
= [x ≥3 ∧x + 1 = 4 ∧y = y]
[arithmetic and reﬂection]
= [x ≥3 ∧x = 3]
[arithmetic]
= true
⊓⊔
2.2.2
Designs
Designs are a subset of the alphabetised predicates that use a particular variable ok
to record information about the start and termination of programs. The behaviour of
a design is described from initial observation and ﬁnal observation by relating its
precondition P (assumption) to the postcondition Q (guarantee) as (P ⊢Q) [18, 40]
(assuming P holds initially, then Q is established). The theory of designs is actually
the theoretical setting for assume-guarantee reasoning [15].
Deﬁnition 4 (Design)
(P ⊢Q) ≜

P ∧ok =⇒Q ∧ok′
A design is deﬁned in Deﬁnition 4 where ok records that the program has started
and ok′ that it has terminated. It states that if the design has started (ok = true)
in a state satisfying its precondition P, then it will terminate (ok′ = true) with its
postcondition Q established. Therefore, with designs we are able to reason about
total correctness of programs. We introduce some basic designs.
Deﬁnition 5 (Basic Designs)
⊤D
≜
(true ⊢false) = ¬ok
[Miracle]
⊥D
≜true
[Abort]
(x := e) ≜

true ⊢x′ = e ∧u′ = u

[Assignment]
IID
≜
(true ⊢II)
[Skip]
Abort (⊥D) and miracle (⊤D) are the bottom and top element of a complete lattice
formed from designs under the reﬁnement ordering. ⊥D is deﬁned as a design true.
The precondition of true is the predicate false and its postcondition can be any
arbitrary predicate P. Actually, for any P, (false ⊢P) is equal to true.

222
K. Ye et al.
Theorem 1 (Abort)
(false ⊢P)
[Deﬁnition 4]
=

false ∧ok =⇒P ∧ok′
[false zero for conjunction]
=

false =⇒P ∧ok′
[vacuous implication]
= true
An interesting example is (false ⊢true) = (false ⊢false).
Abort is never guaranteed to terminate and miracle establishes the impossible. In
addition, abort is reﬁned by any other design and miracle reﬁnes any other design.
Assignment has precondition true provided the expression e is well-deﬁned and
establishes that only the variable x is changed to the value of e and other variables
have not changed. The skip IID is a design identity that always terminates and leaves
all variables unchanged.
Reﬁnement of designs is given in the theorem below.
Theorem 2 (Reﬁnement of Designs)
(P1 ⊢Q1 ⊑P2 ⊢Q2)
=
([P1 =⇒P2] ∧[P1 ∧Q2 =⇒Q1])
Reﬁnement of designs is achieved by weakening the precondition, and strengthening
the postcondition in the presence of the precondition.
Designs can be sequentially composed with the following theorem:
Theorem 3 (Sequential Composition)
(P1 ⊢Q1; P2 ⊢Q2)
=
((¬ (¬P1; true) ∧(Q1 wp P2)) ⊢Q1; Q2)
where Q1 wp P2 denotes the weakest precondition (Deﬁnition 6) for Q1 to be
guaranteed to establish P2.
Deﬁnition 6 (Weakest Precondition) Q wp r ≜¬ (Q; ¬r)
A sequence of designs terminates when P1 holds and Q1 guarantees to establish
P2. On termination, sequential composition of their postconditions is established. If
P1 is a condition (denoted as p1), the theorem is further simpliﬁed.
Theorem 4 (Sequential Composition (Condition))
(p1 ⊢Q1; P2 ⊢Q2)
=
((p1 ∧(Q1 wp P2)) ⊢Q1; Q2)
A condition p1 is a particular predicate that only has input variables in its alphabet. In
other words, a design of which its precondition is a condition only makes the assump-
tion about its initial observation (input variables) and without output variables. That is
the same case for our treatment of Simulink blocks. Furthermore, sequential compo-
sition has two important properties: associativity and monotonicity which are given
in the theorem below.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
223
Theorem 5 (Associativity, Monotonicity)
P1; (P2; P3) = (P1; P2) ; P3 [Associativity]
Suppose P1 ⊑P2 and Q1 ⊑Q2, then
(P1; Q1) ⊑(P2; Q2)
[Monotonicity]
In addition, we deﬁne two notations preD and postD.
Deﬁnition 7 (preD and postD)
preD (P) = ¬P[true, f alse/ok, ok′]
postD (P) = P[true, true/ok, ok′]
preD and postD can be used to retrieve the precondition of the design and the post-
condition in the presence of the precondition respectively according to the following
theorem.
Theorem 6 (preD and postD)
preD (P ⊢Q) = P
postD (P ⊢Q) = (P =⇒Q)
3
Semantic Representation of Simulink Blocks
In this section, we focus on the methodology to map individual Simulink blocks to
designs in UTP semantically. Basically, a block or subsystem is regarded as a relation
between inputs and outputs. We use an undashed variable and a dashed variable to
denote input signals and output signals respectively.
We start with assumptions made to Simulink models when developing this work.
3.1
Assumptions
Discrete-time In this paper, only discrete-time Simulink models are taken into
account.
Causality We assume the systems modelled in Simulink diagrams are causal
where the output at any time only depends on values of present and past inputs.
Consequently, if inputs to a casual system are identical up to some time, their corre-
sponding outputs must also be equal up to this time [31, Sect.2.6.3].
Single-rate This work captures single sampling rate Simulink models, which
means the timestamps of all simulation steps are multiples of a base period T . Steps
are abstracted and measured by step numbers (natural numbers N) and T is removed
from its timestamp. In the future, we will extend the reasoning framework in this
paper to support multi-rate models. The basic idea is to introduce a base period T

224
K. Ye et al.
which is the greatest common divisor (gcd) of the sampling periods of all blocks.
Then for each block, its sampling period Tb must be multiples of T . At any simulation
time n ∗T of the n step, if n ∗T is multiples of Tb, it is a hit. Otherwise, it is a miss.
A block reads inputs and updates its states and outputs only when there is a hit. The
states and outputs are left unchanged when there is a miss.
An algebraic loop occurs in simulation when there exists a signal loop with only
direct feedthrough blocks (instantaneous feedback without delay) in the loop. [8,
9, 14] assume there are no algebraic loops in Simulink diagrams and RCRS [32]
identiﬁes it as a future work. Amalio et al. [2] detects algebraic loops of SysML [29]
models by a reﬁnement model checker FDR3 [17]. Our theoretical framework can
reason about a discrete-time block diagram with algebraic loops: speciﬁcally check
if it has a unique solution.
The signals in Simulink can have many data types, such as signed or unsigned
integer, single ﬂoat, double ﬂoat, and boolean. The default type for signals are double
in Simulink. This work uses real numbers in Isabelle/HOL as a universal type for
all signals. Real numbers in Isabelle/HOL are modelled precisely using Cauchy
sequences, which enables us to reason in the theorem prover. This is a reasonable
simpliﬁcation because all other types could be expressed using real numbers, such
as boolean as 0 and 1.
3.2
State Space
The state space of our theory for block diagrams is composed of only one variable
in addition to ok, named inouts. We deﬁned it as a function from natural numbers
(step numbers) to a list of inputs or outputs.
inouts : N →seq R
Then a block is a design that establishes the relation between an initial observation
inouts (a list of input signals) and a ﬁnal observation inouts’ (a list of output signals).
Additionally, this is subject to the assumption of the design.
3.3
Healthiness Condition: SimBlock
This healthiness condition characterises a block with a ﬁxed number of inputs and
outputs. Additionally it is feasible. A design is a feasible block if there exists at least
a pair of inouts and inouts’ that establishes both the precondition and postcondition
of the design.
Deﬁnition 8 (SimBlock) A design (P ⊢Q) with m inputs and n outputs is a
Simulink block if SimBlock (m, n, (P ⊢Q)) is true. In other words, this design is
SimBlock(m, n) healthy.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
225
SimBlock (m, n, (P ⊢Q)) ≜
⎛
⎝
¬ [P =⇒¬Q] ∧
((∀nn · # (inouts(nn)) = m) ⊑(P ∧Q)) ∧

∀nn · #

inouts′(nn)

= n

⊑(P ∧Q)

⎞
⎠
The ﬁrst predicate of the conjunctions in the deﬁnition of SimBlock states that
(1) P always holds, and (2) it is not possible to establish ¬Q providing P holds.
This excludes abortion and miracle deﬁned in Deﬁnition 5, as shown in Theorem 7,
because they are always not desired in Simulink diagrams. The second and third
predicates of the conjunctions characterise the number of inputs and outputs of a
block. In short, SimBlock (m, n, (P ⊢Q)) characterises a subset of feasible blocks
which have m inputs and n outputs.
Theorem 7 For all m and n, both ⊤D and ⊥D are not SimBlock (m, n) healthy.
Proof
SimBlock (m, n, ⊤D)
= SimBlock (m, n, true ⊢false)
[Deﬁnition 5]
= (¬ [true =⇒¬ f alse] ∧· · · )
[Deﬁnition 8]
= f alse
[propositional calculus]
SimBlock (m, n, ⊥D)
= SimBlock (m, n, false ⊢false)
[Deﬁnition 5]
= (¬ [ f alse =⇒¬ f alse] ∧· · · )
[Deﬁnition 8]
= f alse
[propositional calculus]
⊓⊔
3.4
Number of Inputs and Outputs
Two operators inps and outps are deﬁned to get the number of input signals and
output signals for a block. They are implied from SimBlock of the block.
Deﬁnition 9 (inpsandoutps)
SimBlock(m, n, P) =⇒(inps(P) = m ∧outps(P) = n)
Provided that P is a healthy block, inps returns the number of its inputs and outps
returns the number of its outputs. Additionally, inps and outps are not directly deﬁned
in terms of P. Instead, in order to get the number of inputs m and outputs n of a
block P, we need to prove P is SimBlock(m, n, P) healthy.
3.5
Simulink Blocks
In order to give deﬁnitions to the corresponding designs of Simulink blocks, ﬁrstly
we deﬁne a design pattern FBlock to facilitate deﬁnitions of other blocks. Then we
illustrate deﬁnitions of three typical Simulink blocks.
We deﬁned a pattern that is used to deﬁne all other blocks.

226
K. Ye et al.
Deﬁnition 10 (FBlock)
FBlock ( f1, m, n, f2)
≜
⎛
⎜⎜⎜⎜⎝
∀nn · f1 (inouts, nn)
⊢
∀nn ·
⎛
⎝
# (inouts(nn)) = m∧
#

inouts′(nn)

= n∧

inouts′(nn) = f2 (inouts, nn)

⎞
⎠
⎞
⎟⎟⎟⎟⎠
FBlock has four parameters: f1 is a predicate that speciﬁes the assumption of the
block and it is a function on input signals; m and n are the number of inputs and
outputs, and f2 is a function that relates inputs to outputs and is used to establish
the postcondition of the block. The precondition of FBlock states that f1 holds for
inputs at any step nn. And the postcondition speciﬁes that for any step nn the block
always has m inputs and n outputs, the relation between outputs and inputs are given
by f2, and additionally f2 always produces n outputs provided there are m inputs.
A block B deﬁned by FBlock( f1, m, n, f2) itself is not automatically SimBlock
(m, n) healthy. It depends on its parameters f1 and f2. If for any step there exists
some inputs that satisﬁes f1, and for these inputs f2 always produces n outputs, then
this block is SimBlock(m, n, B) healthy.
Then the Unit Delay block is deﬁned using this pattern.
Deﬁnition 11 (Unit Delay)
Unit Delay (x0)
≜FBlock (truef, 1, 1, (λx, n · ⟨x0 ◁n = 0 ▷hd (x (n −1))⟩))
=
⎛
⎜⎜⎜⎜⎝
∀nn · true
⊢
∀nn ·
⎛
⎝
# (inouts(nn)) = 1∧
#

inouts′(nn)

= 1∧

inouts′(nn) = ⟨x0 ◁nn = 0 ▷hd (inouts(nn −1))⟩

⎞
⎠
⎞
⎟⎟⎟⎟⎠
[Deﬁnition 10]
where hd is an operator to get the head of a sequence, and truef = (λx, n · true)
that means no constraints on input signals.
Deﬁnition 11 of the Unit Delay block is straightforward: it accepts one input and
produces one output. The value of the output is equal to x0 at the ﬁrst step (0) and
equal to the input at the previous step at subsequent steps.
Theorem 8 Unit Delay(x0) is SimBlock(1, 1) healthy.
Proof SimBlock(1, 1,Unit Delay(x0))istrueifallthreepredicatesinitsdeﬁnition
are true. Use P and Q to denote the precondition and postcondition of Unit Delay
respectively. Firstly,
¬ [P =⇒¬Q]
= ¬

∀inouts, inouts′ · P =⇒¬Q

[Deﬁnition 2 of universal enclosure]
= ¬

∀inouts, inouts′ · ¬Q

[P = true]
=

∃inouts, inouts′ · Q

[Negation of universal quantiﬁer]
= true
[Witness inouts = inouts′ = ⟨x0⟩]

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
227
Then
((∀nn · # (inouts(nn)) = 1) ⊑(P ∧Q))
= ∀inouts, inouts′ · ((P ∧Q) =⇒(∀nn · # (inouts(nn)) = 1))
[Deﬁnition 2 of reﬁnement and Deﬁnition 3 of universal closure]
= ∀inouts, inouts′ · Q =⇒(∀nn · # (inouts(nn)) = 1)
[P = true]
= ∀inouts, inouts′ · (¬Q ∨(Q ∧(∀nn · # (inouts(nn)) = 1))) [Implication]
= ∀inouts, inouts′ · (¬Q ∨Q)
[Q is the postcondition of Unit Delay]
= true
[Q is the postcondition of Unit Delay]
Similarly,

∀nn · #

inouts′(nn)

= 1

⊑(P ∧Q)

= true
Hence, SimBlock(1, 1,Unit Delay(x0)) is true.
⊓⊔
The Divide block outputs the result of dividing its ﬁrst input by its second. It is
deﬁned below.
Deﬁnition 12 (Divide)
Div2 ≜FBlock ((λx, n · hd(tl(x(n)) ̸= 0) , 2, 1, (λx, n · ⟨hd(x(n))/hd(tl(x(n)))⟩))
where tl is an operator to get the tail of a sequence.
Deﬁnition 12 of the Divide block is slightly different because it has a precondition
that assumes the value of its second input is not zero at any step. In the presence
of this assumption, its postcondition is guaranteed to be established. Therefore, the
divide-by-zero error is avoided. By this way, the precondition enables modelling of
non-input-receptive systems that may reject some inputs at some points.
Theorem 9 Div2 is SimBlock(2, 1) healthy.
The Sum block of two inputs is deﬁned below.
Deﬁnition 13 (Sum2)
Sum2 ≜FBlock (truef, 2, 1, (λx, n · ⟨hd(x(n)) + hd(tl(x(n)))⟩)) .
Theorem 10 Sum2 is SimBlock(2, 1) healthy.
4
Block Compositions
In this section, we deﬁne three composition operators that are used to compose sub-
systems and systems from blocks. We also use three virtual blocks to map Simulink’s
connections in our designs. We start with the integrator example and describe how
to compose individual blocks.

228
K. Ye et al.
Fig. 2 PID integrator
4.1
The PID Integrator Example
For the PID integrator illustrated in Fig.1, in order to clearly present how the four
blocks (inport, outport, sum and unit delay) compose to form this diagram, we rear-
range it (Fig.2a) into Fig.2b by changing the icon of Sum and moving the unit delay
block to the left of the sum block.
Input and output ports merely provide interfaces between this diagram and its
environment. They are not functional blocks. Therefore, we will not translate them
explicitly in our theory.3 We treat the feedback as the outermost composition. Inside
the feedback it is regarded a block with two inputs and two outputs (like breaking
the feedback wire). The input goes to the sum block directly and the second input
(after broken) goes to the sum block through the unit delay block. First, we introduce
a virtual identity block Id in the ﬁrst input, then make it compose with the unit
delay block in parallel (like stack) because both of them do not share any inputs and
outputs (disjoint). The composition will have two inputs and two outputs. Then this
composite sequentially composes with the sum block to output their addition (one
output). Afterwards, the output is split into two equal signals: one to the output port,
one for feedback. In addition, the feedback wire establishes that the split signal for
feedback is equal to the second input.
In sum, the composition of this diagram from individual blocks would be
(((Id ∥B Unit Delay(0)) ; Sum2; Split2) fD(1, 1))
Thus, in order to compose a diagram from blocks, we need to introduce several
composition operators: ;, ∥B, and fD.
4.2
Sequential Composition
The meaning of sequential composition of designs is deﬁned in Theorem 3. It corre-
sponds to composition of two blocks as shown in Fig.3 where all outputs of B1 are
3However, because the order of input or output ports matter, we deﬁne inouts as a sequence of
inputs or outputs. By this way, the order information has kept in our translation.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
229
Fig. 3 Sequential
composition
B1
B2
connected to the inputs of B2 (note that we use one arrowed connection to represent
a collection of signals, and not one signal). A new block sequentially composed from
B1 and B2 is denoted as B1; B2.
Provided that
P = (FBlock (p1, m1, n1, f1))
SimBlock (m1, n1, P)
Q = (FBlock (p2, n1, n2, f2))
SimBlock (n1, n2, Q)
Sequential composition of two blocks can be simpliﬁed into one block using
Theorem 11.
Theorem 11 (Simpliﬁcation)
P; Q = FBlock

(λx, n · (p1(x, n)) ∧((p2 ◦f1)(x, n)) ∧#(x(n)) = m1)
, m1, n2, ( f2 ◦f1)

[Simpliﬁcation]
This theorem establishes that sequential composition of two blocks, where the num-
ber of outputs of the ﬁrst block is equal to the number of inputs of the second block, is
simply a new block with the same number of inputs as the ﬁrst block P and the same
number of outputs as the second block Q, and additionally the postcondition of this
composed block is function composition. In addition, the sequentially composed
block is still SimBlock(m1, n2) healthy which is shown in the closure theorem
below.
Theorem 12 (Closure)
SimBlock (m1, n2, (P; Q)) [SimBlock Closure]
If both p1 and p2 are true, then Theorem 11 is further simpliﬁed.
Theorem 13 (Simpliﬁcation)
(P; Q) = FBlock (truef, m1, n2, ( f2 ◦f1)) [Simpliﬁcation]
The theorems above assume the number of outputs in P is equal to the number
of inputs in Q. However, if this is not the case, it is miraculous.
Theorem 14 (Miraculous) if n1 ̸= m2, then
(FBlock (truef, m1, n1, f1)) ; (FBlock (truef, m2, n2, f2)) = ⊤D

230
K. Ye et al.
Fig. 4 Parallel composition
B1
B2
4.3
Parallel Composition
Parallel composition of two blocks is a stack of inputs and outputs from both blocks
and is illustrated in Fig.4. We use the parallel-by-merge scheme [18, Sect.7.2] in
UTP. This is the scheme used to deﬁne parallel composition for reactive processes
(such as ACP [5] and CSP [19]) in UTP. Parallel-by-merge is denoted as P ∥M Q
where M is a special relation that explains how the output of parallel composition
of P and Q should be merged following execution.
However, parallel-by-merge assumes that the initial observations for both predi-
cates should be the same. But that is not the case for our block composition because
the inputs to the ﬁrst block and that to the second block are different. Therefore,
in order to use the parallel by merge, ﬁrstly we need to partition the inputs to the
composition into two parts: one to the ﬁrst block and another to the second block.
This is illustrated in Fig.5 where we assume that P has m inputs and i outputs, and
Q has n inputs and j outputs. Finally, both parts of the parallel composition have the
same inputs (m + n), and the outputs of P and Q are merged to get i + j outputs.
Parallel composition of two blocks is deﬁned below.
Deﬁnition 14 (Parallel Composition)
P ∥B Q ≜
⎛
⎝
(takem(inps(P) + inps(Q)) inps(P); P)
∥BM
(dropm(inps(P) + inps(Q)) inps(P); Q)
⎞
⎠
Fig. 5 Parallel composition of two blocks

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
231
where takem and dropm are two blocks to partition inputs into two parts (the ﬁrst
part—inps(P) inputs— for P and another part—inps(Q) inputs—for Q).
And the merge relation BM is deﬁned below.
Deﬁnition 15 (BM)
BM ≜

ok′ = 0.ok ∧1.ok

∧

inouts′ = 0.inouts ⌢1.inouts

The merge operator BM states that the parallel composition terminates if both blocks
terminate. On termination, the output of parallel composition is concatenation of
the outputs from the ﬁrst block and the outputs from the second block. Due to the
concatenation, this merge operator is not commutative, which is different from the
merge operators for ACP and CSP.
Parallel composition has various interesting properties.
Theorem 15 (Associativity, Monotonicity, and SimBlock Closure) Assume that
SimBlock (m1, n1, P1)
SimBlock (m2, n2, P2)
SimBlock (m3, n3, P3)
SimBlock (m1, n1, Q1)
SimBlock (m2, n2, Q2)
P1 ⊑Q1
P2 ⊑Q2
then
P1 ∥B (P2 ∥B P3) = (P1 ∥B P2) ∥B P3
[Associativity]
(P1 ∥B Q1) ⊑(P2 ∥B Q2)
[Monotonicity]
SimBlock (m1 + m2, n1 + n2, (P1 ∥B P2)) [SimBlock Closure]
inps (P1 ∥B P2) = m1 + m2
outps (P1 ∥B P2) = n1 + n2
Parallel composition is associative, monotonic in terms of the reﬁnement rela-
tion, and SimBlock healthy. The inputs and outputs of parallel composition are
combination of the inputs and outputs of both blocks.
Theorem 16 (Parallel Composition Simpliﬁcation and Closure) Provided
P = (FBlock (truef, m1, n1, f1))
SimBlock (m1, n1, P)
Q = (FBlock (truef, m2, n2, f2))
SimBlock (m2, n2, Q)
then,
(P ∥B Q) = FBlock
⎛
⎝
truef, m1 + m2, n1 + n2,

λx, n ·
( f1 ◦(λx, n · take (m1, x(n))))
⌢( f2 ◦(λx, n · drop (m1, x(n))))

⎞
⎠
[Simpliﬁcation]
SimBlock (m1 + m2, n1 + n2, (P ∥B Q))
[SimBlock Closure]
Parallel composition of two FBlock deﬁned blocks is expanded to get a new block.
Its postcondition is concatenation of the outputs from P and the outputs from Q.

232
K. Ye et al.
Fig. 6 Feedback
B
The outputs from P (or Q) are function composition of its block deﬁnition function
f1 (or f2) with take (or drop).
4.4
Feedback
The feedback operator loops an output back to an input, which is illustrated in Fig.6.
The deﬁnition of the feedback is given in Deﬁnition 16. The basic idea to construct
a feedback operator is to use existential quantiﬁcation to specify that there exists one
signal sig that it is the ith input and oth output, and their relation is established by
the block P. This is illustrated in Fig.7 where m and n are the number of inputs and
outputs of P. PreFD adds a signal into the inputs at i. And then P takes assembled
inputs and produces an output in which the oth output is equal to the supplied signal.
Finally, theoutputs of feedbackaretheoutputs of P without theothoutput. Therefore,
a block with feedback is translated to a sequential composition of PreFD, P, and
Post FD.
Deﬁnition 16 ( fD)
P fD(i, o) ≜(∃sig · (PreFD(sig, inps(P), i); P; Post FD(sig, outps(P), o)))
where i and o denotes the index number of the output signal and the input signal,
which are looped. PreFD denotes a block that adds sig into the ith place of the
inputs.
Fig. 7 Feedback

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
233
Deﬁnition 17 (PreFD)
PreFD(sig, m, idx) ≜FBlock (truef, m −1, m, f _PreFD(sig, idx))
where
f _PreFD(sig, idx) = λx, n ·

take(idx, x(n)) ⌢⟨sig(n)⟩⌢drop(idx,
x(n)))
and Post FD denotes a block that removes the oth signal from the outputs of P and
this signal shall be equal to sig.
Deﬁnition 18 (Post FD)
Post FD(sig, n, idx) ≜
⎛
⎜⎜⎜⎜⎜⎜⎝
true
⊢
∀nn ·
⎛
⎜⎜⎝
# (inouts(nn)) = n∧
#

inouts′(nn)

= n −1∧
inouts′(nn) = f _Post FD (sig, idx, inouts, nn) ∧
sig(nn) = inouts(nn)!idx
⎞
⎟⎟⎠
⎞
⎟⎟⎟⎟⎟⎟⎠
where f _Post FD(idx) = λx, n ·

take(idx, x(n)) ⌢drop(idx + 1, x(n))

and !
is an operator to get the element in a list by its index.
Theorem 17 (Monotonicity) Provided that
SimBlock (m1, n1, P1)
SimBlock (m1, n1, P2)
P1 ⊑P2
i1 < m1 ∧o1 < n1
then,
(P1 fD(i1, o1)) ⊑(P2 fD(i1, o1))
The monotonicity law states that if a block is a reﬁnement of another block, then
its feedback is also a reﬁnement of the same feedback of another block.
The feedback of a FBlock deﬁned block can be simpliﬁed to a block if the original
block has one unique solution for feedback and the solution sig has supplied.
Theorem 18 (Simpliﬁcation) Provided that
P = FBlock (truef, m, n, f )
SimBlock (m, n, P)
Solvable_unique(i, o, m, n, f )
is_Solution(i, o, m, n, f, sig)
then,
(P fD(i, o))
= FBlock
truef, m −1, n −1,
(λx, n · ( f _Post FD(o) ◦f ◦f _PreFD (sig(x), i)) (x, n))

[Simpliﬁcation]
SimBlock (m −1, n −1, (P fD(i, o)))
[SimBlock Closure]

234
K. Ye et al.
The postcondition of the feedback is simply the function composition of
f _PreFD,
f
and
f _Post FD.
The
composed
feedback
is
SimBlock
(m −1, n −1) healthy.
In the theorem above, Solvable_unique is deﬁned below.
Deﬁnition 19 (Solvable_unique)
Solvable_unique (i, o, m, n, f ) ≜
⎛
⎜⎜⎝
(i < m ∧o < n) ∧
∀sigs ·
⎛
⎝
(∀nn · # (sigs(nn)) = (m −1)) =⇒
∃1sig ·

∀nn ·
 sig(nn) =
( f (λn1 · f _PreFD (sig, i, sigs, n1) , nn))!o

⎞
⎠
⎞
⎟⎟⎠
Solvable_unique characterises that the block with feedback has a unique solution
that satisﬁes the constraint of feedback: the corresponding output and input are equal.
Deﬁnition 20 (is_Solution)
is_Solution (i, o, m, n, f, sig) ≜
⎛
⎝∀sigs ·
⎛
⎝
(∀nn · # (sigs(nn)) = (m −1)) =⇒

∀nn ·
 sig(sigs, nn) =
( f (λn1 · f _PreFD (sig(sigs), i, sigs, n1) , nn))!o

⎞
⎠
⎞
⎠
The is_Solution evaluates a supplied signal to check if it is a solution for the feed-
back.
The simpliﬁcation law of feedback assumes the function f , that is used to deﬁne
the block P, is solvable in terms of i, o, m and n. In addition, it must have one unique
solution sig that resolves the feedback.
Our approach to model feedback in designs enables reasoning about systems with
algebraic loops. If a block deﬁned by FBlock and Solvable_unique (i, o, m, n, f )
is true, then the feedback composition of this block in terms of i and o is feasible no
matter whether there are algebraic loops or not.
4.4.1
An Example
Provided that a block P, which is similar to the Sum block but has two identical
outputs, is given below.
P = FBlock (truef, 2, 2, f )
f =

λx, n ·
 hd(x(n)) −hd(tl(x(n))),
hd(x(n)) −hd(tl(x(n)))

The feedback of P, as shown in Fig.8, can be simpliﬁed using the simpliﬁcation
Theorem 18. In order to use this theorem, three lemmas are required.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
235
Fig. 8 Feedback with an algebraic loop
Lemma 1 (SimBlock) P is SimBlock(2, 2) healthy.
The feedback of P in terms of the ﬁrst output and the ﬁrst input has a unique
solution.
Lemma 2 (Solvable_unique)
Solvable_unique(0, 0, 2, 2, f ) = true
Lemma 3 (is_Solution)
is_Solution(0, 0, 2, 2, f, (λx, n · (hd(x(n))/2))) = true
Then, the feedback of P can be simpliﬁed.
FBlock

truef, 2, 2, f

fD(0, 0)
=FBlock
 truef, 1, 1,
(λx, n · ( f _Post FD(0) ◦f ◦f _PreFD (λnn · hd(x(nn))/2, 0)) (x, n))

[Theorem 18, Lemmas 1, 2 and 3]
=FBlock

truef, 1, 1, (λx, n · ⟨hd(x(n))/2⟩)

[Deﬁntions of f _PreFD, f _Post FD, and f ]
Finally, the feedback of P is proved to be equal to a block Q (Fig.8) which
speciﬁes that its output is always half of its input.
4.5
Virtual Blocks
In addition to Simulink blocks, we introduce three virtual blocks for the purpose of
composition: Id, Split2, and Router.
The identity block Id is a block that has one input and one output, and the output
value is always equal to the input value.
Deﬁnition 21 (Id) Id ≜FBlock (truef, 1, 1, (λx, n · ⟨hd (x(n))⟩))
It establishes a fact that a direct signal line in Simulink could be treated as sequential
composition of many Id blocks, as shown in the theorem below.

236
K. Ye et al.
Fig. 9 Id
B1
Id
B2
Fig. 10 Split
B1
B2
Split
Theorem 19 Id; Id = Id
Its healthiness condition is proved.
Theorem 20 Id is SimBlock(1, 1) healthy.
The usage of Id is shown in Fig.9 where the dotted frame denotes a virtual block
(not a real Simulink block). The diagram is translated to (B1 ∥B Id) ; B2.
Split2 corresponds to the signal connection splitter that produces two signals
from one and both signals are equal to the input signal.
Deﬁnition 22 (Split2)
Split2 ≜FBlock (truef, 1, 2, (λx, n · ⟨hd (x(n)) ,
hd (x(n))⟩))
It is SimBlock healthy.
Theorem 21 Split2 is SimBlock(2, 1) healthy.
The usage of Split2 is shown in Fig.10. One input signal is split into two signals:
one to B1 and another to B2. In addition, the value of the signal to B1 must be always
equal to that of the signal to B2. The diagram is represented as Split2; (B1 ∥B B2).
Furthermore, signal lines may cross, that is, they are not connected but their line
order has changed. For example, Fig.11 shows two input signals are split into four,
then the second and the third among the four signals cross. Finally, the second and
the third become the third and the second. We introduce a virtual block Router for
this purpose to reorder inputs.
Deﬁnition 23 (Router)
Router (m, table) ≜FBlock (truef, m, m, (λx, n · reorder (x(n), table)))

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
237
Fig. 11 Router
B1
B2
Router
Router changes the order of m input signals according to the supplied table. table
is a sequence of natural numbers. The order of the sequence denotes the order of
outputs, and each element is an index of the inputs. For instance, table = [2, 0, 1]
denotes the new order: the third input, the ﬁrst input, and the second input.
The healthiness condition of Router requires well-deﬁnedness of table.
Theorem 22 Router(m, table) is SimBlock(m, m) healthy, provided that table
has m elements.
The usage of Router is shown in Fig.11. In this example, it should be instantiated
to Router(4, [0, 2, 1, 3]). Eventually, The diagram is translated to
(Split2 ∥B Split2) ; Router(4, [0, 2, 1, 3]); (B1 ∥B B2)
4.6
Subsystems
The treatment of subsystems (no matter whether hierarchical subsystems or atomic
subsystems) in our designs is similar to that of blocks. They could be regarded as a
bigger black box (as a design) that relates inputs to outputs.
4.7
Semantics Calculation
The theorems proved in this section and previous Sect.3 enable us to be possible to
automatically derive the mathematical semantics of block diagrams. These theorems
are called calculation laws or rules. For instance, in order to calculate the semantics
of a sequentially composed block P; Q, we can use the simpliﬁcation Theorem 11
provided that its assumptions are satisﬁed. An example is Sum2; Split2.

238
K. Ye et al.
(Sum2; Split2)
={Deﬁnition 13, Theorem 10, Deﬁnition 22, Theorem 22 and Theorem 11}
FBlock
⎛
⎝truef, 2, 2,
⎛
⎝
(λx, n · ⟨hd (x(n)) , hd (x(n))⟩)
◦
(λx, n · ⟨hd(x(n)) + hd(tl(x(n)))⟩)
⎞
⎠
⎞
⎠
={function composition}
FBlock

truef, 2, 2,

λx, n ·
 hd(x(n)) + hd(tl(x(n))),
hd(x(n)) + hd(tl(x(n)))

If there are more blocks in the diagrams, we need to recursively apply calculation
laws to simplify compositions and calculate their semantics. The simpliﬁcation of
the integrator example is illustrated below.
(((Id ∥B Unit Delay(0)) ; Sum2; Split2) fD(1, 1))
=
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
((Id ∥B Unit Delay(0)) ; Sum2; Split2)
= {. . .}
= {Closure and Simpliﬁcation Theorems 10, 8, . . . , 11 and 16}
FBlock
⎛
⎝
truef, 2, 2, fSplit2 ◦fSum2◦

λx, n ·
 ( fId ◦(λx, n · take (1, x(n))))
⌢( fU D ◦(λx, n · drop (1, x(n))))

⎞
⎠
= {function composition}
FBlock
⎛
⎝
truef, 2, 2, λx, n·
 hd(x(n)) +

0 n = 0 hd(tl(x(n −1)))

,
hd(x(n)) +

0 n = 0 hd(tl(x(n −1)))


⎞
⎠
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
[l1]
SimBlock (2, 2, FBlock (truef, 2, 2, f0))
[l2]
Solvable_unique(1, 1, 2, 2, f0)
[l3]
is_Solution(1, 1, 2, 2, f0, sol)
[l4]
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
=
{[l1], [l2], [l3], [l4] and Theorem 18}
FBlock
 truef, 1, 1,
(λx, n · ( f _Post FD(1) ◦f0 ◦f _PreFD (sol(x), 1)) (x, n))

= {function composition}
FBlock (truef, 1, 1, λx, n · ⟨sol(x, n)⟩)
[r]
where L  b  R is a if-then-else conditional operator. fId, fU D, fSum2, and
fSplit2 denote the function in their deﬁnitions to specify their outputs with regards to
inputs. f0 stands for the function below
λx, n ·
 hd(x(n)) + (0 n = 0 hd(tl(x(n −1)))) ,
hd(x(n)) + (0 n = 0 hd(tl(x(n −1))))

sol is the unique solution for the feedback.
sol ≜λx, n · (hd(x(n)) + 0 n = 0 sol(x, n −1))
Finally, the composition of the integrator is simpliﬁed to a block ([r]) that has
one input and one output. Additionally, the output is equal to current input plus

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
239
previous output, and the initial output is the initial input. In short, the output is the
sum of all inputs, which is consistent with the deﬁnition of the PID integrator.
5
Veriﬁcation Strategies and Case Study
5.1
General Procedure of Applying Assumption-Guarantee
Reasoning
Simulink blocks are semantically mapped to designs in UTP where additionally we
model assumptions of blocks to avoid unpredictable behaviour (such as a divide-
by-zero error in the Divide block) and ensure healthiness of blocks. The general
procedure of applying AG reasoning to verify correctness (S ⊑I) of a Simulink
subsystem or diagram (I) against its contracts S is given below.
S.1
Single blocks and atomic subsystems are translated to single designs with
assumptions and guarantees, as well as block parameters. This is shown in
Sect.3. Now we have the UTP semantics for each block.
S.2
Hierarchical block compositions are modelled as compositions of designs by
means of sequential composition (;), parallel composition (∥B), feedback ( fD),
and three virtual blocks. Composition of blocks is given in Sect.4. With these
calculation laws in Sects.3 and 4, the composition will be simpliﬁed and its
UTP semantics is calculated. Finally, we obtain the semantics for the diagram
(I).
S.3
Requirements (or contracts) of the block diagram (S) to be veriﬁed are mod-
elled as designs as well.
S.4
The reﬁnement relation (S ⊑I) in UTP is used to verify if a given property
is satisﬁed by a block diagram (or a subsystem) or not. Because both the
diagram and the contracts have the UTP semantics, the reﬁnement check could
be applied.
S.5
However, in order to apply the simpliﬁcation rules, we need to prove some
properties of the blocks to be composed, which is not always practical. Par-
ticularly, the simpliﬁcation Theorem 18 of feedback requires (1) the block
is SimBlock healthy, (2) it has one unique solution, and (3) the solution is
supplied. To ﬁnd the solution may be difﬁcult. To cope with this difﬁculty, our
approach supports compositional reasoning (illustrated in Fig.12) according
to monotonicity of composition operators in terms of the reﬁnement relation.
Provided a property S to be veriﬁed is decomposed into two properties S1 and
S2, and S1 and S2 are veriﬁed to hold in two blocks or subsystems I1 and I2
respectively, then S (the composition of the properties) is also satisﬁed by the
same composition of the blocks or subsystems.
(S1 ⊑I1 ∧S2 ⊑I2) =⇒(S1 op S2 ⊑I1 op I2)

240
K. Ye et al.
Fig. 12 Compositional
reasoning
where op ranges over sequential composition ;, parallel composition ∥B, and
feedback fD. With compositional reasoning, this problem is largely mitigated
because we do not need to simplify the composition of blocks further. The
veriﬁcation goal ⊑1 becomes three subgoals: ⊑2, ⊑3, and reﬁnement between
S and S1 op S2.
Remark 1 Since we have compositional reasoning (S.5) to cope with the difﬁculty of
semantics calculation, what is the purpose of the simpliﬁcation laws? Actually direct
semantics calculation by simpliﬁcation and compositional reasoning are complemen-
tary. Semantics calculation by simpliﬁcation retains the semantics of the composition
of blocks (say a subsystem), which makes reﬁnement check against contracts simpler
and make further composition of the subsystem have the full semantics. For instance,
if there are various contracts to be veriﬁed for the subsystem, we only need to check
reﬁnement once for each contract. However, for compositional reasoning, different
contract has different decomposed subcontracts. Hence, for each contract, we have
to prove correctness of each subcomponent against its corresponding subcontract.
5.2
Mechanisation
Our work in this paper has been mechanised in Isabelle/UTP. The mechanisation
includes three theory ﬁles: one for deﬁnitions of our theory, one for laws and theo-
rems, and one for the case study (shown later in Sect.5.3).
The deﬁnitions begin with the state space inouts, the healthiness condition
SimBlock, inps and outps. Then based on them, we are able to give deﬁnitions
to block composition operators ;,4 ∥B and fD, as well as deﬁnitions to each individ-
ual block. The deﬁnition of each block starts with a function to characterise its inputs
as its assumption and a function to specify its outputs in terms of its inputs and block
parameters as a guarantee, then uses the block pattern FBlock to deﬁne the block.
4Sequential composition of blocks is the same as sequence of designs, and therefore has been
deﬁned in the theory of designs.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
241
Fig. 13 Post landing ﬁnalize. Source [6]
Then a variety of theorems are proved for veriﬁcation. All lemmas and theorems
presented in this paper are mechanised. In addition, for each block deﬁned, it is
proved to be SimBlock healthy. These theorems form our calculation laws.
Using these deﬁnitions and laws, along with our veriﬁcation strategies, a case
study is veriﬁed and mechanised.
5.3
Case Study
This case study is to verify a post_landing_ﬁnalize subsystem, which is a part
of an aircraft cabin pressure control application. The original Simulink model is
from Honeywell5 through our industrial link with D-RisQ.6 The same model is also
studied in [6] and the diagram shown in Fig.13 is from the paper. It aims to model the
behaviour of the output signal ﬁnalize_event and ensure the signal is only triggered
after the aircraft door has been open for a minimum speciﬁc amount of time following
a successful landing. This speciﬁc signal will be used to invoke subsequent activities
such as calibration, initialisation or tests of sensors and actuators. Therefore, it is
very important to ensure the signal always is triggered as expected.
The model has four inputs
• door_closed is a boolean signal and denotes if the door is closed or not,
• door_open_time is the minimum amount of time (in seconds) that the door is
continuously open before the ﬁnalize_event is triggered,
• mode means the current aircraft state, such as LANDING (4) and GROUND (8),
• ac_on_ground is a boolean signal and means if the aircraft is on ground or not.
5Honeywell: https://www.honeywell.com/.
6D-RisQ: http://www.drisq.com/.

242
K. Ye et al.
one output ﬁnalize_event and three subsystems
• variableTimer ensures its output is true only after the door is open continuously
for door_open_time,
• rise1Shot models a rising edge-triggered circuit,
• latch models a SR AND-OR latch circuit.
InordertoapplyourAGreasoningintothisSimulinkmodel,ﬁrstlywetranslatethe
model (or the system) as shown in Sect.5.3.1. Then we verify a number of properties
for three subsystems in this model, which is given in Sect.5.3.2. Finally, in Sect.5.3.3
we present veriﬁcation of four requirements of this model. Our veriﬁcation strategies
will be illustrated in these subsections.
5.3.1
Translation and Simpliﬁcation
We start with translation of three small subsystems (variableTimer, rise1Shot and
latch) according to our block theories.
The original model of the subsystem latch is illustrated in the left diagram of
Fig.14. It has two boolean inputs: S and R, one output, and one feedback. It is rear-
ranged to get the right diagram of Fig.14. This diagram is modelled as a composition
of design blocks below.
Remark 2 Our rearrangement of the diagram only ﬂips the unit delay block and
changes its location, which will not change the behaviour of the diagram. The purpose
of the rearrangement is to make the translated latch in our theory easy to compare
with the diagram.
latch ≜
⎛
⎝
⎛
⎝
((Unit Delay(0) ∥B Id) ; LopO R(2))
∥B
(Id; LopN OT)
⎞
⎠; LopAN D (2) ; Split2
⎞
⎠fD(0, 0)
The blocks LopO R, LopN OT and LopAN D correspond to the OR, NOT and
AND operators in the logic operator block. For brevity, their deﬁnitions are omitted.
Then we apply composition deﬁnitions, simpliﬁcation and SimBlock closure laws
to simplify the subsystem. Finally, the latch subsystem is simpliﬁed to a design block
as shown below.
Fig. 14 Latch subsystem

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
243
Theorem 23
latch = FBlock (truef, 2, 1,latch_simp_pat_ f )
where
latch_simp_pat_ f
≜(λx, na · ⟨latch_rec_calc_output (λn1 · hd(x(n1)), λn1 · x(n1)!1, na)⟩)
latch_rec_calc_output(S, R, n)
≜
⎛
⎝
((0 S(0) = 0 1) R(0) = 0 0)
 n = 0
((latch_rec_calc_output(S, R, n −1) S(n) = 0 1) R(n) = 0 0)
⎞
⎠
L  b  R is a if-then-else conditional operator. latch_simp_pat_ f is the function
to characterise the relation between the inputs x and outputs of the latch. It has one
output which is in turn deﬁned bylatch_rec_calc_output.latch_rec_calc_output
is a recursively deﬁned function that establishes the relation of current outputs with
regards to current inputs as well as its history. It has three parameters: two inputs S
and R, and the step number n. The recursion corresponds to the feedback and the
step number n −1 that denotes previous step corresponds to one unit delay block in
the feedback.
Remark 3 latch_simp_pat_ f is calculated using the simpliﬁcation Theorem 18
from the supplied solution. Our current work does not yet has an automated way to
ﬁnd the unique solution. It is a research problem that we need to handle in the future.
Similarly, the subsystems variableTimer (Fig.15) and rise1Shot are translated
and simpliﬁed. variableTimer has two inputs: door_open (which is negation of
door_closed) and door_open_time. Basically, its model can be seen as three parts:
Fig. 15 variableTimer subsystem

244
K. Ye et al.
the ﬁrst part is from the block 4 to the block 14, the second part from the block 12
to varConﬁrm_1, and the third part varConﬁrm_1. The second part is to compute
the desired door open times according to door_open_time and the sampling rate
(the rate block). The ﬁrst part is to (1) increase the door open times (if the door is
open) until it reaches the desired door open times, and (2) reset to 0 (if the door is
closed). Then varConﬁrm_1 is simply to compare if the desired door open times
has reached (the output is true) or not (the output is false).
variableTimer1 ≜
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎝
⎛
⎝
(Min2; Unit Delay(0))
∥B
Const(1)
⎞
⎠; Sum2
⎞
⎠
∥B
Id
∥B
Const(0)
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
; Switch1(0.5); Split2
variableTimer2 ≜
⎛
⎝
Const(0)
∥B
Id
⎞
⎠; Max2; (GainRate) ; RoundCeil; DataT ypeConvInt32Zero; Split2
variableTimer ≜
⎛
⎝
⎛
⎝
⎛
⎝
variableTimer1
∥B
variableTimer2
⎞
⎠fD(0, 0)
⎞
⎠fD(0, 2)
⎞
⎠; RopGT
It is simpliﬁed to a block variableTimer_simp_pat (its deﬁnition is emitted).
Theorem 24 variableTimer = variableTimer_simp_pat
Finally, we can use the similar way to compose the three subsystems with
other blocks in the diagram (Fig.13) to get the corresponding composition of
post_landing_ﬁnalise_1, and then apply the similar laws to simplify it further
into one block and verify requirements for this system. However, for the outermost
feedback, it is difﬁcult to use a similar way to simplify it into one block because
it is more complicated than the feedbacks in the three small subsystems. In order
to use the simpliﬁcation Theorem 18 of feedback, we need to ﬁnd a solution for
the block and prove the solution is unique. With increasing complexity of blocks,
the application of this simpliﬁcation law is becoming harder and harder. Therefore,
post_landing_ﬁnalise_1 has not been simpliﬁed into one block. Instead, it is sim-
pliﬁed to a block with a feedback.
Theorem 25 (System Simpliﬁcation)
post_landing_ f inalize_1 = pl f _rise1shot_simpfD(4, 1)

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
245
5.3.2
Subsystems Veriﬁcation
After simpliﬁcation, we can verify properties of the subsystems using the reﬁnement
relation.
We start with veriﬁcation of a property for variableTimer: vt_req_00. This
property states that if the door is closed, then the output of this subsystem is
always false. However, this property cannot be veriﬁed in absence of an assump-
tion made to the second input: door_open_time. This is due to a type conversion
block int32 used in the subsystem. If the input to int32 is larger than 231 −1 (that is,
door_open_time larger than (231 −1)/10 (where 10 is the sampling rate), its output
is less than zero and ﬁnally the output is true. That is not the expected result. Prac-
tically, door_open_time should be less than (231 −1)/10. Therefore, we can make
an assumption of the input and eventually verify this property as given in Lemma 4.
In the lemma, the left side of ⊑is the contract and the right side is the variable-
Timer. The precondition of the contract is the assumption, which assumes the ﬁrst
input door_closed is boolean ([p1]) and the second input door_open_time is pos-
itive and less than (231 −1)/10 = 214 748 364 (/ as integer division) ([p2]). The
postcondition of the contract guarantees that variableTimer has two inputs and one
output ([p3]), and establishes the property (if the door is closed, then the output
is always false) ([p4]). Additionally, we suggest replacing int32 by uint32, or a
change of the data type for the input from double to unsigned integer, such as uint32.
Lemma 4 (vt_req_00)
⎛
⎜⎜⎜⎜⎝
∀n ·

(hd(inouts(n)) = 0 ∨hd(inouts(n)) = 1) ∧
[p1]
(hd(tl(inouts(n))) ≥0 ∧hd(tl(inouts(n))) ≤214748364) [p2]

⊢
∀n ·
# (inouts(n)) = 2 ∧#

inouts′(n)

= 1∧
[p3]
hd(inouts(n)) = 0 =⇒hd(inouts′(n)) = 0 [p4]

⎞
⎟⎟⎟⎟⎠
⊑variableTimer
Furthermore, one property for the latch subsystem (a SR AND-OR latch) is
veriﬁed. The property latch_req_00 states that as long as the second input R is true,
its output is always false. This is consistent with the deﬁnition of the SR latch in
circuits.
Lemma 5 (latch_req_00)
⎛
⎜⎜⎜⎜⎜⎜⎝
∀n ·

(hd(inouts(n)) = 0 ∨hd(inouts(n)) = 1) ∧
(hd(tl(inouts(n))) = 0 ∨hd(tl(inouts(n))) = 1)

⊢
∀n ·
⎛
⎝
# (inouts(n)) = 2∧
#

inouts′(n)

= 1∧
hd(tl(inouts(n))) ̸= 0 =⇒hd(inouts′(n)) = 0
⎞
⎠
⎞
⎟⎟⎟⎟⎟⎟⎠
⊑latch

246
K. Ye et al.
5.3.3
Veriﬁcation of Requirements
In the presence of certain assumptions in Table1, four requirements to be veriﬁed
for the system are illustrated in Table2.
Our approach to cope with the difﬁculty to simplify this system into one design
(Sect.5.3.1 and Theorem 25) is to apply compositional reasoning (see S.5). As a
speciﬁc example, the procedure to verify the requirements of this system using com-
positional reasoning is shown below (also illustrated in Fig.16).
In order to verify a requirement S satisﬁed by post_landing_ﬁnalise_1: S ⊑
post_landing_ f inalise_1, we need to ﬁnd a decomposed contract S1 and verify
S ⊑(S1 fD(4, 1)) and (S1 ⊑pl f _rise1shot_simp).
For brevity, only veriﬁcation of the ﬁrst requirement is demonstrated below. The
veriﬁcation of other three requirements are very similar.
Table 1 Assumptions for the system
Assumption 1
ac_on_ground can be true before the mode transitions to GROUND
Assumption 2
The mode can transition directly from LANDING to GROUND
Assumption 3
door_open_time does not change while the aircraft is on the ground
Assumption 4
door_closed must be true if ac_on_ground is false
Source [6]
Table 2 Requirements for the system
Requirement 1
A ﬁnalize event will be broadcast after the aircraft door has been open
continuously for door_open_time seconds while the aircraft is on the
ground after a successful landing
Requirement 2
A ﬁnalize event is broadcast only once while the aircraft is on the ground
Requirement 3
The ﬁnalize event will not occur during ﬂight
Requirement 4
The ﬁnalize event will not be enabled while the aircraft door is closed
Source [6]
Fig. 16 Illustration of compositional reasoning of this model

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
247
Requirement 1
According to Assumption 3 “door_open_time does not change while the aircraft is
on the ground” (see Table1) and the fact that this requirement speciﬁes the aircraft
is on the ground, therefore door_open_time is constant for this scenario. In order
to simplify the veriﬁcation, we assume it is always constant. The requirement is
modelled as a design contract req_01_contract (S) which is shown below.
req_01_contract ≜
⎛
⎝∀n ·
⎛
⎝λx, n ·
⎛
⎝
(hd(x(n)) = 0 ∨hd(x(n)) = 1) ∧[p1]
(x(n)!1 = c) ∧
[p2]
(x(n)!3 = 0 ∨x(n)!3 = 1)
[p3]
⎞
⎠
⎞
⎠(inouts, n)
⎞
⎠
⊢
∀n·
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
# (inouts(n)) = 4∧
[r1]
#

inouts′(n)

= 1∧
[r2]
∀m·
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎝
⎛
⎝
inouts(m)!3 = 1∧
inouts(m)!2 = 4∧
inouts(m)!0 = 1∧
⎞
⎠∧
[r3.1]
⎛
⎝
inouts(m + 1)!3 = 1∧
inouts(m + 1)!2 = 8∧
inouts(m + 1)!0 = 1
⎞
⎠[r3.2]
⎞
⎟⎟⎟⎟⎟⎟⎠
=⇒
∀p·
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
 ∀q · (q ≤c ∗rate) =⇒
inouts(m + 2 + p + q)!0 = 0

∧
[r3.3]
∀q ·
⎛
⎝
(q ≤p + c ∗rate) =⇒
 inouts(m + 2 + q)!3 = 1∧
inouts(m + 2 + q)!2 = 8

⎞
⎠∧
[r3.4]
(inouts(m + 2 + p −1)!0 = 1) ∧
[r3.5]

∀q · (q < p) =⇒

inouts′(m + 2 + q) = 0

[r3.6]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
=⇒

inouts′ (m + 2 + p + c ∗rate) = ⟨1⟩

[r3.7]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
[r3]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Recall that the model has four inputs: door_closed, door_open_time, mode,
and ac_on_ground. The precondition of the contract assumes for every step n such
that
• the data type of the ﬁrst input door_closed is boolean ([p1]);
• the second input door_open_time is constant c ([p2]);
• the data type of the fourth input ac_on_ground is boolean ([p3]).

248
K. Ye et al.
Fig. 17 Scenario of requirement 1
Its postcondition speciﬁes that
• it always has four inputs ([r1]) and one output ([r2]);
• then a scenario is speciﬁed (illustrated in Fig.17, [r3] in its deﬁnition):
– after a successful landing at step m and m + 1: the door is closed, the aircraft is
on ground, and the mode is switched from LANDING (4 at step m, [r3.1])
to GROUND (8 at step m + 1, [r3.2]),
– then the door has been open continuously for door_open_time seconds from
step m + 2 + p to m + 2 + p + door_open_time ∗rate ([r3.3]), therefore
the door is closed at the previous step m + 2 + p −1 ([r3.5]),
– while the aircraft is on ground: ac_on_ground is true and mode is GROUND
([r3.4]),
– additionally, between step m and m + 2 + p, the f inalize_event is not enabled
([r3.6]),
– then a f inalize_event will be broadcast at step m + 2 + p + door_open_time
([r3.7]).
Remark 4 In order to verify this requirement, the Sim2SAL, the tool used in [6] to
verify this system, introduces two auxiliary variables (latch and timer_count) and
splits this requirement into three properties which are expressed in LTL. However,
our approach models this complex scenario just in a design and has a powerful
expressiveness.
The decomposed contract req_01_1_contract (S1) is shown below where only
its postcondition is displayed. Its precondition is omitted for brevity because it reuses
the precondition of req_01_contract.

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
249
req_01_1_contract ≜
· · ·
⊢
∀n·
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
# (inouts(n)) = 5∧
[r1]
#

inouts′(n)

= 2∧
[r2]
∀m·
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎝
inouts(m)!3 = 1∧
inouts(m)!2 = 4∧
inouts(m)!0 = 1∧
⎞
⎠∧
[r3.1]
⎛
⎝
inouts(m + 1)!3 = 1∧
inouts(m + 1)!2 = 8∧
inouts(m + 1)!0 = 1
⎞
⎠∧
[r3.2]

∀q · inouts′(q)!1 = inouts(q)!4

[r3.3]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
=⇒
∀p·
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
 ∀q · (q ≤c ∗rate) =⇒
inouts(m + 2 + p + q)!0 = 0

∧
[r3.4]
∀q ·
⎛
⎝
(q ≤p + c ∗rate) =⇒
 inouts(m + 2 + q)!3 = 1∧
inouts(m + 2 + q)!2 = 8

⎞
⎠∧
[r3.5]
(inouts(m + 2 + p −1)!0 = 1) ∧
[r3.6]

∀q · (q < p) =⇒

inouts′(m + 2 + q) = 0

[r3.7]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
=⇒

inouts′ (m + 2 + p + c ∗rate) = ⟨1, 1⟩

[r3.8]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
[r3]
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Itspostconditionisdifferentfromthatofreq_01_contract inthenumberofinputs
and outputs (one more input and one more output), additional predicate ([r3.3])
to insist the second output is always equal to the ﬁfth input (because they will be con-
nected by the feedback, req_01_1 = req_01_1_contract fD(4, 1)), and two equal
outputs ([r3.8], correspond to split outputs).
According to Fig.16, we need to prove two auxiliary lemmas in order to verify
the contract.
Lemma 6 (Reﬁnement of decomposed contract with regards to the contract)
req_01_contract ⊑req_01_1_contract fD(4, 1)
Lemma 7 (Reﬁnement of decomposed contract with regards to decomposed dia-
gram)
req_01_1_contract ⊑pl f _rise1shot_simp
Monotonicity is achieved by Theorem 17.
Lemma 8 (Monotonicity of feedback)
req_01_1_contract fD(4, 1) ⊑pl f _rise1shot_simp fD(4, 1)

250
K. Ye et al.
From Lemmas 6, 7, 8, and the transitivity property of the reﬁnement relation, the
requirement is veriﬁed.
Theorem 26 (Requirement 1)
req_01_contract ⊑post_landing_ f inalize_1
5.4
Summary
In sum, we have translated and mechanised the post_landing_ﬁnalize diagram in
Isabelle/UTP, simpliﬁed its three subsystems (variableTimer, rise1Shot and latch)
and the post_landing_ﬁnalize into a design with feedback, and ﬁnally veriﬁed all
four requirements of this system. In addition, our work has identiﬁed a vulnerable
block in variableTimer. This case study demonstrates that our veriﬁcation frame-
work has rich expressiveness to specify scenarios for requirement veriﬁcation and
our veriﬁcation methodology is illustrated.
6
Conclusions
6.1
Related Work
Our work in this paper is inspired by the interface theory [37] and RCRS [32, 33] in
various aspects, and shares some ideas with other approaches.
Contract. A variety of Assume-Guarantee reasoning or contract-based veriﬁcation
facilitiesforSimulinkexist.SimCheck[35]deﬁnesacontractannotationlanguagefor
Simulink types including unit and dimension constrains, and then use a Satisﬁability
Modulo Theories (SMT) solver to check the well-formedness of Simulink diagrams
with respect to these contracts. [7, 8] deﬁne a contract syntax that is used to annotate
Simulink diagrams, and translate both contracts and Simulink diagrams to SDF, and
then use the notion of data reﬁnement to check correctness. The interface theory [37]
simply uses relations (predicates) as contracts to specify a set of valid input/output
pairs. And RCRS [32] gives contracts to components or systems in either Symbolic
Transition System (STS) based or Quantiﬁed Linear Temporal Logic (QLTL) based
monotonic property transformers. Contracts in our approach are simply designs, a
subset of alphabetised relations, which is similar to [37].
Composition operators. The way to compose blocks by sequential composition,
parallel composition, and feedback in our approach is similar to those in the interface
theory and RCRS.
Theorem proving. ClawZ [3] uses a theorem prover ProofPower-Z [34]. The tool
used in [9] is called S2L which is written in Java. Sim2SAL, the tool that is used in
[24] and [6] (to verify the same case study as ours), is SMT solver based. Similar to

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
251
RCRS, our approach also uses theorem proving for veriﬁcation in Isabelle/UTP, an
implementation of UTP in Isabelle.
Data types and typechecking. Reference [9] implements a type inference mech-
anism prior to the translation to get type information for each block and then uses
the information during translation. ClawZ [3] and its related approaches [10–12]
use a universal type (real numbers) for all blocks. RCRS deﬁnes well-formedness of
components by induction on the structure of the components. Our current work is
similar to ClawZ and uses a universal type (real numbers).
Multi-rate. Reference [9] implements clock inference to infer timing information
and uses the information to generate Lustre programs. Veriﬁcation of multi-rate
diagrams is supported by both CircusTime [12] and [8]. In [8], each stream is
associated with a period, and the stream can be upsampled or downsampled to match
agivenbaseperiod T .CircusTime introducesasolvertosynchroniseallblocks(each
block as a process) to model the cycles of the Simulink diagrams. In fact, the cycles
are deﬁned by the simulation time parameter. Each block models the behaviour of
one cycle (parametrised by the simulation time). Depending on the simulation time,
a block could be a sample time miss (quiescent) or a hit (update) at this step. We
have not yet supported veriﬁcation of multi-rate diagrams now. It is a part of our
future plans. We will use the similar idea as CircusTime to have a miss and a hit,
but without synchronisation (purely relational).
Abstraction and internal states. Our deﬁnitions of blocks and composition opera-
tors actually do not need to introduce additional state variables to explicitly keep its
internal state for future use, which is common in other approaches. For instance,
[7, Fig.5] shows a representation of the unit delay block in Synchronous Data
Flow (SDF) [23] by using an additional variable x for its internal state. [12] also
needs to add variables in the state space of stateful blocks. This is necessary in their
approaches. But it is not the case for the relation-based solutions: ClawZ , the inter-
face theory, RCRS, and our work. ClawZ and RCRS [32] deﬁne the unit delay block
also by introducing additional variables, but these variables are auxiliary (eliminated
during relation calculation but useful for modelling). Our deﬁnition of the unit block
uses inouts(n-1) to represent its previous input, which is more abstract, direct and
natural to its mathematical semantics. Therefore, our approach minimises the state
spaces of translated block diagrams as well as the relation calculation, which may
ease the veriﬁcation of large scale discrete-time systems.
Generalising and unifying semantics. Our work has its semantics based on UTP.
Therefore, our framework is capable of unifying and generalising semantics from
different paradigms. One example is to integrate our approach with CircusTime
to deliver a solution from contracts to implementations because the semantics of
CircusTime is also given in UTP. Our current work is able to verify the abstract side
from contracts to Simulink diagrams, and CircusTime is capable of verifying the
implementation side from Simulink diagrams to code (such as Ada and C programs).
In future, we will similarly support reﬁnement of Simulink control law diagram to
implementation using Isabelle/UTP.

252
K. Ye et al.
6.2
Conclusion and Future Work
We have presented a compositional assume-guarantee reasoning framework for
discrete-time Simulink diagrams. Our approach is based on the theory of designs
in UTP. In this paper, we present deﬁnitions for various blocks and block composi-
tion operators, as well as a set of calculation laws. These deﬁnitions and laws allow
us to calculate the semantics for a diagram. After that, we can verify the contracts
against the diagram (using the calculated semantics). One industrial example has
been veriﬁed and our approach identiﬁes one vulnerable block.
As we discussed in Sect.1, we would like to develop an approach that is general
in contracts, able to reason about algebraic loops and multi-rate models, and able
to unify semantics from various paradigms. We have fulﬁlled some of them and
presented in this paper, but have left some for future work.
• One extension is to support more precise type information for signals and blocks,
which allows us to infer data types for each block and each signal as well as type-
check the models. The inferred type information will further allow us to specify
types in contracts and verify them.
• To mechanise most of discrete-time Simulink blocks in our theory and develop
a translator is our subsequent extension, which will make our approach more
applicable.
• In addition, as discussed in Sect.3, our idea to support veriﬁcation of multi-rate
systems will be further developed and integrated into this framework. The capabil-
ity to verify multi-rate models extends usability of our approach to a wider variety
of systems.
• In order to generalise our framework, we plan to extend it to support reﬁnement
of Simulink control law diagram to implementation. Then our framework is able
to reason about Simulink diagrams from contract to implementation.
Acknowledgements This project is funded by the National Cyber Security Centre (NCSC) through
UK Research Institute in Veriﬁed Trustworthy Software Systems (VeTSS) [39]. The second author
is partially supported by EPSRC grant CyPhyAssure, EP/S001190/1. We thank Honeywell and
D-RisQ for sharing of the industrial case study.
References
1. Add2: Jaguar Reduces Development Costs with MathWorks—Rapid Prototyping and Code
Generation Tools. http://www.add2.co.uk/wp-content/uploads/add2JaguarUSERStory.pdf
2. Amalio, N., Cavalcanti, A., Miyazawa, A., Payne, R., Woodcock, J.: Foundations of the SysML
for CPS modelling. Technical Report, INTO-CPS Deliverable, D2.2a (2016)
3. Arthan, R.D., Caseley, P., O’Halloran, C., Smith, A.: ClawZ: control laws in Z. In: Proceedings
of 3rd IEEE International Conference on Formal Engineering Methods, ICFEM 2000, York,
England, UK, 4–7 Sept 2000, pp. 169–176. IEEE Computer Society (2000). https://doi.org/
10.1109/ICFEM.2000.873817

Compositional Assume-Guarantee Reasoning of Control Law Diagrams Using UTP
253
4. Bauer, S.S., David, A., Hennicker, R., Larsen, K.G., Legay, A., Nyman, U., Wasowski, A.:
Moving from Speciﬁcations to Contracts in Component-Based Design. In: de Lara, J., Zisman,
A. (eds.) Fundamental Approaches to Software Engineering—Proceedings of 15th Interna-
tional Conference, FASE 2012, Held as Part of the European Joint Conferences on Theory and
Practice of Software, ETAPS 2012, Tallinn, Estonia, 24 Mar–1 Apr 2012. Lecture Notes in
Computer Science, vol. 7212, pp. 43–58. Springer (2012). https://doi.org/10.1007/978-3-642-
28872-2_3
5. Bergstra, J.A., Klop, J.W.: Process algebra for synchronous communication. Inf. Control 60(1–
3), 109–137 (1984)
6. Bhatt, D., Chattopadhyay, A., Li, W., Oglesby, D., Owre, S., Shankar, N.: Contract-based
veriﬁcation of complex time-dependent behaviors in avionic systems. In: Rayadurgam, S.,
Tkachuk, O. (eds.) Proceedings of 8th International Symposium on NASA Formal Methods,
NFM 2016, Minneapolis, MN, USA, 7–9 June 2016. Lecture Notes in Computer Science, vol.
9690, pp. 34–40. Springer (2016). https://doi.org/10.1007/978-3-319-40648-0_3
7. Boström, P.: Contract-based veriﬁcation of simulink models. In: Qin, S., Qiu, Z. (eds.) Pro-
ceedings of 13th International Conference on Formal Engineering Methods and Software Engi-
neering , ICFEM 2011, Durham, UK, 26–28 Oct 2011. Lecture Notes in Computer Science,
vol. 6991, pp. 291–306. Springer (2011). https://doi.org/10.1007/978-3-642-24559-6_21.
8. Boström, P., Wiik, J.: Contract-based veriﬁcation of discrete-time multi-rate Simulink models.
Softw. Syst. Model. 15(4), 1141–1161 (2016). https://doi.org/10.1007/s10270-015-0477-x
9. Caspi, P., Curic, A., Maignan, A., Sofronis, C., Tripakis, S.: Translating discrete-time simulink
to lustre. In: Alur, R., Lee, I. (eds.) Proceedings of Third International Conference on Embedded
Software, EMSOFT 2003, Philadelphia, PA, USA, 13–15 Oct 2003. Lecture Notes in Computer
Science, vol. 2855, pp. 84–99. Springer (2003). https://doi.org/10.1007/978-3-540-45212-6_7
10. Cavalcanti, A., Clayton, P., O’Halloran, C.: From control law diagrams to Ada via circus
11. Cavalcanti, A., Clayton, P., O’Halloran, C.: Control law diagrams in circus. In: Fitzgerald, J.S.,
Hayes, I.J., Tarlecki, A. (eds.) Proceedings of FM 2005: Formal Methods, International Sympo-
sium of Formal Methods Europe, Newcastle, UK, 18–22 July 2005. Lecture Notes in Computer
Science, vol. 3582, pp. 253–268. Springer (2005). https://doi.org/10.1007/11526841_18
12. Cavalcanti, A., Mota, A., Woodcock, J.: Simulink timed models for program veriﬁcation. In:
Liu, Z., Woodcock, J., Zhu, H. (eds.) Theories of Programming and Formal Methods—Essays
Dedicated to Jifeng He on the Occasion of His 70th Birthday. Lecture Notes in Computer
Science, vol. 8051, pp. 82–99. Springer (2013). https://doi.org/10.1007/978-3-642-39698-
4_6
13. Cavalcanti, A., Woodcock, J.: A tutorial introduction to CSP in unifying theories of program-
ming. In: Cavalcanti, A., Sampaio, A., Woodcock, J. (eds.) First Pernambuco Summer School
on Software Engineering, Reﬁnement Techniques in Software Engineering, PSSE 2004, Recife,
Brazil, 23 Nov–5 Dec 2004, Revised Lectures. Lecture Notes in Computer Science, vol. 3167,
pp. 220–268. Springer (2004). https://doi.org/10.1007/11889229_6
14. Dragomir, I., Preoteasa, V., Tripakis, S.: Compositional semantics and analysis of hierarchical
block diagrams. In: Bosnacki, D., Wijs, A. (eds.) Proceedings of 23rd International Sympo-
sium on Model checking software, SPIN 2016, Co-located with ETAPS 2016, Eindhoven, The
Netherlands, 7–8 Apr 2016. Lecture Notes in Computer Science, vol. 9641, pp. 38–56. Springer
(2016). https://doi.org/10.1007/978-3-319-32582-8_3
15. Foster, S., Cavalcanti, A., Canham, S., Woodcock, J., Zeyda, F.: Unifying theories of reactive
design contracts. In preparation for Theoretical Computer Science (2017). arXiv:1712.10233
16. Foster, S., Zeyda, F., Woodcock, J.: Isabelle/UTP: a mechanised theory engineering framework.
In: Naumann, D. (ed.) 5th International Symposium on Unifying Theories of Programming,
UTP 2014, Singapore, 13 May 2014, Revised Selected Papers. Lecture Notes in Computer
Science, vol. 8963, pp. 21–41. Springer (2014). https://doi.org/10.1007/978-3-319-14806-
9_2
17. Gibson-Robinson, T., Armstrong, P., Boulgakov, A., Roscoe, A.: In: Proceedings of FDR3—A
Modern Reﬁnement Checker for CSP. Tools and Algorithms for the Construction and Analysis
of Systems. LNCS, vol. 8413, pp. 187–201 (2014)

254
K. Ye et al.
18. Hoare, C., He, J.: Unifying Theories of Programming, vol. 14. Prentice Hall (1998)
19. Hoare, C.A.R.: Communicating Sequential Processes. Prentice-Hall (1985)
20. Hoare, C.A.R., Roscoe, A.W.: Programs as Executable Predicates. In: Proceedings of FGCS,
pp. 220–228 (1984)
21. Jones, C.B.: Wanted: a compositional approach to concurrency, pp. 5–15. Springer, New York,
NY (2003). https://doi.org/10.1007/978-0-387-21798-7_1.
22. Jones, R.B.: ClawZ—The Semantics of Simulink Diagrams. Lemma 1 Ltd. (2003)
23. Lee, E.A., Messerschmitt, D.: Synchronous data ﬂow. Proc. IEEE 75, 1235–1245 (1987)
24. Li, W., Gérard, L., Shankar, N.: Design and veriﬁcation of multi-rate distributed systems.
In: 2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign
(MEMOCODE), pp. 20–29. IEEE (2015)
25. Marian, N., Ma, Y.: Translation of Simulink Models to Component-based Software Models,
pp. 274–280. Forlag uden navn (2007)
26. MathWorks: Simulink. https://www.mathworks.com/products/simulink.html
27. Meyer, B.: Applying “Design by Contract”. IEEE Comput. 25(10), 40–51 (1992). https://doi.
org/10.1109/2.161279
28. Nipkow, T., Paulson, L.C., Wenzel, M.: Isabelle/HOL—a proof assistant for higher-order logic.
Lecture Notes in Computer Science, vol. 2283. Springer (2002). https://doi.org/10.1007/3-540-
45949-9
29. Object Management Group: OMG Systems Modeling Language (OMG SysMLTM). Technical
Report. Version 1.4 (2015). http://www.omg.org/spec/SysML/1.4/
30. OpenModelica. https://openmodelica.org/
31. Oppenheim, A.V., Willsky, A.S., Nawab, S.H.: Signals and Systems, 2nd edn. Prentice-Hall
Inc, Upper Saddle River, NJ, USA (1996)
32. Preoteasa, V., Dragomir, I., Tripakis, S.: The reﬁnement calculus of reactive systems. CoRR
(2017). arXiv:1710.03979
33. Preoteasa, V., Tripakis, S.: Reﬁnement calculus of reactive systems. CoRR (2014).
arXiv:1406.6035
34. ProofPower. http://www.lemma-one.com/ProofPower/index/index.html
35. Roy, P., Shankar, N.: SimCheck: a contract type system for Simulink. Innov. Syst. Softw. Eng.
7(2), 73 (2011). https://doi.org/10.1007/s11334-011-0145-4.
36. TeraSoft: The MathWorks in the Automotive Industry. http://www.terasoft.com.tw/product/
doc/auto.pdf
37. Tripakis, S., Lickly, B., Henzinger, T.A., Lee, E.A.: A theory of synchronous relational inter-
faces. ACM Trans. Program. Lang. Syst. (TOPLAS) 33(4), 14 (2011)
38. Tripakis, S., Sofronis, C., Caspi, P., Curic, A.: Translating discrete-time simulink to lustre.
ACM Trans. Embed. Comput. Syst. 4(4), 779–818 (2005). https://doi.org/10.1145/1113830.
1113834
39. VeTSS: UK Research Institute in Veriﬁed Trustworthy Software Systems. https://vetss.org.uk/
40. Woodcock, J., Cavalcanti, A.: A tutorial introduction to designs in unifying theories of pro-
gramming. In: Boiten, E.A., Derrick, J., Smith, G. (eds.) Integrated Formal Methods, pp. 40–66.
Springer, Berlin Heidelberg, Berlin, Heidelberg (2004)
41. Zeyda, F., Ouy, J., Foster, S., Cavalcanti, A.: Formalising cosimulation models. In: Proceed-
ings of Software Engineering and Formal Methods (2018). https://doi.org/10.1007/978-3-319-
74781-1_31.

Sound and Relaxed Behavioural
Inheritance
Nuno Amálio
Abstract Object-oriented (OO) inheritance establishes taxonomies of OO classes.
Behavioural inheritance (BI), a strong version, emphasises substitutability: objects
of child classes replace objects of their ascendant classes without any observable
effect difference on the system. BI is related to data reﬁnement, but reﬁnement’s
constrictions rule out many useful OO subclassings. This paper revisits BI at the light
of Z and the theory of data reﬁnement. It studies existing solutions to this problem,
criticises them, and proposes improved relaxations. The results are applicable to
any OO language that supports design-by-contract (DbC). The paper’s contributions
include three novel BI relaxations supported by a mathematical model with proofs
carried out in the Isabelle proof assistant, and an examination of BI in the DbC
languages Eiffel, JML and Spec♯.
Keywords Reﬁnement · Z · Object-orientation · Inheritance ·
Design-by-contract · JML · Eiffel · Spec♯· Behavioural subtyping
1
Introduction
The object-oriented (OO) paradigm has a great deal in common with biological clas-
siﬁcation (taxonomy) and the ever present human endeavour to establish taxonomies
that reﬂect degree of relationship [30]. This is, perhaps, a factor behind OO’s popu-
larity, which uses classiﬁcation to tame diversity and complexity. Whilst biologists
go from life into classiﬁcation, computer scientists use classiﬁcation as templates
that generate computing life.
N. Amálio (B)
School of Computing and Digital Technology, Birmingham City University,
Millennium Point, Curzon Street, Birmingham B4 7XG, UK
e-mail: nuno.amalio@bcu.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_11
255

256
N. Amálio
OO design builds taxonomies around classes: abstractions representing living
computing objects with common characteristics. This resembles biological classiﬁ-
cations, which group similar entities into taxa [30]. OO classes deﬁne both static and
dynamic characteristics. Each object of a class is a distinct individual with its own
identity. A class has a dual meaning: intension and extension [20]. Intension sees a
class in terms of the properties shared by all its objects (for example, a class Person
with properties name and address), whereas extension views a class in terms of
its living objects (for example, class Person is {MrSilva, MsHakin, MrPatel}).
Biologicalclassiﬁcationshierarchicallyrelatetaxathroughcommonancestry[30].
This is akin to OO inheritance, which builds hierarchies from similarity to speci-
ﬁcity in which higher-level abstractions (superclasses or ancestors) capture common
characteristics of all descendant abstractions (subclasses). Inheritance provides a
reuse mechanism: descendants reuse their ancestor deﬁnitions, and may deﬁne extra
characteristics of their own.
The essence of OO inheritance lies in its is-a semantics. A child abstraction (a
subclass) is a kind of a parent abstraction. The child may have extra characteristics,
but it has a strong link with the parent: a living object of a descendant is at the same
time also an object of its ascendant classes, and a parent class includes all objects
that are its own direct instances plus those of its descendants. For example, when
we say that a human is a primate, then any person is both a human and a primate;
characteristics of primates are also characteristics of humans, however humans have
characteristics of their own which they do not share with other primates.
OO computing life becomes more complicated when it comes to dynamics
or behaviour, which is concerned with objects doing something when stimulated
through operations. To understand operations, we resort to a button metaphor: oper-
ations are buttons, part of an object’s interface, triggered from the outside world to
affect the internal state of an object and produce observable outputs. Often, such
button-pushes require data (or inputs). Inheritance implies that ancestor buttons
belong to descendants also and that descendants may specialise them. For example,
walk on primate could be specialised differently in humans and gorillas as upright-
walk and knuckle-walk, respectively, to give rise to polymorphism. Operations apply
the principle of information hiding; we have the interface of an operation—its name
and expected data with respect to inputs outputs—, which is what the outside world
sees, and its deﬁnition in terms of what it actually does (programs at a more con-
crete computing level). Through operations the outside world derives and instils
meaningful outcomes from and into the abstraction.
Inheritance’s is-a semantics entails substitutability: a child object can be used
whenever a parent object is expected. For instance, a human is suitable whenever a
primate is expected. This, in turn, entails a certain uniformity to prevent unwanted
divergence, which is enforced at two levels. Interface conformity, the more superﬁ-
cial level, requires that the interfaces of the shared buttons, in sub- and super-class,
conform with each other with respect to the data being interchanged (inputs and

Sound and Relaxed Behavioural Inheritance
257
outputs).1 This guarantees that subclasses can be asked to do whatever their super-
classes offer, but leaves room for unwanted deviation. For example, a gorilla class
may comply with a primate walk button, but be actually deﬁned just as standing
upright without any movement. The second deeper level of enforcement tackles this
issue through behavioural inheritance (BI) [21, 28]: not only the interfaces must con-
form, the behaviour must conform also to ensure that subclass objects may stand for
superclass objects without any difference on the object’s observable behaviour from
a superclass viewpoint. In our primates example, standing upright would not meet
the motion expectations. Only through proof can the satisfaction of BI be veriﬁed.
Deep substitutability is captured by the theory of data reﬁnement [16, 22, 39].
Inheritance relations induce reﬁnement relations between parent and child.2 This
paper tackles the reﬁnement restrictions, a major obstacle to BI’s ethos of correctness
already acknowledged by Liskov and Wing [28].
This paper delves into BI’s foundations to propose relaxations that tackle reﬁne-
ment’s overkills and constrictions. The investigation is in the context of Z [24, 39],
a formal modelling language with a mature reﬁnement theory [16, 39]. The work
builds up on ZOO, the OO style for Z presented in [2, 3, 10], that is the seman-
tic domain of UML + Z [2, 11, 12] and the Visual Contract Language (VCL) for
graphical modelling of software designs [6–9].
Contributions. This paper’s contributions are follows:
• The paper presents four relaxations to BI. Three of these relaxations are novel. A
fourth unproved relaxation proposed elsewhere is proved here with the aid of the
Isabelle proof assistant.
• A thorough examination of the BI relaxations that underpin the design by contract
languages JML, Eiffel and Spec♯.
Paper outline. The remainder of this paper is as follows. Section2 presents the
mathematical model that underpins the paper’s BI study. Section3 introduces the
paper’s BI setting and derives conjectures for BI. Section4 presents the running
example, which is analysed in Sect. 5 to better understand how BI’s restrictions affect
inheritance. Section6 performs a thorough examination of BI in the DbC languages
JML, Eiffel and Spec♯. Section7 presents the paper’s four relaxations which are
applied to the running example. Finally, the paper concludes by discussing its results
(Sect. 8), comparing the results with related work (Sect. 9) and by summarising its
main ﬁndings (Sect. 10). The appendix of Sect. 11 provides several mathematical
deﬁnitions. The accompanying technical report [4] provides supplementary material
not included in the main text.
1This involves type-checking, a computationally efﬁcient means of veriﬁcation which checks that
variables hold valid values according to their types (e.g. boolean variables cannot hold integers).
2Whereas in data reﬁnement the reﬁnement relation varies, in BI this relation is always a function
from subclass to superclass. BI is a specialisation of data reﬁnement.

258
N. Amálio
2
An Abstract Mathematical Model of OO
This section presents the paper’s OO mathematical model, drawn from ZOO [2, 3,
10], our approach to couch OO models in Z. The model rests on abstract data types
(ADTs), enabling a connection to data reﬁnement.
The sequel refers to mathematical deﬁnitions of Sect. 11, which abridge the def-
initions of the accompanying technical report [4].
2.1
The ADT Foundation
ADTs, depicted in Fig.1, are used to represent state-based abstractions made-up of
structural and dynamic parts that capture computing life-forms. They comprise a
state deﬁnition and a set of operations (Fig.1a).
Figure 1b depicts ADTs’ mathematical underpinnings. There are sets of all possi-
bletypestatesS,allpossibleenvironmentsE,allpossibleidentiﬁersI,andallpossible
objectsO(Deﬁnition1).AnADT(Deﬁnition3)isaquadrupleT = (ST, i, f , os)com-
prising a set of states ST ⊆S (the state space), an initialisation i : E ↔ST (a relation
between environment and state space), a ﬁnalisation f : ST ↔E (a relation between
state space and environment) and an indexed set of operations ops : I →ST ↔ST
(a function from operation identiﬁers to relations between states). Functions sts, ini,
ﬁn, and ops (Deﬁnition 3) extract the different ADT components (e.g. for T above,
sts T = ST).
Fig. 1 An abstract data type (ADT) (a) comprises state (SA, SB) and operations (OA, OB). Math-
ematically (b), an ADT T is made-up of a set of states ST, an initialisation i (a relation from
environments E to states ST), a ﬁnalisation f (a relation from states ST to environments E) and
operations os (an indexed set of relations between states ST)

Sound and Relaxed Behavioural Inheritance
259
2.2
Classes
ADTs lack an intrinsic identity. In the model of Fig.1b (Deﬁnition 3), two ADT
instances with the same state denote a single instance. Classes are populations of
individuals, suggesting uniquely identiﬁable individuals that retain their identity
irrespective of the state in which they are in. Z promotion [35, 39] is a modular
technique that builds ADTs for a population of individuals by promoting a local
ADT in a global state without the need to redeﬁne the encapsulated ADT; promoted
operations are framed, as only a portion of the global state changes. Figure2 depicts
classes as promoted ADTs (PADTs), which underpin the OO model presented here.
A PADT PT (Fig.2a), is made up of an inner (or local) type T that is encapsulated
and brought into a global space to make the compound (or outer) type PT. The inner
and outer type correspond to class intension and extension, respectively.
The mathematical underpinnings of a class as PADT are pictured in Fig.2b. A
class (Deﬁnition 6) is a 9-tuple C = (ci, t, os, stm, ot, c, d, ms), comprising a class
identiﬁer ci : I, an inner type t : ADT, a set of objects os ⊆O of all possible object
identities of the class, a global class state made up of an object to state mapping
stm ∈os →sts t, a typing mapping ot ∈os →I indicating the direct class of the
classes living objects, a constructor class operation c ∈nOps, a destructor class
operation d ∈dOps and class modiﬁers ms ∈uOps (see Deﬁnition 5 for nOps, dOps
and uOps). Functions icl, ity, osu, los, csts, ost, ocl, cop, dop and mops extract
information from class compounds (Deﬁnition 6) to yield: class identiﬁer (icl), inner
type (ity), universe of the class’s possible objects (osu), class’s living objects (los),
set of class states (csts), object to state mapping (ost), object to direct class mapping
(ocl), and constructor (cop), destructor (dop) and modiﬁer operations (mops).
(a) Depiction of the structure
of a promoted ADT
(b) Mathematical structure
of a class as a promoted ADT
Fig. 2 Promoted abstract data types (PADTs) (a) comprise an inner type T and global operations
(GOA, GOB) promoting inner operations (OA, OB); OO classes are PADTs here. Mathematically
(b), classes have a global function (stm) mapping class objects os (identities) to their inner states
(type T); this is the basis for constructor (c from inner initialisation), destructor (d, from inner
ﬁnalisation) and modiﬁer operations (ms from inner operations)

260
N. Amálio
2.3
Inheritance
Inheritance, pictured in Fig.3, embodies a constructive approach to build speciﬁcity
on top of commonality (Fig.3a). Its is-a semantics implies that a child is a parent
with possibly something extra (Fig.3a), which has implications at the level of inner
and outer types.
The inner type captures how the child inherits the characteristics of the parent
and adds something of its own through ADT extension (Deﬁnition 8), expressed in Z
as schema conjunction (Deﬁnition 9): C == A ∧X. Given inner ADTs C (concrete
or child) and A (abstract or parent), then C is deﬁned as being A with something
extra, X.
From a child state space it is possible to derive the parent’s (by removing what
is extra) as captured in the abstraction function ϑ (Deﬁnition 10). The mathematical
relation between classes parent Cp and child Cc, depicted in Fig.3b, rests on this ϑ
function that maps inner object states of child to the ones of parent. In all states of the
system the relation between child and parent must preserve the diagram commuting
of Fig.3b (Deﬁnition 11), which materialises is-a at the mathematical level of classes:
: at any system state a child object can be seen as a parent object.
Abstract classes (Deﬁnition 12) have no direct instances; they lack a direct exis-
tence and are used to capture general ancestors in a hierarchy, such primate whose
existence is indirectly deﬁned by the specimens of its descendants, such as human
and gorilla.
(a) Depiction of class in-
heritance
(b) Mathematical underpin-
nings of inheritance
↑⊆
Fig. 3 Class inheritance: descendants inherit their ancestor characteristics and add something of
their own. In (a), state and operations of Parent are inherited by Child, which adds SC and OC.
Mathematically (b), inheritance involves a pair of functions that preserve the class’s object to state
mappings (expressed as diagram commutativity); one function is identity (id)—child objects are a
subset of their parent objects—, the other is the abstraction function ϑ.

Sound and Relaxed Behavioural Inheritance
261
3
Behavioural Inheritance (BI) and Reﬁnement
The following investigates BI under the prism of data reﬁnement; it refers to math-
ematical deﬁnitions from Sect. 11.
3.1
Data Reﬁnement
Reﬁnement is a stepwise approach to software development, in which abstract mod-
els are increasingly reﬁned into more concrete models or programs with each step
carrying certain design decisions [38]. Data reﬁnement [22] provides a foundation
to this process through a theory that compares ADTs with respect to substitutability
and preservation of meaning.
Data reﬁnement is founded on total relations [22]; operations are relations over a
data type, programs are sequence of operations. Complete programs over a ADT start
with an initialisation, carry out operations and end with a ﬁnalisation (Deﬁnition 13).
In this setting, data reﬁnement is set inclusion: given ADTs C and A, then for all
complete programs pC and pA, with the same underlying operations over C and A
respectively, C reﬁnes A (C ⊒A) if and only if pC ⊆pA (Deﬁnition 14).
It is difﬁcult to prove reﬁnements through complete programs. In practice, reﬁne-
ment proofs resort to simulations (Fig.4) where ADTs are compared inductively [22]
through a simulation relation (R in Fig.4). For each operation in the abstract type,
there must be a corresponding operation in the concrete type. A reﬁnement is veriﬁed
by proving conjectures (or simulation rules), given in Deﬁnitions 15 for forwards (or
downwards) simulation and 16 for backwards (or upwards) simulation and which
are related to the three commutings of Fig.4. The two types of simulations are sufﬁ-
cient for reﬁnement (anything they can prove is a reﬁnement) and together they are
necessary (any reﬁnement can be proved using either one of them) [22].
In OO with design by contract [31], operations are described in terms of pre-
and post-conditions. Operations are partial relations applicable only in those ADT
states that satisfy the pre-condition (the relation’s domain). The language Z operates
in this partial setting. Reﬁnement based on total relations is adapted to Z in [39]
Fig. 4 Data reﬁnement
simulation: every step in the
concrete type is simulated by
a step in the abstract type

262
N. Amálio
Fig. 5 Contractual
totalisation of a relation [39]
by deriving simulation rules based on a totalisation of partial relations. There are
two Z reﬁnement settings [16]: non-blocking (contractual) reﬁnement interprets an
operation as a contract and so outside the precondition anything may happen, while
blocking (behavioural) reﬁnement says that outside the precondition an operation
is blocked. Figure5 gives the contractual totalisation of relation r = {a →a, a →
b, b →b, b →c} where undeﬁnedness (⊥) and all elements outside the relation’s
domain are mapped to every possible element in the target set augmented with ⊥.
This paper focuses on the contractual interpretation, the most relevant for our OO
context; [4] covers both interpretations. Simulation rules for contractual reﬁnement
are given in Facts 1 (forwards) and 2 (backwards).
3.2
BI Reﬁnement
Although developed to support reﬁnement (from abstract to concrete) or abstraction
(other way round), data reﬁnement compares data types with respect to substitutabil-
ity making it applicable to BI.
BI needs to relate the types being compared (R in Fig.4). Such a relation can
be discerned from Fig.3b by looking into how child and parent are related through
inheritance (Deﬁnition 11). This gives a basis for the BI class simulation portrayed in
Fig.6, depicting inherited child operations (icop) simulated by the parent operation
they inherit from (pcop) and child-only operations being simulated by parent step
operations. Figure6 suggests a reﬁnement relation as illustrated in Fig.3b made up
of a morphism comprising two functions: the ϑ abstraction function (Deﬁnition 10)
and identity. This hints at a modular approach for BI: we start with inner type reﬁne-
ment (class intension) through function ϑ, followed by outer type reﬁnement (class
extension), which is related to Z promotion reﬁnement [16, 29, 39].

Sound and Relaxed Behavioural Inheritance
263
Fig. 6 Behavioural inheritance class simulation. An inherited class operation icop is simulated
by a corresponding parent class operation pcop from which it inherits. There may be child only
class operations (represented as coop), which are simulated by some step operation in the parent
(pstepop)
3.3
Inner BI as ADT Extension Reﬁnement
For any classes ClA, ClC : Cl (Deﬁnition 6), such that ClC is a child of ClA (ClC
inh ClA, Deﬁnition 11), we have that inner BI equates to ADT extension reﬁnement
of the class’s inner types: ity ClC ⊒Ext ity ClA.
Two alternative extension reﬁnement settings are considered: one based on the
general function ϑ (Deﬁnition 17) and a ϑ speciﬁc to the Z schema calculus (Deﬁ-
nition 18) to cater to the ZOO approach. Simulation rules were derived with the aid
of Isabelle (see [4] for details). For backwards and forwards simulation, the rules
reduce to a single set (unlike the general case with separate rule sets)—Corollary 1
of Appendix 1.A, a consequence of Fact 3.
Let A, C : ADT be two ADTs—such as the inner ones of classes ClC and ClA
above, such that A = ity ClA and C = ity ClC—where C extends A (Deﬁnition 8). If
C and A are two Z schema ADTS, then their their relation is described by the schema
calculus formula C == A ∧X. Let A and C have initialisation schemas AI and CI,
operations AO and CO, and ﬁnalisation schemas AF and CF.3 As established by
Fact 5, C ⊒Ext A if and only if:
1. ⊢? ∀C ′ • CI ⇒AI
(Initialisation)
2. ⊢? ∀C; i? : V • pre AO ⇒pre CO
(Applicability)
3. ⊢? ∀C ′; C; i?, o! : V • pre AO ∧CO ⇒AO
(Correctness)
4. ⊢? ∀C • CF ⇒AF
(Finalisation)
The ﬁrst rule allows initialisations to be strengthened. The second rule allows the
weakening of the precondition of a concrete operation (CO). The third rule says
that the extended operation (CO) must conform to the behaviour of the base oper-
ation (AO) whenever the base operation is applicable—the postcondition may be
strengthened. The last rule allows ﬁnalisation strengthening, but reducing to true
if the ﬁnalisation is total (the ADTs lack a ﬁnalisation condition). Fact 4 captures
extension reﬁnement in the more general relational setting.
3The ﬁnalisation condition describes a condition for the deletion of objects; e.g. a bank account
may be deleted provided its balance is 0.

264
N. Amálio
3.4
Extra Operations
Reﬁnement requires that each execution step in the concrete type is simulated by
the abstract. A non-inherited operation in a child class (concrete) needs to simulate
something in the parent (abstract).
A common approach to this issue involves an abstract operation that does nothing
and changes nothing (called a stuttering or a skip operation). The proofs verify that
the new concrete operation reﬁnes skip: in the abstract type, skip does nothing;
in the concrete type, the button executes the new operation. The rules for checking
child-extra operations are obtained from the rules above by replacing AO with skip
(ΞA in Z).
3.5
Outer BI
The BI simulation rules above (Sect. 3.3) cater to the class’s inner (or local) ADT
only. In the class’s outer ADT, the concern is whether the reﬁnement proved locally
is preserved globally.
Z promotion reﬁnement relies on promotion freeness [16, 29, 39]: a class reﬁnes
another if there is a reﬁnement between the inner types and the child class is free or
unconstrained from the global state [29]. Figure7 describes freeness as a diagram
commuting: a class is free if the set of global object states (function ists) is the
same as the set of states of its inner type (function composition sts ◦ity)—as per
Deﬁnition 19. This means that the rules of Sect. 3.3 can be carried safely to contexts
in which freeness holds (Deﬁnition 7)
Fig. 7 Class (or promotion)
freeness

Sound and Relaxed Behavioural Inheritance
265
4
The ZOO Model of Queues
Figure8
presents the running
example
of a hierarchy
of queues.
Class
QueueManager holds an indexed set of queues (HasQueues); the hierarchy is as
follows:
• Abstract class Queue holds a sequence of items. It has two operations: join
adds an element to the queue, and leave removes the queue’s head.
• Class BQueue (bounded queue) bounds the size of the queue.
• Class PBQueue (privileged-bounded queue) reserves the last place in the queue
to some privileged item.
• Class RBQueue (resettable-bounded queue) adds operation reset to empty the
queue.
• Class JQBQueue (jump-the-bounded-queue) add an extra behaviour to operation
join: the item taking the queue’s last place jumps the queue.
• Class RABQueue (resettable-abandonable-bounded-queue) adds abandon
enabling any element to leave the queue irrespective of its position.
The following presents excerpts of the ZOO model formalising the class diagram
of Fig.8. The complete model is given in [4]. Further information on ZOO can be
obtained from [2–4, 10].
4.1
ZOO Model Excerpt: Inner ADTs
Inner ADT of class Queue holds a sequence of items, which is initially empty.
Operation join receives an item and adds it to the back of the sequence. Operation
leave removes and outputs the sequence’s head.
Fig. 8 A UML class diagram describing an inheritance hierarchy of queues made-up of
classes Queue, BQueue (bounded-queue), RBQueue (resettable-bounded-queue), RABQueue
(resettable-abandonable-bounded-queue)

266
N. Amálio
Queue[Item]
items : seq Item
QueueInit[Item]
Queue[Item] ′
items′ = ⟨⟩
QueueJoin[Item]
ΔQueue[Item]
item? : Item
items′ = items ⌢⟨item?⟩
QueueLeave[Item]
ΔQueue[Item]; item! : Item
items ̸= ⟨⟩∧item! = head items
items′ = tail items
BQueue extends Queue by bounding the queue with constant maxQ.
maxQ : N1
BQueue[Item]
Queue[Item]
# items ≤maxQ
BQueueInit[Item]
BQueue[Item] ′
QueueInit[Item]
BQueueLeave[Item]
ΔBQueue[Item]
QueueLeave[Item]
BQueueJoin[Item]
ΔBQueue[Item]
QueueJoin[Item]
RBQueue extends BQueue; extra operation Reset empties the queue.
RBQueue[Item]
BQueue[Item]
RBQueueReset[Item]
ΔRBQueue[Item]
items′ = ⟨⟩
PBQueue extends BQueue by adding a set of privileged items, set at ini-
tialisation, and reserving the last place in the sequence to such an item.
PBQueue[Item]
BQueue[Item]
privileged : P1 Item
# items = maxQ ⇒last items ∈privileged
PBQueueInit[Item]
PBQueue[Item] ′
BQueueInit[Item]
privileged? : P1 Item
privileged′ = privileged?

Sound and Relaxed Behavioural Inheritance
267
JQBQueue slightly modiﬁes operation join inherited from BQueue: the item
occupying the last place left in the queue is placed at the queue’s head.
JQBQueue[Item]
BQueue[Item]
JQBQueueInit[Item]
JQBQueue[Item] ′
BQueueInit[Item]
JQBQueueLeave[Item]
ΔJQBQueue[Item]
BQueueLeave[Item]
JQBQueueJoin[Item]
ΔPBQueue[Item]; item? : Item
# items < maxQ −1 ⇒BQueueJoin[Item]
# items = maxQ −1 ⇒items′ = ⟨item?⟩⌢items
RABQueue extends RBQueue by adding abandon, allowing elements to leave
the queue no matter their position.
RABQueue[Item]
RBQueue[Item]
RABQueueInit[Item]
RABQueue[Item] ′
BQueueInit[Item]
ABQueueLeave[Item]
ΔRABQueue[Item]
RBQueueLeave[Item]
ABQueueJoin[Item]
ΔRABQueue[Item]
RBQueueJoin[Item]
RABQueueReset[Item]
ΔRABQueue[Item]; RBQueueReset[Item]
RABQueueAbandon[Item]
ΔRABQueue[Item]; item? : Item
∃q1, q2 : seq Item • items = q1 ⌢⟨item?⟩⌢q2 ∧items′ = q1 ⌢q2
4.2
Global Properties
Class extensions are obtained by instantiating the SCl Z generic (see [10]). State
extensions of Queue, BQueue, and RBQueue are:
SQueue[Item] == SCl[O QueueCl, Queue[Item]][stQueue/oSt]
SBQueue[Item] == SCl[O BQueueCl, BQueue[Item]][stBQueue/oSt]

268
N. Amálio
SRBQueue[Item] == SCl[O RBQueueCl, RBQueue[Item]][stRBQueue/oSt]
Extension initialisations say that classes have no living instances:
SQueueInit[Item] == [SQueue[Item] ′ | stQueue′ = ∅]
SBQueueInit[Item] == [ SBQueue[Item] ′ | stBQueue′ = ∅]
SRBQueueInit[Item] == [ SRBQueue[Item] ′ | stRBQueue′ = ∅]
Association HasQueues is represented as a function relating QueueManager
objects with sets of Queues indexed by set QId (the queue identiﬁer).
AHasQueues
rHasQueues : O QueueManagerCl →(QId →O QueueCl)
The next invariant says that the RBQueue instances held by a QueueManager
must have queues of size at most 5.
RBQueuesInHasQueuesSizeLeq5[Item]
SystemGblSt[Item]
∀oqm : O QueueManagerCl; rq : O RBQueueCl | oqm ∈dom rHasQueues ·
rq ∈(ran (rHasQueues oqm)) ∩sRBQueue ⇒#(stRBQueue abq).items ≤5
5
The Reﬁnement Straight-Jacket and Some Loopholes
The BI proof rules derived in Sect. 3 are over-restrictive. Trivial inheritance hierar-
chies, such as the queues example of Fig.8, fail to be pure BIs. Furthermore, the rules
may be misleading as inner BI does not entail overall BI when global constraints
invalidate what is proved locally (Sect. 3.5).
Table1 summarises the BI analysis for the example of Fig. 8. The next sections
discuss the four issues that emerged.
5.1
Applicability
In Fig.8, class BQueue fails to reﬁne Queue and PBQueue fails to reﬁne BQueue;
applicability fails for operation join on both accounts:

Sound and Relaxed Behavioural Inheritance
269
Table 1 Results of the BI analysis of the queues example of Fig.8
Relevant outcome
Issue
BQueue.join
Applicability proof fails
Applicability
PBQueue.join
Applicability proof fails
Applicability
RBQueue.reset
Does not reﬁne skip
(ΞBQueue)
Reﬁnement of skip
ABQueue.abandon
Does not reﬁne skip
(ΞBQueue)
Reﬁnement of skip
JQBQueue.join
Correctness proof fails
Operation overriding
QueueManager
Global invariant breaches
freeness assumption
Global interference
• Precondition of Queue.join is true, whilst that of BQueue.join is # items <
maxQ. The former does not imply the latter and so applicability fails.
• Precondition of PBQueue.join includes # items = maxQ −1 ⇒item? ∈
privileged, which does not imply precondition of BQueue.join.
In reﬁnement, the concrete type may weaken the precondition; here, the subclass
preconditions are stronger.
These failures happen because the concrete operations strengthen the inherited
pre-condition, violating substitutability as the behaviour becomes observably differ-
ent when the concrete type is used in place of the abstract one. Suppose a braking
system of a car; the abstract type says “upon brake slow down” (precondition true),
and the concrete type says, “upon brake slow down when speed is less than 160 km
per hour” (precondition speed <160)—substitutability cannot possibly hold when
pre-conditions are strengthened.
5.2
Reﬁnement of Skip
InnerBIforRBQueueandABQueuealsofailwithoperationsresetandabandon
failing to correctness-reﬁne skip because both breach skip’s (ΞBQueue) con-
straint saying that inherited state remains unaltered.
5.3
Operation Overriding
The BI proofs for JQBQueue.join, which overrides BQueue.join, also fail.
The correctness conjecture cannot be proved as JQBQueue.join changes the
inherited post-condition.

270
N. Amálio
5.4
Global Interference
The inner BI rules of Sect. 3.3 have a local scope (Sect. 3.5, Fact 6). They can safely
be used in contexts where objects of some class hierarchy are not constrained by
the environment, following from the freeness rule of promotion reﬁnement. How-
ever, when freeness is breached, BI checks become severely complicated due to the
complexity of global proofs.
Global constraint RBQueuesInHasQueuesSizeLeq5 (Sect. 4.2) illustrates
this issue. For any client of QueueManager that uses its queues, the behaviour
of instances of RBQueue and its descendants are observably different from the
remainingqueues. Inthat context, freeness is breachedandthelocallyprovedinner BI
no longer holds globally. This global interference is due to divergence of RBQueue
with respect to the behaviour of other classes in the hierarchy, such as, BQueue,
PBQueue and JQBQueue. Suppose that we create, using some QueueManager,
objects oBQ of class BQueue and oRABQ of class RABQueue (initially, both queues
are empty). If we execute operation join ﬁve times on them, the observed behaviour
is the same. However, a sixth call to join on oBQ allows an item to be added to
the sequence, but fails on oRABQ because the precondition is breached (the queue
already holds ﬁve items); substitutability is violated: oRABQ cannot replace of oBQ.
6
A BI Examination
Apart from the insufﬁciently discussed global interference, the problems of the previ-
ous section are acknowledged in Liskov and Wing’s seminal paper [28]. This section
investigates BI in OO programming languages that support design by contract (DbC),
namely: JML [15], Eiffel [33] and Spec♯[27]. DbC languages are more formal and
they support BI following [28], overcoming BI’s restrictions through relaxations
known as speciﬁcation inheritance [17].
The next sections present (a) BI’s support in examined languages, (b) the exami-
nation results, and (c) an appraisal of speciﬁcation inheritance [17].
6.1
BI in DbC Languages
DbC languages express invariants, contracts made of a pre- and a post-condition and
various code checks in the form of assertions. Assertions are used for both static-
and run-time veriﬁcation; the former involves theorem proving and aims to prevent
run-time errors. JML is a satellite Java language; assertions are written as program
annotations. Eiffel and Spec♯make assertion speciﬁcation an integral part of the
language.

Sound and Relaxed Behavioural Inheritance
271
Fig. 9 A simple Counter class in Eiffel, Java/JML and Spec♯
Figure9 presents a Counter class in Eiffel (Fig.9a), Java/JML (Fig.9b) and
Spec♯(Fig.9a). The Eiffel assertions of Fig.9a are included in the require (pre-
condition) and ensure (post-condition) clauses. The JML speciﬁcations of Fig.9b
start with an @ symbol; the predicates coming after requires and ensures
denote pre- and post-conditions, respectively. Spec♯uses the same clauses to denote
contracts (Fig.9b).
In DbC languages, both contracts and invariants are inherited following speciﬁ-
cation inheritance [17]. This is based on two principles of reﬁnement: weakening
of precondition and strengthening of postcondition. Inherited operations may be
extended or overridden and the accompanying contracts may be combined in the
following ways:
• Disjunction of pre-condition of parent and child.
• Conjunction of post-condition of parent and child.
This enables contract extension. JML provides the also clause to deﬁne new pre-
and post-condition pairs; Eiffel provides require else and ensure then to
achieve the same effect. Spec♯disallows pre-condition extension, however, child
invariants may result in extra inherited preconditions; new post-conditions may be
added (strengthening) using the usual requires clause.

272
N. Amálio
Fig. 10 A UML class diagram of an inheritance hierarchy of queues of natural numbers
Given a superclass A (abstract), a subclass C (concrete) and an operation Op of
A specialised in C, the pre-condition of Op in C is:
pre A.Op ∨pre C.Op
The languages differ slightly in the way the subclass post-condition is constructed.
Eiffel and Spec♯use a simple conjunction [33]:
post A.Op ∧post C.Op
JML uses a conjunction of implications [17, 26]:
pre A.Op ⇒post A.Op ∧pre C.Op ⇒post C.Op
6.2
The BI Examination Proper
The BI examination uses the queues example of Fig.10, a slight variation from Fig.8
speciﬁed in Z in Sect. 4, with the following changes:
• Generic Item is removed as generics are not supported in JML and Spec♯.
• To avoid complications with unbounded queues (in machines things are always
bounded), Queue is removed and BQueue is made the root of the hierarchy.
Table2 presents the tool versions used in the examination carried out in January
2014: version 5.6 of JML compiler (part of the JML tools) and version 2.0.5 of
ESC/Java,4 which, at the time the examination took place, were more stable than the
4JML tools are available from http://bit.ly/1aYSdHZ and ESC/Java 2 from http://bit.ly/1a4VTUS.

Sound and Relaxed Behavioural Inheritance
273
Table 2 Tool versions used in the BI examination of Java/JML, Eiffel and Spec♯. (Abbreviations:
RAC = run-time assertion checking)
Java/JML
Eiffel
Spec♯
RAC
JML version 5.6 rc4
Eiffel studio 7.3
Spec♯compiler version
1.0.211.26.0 on
Microsoft Visual
Studio 2010
Proof
ESC/Java 2.0.5,
simplify theorem
prover
N/A
Boogie with theorem
prover Z3 version 2.15
(as required by the
used version of Spec♯)
Table 3 Results of the BI examination in JML, Eiffel and Spec♯using static veriﬁcation (proof) and
runtime assertion checking. Each cell can have the values no error (no error raised or signalled),
error raised and N/A (not applicable, because test could not be run). (Green or medium gray =
correct. Yellow or light gray = Requiring attention. Red or dark gray = Incorrect.)
Java/JML
Eiffel
Spec #
Proof
Runtime
Proof
Runtime
Proof
Runtime
PBQueueN.join
no error
no error
N/A
error
no error
error
JQBQueueN.join
error
error
N/A
error
error
error
RBQueueN.reset
no error
no error
N/A
no error
no error
no error
RABQueueN.abandon
no error
no error
N/A
no error
no error
no error
QueueManager
no error
N/A
N/A
error
no error
no error
newer open JML. The Eiffel part used release 7.3 of Eiffel studio5 and Eiffel’s static
veriﬁcation.6
The code implementing the diagram of Fig.10 is given in [4], using an implemen-
tation of queues as circular arrays. The examinations consisted of:
• Static assertion veriﬁcation using theorem proving.
• Runtime assertion checking (RAC) by running code tests.
Table3 summarises the results, indicating, for each test, whether an error was
raised or not. The colouring sets this paper’s expectation: green (or medium gray)
indicates that the result is seen as correct, red (or dark gray) denotes incorrect, and
yellow (or light gray) says that the marked issue deserves further attention. The
examination focused on BI’s critical points discussed in Sect. 5, namely:
• Applicability reﬁnement was examined through class PBQueueN, its privileged
items invariant, and the operation join. The results are inconsistent. JML does
5Available from http://bit.ly/1iTH8fn.
6The examination used the research version of Eiffel veriﬁcation environment (EVE), available
from http://bit.ly/1g7avuS, which includes the auto-proof tool [36] that uses Boogie and the Z3
theorem prover as the veriﬁcation back-end. Despite many efforts, all attempts to try EVE have
failed; EVE’s team was contacted but the issue was not solved.

274
N. Amálio
not signal any errors (statically or at runtime); at run-time, both Eiffel and Spec♯
raise pre-condition violation exceptions; Spec♯’s static checking does not signal
any error.7
• Subclass extra operations altering inherited state is exercised through operations
RBQueueN.reset and RABQueueN.abandon. These are not signalled as
errors in any of the examined languages.
• Operation overriding is exercised through operation JQBQueueN.join; errors
are signalled by all examined languages both statically and at runtime.
• Global interference is exercised here through the class QueueManager and its
invariant. The static checks do not raise any errors or warnings. Eiffel raises an
invariant violation exception at run-time. The runtime test could not be run in JML;
Spec♯does not raise any error.
As mentioned, the DbC languages make certain relaxations to the rules of Liskov
and Wing [28]. This is why the results of Table 3 differ from those of Table 1, which
emerge from the strictest setting. This paper’s position concerning the results of
Table 3 is as follows:
• A striking difference from Table 1 is that the reﬁnements involving the subclass
extra operations (RBQueueN.reset and RABQueueN.abandon) are deemed
valid by the DbC languages. This is because the DbC languages use a relaxation
that is proved in the next section.
• The results involving JQBQueueN.join are consistent with those of table 1: it
is not a reﬁnement and rightly so.
• It is erroneous to deem PBQueueN.join as valid, as conﬁrmed statically in JML
and Spec♯and at runtime in JML, as it is not a reﬁnement. This is an issue that
stems from the pre-condition rule of speciﬁcation inheritance [17]. It is this paper’s
position that an error should be signalled; only the Eiffel and Spec♯runtime checks
are correct. In PBQueueN queues, the last place in the queue must be occupied by
a privileged item, implying that the subclass precondition is strengthened which
causes divergence: when there is only one place left in the queue the superclass
allows any item to be added, whereas the subclass (PBQueueN) only allows priv-
ileged items. The reﬁnement static checks based on proof ignore this problem; it is
only Eiffel and Spec♯that correctly signal this problem at runtime. This highlights
an inconsistency: the static checks guarantee absence of such runtime errors, but
the tests witness such errors on Eiffel and Spec♯.
• Static veriﬁcation ignores global interference. Locally-proved BI checks are
context-dependent; the user should be informed about contexts that may inval-
idate the local check. No such warnings were provided by the static checks, but
errors were observed at runtime (as invariant violation exceptions) in Eiffel. Spec♯
does not raise an exception. Compilation errors prevented the execution of the test
in JML.
7Spec♯does not allow preconditions to be added to inherited operations; however, the precondition
of the inherited operation PBQueueN.join could be specialised through PBQueueN’s invariant.

Sound and Relaxed Behavioural Inheritance
275
The next section looks into speciﬁcation inheritance [17] to investigate the prob-
lem with the pre-condition rule observed in the examination.
6.3
A Critique of speciﬁcation inheritance [17]
The rules of speciﬁcation inheritance are as follows: (a) pre-condition of parent
and child are combined using disjunction (weakening), (b) post-condition of parent
and child are combined using conjunction (strengthening). This is contrived; the
precondition rule implies that applicability is always true, which may not reﬂect
what the actual precondition says. In the braking example given above—“upon brake
slow down” (parent precondition, true) and “upon brake slow down when speed is
less than 160km/h” (child precondition speed <160)—, the subclass precondition
is effectively strengthened, but this is not reﬂected in the proof which always yields
true and should not because the inherited precondition is being strengthened.
7
Relaxing the Reﬁnement Constraints
There are two ways to address the reﬁnement restrictions: (a) we live with the con-
straints and refactor the OO models to conform to them, or (b) we relax the restric-
tions. Refactoring seeks to change a model while preserving its meaning and is always
a useful remedy. However, in this example, the sole use of refactoring would mean
giving up on inheritance as all classes would have to be merged into a single one.
Operations reset and abandon change abstract state, so they need moving in to
the superclass; BQueue’s special behaviour requires moving into the superclass;
overall, we achieve a valid refactoring at the cost of inheritance’s modularity.
Classical reﬁnement, established to cater to stepwise software development,
requires that programs or concrete models conform to the more abstract speciﬁca-
tions to ensure that vital aspects captured by the abstraction are fulﬁlled by the more
concrete abstraction levels. The relaxations that follow liberate BI by challenging
assumptions of reﬁnement. The sequel introduces virtual operations which underpin
all three relaxations, followed by an explanation on the particularities of OO abstract
classes that are exploited for relaxations before explaining each relaxation.
7.1
Virtual Operations
The remedy that counters all identiﬁed BI malaises (Sect. 5) is the notion of virtual
operation, depicted in Fig.11. An operation is virtual if it is deﬁned in the class’s
inner type but it is not available to the environment. In Fig.11, inner operations OC
and OD are virtual because they are not promoted (Deﬁnition 20).

276
N. Amálio
Fig. 11 Virtual Operations.
Inner operations OC and OD
are not promoted and
invisible to the class’s
environment—they are
virtual
Virtual operations provide the right conditions to lift the applicability reﬁnement
proof obligation. There is no need to comply with the circumstances in which the
abstract button may be pressed because it cannot be pressed (it is invisible); correct-
ness, however, remains as the concrete operation needs to comply with the effects of
the abstract operation. Formally, any inner operation co extension-reﬁnes a virtual
operation ao (co ⊒Ext ao) if and only if reﬁnement-correctness holds—applicability
is dropped (Deﬁnition 21).
7.2
The Particularities of OO Abstract Classes
Certain relaxations exploit the particularities of OO abstract classes,8 which lack
a direct existence as all their instances directly belong to their descendant classes
(Deﬁnition 12). Operations of an abstract class are inherently virtual as they cannot
possibly be executed; they have two purposes: (i) set a template model of behaviour to
be varied and specialised by descendants without a commitment to the environment,
and (ii) provide polymorphism in the outer view by offering a multitude of possible
descendant behaviours chosen dependently on the class of the object on which the
operation is called. Figure12 uses the button analogy (rounded-rectangles) to contrast
inheritance of non-abstract and abstract classes; the former (Fig.12a) have an actual
existence determined by their direct living instances with their buttons being pushable
from the environment; abstract parents (Fig.12b), on the other hand, have an indirect
existence determined by the living instances of their progeny with their environment-
hidden buttons acting as templates to be specialised and elaborated by child classes
in the inner view, and those environment-exposed buttons being polymorphic in the
outer view.
8not to be confused with a class that is abstract in the context of formal reﬁnement!.

Sound and Relaxed Behavioural Inheritance
277
(a) Class inheritance of a
non-abstract parent
(b) Class inheritance of an
abstract parent
Fig. 12 Inheritance of non-abstract (a) and abstract classes (b)
7.3
The Child-Extra Operations Relaxation
Subclasses may have extra (or non-inherited) operations. The classical approach to
this problem involves a skip parent operation doing nothing (Sect. 5) and ensuring
substitutability: the skip button does nothing with the concrete button doing some-
thing but respecting skip. This is too restrictive (Sect. 5) as it implies that inherited
state cannot be altered.
The relaxation proposed here replaces skip with an operation that simulates
the concrete operation in the abstract world. This operation is virtual as it is not
expected by the environment. For example, when RBQueue is used when a BQueue
is expected, all is needed are operations join and leave, the simulating substitute
of reset is invisible to the environment.
Virtual simulating operations are constructed from subclass operations. Given a
subclass operation co (concrete) and the BI reﬁnement function ϑ (Deﬁnition 10),
the required simulating abstract operation is (Deﬁnition 22):
ao∅= ϑ ∼co  ϑ
Any concrete operation co extension-reﬁnes its corresponding abstract virtual opera-
tion ao∅—co ⊒Ext ao∅by Fact 7. Correctness was proved for all cases in the Isabelle
proof assistant (further details in [4]) and as ao∅is virtual there is no need to prove
applicability. Hence, child extra operations can be added freely: for any concrete
operation co there is always an abstract virtual operation ao∅that simulates it!

278
N. Amálio
7.4
The Abstract Class Relaxation
This relaxation exploits the inner facet of abstract class operations discussed above
(Sect. 7.2), namely the fact that internally such operations are inherently virtual (Def-
inition 20). The relaxation stipulates that the inner BI checks for inherited operations
of an abstract class require the correctness proof only as applicability proofs are lifted
because the operations are virtual (Deﬁnition 23).
7.5
The Soft Parent Relaxation
This relaxation builds up on the abstract class relaxation explained above. It tackles
the problem of divergent subclass behaviour, which the abstract class relaxation also
tackles, but avoids the need for abstract classes. The soft parent relaxation (Fig.13)
should be used whenever we need divergent subclass behaviour and a non abstract
parent, exempliﬁed in Fig.13a by classes ClB and ClA respectively; the assertion that
the parent is soft (Fig.13b) results in an underpinning conﬁguration with a virtual
abstract class (VClA), an abstraction of the parent class ClA, subclassed by both ClB
and ClA (Fig.13c).
The relaxation introduces a virtual abstract superclass behind the scenes, an
abstraction of the soft parent, to exploit the divergence offered by abstract classes
(Fig.13c). It should be used whenever we need both (i) subclass divergent behaviour,
and (ii) parent instantiability. Due to unwanted divergence, it should be used with
care. It cannot be applied when the parent is a child of a hard parent (neither abstract
nor soft).
7.6
The Inheritance Freeness Relaxation
Global interference occurs whenever global constraints cause divergence between
the behaviour of objects in the global space and what would be observed in a locally-
conﬁned space. Inheritance-divergence concerns the behaviour of child objects
Fig. 13 The soft parent
relaxation

Sound and Relaxed Behavioural Inheritance
279
Fig. 14 Inheritance freeness
becoming observably different from their parent counter-parts. Global interfer-
ence can be seen as the often inevitable effect of the environment upon the living
objects that inhabit it. If inheritance-divergence is caused by global interference,
locally proved BIs do not hold globally. The classical condition ensuring global BI-
preservation is class freeness (Fig.7, Deﬁnition 19), which is breached whenever
global constraints affect the local spaces of classes. As class freeness is an overkill
(Fact 6), this paper proposes the inheritance freeness relaxation.
Inheritance freeness stipulates that child classes should not be more globally con-
strained than their hard parents (those neither abstract nor soft) to prevent unwanted
divergence. This covers two cases: (i) the parent is either abstract or soft and the child
is directly affected by global constraints with inheritance divergence being allowed
because the parent operation in the outer view is polymorphic; (ii) the parent is hard
and all relevant global constraints are expressed in terms of the parent, which implies
absence from inheritance divergence because global constraints affect equally both
parent and descendants. The relaxation relies on a more relaxed freeness condition,
which widens the range of situations in which BIs hold globally as captured by the
commuting in Fig.14 (Deﬁnition 24), resulting in the following equations:
α r xs = r  xs 
((α ϑ) ◦ists) ClC = ists ClA ∩((α ϑ) ◦sts ◦ists) ClC
Above, function α applies the relation image to the given relation; it is used to take
the relation image of function ϑ (Deﬁnition 10). The left-hand side of the equation
obtains the set of object states of ClC and casts them to A using ϑ. The right-hand
side obtains the unconstrained objects states and casts them to the abstract inner type
A to yield the intersection with the allowed object states of ClA. Overall, it says that
the set of allowed objects states of ClC in ClA must be the same as the set of allowed
object states in ClC.
Hence, applicability-relaxed BI (Deﬁnition 23) is elaborated to arrive at a relaxed
BI deﬁnition which replaces class freeness with inheritance freeness (Deﬁnition 25):
vos = vopsids ClA ◁((ops ◦ity)ClA)
ClC ⊒BI ClA ⇔ity ClC ⊒Ext ity ClA ▷◁vos ∧ClA Inhfree ClC

280
N. Amálio
This relaxation implies two things: (a) children may diverge from non-hard par-
ents; (b) if the parent is hard, then inner children may not be directly affected by
global constraints. This is translated into a design guideline:
Global Invariants constraining the local deﬁnitions of classes that are part of an
inheritance hierarchy, should be formulated in terms of classes without hard
parents (neither abstract nor soft).
In the queues example of Fig.8 (p. 11), any invariants affecting the local deﬁnitions
of RBQueue and RABQueue should be stated in terms of the abstract Queue or
RBQueue, the uppermost non-abstract class.
7.7
Queues Revisited
The relaxations above would render the queues example of Fig.8 BI-conformant,
albeit with a refactoring. Figure15 presents the refactored class model, which is as
follows:
• Abstract class Sequence, the root of the hierarchy, provides a non-deterministic
join: an item can be added to either the sequence’s front or back. Operation
Sequence.leave does de-queuing (head of sequence is removed from it).
Abstract class BSequence puts a bound on the sequence. These classes accom-
modate JQBQueue and its peculiar queue-jumping behaviour which caused
Fig. 15 A UML class diagram of an inheritance hierarchy of queues, resulting from a refactoring
of the paper’s running example given in Fig.8

Sound and Relaxed Behavioural Inheritance
281
correctness-reﬁnement to fail. This paper offers no remedy other than refac-
toring for correctness-reﬁnement malaises. The refactoring together with the
abstract class relaxation (Sect. 7.4) would entail: JQBQueue ⊒BI BSequence ⊒BI
Sequence.
• Class BQueue restores the normal queue behaviour with elements added to the
back of the sequence, and BQueue ⊒BI BSequence as BSequence is abstract. The
soft parent relaxation would entail PBQueue ⊒BI BQueue and retain BQueue’s
instantiability (soft annotation indicates that BQueue is a soft parent of
PBQueue).
• The child-extra operations relaxation (Sect. 7.3) implies RBQueue ⊒BI BQueue
and RABQueue ⊒BI RBQueue. BQueue provides only two operations, join and
leave;
this
relaxation
allows
the
addition
of
RBQueue.reset
and
ABQueue.abandon without any proof obligations.
• The inheritance freeness relaxation resolves the global interference issue as
the globally interfering constraint is being stated in terms of RBQueue, which
descends from a non-hard parent.
8
Discussion
BI relaxations. With a pair of spectacles focussed on rigour and correctness we see
that inheritance induces reﬁnement relations. With another pair focussed on prac-
tice and expressibility, we see how reﬁnement’s restrictions impair inheritance. The
queues running example (Fig.8) shows how trivial inheritance hierarchies fail to be
reﬁnements in the strictest sense.9 Section5 identiﬁed the hurdles faced when prov-
ing BI. This paper conciliates correctness with both ﬂexibility and expressibility by
proposing four relaxations that offer BI without sacriﬁces to inheritance’s ﬂexibility,
reuse or capacity for incremental deﬁnition.
Virtual operations are the key remedy against applicability issues, which is what
lies beneath most reﬁnement hurdles highlighted in Sect. 5. Virtual operations belong
to the class’s inner type but are invisible to the environment. Because they are never
executed, the reﬁnement applicability constraint may be lifted—the child operation
no longer needs to be applicable whenever the parent operation is also applicable—,
implying that child preconditions may be narrowed.
The four relaxations are as follows:
• The child extra operations relaxation allows the addition of extra subclass opera-
tions without any proof obligations. For any concrete child extra operation, it is
possible to ﬁnd an abstract simulating operation in the parent, which is virtual and
therefore free from applicability restrictions and which always correctness-reﬁne
the simulating operation.
9The subclassing of an unbounded by a bounded queue is a common specialisation that is not a
classical reﬁnement. In general, a bounded type does not reﬁne an unbounded one.

282
N. Amálio
• The OO abstract class relaxation caters to subclass behavioural diversity. Because
abstract class operations are virtual—an abstract class has no direct instances,
implying that its inner operations are invisible to the environment—, applicability
is lifted and all that is required to prove is correctness. This relaxation is consistent
with the view that abstract classes are ﬂexible templates with their objects being
polymorphic, meaning they are allowed to have a multitude of slightly diverging
behaviours. As the queues model and other models in [2] show, with due caution,
this relaxation is extremely useful whenever precondition narrowing is required;
it enables OO inheritance designs that are ﬂexible, make use of polymorphism
and preserve semantic behaviour, enabling a whole range of behavioural diver-
sity whereby the conditions for triggering specialised operations may vary freely.
For example, suppose we capture mating as a means of sexual reproduction in
mammals; abstract class Mammal captures the general notion of mating; the exact
circumstances in which this occurs would differ from species to species—the mat-
ing circumstances of primates being different from the cetaceans with possibly
further diversity at the different sub-species of primate—, however, sub-species
must comply with the general mammal notion of mating if there is one.
• The soft parent relaxation caters to both behavioural diversity and reuse. A soft
parent entails a virtual abstract class (lacks a direct existence), an abstraction of
the soft parent; the parent and all its soft children become children of the virtual
abstract class, enabling the abstract class relaxation. It should be used whenever
we need more liberal diversity than the one provided by classical (hard) inheritance
and, for the sake of reuse and instantiability, we need to avoid making the parent
abstract. When going from the queues example of Fig.8 into the BI-compliant
model of Fig.15, this relaxation kept BQueue’s instantiability and accommodated
the peculiar PBQueue, which in Fig.15 was marked as a soft descendant.
• The inheritance-freeness relaxation tackles global interference. As the queues
example highlights, the outside world may constrain the internal states of classes,
invalidating locally proved BIs. This relaxation widens the range of situations in
which proved BIs hold globally improving upon promotion (or class) freeness [29].
Inheritance-freeness requires child classes not to be more globally constrained
than their hard parents (neither abstract nor soft). It results in a design guideline:
global constraints affecting the inner states of classes should not be stated in terms
of instantiable classes with hard parents. For example, suppose two sub-species
of human and an environment that constrains one of these sub-species to the point
that the characteristic human up-right walking is severely limited; in this context,
any locally proved BI for walking would only hold in this environment by appeal
to this relaxation if class human is made abstract, the sub-species are made soft
descendants, or the environment constraint is stated in terms of class human. The
paper justiﬁes this relaxation using formal-based argumentation. When this relax-
ation is not applicable there is not a practical way to verify BI; global reﬁnement
proofs are very complicated even in small systems.
The paper’s queues example and other examples from [2] combine refactoring
with the paper’s relaxations to construct BI hierarchies. All BI conjectures of the

Sound and Relaxed Behavioural Inheritance
283
queues example were proved in Z/Eves. Usually, proofs at the level of local types
are trivial; most of them are automatically provable in Z/Eves.
BI in design by contract languages. The BI examination of Java/JML, Eiffel and
Spec♯(Sect. 6) draws attention to the following:
• The precondition rule is a problem in all examined languages (originating from
speciﬁcation inheritance [17]). The examination highlighted inconsistencies
between static and runtime veriﬁcation for a specialised operation that narrows
the precondition. The static checks concluded that there were no problems with
the pre-condition of operation PBQUeueN.join, but the runtime tests triggered
pre-condition violations in Eiffel and Spec♯. This is inconsistent; the static checks
are there to ensure that such errors do not occur at runtime. A thorough analysis
revealed a problem with contract inheritance [17], causing discrepancies between
contract and code.
• The examined languages ignore global interference. The paper shows how locally
-proved BIs fail to be preserved in all global contexts. DbC languages should warn
users of such problems because they may breach substitutability and, hence, cause
unexpected run-time errors.
It is interesting to compare the examined languages. Eiffel was the best in runtime
veriﬁcation, but its static veriﬁcation approach could not be examined as it was being
researched at the time of writing and was not part of Eiffel’s ofﬁcial release; all
efforts to try Eiffel’s veriﬁcation environment failed. Spec♯was the best language
in its support for static veriﬁcation with good back-end theorem proving based on
Boogie and Z3; however, some novel features of Spec♯, tuned for veriﬁcation, appear
to be non-intuitive.
On reﬁnement. The reﬁnement theory founded on total relations [22] is simple and
intuitive, but rests on an assumption not applicable to all computing settings: that
computing behaviours can be captured by a total relation. Once we cater to partial
behaviours the rules become more complicated and they bring out the interesting
applicability condition. A large part of the reﬁnement work presented here involves
a careful study of applicability to understand the circumstances that allow this rule
to be lifted.
This work helps to clarify the relation between various concepts that have distinct
designations in the literature, such as, behavioural subtyping, behavioural inheri-
tance, data reﬁnement, class reﬁnement and promotion reﬁnement. The original con-
cept of behavioural subtyping equates to data reﬁnement in the OO setting, where an
arbitrary reﬁnement relation is allowed. Class reﬁnement extends ADT-based data
reﬁnement to classes, where class reﬁnement equates to Z promotion reﬁnement.
Behavioural inheritance is just one speciﬁc class reﬁnement because the reﬁnement
relation is ﬁxed.

284
N. Amálio
On OO Inheritance. This paper proposes soft inheritance as opposed to the classical
hard inheritance to help deﬁning BI hierarchies. Languages concerned with BI could
consider the soft annotation to say that particular inheritances are soft (some degree
of inheritance-divergence is allowed).
9
Related Work
This paper extends the work presented in [2, 3], elaborating and reinforcing the
paper’s BI relaxations. This extension has four key components:
• The abstract model of OO of Sect. 2 results from the insight gained from ZOO, a Z
style of OO [2, 3, 10] that builds-up on Hall’s work [19, 20]. It represents a class as
two ADTs, making it consistent with models of OO programming languages with a
formal semantics (like those based on design-by-contract, such as Eiffel, JML and
Spec♯); Meyer [32] sees the OO paradigm founded on ADTs with classes having a
type view and a module view, which correspond to the inner and outer ADTs that
make-up a class in the paper’s OO model. This model clearly frames OO within
data reﬁnement, contributing to the generalisability of the paper’s results, which
go beyond Z or ZOO.10
• The effort on mechanical veriﬁcation with the Isabelle theorem prover provided
insight and feedback, which helped o elaborate the BI relaxations, and made the
proof effort more reliable by diminishing the possibility of human error through
the use of state of the art proof technology. The derivations of the BI reﬁnement
rules have been proved in Isabelle; the child-extra operations relaxation has been
proved in Isabelle also.
• The novel soft parent relaxation and the accompanying notion of soft inheritance.
• The paper’s BI examination of the three design by contract (DbC) languages, JML,
Eiffel and Spec♯, which is entirely new. It highlights a problem in the precondition
rule that stems from [17].
The paper addressees the tension between the constraints of formal reﬁnement
and the practical needs of software engineering [13]. Retrenchment [13, 14] is a more
liberal formal-reﬁnement approach that tackles this problem. This liberalisation idea
drives the paper’s relaxations. However, the paper’s approach to relaxation differs
from retrenchment; whilst retrenchment provides conjectures that give room for
narrowing the pre-condition and widening the post-condition, this paper follows the
reﬁnement tradition of deriving rules from a more general setting, which are then
further analysed with respect to substitutability to produce relaxations.
Dhara and Leavens [17] used subclass extra operations BI relaxation. However,
thisrelaxationisneitherprovednorjustiﬁedonformalgroundsin[17].Totheauthor’s
10The paper’s model, differs from ﬁrst-order models, such as Alloy’s [25], which represents class
attributes as relations with the resulting models being global and ﬂat.

Sound and Relaxed Behavioural Inheritance
285
knowledge, this paper provides the ﬁrst mechanical proof of this relaxation carried
out using proof technology to reinforce its foundations.
Abrial [1] proposes keep operations to overcome the restrictions of the skip
approach. keep operations are non-deterministic and guaranteed to preserve the
invariant; they may be safely added to abstract types [1]. They resemble simulat-
ing abstract operations used in the child-extra operations relaxation, which are safe
because they are not visible to the environment.
While the OO model of Liskov and Wing [28] is similar to ZOO’s (there is a
mapping from objects to their state), their approach is based on a earlier method of
data reﬁnement [23] that does not consider initialisation and ﬁnalisation. This paper
uses data reﬁnement based on simulation [22], the enduring basis of the theory, which
accounts for object creation (initialisation) and deletion (ﬁnalisation); BI cannot be
guaranteed if these are not checked. The rules of [28] correspond to the rule for
blocking reﬁnement given in [4]. In [28], the BI overkill is highlighted, but relaxations
are not considered.
Wehrheim and Fischer [18, 37] investigate BI in the context of concurrency and
the CSP process algebra. They studied how extra subclass operations may interfere
with the behaviour of the superclass as observed from the environment, and under
which conditions are safety and liveness properties preserved by the subclasses. They
propose several inheritance reﬁnement relations; the more liberal they are, the higher
the risk of interference. The one that is closer to ZOO’s relaxation on extra operations
is weak subtyping, which says that the subclass should have the same behaviour as
its superclass as long as no extra operations are called; the extra operations are not
considered in the comparison. The authors also proposed a more restricted relation,
optimal subtyping, which does not allow altering the behaviour of the superclass at
all; it is the same as the skip behaviour.
Object-Z [16, 34] deﬁnes a formal semantics for inheritance and a notion of class
reﬁnement, but a discussion of BI is generally absent in its books. In [16], BI and its
relation to reﬁnement is discussed, but no proof obligations are proposed to check
its correctness.
10
Conclusions
This paper investigates behavioural inheritance (BI) [28] by building up on insight
gained from previous work [2, 3]. It delves into BI’s foundations through an abstract
mathematical model of object orientation to come up with sound ways of reconciling
the correctness-sensitive reﬁnement facet of BI with the ﬂexibility that characterises
inheritance and the object-oriented (OO) paradigm. The paper’s accompanying tech-
nical report [4] provides further details not given here; the Isabelle proofs supporting
the work presented here are given in [5]; all BI proofs related to the Z models corre-
sponding to Figs.8 and 15 were undertaken in the Z/Eves Z prover.
BI’s constrictions are known since BI’s inception [28]. As this paper shows,
existing BI relaxations [17] are problematic and unconvincing; some relaxations

286
N. Amálio
are over-permissive (everything is a BI) and untrue to reﬁnement, resulting from an
over-simpliﬁcation of an intricate reality. This paper revisits BI by clearly framing
it within the theory of data reﬁnement [22]; it derives BI proof rules from the foun-
dations of the theory whilst trying to relax the reﬁnement constraints that severely
limit BI with respect to an ethos of inheritance and OO characterised by ﬂexibility.
The mathematical model of OO developed in Sect. 2 supports the framing of BI
under the umbrella of data-reﬁnement [22] whilst providing a mathematical foun-
dation to the paper’s major contributions. Data reﬁnement is simply and elegantly
expressed as sub-setting in the world of total relations. If totality is appropriate
for the concrete computing world of programs where machine processing requires
everything to be deﬁned, it becomes inadequate for the higher-levels of abstrac-
tion focussed on the essence of problems and which, for the sake of abstraction,
demand partial relations. Furthermore, design by contract (DbC) languages support
a computing paradigm based on explicitly declared assumptions (pre-conditions) and
expectations (post-conditions) which requires partiality. To accommodate partiality
and BI, the paper adapts the rules of data reﬁnement to partial relations speciﬁc to
BI. To accommodate object orientation, the paper takes the theory of data reﬁnement
for abstract data types (ADTs) and adapts it to OO classes. The abstract OO model
of Sect. 2 based on ADTs enables the derivation of BI-speciﬁc conjectures from the
theory of data of reﬁnement.
The BI conjectures of Sect. 3 are derived through the totalisation technique used
in Z reﬁnement. The simple reﬁnement relation (ϑ function, Deﬁnition 10) capturing
inheritance’s child-parent relation reﬂects the fact that there are actually two ADTs
involved in a class construction known in Z as promotion, and this provides a separa-
tion: the inner and outer types as representative of the corresponding views or worlds
of a class. Most derivation work was grounded on the ϑ reﬁnement function, which
is conﬁned to the inner type. By exploiting the properties of ϑ, the paper proved
that the inner BI conjectures reduce to a single set (Corollary 1). From here, the
paper derived the actual inner BI conjectures (or ADT extension reﬁnement) in the
relational (Fact 4) and Z-schema settings (Fact 5), alerting that overall BI reﬁnement
requires class freeness (Fact 6).
The derived BI conjectures of Sect. 3 were applied to the paper’s running example
in Sect. 5, which endorsed what is long known: trivial inheritance relations widely
used in OO programming are not reﬁnements in the strictest sense. The analysis of
Sect. 5 identiﬁed issues with child-extra operations, operation overriding, child oper-
ations that strengthen the pre-condition and global interference. Global interference,
neglected in the literature, is the realisation that locally proved BIs do not neces-
sarily hold globally. The analysis resulting from the paper’s running example poses
two questions: (i) how to reconcile BI with an ethos of inheritance characterised by
ﬂexibility? (ii) How do DbC languages deal with BI’s over-restrictiveness?
The examination of the DbC languages Eiffel, JML and and Spec♯of Sect. 6
highlighted inconsistencies between static and dynamic veriﬁcation, a neglect for
global interference, and issues with the pre-condition conjecture of [17], which is
over-permissive (everything is a reﬁnement) validating statically BIs that turned out
to be invalid at run-time.

Sound and Relaxed Behavioural Inheritance
287
The BI analysis (Sects. 5 and 6) motivates the quest for improved relaxations.
The paper’s relaxations target two issues identiﬁed in Sect. 5, namely: applicability’s
restrictions and global interference. The core remedy against applicability malaises,
virtual operations, results from the observation that certain inner class operations are
invisible to the environment (hence, virtual), giving room to lift the applicability rule:
the child operation no longer needs to be applicable whenever the parent operation
is applicable because the parent operation is invisible to the environment. This idea
underpins all the paper’s relaxations. For global interference, this paper take as nor-
mative the restriction of promotion reﬁnement’s freeness constraint, which is relaxed
through the novel inheritance freeness based on the idea that children should not be
more constrained than their hard parents. Together with refactoring, the relaxations
were used to transform the queues running example of Fig.8 into the BI compliant
alternative of Fig.15.
This paper has the following contributions:
• The abstract class relaxation and the underlying notion of virtual operations.
Although, they were both proposed in [2, 3], they have been elaborated here.
This paper emphasises virtual operations and the fact that applicability is lifted
whenever a virtual operation is reﬁned.
• The novel soft parent relaxation and the accompanying notion of soft inheritance,
which is useful in situations requiring both inheritance-divergence and parent
instantiability.
• The reinforcement of the child-extra operations relaxation, which is also proposed
in [2, 3] and [17], by carrying out the required formal proofs in the state of the art
Isabelle prover; this increases the reliability of proof by diminishing the possibility
of human error. Using the virtual operation notion, the applicability proof obliga-
tion is lifted; it was proved that child extra operations always correctness-reﬁne
their corresponding simulating abstract operations—hence, child extra operations
may be added freely.
• The inheritance-freeness relaxation dealing with global interference elaborates
on [2, 3]. Due to its complexity, this relaxation was proved informally using
formal argumentation in the accompanying technical report [4].
• The critique to the speciﬁcation inheritance relaxations [17] grounded on the exam-
ination of three DbC languages relying on [17]. The pre-condition (or applicabil-
ity) conjecture of [17] as being problematic with the examination highlighting
inconsistencies between static and dynamic veriﬁcation.
Acknowledgements The work presented here commenced when I was a Ph.D. student at the
University of York supervised by Prof. Susan Stepney and Dr. Fiona Polack. I am very happy that
this paper is part of this volume because Susan Stepney was immensely inﬂuential and inspiring
not only to the whole work presented here, but to myself and my academic work.

288
N. Amálio
11
Mathematical Deﬁnitions
Deﬁnition 1 (Given Sets) Given sets (all disjoint) O, S, I and E represent objects,
states, identiﬁers and environments respectively.
⊓⊔
Deﬁnition 2 (Sets for ADTs)
The following deﬁnitions support abstract data types (ADTs):
• Set Init = E ↔S, of relations between environments E (Deﬁnition 1) and initial
states S (Deﬁnition 1), represents all possible ADT initialisations (initial states).
• Set Fin = S ↔E, of relations between ﬁnal states S (Deﬁnition 1) and environ-
ments E (Deﬁnition 1), represents all possible ADT ﬁnalisations (ﬁnal states).
• Set Op = I →(S ↔S), of functions from identiﬁers (Deﬁnition 1) of operations
(names), to a relation between states (representing a transition between a a before-
and an after-state), represents all possible ADT operations.
⊓⊔
Deﬁnition 3 (ADTs) An abstract data type (ADT) t = (s, i, f , os) is made up of set
s ⊆S of all type states (Deﬁnition 1), an initialisation i : Init (Deﬁnition 2), a ﬁnali-
sation f : Fin (Deﬁnition 2), and an indexed set of operations os : Op (Deﬁnition 2).
The set of ADTs, such that t : ADT, is deﬁned as:
ADT = {(s, i, f , os) | s ∈P S ∧i ∈E ↔s ∧f ∈s ↔E ∧os ∈I →(s ↔s)}
Auxiliary Deﬁnitions. Several functions extract information from an ADT:
sts (s, i, f , os) = s init (s, i, f , os) = i ﬁn (s, i, f , os) = f ops (s, i, f , os) = os
⊓⊔
Deﬁnition 4 (Sets for Classes) The following sets support classes:
• Set OSt = O →S, of partial functions from an object to a state, represents the
state of a class: its living objects are mapped to their states.
• Set ONew = E ↔(OST ↔OST), of relations between environments E and a
transition of class global states (OST ↔OST), represents the effect of a class
constructor on the global class state.
• Set ODel = (OST ↔OST) ↔E, of relations between a transition of class global
states (OST ↔OST) and environments E, represents the effect of a class destructor
on the global class state.
• Set OUpd = I →I →(OST ↔OST), of partial functions from identiﬁers of
global operations to another function from identiﬁers of local operations to a
transition of class global states (described as a relation, OST ↔OST), which rep-
resents the effect of a class update operation: the operation results in a transition

Sound and Relaxed Behavioural Inheritance
289
from a particular class state to another class state in which the state of the affected
object is updated through a local operation.
⊓⊔
Deﬁnition 5 (Promoted Operations) Partial functions nOps : ADT × OST →ONew,
dOps : ADT × OST →ODel, and uOps : ADT × OST →OUpd give constructor,
destructor and modiﬁer operations of a class, respectively, which are built from the
given inner type (see Deﬁnitions 3 and 4 for the used sets). The functions are as
follows:
nOps = (λ t : ADT; ost : OST • {nop : ONew |
∃e : E; o : O \ dom ost; s : S | (e, s) ∈init t • e →{(ost, ost ∪{o →s})} ∈nop})
dOps = (λ t : ADT; ost : OST • {dop : ODel |
∃e : E; o : dom ost | (ost o, e) ∈ﬁn t • {(ost, {o} −◁ost)} →e ∈dop})
uOps = (λ t : ADT; ost : OST • {uop : OUpd |
∃ig, il : I; o : dom ost; s′ : sts t | il →{(ost o, s′)} ∈ops t•
ig →{il →{(ost, ost ⊕{o →s′})}} ∈uop})
⊓⊔
Deﬁnition 6 (Class) A class is C = (ci, t, is, os, stm, ocl, c, d, ms) is made-up of
a class identiﬁer ci : I, type t : ADT (Deﬁnition 3), a set of possible object states
is ⊆sts t, a set of objects os ⊆O (all possible class objects), a global class state
stm ∈OST, a mapping ot : os →I relating objects to the identiﬁers of their direct
classes, a constructor c ∈ONew, a destructor d ∈ODel and modiﬁers ms ∈OUpd
(see Deﬁnition 4 for ONew, ODel and OUpd).
The set of classes, such that C : Cl is deﬁned as:
Cl = {(ci, t, is, os, stm, ot, c, d, ms) | ci ∈I ∧t ∈ADT ∧is ⊆sts t ∧os ∈P O
∧stm ∈os →is ∧ot ∈os →I ∧dom ot = dom stm
∧c ∈nOps (t, stm) ∧ms ∈uOps (t, stm) ∧d ∈dOps (t, stm)}
Above, it is asserted that both the global class state function stm and the typing
mapping ot are deﬁned for the set of living objects of the class.
Auxiliary Deﬁnitions. Several functions extract information from a class:
icl (ci, t, is, os, stm, ot, c, d, ms) = ci
ity (ci, t, is, os, stm, ot, c, d, ms) = t
ists (ci, t, is, os, stm, ot, c, d, ms) = is
osu (ci, t, is, os, stm, ot, c, d, ms) = os
los (ci, t, is, os, stm, ot, c, d, ms) = dom stm
ost(ci, t, is, os, stm, ot, c, d, ms) = stm
ocl(ci, t, is, os, stm, ot, c, d, ms) = ot
cop (ci, t, is, os, stm, ocl, c, d, ms) = c
dop (ci, t, is, os, stm, ocl, c, d, ms) = d
mops (ci, t, is, os, stm, ocl, c, d, ms) = ms
pops (ci, t, is, os, stm, ocl, c, d, ms) =
{im : dom ms • dom(ms im)}
Above, icl gives the class’s identiﬁer; ity yields the inner type, ists yields the set
of object states; osu yields the universe of objects of the class (all possible class

290
N. Amálio
objects); los yields the living (or existing) objects of the class; ost yields the class’s
global state; ocl gives the mapping from living objects into the identiﬁers of their
direct classes; cop, dop and mops yield the constructor, destructor, and modiﬁer
operations, respectively; and pops yields the set of promoted inner type operations.
⊓⊔
Deﬁnition 7 (State Space Composition) Function ω : P S × P S →P S builds larger
state spaces from smaller ones. Given states spaces S1, S2 : P S, ω(S1, S2) yields their
composition.
⊓⊔
Deﬁnition 8 (State Extension) Given C, A : ADT (Deﬁnition 3), C is a state exten-
sion of A if and only if C’s state space is made up of A’s with something extra (see
Deﬁnition 7 for ω):
C extends A ⇔∃Sx ∈P S | sts C = ω(sts A, Sx)
⊓⊔
Deﬁnition 9 (Z schema state extension) Given ADTs C, A : ADT (Deﬁnition 3),
deﬁned as Z schemas, C is a state extension of A as per Deﬁnition 8 if it is deﬁned
using the following Z schema calculus formula:
C == A ∧X
This says, using the Z schema conjunction operator (equivalent to ω of Deﬁnition 7),
that the state of C is that of A with something extra (X).
⊓⊔
Deﬁnition 10 (ϑ-function) Given C, A : ADT (Deﬁnition 3), such that C extends A
(Deﬁnition 8), function ϑ : sts C →sts A yields the state being extended:
ϑ (ω(sts A, Sx)) = sts A
This recovers the state space that gave rise to the ADT’s compound state.
Function tϑ : ADT × ADT →(P S →P S), described as a deﬁnite description
(operator μ), yields a ϑ given two ADTs:
tϑ(C, A) = μ ϑ : sts C →sts A | C extends A ∧ϑ (ω(sts A, Sx)) = sts A
⊓⊔
Deﬁnition 11 (Inheritance) Given ClC, ClA : Cl (Deﬁnition 6), ClC (child) inherits
from ClA (parent) if and only if the inner state space of ClC extends that of ClA
(Deﬁnition 8), the set of objects of ClC is a subset of that of ClA, and there exists a
function ϑ (Deﬁnition 10):
ClC inh ClA ⇔ity ClC extends ity ClA ∧osu ClC ⊆osu ClA ∧los ClC ⊆los ClA
∧∃ϑ | ϑ = tϑ(ity ClC, ity ClA) ∧(ost Clp) ◦id = ϑ ◦(ost Clc)

Sound and Relaxed Behavioural Inheritance
291
Above, the equation with function composition (symbol ◦) captures the diagram
commutativity of Fig.3b.
⊓⊔
Deﬁnition 12 (Abstract Class) An abstract class lacks direct instances. Set of
abstract classes ACl ⊆Cl, a subset of Cl (Deﬁnition 6), is deﬁned as:
ACl = {c : Cl | (ocl c) ∼{icl c}  = ∅}
This says that the living instances preclude direct instances.
⊓⊔
Deﬁnition 13 (Programs) A program is a sequence of operations upon a data type.
A complete program begins with an initialisation and ends with a ﬁnalisation. A
program with operations i1, i2 over a type T : ADT (Deﬁnition 3), results in the
complete program: init T  ops T i1  ops T i2  ﬁn T.
Function cmpops : ADT × seq I →(S ↔S) yields the overall state transition (a
relation between states):
cmpops(T, ⟨⟩) = id
cmpops(T, ⟨i⟩⌢is) = (ops T i)  cmpops (T, is)
Above  is relation composition.
Function cprog : ADT × seq I →E ↔E, computes the effect of a complete pro-
gram, yielding a relation between environments corresponding to the environment
expectations and the environment effects of the program:
cprog(T, is) = (init T)  cmpops(T, is)  (ﬁn T)
⊓⊔
Deﬁnition 14 (DataReﬁnement)ForA, C : ADT (Deﬁnition3),C reﬁnesA(C ⊒A)
if and only if for each ﬁnite sequence of operations os over the indexing set I common
to both C and A (dom(ops A) = I = dom(ops C)), we have that (cprog is as per
Deﬁnition 13):
cprog (C, os) ⊆cprog (A, os)
⊓⊔
Deﬁnition 15 (ForwardsSimulation)GivenA, C : ADT (Deﬁnition3)andarelation
relating their state spaces r : sts A ↔sts C, we say that C reﬁnes A (C ⊒A) with r
being a forwards simulation if:
(init C) ⊆(init A)  r
r  (ﬁn C) ⊆(ﬁn A)
r  (ops C io) ⊆(ops A io)  r

292
N. Amálio
Above, io is an operation of A and C—io ∈dom(ops A) ∩dom(ops C). The rules
are as per the commuting of Fig.4 when the relation is upwards.
⊓⊔
Deﬁnition 16 (Backwards Simulation) Given A, C : ADT (Deﬁnition 3) and a rela-
tion relating their state spaces s : sts C ↔sts A, we say that C reﬁnes A (C ⊒A) with
s being a backwards simulation if:
(init C)  s ⊆(init A)
(ﬁn C) ⊆s  (ﬁn A)
(ops C io)  s ⊆s  (ops A io)
Above, io is an operation of A and C—io ∈dom(ops A) ∩dom(ops C). The rules
are as per the commuting of Fig.4 when the relation is downwards.
⊓⊔
Fact 1 (Forwards Simulation, partial relations) Given A, C : ADT (Deﬁnition 3)
and a relation relating their state spaces r : sts A ↔sts C. In a setting of non-
communicating partial operations, C reﬁnes A (C ⊒A) and r is a forwards simulation
in the non-blocking (or contractual) setting if:
(init C) ⊆(init A)  r
(initialisation)
r  (ﬁn C) ⊆(ﬁn A)
(ﬁnalisation)
r  dom(ops A io)  ⊆dom(ops C io)
(applicability)
dom(ops A io) ◁r  (ops C io) ⊆(ops A io)  r (correctness)
Above, io is an operation of A and C—io ∈dom(ops A) ∩dom(ops C).
Proof. These rules are proved in [39] and [16] from the rules for total relations (Def-
inition 15) using a totalisation technique (see [39] for details).
⊓⊔
Fact 2 (Backwards Simulation, partial relations) Given A, C : ADT (Deﬁnition 3)
and a relation relating their state spaces s : sts C ↔sts A. In a setting of non-
communicating partial operations, C reﬁnes A (C ⊒A) and s is a backwards simu-
lation in the non-blocking (or contractual) setting if:
(init C)  s ⊆(init A)
(initialisation)
(ﬁn C) ⊆s  (ﬁn A)
(ﬁnalisation)
dom(ops C io) ⊆dom(s −▷(dom(ops A io)))
(applicability)
dom(s −▷(dom(ops A io))) −◁(ops C io)  s ⊆s  (ops A io) (correctness)
Above, io is an operation of both A and C—io ∈dom(ops A) ∩dom(ops C).
Proof. These rules are proved in [39] and [16] from the rules for total relations (Def-
inition 16) using a totalisation technique (see [39] for details).
⊓⊔

Sound and Relaxed Behavioural Inheritance
293
Deﬁnition 17 (Extension reﬁnement function, general)
Let C, A : ADT (Deﬁnition 3) such that C extends A (Deﬁnition 8), the reﬁnement
function ϑ is obtained from function tϑ (Deﬁnition 10):
ϑ = tϑ(C, A)
⊓⊔
Deﬁnition 18 (Extension reﬁnement function, schemas) Let A, C : ADT be two
inner Z schema ADTs such that C extends A as described by the Z schema cal-
culus formula C == A ∧X. The ϑ BI function of Deﬁnition 10 is described by the
Z formula:
ϑ = λ C • θ A
This casts the general ϑ of Deﬁnition 17 into the context of the Z schema calculus.
⊓⊔
Fact 3 (Single set of reﬁnement rules) Given C, A : ADT (Deﬁnition 3), if the reﬁne-
ment relation is a total function f : sts C →sts A, then data reﬁnement reduces to a
single set of rules with forwards simulation (Deﬁnition 15) being equivalent to back-
wards simulation (Deﬁnition 16):
(init C) ⊆(init A)  f ∼⇔(init C)  f ⊆(init A)
f ∼(ﬁn C) ⊆(ﬁn A) ⇔(ﬁn C) ⊆f  (ﬁn A)
f ∼(ops C io) ⊆(ops A io)  f ∼⇔(ops C io)  f ⊆f  (ops A io)
Above, io is an operation identiﬁer deﬁned in concrete and abstract type—io ∈
dom(ops A) ∩dom(ops C).
Proof. The equivalences were proved in Isabelle. Hand-written proofs are given in [4].
⊓⊔
Corollary 1 (Single Set of Rules for Extension Reﬁnement) The rules of extension
reﬁnement reduce to a single set. This results from applying Fact 3 to the BI reﬁnement
functions of Deﬁnitions 17 (relations) and 18 (schemas).
⊓⊔
Fact 4 (Extension Reﬁnement, relations) Given C, A : ADT (Deﬁnition 3), such
that C extends A (Deﬁnition 8), C extension-reﬁnes A (C ⊒Ext A) with ϑ being the
reﬁnement function (Deﬁnition 17) if the three major data reﬁnement conditions—
initialisation (init), ﬁnalisation (ﬁn) and operations (ops)—are satisﬁed:
ϑ = tϑ(C, A)
C ⊒Ext A ⇔C ⊒init
Ext A ∧C ⊒ﬁn
Ext A ∧C ⊒ops
Ext A

294
N. Amálio
We deﬁne the initialisation and reﬁnement conditions as:
C ⊒init
Ext A ⇔(init C) ⊆init A  ϑ ∼(initialisation)
C ⊒ﬁn
Ext A ⇔ϑ ∼(ﬁn C) ⊆(ﬁn A) (ﬁnalisation)
An operation extension-reﬁnes another if the applicability (appl) and correctness
(corr) conditions are met:
C ⊒ops
Ext A ⇔∀io : dom(ops C) • (ops C io) ⊒appl
Ext (ops A io) ∧(ops C io) ⊒corr
Ext (ops A io)
co ⊒appl
Ext ao ⇔dom(ϑ  ao) ⊆dom(co)
(applicability)
co ⊒corr
Ext ao ⇔dom ao ◁(ϑ ∼co  ϑ) ⊆ao (correctness)
Proof. The rules above were proved in Isabelle (see [4] for further details).
⊓⊔
Fact 5 (Extension Reﬁnement, schemas) Given Z schema inner ADTs C, A : ADT
(Deﬁnition 3) such that C extends A (Deﬁnition 8)—which in Z is described as
C == A ∧X (where X is the extension)—, then C ⊒Ext A if and only if:
∀C′ • CI ⇒AI
(initialisation)
∀C • CF ⇒AF
(ﬁnalisation)
∀C; i? : V • pre AO ⇒pre CO
(applicability)
∀C′; C; i?, o! : V • pre AO ∧CO ⇒AO (correctness)
If the ﬁnalisation is total (the ADT does not have a ﬁnalisation condition) the ﬁnali-
sation rule reduces to true.
Proof. The rules above were proved in Isabelle (see [4] for further details).
⊓⊔
Deﬁnition 19 (Free Classes) Given any class ClB : Cl (Deﬁnition 6), we say that
ClB is free (or that the underlying promotion is free) if the following holds:
free ClB ⇔ists ClB = (sts ◦ity) ClB
This says that the inner states are free from global constraints (commuting of Fig.7).
⊓⊔
Fact 6 (BI Reﬁnement) Let ClA, ClC : Cl (Deﬁnition 6) such that ClC inh ClA (Def-
inition 11). ClC is BI conformant with ClA (ClC ⊒BI ClA) if and only if:
ClC ⊒BI ClA ⇔ity ClC ⊒Ext ity ClA ∧free ClC ∧free ClA
This requires that ClC’s inner type extension-reﬁnes ClA’s and both ClC and ClA are
free (Deﬁnition 19).
Proof. This applies promotion reﬁnement and freeness of [29] to BI.
⊓⊔

Sound and Relaxed Behavioural Inheritance
295
Deﬁnition 20 (Virtual operations) Function vopids : Cl →P I identiﬁes the set of
possible inner virtual operations of a class (set Cl, Deﬁnition 6):
vopids cl =
(dom ◦ops ◦ity) cl \ pops cl ifcl /∈ACl
(dom ◦ops ◦ity) cl
otherwise
Above, vopids yields all identiﬁers of inner type operations not being promoted or
the identiﬁers of all inner operations if the class is abstract.
⊓⊔
Deﬁnition 21 (Extension Reﬁnement with virtual operations) Extension reﬁnement
(Deﬁnition 4) is extended to cater to virtual operations vops ⊆I. Given C, A : ADT
(Deﬁnition3),suchthatC extends A(Deﬁnition8),C extension-reﬁnesAwithvirtual
operations vops (C ⊒Ext A ▷◁vops) if the following holds:
C ⊒Ext A ▷◁vops ⇔vops ⊆(dom ops A) ∧C ⊒init
Ext A ∧C ⊒ﬁn
Ext A ∧(C ⊒ops
Ext A ▷◁vops)
The conditions for the reﬁnement of operations then becomes:
C ⊒ops
Ext▷◁vops ⇔
∀io : dom(ops C) \ vops • (ops C io) ⊒appl
Ext (ops A io) ∧(ops C io) ⊒corr
Ext (ops A io)
∧∀io : dom(ops C) ∩vops • (ops C io) ⊒corr
Ext (ops A io)
This says that correctness sufﬁces to prove that a concrete operation reﬁnes a virtual
operation. All other deﬁnitions are as per Fact 4.
⊓⊔
Deﬁnition 22 (Simulating abstract operations) Given C, A : ADT (Deﬁnition 3),
such that C extends A (Deﬁnition 8), then for any child (or concrete) operation
co ∈ops C, it is possible to calculate a simulating operation ao∅, which simulates
co in the abstract world, using function tϑ (Deﬁnition 10):
ϑ = tϑ(C, A)
ao∅= ϑ ∼co  ϑ
Operation ao∅is virtual.
⊓⊔
Fact 7 (Reﬁnement of virtual operations) Given C, A : ADT (Deﬁnition 3), such that
C extends A (Deﬁnition 8), then any concrete operation co ∈ops C extension-reﬁnes
the simulating operation ao∅(Deﬁnition 22)—co ⊒Ext ao∅.
Proof. Correctness was proved in Isabelle (further details in [4]). Applicability is
dismissed by the virtual operation principle captured in Deﬁnition 21.
⊓⊔
Corollary 2 (BI of child extra operations) Let ClA, ClC : Cl (Deﬁnition 6) such that
ClC inh ClA (Deﬁnition 11). Any child extra operation co not deﬁned in ClA is inner
BI conformant as a result of Fact 7.
⊓⊔

296
N. Amálio
Deﬁnition 23 (Applicability-relaxed BI) Given ClA, ClC : Cl (Deﬁnition 6) such
that ClC inh ClA (Deﬁnition 11). ClC is BI conformant with ClA (ClC ⊒BI ClA) when
the following holds:
vos = vopsids ClA ◁((ops ◦ity)ClA)
ClC ⊒BI ClA ⇔(ity ClC ⊒Ext ity ClA ▷◁vos) ∧free ClC ∧free ClA
Above, vos holds virtual operations of ClA.
⊓⊔
Deﬁnition 24 (Inheritance freeness) Let ClC, ClA : Cl (Deﬁnition 6) such that
ClC inh ClA (Deﬁnition 11), ϑ be the function relating inner states of ClC and ClA
(Deﬁnition 10) and α be a function that applies the relation image ( ):
ϑ = tϑ(ity ClC, ity ClA)
α r xs = r  xs 
We say that this inheritance relation is free if the following holds:
ClC Inhfree ClA ⇔((α ϑ) ◦ists) ClC = ists ClA ∩((α ϑ) ◦sts ◦ists) ClC
This says that the set of inner states of child ClB held by parent ClA must be the same
as set of inner states held by ClB, which captures the commuting of Fig. 14 and says
that the child cannot be more globally constrained than the parent.
⊓⊔
Deﬁnition 25 (Relaxed BI Reﬁnement) Let ClA, ClC : Cl (Deﬁnition 6) such that
ClC inh ClA (Deﬁnition 11). Class ClC is BI conformant with ClA (ClC ⊒BI ClA)
with relaxed applicability and freeness when the following holds:
vos = vopsids ClA ◁((ops ◦ity)ClA)
ClC ⊒BI ClA ⇔ity ClC ⊒Ext ity ClA ▷◁vos ∧ClA Inhfree ClC
Above, vos holds virtual operations of ClA; Inhfree is as per Deﬁnition 23.
⊓⊔
References
1. Abrial, J.R., Cansell, D., Méry, D.: Reﬁnement and reachability in Event B. In: Proceed-
ings of ZB2005. LNCS, vol. 3455, pp. 222–241. Springer (2005). https://doi.org/10.1007/
11415787_14
2. Amálio, N.: Generative frameworks for rigorous model-driven development. Ph.D. thesis,
Department Computer Science, University of York (2007)
3. Amálio, N.: Relaxing behavioural inheritance. In: Proceedings of Reﬁne 2013, EPTCS, vol.
115, pp. 68–83 (2013)
4. Amálio, N.: Behavioural inheritance with relaxed but safe constraints grounded on data reﬁne-
ment. Technical report, Birmingham City University (2018). http://bit.ly/BI5nl0I
5. Amálio, N.: Isabelle proofs of behavioural inheritance and relaxations (2018). http://bit.ly/
2o0KaI2

Sound and Relaxed Behavioural Inheritance
297
6. Amálio, N., Glodt, C.: A tool for visual and formal modelling of software designs. Sci. Comput.
Program. Part 198, 52 – 79 (2015). https://doi.org/10.1016/j.scico.2014.05.002
7. Amálio, N., Glodt, C., Kelsen, P.: Building VCL models and automatically generating Z spec-
iﬁcations from them. In: Proceedings of FM 2011. LNCS, vol. 6664, pp. 149–153. Springer
(2011)
8. Amálio, N., Kelsen, P.: Modular design by contract visually and formally using VCL. In:
Proceedings of VL/HCC 2010, pp. 227–234. IEEE (2010). https://doi.org/10.1109/VLHCC.
2010.39
9. Amálio, N., Kelsen, P., Ma, Q., Glodt, C.: Using VCL as an aspect-oriented approach to
requirements modelling. TAOSD VII, 151–199 (2010)
10. Amálio, N., Polack, F., Stepney, S.: An object-oriented structuring for Z based on views. In:
Proceedings of ZB 2005. LNCS, vol. 3455, pp. 262–278. Springer (2005)
11. Amálio,N.,Polack,F.,Stepney,S.:UML+Z:AugmentingUMLwithZ.In:Abrias,H.,Frappier,
M. (eds.) Software Speciﬁcation Methods. ISTE (2006)
12. Amálio, N., Polack, F., Stepney, S.: Frameworks based on templates for rigorous model-driven
development. ENTCS 191, 3–23 (2007)
13. Banach, R., Poppleton, M.: Retrenchment: an engineering variation on reﬁnement. In: Pro-
ceedings of B’98. LNCS, vol. 1393, pp. 129–147. Springer (1998). https://doi.org/10.1007/
BFb0053358
14. Banach, R., Poppleton, M., Jeske, C., Stepney, S.: Engineering and theoretical underpinnings
of retrenchment. Sci. Comput. Program. 67(2–3), 301–329 (2007)
15. Chalin, P., Kiniry, J.R., Leavens, G.T., Poll, E.: Beyond assertions: advanced speciﬁcation and
veriﬁcation with JML and ESC/Java2. In: de Boer, F.S., et al. (eds.) Proceedings of FMCO
2005. LNCS, vol. 4111, pp. 342–363. Springer (2006)
16. Derrick, J., Boiten, E.: Reﬁnement in Z and Object-Z: Foundations and Advanced Applications.
Springer (2001)
17. Dhara, K.K., Leavens, G.T.: Forcing behavioural subtyping through speciﬁcation inheritance.
In: 18th International Conference on Software Engineering, ICSE-18, pp. 258–267. Also pub-
lished as TR # 95 −20c, Department of Computer Science, Iowa State University, (1996)
18. Fischer, C., Wehrheim, H.: Behavioural subtyping relations for object-oriented formalisms. In:
Proceedings of AMAST 2000. LNCS, vol. 1816, pp. 469–483. Springer (2000)
19. Hall, A.: Using Z as a speciﬁcation calculus for object-oriented systems. In: Hoare, A., Bjørner,
D., Langmaack, H. (eds.) Proceedings of VDM ’90. LNCS, vol. 428, pp. 290–318 (1990)
20. Hall, A.: Specifying and interpreting class hierarchies in Z. In: Z User Workshop, Workshops
in Computing, pp. 120–138. Springer (1994)
21. Harel, D., Kupferman, O.: On object systems and behavioural inheritance. IEEE Trans. Softw.
Eng. 28(9), 889–903 (2002)
22. He, J., Hoare, A., Sanders, J.W.: Data reﬁnement reﬁned. In: Proceedings of ESOP’86. LNCS,
vol. 213, pp. 187–196. Springer (1986). https://doi.org/10.1007/3-540-16442-1_14
23. Hoare, A.: Proof of correctness of data representations. Acta Inform. 1(1), 271–281 (1972).
https://doi.org/10.1007/BF00289507
24. ISO: Information technology–Z formal speciﬁcation notation–syntax, type system and seman-
tics. ISO/IEC 13568:2002. International Standard (2002)
25. Jackson, D.: Software Abstractions: Logic, Lanaguage, and Analysis. MIT Press (2006)
26. Leavens, G.T.: JML’s rich, inherited speciﬁcations for behavioural subtypes. In: Proceedings
of ICFEM 2006, vol. 4260, pp. 2–34. Springer (2006)
27. Leino, K.R.M., Müller, P.: Using the spec# language, methodology, and tools to write bug-
free programs. In: Advanced Lectures on Software Engineering: LASER Summer School
2007/2008, pp. 91–139. Springer (2010)
28. Liskov, B., Wing, J.: A behavioral notion of subtyping. ACM Trans. Program. Lang. Syst.
16(6), 1811–1841 (1994)
29. Lupton, P.J.: Promoting forward simulation. In: Z User Workshop, pp. 27–49. Springer (1990)
30. Mayr, E.: Biological classiﬁcation: toward a synthesis of opposing methodologies. Science
214(30) (1981)

298
N. Amálio
31. Meyer, B.: Applying “design by contract”. Computer 25(10), 40–51 (1992)
32. Meyer, B.: Object-Oriented Software Construction. Prentice-Hall (1997)
33. Meyer, B.: Touch of Class: Learning to Program Well with Objects and Contracts. Springer
(2009)
34. Smith, G.P.: The Object-Z Speciﬁcation Language. Kluwer Academic Publishers (2000).
https://doi.org/10.1007/978-1-4615-5265-9
35. Stepney, S., Polack, F., Toyn, I.: Patterns to guide practical refactoring: examples targetting
promotion in Z. In: Proceedings of ZB 2003. LNCS, vol. 2651, pp. 20–39. Springer (2003)
36. Tschannen, J., Furia, C.A., Nordio, M., Meyer, B.: Automatic veriﬁcation of advanced object-
oriented features: the autoproof approach. In: Proceedings of LASER 2012. LNCS, vol. 7682,
pp. 133–155 (2012)
37. Wehrheim, H.: Behavioral subtyping and property preservation. In: Smith, S.F., Talcott, C.L.
(eds.) Proceedings of FMOODS 2000, pp. 213–231. Kluwer (2000)
38. Wirth, N.: Program development by stepwise reﬁnement. Commun. ACM 14(4), 221–227
(1971)
39. Woodcock, J., Davies, J.: Using Z: Speciﬁcation, Reﬁnement, and Proof. Prentice-Hall (1996)

Growing Smart Cities
Philip Garnett
Abstract As the world’s population becomes increasingly urbanised the problems
of building sustainable cities also grows. Using Susan Stepney’s response, “Mighty
Oaks from Little Acorns Grow”, to a science ﬁction story by Adam Marek titled
“Growing Skyscrapers”, this chapter looks at what a living city of the future might
look like, and how that might solve some of the problems of the control and devel-
opment of cities. There is a long history of the application of systems thinking,
cybernetics, and complex systems and the growth and control of cities. However,
many problems still remain in the deployment and applications of these frameworks
and methodologies, and in the potential consequences of their use. However, perhaps
many of these could be solved by the development of a living city.
1
Introduction
The control of the growth and development of cities presents a signiﬁcant challenge to
their human designers. The services that a city needs to provide for its inhabitants, or
the services that the inhabitants require from their city, will radically change between
the moment of its emergence as a group of buildings and the point at which it becomes
recognisably a city (which is not a point of completion, it is reasonable to say that
cities are never ﬁnished). This is true of cities that are planned, but even more so of
cities that just happen (which is historically at least a signiﬁcant proportion of them).
A city exists on a different temporal scale to it’s inhabitants, and the infrastructure it
provides is in a constant state of revision; a process without end.
This constant evolution might, if not somehow regulated, not produce positive
change. The idea of the evolving city and how this can have a negative impact on a city
is a central point of Jane Jacobs’ 1961 book, The Death and Life of Great American
Cities [20]. Jacobs’ work tells a story of how uncontrolled suburban growth (often
P. Garnett (B)
York Cross-disciplinary Centre for Systems Analysis and School of Management,
University of York, Heslington, York YO105DG, UK
e-mail: philip.garnett@york.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_12
299

300
P. Garnett
referred to as sprawl), coupled with the rise of the automobile and the parallel death
of public transport, is killing American cities. The functionality and utility of a city
as a place to live is being eroded by suburban growth. Although the city is growing
in terms of its housed population, the infrastructure (transport, utilities etc), green
spaces, commercial space, is not developing in step with the increasing population.
Some took this as a argument for centralised planning; top-down approaches that
could introduce order into the sprawl. However Jacobs is perhaps arguing, and even
more so in later work, for more bottom-up decentralised change [21, 22]. The notion
of urban sprawl suggests that the required feedback mechanisms and regulation
required for positive growth are not present, and if decentralised feedback systems
are put in place the city will adapt to support its growth. One view of what is being
described here is a self-organising complex adaptive system, where the growth of
that system is regulated by internal feedback systems.
Complexity theory describes a relational model of systems, where a system of
interest can be broken down into parts, and the interactions between those parts.
The behaviour of the system is then an emergent property of the parts and their
interactions (often these systems are self-organising). Complex systems often have
a number of common features. They can adapt their environment and therefore have
the capacity for change, and are able to learn. This is often linked with the idea of
system memory, the current state of the system is a product of past sates, and future
statesareinﬂuencebythecurrentstate.Complexsystemsalsodemonstrateresilience,
and therefore are able to respond to changes in their environment without failure.
However, somewhat counter to this, their behaviour is also non-linear, and therefore
a small perturbation could produce no response, a proportionate response, or perhaps
produce massive changes. For a more in depth characterisation of complexity and
the features of complex systems see [23, 26, 35, 42].
Cites can therefore be described as complex adaptive systems which are subject
to constant change, and the functionality of the city is an emergent property of all
the different interacting parts within it [2, 3, 5]. Or put a different way, the city itself
is a manifestation of the interaction of the parts. It is an emergent property of all the
people, buildings, transport systems, and businesses within it. How cities are gov-
erned therefore presents a signiﬁcant challenge to their inhabitants. Understanding
and controlling the processes of change and revision is difﬁcult and hard to manage,
as it is hard to locate agency in complex systems in order to make interventions [37].
In the context of a city, it is not as simple as the humans in the system being the sole
agents of change. They are components of the larger complex system, but are not the
only drivers of change. Therefore the human parts of the city might be (or rather, are
quite likely to be) unable to see (or predict) the full consequences of their actions
within the larger system that is the city. Other processes need to be recognised and
responded to, as demonstrated by the unregulated (where by regulation we mean by
feedbacks within the system, more in the sense of regulatory feedback in biological
systems) city expansion in America that produced the sprawl. This highlights another
difﬁculty of complex systems; where does one system end and another begin. Where

Growing Smart Cities
301
does one draw a boundary between one system and another, or one city and another.
Often system boundaries are drawn for pragmatic reasons, rather than because there
exists in reality a deﬁnable boundary, and it is understood that there is ﬂow across
that boundary (the boundary is fuzzy). Problems of nestedness, embeddedness, and
boundary determination also apply to cities [6]. Where does a city end? Is it before
or after the sprawl, and what about when one city merges with another? Where ever
the boundary is drawn, people, goods, policies, and ideas will ﬂow across the limits
of a city, contributing to the city environment.
We know from the governance of complex systems that it is hard to have sufﬁcient
knowledge of a complex adaptive system to be able to make adjustments, and have an
understanding of what the impact, across the system as a whole, will be [23]. Changes
to one part of a system could inﬂuence or produce emergent behaviours elsewhere in
the system [32], which may even result in failure [16]. In a city for example, changes
to a road system in one area could produce trafﬁc problems or pollution in another. Or
the development (or redevelopment) of land could have similar unexpected effects
to other aspects of the city’s infrastructure. A lot of these problems could be due to
the human mediation of the response of the city to its changing environment. Human
interference in the evolution or growth of the city. If new housing is needed, it is
humans that have to sense the need for the housing, determine where it should be
positioned, and implement the change. A process that can often be more politically
driven rather than need driven. It is not an organic response by the city to a perceived
need for additional capacity for housing in a particular region of the city (change
driven by internal feedbacks or regulation). The same would be true of any other
necessary change in infrastructure. The human inhabitants would have to perceive
the need for the change, and then enact it on behalf of the city. As cities cannot grow
on their own, adapting their internal dynamics to the environment, like a biological
organism is able to.
Frameworks and methodologies have been proposed as to how a complex systems
could be steered or governed, or how desirable attributes could be maintained and
enhanced. In their paper Engineering Emergence (2006) Stepney and Polack describe
how the resilience (fault tolerance, robustness, and adaptability) of systems is often
linked to a system’s emergent behaviour, and therefore it would be advantageous that
when designing systems, like cities, if emergence could be engineered in [36]. Later
work explores what the features of a design framework might be, suggesting that in
the future we might have frameworks to build large scale engineering projects that
exhibit desirable emergent properties [41]. This chapter will investigate the concept
of a growing, living, smart city. A living city would need to demonstrate the properties
of emergence, including fault tolerance, robustness, and adaptability. Beyond that,
could a living city sense the requirements for change itself and enact those changes,
and what might the consequences of allowing a living city evolve to adapt to those
perceived needs be?

302
P. Garnett
2
Cities as Complex Systems
Cities are themselves complex adaptive systems, where one element with the pos-
sibility of producing change with in them are the humans. The human inhabitants
are part of the system, and their interactions with the other parts can alter the emer-
gent global behaviour of that system, but not always in ways which are expected or
desired. Additionally cities are human infrastructure projects, designed by humans to
provide for the needs of their inhabitants, both domestic and commercial. However
this does not necessarily mean that cities lack any agency of their own, or that the
inhabitants are the only agents for change within a city.
Aspects of complexity theory and complex systems models have been applied to
the problem of growth of cities in a number of areas, and cellular automata one exam-
ple of the application of ideas of self-organisation and emergence to city growth and
change. Cellular automata are an example of a complex system, which through the
application of a simple set of rules can demonstrate complex phenomena [47]. They
are often modelled as cells on a two-dimensional grid which respond to their envi-
ronment by either persisting, dying, or reproducing, and in doing so create changes
in the environment. Information from the environment is communicated to the cell
which determines its fate, in a process that was thought to be analogous to the devel-
opment of biological cells [40, 44]. Perhaps the most famous cellular automina is
Conway’s game of life [14, 15]. The game has the following rules:
1. Any live cell with fewer than two live neighbours dies, as if by under population.
2. Any live cell with two or three live neighbours lives on to the next generation.
3. Any live cell with more than three live neighbours dies, as if by overpopulation.
4. Any dead cell with exactly three live neighbours becomes a live cell, as if by
reproduction.
The environment is seeded with a number of cells and the generations of cells are pro-
duced by applying the rules simultaneously to all the cells in the environment. Each
generation is therefore a product of the preceding one. These simple rules deﬁne the
relationships between the parts of our system (the cells) and are capable of producing
all manor of complex behaviours that could be difﬁcult, or perhaps impossible, to
predict from the rules alone. However, these behaviours reveal themselves when the
game is played.
Where cellular automata have been applied to the modelling of cities is in the
possibility of deﬁning distributed rules that describe how one area of a city might
develop in the future based on its own state, and the state of the surrounding areas.
Models follow the basic principle that an area, or neighbourhood, of a city changes
in response to its current state and the state of the surrounding neighbourhoods. One
could imagine that this might be true to a certain extent, gentriﬁcation or urban decay
do seem to spread through cities, however this is unlikely to be simply due to neigh-
bour effects alone. Such neighbourhood, or city area, models can also demonstrate
emergent properties, where groups of cells form stable or quasi-stable neighbour-
hoods, for example 5 × 5 or 3 × 3 groups of cells that maintain some sort of property

Growing Smart Cities
303
over multiple generations [3, 33, 45]. Much like how some city areas can maintain
a character over long periods of time. One of the more signiﬁcant limitations of
cellular automata as a model of city evolution, which we alluded to already, is that
they are only really designed to model neighbour effects. It might be that some of
the feedbacks operating in cities cannot be modelled in this way.
Other complex systems models of cities have been built with agent based mod-
elling, where an agent is “a part of the environment that senses that environment and
and acts on it, in time, in pursuit of its own agenda and so effects what it sense in
the future” [13]. Agent based models map well to complex systems as the agents
describe the parts of the system, and the system behaviour is a product of the interac-
tions between the agents. Agent based models of cities (or other aspects of society)
extend from large scale, even multi-city models, down to models that work on the
scale of individuals, demonstrating the capacity for modelling at different scales [39].
In order to investigate a problem, or the possible outcome of an intervention, aspects
of the city could be captured by agents and relationships between those agents and
the environment deﬁned. The simulation of the model then allows for the testing of
hypotheses about the cause of the problem, or the possible different outcomes of
the intervention. However one difﬁculty is that agent based modelling still presents a
challenge in terms of computational capacity [10]. Weather it is possible to design and
implement agent based models that could be used to model and understand a whole
city, or network of cities, remains to be seen. An appropriate level of abstraction
would also need to be arrived at for it to be useful tool.
2.1
The Cybernetic City
The models presented in Sect. 2 demonstrate how complex systems modelling
approaches could help us understand the cities that we have. The intention is that
this improved understanding should allow for better interventions to be made, how-
ever knowing where and how to intervene in a complex system is very difﬁcult.
It is worth mentioning here the (trans)discipline of Cybernetics. Cybernetics was
intended as the “scientiﬁc study of control and communication in the animal and the
machine” [46]. Studying something as a cybernetic system would provide insight
into how to steer, or govern, the system. Precise control might not be possible, how-
ever the system could be navigated in a desirable direction. There were attempts to
use the principles of cybernetics to design self-regulating control systems for real-
time planned economies, including work by Glushkov [1, 18] and Stafford Beer’s
Cybersyn project [12, 29], and urban systems more generally [28]. More recently
the idea of the adaptive city, a city that is able to adapt to the day to day changes in
conditions in cities, is been explored in the context of cybernetics [17].
Beer’s Cybersyn is an example of a dashboard control room that was intended
to allow for the operation and control of the Chilian economy [12, 29, 30]. One
potential problem with such dashboards is that they assume that such control is even
possible, and could perhaps inspire thoughts of top-down control system in their users

304
P. Garnett
(if not their designers). Dashboards (in general) may suffer from the assumption that
somehow the complexity of an economy (or even a city) can be reduced and displayed
on a dashboard in such away that it could be used to understand, and then intervened
in, such a system [4]. Dashboards, or control centres, are still being built that are
intended to manage a city, and make sense out of the myriad indicators that could
be constructed from different data sources. One problem with this is the central
assumption that management of such a complex system is possible, or they might
inspire a belief in a level of management that is not possible. Another criticism is
that the construction of control rooms and dashboards potentially forgets that the
data that is chosen to be collected is likely to be socially constructed and probably
biased in numerous ways. What the designers choose to feed into the control room
or dashboard is likely to inﬂuenced by their own internal biases, a difﬁcult trap to
avoid. Furthermore, the use of dashboards could potentially hide the role of socially
constructed data and therefore present an essentially false view of what is actually
happening [24].
A criticism therefore of implementations of cybernetic theory, and more so the
often accompanying related dashboards, is perhaps that they embody an external
control system that takes information in from a system and then outputs changes to
that system. Despite the work of von Forester (see also Mead (1950) and Bateson
(1952) [31]) on second order cybernetics that sought to ﬁrmly embed the observer
in the system [43]. Therefore also forgetting, perhaps not intentionally, some of the
features of complex systems. The builders of such systems could even be aware of
the need for the control room to be ‘part of the system’, however the dashboard still
manifests physically as an external control room for the collection and presentation of
data. Separating the control room and its occupants from the rest of the city. Inviting
decision making processes that have at their heart simplistic and reductionist thinking
processes and evidence, even if that was explicitly not the intention [7].
Therefore perhaps a change in the process of thinking is also required. Caves and
Teixeira de Melo (2018) propose the concept of gardening to develop “a relational
framework for complex thinking about complex systems” [8]. They invite the poten-
tial user of the framework to think about “what are the things I need to think about
when I think of change in the system of interest?”, and “how do I need to think
about them, and the relations between them?” [8]. Embedding the relational aspect
of complex systems into the thinking, and complexity into the thinking itself. For a
city to be viable as a living system the relationships between the parts would need
to be captured and understood, otherwise the city would not grow as intended, and
new frameworks for thinking may be needed to achieve that.
3
Growing Skyscrapers
In the 2014 volume, Beta Life Stories from an A-Life Future [19], Susan Stepney
responds with Mighty Oaks from Little Acorns Grow to a Adam Marek’s chapter
Growing Skyscrapers [19, pp. 41–62]. The story presents a future where massive

Growing Smart Cities
305
skyscrapers are not built by an untidy process of construction by humans, but rather
theygrowinplacefromaseed.Theseedgrowsanddevelopsgraduallyintoabuilding,
taking in from the environment the raw materials required to construct the different
components of the building. The walls, internal structures, electrical infrastructure,
and its foundations, develop and expand in a manor not unlike a plant such as a tree.
Atreedevelopsfoundationalrootsthatprovidestabilityandwater,buildsacirculation
system for transport, and a network of branches that support leaves which in turn
collect sunlight and CO2 from the air. All requirements to support the growth of the
tree.
Three protagonists in the story are investigating a rouge building, “The Jetty”,
developingasamassivejettystretchingoutintotheseafromthecoast.Agrowthstem-
ming from the unauthorised planting of a seed, “stolen technology”. We encounter
them measuring the foundation, “roots”, of the building to check that their size will
support the building, and its inhabitants above. Essentially checking that the internal
feedbacks in the complex system are correctly regulating the development of the
foundations. With a view to whether the building is safe. We learn that “The Jetty”
has grown chaotically, without “the guidance of a trellis or an architect”, or perhaps
a gardener [8].
The story also follows two inhabitants of the building, as they live (and grow) in
theirgrowinghabitation“pod”,living quartersthatdevelopandmatureasthebuilding
does. There is a symbiotic (or perhaps parasitic, it is not clear) relationship between
the building and its inhabits. The building produces an edible sap that they can eat,
perhaps the building can draw nutrients from the waste products of the inhabitants.
Thebuildingmaybeneﬁtinotherwaysfromhabitation,carefromhumansintheevent
of disease, or maintenance and the repair of injury should the building suffer damage,
however this isn’t clear. Stepney, in the response, discusses the concept of gardening
of complex systems (a concept also discussed as a model of thinking in Caves and
Teixeira de Melo [8]), to encourage development of the system in a particular way.
Perhapsgardeningofagrowingskyscrapercouldconstitutesymbiosis,astheremoval
or pruning of vulnerable or injured sections to prevent further damage or disease, or
the control of pests, is of mutual beneﬁt. The inhabitants of a building would beneﬁt
from increasing the longevity of the building, and utility of the space, through pruning
(or gardening more generally), and the building beneﬁts from a live in maintenance
crew and in a sense an immune system (as humans acting to prevent disease from
inside the building could be viewed as such).
The book ends with what appears to be a an attack on the building. There are
hints in the story that the owners of the technology that is the growing building, have
also developed a way to restrict or control the use of the technology. A girl sits in
her pod, her developing space for habitation, when an large black beetle lands on
the window, soon joined by another, this is something new that has not been seen
before. The implication is that this is a pest sent by the owners to destroy the building.
Reminding us that a sophisticated new living technology would be vulnerable to a
new form of sophisticated living attack, both biological control developed by humans
but also hostile acts or perhaps other organisms that evolved to take advantage of a
new environment.

306
P. Garnett
4
The Living City
The concept of the living city would be a natural progression of that of a living
skyscraper, which are in many ways are small cities in their own right. The chapter
Growing Skyscrapers talks about enormous buildings over a mile high, which would
presumably have many thousands of inhabitants and both living and commercial
areas [19, pp. 41–62]. There is no reason why this would have to be the only model
for a living environment.
A living city could also resemble more the cities of today, that are often a mixture
of buildings of different sizes and uses. These could perhaps all be part of one massive
living city structure, that would spread over the entirety of city limits providing all
the different types of infrastructure required by their inhabitants. Perhaps the nearest
living example of this would be the Humongous Fungus, which is thought to spread
over an area of as much as 4 square miles [34]. Armillaria ostoyae fungi can exist over
very large spaces and present a variety of different structures, including underground
networks for nutrient collection, with fruiting bodies (mushrooms) that sprout up out
of the ground for the purpose of reproduction. These massive fungi that spread over
very large areas are considered to be a single organism.
The advantage of a single city organism is that information about the environment
could be collected an distributed around the entire city with ease. Allowing the feed-
backs that change the city to work over long distances, as well as short ones. The
biological processes that control the growth of organisms like fungi are coordinated
but decentralised, demonstrating other features that would be desirable in a living
city as it would negate the need for any central control system or dashboard. One
potential problem would be that one organism would need to provide all the different
types of structure and infrastructure required. Encoding this into a single genomic
DNA blueprint might prove to be a challenge, even for the geneticists of the future.
Perhaps it might be better to think in terms of a symbiotic ecosystem of multiple
organisms, including the human inhabitants, co-existing to produce the living city.
The cities would be a decentralised, multi-organism, ecosystem of mutually bene-
ﬁcial symbiotic relationships. Designed to display the required emergent properties
that a city would need; fault tolerance, robustness, and adaptability.
4.1
Growing a Smart City
If we put to one side the science ﬁction of a literal living city and look more at
developing and improving what we have then the concept of a living city still has a
lot to offer. Excepting that a city currently grows as a consequence of the activity of
their human inhabitants, but moving away from the inhabitants as the only agents
of change. In Sects. 2 and 2.1 we explored how complexity and cybernetics can be
applied to the governance and planning of cities, but what if the city itself could
regulate its growth? This would make a city more of an ecology, a living entity (or

Growing Smart Cities
307
entities) in its own right that can grow and adapt in response to feedbacks from the
organisms that live within it.
There are a number of potential problems with moving agency away from the
humans and more towards the adaptive system as a whole. By which, no single
part of the complex system should have ultimate control of how the system develops.
Instead the system should be allowed to grow and develop in response to the multiple
feedbacks within the complex system. This might not be possible in situations where
there are self-interested parties that do not want to relinquish their control over
how a city develops, perhaps out of fear that their own situation in terms of living
standards might be negatively effected somehow. Loss of control in this way is also
not condusive with our current modes of living, which are essentially fairly static.
One constraint on the development of living cities would therefore be that some
of the features of living systems are not desirable for human systems, such as death
or failure. In ecosystems parts of the system are allowed to fail, organisms die and
make way for other organisms (or provide resources to other organisms). In a living
city it might be necessary for parts of it to die back, and be replaced with different
structures as the city grows. Or areas of the city might change and adapt around its
inhabitants, essentially changing their position in the greater whole. This happens
in cities already, old parts are taken down and replaced, and much of this type of
change would be normal to some inhabitants of today’s cities. However, others might
not welcome a reduction in their own power to control and shape a city in order to
allow it to adapt better for the needs of its inhabitants as whole. In a living city a
mechanism would be required such that this type of change was seen as normal,
and people having to move because their current housing was dying so it could be
replaced with transport infrastructure was seen as a necessary adjustment due to the
changing nature of the city in that area. A reality that many face already.
As with large decentralised living organisms, like the Humongous Fungus, bio-
logical systems have perhaps also developed huge growing smart cities before us.
Researchers discovered a massive city of termites, roughly the size of Great Britain
and as much as 4000years old [27]. The complex of 200 million interconnected ter-
mite mounds perhaps demonstrates many of the features we would like for our own
growing smart cities. It is self-organisation and decentralised on the city scale and
also at the level of individual mounds or buildings [48], and has remained stable for
a very long time. Proving that we have a lot to learn about the organisation of our
own societies.
5
Conclusion
The concept of a city as a living system at this time seems distant to the point of
far fetched. However, thinking of cities as living is less so and has its uses. Cites, as
they exist today, are human impositions on an environment, radically changing the
environment in order to serve the purpose of human activity. Humans impose urban
structures on the environment and in doing so cause signiﬁcant damage. Swynge-

308
P. Garnett
douw’s (2006) paper, Circulations and metabolisms: (Hybrid) Natures and (Cyborg)
cities, describes how the “intermingling of things material and things symbolic pro-
duces a particular socio-environmental milieu that welds nature, society and the city
together, often through many layers of networked technostructures (like pipes, cables,
relay stations, logistical apparatuses and the like)” [38]. The city is an assemblage, or
complex system, of parts and their interactions in space. However, Swyngedouw goes
on to say that “[a]lthough the city turned into a metabolic vehicle, the rift between
the social and nature became in fact rather deeper than ever engrained in the urban
or modern imagination” [38]. Suggesting that the function of the city has a place of
human habitation has become increasingly divorced from nature. Re-imagining the
city as part of a living ecology, that is itself alive, should motivate us to change the
relationship of cities with the rest of the environment (and perhaps our interactions
with the environment in general). A living city would need to co-exist with the rest
of its environment to be sustained by it, not be an imposition and source of damage
in terms of toxic wastes and other pollution. Moving further in this direction would
also improve the resilience of the cities to change and allow them to adapt to the
changing environment around them. We should develop cities that are part of the
environment, not an imposition on it.
Current development of smart and eco-cities has been criticised for producing
cities that are not connected, rather they are desperate parts that do not efﬁciently or
effectively work together, or exhibit the desired sustainability. A reality that is at odds
with the goals of smart and eco-city projects [9]. Cugurullo’s (2018) paper, Exposing
smart cities and eco-cities: Frankenstein urbanism and the sustainability challenges
of the experimental city, frames this in terms of Frankestien urbanism “which draws
upon Mary Shelleys novel as a metaphor for unsuccessful experiments generated by
the forced union of different, incompatible elements” [11]. Current development of
smart cities is not being developed using a complex thinking approach [8], and the
result is a mess of loosely connected parts that will not produce the change that was
intended.
Young’s (2006) paper, Cybernetic Beunos Aires captures the changing nature
cities more through changing culture [49]. Where he describes tangos that lament
the creation of an “urban dystopia”, including “the impact of the shift from human
to machine-mediated contact” and surveillance, both are a likely consequence of the
implementation of the goals smart-city projects. In order to control a smart cities
data is required from sensors, this has signiﬁcant potential downsides, including the
transforming of citizens into sensors and the enhanced surveillance that comes with
that. The technologies deployed to control smart cities could therefore be used to
control society, as well as the city which houses it [25].
A truly living city would not require such oppressive top-down control systems,
instead control would be decentralised and perhaps impossible to locate. It would be
embedded into the system as a whole, woven into the connections and relationships.
Interventions would have to be made using the framework of complexity thinking
if they are to be successful, ot avoid a return to simplistic and reductionist thought

Growing Smart Cities
309
processes and decision making. However signiﬁcant challenges remain, not least a
radical re-framing of our own role within a city, if we are ever to inhabit such a living
city.
References
1. Afanasev, V.G.: The Scientiﬁc Management of Society, vol. 54482. Progress Publishers (1971)
2. Allen, P.M.: Cities and regions as evolutionary, complex systems. Geogr. Syst. 4, 103–130
(1997)
3. Batty, M.: Cities and Complexity: Understanding Cities with Cellular Automata, Agent-based
Models, and Fractals. MIT Press (2007)
4. Batty, M.: A perspective on city dashboards. Reg. Stud. Reg. Sci. 2(1), 29–32 (2015)
5. Bettencourt, L.M.A.: Cities as complex systems. Modeling Complex Systems for Public Poli-
cies (2015)
6. Byrne, D.: What is complexity science? thinking as a realist about measurement and cities and
arguing for natural history. Emergence 3(1), 61–76 (2001)
7. Capra, F.: The Web of Life: A New Synthesis of Mind and Matter (1996)
8. Caves, L.S.D., Melo, A.T.: (gardening) Gardening: a relational framework for complex thinking
aboutcomplexsystem.In:Walsh,R.,Stepney,S.(eds.)NarratingComplexity.Springer,London
(2018)
9. Chang, I.C.C.: Failure matters: reassembling eco-urbanism in a globalizing China. Environ.
Plan. A 49(8), 1719–1742 (2017)
10. Crooks, A., Castle, C., Batty, M.: Key challenges in agent-based modelling for geo-spatial
simulation. Comput. Environ. Urban Syst. 32(6), 417–430 (2008)
11. Cugurullo, F.: Exposing smart cities and eco-cities: Frankenstein urbanism and the sustainabil-
ity challenges of the experimental city. Environ. Plan. A 50(1), 73–92 (2018)
12. Espejo, R.: Cybernetics of governance: the cybersyn project 1971–1973. In: Metcalf, G.S. (ed.)
Social Systems and Design, pp. 71–90. Springer, Tokyo, Japan (2014)
13. Franklin, S., Graesser, A.: Is it an agent, or just a program?: a taxonomy for autonomous
agents. In: International Workshop on Agent Theories, Architectures, and Languages, pp. 21–
35. Springer (1996)
14. Gardner, M.: Mathematical games—the fantastic combinations of John Conway’s new solitaire
game “life”. Sci. Am. 223, 120–123 (1970)
15. Gardner, M.: Cellular automata, self-reproduction, garden of Eden and game life. Sci. Am.
(1971)
16. Garnett, P.: Total systemic failure? Sci. Total Environ. 626, 684–688 (2018)
17. Gershenson, C., Santi, P., Ratti, C.: Adaptive cities: a cybernetic perspective on urban systems
(2016)
18. Glushkov, V.M.: Thinking and cybernetics. Sov. Stud. Philos. 2(4), 3–13 (1964)
19. Harwood-Smith, J.: Beta-life: stories from an a-life future. Foundations 45(125), 109 (2016)
20. Jacobs, J.: The Death and Life of American Cities (1961)
21. Jacobs, J.: The Nature of Economies. Modern Library (2000)
22. Jane, J.: Economy of Cities (1969)
23. Kauffman, S., Clayton, P.: On emergence, agency, and organization. Biol. Philos. 21(4), 501–
521 (2006)
24. Kitchin, R., Lauriault, T.P., McArdle, G.: Knowing and governing cities through urban indica-
tors, city benchmarking and real-time dashboards. Reg. Stud. Reg. Sci. 2(1), 6–28 (2015)
25. Krivý, M.: Towards a critique of cybernetic urbanism: the smart city and the society of control.
Plan. Theory 17(1), 8–30 (2018)
26. Manson, S.M.: Simplifying complexity: a review of complexity theory. Geoforum 32(3), 405–
414 (2001)

310
P. Garnett
27. Martin, S.J., Funch, R.R., Hanson, P.R., Yoo, E.H.: A vast 4,000-year-old spatial pattern of
termite mounds. Curr. Biol. 28(22), R1292–R1293 (2018)
28. McLoughlin, J.B., Webster, J.N.: Cybernetic and general-system approaches to urban and
regional research: a review of the literature. Environ. Plan. A 2(4), 369–408 (1970)
29. Medina, E.: Cybernetic Revolutionaries: Technology and Politics in Allende’s Chile. MIT Press
(2011)
30. Morozov, E.: The planning machine: project cybersyn and the origins of the big data nation.
New Yorker 13 (2014)
31. Pias, C.: Cybernetics-the Macy Conferences 1946–1953: The Complete Transactions. Univer-
sity of Chicago Press (2016)
32. Plowman, D.A., Baker, L.T., Beck, T.E., Kulkarni, M., Solansky, S.T., Travis, D.V.: Radical
change accidentally: the emergence and ampliﬁcation of small change. Acad. Manag. J. 50(3),
515–543 (2007)
33. Santé, I., García, A.M., Miranda, D., Crecente, R.: Cellular automata models for the simulation
of real-world urban processes: a review and analysis. Landsc. Urban Plan. 96(2), 108–122
(2010)
34. Schmitt, C.L., Tatum, M.L.: The Malheur national forest location of the world’s largest living
organism [the humongous fungus]. Agriculture Forest Service Paciﬁc Northwest Region 17(11)
(2008). http://www.fs.usda.gov/Internet/FSE_DOCUMENTS/fsbdev3_033146.pdf
35. Stepney, S.: Complex systems for narrative theorists. In: Walsh, R., Stepney, S. (eds.) Narrating
Complexity, pp. 27–36. Springer International Publishing, Cham (2018)
36. Stepney, S., Polack, F.A.C., Turner, H.R.: Engineering emergence. In: 11th IEEE International
Conference on Engineering of Complex Computer Systems (ICECCS’06), p. 9 (2006). http://
ieeexplore.ieee.org
37. Sterman, J.D.: Learning from evidence in a complex world. Am. J. Public Health 96(3), 505–
514 (2006)
38. Swyngedouw, E.: Circulations and metabolisms: (hybrid) natures and (cyborg) cities. Sci. Cult.
15(2), 105–121 (2006)
39. Terna, P.: Creating artiﬁcial worlds: a note on sugarscape and two comments. J. Artif. Soc.
Soc. Simul. 4(2), 9 (2001)
40. Turing, A.M.: The chemical basis of morphogenesis. Philos. Trans. R. Soc. Lond. B Biol. Sci.
237(641), 37–72 (1952)
41. Turner, H.R., Stepney, S., Polack, F.A.C.: Rule migration: exploring a design framework for
emergence. Int. J. Unconv. Comput. 3(1), 49 (2007)
42. Vicsek, T.: The bigger picture. Nature 418(6894), 131 (2002)
43. von Foerster, H.: Cybernetics of cybernetics. In: von Foerster, H. (ed.) Understanding Under-
standing: Essays on Cybernetics and Cognition, pp. 283–286. Springer, New York, NY (2003)
44. Von Neumann, J., Burks, A.W.: Theory of Self-reproducing Automata. University of Illinois
Press Urbana (1996)
45. White, R.: Cities and cellular automata. Discrete Dyn. Nat. Soc. 2(2), 111–125 (1998)
46. Wiener, N.: Cybernetics: Control and Communication in the Animal and the Machine. Wiley
(1948)
47. Wolfram, S., Mallinckrodt, A.J.: Cellular automata and complexity. Comput. Phys. 9(1), 55–55
(1995)
48. Worall, M.: Homeostasis in nature: nest building termites and intelligent buildings. Intell. Build.
Int. 3(2), 87–95 (2011)
49. Young, R.: Cybernetic buenos aires. Revista Canadiense de Estudios Hispánicos 31(1), 131–
146 (2006)

On Buildings that Compute. A Proposal
Andrew Adamatzky, Konrad Szaciłowski, Zoran Konkoli, Liss C. Werner,
Dawid Przyczyna and Georgios Ch. Sirakoulis
Abstract We present ideas aimed at bringing revolutionary changes on architec-
tures and buildings of tomorrow by radically advancing the technology for the build-
ing material concrete and hence building components. We propose that by using
nanotechnology we could embed computation and sensing directly into the mate-
rial used for construction. Intelligent concrete blocks and panels advanced with
stimuli-responsive smart paints are the core of the proposed architecture. In par-
ticular, the photo-responsive paint would sense the buildings internal and external
environment while the nano-material-concrete composite material would be capable
of sensing the building environment and implement massive-parallel information
processing resulting in distributed decision making. A calibration of the proposed
materials with in-materio suitable computational methods and corresponding build-
ing information modelling, computer-aided design and digital manufacturing tools
could be achievedvia models and prototypes of information processing at nano-level.
A. Adamatzky (B)
Unconventional Computing Laboratory, UWE Bristol, Bristol, UK
e-mail: andrew.adamatzky@uwe.ac.uk
K. Szaciłowski · D. Przyczyna
AGH University of Science and Technology, Academic Centre for Materials
and Nanotechnology, Kraków, Poland
D. Przyczyna
AGH University of Science and Technology, Faculty of Physics and Applied
Computer Science, Kraków, Poland
Z. Konkoli
Chalmers University of Technology, Department of Microtechnology and Nanoscience,
Göthenburg , Sweden
L. C. Werner
Institute of Architecture, Technical University of Berlin, Berlin, Germany
G. Sirakoulis
Department of Electrical & Computer Engineering, Democritus University of Thrace,
Xanthi, Greece
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_13
311

312
A. Adamatzky et al.
The emergent technology sees a building as high-level massive-parallel computer—
assembled of computing concrete blocks. Based on the generic principles of neuro-
morphic computation and reservoir computing we envisage a single building or an
urban quarter to turn into a large-scale sensing substrate. It could behave as a univer-
sal computer, collecting and processing environmental information in situ enabling
appropriate data fusion. The broad range of spatio-temporal effects include infras-
tructural and human mobility, energy, bio-diversity, digital activity, urban manage-
ment, art and socializing, robustness with regard to damage and noise or real-time
monitoring of environmental changes. The proposed intelligent architectures will
increase sustainability and viability in digitised urban environments by decreasing
information transfer bandwidth by e.g, utilising 5G networks. The emergence of
socio-cultural effect will create a cybernetic relationship with our dwellings and
cities.
1
Introduction
Nowadays, trends of smart homes are entering in our everyday lives [9, 10, 16, 18,
39]. Multinational enterprises tend to providing virtual integration not only in our
smart phones but also in smart speakers installed in our houses. With the addition of
home appliances participating in the Internet-of-things (embedded with electronics,
software, sensors, actuators andconnectivity) our houses arebecomingsmarter (more
responsive to their inhabitants) [23, 36]. Even before they are built we can—with
the assistants of virtual reality and augmented reality—experience the spaces we are
planning to inhabit in future. With the help of touch-sensitive virtual environments
our senses start merging with the matter to be applied in the material world outside
of the ubiquitous world of bits and bytes.
What if the process could go either way and also the building could sense us?
What if intelligent matter of our surrounding could understand us humans, give
us feedback and communicate with us. What if the walls surrounding us were not
only supporting our roofs, but had increased functionality, i.e. sensing, actuating,
calculating, processing, communicating and producing power? What if each brick,1
or building block, was a self-powered, decentralised computing entity that would
comprise a part of an emerging, large-scale parallel computation? Then our smart
buildings would be transformed to intelligent, computing, cerebral organisations
that we could not only live in but also interact in a holistic cybernetic way. Those
organisations would also offer an active unparalleled protection from, e.g. crime
or natural disasters, could warn us against a dangerous structural damage hidden
deeply in the walls, or simply give us joy by sending a jolly image to our human
visual apparatus. In this chapter we discuss how to build such homes.
1We are well aware that modern building technologies have a wide range of building blocks and
panels; however, we are using word ‘brick’ here for simplicity and readability reasons.

On Buildings that Compute. A Proposal
313
We address the challenge of implementation of new building materials in buildings
that are capable of power production, sensory readings extraction, communication
and computation. The vision is moving from concurrent smart homes that have lim-
ited sensing capabilities and can connect to the internet, to advanced computing
homes of the future that have advanced capabilities in sensing, actuating, process-
ing, communications and computing. This will be achieved by employing materials
with embedded information processing abilities and multiple, elementary devices
interconnected in a massive grid.
We envisage embedding sensing and associated cognitive abilities directly in a
building material and implement computation at several levels of hierarchy. Similar
to the human body each millimetre of a concrete block/brick will be able to sense its
environment; sensory processing and decision making is done at the level of a single
block/brick, more complex problems (shape recognition, learning, predictions) are
solved by a cluster of blocks/bricks co-operating in a wall. Thus a building becomes a
super-computer at macro-scale while walls will be massive-parallel array processors
Fig. 1 Prototyping computing architectures. Multi-scale computation implies generation of infor-
mation at the micro-scale in the material, propagation and modulation of information by passing
through higher structural elements: the concrete block, wall, building. See further descriptions in
the text

314
A. Adamatzky et al.
at a meso-scale and each component of a wall acts as massive-parallel computing
device at micro-scale encompassing in-memory computing.
Key concepts of the computing architecture are illustrated in Fig.1. Functional
nanoparticles with photo-, chemo- and electro-sensitive and a range of electrical
properties spanning all possible electronic elements are mixed in a concrete. Build-
ing blocks are made of the concrete. The building blocks are also equipped with
processors for gathering information from distributed sensory elements, aids in deci-
sion making and location communication and enabling them to perform advanced
computing coupled with modern in-memory principles. The blocks are assembled
into a wall, which constitutes with massive parallel array processor.
These our ideas are in tune Susan Stepney et al. thinking on gardening cyber-
physical cities [64] however we adopted more pragmatic component of computing
materials.
The chapter is structured as follows. Section 2 presents our suggestion to develop
new semiconducting nano-materials capable of advanced sensing and information
processing. In Sect. 3 we present the theoretical framework of massive-parallel com-
puting and reservoir computing, the principle foundations of an efﬁcient and effective
computing system to be applied ﬁrstly on a wall-scale, and in a next step on a building
scale. Section 4 suggests using modular standard sized blocks in order to establish
the massively-parallel array processor and a communication grid between the blocks
that are equipped with high-level processing units. In Sect. 5 we engage with an
architectural vision featuring computing architectures and in Sect. 6 we discuss our
overall vision as presented analytically before.
2
Computing Concrete
Novel semiconducting nanomaterials capable of advanced sensing and information
processing include construction and decorative materials, in particular we focus on
nanomaterials on titanium dioxide and other nanopigments already present in con-
crete and wall-paints featuring photo-catalytic, photoelectric and memristive prop-
erties. We will develop novel in-materio computing schemes, including fusion of
binary, ternary and fuzzy logic [13, 25, 66, 77], with recurrent neural networks [40,
79] and memristive synaptic devices [19, 30, 50]. The dynamics of charge carriers
within semiconducting nanostructures [67], combined with charge trapping proper-
ties of carbonaceous nanostructures [51] will be utilised in construction of unique
distributed reservoir computing systems with ﬂexible connections between individ-
ual nodes. The large fraction of sensing and computing will be performed in specially
designed materials incorporated into or deposited onto construction materials (con-
crete blocks, bricks, parge coat and wall paints). Material-based sensing will address
light/temperature sensing, sounds/vibrations, and some pre-deﬁned chemical species
(e.g. carbon monoxide or other combustion products).
Wide band-gap semiconductors belong to the most widely studied materials.
Whereas studies on photovoltaics, solar fuels and heterogeneous photocatalysis

On Buildings that Compute. A Proposal
315
involving semiconducting nanoparticles seem to dominate current material science,
the information-processing aspect of nanoscale semiconducting materials is still
underrepresented. Semiconducting nanomaterials offer a tremendous diversity and
versatility of material properties, including optical and electrical properties, which
can be utilised for computational purposes. Some semiconducting nanomaterials,
including titanium dioxide, cadmium sulﬁde and lead iodide perovskites has been
already employed in various computing architectures, including binary logic [25]
(either simple logic gates or more complex computing circuits, like reconﬁgurable
logic gates [66], demultiplexers [35] or binary half adders) [53], ternary logic [77],
fuzzy logic [13] and neuromorphic computing [51]. Moreover, the same class of
materials, and with very similar surface modiﬁcations, have been used for advanced
sensing applications [52]. Furthermore, despite their photocatalytic activity, some
organic-inorganic hybrid materials are exceptionally stable towards photo- and elec-
trodegradation. Therefore, we postulate, that this class of materials, fulﬁlling the
paradigms of in-materio computing, is suitable as concrete/paint additives which
will add computational power to construction materials. These materials, however
(like any other information-processing system) cannot operate without sources of
data and energy. Due to photovoltaic properties of wide band gap semiconductors
they can work in computing devices as the information processing structures and
power generators at the same time. Furthermore, due to appropriate chemical struc-
ture of the surface, semiconducting nanostructures based on wide band gap oxide
materials should easily integrate with classical construction materials, e.g. concrete.
Thus, we should obtain hybrid construction materials capable of collecting infor-
mation from users and environment, at the same time performing some forms of
computation and yielding desired response to the users via their internal electrical
activity. They should be able to monitor their own internal structure (e.g. detect local
increase of humidity or cracks within the construction material). All these functions
will be possible due to three factors:
• compatibility of inorganic wide band gap semiconductors with currently used
construction materials such as concrete, clay bricks, steel
• responsiveness of these materials to various stimuli, mainly optical and electrical,
but also mechanical and chemical
• the ability of implementation of complex logic functionality including ternary and
fuzzy logic on the basis of simple physical phenomena.
Our preliminary study indicates, that traditional concrete-based materials can be
used for advanced electrical signal processing. Concrete containing ca. 10% percent
ofaluminiumandsteelshavingsand5%ofsemiconductingnanoparticles—antimony
sulfoiodide nanowires carbon nanotubes and cadmium sulﬁde nanoparticles—gains
unique electrical properties, which can be approximated by distributed random
resistor-capacitor network with some hysteresis. The latter results from slow ionic
movements within the concrete matrix combined with rapid electric response result-
ing from sub-percolation threshold arrangement of metallic particles embedded in
concrete matrix, which may act as a complex signal conditioning circuit (Fig.2).

316
A. Adamatzky et al.
Fig. 2 A photo taken during signal modulation using a piece of nanoparticle-doped concrete (a)
and a close-up view of concrete computing element’ (b)
The device was subjected to square wave, saw tooth and sine wave stimulation.
Two independent arbitrary signal generators, connected with randomly chosen con-
tacts, were used to stimulate the device. The square wave generator was tuned to the
frequency of 100Hz, and the other generator (square wave, sine wane, saw tooth)
was tuned to the frequency of 101Hz. An output signal was recorded also at ran-
domly chosen pin of the pin-grid array socket. The response of the system becomes
extremely complex due to superposition of capacitive and resistive coupling within
the concrete block and also due to an interplay of electrical and ionic conductivity,
like in ion-diffusion memristors. The complexity of the response of ‘concrete-based
microprocessor’ is shown in Fig.3.
The quasi-chaotic character is a result of nonlinear characteristic of some of the
junctions present in the material, possibly of memristive character. A simple model
involving randomly connected resistor-capacitor loops, implemented in Multisim
(Fig.4) yields much less complex attractors.
This suggests that a concrete block with appropriate admixtures of metallic macro-
particles and semiconducting nanoparticles offers signiﬁcantly complex nonlinear
behaviour and internal dynamics, associated with capacitive effects superimposed
on some ion diffusion processes, that can be used as a computing node in a reservoir
computing system. This dynamics, in turn, can be utilized for computation provided
that the formal requirements of computation in physical systems are fulﬁlled [27].
In the worst case novel hybrid materials will still provide distributed multisensory
sensing system and will provide signal processing capabilities.
Furthermore, in accordance to the aforementioned reconﬁgurability of the pro-
posed nanomaterials, logic conﬁgurable circuits that constitute a key architecture for
integrated nanoscale electronics can be developed in the memristive networks [73].
Such memristive logic circuits can overcome the disadvantages of memristive impli-
cation logic (stateful logic) [14], namely the necessity to perform lengthy sequences
of stateful logic operations in order to synthesize a given Boolean function. The focus
then should be ﬁrstly on hybrid transistor-memristor structures (together with logic
gate implementation and/or the device-level requirements), where the logical state

On Buildings that Compute. A Proposal
317
Fig. 3 Chaotic-like attractors produced by the ‘concrete-based microprocesor’ upon stimulation
with the following signal combinations: a square wave and square wave, b square wave and sine
wave, c square wave and saw tooth (c). The same connectivity was used in all these cases
is represented as a voltage, instead of the normally used (mem)resistance, apply-
ing different logic circuit schemes like memristor-based combinational logic circuit
design [73] or Memristor-Ratioed Logic (MRL). Moreover, all memristive based
logic/computational circuits, i.e. computation of Boolean functions in memristor-
only circuits [48] as well as logic computation parallelism with memristors [49]
would be examined as feasible promising in-memory computing nano-architectures.
As an alternative novel analog computing concepts utilizing massively-parallel com-
puting architectures, enabled by networks of memristors could be also equally con-
sidered [76]. Having in mind semiconducting nanomaterials, as the ones earlier
described in this section, with possibly different switching abilities, could result to
better performance of the proposed computing medium and thus permit produc-
tive interfering in computations. In principle, such network-based computations [5]

318
A. Adamatzky et al.
Fig. 4 A schematics of the random resistor—capacitor network takes as a a model of metal-
and semicondictor-doped ‘computing concrete’ and b an attractor produced by the model upon
stimulation with the square wave and sine wave combination
meaning massively parallel computations within array-like structures which accom-
modate networks of memristive components [74] could be also another option for
enabling the computation in the proposed novel semiconducting nanomaterials. Open
issues to be discussed in this last option for appropriate computing, include among
others, the network initialization, the requested number of devices to initialize and
read, the suggested voltage supply, networks’ homogeneity and regularity and com-
putation time and resulting devices densities [75].
3
Computing Bricks
During the design and prototyping of computing architectures we combine massive-
parallel amorphous computing substrates (concrete block level) to act as an array
processor with a regular grid-based structure (wall level) (Fig.1). As an overarching
computing principle, we propose to use reservoir computing.
A typical reservoir computer features two parts, a dynamical system (the reservoir)
that responds to an external signal (the input to the computation), and a readout
layer that is used to analyse the state of the system (to produced the output of the
computation) [37–39, 71]. The general idea is that the readout layer should be simple,
i.e. the computation should be performed by the reservoir. The programming process
involves tuning of the readout layer. This is usually done through a rather direct
supervised learning procedure, that does not involve deep learning or elaborate back-
propagation methods. Such artiﬁcial intelligence is easy to train, e.g., using gradient
descent machine learning algorithms—towards a speciﬁc task, and does not require
an extensive technological or engineering overhead to implement [29]. As a result,
reservoir computing has proven extremely useful for the design of a plethora of

On Buildings that Compute. A Proposal
319
neuromorphic information processing applications that require fast real-time analysis
of the time-series data [28, 32, 61, 68, 80].
Often, to build reservoir computing solutions, one simulates the operation of a
reservoir computer on the standard digital computer. Interestingly, up to now there
are very few practical implementation of reservoir computers in hardware. Up to
date pioneering efforts exploit photonic systems [31, 69, 70], complex amorphous
materials [15, 17, 21, 22, 72], etc. In here we suggest yet another computing sub-
strate: the building and its components; walls, blocks or other pre-fabricated con-
struction components, and ultimately the material it is made of. We suggest that
innovative prototypical building materials offer new computing paradigms without
advanced electronic engineering burden. There is no need to carefully micro-engineer
each component individually, which would be hardly feasible, since the charge car-
rier dynamics in self-assembled structures automatically provides a molecular or
nanoparticle-scale computing platform. While the sensing or computing performance
of individual structure (chemically engineered nanoparticle) will be low, a large num-
ber of particles present in construction/decorative materials will result in complexity
that provides sufﬁcient computing efﬁciency and energy effectiveness.
How much intelligence can one squeeze in a single brick? In principle, if one
is to trust philosophers, the answer is: every computation one can think of [55]. In
the appendix of his cited book [55], Hillary Putnam conceived a thought experiment
to challenge the idea that the state of the mind can be described as a state of a
computing automaton. To show that the ability to compute does not automatically
constitute intelligence, Putnam provided an elaborate construction how to compute
with a rock, and the computing brick is not far away. Naturally, Putnam’s arguments
have been severely criticised on the account that to turn a rock in a computer one
would need to equip it with computationally heavy interface layer. There are other
hidden paradoxes associated with that idea, that are not that obvious. For example,
how come that we have to struggle so much to achieve computation if such a simple
object as a rock or a brick has an intrinsic ability to compute? As it turns out, to
resolve such paradoxes, the right question to ask is what is the computation that can
be performed naturally by an object. The turning point where the complexity of the
interface layer overpowers the complexity of the system marks the natural boundary
of what the system can compute naturally [81]. What is this naturally boundary for
a brick? Without a dedicated engineering effort, a brick cannot compute, obviously.
However, we argue that by using the materials discussed above, a brick can compute
not only in the Putnam’s sense (as any rock would do), but as a practical computing
device, if used as a reservoir computer.
A reservoir computer consists of two parts, a dynamical system that converts
inputs into the states of the reservoir, and the readout layer that is used to analyse the
state of the reservoir. The act of computation is the transformation of the external
input into the internal state of the reservoir. This step is carried out automatically
by the system. In principle, one should be able to use every dynamical system this
way. However, by using rigorous mathematical arguments it has been proven that
only those systems that separate inputs can operate like that. As a rule of thumb, the
more complex the system, the more likely it is a good reservoir. Naturally, there is a

320
A. Adamatzky et al.
plethora of possibilities to achieve such complexity in the design of the computing
building.
For example, the interconnected network of components embedded into the build-
ing constitutes a complex dynamical system. This system can be used as a reservoir
in order to achieve reservoir computing. If one could construct such buildings, then a
great deal of information could be pre-processed by the building before this informa-
tion is sent to a central unit for analysis and decision making, and the building would
essentially function as an intelligent, cognitive information processing substrate.
Embedded intelligence could be used for both in situ and real-time computation.
The key beneﬁts of such reservoir computing are indeed numerous:
• The computing is neuromorphic by construction, which implies that the system
would be robust and it would function even if part of it is damaged.
• Noise-tolerance: occasional ﬂuctuations would be ignored.
• Ability to generalise: any neuromorphic architecture does it, and if a genuinely
different conditions are exhibited, the system would respond in a reasonable way.
• Information bandwidth (and energy) reduction: due to the pre-processing, the
information bandwidth (and energy consumption) can be signiﬁcantly reduced.
Developing a theoretical mechanistic understanding regarding how a single brick
can respond to the external stimuli, and how to interpret these responses for informa-
tion processing purposes, is not an easy task. What are the challenges? Very likely,
theoretical insights are only possibly through virtual experiments, where the com-
puting operation of bricks is simulated digitally. To do this, we ﬁrstly need to develop
a dynamical model of the amorphous material that the brick is made off. Once this
is done and the principles are clear, ideally we can develop a circuit analogue of the
previously developed model.
What should an adequate model look like? The materials will respond to envi-
ronmental inﬂuences, e.g. light, temperature, mechanical stresses. These stimuli will
very likely be converted into electrical signals in the material. The key challenges
are to predict
• how these signals propagate through the brick, and
• how to use them for computation (e.g. sensing).
Should it be possible to understand the interplay between the brick design and its
information processing features, then one could identify the most optimal designs
with practical relevance for architectural purposes.
Developing an adequate amorphous material model might be the biggest theo-
retical and, indeed, practical challenge. Once the liquid concrete hardens, it forms
at random a network of linked objects that conduct, and potentially modify/tune
electrical current. This amorphous material can be modelled as a random network
of electronic components, but the challenge is to suggest an appropriate topology
for the network and the components. One can envision a series of models with an
increasing degree of complexity.
As a ﬁrst approximation, it might be reasonable to consider memristor-like com-
ponents only, and assume the nearest neighbour architecture. If one could infer from

On Buildings that Compute. A Proposal
321
the experiments the probability distribution function that governs the shape of the
network one could perform realistic simulations. Given the fact that a brick is a
macroscopic object, statistical physics approaches naturally suggest themselves as
a modelling technique. By using the models one can extract the typical response
behaviour that is stable across all network realizations, and ﬁt to the experimental
data. The model could be further augmented with an additional layer of detail, by
e.g. considering charge blockade effects. As the readout layer it is natural to use
microprocessor and embed it in the brick.
A major challenge is to model environmental inﬂuences. Sure, one can model
these as current/voltage sources or as environment-sensitive electronic components,
but it is not entirely clear how to represent the appropriate input to the network. For
example, while one can develop a model of the brain, it is hard to know how to
simulate an experience of seeing a ﬂower; which voltage signal should one assume
at the axon that comes from the retina?
The fully parametric model can be used to characterise the single-brick infor-
mation processing capacity. One can simulate a fully functional brick for a range
of information processing applications. Performance metrics in terms of computing
ability and speed, power consumption, data rate, etc. of the proposed circuitry can
be deﬁned and studied, and help to identify the most optimal brick designs and ideal
use cases. This mechanistic understanding is important, since it can aid the design
of experimental protocols, physical and software interface, and communication pro-
tocols to implement computation with a single brick.
Some key strategic tasks might involve the evaluation of few selected use cases
for practical demonstration. In the process one needs to weight in factors as practical
relevance versus ease of implementation.
4
Computing Walls
Drawing from the collection of thoughts and design ideas in the previous section this
part of the chapter describes a feasible proposal for construction. We aim to build a
wall of computing bricks2 with a small power density that collectively perceives its
environment, makes decisions about reconﬁguration of the informational contents
and performs basic computational primitives with their closest neighbours.
A wall can be seen as hexagonal array of processing elements, where each brick,
apart of those placed on the edges, has six neighbour (Fig.5a). The wall therefore
can be abstracted as a hexagonal cellular automaton. This is an array of ﬁnite-state
machines, or cells. Each cell updates its state in a discrete time, depending on states
of its closest neighbours. An example is shown in (Fig.5b, c, d, e and f). This is an
excitable cellular automaton. Each brick-cell takes three states: resting, excited and
refractory. In the automaton illustrated, a resting brick becomes excited if at least
one of its six neighbours is excited. An excited brick moves to refractory state and
2We want to stress again that ‘brick’ is a generalisation of various technological units of a building.

322
A. Adamatzky et al.
Fig. 5 a Local neighbourhood of a computing brick. Coordinates of neighbours of the brick (x, y)
are shown explicitly. b–f Brick wall as an excitable medium. Propagation of the excitation wave-
fronts. Excited bricks are red, refractory are blue, resting are brown
then to resting state unconditionally, i.e. independently on states of its neighbours.
In this example, information can be transmitted along a building’s wall by the wave
of excitation. This is a broadcasting, or one-to-all transmission. By adjusting rules of
brick-cell transition, as e.g. demonstrated in [4, 6] we can achieve localised transmis-
sion of information. In this case, ‘quanta’ of information are represented by compact
travelling patterns of activator, or excited, states which propagate in a predetermined
direction whilst conversing the velocity vector and shape. See examples in Fig.6.
The key features of the computing wall are
• parallelism and multitasking: each brick senses several states of environment and
its neighbours, processes several tasks at a time;
• distributed sensing: each brick is a massive distributed sensor of millions of nano-
and micro-scale computing elements (smart paint and ﬁctionalised cement);
• changes in luminosity and temperature affect thousands of macro-computing ele-
ments;
• task ﬂexibility: the array of computing-bricks is dynamically reprogrammed to
switch between tasks;
• fault tolerance: the modular structure of the wall allows operation despite faults,
scalability, lower maintenance costs.

On Buildings that Compute. A Proposal
323
Fig. 6 Exemplar conﬁgurations of reaction-diffusion automata [4] with localised transmission of
information. a A typical quasi-stable conﬁguration of the reaction-diffusion cellular automaton, see
rules in [4, 6], which started its development in a random initial conﬁguration (with 1/3 probability
of each cell-state). Cell-state I (inhibitor) is shown by a black disk, cell-state A (activator) by a
circle, and cell-state S (substrate) by a dot. We can see there are two types of stationary localisations
(glider eaters) and a spiral glider-gun, which emits six streams of gliders, with frequency a glider
per six time steps in each glider stream. b A mobile glider gun travel west and emits three streams
of gliders travelling north-east, east and south-east. From [4, 6]
The bricks will be made of and coated with smart materials, they will implement
sensory fusion of data collected by smart materials, perceive results of low-level
information processing undertaken by the smart materials and do high-level compu-
tation.
The bricks will be equipped with high-level processing units (CPU, memory,
analog and digital input and outputs). The units will be gathering outputs form the
low-level computing materials, analysing the data and exchanging the data with pro-
cessing units in six neighbouring building blocks. The task will deal with evaluating
parameters of building blocks, outsourcing or designing in the house elementary
processing units, undertaking functionality and structural integrity tests. The brick
might be supplied with photovoltaic panels (Fig.9), to provide an autonomous power
supply. In the result we will get manufacturing protocols and prototypes of brick-
processors, fromwhichthecomputinghousewill bebuilt. Benchmarkingwill include
processingpower,heatgeneration(andpotentialassociatedcoolingproblems),power
consumption, water tightness, frost resistance, mechanical strength.

324
A. Adamatzky et al.
Direct communication between bricks is possible, if e.g. a conductive mortar is
uses, however developing a wireless communication network might be more prac-
tical. To develop such a network we should analyse a feasible range of wireless
transmitters and receivers, evaluate antenna size as a function of frequency, commu-
nication neighbourhood size, develop a hardware prototype (just processing units) of
the network, analyse communication protocols and address security issues. Particular
attention should be paid to fault tolerance and energy saving in the communication
network, routing protocols in scenarios of several failed units and distributed storage
of information and allocation of concurrent tasks.
A physical prototype could be a 3m by 3m wall of 600 computing bricks, with
high-level processing units embedded, saturated with carbon nanostructures sensi-
tive towards pressure, sounds and coated with photoresponsive smart paints. This
massive-parallel array of multiscale computing units will have parallel optical and
mechanical inputs and parallel outputs. The computational potential of the array can
be benchmarked on the following tasks: computational geometry (plane tessella-
tions or Voronoi diagrams), image processing (contouring, dilation, erosion, detec-
tion of features, shape restoration), pattern recognition (via perceptron like models
and learning neural networks), optimisation on graphs. There are cellular automata
implementations of the solutions of these problems, see e.g. [1, 2, 12, 54, 56–59],
therefore practical implementation would be a matter of technicalities.
As an early proof of concepts we consider our design of a hexagonal array of
processors and manipulators capable for object recognition, sorting and transporta-
tion (Fig.7). The processor boards share power and communicate locally [78]. Each
processor has also a colour sensor to recognise different objects. Each actuator is con-
trolled by a microprocessor. The microprocessor in each cell can detect if an object
was present above the cell and the object’s colour, control actuation, communicate
the cell’s state to its six neighbours. In experiments with this physical prototype we
developed efﬁcient algorithms for neighbour communication sharing, and solutions
for determining shapes objects, based on a conﬁguration of an object’s corners.
Hexagonal arrangement of bricks is just an example. Indeed, a wide variety of
computing architecture, even those supporting cellular automaton algorithms, can
be realised on various classes of tilings. Thus, a Game of Life cellular automaton
rules [3] can be run on Penrose tilings [47]. An example of a period four oscillator is
shown in Fig.8a: live cells are shown in black, cells that are always dead are white,
and dead cells to be alive next step are grey. A series of universal cellular automata
on a hyperbolic plane, heptagrid and dodecagrid tiling, are designed in [41–43].
An example of evolution of a strongly universal cellular automaton is shown in
Fig.8a, at the moment shown the automaton detects halting of the simulated Turing
machine.

On Buildings that Compute. A Proposal
325
Fig. 7 Hexagonal array of processors and manipulators. From [78]
(a)
(b)
Fig. 8 a Conﬁgurations of a stationary oscillator in Game of Life like cellular automaton on Penrose
tiling discovered by Owens and Stepney [47, 65]. b Conﬁgurations of a strongly universal cellular
automaton on a heptagrid, designed by Margenstern [42]. The conﬁguration shows detection of the
halting state of the simulated Turing machine. From [44]

326
A. Adamatzky et al.
5
Computing Architecture in Buildings and Urban Systems
‘Smart materials’, ‘physical computing’ and ‘interacting surfaces’ are still rare goods
in architectural design and construction, but have together with the rise of material
intelligence in natural materials, gained increasing relevance. Such ‘thinking mate-
rials’ bring advantages in performance monitoring or function and as enablers for
social interaction, the space and user-architecture-relationship [8, 46]. Programmed
materials, e.g., through simple operations such as slicing, or in composite with coop-
erating materials, have been tested in experimental architectural design studios and
labsoverthelastdecade.Materialsciencesandarchitecturearebeginningtomerge[7,
11].
A number of architecture schools, scholars and researchers have shifted from
designing forms with phenomenological sensing to designing reactive, responsive
and interactive surfaces as well as kinetic components and materials featuring a com-
bination of material intelligence and adaptive mechanisms. The implementation of
machine learning, deep learning and interconnected sensors so far is in its embryonic
age; nevertheless an approach to conceptualise neural networks for architecture is
well on its way. Material sciences and physical computing in architectural design are
relatively new terrains for architects, educational institutions, building regulations,
governments and last but not least the user of a building, the human - and soon also
humanoid and robotic agents and instances. This section focuses on the development
of intelligent buildings by including the computing concrete for bricks in vertical and
horizontal pre-fabricated components, walls (Fig.9).
The design process of a ‘Computing Architecture’, a brick, a component, a build-
ing or a city is highly interdisciplinary and affords a good understanding from each
discipline of the other disciplines methods, scale of accuracy, terminology and time
required for discipline speciﬁc processes. Figure10 shows the four different steps
from concept development (what should the material focused computational archi-
tecture do) via technological challenges and the development of a smart composite
material, the application as building architecture and reservoir computer and ﬁnally
the implementation in the built environment. Throughout the process multiple feed-
back loops steer the ﬁnal product, whereby the demands on the ﬁnal product, its
parameters for success are already being considered in the ﬁrst step of the concept
development. The means of production of the ﬁnal product—here a parametrically
designed and digitally manufactured building component—and its function in the
domain architecture set a number of parameters in stages 1, 2 and 3. In a testing
phase at the end of the process we monitor the prototype’s performance against a
selection of parameters and benchmarks (including human comfort of the occupant
of a computing building) in order to feedback into next iteration of the design pro-
cess. A method based on cybernetics that we developed as ‘closed-loop digital design
process’.
By embedding computing circuits into low-level building materials we will lay
foundations for the design of an environment-sensitive computing infrastructure
deeply embedded into a body plan for a new building typology. Once applied and

On Buildings that Compute. A Proposal
327
Fig. 9 Power supply to computing building. a Solar panel powered computing brick. b An assembly
of bricks and panels

328
A. Adamatzky et al.
COMPUTING ARCHITECTURE: FROM COMPUTING CONCRETE TO COGNITIVE BUILDINGS.
Partner Role 
Partners
UWE Bristol, Unconventional Computing Lab, UK
AGH University of Science and Technology,
Academic Centre for Materials and
Nanotechnology, Poland
Department of Electrical Computer Engineering,
Democritus University of Thrace, Xanthi, Greece
Chalmers University of Technology, Department of 
Microtechnology and Nanoscience, Sweden
 Institute of Architecture, Technical University of Berlin, 
 Germany
Nano Tech
ACMN, Poland 
DoMN, Sweden 
Computing
UWE, UK 
DECE, Greece 
Architecture 
TUB 
Process
Start
Concept
Development
Innovation Field 1 
Building Material 
Innovation Field 2 
Intelligent Building
Component 
Innovation Field 3 
Application in
Construction 
Architecture 
Nano Material
Construction
ready semi-
conducting
material
Computing 
"Bricks"
Computing  
"Walls" 
Layer 1 
Design of
Computing
Building
Components 
Layer 2 
Integration with
BMS for Analysis
and Control 
Prototype for  
Building Component 
End
Project Monitoring 
& Feedback 
CONCEPT – Section 1
TECHNOLOGY – Section 2, 3, 4
APPLICATION – Section 5
EVALUATION
Innovation Monitoring 
& Feedback 
RESERVOIR COMPUTER
Input / Output
Publication
Construction
Product
Nano Tech.
Product 
Database
Database
Fig. 10 Development and design process of a computing building component

On Buildings that Compute. A Proposal
329
integrated in the urban environment, the novel building material offers an unforeseen
plethora of applications, increased robustness, sustainability and a hence a new strat-
egy for human-centred design. The assembly-instructions for construction would be
partly programmed into the material itself, directing digital industrial pre-fabrication
methods to arrive at the desired product. The architect becomes partly designer of
form and beauty and partly a designer of a construction process. Depending on
the advancement of the computational material, the material itself could design the
program for self-formation and assembly. Different to known materials that require
external force for assembly, advanced computing materials shows the behaviour of
what we call ‘auto-morphological assembly behaviour’. Improved building industry
service-and product integration developed during the previous decade known as the
age of industry 4.0 [33] laid the foundations for they process. The building typology
we are proposing is a cyber-physical system in its material sense and also in the way
it is constructed. We envisage the components as information transfer interfaces post-
construction and as information carrier pre- and during the construction process of
the building. The computing building materials will allow for faster, improved design
and design-to-construction process [34].
The application of photosensitive smart paint on the interior—and possibly exte-
rior surfaces of buildings at ground ﬂoor level—elevates the physical interface from
one room to another or from inside to outside, from a passive to an active part of
the building. The proposed materials (doped concrete plus smart paint) are a new
approach to architectural construction. Applying smart materials in building archi-
tecture in combination with an IoT network has a number of beneﬁts for the building
and its occupants.
The architectural prototype we are proposing at this stage has been described
as a 3 × 3m wall made of nearly 600 standard bricks or a wall constructed of
pre-fabricated slabs. On the architectural level we push the physical prototype to
a design proposal tested through simulations and physical modelling. The architec-
tural design includes on the one hand material intelligence and on the other the ability
to kinetically move slightly in order to advance and compliment the digital computa-
tion (enabled through the reservoir computer) with analogue computational (enabled
through local material response to the environment. The design solution in combina-
tion with sensory digital intelligence for performance evaluation and data collection
aims at an adaptable, truly intelligent building, similar to a natural organism. One part
of the concept is to investigate wireless strategies for buildings and cities and living
environments. Another is to investigate new architectural topologies and how they
affect human society in an era post-digital towards computational, biological and
organisational. We will develop a testing ground for urban communication through
principles of the Internet of Things—combining physical prototyping and digital
simulation/modelling. Detailed objectives before construction include [45]
• interface computational materials (concrete, paint and other mineral-based mate-
rial) with the design processes of a building;
• increase sustainable building design;

330
A. Adamatzky et al.
• address the rapid increase in world population necessity for housing and sustain-
ability;
• address the relevant industries (building industry and housing associations) and
markets for industrial pre-fabricated building components;
• developing new efﬁcient and effective tools for building design and performance
(qualitative and quantitative), including scripting tools and data.
The objects after construction would be
• self-diagnosis (structural failure)
• energy efﬁciency
• adaptability to light and darkness—further development of smart light
• pattern recognition leading to increased safety
• assistance for bodily or mentally impaired humans.
The last objective has been tested in environments for blind people in an outdoor
way-ﬁnding system [24]. Emerson [24] mentions a project developed by Ohio State
University testing smart outdoor paint for visually impaired pedestrians, equipped
with a speciﬁcally designed cane. In this respect in architecture we could be looking
at the communication between building and human resulting in a mutual interac-
tive relationship. Equipped with a smart external device, such as a smart phone, or a
smart internal device, such as a microchip implanted under the skin, the human would
directly instruct, communicate or teach the building—and vice versa. A future sce-
nario draws high potential for understanding human behaviour in a speciﬁc building
and more efﬁcient and improved monitoring of performance, especially failures in
parts of the building. The ﬁrst related to machine learning to beneﬁt the occupant, the
latter relates to machine learning to beneﬁt the building fabric and function. Every
building block will monitor its own states and also will be aware about states of its
neighbours. A feature especially relevant in un-manned or hardly manned buildings
such as data-centres, power stations or large bridges.
Computational architecture in architecture and design stems from the 1950s,
where human-computer interaction was ﬁrst explored to assist architectural design
and to make buildings that could react, interact and somewhat think. Cybernetic
architecture as the ﬁrst strand of computational architecture had been pushed to the
extend that cities were envisaged to be super-cities that would operate like super-
computers. Concepts known as architecture of utopia developed primarily in the
1960s and 1970s by Superstudio or Archizoom and discussed by Tafuri, Rowe and
others circulated around the discussion of technology, society and space. ‘No-Stop
City’ by Archizoom was pioneering the idea of an architecture that could grow and
continue without boundaries or hierarchies; its repetitive grid reminded at the cir-
cuit board of a computer. It suggested that advanced technology would destroy the
modern city based on centralization [20, 60, 63].
Applying a regular grid of programmed micro-computers gives breeding ground
for an architecture as network. Similarly to the brain it is not only an idea or utopia,
but a material and immaterial phenomenon throughout the history of architecture
and urban design.

On Buildings that Compute. A Proposal
331
Architectural design on a building level describes the process in which an architect
achieves a solution to combine spatial and functional requirements, with user require-
ments, external parameters (climate, noise, urban context), material choice (struc-
tural, aesthetic, cultural, ecological, economical), fabrication method, construction
method, performance measurement and sustainability [26, 62]. Design by making, a
standard research method in architecture will be applied. We will use computational
tools for surface topography and structural optimisation, material reduction and tool
instruction. Architectural design or urban planning will utilise generative scripting
software for bottom up form-ﬁnding. Digitally driven tools will allow us to negotia-
tion between traditional top-down planning and bottom-up self-organisation. Open
source software will be used to generate form and surface topography of the physi-
cal architectural building prototype of a wall. The building prototype is developed to
serve ‘Direct Digital Design to Assembly’ for an efﬁcient work-ﬂow. The computing
walls will be designed to ﬁt CAM digitally prefabrication processes.
The implementation will be dealt with at several scales. At an architectural scale
we will design a prototype using scripting (algorithms based on neural behaviour),
building information modelling packages including material intelligence, the inte-
gration of climatic data and physical material property data, genetic algorithms. At
the building scale we will construct a digitally manufactured (robotic manufactur-
ing and stereo-lithography printing), intelligent component prototype for a building;
surface topography will change in accordance to the ‘behaviour’ of the building.
At this state a particular attention should be paid to investigate spatial impact and
quality through virtual and augmented reality. At the urban scale we will develop
and test design strategies for industrial pre-fabrication integrating. The urban design
strategies will be developed through simulations and virtual reality for future sus-
tainable and self-organising cities, driven by material intelligence and neuro-network
distributed computing.
6
Discussion
We presented our vision of computing architectures. Functional nanoparticles and
ﬁbres are mixed in a concrete. Blocks made of this concrete are capable for reservoir
computing with potentially thousands of inputs and outputs. Each block is supplied
with a microprocessor for input and output interface and communication with neigh-
bouring blocks. A wall made of the blocks is a massively parallel array processor,
while each block is a massively parallel reservoir computer. Practical implementa-
tion of the idea would involve a high-disciplinary task force. In computer science,
we will need to employ a theory of embedded computation with amorphous sub-
strates and to identifying possible advanced sensing applications with amorphous
reservoirs and the sensing capacity information of deeply embedded sensing sys-
tems in architecture. From nanotechnology and novel construction materials we will
need new nano-materials for in-materio computing and data acquisition (sensing),
development of composite materials combining mechanical properties of concrete

332
A. Adamatzky et al.
and functional properties of nano-materials, new hybrid, stimuli-responsive materi-
als sensitive to light, electric currents, vibrations, sound and humidity. The ﬁeld of
architecture and building environment will give us new building operations monitor-
ing technology through integrated intelligence in the building material, integration
of existing computational design strategies into an intelligent building material.
Acknowledgements KS and DP acknowledge the ﬁnancial support from the National Science
Centre (Poland) within the OPUS project, contract No. UMO-2015/17/B/ST8/01783 and from
Polish Ministry of Science and Higher Education. Authors thank Neil Phillips for precious technical
discussions and Julian F. Miller for helping to improve the paper further.
References
1. Adamatzky, A.I.: Computation of shortest path in cellular automata. Math. Comput. Model.
23(4), 105–113 (1996)
2. Adamatzky, A.I.: Voronoi-like partition of lattice in cellular automata. Math. Comput. Model.
23(4), 51–66 (1996)
3. Adamatzky, A. (ed.): Game of Life Cellular Automata. Springer, Berlin (2010)
4. Adamatzky, A.: Reaction-Diffusion Automata: Phenomenology, Localisations, Computation,
vol. 1. Springer Science & Business Media, Berlin (2012)
5. Adamatzky, A., Chua, L.: Memristor Networks. Springer International Publishing, Berlin
(2013)
6. Adamatzky, A., Wuensche, A.: Computing in spiral rule reaction-diffusion hexagonal cellular
automaton. Complex Syst. 16(4), 277–298 (2006)
7. Addington, M., Schodek, D.: Smart Materials and Technologies in Architecture: For the Archi-
tecture and Design Professions. Routledge, Abingdon (2012)
8. Ahlquist, S., Ketcheson, L., Colombi, C.: Multisensory architecture: the dynamic interplay of
environment, movement and social function. Arch. Des. 87(2), 90–99 (2017)
9. Alam, M.R., Reaz, M.B.I., Ali, M.A.M.: A review of smart homes—past, present, and future.
IEEE Trans. Syst. Man Cybern. Part C (Appl. Rev.) 42(6), 1190–1203 (2012)
10. Augusto, J.C., Nugent, C.D.: Designing Smart Homes: The Role of Artiﬁcial Intelligence, vol.
4008. Springer, Berlin (2006)
11. Bechthold, M., Weaver, J.C.: Materials science and architecture. Nat. Rev. Mater. 2(12), 17082
(2017)
12. Behring, C., Bracho, M., Castro, M., Moreno, J.A.: An algorithm for robot path planning with
cellular automata. Theory and Practical Issues on Cellular Automata, pp. 11–19. Springer,
Berlin (2001)
13. Blachecki, A., Mech-Piskorz, J., Gajewska, M., Mech, K., Pilarczyk, K., Szaciłowski, K.:
Organotitania-based nanostructures as a suitable platform for the implementation of binary,
ternary, and fuzzy logic systems. ChemPhysChem 18, 1798–1810 (2015)
14. Borghetti, J., Snider, G.S., Kuekes, P., Yang, J.J., Stewart, D.R., Stanley Williams, R.: ‘Mem-
ristive’ switches enable ‘stateful’ logic operations via material implication. Nature 464(7290),
873–876 (2010)
15. Broersma, H., Miller, J.F., Nichele, S.: Computational matter: evolving computational functions
in nanoscale materials. Advances in Unconventional Computing, pp. 397–428. Springer, Berlin
(2017)
16. Brush, A.J., Hazas, M., Albrecht, J.: Smart homes: undeniable reality or always just around
the corner? IEEE Pervasive Comput. 17(1), 82–86 (2018)
17. Burkow, A.V.: Exploring physical reservoir computing using random boolean networks. Mas-
ter’s thesis, (2016). NTNU

On Buildings that Compute. A Proposal
333
18. Chan, M., Estève, D., Escriba, C., Campo, E.: A review of smart homes - present state and
future challenges. Comput. Methods Programs Biomed. 91(1), 55–81 (2008)
19. Chang, T., Jo, S.-H., Kim, K.-H., Sheridan, P., Gaba, S., Wei, L.: Synaptic behaviors and
modeling of a metal oxide memristive device. Appl. Phys. A 102(4), 857–863 (2011)
20. Contandriopoulos, C.: Architecture and utopia in the 21st-century. J. Arch. Educ. 67(1), 3–6
(2013)
21. Dale, M., Miller, J.F., Stepney, S., Trefzer, M.A.: Evolving carbon nanotube reservoir comput-
ers. In: International Conference on Unconventional Computation and Natural Computation,
pp. 49–61. Springer (2016)
22. Dale, M., Miller, J.F., Stepney, S.: Reservoir computing as a model for in-materio computing.
In: Advances in Unconventional Computing, pp. 533–571. Springer (2017)
23. Darianian, M., Michael, M.P.: Smart home mobile RFID-based internet-of-things systems and
services. In: International Conference on Advanced Computer Theory and Engineering, 2008,
ICACTE’08, pp. 116–120. IEEE (2008)
24. Emerson, R.W.: Outdoor wayﬁnding and navigation for people who are blind: accessing the
built environment. In: International Conference on Universal Access in Human-Computer
Interaction, pp. 320–334. Springer (2017)
25. Gawe˛da, S., Podborska, A., Macyk, W., Szaciłowski, K.: Nanoscale optoelectronic switches
and logic devices. Nanoscale 1, 299–316 (2009)
26. Hevner, A., Chatterjee, S.: Design Research in Information Systems: Theory and Practice, vol.
22. Springer Science & Business Media, Berlin (2010)
27. Horsman, C., Stepney, S., Wagner, R.C., Kendon, V.: When does a physical system compute?
Proc R. Soc. A 470, 20140182 (2014)
28. Kasabov, N., Scott, N.M., Tu, E., Marks, S., Sengupta, N., Capecci, E., Othman, M., Doborjeh,
M.G., Murli, N., Hartono, R., et al.: Evolving spatio-temporal data machines based on the neu-
cube neuromorphic framework: design methodology and selected applications. Neural Netw.
78, 1–14 (2016)
29. Konkoli, Z.: On reservoir computing: from mathematical foundations to unconventional appli-
cations. Theory, vol. 1. Springer, Berlin (2016)
30. Kuzum, D., Yu, S., Wong, H.S.P.: Synaptic electronics: materials, devices and applications.
Nanotechnology 24(38), 382001 (2013)
31. Larger, L., Soriano, M.C., Brunner, D., Appeltant, L., Gutiérrez, J.M., Pesquera, L., Mirasso,
C.R., Fischer, I.: Photonic information processing beyond turing: an optoelectronic implemen-
tation of reservoir computing. Opt. Express 20(3), 3241–3249 (2012)
32. Larger, L., Baylón-Fuentes, A., Martinenghi, R., Udaltsov, V.S., Chembo, Y.K., Jacquot, M.:
High-speed photonic reservoir computing using a time-delay-based architecture: million words
per second classiﬁcation. Phys. Rev. X 7(1), 011015 (2017)
33. Lasi, H., Fettke, P., Kemper, H.-G., Feld, T., Hoffmann, M.: Industry 4.0. Bus. Inf. Syst. Eng.
6(4), 239–242 (2014)
34. Lee, J., Bagheri, B., Kao, H.-A.: A cyber-physical systems architecture for industry 4.0-based
manufacturing systems. Manuf. Lett. 3(2015), 18–23 (2015)
35. Lewandowska, K., Podborska, A., Kwolek, P., Kim, T.-D., Lee, K.-S., Szaciłowski, K.: Optical
signal demultiplexing and conversion in the fullerene–oligothiophene–CdS system. Appl. Surf.
Sci. 319, 285–290 (2014)
36. Li, X., Lu, R., Liang, X., Shen, X., Chen, J., Lin, X.: Smart community: an internet of things
application. IEEE Commun. Mag. 49(11), 68–75 (2011)
37. Lukoševiˇcius, M., Jaeger, H.: Reservoir computing approaches to recurrent neural network
training. Comput. Sci. Rev. 3(3), 127–149 (2009)
38. Lukoševiˇcius, M., Jaeger, H., Schrauwen, B.: Reservoir computing trends. KI-Künstliche Intel-
ligenz 26(4), 365–371 (2012)
39. Maass, W., Natschläger, T., Markram, H.: Real-time computing without stable states: a new
framework for neural computation based on perturbations. Neural Comput. 14(11), 2531–2560
(2002)

334
A. Adamatzky et al.
40. Mandic, D.P., Chambers, J.: Recurrent Neural Networks for Prediction: Learning Algorithms,
Architectures and Stability. Wiley, Hoboken (2001)
41. Margenstern, M.: New tools for cellular automata in the hyperbolic plane. J. Univers. Comput.
Sci. 6(12), 1226–1252 (2000)
42. Margenstern, M.: A universal cellular automaton on the heptagrid of the hyperbolic plane with
four states. Theor. Comput. Sci. 412(1–2), 33–56 (2011)
43. Margenstern, M.: Small Universal Cellular Automata in Hyperbolic Spaces: A Collection of
Jewels, vol. 4. Springer Science & Business Media, Berlin (2013)
44. Margenstern, M.: Hyperbolic gallery. Designing Beauty: The Art of Cellular Automata, pp.
65–71. Springer, Berlin (2016)
45. Mohamed, A.S.Y.: Smart materials innovative technologies in architecture; towards innovative
design paradigm. Energy Procedia 115, 139–154 (2017)
46. Nabil, S., Plötz, T., Kirk, D.S.: Interactive architecture: Exploring and unwrapping the poten-
tials of organic user interfaces. In: Proceedings of the Eleventh International Conference on
Tangible, Embedded, and Embodied Interaction, pp. 89–100. ACM (2017)
47. Owens, N., Stepney, S.: Investigations of game of life cellular automata rules on penrose tilings:
lifetime, ash, and oscillator statistics. J. Cell. Autom. 5(3), 207–225 (2010)
48. Papandroulidakis, G., Vourkas, I., Vasileiadis, N., Sirakoulis, G.C.: Boolean logic operations
and computing circuits based on memristors. IEEE Trans. Circuits Syst. II Express Briefs
61(12), 972–976 (2014)
49. Papandroulidakis, G., Vourkas, I., Abusleme, A., Sirakoulis, G.C., Rubio, A.: Crossbar-based
memristive logic-in-memory architecture. IEEE Trans. Nanotechnol. 16(3), 491–501 (2017)
50. Park, S., Chu, M., Kim, J., Noh, J., Jeon, M., Lee, B.H., Hwang, H., Lee, B., Lee, B.-G.:
Electronic system with memristive synapses for pattern recognition. Sci. Rep. 5, 10123 (2015)
51. Pilarczyk,K.,Podborska,A.,Lis,M.,Kawa,M.,Migdał,D.,Szaciłowski,K.:Synapticbehavior
in an optoelectronic device based on semiconductor-nanotube hybrid. Adv. Electron. Mater. 2,
1500471 (2016)
52. Pilarczyk, K., Wlaálak, E., Przyczyna, D., Blachecki, A., Podborska, A., Anathasiou, V.,
Konkoli, Z., Szaciłowski, K.: Molecules, semiconductors, light and information: towards future
sensing and computing paradigms. Coord. Chem. Rev. 265, 22–40 (2017)
53. Podborska, A., Szaciłowski, K.: ‘computer-on-a-particle’ devices: optoelectronic 1:2 demulti-
plexer based on nanostructured cadmium sulﬁde. Aust. J. Chem. 63, 165–168 (2010)
54. Popovici, A., Popovici, D.: Cellular automata in image processing. In Proceeding of the Fif-
teenth International Symposium on Mathematical Theory of Networks and Systems, vol. 1, pp.
1–6. Citeseer (2002)
55. Putnam, H.: Representation and Reality. MIT Press, Cambridge (1988)
56. Raghavan, R.: Cellular automata in pattern recognition. Inf. Sci. 70(1–2), 145–177 (1993)
57. Rosin, P.L.: Training cellular automata for image processing. IEEE Trans. Image Process.
15(7), 2076–2087 (2006)
58. Rosin, P.L.: Image processing using 3-state cellular automata. Comput. Vis. Image Underst.
114(7), 790–802 (2010)
59. Rosin, P., Adamatzky, A., Sun, X.: Cellular Automata in Image Processing and Geometry.
Springer, Berlin (2014)
60. Scott, F.D.: Architecture or techno-utopia. Grey Room 112–126 (2001)
61. Sillin, H.O., Aguilera, R., Shieh, H.-H., Avizienis, A.V., Aono, M., Stieg, A.Z., Gimzewski,
J.K.: A theoretical and experimental study of neuromorphic atomic switch networks for reser-
voir computing. Nanotechnology 24(38), 384004 (2013)
62. Simon, H.A.: Networks, complexity, models and measures. Access, Property and American
Urban Space, p. 58 (2016)
63. Stauffer, M.T.: Utopian reﬂections, reﬂected utopias urban designs by archizoom and super-
studio. AA Files (47), 23–36 (2002)
64. Stepney, S., Diaconescu, A., Doursat, R., Giavitto, J-L., Kowaliw, T., Leyser, O., Maclennan, B.,
Michel, O., Miller, J., Nikolic, I., Spicher, A., Teuscher, Ch., Tufte, G., Vico, F.J., Yamamoto,
L.: Gardening cyber-physical systems. In: Unconventionnal Computation and Natural Com-
putation (UCNC’2012), pp. 237–238 (2012)

On Buildings that Compute. A Proposal
335
65. Stepney, S.: The art of Penrose life. In: Adamatzky, A., Martineze, G. (eds.) Designing Beauty:
The Art of Cellular Automata, pp. 103–109. Springer, Berlin (2016)
66. Szaciłowski, K., Macyk, W., Stochel, G.: Light-driven or and xor programmable chemical logic
gates. J. Am. Chem. Soc. 128, 4550–4551 (2006)
67. Ulbricht, R., Hendry, E., Shan, J., Heinz, T.F., Bonn, M.: Carrier dynamics in semiconductors
studied with time-resolved terahertz spectroscopy. Rev. Mod. Phys. 83(2), 543 (2011)
68. Van der Sande, G., Brunner, D., Soriano, M.C.: Advances in photonic reservoir computing.
Nanophotonics 6(3), 561–576 (2017)
69. Vandoorne, K., Dierckx, W., Schrauwen, B., Verstraeten, D., Baets, R., Bienstman, P., Van
Campenhout, J.: Toward optical signal processing using photonic reservoir computing. Opt.
Express 16(15), 11182–11192 (2008)
70. Vandoorne, K., Mechet, P., Van Vaerenbergh, T., Fiers, M., Morthier, G., Verstraeten, D.,
Schrauwen, B., Dambre, J., Bienstman, P.: Experimental demonstration of reservoir computing
on a silicon photonics chip. Nat. Commun. 5, 3541 (2014)
71. Verstraeten, D., Schrauwen, B., d’Haene, M., Stroobandt, D.: An experimental uniﬁcation of
reservoir computing methods. Neural Netw. 20(3), 391–403 (2007)
72. Vissol-Gaudin, E., Kotsialos, A., Massey, M.K., Zeze, D.A., Pearson, C., Groves, C., Petty,
M.C.: Data classiﬁcation using carbon-nanotubes and evolutionary algorithms. In: International
Conference on Parallel Problem Solving from Nature, pp. 644–654. Springer (2016)
73. Vourkas, I., Sirakoulis, G.C.: Memristor-based combinational circuits: a design methodology
for encoders/decoders. Microelectron. J. 45(1), 59–70 (2014)
74. Vourkas, I., Sirakoulis, G.C.: On the generalization of composite memristive network structures
for computational analog/digital circuits and systems. Microelectron. J. 45(11), 1380–1391
(2014)
75. Vourkas, I., Sirakoulis, G.C.: Memristor-Based Nanoelectronic Computing Circuits and Archi-
tectures: Foreword by Leon Chua. Emergence, Complexity and Computation. Springer Inter-
national Publishing, Berlin (2015)
76. Vourkas,I.,Stathis,D.,Sirakoulis,G.C.: Massivelyparallel analogcomputing: Ariadne’sthread
was made of memristors. IEEE Trans. Emerg. Top. Comput. 6(1), 145–155 (2018)
77. Warzecha, M., Oszajca, M., Pilarczyk, K., Szaciłowski, K.: A three-valued photoelectrochem-
ical logic device realising accept anything and consensus operations. Chem. Commun. 51,
3559–3561 (2015)
78. Whiting, J.G.H., Mayne, R., Adamatzky, A.: A parallel modular biomimetic cilia sorting plat-
form. Biomimetics 3(2), 5 (2018)
79. Williams, R.J., Zipser, D.: A learning algorithm for continually running fully recurrent neural
networks. Neural Comput. 1(2), 270–280 (1989)
80. Yi, Y., Liao, Y., Wang, B., Xin, F., Shen, F., Hou, H., Liu, L.: FPGA based spike-time dependent
encoder and reservoir design in neuromorphic computing processors. Microprocess. Microsyst.
46, 175–183 (2016)
81. Zoran, K.: A perspective on Putnam’s realizability theorem in the context of unconventional
computation. Int. J. Unconv. Comput. 11, 83–102 (2015)

Complex Systems of Knowledge
Integration: A Pragmatic Proposal
for Coordinating and Enhancing
Inter/Transdisciplinarity
Ana Teixeira de Melo
and Leo Simon Dominic Caves
Abstract Humanity’s biggest challenges call for organised collective action,
informed by the most complex forms of thinking. Different forms of knowledge
and practices of knowing operate at different levels of organisation within society.
Scientiﬁc knowledge is one form of knowing, but the development of science under
a culture of disciplinisation and increasing specialisation has led to its fragmentation
and blinded it to the possibilities offered by the integration of knowledge. Interdisci-
plinarity and transdisciplinarity are privileged routes for rich knowledge construction
and integration. There is a pressing need for efforts directed toward the intentional
construction of a culture where interdisciplinary and transdisciplinary practices may
ﬂourish. However, we believe signiﬁcant change will only occur through the orches-
tration of a set of activities that attend to the complexity of knowledge construction
and integration as emergent outcomes of a complex network of processes and rela-
tions that constitute an evolving inter and transdisciplinary ecosystem. In this paper
we present a proposal for the organisation of an Alliance for Knowledge Integra-
tion and of Inter/Transdisciplinary Hubs aimed at coordinating collaborative actions
and contributions from a diversity of agents and systems from different levels of
organisation of society towards richer and more integrated practices of knowing.
Keywords Knowledge integration · Interdisciplinarity · Transdisciplinarity ·
Complex systems
A. T. de Melo (B)
Colégio de S. Jerónimo, Largo D. Dinis, Apartado 3087, 3000-995 Coimbra, Portugal
e-mail: anatmelo@ces.uc.pt
Centre for Social Studies of the University of Coimbra, Coimbra, Portugal
L. S. D. Caves
Apartado 2001, EC Praia da Granja, 4410-911 São Félix da Marinha, Portugal
York Cross-disciplinary Centre for Systems Analysis, University of York,
Heslington, York YO10 5GE, UK
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_14
337

338
A. T. de Melo and L. S. D. Caves
1
Context
Different forms of knowledge and practices of knowing operate within different
levels of organisation of society [13, 22]. Scientiﬁc knowledge is but one form of
knowing, to which a diversity of social processes contribute [45]. Nevertheless, at
times, science has exceeded its explicit or implicit claims of supremacy [23], becom-
ing disconnected from other forms of knowing (e.g. [38, 51]). Additionally, a culture
of specialisation and strong disciplinarity within science has led to a state of inter-
nal fragmentation [62] that leaves it blind to complexity and prone to accumulate
ignorance [42]. Many voices have called for fundamental revisions in the modes of
thinking and practices of science, for example towards increasing complexity [42]
and for incorporating more integrative, dialogic and collaborative modes of knowl-
edge production and validation [21, 45].
There is a growing need for building new pathways towards a future capable of
holding more diverse, albeit coordinated and creative ways of producing, applying,
transforming and integrating knowledge. Interdisciplinarity and transdisciplinarity
may be privileged routes for the production of knowledge that is congruent with the
complexity of the world [3, 14].
There is a diversity of practices that, although often labelled as interdisciplinary,
in reality, involve minimal interaction between different domains of knowledge, and
would be better classiﬁed, at best, as multidisciplinary [6, 13, 31], or “patchwork
interdisciplinarity” [27]. As Huutoniemi and collaborators have stated “in multi-
disciplinary research, the ingredients of new knowledge are imported, exported, or
pooled across boundaries without being substantially adapted in the course of inter-
action. This kind of research is cumulative or additive rather than integrative by
nature.” [28]. The lack of clarity in the usage of terminology can lead to misunder-
standings that undermine the potentialities of interdisciplinarity [4]. As other authors
have stated, interdisciplinarity is not synonymous with collaboration [31] nor does
it occur naturally or automatically [35].
We adopt the view of inter/transdisciplinarity (ITD) as strong forms of knowledge-
based interaction between disciplines (and the individuals representing them), that,
from a process perspective, implies some degree of perturbation in one or all of the
interacting elements (individuals, disciplines) [6, 31, 40], where “the concepts and
insights of one discipline contribute to the problems and theories of another” ([6],
p. 20). The interdisciplinary interaction may feedback on the disciplines and lead
to their transformation (e.g. epistemological, conceptual, methodological). Inter-
disciplinarity requires an “active interaction across ﬁelds” [28] and some form of
integration that is “more than the sum of its parts”.
Transdisciplinarity has often been deﬁned as a type of interaction that engages
science and other social agents and systems in generating knowledge in contexts
of application [13, 45], “‘transcending’ the academic disciplinary structure” ([35],
p. 14). This notion often implies a clear focus on a particular real-world phenomenon,
challenge or problem around or about which knowledge is constructed. Transdisci-
plinarity occurs through the integration of the synergetic, dynamic, processual and

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
339
relational nature of the contributions, lenses and practices that are deemed necessary
to embrace real-world complexity [13]. It may also imply the creation of concep-
tual or pragmatic frameworks that not only integrate, but truly transcend, disciplines
[13, 31, 58]. In this sense, transdisciplinarity can also be deﬁned as a type of out-
come, emerging from interactions (often interdisciplinary, but also involving diverse
social agents) that exhibit an integrative nature, as a coherent ‘wholesome’ body of
knowledge, eventually, with a corresponding set of practices for action. Additionally,
transdisciplinarity should also lead to some degree of perturbation and change, and
to transformation in the broader scientiﬁc and societal contexts where the practices
of knowledge production unfold [13, 45, 46].
The congruence between our modes of thinking and the complexity of the world
is fundamental for an expansion of our possibilities for action in promoting positive
and sustainable change in the complex world [14, 16], and to improve our individual
and collective capacity to act upon those possibilities. However, interdisciplinarity
and transdisciplinary are also domains that call not just for more coordinated practice
but for a deeper understanding of their underlying processes. Through their capacity
for the emergence of more complex knowledge and practices for coupling with the
complex world, inter and transdisciplinarity constitute key resources for the under-
standing and management of positive change in human-related systems, increasing
both our theoretical and pragmatical capacity [14].
In the last decades, there has been a signiﬁcant growth in the literature on interdis-
ciplinarity. It is not the purpose of this paper to review taxonomies and indicators to
support the deﬁnition and assessment of different types of interdisciplinary research
and we refer the reader to the numerous works in this area (e.g. [13, 28, 31]). Some
studies have focused on mapping territories and identifying the factors that affect
the practice of interdisciplinarity, offering several recommendations [4, 10, 13, 35].
Attempts have also been made at deﬁning indicators for interdisciplinary activities
[37]. The evaluation of interdisciplinarity at different levels (individual; project/team;
institutional; peer scientiﬁc community/journals; national/funding strategies) has
also received attention, being a critical factor for the dissemination of interdisciplinar-
ity practices [33, 39, 60]. However, interdisciplinarity per se, as a mode of research
activity and knowledge acquisition, still calls for attention. As Lyall and collaborators
have stated, “much of the knowledge regarding interdisciplinary research capacity-
building is tacit, with practitioners often ‘learning by doing’ through a process of
apprenticeship” ([34], p. 10).
At a time when different research councils and funding agencies are calling for
more interdisciplinarity, the lack of standard and generally accepted guidelines on
what constitutes high quality interdisciplinary research, risks a deterioration of its
value [37]. This may be particularly true in face of the usage of ‘interdisciplinarity’
as a buzz word, increasingly applied to any activity or collaboration involving more
than one discipline, without clear reference to the particular sets of processes and
diverse modes of working it encompasses [13].
The quality of ITD can also be enhanced if purposively managed. There is a
signiﬁcant amount of accumulated experience and practice that needs to be consid-
ered [35]. Nevertheless, we believe that the practice of ITD needs to be linked, by

340
A. T. de Melo and L. S. D. Caves
a self-referential recursive loop, to the scientiﬁc (interdisciplinary) investigation of
inter/trandisciplinarity [43].
Even though we are increasing our knowledge and understanding of the factors
at multiple levels constraining or facilitating interdisciplinary collaborations, the
processes that underlie the interaction of different types of factors, and how these
lead to particular types of outcomes is still largely unknown [8, 59]. It is necessary
to invest in more process-focused research [58] aimed at identifying how different
factors and components of an inter/transdisciplinary system couple and transform,
through time, and in relation to each other [36, 59].
Inter/transdisciplinary interactions may lead to the type of abductive [20, 40] and
imaginative [63] leaps that have been the key drivers of the development of sci-
ence, technology and society [50], albeit sometimes serendipitously [17]. On this
basis, we believe that the investigation of the processes of ITD may beneﬁt from
complex systems-inspired perspectives and models. In this paper we approach inter-
disciplinarity and transdisciplinarity together (ITD) as we believe they both imply
the emergence of some degree of novelty and hence require additional attention to
the processes sustaining this complexity.
2
The Complexity of Inter/Transdisciplinarity: A Complex
Systems-Informed Relational Worldview
Interdisciplinary systems exhibit properties of complex systems e.g. social systems
[44]. They are based on a variety of dynamic interactions from which novelty, surprise
and creativity emerge, along with knowledge that could not be foreseen from the
perspective of any individual or discipline alone, nor from the mere juxtaposition of
their skills. This creative potential is likely associated with non-linear interactions
between a diversity of components and processes involved in ITD activity.
Weassumeinter/transdisciplinaryknowledgeandintegrationtobeemergentprod-
ucts of complex social systems. Therefore, one needs to recognise the multidimen-
sional, relational, coupled and non-linear nature of the dynamics of any knowledge
production system. Inter/transdisciplinary interactions can be organised in many
ways and, under different conditions, can lead to a variety of (more or less) posi-
tive or negative emergent outcomes. In this paper, ‘positive’ inter/transdisciplinary
encompasses the emergence of some form of novelty (theoretical, methodological,
pragmatic) and knowledge integration and the enrichment of the interacting elements
(e.g. individuals, disciplines). Additionally, it needs to lead to positive outcomes in
relation to a framework of the purposes and values that motivate, guide or legiti-
mate its activities, as assessed by a given observer [32]. By contrast, ‘negative’ ITD
leads to fragmentation, conﬂict or unproductive/destructive outcomes or the absence
of novelty, both in relation to the purpose of the interaction and its interacting ele-
ments. Additionally, it represents a negative evaluation of its outcomes, from a given
observer, in relation to a purpose or value framework.

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
341
Every inter/transdisciplinary system will likely be organised differently, not only
as a function of the diversity of its interacting elements (e.g. individuals, disciplines,
social systems) and the nature of their interactions, but also of the surrounding envi-
ronment and, above all, the nature of the couplings within and between its levels of
organisation (e.g. individual, team, disciplines, institutional environment, scientiﬁc
community, society in general) [8].
In this section we present a scheme that incorporates multiple levels of organisa-
tion of ITD, as in other proposals (e.g. [59]), but where we wish to place the empha-
sises on the coupling processes and their role in the emergent outcomes. Based on
a previous proposal of a relational metamodel to conceptualise change in complex
systems [14], and a subsequent elaboration for particular types of social systems
[41], we present, in Fig. 1, a possible representation of a complex systems-informed
relational worldview for ITD.
We call attention to the coupling processes within and between levels, and to the
fact that different conﬁgurations of properties and processes will lead to and sus-
tain different types of positive and negative emergent outcomes. While we antic-
ipate that there will be some content/context-dependent speciﬁcity to particular
inter/transdisciplinary interactions, it is likely that there are also general underly-
ing processes. Strategically, we take the inter/transdisciplinary team as our start-
ing/target point to understand the broader multi-level inter/transdisciplinary system
and to illustrate the variety of dimensions, relations and coupling processes that need
to be considered when exploring the complexity of ITD. Understanding the inter-
Fig. 1 A relational worldview for intra/trandisciplinarity systems

342
A. T. de Melo and L. S. D. Caves
nal organisation of the target system and its sub- and supra-system(s) is of as much
importance as understanding the coupling processes within and between them due
to their roles in enabling or disabling constraints.
Different conﬁgurations of relations of different systems and coupling processes
(in this relational world) will likely lead to and sustain different types of emer-
gent outcomes, which cannot be reduced to the level of single component sub-
systems or processes. This complexity calls for a systematic investigation of the
structural/organisational and dynamical complexity of ITD, not only at a theoretical
level but also empirically, with a strong action-based, pragmatic orientation. Simi-
larly, the practices to promote positive ITD need to be informed by knowledge of
how certain types of outcomes are potentiated or inhibited by particular conﬁgu-
rations of relations. There is a variety of contextual factors that shape the work of
inter/transdisciplinary teams [58, 59]. Figure 2 adopts an ecosystemic focus [11]
to represent how the interactions that occur between individuals are constrained by
the characteristics of the larger systems in which they are embedded. On the other
hand, it presents a simpliﬁed representation of the multiple dimensions at which the
individuals interact, both at a personal and at a disciplinary level, under the inﬂu-
ence of larger systems in which they are embedded. In an interdisciplinary team this
complexity is increased in (non-linear) relation to the number of individuals that are
involved in the interactions, and their diversity.
Fig. 2 Multiple levels of inter/transdisciplinary interaction, mediated by interactions between indi-
viduals

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
343
In inter/transdisciplinary interactions it is not only the individuals, with all their
assets, capacities, personalities, cultures and preferred patterns of interaction that are
coordinated. There are processes that relate to higher-order levels, such as the extent
to which the individuals act under the inﬂuence of their disciplinary modes, prac-
tices (e.g. modes of thinking/worldviews, theories/concepts, methods/technology)
and cultures, within particular frameworks of values and assumptions that are taken
for granted [59]. The individuals’ interaction also occurs under the top-down inﬂu-
ence of the constraints imposed by their representations of the disciplines (e.g. they
may have been brought into a team to bring a particular set of methods/approaches),
their immediate research environments as well as their larger research organisations.
Additionally, the interactions between the modes of thinking, the concepts, methods,
and general research practices of each discipline are shaped by the individuals’ per-
sonalities, their ecosystemic environments and their own susceptibilities. There are
different tasks and stages at the level of the interpersonal interaction and the nego-
tiation of intercultural protocols that may need to be successfully addressed before
deeper work can be performed [34] at the level of the interaction between concepts
and methods [40].
To reiterate, although there has been some work on the development of concep-
tual frameworks for ITD, particularly at the level of teamwork [58], it is necessary
to continue to develop and expand them, with a focus on processes, in order to cap-
ture their full complexity and potential. As Stokols and collaborators afﬁrm “(…)
the outcomes of TS [team science] initiatives are highly inﬂuenced by social and
interpersonal processes (…) Yet, the precise way in which these social processes
(…) inﬂuence scientiﬁc productivity and TD [transdisciplinary] integration are not
known. The empirical links between interpersonal and intellectual dimensions of
scientiﬁc collaborations remain unclear” ([58], p. 477). More research is needed that
is capable of addressing the questions of the when, where, what, with whom, how and
why of the inter/transdisciplinary research processes [8]. It is, therefore, necessary to
invest in research programmes targeting process-focused questions related to what
works, under which conditions, through which type of processes/interactions, that
lead to which type of outcomes. A similar task was initiated some decades ago in the
process-focused and common factors approaches to psychotherapies [18, 19]. The
thinking in this ﬁeld moved from a restricted concern about the absolute efﬁcacy of
treatments, as evaluated by randomised control trials, to an exploration of the pro-
cesses of change that would afford an understanding of how, for whom and under
what circumstances treatments work and the prescription for individually-tailored
interventions [5] as well as family approaches [56]. A new wave of process-focused
research needs to be capable of focus the complexity of the interactions and the
nature of the coupling processes that sustain the inter/transdisciplinary system. It
needs to adopt a broad multi-level and multi-method approach capable of a deep
investigation of the different processes of ITD, at its different levels of organisation,
under an overarching (complex) systems framework [8].
There need to be activities that not only strengthen each level of the ITD system,
but that above all strengthen the coupling between different levels of organisation.
We believe that a focus on the coordination of activities is at least as important, if

344
A. T. de Melo and L. S. D. Caves
not more important, than the individual activities per se. We also believe that ITD
and knowledge integration activities requires active facilitation, but that individuals
involved in the ITD activities, or the organisations in which they are embedded, are
not necessarily willing or prepared to do it.
3
Coordinating and Enhancing Knowledge Integration:
A Proposal
Wepresentaproposalfortheorganisationoftwolargeanddistinct,buthighlycoupled
sets of activities aimed at promoting, supporting, disseminating and evaluating ITD
and knowledge integration. Our proposal encompasses the creation of an Alliance
for Knowledge Integration and the development of Inter/Transdisciplinary Hubs. We
believe that the relevance of our proposal lies not in the individual activities that we
suggest in themselves (as we know that many of them are already being conducted
on different types of contexts), but in the coordination of these activities, under the
scope of two organisational structures designed to attend to the different levels of
ITD systems and their coupling processes. We believe that such orchestration of
activities will lead to a more coherent and self-organising ITD culture that will result
in enhanced impact.
We are aware that many of the individual activities that we suggest have been
already been reported based on experiences of different groups and researchers, in
different contexts. While acknowledging some of these contributions, for the purpose
ofthispaper,wehavenotconsideredthemcomprehensively.Thus,weassumethatthe
paper falls short in giving full credit to a diversity of contributions from researchers
and centres that, in different contexts, have planted and nurtured the seeds of ITD,
preparing the ground in which it may ﬂourish.
We believe that the full development of ITD will require orchestration of efforts
and resources from individuals and systems stemming from communities both within
science and other social systems. As Hall et al. ([25], p. 248) state “A new era
pof creativity and innovation in transdisciplinary science can be achieved through
simultaneousandcoordinatedeffortsthatremovecollaborativebarriersandbuildnew
linkages across multiple sectors of society and across spheres of research”. We hope
this paper contributes to a vision of an ecosystem for the evolution and ﬂourishing
of ITD. Hence while the novelty of the isolated components of our proposal may be
limited, we hope that the whole of the proposal brings more than the sum of its parts,
suggesting ways for the coordination of actions within and between individuals,
programmes and institutions at different levels of organisation of science and society
at large.
The overall goal of the proposal is to (i) create a broad cultural and social context
that supports positive ITD; (ii) support the development of theoretical and pragmat-
ical frameworks for ITD that support the orchestration of their processes, interac-
tions and practices, at multiple levels of its organisation, towards positive outcomes;

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
345
(iii) promote a culture of extensive collaborative engagement and participation for
knowledge creation and integration; (iv) promote local and global positive practices
of knowledge integration along with critical and reﬂexive societal debates on the
nature of knowledge creation and application.
The
Alliance
for
Knowledge
Integration
and
the
assemblage
of
Inter/Transdisciplinary Hubs are structures that both emerge from and organ-
ise the activities in our proposal. By being organised as effectively stable, albeit
dynamic and evolving, social structures, they may serve to support longstanding
positive changes. We envisage these structures as operating in close collaborative
partnerships with other social systems with which they interact, both to inﬂuence
and to be inﬂuenced in a sense that allows their dynamic adjustment for ﬁtness in
terms of the relevance of their outputs for society. In Fig. 3 we present a simpliﬁed
general scheme of the ecosystemic view underlying our proposal.
The development of the Alliance for Knowledge Integration (AKI) will rely on
critical partnerships between existing and prospective inter/transdisciplinary centres.
These centres may or may not be operating as Inter/Trandisciplinary Hubs (ITH),
in the sense developed in this paper. However, the development of an AKI would
build upon their accumulated experience. The AKI and ITH are designed to trans-
act knowledge and resources and interact in complementary ways. Both of these
systemic entities should develop different ways of effectively coupling with the mul-
tiple levels of organisation of society and of supporting each other’s activities. In
doing so, each may have preferential relations with other types of social systems,
acting as mediators for the other. For example, while the relation of the AKI with
teaching and research organisations is likely to be mostly mediated by the ITH,
the ITH may connect with larger audiences and different types of social systems
under the inﬂuence of AKI’s guidelines and activities. On the other hand, both of
these structures, but especially the ITH, can interact with society at large through
the wider organisations that host them. In turn, the AKI will not just support the
ITH (and, as appropriate, their host organisations), but will also develop strategies
and processes that allow for collaborations regarding the creation and integration of
knowledge, in general, across the different levels of organisation of society and, for
given challenges, between critical stakeholders. The AKI may act to support ITH
by offering overarching perspectives on the state of inter/transdiscipinarity, building
general knowledge and guidelines, while also promoting synergetic encounters of
different, formal and informal communities of knowing; the ITH directly promote
and support practices of ITD that lead to positive outcomes.
3.1
The Organisation of Inter/Transdisciplinary Hubs (ITH)
The role of Universities, as research and teaching organisations, is increasingly
under discussion (e.g. [15, 52]). Nevertheless, they are still institutions where most
inter/transdisciplinary activities take. However if, on the one hand, these modes of
knowledge production and dissemination are increasingly being advocated, on the

346
A. T. de Melo and L. S. D. Caves
Fig. 3 A simpliﬁed representation of a proposal for knowledge integration activities. For simpliﬁ-
cation we have included only a few arrows to represent the interactions and inﬂuence between the
different levels of systems portrayed. However, these arrows should be viewed as bifurcating into
a multiplicity of connections, of differing strengths, between all the systems considered, represent-
ing recursive inﬂuence between them. The multiple interactions within and between the multiple
systems and levels of organisation of knowledge portrayed in Fig. 1 may create conditions for the
emergence, integration and application of new, expanded and enriched forms of knowledge and
ways of knowing

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
347
other, there is an increasing recognition of the obstacles and pitfalls involved. These
challenges include the evaluation of career management issues, funding, publication
and the management of research processes [10], amongst others.
Theory, research and practice on ITD need to be more tightly coupled. There
is a signiﬁcant amount of practice experience that deserves investigation and there
is a need for a type of process-focused research that could beneﬁt from a strong
coupling with the practice of interdisciplinarity. At the same time both the research
and practice are best embedded in favourable and supportive contexts. Therefore,
promoting positive ITD requires an investment in nurturing a supporting culture and
positive coupling processes within and between the different levels of organisation
of an inter/transdisciplinary system [8, 59].
In the absence of a favourable organisational culture, ITD research will either
not ﬂourish or remained restricted to ‘weak’ modes of disciplinary interaction, with
limited outcomes [31]. Positive ITD does not come for free and demands active
facilitation [34, 35]; there are both micro and macro cultural and socio-environmental
aspects that, being critical to the formation of conditions for creative and productive
interactions, require intentional nurturing. It is the dynamic processes implicated
in inter/transdisciplinary interactions that, as in most complex systems, makes it
difﬁcult to manage; however, it is in this very complexity that its greatest strength
and potential to address complex issues lies.
To establish an ecology to support and nurture ITD will require special investment
and intentional planning. Universities and other research environments don’t just
need to promote inter/transdisciplinary or develop inter/transdisciplinary centres;
they need to have their inter/transdisciplinary aspirations and practices supported by
structures dedicated to promoting processes that lead to positive outcomes. This can
occur under the scope of the activities of Inter/Transdisciplinary Hubs (ITH). ITH
will most likely be located within Universities or other research organisations, but
they may also arise in other contexts such as companies or NGOs, or from grass-roots
organisations. Whatever their context, ITH should be strongly coupled to their local
environments and research organisations in order to support and facilitate positive
ITD. We highlight that ITH differ from other types of interdisciplinary centres to the
extent that, although they may be places to conduct ITD research, their main purpose
is to promote, coordinate, facilitate, expand and integrate ITD research within their
host organisations and with key partners.
Additionally, we conceive of ITH as key research sites for the development
and evaluation of knowledge and technology on ITD research processes. They
will, therefore, constitute “second-order” research centres [43], using internal
inter/transdisciplinary research projects as a source of data to support studies on
the inﬂuence of processes, practices and other dimensions on ITD. But they also
go beyond these activities to develop, deliver and evaluate theory and technology to
promote positive ITD to a broader audience. They are designed to create the time and
space to intentionally facilitate and build positive inter/transdisciplinary interactions.
When integrated within Universities, ITH may take advantage of the disciplinary-
based organisation, while attempting to overcome their limitations and explore the
potential of science beyond the disciplinary boundaries. They may operate as (semi-)

348
A. T. de Melo and L. S. D. Caves
Fig. 4 A model for the internal organisation of the inter/transdisciplinary hubs (ITH)
independent entities with their own appointments and staff and/or draw on the human
and other resources available in their hosts’ departments, across faculties. Addition-
ally, they may include academic/professionals who specialise in the processes of
managing and promoting positive inter/transdisciplinary. We envisage that staff with
a background in the social sciences may play a special role in the management of
the activities of ITH: we see scope for the development of a new research ﬁeld
of the Social Sciences of ITD and for a Psychology of Interdisciplinary Relations.
Nevertheless, as new training programmes and specialisations are advanced, other
researchers may acquire the necessary skills to facilitate and investigate the processes
of positive ITD, beyond the stricter scope of their other research activities.
We envisage the organisation of the ITH around three, closely interrelated, lines
of activities, namely: research, facilitation and teaching, as illustrated in Fig. 4.
These activities are aimed to serve the community of researchers of the organisa-
tions where these Hubs are embedded or with key partners. They are also oriented
to support the activities of the Alliance for Knowledge Integration through partner-
ships with similar Hubs. We next present a brief description of the proposed activities,
highlighting the fact that it is their co-existence and coordination that may lead to
the coherence and culture to support critical positive outcomes.
3.1.1
Research
ITH could host a variety of research projects, in different forms (resident
researchers/teams; academic visitors/fellowships; hosted research projects), how-
ever what is distinctive is that the projects must be strategically integrated in order
to contribute to ITH activities through a variety of means (human resources/time
dedicated to facilitate activities; disciplinary skills, conceptual, methodological and
technical skills; provision of open data on the processes of inter/transdisciplinary
research). Hence, research with ITH has a double complementary function. On the
one hand, these hubs may operate as research centres, building on the synergies of

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
349
the ‘resident’ academics and research projects. On the other hand, they are research
centres of/for ITD. Through this line of activity, they pursue the development, eval-
uation and dissemination of theory and technology (strategies, tools, heuristics) to
promote positive inter/transdisciplinary interactions. To support this, alongside the
internal research projects, ITH integrate Laboratories of ITD, drawing data from the
internal research projects, but also reaching to other contexts and sources to develop
theoretical frameworks and pragmatic applications (e.g. tools, strategies, resources)
aimed at promoting positive ITD. These Laboratories should have a strong transla-
tional focus and develop action-based research in contexts of application, bridging
theory and practice, and in close collaboration with the other activities of the ITH.
3.1.2
Facilitation
ITD requires a supporting culture that favours creative interactions and conditions
for researchers to meet within and beyond the scope of their disciplines [34]. There is
a need for environments that stimulate and support formal and informal encounters
for researchers where intellectual playfulness, engagement and curiosity are stim-
ulated as key ingredients for the fostering of creative leaps and innovation. There
are many reports, both documented and anecdotal, of inter/transdisciplinary experi-
ences where informal and free interactions have played key roles in building positive
inter/transdisciplinary relations. It is necessary to dedicate time and effort into build-
ing a favourable working environment, addressing key preparatory tasks for inter-
disciplinary work (e.g. building a common language, goals, values, expectations, a
climate of openness and trust and ways of coordinating practices). Not all individu-
als and teams present the same level of readiness and skill for inter/transdisciplinary
collaborations [57]. Nor are they all necessarily aware of the full complexity of
those interactions or present the knowledge, skills or proﬁle to be facilitators in
inter/transdisciplinary teams [24, 26]. There is a diversity of processes and contribu-
tions to ITD that needs to be managed. Facilitation can be performed internally, or
by external facilitators, however, in any event, it should be guided by the available
evidence collected from a variety of contexts and by empirically-grounded theoreti-
cal frameworks, as much as by the team members’ previous experiences. Although
interdisciplinarity may be practiced by individual researchers, the challenges at the
team level are likely to be greater and call for specialised support [57]. Facilitation of
inter/transdisciplinary work may occur at different stages of a project, depending on
its speciﬁc features and the conditions of the team (e.g. goals, resources, history of
working together). Many organisations have group facilitators, most commonly with
a training in psychology or other social sciences, working to promote team build-
ing or foster particular types of creative or collaborative processes (e.g. [54]). While
there are generic team processes to be considered, there are also unique aspects of the
inter/transdisciplinary interactions that need special attention and that ITH should
be equipped to address. In the context of ITH, facilitation activities should be tightly
coupled with, or occur under the scope of Laboratories for ITD.

350
A. T. de Melo and L. S. D. Caves
Facilitation in ITH can target audiences both internal and external to the organisa-
tions that host them or key partners (e.g. Universities) in two ways: (i) by facilitating
a suite of on-going activities aimed at creating an environment and culture favourable
to positive and creative interdisciplinary interactions and (ii) by providing on-demand
tailored services. The aim is to create playful, creative, informal contexts (i.e. envi-
ronments/processes) for different types of interactions between individuals, ideas,
disciplinary cultural practices, concepts, modes of thinking, methods and tools in
order to meet the needs of researchers with different degrees of inter/transdisciplinary
engagement.
On-Going Facilitation
ITH may offer a suite of activities including:
(i) Facilitated open workspaces
These are spaces that during designated periods of time can serve as hot desk environ-
ments. The distinctive feature is that they are organised and supported by a facilitator
so as to provide opportunities for the engagement between individuals from different
disciplines or areas of study, both directly and indirectly. The minimal requirement
for a researcher to spend time in these spaces is that they be willing to discuss their
work with others, to disclose the ideas, concepts and methods they are currently
working on and to engage in group activities managed by a facilitator in order to
promote dialogue. Activities may be as simple as asking the participants to visu-
ally represent the essence of the concepts, ideas, or methods they are working on,
have these exposed to others, and then be available to be approached or engage in
conversations. Additionally, during coffee or lunch breaks, facilitators may propose
interactive exercises to stimulate a particular type of creative interaction or to engage
the participants in ‘end of day’ discussions focused on a creative and free exploration
of possible interactions between the different facets of the participants’ work. Activi-
ties would be designed to overcome obstacles of language and expertise and facilitate
positive communication and interaction between different worldviews, concepts and
methods.
(ii) Interdisciplinary seminars and workshops
The ITH should follow the traditions of many research centres in promoting open
seminars on cross-disciplinary topics to a broad range of audiences, to the extent that
they serve to promote and disseminate inter/transdisciplinary experiences.
(iii) Reverse inter/transdisciplinary seminars
One of the main strengths of inter/transdisciplinary work is the exposure to and inter-
action with different perspectives and views on a given subject/phenomena. These
views derive from different lenses, modes of thinking and concepts, ideas and meth-
ods used as exploratory tools. However, established habits, or internal and external
disciplinary and contextual constraints may limit the extent to which interactions are

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
351
able to occur in a sustained context of respect, curiosity, openness and exploration,
where individuals allow themselves to engage in true reﬂections and to be perturbed
by the views of others.
In some ways, interdisciplinary interactions can be explored like interpersonal
relations, namely family relations. Not only is it necessary to keep a focus on the pro-
cesses of coupling supporting these relationships as it is possible to create opportuni-
ties for these relations to rehearse new modes of organisation. In (psycho-)therapeutic
contexts some approaches have been developed to allow a collaborative and reﬂex-
ive, but also respectful exposure of families to multiple views on their lives (and
realities), and through that, to create opportunities for the emergence of new pos-
sibilities for thinking, feeling and action. Techniques that have been developed to
facilitate change in family relations could be productively adapted to facilitate posi-
tive inter/transdisciplinary interactions (e.g. reﬂecting teams, [1]).
We have experimented with this kind of approach in the facilitation of interdis-
ciplinary dialogues and meetings and believe that these and other forms of collabo-
rative and reﬂexive dialogue strategies (e.g. [12]) can successfully be imported into
ITD contexts, but adaptation and application deserves more attention and should be
evaluated in relation to different types of outcomes and experiences.
(iv) Cross-fertilization events
Creativity beneﬁts from the exploration and enlargement of a space of possibilities.
To some degree, this may occur through the reconﬁguration and recombination of
the existing building blocks and contents of our thinking but also by engaging in
exploratory movements towards new modes of thinking or pushing their boundaries
to the point where new possibilities emerge and the landscape is transformed [7].
ITD activities constitute powerful instruments of scientiﬁc creativity [17]. How-
ever, it is well known that there are many barriers to positive inter/transdisciplinary
interactions, particularly those associated with differences of language or worldviews
(ontologies and epistemologies). Inter/transdisciplinary communities need to invest
time in building common ground and ﬁnding ways of communicating, outside of
the linguistic frames and inevitable jargon of their disciplines. In coordination with
the AKI, the ITH may work towards developing a necessary language and support-
ing metaphors that serve as a lingua franca to support interdisciplinary exchanges;
ideally, this would evolve (through a variety of processes) to support a wide range
of inter/transdisciplinary activities, but we anticipate domain-speciﬁc branches (as
dialects).
A series of activities can be designed to reduce obstacles to communication
whilst also promoting cross-fertilization across domains by facilitating the circula-
tion of ideas and concepts, methods and tools. ITH may organise activities that afford
researchers the chance to encounter, embrace and engender novelty and engage in
free explorations of other domains. This may occur in contexts that are free from the
everyday pressures of having to deliver outcomes for a particular project or grant
proposal. ITH can organise seasonal activities like ‘interdisciplinary markets’ where
researchers may explore each other’s work and showcase their preferred concepts,
ideas or tools. For example, researchers may be invited to enter an auction, where

352
A. T. de Melo and L. S. D. Caves
others may “buy” or exchange the ideas, resulting in a commitment from both seller
and buyer of later engaging in a deeper interdisciplinary engagement.
Researchers may also be required to make short talks in competitions where they
have to communicate without using ‘forbidden’ jargon words.
Activities should be organised so as to promote rich dialogues, between scien-
tists from different disciplines and with other stakeholders, based on exercises that
foster the importation, circulation, adaptation or ﬁltering and exploration of a ﬁeld
through the lenses of another one. This could involve the use of alternative modes
of communication based on artistic expression or the crafts. ITH should provide a
mixed portfolio of activities, balancing (all too common) goal-directed events with
(rarer) open-ended opportunities, that promote a free exchanges of ideas and the
strengthening of relations with the aim of building interdisciplinary capacity to meet
future challenges.
(v) Interdisciplinary reading and discussion groups
The deﬁnition of standards for the evaluation of inter/transdisciplinary research is
still a work in progress [39]. The ITH may contribute to these efforts through pro-
moting opportunities for the development of speciﬁc skills to assess and understand
interdisciplinary research processes and outcomes.
ITH can organise different types of reading groups, focused on the exploration
of ITD publications (papers; reports; themed discussions). Existing guidelines (e.g.
[34, 35, 39]) can be taken as a starting point for the reﬂection and discussion around
the papers. These groups can work towards the development of progressively more
complex inter/transdisciplinary modes of thinking and tools to manage the research
process from planning to publication. Through the discussion of papers, with a focus
on how interdisciplinary processes and outcomes were conducted and reported, they
may contribute to promote the sensitivity and capacity of the participants to analyse
inter/transdisciplinary research. If these activities are coordinated, across ITH, under
the scope of guidelines provided by AKI (see below), they may also play a critical
role in developing and reﬁning agreed processes and methods for exploring and
evaluating inter/transdisciplinary that may be widely adopted [13].
(vi) Databases of Inter/Transdisciplinary skills and networks
The ITH can play a role in mapping the networks (and types) of interdisciplinary
collaborations within their host or partner organisations by developing and manag-
ing databases that capture existing interdisciplinary projects, partnerships and col-
laborations. However, we believe that it is necessary to move beyond information
on interdisciplinary projects and collaborations, to building platforms that provide
information on speciﬁc proﬁles of ITD skills, that may lie at the level of the indi-
viduals or in the collaborative team/network. Thus, researchers registered on such
a platform, should be invited to provide information not just on their individual
work, but also encouraged to build a proﬁle of interests, skills, capacities and expe-
riences in relation to different types of ITD interactions and collaborations aiming at
particular types of goals [13, 28]. The information could include both disciplinary-
based skills and skills that are speciﬁc to ITD contexts (e.g. individual’s capacity to

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
353
facilitate meetings; building communication bridges within a team; capacity to be
challenged and take risks; writing skills, etc.). Researchers could optionally indicate
their stance/orientation in relation to particular kinds of work (e.g. readiness and
capacity to pursue a particular type of project, or play a particular kind of role in
a team). These skills databases will grow naturally if researchers invite their col-
laborators to participate. In this way, the interdisciplinary potential and capability
contained in dynamic collaborative networks may be captured.
We envisage that such databases will be of great value to a variety of agents.
They could be used by the researcher community themselves to search for potential
collaborators. This may be especially important for younger researchers, who are
seeking to engage in inter/transdisciplinary collaborations and who are still building
their professional networks. An obvious beneﬁciary are agents, particularly for the
civil society, seeking to form interdisciplinary teams for strategic purposes and/or in
response to a particular challenge and/or funding call. The databases may serve the
development of an ITD culture, by supporting the planning and management of both
the human resources and the contextual conditions inﬂuencing interactions between
individual researchers and disciplines; they could also support needs-assessment in
requests for facilitation services, e.g. in considering the degree of “collaborative
readiness” [24] of an ITD team; hence, they should allow for an exploration of an
individual’s general and speciﬁc competencies for ITD [24]. They should also include
information on teams, such as their stage of development (e.g. degree and quality
of previous engagement; levels of trust; type of shared experience in relation to the
nature of the interactions) in relation to the nature and stage of development of the
project (e.g. scope of ITD; clarity of common goals and desired outcomes; deﬁnition
of processes for ITD interaction) as well as contextual conditions (e.g. expectations
of funders; availability of equipment, capacities/constraints of the physical environ-
ment) [24].
We note that such databases are a valuable resource and there should be a strong
emphasis on developing appropriate ethical frameworks and security protocols to
safeguard individuals’ information and privacy.
On-Demand Facilitation
With properly trained teams, the ITH can also deliver on-demand facilitation ser-
vices, informed by the latest theoretical and methodological developments in the
ﬁeld and under the scope of an action-research [49] applied, collaborative approach.
These services should include an assessment of the needs of each client so that
the facilitation can be tailored to the degree of experience, maturity and current
needs of a particular group. These services may include the provision of (i) spaces
providing a neutral context for teams to meet. The spaces should be adaptive and
reconﬁgurable to create a physical environment that can be tailored to the needs of
a group at the particular stage of development of their work and their current state
as an inter/transdisciplinary team; (ii) facilitators; (iii) facilitation toolkits with sets

354
A. T. de Melo and L. S. D. Caves
of activities and strategies that the teams may use to facilitate their own internal
processes. These services can be provided in isolation or in combination.
3.1.3
Teaching
There is an increasing number of offers of (mostly) graduate and undergraduate
interdisciplinary teaching/learning programmes. However, this type of course is not
always delivered in a truly interdisciplinary fashion, nor do they necessarily include
speciﬁc training in interdisciplinary skills. Such interdisciplinary programmes don’t
necessarily expose students to the dynamics of inter/transdisciplinary team interac-
tions or provide opportunities to conduct their own research projects in a proper inter-
disciplinary fashion. Therefore, they don’t necessarily prepare students to be able to
engageinpositiveinter/transdisciplinaryteamprojects.Futureinter/transdisciplinary
researchers or practitioners need to have speciﬁc training in key skills, engage in live
interactions and have opportunities to observe the work of inter/transdisciplinary
teams or to be mentored in their own projects by experienced inter/transdisciplinary
researchers. Many of the obstacles and difﬁculties present in ITD can be avoided or
effectively managed if one is aware of them and has the skills to address them. Some
lessons are difﬁcult and often painstaking to learn without guidance or awareness of
the bumps and turns likely to be found along the roads of ITD.
We believe that ITH can play a unique role in (i) creating a context where col-
laborators from different disciplines can meet and work together in the conception,
planning and delivery of inter/transdisciplinary graduate research programmes; (ii)
support inter/transdisciplinary research in the context of undergraduate or gradu-
ate programmes; (iii) teaching the skills necessary for a positive engagement in
inter/transdisciplinary; (iv) creating opportunities for inter/transdisciplinary skills
and knowledge to be applied to real-world contexts in the context of continuous
professional development courses.
We envisage the following lines of teaching for the ITH:
(i) Skills courses
ITH could be responsible for delivering interdisciplinary skills modules on
inter/transdisciplinary undergraduate or graduate courses for their host research
organisations and/or partners [35]; cf. [29]). Advanced skills courses can also be
organised targeting established academics and researchers.
(ii) Graduate programmes
ITH can promote the organisation and the delivery of (i) inter/transdisciplinary grad-
uate programmes, focused on a variety of themes; (ii) inter/transdisciplinary grad-
uate programmes targeting the investigation of ITD itself (in a recursive fashion),
in its different dimensions and its research processes (in close collaboration, when
relevant to the Laboratories for ITD and the Facilitation Services) at its different
levels of organisation; (iii) practice-oriented graduate courses focusing on inter-
disciplinarity in terms of its management (e.g. “Masters of Inter/Transdisciplinary

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
355
Research Administration” targeting research ofﬁcers) and facilitation (e.g. Masters
programmes for inter/transdisciplinary facilitators).
(iii) Postgraduate and continuing professional development courses
ITH can facilitate inter/transdisciplinary courses focused on real-world challenges
and critical social issues, aimed at knowledge construction, integration and applica-
tion in real-world contexts. These courses can target (separately or simultaneously)
different stakeholders including practitioners, business managers and policy-makers.
(iv) Interdisciplinary summer schools
Some research centres have a tradition of conducting interdisciplinary summer
schools where students have an opportunity to engage in deep interdisciplinary
interactions, both directly and indirectly (through exposure to the interactions of
supervisors from different disciplines—cf. [64]). ITH are privileged places for host-
ing a variety of programmes with different levels and types of inter/transdisciplinary
engagement and, through the Laboratories of ITD, for the development and evalua-
tion of new interdisciplinary teaching technologies.
(v) Support and integration in an Alliance of Knowledge Integration
Finally, the ITH should play a critical role in supporting the activities of an AKI
through afﬁliation. The development and administration of the AKI should rely on
human resources coming from the ITH and synergies must be established between
their respective activities.
3.2
A Case for an Alliance for Knowledge Integration (AKI)
This paper is based on the assumption that knowledge should be pursued that pro-
motes sustainable and integrated well-being and human ﬂourishing in all dimensions
of human existence. However, it also assumes that are multiple types of knowledge
and multiple ways of knowing, as well as many views about them [30, 53]. There are
forms of local and contextually-bound knowledge that are particularly relevant for
a given cultural and social setting with a degree of ﬁtness for (local) problems that
may exceed that of traditional scientiﬁc knowledge. The complexity of the world
calls for an the integration of a plurality of ways of knowing and types of knowledge,
and an expansion of our horizons [38], as spaces of possibilities for action. But these
processes require special attention and purposeful investigation [48, 61].
The rhythms of scientiﬁc production are slow compared to the demands of the
world and society needs to develop responses based on incomplete knowledge [21]
that, nevertheless, have pragmatic value. There is a need for development of appropri-
ate frameworks that address aesthetic, ethical and methodological dimensions along
with issues of validation and decision-making associated with knowledge develop-
ment, implementation and integration in the contexts of application [45, 47].

356
A. T. de Melo and L. S. D. Caves
On the other hand, knowledge enterprises risk becoming just another commercial
activity, evaluated mostly in terms of its economic revenue or impact according to
metrics (e.g. impact factors; league tables, citations) that are blind to their aesthetic,
ethical and pragmatic value in relation to a common social good. A culture founded in
the pleasure and beauty of learning may need to be reinvented as Universities become
more and more focused on their balance sheets than in the celebration and apprecia-
tion of knowledge per se, and its relation to positive human development. Addition-
ally, if not reframed towards positive goals, the practices of knowledge production
may well be caught up in the same type of problem-focused, negative approaches
that have delayed a journey towards human ﬂourishing and general well-being [55].
There is an aesthetic dimension in the pursuit of knowledge [9] that, having played
a fundamental role in the development of human civilizations, risks being lost under
the contemporary pressures for problem-solving and (narrowly conceived) economic
targets.
3.2.1
Aims of an AKI
We propose the foundation of an Alliance for Knowledge Integration aimed at: (i)
promoting and disseminating high quality positive practices of ITD and conditions
to promote them, in all of their dimensions and levels of organisation; (ii) promot-
ing and facilitating extended debates about the aesthetics and ethics of knowing;
(iii) promoting and facilitating extended debates about the nature and possibilities
of knowing (its complementary pairs—the unknown/unknowable) and knowledge
integration towards a common social good; (iv) promoting positive practices for the
creation, dissemination and application of scientiﬁc knowledge in relation to other
socially, culturally or pragmatically relevant ways of knowing; (v) promoting posi-
tive collaborations within and between the different levels of organisation of society
regarding the promotion, integration, application and dissemination of knowledge
towards sustainable well-being and positive human ﬂourishing.
3.2.2
Foundation and Core Activities of an AKI
As already stated, the development of an AKI should rely heavily on the human, tech-
nological, material and knowledge resources of the ITH and other existing interdis-
ciplinary centres, constituted as critical founding partners for the AKI. Additionally,
from its inception, the AKI should involve critical stakeholders and representatives
from different levels of organisation of society in the planning, operationalisation
and evaluation of its development. The AKI should result from a process of co-
construction and coordination between different perspectives and views on knowl-
edge and knowledge integration.
Table 1 summarises our proposal for an AKI regarding its core activities, includ-
ing those more oriented to the academic community and some more oriented to
cooperation among different agents and levels or organisation of society. While the

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
357
Table 1 Summary of proposed activities to be developed for/by an alliance for knowledge integra-
tion
Core activities of an alliance for knowledge integration (FKI)
Journal of knowledge integration: processes and practices
A multidimensional process-focused journal both reporting and producing theory, research and
practices on inter/transdisciplinarity and KI; brief reports of praxis; commentaries and responses
focused on inter/transdisciplinary and KI; special issues with papers reporting meta-level
reﬂexive, recursive process-focused and critical syntheses and analyses of previously published
research; papers on review methods and reports of evolving peer-review methods and training of
inter/transdisciplinary and KI reviewers (developed and reﬁned as part of the journal’s
activities); development and integration of methodologies for lay/critical stakeholders reviews of
papers regarding the processes of production and application of integrated knowledge
Observatoire of inter/transdisciplinarity and KI Undertaking surveys and producing summary
research reports; mapping and monitoring inter/transdisciplinary research and KI practices
landscape; recommendations
Charter and awards for inter/transdisciplinarity and KI For institutions and individuals;
speciﬁcation of award levels and corresponding conditions for positive inter/transdisciplinarity
Database of inter/transdisciplinarity expertise and activity Containing information on
inter/transdisciplinary potentialities/skills/orientation/availability at the level of individual
researchers/teams/programmes/centres for ﬁnding/shaping/supporting collaborations, etc.)
Hubs and fora for integrated critical knowing (Extended critical and reﬂexive collaborative fora
and hubs to facilitate and disseminate dialogues and practices aiming at knowledge formation,
application and integration that implicates multiple levels of society (key-stakeholders and
common citizens) as well as formal and informal communities of knowers/knowing; Fora for
wider critical validation/transformation of knowledge and knowing practices integrating cultural,
social and ethical dimensions)
Citizen-science fora and public engagement for knowledge creation and KI (fora and hubs to
facilitate collaborative dialogues and practices regarding the development, implementation and
evaluation of different approaches to knowledge and its integration and for promoting the
involvement of different agents in scientiﬁc activities)
‘Wiki’ praxis Open source repository for the dissemination of positive practices and processes
associated with inter/transdisciplinarity and KI
table presents a brief description of each type of activity, we wish to highlight the
activities of the Journal for Knowledge Integration (JKI) and the Charter and Awards
for ITD and Knowledge Integration.
Open Journal for Knowledge Integration (JKI)
The JKI is conceived as an entity with a dual purpose both for disseminating knowl-
edge, but also for becoming a context and a Laboratory for the production of knowl-
edge regarding ITD and knowledge integration. The journal will be a space for
the open publication and dissemination of varied types of inter/transdisciplinary
research, without a focus on a particular domain or subject. Associated with its role
in publishing research, the journal should organise opportunities for the debate and

358
A. T. de Melo and L. S. D. Caves
dialogue of key issues relating to ITD through the discussion of speciﬁc papers in
multiple formats (e.g. round tables; commentary papers; focused seminars exploring
a paper from multiple perspectives; targeted reviews of related publications). These
dialogues and debates will also—in turn—be published, opening the way for further
discussion and commentaries, in a recursive manner. Published commentaries to the
papers can focus both on the processes and outcomes of inter/transdisciplinary as
well as on their relevance for society at large or speciﬁc groups. The Journal assumes
a reﬂexive and recursive stance to the extent that it will also investigates, critically
evaluates and reﬂects upon its own activities, publishing about its own processes and
results. Its editorial board will periodically investigate the journal output in order
to produce knowledge on the status of ITD and its key processes. Critical synthe-
ses, reviews and meta-studies, with a focus on the inter/transdisciplinary research
processes, will constitute a major activity. Hence the journal will simultaneously
be a ﬁrst-order, as a well as a second-order science journal, operating in recursive
loops. Effectively, itself a laboratory and platform for inter/transdisciplinary pro-
cesses and knowledge integration, the editorial board may constitute, integrate or
coordinate working groups focused on the development and reﬁnement of guide-
lines for inter/transdisciplinary peer review as well as for developing procedures
and guidelines for reviews conducted by lay people and representatives of critical
stakeholders groups. The reviews themselves will be interdisciplinary, as well as
transdisciplinary, engaging a variety of stakeholders and critical voices from differ-
ent sectors of society. Hence it should be organised in the spirit of an open science,
oriented towards public good and engaged in producing knowledge that is recognised,
critiqued and validated by representatives of a variety of critical social systems who
are either potentially affected by, or involved in, the knowledge production. Devel-
oping training programs for non-academic reviewers should be part of the Journal’s
activities.
Through the Journal and its other key activities, the AKI aims at becoming a
dynamic and transformative vehicle for a new type of science where ITD and diverse
practices of knowledge integration and evaluation play a critical role.
Charter and Awards for Inter/Transdisciplinarity and Knowledge Integration
A key feature of the ITD landscape is the diversity of views concerning deﬁnitions,
practices and evaluation. There is a need to integrate this knowledge and terminol-
ogy to develop an agreed set of terms and practices that will serve as benchmarks for
inter/transdisciplinary activity. We propose that the AKI will, through consultation
with the ITD community, draw up a Charter of ITD that sets out agreed deﬁnitions,
guidelines for supporting practices, indicators and frameworks for evaluation. Fur-
ther, in recognition of the different stages of investment/development in ITD of an
institution/centre, certain levels of attainment, regarding the extent to which they
create conditions for positive ITD at different levels (e.g. individual career support,
team level, institutional and evaluation structures) could be deﬁned and recognised
through a tiered series of Awards (e.g. Bronze, Silver, Gold etc.).

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
359
The Charter and Awards will serve multiple roles, including: (i) For institu-
tions/centres that seek a recognised framework to guide investment in the develop-
ment of their inter/transdisciplinary practices and culture, (ii) For institutions/centres
that seek to communicate their existing inter/transdisciplinary investment and cul-
ture, through alignment with the Charter, (iii) For funding and regulatory bodies,
to have a clear and recognised way of evaluating the commitment/capability of an
institution/centretoundertakeinter/transdisciplinaryresearch,and(iv)Forindividual
researchersseekingjobs,asaguideinevaluatingthereceptivityofinstitutions/centres
for undertaking and recognising inter/transdisciplinary research.
The idea of the Charter and Awards is inspired by the Athena Swan awards in
the UK-based Athena SWAN Charter (Athena Swan) that seeks to encourage and
recognise commitment to gender equality in advancing careers in higher education
institutions. Established in 2005, Athena SWAN has been very successful in chang-
ing the culture, with over 140 institutions as Charter members to date (2018) [2].
Another motivation is the power of standardised protocols. We draw an analogy of
the current state of ITD with the days of computer networking, characterised by
multiple protocols and limited compatibility, that constrained access and scalability.
We seek, through the Charter and Awards, the deﬁnition and adoption of agreed
standards and protocols (cf. TCP/IP or HTTP) for ITD that allow for widespread
adoption and rapid growth, and afford the possibility of stimulating the emergence
of vigorous and creative cultures for knowledge integration.
4
Conclusion
In this paper we present a proposal for knowledge production and integration
organised around the constitution Inter/Transdisciplinary Hubs and an Alliance for
Knowledge Integration, as highly coupled complementary entities. This proposal is
grounded in an ecosystemic, complex systems-informed view of ITD. While some
of the proposed activities have been carried out before, in different contexts and
to various extents, we hope that, from the careful consideration of the organisation
and dynamics of a range of coupled activities, something novel and valuable will
emerge and ﬂourish into the future. The implementation of these activities needs
to attend to the complexity of the ecosystemic relational processes involved in ITD
and knowledge integration to ensure their evolution and a positive adaptive ﬁtness
of its outcome, in the face of changing circumstances. The interactions within and
between the proposed activities may lead to the emergence of new opportunities for
knowledge production and to increase its complexity to a level more congruent with
the complexity of the world. This may lead to new or enhanced understanding and
afford new possibilities for action. More complex ways of knowing are required for a
transformation of the conditions leading to positive human ﬂourishing and future sus-
tainable development. We believe the ideas presented here deserve attention across
theoretical, empirical and pragmatic levels and hope that future research and social
initiatives will be developed around them.

360
A. T. de Melo and L. S. D. Caves
Acknowledgements The authors wish to thank Prof. Susan Stepney, from the York Cross-
Disciplinary Centre for Systems Analysis of the University of York for the many enriching and
stimulating conversations that have supported us and provided the motivation for the development
of the ideas presented in this paper.
The ﬁrst author was funded by FCT—Fundação para a Ciência e Tecnologia, Portugal (REF
DL57/2016/CP1341/CT0011).
References
1. Andersen, T.: The Reﬂecting Team: Dialogues and Dialogues about the Dialogues. Borgmann,
Broadstairs (1990)
2. Athena SWAN [Equity Charter for Gender Equality in Higher Education Institutions]. https://
www.ecu.ac.uk/equality-charters/athena-swan/
3. Bammer, G.: Disciplining Interdisciplinarity. Integration and Implementation Sciences for
Researching Complex Real-World Problems. Australian National University, Canberra (2013)
4. Bammer, G.: Strengthening interdisciplinary research: what it is, what it does, how it does
it and how it is supported. Report for the Australian Council of Learned Academies (2012).
http://www.acola.org.au
5. Beutler, L.E., Clarkin, J.F.: Systematic Treatment Selection: Toward Targeted Therapeutic
Interventions. Routledge, New York (2014)
6. Boden, M.A.: What is interdisciplinarity. In: Cunningham, R. (ed.) Interdisciplinarity and
the Organisation of Knowledge in Europe, pp. 13–23. Ofﬁce for Ofﬁcial Publications of the
European Communities, Luxembourg (1999)
7. Boden, M.A.: The Creative Mind: Myths and Mechanisms. Routledge, London (2004)
8. Börner, K., Contractor, N., Falk-Krzesinski, H.J., Fiore, S.M., Hall, K.L., Keyton, J., Spring,
B., Stokols, D., Trochim, W., Uzzi, B.: A multi-level systems perspective for the science of
team science. Sci. Transl. Med. 2(49), 1–5 (2010)
9. Borrelli, A., Grieser, A.: Recent research on the aesthetics of knowledge in science and in
religion. Approach. Relig. 7(2), 4–21 (2017)
10. British Academy: Crossing paths: interdisciplinarity, institutions, careers, education
and applications. London (2016). https://www.britac.ac.uk/news/british-academy-launches-
interdisciplinarity-report. Accessed 15 Feb 2018
11. Bronfenbrenner, U.: The Ecology of Human Development. Harvard University Press, Cam-
bridge, MA (1979)
12. Brown, J., Isaacs, D., The World Cafe: The World Café: Shaping Our Futures Through Con-
versations That Matter. Berrett-Koehler, San Francisco, CA (2005)
13. Bruun, H., Hukkinen, J., Huutoniemi, K., Klein, J.T.: Promoting interdisciplinary research.
The case of the academy of Finland. Academy of Finland, Helsinki (2005). http://www.
aka.ﬁ/globalassets/awanhat/documents/tiedostot/julkaisut/8_05-promoting-interdisciplinary-
research_-the-case-of-the-academy-of-ﬁnland.pdf
14. Caves, L., Melo, A.T.: (Gardening) Gardening: a relational framework for complex thinking
about complex system. In: Walsh, R., Stepney, S. (eds.) Narrating Complexity, pp. 149–196.
Springer, London (2018)
15. Collini, S.: Speaking of Universities. Verso Books, New York (2017)
16. Costanza,R.,Daly,L.,Fioramonti,L.,Giovannini,E.,Kubiszewski,I.,Mortensen,L.F.,Pickett,
K., Ragnarsdottir, K.V., De Vogli, R., Wilkinson, R.: Modelling and measuring sustainable
wellbeing in connection with the UN sustainable development goals. Ecol. Econ. J. Int. Soc.
Ecol. Econ. 130, 350–355 (2016)
17. Darbellay, F., Moody, Z., Sedooka, A., Steffen, G.: Interdisciplinary research boosted by
serendipity. Creat. Res. J. 26(1), 1–10 (2014)

Complex Systems of Knowledge Integration: A Pragmatic Proposal …
361
18. Duncan,B.L.,Miller,S.D.,Wampold,B.E.,Hubble,M.A.(eds.):TheHeartandSoulofChange:
Delivering What Works in Therapy, 2nd edn. American Psychological Association, Washington
DC, USA (2010)
19. Elliott, R.: Psychotherapy change process research: realizing the promise. Psychother. Res. J.
Soc. Psychother. Res. 20(2), 123–135 (2010)
20. Fann, K.T.: Peirce’s Theory of Abduction. Martinus Nijhoff, The Hague (1970)
21. Funtowicz, S.O., Ravetz, J.R.: Uncertainty, complexity and post-normal science. Environ. Tox-
icol. Chem. SETAC 13(12), 1881–1885 (1994)
22. Gibbons, M., Limoges, C., Nowotny, H., Schwartzman, S., Scott, P., Trow, M.: The New
Production of Knowledge. The Dynamics of Science and Research in Contemporary Societies.
Sage, London (1994)
23. Haack, S.: Defending Science—Within Reason: Between Scientism and Cynicism. Prometheus
Books, New York (2011)
24. Hall, K.L., Feng, A.X., Moser, R.P., Stokols, D., Taylor, B.K.: Moving the science of team
science forward: collaboration and creativity. Am. J. Prev. Med. 35(2 Suppl), S243–S249
(2008)
25. Hall, K.L., Stokols, D., Moser, R.P., Taylor, B.K., Thornquist, M.D., Nebeling, L.C., Ehret,
C., Barnett, M.J., McTiernan, A., Berger, N.A., Goran, M.I., Jeffery, R.W.: The collaboration
readiness of transdisciplinary research teams and centers ﬁndings from the National Cancer
Institute’s TREC Year-One evaluation study. Am. J. Prev. Med. 35(2 Suppl), S161–S172 (2008)
26. Hall, K.L., Vogel, A.L., Stipelman, B., Stokols, D., Morgan, G., Gehlert, S.: A four-phase
model of transdisciplinary team-based research: goals, team processes, and strategies. Transl.
Behav. Med. 2(4), 415–430 (2012)
27. Hardon, G.H.: Unity of Knowledge (in Transdisciplinary Research for Sustainability), vol. II.
EOLSS Publications, Oxford (2008)
28. Huutoniemi, K., Klein, J.T., Bruun, H., Hukkinen, J.: Analyzing interdisciplinarity: typology
and indicators. Res. Policy 39(1), 79–88 (2010)
29. Interdisciplinary
wiki.
https://www.wiki.ed.ac.uk/display/ISSTIInterdisciplinary/
Interdisciplinary+wiki
30. Jakubik, M.: Exploring the knowledge landscape: four emerging views of knowledge. J. Knowl.
Manag. 11(4), 6–19 (2007)
31. Klein, J.T.: A taxonomy of interdisciplinarity. In: Frodeman, R., Klein, J.T., Mitcham, C. (eds.)
TheOxfordHandbookofInterdisciplinarity,pp.15–30.OxfordUniversityPress,Oxford(2010)
32. Lissack, M.: Second order science: examining hidden presuppositions in the practice of science.
Found. Sci. 22(3), 557–573 (2017)
33. Lyall, C., King. E.: International good practice in the peer review of interdisciplinary research.
Report of a scoping study conducted for the RCUK Research Group by Catherine Lyall
and Dr. Emma King, The University of Edinburgh. https://www.research.ed.ac.uk/portal/ﬁles/
23461807/Lyall_and_King_Interdisciplinary_Peer_Review.pdf. Accessed 28 Feb 2018
34. Lyall, C., Bruce, A., Marsden, W., Meagher, L.: Identifying key success factors in the quest for
interdisciplinary knowledge. Report to NERC (2011)
35. Lyall, C., Bruce, A., Tait, J., Meagher, L.: Interdisciplinary Research Journeys. Practical Strate-
gies for Capturing Creativity. Bloomsbury, London (2011)
36. Mansilla, V.B.: Learning to synthesize. The development of interdisciplinary understanding.
In: Frodeman, R., Klein, J.T., Mitcham, C. (eds.) The Oxford Handbook of Interdisciplinarity,
pp. 289–306. Oxford: Oxford University Press (2010)
37. Mansilla, V.B., Feller, I., Gardner, H.: Quality assessment in interdisciplinary research and
education. Res. Eval. 15(1), 69–74 (2006)
38. Mazzocchi, F.: Western science and traditional knowledge. Despite their variations, different
forms of knowledge can learn from each other. EMBO Rep. 7(5), 463–466 (2006)
39. McLeish, T., Strang, V.: Evaluating interdisciplinary research: the elephant in the peer-
reviewers’ room. Palgrave Commun. 2, 16055 (2016)
40. Melo, A.T.: Abducting. In: Luria, C., Clough; P., Michael, M., Fensham, R., Lammes, S.,
Last, A., Uprichard, E. (org.) Routledge Handbook of Interdisciplinary Research Methods,
pp. 90–93. Routledge (2018)

362
A. T. de Melo and L. S. D. Caves
41. Melo, A.T.: The family as a complex systems. Contributions to understanding change and
resilience. Routledge (in preparation)
42. Morin, E.: Science avec conscience. Nouvelle édition. Fayard, Paris (1990)
43. Müller, K.H., Riegler, A.: Second-order science: a vast and largely unexplored science frontier.
Constr. Found. 10(1), 7–15 (2014)
44. Nicolis, G., Rouvas-Nicolis, C.: Complex systems. Scholarpedia 2(11), 1473 (2007). https://
doi.org/10.4249/scholarpedia.1473
45. Nowotny, H., Scott, P., Gibbons, M.: Re-thinking science. Knowledge and the public in the age
of uncertainty. Blackwell publishers, Cambridge (2001)
46. Polk, M.: Transdisciplinary co-production: designing and testing a transdisciplinary research
framework for societal problem solving. Futures 65, 110–122 (2015)
47. Ravetz, J.R.: Post-normal science and the complexity of transitions towards sustainability. Ecol.
Complex. 3(4), 275–284 (2006)
48. Raymond, C.M., Fazey, I., Reed, M.S., Stringer, L.C., Robinson, G.M., Evely, A.C.: Integrat-
ing local and scientiﬁc knowledge for environmental management. J. Environ. Manag. 91(8),
1766–1777 (2010)
49. Reason, P., Bradbury, H.: The Sage Handbook of Action Research: Participative Inquiry and
Practice. Sage, London (2013)
50. Rozenboom, W.W.: Good science is abductive, not hypothetico-deductive. In: Harlow, L.L.,
Mulaik, S.A., Steiger, J.H. (eds.) What If There Were No Signiﬁcance Tests?, pp. 366–391.
Earlbaum, New Jersey (1997)
51. Santos, B.S.: The End of the Cognitive Empire: The Coming of Age of Epistemologies of the
South. Duke University Press (2018)
52. Santos, B.S.: Decolonising the University: The Challenge of Deep Cognitive Justice. Cam-
bridge Scholars Publishing, Cambridge (2018)
53. Santos, B.S.: Epistemologies of the South. Justice Against Epistemicide. Routledge, Oxon
(2016)
54. Schwarz, R.M.: The Skilled Facilitator: A Comprehensive Resource for Consultants, Facilita-
tors, Coaches, and Trainers. Wiley, Hobokan, New Jersey (2016)
55. Seligman, M.E.P., Csikszentmihalyi, M.: Positive psychology: an introduction. In: Csikszent-
mihalyi, M. (ed.) Flow and the Foundations of Positive Psychology: The Collected Works of
Mihaly Csikszentmihalyi, pp. 279–298. Springer, Dordrecht, The Netherlands (2014)
56. Sprenkle, D.H., Davis, S.D., Lebow, J.L.: Common Factors in Couple and Family Therapy:
The Overlooked Foundation for Effective Practice. The Guilford Press, New York (2009)
57. Stokols, D.: Toward a science of transdisciplinary action research. Am. J. Community Psychol.
38(1–2), 63–77 (2006)
58. Stokols, D.H.D., Hall, K.L., Moser, R.P., Feng, A., Misra, S., Taylor, B.K.: Cross-disciplinary
team science initiatives: research, training and translation. In: Frodeman, R., Klein, J.T.,
Mitcham, C. (eds.) The Oxford Handbook of Interdisciplinarity, pp. 471–493. Oxford Uni-
versity Press, Oxford (2010)
59. Stokols, D., Misra, S., Moser, R.P., Hall, K.L., Taylor, B.K.: The ecology of team science:
understanding contextual inﬂuences on transdisciplinary collaboration. Am. J. Prev. Med. 35(2
Suppl), S96–S115 (2008)
60. Strang, V.S., McLeish, T.C.B.: Evaluating Interdisciplinary Research: A Practical Guide.
Durham University, Institute of Advanced Studies (2015)
61. Sutherland, W.J., Gardner, T.A., Haider, L.J., Dicks, L.V.: How can local and traditional knowl-
edge be effectively incorporated into international assessments? Fauna & ﬂora international.
Oryx 1–2 (2013)
62. Weingart, P.: A short history of knowledge formations. In: Frodeman, R., Klein, J.T., Mitcham,
C. (eds.) The Oxford Handbook of Interdisciplinarity, pp. 4–14. Oxford University Press,
Oxford (2010)
63. Whitehead, A.N.: Process and Reality, Corrected edn. The Free Press, New York (1978)
64. YCCSA [York Cross-Disciplinary Centre for Systems Analysis] Summer School. https://www.
york.ac.uk/yccsa/activities/summerschool/

On the Emergence of Interdisciplinary
Culture: The York Centre for Complex
Systems Analysis (YCCSA)
and the TRANSIT Project
Leo Simon Dominic Caves
Abstract The York Centre for Complex Systems Analysis (YCCSA) is an interde-
partmental, cross-disciplinary centre at the University of York with a focus on inter-
disciplinary research on complex systems. At an early stage, YCCSA was awarded
a 3 year grant, the TRANSIT programme, whose goal was the development of a
research culture that lowered the barriers to interdisciplinary engagement. TRAN-
SIT was conceived as a (complex) system of interrelated activities that focused on
“coming together”, “thinking together” and “working together”. Experimental activ-
ities included non-traditional seminar formats with eclectic programming, a focus
on group orchestration through facilitation and thinking systems, and mechanisms
for supporting feasibility studies though lightweight access to funds and to summer
students. The programme was community-driven with an ethos of openness, creativ-
ity and risk-taking. This period saw the emergence of a distinctive culture of deep
interdisciplinarity, exempliﬁed in new language and patterns of interaction, novel and
reﬂexive proposals for (the organisation of) processes of interdisciplinary research
in complex systems, a levelling of the academic hierarchy, and the self-organisation
of teaching, learning and supervision.
1
Context and Framing
The focus here is the York Centre for Complex Systems Analysis at the University of
York and its internal and external context and activities during the period 2008–2011,
but also both before (from ~2004) and to the time of writing (late 2018). This is a
personal view of the evolution of an interdisciplinary research environment that
encompasses researchers and their ﬁelds, research processes, the institutional setting
and the contemporary scientiﬁc trends and funding climate. This is not a formal
L. S. D. Caves (B)
Apartado 2001, EC Praia da Granja, 4411-901 São Félix da Marinha, Portugal
e-mail: leo.caves@gmail.com
York Cross-disciplinary Centre for Systems Analysis (YCCSA),
University of York, York, UK
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_15
363

364
L. S. D. Caves
Case Study, but more of a … story: it’s an ego-centric anecdotal account based on
memories and other imprints of this period, backed with dates/notes from personal
records. This may not align with others’ recollections and might warrant further
discussion or apology. I have done my best.
1.1
A Note on My Original Meeting and Working with Susan
Susan arrived in York as a Professor in Computer Science (CS) in 2002, escaping
her job as a Consultant in the IT sector. I had just changed my job in York from the
Department of Chemistry to Biology, and thus, in some sense, we were both starting
anew. I was Programme Director for the Masters of Research (MRes) in Bioinfor-
matics, a programme delivered by Biology, Chemistry and CS. Susan attended some
of our seminars at the time, but we did not connect.1 My ﬁrst record of interacting
with Susan was in late 2003. I was leading a reorganisation of the Bioinformatics
course to what became the MRes in Computational Biology. One of the new modules
was to be on the simulation of biological systems. Susan saw the outline for the new
module and suggested that it might be shaped such that it could also be offered on the
MRes Natural Computation that she was developing in CS. We planned and delivered
a module on “Simulating Complex Biosystems” to a mixed cohort of students from
both biological and mathematics/engineering backgrounds. For the ﬁrst couple of
years Susan and I attended each other’s sessions and offered different perspectives
on the topics being considered to the appreciation or bemusement of students. These
sessions were amongst the most enjoyable of my teaching career. If ‘to teach is the
best way to learn’, then co-teaching, is a catalyst of this process. I learned a lot, as
we all continue to do, from Susan.
2
Background
The world has problems, but universities have departments—Brewer [2].2
2.1
A Growing Demand for Interdisciplinary Research
There has been a growing call for interdisciplinary research from funding agencies
aroundtheglobe(seee.g.[15]).Ashierarchically-structuredinstitutions,Universities
have found it challenging to respond. One solution is the interdisciplinary research
centre (IRC) that sits between departments (though rarely crosses faculty lines). For
1They were not large seminar rooms, so it is a testament to our social skills.
2A catchy phrase, and one that the author attributes to “cynics”.

On the Emergence of Interdisciplinary Culture: The York Centre …
365
example, at the University of York in 2003, £12.5 M in funds3 were targeted at
the creation of new interdisciplinary research centres4 and new senior (often cross-
departmental) academic positions.
2.2
Origins of the York Centre for Complex Systems Analysis
(YCCSA)
In early 2004, in response to a growth (or resurgence) of systems approaches in Biol-
ogy—Systems Biology (see e,g. [10])—the then Chair of the Research Committee
in the Department of Biology conceived of a “York Centre for Complex Systems
Analysis” (YCCSA) to support the development of such approaches at York. This
concept was ﬂeshed out in the form of a bid to the BBSRC call for Centres for
Integrative Systems Biology (CISB) in June 2004. This large and ambitious bid cen-
tred around the strong biological, physical and computational research expertise and
interests of staff from the Departments of Biology, Chemistry, Computer Science,
Electronics, Mathematics and Physics. The proposal included a core staff (with sig-
niﬁcant dedicated time), new cross-departmental lectureships, and a £1 M fund to
support sabbaticals, pump-priming projects and Ph.D. and professional development
training. The University committed dedicated space for the centre on the prospective
campus expansion on Heslington East. The bid was unsuccessful, however the plans
for physical space for the proposed Centre were incorporated into the University’s
strategic infrastructure plans.5
2.3
YCCSA Special Interest Group
Followingthebid,colleaguesfromaroundtheUniversitybeganconveningaroundthe
topic of Complex Systems, becoming the YCCSA Special Interest Group (YCCSA
SIG)6 (see Fig. 1). The SIG met every 2 weeks or so at various locations around
3Supported by an award to the University of York from a bid to the Science Research Investment
Fund (SRIF) administered by the Higher Education Funding Council for England (HEFCE) ﬁnanced
by the UK Government Ofﬁce of Science and Technology.
4E.g. York Neuroimaging Centre (YNIC)—involving the Departments of Psychology, Computer
Science, Electronics, and Chemistry; Immunology & Infection Unit (IIU)—Hull York Medical
School (HYMS) and Department of Biology; Centre for Magnetic Resonance—Departments of
Chemistry and Biology; the York JEOL Nanocentre Departments of Physics, Electronics, and Chem-
istry.
5Via a bid to the Science Research Investment Fund (SRIF) administered by HEFCE.
6The SIG was conceived in a meeting of Susan with Tim Clarke (Electronics) on 21/09/2004.

366
L. S. D. Caves
Fig. 1 Presentation for the ﬁrst YCCSA SIG meeting on November 26, 2004 (Slides courtesy of
Susan Stepney)
campus. The focus of these initial meetings was on a topic related to complexity7;
the format was 3–4 speakers addressing the topic from different disciplinary per-
spectives, followed by a group discussion.8 A topic for the next session was typically
agreed on-the-ﬂy at the end of a session. This was a very interesting, organic, and
slow process of cross-disciplinary communication, through which we began to move
towards some (limited) mutual understanding. This process was nonlinear, discon-
tinuous and heavily feedback-driven.9 It could be revelatory in the surfacing of sur-
prising semantics (e.g. “same word, different meaning” and also, through discussion,
“different word, same meaning”) and always interesting in affording a glimpse of
the worldview of (colleagues from) different disciplines. The SIG postings and pre-
sentations were hosted on the Department of Mathematics moodle platform, which
facilitated communication10 of the distributed community.
2.4
A Physical Home for YCCSA: The Portakabin
Lacking external funding, the vision for YCCSA was pursued internally at the Uni-
versity. In late February 2005 the author was asked to take the lead in establishing a
physical home for YCCSA adjacent to the Department of Biology. The Portakabin11
(“Biology S-Block”12), had been a home to a number of startup initiatives over the
7The ﬁrst series of meetings were on “(Computer) Simulation”, “Emergence”, “Robustness”, “Evo-
lution”, “Communication and Cooperation”, “Self-Organisation and Self-Assembly” and “Stochas-
tic and Deterministic Systems”.
8Early meetings beneﬁtted from the skilful and reﬂective facilitation of Tim Clarke.
9Commensurate with its topic of interest.
10Believe it or not, this was something of a cultural innovation in those days ….
11These are pre-fabricated building “modules” named after the (York-based) company that produces
them. They are a common feature of many University campuses in the U.K.
12The astute reader would have noticed that this can be read as BS Block: ‘nuff said …’.

On the Emergence of Interdisciplinary Culture: The York Centre …
367
Fig. 2 Biology S-block, the initial physical home for YCCSA was on the upper ﬂoor (Aka the
“Portakabin in the Sky”)
years, and was in “previously enjoyed” condition (see Fig. 2). We were given the
top ﬂoor,13 which had 16 rooms, one of which we identiﬁed as a common room and
another that we (initially) used as a meeting room (see Fig. 3). A small budget was
provided to bootstrap YCCSA (including furnishing ofﬁces, funds for a seminar pro-
gramme, visitors, etc.), and the costs were split between participating departments.
The common room was furnished using “found” furniture in a dumpster outside
Biology; a discarded laser printer was coaxed into service. The building was mostly
functional14 but not pretty. It was space that was not coveted by others: we made it
our home.
13Our friendly neighbours downstairs were BioArCh—the multidisciplinary Bioarcheaeology
group. They were resident in the Portakabin until 2016, when they moved to the new Environ-
ment building adjacent to Biology.
14When it rained, water tended to ﬂow down the electric conduits ….

368
L. S. D. Caves
Fig. 3 Layout of the Portakabin. Schematic of the core ofﬁce space and occupants (ca. 2008). The
key conﬁguration of facilities at the end of the building have been added. Note, on entering from
the stairs, you could turn right, put your head around the door to say hello to the administrator
and/or spokesperson, peek at who was having coffee in the common room and bump into someone
coming back from the loo, or whoever was picking up their printing (situated outside common and
administrator rooms—not shown). Each ofﬁce had a glass-panelled door, but most were left open.
A quick walk down the corridor could alert all occupants to an impromptu meeting, or the arrival
of cake. Often no words needed to be used
2.5
YCCSA Staff and Community
On the one hand, the Portakabin was not large enough for all members of the nascent
YCCSA community and on the other, some members needed to reside in their own
their departments due to having laboratories and/or other facilities/duties that tied
them there. This led to an early “resident” and “non-resident” distinction in the
community. However it was decided that, whenever possible, students and RAs of
non-resident staff would be resident so as to beneﬁt from the interdisciplinary envi-
ronment and project meetings/supervisions would take place in the Portakabin.
3
Preparing for TRANSIT
3.1
MRC Institutional Discipline Bridging Award—PonteVita
While establishing an interdisciplinary environment we looked for opportunities to
support its development. One opportunity was the Interdisciplinary Bridging Awards
(IDBA) in September 2005, a joint MRC/EPSRC funded scheme aimed at institutions
who were committed to developing collaborative research programmes between the
Physical and Life Sciences and who had an innovative approach to interdisciplinary

On the Emergence of Interdisciplinary Culture: The York Centre …
369
working. The funders were looking for strategic plans and the appointment of a
‘Research Facilitator’ to oversee the proposed programme. The Research Facilitator
will be expected to liaise with the researchers across the interface to build up mul-
tidisciplinary programmes, through the organisation of workshops, away-days etc.
We worked as a team to build an Expression of Interest (EoI) off the pillars of the
University’s commitment to YCCSA, the SIG, the dedicated physical environment
(Portakabin) and our online community. We were also able to point at a signif-
icant portfolio of interdepartmental Masters training programmes, many of which
were coordinated and/or taught by YCCSA staff. We crystallised 3 cross-disciplinary
researchthemesofBiosystemsSimulation,Bio-inspiredComputation andBiosystems
DataAnalysis.Theproposedprogramme,PonteVita,comprisedaseriesofworkshops
related to our themes and also funds to allow researchers to move around via internal
secondments, visit collaborators, and to perform feasibility studies. An interesting
aspect of the proposal was to identify staff to act as interface coordinators between
departments, something that we could naturally support as many YCCSA staff were
cross-departmental appointments or had background/expertise placed them close to
another department.15 We were encouraged when our EoI was accepted but ulti-
mately our full proposal was not successful. Nevertheless, we had deﬁned ourselves
and formed a vision and this was an important step.
3.2
EPSRC Bridging the Gaps Award—TRANSIT
In April 2007, we became aware of the EPSRC Bridging the Gaps call, clearly inﬂu-
enced by the MRC IBDA, that had the principal objectives to: initiate new, long-term
collaborations between researchers across the EPSRC remit; stimulate innovative
approaches to collaboration between disciplines; increase the cross-fertilisation of
ideas and the take up of advances across the boundaries between disciplines; and
enable research organisations to encourage and embed multidisciplinary research
between departments and alleviate barriers to collaboration.
3.2.1
The TRANSIT Bid
Building off our thinking for the MRC IBDA, we approached the bid by “answering
the exam question”16 and set about identifying the gaps to interdisciplinary working.
The following extracts are taken largely verbatim from the TRANSIT proposal (see
Fig. 4).
15YCCSA had a lot of “boundary actors”, people that did not naturally sit squarely in their host
department(s). Another way of phrasing this was that we were a collection of outcasts, sometimes
termed the “lunatic fringe”. As a counterpoint to this, Susan quotes a colleague saying that we were
the “lunatic core”.
16Susan would invoke this framing device from time to time to keep us on track.

370
L. S. D. Caves
Fig. 4 Schematic of the TRANSIT process: identiﬁed gaps to interdisciplinarity (in red), the modes
of activity to address them (in blue) and example activities within these modes (in black in the
sidebar to right). Indicative feedback between different modes is shown (in green) (Figure taken
from TRANSIT proposal)

On the Emergence of Interdisciplinary Culture: The York Centre …
371
3.3
Objectives
1. To increase awareness of expertise and initiatives across disciplinary boundaries
at the University of York by establishing a dedicated day of activities—TRAN-
SITday—to include seminars and visitors supported by a Help Desk (“coming
together”)
2. To enhance interactions and the development of novel ideas through the explicit
fostering of creativity (“thinking together”)
3. To provide resources to support cross-disciplinary feasibility studies through
targeted pump-priming and a scholarship programme (“working together”)
We will explore mechanisms to support collegiality by acknowledging and recog-
nising individuals’ contributions to the multidisciplinary community through
peer-led feedback.
3.4
Gaps to Be Bridged
Awareness Gap: As in any organisation, staff are not always fully familiar with
their colleagues’ expertise, skills and research interests. This lack of awareness in
large communities of researchers has been recognised in research into large multi-
disciplinary teams in the US (giving rise to the refrain “if only we knew, what we
knew”—[9]. Staff need greater awareness of concepts, tools and problems outside of
their own research area to enter into the realm of cross-disciplinary research.
Interaction Gap: There is a lack of sustained interaction of staff across disciplines.
Different people and relationships need different ways to interact and prosper: every-
thing from casual face to face chats to intensive sandpit events; informal seminars to
discussions on an electronic bulletin board. There can be barriers to cross-disciplinary
engagement arising from the lack of a common conceptual framework, vocabulary or
methodology, and this can impede or limit the generation of novel ideas for research.
ResourcesGap:Novelcross-disciplinaryproposalsmaybeperceivedastoohighrisk
or simply too premature to warrant full scale funding. There is a need to evaluate early
ideas and then to progress the most promising, by providing the resources to take them
forward through feasibility studies, to generate preliminary results and prototypes.
Such evaluation would then provide a basis for well-motivated and strongly-justiﬁed
proposals for consideration by external bodies such as EPSRC and other funding
agencies. The University has several mechanisms for pump-priming new research
ideas, but these are always over-subscribed and often a cash sum is not all that is
needed; there is therefore a need for more—and more varied resources to support
novel science.

372
L. S. D. Caves
3.5
Management
Management was to be through colleagues from across departments who would act
as “Discipline Champions” within their own host departments, and who would take
the lead in various strands of the activities.
4
In TRANSIT
The TRANSIT bid was evaluated and awarded funding by the EPSRC, ranking 1st
in its round.17 The following brieﬂy outlines how the TRANSIT programme was
implemented and offers some experiences and lessons.
4.1
“Coming Together”—Seminars, Workshops, Away Days
4.1.1
TRANSITday
In order to increase awareness of the programme, build the community and
develop the research culture, we held a weekly programme of people-centred cross-
disciplinary events, that physically brought people together to meet, interact and
share. Friday became TRANSITday, our central device for building a cohesive cross-
disciplinary community by generating regular interactions and by providing the time,
space and support for creative thinking towards the generation of novel collaborative
research activities. The idea of TRANSITday was to tackle the essentially unbridge-
able gap of “never enough time”, by providing members with a tempo and focus
to help them organise their limited research time most effectively. YCCSA’s collec-
tive activities, such as training courses, workshops, sandpits etc. as well as individual
members’ activities such as cross-disciplinary supervisory meetings, project progress
meetings etc. were scheduled on TRANSITday whenever possible.
4.1.2
Interdisciplinary Seminar Series (Aka “Scone” Seminars)
Afocal point of TRANSITdaywas the“TRANSITInterdisciplinarySeminar Series”.
These seminars were held in a neutral (i.e. non-departmental) venue with comfort-
able, conﬁgurable furniture.18 The seminars extended up to 2 h: with a presentation
17http://gow.epsrc.ac.uk/ViewPanelROL.aspx?PanelId=4512&RankingListId=5842.
18A senior common room in one of York’s colleges. A key feature of this space was a large open
space with casual seating with an adjacent small room its own entrance, separated from the main
room by a movable partition. Here food/drink could be assembled while the seminar was running,
and then opened up to the main room when required. This is a very effective design pattern.

On the Emergence of Interdisciplinary Culture: The York Centre …
373
for the ﬁrst 45–60 min followed by a short break for drinks and refreshments (tra-
ditionally freshly-baked scones), which then led into a collective discussion with
the speaker. The discipline for the Chair of sessions with this format is to not allow
questions following the presentation itself, but to hold them back, as an incentive
for people to engage in the following discussion. The topics of these seminars were
very wide-ranging, with a broad theme of modelling and simulation of complex
systems involving the (various) couplings of physical, biological, ecological, social,
economic and technical systems, but also spanning ethics, creative arts, musical
composition, data visualisation/soniﬁcation, archaeology, robotics and non-standard
computation.19
4.1.3
Other Activities: Workshops, Training, Reﬂections
In addition to the Seminar series, a number of other events were part of the “Coming
Together” suite. Workshops: were half to one day cross-disciplinary events focused
on bringing together researchers in speciﬁc areas of interest (topics included: Col-
lective Dynamics; Systems Biology; The Art of Modeling). Training: Peer-led staff
training events in scientiﬁc/technical skills were offered by YCCSA staff to the
wider community at the University (topics included R statistical programming sys-
tem; network analysis; statistical mechanics). Reﬂections: We developed a tradition
of coming together to reﬂect on our cross-disciplinary activities and to provide feed-
back and suggestions for improvement of future activities (this spanned a wide range
of activities e.g. grant proposals, seminars, workshops etc.).
4.1.4
Away Days
We established an annual away day for YCCSA, focusing on strategic issues, reﬂec-
tion and interdisciplinary vision. These events are typically facilitated and highly
interactive. The location for these events was is a local “Scout Hut” adjacent to a
playing ﬁeld, surrounded by woodland. The hut itself, provided an open conﬁgurable
meeting area with an adjacent kitchen, connected by a serving hatch.20 It provided
just the kind of informality, intimacy and (relative) isolation to support effective
community interactions.
4.1.5
Experiences and Lessons
Do such activities raise awareness of, and encourage engagement in, interdisciplinary
activity? Our experiences have been that some kind of synchronisation of time to
afford interdisciplinary interaction is valuable, but that it is very hard to establish
19For a taste, please see https://www.york.ac.uk/yccsa/activities/news-events/seminar-archive/.
20Another instance of this effective design pattern.

374
L. S. D. Caves
and needs to be actively maintained. One route that we found is to put key YCCSA
events, staff meetings etc. in people’s individual timetable, linked to their institutional
(e.g. teaching) duties. Our experience is that it can take a year or two for this to
actually take effect, and while not guaranteeing availability, it at least provides a
constrainttobesatisﬁed.WeachievedthisforsomeacademicstaffwithintheYCCSA
community, but ideally, to facilitate wider cross-disciplinary interactions, this needs
an institutional-level buy-in.21
Academics’ free time is increasingly limited and deciding where to invest time can
be difﬁcult. Generally, there are too many seminars to attend even if you restrict your-
self to a single University department, without broadening your horizons. The broad
range of topics within the YCCSA/TRANSIT seminar series is, by nature, eclectic
and may be considered a luxury when time is limited. Location is also very impor-
tant, especially when a corridor in academia can be a gulf: if possible, try to secure a
central and/or attractive venue. A distinctive format and gratuitous refreshments can
provide some incentive but are perhaps not enough to encourage regular attendance.
Targeted advertising and use of “Discipline Champions”, with their knowledge of
internal communication channels, can yield good attendance at speciﬁc events. The
discussions following seminars can be a valuable seedbed for enhancing collective
understanding and for the growth of new ideas and collaborations.22 In one case, a
speaker’s experience led to their choosing YCCSA as the host for their fellowship
application, which subsequently resulted in a proleptic academic appointment.
Peer-provision of (research) training worked well. As Universities are full of
researchers who also teach (and researchers who are often looking to teaching expe-
rience), it seems such a natural (and mutually beneﬁcial) arrangement. There are
pressures that work against such collegial activities: for example, constant pressure
on people’s time and a lack of formal acknowledgement in workload models. On
reﬂection, it would be interesting to make offering such training be part of what
it means to be a member of an interdisciplinary community, it would then become
a community expectation. Such a commitment could then be argued for in con-
sideration of a members’ individual workload. In any event, it is suggested that a
vibrant peer-led cross-disciplinary training culture would be a sign of health of any
University (or indeed other institutions).23
The communal Reﬂections were surprisingly rich and effective. To bring people
together, across disciplines and the academic hierarchy, to share experiences and
(legitimately) feelings, evaluate pros and cons, and to mutually agree upon changes
to future activities, on a natural timescale, is not an innovation in itself, but part
of Reﬂective Practice. However, its explicit practice within an academic research
21Note that this is about synchronising potential time to interact, it’s not about dedicated time for
interdisciplinary interaction, which is a different consideration.
22These sessions could be regarded as constituting some form of “invisible college” c.f. the informal
gatherings of scholars predating the formation of formal scientiﬁc societies.
23The debate around the effect of GDP vs other indicators of an economy (such as General Well
Being) comes to mind.

On the Emergence of Interdisciplinary Culture: The York Centre …
375
environment is not common: it proved to be a great vehicle for sharing wider ideas
and exploring and forming norms.
4.2
“Thinking Together”—Facilitation and Structured
Thinking
In conceiving TRANSIT, we saw that regular physical interaction events, such as the
seminars, would build a community of interdisciplinary researchers, but that bringing
people together was not enough in itself. We saw that in order to get people to work
together effectively, we needed to pay attention to how we were going to support them
to think together effectively e.g. to understand each other’s (disciplinary) language
and operational culture, to be able to effectively exchange ideas, and to harness
their creativity to build new interdisciplinary (and eventually transdisciplinary24)
concepts and tools. To this end, we proposed that more targeted, and small(er)-group
events would be supported by Creative Facilitators. We contacted a consultancy for
training in the de Bono Six Thinking Hats (STH) and Lateral Thinking techniques
and developed a framework for the bid geared towards maximising cross-disciplinary
engagement and creative output.
4.2.1
Facilitation
We explored the use of facilitation in orchestrating engagement in a variety of cross-
disciplinary settings where a key challenge is bridging gulfs in language and oper-
ational cultures. A key feature was the training of an in-house team of facilitators,
rather than relying on the more common practice of buying-in external facilitators.
These facilitators were used in a variety of settings, but principally in proposal devel-
opment contexts. At the end of the TRANSIT programme, 11 local facilitators had
been trained, spanning academic and administrative staff, teaching fellows and post-
graduate researchers.
24Rosenﬁeld [14] identiﬁes three stages that represent the transition from discipline-centric to cross-
disciplinary research: Multidisciplinary researchers from different ﬁelds work in parallel or sequen-
tially, each from their speciﬁc area of expertise, to address a common problem; Interdisciplinary
researchers work jointly, but anchored from each of their respective disciplinary perspectives, to
address a common problem; Transdisciplinary researchers work jointly by breaking down the bor-
ders of their speciﬁc specialties and integrating discipline-speciﬁc theories, concepts and approaches
to synthesise a hybrid ﬁeld to address a common problem. Cross-disciplinarity is an umbrella term
that encompasses all of the above approaches.

376
L. S. D. Caves
4.2.2
Structured Thinking
We investigated the use of structured thinking methods (e.g. de Bono’s Six Thinking
Hats and Lateral Thinking) to increase effectiveness (creativity, productivity) of
staff interactions. Although training was initially provided by an external agency, the
YCCSA Administrator became an accredited trainer which allowed for a roll-out of
training across a wide cross-section of research (and some administrative) staff at
the University of York. Additionally, a special course was run for representatives of
EPSRC Bridging the Gaps programmes from other institutions. Feedback on these
courses was consistently excellent. At the end of the TRANSIT programme, 194
people had been trained. As a legacy, the University of York adopted Six Thinking
Hats as part of its Professional and Organisational Development Training Portfolio
and to date it is being used to assist efforts at reorganisation within the University.
We note these developments as an identiﬁable change in institutional culture (outside
of YCCSA) brought about by the TRANSIT programme.
4.2.3
Experiences and Lessons
Our experience with facilitation as a common feature of events was generally very
positive. We found that a non-vested facilitator provides a valuable role in mediat-
ing diverse cross-disciplinary groups by acting as a neutral party to highlight and
challenge the language and assumptions that are in play amongst participants. The
presence of a facilitator can also add “a sense of occasion” that makes participants
feel valued and “in good hands”. A good facilitator should be attuned to their environ-
ment and to the goal of the participants, respecting the needs and predilections of the
group. This is challenging to get right and sometimes a facilitator can misjudge the
language, tone or tempo of a group, which can lead to loss of goodwill and perfunc-
tory (or even non-) participation. It’s useful to frame facilitation as an experiment,
rather than a service, and thereby engender some goodwill and willingness to learn
on both sides. We found that facilitation can lead to genuinely richer, more creative
and more productive interactions, and to interesting creative outcomes.25
The introduction of Structured Thinking into a University environment was chal-
lenging at times as, in this context, thinking may be regarded as currency, and quality
of thought is, for many, their deﬁning attribute. However, we had a very particular
focus, that of promoting more effective and creative thinking in cross-disciplinary
contexts. We found that the use of STH, as a lightweight orchestration of modes
of thinking, had the desirable effect of promoting interactions by avoiding unpro-
ductive (and often premature) ping-pong style arguments, by opening up space for
creative thinking and, by virtue of making explicit the “rules” of engagement, it had
the effect of lowering barriers to participation across the academic hierarchy. We
found that STH works best when it is not over-engineered in terms of the sequencing
25A facilitator can be thought of as a catalyst, playing a role in accelerating processes without being
consumed by them.

On the Emergence of Interdisciplinary Culture: The York Centre …
377
of the modes of thinking, and when the goal of a meeting is well deﬁned.26 STH
is predominantly framed and trained as an effective problem-solving methodology
and primarily targeted to businesses. In academic research we spend a lot of the
time trying to ﬁnd good questions rather than solutions and thus the framing (and
training) of STH needs some adjustment for academic settings. Our interaction with
the consultants was very constructive and I believe both sides gained a lot from
the co-learning experience. Currently, facilitation is considered to be something of
a luxury in academic research environments and is most likely to occur (if at all)
in specialised, funded events (e.g. sandpits etc.). For a people-based organisation,
increasingly reliant on rapid, creative interdisciplinary responses, facilitation perhaps
should be regarded as an ecosystem service, and valued as such.27
4.3
“Working Together”—Venture Fund, Summer School &
Research Proposals
4.3.1
Venture Fund
The Venture Fund was a ﬂexible resource for responsively funding a variety of
interdisciplinary research activities—principally short feasibility studies but also for
supporting short visits. Feasibility studies typically involved the support of a research
student or RA for periods of 1–3 months. Applications were open to all levels and the
process was light-touch. Proposals were reviewed by a cross-disciplinary panel who
provided detailed feedback to applicants (often in person). The ethos was that funded
projects should be performed with a degree of openness as to unanticipated changes
of direction or difﬁculties and that, at the end of the project, the work (and learning)
would be reported back to the community. The Venture Fund supported activities
in diverse areas including Cancer Research, Environmental Policy, Non-standard
Computation and Drug discovery (18 applications funded).
4.3.2
Summer Scholarship Programme
TRANSIT supported summer internships to work on interdisciplinary feasibil-
ity projects, supervised by cross-disciplinary supervisory teams. Targeting high-
achieving undergraduate students entering their ﬁnal year of studies, we developed
a ~10 week long cohort-based programme, providing the students with a mentor,
26I remember one meeting for planning an international conference, involving about 8 participants,
where the key business of themes, roles, venues, timing etc. were largely settled in 30 min.
27Another analogy is with that of the secretariat, which was necessary when individuals lacked
the skills or capability to perform administrative functions, which are now increasingly supported
by software tools. Perhaps the new demand is for the effective harnessing of human potential,
suggestive of the need for a facilitariat.

378
L. S. D. Caves
research training and dedicated seminars from YCCSA researchers. Typically, the
summer school cohort was 8–12 students. The programme culminated with the stu-
dents presenting their projects to the community. The projects provided the means
to develop collaborations between colleagues across disciplines in novel areas of
research. Examples of projects from the ﬁrst year include “Developing software for
the study of complex networks”, “A metabolic subsumption architecture”, “Cou-
pled dynamical systems”, “Simulating Auxin transport canalisation in plants”. Later
years included collaborative projects with colleagues in the Departments of English,
Management, Music, Psychology, Archaeology and more. The summer school pro-
gramme continues to ﬂourish and is expanding into the exploration of group projects,
with teams of cross-disciplinary supervisors and students.28
4.3.3
Research Proposals
Key cross-disciplinary work within the TRANSIT programme was the generation of
research proposals. These included applications to a wide variety of funding agencies
includingEPSRC,BBSRC,LeverhulmeTrust,WellcomeTrustandYorkshireCancer
Research. These applications were characterised by wide and deep cross-disciplinary
engagement initiated and supported by a variety of the activities developed in the
TRANSITprogramme,includingstructuredthinking,facilitatedmeetings,feasibility
studies and reﬂections. Over the period of the TRANSIT grant 10 proposals were
supported to submission.
Developing research proposals provided our richest interdisciplinary experiences
and these were very inﬂuential in shaping the culture of the community and, in some
cases, transformative for individuals. For a period of 3–4 years, YCCSA had a unique
capacity at the institution for being able to respond to calls for proposals for large
interdisciplinary grant funding. We had developed a way (or dao) of developing
proposals which was built off our rich cross-disciplinary network of colleagues, our
skills and resources for facilitation, our administrative systems for coordinating input
from multiple departments, our (complex) systemic sensibility and (not least) our
openness to embrace challenge and new ways of thinking and working. We sought to
be inclusive in our work, inviting members of the institution’s own research support
ofﬁce to be involved with our processes. However, we were not very successful with
coupling with the University’s own research development systems, and over time we
formed the impression that we were not very well understood and that we began to
be regarded as “non-self” by the organisational immune system.
28For more information on the YCCSA summer school programme see https://www.york.ac.uk/
yccsa/activities/summerschool.

On the Emergence of Interdisciplinary Culture: The York Centre …
379
4.3.4
Collaborations
Through the TRANSIT programme YCCSA developed interdisciplinary research
collaborations with staff from the Cancer Research Unit, University of York (http://
www.york.ac.uk/biology/units/cru/); Centre for Immunology and Infection, Uni-
versity of York (http://www.york.ac.uk/cii/), Interdisciplinary Centre for Narrative
Studies, University of York (https://www.york.ac.uk/narrative-studies/), Stockholm
Environment Institute (http://sei-international.org/) and Stockholm Resilience Cen-
tre (http://www.stockholmresilience.org/), Goldsmiths College, University of Lon-
don (http://www.gold.ac.uk/sociology) and the Technical University of Delft (www.
tudelft.nl).
4.3.5
Experiences and Lessons
The Venture Fund was a fairly conventional pump-priming resource, with perhaps
wider eligibility, lower bureaucracy and increased openness and feedback. One of
the challenges in supporting short-term research is ﬁnding people to do the work who
are not already committed to other projects. We did have some internal secondments
and “end of contract” availability, as well as some visiting researchers, but resources
were limited and this was a common bottleneck in proposals. We generally found
the summer school an easier route for a pair of hands, although the timing was less
ﬂexible. The fund was useful for community projects, such as the development of
specialised computing resources (e.g. diskless computing nodes, that were realised
through a separate bid for University equipment fund) or telepresence initiatives (e.g.
a virtual teleconference wall to enable non-residents to join in common room chats
or group meetings in the Portakabin). Overall, there was value in having ﬂexible
funds available, and being able to encourage emergent ideas to be taken forward, but
there was a realisation that money was not the whole story.
Some thoughts on bridging the “personnel gap” were that line managers
(PIs/Supervisors) could sanction a proportion of time for their researchers to be
available for other projects, and this might constitute a ﬂexible, deployable resource
in its own right. A proposal for a feasibility study could then be presented to this
researcher resource, and if it elicited interest from suitable researchers, this could
become part of the evaluation of the bid. Although very much a sketch, one can imag-
ine a very dynamic and ﬂexible research capacity resulting from such a scheme. Some
colleagues saw that release of their researchers to some degree would be valuable in
developing their research proﬁle and portfolio. Others worried that such an arrange-
ment would need to be explicitly sanctioned by the funding agency and doubted that
a strong enough case could be made. It remains a relatively unexplored idea that
might ﬁnd a proving ground within a Ph.D. training programme?
The Summer School has proven to be very valuable in acting as the glue to bind
new cross-disciplinary collaborative links between (academic staff and postgraduate)
researchers as well as providing an available and eager pair of hands to undertake
feasibility studies. We have been consistently impressed by the quality (and quantity)

380
L. S. D. Caves
of research that can be achieved over a few short weeks, with progress often on a par
with Ph.D. students (acknowledging the inherently different tempos).29 The summer
school extends the research community over the summer months when conferences
and vacations can thin out personnel. The students themselves have generally been
very positive about their experiences and recognise something of the unique charac-
ter of the research environment itself. Interestingly, we see clusters of applications
from some institutions, which can often be traced to a recommendation from one of
the alumni. We recognise the importance of keeping the projects within the cross-
disciplinary remit and not allowing a “reversion to type” to a standard summer student
scheme. We also stress the importance of having a dedicated student mentor from
outside of the supervisory pool. We recognise that we have missed opportunities to
make more of the student experience and the outputs of the projects, in terms of
tracking and developing an archive of the enterprise. We see the value of nurturing
a community of summer school alumni, a task which may yet be undertaken.
4.4
Administrator
A good administrator is at the heart of a successful research group. In YCCSA we
were very fortunate in attracting someone who was already familiar with academic
research administration and who also had a background in corporate training and
organisational development. Our administrator was that rare and prized combination
of intelligence, high social skills and self-motivation wrapped in excellent judgement
and taste. The task undertaken was formidable, involving a loose deﬁnition of (an
ever-expanding number of) roles, and the necessity to bootstrap new systems and
processes that allowed YCCSA to operate across multiple departments and support
services at the University. Perhaps what the host institution did not know was that in
YCCSA, there was a single research unit with such a wide-ranging and integrated
set of contacts and processes.30 Perhaps, above all, we had someone who naturally
embraced our initiatives in facilitation and fostering creativity and who understood
community and the importance of culture. Our administrator was a ﬁrst-class member
of the TRANSIT team, instrumental in steering and shaping our progress.
4.5
Evolution of the Research Process
On reﬂection, it was in the development of research proposals where most of the
more fruitful, creative and rich experiences of TRANSIT were gained. For many of
us it was (and is) “where the thinking gets done”. Because of this, it is important
29Given the opportunity, and a supportive environment, young people can excel at research: it’s
a wonder why we keep them away from it for so long in our education system; one of our most
exceptional summer students still had another year to ﬁnish school ….
30An individual grant from YCCSA typically involved liaison with 6 or more Departments.

On the Emergence of Interdisciplinary Culture: The York Centre …
381
DTC
"CIDCATS "
Wellcome Trust
EoI
Full Proposal
Anthropospherics
Leverhulme Trust
Full Proposal
Cross-Disciplinary
Feasibility
Account 1
EPSRC
Cross-Disciplinary
Feasibility
Account 2
EPSRC
"Resilience"
Leverhulme Trust
May 2008
July 2009
June 2010
October 2010
January 2011
Complexity in the 
Real World
EPSRC
January 2010
Reﬂection
January 2008
Promoting 
Cross-Disciplinary 
Research
EPSRC
EoI
Fig. 5 Timeline and relationship of the major collaborative proposals during the TRANSIT pro-
gramme
to try to convey something of the character of these proposals to provide the reader
with a view into the thinking of the community at this time. A selection of proposals
(accompanied by comments) is provided to give a ﬂavour of our approach (see Fig. 5).

382
L. S. D. Caves
4.5.1
Anthropospherics31: A Transdisciplinary System-of-Systems
Approach to the Human Environment
Funders: Leverhulme Trust: Embedding of Emerging Disciplines call, 2008 (Sta-
tus: Unfunded at Expression of Interest stage). Partners: Environment, Biology,
Stockholm Environment Institute-York, Sociology, Archaeology, Law, Philosophy,
Physics, Mathematics, Computer Science and YCCSA.
Comment: This was the ﬁrst large YCCSA-coordinated research proposal. As an
emerging discipline we proposed the study of the coupled natural, behavioural and
instrumental systems of our world, using a complex systems (of systems) approach
(see Fig. 6a).32
4.5.2
A Learning Organisation for Researching Real World Complexity
Funder: EPSRC: Complexity Science in the Real World call, 2009 (Status: Unfunded;
Rejected by panel before Peer Review). Partners: Environment, Biology, Stockholm
Environment Institute-York, Sociology, Archaeology, Law, Philosophy, Physics, Pol-
itics, Mathematics, Computer Science, Electronics, Theatre Film and Television,
Health Sciences and YCCSA (plus many external organisations).
Comment: After an extensive multi-stage facilitated bid development process, we
believed that the only credible way for effective research on dynamic complex real
world systems was to have a commensurately complex organisation for research (see
Fig. 6b). We developed a model for an evolvable research organisation, as a type of
learning organisation [16], adopting a bioinspired process architecture incorporating
principles conferring evolvability and robustness. The proposal (aka “Manifesto”)
identiﬁed research themes that relate to the challenge of real world complexity.
4.5.3
Governing Complexity
Funder: EPSRC: Cross-disciplinary Feasibility Account 2nd call, 2010; Status:
Unfunded (at full proposal stage). Partner Departments: Biology, Chemistry, Com-
puter Science, Electronics, Management, Mathematics, Physics, Sociology.
Comment: This bid is notable for building off our TRANSIT experience and
proposing (and explicitly representing in a diagrammatic workplan) a non-linear,
feedback-driven emergent research process (see Fig. 6c).
31A term coined by Johan Kuylenstierna of the Stockholm Environment Institute-York.
32I hesitated as to whether to include this section, as it is perhaps the process of putting the pro-
posals together which should be the focus of the chapter; however, it’s interesting to note how
much of the outputs of the proposal generation were in themselves processes. Based on our expe-
rience, we reluctantly have to admit that process-led proposals are generally not rated as highly as
content/output-focused proposals (e.g. “But what is it that they are actually going to produce?”). I
still believe that if you get the process right, have the right environment and willing (and capable)
people, the (interesting, novel, excellent) output will take care of itself ….

On the Emergence of Interdisciplinary Culture: The York Centre …
383
Methodology
Position
Papers
Management
0
Creative
Events
18
12
6
Shaping
Reflective
Workshops
Ant
Robot
Human
Discovery
Discovery
Shaping
Discovery
Shaping
System Level and Impact
Organisation
Enterprise
Society
1
2
3
Narrative
Mapping
Mining, Mapping, Analysis, Synthesis, Interpretation
Monitoring, Reviewing, Evaluation, Brokerage, Shaping
Roadmap
Demonstrators
(a)
(d)
(b)
(c)
Fig. 6 Gallery of illustrative TRANSIT interdisciplinary research proposals: a Anthropospherics:
a proposal for a new intellectual framework that takes a holistic view of the biosphere, interactions
between it and humans (the anthroposphere), and how these interactions can be managed; b a
Learning Organisation for researching Real World Complexity: bio-inspired process architecture
for a robust, evolvable research organisation using the conceptual model of a bow-tie architecture;
c Governing Complexity: non-linear feedback-driven work programme for an emergent study of the
management of complex systems across levels and time; d CIDCATS: opportunities for “Just-in-
Time” research training from the range of elective modules available from our interdepartmental and
interdisciplinary Masters programmes, many of which were run/taught by YCCSA staff. Figures
taken from respective proposals
4.5.4
Combating Infectious Disease: Computational Approaches
to Translational Science (CIDCATS)
Funder: The Wellcome Trust: Centres for Doctoral Training, 2010; Status: Funded.
25 studentships over 5 years (~£3.6 M) (see www.york.ac.uk/biology/cidcats). Part-
ners: Biology, Chemistry, Computer Science, Mathematics, Physics, Electronics.
This initiative was underpinned by YCCSA and the Centre for Immunology and
Infection, University of York/HYMS, and the York Structural Biology Laboratory
(YSBL)
Comment: We began to see how some of practices, experiences and environment
could be of beneﬁt within a more formal Ph.D. training programme structure. We

384
L. S. D. Caves
proposed a 1+3 programme, with the ﬁrst year structured around ﬂexible (“Just-in-
Time”) cross-disciplinary training (see Fig. 6d), a laboratory-based group project
and 2 individual rotation projects.33
5
Emergent (Interdisciplinary) Culture
5.1
Community and Common Space
One of the key outcomes of the YCCSA/TRANSIT experience was the emergence
of a dynamic community of researchers. This brought a sense of belonging (and
identity) that was an interesting complementary world in relation to the standard
departmental structures (and buildings) of the university. There was a sense of a
shared “safe space” created by an openness and acceptance of the value of different
backgrounds, experiences, skills and disciplines. There was also a loosening of the
traditional line management-like relationships of Principal Investigator →Research
Assistant →Research Student, resulting in an effective ﬂattening of the academic
hierarchyandtheemergenceofanetworkofinteractionsacrosslevelsand(becauseof
YCCSA’s composition) across disciplines. From the perspective of interdisciplinary
research training, this kind of environment could be seen as “distributed supervision”.
The safe space was both a state of mind as well as a physical space in terms
of the Portakabin and its Common Room. The Common Room became a place of
friendship and collegiality (who can I ﬁnd to chat with?), of challenge (what ideas are
being thrashed out?), of surprise (what new artifact has appeared?) and of comfort
(what is there to eat?34). Notably, over time, the physical fabric also became part
of the community’s expression—with furniture being spontaneously painted, and its
arrangement subject to ongoing experimentation.35 It was a space that was owned
by and reﬂected the values of the community it served.36
33This emphasis on research-led training, through multiple research projects was a key feature of
our Masters of Research programmes of that time.
34Doughnuts were a particular favourite, however we found that no matter how questionable the
edibility of a given entity, if it remained in the common room long enough, it would invariably
disappear: late night working tends to lower powers of discrimination ….
35Only later when YCCSA moved location did we realise quite how much reﬂection and modelling
had gone into the furniture arrangement, as these ideas became part of the discussion of how best
to conﬁgure the common spaces in the new building.
36And beyond the community. One of the best testaments of that space was when a (non-YCCSA)
colleague was found sitting in the common room knitting. When queried as to why they were there,
they simply remarked “It’s just a nice place to be”.

On the Emergence of Interdisciplinary Culture: The York Centre …
385
Fig. 7 “Natural Complexity” a multi-panelled painting from YCCSA’s Art Club that includes
imagery representing members’ research interests, including virus structure and ﬁsh population
dynamics. The painting hung with pride in the common room in the Portakabin and became the
central feature of the entrance lobby when YCCSA moved to its new location in the Ron Cooke
Hub on Heslington East. The image was adopted as YCCSA’s visual identity
5.2
Playfulness and Creativity
To be playful and serious at the same time is possible, and it deﬁnes the ideal mental
condition—Dewey [6]
Creativity was celebrated. This took various forms, from the formation of the
YCCSA Art Club (see Fig. 7) to a Games Club that undertook long-running strategy
games. But playfulness was pervasive: the common room had Lego™and magnetic
construction sets, that were continuously formed into new and ever more elaborate
conﬁgurations. Additionally, as the sense of community grew, people became more
willing to share their own interests and talents. Notably, in “The Happening”—an
emergent social event—a whole bill of performances was assembled from the talents
of the community, including singing, guitar, violin, juggling and magic tricks.37
Crucially, playfulness also permeated discussions around research where there was a
general openness to creative thinking. An atmosphere was generated where “crazy”
ideas were welcomed and this reduced the fear of sharing half-formed/intuitive ideas
(or indeed well-considered ideas, but which were rather left-ﬁeld and which perhaps
had not yet been shared). In essence, there was a playfulness about the serious work
undertaken.
37Alongside the performances, a communal artwork was generated, with people painting anew
(randomly selected) sections of an existing painting that had been divided into a grid. The emergence
of a new painting constructed from the individual, but coordinated, contributions of many, with their
own capacities and talents, was a strong metaphor for YCCSA and its working culture.

386
L. S. D. Caves
5.3
Trust and Ethics
At the root of the emergent culture was Trust. When Trust exists, it allows people to
be free to give of themselves, but importantly it is also complementary to a sense of
responsibility to the community: if I am trusted, then I feel a responsibility for that
trust, and will act accordingly, and if someone trusts me, then I afford them recipro-
cal trust etc. An interesting side-effect of trust is the decreased requirement for (or
irrelevance of) a regulatory framework. Thus, YCCSA and the TRANSIT project
operated largely on the basis of Trust and without signiﬁcant explicit regulation.
This is remarkable and rare.38 One speciﬁc example of Trust was the Repository of
Proposals. We realised that a lot of our thinking, learning and practice was embodied
in proposal development. Most proposals are not funded (and we were no excep-
tion to this rule!), but many unsuccessful (or even successful) proposals languish in
individuals’ ﬁle systems. These documents and their ideas are typically not circu-
lated.39 We decided, in line with our community ethos, that the full lifecycle of a
proposal would be documented, including initial funding agency call documents, key
emails and development meeting notes, drafts summaries of the developing pitch, full
expression of interest and/or full proposal documents, referee comments and rebut-
tals. This lifecycle was assembled for several of our large proposals and placed on
an online resource.40 This resource was freely available to members of the YCCSA
community. In this way, the thinking that was generated from the community (even
if principally by a small subset of members) was fed back to the (wider) community:
ideas could be revisited, revised, retargeted as appropriate. This is a great (ecosys-
temic) model, as it circulates ideas and allows them to be adapted and to evolve.
However, in times of increasing pressure on individuals for output and with intense
competition for funding, this model is likely becoming more difﬁcult to put in place.
5.4
Learning
5.4.1
Discussion and Spontaneous Tutorials
Within the community, there was a strong element of co-learning. Within the common
room, there were often dynamic and wide-ranging discussions of different topics,
38I later read von Foerster [7] concerning ethics from a second order cybernetics perspective, who
quotes Wittgenstein ([18]—Proposition 6.422): “It is clear however, that ethics has nothing to do
with punishment and reward in the usual sense of the terms. Nevertheless, there must indeed be
some kind of ethical reward and punishment, but they must reside in the action itself.” Thus, I
believe that in the vessel of YCCSA, for some period of time, ethics were implicit and that people
found reward (and punishment) in their own actions.
39Here, I draw an explicit distinction with proposals from funded grants which an institution or
centre may make available as “exemplars” of how to write proposals.
40Unfortunately, this resource was lost during transitions between various online platforms. A great
shame.

On the Emergence of Interdisciplinary Culture: The York Centre …
387
with heavy use of the whiteboard much like you might see in any vibrant academic
“common room”. However, it was interesting to see the emergence of “spontaneous
tutorials” by some members of the community. These might occur in the context of
an interdisciplinary discussion when a concept was introduced (such as “epigenetics”
or “Bayesian statistics”) and it became clear that some participants were not fully
familiar with it. Leaping to their feet, the (spontaneous) tutor would grab a pen
and head to the whiteboard for a semi-structured tutorial introducing the necessary
rudiments of the concept in a manner accessible to the cross-disciplinary group. This
is teaching and learning in the wild—spontaneous, creative and for the love of it.
5.4.2
Dialogue
Another important mode of learning was dialogue i.e. “a freely ﬂowing group conver-
sation in which participants attempt to reach a common understanding, experiencing
everyone’s point of view fully, equally and nonjudgmentally” [1].41 This practice
emerged from, and became an invaluable feature of, the collective grant proposal
development activities. Typically, the dialogues would start in larger group contexts
and, as the bid developed, they would coalesce around a core “bid team”. Consid-
erable time, sometimes a few hours, could be invested in these conversations. Some
colleagues, after periods of extended dialogue reported experiences of “ﬂow” [5]
and of an extended self, resulting from extended and coherent interaction with others
[1].42,43
5.5
New Language and Patterns
Other indicators of the emergence of a distinctive culture include new language and
patterns (of activity). Here are some examples44:
5.5.1
Language
As a side effect of training in Six Thinking Hats, we found that the terms “White Hat”
(a mode of thinking related to capturing data/information) and “Red Hat” (relating
to the expression of feelings, without the need for logical justiﬁcation) became part
41This is the deﬁnition of Bohmian Dialogue see https://en.wikipedia.org/wiki/Bohm_Dialogue
taken from a proposal by David Bohm at: http://www.david-bohm.net/dialogue/dialogue_proposal.
html. Note that the dialogues undertaken in YCCSA were not knowingly conducted according to
Bohm’s principles, but have been mapped back to his conception in retrospect.
42Bohm likened the effect of dialogue to the superconducting state, which exhibits collective
behaviour resulting from the coherent dynamics of electrons.
43As I write this, I can imagine Susan’s eyes rolling slightly: maybe just a bit to “woo” for her ….
44Some other terms and patterns of activity have been lost over time ….

388
L. S. D. Caves
of the vernacular. These terms became triggers for orchestrating particular modes of
thinking and were very useful for keeping a small group working effectively. With
extensive use of facilitation we became attuned to the group dynamics, e.g. becoming
acutely aware of the “energy in the room” and how this can be rapidly dissipated if
the facilitator misreads the orientation, momentum or mood of a group.
In the extensive grant-proposal generation activities, we found that conventional
terms such as “PI” or “bid lead” did not seem to ﬁt well with our collective approach.
We found it was important that there was someone (or sometimes more than one
person) that kept an eye on the overall direction of a bid and this role became known
as the bid “host” (or sometimes “keeper of the vision”). The “host” of a bid might
not necessarily be the PI on the submitted grant application, as this dicision might
subject to other strategic concerns. Another emergent role was that of the “wrangler”,
someone who would chase up the agreed actions and generally keep things moving.
Interestingly, we also saw the emergence of “taste police”, colleagues who would
somehow act as moderators to keep the more extreme ideas and activities in check.
Related to how the notion of “leadership” is realised in such a collective, we
reﬂexively developed the notion of stewardship as a model of management, with
its connotations of “taking care of”, rather than directing activities. For its forma-
tive years, YCCSA did not have a Director, but a “Spokesperson”, reﬂecting the
communal ethos.
5.5.2
Patterns
In our activities relating to generating grant proposals, some interesting patterns were
established, including some/all of the following elements and processes45:
Environment: the beneﬁts of a dedicated space in which the bid development is
mainly centred. This became known variously as the “bid” or “war”46 room (or even
a “lair”!). This environment might become decorated with artefacts, such as key call
documents, graphical ﬁgures, draft budgets, grafﬁti, etc. The beneﬁt of having a ﬁxed
and known location that can be advertised is that it facilitates “rolling meetings” that
accommodate the various availabilities of an (extending) range of busy people.
Continuous Brieﬁng: One side effect of a rolling meeting is the need to brief
incoming colleagues to the current state of the evolving thinking. This requires con-
tinual analysis, synthesis and consensus as to where things are at. Articulation is
typically via verbal updates, but the process facilitates the preparation/update of
a lightweight brieﬁng document. Such a (concise, up-to-date) document is invalu-
able in approaching new colleagues or partners or to inform institutional contacts
of progress. The brieﬁng process was also useful in communicating the state of the
bid to the wider community, either through emails and/or an informal meeting in the
common room.
45Some aspects of these patterns have similarities to the Agile Software Development Methodology:
see https://en.wikipedia.org/wiki/Agile_software_development.
46Those of you familiar with Dr. Strangelove are probably pulling a wry smile?

On the Emergence of Interdisciplinary Culture: The York Centre …
389
Coalescenceofcoreteam:Duringtheprocessofdevelopingabidwesawtheemer-
gence of a set of colleagues (from Professors to graduate students) who somehow
remained attached to the bid: sometimes this was a tacit attachment e.g. colleagues
who kept turning up for meetings, or otherwise it was explicit, with people approach-
ing the emerging bid host(s) to express a desire to be involved. Over a period, a more
or less identiﬁable core team would coalesce, though there was sometimes (a neces-
sary?) ambiguity and uncertainty in this, which could cause friction if not carefully
managed. One of the indicators that a team was gelling was that they would begin
to risk being rude to each other, suggesting a base level of familiarity and trust had
been established.
Evolution via exploratory/constraining dynamics: The character of bid develop-
ment was evolutionary, broadly driven by the interleaving cycles of two concurrent
processes that feedback on each other: 1/Generative process: creative exploration and
expansion e.g. as a result of the use of structured thinking or dialogue and 2/Com-
pliance process: a narrowing down of scope through sanity checking and application
of known/anticipated constraints (c.f. “taste police” above). The ﬁrst process is gen-
erally characterised by relatively fast cycles of (reﬂective and reﬂexive) feedback
within the core team. The second process was characterised by 2 types of feed-
back operating on slower timescales: an intermediate scale that constituted the san-
ity check of close colleagues, and a slower timescale associated with the feedback
from the “Elders” i.e. more senior colleagues with great(er) experience of review-
ing, institutional dynamics and research council priorities. Over time, incorporating
multiple feedback cycles, it was common that bids would evolve signiﬁcantly over
their period of development (sometimes to the surprise, or even dismay of those not
coupled directly to its development)47: it was not a linear, incremental process, but
highly non-linear and emergent.
6
Post TRANSIT
Since the formal end of the funded Bridging the Gaps grant (Jan 2011) the YCCSA
culture and community have evolved, however some of the activities established in
TRANSIT have continued.
6.1
Change of Location
Almost from its inception, YCCSA’s physical space was destined to be on the Univer-
sity’s prospective East Campus extension. As plans developed, the location was iden-
tiﬁed as the Ron Cooke Hub. The Hub is the University’s ﬂagship building, intended
to be a microcosm of the University, incorporating research, teaching, knowledge
47“Beautiful things grow out of shit”—Brian Eno (see https://youtu.be/We1Cvs44i-Q).

390
L. S. D. Caves
exchange and public engagement. Some YCCSA staff were heavily drawn into the
design of the building48 and much time was spent discussing within the community
as to how to make the most of the move. In November 2010, YCCSA, humbly housed
in the top ﬂoor of an old Portakabin, moved to occupy two whole ﬂoors of the new
building. Although a very lovely piece of architecture, the new space was larger, and
split over 2 ﬂoors. The intimacy of the Portakabin was lost, and in losing some of
its constraints, we also lost some of its natural affordances for interaction and this
resulted in changes in patterns of behaviour. Architecture was not the only issue, as
the resident community also increased signiﬁcantly and this likely had some effect.
Also, the location on the new campus extension increased the barrier for interaction
with established colleagues on the original campus. We spent a lot of time working
on strategies to try to reduce this barrier, with rather limited success. Thus, the move
was a signiﬁcant change and it had some large and lasting effects in terms of the
interactions both within the community and with other University departments.
6.2
On-Going Activities
Following the move, YCCSA became more formalised in terms of the University’s
organisational structure, and gained a Director,49 rather than the previous role of
Spokesperson, which was not well understood by the institution. It also gained a
Steering Group, populated by representatives of partner departments, and with it a
modest annual budget. Thus, although the TRANSIT grant funding had ﬁnished,
there were resources for some of the activities to continue. The YCCSA seminar
programme, retaining its original format,50 remains an eclectic offering, attracting
some very interesting and stimulating speakers. The Summer School remains vibrant
and continues to provide great opportunities for cross-disciplinary staff collaboration,
and for undergraduate students to gain genuine interdisciplinary research experience.
The annual Away Day is a ﬁxture on the calendar and provides an opportunity for
reﬂection and planning. A recent development has been the introduction of an Inter-
national TRANSIT workshop on Cross-disciplinary Research. The ﬁrst workshop
was on Evolution, Evolvability and Change and was a very interesting and productive
event.51
Another recent change in YCCSA was the change from the title of York Centre for
Complex Systems Analysis to the York Cross-disciplinary Centre for Systems Anal-
ysis, signalling the deﬁning characteristic of the broader research activity. YCCSA
remains a signiﬁcant and recognised interdisciplinary centre at the University of York
and beyond.
48From modelling whole ﬂoor layouts, to the location of power and data sockets ….
49YCCSA’s ﬁrst and only Director to date is Susan Stepney.
50Scones have been replaced by cakes ….
51See https://www.york.ac.uk/yccsa/activities/evolutionevolvabilityandchange/.

On the Emergence of Interdisciplinary Culture: The York Centre …
391
7
Personal Reﬂections on Interdisciplinarity
One of my personal lessons in this experience has been that of the importance of
appreciating the richness of the whole person in interdisciplinary encounters. At
times, we experimented with capturing the visions and values of researchers in the
community: the results were illuminating. The drivers for people’s engagement in
research are numerous and diverse. Thus, in an interdisciplinary context, around the
table (or in the seminar room), you not only have widely different disciplines (i.e.
philosophies and operational cultures) represented, but also you have people who
may have very different motivations and goals.52 I believe that these factors need to
be anticipated, surfaced and embraced in order for successful interaction. At present,
these factors perhaps represent (one of the) elephants in the room in interdisciplinary
engagement. Thus, for effective collective action, environments and processes that
acknowledge, embrace and celebrate the (complexity of the whole) person should
be encouraged.
YCCSA continues to be an extraordinary entity, operating within the conventional
and predominantly hierarchical organisation of a University. In the midst of consid-
erable forces operating on YCCSA to conform to the prevailing norms of the wider
system in which it is embedded, maintaining a distinct space for quasi-autonomous
operation, that affords opportunities for creativity and relative freedom of expression,
is very demanding in terms of the effort required for modulating and transducing the
energy and information across the manifold interfaces with its environment. This bur-
den falls disproportionately and necessarily on the boundary actors—as they span
both worlds—and, for robustness, processes should be put in place to repair and
restore them.53
In my own intellectual journey, I have been drawn to learning more about the
origins of systemic thought (and how they relate to more contemporary complex
systems approaches). However, my learning has been deeply informed by the practice
and experiences of YCCSA and the TRANSIT programme. To some degree, at the
time, we were reﬂexive about nurturing a complex system (in order to study complex
systems), but as time passes there is a chance to look at the situation from a broader
perspective. YCCSA was an interdisciplinary centre (with a complex systems focus)
which developed a (complex systemic) proposal to foster interdisciplinarity. A range
of activities then followed which had strong community engagement. The result
was the emergence of a distinctive and vibrant interdisciplinary environment and
52You can argue that this is always the case in the workplace, and that it is a good understanding of
these factors that is key to a successful (i.e. happy, ﬂourishing, resilient) organisation. However, in
most (Western?) cultures, it is the norm to expect a person to express a subset of their characteristics
within the workplace, and that others have no place and should be suppressed. It is these factors
which may well leak into, disrupting or even derailing, a group activity.
53One could argue that if the cost of coupling with the environment was so high, that the system was
not sufﬁciently adapted to its milieu. To counter, for a (quasi-)autonomous system, it’s ok to wear
out certain of your components, provided there are intrinsic processes in place which will naturally
repair/replace them. This principle of organisational closure in the midst of an open ﬂow of energy
and material is the basis of the description of autopoietic systems [13].

392
L. S. D. Caves
(a)
(b)
Fig. 8 A relational view of interdisciplinary activity as an intervention in a complex system. a The
base relata to be considered; b some illustrative subdimensions of the base relata whose coupling and
dynamics needs to be considered in developing a better understanding of the emergent behaviour
(or culture)
culture. However, on reﬂection it is interesting to ask: What were the key elements
or processes in this? To what extent were the events and outcomes contingent on the
time, place and people involved? The answers will clearly depend on your criteria
and perspective.
I have been working with a collaborator on approaches to effect positive change
in complex systems. One approach to this we call Gardening, employing a metaphor
for our relations with complex systems.54 We adopted a reﬂexive, relational, process-
oriented worldview that considers interventions on a complex system in the context of
their environment [4]. Thus, for an intervention designed to foster interdisciplinarity,
we need to consider (at the very least) a group of People, in a particular Environment,
undertaking a series of Activities (which may be considered an intervention). These
aspects may be considered in relation to each other (as relata), with the relations
indicating some kind of coupling (see Fig. 8a). This situation is already suggestive
of some complexity. However, if we also consider some additional characteristics
of our relata and the dynamic coupling between them, the situation clearly becomes
complex (Fig. 8b).
From this perspective, it is not clear that you can take look at the outcomes
of a series of activities that were developed in one context (and particular period
of time) and make any particular inference as to their effectiveness for achieving
speciﬁc outcomes in another context (and/or at another period of time). To approach
this requires a much better understanding of which type of interactions, under what
kind of conditions, lead to what kind of outcomes: i.e. we would need to Garden
Interdisciplinarity …55
54A metaphor originating in the Complexity in the Real World bid, that has subsequently been
signiﬁcantly developed and reconceptualised.
55For more thoughts on a relational (ecosystemic) perspective of interdisciplinarity, please see Chap.
14 in this volume by Ana Teixeira de Melo and Leo Simon Dominic Caves.

On the Emergence of Interdisciplinary Culture: The York Centre …
393
8
Concluding Thoughts
This is an attempt to provide an account of the rich and complex experience of enact-
ing the TRANSIT programme within the evolving context of YCCSA. Something
interesting happened. I believe that an open, creative, positive, supportive community
was created which embraced risk-taking, challenged norms and looked to make a dif-
ference. YCCSA became an adaptive, evolvable learning organisation with tremen-
dous energy and momentum. However, the conditions for this kind of emergence are
contingent on the complex relational dynamics that sustain it. All of these relations
are subject to continuous change at different timescales (e.g. the people involved,
the physical environment, the policies and goals of the organisations involved) and
thus in order to sustain, the emergent processes will necessarily adapt and evolve,
resulting in different outcomes. The extent to which communities can be controlled
to deliver particular outcomes is conventionally a question of “management”, how-
ever it is also a question of ethics. YCCSA and the TRANSIT programme were very
much focused on the creation of a culture in which interdisciplinarity ﬂourishes:
in that the community consisted of researchers, operating within the context of a
university, the anticipated outcome was creative, high quality research, but it was
not explicit objectives that drove or shaped processes, rather it was implicit expec-
tation. Of course, explicit metric-driven lenses of evaluation existed outside of the
programme, in the institutional and funding agency context, but these considerations
were, to some extent, not dominant within the community and did not become instru-
mental within the culture. From a complex systems perspective, I believe this to be a
ﬁtting approach: if you need a certain degree of decoupling of system behaviour from
the lower level components and relational dynamics to confer evolvability (and the
creativity that implies) and robustness (see e.g. [11]). Sadly, this is not the prevailing
management ethos within the University sector.56
Interdisciplinarity is a difﬁcult term: everyone seems to know what it is, and yet
it is not commonly well deﬁned. Almost every academic institution or centre say
they practice it, and thus, at face value, it is not a very distinctive or distinguishing
pursuit. However, there are different kinds of practices that lie under this umbrella
term, that have qualitatively different characteristics. Acknowledging the signiﬁcant
literature on inter/transdisciplinarity and team science [8, 12, 17], there is still a need
for serious intentional and coordinated study to allow us to get a better understanding
of this complex phenomenon. This would then allow us to better deﬁne interdisci-
plinarity (and its relation to other modes of cross-disciplinary activity), understand
and articulate its value, and to facilitate the development of dynamic cultures of
interdisciplinarity to support positive outcomes.
56Interdisciplinary environments are more vulnerable to metric-driven research management cul-
tures (see e.g. [3]).

394
L. S. D. Caves
9
Endnote
This is an individual perspective. Different views, interpretations and conceptual-
isations are possible (Fig. 8 provides an indicative sketch of different factors and
relations that would contribute to distinguishing different observers). However, it is
my hope that for a few years, a community of graduate students and RAs in YCCSA,
as friends and colleagues, enjoyed an empowering and creative research environment
and that their experiences might be a positive inﬂuence on what they value, seek or
build on their own paths.
Acknowledgements I would like to express my thanks to:
Susan Stepney for her support, mentorship and friendship through challenging and exhilarating
times. Her openness, curiosity and generosity are inseparable from her ﬁerce and ﬂuid intelligence
and her humility and humanity.
Other members of the TRANSIT team (Gustav Delius, Angelika Sebald, Jon Timmis, Jamie
Wood) for their roles in developing and steering the programme.
Caryn Douglas, as the original YCCSA Administrator, for embracing the craziness and keeping
us on-track.
Sarah Christmas, who continues as YCCSA Manager, with great grace and skill.
Tim Clarke, for contributions to and information on the origins of the YCCSA SIG.
James Dyke, for your contributions and commitment to the Real World bid, and for spending
hours on Skype, often with children at your feet … (oh and your forgiveness for Fontgate).
Emma Uprichard, for turning up at a key time and asking “What can I do?”.
John Forrester, our resident Anthropologist, for steering me into transdisciplinarity.
Adam Prothero, Chuck Dymer and James Meyer for your ﬂexibility and patience in learning
how to facilitate in an academic environment (“a wheelbarrow full of frogs”?).
Matthew Collins, Director of the highly interdisciplinary BioArCH centre, for being our neigh-
bour and enthusiastic friend.
RichardLaw,forhisunendingenthusiasm,support,encouragementandcommitmenttoYCCSA.
Ottoline Leyser, for having the original vision for a complex systems centre at York.
YCCSA stalwarts: Alastair Droop, Corrado Topi, Simon Hickinbotham, Phil Garnett, Jenny
Burrow, Jess Wardman, for their selﬂess energy, enthusiasm, and support.
The YCCSA community.
The EPSRC for funding (and trusting) such an interesting series of projects in the ﬁrst few
rounds of the Bridging the Gaps programme.
A special thank you to Simon Hickinbotham for his review (and for waiving anonymity in the
YCCSA tradition of openness) that prompted valuable improvements to the manuscript. The one
thing I could not do was tell the story from others’ perspectives. Maybe this will happen one day?.
References
1. Bohm, D.: On Dialogue. In: Nichol, L. (ed.). Routledge, London
2. Brewer, G.D.: The challenges of interdisciplinarity. Policy Sci. 32(4), 327–337 (1999)
3. Bromham, L., Dinnage, R., Hua, X.: Interdisciplinary research has consistently lower funding
success. Nature 534(7609), 684–687 (2016)
4. Caves, L.S.D., Melo, A.T.: (Gardening) gardening: a relational framework for complex thinking
about complex system. In: Walsh, R., Stepney, S.: Narrating Complexity. Springer, London
(2018)

On the Emergence of Interdisciplinary Culture: The York Centre …
395
5. Czikszentmihalyi, M.: Flow: The Psychology of Happiness. Rider, London (1992)
6. Dewey, J.: How We Think. Courier Corporation (1910) (1997)
7. Foerster, von H. (ed.): Ethics and second-order cybernetics. Understanding Understanding:
Essays on Cybernetics and Cognition, pp. 287–304. Springer, New York, NY (2003)
8. Huutoniemi, K., Klein, J.T., Bruun, H., Hukkinen, J.: Analyzing interdisciplinarity: typology
and indicators. Res. Policy 39(1), 79–88 (2010)
9. Katz, N., Lazer, D., Arrow, H., Contractor, N.: The network perspective on small groups: theory
and research. In: Poole, M., Hollingshead, A.: Theories of Small Groups: Interdisciplinary
Perspectives, pp. 277–312. SAGE Publications Inc, United States (2005)
10. Kitano, H.: Systems biology: a brief overview. Science 295(5560), 1662–1664 (2002)
11. Kitano, H.: Biological robustness. Nat. Rev. Genet. 5(11), 826–837 (2004)
12. Lyall, C., Bruce, A., Tait, J., Meagher, L.: Interdisciplinary Research Journeys: Practical Strate-
gies for Capturing Creativity. Bloomsbury Publishing (2015)
13. Maturana, H.R., Varela, F.J.: Autopoiesis and Cognition: The Realization of the Living, vol.
42. Springer Science & Business Media (1991)
14. Rosenﬁeld, P.L.: The potential of transdisciplinary research for sustaining and extending link-
ages between the health and social sciences. Soc. Sci. Med. 35(11), 1343–1357 (1992)
15. Rylance, R.: Grant giving: global funders to focus on interdisciplinarity. Nature 525(7569),
313–315 (2015)
16. Senge, P.M.: The Fifth Discipline: The Art and Practice of the Learning Organization. Random
House, Revised and updated (1990) (2006)
17. Stokols, D., Hall, K.L., Moser, R.P., Feng, A., Misra, S., Taylor, B.K.: Evaluating Cross-
Disciplinary Team Science Initiatives: Conceptual, Methodological, and Translational Per-
spectives, pp. 471–493. Oxford Handbook on Interdisciplinarity. Oxford University Press,
New York (2010)
18. Wittgenstein, L.: Tractatus Logico-Philosophicus. Routledge and Kegan Paul, London (1921)
Translated by D.F. Pears and B.F. McGuinness (1961)

On the Simulation (and Energy Costs)
of Human Intelligence, the Singularity
and Simulationism
Alan F. T. Winﬁeld
Abstract For many the Holy Grail of robotics and AI is the creation of artiﬁcial
persons: artefacts with equivalent general competencies as humans. Such artefacts
would literally be simulations of humans. With the theme of simulation this essay
reﬂects on both the simulation of intelligence and the associated energy costs across
threebroadandcontroversialtopics:ﬁrst,howtodesign(orevolve)human-equivalent
AI, second, the prospects of an intelligence explosion once the ﬁrst has been achieved,
and third, simulationism—the idea that we are ourselves simulations in a simulated
universe.
Keywords Artiﬁcial intelligence (AI) · Artiﬁcial general intelligence (AGI) ·
Energy · Simulation · The singularity · Simulationism
1
Introduction
This essay offers some personal reﬂections on three broadly related subjects. The ﬁrst
is human-equivalent artiﬁcial intelligence (AI), that is an AI or a robot, its embodied
counterpart, which would be regarded as having equivalent general competencies of
a human (the question of what competencies and which human is one I will touch
upon). The second, is the question of what happens after human-equivalent AI has
been achieved: the technological singularity. And the third is simulationism: the idea
that we are living in a computer simulation. These three subjects are linked in two
ways. One is that they all deal with the simulation of complex systems—given that an
AI is a simulation of natural intelligence—and the other is energy, since simulation,
like any physical process, costs energy.
A. F. T. Winﬁeld (B)
Bristol Robotics Laboratory, UWE, Bristol, UK
e-mail: alan.winﬁeld@brl.ac.uk
© Springer Nature Switzerland AG 2020
A. Adamatzky and V. Kendon (eds.), From Astrophysics to Unconventional
Computation, Emergence, Complexity and Computation 35,
https://doi.org/10.1007/978-3-030-15792-0_16
397

398
A. F. T. Winﬁeld
2
Towards Human-Equivalent AI
2.1
Robots as Smart-as-Humans
In recent years we’ve seen many headlines conﬁdently predicting robots with human
equivalent AI. One such headline, from 2014, read ‘2029: the year when robots will
have the power to outsmart their makers’,1 occasioned by an Observer interview
with Google’s director of engineering Ray Kurzweil. A survey of AI experts found
a median prediction of a 50% chance of human-level AI during the 2040s [10].
Much as I respect Kurzweil’s achievements as an inventor, I believe he is mistaken.
Of course I can understand why he would like it to be so—he would like to live long
enough to see this particular prediction come to pass. But optimism doesn’t make for
sound predictions. Here are several reasons that robots will, I believe, not be smarter
than humans by 2029, 2039 or 2049.
• What exactly does as-smart-as-humans mean? Intelligence is very hard to pin
down [1, 6]. One thing we do know about intelligence is that it is not one thing that
humans or animals have more or less of [17]. Humans have several different kinds
of intelligence—all of which combine to make us human. Analytical or logical
intelligence of course—the sort that makes you good at IQ tests. But emotional
intelligence is just as important, especially (and oddly) for decision making. So is
theory of mind—the ability to infer others’ beliefs and intentions.
• Human intelligence is embodied. As Pfeifer and Bongard explain in their outstand-
ing book you can’t have one without the other [11]. The old Cartesian dualism—the
dogma that robot bodies (the hardware) and mind (the software) are distinct and
separable—is wrong and deeply unhelpful. We now understand that the hardware
and software have to be co-designed; animal brains and bodies clearly did not
evolve independently. But we really don’t understand how to do this—none of our
engineering paradigms ﬁt. A whole new approach may need to be invented.
• As-smart-as-humans probably doesn’t mean as-smart-as newborn babies, or even
two year old infants. Somehow comparable in intelligence to adult humans per-
haps? But an awful lot happens between birth and adulthood. And the Kurzweil-
ians probably also mean as-smart-as-very-well-educated-humans. But of course
this requires both development—a lot of which somehow happens automatically—
and a great deal of nurture. Again we are only just beginning to understand the
problem, and developmental robotics—if you’ll forgive the pun—is still in its
infancy.
• Moore’s Law will not help. Building human-equivalent robot intelligence needs
far more than teraFLOPS of computing power. It will certainly need computing
power, but that’s not all. It’s like saying that all you need to build a cathedral is
150,000 tons of marble. You certainly do need large quantities of marble—the raw
1https://www.theguardian.com/technology/2014/feb/22/computers-cleverer-than-humans-15-
years.

On the Simulation (and Energy Costs) of Human Intelligence …
399
material—but without (at least) two other things: the design for a cathedral, and the
know how and tools to transform the architectural drawings into a building—there
will be no cathedral. The same is true for human-equivalent robot intelligence.
• The hard problem of learning and the even harder problem of consciousness.
(I’ll concede that a robot as smart as a human doesn’t have to be conscious—a
philosophers-zombie-bot would do just ﬁne). But the human ability to learn, then
generalise that learning and apply it to completely different problems is funda-
mental and remains an elusive goal for robotics and AI. This is called Artiﬁcial
General Intelligence, which remains as controversial as it is unsolved.
These are the reasons I can be conﬁdent in asserting that robots will not be smarter
than humans within 15 years. It’s not just that building robots as smart as humans
is a very hard problem. We have only recently started to understand how hard it is
well enough to know that whole new theories (of intelligence, emergence, embodied
cognition and development, for instance) will be needed, as well as new engineering
paradigms. Even if we had solved these problems and a present day Noonian Soong
had already built a robot with the potential for human equivalent intelligence—it still
might not have enough time to develop adult-equivalent intelligence by 2029.
2.2
The Energy Cost of Evolving Human-Equivalent AI
In the quest for human-equivalent AI there are, broadly speaking, three approaches
open to us: design it, reverse-engineer it2 or evolve it. The third of these—artiﬁcial
evolution—is attractive because it sidesteps the troublesome problem of having to
understand how human intelligence works. It’s a black box approach: create the initial
conditions then let the blind watchmaker of artiﬁcial evolution do the heavy lifting.
This approach has some traction. For instance David Chalmers, in his philosophical
analysis of the technological singularity [4], writes “if we produce an AI by artiﬁcial
evolution, it is likely that soon after we will be able to improve the evolutionary
algorithm and extend the evolutionary process, leading to AI+”. And since we can
already produce very simple AI by artiﬁcial evolution, then all that’s needed is to
“improve the evolutionary algorithm”. If only it were that straightforward.
Some time ago I asked myself (and anyone else who would listen): ok, but even
if we had the right algorithm, what would be the energy cost of artiﬁcially evolving
human-equivalent AI? My hunch was that the energy cost would be colossal; so great
perhaps as to rule out the evolutionary approach altogether. That thinking, and some
research, resulted in a short paper in ALIFE 14 [16].
That paper explores the question: what is the energy cost of evolving complex
artiﬁcial life? The paper takes an unconventional approach by ﬁrst estimating lower
and upper bounds on the energy cost of natural evolution and, in particular, the species
2I will not discuss whole brain emulation (WBE) in any depth in this essay, but the WBE of a
very simple animal—C-elegans—will play a part. For an account of WBE see Murray Shanahan’s
excellent book The Technological Singularity [12].

400
A. F. T. Winﬁeld
Homo Sapiens Sapiens. A lower bound of ∼8000EJ3 is obtained by estimating and
summing the caloriﬁc energy costs of populations of 7 concestor species stretching
back from hominids to single-celled organisms over about 3.5B years. The upper
bound of ∼5.7 × 1012 EJ is obtained by simply estimating the amount of solar energy
available to the evolution of all living things. Even though these are very crude
estimates they have value because we are forced us to think about the energy costs
of co-evolution, and hence the energy costs of evolving complexity.
Of course the processes and mechanisms of biological and artiﬁcial evolution
are profoundly different (except for the meta-level equivalence of the Darwinian
evolutionary operators: variation, selection and heredity), but there is an ineluctable
truth: artiﬁcial evolution still has an energy cost. Virtual creatures, evolved in a virtual
world, have a real energy cost. And we can estimate that energy cost. For a simple
robot with an artiﬁcial neural network of comparable size to that of the nematode
worm C. elegans (with 302 neurons), each simulated robot has a real energy cost of
about 9J/h, which interestingly is about 2000 times greater than the energy cost of a
very small (1mg) organism, 0.004J/h. It is clear that ‘larger’ artiﬁcial creatures, i.e.
with more artiﬁcial neurons, must incur a greater computational energy cost.
In general, if the energy cost of simulating and ﬁtness testing a virtual creature is e,
then the energy cost of evolving that creature will be E = gpe, where g is the number
of generations required and p the population size. Energy cost e is clearly a function
of the complexity of that virtual creature, but how might e scale with complexity?
Kleiber’s law [5, p. 422] relates the mass of an organism to its energy consumption
and, plotted on logarithmic axes, shows a remarkably consistent linear relationship
from micro-organisms to the largest animals. Perhaps a similar relationship might
exist between, say, neural complexity and energy cost e for virtual creatures: an
artiﬁcial life equivalent of Kleiber’s law?
Figure1 imagines such a plot, of neural complexity against energy cost e. We
cannot yet plot such a relationship since we have, to date, only one or two points at
the very bottom of the artiﬁcial neural complexity scale. But, if we assume that a
human-equivalent AI will require roughly comparable neural complexity to Homo
Sapiens,4 with 85 × 109 neurons and 1014–1015 synapses (and noting that neural
complexity must take account of the number of synapses, since neural connections
incur a computational energy cost), then e for an artiﬁcial creature of this synaptic
complexity could be 1010–1012 times greater than for something equivalent to C.
elegans. But this scale factor is still likely to be too low because ﬁtness testing of
increasingly complex artiﬁcial creatures will take longer and incur greater energy
cost. It seems likely that the gradient of our ALife version of Kleiber’s law will be
greater than 1.
I estimate that the computational energy cost of simulating and ﬁtness testing
something with an artiﬁcial neural and synaptic complexity equivalent to humans
could be around 1014 KJ, or 0.1EJ. But evolution requires many generations and
many individuals per generation, and many co-evolving artiﬁcial species. Also taking
3An exajoule (EJ) is 1018 J.
4Almost certainly not a safe assumption, but it’s all we have to go on here.

On the Simulation (and Energy Costs) of Human Intelligence …
401
Fig. 1 The computational energy cost of artiﬁcial neural complexity, after Kleiber’s Law
account of the fact that many evolutionary runs will fail (to produce smart AI), the
whole process would almost certainly need to be re-run from scratch many times over.
If multiplying those population sizes, generations, species and re-runs gives us (very
optimistically) a factor of 1,000,000—then the total energy cost would be 100,000EJ.
In 2010 total human energy use was about 539EJ. So, artiﬁcially evolving human-
equivalent AI would need the whole human energy generation output for about 200
years.
3
The Technological Singularity
The singularity. Or to give it it’s proper title, the technological singularity. It’s a
Thing: an idea that has taken on a life of its own; more of a life, I suspect, than the
very thing it predicts ever will. It’s a Thing for the techno-utopians: wealthy middle-
aged men who regard the singularity as their best chance of immortality. They are
Singularitarians, some of whom appear prepared to go to extremes to stay alive for
long enough to beneﬁt from a benevolent super-AI—a manmade god that grants
transcendence.
And it’s a Thing for the doomsayers, the techno-dystopians. Apocalypsarians
who are equally convinced that a superintelligent AI will have no interest in curing
cancer or old age, or ending poverty, but will instead—malevolently or maybe just
accidentally—bring about the end of human civilisation as we know it [3]. History
and Hollywood are on their side. From the Golem to Frankenstein’s monster, Skynet
and The Matrix, we are fascinated by the old story: man plays god and then things
go horribly wrong.

402
A. F. T. Winﬁeld
The singularity is basically the idea that as soon as artiﬁcial intelligence exceeds
human intelligence then everything changes [12]. There are two central planks to the
singularity hypothesis: one is the idea that as soon as we succeed in building AI as
smart as humans then it rapidly re-invents itself to be even smarter, starting a chain
reaction of smarter-AI inventing even-smarter-AI until even the smartest humans
cannot possibly comprehend how the superintelligent AI works. The other is that the
future of humanity becomes unpredictable and in some sense out-of-control from
the moment of the singularity onwards.
So, should we be worried, or optimistic, about the technological singularity? Well
I think we should be a little worried—cautious and prepared may be a better way
of putting it—and at the same time a little optimistic (that’s the part of me that
would like to live in Iain M Banks’ The Culture). But I don’t believe we need to be
obsessively worried by a hypothesised existential risk to humanity. Why? Because,
for the risk to become real, a sequence of things all need to happen. It’s a sequence of
big ifs. If (1) we succeed in building human equivalent AI and if (2) that AI acquires
a full understanding of how it works,5 and if (3) it then succeeds in improving
itself to produce super-intelligent AI, and if (4) that super-AI, either accidentally or
maliciously, starts to consume resources, and if (5) we fail to pull the plug then, yes,
we may well have a problem. The risk, while not impossible, is improbable.
Each of these ifs needs detailed consideration. The ﬁrst I have discussed in Sect.2
of this paper. Consider now the second: for that AGI to be able to understand itself
well enough to be able to then re-invent itself—hence triggering an Intelligence
Explosion—is not a given. An AGI as smart and capable as most humans would
not be sufﬁcient—it would need to have the complete knowledge of its designer (or
more likely the entire team who designed it)—and then some more: it would need to
be capable of additional insights that somehow its team of human designers missed.
Not impossible but surely very unlikely.
Take the third if: the AGI succeeds in improving itself. There seems to me no
sound basis for arguing that it should be easy for an AGI—even one as smart as a
very smart cognitive scientist—to ﬁgure out how to improve itself. Surely it is more
logical to suppose that each incremental increase in intelligence will be harder than
the last, thus acting as a brake on the self-improving AI.
By worrying unnecessarily I think we’re falling into a trap: the fallacy of privi-
leging the hypothesis. And—perhaps worse—taking our eyes off other risks that we
should really be worrying about, like climate change, inequality or bioterrorism.
Wait a minute, I hear you say, there are lots of AI systems in the world already,
surely it’s just a matter of time? Yes we do have lots of AI systems, like chess
programs, search engines, automated ﬁnancial transaction systems, or the software in
driverless car autopilots. And some AI systems are already smarter than all humans,
like chess programs or AlphaGo [13]; some are smarter that most humans, like
language translation systems. Others are as good as some humans, like driverless
cars or natural speech recognition systems (like Siri) and sooner or later will be
better than most humans. But none of this already-as-smart-as-some-humans AI has
5Noting that we humans still have a very incomplete understanding of how we work.

On the Simulation (and Energy Costs) of Human Intelligence …
403
brought about the end of civilisation. The reason is that these are all narrow-AI
systems: very good at doing just one thing.
A human-equivalent AI would need to be a generalist, like we humans. It would
need to be able to learn, most likely by developing over the course of some years, then
generalise what it has learned—in the same way that you and I learned as toddlers that
wooden blocks could be stacked, banged together to make a noise, or as something
to stand on to reach a bookshelf. It would need to understand meaning and context,
be able to synthesise new knowledge, have intentionality and—in all likelihood—be
self-aware, so it understands what it means to have agency in the world.
There is a huge gulf between present day narrow-AI systems and the kind of
Artiﬁcial General Intelligence I have outlined. Opinions vary of course, but I think
it’s as wide a gulf as that between current space ﬂight and practical faster than light
spaceﬂight; wider perhaps, because we don’t yet have a theory of general intelligence,
whereas there are several candidate FTL drives consistent with general relativity, like
the Alcubierre drive.6
So I don’t think we need to be obsessing about the risk of superintelligent AI but
I do think we need to be cautious and prepared. I strongly believe that all science
and technology research should be undertaken within a framework of Responsible
Innovation [9], and have argued that we should be thinking about subjecting robotics
and AI research to ethical approval, in the same way that we do for human-subject
research. Frameworks for the ethical design of robotics and AI have been developed
in recent years, alongside professional codes of ethical conduct. IEEE Standards
Association discussion document Ethically Aligned Design [7], for instance, has
a section on the ‘safety and beneﬁcence of AGI and artiﬁcial superintelligence’,
outlining ethical issues and recommendations for addressing them.7
4
Simulationism
Since Elon Musk’s admission that he is a simulationist,8 the proposition that we are
all living inside a computer simulation has attracted some attention. Of course the
idea is not new. It can be traced back at least as far as Descartes’ ‘dream argument’
and in recent years has been brought to prominence by Nick Bostrom’s inﬂuential
essay ‘Are You Living In a Computer Simulation?’ [2] My view is very ﬁrmly that
the universe we are right now experiencing is real. Here are my reasons.
Firstly, Occam’s razor; the principle of explanatory parsimony. The problem with
the simulation argument is that it is a fantastically complicated explanation for the
universe we experience. It’s about as implausible as the idea that some omnipotent
being created the universe. No. The simplest and most elegant explanation is that
6https://en.wikipedia.org/wiki/Alcubierre_drive.
7https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead_
safety_beneﬁcence_v2.pdf.
8https://futurism.com/are-we-living-in-a-computer-simulation-elon-musk-thinks-so/.

404
A. F. T. Winﬁeld
the universe we see and touch, both ﬁrst hand and through our telescopes, LIGOs
and Large Hadron Colliders, is the real universe and not an artifact of some massive
computer simulation.
Second, is the problem of the reality gap [8]. Anyone who uses simulation as a
tool to develop robots is well aware that robots which appear to work perfectly well
in a simulated virtual world often don’t work very well at all when the same design
is tested in the real robot. This problem is especially acute when we are artiﬁcially
evolving those robots. The reason for these problems is that the model of the real
world and the robot(s) in it inside our simulation is an approximation. The reality
gap refers to the less-than-perfect ﬁdelity of the simulation; a better (higher ﬁdelity)
simulator would reduce the reality gap.
Anyone who has actually coded a simulator is painfully aware of the cost, not
just computational but coding costs, of improving the ﬁdelity of the simulation—
even a little bit—is very high indeed. My long experience of both coding and using
computer simulations teaches me that there is a law of diminishing returns, i.e. that
the cost of each additional 1% of simulator ﬁdelity is far more than 1%. I rather
suspect that the computational and coding cost of a simulator with 100% ﬁdelity is
inﬁnite. Rather as in HiFi audio, the amount of money you would need to spend to
perfectly reproduce the sound of a Stradivarius ends up higher than the cost of hiring
a real Strad and a world-class violinist to play it for you.
At this point the simulationists might argue that the simulation we are living in
doesn’tneedtobeperfect,justgoodenough.Goodenoughtodowhatexactly?Tofool
us that we’re living in a simulation, or good enough to run on a ﬁnite computer (i.e.
one that has ﬁnite computational power and runs at a ﬁnite speed). The problem with
this argument is that every time we look deeper into the universe we see more: more
galaxies, more sub-atomic particles, etc. In short we see more detail. The Voyager 1
spacecraft has left the Solar System without crashing, like Truman, into the edge of
the simulation. There are no glitches like deja vu in The Matrix.
My third argument is about the computational effort, and therefore energy cost
of simulation. I conjecture that to non-trivially simulate a complex system x (i.e. a
human), requires more energy e(x) than the real x consumes. An equation to express
this inequality looks like this; how much greater depends on how high the ﬁdelity of
the simulation.
e(sim(x)) > e(x)
(1)
Letmeexplain.Theaveragehumanburnsaround2000kcaladay,orabout9000KJ
of energy. How much energy would a computer simulation of a human require,
capable of doing all the same stuff (even in a virtual world) that you can in your
day? Well that’s impossible to estimate because we can’t simulate complete human
brains (let alone the rest of a human). But here’s one illustration. In 2016 Lee Sedol
was famously defeated by AlphaGo [13]. In a single 2h match Sedol burned about
170cal—the amount of energy you’d get from an egg sandwich. In the same 2h the
AlphaGo machine consumed around 50,000 times more energy.
What can we simulate? The only whole brain emulation of a complex organism
demonstrated so far is the Nematode worm C-elegans—it is the only animal for

On the Simulation (and Energy Costs) of Human Intelligence …
405
which we have the complete connectome.9 I estimate the energy cost of simulating
the nervous system of a C-elegans is (optimistically) about 9J/h, which is about 2000
times greater than the real animal (0.004J/h).
What’s more the relationship between energy cost and mass is logarithmic, fol-
lowing Kleiber’s Law, and I strongly suspect the same law applies to scaling up
computational effort as outlined in Sect.2.2 above. If the complexity of an organism
o is C, then following Kleiber’s Law the energy cost of simulating that organism, e
will be
eo ∝C X(F)
o
(2)
Furthermore, the exponent X (which in Kleiber’s law is reckoned to be between
0.66 and 0.75 for animals and 1 for plants), will itself be a function of the ﬁdelity of
the simulation, hence X(F), where F is a measure of ﬁdelity.
By using the number of synapses as a proxy for complexity and making some
guesses about the values of X and F we could probably estimate the energy cost
of simulating all humans on the planet (very much harder would be estimating the
energy cost of simulating every living thing on the planet). It would be a very big
number indeed, but that’s not really the point I’m making here.
The fundamental issue is this: if my conjecture that to simulate complex system
x requires more energy than the real x consumes is correct, then to simulate the
base level universe would require more energy than that universe contains—which is
clearly impossible. Thus—even in principle—we could not simulate the whole of our
own observable universe to a level of ﬁdelity sufﬁcient for our conscious experience.
And, for the same reason, neither could our super advanced descendents create a
simulation of a duplicate ancestor universe for us to (virtually) live in. Hence we are
not living in such a simulation.
5
Some Concluding Remarks
For many researchers the Holy Grail of robotics and AI is the creation of artiﬁcial
persons: artefacts with equivalent general competencies as humans. Such artefacts
would literally be simulations of humans. Some researchers are motivated by the
utility of AGI; others have an almost religious faith in the transhumanist promise
of the technological singularity. Others, like myself, are driven only by scientiﬁc
curiosity. Simulations of intelligence provide us with working models of (elements
of) natural intelligence. As Richard Feynman famously said ‘What I cannot create, I
do not understand’. Used in this way simulations are like microscopes for the study
of intelligence; they are scientiﬁc instruments [14].
Like all scientiﬁc instruments simulation needs to be used with great care; sim-
ulations need to be calibrated, validated and—most importantly—their limitations
understood. Without that understanding any claims to new insights into the nature
9See the Open Worm project http://openworm.org/getting_started.html.

406
A. F. T. Winﬁeld
of intelligence—or for the quality and ﬁdelity of an artiﬁcial intelligence as a model
of some aspect of natural intelligence—should be regarded with suspicion.
In this essay I have critically reﬂected on some of the predictions for human-
equivalent AI (AGI); the paths to AGI (and especially via artiﬁcial evolution); the
technologicalsingularity,andtheideathatweareourselvessimulationsinasimulated
universe (simulationism). The quest for human-equivalent AI clearly faces many
challenges. One (perhaps stating the obvious) is that it is a very hard problem.
However, I believe that the task is made even more difﬁcult for two further rea-
sons. The ﬁrst is—as hinted above—that we have failed to recognize simulations
of intelligence (which all AIs and robots are) as scientiﬁc instruments, which need
to be designed, operated and results interpreted, with no less care than we would a
particle collider or the Hubble telescope.
Thesecond,andmoregeneralobservation,isthatwelackageneral(mathematical)
theory of intelligence. This lack of theory means that a signiﬁcant proportion of AI
research is not hypothesis driven, but incrementalist and ad-hoc. Of course such an
approach can and is leading to interesting and (commercially) valuable advances in
narrow AI. But without strong theoretical foundations, the grand challenge of human-
equivalent AI seems rather like trying to build particle accelerators to understand the
nature of matter, without the Standard Model of particle physics.
Acknowledgements I am deeply grateful to Susan Stepney, not only for many conversations on
topics as broad ranging as AI, Complexity, Narrative, Science Fiction and Simulation, but also
for the opportunity to participate in several projects, including EPSRC funded Complex Systems
Modelling and Simulation (CoSMoS) [14], and the York Workshops on Narrative and Complexity
[15]; it is these conversations and projects that forced me to really think about simulation.
References
1. Bhatnagar, S., Alexandrova, A., Avin, S., Cave, S., et al.: Mapping intelligence: requirements
and possibilities. In: Müller, V.C. (ed.) Philosophy and Theory of Artiﬁcial Intelligence 2017
(PT-AI 2017), vol. 44, pp. 117–135. Springer, Cham (2018)
2. Bostrum, N.: Are you living in a computer simulation? Philos. Q. 53(211), 243–255 (2003)
3. Bostrom, N.: Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Oxford
(2014)
4. Chalmers, D.: The singularity: a philosophical analysis. J. Conscious. Stud. 17(9–10), 765
(2010)
5. Dawkins, R.: The Ancestors Tale. Weidenfeld and Nicolson, London (2004)
6. Hernández-Orallo, J.: The Measure of all Minds: Evaluating Natural and Artiﬁcial Intelligence.
Cambridge University Press, Cambridge (2017)
7. IEEE Standards Association: Ethically aligned design: a vision for prioritizing human well-
being with autonomous and intelligent systems, version 2. IEEE Standards Association (2017).
https://ethicsinaction.ieee.org/
8. Jakobi, N., Husbands, P., Harvey, I.: Noise and the reality gap: the use of simulation in evolu-
tionary robotics. Advances in Artiﬁcial Life, pp. 704–720. Springer, Berlin (1995)
9. Jirotka, M., Grimpe, B., Stahl, B., Eden, G., Hartswood, M.: Responsible research and inno-
vation in the digital age. Commun. ACM 60, 62–68 (2017)

On the Simulation (and Energy Costs) of Human Intelligence …
407
10. Müller, V.C., Bostrom, N.: Future progress in artiﬁcial intelligence: a survey of expert opinion.
In: Müller, V.C. (ed.) Fundamental Issues of Artiﬁcial Intelligence. Synthese Library (Studies
in Epistemology, Logic, Methodology, and Philosophy of Science), vol. 376. Springer, Cham
(2016)
11. Pﬁefer, R., Bongard, J.: How the Body Shapes the Way We Think: A New View of Intelligence.
MIT Press, Cambridge (2007)
12. Shanahan, M.: The Technological Singularity. MIT Press, Cambridge (2015)
13. Silver, D., et al.: Mastering the game of go with deep neural networks and tree search. Nature
529, 484–489 (2016)
14. Stepney, S., Polack, F.A.C., Alden, K., Andrews, P.S., et al.: Engineering Simulations as Sci-
entiﬁc Instruments: A Pattern Language. Springer, Berlin (2018)
15. Walsh, R., Stepney, S. (eds.): Narrating Complexity. Springer, Berlin (2018)
16. Winﬁeld, A.F.: Estimating the energy cost of (artiﬁcial) evolution. In: Sayama, H., Rieffel, J.,
Risi, S., Doursat, Ren and Lipson, H. (eds.) Artiﬁcial Life 14: 14th International Conference
on the Synthesis and Simulation of Living Systems, New York, 31 July–2 August 2014, pp.
872–875 (2014)
17. Winﬁeld,
A.F.:
How
intelligent
is
your
intelligent
robot?
(2017).
arXiv
preprint
arXiv:1712.08878

