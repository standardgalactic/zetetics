Why think step-by-step? Reasoning emerges from the
locality of experience
Ben Prystawski
Department of Psychology
Stanford University
Stanford, CA 94305
benpry@stanford.edu
Noah D. Goodman
Departments of Psychology and Computer Science
Stanford University
Stanford, CA 94305
ngoodman@stanford.edu
Abstract
Humans have a powerful and mysterious capacity to reason. By working through a
series of purely mental steps, we can make inferences we would not be capable of
making directly—despite that fact that we get no additional data from the world.
Similarly, large language models can perform better at complex tasks through chain-
of-thought reasoning, where they generate intermediate steps before answering a
question. We use language models to investigate the questions of when and why
reasoning is helpful, testing the hypothesis that reasoning is effective when training
data consisting of local clusters of variables that inﬂuence each other strongly.
These training conditions enable the chaining of accurate local inferences in order
to estimate relationships between variables that were not seen together in training.
We train an autoregressive transformer on samples from joint distributions deﬁned
by Bayes nets, but only include a subset of all the variables in each sample. We
compare language models’ ability to match conditional probabilities both with
and without intermediate reasoning steps, ﬁnding that intermediate steps help
only when the training data is locally structured with respect to dependencies
between variables. Furthermore, intermediate variables need to be relevant to
the relationship between observed information and target inferences. Our results
illustrate how the statistical structure of training data drives the effectiveness of
reasoning step by step.
1
Introduction
The human mind is a ship – the immediate inferences we make from instinct keep us aﬂoat, but reason
is the compass and lighthouse that brings us to the shore of wisdom. Indeed, many tasks that we ﬁnd
hard to do immediately – solving math problems, planning vacations, understanding our relatives –
become much easier when we talk ourselves through a reﬂective reasoning process. Likewise, by
considering thought experiments or “intuition pumps” in science we can form strong beliefs – such
as that the rate at which an object falls should not depend on its mass – purely by thinking through a
set of steps [1, 2]. It is not a priori obvious that step-by-step reasoning should be helpful. Reasoning
does not give us any new data from the world, yet it can still improve our inferences. In investigating
the origins of human reasoning, we must thus ask, why does reasoning help?
Large language models have been shown capable of performing a wide variety of tasks by immediately
answering a question. However, they struggle with some complex tasks, like math word problems
[3, 4]. A recent line of work has demonstrated that inducing language models to produce a “chain of
thought” consisting of intermediate steps toward a solution, before giving an answer, leads to better
performance than prompting them to produce an answer directly [5–7]. Other work has built on these
ﬁndings, showing that providing worked solutions in context is helpful across a wide variety of tasks,
including logical question-answering and common sense question-answering [8, 9]. These ﬁndings
arXiv:2304.03843v1  [cs.AI]  7 Apr 2023

X2
X1
X8
X7
X17
X14
X13
###
target: X16
X12=0
X4=1
X11=1
X16=0
###
target: X5
X12=0
X5=_
###
target: X1
X12=0
X11=1
X16=0
X3=0
X15=1
X10=1
X5=_
Training samples
(x1,000,000)
direct prediction
the reasoning
gap
free generation
model-generated
X9
= better
X6
X5
X4
X12
X11
X16
X3
X15
X10
0.00
0.05
0.10
0.15
fully observed
local
wrong local
training condition
Mean squared error
estimator
direct prediction
free generation
Figure 1: Overview of our training and estimation setup. Top left: visualization of a Bayes net. The
green variable is an example observed variable and the yellow variable is an example target variable.
Grey variables are along the path between the observed and target variables and are examples of
useful intermediate variables for reasoning. Dotted lines indicate examples of local observation
neighborhoods from which training samples are drawn. Top right: format of the training samples.
Bottom: Illustration of our estimation setup using direct prediction and free generation. We prompt
the model according to the estimator, either immediately computing the probability of the target
variable (direct prediction), or doing so after freely generating intermediate variables and their values
(free generation). Finally, we compute the mean squared error between estimated and true conditional
probabilities. The bar plot shows mean squared error with and without reasoning across training
conditions.
raise an important question: why is chain-of-thought reasoning useful? Exploring this question may
also provide insight into the origins of human reasoning.
In this work, we explore the hypothesis that chain-of-thought reasoning is useful in language models
due to local structure in the training data. Human experience is governed by our ﬁrst-person
perspective: we see and hear aspects of the world that are near to us in time and space. Yet our
reasoning transcends these local bounds, supporting plans and conclusions that span distance and
years. Similarly, language models are trained on large collections of documents. Documents in
natural language have a topic structure: a document is usually about a few topics that are closely
interconnected [10, 11]. When concepts co-occur frequently in experience or training data, estimating
the effect of one on the other is easy to do directly with simple statistical estimators. However, when
we need to infer the effect of one piece of information on another that we have not encountered in the
same context, we must make a series of inferences, jumping between pairs of concepts that connect
what we know to what we want to infer. We posit that chain-of-thought reasoning becomes useful
exactly when the training data is structured locally, in the sense that observations tend to occur in
overlapping neighborhoods of concepts.
Our hypothesis is similar to that advanced by Chan et al. [12] to explain why in-context learning
(generalizing from examples shown in context) occurs. They show that “burstiness,” the tendency of
the same classes to be clustered close together, in the training data is important for language models
to learn to infer tasks based on examples in the context window. Burstiness and locality are two
different statistical properties of training data that could arise from topic structures in documents for
language models or an ego-centered perspective in humans.
We investigate this hypothesis in the context of conditional inference in a Bayesian network. To
illustrate, we may know the value of some variable A and want to know about another variable C, so
2

we would try to estimate P(C|A). However, if we need to estimate probabilities from samples and we
have not seen A and C together often we would struggle to estimate P(C|A) directly from experience.
Instead, we might estimate it by reasoning through intermediate variables. If conditioning on an
intermediate variable B renders A and C independent of each other, we can compute the conditional
probability by marginalizing over B, using the fact that P(C|A) = E[P(C|B)|A]. Reasoning
through B means that we never need to directly learn the joint distribution of A and C, but we can
infer it from existing knowledge of the joint distributions between A and B and between B and C.
If, on the other hand, A and C were often observed together, it is superﬂuous to reason via B – our
direct estimate of P(C|A) will be just as good (or better). Thus, to understand when reasoning is
necessary and useful we should vary the locality structure of the observation distribution (i.e. which
variables are observed in each episode, assuming variable values are sampled from the global joint
distribution), and whether and how intermediate variables are used in estimating target conditional
probabilities. We do this, evaluating different conditions on the accuracy of their estimates of the true
conditional probability.
Our results show that performing conditional inference by ﬁrst generating intermediate variables
improves the ability of a language model to match true conditional probabilities only when the
training data has a locality structure that corresponds to the variables that inﬂuence each other
strongly. Furthermore, we show that generating variables that d-separate the observed variable from
the target variable is useful for improving conditional inference while generating irrelevant variables
is not. Allowing the model to generate intermediate variables and their values is similarly helpful to
being directed toward d-separating variables for estimating target probabilities. This suggests that
reasoning is useful for language models because: 1) Direct prediction of conditional probabilities
is inaccurate for some inferences because the relevant variables are rarely seen together in training;
2) Chain-of-thought reasoning improves estimation because it can chain together local statistical
dependencies that are frequently observed in training.
2
Methods
Our strategy for testing the main hypothesis is to train a language model (i.e. an auto-regressive
density estimator with a transformer architecture) to perform conditional inference in a Bayesian
network. We manipulate the structure of the training data and explore different ways to use the trained
model to estimate conditional probabilities. The key question will be whether there are training
conditions under which estimating conditional probabilities through intermediate steps out-performs
predicting them directly.
2.1
Training Data
The ﬁrst step in generating our training data is to create a ground truth distribution. Each distribution
consists of 100 Boolean-valued random variables and is represented as a Bayesian network. We
generate a random topology for the network, incrementally adding 100 edges between pairs of
variables. The pairs are selected randomly conditioned on (a) the two variables not already having
an edge between them and (b) the edge not creating a cycle. After generating a topology, we create
conditional probability tables for each variable.
In order to compare reasoning against direct prediction, we need pairs of variables that are not
adjacent to each other in the Bayes net yet have high mutual information. Otherwise, the conditional
probabilities would be very close to the marginal probabilities of the target variable. The language
model could then come very close to matching the true conditional probabilities without learning
anything about the relationships between the variables. We design the conditional probability tables
and select among generated Bayes nets to maximize the mutual information between distant pairs.
The probability of a variable being 1 given all the values of its parents is randomly sampled from
Beta( 1
5, 1
5). This distribution ensures that probabilities are usually close to 0 or 1, so the inﬂuence of
variables on each other tends to carry further than if probabilities were closer to 0.5. We generate
100 Bayes nets according to this process, then compute the mutual information between each pair of
variables. We select the 50 pairs of non-adjacent variables that have the highest mutual information,
then randomly choose 25 of those pairs randomly to hold out from the training set. Only a subset of
high-inﬂuence pairs are held out to ensure there are enough variables that inﬂuence each other in the
training set that the model must learn to handle dependencies between distant variables. The ﬁnal
3

step is to select the 10 Bayes nets, out of the initial 100, for which the 25 held-out pairs of variables
have the highest mean mutual information. Pseudocode for the algorithm used to generate a Bayes
net is shown in Algorithm 1 in Appendix B.
For each of the 10 selected Bayes nets, we generate a training set consisting of 1 million samples
formatted as strings. The strings consist of the name of each variable and its value in a random order.
Variable names are the letter ‘X’ followed by a number from 0 to 99. For example, if the variable
‘X42‘ has value 1, the string would include the line “X42=1”. We mention at the beginning of the
string the variable that will occur at the end of the string, marking it with the word “target.” We also
include ‘###’ before the sample to indicate where it begins. An illustrative example of a formatted
sample string is shown in Figure 1 and a longer example is shown in Appendix A.
If all samples included all of the variables in the distribution, the language model would be able to
directly learn each conditional probability; chain-of-thought reasoning would likely be unhelpful. We
therefore vary which variables the model sees together in training. From each complete sample from
the true distribution, we select a subset of sampled variables according to the observation distribution,
which is a distribution over subsets of variables in the full Bayes net. Pseudocode for getting a subset
of variables from an observation distribution is shown in Algorithm 2 in Appendix B. The observation
distributions we use have three important properties:
Locality structure
We create a local observation structure in the training data by sampling subsets
of variables from local neighborhoods. A local neighborhood consists of a central variable as
well as all variables of distance at most k from it, where distance is deﬁned as the number of
edges in the shortest undirected path between two variables in the Bayes net. The observation
distribution is generated by sampling the central variable uniformly randomly, then sampling k. In
our experiments,we draw k either from a geometric distribution with a parameter of 0.5 or a Zipﬁan
distribution with a parameter of 2, depending on the condition.
Variable dropout
Even within a local subset of the world, we may not see the entire subset at once.
Instead, certain variables may be missing or may go unnoticed. We formalize this intuition with
variable dropout. With some probability (0.2 in our experiments), variables are dropped from a local
neighborhood and do not occur in the sample. Variable dropout may also help the model generalize to
pairs of variables that were unseen in training as the model sees many different unique combinations
of variables in its training samples. Without variable dropout, the model would only see the exact
local neighborhoods in the Bayes net and may struggle to generalize beyond them.
Held-out pairs
Finally, some pairs of variables are held out across the entire training set. This
ensures that we cannot simply memorize all the conditional probabilities between pairs of variables.
If a local neighborhood with variable dropout would include a pair of variables we decided to hold
out, we randomly remove one of the two variables in the pair from the sample.
As one control condition, we consider training data made up of local neighborhoods from the wrong
Bayes net. This maintains the co-occurrence distribution structure, but the co-occurrences do not
reﬂect the structure of which variables inﬂuence each other. As a second control, we use a fully
observed condition in which all variables are included in every sample. This lets us test whether
chain-of-thought reasoning is still useful when the training data does not have any locality structure.
2.2
Estimators
After training the language model, we estimate the conditional probability of the target variable using
one of four estimators. In all cases, we compute the target probability by taking the softmax (with a
temperature of 1) of the log-probabilities the model assigns to the tokens ‘0’ and ‘1’ following the
name of the target variable. The estimators differ in what sequence the language model produces
before the name of the target variable.
Direct prediction
This estimator does not involve any reasoning. We simply give the model the
observed variable’s name and value, followed by the target variable’s name. For example, if we want
to infer p(X4|X1 = 1) we use the prompt “target: X4\nX1=1\nX4=” then get the probability of 1.
4

fully observed
local (geom)
local (zipf)
wrong local (geom)
wrong local (zipf)
ﬁxed
.002 [.001, .003]
.036 [.029, .044]
.042 [.035, .049]
.104 [.097, .111]
.106 [.100, .112]
scaffolded
.001 [.001, .002]
.012 [.009, .015]
.012 [.010, .015]
.097 [.090, .104]
.105 [.098, .113]
free
.010 [.009, .012]
.011 [.010, .013]
.014 [.012, .016]
.097 [.090, .104]
.105 [.098, .113]
negative scaffolded
.002 [.001, .004]
.035 [.028, .043]
.042 [.035, .049]
.104 [.096, .112]
.105 [.099, .112]
Table 1: Mean squared errors with 95% conﬁdence intervals between estimated and true conditional
probabilities for held-out pairs with high mutual information. Both free and scaffolded generation
signiﬁcantly out-perform direct prediction when the training data is locally structured.
fully observed
local (geom)
local (zipf)
wrong local (geom)
wrong local (zipf)
true
.002 [.001, .004]
.007 [.005, .008]
.006 [.005, .008]
.085 [.077, .092]
.096 [.088, .103]
marginal
.094 [.086, .101]
.102 [.095, .110]
.103 [.096, .111]
.009 [.007, .011]
.001 [.0004, .001]
Table 2: Mean squared errors with 95% conﬁdence intervals between direct prediction probabilities
and either true or marginal probabilities for non-held-out pairs with high mutual information. In
the local and fully observed training conditions, direct prediction is close to perfect at matching the
conditional probabilities.
Scaffolded generation
The scaffolded generation estimators represents ideal reasoning if we knew
the best set of steps to consider. To compute the ideal set of variables to reason through, we ﬁnd
the minimum set of variables that d-separates the observed variable from the target variable. We
sort these variables by their distance (in the Bayes net) from the observed variable, then estimate the
target probability with a Monte Carlo estimate of the intermediate variables from the language model.
For each of 10 samples, we incrementally add each scaffold variable’s name to the prompt, then let
the model generate its value. Finally, we average the target probabilities from these samples.
Free generation
The free generation estimator is similar to scaffolded generation in that we
generate intermediate variables before the target, but we use the language model to choose the
intermediate variables rather than choosing the d-separating set beforehand. We simply sample
variable names and values from the model with temperature 1 until it generates the name of the
target variable. At that point, we compute the probability of the target variable. We average over
10 such samples. This estimator tests whether language models can spontaneously generate useful
intermediate variables.
Negative-scaffolded generation
We use negative-scaffolded generation as a control condition
to rule out the possibility that generating any intermediate variables helps to predict conditional
probabilities. For each pair of variables, we create a negative scaffold, which is a randomly-selected
set of variables equal in size to the scaffold (as described above), but that does not include the
d-separating scaffold variables. The model generates values for the intermediate variables in the
negative scaffold identically to the scaffolded generation estimator.
2.3
Model Architecture
We use a smaller version of the GPT-2 autoregressive transformer architecture [13]. Our model has
512-dimensional embeddings, 10 layers, and 8 attention heads. We trained this architecture with
randomly-initialized weights for 5 epochs on a training dataset consisting of 1, 000, 000 samples
from the distribution, using the AdamW optimizer [14]. Each language model was trained only on
samples from a single Bayes net. The models achieved near-perfect performance on the training task.
3
Results
We measure a language model’s ability to accurately match the conditional distribution of the target
variable by computing the mean squared error (MSE) between the estimated probability from the
language model and the true probability. Each held-out pair gives four data points: if we hold out the
pair X1, X2 we can compute p(X1|X2 = 0), p(X1|X2 = 1), p(X2|X1 = 0), and p(X2|X1 = 1).
Pooling pairs of estimated and true probabilities across the 10 distributions we trained language
5

local
wrong local
fully observed
0
25
50
75
100
0
25
50
75
100
0
25
50
75
100
0.0
0.2
0.4
0.6
0.8
Number of intermediate variables
Distance from target probability
Distance from true probability by number of intermediate variables generated
Figure 2: Number of intermediate variables generated in free generation vs. absolute distance between
true and estimated conditional probabilities for each training condition. Lower dots indicate more
accurate estimation, while dots further to the right indicate more intermediate variables generated.
models on gives us 1, 000 data points for each estimator. The MSEs for direct prediction and free
generation are shown in Figure 1. Table 1 shows the precise MSEs for all pairs of training condition
and estimator.
3.1
When reasoning helps
The ﬁrst clear result is that using the right locality structure in training is important for reasoning
to be possible. When the training data has the right locality structure, both scaffolded and free
generation perform signiﬁcantly better than direct prediction or negative scaffolded generation. These
differences indicate that generating relevant intermediate variables helps in predicting the target
variable, but generating irrelevant intermediate variables does not. Furthermore, a model trained
on local subsets of a Bayes net can generate helpful intermediate variables even when they are not
explicitly given, as evidenced by the success of free generation. MSE is much higher (performance
worse) for all estimators when the training data uses local neighborhoods based on the wrong Bayes
net. Additionally, there is little difference between the estimators that do and do not use reasoning
when the wrong locality structure is used. This demonstrates that the observation distribution favoring
local subsets of the true Bayes net is important for the success of reasoning.
In free generation, we generate variables until hitting the target variable. This results in reasoning
traces of varying length, including many very long traces. One might expect longer reasoning traces
to distract the model, but this did not occur in our experiments: the length of the reasoning trace does
not relate strongly to the accuracy, as is shown in Figure 2. However, free generation performs worse
than direct prediction when the training data is fully observed and reasoning traces are also all very
long in that condition. Generating too many irrelevant intermediate variables might harm estimation
by adding noise when direct prediction would have done well, but that additional noise is dwarfed
by the beneﬁt of reasoning for locally-structured training data with unobserved pairs of variables.
Training on data with the right locality structure reduces the average length of reasoning traces. This
may be because the selected pairs of variables tend to be close to each other in the Bayes net (though
not adjacent), so training on variables that are close together in the true Bayes net makes the language
model more likely to generate the target variable early.
In training conditions with the right locality structure, free generation usually produces a set of
variables that d-separate the observed variable from the target variable – 74% of the time in the
geometric local joint condition and 76% of the time in the Zipﬁan local joint condition. In contrast,
the training conditions with the wrong locality structure only lead to d-separating scaffolds in 35%
(geometric) and 40% (Zipﬁan) of reasoning traces. These results suggest that training on local
clusters of variables is valuable in that it helps free generation produce scaffolds that are relevant
to the relationship between the observed and target variables. d-separation is important because it
ensures that all the inﬂuence that the observed variable has on the target variable is captured by the
intermediate variables. Still, our results suggest that chain-of-thought reasoning can be helpful even if
the reasoning trace does not completely d-separate the two variables. In the local training conditions,
6

0.00
0.05
0.10
0.15
fully observed
local (geom)
local (zipf)
wrong local (geom) wrong local (zipf)
train condition
Mean squared error
estimator
direct prediction
free generation
negative scaffolded generation
scaffolded generation
Mean squared error with marginal probabilities by train and generation condition
Figure 3: Mean squared errors between estimated conditional probabilities and marginal probabilities
of target variables. Error bars show 95% conﬁdence intervals. Lower bars indicate a closer match to
the marginal probabilities.
the model does not generate a d-separating reasoning trace 25% of the time, but there is virtually no
difference in aggregate performance between free and scaffolded generation.
3.2
When reasoning is unnecessary
Table 2 shows correlations between direct prediction probabilities and true probabilities for the 25
pairs from the 50 selected high-mutual-information variable pairs that were not held out from the
training set. In the conditions with the right locality structure, direct prediction matches the true
distributions almost perfectly. This indicates that our language models learn to match conditional
distributions well without reasoning when they have seen pairs of variables together in training.
Furthermore, in the full observation condition the language model is able to match the true distribution
of the target variables almost perfectly. It performs very well in all conditions and actually does worse
using the free generation estimator. The worse performance resulting from free generation could be
because the Monte Carlo estimate of intermediate variables introduces noise into the estimator that is
not present in direct prediction. In this case, reasoning only creates an opportunity for self-distraction
and confusion. Taken together, these results illustrates that seeing variables together frequently in
training obviates the need for step-by-step reasoning.
3.3
When reasoning fails
To understand the behavior of language models in the training conditions where they fail to match
the true conditional probabilities, we compare them against the marginal probabilities of the target
variables. The mean squared errors between the estimated conditional probabilities and target variable
marginal probabilities are shown in Figure 3. We can see the opposite of the trend for the true
probabilities: the worse a training condition does at matching the true conditional probability, the
better it matches the marginal. The language models trained on data with the wrong locality structure
generated estimates that were particularly close to the marginal probabilities.
When the variables that co-occur with each other frequently are not local in the Bayes net, they often
have very little inﬂuence on each other. This means that the joint distribution over co-occurring
variables is usually very close to the product of the marginal probabilities, i.e. P(X1, X2, X3) ≈
P(X1)P(X2)P(X3) for non-local X1, X2, X3. The language model then learns to match the
marginal distribution of each variable instead of the joint distribution. In this situation there are no
reliable ‘steps’ for step-by-step reasoning to use.
7

4
Discussion
Reasoning about intermediate variables, both with scaffolded and free generation, out-performs
direct prediction in matching conditional probabilities. However, this is only the case when the
training data is based on local observations with respect to the underlying distribution for the pairs of
variables that were held out in training. When the training data included all the variables, reasoning
was not necessary because direct prediction was already at ceiling performance. When the training
data had the wrong locality structure, reasoning was not useful and the language models learned
to match the marginal probability of each variable rather than the joint distribution. Our results
demonstrate a minimal case in which chain-of-thought reasoning is helpful and suggest weak, yet
important, conditions under which it is likely to be helpful in more naturalistic settings: we can expect
chain-of-thought reasoning to aid when a model is tasked with making inferences that span different
topics or concepts that do not co-occur often in its training data, but can be connected through topics
or concepts that do.
Our results also suggest one reason why human learning is more data-efﬁcient than learning in
language models: the information humans encounter may be more strongly and naturally structured
in a way that enables us to reason across contexts. Since humans experience the world from a
ﬁrst-person perspective, the information we encounter is structured in small subsets of our world
consisting of items and objects that are tightly coupled with each other. This idea could be relevant
to data curation for the training of language models. Constructing datasets with tightly-correlated
observation neighborhoods that collectively cover the full space of relevant concepts may amplify
language models’ ability to perform chain-of-thought reasoning in natural language.
Of the many forms of chain-of-thought prompting that exist in the literature, our ﬁndings are most
relevant to zero-shot prompting [e.g. 5]. Our ﬁndings pertain to cases where a model instantiates
intermediate variables on the way to an answer without any other examples of reasoning traces in its
context window. This contrasts with approaches that involve giving the model examples of reasoning
[e.g. 7]. We leave the question of how in-context examples of reasoning inﬂuence chain-of-thought
reasoning for future work. Our results are also in the context of simple propositional worlds speciﬁed
by Bayes nets. Future work should explore more richly structured worlds, especially those that have
higher-level structure, such as would come from hierarchical generative models and probabilistic
programs. Reasoning in such worlds may require more expressive languages with which to reason
and specify hypothetical scenarios.
An interesting aspect of our ﬁndings is that even direct prediction performs better for held-out pairs
when the language model is trained with the right locality structure. This could be due to shared
contexts when each of the two variables is seen. If the transformer has seen that A = 1, B = 1
and B = 1, C = 0 are both relatively common, then it may learn to associate A = 1 with C = 0
in its weights even if it never observes that pair of variables directly. This explanation closely
matches Shepard’s [1] account of the value of thought experiments: they surface deeply-internalized
understandings of symmetries and principles that we learn implicitly. Our results show that locally-
clustered training data is important to developing those implicit understandings.
These results also suggest a hypothesis as to how human reasoning originated. Like language models,
the human mind has the capacity to perform sequential density estimation: we can predict possible
futures given the past [15, 16]. We also tend to see information that is spatially and temporally close
and within our ﬁeld of vision. This may have led us to be good at making predictions involving
items that we encounter together frequently, but bad at making predictions involving items more
distant in our experience. Once we had sequential density estimation with locally-structured data,
reasoning became valuable in principle. It may have emerged in practice from a small evolutionary
change allowing our minds to run forward freely into potential futures, bolstering our capacity to
make inferences.
Future work should explore the structure of the observation distribution for human learners to
understand whether and how the information we observe facilitates reasoning and forward imagination.
Using modern language models as toy models in which we can study reasoning is a promising
direction for computational cognitive science to study long-standing problems such as reasoning,
problem solving, and thought experiments. Finally, we plan to explore questions of how people learn
to reason more effectively, both on the timescale of an individual human lifetime through education
and experience and over many generations through cultural learning.
8

References
[1] R. N. Shepard, “The step to rationality: The efﬁcacy of thought experiments in science, ethics,
and free will,” Cognitive Science, vol. 32, no. 1, pp. 3–35, 2008.
[2] D. C. Dennett, Intuition pumps and other tools for thinking. WW Norton & Company, 2013.
[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al., “Language models are few-shot learners,” Advances in neural
information processing systems, vol. 33, pp. 1877–1901, 2020.
[4] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J. Hilton, R. Nakano, et al., “Training veriﬁers to solve math word problems,” arXiv preprint
arXiv:2110.14168, 2021.
[5] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are zero-shot
reasoners,” arXiv preprint arXiv:2205.11916, 2022.
[6] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan,
A. Lewkowycz, M. Bosma, D. Luan, et al., “Show your work: Scratchpads for intermediate
computation with language models,” arXiv preprint arXiv:2112.00114, 2021.
[7] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou, “Chain of thought
prompting elicits reasoning in large language models,” arXiv preprint arXiv:2201.11903, 2022.
[8] A. K. Lampinen, I. Dasgupta, S. C. Chan, K. Matthewson, M. H. Tessler, A. Creswell, J. L.
McClelland, J. X. Wang, and F. Hill, “Can language models learn from explanations in context?,”
arXiv preprint arXiv:2204.02329, 2022.
[9] E. Zelikman, Y. Wu, J. Mu, and N. Goodman, “Star: Bootstrapping reasoning with reasoning,”
in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal,
D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 15476–15488, Curran Associates, Inc., 2022.
[10] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal of machine
Learning research, vol. 3, no. Jan, pp. 993–1022, 2003.
[11] D. M. Blei and J. D. Lafferty, “A correlated topic model of Science,” The Annals of Applied
Statistics, vol. 1, no. 1, pp. 17 – 35, 2007.
[12] S. C. Chan, A. Santoro, A. K. Lampinen, J. X. Wang, A. K. Singh, P. H. Richemond, J. Mc-
Clelland, and F. Hill, “Data distributional properties drive emergent in-context learning in
transformers,” in Advances in Neural Information Processing Systems, 2022.
[13] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., “Language models are
unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[14] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in International Confer-
ence on Learning Representations, 2019.
[15] T. L. Grifﬁths and J. B. Tenenbaum, “Optimal predictions in everyday cognition,” Psychological
science, vol. 17, no. 9, pp. 767–773, 2006.
[16] A. Bubic, D. Y. Von Cramon, and R. I. Schubotz, “Prediction, cognition and the brain,” Frontiers
in human neuroscience, p. 25, 2010.
9

A
Example of a complete formatted sample
The following is an example of a sample from a local neighborhood of a Bayes net in string format,
as used in training our models:
###
target: X5
X17=0
X92=0
X13=0
X52=1
X24=1
X26=1
X91=0
X36=0
X34=0
X12=1
X20=0
X5=1
B
Pseudocode for data generation
This section contains pseudocode for the algorithms we use to generate Bayes nets and choose a
subset of variables to include according to an observation distribution.
First, Algorithm 1 is used to create Bayes nets. This algorithm takes a number of nodes and edges and
creates a directed acyclic graph by randomly adding edges between pairs of nodes. Next, it assigns
conditional probability tables to the nodes given the values of their parents to create a Bayes net.
Algorithm 1 Algorithm for generating a Bayes net
Input: Number of nodes N, number of edges M
G = (E, V ) ←empty graph
for i ∈{1, . . . , N} do
V ←V ∪{Xi}
end for
for i ∈{1, . . . , M} do
v1, v2 ←random pair of vertices ∈V
while (v1, v2) ∈E or (v1, v2) would create a cycle do
v1, v2 ←another random pair of vertices ∈V
end while
E ←E ∪(v1, v2)
end for
T ←{}
for v ∈TOPOLOGICALSORT(G) do
t ←empty conditional probability table
for c ∈possible conﬁgurations of v’s parents do
p ←BETA( 1
5, 1
5)
t[c] ←p
end for
T ←T ∪{(v, t)}
end for
D ←Bayes net deﬁned by graph G and conditional probability tables T
return D
For each sample, we only show a subset of all the variables according to an observation distribution.
Algorithm 2 shows the procedure we use to select which variables to display in a given sample. We
ﬁrst sample central variable c and a distance k, then get all the variables within distance k of c in
the graph. Next, we drop each variable with probability 0.2. If any of the held-out pairs remain, we
10

randomly remove one of them. In the wrong locality structure training conditions, we use a graph
G that does not correspond to the net from which our samples are drawn, but has the same variable
names.
Algorithm 2 Algorithm for masking out variables from observation distribution
Input: Bayes net graph G = (V, E), set of held-out pairs P, size distribution D
c ←random variable ∈V
k ←SAMPLE(D)
R ←{}
for v ∈V do
if v is within distance k of c in G then
R ←R ∪{v}
end if
end for
for v ∈R do
if SAMPLE(Bernoulli, 0.2) = 1 then
R ←R \ {v}
end if
end for
for (v1, v2) ∈P do
if v1 ∈R and v2 ∈R then
if SAMPLE(Bernoulli, 0.5) = 1 then
R ←R \ {v1}
else
R ←R \ {v2}
end if
end if
end for
return R
11

