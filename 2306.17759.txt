The Shaped Transformer:
Attention Models in the Infinite Depth-and-Width Limit
Lorenzo Noci∗†
Chuning Li∗‡
Mufan Li∗‡
Bobby He§
Thomas Hofmann†
Chris Maddison‡
Daniel M. Roy‡
Abstract
In deep learning theory, the covariance matrix of the representations serves as a proxy
to examine the network’s trainability. Motivated by the success of Transformers, we study
the covariance matrix of a modified Softmax-based attention model with skip connections in
the proportional limit of infinite-depth-and-width. We show that at initialization the limiting
distribution can be described by a stochastic differential equation (SDE) indexed by the depth-
to-width ratio. To achieve a well-defined stochastic limit, the Transformer’s attention mechanism
is modified by centering the Softmax output at identity, and scaling the Softmax logits by a
width-dependent temperature parameter. We examine the stability of the network through
the corresponding SDE, showing how the scale of both the drift and diffusion can be elegantly
controlled with the aid of residual connections. The existence of a stable SDE implies that
the covariance structure is well-behaved, even for very large depth and width, thus preventing
the notorious issues of rank degeneracy in deep attention models. Finally, we show, through
simulations, that the SDE provides a surprisingly good description of the corresponding finite-size
model. We coin the name shaped Transformer for these architectural modifications.
1
Introduction
Pre-trained large language models have experienced a remarkable increase in popularity due to their
eerily human-like ability to puzzle through complex reasoning tasks, solve coding challenges, and
produce pages of logically sound text [1]. Arguably, the Transformer is the foundation of these
successes [2]. Recent research has found evidence for scaling laws, linking the performance of these
architectures to their parameter counts and the quantity of training data, fueling the desire to train
deeper and wider models on ever larger datasets in order to unlock new levels of performance [3–7].
Bundled with the increased expressivity of deep architectures, however, is increased numerical
instability, both in the forward pass and gradients, which hinders training. One of the clearest
examples of instability is the so-called rank collapse phenomenon [8, 9] – the observation that, in
Softmax-based attention models, the network’s representation of different tokens tend to perfectly
align at large depth. The resulting poorly conditioned covariance and correlation between tokens
leads to exploding and/or vanishing gradients at initialization, disrupting gradient updates of the
affected parameters. This situation violates a well-known guiding principle from the literature of
deep signal propagation: a stable covariance is a necessary condition for stable training [10–15]. In
∗Equal contribution. Correspondence to:
lorenzo.noci@inf.ethz.ch, chuning.li@mail.utoronto.ca, mufan.li@mail.utoronto.ca
†ETH Zurich
‡University of Toronto and Vector Institute
§University of Oxford
1
arXiv:2306.17759v1  [stat.ML]  30 Jun 2023

fact, the instability of Transformers is evident when considering the critical role of hyperparameter
tuning and the judicious use of normalization layers. In this work, we study Transformers in a
novel infinite limit, rectify sources of instability with a novel modification, and derive the SDEs
characterizing the covariance and output distribution.
Scaling limits have been used successfully to provide guidance on architecture [16–18] and tuning
hyperparameters settings [19]. Our work represents a contribution in this direction. The ability to
use such limits to diagnose instabilities depends on their tractability and faithfulness to real-world
(finite) networks. In this regard, not all limits are created equal. In particular, the faithfulness of
scaling limits depends critically on how other parameters are scaled with width. One of the simplest
(and thus most popular) limits to work with – the “NTK” limit [20–24] – treats the depth of the
network as fixed. As a result, at initialization, this limit does not accumulate sufficient random
fluctuations over the depth of the network, leading to deterministic covariance matrices that do
not agree with those of standard (finite) networks. Such networks have another defect: they are
incapable of learning features in the limit [25]. Various other limits have been studied, towards
identifying tractable yet faithful models of initialization and/or training. These include mean field
limits [26–29] and the perturbative regime [30–34].
This work operates in a relatively new regime – the proportional infinite depth-and-width limit –
where depth d and width n diverge as the ratio d/n tends to a positive constant. This limit, first
studied by Hanin and Nica [35], has been the recent subject of intense study [35–39, 18]. This regime
retains the network’s stochasticity and, at initialization, has been shown to closely resemble the
behaviour of finite architectures, yet still yield a relatively simple limiting description, expressible
in terms of stochastic differential equations [38, 18]. In this work, we fully characterize the initial
output distributions of a network with skip connections and Softmax-based attention mechanisms,
in the proportional infinite-depth-and-width limit.
Inspired by the idea of shaping activation functions [16–18, 40], our theoretical approach finds
an adequately modified attention mechanism via its SDE limit. Our modification involves making
the attention matrix closer to the identity, and appropriately choosing the temperature parameter τ,
which re-scales the logits of the Softmax. Similar to shaping activation functions, the temperature
scaling we devise linearizes and reduces the saturation of the Softmax, a known source of training
instability in Transformers [41]. In order to model the feedforward layer of a Transformer’s block, we
extend existing results [18] to derive an SDE for the proportional limit of shaped-ReLU feedforward
multi-layer perceptrons (MLPs) with skip connections. Combined, we fully characterize the output
distribution of a Transformer with shaped non-linearities (Corollary 4.3).
Notably, our modification successfully prevents a poorly conditioned covariance matrix, whereas
the vanilla Softmax-based attention model without LayerNorm [43] fails in this regard, and the
corresponding Pre-LN architecture provides only marginal improvements (see Figure 1). Given
that our modification is inspired by previous work on shaping activation functions, we coin the
terms shaped attention for the proposed attention mechanism and shaped Transformer for the
overall architecture that includes the MLP block and residual connections. Through simulations
(e.g., Figure 1), we show that the limiting neural covariance SDE approximates the distribution
of finite-size Transformers with shaped attention mechanism surprisingly well. We also provide
preliminary training experiments for our proposed shaped attention architecture on standard language
modeling tasks, demonstrating the feasibility of the new architecture in practice (see Section 5 and
Appendix D).
In summary, our contributions are as follows:
1. We study the effect of skip connections in the proportional limit, showing that under a precise
relation between the scaling parameters of the shortcut and residual branches, the feature
2

0
50
100
150
Layer 
0.2
0.4
0.6
0.8
1
Mean Correlation 
Shaped (Ours)
Unshaped
Unshaped+Pre-LN
1
0.5
0
0.5
1
Correlation 
d
0
0.25
0.50
0.75
1
Density
SDE (Ours)
Shaped NN
Figure 1: Our shaped Transformer prevents token representations from becoming perfectly aligned, i.e.
rank collapse. Left: mean correlation ραβ
ℓ
of Transformers (Eq. 11) with and without shaped attention
(Eq. 9) and Pre-LN [42]. Right: kernel density estimate and histogram of correlations from covariance
SDE in Theorem 4.2 and shaped attention NN. Here we note correlation converging to 1 implies a
poorly conditioned covariance matrix. Simulated with n = 200, d = 150, γ = 1/
√
8, τ0 = 1, ραβ
0
= 0.2,
SDE step size 0.01, and 212 samples.
covariance converges to the solution of a weighted version of the neural covariance SDE for
MLPs (Theorem 3.2). The dependence on the depth-to-width ratio implies the existence of
a stable non-commutative limit for residual networks, complementing the commutative limit
studied in Hayou and Yang [44].
2. We propose shaped attention, where we modify the Softmax-based attention mechanism to be a
perturbation of the identity. We demonstrate that shaped attention successfully prevents the
degeneracy of correlation in contrast to existing Transformer architectures (Figure 1).
3. For the proposed shaped attention architecture, we derive the neural covariance SDE character-
izing the initial distribution in the proportional limit (Theorem 4.2). Consequently, we provide
the first characterization of Transformer-type architectures, i.e. the shaped Transformer, in the
large depth-and-width regime (Corollary 4.3).
4. We provide simulations to validate the theory and to interpret the effects of network hyperpara-
maters on the covariance matrix of the shaped Transformer. Specifically, we study finite time
stability of the SDE and provide explicit guidance on hyperparameters to prevent numerical
instability.
The paper is organized as follows: In Section 2, we provide the basic setup and some background
on existing results. In Section 3, we generalize the SDE results of Li et al. [18] to include skip
connections. This serves as a model to understand the effect of skip connections in isolation from the
attention model. In Section 4, we present our main result, first pinpointing the origins of instability
in the Softmax, then showing how the modifications underlying shaped attention allow us to derive
a non-trivial SDE limit. Finally, in Section 5, we discuss the implications of our results and some
future directions. Proofs of all theorems and additional experiments are deferred to the Appendix.
3

2
Background
Setup.
Let Xℓ∈Rm×n be the data matrix representing a sequence of m tokens embedded in n
dimensions at layer ℓ∈[d], where d is the depth of the network. We elide the explicit dependence
on ℓwhen it is clear from the context, and use superscript Greek letters to indicate specific tokens’
representations, for instance xα
ℓ∈Rn is the α-th row of Xℓ. We consider the following attention
model with residual connections:
Xℓ+1 = λXℓ+ γAℓXℓ
1
√nW V
ℓ
(1)
where γ, λ ∈[0, 1] are parameters that control the strength of the shortcut and residual branch,
respectively, W V
ℓ∈Rn×n is the weight matrix of the values, and Aℓ∈Rm×m is the attention matrix.
We consider Softmax-based scaled dot-product attention, where Aℓhas the form:
Aℓ= Softmax
1
τ Xℓ
1
√nW Q
ℓ
1
√nW K,⊤
ℓ
X⊤
ℓ

,
(2)
where the Softmax is applied row-wise, W Q
ℓ, W K
ℓ
∈Rn×nk are additional random weights, and τ is a
temperature parameter, which controls the entropy of the distribution. Here we let all the weight
matrices W Q
ℓ, W K
ℓ, W V
ℓ
have N(0, 1)-iid entries. In the case where λ, γ = 1, with the application
of LayerNorm on the residual branch [42], and with τ = √nk, we recover the attention block of
the vanilla "Pre-LN" Transformer architecture [2]. Here we note that we pull the conventional
n−1/2 factor outside of the weight matrices, which preserves the forward pass, and yields equivalent
training dynamics up to a reparameterization of the learning rate [25]. In this work, we consider
unnormalized architectures, and control the variance propagation with the condition λ2 + γ2 = 1
[38]. We are interested in studying the so-called neural covariance for the attention model (Eq. 1) in
the proportional limit.
Neural Covariance.
In deep learning theory, researchers have long sought to understand how
networks internally represent different inputs and how different architectural choices affect these
representations. The approach followed by work on signal propagation has been to study how
the relative alignment of different inputs evolves across the network, as measured by the neural
covariance V αβ
ℓ
:= 1
n⟨xα
ℓ, xβ
ℓ⟩(or ραβ := (V αα
ℓ
V ββ
ℓ
)−1/2V αβ
ℓ
if interested only in the correlation). At
initialization, characterizations of this covariance structure have been exploited to infer important
properties of neural networks [10, 11]. As an example, in the sequential infinite-width-then-depth
limit, the correlation ραβ
d
of MLPs is known to converge to a fixed point independent of the input
[11, 16]. In this regime, the model is not able to discriminate different data points, which severely
hinders training, as the gradient step for the deep layers is taken in the same direction regardless of
the input. In the context of Softmax-based attention models, Dong et al. [8] proved that the feature
matrix Xℓloses rank doubly exponentially fast with depth, and Noci et al. [9] showed how this leads
to vanishing gradients of the queries and keys parameters, thus further highlighting how the stability
of forward and backward passes are deeply entangled.
Stabilizing the Effect of Non-Linear Layers.
Central to the issue of degeneracy of the neural
covariance are commonly used non-linear activation functions that severely deviate from the identity.
The recent line of work of Deep Kernel Shaping (DKS) [16–18] addresses the issue by considering the
cumulative amount of non-linearity throughout layers, and shaping the activation function by making
it closer to the identity map. Inspired by this line of work, B. He et al. [40] devise an initialization for
Transformers that avoid the rank collapse problem without the aid of skip connections or LayerNorm.
4

In an alternative approach, the line of work behind Stable ResNets [44–47] considers scaling the
residual branches by γ = 1/√depth, and postulates this scaling is sufficient to stabilize the neural
covariance with minimal assumptions on the activation function. Noci et al. [9] adopts this scaling
to give precise formulas on the expected covariance of a Transformer at initialization. In this work,
we consider γ constant in width and depth, and derive a complementary limiting result.
The Proportional Infinite-Depth-and-Width Limit.
In the context of feed-forward MLPs, the
output distribution with respect to a single input was studied in [36, 39], where it was shown that for
the ReLU nonlinearity, the norm of the activations V αα converges to a log-normal random variable.
To resolve the degeneracy of covariance and provide a characterization of output distributions for
multiple inputs, Li et al. [18] shapes the ReLU by setting its slope 1/
√
width-away from linearity.
In the proportional limit, the effect of the non-linearity accumulates over the d layers, and the
covariance matrix Vℓ= [V αβ
ℓ
]αβ converges weakly to the solution of the SDE
dVt = bReLU(Vt) dt + Σ1/2
lin (Vt) dBt ,
(3)
where the formulae for coefficients bReLU, Σlin can be found in Theorem 3.2.
We note that the output neuron distributions are directly recovered as a conditional Gaussian with
covariance VT for T = d
n, in a similar spirit as the neural network Gaussian process (NNGP) results
[20–22]. For example, the i-th output Xout,i conditioned on Vd are asymptotically iid. N(0, VT ) as
d, n →∞. The reader is referred to Appendix A for more technical background on the covariance
SDE and the convergence result.
While the existing results are limited to initialization, we remind the reader that this is a necessary
step before we can study training dynamics. In particular, the NNGP techniques developed for
infinite-width networks at initialization were directly used to study the training dynamics in the
same limit [24, 48]. We will provide further discussions on this topic in Section 5.
3
Warm-Up: a Neural Covariance SDE for ResNets
To understand the effect of skip connections, it is helpful to look at a simplified model composed of
a shaped ReLU-activated layer and skip connections:
Xℓ+1 = λXℓ+ γσs

Xℓ
1
√nW pre
ℓ
 r c
nW post
ℓ
,
(4)
where σs(x) := s+ max(x, 0)+s−min(x, 0) is the shaped ReLU with slopes s± := 1+c±n−1/2 for some
constants c+, c−∈R . We assume i.i.d weights (W pre
ℓ
)ij, (W post
ℓ
)ij
iid
∼N(0, 1), and c−1 = E σs(g)2
for g ∼N(0, 1) is a constant that ensures that the activations are normalized [49]. Notice that this
is the form of the feedforward layer in a Transformer [2].
We will next define the notion of convergence for our covariance matrices and state our first
main result. We refer the reader to Appendix A for more precise details on the Skorohod topology.
Definition 3.1 (Convergence of Covariance).
Let Xℓ∈Rm×n be the ℓ-th layer matrix of represen-
tations, and define the feature covariance as Vℓ= 1
nXℓX⊤
ℓ. Let V (n)
t
= V⌊tn⌋∈Rm(m+1)/2 be the the
continuous time interpolation of the upper triangular entries as a vector. We say the covariance V (n)
converges to V , if in the limit as n, d →∞, d
n →T, the process {V (n)
t
}t∈[0,T] converges to {Vt}t∈[0,T]
weakly in the Skorohod topology.
5

Figure 2: Left: Kernel density estimates of correlation ραβ
d
for various values of the residual strength
parameter γ. In particular, γ = 1 recovers a shaped-ReLU MLP without skip connections, and
γ = 1/
√
d is the setting studied in Noci et al. [9] and Hayou and Yang [44]. Solid lines represent
finite networks, while our SDE simulations are presented in dashed lines. Right: 95th percentile of
the absolute value of the correlation distribution as a function of γ. Note reducing γ reduces the
concentration around ραβ = 1, and our SDE reliably approximates finite size networks. Simulated
with n = 300, d = 100, ραβ
0
= 0.2, c+ = 0, c−= −1, and 213 samples.
Theorem 3.2.
Let Xℓbe the hidden layers of a ResNet defined in Eq. 4 with λ2 + γ2 = 1, where
both λ and γ do not depend on d, n. Then the feature covariance Vℓconverges to the solution of the
following SDE (in the sense of Definition 3.1)
dVt = bres(Vt) dt + Σres(Vt)1/2 dBt ,
V0 = 1
nX0X⊤
0 ,
(5)
where bres(V ) = γ2bReLU(V ) = γ2[ν(ραβ)
√
V ααV ββ]α≤β with ραβ = V αβ(V ααV ββ)−1/2 and
ν(ρ) = (c+ −c−)2
2π
p
1 −ρ2 −ρ arccos ρ

,
(6)
furthermore, Σres(V ) = 2γ2Σlin(V ) = 2γ2[V αδV βω + V αωV βδ]α≤β,δ≤ω.
Notice how the limiting SDE closely resembles the MLP case (Eq. 3), which is recovered exactly
when γ = 1. The only difference is the extra 2 factor, which comes from the fact that in our definition
each layer has effectively two times the number of weight matrices than the standard formulation for
MLPs. As the drift depends solely on the nonlinearity, and the diffusion depends soley on the random
weights, only the diffusion variance is doubled. The residual branch parameter γ < 1 dampens both
the drift and the variance of the Brownian motion by γ2, thus it can be interpreted as a time change.
In other words, the effect of γ at initialization is equivalent to reducing depth-to-width ratio, inline
with existing intuitions that ResNets have a lower “effective-depth” [50]. To visualize the stabilizing
effect of γ on the distribution, in Figure 2 (right) we plot the 95th percentile correlation as a function
of γ. The increasing trend indicates a larger probability of perfect alignment between two tokens. In
Figure 2 (left) we plot the densities of both the residual SDE and the corresponding residual network
for various values of γ. Notice how the samples from the SDE well-approximates the histogram of a
finite network.
A Stable Non-Commutative Limit.
Our results complement those of Hayou and Yang [44],
where the authors have shown that for a similar ResNet under the parameter scaling λ = 1, γ = 1/
√
d,
6

the depth and width limits commute. More precisely, the covariance V αβ converges to the same
limit regardless of the order with respect to which the limit is taken or the depth-to-width ratio.
Furthermore, the limit is deterministic, and can be described by an ordinary differential equation
(ODE). Intuitively, the convergence to a deterministic quantity occurs because γ = 1/
√
d suppresses
the random fluctuations enough to vanish in the limit. On the other hand, our results show that for
λ, γ constant in n, d, the random fluctuations are on the right order of O(n−1/2) as in the MLP case
(Eq. 3), hence they do not vanish in the simultaneous limit. The most notable difference is that our
limiting regime is non-commutative as it depends on the depth-to-width ratio of the network. We
remark that both regimes prevents degeneracy of covariance for residual architectures, forming two
theories that complement each other.
4
Neural Covariance SDE for Softmax-Based Attention
4.1
Unshaped Attention and Its Taylor Expansion
A central piece to the neural covariance SDE theory for MLPs [18] is identifying the exact scaling of
shaped activation functions. In particular, the effect of the activations on the covariance Markov
chain Vℓmust be on the same order as the random weights in an MLP, thus forming an approximate
Euler-discretization
Vℓ+1 = Vℓ+ b(Vℓ)
n
+ Σ(Vℓ)1/2ξℓ
√n
+ O(n−3/2) ,
(7)
where b, Σ are deterministic coefficients, and ξℓare random vectors with zero mean and identity
covariance. From here onwards, we use O(n−p) to denote a random variable Z such that npZ has
all moments bounded by universal constants (i.e. independent of n). Since the update can be
interpreted as discretization with step size n−1, naturally the Markov chain converges to an SDE.
We again note that a stable SDE implies a stable covariance structure for finite size networks.
To achieve the same goal for modified attention mechanisms, we consider a similar approach as
Li et al. [18] for smooth activation functions, and Taylor expand the Softmax function in terms of a
large temperature parameter τ. To this end, let Yℓto be the matrix of dot-products between queries,
and keys, i.e. Yℓ:= Xℓ
1
√nW Q
ℓ
1
√nW K,⊤
ℓ
X⊤
ℓ.
More specifically, given a row yα ∈R1×m of the logits Yℓ∈Rm×m, we can Taylor expand the
row-wise Softmax in terms of τ −1:
Softmax(τ −1yα) = 1
m1⊤+ 1
τm(yα −yα) +
1
2τ 2m
h
(yα −yα)2 −

yα2 −(yα)2
i
+ O(τ −3),
(8)
where yα := 1
m
P
β yαβ1⊤and (yα)2 is the vector with squared entries of yα, and 1 ∈Rm×1 is the
(column) vector of ones. We note in practice τ is often set to √nk, which is often quite large and
allows for asymptotic analysis [9].
We observe that the zero-th order term m−11⊤is independent of τ. Considering the form of the
attention block as AℓXℓ1
√nW V
ℓ, this yields an update that is no longer a small perturbation of Vℓ,
regardless of how τ is chosen. Therefore, to form a Markov chain like Eq. 7, we actually require Aℓ
to be approximately the identity matrix.
7

Figure 3: Mean correlation (left) and covariance (right) (in absolute value) under various interventions
on the proposed shaped attention. In particular, we remove either one or two of the three modifications
from the shaped attention in Eq. 9. For instance "τ 2 = nnk, center" indicates that we use the
proposed temperature, and we center by m−1, but we do not add the identity matrix, while in "only
id" we add the identity matrix but use τ = √nk and do not center. We note in this "only id" case,
the covariance remains unstable due to incorrect scaling. Due to exploding covariance, we choose
to not include the cases "id, τ 2 = nnk" and "only id" in the correlation plot (but only in the
covariance plot). Simulated with n = 300, d = 150, ραβ
0
= 0.2, γ = 1/
√
2 and 213 samples.
4.2
Shaped Attention
To shape the Softmax-attention mechanism as a perturbation of the identity matrix, we propose the
following modifications which we call the shaped attention 1
Aℓ= I + Softmax(τ −1Yℓ) −1
m11⊤,
τ = τ0
√nnk .
(9)
The shaped attention presents three modifications to the Softmax attention in Eq. 2. Firstly, the
zero-order term m−111⊤of the Taylor expansion (Eq. 8) is removed as it causes a non-infinitesimal
drift in the Markov Chain that ultimately leads to instability in the covariance (see Section 4.1).
Secondly, we also observe that when τ is very large, the centered Softmax is a perturbation around
zero. To recover an approximate Euler-update like in Eq. 7, we simply add back the identity matrix.
By biasing the attention matrix towards the identity, we encourage each token to self-attend. This
type of modification was also previously considered by B. He et al. [40]. Finally, the Softmax’s
temperature is chosen to scale as τ = τ0√nnk, for some constant τ0 > 0, which guarantees a
non-degenerate limit as (d, n) →∞(Theorem 4.2). Note that the extra √n term is a departure
from the standard parameterization.
In Figure 3, we show how removing any of the proposed changes individually alters the neural
covariance structure, which becomes degenerate for large depths, while the proposed modifications
remains stable.
1In principle, it could be possible to have a close-to-identity Softmax matrix when the logits are large. However,
this regime also corresponds to a very saturated Softmax, thus making training unstable [51]. As a result, we will
avoid this direction in this work.
8

4.3
Main Result – Neural Covariance SDEs for Shaped Attention Models and
Shaped Transformers
Before we state our main results, we will first define a weakened notion of convergence, which is
required whenever the drift and covariance coefficients are not Lipschitz. This was also required for
the case of shaped MLPs with smooth activations [18].
Definition 4.1 (Local Convergence).
We say the covariance V (n) converges locally to V if the
stopped process {V (n)
t∧Tr}t≥0 converges to {Vt∧Tr}t≥0 in the sense of Definition 3.1 for all stopping
times of the form Tr = inf{t > 0 : ∥Vt∥≥r} with r > 0.
Let the covariance with respect to the average token be defined as V α¯x := m−1 Pm
ν=1 V αν, and
the average trace be ¯V := m−1 Pm
ν=1 V νν. We will need to compute a couple of important moments
from the Taylor expansion terms of the Softmax (Lemma C.2)
Sαδ,βω
1
:= n−1
k E(Y αδ −yα)(Y βω −yβ) = V αβ 
V δω −V δ¯x −V ω¯x + V ¯x¯x
,
Sαδ
2
:= n−1
k E
h
(Y αδ −yα)2 −((Y α)2 −yα2)
i
= V αα 
V δδ −2V δ¯x + 2V ¯x¯x −¯V

.
(10)
We are now ready to state our main result.
Theorem 4.2.
Let Xℓbe the hidden layers of a residual attention network defined in Eq. 1 with
shaped attention in Eq. 9, parameters λ2 + γ2 = 1 and τ = τ0√nnk, where λ, γ, τ0 all do not depend
on d, n. Then the feature covariance Vℓconverges locally to the solution of the following SDE (in the
sense of Definition 4.1)
dVt = b(Vt)dt + Σ(Vt)1/2dBt ,
V0 = 1
nX0X⊤
0 ,
where the drift has the following form
b(V αβ
t
) = γ2
τ 2
0

1
m2
m
X
ν,κ=1
V νκSαν,βκ
1
+ 1
2m
m
X
ν=1
(V βνSαν
2
+ V ανSβν
2 )


α≤β
,
the diffusion coefficient is defined by Σ(V ) = γ2(2 −γ2)Σlin(V ) + γ4τ −2
0 [Aαβ,δω]α≤β,δ≤ω, and
Aαβ,δω := 1
m2
m
X
ν,κ=1

V ακV δνSβκ,ων
1
+ V ακV ωνSβκ,δν
1
+ V βνV δκSαν,ωκ
1
+ V βνV ωκSαν,δκ
1

.
The drift depends on the shaped attention mechanism through Sαδ,βω
1
and Sαδ
2 , the moments
of the first and second order terms of the Softmax’s Taylor expansion. On the other hand, the
diffusion term depends on the attention solely through S1, present in the additional term Aαβ,δω.
The presence of Aαβ,δω is an intriguing difference compared to shaped ReLU networks, where the
diffusion is not affected by the activation function. Both components of the SDE depend on averages
over the tokens, reflecting the mixing property of the self-attention mechanism, in which every pair
of tokens is compared through dot products to form the attention weights. Finally, notice how the
residual branch parameter γ2 has a dampening effect on the scale of both the drift and the diffusion
in a similar way as in fully-connected residual network.
9

We are now ready to introduce the full shaped Transformer architecture, where we combine the
attention and residual layers:
Zℓ= λXℓ+ γAℓXℓ
1
√nW V
ℓ,
Xℓ+1 = λZℓ+ γσs

Zℓ
1
√nW pre
ℓ
 r c
nW post
ℓ
,
(11)
where Aℓis the shaped attention defined by Eq. 9. We note that covariance SDE handle stacking of
different layer types very conveniently by simply adding the drift and covariance of the diffusion
coefficients, which we summarize in the Corollary below.
Corollary 4.3 (Shaped Transformer Covariance SDE). Let Xℓbe the hidden layers of a shaped
transformer defined in Eq. 11 with parameters λ2 + γ2 = 1 and τ = τ0√nnk, where λ, γ, τ0 all do
not depend on d, n. Then the feature covariance Vℓconverges locally to the solution of the following
SDE (in the sense of Definition 4.1)
dVt = [b(Vt) + bres(Vt)] dt + [Σ(Vt) + Σres(Vt)]1/2 dBt ,
(12)
where the coefficients are defined in Theorem 3.2 and Theorem 4.2.
4.4
On Finite Time Stability of the SDE and Shaped Attention Networks
Although we did not observe numerical instability in majority of our simulations of the shaped
attention networks and the corresponding SDE, we did observe that the drift component b(Vt) in
Theorem 4.2 is cubic in the entries of Vt. Whenever the drift is not Lipschitz as in this case, we
do not have general guarantees for the existence of a solution for all time (see the Feller test for
explosions [52, Theorem 5.5.29]). In fact, MLPs with smooth activations also yield non-Lipschitz
drift coefficients as seen in Li et al. [18].
However, locally Lipschitz coefficients are sufficient to guarantee the existence of local solutions,
in the sense of up to a stopping time [53, Proposition 6.9]. Not only does this fact help us establish a
precise notion of convergence (Definition 4.1), we can also study the practical implications of this for
finite sized attention networks. More specifically, we can inspect the effect of architectural changes
to a stopping time.
To demonstrate the potential numerical instabilities, we had to choose an adversarial set of
parameters: in particular, an unrealistically large norm (approx. 10√n) for the initial tokens X0,
which enlarges the eigenvalues of V0 to the order of 100. Given these initial conditions and a large
residual connection weight γ, we were able to consistently generate numerically unstable behaviour
in shaped attention networks (see Figure 4 (left)).
That being said, it is very straight forward to stabilize the network by tweaking parameters such
as γ, τ0 and the depth-to-width ratio of the network. We demonstrate the effect of tuning γ on both
sample trajectories of the maximum eigenvalue of Vℓand the stopping time in Figure 4. As we may
intuitively expect, tuning γ smaller will delay the time scale of numerical instabilities, hence allowing
for larger depth networks to remain stable.
5
Discussion
Architecture Design and Hyperparameter Tuning.
Previous work have demonstrated the
practical impact scaling limits can have on designing activation functions [16, 17] and tuning
hyperparameters [19]. We follow this line of motivations and proposed a novel attention mechanism,
which successfully stabilizes the covariance structure in arbitrarily deep Transformers (e.g. Figure 1).
10

Figure 4: Left: Trajectories of the maximum eigenvalue of the covariance matrix in a shaped
attention network, with adversarially large initial condition. Right: Stopping time of the shaped
attention neural network, capped at 1. Stopping time is defined as t∗= d∗/n with d∗the maximum
depth beyond which one of the eigenvalues of the covariance matrix exceeds 104 or drops below 10−4.
Simulated with n = d = 200, τ0 = 1, and 100 samples used for median and 10th percentile.
The natural next step is to investigate the scaling of gradients in the infinite-depth-and-width limit.
As Yang et al. [19] illustrated, the existence of an infinite-width limit for the gradient implies the
optimal hyperparameters for the training algorithm will also converge. This type of results allows for
tuning of hyperparameters on networks with a much smaller width, yet extends easily to arbitrarily
large networks that approximates the same limit, saving massive amounts of computing cost in the
process. Given the existence of an infinite-depth-and-width limit for the forward pass, we believe it’s
possible to extract optimal hyperparameters from networks with not only a much smaller width, but
smaller depth as well.
Preliminary Experiments.
Although this work is primarily theoretical, it is important to consider
whether or not the proposed architecture is useful in practice. Given limited computing resources,
we chose to only briefly test the feasibility of training the shaped Transformer. Nevertheless, our
preliminary experiments show promising results when it comes to training stability. In particular,
the shaped Transformer (without LayerNorm) does indeed train at approximately the same speed as
well tuned Transformer architectures. Full details of the experiment and results can be found in
Appendix D. A more comprehensive set of experiments with different tasks, datasets, and larger
networks will be required to confidently determine the practical feasibility of the shaped Transformer,
which we defer to future work.
Training Dynamics and Generalization.
As mentioned in the introduction, the limitations of
infinite-width NTK theories motivates our study of the proportional infinite-depth-and-width limit.
In particular, to address many of the open problems in deep learning theory, we need a faithful
and tractable description of training dynamics. Given the results at initialization, the proportional
limit holds the potential for such a theory of training as well. Another promising indicator is that
deep networks learn features in the proportional regime [35], which has been identified as a key
advantage of neural networks over kernel methods [25, 54–60]. A precise theory of training will help
us understand other types of instabilities during training and improve existing optimization methods.
Furthermore, determining the network which training converges to is a necessary step towards a
theory of generalization, as demonstrated by the infinite-width approach [61]. In light of our results,
11

we believe that our theory sets the stage for future work on training and generalization in deep
learning.
Acknowledgement
CL and ML would like to thank Keiran Paster for insightful discussions. LN would like to thank
Sotiris Anagnostidis for support in pre-processing the dataset used for the training experiments of
this manuscript. ML is supported by the Ontario Graduate Scholarship and Vector Institute. DMR
is supported in part by Canada CIFAR AI Chair funding through the Vector Institute, an NSERC
Discovery Grant, Ontario Early Researcher Award, a stipend provided by the Charles Simonyi
Endowment, and a New Frontiers in Research Exploration Grant.
References
[1]
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P.
Shyam, G. Sastry, A. Askell, et al. “Language models are few-shot learners”. In: Advances in
Neural Information Processing Systems 33 (2020), pp. 1877–1901.
[2]
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin. Attention is all you need. 2017. arXiv: 1706.03762.
[3]
J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y.
Yang, and Y. Zhou. Deep learning scaling is predictable, empirically. 2017. arXiv: 1712.00409.
[4]
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,
J. Wu, and D. Amodei. Scaling laws for neural language models. 2020. arXiv: 2001.08361.
[5]
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas,
L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models.
2022. arXiv: 2203.15556.
[6]
Y. Tay, M. Dehghani, S. Abnar, H. W. Chung, W. Fedus, J. Rao, S. Narang, V. Q. Tran,
D. Yogatama, and D. Metzler. Scaling Laws vs Model Architectures: How does Inductive Bias
Influence Scaling? 2022. arXiv: 2207.10551.
[7]
A. Aghajanyan, L. Yu, A. Conneau, W.-N. Hsu, K. Hambardzumyan, S. Zhang, S. Roller,
N. Goyal, O. Levy, and L. Zettlemoyer. Scaling Laws for Generative Mixed-Modal Language
Models. 2023. arXiv: 2301.03728.
[8]
Y. Dong, J.-B. Cordonnier, and A. Loukas. “Attention is not all you need: Pure attention loses
rank doubly exponentially with depth”. International Conference on Machine Learning. PMLR.
2021, pp. 2793–2803.
[9]
L. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. P. Singh, and A. Lucchi. Signal Propagation in
Transformers: Theoretical Perspectives and the Role of Rank Collapse. 2022. arXiv: 2206.03126.
[10]
B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. “Exponential expressivity in
deep neural networks through transient chaos”. In: Advances in Neural Information Processing
Systems 29 (2016).
[11]
S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. “Deep Information Propagation”.
ICLR. 2017.
12

[12]
G. Yang and S. S. Schoenholz. “Mean field residual networks: on the edge of chaos”. Advances
in Neural Information Processing Systems. 2017, pp. 2865–2873.
[13]
S. Hayou, A. Doucet, and J. Rousseau. “On the impact of the activation function on deep neural
networks training”. International Conference on Machine Learning. PMLR. 2019, pp. 2672–
2680.
[14]
M. Murray, V. Abrol, and J. Tanner. “Activation function design for deep networks: linearity
and effective initialisation”. In: Applied and Computational Harmonic Analysis 59 (2022),
pp. 117–154.
[15]
L. Xiao, J. Pennington, and S. Schoenholz. “Disentangling trainability and generalization in
deep neural networks”. International Conference on Machine Learning. PMLR. 2020, pp. 10462–
10472.
[16]
J. Martens, A. Ballard, G. Desjardins, G. Swirszcz, V. Dalibard, J. Sohl-Dickstein, and S. S.
Schoenholz. Rapid training of deep neural networks without skip connections or normalization
layers using deep kernel shaping. 2021. arXiv: 2110.01765.
[17]
G. Zhang, A. Botev, and J. Martens. Deep Learning without Shortcuts: Shaping the Kernel
with Tailored Rectifiers. 2022. arXiv: 2203.08120.
[18]
M. B. Li, M. Nica, and D. M. Roy. The Neural Covariance SDE: Shaped Infinite Depth-and-
Width Networks at Initialization. 2022. arXiv: 2206.02768.
[19]
G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen,
and J. Gao. Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter
Transfer. 2022. arXiv: 2203.03466.
[20]
R. M. Neal. “Bayesian learning for neural networks”. PhD thesis. University of Toronto, 1995.
[21]
J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. “Deep
Neural Networks as Gaussian Processes”. ICLR. 2018.
[22]
A. G. d. G. Matthews, M. Rowland, J. Hron, R. E. Turner, and Z. Ghahramani. Gaussian
process behaviour in wide deep neural networks. 2018. arXiv: 1804.11271.
[23]
J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. “Infinite attention: NNGP and NTK
for deep attention networks”. International Conference on Machine Learning. PMLR. 2020,
pp. 4376–4386.
[24]
A. Jacot, F. Gabriel, and C. Hongler. “Neural tangent kernel: Convergence and generalization
in neural networks”. Advances in Information Processing Systems (NeurIPS). 2018. arXiv:
1806.07572.
[25]
G. Yang and E. J. Hu. Feature learning in infinite-width neural networks. 2020. arXiv: 2011.
14522.
[26]
J. Sirignano and K. Spiliopoulos. “Mean field analysis of neural networks: A law of large
numbers”. In: SIAM Journal on Applied Mathematics 80.2 (2020), pp. 725–752.
[27]
S. Mei, A. Montanari, and P.-M. Nguyen. “A mean field view of the landscape of two-layer
neural networks”. In: Proceedings of the National Academy of Sciences 115.33 (2018), E7665–
E7671.
[28]
L. Chizat and F. Bach. “On the global convergence of gradient descent for over-parameterized
models using optimal transport”. In: Advances in Neural Information Processing Systems 31
(2018).
13

[29]
G. M. Rotskoff and E. Vanden-Eijnden. Trainability and Accuracy of Neural Networks: An
Interacting Particle System Approach. 2018. arXiv: 1805.00915.
[30]
S. Yaida. “Non-Gaussian processes and neural networks at finite widths”. Mathematical and
Scientific Machine Learning. PMLR. 2020, pp. 165–192.
[31]
D. A. Roberts, S. Yaida, and B. Hanin. The principles of deep learning theory. Cambridge
University Press, 2022.
[32]
J. Zavatone-Veth and C. Pehlevan. “Exact marginal prior distributions of finite Bayesian neural
networks”. In: Advances in Neural Information Processing Systems 34 (2021).
[33]
B. Hanin. Correlation Functions in Random Fully Connected Neural Networks at Finite Width.
2022. arXiv: 2204.01058.
[34]
E. Dinan, S. Yaida, and S. Zhang. Effective Theory of Transformers at Initialization. 2023.
arXiv: 2304.02034.
[35]
B. Hanin and M. Nica. Finite depth and width corrections to the neural tangent kernel. 2019.
arXiv: 1909.05989.
[36]
B. Hanin and M. Nica. “Products of many large random matrices and gradients in deep neural
networks”. In: Communications in Mathematical Physics (2019), pp. 1–36.
[37]
Z. Hu and H. Huang. “On the Random Conjugate Kernel and Neural Tangent Kernel”.
International Conference on Machine Learning. PMLR. 2021, pp. 4359–4368.
[38]
M. B. Li, M. Nica, and D. Roy. “The future is log-Gaussian: ResNets and their infinite-depth-
and-width limit at initialization”. In: Advances in Neural Information Processing Systems 34
(2021), pp. 7852–7864.
[39]
L. Noci, G. Bachmann, K. Roth, S. Nowozin, and T. Hofmann. “Precise characterization of
the prior predictive distribution of deep ReLU networks”. In: Advances in Neural Information
Processing Systems 34 (2021).
[40]
B. He, J. Martens, G. Zhang, A. Botev, A. Brock, S. L. Smith, and Y. W. Teh. Deep
Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation. 2023.
arXiv: 2302.10322.
[41]
M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron,
R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. 2023.
arXiv: 2302.05442.
[42]
R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and
T. Liu. “On layer normalization in the transformer architecture”. International Conference on
Machine Learning. PMLR. 2020, pp. 10524–10533.
[43]
J. L. Ba, J. R. Kiros, and G. E. Hinton. “Layer normalization”. In: (2016). arXiv: 1607.06450.
[44]
S. Hayou and G. Yang. Width and Depth Limits Commute in Residual Networks. 2023. arXiv:
2302.00453.
[45]
S. Hayou, E. Clerico, B. He, G. Deligiannidis, A. Doucet, and J. Rousseau. “Stable resnet”.
International Conference on Artificial Intelligence and Statistics. PMLR. 2021, pp. 1324–1332.
[46]
W. Tarnowski, P. Warchoł, S. Jastrzebski, J. Tabor, and M. Nowak. “Dynamical isometry
is achieved in residual networks in a universal way for any activation function”. The 22nd
International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 2221–2230.
[47]
S. Hayou. On the infinite-depth limit of finite-width neural networks. 2022. arXiv: 2210.00688.
14

[48]
G. Yang. “Tensor programs ii: Neural tangent kernel for any architecture”. In: arXiv preprint
arXiv:2006.14548 (2020).
[49]
K. He, X. Zhang, S. Ren, and J. Sun. “Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification”. Proceedings of the IEEE international conference on
computer vision. 2015, pp. 1026–1034.
[50]
A. Veit, M. J. Wilber, and S. Belongie. “Residual networks behave like ensembles of relatively
shallow networks”. In: Advances in neural information processing systems 29 (2016).
[51]
S. Zhai, T. Likhomanenko, E. Littwin, D. Busbridge, J. Ramapuram, Y. Zhang, J. Gu, and
J. Susskind. Stabilizing Transformer Training by Preventing Attention Entropy Collapse. 2023.
arXiv: 2303.06296.
[52]
I. Karatzas and S. Shreve. Brownian motion and stochastic calculus. Vol. 113. Springer Science
& Business Media, 2012.
[53]
J. Miller. Stochastic Calculus (Lecture Notes). http://www.statslab.cam.ac.uk//~jpm205/
teaching/lent2016/lecture_notes.pdf. 2015.
[54]
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. “When do neural networks outperform
kernel methods?” In: Advances in Neural Information Processing Systems 33 (2020), pp. 14820–
14830.
[55]
E. Abbe, E. B. Adsera, and T. Misiakiewicz. “The merged-staircase property: a necessary and
nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks”.
Conference on Learning Theory. PMLR. 2022, pp. 4782–4887.
[56]
J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. “High-dimensional asymptotics
of feature learning: How one gradient step improves the representation”. Int. Conf. Learning
Representations (ICLR). 2022. url: https://openreview.net/forum?id=akddwRG6EGi.
[57]
A. Damian, J. Lee, and M. Soltanolkotabi. “Neural networks can learn representations with
gradient descent”. Conference on Learning Theory. PMLR. 2022, pp. 5413–5452.
[58]
A. Mousavi-Hosseini, S. Park, M. Girotti, I. Mitliagkas, and M. A. Erdogdu. Neural Networks
Efficiently Learn Low-Dimensional Representations with SGD. 2022. arXiv: 2209.14863.
[59]
E. Abbe, E. Boix-Adsera, and T. Misiakiewicz. SGD learning on neural networks: leap complexity
and saddle-to-saddle dynamics. 2023. arXiv: 2302.11055.
[60]
R. Berthier, A. Montanari, and K. Zhou. Learning time-scales in two-layers neural networks.
2023. arXiv: 2303.00055.
[61]
P. L. Bartlett, A. Montanari, and A. Rakhlin. “Deep learning: a statistical viewpoint”. In: Acta
numerica 30 (2021), pp. 87–201.
[62]
O. Kallenberg. Foundations of Modern Probability. Probability theory and stochastic modelling.
Springer, 2021. isbn: 9783030618728.
[63]
HuggingFace. Wikipedia data set. https://huggingface.co/datasets/wikipedia.
[64]
HuggingFace. Bookcorpus data set. https://huggingface.co/datasets/bookcorpus.
[65]
D. P. Kingma and J. Ba. “Adam: A method for stochastic optimization”. In: arXiv preprint
arXiv:1412.6980 (2014).
[66]
HuggingFace. Hugging face Bert implementation. https : / / github . com / huggingface /
transformers/blob/main/src/transformers/models/bert/modeling_bert.py.
15

A
Preliminaries: Covariance SDE Framework
In this section, we will review existing results on Markov chain convergence to an SDE, as well as
existing results for the covariance SDE. See also the Appendix of Li et al. [18] for more details.
Firstly, we will define the Skorohod J1-topology, or just the Skorohod topology for short [62,
Appendix 5]. The topology is used to describe convergence of continuous time processes with
discontinuities, in particular, Markov chains with a continuous time interpolation fits in this category.
Let S be a complete separable metric space, and DR+,S be the space of càdlàg functions (right
continuous with left limits) from R+ →S. We use xn
ul
−→x to denote locally uniform convergence
(uniform on compact subsets of R+), and consider the class of bijections λ on R+ so that λ is strictly
increasing with λ0 = 0 (can be interpreted as a time change).
Definition A.1.
We define Skorohod convergence xn
s−→x on DR+,S if there exists a sequence
of bijections λn satisfying above conditions and
λn
ul
−→Id ,
xn ◦λn
ul
−→x .
(13)
In particular, we call the topology that induces this convergence the Skorohod topology [62, Theorem
A5.3].
On a heuristic level, if we have sequences of Markov chains Y n that satisfy the following type of
Euler updates
Y n
ℓ+1 = Y n
ℓ+ b(Y n
ℓ)
n
+ σ(Y n
ℓ) ξℓ
√n
,
(14)
where ξℓare iid random variables with zero mean and identity covariance, then we can interpolate
this process in continuous time with Xn
t = Y n
⌊tn⌋and show that as n →∞, we have that Xn converges
to the solution of the following SDE (weakly with respect to the Skorohod topology)
dXt = b(Xt) dt + σ(Xt) dBt ,
X0 = lim
n→∞Y n
0 .
(15)
Our next theorem essentially weakens this result in several ways. Firstly, we don’t need to take
the step size n−1, but instead replace it with n−2p for all p > 0. Next, we can allow the update to
contain higher order terms that vanish, in particular, any terms of order O(n−2p−δ) for all δ > 0.
Here, we remind the readers that O(n−p) denotes a random variable Z such that npZ has all moment
bounded by a universal constant (independent of n). Thirdly, we can allow b(y) to be random, or
more precisely replace it with bb(y, ωn) such that Ebb(y, ωn) = b(y). Fourthly, we can also allow b, σ to
be a sequence bn, σn such that they converge to b, σ on uniformly on compact sets. Finally, we can
also weaken the topology of convergence to locally, that is all processes are stopped by a stopping
time as in Definition 4.1.
Now we will state the main technical result in this section.
Proposition A.2 (Convergence of Markov Chains to SDE, Proposition A.6, [18]).
Let Y n be a
discrete time Markov chain on RN defined by the following update for p, δ > 0
Y n
ℓ+1 = Y n
ℓ+
bbn(Y n
ℓ, ωn
ℓ)
n2p
+ σn(Y n
ℓ)
np
ξn
ℓ+ O(n−2p−δ) ,
(16)
where ξn
ℓ∈RN are iid random variables with zero mean, identity covariance, and moments uniformly
bounded in n. Furthermore, ωn
ℓare also iid random variables such that E[bbn(Y n
ℓ, ωn
ℓ)|Y n
ℓ= y] = bn(y)
16

and bbn(y, ωn
ℓ) has uniformly bounded moments in n. Finally, σn is a deterministic function, and the
remainder terms in O(n−2p−δ) have uniformly bounded moments in n.
Suppose bn, σn are uniformly Lipschitz functions in n and converges to b, σ uniformly on compact
sets, then in the limit as n →∞, the process Xn
t = Y n
⌊tn2p⌋converges in distribution to the solution
of the following SDE in the Skorohod topology of DR+,RN
dXt = b(Xt) dt + σ(Xt) dBt ,
X0 = lim
n→∞Y n
0 .
(17)
Suppose otherwise bn, σn are only locally Lipschitz (but still uniform in n), then Xn converges
locally to X in the same topology (see Definition 4.1). More precisely, for any fixed r > 0, we
consider the stopping times
τ n := inf {t ≥0 : |Xn
t | ≥r} ,
τ := inf {t ≥0 : |Xt| ≥r} ,
(18)
then the stopped process Xn
t∧τ n converges in distribution to the stopped solution Xt∧τ of the above
SDE in the same topology.
We will briefly recall the main result of Li et al. [18] next. As mentioned earlier in the text, the
setting is for MLPs defined as follows
Xℓ+1 = σs(Xℓ)
r c
nWℓ,
(19)
where σs(x) = s+ max(x, 0) + s−min(x, 0) with s± = 1 + c±
√n, c−1 = E σs(g)2 for g ∼N(0, 1), n is
the width of the network, and Wℓ,ij ∼N(0, 1) are iid random weights.
Then it was shown that in the limit as n, d →∞with d
n = T > 0, the covariance matrix
Vℓ=
1
nXℓX⊤
ℓ
(or equivalent the post activation
c
nσs(Xℓ)σs(Xℓ)⊤since the activation becomes
infinitesimal) converges the solution of the following SDE in the Skorohod topology [18, Theorem
3.2]
dVt = bReLU(Vt) dt + Σlin(Vt)1/2 dBt ,
V0 = 1
nX0X⊤
0 ,
(20)
where the coefficients were given in Theorem 3.2.
We remark that if the output is defined as Xout =
1
√nWoutXd ∈Rnout, then we can recover the
output distribution as
Xout ∼N(0, VT ⊗Inout) ,
(21)
where we treated Xout as a vector in Rmnout.
B
SDE for Residual Network
Recall that we adopt the following model:
Xℓ+1 = λXℓ+ γ 1
√nσs
r c
nXℓW pre
ℓ

W post
ℓ
,
(22)
where σs(x) := s+ max(x, 0) + s−min(x, 0) is the shaped ReLU with slopes s± := 1 + c±
√n for some
constants c+, c−∈R . We let the weights be (W pre
ℓ
)ij(W post
ℓ
)ij
iid
∼N(0, 1) and c−1 = Eσs(g)2 for
g ∼N(0, 1) is the He initialization constant [49].
17

From here onwards, we will define the filtration Fℓ= σ({Xk}k∈[ℓ]), where σ(·) denotes the sigma-
algebra generated by the random variable. Furthermore, we will define the conditional expectation
Eℓ[ · ] = E[ · |Fℓ].
We are interested in studying the neural covariance, i.e. V αβ
ℓ
:= c
n⟨xα
ℓ, xβ
ℓ⟩where ℓ∈[d] indexes
the layers and d is the depth. In particular, we are interested in understanding the simultaneous
limit (n, d) →∞, where the ratio t = d/n remains constant.
For the cross product terms, we get (removing the dependence on ℓ):
⟨xα, f(X, W)β⟩=
√c
n ⟨xα,
 σs (XW pre) W postβ⟩=
√c
n
n
X
i,j
xα
i W post
ji
σs

X
j′
xβ
j′W pre
j′j

,
⟨xβ, f(X, W)α⟩=
√c
n
n
X
i,j
xβ
i W post
ji
σs

X
j′
xα
j′W pre
j′j

.
For the term in γ2:
c
n2 ⟨
 σs (XW pre) W postα ,
 σs (XW pre) W postβ⟩= c
n2
n
X
i=1
 σs (XW pre) W post
αi
 σs (XW pre) W post
βi
= c
n2
n
X
i,j,j′=1
W post
ji
W post
j′i σs
 X
k
xα
kW pre
kj
!
σs
 X
k′
xβ
k′W post
k′j′
!
.
We will define the following terms
T αβ
1
:= c√c
n√n
n
X
i,j=1
W post
ji

xα
i σs

X
j′
xβ
j′W pre
j′j

+ xβ
i σs

X
j′
xα
j′W pre
j′j



,
and
T αβ
2
:=
c2
n2√n
n
X
i,j,j′=1
W post
ji
W post
j′i σs
 X
k
xα
kW pre
kj
!
σs
 X
k′
xβ
k′W pre
k′j′
!
.
Hence we get the following update for V αβ
ℓ
:= c
n⟨xα
ℓ, xβ
ℓ⟩:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ γλ
√nT αβ
1
+ γ2
√nT αβ
2
.
It is easy to see that the cross product terms have (conditional) mean zero. For the term with γ2:
Eℓ[T αβ
2
] = √n
√
V ααV ββcK1(ραβ),
(23)
where K1(ραβ) := E[σs(gα)σs(gβ)] where gα, gβ ∼N(0, 1) and E[gαgβ] =
⟨xα,xβ⟩
∥xα∥∥xβ∥.
Here we
recall Eℓ[ · ] = E[ · |Fℓ] is the conditional expectation given the sigma-algebra generated by Fℓ=
σ({Xk}k∈[ℓ]).
Li et al. [18] showed that if the non linearity scaling exponent is p = 1/2, then:
cK1(ρ) = ρ + ν(ρ)
n
+ O(n−3/2),
18

where ν(ρ) = (c++c−)2
2π
p
1 −ρ2 + ρarccos(ρ)

. Using this result, and summing and subtracting
the mean of T2:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ γ2√
V ααV ββcK1(ραβ) + γλ
√nT αβ
1
+ γ2
√n(T αβ
2
−Eℓ[T αβ
2
])
= λ2V αβ
ℓ
+ γ2V αβ
ℓ
+ γ2√
V ααV ββ ν(ραβ)
n
+ γλ
√nT αβ
1
+ γ2
√n(T αβ
2
−Eℓ[T αβ
2
]) + O(n−3/2) .
Using λ2 + γ2 = 1:
V αβ
ℓ+1 = V αβ
ℓ
+ γ2√
V ααV ββ ν(ραβ)
n
+ γλ
√nT αβ
1
+ γ2
√n(T αβ
2
−Eℓ[T αβ
2
]) + O(n−3/2) .
Furthermore, we need second order moments of T1 and T2. We derive them in the following
Lemmas:
Lemma B.1.
Eℓ[T αβ
1
T δω
1 ] = V αδ√
V ββV ωωcK1(ρβω) + V αω√
V ββV δδcK1(ρβδ)
+ V βδ√
V ααV ωωcK1(ραω) + V βω√
V ααV δδcK1(ραδ)
= 2V αδV βω + 2V αωV βδ + O(n−1) .
Proof. Recall the definition of T αβ
1
:
T αβ
1
:= c√c
n√n
n
X
i,j=1
W post
ji

xα
i σs

X
j′
xβ
j′W pre
j′j

+ xβ
i σs

X
j′
xα
j′W pre
j′j



,
We have that:
Eℓ[T αβ
1
T δω
1 ] = c3
n3
X
iji′j′
E

WjiWj′i′
Eℓ
h
xα
i
xβ σs(gβ
j ) + xβ
i ∥xα∥σs(gα
j )
 
xδ
i ∥xω∥σs(gω
j ) + xω
i
xδ σs(gδ
j)
i
= c3
n3
X
ij
Eℓ
h
xα
i xδ
i
xβ ∥xω∥σs(gβ
j )σs(gω
j ) + xα
i xω
i
xβ
xδ σs(gβ
j )σs(gδ
j)
+ xβ
i xδ
i ∥xα∥∥xω∥σs(gα
j )σs(gω
j ) + xβ
i xω
i ∥xα∥
xδ σs(gα
j )σs(gδ
j)
i
= V αδ√
V ββV ωωcK1(ρβω) + V αω√
V ββV δδcK1(ρβδ)
+ V βδ√
V ααV ωωcK1(ραω) + V βω√
V ααV δδcK1(ραδ)
Lemma B.2.
Eℓ[T αβ
2
] = c√n
√
V ααV ββK1(ραβ) = √nV αβ + 1
√n
√
V ααV ββν(ρ) + O(n−1) ,
Eℓ[T αβ
2
T δω
2 ] −Eℓ[T αβ
2
]E[T δω
2 ] = 2V αδV βω + 2V αωV βδ + O(n−1) .
19

Proof. Recall the definition of T αβ
2
:
T αβ
2
: =
c2
n2√n
n
X
i,j,j′=1
W post
ji
W post
j′i σs
 X
k
xα
kW pre
kj
!
σs
 X
k′
xβ
k′W pre
k′j′
!
=
c2
n2√n ∥xα∥
xβ
n
X
i,j,j′=1
W post
ji
W post
j′i σs(gα
j )σs(gβ
j′)
For the mean, using the independence between W pre and W post, we have that:
Eℓ
h
T αβ
2
i
=
c2
n2√n ∥xα∥
xβ
n
X
i,j,j′=1
E
h
W post
ji
W post
j′i
i
E
h
σs(gα
j )σs(gβ
j′)
i
=
c2
n√n ∥xα∥
xβ
n
X
j=1
E
h
σs(gα
j )σs(gβ
j )
i
= c2
√n ∥xα∥
xβ K1(ραβ)
= c√n
√
V ααV ββK1(ραβ),
which is the desired result. The final expression is the result of the aforementioned expansion for K1
[18]. For the covariance, we have that:
Eℓ[T αβ
2
T δω
2 ] = c4
n5 ∥xα∥
xβ
xδ ∥xω∥
X
i,j,j′,i′,k,k′
E
h
W post
ji
W post
j′i W post
ki′ W post
k′i′
i
E
h
σs(gα
j )σs(gβ
j′)σs(gδ
k)σs(gω
k′)
i
= c2
n3
√
V ααV ββV δδV ωω
X
i,j,j′,i′,k,k′
 δjj′δkk′ + δjkδii′δj′k′ + δjk′δii′δj′k

E
h
σs(gα
j )σs(gβ
j′)σs(gδ
k)σs(gω
k′)
i
.
Let’s look at each term of the sum separately. For the first term we have that:
X
i,j,j′,i′,k,k′
δjj′δkk′E
h
σs(gα
j )σs(gβ
j′)σs(gδ
k)σs(gω
k′)
i
= n2 X
j,k
E
h
σs(gα
j )σs(gβ
j )σs(gδ
k)σs(gω
k )
i
= n2 X
j
E
h
σs(gα
j )σs(gβ
j )σs(gδ
j)σs(gω
j )
i
+ n2 X
j̸=k
E
h
σs(gα
j )σs(gβ
j )
i
E
h
σs(gδ
k)σs(gω
k )
i
= n3K2(ραβδω) + n3(n −1)K1(ραβ)K1(ρδω),
where we have defined the fourth moment of the shaped activation: K2(ραβδω) := E

σs(gα)σs(gβ)σs(gδ)σs(gω)

for which it holds that [18, Lemma C.2]:
K2(ραβδω) = E[gαgβgδgω] + O(n−1/2) = ραβρδω + ραδρβω + ραωρβδ + O(n−1/2).
The other two summands can be solved similarly:
X
i,j,j′,i′,k,k′
δjkδii′δj′k′E
h
σs(gα
j )σs(gβ
j′)σs(gδ
k)σs(gω
k′)
i
= n2K2(ραβδω) + n2(n −1)K1(ραδ)K1(ρβω) ,
20

and
X
i,j,j′,i′,k,k′
δjk′δii′δj′kE
h
σs(gα
j )σs(gβ
j′)σs(gδ
k)σs(gω
k′)
i
= n2K2(ραβδω) + n2(n −1)K1(ραω)K1(ρβδ) .
Hence, summing the three terms:
Eℓ[T αβ
2
T δω
2 ] = c2√
V ααV ββV δδV ωω

K2(ραβδω) + (n −1)K1(ραβ)K1(ρδω)
+ 1
n

K2(ραβδω) + (n −1)K1(ραδ)K1(ρβω)

+ 1
n

K2(ραβδω) + (n −1)K1(ραω)K1(ρβδ)
 
= c2√
V ααV ββV δδV ωω

K2(ραβδω) + nK1(ραβ)K1(ρδω) −K1(ραβ)K1(ρδω)
+ K1(ραδ)K1(ρβω) + K1(ραω)K1(ρβδ)

+ O(n−1).
Now, subtracting Eℓ[T αβ
2
]Eℓ[T δω
2 ], using the aforementioned expansions for K1 and K2:
Eℓ[T αβ
2
T δω
2 ] −Eℓ[T αβ
2
]Eℓ[T δω
2 ] = c2√
V ααV ββV δδV ωω

ραβρδω + ραδρβω + ραωρβδ
−V αβV δω + V αδV βω + V αωV βδ + O(n−1)
= 2V αδV βω + 2V αωV βδ + O(n−1) ,
where in the final step we have used the fact that c = 1 + O(n−1/2). This completes the proof.
Finally we also have that T1 and T2 are uncorrelated, i.e.
Lemma B.3.
Eℓ[T αβ
1
T δω
2 ] = 0 .
Proof. It is easy to see that EℓT αβ
1
T δω
2
involve odd standard Gaussian moments (in particular third
order moments), which vanish due to the parity of the centered Gaussian measure.
Theorem 3.2.
Let Xℓbe the hidden layers of a ResNet defined in Eq. 4 with λ2 + γ2 = 1, where
both λ and γ do not depend on d, n. Then the feature covariance Vℓconverges to the solution of the
following SDE (in the sense of Definition 3.1)
dVt = bres(Vt) dt + Σres(Vt)1/2 dBt ,
V0 = 1
nX0X⊤
0 ,
(5)
where bres(V ) = γ2bReLU(V ) = γ2[ν(ραβ)
√
V ααV ββ]α≤β with ραβ = V αβ(V ααV ββ)−1/2 and
ν(ρ) = (c+ −c−)2
2π
p
1 −ρ2 −ρ arccos ρ

,
(6)
furthermore, Σres(V ) = 2γ2Σlin(V ) = 2γ2[V αδV βω + V αωV βδ]α≤β,δ≤ω.
Proof. We know that:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ γλ
√nT αβ
1
+ γ2
√nT αβ
2
.
Using λ2 + γ2 = 1, and summing and subtracting the mean of T2, we have that:
V αβ
ℓ+1 = V αβ
ℓ
−γ2V αβ
ℓ
+ γ2
√nEℓ
h
T αβ
2
i
+ γλ
√nT αβ
1
+ γ2
√n

T αβ
2
−Eℓ
h
T αβ
2
i
.
21

Using Lemma B.2 for the mean of T2, we have that:
V αβ
ℓ+1 = V αβ
ℓ
+ 1
n
√
V ααV ββν(ρ) + γλ
√nT αβ
1
+ γ2
√n

T αβ
2
−Eℓ
h
T αβ
2
i
+ O(n−3/2),
which gives us an expression in the right Markov Chain form with drift term
√
V ααV ββν(ρ). For
the diffusion term, we need to compute the covariance between two different neural covariances
V αβ
ℓ+1, V δω
ℓ+1. Using Lemmas B.1 to B.3, we have
Covℓ

V αβ
ℓ+1, V δω
ℓ+1

= Eℓ
h
λγT αβ
1
+ γ2T αβ
2
−γ2Eℓ[T αβ
2
]
 
λγT δω
1
+ γ2T δω
2
−γ2Eℓ[T δω
2 ]
i
= λ2γ2Eℓ
h
T αβ
1
T δω
1
i
+ γ4Eℓ
h
T αβ
2
T δω
2
i
−γ4Eℓ
h
T αβ
2
i
Eℓ
h
T δω
2
i
= 2λ2γ2(V αδV βω + V αωV βδ) + 2γ4(V αδV βω + V αωV βδ) + O(n−1)
= 2γ2(V αδV βω + V αωV βδ) + O(n−1) ,
where we use Covℓto denote the covariance conditioned on the sigma-algebra Fℓ= σ({Xk}k∈[ℓ]).
Finally, applying Proposition A.2, we get the desired result.
C
SDE for Softmax-based Attention
C.1
Dot-Product Attention
Dot-product attention applies the Softmax row-wise to the following matrix:
Yℓ= 1
nXℓW K
ℓW Q,⊤
ℓ
|
{z
}
W B
ℓ
X⊤
ℓ,
where W K
ℓ, W Q
ℓ∈Rn×nk are Gaussian matrices with unit variance entries, and nk is the queries and
keys’ dimension. Here we study the first two moments of Yℓ. In particular, note that Eℓ[Yℓ] = 0,
where Eℓ[ · ] = E[ · |Fℓ] denotes the conditional expectation given the sigma-algebra generated by
Fℓ= σ({Xk}k∈[ℓ]).
For the second moment we have the following Lemma.
Lemma C.1. Let
Yℓ= 1
nXℓW K
ℓW Q,⊤
ℓ
X⊤
ℓ,
be the dot-product attention parametrized by W K
ℓ, W Q
ℓ∈Rn×nk. Then:
Eℓ[Y αβY δω] = nkV αδV βω .
(24)
Proof.
Eℓ[Y αβY δω] = 1
n2
X
kk′jj′
Xα
k Xβ
k′Xδ
j Xω
j′E[W B
kk′W B
jj′].
22

For the expectation, we have that:
E[W B
kk′W B
jj′] =
nk
X
ii′
E
h
W K
ki W K
ji′W Q
k′iW Q
j′i′
i
=
X
ii′
E

W K
ki W K
ji′

E
h
W Q
k′iW Q
j′i′
i
=
X
ii′
δkjδk′j′δii′
= nkδkjδk′j′ ,
where we recall W B = W KW Q,⊤.
Hence:
Eℓ[Y αβY δω] = nk
n2
X
kk′
Xα
k Xβ
k′Xδ
kXω
k′ = nkV αδV βω .
C.2
Shaping the Softmax
In order to have infinitesimal updates in Vℓ, we would intuitively like the attention matrix to be of
the form Aαβ = δαβ + O(n−1). In the case of the usual Softmax based attention, we have:
Aℓ:= Softmax(τ −1Yℓ),
where τ −1 is a temperature parameter that regulates the entropy of the resulting categorical
distribution; τ is often chosen to be large (scale as a power of n or d), as low temperature results in
unstable training.
To transform the Softmax into the desired form of Aαβ = δαβ + O(n−1), we first center the
attention matrix around the identity matrix:
Aℓ= I + Softmax(τ −1Yℓ),
and then examine the Taylor-expansion of the Softmax w.r.t τ −1:
Aℓ= I + 1
m11⊤+ O(τ −1) .
(25)
The first few terms of the expansion provides a good approximation of Aℓwhen τ is large.
Observe that for fixed m, the term
1
m11⊤is not of O(n−1). In particular, we can show that:
γ2
m
X
δ,ω=1
Eℓ[AαδAβω]V δω = γ2V αβ + γ2
m
X
ω
V αω + γ2
m
X
δ
V δβ + γ2
m2
X
δω
V δω + O(τ −1).
where the terms scaled by
1
m leads to large incremental updates in expected value of Vℓw.r.t the
previous layer, and precludes the possibility of convergence to an SDE. Hence, we choose to re-center
the Softmax by removing the term
1
m11⊤, as follows:
Aℓ= I + Softmax(τ −1Yℓ) −1
m11⊤,
23

which admits the following Taylor-expansion:
[Softmax(τ −1Yℓ)−m−111⊤] =
1
τm[Y αβ−Y α]α,β+
1
2τ 2m
h
(Y αβ −Y
α)2 −((Y α)2 −Y α2)
i
α,β+O(τ −3),
where Y α := 1
m
P
ν Y αν, and (Y α)2 := 1
m
P
ν(Y αν)2.
Hence, up to third order:
Aℓ= I + 1
τm[Y αβ −Y α]α,β +
1
2τ 2m
h
(Y αβ −Y
α)2 −((Y α)2 −Y α2)
i
αβ + O(τ −3) .
For sufficiently large τ, the formulation above allows infinitesimal updates in Vℓ, and as we will
show rigorously in the rest of this section, permits convergence to an SDE.
C.3
Lemmas on Moments of Shaped Attention
Define:
F αβ
1
= Y αβ −Y α ,
F αβ
2
= (Y αβ −Y
α)2 −((Y α)2 −Y α2) .
Hence, Aℓcan be written as:
Aαβ
ℓ
= δαβ + 1
τmF αβ
1
+
1
2τ 2mF αβ
2
+ O(τ −3).
We now compute the moments of Aℓ. We define the following quantities:
Sαδ,βω
1
:= 1
nk
Eℓ(Y αδ −Y α)(Y βω −Y β) = 1
nk
EℓF αδ
1 F βω
1
,
Sαδ
2
:= 1
nk
Eℓ
h
(Y αδ −Y
α)2 −((Y α)2 −Y α2)
i
= 1
nk
EℓF αδ
2
,
where we recall Eℓ[ · ] = E[ · |Fℓ] is the conditional expectation given the sigma-algebra generated by
Fℓ= σ({Xk}k∈[ℓ]).
Lemma C.2 (Moments of Taylor Expansion).
Sαδ,βω
1
= V αβ 
V δω −V δ¯x −V ω¯x + V ¯x¯x
,
Sαδ
2
= V αα 
V δδ −2V δ¯x + 2V ¯x¯x −¯V

,
where ¯V = 1
m
P
ν V νν and ¯x = 1
m
P
ν xν is the average token.
Proof. Using Lemma C.1 and linearity of expectation:
Sαδ,βω
1
= 1
nk

Eℓ[Y αδY βω] −Eℓ[Y αδY β] −Eℓ[Y βωY α] + Eℓ[Y αY β]

=

V αβV δω −V αβV δ¯x −V αβV ω¯x + V αβV ¯x¯x
= V αβ 
V δω −V δ¯x −V ω¯x + V ¯x¯x
,
24

and:
Sαδ
2
= 1
nk

Eℓ
h
(Y αδ −Y
α)2i
−Eℓ
h
((Y α)2 −Y α2)
i
= 1
nk

Eℓ[(Y αδ)2] −2Eℓ[Y αδY α] + 2Eℓ[Y α2] −Eℓ[(Y α)2]

=

V ααV δδ −2V ααV δ¯x + 2V ααV ¯x¯x −V αα ¯V

= V αα 
V δδ −2V δ¯x + 2V ¯x¯x −¯V

,
where ¯V = 1
m
P
β V ββ.
Lemma C.3.
Eℓ[AαδAβω] = δαδδβω +
nk
τ 2m2 Sαδ,βω
1
+
nk
2τ 2m(δβωSαδ
2
+ δαδSβω
2 ) + O(nkτ −3) .
Proof.
Eℓ[AαδAβω] = δαδδβω + δβω
τmEℓ[Y αδ −Y α] + δαδ
τmEℓ[Y βω −Y β] +
1
τ 2m2 Eℓ(Y αδ −Y α)(Y βω −Y β)
+ δβω
2τ 2mEℓ
h
(Y αδ −Y
α)2 −((Y α)2 −Y α2)
i
+
δαδ
2τ 2mEℓ
h
(Y βω −Y
β)2 −((Y β)2 −Y β2)
i
+ O(nkτ −3)
= δαδδβω +
nk
τ 2m2 Sαδ,βω
1
+
nk
2τ 2m(δβωSαδ
2
+ δαδSβω
2 ) + O(nkτ −3) .
Lemma C.4.
Eℓ
h
Aαα′Aββ′Aδδ′Aωω′i
= δαα′δββ′δδδ′δωω′
+
nk
τ 2m2

δαα′δββ′Sδδ′,ωω′
1
+ δαα′δδδ′Sββ′,ωω′
1
+ δαα′δωω′Sββ′,δδ′
1
+ δββ′δδδ′Sαα′,ωω′
1
+ δββ′δωω′Sαα′,δδ′
1
+ δωω′δδδ′Sαα′,ββ′
1

+
nk
2τ 2m

δαα′δββ′δδδ′Sωω′
2
+ δαα′δββ′δωω′Sδδ′
2
+ δαα′δωω′δδδ′Sββ′
2
+ δωω′δββ′δδδ′Sαα′
2

+ O(nkτ −3) .
25

Proof.
Eℓ
h
Aαα′Aββ′Aδδ′Aωω′i
= Eℓ
" 
δαα′ + 1
τmF αα′
1
+
1
2τ 2mF αα′
2
 
δββ′ + 1
τmF ββ′
1
+
1
2τ 2mF ββ′
2


δδδ′ + 1
τmF δδ′
1
+
1
2τ 2mF δδ′
2
 
δωω′ + 1
τmF ωω′
1
+
1
2τ 2mF ωω′
2
 #
+ O(τ −3)
= Eℓ
"
δαα′δββ′δδδ′δωω′ +
1
τ 2m2 δαα′δββ′F δδ′
1 F ωω′
1
+
1
τ 2m2 δαα′δδδ′F ββ′
1
F ωω′
1
+
1
τ 2m2 δαα′δωω′F δδ′
1 F ββ′
1
+
1
τ 2m2 δββ′δδδ′F αα′
1
F ωω′
1
+
1
τ 2m2 δββ′δωω′F αα′
1
F δδ′
1
+
1
τ 2m2 δδδ′δωω′F αα′
1
F ββ′
1
+
1
2τ 2mδαα′δββ′δδδ′F ωω′
2
+
1
2τ 2mδαα′δββ′δωω′F δδ′
2
+
1
2τ 2mδαα′δωω′δδδ′F ββ′
2
+
1
2τ 2mδωω′δββ′δδδ′F αα′
2
#
+ O(τ −3) .
Using the linearity of expectation:
Eℓ
h
Aαα′Aββ′Aδδ′Aωω′i
= δαα′δββ′δδδ′δωω′
+
nk
τ 2m2

δαα′δββ′Sδδ′,ωω′
1
+ δαα′δδδ′Sββ′,ωω′
1
+ δαα′δωω′Sββ′,δδ′
1
+ δββ′δδδ′Sαα′,ωω′
1
+ δββ′δωω′Sαα′,δδ′
1
+ δωω′δδδ′Sαα′,ββ′
1

+
nk
2τ 2m

δαα′δββ′δδδ′Sωω′
2
+ δαα′δββ′δωω′Sδδ′
2
+ δαα′δωω′δδδ′Sββ′
2
+ δωω′δββ′δδδ′Sαα′
2

+ O(nkτ −3) .
Note that the terms above can be computed using Lemma C.2.
Lemma C.5.
Eℓ
h
Aαα′Aββ′Aδδ′Aωω′i
−Eℓ
h
Aαα′Aββ′i
Eℓ
h
Aδδ′Aωω′i
=
nk
τ 2m2

δαα′δδδ′Sββ′,ωω′
1
+ δαα′δωω′Sββ′,δδ′
1
+ δββ′δδδ′Sαα′,ωω′
1
+ δββ′δωω′Sαα′,δδ′
1

+ O(nkτ −3) .
Proof. The results is an immediate consequence of Lemma C.4 and Lemma C.3, where only the
terms that do not cancel out are kept.
C.4
Neural Covariance SDE for Stable Attention
Recall that:
Vℓ+1 = λ2Vℓ+ λγ
n√n

XℓW ⊤
ℓX⊤
ℓA⊤
ℓ+ AℓXℓWℓX⊤
ℓ

+ γ2
n2 AℓXℓWℓW ⊤
ℓX⊤
ℓA⊤
ℓ.
26

We define:
T αβ
1
:= 1
n

XℓW ⊤
ℓX⊤
ℓA⊤
ℓ+ AℓXℓWℓX⊤
ℓ
αβ
,
T αβ
2
:=
1
n√n

AℓXℓWℓW ⊤
ℓX⊤
ℓA⊤
ℓ
αβ
.
Hence, the expression for Vℓsimplifies to:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ λγ
√nT αβ
1
+ γ2
√nT αβ
2
.
We need to compute the moments for these two quantities:
Lemma C.6 (Moments of T1).
Eℓ[T αβ
1
] = 0 ,
and
Eℓ
h
T αβ
1
T δω
1
i
= 2(V αδV βω + V αωV βδ) + O(nkτ −3) .
Proof.
T αβ
1
T δω
1
= 1
n2

XℓW ⊤
ℓX⊤
ℓA⊤
ℓ+ AℓXℓWℓX⊤
ℓ
αβ 
XℓW ⊤
ℓX⊤
ℓA⊤
ℓ+ AℓXℓWℓX⊤
ℓ
δω
= 1
n2
" 
XℓW ⊤
ℓX⊤
ℓA⊤
ℓ
αβ 
XℓW ⊤
ℓX⊤
ℓA⊤
ℓ
δω
+

XℓW ⊤
ℓX⊤
ℓA⊤
ℓ
αβ 
AℓXℓWℓX⊤
ℓ
δω
+

AℓXℓWℓX⊤
ℓ
αβ 
XℓW ⊤
ℓX⊤
ℓA⊤
ℓ
δω
+

AℓXℓWℓX⊤
ℓ
αβ 
AℓXℓWℓX⊤
ℓ
δω
#
.
Let’s look at the first summand:
1
n2

XℓW ⊤
ℓX⊤
ℓA⊤
ℓ
αβ 
XℓW ⊤
ℓX⊤
ℓA⊤
ℓ
δω
= 1
n2
X
νκ
X
kk′jj′
Xα
k Wk′kXν
k′AβνXδ
j Wj′jXκ
j′Aωκ .
Hence, in expectation with respect to W:
1
n2
X
νκ
X
kk′jj′
Xα
k E

Wk′kWj′j

Xν
k′AβνXδ
j Xκ
j′Aωκ = 1
n2
X
νκ
X
kk′jj′
Xα
k δkjδk′j′Xν
k′AβνXδ
j Xκ
j′Aωκ
= 1
n2
X
νκ
X
kk′
Xα
k Xν
k′AβνXδ
kXκ
k′Aωκ
=
X
νκ
V αδV νκAβνAωκ.
An identical argument can be made for the remaining three summands. Hence, taking expectation
with respect to the Softmax weights:
Eℓ
h
T αβ
1
T δω
1
i
=
X
νκ

V αδV νκEℓ[AβνAωκ] + V αωV νκEℓ[AβνAδκ] + V βδV νκEℓ[AανAωκ] + V βωV νκEℓ[AανAδκ]

.
27

Now, using Lemma C.3:
Eℓ
h
T αβ
1
T δω
1
i
=
X
νκ
V νκ 
V αδEℓ[AβνAωκ] + V αωEℓ[AβνAδκ] + V βδEℓ[AανAωκ] + V βωEℓ[AανAδκ]

=
X
νκ
V νκ 
V αδδβνδωκ + V αωδβνδδκ + V βδδανδωκ + V βωδανδδκ

+ O(nkτ −2)
= V βωV αδ + V αωV βδ + V βδV αω + V βωV αδ + O(nkτ −2)
= 2(V αδV βω + V αωV βδ) + O(nkτ −2) .
Lemma C.7 (Moments of T2).
Eℓ[T αβ
2
] = √n
X
νκ
V νκEℓ
h
AανAβκi
,
Eℓ[T αβ
2
T δω
2 ] =
X
νκν′κ′
Eℓ[AανAβκAδν′Aωκ′]

nV νκV ν′κ′ + V νν′V κκ′ + V νκ′V ν′κ
.
Proof.
T αβ
2
:=
1
n√n

AℓXℓWℓW ⊤
ℓX⊤
ℓA⊤
ℓ
αβ
=
1
n√n
X
νκ
X
kk′j
AανXν
kWkk′Wjk′Xκ
j Aβκ.
Taking expectation with respect to W:
1
n√n

AℓXℓWℓW ⊤
ℓX⊤
ℓA⊤
ℓ
αβ
=
1
n√n
X
νκ
X
kk′j
AανXν
kE[Wkk′Wjk′]Xκ
j Aβκ
=
1
n√n
X
νκ
X
kk′j
AανXν
kδkjXκ
j Aβκ
=
1
n√n
X
νκ
X
kk′
AανAβκXν
kXκ
k
=
1
√n
X
νκ
X
k
AανAβκXν
kXκ
k
= √n
X
νκ
V νκAανAβκ.
Taking expectation w.r.t the Softmax weights, we get the desired result.
For second moment, we can take the conditional expectation:
Eℓ[T αβ
2
T δω
2 ] = 1
n3
X
νκν′κ′
X
kk′jii′j′
AανAβκAδν′Aωκ′Xν
kXκ
j Xν′
i Xκ′
j′ E[Wkk′Wjk′Wii′Wj′i′] ,
where we recall Eℓ[ · ] = E[ · |Fℓ] is the conditional expectation given the sigma-algebra generated by
Fℓ= σ({Xk}k∈[ℓ]).
Using Isserlis Theorem, we have that:
E[Wkk′Wjk′Wii′Wj′i′] = δkjδij′ + δkiδk′i′δjj′ + δkj′δk′i′δji .
28

Hence:
Eℓ[T αβ
2
T δω
2 ] = 1
n3
X
νκν′κ′
AανAβκAδν′Aωκ′
X
kk′jii′j′
Xν
kXκ
j Xν′
i Xκ′
j′

δkjδij′ + δkiδk′i′δjj′+
+ δkj′δk′i′δji

= 1
n3
X
νκν′κ′
AανAβκAδν′Aωκ′ X
kk′ii′
Xν
kXκ
k Xν′
i Xκ′
i +
X
kk′j
Xν
kXκ
j Xν′
k Xκ′
j
+
X
kk′j
Xν
kXκ
j Xν′
j Xκ′
k

= 1
n3
X
νκν′κ′
AανAβκAδν′Aωκ′
n4V νκV ν′κ′ + n3V νν′V κκ′ + n3V νκ′V ν′κ
=
X
νκν′κ′
AανAβκAδν′Aωκ′
nV νκV ν′κ′ + V νν′V κκ′ + V νκ′V ν′κ
.
By taking expectation w.r.t the Softmax parameters, we get the desired result.
Lemma C.8 (Covariance of T2).
Eℓ[T αβ
2
T δω
2 ] −Eℓ[T αβ
2
]Eℓ[T δω
2 ] = nnk
τ 2 Aαβδω + V αδV βω + V αωV βδ + O
nnk
τ 3 + nk
τ 2

,
where:
Aαβδω := 1
m2
X
νκ

V ακV δνSβκ,ων
1
+ V ακV ωνSβκ,δν
1
+ V βνV δκSαν,ωκ
1
+ V βνV ωκSαν,δκ
1

.
Proof. Using Lemma C.7, we have that:
Eℓ[T αβ
2
T δω
2 ] −Eℓ[T αβ
2
]Eℓ[T δω
2 ] =
X
νκν′κ′
Eℓ[AανAβκAδν′Aωκ′]

nV νκV ν′κ′ + V νν′V κκ′ + V νκ′V ν′κ
−n
X
νκν′κ′
V νκV ν′κ′Eℓ
h
AανAβκi
Eℓ
h
Aδν′Aωκ′i
= n
X
νκν′κ′
V νκV ν′κ′ 
Eℓ[AανAβκAδν′Aωκ′] −Eℓ
h
AανAβκi
Eℓ
h
Aδν′Aωκ′i
+
X
νκν′κ′
Eℓ[AανAβκAδν′Aωκ′]

V νν′V κκ′ + V νκ′V ν′κ
.
Now we can use Lemma C.3 and Lemma C.4 to compute the moments of A. For the second
summand, we simply have:
Eℓ
h
AανAβκAδν′Aωκ′i
= δανδβκδδν′δωκ′ + O(nkτ −2),
hence:
X
νκν′κ′
Eℓ[AανAβκAδν′Aωκ′]

V νν′V κκ′ + V νκ′V ν′κ
= V αδV βω + V αωV βδ + O(nkτ −2) .
29

For the first summand, recall from Lemma C.5 that:
Eℓ
h
Aαα′Aββ′Aδδ′Aωω′i
−Eℓ
h
Aαα′Aββ′i
Eℓ
h
Aδδ′Aωω′i
=
nk
τ 2m2

δαα′δδδ′Sββ′,ωω′
1
+ δαα′δωω′Sββ′,δδ′
1
+ δββ′δδδ′Sαα′,ωω′
1
+ δββ′δωω′Sαα′,δδ′
1

+ O(nkτ −3) .
Hence:
n
X
νκν′κ′
V νκV ν′κ′ 
Eℓ[AανAβκAδν′Aωκ′] −Eℓ
h
AανAβκi
Eℓ
h
Aδν′Aωκ′i
= nnk
τ 2m2
X
νκν′κ′
V νκV ν′κ′
δανδδν′Sβκ,ωκ′
1
+ δανδωκ′Sβκ,δν′
1
+ δβκδδν′Sαν,ωκ′
1
+ δβκδωκ′Sαν,δν′
1

+ O(nnkτ −3)
= nnk
τ 2
1
m2
X
νκ

V ακV δνSβκ,ων
1
+ V ακV ωνSβκ,δν
1
+ V βνV δκSαν,ωκ
1
+ V βνV ωκSαν,δκ
1

|
{z
}
Aαβδω
+ O(nnkτ −3).
We are now ready to re-state and proof of Theorem 4.2.
Theorem 4.2.
Let Xℓbe the hidden layers of a residual attention network defined in Eq. 1 with
shaped attention in Eq. 9, parameters λ2 + γ2 = 1 and τ = τ0√nnk, where λ, γ, τ0 all do not depend
on d, n. Then the feature covariance Vℓconverges locally to the solution of the following SDE (in the
sense of Definition 4.1)
dVt = b(Vt)dt + Σ(Vt)1/2dBt ,
V0 = 1
nX0X⊤
0 ,
where the drift has the following form
b(V αβ
t
) = γ2
τ 2
0

1
m2
m
X
ν,κ=1
V νκSαν,βκ
1
+ 1
2m
m
X
ν=1
(V βνSαν
2
+ V ανSβν
2 )


α≤β
,
the diffusion coefficient is defined by Σ(V ) = γ2(2 −γ2)Σlin(V ) + γ4τ −2
0 [Aαβ,δω]α≤β,δ≤ω, and
Aαβ,δω := 1
m2
m
X
ν,κ=1

V ακV δνSβκ,ων
1
+ V ακV ωνSβκ,δν
1
+ V βνV δκSαν,ωκ
1
+ V βνV ωκSαν,δκ
1

.
Proof. Recall that:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ λγ
√nT αβ
1
+ γ2
√nT αβ
2
.
30

Drift.
Summing and subtracting Eℓ[T2] (and using Lemma C.7), we have that:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ γ2 X
νκ
V νκEℓ
h
AανAβκi
+ λγ
√nT αβ
1
+ γ2
√n

T αβ
2
−Eℓ[T2]

,
where we recall Eℓ[ · ] = E[ · |Fℓ] is the conditional expectation given the sigma-algebra generated by
Fℓ= σ({Xk}k∈[ℓ]).
Re-stating Lemma C.3, we have that:
Eℓ[AανAβκ] = δανδβκ +
nk
τ 2m2 Sαν,βκ
1
+
nk
2τ 2m(δβκSαν
2
+ δανSβκ
2 ) + O(nkτ −3).
Plugging in the expression, and using λ2 + γ2 = 1, we have that the drift is:
λ2V αβ
ℓ
+ γ2V αβ
ℓ
+ γ2 nk
τ 2m
X
νκ
V νκ
 1
mSαν,βκ
1
+ 1
2(δβκSαν
2
+ δανSβκ
2 )

+ O(nkτ −3)
= V αβ
ℓ
+ γ2 nk
τ 2
 
1
m2
X
νκ
V νκSαν,βκ
1
+ 1
2m
X
ν
(V βνSαν
2
+ V ανSβν
2 )
!
+ O(nkτ −3) .
From here, it is evident that in order to have the drift scaling as O(1/n) we need to choose:
τ 2 = τ 2
0 nnk,
(26)
where τ0 > 0 is a constant.
Covariance.
Recall that:
V αβ
ℓ+1 = λ2V αβ
ℓ
+ γ2 X
νκ
V νκEℓ
h
AανAβκi
+ λγ
√nT αβ
1
+ γ2
√n

T αβ
2
−Eℓ[T2]

.
Furthermore, we have set λ2 + γ2 = 1 and τ 2 = τ 2
0 nnk to have the right scaling for the drift.
What’s left to is to compute the conditional covariance for Vℓ+1 given Fℓ. Noting that T αβ
1
, T δω
2
are uncorrelated, i.e. Eℓ
h
T αβ
1
T δω
2
i
= 0 (similarly to the case of Resnet with shaped ReLU Lemma
B.3), we have that:
Covℓ

V αβ
ℓ+1, V δω
ℓ+1

= Eℓ
h
λγT αβ
1
+ γ2T αβ
2
−γ2Eℓ[T αβ
2
]
 
λγT δω
1
+ γ2T δω
2
−γ2Eℓ[T δω
2 ]
i
= λ2γ2Eℓ
h
T αβ
1
T δω
1
i
+ γ4Eℓ
h
T αβ
2
T δω
2
i
−γ4Eℓ
h
T αβ
2
i
Eℓ
h
T δω
2
i
,
where we use Covℓto denote the conditional covariance given the sigma-algebra generated by
Fℓ= σ({Xk}k∈[ℓ]).
Using Lemma C.6 and Lemma C.8, we have that:
Covℓ

V αβ
ℓ+1, V δω
ℓ+1

= λ2γ2 
2(V αδV βω + V αωV βδ)

+ γ4
 1
τ 2
0
Aαβδω + V αδV βω + V αωV βδ

= γ2(2 −γ2)

V αδV βω + V αωV βδ
+ γ4
τ 2
0
Aαβδω .
Now we can apply Proposition A.2 for locally Lipschitz drift and covariance coefficients, which gives
us the desired result in local convergence in the Skorohod topology.
31

We will also restate and prove Corollary 4.3.
Corollary 4.3 (Shaped Transformer Covariance SDE). Let Xℓbe the hidden layers of a shaped
transformer defined in Eq. 11 with parameters λ2 + γ2 = 1 and τ = τ0√nnk, where λ, γ, τ0 all do
not depend on d, n. Then the feature covariance Vℓconverges locally to the solution of the following
SDE (in the sense of Definition 4.1)
dVt = [b(Vt) + bres(Vt)] dt + [Σ(Vt) + Σres(Vt)]1/2 dBt ,
(12)
where the coefficients are defined in Theorem 3.2 and Theorem 4.2.
Proof. To combine the results of Theorem 3.2 and Theorem 4.2, it is sufficient to combine the
following (simplified) iterated Markov updates into one Markov chain
Uℓ= Vℓ+ b(Vℓ)
n
+ Σ(Vℓ)1/2ξℓ
√n
,
Vℓ+1 = Uℓ+ bReLU(Uℓ)
n
+ Σlin(Uℓ)1/2ξ′
ℓ
√n
,
(27)
where ξℓ, ξ′
ℓare independent zero mean and identity covariance random vectors.
Since in the limit, we have that either updates are infinitesimal, i.e.
|Uℓ−Vℓ| n→∞
−−−→0
almost surely,
(28)
then we can write bReLU(Uℓ) = bbReLU,n(Vℓ, ωℓ), where eventually we have that
lim
n→∞Eℓ
h
bbn(Vℓ, ωℓ)
i
= b(Vℓ) ,
(29)
so it will not contribute towards affecting the limit. Here we recall Eℓ[ · ] = E[ · |Fℓ] is the conditional
expectation given the sigma-algebra generated by Fℓ= σ({Xk}k∈[ℓ]). We can treat Σlin(Uℓ) similarly
to get the Markov chain update
Vℓ+1 = Vℓ+ b(Vℓ) + bbReLU,n(Vℓ, ωℓ)
n
+ Σ(Vℓ)1/2ξℓ
√n
+
bΣlin,n(Vℓ)1/2ξ′
ℓ
√n
,
(30)
which converges to the following SDE with two Brownian motions using Proposition A.2
dVt = [b(Vt) + bres(Vt)] dt + Σ(Vt)1/2 dBt + Σres(Vt)1/2dB′
t .
(31)
Observe that since the two Brownian motions Bt, B′
t are independent, it’s equivalent to write
Σ(Vt)1/2 dBt + Σres(Vt)1/2dB′
t
d= [Σ(Vt) + Σres(Vt)1/2]1/2 dBt .
(32)
We recover the desired results from considering a more general form of the iterated Markov
updates as in Proposition A.2, which do not hinder the above derivation.
D
Preliminary Experiments
We perform preliminary experiments to understand the effect of shaped attention on training deep
Transformer architectures. In particular, we consider a pre-training masked language modeling task,
where we mask 15% of the tokens. We use a subset of the English Wikipedia 20220301.en and English
bookcorpus datasets [63, 64]. As a baseline, we adopt a Pre-LN Transformer encoder architecture
32

with 18 or 24 blocks. For the residual feedforward layer, we shape the ReLU activation according
to Eq. 4, by changing its negative slope to s−= 1 −1/√n instead of 0. We then incorporate our
shaped attention by replacing the attention mechanism as dictated in Eq. 9. We also add scalar
multipliers γ1, γ2 ∈R both to the identity and centering terms of the shaped attention:
Aℓ= γ1I + Softmax(τ −1Yℓ) −γ2
1
m11⊤,
τ = τ0
√nnk ,
(33)
and propose two ways to set γ1, γ2 during training. In both alternatives we initialize γ1, γ2 = 1, thus
leveraging the stability properties of shaped attention at initialization. During training, we either:
1. Recover. Linearly decrease γ1, γ2 to zero with in the first 4000 steps, thus recovering the
standard attention layer. Apply the same schedule for the shaped-ReLU slope s−, recovering
the usual ReLU activation.
This approach recovers the vanilla Transformer architecture
(without LayerNorm).
2. Learn. Learn all the shaping parameters γ1, γ2 and s−.
The intuitive rationale behind these choices is that at initialization we want good signal propagation
and a non-degenerate covariance (according to our theory, this requires the shaped attention and
ReLU). On the other hand, we also allow the model to more dynamically make use of the nonlinearity
during training to modify the correlation structure with Recover or Learn. We report that without
either adjustment, shaped attention is still trainable but at much slower rates.
We also incorporate the
1
√n factor into the initialization of the queries and keys weights by
decreasing the variance of their entries by a factor of 1
n. This allows us to not re-tune the learning
rate for the queries and keys. We stress that at at initialization, the two formulations (τ = √nnk
and τ = √nk with decreased weight’s variance) are equivalent. In both alternatives we train all the
skip connection parameters λ, γ, and initialize γ in the grid (0.05, 0.1, 0.2) and set τ0 = 1. We report
training instabilities (loss divergence) for larger values of γ. All models —including the baselines
— use Adam [65] with learning rate warmup of 4000 steps, the learning rate is tuned in the grid
(0.0001, 0.0005, 0.001, 0.005). We report the train/test loss after 100K optimization steps, averaged
over 2 random seeds. All the other experimental details can be found in Appendix D.1.
The Shaped Transformer is Trainable.
In Table 1, we compare the train/test loss of the two
variant of shaped attention with the baseline Pre-LN model. Notice that our model (in both variants)
achieves comparable performance to standard Transformers across all the reported values of γ.
The Shaped Transformer Allows for Larger Learning Rates.
In Figure 5, we compare the
training loss curves of standard Transformers with our shaped variants when varying the learning
rate. For Softmax attention-based Transformers, we observe a degradation in training speed at
lr = 0.0005, and a divergent behavior at lr = 0.001. On the other hand, shaped attention-based
Transformers exhibit the same convergent behaviour across these learning rates. We observe the
same phenomenon for both variants of shaped attention proposed here, and for all γ ∈[0.005, 0.1, 0.2].
For completeness, we tried with even larger learning rates, observing that the network’s training
diverges at lr = 0.005.
Entropy Collapse for Large Learning rates.
To understand the sources of training instability,
we keep track of the entropy of the probability distribution induced by the Softmax, as it has been
observed that the Transformer’s training is unstable in the low-entropy regime [51]. The entropy
33

d = 18
d = 24
γ
Train Loss
Test Loss
Train Loss
Test Loss
Learn
0.05
2.26±0.01
2.10±0.02
—
—
0.1
2.21±0.02
2.08±0.03
2.21±0.02
2.02±0.03
0.2
2.20±0.02
2.07±0.01
2.28±0.01
2.09±0.03
Recover
0.05
2.22±0.03
2.04±0.01
—
—
0.1
2.19±0.04
2.05±0.02
2.18±0.02
1.99±0.02
0.2
2.19±0.03
2.03±0.01
2.17±0.02
2.00±0.01
Baseline
2.26±0.04
2.19±0.01
2.16±0.02
2.02±0.04
Table 1: train/test loss of the proposed shaped attention, both in the "Recover" and "Learn"
alternatives. Baseline refers to the vanilla Pre-LN Transformer. We report the best run under the
learning rates (0.0001, 0.0005, 0.001, 0.005), averaged over 2 random seeds. We do not perform the
case γ = 0.05 for the deeper network (d = 24). We include the confidence interval of ± one standard
deviation.
(a) d = 18
(b) d = 24
Figure 5: Training loss curves across 3 different learning rates for standard Transfomers (dashed
lines) and our shaped attention in the recover setting (solid lines). The two plots indicate different
depths (18 blocks on the left, 24 blocks on the right).
is calculated for each row of the Softmax matrix, and it is averaged across rows and heads. The
results are in Fig. 6. Notice how for the large learning rate regime observed in Fig. 5, the entropy
collapses for the baseline model, but not for the proposed shaped Transformer. Entropy collapse
indicates that the Softmax distribution degenerates to a point mass, which is itself caused by large
logits. Remarkably, this phenomenon does not affect the recover setting, despite recovering the
Transformer architecture (without layer normalization) after the warm-up period.
D.1
Experimental details
Dataset.
We use a subset of the English Wikipedia 20220301.en and English bookcorpus datasets
[63, 64]. The sentences are tokenized using by pre-training a tokenizer on the training set. We use a
vocabulary size of 32000, and a maximum sequence length of 128 tokens.
34

Figure 6: Dynamics of the mean entropy across heads for selected layers (first and seventh) and
learning rates. (Above): shaped attention in recover setting. (Below): Baseline transformers. Notice
that at lr = 0.001 the entropy collapses for the baseline model.
Model parameters.
We use an embedding size of n = 768 and 8 multi-attention heads. The
batch size is fixed to 32 sequences. All the initial weights are sampled from N(0, n−1), with the
exception of the queries and keys’ weights W K, W Q in the shaped attention case, that are sampled
from N(0, n−3/2) (as explained in Appendix D). The feedforward layer maps the n = 768-dimensional
embedding to the larger dimension 3072, as in the Hugging face implementation of Bert [66].
Optimization.
We train using Adam [65] with betas parameters (0.9, 0.999) and learning rate
chosen in the grid (0.0001, 0.0005, 0.001, 0.005). We do not use weight decay.
Computational Resources.
The experiments are executed on Nvidia DGX-1 GPU nodes equipped
with 4 20-core Xeon E5-2698v4 processors, 512 GB of memory and 8 Nvidia V100 GPUs.
E
Additional Figures
35

Figure 7: Kernel density estimate and histogram of covariances from the covariance SDE in
Theorem 4.2 and shaped attention NN (Eq. 9). Simulated with n = 200, d = 150, γ = 1/
√
8, τ0 =
1, V αβ
0
= 0.2, SDE step size 0.01, and 212 samples.
36

Figure 8: Median of the stopping time capped at 1, of the shaped attention neural network with
respect to its parameters γ and τ0. Stopping time is defined as t∗= d∗/n with d∗the maximum
depth beyond which one of the eigenvalues of the covariance matrix exceeds 104 or drops below 10−4.
Simulated with n = d = 200, and 100 samples used to estimate the median. To demonstrate the
potential numerical instabilities, we had to choose an adversarial set of parameters: in particular, an
unrealistically large norm (approx. 10√n) for the initial tokens X0, which enlarges the eigenvalues
of V0 to the order of 100.
37

