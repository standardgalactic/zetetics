COMPOSABLE FUNCTION-PRESERVING EXPANSIONS
FOR TRANSFORMER ARCHITECTURES
Andrea Gesmundo1, Kaitlin Maile1,2
1 Google DeepMind, 2 IRIT, University of Toulouse,
{agesmundo,kmaile}@google.com
ABSTRACT
Training state-of-the-art neural networks requires a high cost in terms of compute
and time. Model scale is recognized to be a critical factor to achieve and improve
the state-of-the-art. Increasing the scale of a neural network normally requires
restarting from scratch by randomly initializing all the parameters of the model,
as this implies a change of architecture’s parameters that does not allow for a
straightforward transfer of knowledge from smaller size models.
In this work, we propose six composable transformations to incrementally increase
the size of transformer-based neural networks while preserving functionality, al-
lowing to expand the capacity of the model as needed. We provide proof of exact
function preservation under minimal initialization constraints for each transfor-
mation. The proposed methods may enable efficient training pipelines for larger
and more powerful models by progressively expanding the architecture throughout
training. 1
1
INTRODUCTION
Transformer-based neural networks have gained widespread attention in recent years due to their im-
pressive performance. The Transformer architecture, introduced by Vaswani et al. (2017), has become
the standard for many natural language processing (NLP) tasks, including machine translation, text
generation, and question answering. The success of transformer-based models is not limited to NLP:
they have also been applied to various other domains, including computer vision, speech recognition,
and recommendation systems. The largest and most performant of these models, large language
models (LLMs) and vision and multimodal foundation models, are reaching billions to trillions of
parameters (Dehghani et al., 2023; Touvron et al., 2023; Rae et al., 2021; Raffel et al., 2020).
However, each new model is generally trained from scratch, without reusing the capabilities acquired
by previously trained smaller models. Furthermore, the size of the model is constant throughout
training. The computational cost of training scales quadratically with model size due to the necessary
increase in amount of training data (Hoffmann et al., 2022; Google, 2023; Kaplan et al., 2020). The
ability to reuse parameters of a pretrained model or dynamically increase a model’s size during
training could thus reduce the overall cost of training, but how to accomplish parameter reuse
effectively without losing training progress is not straightforward.
To address these limitations, we propose parameter expansion transformations for transformer-based
models that are exactly function preserving. These transformations increase the model size and
thus the potential capacity of the model without changing its functionality, permitting continued
training. These composable transformations operate on independent dimensions of the architecture,
allowing for fine-grained architectural expansion.
Some previous works have also proposed function preserving parameter expansion transformations
for transformer-based models (Chen et al., 2022; Shen et al., 2022; Wang et al., 2023; Mazzawi
et al., 2023), extending from techniques for smaller convolutional and dense models (Chen et al.,
2016; Evci et al., 2022). Our framework is so far the most comprehensive and composable set
of function preserving transformations.
1Implementation of the proposed transformations and empirical tests of the function preservation property
are available at: http://goo.gle/TransformerExpansions.
1
arXiv:2308.06103v1  [cs.LG]  11 Aug 2023

N ✕
Input
Input
Embedding
Positional
Encoding
Multi Head 
Attention
Normalization
Normalization
Multi Layer
Perceptron
Head
Transformer Layer
Output
Linear
Figure 1: Representation of a standard Neural Network based on the Transformer architecture.
The contributions of this paper are six composable function preserving transformations applicable
to Transformer architectures: 1) size of MLP internal representation, 2) number of attention heads,
3) size of the attention heads output representation, 4) size of the attention input representation, 5)
size of the transformer layers input/output representations, 6) number of layers, summarized in Table
1. For each transformation, we provide proof of how the exactly function preserving property is
achieved with a minimal set of constraints on the initialization of the added parameters.
2
TRANSFORMER ARCHITECTURE FORMALIZATION
This presentation is based on a particular instantiation of the transformer architecture: applica-
tions to variants (e.g. Encoder+Decoder, different normalization placement) can be obtained with
simple extensions.
Figure 1 represents the standard Transformer architecture (Vaswani et al., 2017). The Input Embedding
module maps the arbitrary input modality (e.g. image, text) into a bidimensional tensor I
s×h, where s is
the sequence dimension and h is the hidden dimension. The TransformerArchitecture(·) is defined
as a function that maps:
I
s×h →O
s×o, where o is the hidden dimension of the output representation.
The Head component represents the output modality specific logic that maps O
s×o into a specific
output (e.g. a distribution over classes or text tokens).
TransformerArchitecture(·) is defined as:
TransformerArchitecture( I
s×h) = TransformerLayer◦N( I
s×h+ P
s×h) × Wout
h×o ,
(1)
where Wout
h×o are the parameters of the final linear projection, P
s×h are the positional embedding
parameters, and TransformerLayer◦N(·) represents the recursive application of N transformer
2

layers. The nth transformer layer is defined as:
TransformerLayern( In
s×h
) = I
′
n
s×h
+ MLPn(NormMLP
n
(I
′
n
s×h
)),
I
′
n
s×h
= In
s×h
+ MHAn(NormMHA
n
( In
s×h
))
∀n ∈[1, N].
(2)
MLPn(·) is the Multi Layer Perceptron (i.e. feed forward layers), defined as:
MLPn( X
s×h) = ReLU( X
s×h × Wl1
n
h×p
+ Bl1
n
s×p
) × Wl2
n
p×h
+ Bl2
n
s×h
,
(3)
where Wl1
n is the matrix of parameters of the first fully connected layer and Bl1
n are its bias parameters
broadcasted along the sequence dimension: Bl1
n
s×h
= 1
s×1 × bl1
n
1×h
. Wl2
n and Bl2
n are the parameters of
the second fully connected layer. The broadcast operator applied to the bias parameters is omitted
for simplicity. The size of the internal dimension of the MLP component is represented with p.
The considered architecture instantiation assumes the uses of ReLU(·) (Glorot et al., 2011) as a
non-linearity function as this is a common choice. The proposed transformations also maintain the
function preserving property with alternative choices such as GELU(·) (Hendrycks & Gimpel, 2016).
MHAn(·) is the Multi Head Attention defined as:
MHAn( X
s×h) =

H1
s×v · · · HE
s×v

× WO
n
(E·v)×h
,
He
s×v = Attention( X
s×h×WQ
n,e
h×k
, X
s×h×WK
n,e
h×k
, X
s×h×WV
n,e
h×v
)
∀e ∈[1, E],
Attention( Q
s×k
, K
s×k, V
s×v) = Softmax( 1
√
k · Q
s×k
× K⊤
k×s) × V
s×v,
(4)
where E is the number of heads, k is the hidden dimension of key, K, and query, Q, and v is the hidden
dimension of value, V. K⊤represents the transpose of K. The concatenation of the representations
produced by the attention heads is represented with the block notation: C = [A B].
As the normalization function in each component, we use RMSNorm (Zhang & Sennrich, 2019). The
original definition of the transformer architecture uses LayerNorm, but RMSNorm has become a more
common design choice in large language models (Raffel et al., 2020; Rae et al., 2021; Touvron et al.,
2023). The key difference is only scaling the variance of the inputs and using scaling parameters,
rather than also subtracting their mean and using bias parameters. Thus, we define Norm(·) as:
Normc
n( X
s×h) =

xi,j · gc
n,j
q
1
h
Ph
γ=1(xi,γ)2
| i∈[1, s] ∧j ∈[1, h]

∀n∈[1, N] ∧c∈{MHA, MLP}, (5)
where gc
n
1×h
identifies the vector of the scaling parameters of the Norm(·) instance of component
c in the nth layer.
3
FUNCTION PRESERVING TRANSFORMATIONS
In this section, we define six function preserving transformations that can be applied to extend a
transformer architecture to increase its scale while keeping its function unaltered, thus allowing to
introduce new parameters to store additional knowledge while preserving the knowledge acquired
so far. Each transformation is defined to target the expansion of one of the hyper-parameters of the
architecture: p, E, v, k, h, and N, each controlling a distinct dimension of the scaling. The proposed
transformations are summarized in Table 1.
3

Name
Transformation
Function preserving constraint
Sec. 3.1:
MLP
expansion
Def. 3.1: to increase the MLP internal dimension p to ˆp, add ˆp −p
columns to the the first MLP weight matrix and bias vector and add
ˆp −p rows to the second MLP weight matrix.
Thrm. 3.1: zero initialize the new ˆp −p rows
of the second MLP weight matrix.
Sec. 3.2:
Head
addition
Def. 3.2: to increase the number of attention heads E, per head added,
add v rows to the MHA output weight matrix.
Thrm. 3.2: zero initialize the new v rows of the
MHA output weight matrix.
Sec. 3.3:
Heads
expansion
Def. 3.3: to increase the attention head representation dimension v to
ˆv, add ˆv −v columns to the value weight matrix and insert ˆv −v rows
to each of E splits of the MHA output weight matrix.
Thrm. 3.3: zero initialize the new ˆv −v rows
inserted to each of E splits of the MHA output
weight matrix.
Sec. 3.4:
Attention
expansion
Def. 3.4: to increase the key/query representation dimension k to ˆk,
add ˆk −k columns to the key/query weight matrices and scale the key
weight matrix by
p
ˆk/
√
k.
Thrm. 3.4: zero initialize the new ˆk−k columns
of the key weight matrix.
Sec. 3.5:
Hidden
dimension
expansion
Def. 3.5: to increase the transformer hidden dimension h to ˆh, add
ˆh −h columns to the positional encoding matrix, norm scaling vector,
second MLP weight matrix and bias vector, MHA output weight matrix,
and input representation matrix; add ˆh −h rows to the transformer
output weight matrix, first MLP weight matrix, and key/query/value
weight matrices; scale norm scaling vector by
√
h/
p
ˆh.
Thrm. 3.5: zero initialize the new ˆh−h columns
of the positional encoding matrix, norm scaling
vector, second MLP weight matrix and bias
vector, and MHA output weight matrix.
Sec. 3.6:
Layer
addition
Def. 3.6: to increase the number of layers N to ˆ
N, per layer added,
insert new layer at position n and increment index of all following
layers.
Thrm. 3.6: zero initialize the new layer’s MHA
output weight matrix and weight matrix and
bias vector of the second MLP layer.
Table 1: Summary of proposed function preserving transformations.
For each transformation, we define how the existing parameters must be expanded and propose a set
of minimal initialization constraints to obtain the function preserving property with proof.
The presented transformations can be combined to allow the joint extension of multiple dimen-
sions of the transformer architecture. Furthermore, different subsets of such transformations can
be applied incrementally, interleaving training iterations, as well as independently to different
parts of the architecture.
Symbols denoting parameters, representations, and functions resulting from the application of the
transformation discussed in each of the following subsection are indicated with the “hat” symbol: ˆ.
3.1
MLP EXPANSION
The MLP expansion transformation can be applied to expand the scale of the MLP by expanding the
dimension of its internal representation. This scaling dimension is controlled by the hyper-parameter
p introduced in Equation 3.
Definition 3.1 (MLP expansion). Given a Transformer model as defined in Section 2, the internal
dimension of MLPn ∀n∈[1, N] can be increased from p to ˆp by applying the following parameter-
matrix transformations:
Wl1
n
h×p
7→ˆ
Wl1
n
h×ˆp
:=
"
Wl1
n
h×p
MW l1
n
h×(ˆp−p)
#
,
(6)
bl1
n
1×p
7→ˆbl1
n
1×ˆp
:=
"
bl1
n
1×p
mbl1
n
1×(ˆp−p)
#
,
(7)
4

Wl2
n
p×h
7→ˆ
Wl2
n
ˆp×h
:=


Wl2
n
p×h
MW l2
n
(ˆp−p)×h

,
(8)
where MW l1
n
h×(ˆp−p)
, mbl1
n
1×(ˆp−p)
, and MW l2
n
(ˆp−p)×h
are matrices of the specified shape. For the purpose of defining
of the MLP expansion transformation, the values of these matrices can be assumed to be arbitrary.
Constraints on their initializer functions are introduced below to achieve the function preserving
property.
No other modifications to the Transformer architecture are required since the MLPn(·) function
(Equation 3) still inputs and outputs matrices of shape s × h after the transformation.
Theorem 3.1 (Function preserving MLP expansion).
MW l2
n
(ˆp−p)×h
:=
0
(ˆp−p)×h
(9)
=⇒
ReLU( X
s×h × Wl1
n
h×p
+ Bl1
n
s×p
) × Wl2
n
p×h
+ Bl2
n
s×h
= ReLU( X
s×h × ˆ
Wl1
n
h×p
+ ˆBl1
n
s×p
) × ˆ
Wl2
n
p×h
+ Bl2
n
s×h
(10)
Informally: zero initializing MW l2
n
(ˆp−p)×h
implies the function preservation property for the MLP expan-
sion transformation.
See Appendix A.1 for proof.
The MLP expansion transformation can be applied to all the MLP blocks to maintain the MLP
internal dimension uniformly across all the layers. However, it can also be applied to only a subset of
the layers independently to allow experimenting with different capacity at different depths.
3.2
HEAD ADDITION
The Head addition transformation can be applied to add new heads in a MHA component. This
scaling dimension is controlled by the hyper-parameter E introduced in Equation 4.
Definition 3.2 (Head addition). Given a Transformer model as defined in Section 2, a new
head can be added to MHAn(·) ∀n ∈[1, N] by introducing new input projection matrices:
WQ
n,E+1
h×k
, WK
n,E+1
h×k
, WV
n,E+1
h×v
and applying the following parameter-matrix transformation to the
output projection matrix:
WO
n
(E·v)×h
7→
ˆ
WO
n
((E+1)·v)×h
:=


WO
n
(E·v)×h
MWO
n
v×h

.
(11)
No other modifications to the Transformer architecture are required since the MHAn(·) function
(Equation 4) still inputs and outputs matrices of shape s × h after the transformation.
The Head addition transformation is defined to add one new head. The transformation can be applied
multiple times to add an arbitrary number of new heads.
5

Theorem 3.2 (Function preserving head addition).
MWO
n
v×h
:= 0
v×h =⇒

H1
s×v · · · HE
s×v

× WO
n
(E·v)×h
=
"
H1
s×v · · · H(E+1)
s×v
#
×
ˆ
WO
n
((E+1)·v)×h
(12)
Informally: zero initializing MWO
n
v×h
implies the function preservation property for the head addition
transformation.
See Appendix A.2 for proof.
The head addition transformation can be applied to all the MHA blocks to maintain the number of
MHA heads uniformly across all the layers. However, it can also be applied to only a subset of the
layers independently to allow experimenting with different capacity at different depths.
3.3
HEADS EXPANSION
The Heads expansion transformation can be applied to expand the dimension of the representation
generated by each attention heads. This scaling dimension is controlled by the hyper-parameter
v introduced in Equation 4.
Definition 3.3 (Heads expansion). Given a Transformer model as defined in Section 2, the dimension
of representation generated by the attention heads, He
s×v ∀e∈[1, E], of MHAn ∀n∈[1, N] can be
increased from v to ˆv by applying the following parameter-matrix transformations:
WV
n,e
h×v
7→ˆ
WV
n,e
h×ˆv
:=
"
WV
n,e
h×v
MWV
n,e
h×(ˆv−v)
#
∀e ∈[1, E],
(13)
WO
n,e
v×h
7→ˆ
WO
n,e
ˆv×h
:=


WO
n,e
v×h
MWO
n,e
(ˆv−v)×h


∀e ∈[1, E],
(14)
where WO
n,e
v×h
is the eth “split” of
WO
n
(E·v)×h
along the (E · v) dimension:
WO
n
(E·v)×h
:=


...
WO
n,e
v×h
...
| e ∈[1, E].


(15)
No other modifications to the Transformer architecture are required since the MHAn(·) function
(Equation 4) still inputs and outputs matrices of shape s × h after the transformation.
Theorem 3.3 (Function preserving heads expansion).
MWO
n,e
(ˆv−v)×h
:=
0
(ˆv−v)×h =⇒

H1
s×v · · · HE
s×v

× WO
n
(E·v)×h
=

ˆH1
s×ˆv
· · · ˆHE
s×ˆv

×
ˆ
WO
n
(E·ˆv)×h
(16)
where:
ˆHe
s×ˆv
= Attention( X
s×h×WQ
n,e
h×k
, X
s×h×WK
n,e
h×k
, X
s×h× ˆ
WV
n,e
h×ˆv
)
(17)
6

Informally: zero initializing MWO
n,e
(ˆv−v)×h
implies the function preservation property for the head expansion
transformation.
See Appendix A.3 for proof
The heads expansion transformation can be applied to all heads of all the MHA blocks to maintain
the attention head representation dimension uniformly across all the layers. However, it can also
be applied to only a subset of the layers or even a subset of attention heads independently to allow
experimenting with different capacity at different parts of the architecture.
3.4
ATTENTION EXPANSION
The Attention expansion transformation can be applied to expand the key and query representations
whose inner product produces the attention weights matrix. This scaling dimension is controlled
by the hyper-parameter k introduced in Equation 4.
Definition 3.4 (Attention expansion). Given a Transformer model as defined in Section 2, the
dimension of representations generating the attention weights of MHAn ∀n∈[1, N] can be increased
from k to ˆk by applying the following parameter-matrix transformations:
WQ
n,e
h×k
7→ˆ
WQ
n,e
h×ˆk
:=

WQ
n,e
h×k
MWQ
n,e
h×(ˆk−k)


∀e ∈[1, E],
(18)
WK
n,e
h×k
7→ˆ
WK
n,e
h×ˆk
:=


p
ˆk
√
k
· WK
n,e
h×k
MWK
n,e
h×(ˆk−k)


∀e ∈[1, E].
(19)
Theorem 3.4 (Function preserving attention expansion).
MWK
n,e
h×(ˆk−k)
:=
0
h×(ˆk−k)
(20)
=⇒
Attention( X
s×h×WQ
n,e
h×k
, X
s×h×WK
n,e
h×k
, X
s×h×WV
n,e
h×v
) = Attention( X
s×h× ˆ
WQ
n,e
h×ˆk
, X
s×h× ˆ
WK
n,e
h×ˆk
, X
s×h×WV
n,e
h×v
)
(21)
Informally: zero initializing MWK
n,e
h×(ˆk−k)
implies the function preservation property for the attention
expansion transformation.
See Appendix A.4 for proof.
In most transformer implementations, k = v. In such cases, the attention expansion may be
performed jointly with the head expansion.
The attention expansion transformation can be applied to all heads of all the MHA blocks to maintain
the key/query representation dimension uniformly across all the layers. However, it can also be
applied to only a subset of the layers or even a subset of attention heads independently to allow
experimenting with different capacity at different parts of the architecture.
3.5
HIDDEN DIMENSION EXPANSION
The Hidden dimension expansion transformation can be applied to expand the dimension of the
representation produced by the transformer layers. This scaling dimension is controlled by the
hyper-parameter h introduced in Equation 1.
7

Definition 3.5 (Hidden dimension expansion). Given a Transformer model as defined in Section 2,
the dimension of the transformer layers’ input/output representation can be increased from h to ˆh by
applying the following parameter-matrix transformations:
P
s×h 7→ˆP
s×ˆh
:=
"
P
s×h
MP
s×(ˆh−h)
#
,
(22)
Wout
h×o 7→ˆ
Wout
ˆh×o
:=


Wout
h×o
MW out
(ˆh−h)×o

,
(23)
gc
n
1×h
7→ˆgc
n
1×ˆh
:=
" √
h
p
ˆh
· gc
n
1×h
mg,c
n
1×(ˆh−h)
#
∀n∈[1, N] ∧c∈{MHA, MLP},
(24)
Wl1
n
h×p
7→ˆ
Wl1
n
ˆh×p
:=


Wl1
n
h×p
MW l1
(ˆh−h)×p

∀n∈[1, N],
(25)
Wl2
n
p×h
7→ˆ
Wl2
n
p×ˆh
:=
"
Wl2
n
p×h
MW l2
n
p×(ˆh−h)
#
∀n∈[1, N],
(26)
bl2
n
1×h
7→ˆbl2
n
1×ˆh
:=
"
bl2
n
1×h
mbl2
n
1×(ˆh−h)
#
∀n∈[1, N],
(27)
WQ
n,e
h×k
7→ˆ
WQ
n,e
ˆh×k
:=


WQ
n,e
h×k
MWQ
n,e
(ˆh−h)×k

∀n∈[1, N] ∧e∈[1, E],
(28)
WK
n,e
h×k
7→ˆ
WK
n,e
ˆh×k
:=


WK
n,e
h×k
MWK
n,e
(ˆh−h)×k

∀n∈[1, N] ∧e∈[1, E],
(29)
WV
n,e
h×v
7→ˆ
WV
n,e
ˆh×v
:=


WV
n,e
h×v
MWV
n,e
(ˆh−h)×v

∀n∈[1, N] ∧e∈[1, E],
(30)
WO
n
(E·v)×h
7→
ˆ
WO
n
(E·v)×ˆh
:=
"
WO
n
(E·v)×h
MWO
n
(E·v)×(ˆh−h)
#
∀n∈[1, N],
(31)
8

and modifying the embedding function to produce an extended input representation:
ˆI
s×ˆh
:=
"
I
s×h
MI
s×(ˆh−h)
#
.
(32)
For example, a token embedding table can be expanded by adding (ˆh −h) randomly initialized
columns, mapping the same vocabulary into an extended embedding.
Theorem 3.5 (Function preserving hidden dimension expansion).
MP
s×(ˆh−h)
:=
0
s×(ˆh−h)
(33)
MW l2
n
p×(ˆh−h)
:=
0
p×(ˆh−h)
∀n∈[1, N]
(34)
mbl2
n
1×(ˆh−h)
:=
0
1×(ˆh−h)
∀n∈[1, N]
(35)
MWO
n
(E·v)×(ˆh−h)
:=
0
(E·v)×(ˆh−h)
∀n∈[1, N]
(36)
MI
s×(ˆh−h)
:=
0
s×(ˆh−h)
(37)
=⇒
ˆIn
s×ˆh
= [ In
s×h
0
s×(ˆh−h)
]
∀n∈[1, N + 1]
(38)
=⇒
TransformerLayer◦N( I
s×h+ P
s×h) × Wout
h×o =
ˆ
TransformerLayer
◦N( I
s×h+ ˆP
s×ˆh
) × ˆ
Wout
ˆh×o
(39)
where IN+1
s×h
refers to the representations outputted by the last transformer layer, and In
s×h
∀n∈[1, N]
refers to the representation inputted by the nth transformer layer. Symbols denoting parameters,
representations and functions resulting from the application of the transformation discussed in this
section are indicated with the “hat” ˆ symbol.
Informally: zero initializing the specified matrices implies the function preservation property for the
hidden dimension expansion transformation.
See Appendix A.5 for proof.
The hidden dimension expansion transformation must be applied to all MHA blocks to maintain the
hidden dimension uniformly across all the layers, due to the skip connections used throughout
the architecture.
3.6
LAYER ADDITION
The Layer addition transformation can be applied to insert an new layer at any depth of the cur-
rent Transformer architecture. This scaling dimension is controlled by the hyper-parameter N
introduced in Equation 1.
9

Definition 3.6 (Layer addition). A new TransformerLayer(·) whose parameters allow to input and
output matrices of x × h can be inserted in the sequence of the pre-existing N layers. The new
transformer layer can be inserted at any position n ∈[1, N +1]. The index of the downstream layers
is incremented by one.
Theorem 3.6 (Function preserving layer addition). With n being the index of the added layer:
WO
n
(E·v)×h
:=
0
(E·v)×h
Wl2
n
p×h
:= 0
p×h
bl2
n
1×h
:= 0
1×h













=⇒TransformerLayern( In
s×h
) = In
s×h
(40)
Informally: Zero initializing the parameters of the output projections of the MLP and MHA implies
that the added transformer layer output is equivalent to the input.
See Appendix A.6 for proof.
4
RELATED WORK
Some existing works have proposed function preserving transformer expansion operators, but none
cover all six dimensions as proposed in this work. Bert2BERT (Chen et al., 2022) proposes function
preserving width expansions of the MLP internal dimension, hidden dimension, and number of
attention heads. Shen et al. (2022) achieve function preserving width expansion, although constrained
to doubling of all matrix and vector dimensions, and depth expansion via zero initialization of
LayerNorm and bias parameters. Yao et al. (2023) use masking on new hidden MLP neurons, attention
heads, and layers to achieve function preservation. Wang et al. (2023) use an inner optimization
to learn a linear mapping for parameter expansion in depth and width, but without constraints for
function preservation. Notably, our transformations form a function preserving subspace of their
learnable space. Deep Fusion (Mazzawi et al., 2023) extends the concept of expansion to multiple
source models, where the special case of self-fusion achieves function preserving width expansion.
Of these works, some methods are nearly function preserving but admit gaps due to LayerNorm
discrepancies (Chen et al., 2022; Mazzawi et al., 2023). No known works consider scaling factors,
as we address in Equations 19 and 24, nor RMSNorm.
5
CONCLUSION
We have defined six transformations that can be applied to a transformer model to increase the
scale of all the different aspects of the architecture: 1) size of MLP internal representation, 2)
number of attention heads, 3) size of the attention heads output representation, 4) size of the
attention input representation, 5) size of the transformer layers input/output representations, 6)
number of layers. For each of these transformations, we have provided a proof of exact function
preservation given a minimal set of constraints on the initialization of the added parameters. These
six transformations are composable to permit many different ways to scale a transformer-based
model while preserving its function.
We note that, there exist alternative definitions to such transformations that achieve function-
preservation without requiring zero initialization. However, the form of the proposed transformations
is intended to be simple yet minimally constraining. The space of possible initialization strategies
may be explored with the aim to optimize for training in an empirical context.
In future work, these transformations may be applied in the training of a new large model by initializ-
ing a smaller model, training it under reduced data and computational complexity requirements, and
incrementally scaling it to larger sizes throughout training to the desired final size. They may also
be used to generate a family of models that are trained for the same task but at different sizes: all
models within the family can begin from the same checkpoint from training the smallest model, then
10

each successively sized model can be branched and finetuned at its final size. Finally, neural archi-
tecture search (NAS) techniques could be applied to determine optimal transformation scheduling
and architectural progression for a given task and compute budget.
6
ACKNOWLEDGEMENTS
We would like to thank Jeffrey Pennington and Utku Evci for their input to this work.
REFERENCES
Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao
Chen, Zhiyuan Liu, and Qun Liu. bert2BERT: Towards reusable pretrained language models. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 2134–2148, 2022.
Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge
transfer. CoRR, abs/1511.05641, 2016.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,
Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim M. Alabdulmohsin, Rodolphe Jenatton,
Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer,
Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh
Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A.
Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander
Kolesnikov, Filip Paveti’c, Dustin Tran, Thomas Kipf, Mario Luvci’c, Xiaohua Zhai, Daniel
Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters.
ArXiv, abs/2302.05442, 2023.
Utku Evci, Max Vladymyrov, Thomas Unterthiner, Bart van Merrienboer, and Fabian Pedregosa.
GradMax: Growing neural networks using gradient information. ArXiv, abs/2201.05125, 2022.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
International Conference on Artificial Intelligence and Statistics, 2011.
Google. PaLM 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv: Learning, 2016.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv,
abs/2001.08361, 2020.
Hanna Mazzawi, Xavi Gonzalvo, and Michael Wunder. Deep fusion: Efficient network training via
pre-trained initializations. arXiv preprint arXiv:2306.11903, 2023.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,
Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,
Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron
Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese,
Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Suther-
land, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna
Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur
Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux,
11

Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume,
Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura
Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell,
Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray
Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from
training Gopher. ArXiv, abs/2112.11446, 2021.
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. ArXiv, abs/1910.10683, 2020.
Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged
training for transformer language models. In International Conference on Machine Learning, pp.
19893–19908. PMLR, 2022.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. LLaMa 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.
Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky,
Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained
models for efficient transformer training. In The 11th International Conference on Learning
Representations, 2023.
Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. 2x faster language model pre-training via
masked structural growth. arXiv preprint arXiv:2305.02869, 2023.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. ArXiv, abs/1910.07467, 2019.
12

A
PROOFS
A.1
MLP EXPANSION
Proof.
ReLU( X
s×h × ˆ
Wl1
n
h×p
+ ˆBl1
n
s×p
) × ˆ
Wl2
n
p×h
= ReLU
 
X
s×h ×
"
Wl1
n
h×p
MW l1
n
h×(ˆp−p)
#
+
"
Bl1
n
1×p
Mbl1
n
1×(ˆp−p)
#!
×


Wl2
n
p×h
0
(ˆp−p)×h


= ReLU
 "
X
s×h × Wl1
n
h×p
X
s×h × MW l1
n
h×(ˆp−p)
#
+
"
Bl1
n
1×p
Mbl1
n
1×(ˆp−p)
#!
×


Wl2
n
p×h
0
(ˆp−p)×h


= ReLU
 "
X
s×h × Wl1
n
h×p
+ Bl1
n
1×p
X
s×h × MW l1
n
h×(ˆp−p)
+ Mbl1
n
1×(ˆp−p)
#!
×


Wl2
n
p×h
0
(ˆp−p)×h


=
"
ReLU( X
s×h × Wl1
n
h×p
+ Bl1
n
1×p
)
ReLU( X
s×h × MW l1
n
h×(ˆp−p)
+ Mbl1
n
1×(ˆp−p)
)
#
×


Wl2
n
p×h
0
(ˆp−p)×h


=
 
ReLU( X
s×h × Wl1
n
h×p
+ Bl1
n
1×p
) × Wl2
n
p×h
!
+
 
ReLU( X
s×h × MW l1
n
h×(ˆp−p)
+ Mbl1
n
1×(ˆp−p)
) ×
0
(ˆp−p)×h
!
= ReLU( X
s×h × Wl1
n
h×p
+ Bl1
n
1×p
) × Wl2
n
p×h
(41)
Note that it is not necessary to impose any constraints on the values of MW l1
n
h×(ˆp−p)
and mbl1
n
1×(ˆp−p)
to achieve
function preservation property. Thus, these two matrices can be initialized arbitrarily.
A.2
HEAD ADDITION
Proof.
"
H1
s×v · · · H(E+1)
s×v
#
×
ˆ
WO
n
((E+1)·v)×h
=
"
H1
s×v · · · H(E+1)
s×v
#
×


WO
n
(E·v)×h
0
v×h


13

=
"
H1
s×v · · · HE
s×v

H(E+1)
s×v
#
×


WO
n
(E·v)×h
0
v×h


=
 
H1
s×v · · · HE
s×v

× WO
n
(E·v)×h
!
+
 
H(E+1)
s×v
× 0
v×h
!
=

H1
s×v · · · HE
s×v

× WO
n
(E·v)×h
(42)
A.3
HEADS EXPANSION
Proof.
Sn,e
s×s
:= Softmax
 
1
√
k
· ( X
s×h×WQ
n,e
h×k
) × ( X
s×h×WK
n,e
h×k
)⊤
!
(43)
=⇒
ˆHe
s×ˆv
= Attention( X
s×h×WQ
n,e
h×k
, X
s×h×WK
n,e
h×k
, X
s×h× ˆ
WV
n,e
h×ˆv
)
= Sn,e
s×s
×
 
X
s×h× ˆ
WV
n,e
h×ˆv
!
= Sn,e
s×s
×
 
X
s×h×
"
WV
n,e
h×v
MWV
n,e
h×(ˆv−v)
#!
= Sn,e
s×s
×
"
X
s×h×WV
n,e
h×v
X
s×h× MWV
n,e
h×(ˆv−v)
#
=
"
Sn,e
s×s
× ( X
s×h×WV
n,e
h×v
)
Sn,e
s×s
× ( X
s×h× MWV
n,e
h×(ˆv−v)
)
#
=
"
He
s×v
Sn,e
s×s
× ( X
s×h× MWV
n,e
h×(ˆv−v)
)
#
(44)
=⇒

ˆH1
s×ˆv
· · · ˆHE
s×ˆv

×
ˆ
WO
n
(E·ˆv)×h
=

· · · ˆHe
s×ˆv
· · · | e ∈[1, E]

×


...
ˆ
WO
n,e
v×h
...
| e ∈[1, E]


14

=
"
· · · ˆHe
s×ˆv
× ˆ
WO
n,e
v×h
· · · | e ∈[1, E]
#
=

· · · ˆHe
s×ˆv
×


WO
n,e
v×h
0
(ˆv−v)×h

· · · | e ∈[1, E]


=

· · ·
"
He
s×v
Sn,e
s×s
× ( X
s×h× MWV
n,e
h×(ˆv−v)
)
#
×


WO
n,e
v×h
0
(ˆv−v)×h

· · · | e ∈[1, E]


=
"
· · ·
"
He
s×v × WO
n,e
v×h
+ Sn,e
s×s
× ( X
s×h× MWV
n,e
h×(ˆv−v)
) ×
0
(ˆv−v)×h
#
· · · | e ∈[1, E]
#
=
"
· · ·
"
He
s×v × WO
n,e
v×h
+ 0
s×h
#
· · · | e ∈[1, E]
#
=
"
· · · He
s×v × WO
n,e
v×h
· · · | e ∈[1, E]
#
=

· · · He
s×v · · · | e ∈[1, E]

×


...
WO
n,e
v×h
...
| e ∈[1, E]


=

H1
s×v · · · HE
s×v

× WO
n
(E·v)×h
(45)
A.4
ATTENTION EXPANSION
Proof.
1
p
ˆk
· ( X
s×h× ˆ
WQ
n,e
h×ˆk
) × ( X
s×h× ˆ
WK
n,e
h×ˆk
)⊤
=
1
p
ˆk
·

X
s×h×

WQ
n,e
h×k
MWQ
n,e
h×(ˆk−k)



×
 
X
s×h×
"p
ˆk
√
k
· WK
n,e
h×k
0
h×(ˆk−k)
#!⊤
=
1
p
ˆk
·

X
s×h×WQ
n,e
h×k
X
s×h× MWQ
n,e
h×(ˆk−k)

×
"p
ˆk
√
k
· X
s×h×WK
n,e
h×k
X
s×h×
0
h×(ˆk−k)
#⊤
15

=
1
p
ˆk
·

X
s×h×WQ
n,e
h×k
X
s×h× MWQ
n,e
h×(ˆk−k)

×
"p
ˆk
√
k
· X
s×h×WK
n,e
h×k
0
s×(ˆk−k)
#⊤
=
1
p
ˆk
·
p
ˆk
√
k
·

X
s×h×WQ
n,e
h×k
X
s×h× MWQ
n,e
h×(ˆk−k)

×
"
X
s×h×WK
n,e
h×k
0
s×(ˆk−k)
#⊤
=
1
√
k
·

X
s×h×WQ
n,e
h×k
X
s×h× MWQ
n,e
h×(ˆk−k)

×
"
X
s×h×WK
n,e
h×k
0
s×(ˆk−k)
#⊤
=
1
√
k
·

X
s×h×WQ
n,e
h×k
X
s×h× MWQ
n,e
h×(ˆk−k)

×


( X
s×h×WK
n,e
h×k
)⊤
0
(ˆk−k)×s


=
1
√
k
·

( X
s×h×WQ
n,e
h×k
) × ( X
s×h×WK
n,e
h×k
)⊤+ ( X
s×h× MWQ
n,e
h×(ˆk−k)
) ×
0
(ˆk−k)×s


=
1
√
k
·
 
( X
s×h×WQ
n,e
h×k
) × ( X
s×h×WK
n,e
h×k
)⊤+ 0
s×s
!
=
1
√
k
· ( X
s×h×WQ
n,e
h×k
) × ( X
s×h×WK
n,e
h×k
)⊤
(46)
A.5
HIDDEN DIMENSION EXPANSION
Proof. We demonstrate ˆIn
s×ˆh
= [ In
s×h
0
s×(ˆh−h)
] ∀n∈[0, N] by induction on n.
Base case n = 0:
ˆI0
s×ˆh
= ˆI
s×h+ ˆP
s×ˆh
=
"
I
s×h
0
s×(ˆh−h)
#
+
"
P
s×h
0
s×(ˆh−h)
#
=
"
I
s×h + P
s×h
0
s×(ˆh−h)
#
.
(47)
Induction step, assuming ˆIn
s×ˆh
= [ In
s×h
0
s×(ˆh−h)
] holds:
16

NormMHA
n
( ˆIn
s×h
) =

ˆiµ,j · ˆgMHA
n,j
q
1
ˆh
Pˆh
γ=1(ˆiµ,γ)2
| µ∈[1, s] ∧j ∈[1, ˆh]

= NormMHA
n
([ In
s×h
0
s×(ˆh−h)
])
=



iµ,j · ˆgMHA
n,j
q
1
ˆh
Pˆh
γ=1(ˆiµ,γ)2
| µ∈[1, s] ∧j ∈[1, h]

0 · ˆgMHA
n,j
q
1
ˆh
Pˆh
γ=1(ˆiµ,γ)2
| µ∈[1, s] ∧j ∈[h + 1, ˆh]


=



iµ,j · ˆgMHA
n,j
q
1
ˆh
Pˆh
γ=1(ˆiµ,γ)2
| µ∈[1, s] ∧j ∈[1, h]

0
s×(ˆh−h)


=



iµ,j · ˆgMHA
n,j
q
1
ˆh(Ph
γ=1(iµ,γ)2 + Pˆh
γ=h+1 0)
| µ∈[1, s] ∧j ∈[1, h]

0
s×(ˆh−h)


=



iµ,j · ˆgMHA
n,j
q
1
ˆh
Ph
γ=1(iµ,γ)2
| µ∈[1, s] ∧j ∈[1, h]

0
s×(ˆh−h)


=


 iµ,j ·
√
h
√
ˆh · gMHA
n,j
q
1
ˆh
Ph
γ=1(iµ,γ)2
| µ∈[1, s] ∧j ∈[1, h]

0
s×(ˆh−h)


=



iµ,j · gMHA
n,j
q
1
h
Ph
γ=1(iµ,γ)2
| µ∈[1, s] ∧j ∈[1, h]

0
s×(ˆh−h)


=
"
NormMHA
n
( In
s×h
)
0
s×(ˆh−h)
#
(48)
For conciseness, we use the following notation: Nc
n
s×h
:= Normc
n( In
s×h
) and ˆNc
n
s×ˆh
:= [Nc
n
s×h
0
s×(ˆh−h)
].
=⇒
ˆI
′
n
s×ˆh
= ˆIn
s×ˆh
+
ˆ
MHAn(ˆNMHA
n
s×ˆh
)
= ˆIn
s×ˆh
+
"
· · · Attention(ˆNMHA
n
s×ˆh
× ˆ
WQ
n,e
ˆh×k
, ˆNMHA
n
s×ˆh
× ˆ
WK
n,e
ˆh×k
, ˆNMHA
n
s×ˆh
× ˆ
WV
n,e
ˆh×v
) · · · | ∀e∈[1, E]
#
×
ˆ
WO
n
(E·v)×ˆh
= ˆIn
s×ˆh
+

· · · Attention([NMHA
n
s×h
0
s×(ˆh−h)
]×


WQ
n,e
h×v
MWQ
n,e
(ˆh−h)×v

, ˆNMHA
n
s×ˆh
× ˆ
WK
n,e
ˆh×k
, ˆNMHA
n
s×ˆh
× ˆ
WV
n,e
ˆh×v
) · · · | ∀e∈[1, E]

× ˆ
WO
n
(E·v)×ˆh
= ˆIn
s×ˆh
+
"
· · · Attention(NMHA
n
s×h
×WQ
n,e
h×k
, NMHA
n
s×h
×WK
n,e
h×k
, NMHA
n
s×h
×WV
n,e
h×v
) · · · | ∀e∈[1, E]
#
×
ˆ
WO
n
(E·v)×ˆh
= ˆIn
s×ˆh
+

· · · He
s×v · · · | ∀e∈[1, E]

×
"
WO
n
(E·v)×h
0
(E·v)×(ˆh−h)
#
= ˆIn
s×ˆh
+
"
MHAn(NMHA
n
s×h
)
0
s×(ˆh−h)
#
17

=
"
In
s×h
0
s×(ˆh−h)
#
+
"
MHAn(NMHA
n
s×h
)
0
s×(ˆh−h)
#
=
"
In
s×h
+ MHAn(NMHA
n
s×h
)
0
s×(ˆh−h)
#
=
"
I
′
n
s×h
0
s×(ˆh−h)
#
(49)
=⇒
Following the demonstration provided for
ˆ
Norm
MHA
n
(·):
ˆ
Norm
MLP
n
( ˆIn
s×h
) =
"
NormMLP
n
( ˆI
′
n
s×h
)
0
s×(ˆh−h)
#
(50)
ˆNMLP
n
s×ˆh
:=
ˆ
Norm
MLP
n
( ˆIn
s×h
)
(51)
=⇒
ˆIn+1
s×ˆh
=
ˆ
TransformerLayern( ˆIn
s×ˆh
)
= ˆI
′
n
s×ˆh
+
ˆ
MLPn(ˆNMLP
n
s×ˆh
)
= ˆI
′
n
s×ˆh
+
ˆ
MLPn(ˆNMLP
n
s×ˆh
)
= ˆI
′
n
s×ˆh
+ ReLU(ˆNMLP
n
s×ˆh
× ˆ
Wl1
n
ˆh×p
+ Bl1
n
s×p
) × ˆ
Wl2
n
p×ˆh
+ ˆBl2
n
s×ˆh
= ˆI
′
n
s×ˆh
+ ReLU([NMLP
n
s×h
0
s×(ˆh−h)
] ×


Wl1
n
h×p
MW l1
(ˆh−h)×p

+ Bl1
n
s×p
) × ˆ
Wl2
n
p×ˆh
+ ˆBl2
n
s×ˆh
= ˆI
′
n
s×ˆh
+ ReLU(NMLP
n
s×h
× Wl1
n
h×p
+ Bl1
n
s×p
) × ˆ
Wl2
n
p×ˆh
+ ˆBl2
n
s×ˆh
= ˆI
′
n
s×ˆh
+ ReLU(NMLP
n
s×h
× Wl1
n
h×p
+ Bl1
n
s×p
) ×
"
Wl2
n
p×h
0
p×(ˆh−h)
#
+
"
Bl2
n
s×h
0
s×(ˆh−h)
#
= ˆI
′
n
s×ˆh
+
"
ReLU(NMLP
n
s×h
× Wl1
n
h×p
+ Bl1
n
s×p
) × Wl2
n
p×h
0
s×(ˆh−h)
#
+
"
Bl2
n
s×h
0
s×(ˆh−h)
#
= ˆI
′
n
s×ˆh
+
"
ReLU(NMLP
n
s×h
× Wl1
n
h×p
+ Bl1
n
s×p
) × Wl2
n
p×h
+ Bl2
n
s×h
0
s×(ˆh−h)
#
= ˆI
′
n
s×ˆh
+
"
MLPn(NMLP
n
s×h
)
0
s×(ˆh−h)
#
=
"
I
′
n
s×h
+ MLPn(NMLP
n
s×h
)
0
s×(ˆh−h)
#
=
"
ˆ
TransformerLayern( In
s×h
)
0
s×(ˆh−h)
#
18

=
"
In+1
s×h
0
s×(ˆh−h)
#
(52)
Having demonstrated that, after applying the hidden dimension expansion:
ˆIn+1
s×ˆh
=
"
In+1
s×h
0
s×(ˆh−h)
#
∀n∈[1, N + 1]
(53)
The output equivalence can be proven as follows:
ˆ
TransformerArchitecture( ˆI
s×ˆh
) =
ˆ
TransformerLayer
◦N( ˆI
s×ˆh
+ ˆP
s×ˆh
) × ˆ
Wout
ˆh×o
= ˆIN+1
s×ˆh
× ˆ
Wout
ˆh×o
=
"
IN+1
s×h
0
s×(ˆh−h)
#
×


Wout
h×o
MW out
(ˆh−h)×o

= IN+1
s×h
× Wout
h×o
= TransformerArchitecture( I
s×h)
(54)
A.6
LAYER ADDITION
Proof.
MHAn(Xn
s×h
) =

H1
s×v · · · HE
s×v

×
0
(E·v)×h = 0
s×h
(55)
MLPn(Xn
s×h
) = ReLU(Xn
s×h
× Wl1
n
h×p
+ Bl1
n
s×p
) × 0
p×h + 0
s×h = 0
s×h
(56)
I
′
n
s×h
= In
s×h
+ MHAn(NormMHA
n
( In
s×h
)) = In
s×h
+ 0n
s×h
= In
s×h
(57)
TransformerLayern( In
s×h
) = In
s×h
+ MLPn(NormMLP
n
( In
s×h
)) = In
s×h
+ 0n
s×h
= In
s×h
(58)
Note that the function preserving property holds even if normalization is applied after the MLP and
MHA components as Norm(·) outputs zeros for zeros input.
19

