Compact Textbooks in Mathematics
Simeon Ball
A Course in 
Algebraic 
Error-Correcting 
Codes

Compact Textbooks in Mathematics

Compact Textbooks in Mathematics
This textbook series presents concise introductions to current topics in math-
ematics and mainly addresses advanced undergraduates and master students.
The concept is to offer small books covering subject matter equivalent to 2- or 3-
hour lectures or seminars which are also suitable for self-study. The books provide
students and teachers with new perspectives and novel approaches. They may
feature examples and exercises to illustrate key concepts and applications of the
theoretical contents. The series also includes textbooks specifically speaking to
the needs of students from other disciplines such as physics, computer science,
engineering, life sciences, finance.
•
compact: small books presenting the relevant knowledge
•
learning made easy: examples and exercises illustrate the application of the
contents
•
useful for lecturers: each title can serve as basis and guideline for a semester
course/lecture/seminar of 2–3 hours per week.
More information about this series at http://www.springer.com/series/11225

Simeon Ball
A Course in Algebraic
Error-Correcting
Codes

Simeon Ball
Department of Mathematics
Polytechnic University of Catalonia
Barcelona, Spain
ISSN 2296-4568
ISSN 2296-455X
(electronic)
Compact Textbooks in Mathematics
ISBN 978-3-030-41152-7
ISBN 978-3-030-41153-4
(eBook)
https://doi.org/10.1007/978-3-030-41153-4
Mathematics Subject Classification (2010): 94BXX, 51EXX, 94AXX
© Springer Nature Switzerland AG 2020
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole
or part of the material is concerned, specifically the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical
way, and transmission or information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a specific statement, that such names are
exempt from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information
in this book are believed to be true and accurate at the date of publication. Neither the
publisher nor the authors or the editors give a warranty, expressed or implied, with respect
to the material contained herein or for any errors or omissions that may have been made.
The publisher remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.
This book is published under the imprint Birkhäuser, www.birkhauser-science.com by the
registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

v
Preface
This book is based on lecture notes for a course on coding theory given
as part of the Applied Mathematics and Mathematical Engineering master’s
degree at the Universitat Politécnica de Catalunya. The aim of the course is
to give an up-to-date account of error-correcting codes from a mathematical
point of view, an analysis of the construction of these codes, and of the
various algorithms which are implemented to correct corrupted messages.
The lectures were prepared for an audience at master’s level, although a
large proportion of the book should be accessible to students at undergradu-
ate level and to engineering and physics students too. There is some formal
algebra that may not be familiar, mainly in the introduction of ﬁnite ﬁelds in
Chapter 2, but this is not essential to be able to follow the main part of the
content. It is enough to know how to perform ﬁnite ﬁeld arithmetic and how
to factorise polynomials over ﬁnite ﬁelds, all of which are explained in detail
in Chapter 2.
A large part of the material included in the text dates back to the latter
part of the last century. However, there have been recent advances in the
algebraic theory of error-correcting codes, many of which are included here.
There are still questions and problems which remain unresolved despite
considerable effort having been directed towards their resolution. Many of
these are highlighted in the text. The book takes a combinatorial, algebraic,
and geometric view of coding theory but combines this with a practical
consideration of how the codes constructed are implemented.
Shannon’s theorem, the highlight of Chapter 1 and which dates back
to 1947, tells us that given a noisy, not totally unreliable, communication
channel, there are codes which provide a means of reliable communication
at a transmission rate arbitrarily close to the capacity. However, Shannon’s
theorem only tells us that reliable communication is possible, it does not
provide us with a feasible way in which to encode or decode the transmission.
One of the aims of the book is to ﬁnd codes with a high transmission rate,
which allow fast encoding and decoding. On the way towards this objective,
we construct various types of codes and consider a number of decoding
algorithms.
Chapter 2 is a brief introduction to ﬁnite ﬁelds and the geometries
associated with these ﬁelds. An emphasis is given to the factorisation of
cyclotomic polynomials over ﬁnite ﬁelds, which is put to use in Chapter 5.
The concept of a minimum distance between any two codewords of a
block code is introduced in Chapter 3. The larger the minimum distance, the
more errors one can correct and, together with the length and size of the code,
are the fundamental parameters of a block code. There are various bounds
on these parameters proven in Chapter 3, both the Gilbert–Varshamov
lower bound, given by the greedy algorithm, and upper bounds, a discrete

vi
Preface
sphere-packing bound and the better Plotkin and Elias-Bassalygo bounds.
The codes given by the Gilbert–Varshamov bound are asymptotically good
codes in the sense that both the transmission rate, the proportion of the bits of
the message which contains information, and the relative minimum distance
are bounded away from zero as the length of the code increases. However,
in practice, as in the case of the randomly chosen code, there is no efﬁcient
way in which to encode or decode the message using these codes.
The advantage of linear codes, the focus of Chapter 4 and fundamental
to most of the rest of the book, is that they are efﬁcient to encode. One can
encode by simply multiplying a vector by a matrix. We consider a decoding
algorithm for linear codes based on syndromes. The question of existence
of a vector of small weight with a speciﬁed syndrome is shown to be NP-
complete, which implies that the decoding algorithm is not feasible for long
codes, although it is used in practice for short length codes.
The classical 2-error correcting and 3-error correcting perfect codes
are constructed in Chapter 5, as well as the general class of BCH codes.
Although BCH codes exist for all lengths, it is known that there are no
sequences of BCH codes for which the transmission rate and the relative
minimum distance are both bounded away from zero.
Reed–Solomon codes are codes whose codewords are the evaluation of
polynomials of low degree. In Chapter 6, we will exploit this algebraic
structure to explicitly develop a polynomial time decoding algorithm for
Reed–Solomon codes which will correct any error bits, providing the number
of errors is less than half the minimum distance. We will also show that
there is a polynomial time list decoding algorithm which produces a short
list of possible candidates for the sent codeword when far more errors
occur. By employing two Reed–Solomon codes, this allows one to decode
correctly well beyond the half the minimum distance bound with very high
probability. Another important application of MDS codes, and in particular
Reed–Solomon codes, is the storage of data in distributed storage systems.
The fact that the codewords of an MDS code are uniquely determined by
relatively few bits means that data, stored across a number of different
servers, can be recovered from just a few.
In Chapter 7 we prove that there are subﬁeld subcodes of generalised
Reed–Solomon codes meeting the Gilbert–Varshamov bound. Since these
codes are linear, they are fast to encode and the fact that they have an
algebraic structure allows us to decode using the list decoding algorithm
from Chapter 6. We then go on to consider codes constructed from algebraic
curves. These algebraic-geometric codes include codes which surpass the
Gilbert–Varshamov bound for codes over large alphabets.
There are linear codes constructed from low-density parity check matri-
ces for which both the transmission rate and the relative minimum distance
are bounded away from zero. We will prove in Chapter 8 that we can encode
and decode certain low-density parity check codes with polynomial time

Preface
vii
algorithms. These codes are widely implemented in wireless communication,
with a transmission rate close to the capacity set out by Shannon’s theorem.
Although not asymptotically good, Reed–Muller codes and their sub-
codes, the theme of Chapter 9, are widely implemented since there are
fast decoding algorithms for these codes, such as a majority logic decoding
algorithm which is detailed here. Kerdock codes are certain subcodes of the
second-order Reed–Muller codes. They are of particular interest since there
are Kerdock codes which are non-linear codes with parameters for which it
is known that no linear code exists.
Bringing together p-adic numbers and cyclic codes in Chapter 10, we
construct non-linear codes which are linear over the ring of integers modulo
a prime power. Within this class of codes we again construct a non-linear
binary code with parameters for which it is known no binary linear code
exists. This suggests that, more generally, these codes could be a source of
codes which perform better than linear codes.
The three main conjectures concerning error-correcting codes are
included. The Information Theory and Applications Center of the University
of California San Diego offers prizes for the resolution of any of these
three conjectures. The three conjectures can be roughly stated as, there is no
inﬁnite sequence of binary codes better than the Gilbert–Varshamov bound,
there are no non-trivial constant weight perfect codes, and there are no
linear MDS codes longer than the Reed–Solomon codes, apart from some
three-dimensional even characteristic codes and their duals.
I would like to thank Tim Alderson, Anurag Bishnoi, Aart Blokhuis,
Massimo Giulietti, Victor Hernandez, Michel Lavrauw, Sonia Mansilla,
Valentina Pepe, and Oriol Serra for their comments and suggestions. I would
also like to thank Francesc Comellas for drawing some of the ﬁgures.
Barcelona, Spain
Simeon Ball
October 2019

viii
Preface
The dependencies between the chapters are as follows.

ix
Table of Parameters for Codes in the Text
The following table lists the parameters of the speciﬁc codes constructed in
the text. An [n, k, d]q code refers to a k-dimensional linear code over Fq
of length n and minimum distance d. A (n, K, d)r code refers to a code of
length n, size K, and minimum distance d over an alphabet of size r.
Parameters
Name
Example 3.4
(6, 8, 3)2
Example 3.6
(7, 8, 4)2
Example 3.11 (6, 4, 4)2
Exercise 3.2
(7, 16, 3)2
Exercise 3.4
(10, 6, 6)2
Exercise 3.5
(6, 4, 5)3
Example 4.2
[7, 4, 3]2
Binary Hamming
Example 4.3
[9, 3, 6]3
Example 4.5
[(qm −1)/(q −1), (qm −1)/(q −1) −m, 3]q Hamming
Example 4.8
[8, 4, 4]3
Example 4.16 [q√q + 1, 3, q√q −√q]q
Hermitian curve
Example 4.19 [10, 6, 4]3
Example 4.23 [10, 4, 6]3
Exercise 4.7
[6, 3, 4]5
Exercise 4.8
[7, 3, 5]7
Exercise 4.9
[9, 3, 6]3
Exercise 4.14 [10, 6, 4]3
Example 5.5
[11, 6, 5]3
Ternary Golay
Example 5.5
[12, 6, 6]3
Extended ternary Golay
Example 5.9
[23, 12, 7]2
Binary Golay
Example 5.9
[24, 12, 8]2
Extended binary Golay
Example 5.11 [31, 16, 7]2
Example 5.12 [n, n −d + 1, d]q
Cyclic Reed–Solomon
Exercise 5.6
[15, 7, 5]2
Exercise 5.6
[31, 11, 11]2
Exercise 5.6
[13, 4, 7]3
Exercise 5.7
[17, 9, 5]2
Zetterberg
Exercise 5.7
[18, 9, 6]2
Extended Zetterberg
Exercise 5.8
[11, 6, 5]4
Exercise 5.9
[17, 9, 7]4
(continued)

x
Table of Parameters for Codes in the Text
Example 6.4
[q + 1, q −d + 2, d]q
Reed–Solomon
Exercise 6.9
[2h + 2, 3, 2h]2h
Translation hyperoval
Exercise 6.10 [10, 5, 6]9
Glynn
Exercise 6.11 [2h + 1, 4, 2h −2]2h
Segre
Example 7.2
[10, 7, 3]3
Example 7.14 [8, 3, 5]4
Example 7.14 [8, 5, 3]4
Exercise 7.10 [24, 4, 18]9
Example 8.4
[12, 3, 6]2
Afﬁne plane of order 3
Theorem 9.3
[2m, 1 +
m
1

+ · · · +
m
r

, 2m−r]2 Reed–Muller
Theorem 9.11 (2m, 22m, 2m−1 −2m/2−1)2
Kerdock
Example 9.12 (16, 256, 6)2
Nordstrom–Robinson
Exercise 10.6 (12, 8, 6)2
Exercise 10.7 (16, 256, 6)2
Nordstrom–Robinson

xi
Contents
Table of Parameters for Codes in the Text...........................
ix
1
Shannon’s Theorem ..............................................
1
1.1
Entropy.............................................................
1
1.2
Information Channels ..............................................
4
1.3
System Entropies and Mutual Information..........................
5
1.4
Decoding and Transmission Rate ...................................
10
1.5
Shannon’s Theorem ................................................
11
1.6
Comments..........................................................
14
1.7
Exercises ...........................................................
14
2
Finite Fields .......................................................
17
2.1
Definitions and Construction .......................................
17
2.2
Properties of Finite Fields...........................................
20
2.3
Factorisation of Cyclotomic Polynomials............................
21
2.4
Affine and Projective Spaces over Finite Fields......................
24
2.5
Comments..........................................................
26
2.6
Exercises ...........................................................
26
3
Block Codes .......................................................
29
3.1
Minimum Distance .................................................
29
3.2
Bounds on Block Codes.............................................
32
3.3
Asymptotically Good Codes ........................................
36
3.4
Comments..........................................................
43
3.5
Exercises ...........................................................
43
4
Linear Codes ......................................................
47
4.1
Preliminaries .......................................................
47
4.2
Syndrome Decoding................................................
51
4.3
Dual Code and the MacWilliams Identities ..........................
54
4.4
Linear Codes and Sets of Points in Projective Spaces ...............
58
4.5
Griesmer Bound ....................................................
59
4.6
Constructing Designs from Linear Codes ...........................
63
4.7
Comments..........................................................
66
4.8
Exercises ...........................................................
67

xii
Contents
5
Cyclic Codes .......................................................
71
5.1
Basic Properties.....................................................
71
5.2
Quadratic Residue Codes ...........................................
75
5.3
BCH Codes ..........................................................
78
5.4
Comments ..........................................................
80
5.5
Exercises ............................................................
81
6
Maximum Distance Separable Codes ...........................
83
6.1
Singleton Bound ....................................................
84
6.2
Reed–Solomon Code................................................
84
6.3
Linear MDS Codes ..................................................
91
6.4
MDS Conjecture ....................................................
94
6.5
Comments ..........................................................
100
6.6
Exercises ............................................................
101
7
Alternant and Algebraic Geometric Codes .....................
105
7.1
Subfield Subcodes ..................................................
105
7.2
Generalised Reed–Solomon Codes..................................
107
7.3
Alternant Codes Meeting the Gilbert–Varshamov Bound ...........
109
7.4
Algebraic Geometric Codes .........................................
112
7.5
Algebraic Geometric Codes Surpassing the
Gilbert–Varshamov Bound ..........................................
117
7.6
Comments ..........................................................
119
7.7
Exercises ............................................................
119
8
Low Density Parity Check Codes ................................
123
8.1
Bipartite Graphs with the Expander Property .......................
123
8.2
Low Density Parity Check Codes ....................................
126
8.3
Decoding LDPC Codes ..............................................
128
8.4
Comments ..........................................................
131
8.5
Exercises ............................................................
131
9
Reed–Muller and Kerdock Codes................................
133
9.1
Binary Reed–Muller Codes ..........................................
133
9.2
Decoding Reed–Muller Codes.......................................
135
9.3
Kerdock Codes ......................................................
142
9.4
Non-binary Reed–Muller Codes .....................................
145
9.5
Comments ..........................................................
148
9.6
Exercises ............................................................
149

Contents
xiii
10
p-Adic Codes .....................................................
151
10.1
p-Adic Numbers ....................................................
151
10.2
Polynomials over the p-Adic Numbers..............................
153
10.3
p-Adic Codes .......................................................
155
10.4
Codes over Z/phZ ..................................................
157
10.5
Codes over Z/4Z ...................................................
160
10.6
Comments..........................................................
162
10.7
Exercises ...........................................................
162
Hints and Answers to Selected Exercises ...............................
165
Bibliography ..............................................................
170
Index.......................................................................
175

1
1
Shannon’s Theorem
The content of this chapter is rather different in nature to what appears in the rest
of the book, since Shannon’s theorem is really a theorem from information theory
and not coding theory. However, we include it here because it tells us that reliable
communication can be achieved using a noisy channel and sets a limit for what is
feasible in terms of the proportion of data we can send whilst being almost sure to be able
to recover the original message from the distorted signal. Essentially, this chapter is a
very brief introduction to information theory, the mathematics this entails is probabilistic
in nature, whereas later it will be more algebraic and to some extent geometric. It is not
essential to the rest of the text and can be treated as optional.
1.1
Entropy
Let S = {s1, . . . , sm} be a ﬁnite set of values and suppose that we have a random
variable X which takes the value si ∈S with a probability pi. In other words, we
have a probability function, where the probability that X is si is
P(X = si) = pi.
Therefore, 0 ⩽pi ⩽1, and
m

i=1
pi = 1.
This random variable may be the result of some kind of experiment, where S is the
set of possible results, or a communication, where S is the set of possible symbols which
can be sent (or received). Usually the value of a random variable is a real number (which
allows us to calculate its expectation and other quantities dependent on the random
variable) but we will not be so strict here. We will not calculate these quantities and will
satisfy ourselves that a string of symbols is a legitimate value of the random variable.
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_1

1
2
Chapter 1 • Shannon’s Theorem
Example 1.1
Let S = {2, 3, . . . , 12} and let the probability that X = si be given by
si
2
3
4
5
6
7
8
9
10
11
12
pi
1
36
2
36
3
36
4
36
5
36
6
36
5
36
4
36
3
36
2
36
1
36
the probability that the sum of two dice throws is equal to si.
■
Let f be a function
f : (0, 1] →[0, ∞),
with the property that
(I1)
f (x) is a continuous decreasing function and f (x) = 0 if x = 1,
(I2)
f (xy) = f (x) + f (y).
The function f can be interpreted as a measure of information we get if we consider
f applied to a probability p, where p is the probability that a certain value is selected by
a random variable X. The axiom (I1) indicates that more information is gained when a
lower the probability event occurs. If we imagine that we are repeating the experiment,
then (I2) is saying that the information we get from two elements is equal to the sum of
the information we get from each element.
Lemma 1.2 If f is a function satisfying (I1) and (I2), then
f (x) = −logr(x),
for some r > 1.
Proof
Deﬁne g(x) = f (e−x).
Then (I2) implies
g(x + y) = f (e−(x+y)) = f (e−xe−y) = f (e−x) + f (e−y) = g(x) + g(y).
Therefore, g is a continuous additive function which implies that g(x) = cx for some c ∈R.
Putting y = e−x gives
f (y) = g(−ln y) = −c ln y.
Since ln y is an increasing function of y and, according to (I1) f is a decreasing function, we
have c > 0.
The lemma follows by letting c = (ln r)−1.
⊓⊔

1.1 · Entropy
3
1
To deﬁne a measure of the information we get from a random variable X, we weigh
the sum of the information according to the probabilities.
The (r-ary) entropy Hr(X) of X is
Hr(X) =
m

i=1
pif (pi) = −
m

i=1
pi logr(pi).
We assume throughout that the function x log x is zero when evaluated at 0.
Example 1.3
Suppose that S = {0, 1} and that P (X = 0) = p and P(X = 1) = 1 −p. Then, the binary
entropy H2(X) of X is
h(p) = −p log2 p −(1 −p) log2(1 −p).
This function of p, deﬁned on the real line interval [0, 1], is called the binary entropy
function . Its graph is drawn in ⊡Figure 1.1.
■
Observe that for the entropy to be zero every term in the sum must be zero, since
every term in the sum is non-negative. But pi logr(pi) = 0 implies pi is either 0 or 1.
Therefore, P(X = si) = 1, for some i and P(X = sj) = 0 for all j ̸= i. In this case
X conveys no information, since there is no uncertainty. At the other end of the scale,
intuitively, the most uncertainty, and so the most information, is conveyed when X is
equally likely to be any of the elements of S. This is proven in the following theorem.
⊡Fig. 1.1 The binary entropy
function on the interval [0, 1].
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0

1
4
Chapter 1 • Shannon’s Theorem
Theorem 1.4
Let X be a random variable taking values from a ﬁnite set S. Then
Hr(X) ⩽logr |S|
with equality if and only if P (X = s) = 1/|S|, for all s ∈S.
Proof
Observe that ln x ⩽x −1 with equality if and only if x = 1.
By deﬁnition,
Hr(X) −logr |S| = −

i
pi logr(pi) −

i
pi logr |S|
=

i
pi logr(
1
pi|S|) =
1
ln r

i
pi ln(
1
pi|S|) ⩽
1
ln r

i
pi(
1
pi|S| −1) = 0.
Equality occurs if and only if pi|S| = 1 for all i = 1, . . . , |S|.
⊓⊔
1.2
Information Channels
Let X and Y be random variables which take values from ﬁnite sets S and T ,
respectively, according to the probability distributions P(X = si) = pi and P(Y =
tj) = qj.
We consider the elements of S as the symbols which are sent and the elements of T
as the symbols which are received. The channel through which the symbols are sent is
denoted by , which is deﬁned by the matrix (pij), where
pij = P(Y = tj|X = si)
is the probability that tj is received given that si was sent.
We deﬁne
qij = P(X = si|Y = tj)
and
rij = P(X = si, Y = tj).
The probabilities pij, qij and rij are called the forwards probabilities, the
backwards probabilities and the joint probabilities, respectively.

1.3 · System Entropies and Mutual Information
5
1
Example 1.5
In a binary symmetric channel both S = {0, 1} and T = {0, 1}. The channel is deﬁned by
the matrix
(pij) =

φ
1 −φ
1 −φ
φ

for some φ ∈[0, 1]. The rows and columns of the matrix are indexed by 0 and 1 in that order.
Suppose that X is the random variable deﬁned by the probability
P(X = 0) = p.
From the channel matrix we can calculate the probabilities for Y from
P(Y = b) =

a∈{0,1}
P (Y = b|X = a)P (X = a).
For example,
P(Y = 0) = φp + (1 −p)(1 −φ).
We can calculate the joint probabilities from
P(X = a, Y = b) = P (Y = b | X = a)P (X = a).
For example,
P(X = 1, Y = 0) = P (Y = 0 | X = 1)P (X = 1) = (1 −φ)(1 −p).
We can calculate the backwards probabilities using
P(X = a | Y = b)P(Y = b) = P (X = a, Y = b).
For example,
P(X = 1 | Y = 0) = P (X = 1, Y = 0)
P (Y = 0)
=
(1 −φ)(1 −p)
φp + (1 −p)(1 −φ).
■
1.3
System Entropies and Mutual Information
We deﬁne the input entropy and the output entropy as
H(X) = −

i
pi log pi

1
6
Chapter 1 • Shannon’s Theorem
and
H(Y) = −

j
qj log qj,
respectively.
We suppress the r in the logarithm, but assume that it is the same for both deﬁnitions.
Given that we have received tj ∈T , we can calculate the entropy of X, conditional
on the fact that we know Y = tj, by using the backwards probabilities. This gives
H(X|Y = tj) = −

i
qij log qij.
This tells us the average information of X, knowing that Y = tj.
If this is zero, then it would say that we know everything about X. This would mean
that the backwards probabilities qij would be 1 for some i and zero for the others. In
other words, that if we receive tj, then we know which symbol was sent.
If this is H(X), then this says that we learn nothing about X when we receive tj.
This would happen if
qij = P(X = si | Y = tj) = P(X = si) = pi.
Averaging over tj ∈Y, we obtain the conditional entropy, the average information
of X knowing Y,
H(X|Y) = −

i,j
qjqij log qij.
Similarly, the average information of Y knowing X is
H(Y|X) = −

i,j
pjpij log pij.
The joint entropy H(X, Y) is given by the joint probabilities and is the average
information gained from both the input and output,
H(X, Y) = −

i,j
rij log rij.
Example 1.6
Suppose  is the channel with input random variable X, deﬁned by
P(X = 0) = P (X = 1) = 1
2,

1.3 · System Entropies and Mutual Information
7
1
and output random variable Y taking a value from the set {0, 1, ∗} and that the channel matrix
(pij) =

3
4 0 1
4
0 1
2
1
2

has rows indexed by 0 and 1 in that order and columns indexed by 0, 1, ∗in that order.
We can calculate directly
H(Y|X) = −1
2( 3
4 log 3
4 + 2 1
2 log 1
2 + 1
4 log 1
4) = −3
8 log 3 + 3
2 log 2
and
H(X) = −1
2(log 1
2 + log 1
2) = log 2.
We can calculate the output probabilities using qj = 
i pipij. This gives
(q0, q1, q∗) = ( 3
8, 1
4, 3
8).
We can calculate the backwards probabilities qij using
qij = pijpi
qj
,
and obtain
(qij) =

1 0 1
3
0 1 2
3

.
Therefore,
H(X|Y) = −3
8( 1
3 log 1
3 + 2
3 log 2
3) = 3
8 log 3 −1
4 log 2
and
H(Y) = −2 3
8 log 3
8 −1
4 log 1
4 = 11
4 log 2 −3
4 log 3.
Observe that
H(Y) −H(Y|X) = −3
8 log 3 + 5
4 log 2 = H(X) −H(X|Y).
Finally, we can calculate the joint probabilities from rij = pipij and get
(rij) =

3
8 0 1
8
0 1
4
1
4

.

1
8
Chapter 1 • Shannon’s Theorem
The joint entropy is
H(X, Y) = −2 1
4 log 1
4 −3
8 log 3
8 −1
8 log 1
8 = 5
2 log 2 −3
8 log 3.
■
Observe that in Example 1.6
H(X, Y) = H(X|Y) + H(Y) = H(Y|X) + H(X).
This is no coincidence and holds in general.
Lemma 1.7 For random variables X and Y deﬁned on ﬁnite sets,
H(X, Y) = H(X|Y) + H(Y) = H(Y|X) + H(X).
Proof
Since
P(X = si, Y = tj) = P (X = si | Y = tj)P (Y = tj),
we have that
rij = qjqij.
By direct calculation,
H(X, Y) = −

i,j
rij log rij = −

i,j
qjqij log qjqij
= −

i,j
qjqij log qj −

i,j
qjqij log qij.
Since 
i qij = 1,
H(X, Y) = −

j
qj log qj + H(X|Y) = H(Y) + H(X|Y).
Reversing the roles of X and Y we obtain the second equality.
⊓⊔
The mutual information of X and Y is
I(X, Y) = H(X) −H(X|Y) = H(Y) −H(Y|X),
the amount of information about X conveyed by Y and vice versa.

1.3 · System Entropies and Mutual Information
9
1
If H(X) = H(X|Y), then Y tells us nothing about X, so the mutual information is
zero. This is an unreliable channel and useless as a means of communication.
If H(X|Y) = 0, then knowing Y we know everything about X, so I(X, Y) = H(X).
This is the ideal situation, where when we receive something we know exactly what was
sent.
Example 1.8
Suppose  is the channel with input random variable X deﬁned on {0, 1, 2} in which the
probability P(X = x) = 1
3, for all x ∈{0, 1, 2}. Suppose Y is the output random variable
deﬁned on {0, 1} and that the channel matrix is
(pij) =
⎛
⎜⎝
1 0
0 1
1 0
⎞
⎟⎠,
where the rows are indexed by 0, 1, 2 in that order and the columns are indexed by 0, 1 in
that order.
The conditional entropy
H(Y|X) = 0.
This indicates that knowing X we know Y. Explicitly, we know that if 0 or 2 is sent, then 0
will be received, and if 1 is sent, then 1 will be received.
The entropy of X is
H(X) = −3 1
3 log 1
3 = log 3.
The output probabilities, applying
qj =

i
pipij,
are
(q0, q1) = ( 2
3, 1
3).
The backwards probabilities qij, applying
qij = pijpi
qj
,
are
(qij) =
⎛
⎜⎝
1
2 0
0 1
1
2 0
⎞
⎟⎠.

1
10
Chapter 1 • Shannon’s Theorem
Therefore,
H(X|Y) = −2
32( 1
2 log 1
2) = 2
3 log 2
and the entropy of Y is
H(Y) = −2
3 log 2
3 −1
3 log 1
3 = log 3 −2
3 log 2.
The mutual information is
I(X, Y) = H(X) −H(X|Y) = log 3 −2
3 log 2 = H(Y).
Observe that knowing Y, we do not know X, even though the reverse is true.
■
1.4
Decoding and Transmission Rate
Let  be a channel with input random variable X and output random variable Y deﬁned
on ﬁnite sets S = {s1, . . . , sm} and T = {t1, . . . , tn}, respectively.
A decoding  is a map from {1, . . . , n} to {1, . . . , m}, which induces a map from T
to S by mapping tj to s(j). This map we interpret as the receiver decoding tj as s(j).
For convenience sake, we will sometimes write (v) = u, where v ∈T and u ∈S. The
probability that a decoding is correct is
q(j)j = P(X = s(j)|Y = tj),
the probability that s(j) was sent given that tj was received.
The average probability PCOR of a correct decoding is
PCOR =

j
qjq(j)j.
In most applications we know (pij), how the channel behaves, but not the probability
distributions which deﬁne the random variables X and Y. We choose  to be the
decoding map where (j) is chosen so that p(j)j ⩾pij for all i. This decoding map
is called maximum likelihood decoding.
Suppose that X is deﬁned on the elements of a set A with r symbols.
A block code C is a subset of An. We will often refer to a block code as simply a
code.
The (transmission) rate of C is deﬁned as
R = logr |C|
n
.
Thus, R = 1 if and only if |C| = rn if and only if C = An.

1.5 · Shannon’s Theorem
11
1
The capacity of a channel  is
 = max I(X, Y),
where we maximise over all input random variables X and output random variables Y.
In other words, maximising over all probability distributions pi and qj.
Shannon’s theorem tells us that given the channel, for sufﬁciently large n, there
are block codes of An whose rate R is arbitrarily close to  for which, when we use
maximum likelihood decoding, the probability PCOR is arbitrarily close to 1. To be able
to prove Shannon’s theorem we will require a few lemmas.
For any u, v ∈An, the Hamming distance d(u, v) between u and v is the number
of coordinates in which they differ.
Lemma 1.9 For the binary symmetric channel deﬁned as in Example 1.5 and block code
C ⊆{0, 1}n, maximum likelihood decoding is (v) = u, where u is the closest element of C
to v with respect to the Hamming distance.
Proof
Let φ denote the probability that a symbol does not change when sent through the binary
symmetric channel.
Suppose that d(u, v) = i. Then
P(u was sent | v was received) = φn−i(1 −φ)i = φn(1 −φ
φ
)i,
which is a decreasing function of i (assuming φ > 1 −φ).
Then maximum likelihood decoding will give (v) = u, where u is the closest (with
respect to the Hamming distance) n-tuple to v.
⊓⊔
In general, the decoding map deﬁned by (v) = u, where u is the closest (with
respect to the Hamming distance) n-tuple to v is called nearest neighbour decoding.
1.5
Shannon’s Theorem
We are almost in a position to prove Shannon’s theorem for the binary symmetric
channel. To be able to do so, we ﬁrst calculate the channel capacity.
Lemma 1.10 For the binary symmetric channel the capacity is
 = 1 + φ log2 φ + (1 −φ) log2(1 −φ),
where φ denotes the probability that a symbol does not change.

1
12
Chapter 1 • Shannon’s Theorem
Proof
Let p denote the probability that the input random variable X is 0. Let q denote the
probability that the output random variable Y is 0. Then
H(Y) = −q log2 q −(1 −q) log2(1 −q).
The conditional entropy is
H(Y|X) = −

i,j
pipij log2 pij
= −p(φ log2 φ + (1 −φ) log2(1 −φ)) −(1 −p)(φ log2 φ + (1 −φ) log2(1 −φ)),
and the mutual information is
I(X, Y) = −q log2 q −(1 −q) log2(1 −q) + φ log2 φ + (1 −φ) log2(1 −φ),
since
I(X, Y) = H(Y) −H(Y|X).
To obtain the channel capacity we maximise over all random variables X and Y, which in
this case involves maximising over q. The function
h(q) = −q log2 q −(1 −q) log2(1 −q)
is maximised when q = 1
2 (see ⊡Figure 1.1), where it has the value 1.
⊓⊔
Lemma 1.11 For 0 < λ ⩽1
2,
⌊λn⌋

i=0
n
i

⩽2nh(λ).
Proof
Observe that λ/(1 −λ) ⩽1 implies that, for i = 1, . . . , ⌊λn⌋,

λ
1 −λ
i
⩾

λ
1 −λ
λn
.
By the binomial theorem and the above inequality,
1 = (λ + 1 −λ)n =
n

i=0
n
i

λi(1 −λ)n−i =
n

i=0
n
i

λ
1 −λ
i
(1 −λ)n
⩾
⌊λn⌋

i=0
n
i

λ
1 −λ
i
(1 −λ)n ⩾
⌊λn⌋

i=0
n
i

λ
1 −λ
λn
(1 −λ)n.

1.5 · Shannon’s Theorem
13
1
Hence,
⌊λn⌋

i=0
n
i

⩽λ−λn(1 −λ)−(1−λ)n.
Since λ−λn = 2−nλ log2 λ, the lemma follows.
⊓⊔
The following theorem, Theorem 1.12, is Shannon’s noisy channel coding theorem.
Theorem 1.12
Let δ be an arbitrarily small positive real number and let R be a positive real number
such that R < . For all sufﬁciently large n, there is a code of length n and rate R,
such that when we use maximum likelihood decoding the probability PCOR of a correct
decoding is larger than 1 −δ.
Proof (for the binary symmetric channel)
Choose C to be a random subset of ⌊2nR⌋vectors of {0, 1}n.
Let u ∈C and consider the transmission of u through the binary symmetric channel.
On average, the number of coordinates which will change in the transmission of the n bits
of u is n(1 −φ). As n gets large, the law of large numbers tells us that the probability that
the number of symbols that change in the transmission varies from the average by a ﬁxed
constant tends to zero, so we can assume that the number of symbols which will change in
the transmission of the n bits of u is n(1 −φ).
Suppose that we receive the n-tuple v. By Lemma 1.9, we decode v to the n-tuple in C
that is nearest to v with respect to the Hamming distance.
The probability that we mistakenly decode v to a different element of C, i.e., that there
are other n-tuples of C with Hamming distance at most n(1 −φ) to v, is at most

w∈C\{u}
P(d(w, v) ⩽n(1 −φ)).
Counting the number of n-tuples at Hamming distance at most (1 −φ)n to v, and observing
that there are 2n n-tuples in total,
P(d(w, v) ⩽n(1 −φ)) < 1
2n
⌊(1−φ)n⌋

j=0
n
j

.
Since there are fewer than 2nR n-tuples in C \ {u} and these were chosen randomly,
1 −PCOR < 2nR 1
2n
⌊(1−φ)n⌋

j=0
n
i

.

1
14
Chapter 1 • Shannon’s Theorem
Lemma 1.11 implies that
1 −PCOR < 2n(R−(1+φ log2 φ+(1−φ) log2(1−φ))).
By Lemma 1.10, the capacity of the binary symmetric channel is
 = 1 + φ log2 φ + (1 −φ) log2(1 −φ),
so
1 −PCOR < 2n(R−).
Since R < , for n sufﬁciently large,
2n(R−) < δ,
which proves the theorem.
⊓⊔
We have established that, given a channel with non-zero capacity, there are codes
which allow us to communicate using the channel and decode with a probability of a
correct decoding being close to 1. Our aim will be to ﬁnd such codes which can be
encoded and decoded in an efﬁcient manner.
1.6
Comments
Although ▷Chapter 1 is primarily about Shannon’s theorem [65], it is also a very brief
introduction to information theory. For a more complete introduction, see the excellent
book by Jones and Jones [40] or the classical introduction by Ash [3]. Jones and Jones
[40] also contains an introduction to coding theory, albeit at a lower level to the treatment
here. The idea to measure information dates back to Hartley’s 1928 paper [35], although
Shannon’s work from the 1940s is widely acknowledged as the beginning of information
theory. In 1961, Fano [23] proved that the capacity of a channel bounds the rate at
which reliable transmissions can be achieved. For a proof of this, under some additional
hypothesis, see Jones and Jones [40].
1.7
Exercises
1.1 Calculate the r-ary entropy Hr(X) in Example 1.1 and verify that Hr(X) ⩽logr 11, as
claimed by Theorem 1.4.
1.2 Consider the random variable X deﬁned on the elements of a set with n symbols where
the probability that X is the i-th symbol is γ/2i. Calculate the entropy H2(X) and verify that
H2(X) →2 as n →∞. Compare this with the bound H2(X) ⩽log2 n given by Theorem 1.4.

1.7 · Exercises
15
1
1.3 Let X be the random variable deﬁned on a set S with three symbols with probabilities
1
2, 1
4 and 1
4. Let Xn be the random variable deﬁned on a set Sn in which the probability that
Xn = (s1, . . . , sn) is
n

i=1
pi,
where P(X = si) = pi.
i.
Calculate, for how many symbols, Xn has probability 2j−2n, where j = 0, . . . , n.
ii.
Calculate H2(X) and H2(Xn) and verify that H2(Xn) = nH2(X).
1.4 Calculate the mutual information for the binary erasure channel deﬁned by the channel
matrix
(pij) =

φ 0 1 −φ
0 φ 1 −φ

,
in terms of p and φ, where p is the probability that the input random variable is one of the
symbols.
1.5 Let X and Y be random variables deﬁned on the elements of a set S of size r, where
P(X = s) = 1/r, for all s ∈S, and deﬁne a channel by the matrix (pij), where
pii = φ, and pij = 1 −φ
r −1 , i ̸= j.
Calculate the entropy Hr(Y) and the mutual information I(X, Y).
1.6 Calculate PCOR for the binary symmetric channel, in which a bit changes with
probability φ, using maximum likelihood decoding.
1.7 Prove that for the repetition code C
= {00 . . . 0, 11 . . . 1} ⊂{0, 1}n, applying
nearest neighbour decoding whilst transmitting through a reliable binary symmetric channel,
PCOR →1 and R →0 as n →∞.
1.8 Let  be the channel with input random variable X taking values from {0, 1}, output
random variable Y taking values from {0, 1, ∗} and channel matrix
(pij) =

1 0 0
0 3
4
1
4

.
Let p = P(X = 0).
i.
Calculate H(Y|X), H(Y) and H(X) in terms of p.
ii.
Calculate the capacity of the channel .
iii.
Give an interpretation of the fact that H(X) equals the mutual information I(X, Y).

1
16
Chapter 1 • Shannon’s Theorem
1.9 Let  be the channel with channel matrix
(pij) =

3
4
1
8
1
8
0 1
2
1
2

,
where the input random variable X takes values from {0, 1} and output random variable Y
takes values from {0, 1, ∗}.
Let p = P(X = 0).
i.
Calculate the entropy H(Y) as a function of p.
ii.
Prove that the mutual information is
I(X, Y) = −3
4p log p −1
4(4 −3p) log(4 −3p) + (2 −2p) log 2.
iii.
Calculate the channel capacity of .

17
2
Finite Fields
This chapter is a brief introduction to ﬁnite ﬁelds. The most important facts that will
be established are that ﬁnite ﬁelds necessarily contain ph elements, for some prime
number p and positive integer h, and that the ﬁeld with ph elements is unique, up to
isomorphism. We will study how to factorise cyclotomic polynomials over ﬁnite ﬁelds,
which is used in ▷Chapter 5 to construct cyclic codes. The projective and afﬁne space
over a ﬁnite ﬁeld are also introduced and will appear later in various chapters.
2.1
Definitions and Construction
A commutative ring R is a set with two binary operations, addition and multiplication,
such that it is a commutative group with respect to addition with identity element 0,
and multiplication is commutative (ab = ba), associative ((ab)c = a(bc)), distributive
(a(b + c) = ab + ac) and has an identity element 1.
A ﬁeld is a commutative ring in which every non-zero element has a multiplicative
inverse. In other words, for all a ̸= 0, there is a b such that ab = 1. In particular, this
implies that if ab = 0, then either a = 0 or b = 0.
Example 2.1
The rational numbers Q, the real numbers R and the complex numbers C are all ﬁelds.
■
In the above example, the sum 1 + · · · + 1 is never zero.
Let F be a ﬁeld with multiplicative identity 1. Suppose there is a n for which
summing n ones gives zero and let n be minimal with this property. If p is a divisor
of n, then
1 + · · · + 1



n
= (1 + · · · + 1)



p
(1 + · · · + 1)



n/p
,
which contradicts the minimality of n, since the left-hand is zero implies one of the terms
in the product on the right-hand side is zero. It follows that n is a prime p. The number
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_2

2
18
Chapter 2 • Finite Fields
p is called the characteristic of the ﬁeld. If no such n exists, then the characteristic is
zero.
Throughout the remainder of this chapter p will be a prime number.
Example 2.2
The ring Z/pZ, the integers modulo p, is a ﬁeld of characteristic p and is denoted Fp.
■
The ring Z/nZ is not a ﬁeld when n is not a prime, since it has non-zero elements
which have no multiplicative inverse.
In the following theorem, (f ) denotes the set of elements of the ring of polynomials
Fp[X] which are multiples of the polynomial f . The elements of the quotient ring
Fp[X]/(f ) are cosets of the form g + (f ), where addition is deﬁned as
g + (f ) + h + (f ) = g + h + (f )
and multiplication is deﬁned as
(g + (f ))(h + (f )) = gh + (f ).
One can think of the quotient ring as the polynomials modulo f .
Theorem 2.3
If f is an irreducible polynomial in the ring Fp[X], then Fp[X]/(f ) is a ﬁeld of
characteristic p.
Proof
We have to show that g + (f ) has a multiplicative inverse for all g ∈Fp[X], such that
g + (f ) ̸= 0 + (f ).
Let
B = {gh + (f ) | h + (f ) ∈Fp[X]/(f )}.
If
gh1 + (f ) = gh2 + (f )
then
g(h1 −h2) + (f ) = 0 + (f ).
Since f is irreducible and g is not a multiple of f , this implies that h1 −h2 is a multiple of
f , which implies
h1 + (f ) = h2 + (f ).

2.1 · Definitions and Construction
19
2
Therefore, if
h1 + (f ) ̸= h2 + (f )
then
gh1 + (f ) ̸= gh2 + (f )
and so B contains as many elements as the ﬁnite set Fp[X]/(f ).
In particular, there is an element h + (f ) for which
(g + (f ))(h + (f )) = 1 + (f ),
so g + (f ) has a multiplicative inverse.
⊓⊔
If f is an irreducible polynomial of degree h, then Fp[X]/(f ) is a ﬁeld with ph
elements.
For example, Table 2.1 is the addition and multiplication table of
F2[X]/(X2 + X + 1),
a ﬁnite ﬁeld with four elements and Table 2.2 is the multiplication table of
F3[X]/(X2 + 1),
a ﬁnite ﬁeld with nine elements.
Table 2.1 The addition and multiplication table for the field F2[X]/(X2 + X + 1).
+
0
1
X
1+X
0
0
1
X
1+X
1
1
0
1+X
X
X
X
1+X
0
1
1+X
1+X
X
1
0
.
0
1
X
1+X
0
0
0
0
0
1
0
1
X
1+X
X
0
X
1+X
1
1+X
0
1+X
1
X
Table 2.2 The multiplication table for the field F3[X]/(X2 + 1).
.
0
1
2
X
1+X
2+X
2X
1+2X
2+2X
0
0
0
0
0
0
0
0
0
0
1
0
1
2
X
1+X
2+X
2X
1+2X
2+2X
2
0
2
1
2X
2+2X
1+2X
X
2+X
1+X
X
0
X
2X
2
2+X
2+2X
1
1+X
1+2X
1+X
0
1+X
2+2X
2+X
2X
1
1+2X
2
X
2+X
0
2+X
1+2X
2+2X
1
X
1+X
2X
2
2X
0
2X
X
1
1+2X
1+X
2
2+2X
2+X
1+2X
0
1+2X
2+X
1+X
2
2X
2+2X
X
1
2+2X
0
2+2X
1+X
1+2X
X
2
2+X
1
2X

2
20
Chapter 2 • Finite Fields
2.2
Properties of Finite Fields
Throughout the remainder of this chapter q will be a power of the prime number p.
Theorem 2.4
For all x in a ﬁnite ﬁeld F with q elements xq = x.
Proof
Suppose x ̸= 0. Let A = {ax | a ∈F}. Since a1x ̸= a2x, for a1 ̸= a2, the set A consists of
all the elements of F. If we multiply together the non-zero elements of A, then we obtain the
product of all the non-zero elements of F,

a∈F\{0}
ax =

a∈F\{0}
a,
which implies xq−1 = 1.
⊓⊔
Let Fq denote the splitting ﬁeld of the polynomial Xq −X over the ﬁeld Fp, that is
the smallest ﬁeld extension of Fp in which Xq −X factorises into linear factors.
Theorem 2.5
A ﬁnite ﬁeld with q elements is isomorphic to Fq.
Proof
By Exercise 2.1, q = ph for some prime p. The theorem follows from the uniqueness of
splitting ﬁelds and Theorem 2.4.
⊓⊔
In practice, when we work over a ﬁeld with q elements, we ﬁx an irreducible
polynomial f of degree h and compute in the ring Fp[X]/(f ) which, by Theorem 2.3,
is a ﬁeld with ph elements.
Lemma 2.6 The map σ which maps x →xp is an automorphism of Fq.

2.3 · Factorisation of Cyclotomic Polynomials
21
2
Proof
The characteristic of Fq is p, so
(x + y)p =
p

j=0
p
j

xjyp−j = xp + yp,
for all x, y ∈Fq.
Note that the binomial coefﬁcient is an element of the ﬁeld Fp and since
p
j

is divisible
by p for j = 1, . . . , p −1, it is zero.
Clearly (xy)p = xpyp, so σ preserves the additive and multiplicative structure of the
ﬁeld.
⊓⊔
The automorphism σ in Lemma 2.6 is called the Frobenius automorphism.
Observe that the group of automorphisms generated by σ is a cyclic group of order
h, where q = ph.
2.3
Factorisation of Cyclotomic Polynomials
In this section we will see how cyclotomic polynomials factorise over ﬁnite ﬁelds. These
factorisations will be used principally in ▷Chapter 5 and implicitly in ▷Chapter 10.
As in the previous section q = ph, for some prime p.
By Lemma 2.4, the polynomial Xq−1 −1 factorises into distinct linear factors in
Fq[X].
Lemma 2.7 The polynomial Xq−1 −1 factorises in Fp[X] into distinct irreducible factors
whose degrees are divisors of h.
Proof
Let ϵ ∈Fq and let Fp(ϵ) denote the smallest extension ﬁeld of Fp containing ϵ.
Since ϵ ∈Fq, Fp(ϵ) is a subﬁeld of Fq. This subﬁeld is generated as a vector space
over Fp by 1, ϵ, . . . , ϵr−1, where r is the dimension of this vector space. Hence, there are
a0, . . . , ar ∈Fp, such that
a0 + a1ϵ + · · · + arϵr = 0.
This implies ϵ is a zero of the polynomial
a0 + a1X + · · · + arXr.
For the elements in Fp(ϵ), Lemma 2.4 implies xpr = x. Since xph = x, this implies Xpr −X
divides Xph −X which, by Exercise 2.3, implies r divides h.
⊓⊔
Observe that in the following example X8 −X = X8 + X, since p = 2.

2
22
Chapter 2 • Finite Fields
Example 2.8
The polynomial X8 + X factorises in F2[X] as
(X3 + X + 1)(X3 + X2 + 1)(X + 1)X.
Suppose that e is a root of X3 + X + 1. Then e3 = e + 1, e4 = e2 + e, e5 = e2 + e + 1,
e6 = e2 + 1 and e7 = 1. Therefore, every non-zero element of F2(e) ∼= F8 is a power of e. ■
An element e with the property that every non-zero element x ∈Fq can be written
as x = ei, for some i, is called primitive.
Example 2.9
The polynomial X9 −X factorises in F3[X] as
(X2 + X −1)(X2 −X −1)(X2 + 1)(X −1)(X + 1)X.
■
Observe that the elements of F9 which are roots of X2 + 1 are not primitive.
Example 2.10
The polynomial Xq−1 −1 factorises as
(X(q−1)/2 −1)(X(q−1)/2 + 1)
when q is odd. The roots of the ﬁrst polynomial are the non-zero squares in Fq and the roots
of the second factor are the non-squares in Fq. Note that the squares are never primitive
elements. If −1 is a square, then −1 = a2, for some a ∈Fq, which implies (−1)(q−1)/2 =
aq−1 = 1 and so q ≡1 modulo 4.
■
Lemma 2.11 The polynomial
(X −α1) · · · (X −αm) ∈Fq[X]
if and only if
{α1, . . . , αm} = {αq
1, . . . , αq
m}
as multi-sets.
Proof
Let
f (X) = (X −α1) · · · (X −αm) = a0 + a1X + · · · + amXm.

2.3 · Factorisation of Cyclotomic Polynomials
23
2
Since
(X −αq
1) · · · (X −αq
m) = aq
0 + aq
1 X + · · · + aq
mXm,
f ∈Fq[X] if and only if
f (X) = (X −αq
1) · · · (X −αq
m).
⊓⊔
Suppose we want to factorise Xn −1 in Fq[X].
Our ﬁrst observation is that if (n, q) = q′ > 1, then
Xn −1 = (Xn/q′ −1)q′,
since
q′
j

= 0,
for j = 1, . . . , q′ −1.
Hence, it sufﬁces to know how to factorise Xm −1, where (m, q) = 1, in order to
be able to factorise Xn −1.
We look for an extension ﬁeld of Fq which contains n-th roots of unity by ﬁnding
the minimum h such that n divides qh −1. This is equivalent to calculating the
(multiplicative) order of q in Z/nZ.
An element ϵ ∈Fqh is a primitive n-th root of unity if {1, ϵ, . . . , ϵn−1} is the set of
all n-th roots of unity.
Lemma 2.12 Suppose (n, q) = 1 and let ϵ ∈Fqh be a primitive n-th root of unity. The
irreducible factors of Xn −1 in Fq[X] are given by polynomials
(X −ϵr)(X −ϵrq) . . . (X −ϵrqd−1),
(2.1)
for r = 0, . . . , n−1, where d, which depends on r, is the minimum positive integer such that
rqd ≡r modulo n.
Proof
By Lemma 2.11,
g(X) = (X −ϵr)(X −ϵrq) . . . (X −ϵrqd−1) ∈Fq[X].
Suppose that
f =
m

i=0
aiXi ∈Fq[X]

2
24
Chapter 2 • Finite Fields
and that f (α) = 0. Then
0 =
m

i=0
aiαi =
m

i=0
aiαiq,
which implies f (αq) = 0.
Therefore, g(X) is the minimal degree polynomial in Fq[X] which is zero at ϵr.
Hence, g is irreducible in Fq[X].
⊓⊔
For each r ∈{0, . . . n −1}, the set
{r, rq, rq2, . . . , rqd−1},
where the elements of the set are computed modulo n, is called a cyclotomic coset.
The set {0, . . . n −1} splits into the disjoint union of cyclotomic cosets. Considering
the polynomial in (2.1), we see that a cyclotomic coset of size d corresponds to an
irreducible factor of degree d of Xn −1 in its factorisation over Fq.
Example 2.13
Suppose we wish to factorise X12 −1 in F17[X].
Since 17 ≡5 mod 12, the cyclotomic cosets are
{0}, {1, 5}, {2, 10}, {3}, {4, 8}, {6}, {7, 11}, {9}.
By Lemma 2.12, there are four factors in F17[X] of degree 2 and four factors of degree one.
Alternatively, and somewhat intuitively, X12 −1 factorises as (X6 −1)(X6 + 1) and
X6 −1 = (X3 −1)(X3 + 1) = (X −1)(X2 + X + 1)(X + 1)(X2 −X + 1).
Moreover, −((X/4)6 −1) ≡X6 + 1 mod 17, so
X6 + 1 ≡−((X/4 −1)((X/4)2 + (X/4) + 1)((X/4) + 1)((X/4)2 −(X/4) + 1))
≡(X −4)(X2 + 4X −1)(X + 4)(X2 −4X −1) (mod 17),
which gives an explicit factorisation of X12 −1.
■
2.4
Affine and Projective Spaces over Finite Fields
The projective space PG(k −1, q) is the geometry whose i-dimensional subspaces are
the (i +1)-dimensional subspaces of the vector space Fk
q, for i = 0, . . . , k −2. The 0, 1,
2-dimensional subspaces of PG(k −1, q) are called points, lines, planes, respectively.
The dimension shift is necessary so that familiar geometric properties hold, such as two
points being joined by a line or three non-collinear points span a plane. A hyperplane

2.4 · Affine and Projective Spaces over Finite Fields
25
2
⊡Fig. 2.1 The projective plane over the field of two elements.
is a subspace of co-dimension 1, that is a subspace of one dimension less than the whole
space. A hyperplane of a projective space can be deﬁned as the kernel of a linear form
(i.e. the set of zeros of a linear form). We use the notation
(x1 : x2 : . . . : xk)
to denote the point of the projective space PG(k −1, q) which corresponds to the one-
dimensional subspace spanned by the vector (x1, x2, . . . , xk) of Fk
q.
The geometry of points and lines drawn in ⊡Figure 2.1 is PG(2, 2). In the left-
hand copy the points have been labelled and in the right-hand copy the lines have been
labelled.
The relevance of projective geometries in the study of error-correcting codes is partly
due to the following. Suppose that the rows of a k × n matrix G form a basis of a k-
dimensional subspace of Fn
q. A vector of the subspace is
(u1, . . . , un) = (a1, . . . , ak)G
for some a = (a1, . . . , ak) ∈Fk
q.
We will be particularly interested in how many zero coordinates the vector u has.
Let si be the i-th column of the matrix G. We suppose that si ̸= 0, for all i = 1, . . . , n.
Let · denote the standard scalar product deﬁned on Fk
q.
Observe that ui = 0 if and only if
a · si = 0
if and only if
μa · λsi = 0
for any non-zero λ, μ ∈Fq. Consider the columns of G as a set (or possibly multi-set)
of points in the projective space PG(k −1, q).

2
26
Chapter 2 • Finite Fields
⊡Fig. 2.2 The affine plane over the field of three elements.
Let πa be the hyperplane which is the kernel of the linear form
a1X1 + · · · + akXk.
In geometric terms, the above says that ui = 0 if and only if the point si is incident with
the hyperplane πa. We will return to this in ▷Section 4.4.
The afﬁne space AG(k, q) is the geometry whose i-dimensional subspaces are the
cosets of the i-dimensional subspaces of the vector space Fk
q, i = 0, . . . , k −1. The
geometry of points and lines drawn in ⊡Figure 2.2 is AG(2, 3). As in the projective
space, the 0, 1, 2 and (k −1)-dimensional subspaces of AG(k, q) are called points,
lines, planes and hyperplanes, respectively.
2.5
Comments
The standard reference for ﬁnite ﬁelds is Lidl and Neiderreiter [46], a comprehensive
text dedicated to ﬁnite ﬁelds. The more recent
[52] contains a wealth of results
concerning ﬁnite ﬁelds and their applications. There is a chapter on ﬁnite geometries
by Cameron in [16] and Ball [6] is a textbook dedicated to the subject.
2.6
Exercises
2.1 Prove that a ﬁnite ﬁeld has ph elements, for some prime p and positive integer h.
2.2 Construct the addition and multiplication table for a ﬁeld F3[X]/(X2 + X + 2).
2.3 Prove that Xpr −X divides Xph −X if and only if r divides h. Conclude that a ﬁnite
ﬁeld with ph elements has a subﬁeld with pr elements if and only if r divides h.
2.4 Use one of the irreducible factors of degree 3 in the factorisation of X7 −1 in F2[X] to
construct the multiplication table for a ﬁeld with eight elements.

2.6 · Exercises
27
2
2.5 Determine the degrees of the irreducible factors of
i.
X15 −1 in F17[X],
ii.
X23 −1 in F2[X],
ii.
X12 −1 in F5[X].
2.6
i.
Factorise X9 −1 in F7[X].
ii.
Factorise X11 −1 in F19[X].
iii.
Factorise X8 −1 in F19[X].
2.7 Suppose n is prime and q is primitive in Fn. Prove that Xn−1 factorises into irreducible
factors in Fq[X] as
(X −1)(Xn−1 + · · · + X + 1).
2.8 Prove that

x∈Fq
xj =

0 if q −1 does not divide j,
−1 if q −1 does divide j ̸= 0.
2.9 Label the points of ⊡Figure 2.2 with the vectors from F2
3 (which are the cosets of the
zero-vector) in such a way that three points are collinear if and only if the three vectors are
contained in a coset of a one-dimensional subspace of the vector space. Note that a coset
of a one-dimensional subspace of the vector space contains the vectors (x, y) which are the
zeros of an equation y = mx + c for some m, c ∈F3 or an equation x = c, for some c ∈F3.
2.10 By adding four points and one line to ⊡Figure 2.2 and extending the lines of the
afﬁne plane with one point each, complete the afﬁne plane AG(2, 3) to the projective plane
PG(2, 3).
2.11
i.
Prove that the number of ordered r-tuples of r linearly independent vectors in Fk
q is
(qk −1)(qk −q) · · · (qk −qr−1).
ii.
Prove that the number of (r −1)-dimensional subspaces in PG(k −1, q) is
(qk −1)(qk−1 −1) · · · (qk−r+1 −1)
(qr −1)(qr−1 −1) · · · (q −1)
.
iii.
Prove that the number of (r −1)-dimensional subspaces in PG(k −1, q) containing a
ﬁxed (s −1)-dimensional subspace is equal to the number of (r −s −1)-dimensional
subspaces in PG(k −s −1, q)
2.12 Prove that the geometry one obtains by deleting a hyperplane from the projective space
PG(k, q) is isomorphic to the afﬁne space AG(k, q).

29
3
Block Codes
The main parameters of an error correcting block code, which we will often refer to
simply as a code, are its length and minimum distance. In this chapter, we shall primarily
be concerned with the relationship between the size of the code and these parameters.
If we ﬁx the length of the code, then we wish to maximise the minimum distance and
the size of the code, which are contrary aims. If we ﬁx the minimum distance too, then
we simply consider the problem of maximising the size of the code. We shall prove
the Gilbert–Varshamov lower bound, which is obtained by constructing block codes
of a given length and minimum distance by applying the greedy algorithm. We will
prove various upper bounds which will put limits on just how good a block code one
can hope to ﬁnd of a ﬁxed length and minimum distance. Since Shannon’s theorem is
an asymptotic result telling us what rates we can achieve with a code of arbitrarily long
length, we shall for a large part of this chapter focus on sequences of codes whose length
tends to inﬁnity. If we use nearest neighbour decoding then, so that the probability we
decode correctly does not tend to zero, we will be interested in ﬁnding sequences of
codes for which both the transmission rate and the ratio of the minimum distance to the
length are bounded away from zero. We set aside trying to answer the question of how
these codes are implemented until later chapters in which we work with codes which
have more structure.
3.1
Minimum Distance
Let A be a ﬁnite set of r elements called the alphabet. Recall that a block code (or
simply a code) C is a subset of An. We say that C is an r-ary code of length n. A 2-ary
code is called a binary code and a 3-ary code is called a ternary code.
Our aim will be to choose C so that the codewords, the elements of C, are far
apart with respect to the Hamming distance. In this way, the code will have good error-
correcting properties when we use nearest neighbour decoding.
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_3

3
30
Chapter 3 • Block Codes
Lemma 3.1 Let u, v, w ∈An. The Hamming distance satisﬁes the triangle inequality
d(u, v) ⩽d(u, w) + d(w, v).
Proof
If u and v differ in the i-th coordinate, then w differs from one of u or v in the i-th coordinate.
⊓⊔
Since we will use no other metric on An, we will refer to the Hamming distance
between two elements u and v of An as the distance between u and v.
The minimum distance d of a code C is the minimum distance between any two
codewords of C.
Lemma 3.2 Using nearest neighbour decoding, a block code of minimum distance d can
correct up to 1
2(d −1) errors.
Proof
For any w ∈An and codewords u and v, Lemma 3.1 implies
d ⩽d(u, v) ⩽d(u, w) + d(w, v).
Hence, there is at most one codeword at distance at most 1
2(d −1) from w.
⊓⊔
Example 3.3
Let A = {a1, . . . , ar}. The r-ary repetition code C of length n is a block code with r
codewords where, for each a ∈A, there is a codeword in which a is repeated n times. The
minimum distance of C is n, so C can correct up to 1
2(n −1) errors using nearest neighbour
decoding. The transmission rate of C is logr |C|/n = 1/n.
bit
a1
a2
...
ar
codeword
a1 · · · a1
a2 · · · a2
...
ar · · · ar
■
Example 3.4
The code
C = {000000, 001011, 010101, 100110, 011110, 101101, 110011, 111000}
is a block code of length 6 with 8 codewords which can correct up to 1 error, since it has
minimum distance 3. We can assign to each codeword a distinct triple, which corresponds to
the ﬁrst three bits of the codeword. In this way, for each 3-tuple of bits, we send 6 bits. This
is coherent with the deﬁnition of the transmission rate of C, which is (log2 8)/6 = 1/2.

3.1 · Minimum Distance
31
3
triple
000
001
010
100
011
101
110
111
codeword
000000
001011
010101
100110
011110
101101
110011
111000
■
Let C be a block code of length n and minimum distance d. An extension of C is
code C of length n + 1 obtained by adding a coordinate to each codeword of C in such
a way that C has minimum distance d + 1. The code C is called an extended code of
the code C.
Theorem 3.5
If C is a binary code of length n and minimum distance d and d is odd, then C has an
extension.
Proof
We can suppose that C ⊂{0, 1}n. Let
C = {(u1, . . . , un, un+1) | (u1, . . . , un) ∈C},
where
un+1 = u1 + u2 + · · · + un (mod 2).
Suppose that u and v are two codewords of C.
If d(u, v) ⩾d + 1, then their corresponding codewords are at distance at least d + 1 in
C too.
Suppose d(u, v) = d. Consider the sum of the coordinates of u and v modulo 2. In n −d
coordinates u and v are the same, so these n −d coordinates contribute zero to the sum. In d
coordinates they are different, so we sum d ones and d zeros, which gives d. Since d is odd,
the sum is non-zero modulo 2. Therefore, un+1 ̸= vn+1 and the distance between u and v in
C is d + 1.
⊓⊔
Example 3.6
Let C be the binary block code of length 6 and minimum distance 3 from Example 3.4. The
extended code C is a binary block code of length 7 and minimum distance 4.
codeword in C
000000
001011
010101
100110
codeword in C
0000000
0010111
0101011
1001101
codeword in C
011110
101101
110011
111000
codeword in C
0111100
1011010
1100110
1110001

3
32
Chapter 3 • Block Codes
3.2
Bounds on Block Codes
Let Ar(n, d) denote the maximum |C|, for which there exists a r-ary block code C of length
n and minimum distance d.
Theorem 3.7 (Gilbert–Varshamov bound)
We have the lower bound
Ar(n, d)(1 +
n
1

(r −1) + · · · +

n
d −1

(r −1)d−1) ⩾rn.
■
Proof
Let A be the alphabet, a set of r elements such that C ⊆An, and suppose that C is a block
code of size Ar(n, d).
Let u ∈C.
The set Bd−1(u), of n-tuples in An at distance at most d −1 to u, has size
1 +
n
1

(r −1) + · · · +
n
r

(r −1)d−1,
since there are precisely
n
j

(r −1)j of the n-tuples of An at distance j to u.
If
|C|(1 +
n
1

(r −1) + · · · +

n
d −1

(r −1)d−1) < rn,
then there is an n-tuple which is at distance at least d from all the codewords of C. Therefore,
C is not a code of length n and minimum distance d of maximum size, a contradiction.
■
Recall that the binary entropy function was deﬁned as
h(p) = −p log2(p) −(1 −p) log2(1 −p).
For a code of length n and minimum distance d, we deﬁne the relative minimum
distance to be δ = d/n.
Corollary 3.8 The following inequality holds
1
n log A2(n, d) ⩾1 −h(δ).

3.2 · Bounds on Block Codes
33
3
Proof
By Lemma 1.11 and Theorem 3.7,
A2(n, d)2nh(δ) ⩾2n.
Take logarithms of both sides and divide by n.
■
The sphere-packing problem in Rn asks how many spheres can we pack into a box of
given dimensions or, equivalently, what is the maximum ratio of spheres to the volume of the
box one can achieve. In three dimensions, one can think of packing oranges into a box and
trying to maximise the percentage of space inside the box which is taken up with oranges. In
the discrete world, the analogous problem of packing spheres gives us the following theorem.
Let t = ⌊(d −1)/2⌋. In deference to Lemma 3.2, we will sometimes call a code with
minimum distance d, a t-error correcting code.
Theorem 3.9 (Sphere packing bound)
We have the upper bound
Ar(n, d)(1 +
n
1

(r −1) + . . . +
n
t

(r −1)t) ⩽rn.
Proof
Let A be the alphabet of size r.
Let C ⊆An be a code of size Ar(n, d).
Suppose that u ∈C.
The set Bt(u), of n-tuples in An at distance at most t to u, has size
|Bt(u)| = 1 +
n
1

(r −1) + . . . +
n
t

(r −1)t.
For any pair u, v ∈C, suppose that w ∈Bt(u) ∩Bt(v). By Lemma 3.1,
d(u, v) ⩽d(u, w) + d(w, v) ⩽2t ⩽d −1.
This is a contradiction, since the distance between u and v is at least d.
Therefore,
Bt(u) ∩Bt(v) = ∅.
The total number of n-tuples is rn, so

u∈C
|Bt(u)| ⩽rn,
from which the required bound follows.
■

3
34
Chapter 3 • Block Codes
In contrast to packing spheres into a box in real space, in spaces over a ﬁnite alphabet it
is possible that the whole space is ﬁlled. A block code which meets the sphere packing bound
in Theorem 3.9 is called a perfect code. For there to exist a perfect code we need parameters
n, r and t so that
1 +
n
1

(r −1) + . . . +
n
t

(r −1)t
divides rn.
For example, it is possible that there is a perfect 2-error correcting binary code of length
90, a perfect 3-error correcting binary code of length 23, a perfect 2-error correcting ternary
code of length 11. The former possibility is ruled out in Exercise 3.3. However, as we shall
see in ▷Chapter 5, the latter two perfect codes do occur, see Example 5.9 and Example 5.5.
The proof of the following lemma, Lemma 3.10, counts in two ways the sum of the
distances between any pair of codewords of an r-ary code. In the exercises related to
Lemma 3.10, Exercise 3.4 treats a special case of equality in the bound and Exercises 3.5
and 3.6 exploit the fact that if r does not divide |C|, then improvements can be made to the
coordinate sum estimates.
Lemma 3.10 (Plotkin lemma) An r-ary code C of length n and minimum distance d
satisﬁes
|C|(d + n
r −n) ⩽d.
Proof
Let A be the alphabet, a set of r elements such that C ⊆An. Fix i ∈{1, . . . , n} and let λa
denote the number of times a ∈A occurs in the i-th coordinate of a codeword of C.
Clearly,

a∈A
λa = |C|.
Since

a∈A
(λa −|C|/r)2 ⩾0,
we have

a∈A
λ2
a −2|C|2/r + |C|2/r ⩾0.
Let
S =

u,v∈C
d(u, v),
the sum of all distances between a pair (u, v) of codewords of C.

3.2 · Bounds on Block Codes
35
3
Let Si be the contribution that the i-th coordinate makes to this sum.
Then,
Si =

a∈A
λa(|C| −λa) ⩽|C|2 −|C|2/r,
using the equality and the inequality from above.
Finally,
d|C|(|C| −1) ⩽S =
n

i=1
Si ⩽n(|C|2 −|C|2/r).
■
Example 3.11
Suppose that we wish to ﬁnd a binary code C of length 2d −2 and minimum distance d.
According to the bound in Lemma 3.10, |C| ⩽d. If we suppose that |C| = d, then we have
equality in all the inequalities in the proof of Lemma 3.10. In particular λa = d/2, which
implies that d must be even. Moreover, the distance between any two codewords must be
exactly d.
For d = 4, it is not difﬁcult to ﬁnd such a code, for example,
C = {(0, 0, 0, 0, 0, 0), (1, 1, 1, 1, 0, 0), (1, 1, 0, 0, 1, 1), (0, 0, 1, 1, 1, 1)}.
For d = 6 it is not so straightforward, see Exercise 3.4.
■
Lemma 3.10 only gives a bound on |C| when n < d + n/r. If n ⩾d + n/r then,
shortening the code by deleting coordinates so that for the shortened code n′ < d′ + n′/r,
we can use Lemma 3.10 repeatedly to obtain bounds for block codes with relatively smaller
minimum distance. In Theorem 3.12, we restrict to binary codes. For a general Plotkin bound
on r-ary codes, see Exercise 3.8. The bound in Theorem 3.12 will be used to bound the rate
of sequences of binary codes in ▷Section 3.3.
Theorem 3.12 (Plotkin bound)
If C is a binary code of length n and minimum distance d ⩽1
2n, then
|C| ⩽d2n−2d+2.
Proof
Let m = n −2d + 1. For each x ∈{0, 1}m, let Cx be the subset of C whose ﬁrst m bits are
the string x, where these m bits are then deleted. Then Cx is a block code of length 2d −1
and minimum distance d + e, for some e ⩾0. By Lemma 3.10,

3
36
Chapter 3 • Block Codes
|Cx| ⩽2(d + e)
2e + 1
⩽2d.
Hence,
|C| =

x∈{0,1}m
|Cx| ⩽2m2d = d2n−2d+2.
■
3.3
Asymptotically Good Codes
If we want to send a large amount of data with short length codes, then we have to cut up a
long string of n bits into strings of a ﬁxed length n0. If the probability of decoding the string
of length n0 correctly is p, then the probability of decoding the string of length n is pn/n0,
which tends to zero as n tends to inﬁnity. Shannon’s theorem, Theorem 1.12, tells us that we
should be able to send the string of n bits, through a channel with capacity , which encodes
almost n bits of information, and then decode correctly with a probability approaching 1. In
the proof of Shannon’s theorem, we used the fact the average number of errors which occur
in the transmission of n bits through a binary symmetric channel is (1 −φ)n. Therefore, our
code of length n should be able to correct a number of errors which is linear in n. For this
reason, in a sequence of codes of length n, we want that the minimum distance of the codes
in the sequence also grows linearly with n.
A sequence of asymptotically good codes is a sequence of codes Cn of length n, where
n →∞, in which d/n and log |Cn|/n are bounded away from zero. In other words, both the
relative minimum distance and the rate are bounded away from zero.
For a sequence of asymptotically good binary codes Cn, deﬁne
R = lim inf(log2 |Cn|)/n.
For the remainder of this chapter, we will consider only binary codes and prove bounds
on R. We start by applying the sphere packing bound to obtain an upper bound on R.
Theorem 3.13 (Sphere packing bound)
For a sequence of asymptotically good binary codes of relative minimum distance δ,
R ⩽1 −h( 1
2δ).
Proof
Using Stirling’s approximation n! ∼(n/e)n√
2πn,
log
n
t

∼n log n −t log t −(n −t) log(n −t),

3.3 · Asymptotically Good Codes
37
3
so
1
n log
n
t

∼log n −τ log(τn) −(1 −τ) log((1 −τ)n) = h(τ),
where τ = t/n.
Taking logarithms of both sides in the bound in Theorem 3.9,
nR + log
n
t

⩽n.
Therefore, for n large enough,
R + h(τ) ⩽1,
which proves the bound, since τ = t/n = ⌊(δn −1)/2⌋/n.
■
Applying Plotkin’s bound we can improve on this for larger values of δ, see ⊡Figure 3.1.
Theorem 3.14 (Plotkin bound)
If δ ⩽1
2, then
R ⩽1 −2δ.
Proof
By Theorem 3.12,
2Rn ⩽δn2n−2δn+2.
Taking logarithms, dividing by n and letting n tend to inﬁnity, the bound follows.
■
Our aim now is to improve on Plotkin’s bound. The main idea behind the proof of
Theorem 3.16 is to bound the number of codewords at a ﬁxed distance w from a ﬁxed
n-tuple v. By changing 0 to 1 and 1 to 0 in the coordinates where v has a 1 (applying
the change to all codewords) we obtain a code with the same parameters (length n and
minimum distance d) in which v is now the all-zero n-tuple. Then the number of codewords
at distance w to v is the number of codewords of weight w, where the weight of u ∈{0, 1}n,
denoted wt(u), is the number of non-zero coordinates that u has. Therefore, Lemma 3.15
is really bounding the number of codewords at a ﬁxed distance w to a ﬁxed n-tuple v. In
Exercise 3.12, we will bound the number of codewords at distance at most w to a ﬁxed
n-tuple v. This is an important bound because it tells us that although we cannot in general
correct more than 1
2d errors, even if more errors occur, there are relatively few codewords

3
38
Chapter 3 • Block Codes
⊡Fig. 3.1 Plotting of the
bounds relative minimum
distance (x) against rate (y).
Gilbert–Varshamov
Plotkin
Sphere packing
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
which could have been sent, providing that not more than 1
2n(1 −√1 −2(d/n)) errors
occur. This opens up the possibility of a list decoding algorithm which creates a short list
of possible codewords which could have been sent, even in the case that the received n-tuple
contains a large amount of errors. It is also possible that the list contains just one codeword.
Although e ⩾1
2d errors occur in the transmission, it may be the case that there is only one
codeword at distance e from the received n-tuple. It is even more useful if we use two codes
simultaneously. Take two codes with high rates R1 and R2, which code the same sequence
of bits and suppose both codes are systematic, which means that they conserve the original
message and add some check bits. Example 3.4 is an example of a systematic code and, as
we shall see, so is a linear code with a generator matrix in standard form. Consider the code
we obtain by sending the original message and the two sets of check bits. This code has rate
R1R2
R1 + R2 −R1R2
.
If we have a list decoding algorithm for both codes, then the sent codeword will be in the
intersection of the two lists and with a high probability will be the only candidate in the
intersection.
Let A(n, d, w) denote the maximum size of a binary code of length n and minimum
distance d in which each codeword has exactly w ones.
Lemma 3.15 The following inequality holds
A(n, d, w) ⩽
nd
2w2 −2wn + dn.

3.3 · Asymptotically Good Codes
39
3
Proof
Let C be a binary code of length n and minimum distance d of size A(n, d, w) in which all
codewords have weight w. For any two codewords u, v ∈C, d(u, v) ⩾d, so there are at
least d coordinates in which they differ. One of the two codewords has ones in at least 1
2d of
these coordinates. Hence, the scalar product
u · v ⩽w −1
2d.
Summing over all pairs of codewords we get

u,v∈C
u · v ⩽(w −1
2d)|C|(|C| −1).
Let λi denote the number of times one appears in the i-th coordinate of a codeword of C.
Counting in two ways the number of triples (ui, vi, i) where u and v are codewords such
that ui = vi = 1 and i ∈{1, . . . , n},
n

i=1
λi(λi −1) =

u,v∈C
u · v.
Since every codeword of C has exactly w ones,
n

i=1
λi = w|C|.
The sum
n

i=1
(λi −w
n |C|)2 ⩾0,
which implies that
n

i=1
λ2
i ⩾w2
n |C|2.
Hence,
w2
n |C|2 −w|C| ⩽(w −1
2d)|C|(|C| −1),
from which the bound follows.
■
The bound in the following theorem improves on the Plotkin bound and the sphere
packing bound, see ⊡Figure 3.2. Observe that if we had a better bound on A(n, d, w),
then we could improve the bound on R. Recall that δ is the relative minimum distance.

3
40
Chapter 3 • Block Codes
⊡Fig. 3.2 Plotting the bounds
of relative minimum distance (x)
against rate (y).
Gilbert–Varshamov
Plotkin
Sphere packing
E–B
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Theorem 3.16 (Elias–Bassalygo bound)
If δ < 1
2, then
R ⩽1 −h( 1
2(1 −
√
1 −2δ)).
Proof
Let C be a binary code of length n and minimum distance d = ⌊δn⌋.
For a ﬁxed w ∈{1, . . . , n}, codeword u ∈C and v ∈{0, 1}n, let
b(u, v) =

1 d(u, v) = w,
0 otherwise.
Then,

u∈C

v∈{0,1}n
b(u, v) =

u∈C
n
w

=
n
w

|C|.
For a ﬁxed v, let C′ be the set of codewords of C at distance w to v. If we interchange 0 and
1 in the coordinates where v has a 1, we obtain a code from C′ of constant weight w. The
number of codewords u for which b(u, v) = 1 is |C′|, which is at most A(d, n, w).

3.3 · Asymptotically Good Codes
41
3
Switching the order of the summations in the above equality implies

v∈{0,1}n

u∈C
b(u, v) ⩽2nA(n, d, w).
Hence,
n
w

|C| ⩽2nA(n, d, w).
By Lemma 3.15,
(2w2 −2wn + dn)
n
w

|C| ⩽nd2n.
Now, choose w ∈N so that
2w
n ≈1 −

1 −2d
n + 2
n.
Then,
2w2 −2wn + dn ≈2(w −1
2n)2 −1
2n2 + dn ≈n.
As in the proof of Theorem 3.13, using Stirling’s approximation,
1
n log2
n
w

> h(w/n),
for n large enough. Taking logarithms of
n
w

|C| ⩽d2n,
gives
log2 |C| + nh(w/n) ⩽n + log2 d.
Dividing by n, and letting n tend to inﬁnity, we get the bound.
■
The following bound is an improvement to Elias–Bassalygo bound for δ > 0.14. We
include it here without a proof. There are other improvements to Elias–Bassalygo bound
known but they are quite complicated, even to state.

3
42
Chapter 3 • Block Codes
Theorem 3.17 (McEliece–Rodemich–Rumsey–Welch bound)
R ⩽h( 1
2 −

δ(1 −δ)).
In ⊡Figure 3.3, the Gilbert–Varshamov lower bound is compared to the best upper
bounds we have seen, the Elias–Bassalygo bound and the McEliece–Rodemich–Rumsey–
Welch bound. There has been little progress in closing the gap between the lower and upper
bounds. The following conjecture is one of the oldest and yet still unresolved conjectures in
coding theory.
Conjecture 3.18 Given δ, the Gilbert–Varshamov bound gives the maximum rate for binary
codes with relative minimum distance δ as n →∞.
The following conjecture has been open since 1973.
Conjecture 3.19 Apart from trivial examples, there are no perfect constant weight binary
codes, i.e. there are no constant weight codes achieving the bound in Exercise 3.9.
Although Theorem 3.7 tells us that asymptotically good codes exist, there is a funda-
mental issue which needs to be addressed. We have no feasible algorithm which allows us to
decode such codes. The best we can do, for such a randomly chosen code, is to simply go
Gilbert–Varshamov
M–R–R–W
E–B
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
⊡Fig. 3.3 Plotting of the bounds relative minimum distance (x) against rate (y).

3.5 · Exercises
43
3
through the codewords, one-by-one, until we ﬁnd a codeword which is at a distance at most t
to the received vector. This is laboriously slow and almost never efﬁcient. In fact, we do not
even have a fast algorithm to assign the codewords. In the next few chapters we shall start to
address these issues.
3.4
Comments
There are many texts which cover block codes, the books by Bierbrauer [10], Hill [37],
Roman [61], Berlekamp [8], MacWilliams and Sloane [50], Ling and Xing [48] and Van
Lint [74] being some classical examples. They are all interesting reads and give more insight
into coding theory. In this book we cover much of the material common to these books and
will progress to topics which have been developed since their publication.
The Gilbert–Varshamov bound is from Gilbert [25], Varshamov proving a similar bound
for linear codes in
[75], see Exercise 4.3. There are no known explicit constructions
of asymptotically good codes meeting the Gilbert–Varshamov bound, unless the relative
minimum distance is close to 1
2, see Ta-Shma [69]. There are many improvements to the
Gilbert–Varshamov bound known but these do not outdo the bound asymptotically.
The Elias–Bassalygo bound is from
[7]. The Plotkin bound is from
[58] and the
McEliece, Rodemich, Rumsey and Welch bound highlighted in the text is from [51]. There
are bounds of Delsarte which better these bounds for some values of δ, see [21].
Conjecture 3.19 is from Delsarte’s thesis, see [21]. The main result of Roos [62] restricts
the range of parameters for which equality can occur.
3.5
Exercises
3.1 Prove that if we use the repetition code and nearest neighbour decoding for the binary
symmetric channel in which the probability that a bit changes is 1 −φ < 1
2, then PCOR →1
as n →∞.
3.2 Prove that
C = {(x1, x2, x3, x4, x1 + x2 + x3, x1 + x2 + x4, x1 + x3 + x4) | x1, x2, x3, x4 ∈F2}
is a perfect 1-error correcting binary code of length 7.
3.3 Prove that there is no perfect 2-error correcting binary code of length 90.
3.4 Construct a binary code of length 10 and minimum distance 6 of size 6 and so prove
that A2(10, 6) = 6. Use the solution to prove that the bound in Lemma 3.15 is attainable for
A2(10, 6, 6).

3
44
Chapter 3 • Block Codes
3.5
i.
Prove that there is no ternary code of length 6, minimum distance 5 of size 5.
ii.
Suppose that C is a ternary code of length 6, minimum distance 5 of size 4. Prove that
for every symbol x ∈{0, 1, 2} and every coordinate i, there is a codeword in C with an
x in the i-th coordinate.
iii.
Construct a ternary code of length 6, minimum distance 5 of size 4 and conclude that
A3(6, 5) = 4.
3.6 Prove that A2(8, 5) = 4.
3.7 Let C be the code obtained from two systematic codes of rates R1 and R2 where the two
sets of check bits are appended to the message bits. Prove that the rate of C is
R1R2
R1 + R2 −R1R2
.
3.8 Prove that if n ⩾dr/(r −1), then an r-ary code C satisﬁes
|C| ⩽
drm+1
dr −(n −m)(r −1),
where m is such that m > n −rd/(r −1).
3.9 Prove the sphere packing bound for binary constant weight codes,
 e

i=0
n
i
n −w
i

A(n, 4e + 2, w) ⩽
n
w

.
3.10 Prove that if d is odd, then
A(n, d, w) ⩽
dn + n
2w2 −2wn + dn + n.
3.11 Prove that A(8, 5, 5) = 2.
3.12
i.
Prove that if v1, . . . , vr ∈Rn have the property that vi · vj ⩽0, then r ⩽2n.
ii.
For u = (u1, . . . , un) ∈{0, 1}n, let σ(u) be the vector whose j-th coordinate is (−1)uj .
Prove that
σ(u) · (1, 1, . . . , 1) = n −2wt(u).

3.5 · Exercises
45
3
iii.
For u, v ∈{0, 1}n, prove that
σ(u) · σ(v) = n −2d(u, v).
iv.
Let C be a binary code of length n and minimum distance d. Prove that for u, v ∈C
(σ(u) −λ(1, 1, . . . , 1)) · (σ(v) −λ(1, 1, . . . , 1)) ⩽λ2n + 2λ(2w −n) + n −2d.
v.
By choosing an appropriate λ in iv., prove that if w ⩽1
2n(1 −√1 −2(d/n)), then
w

j=0
A(n, d, j) ⩽2n.

47
4
Linear Codes
There is a lack of structure in the block codes we have considered in the ﬁrst few
chapters. Either we chose the code entirely at random, as in the proof of Theorem 1.12,
or we built the code using the greedy algorithm, as in the proof of the Gilbert–Varshamov
bound, Theorem 3.7. In this chapter, we introduce some algebraic structure to the block
codes by restricting our attention to linear codes, codes whose codewords are the vectors
of a subspace of a vector space over a ﬁnite ﬁeld. Linear codes have the immediate
advantage of being fast to encode. We shall also consider a decoding algorithm for
this broad class of block codes. We shall prove the Griesmer bound, a bound which
applies only to linear codes and show how certain linear codes can be used to construct
combinatorial designs.
4.1
Preliminaries
If A = Fq and C is a subspace of Fn
q, then we say that C is a linear code over Fq
or simply a linear code. If the subspace has dimension k, then we say that C is a k-
dimensional linear code over Fq. Observe that |C| = qk.
As in the case of an n-tuple, we deﬁne the weight wt(v) of a vector v ∈Fn
q as the
number of non-zero coordinates that v has. Recall that the elements of a code are called
codewords.
Lemma 4.1 The minimum distance d of a linear code C is equal to the minimum weight w
of a non-zero codeword of C.
Proof
Suppose u ∈C is a codeword of minimum non-zero weight w. Since C is a subspace, the
zero vector 0 is in C. Clearly d(u, 0) = w, so w ⩾d.
Suppose u and v are two codewords at minimum distance from each other, so d(u, v) =
d. Since C is linear, u −v ∈C, and d(u −v, 0) = d. Hence, there is a codeword in C with
weight d, which implies that d ⩾w.
⊓⊔
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_4

4
48
Chapter 4 • Linear Codes
We can describe a linear code C by means of a basis. A matrix G whose rows are a
basis for C is called a generator matrix for C. Thus,
C = {vG | v ∈Fk
q}.
We will often use the short-hand notation [n, k, d]q code to mean that the code is
a k-dimensional linear code of length n and minimum distance d over Fq. For a not
necessarily linear code, we use the notation (n, K, d)r code to mean a code of length n,
minimum distance d of size K over an alphabet of size r.
Example 4.2
The minimum weight of the non-zero codewords of the 4-dimensional linear code of length
7 over F2 generated by the matrix
G =
⎛
⎜⎜⎜⎝
1 0 0 0 1 1 1
0 1 0 0 1 1 0
0 0 1 0 1 0 1
0 0 0 1 0 1 1
⎞
⎟⎟⎟⎠
is 3, so it follows from Lemma 4.1 that the code is a [7, 4, 3]2 code.
■
Example 4.3
Consider the [9, 3, d]3 code generated by the matrix
G =
⎛
⎜⎝
1 0 0 1 1 0 1 1 2
0 1 0 1 0 1 1 2 1
0 0 1 0 1 1 2 1 1
⎞
⎟⎠.
Each row of G has weight 6 and it is immediate to verify that a linear combination of two
of the rows also has weight 6. Any linear combination of the ﬁrst two rows has at most 3
coordinates in common with the third row, so we can conclude that the minimum weight of
a non-zero codeword is 6. By Lemma 4.1, G is the generator matrix of a [9, 3, 6]3 code.
■
We can also deﬁne a linear code as the solution of a system of linear equations. A
check matrix for a linear code C is an m × n matrix H with entries from Fq, with the
property that
C = {u ∈Fn
q | uHt = 0},
where Ht denotes the transpose of the matrix H.
Lemma 4.4 Let C be a linear code with check matrix H. If every set of d −1 columns of
H are linearly independent, and some set of d columns are linearly dependent, then C has
minimum distance d.

4.1 · Preliminaries
49
4
Proof
Let u be a codeword of C and let D be the set of non-zero coordinates of u, so |D| = wt(u).
Let hi be the i-th column of H. Since H is a check matrix for C,

i∈D
uihi = 0.
Thus, there is a linear combination of |D| columns of H which are linearly dependent.
Applying Lemma 4.1 concludes the proof.
⊓⊔
Example 4.5
Let C be the linear code over Fq deﬁned by the m × n check matrix H, whose columns are
vectors which span distinct one-dimensional subspaces of Fm
q . In other words, the columns
of H are vector representatives of distinct points of PG(m−1, q). Since any two columns are
H are linearly independent, Lemma 4.4 implies that C has minimum distance at least 3. By
Exercise 2.11, the number of points of PG(m −1, q) is (qm −1)/(q −1), so
n ⩽(qm −1)/(q −1).
If we take
n = (qm −1)/(q −1),
then C is a code of size qk with parameters, d = 3 and
k = (qm −1)/(q −1) −m.
This code C attains the bound in Theorem 3.9, since
|C|(1 + n(q −1)) = qk(1 + qm −1) = qn.
Thus, C is a perfect code.
■
Example 4.5 is called the Hamming code. Example 4.2 is the Hamming code with
q = 2 and m = 3. A check matrix for this code is
H =
⎛
⎜⎝
1 1 1 0 1 0 0
1 1 0 1 0 1 0
1 0 1 1 0 0 1
⎞
⎟⎠.
One can readily check that GHt = 0, where G is as in Example 4.2, by verifying that
the scalar product of any row of G with any row of H is zero.

4
50
Chapter 4 • Linear Codes
Lemma 4.6 Let G be a generator matrix for a k-dimensional linear code C. An m×n matrix
H is a check matrix for C if and only if GHt = 0 and the rank of H is n −k.
Proof
Suppose that H is an m × n check matrix for C. All the rows of G are codewords of C, so
if u is a row of G, then uHt = 0, which implies GHt = 0. The dimension of the code C is
n −rank(H), which implies that the rank of H is n −k.
Suppose that GHt = 0 and that the rank of H is n −k. A codeword u is a linear
combination of the rows of G, so uHt = 0. Hence, the left kernel of Ht contains C. Since the
rank of H is n −k, the left kernel of Ht has dimension k, so the left kernel of Ht is C.
⊓⊔
Let Ir denote the r × r identity matrix.
A generator matrix which has the form
(Ik | A) ,
for some k × (n −k) matrix A, is said to be in standard form. The uncoded string v
is encoded by vG, whose ﬁrst k coordinates are precisely the coordinates of v. There
are obvious advantages in using a generator matrix in this standard form. Once errors
have been corrected, the uncoded string can be recovered from the codeword by simply
deleting the last n −k coordinates. Moreover, the following lemma implies that there is
a check matrix with a similar simple form.
Lemma 4.7 Let C be the linear code generated by
G = (Ik | A) ,
for some k × (n −k) matrix A. Then the matrix
H =

−At | In−k

is a check matrix for C.
Proof
We have to check that the inner product of the i-th row of G = (gij) with the ℓ-th row of
H = (hℓj) is zero. The entries gij = 0 for j ⩽k unless i = j, in which case gii = 1. The
entries hℓj = 0 for j ⩾k + 1 unless ℓ= j −k, in which case hℓ,ℓ+k = 1. Hence,
n

j=1
gijhℓj =
k

j=1
gijhℓj +
n

j=k+1
gijhℓj = hℓi + gi,ℓ+k = −aiℓ+ aiℓ= 0.
⊓⊔

4.2 · Syndrome Decoding
51
4
4.2
Syndrome Decoding
Given a generator matrix G for a linear code C, encoding is fairly simple since we
assign the codeword vG to each vector v of Fk
q. Moreover, if the generator matrix is in
standard form, as described in the previous section, then we can encode by appending the
n −k coordinates of vA to v. Decoding is a far trickier affair. To use nearest neighbour
decoding we have to ﬁnd the codeword of length n which is nearest to the received n-
tuple. For a code with no obvious structure, this can only be done by calculating the
distance between the received n-tuple and each codeword, something which is laborious
and infeasible for large codes. In this section, we consider a decoding algorithm for
linear codes which exploits the linearity property.
Let C be a linear code with check matrix H. The syndrome of a vector v ∈Fn
q is
s(v) = vHt.
Note that s(v) = 0 if and only if v ∈C, since
C = {v ∈Fn
q | vH t = 0}.
To use syndrome decoding we compute a look-up table with entries s(e) for all
vectors e of weight at most t = ⌊(d −1)/2⌋. To decode a vector v we compute s(v),
use the look-up table to ﬁnd e such that s(v) = s(e), and decode v as v −e. Note that
v −e ∈C and the distance between v and v −e is at most t.
Example 4.8
The matrix
G =
⎛
⎜⎜⎜⎝
1 0 0 0 0 1 1 1
0 1 0 0 1 0 1 1
0 0 1 0 1 1 0 1
0 0 0 1 1 1 1 0
⎞
⎟⎟⎟⎠
is the generator matrix of a [8, 4, 4]3 code.
Suppose that a codeword u has been sent and we have received the vector
v = (1, 0, 1, 0, 0, 1, 0, 2).
By Lemma 4.7, the matrix
H =
⎛
⎜⎜⎜⎝
0 2 2 2 1 0 0 0
2 0 2 2 0 1 0 0
2 2 0 2 0 0 1 0
2 2 2 0 0 0 0 1
⎞
⎟⎟⎟⎠
is a check matrix for C.

4
52
Chapter 4 • Linear Codes
Note that −1 = 2, since we are doing arithmetic with elements of F3.
To decode using syndrome decoding, we calculate the syndrome of v,
s(v) = vHt = (2, 2, 2, 0).
Then we look for the low weight vector e, in this example a vector of weight one, such that
s(v) = s(e). If only one error has occurred in the transmission, the syndrome s(v) must be
equal to s(e), for some vector e of F8
q of weight one. Indeed,
s(v) = s((0, 0, 0, 1, 0, 0, 0, 0)).
Therefore, we correct v to the codeword
v −e = (1, 0, 1, 2, 0, 1, 0, 2),
which is (1, 0, 1, 2)G.
■
In general, using a look-up table would involve searching through
t
j=1
n
j

(q −1)j
entries, an entry for each non-zero vector of Fn
q of weight at most t. For n large, this
implies that we would have to search through a table with an exponential number of
entries, since
 n
1
2δn

∼2h( 1
2 δ)n.
This does not imply that there might not be a better method to ﬁnd the vector e with the
property that s(e) = s(v), especially if the linear code has some additional properties
we can exploit. However, we will now prove that decoding a linear code using syndrome
decoding is an NP problem. Under the assumption that P ̸= NP, this implies that there
is no polynomial time algorithm that will allow us to decode using syndrome decoding.
Problems in NP are, by deﬁnition, decision problems. So what we mean by saying
that decoding a linear code using syndrome decoding is an NP problem, is that deciding
if we can decode a linear code using syndrome decoding is an NP problem. A decision
problem is in P if there exists a polynomial time algorithm which gives a yes/no answer
to the problem. A decision problem is in NP, if there exists a polynomial time algorithm
which veriﬁes that a “yes” solution to the problem, really is a solution. For example,
the Hamiltonian path problem asks if there is a path in a graph which visits all the
vertices without repeating any vertex. This is an NP problem since a “yes” solution to
the problem is a Hamiltonian path. This solution can be checked in polynomial time by
checking that each edge in the path is an edge of the graph.

4.2 · Syndrome Decoding
53
4
It is not known if NP is a larger class of problems than P or not. A decision problem
D is said to be NP-complete if there is a polynomial time algorithm which reduces every
problem in NP to D. This implies that if we had a polynomial time algorithm to solve
D, then we would have a polynomial time algorithm to solve all problems in NP.
Let T be a subset of {1, . . . , n}3.
A perfect matching M is a subset of T of size n,
M = {(aj1, aj2, aj3) | j = 1, . . . , n} ⊆T,
where for all i ∈{1, 2, 3},
{aji | j = 1, . . . , n} = {1, . . . , n}.
Deciding whether T has a perfect matching or not is the three-dimensional
matching problem. This decision problem is NP-complete.
For example, let T be the set of triples
{(1, 1, 1), (1, 2, 3), (1, 4, 2), (2, 1, 4), (2, 3, 3), (3, 2, 1), (3, 3, 4),
(4, 3, 2), (4, 3, 3), (4, 4, 4)} .
The three-dimensional matching problem asks if it is possible to ﬁnd a subset M of T
such that each element of {1, 2, 3, 4} appears in each coordinate of an element of M
exactly once. In this example the answer is afﬁrmative,
M = {(1, 4, 2), (2, 1, 4), (3, 2, 1), (4, 3, 3)}.
Theorem 4.9
Decoding a linear code using syndrome decoding is NP-complete.
Proof
To decode a linear code using syndrome decoding, we have to ﬁnd a vector e of weight at
most t, such that eHt = s, where s = s(v) and v is the received vector.
We make this a decision problem by asking if there is a vector e of weight at most t such
that eHt = s. We will show that this decision problem is NP-complete by proving that if
we had a polynomial time algorithm to solve this decision problem, then we would have a
polynomial time algorithm to solve the three-dimensional matching problem.
Let Ri = {1, . . . , n} for i = 1, 2, 3. Let T be a subset of R1 × R2 × R3. Consider
the matrix A whose rows are indexed by the triples in T , whose columns are indexed by
R1 ∪R2 ∪R3, where the ((a1, a2, a3), ri) entry is 1 if ai = ri and zero otherwise. Thus, each
row has three ones and 3n −3 zeros. A perfect matching is given by a vector v of {0, 1}|T |,
necessarily of weight n, such that vA is equal to the all-one vector j. Therefore, if we have a

4
54
Chapter 4 • Linear Codes
polynomial time algorithm which can decide if there is a vector e of weight at most t, such
that eH t = s, then we can use this to solve the three-dimensional perfect matching decision
problem by asking if there is a vector v of weight n such that vA = j.
⊓⊔
4.3
Dual Code and the MacWilliams Identities
Let C be a k-dimensional linear code over Fq.
The dual code of a linear code C is
C⊥= {v ∈Fn
q | u · v = u1v1 + · · · + unvn = 0, for all u ∈C}.
In other words C⊥is the orthogonal subspace to C, with respect to the standard inner
product. The subspace C⊥is the set of solutions of a homogeneous system of linear
equations of rank k in n unknowns. Hence, the dual code C⊥is a (n −k)-dimensional
linear code and length n over Fq.
The following lemma is immediate.
Lemma 4.10 If H is a (n −k) × n check matrix for a k-dimensional linear code C, then H
is a generator matrix for C⊥. Likewise, if G is a generator matrix for C, then G is a check
matrix for C⊥.
If C = C⊥, then we say that C is self-dual.
Example 4.11
The extended code of the binary four-dimensional code in Example 4.2 is a self-dual code. It
has a generator (and check) matrix
⎛
⎜⎜⎜⎝
1 0 0 0 1 1 1 0
0 1 0 0 1 1 0 1
0 0 1 0 1 0 1 1
0 0 0 1 0 1 1 1
⎞
⎟⎟⎟⎠.
■
Let Ai denote the number of codewords of weight i of a linear code C of length n.
The weight enumerator of C is a polynomial deﬁned as
A(X) =
n

i=0
AiXi.
Let A⊥(X) denote the weight enumerator of the dual code C⊥.

4.3 · Dual Code and the MacWilliams Identities
55
4
There is an important relationship between A(X) and A⊥(X), which implies that
one is determined by the other. To be able to prove this relationship, which we shall do
in Theorem 4.13, we introduce the trace map and characters.
Let p be the prime such that q = ph. Then the trace map from Fq to Fp is deﬁned
as
Tr(x) = x + xp + · · · + xq/p.
By Lemma 2.6, it is additive, i.e.
Tr(x + y) = Tr(x) + Tr(y),
and by Lemma 2.4 and Lemma 2.6,
Tr(x)p = Tr(x),
so, again by Lemma 2.4, Tr(x) ∈Fp.
Observe that if Tr(λx) = 0 for all λ ∈Fq, then x = 0, since as a polynomial (in λ) it
has degree q/p. For the same reason, every element of Fp has exactly q/p pre-images
of the trace map from Fq to Fp.
For u ∈Fn
q, we deﬁne a character as a map from Fn
q to C by
χu(x) = e
2πi
p Tr(x·u).
Note that this deﬁnition makes sense since Fp is Z/(pZ).
Lemma 4.12 Let C be a linear code over Fq. Then

u∈C
χu(x) =

0 if x ̸∈C⊥
|C| if x ∈C⊥.
Proof
If x ∈C⊥, then x · u = 0 for all u ∈C which implies χu(x) = 1 for all u ∈C and we are
done.
Suppose x ̸∈C⊥. If χv(x) = 1 for all v ∈C, then Tr(v · x) = 0 for all v ∈C, so
Tr(λ(v · x)) = 0 for all λ ∈Fq and v ∈C. This, we observed before, implies v · x = 0 for
all v ∈C, so x ∈C⊥, a contradiction. Thus, there is a v ∈C such that χv(x) ̸= 1. Then,
χv(x)

u∈C
χu(x) =

u∈C
χu+v(x) =

u∈C
χu(x).
which implies

u∈C
χu(x) = 0.
⊓⊔

4
56
Chapter 4 • Linear Codes
The following theorem relates the weight enumerator of a linear code to the weight
enumerator of its dual code. It is known as the MacWilliams identities.
Theorem 4.13 (MacWilliams)
For a k-dimensional linear code C over Fq of length n we have
qkA⊥(X) = (1 + (q −1)X)nA

1 −X
1 + (q −1)X

.
Proof
Let u = (u1, . . . , un) ∈Fn
q.
If ui ̸= 0, then

wi∈Fq
χwiei(u) = 0,
since we sum each p-th root of unity q/p times, and the sum of the p-th roots of unity is
zero.
Therefore,

wi∈Fq\{0}
χwiei(u) =

q −1
if ui = 0
−1
if ui ̸= 0
and so
n

i=1

1 +

wi∈Fq\{0}
χwiei(u)X

= (1 + (q −1)X)n−wt(u)(1 −X)wt(u).
Multiplying out the brackets,
n

i=1

1 +

wi∈Fq\{0}
χwiei(u)X

=

w∈Fnq
Xwt(w)
n

i=1
χwiei(u) =

w∈Fnq
Xwt(w)χu(w).
Combining the above two equations,

w∈Fnq
Xwt(w)χu(w) = (1 + (q −1)X)n−wt(u)(1 −X)wt(u).
Summing over u ∈C, we have

u∈C

w∈Fnq
Xwt(w)χu(w) = (1 + (q −1)X)nA

1 −X
1 + (q −1)X

,

4.3 · Dual Code and the MacWilliams Identities
57
4
since
A(X) =

u∈C
Xwt(u).
Switching the order of the summations, and applying Lemma 4.12,

w∈Fnq
Xwt(w) 
u∈C
χu(w) =

w∈C⊥
Xwt(w)|C| = |C|A⊥(X).
⊓⊔
Observe that Theorem 4.13 implies that if we know the weights of the codewords
of C, then we know the weights of the codewords of C⊥and in particular the minimum
weight of a non-zero codeword and therefore, by Lemma 4.1, the minimum distance of
C⊥.
If C is a self-dual code, we can get information about the weights of the codewords
of C from Theorem 4.13.
Example 4.14
Let C be a self-dual 4-dimensional binary linear code of length 8, for instance, as in
Example 4.11. Then, equating the coefﬁcient of Xj, for j = 0, . . . , 8, in
A(X) = 2−4(1 + X)8A((1 −X)/(1 + X)),
where
A(X) = 1 +
8

i=1
aiXi,
will give a system of nine linear equations and eight unknowns.
This system has the solution
A(X) = 1 + 14X4 + X8 + λ(X2 −2X4 + X6),
for some λ ∈{0, . . . , 7}. Thus, C must contain the all-one vector and if the minimum distance
of C is 4, then
A(X) = 1 + 14X4 + X8.
■
We will see an important application of the MacWilliams identities in ▷Section 4.6
where we will exploit these equations to prove that, under certain hypotheses, we can
construct combinatorial designs from a linear code.

4
58
Chapter 4 • Linear Codes
4.4
Linear Codes and Sets of Points in Projective Spaces
A linear code C is the row space of a generator matrix G. The multi-set S of columns of
G also contains information about the code and its parameters. The length of C is |S|, the
dimension of C is the length of the vectors in S and, as we shall prove in Lemma 4.15,
the weights of the codewords in C can be deduced from the intersection of S with the
hyperplanes of Fk
q. Observe that S is a multi-set since columns can be repeated.
Lemma 4.15 The multi-set S of columns of a generator matrix G of a [n, k, d]q code C is a
multi-set of n vectors of Fk
q in which every hyperplane of Fk
q contains at most n −d vectors
of S, and some hyperplane of Fk
q contains exactly n −d vectors of S.
Proof
There is a bijection between the vectors of Fk
q and the codewords, given by
v →vG.
For each non-zero vector v of Fk
q, the subspace consisting of the vectors (x1, . . . , xk) ∈
Fk
q, such that
v1x1 + · · · + vkxk = 0,
is a hyperplane of Fk
q, which we denote by πv. The non-zero multiplies of v deﬁne the same
hyperplane, so πv = πλv, for all non-zero λ ∈Fq.
We can label the coordinates of vG by the elements of S. The s-coordinate of the
codeword vG is the value of the scalar product v · s. The scalar product v · s = 0 if and
only if s ∈πv. Therefore, the codeword vG has weight w if and only if the hyperplane πv
contains n −w vectors of S. The lemma follows since, by Lemma 4.1, the minimum weight
of a non-zero vector of C is equal to the minimum distance.
⊓⊔
Lemma 4.15 is still valid if we replace a vector s of S by a non-zero scalar multiple
of s. Thus, we could equivalently state the lemma for a multi-set of points in PG(k −
1, q), assuming that the vectors in S are non-zero vectors. In the projective space, the
hyperplane πv is a hyperplane of PG(k −1, q). The s-coordinate of the codeword vG
is zero if and only if the point s is incident with the hyperplane πv, as we saw in ▷
Section 2.4.
We could also try and construct a multi-set S of points of PG(k −1, q) in which we
can calculate (or at least bound) the size of the intersections of S with the hyperplanes
of PG(k −1, q). Then Lemma 4.15 implies that we can bound from below the minimum
distance of the linear code we obtain from a generator matrix whose columns are vector
representatives of the points of S.
Example 4.16
Let φ(X) = φ(X1, X2, X3) be an irreducible homogeneous polynomial over Fq in three
variables of degree m. Let S be the set of points of PG(2, q) which are zeros of this

4.5 · Griesmer Bound
59
4
polynomial. Since φ is irreducible, each line of PG(2, q) contains at most m points of S.
By Lemma 4.15, the matrix whose columns are a vector representative of the points of S is
a 3 × |S| matrix which generates a code with minimum distance at least n −deg φ. This can
give an easy way to make codes with surprisingly good parameters. For example, suppose q
is a square and we take the Hermitian curve, deﬁned as the zeros of the polynomial
φ(X) = X
√q+1
1
+ X
√q+1
2
+ X
√q+1
3
.
This curve has q√q+1 points and is irreducible. Thus we obtain a [q√q+1, 3, q√q−√q]q
code.
■
We say that two codes are equivalent if one can be obtained from the other by
a permutation of the coordinates and permutations of the symbols in each coordinate.
Note that non-linear codes can be equivalent to linear codes. Indeed, one can obtain a
non-linear code (of the same size, length and minimum distance) from a linear code by
simply permuting the symbols of Fq in a ﬁxed coordinate.
We can use S to obtain a model for all codes that are equivalent to a linear code
C, this is called the Alderson–Bruen–Silverman model. Let S be the multi-set of n
points of  = PG(k −1, q), obtained from the columns of a generator matrix G of the
k-dimensional linear code C of length n. For each point (s1 : . . . : sk) of S, we deﬁne a
hyperplane πs of  = PG(k −1, q) as the kernel of the linear form
αs(X) = s1X1 + · · · + skXk.
We embed  in a PG(k, q) and consider PG(k, q) \  which, by Exercise 2.12, is
isomorphic to AG(k, q). Within PG(k, q), we label each hyperplane (̸= ) containing
πs with an element of Fq. For each point v of the afﬁne space PG(k, q) \  we
obtain a codeword u of C′, a code equivalent to the code C. The coordinates of u are
indexed by the elements of S, and the s-coordinate of u is the label given to the unique
hyperplane of PG(k, q) spanned by πs and v. Observe that two codewords u and u′ of
C′ (obtained from the points v and v′, respectively) agree in an s-coordinate if and only
if αs(v) = αs(v′). The vectors vG and v′G are codewords of C, so agree in at most
n −d coordinates, which implies that there are at most n −d elements s ∈S such that
αs(v) = αs(v′). Thus, u and u′ agree in at most n −d coordinates. Furthermore, there
are two codewords which agree in exactly n −d coordinates. Therefore, the code C′ is
of length n and minimum distance d. It is Exercise 4.10, to prove that the code C′ is
equivalent to the linear code C. This model is used in Exercise 4.11 to prove that if a
linear code has a non-linear extension, then it has a linear extension.
4.5
Griesmer Bound
In ▷Chapter 3 we proved various bounds involving the length, the minimum distance
and the size of a block code. In this section, we shall prove another bound involving these

4
60
Chapter 4 • Linear Codes
parameters, the Griesmer bound, which is speciﬁcally for linear codes. The Griesmer
bound follows almost directly from the following lemma.
Lemma 4.17 If there is a [n, k, d]q code, then there is a [n −d, k −1, ⩾

d
q

]q code.
Proof
Let S be the multi-set of columns of a generator matrix G of a k-dimensional linear code C
of length n and minimum distance d over Fq.
By Lemma 4.15, there is a non-zero vector v ∈Fk
q such that the hyperplane πv of Fk
q
contains n −d vectors of S. Let S′ be this multi-set of n −d vectors. Let G′ be the k ×
(n −d) matrix whose columns are the vectors of S′. The matrix G′ generates a linear code
C′, obtained from G′ by left multiplication by a vector of Fk
q. The matrix G′ is not, strictly
speaking, a generator matrix of C′, since its rows are not linearly independent. The vector v
is in the left nucleus of G′. The code C′ is the subspace spanned by the rows of the matrix
G′.
We want to prove that C′ is a (k −1)-dimensional linear code. The rank of G′ is at most
k −1, since vG′ = 0. If the rank is less than k −1, then there is another vector v′ ∈Fk
q,
not in the subspace spanned by v, for which v′G′ = 0. But then we can ﬁnd a λ ∈Fq
such that (v + λv′)G has zeros in more than n −d coordinates, which implies that C has
non-zero codewords of weight less than d, which contradicts Lemma 4.1. Hence, C′ is a
(k −1)-dimensional linear code.
Let d′ be the minimum distance of the code C′ . By Lemma 4.15, there is a hyperplane
π′ of πv which contains n −d −d′ vectors of S′. By Exercise 2.12, there are precisely
q + 1 hyperplanes of Fk
q containing the co-dimensional two subspace π′. Each one of these
hyperplanes contains at most n −d vectors of S and so at most d′ vectors of S \ π′. Hence,
n ⩽(q + 1)d′ + n −d −d′,
which gives
d′ ⩾
d
q

.
⊓⊔
Theorem 4.18 (Griesmer bound)
If there is a [n, k, d]q code, then
n ⩾
k−1

i=0
 d
qi

.

4.5 · Griesmer Bound
61
4
Proof
By induction on k.
For k = 1 the bound gives n ⩾d, which is clear.
By Lemma 4.17, there is a [n −d, k −1, d′]q code, where
d′ ⩾
d
q

.
By induction,
n −d ⩾
k−2

i=0
 d′
qi

⩾
k−2

i=0
 d
qi+1

=
k−1

i=1
 d
qi

.
⊓⊔
Example 4.19
Consider the problem of determining the largest ternary code C of length 10 and minimum
distance 4. The Plotkin bound from Lemma 3.10 does not apply, since d+n/r−n is negative.
The sphere packing bound, Theorem 3.9, implies
|C| ⩽310/21.
The Griesmer bound tells us that if there is a linear code with these parameters, then
10 ⩾4 + 2 + k −2.
and so
|C| ⩽36.
To construct such a code, according to Lemma 4.15, we need to ﬁnd a set S of 10 points in
PG(5, 3) with the property that any hyperplane is incident with at most 6 points of S. Let G
be the 6 × 10 matrix whose columns are vector representatives of the 10 points of S. The
matrix G is the generator matrix of a [10, 6, 4]3 code. Such a matrix G can be found directly,
see Exercise 4.14. However, we can construct such a code geometrically in the following
way.
Let C⊥be the linear code over Fq generated by the 4×(q2+1) matrix H, whose columns
are the points of an elliptic quadric. For example, we could take the elliptic quadric deﬁned
as the zeros of the homogeneous quadratic form
X1X2 −f (X3, X4),
where f (X3, X4) is an irreducible homogeneous polynomial of degree two. Explicitly the
points of the quadric are

4
62
Chapter 4 • Linear Codes
{(1, f (x, y), x, y) | x, y ∈Fq} ∪{(0, 1, 0, 0)}.
As in the real projective space, the elliptic quadric has no more than two points incident with
any line. To verify this algebraically, consider the line which is the intersection of the planes
deﬁned by X1 = a3X3 + a4X4 and X2 = b3X3 + b4X4. The x3 and x4 coordinates in the
intersection with the quadric satisfy
(a3x3 + a4x4)(b3x3 + b4x4) −f (x3, x4) = 0,
which is a homogeneous polynomial equation of degree two in two variables. It is not
identically zero, since f is irreducible, so there are at most two (projectively distinct or
homogeneous) solutions for (x3, x4); the x1 and x2 coordinates are then determined by
x1 = a3x3 + a4x4 and x2 = b3x3 + b4x4. This checks the intersection with q4 lines, the
intersection with the remaining lines can be checked similarly.
Therefore, any three columns of the matrix H are linearly independent, since three
linearly dependent columns would imply three collinear points on the elliptic quadric. The
elliptic quadric has four co-planar points, so H has four linearly dependent columns. By
Lemma 4.4, C has a minimum distance 4 and is therefore a [q2 + 1, q2 −3, 4]q code.
Substituting q = 3, we obtain a ternary linear code C meeting the Griesmer bound.
The geometry also allows us to calculate the weight enumerator of C⊥and hence the
weight enumerator of C. Since any three points span a plane which intersects the elliptic
quadric in a conic, and a conic contains q + 1 points, there are
(q2 + 1)q2(q2 −1)
(q + 1)q(q −1)
= (q2 + 1)q
planes incident with q + 1 points of the elliptic quadric and the remaining q2 + 1 planes are
incident with exactly one point. This implies that C⊥has (q2 + 1)q(q −1) codewords of
weight q2 −q, (q2 + 1)(q −1) codewords of weight q2 and one codeword of weight zero.
For q = 3, the weight enumerator of C⊥is
A⊥(X) = 1 + 60X6 + 20X9.
The MacWilliams identities, Theorem 4.13, imply that C has weight enumerator,
A(X) = 1 + 60X4 + 144X5 + 60X6 + 240X7 + 180X8 + 20X9 + 24X10.
Even if we do not restrict ourselves to linear codes, there is no larger code known with these
parameters. The best known upper bound is |C| ⩽891.
■
Example 4.20
Consider the problem of determining if there is a (16, 256, 6)2 code C, that is a binary code
of length 16 with minimum distance 6 and size 256. The sphere packing bound, Theorem 3.9,
implies

4.6 · Constructing Designs from Linear Codes
63
4
|C|(1 + 16 +
16
2

) ⩽216,
which is satisﬁed. The Plotkin bound, Theorem 3.12, does not give a contradiction since
|C| ⩽d2n−2d+2 = 384.
Now, suppose that the code is linear, so C is a [16, 8, 6]2 code. The Griesmer bound is
also satisﬁed since,
n ⩾6 +
6
2

+
6
4

+
7

i=3
 6
2i

= 16.
However, Lemma 4.17 implies the existence of a [10, 7, ⩾3]2 code. This code is a 1-error
correcting binary code of length 10, so the sphere packing bound, Theorem 3.9, implies that
(1 + 10)27 ⩽210,
which is a contradiction. Therefore, there is no [10, 7, ⩾3]2 code. Hence, there is no
[16, 8, 6]2 code. However, there is a non-linear (16, 256, 6)2 code and we shall construct
one both in ▷Chapter 9 and in ▷Chapter 10.
■
4.6
Constructing Designs from Linear Codes
A τ-design is a collection D of κ-subsets of {1, . . . , n} with the property that every τ-
subset of {1, . . . , n} is contained in precisely λ subsets of D, for some ﬁxed positive
integer λ. If we want to specify the parameters, then we say that D is a τ-(n, κ, λ)
design.
Let u ∈Fn
q. The support of u = (u1, . . . , un) is a subset of {1, . . . , n} deﬁned as
{i ∈{1, . . . , n} | ui ̸= 0}.
In this section we shall prove that if the codewords of the dual of a linear code C have
few distinct weights, then one can construct τ-designs from the supports of codewords
of C of a ﬁxed weight. Before proving the main theorem, we will prove by counting that
we can construct a 3-design from the extended Hamming code, Example 4.11.
Example 4.21
In Example 4.14, we calculated the weight distribution for the extended Hamming code in
Example 4.11 and deduced that there are 14 codewords of weight 4. Two codewords u and v
of weight 4 have at most two 1’s in common, since otherwise u + v would be a codeword of
weight 2. Therefore, every 3-subset of {1, . . . , 8} is contained in the support of at most one
codeword of weight 4. There are 14
4
3

= 56 subsets of size 3 of the 14 supports of the 14
codewords of weight 4 and
8
3

= 56 subsets of size 3 of {1, . . . , 8}. Hence, each 3-subset is

4
64
Chapter 4 • Linear Codes
contained in a unique support of a codeword of weight 4 and we have deduced that the set of
these supports is a 3-(8, 4, 1) design.
■
In the following theorem, κ can be any number in the set {d, . . . , n} in the case that
q = 2, since the condition is vacuous. If q ̸= 2, then, by Exercise 4.15, the condition is
surely satisﬁed if
κ ∈{d, . . . , d −1 +
d −1
q −2

}.
In order to simplify the statement of the following theorem, we say that C has a weight
w if there is a codeword of C of weight w.
Theorem 4.22
Let C be an [n, k, d]q code such that C⊥has at most d −τ non-zero weights of weight
at most n −τ, for some τ ⩽d −1. If κ has the property that two codewords of C of
weight κ have the same support if and only if they are multiples of each other, then the
set of supports of the codewords of C of weight κ is a τ-(n, κ, λ) design, for some λ.
Proof
Let T be a τ-subset of {1, . . . , n}. Let C \ T be the code obtained from C by deleting the
coordinates indicated by the elements of T . If after deleting τ coordinates the codewords u
and v are the same, then u and v differ in at most τ coordinates. Since τ ⩽d −1, this cannot
occur, so deleting the coordinates does not reduce the number of codewords. Hence, C \ T is
a k-dimensional linear code of length n −τ.
Let C⊥
T be the subset of codewords of C⊥which have zeros in all the coordinates
indicated by the elements of T . Then C⊥
T \ T is a linear code and
C⊥
T \ T ⊆(C \ T )⊥,
since a vector in C⊥
T is orthogonal to all the vectors of C and has zeros in the coordinates
indicated by the elements of T . Furthermore,
dim(C⊥
T \ T ) = dim C⊥
T
since the codewords of C⊥
T have zeros in the coordinates indexed by T , so deleting these
coordinates does not reduce the number of codewords.
Let H be a generator matrix for C⊥. Let L be the set of τ vectors of Fn−k
q
which are the
columns of H indicated by the elements of T . Then
C⊥
T = {vH | v ∈Fn−k
q
, v · s = 0, for all s ∈L},

4.6 · Constructing Designs from Linear Codes
65
4
since vH is a codeword of C⊥and has zeros in the coordinates indexed by T precisely when
v · s = 0, for all s ∈L.
Hence,
dim C⊥
T ⩾n −k −τ.
Now,
dim(C \ T ) = k
implies
dim(C \ T )⊥= n −τ −k
and we just proved that
dim(C⊥
T \ T ) ⩾n −τ −k,
so we have that
C⊥
T \ T = (C \ T )⊥.
The weight of a codeword of C⊥
T \ T is the weight of the corresponding codeword of C⊥. By
hypothesis, C⊥has at most d −τ non-zero weights of weight at most n −τ. Since at least τ
of the coordinates of a codeword of C⊥
T are zero, C⊥
T has weights at most n −τ. Therefore,
(C \ T )⊥has at most d −τ non-zero weights.
Since C \ T has minimum distance at least d −τ, Exercise 4.16 implies that the weight
enumerator of C \ T is determined.
If u is a non-zero codeword, then μu is another codeword with the same support as u,
for all non-zero μ ∈Fq. The number λ(q −1), of codewords of C \ T of weight κ −τ,
is determined by the weight enumerator of C \ T . The number λ does not depend on which
subset T we choose, only the size of the subset T . By induction on κ, for all τ-subsets T of
{1, . . . , n}, there are a ﬁxed number of supports of the codewords of weight κ containing T .
Therefore, the set of the supports of the codewords of C of weight κ is a τ-(n, κ, λ)
design.
⊓⊔
Example 4.23
Consider the [10, 6, 4]3 code from Example 4.19. The dual code C⊥has codewords of weight
0,6 and 9 so, according to Theorem 4.22, the set of supports of the codewords of weight κ
is a 3-design, provided that no two codewords of C of weight κ have the same support. By
Exercise 4.15, we can be assured of this for κ ∈{4, 5, 6, 7}.
To calculate λ, we count in two ways the number of 3-subsets. Each 3-subset of
{1, . . . , 10} is contained in λ 3-subsets of the design, so
10
3

λ =
κ
3

α,

4
66
Chapter 4 • Linear Codes
where α is the number of supports of codewords of C of weight κ. The number of supports
of codewords of weight κ is the number of codewords of weight κ divided by q −1.
Therefore, from the code C we can construct a 3-(10, 4, 1)-design, a 3-(10, 5, 6)-design
and a 3-(10, 6, 5)-design.
■
In Example 4.23, we could have constructed the designs directly from the elliptic
quadric. For example, the 3-(10, 4, 1) design is obtained by taking subsets of 4 co-
planar points and the 3-(10, 5, 6) design is obtained by taking subsets of 5 points,
no 4 co-planar. In ▷Chapter 5 we shall construct codes from polynomial divisors
of Xn −1 which will often satisfy the hypothesis of Theorem 4.22 and allow us to
construct designs. In many cases, these designs cannot be constructed directly from any
geometrical object.
4.7
Comments
The MacWilliams identities from ▷Chapter 4 appear in MacWilliams’ thesis “Com-
binatorial Problems of Elementary Group Theory”, although the standard reference is
[50]. The MacWilliams identities lead to a set of constraints on the existence of an
[n, k, d]q code. We have that A0 = 1 and A1 = · · · = Ad−1 = 0 and that
1 + Ad + · · · + An = qk.
Since
A⊥
i ⩾0,
Theorem 4.13 implies, for a ﬁxed n and q, the linear constraint
n

j=0
AjKi(j) ⩾0.
The coefﬁcients
Ki(j) =
j

r=0
j
r
n −j
i −r

(−1)r(q −1)i−r
are called the Krawtchouk polynomials. Delsarte [21] proved that from the distance
distribution between the codewords of an arbitrary code (not necessarily a linear code)
one can deduce similar inequalities, called the linear programming bound. This can be
a powerful tool, not only in ruling out certain parameter sets, but also for the construction
of codes, since it can give signiﬁcant information about the distance distribution.

4.8 · Exercises
67
4
The Griesmer bound is from [31] and the Hamming code was ﬁrst considered by
Hamming in [34]. The upper bound on the size of the code in Example 4.19 is from
[55].
The Alderson–Bruen–Silverman model for codes equivalent to linear codes in ▷
Section 4.4 is from [2]. The fact that a linear code with a non-linear extension has a
linear extension, Exercise 4.11, is due to Alderson and Gács, see [1].
Theorem 4.22 is the Assmus–Mattson theorem from [4].
The bound in Exercise 4.3 is due to Varshamov [75] and is known as the linear
Gilbert–Varshamov bound.
4.8
Exercises
4.1 Prove that if C is linear, then the extended code C is linear.
4.2 Prove that the code in Example 4.2 is a perfect code.
4.3 Prove that if
d−2

j=0
n −1
j

(q −1)j < qn−k,
then there exists an [n, k, d]q code.
4.4 Prove that the system of equations in Example 4.14 has the solution
A(X) = 1 + 14X4 + X8 + λ(X2 −2X4 + X6).
4.5 Prove that the code in Example 4.8 has minimum distance 4 and decode the received
vector (0, 1, 1, 0, 2, 2, 2, 0) using syndrome decoding.
4.6 Prove that the code C in Example 3.4 is linear but not self-dual although for the weight
enumerator A(X) of C, we have A(X) = A⊥(X). Prove that C is equivalent to C⊥.
4.7 Let C be the linear code over F5 generated by the matrix
G =
⎛
⎜⎝
1 0 0 1 1 2
0 1 0 1 2 1
0 0 1 2 1 1
⎞
⎟⎠.
Calculate the minimum distance of C and decode the received vector (0, 2, 3, 4, 3, 2) using
syndrome decoding.

4
68
Chapter 4 • Linear Codes
4.8 Let C be the linear code over F7 deﬁned by the check matrix
H =
⎛
⎜⎜⎜⎝
1 1 1 1 1 1 1
0 1 2 3 4 5 6
0 1 4 2 2 4 1
0 1 1 6 1 6 6
⎞
⎟⎟⎟⎠.
i.
Prove that C is a [7, 3, 5]7 code.
ii.
Decode the received vector (2, 2, 3, 6, 1, 2, 2) using syndrome decoding.
4.9 Let C be the 3-dimensional linear code over F3 generated by the matrix
⎛
⎜⎝
1 0 0 1 1 2 0 1 1
0 1 0 1 2 1 1 0 1
0 0 1 2 1 1 1 1 0
⎞
⎟⎠.
Prove that C has minimum distance 6 and use syndrome decoding to decode the received
vector
(1, 2, 0, 2, 0, 2, 0, 0, 0).
4.10 Prove that the code C′ obtained from the Alderson–Bruen–Silverman model is
equivalent to the linear code C from which the model is set up.
4.11 Let S be the set of n vectors obtained from the set of columns of a generator matrix of
a linear code C and suppose that C has an extension to a code of length n + 1 and minimum
distance d + 1.
i.
Prove that there is a function
f : Fk
q →Fq
with the property that if f (u) = f (v), then u −v is orthogonal (with respect to the
standard inner product) to less than n −d points of S.
ii.
Let T be the set of vectors of Fk
q which are orthogonal to n −d vectors of S. Let v ∈T
and let u1, . . . , uk−2 be a set of k −2 vectors extending v to a set of k −1 linearly
independent vectors. Prove that for all λ1, . . . , λk−2, λ, μ ∈Fq, λ ̸= μ,
f (λ1u1 + · · · + λk−2uk−2 + λv) ̸= f (λ1u1 + · · · + λk−2uk−2 + μv).
iii.
Prove that if every hyperplane of Fk
q contains a vector of T , then every hyperplane of
Fk
q contains qk−2 vectors u such that f (u) = 0.
iv.
Prove that there is a hyperplane of Fk
q not containing a vector of T .
v.
Prove that C has a linear extension. In other words, it can be extended to a [n+1, k, d+
1]q code.

4.8 · Exercises
69
4
4.12 Prove that for ﬁxed r = n −d, the Griesmer bound implies n ⩽(r −k + 2)q + r.
4.13 Let r = n −d and let S be the set of columns of a generator matrix of a 3-dimensional
linear code C of length (r −1)q + r, so we have equality in the bound of Exercise 4.12.
Prove that S is a set of vectors of Fk
q in which every hyperplane contains 0 or r vectors of S.
Equivalently show that the non-zero codewords of C have weight n or d.
4.14
i.
Verify that equality in the Griesmer bound occurs for the parameters of the code C in
Example 4.19 if and only if q = 3.
ii.
Let G be a 6 × 10 matrix
G =

I6 A

.
Let S be the set of rows of the 6 × 4 matrix A, considered as 6 points of PG(3, 3).
Prove that G is a generator matrix of a [10, 6, 4]3 code if and only if S has the property
all points of S have weight at least three (i.e. the points of S have at most one zero
coordinate), no two points of S are collinear with a point of weight one and that no
three points of S are collinear.
iii.
Find a matrix A so that G is a generator matrix for a [10, 6, 4]3 code.
4.15 Let C be a linear code over Fq, where q ̸= 2.
i.
Prove that if w −⌈w/(q −1)⌉< d, where d is the minimum distance of a linear code
C, then two codewords of C of weight w have the same support if and only if they are
multiples of each other.
ii.
Prove that if w ⩽(d −1)(q −1)/(q −2), then w −⌈w/(q −1)⌉< d.
4.16 Let C be a linear code of length n and minimum distance d with the property that C⊥
has at most d distinct weights, w1, . . . , wd.
i.
Let Aj denote the number of codewords of C of weight j and let A⊥
j denote the number
of codewords of C⊥of weight j. Prove that
qk
n

j=0
A⊥
j (1−X)j = (1+(q −1)(1−X))n+
n

j=d
AjXj(1+(q −1)(1−X)n−j).
ii.
Prove that the n + 1 polynomials Xn−r(1 + (q −1)(1 −X)r) (r = 0, . . . , n −d),
(1 −X)wj (j = 1, . . . , d) are linearly independent.
iii.
Prove that the weight enumerator of C⊥is determined.
iv.
Prove that the weight enumerator of C is determined.

71
5
Cyclic Codes
Although it will turn out that cyclic codes are not asymptotically good codes, they are
an important class of codes which include many useful and widely implemented short
length codes, most notably the Golay codes and the general class of BCH codes. BCH
codes have a prescribed minimum distance which means that, by construction, we can
bound from below the minimum distance and therefore guarantee some error-correction
properties. Cyclic codes also provide examples of linear codes with few weights, which
allows us to construct designs via Theorem 4.22. The cyclic structure of these codes will
appear again in ▷Chapter 10, when we consider p-adic codes.
5.1
Basic Properties
A linear code C is called cyclic if, for all (c1, . . . , cn)
∈
C, the vector
(cn, c1, . . . , cn−1) ∈C.
The map
(c1, . . . , cn) →c1 + c2X + · · · + cnXn−1
is a bijection between the vectors of Fn
q and the polynomials in
Fq[X]/(Xn −1).
We deﬁne the weight wt(u) of a polynomial u(X) ∈Fq[X]/(Xn −1) of degree less
than n, as the weight of the corresponding vector of Fn
q. In other words, the number of
non-zero coefﬁcients that it has.
An ideal I of a polynomial ring is a subspace with the property that if f ∈I, then
Xf ∈I.
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_5

5
72
Chapter 5 • Cyclic Codes
Lemma 5.1 A cyclic code C is mapped by the bijection to an ideal I in Fq[X]/(Xn −1).
Proof
This is precisely the condition that a linear code satisﬁes to be cyclic.
⊓⊔
We assume that (n, q) = 1 so that the polynomial Xn −1 has no repeated factors in
its factorisation, see ▷Section 2.3.
The ring Fq[X]/(Xn −1) is a principal ideal ring, so I in Lemma 5.1 is a principal
ideal. Hence,
I = ⟨g⟩= {fg | f ∈Fq[X]/(Xn −1)}
for some polynomial g, which is monic and of lowest degree in the ideal.
Therefore, a cyclic code C is mapped by the bijection to ⟨g⟩. We will from now on
write C = ⟨g⟩, for some polynomial g.
Lemma 5.2 If C = ⟨g⟩is a cyclic code of length n, then g divides Xn −1 and C has
dimension at least n −deg g.
Proof
If g(X) does not divide Xn−1, then, using the Euclidean algorithm, we can ﬁnd polynomials
a(X) and b(X) such that
a(X)g(X) + b(X)(Xn −1)
is equal to the greatest common divisor of g(X) and Xn −1, which has degree less than g.
This contradicts the property that g has minimal degree in the ideal I. Therefore, g divides
Xn −1.
The polynomials Xjg, for j
= 0, . . . , n −deg(g) −1 are linearly independent
polynomials in ⟨g⟩, so the dimension of C is at least n −deg g.
⊓⊔
In fact, we shall see that the dimension k of C is precisely n −deg g. This follows
from the following theorem.
Theorem 5.3
Let C = ⟨g⟩be a cyclic code of length n. The dual code C⊥is the cyclic code ⟨←−
h ⟩,
where g(X)h(X) = Xn −1 and ←−
h (X) = Xkh(X−1).
Proof
Suppose that
g(X) =
n−k

j=0
gjXj

5.1 · Basic Properties
73
5
and
h(X) =
k

i=0
hiXi.
The code ⟨g⟩contains the row span of the k × n matrix
G =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
g0 . . . gn−k
0
. . . . . .
0
0 g0
. . . gn−k 0 . . .
0
0
0
...
. . . ... ...
...
...
...
...
...
0
0 . . . . . .
0
g0 . . . gn−k
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
and the code ⟨←−
h ⟩contains the row span of the (n −k) × n matrix
H =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
hk . . . h0
0 . . . . . . 0
0 hk . . . h0 0 . . . 0
0
0 ... . . . ... ... ...
...
... ...
... 0
0 . . . . . . 0 hk . . . h0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
The scalar product between the s-th row of G and the r-th row of H, where s ∈{1, . . . , k}
and r ∈{1, . . . , n −k} is
k+r

i=s
gi−shk+r−i,
which is the coefﬁcient of Xk+r−s in gh. Since 1 ⩽k + r −s ⩽n −1, this coefﬁcient is
zero and so GHt = 0.
Since
n = dim C + dim C⊥⩾rank(G) + rank(H) = n,
(5.1)
the theorem follows.
⊓⊔
Corollary 5.4 The code C = ⟨g⟩of length n has dimension n −deg g.
Proof
Let G and H be as in the previous proof. Equation (5.1) implies that the dimension of C is
the rank of G, which is k.
⊓⊔

5
74
Chapter 5 • Cyclic Codes
Example 5.5 (perfect ternary Golay code)
Consider the factorisation of X11 −1 over F3. As in ▷Section 2.3, we calculate the
cyclotomic subsets of the multiples of 3 modulo 11,
{0}, {1, 3, 9, 5, 4}, {2, 6, 7, 10, 8}.
According to Lemma 2.12, there are two factors of degree 5 which are
(X −α)(X −α3)(X −α9)(X −α5)(X −α4)
and
(X −α2)(X −α6)(X −α7)(X −α10)(X −α8),
where α is a primitive 11-th root of unity in F35.
Suppose that
X5 + a4X4 + a3X3 + a2X2 + a1X + a0
is the ﬁrst of these factors. Then a0 = −α22 = −1. Since the roots of the ﬁrst factor are the
reciprocals of the roots of the second factor, the second factor is
X5 −a1X4 −a2X3 −a3X2 −a4X −1.
It is fairly easy to deduce from this that the factorisation is
X11 −1 = (X −1)(X5 −X3 + X2 −X −1)(X5 + X4 −X3 + X2 −1).
The cyclic code C = ⟨X5 −X3 + X2 −X −1⟩over F3 is the perfect ternary Golay code of
length 11. To prove that this is a perfect code we need to show that the minimum weight of a
non-zero codeword is 5 (and hence the minimum distance is 5 according to Lemma 4.1) and
observe that

1 + 2
11
1

+ 4
11
2

36 = 311,
so the sphere-packing bound of Theorem 3.9 is attained.
Adding a column of 1’s to the generator matrix
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
−1 −1 1 −1 0
1
0
0
0 0 0
0 −1 −1 1 −1 0
1
0
0 0 0
0
0 −1 −1 1 −1 0
1
0 0 0
0
0
0 −1 −1 1 −1 0
1 0 0
0
0
0
0 −1 −1 1 −1 0 1 0
0
0
0
0
0 −1 −1 1 −1 0 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠

5.2 · Quadratic Residue Codes
75
5
we get a generator matrix of a self-dual code C of length 12. This we can check by computing
the scalar product of any two rows and verifying that it is zero (modulo 3). Since this code
is self-dual, the codewords have weights which are multiples of 3. If we can rule out the
possibility that a codeword has weight 3, which we will in ▷Section 5.3, then the minimum
weight of a non-zero codeword of C is 6, which implies that the minimum weight of a non-
zero codeword of the cyclic code ⟨X5 −X3 + X2 −X −1⟩is 5. Therefore, C is a [11, 6, 5]3
code and C is a [12, 6, 6]3 code.
■
5.2
Quadratic Residue Codes
Let n and q be primes for which q is a square in Fn, where we consider the ﬁeld Fn ∼=
Z/nZ to be addition and multiplication modulo n, deﬁned on the set {0, 1, . . . , n −1}.
Let α be a primitive n-th root of unity in some extension ﬁeld of Fq.
Deﬁne
g(X) =

(X −αr),
where the product runs over the non-zero squares r in Fn.
Lemma 5.6 The polynomial g(X) divides Xn −1 in Fq[X].
Proof
Since q is a square in Fn, the map
r →qr
is a bijection from the squares of Fn to the squares of Fn, for all non-zero squares r ∈Fn.
Hence,
g(X) =

(X −αr) =

(X −αrq),
where the product runs over the non-zero squares r in Fn.
Lemma 2.11 implies that g(X) ∈Fq[X] and note that the roots of g(X) are distinct n-th
roots of 1.
⊓⊔
Since g(X) is a factor of Xn −1, we can deﬁne the cyclic code ⟨g⟩of length n over
Fq. This code is called the quadratic residue code.
We can obtain evidence that the minimum distance of a quadratic residue code is
quite good from the following theorems.
Theorem 5.7
If u ∈⟨g⟩and u(1) ̸= 0, then wt(u)2 ⩾n.

5
76
Chapter 5 • Cyclic Codes
Proof
Since u ∈⟨g⟩, the n-th roots of unity αr of Fq, where r is a non-zero square in Fn, are zeros
of u(X).
Let t be a non-square of Fn. The n-th roots of unity αs of Fq, where s is a non-square in
Fn, are zeros of u(Xt), since the product of two non-squares is a square. Therefore, all the
n-th roots of unity of Fq, except 1, are zeros of u(X)u(Xt). Hence,
u(X)u(Xt) = (1 + X + · · · + Xn−1)v(X),
for some polynomial v(X). Since u(1) ̸= 0, we have that v(1) ̸= 0.
Therefore, in the ring Fq[X]/(Xn −1),
u(X)u(Xt) = (1 + X + · · · + Xn−1)v(1),
since v(X) = v(1) + (X −1)v1(X), for some polynomial v1(X).
Since u(X) has wt(u) terms, this implies that wt(u)2 ⩾n.
⊓⊔
Theorem 5.8
If n ≡−1 mod 4, u ∈⟨g⟩and u(1) ̸= 0, then wt(u)2 −wt(u) + 1 ⩾n.
Proof
If n ≡−1 mod 4, then −1 is a non-square in Fn, since (−1)(n−1)/2 = −1. Therefore, in the
proof of Theorem 5.7, we can take t = −1. Then,
u(X)u(X−1) = (1 + X + · · · + Xn−1)v(1).
In the product there are at least wt(u) terms of u(X) which multiply with a term of u(X−1)
to give a constant term, since XjX−j = 1. Hence,
wt(u)2 −wt(u) ⩾n −1.
⊓⊔
Example 5.9 (perfect binary Golay code)
Consider the quadratic residue code with n = 23 and q = 2. Let ϵ be a primitive element of
F211 ∼= F2[X]/(X11 + X2 + 1) and let α = ϵ89. Then α is a primitive 23-rd root of unity. By
Lemma 5.6, the factorisation of X23 −1 in F2[X] has a factor
g(X) =

r∈S
(X −αr),
where S = {1, 2, 4, 8, 16, 9, 18, 13, 3, 6, 12} is the set of non-zero squares of F23.

5.2 · Quadratic Residue Codes
77
5
If αj is a root of g(X), then α−j is not, which implies that
X23 −1 = (X −1)g(X)←−
g (X).
Solving this polynomial identity we deduce that one of g(X) or ←−
g (X) is
X11 + X9 + X7 + X6 + X5 + X + 1.
By checking that the sum of the roots of g(X) is zero, we deduce that this polynomial is
g(X).
The quadratic residue code ⟨g⟩is the perfect binary Golay code of length 23. By
Corollary 5.4, it has dimension 12.
Observe that

1 +
23
1

+
23
2

+
23
3

212 = 223,
so the bound in Theorem 3.9 is attained.
The following matrix is a generator matrix for the code ⟨g⟩:
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0
0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0
0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0
0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0
0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0
0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0
0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0
0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0
0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0
0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0
0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0
0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Adding a column of 1’s to this matrix we get a generator matrix for a 12-dimensional linear
code C of length 24. One can verify that all codewords of C have weights which are multiples
of four, see Exercise 5.3. We shall prove in ▷Section 5.3 that the cyclic code ⟨g⟩has
minimum weight at least 5. Therefore, the minimum weight of a non-zero codeword of C is
8, which implies that the minimum weight of a non-zero codeword of ⟨g⟩is 7. By Lemma 4.1,
the minimum distance of ⟨g⟩is 7. Hence, ⟨g⟩is a [23, 12, 7]2 code and C is a [24, 12, 8]2
code.
■

5
78
Chapter 5 • Cyclic Codes
5.3
BCH Codes
Let α be a primitive n-th root of unity in Fqm. BCH codes are a class of cyclic codes in
which we choose α so that α, α2, . . . , αd0−1 are roots of a low degree polynomial g of
Fq[X], for some d0 < n. This allows us to bound the minimum distance of the code ⟨g⟩.
The lower the degree of g, the larger the dimension (and hence the size) of the code.
Suppose that g(X) ∈Fq[X] is the polynomial of minimal degree such that
g(αj) = 0,
for j = 1, . . . , d0 −1.
The code ⟨g⟩is called a BCH code, after Bose, Ray-Chaudhuri and Hocquenghem
who introduced this family of cyclic codes. The parameter d0 is called the prescribed
minimum distance because of the following theorem.
Theorem 5.10
The dimension of the BCH code ⟨g⟩is at least n−m(d0 −1) and its minimum distance
is at least d0.
Proof
Let j ∈{1, . . . , d0 −1}. By Lemma 2.11, the polynomial
(X −αj)(X −αjq) · · · (X −αjqm−1)
is in Fq[X]. Clearly, it is zero at αj. Since this polynomial has degree m this implies that there
is a polynomial of degree m(d0 −1) in Fq[X] which is zero at αj, for all j = 1, . . . , d0 −1.
Thus, the degree of g is at most m(d0 −1) so, by Corollary 5.4, the dimension of ⟨g⟩is
at least n −m(d0 −1).
Suppose that there is an f ∈⟨g⟩for which wt(f ) is at most d0 −1. Then
f (X) = b1Xk1 + · · · + bd0−1Xkd0−1,
for some k1, . . . , kd0−1.
Since f ∈⟨g⟩,
f (αj) = 0
for all j = 1, . . . , d0 −1. Writing this in matrix form these equations are
⎛
⎜⎜⎜⎝
αk1
αk2
. . .
αkd0−1
α2k1
α2k2
. . .
α2kd0−1
.
.
.
.
α(d0−1)k1 α(d0−1)k2 . . . α(d0−1)kd0−1
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎜⎜⎝
b1
b2
.
.
bd0−1
⎞
⎟⎟⎟⎟⎟⎟⎠
= 0.

5.3 · BCH Codes
79
5
The determinant of the matrix is

i̸=j
(αki −αkj ),
which is non-zero. This implies that the only solution to the above system is f (X) = 0.
Hence, the minimum weight of a non-zero codeword of the cyclic code ⟨g⟩is at least d0.
The lemma follows since, by Lemma 4.1, the minimum weight of a non-zero codeword of a
linear code is equal to its minimum distance.
⊓⊔
Example 5.11
Let α be a primitive 31-st root of unity in F32. By Lemma 2.12, we obtain the factorisation
of X31 −1 over F2 by considering the cyclotomy classes
{1, 2, 4, 8, 16}, {3, 6, 12, 24, 17}, {5, 10, 20, 9, 18}, {7, 14, 28, 25, 19},
{11, 22, 13, 26, 21}.
The i-th cyclotomy class gives a polynomial fi(X) in F2[X] which is zero at αj for j in the
cyclotomy class. For example,
f1(X) = (X −α)(X −α2)(X −α4)(X −α8)(X −α16)
is in F2[X] and is zero at αj for j ∈{1, 2, 4, 8, 16}.
Let
g(X) = f1(X)f2(X)f3(X).
According to Corollary 5.4, the cyclic code ⟨g⟩is a 16-dimensional linear code.
Since 1, 2, 3, 4, 5 and 6 appear in the ﬁrst three cyclotomic subsets,
g(αj) = 0,
for j = 1, . . . , 6. Theorem 5.10 implies that ⟨g⟩is a [31, 16, ⩾7]2 code. It is in fact a
[31, 16, 7]2 code. Since there exists a [31, 16, 8]2 code, ⟨g⟩is not an optimal linear code for
this length and dimension.
■
Example 5.12 (shortened Reed–Solomon code)
Let α be a primitive (q −1)-st root of unity in Fq. By Theorem 2.4, the polynomial Xq−1 −1
factorises into linear factors over Fq. Each cyclotomy class has size 1 and the factors are
fi(X) = X −αi,
for i = 0, . . . , q −2.

5
80
Chapter 5 • Cyclic Codes
Let
g(X) = f1(X)f2(X) · · · fd−1(X).
According to Corollary 5.4, ⟨g⟩is a (n −d + 1)-dimensional linear code of length n.
According to Theorem 5.10, ⟨g⟩has minimum distance at least d. This is an example of
an MDS code, which we will study in more depth in ▷Chapter 6.
■
Example 5.13
In Example 5.9, the numbers 1, 2, 3 and 4 appear in the same cyclotomy class, so
Theorem 5.10 implies that the binary Golay code has weight at least 5. As observed in
Example 5.9, this implies that the extended binary Golay code C has no codewords of weight
4, which implies that the minimum distance of C is 8. This, in turn, implies that the minimum
distance of the binary Golay code is 7.
■
Example 5.14
Theorem 5.10 generalises in a straightforward way to Exercise 5.5. We can now establish that
the minimum distance of the ternary Golay code is 5. By Exercise 5.5, since 3, 4 and 5 appear
in the same cyclotomy class (and 6, 7 and 8 appear in the same cyclotomy class), the ternary
Golay code in Example 5.5 has minimum distance at least 4. Therefore, the extended code
C has no codewords of weight three, so the weight of a non-zero codeword of the extended
code is either 6, 9 or 12. As observed in Example 5.5, this implies that the minimum distance
of the ternary Golay code is 5.
■
The following theorem, which we quote without proof, states that there is no
sequence of asymptotically good BCH codes.
Theorem 5.15
There is no inﬁnite sequence of [n.k, d]q BCH codes for which both δ = d/n and
R = k/n are bounded away from zero.
5.4
Comments
The introduction of cyclic codes and quadratic residue codes is widely accredited
to Eugene Prange and Andrew Gleason who proved the automorphism group of an
extended quadratic residue code has a subgroup which is isomorphic to either PSL(2, p)
or SL(2, p), see [12]. The Golay codes were discovered by Golay [27]. The BCH
codes were introduced by Bose and Ray-Chaudhuri in
[13] and independently by
Hocquenghem in [38]. The fact that long BCH codes are asymptotically bad is proven
by Lin and Welden in [47]. The code in Exercise 5.7 is a Zetterberg code, one of a
family of [4m + 1, 4m + 1 −4m, 5]2 codes.

5.5 · Exercises
81
5
5.5
Exercises
5.1 Let C be the extended ternary Golay code from Example 5.5.
i.
Verify that the factorisation of X11 −1 in F3[X] is as in Example 5.5.
ii.
Prove that the weight enumerator of C is
A(X) = 1 + 264X6 + 440X9 + 24X12.
iii.
Let S be the set of 12 points of PG(5, 3) obtained from the set of columns of a generator
matrix of the code C. Label the points of S by the elements of {1, . . . , 12} and deﬁne
a set D of 6-subsets to be the points of S which are dependent (i.e. are contained in a
hyperplane of PG(5, 3)). Prove that D is a 5-(12, 6, 1) design.
iv.
Verify that Theorem 4.22 implies that the set of supports of the codewords of weight 6
of C is a 5-(12, 6, 1) design.
5.2 Prove that in Example 5.9 the code ⟨←−
g ⟩is equivalent to the code ⟨g⟩.
5.3
i.
Prove that the extended Golay code over F2, the code C in Example 5.9, is self-dual and
that the weights of the codewords of C are multiples of 4.
ii.
Prove that the weight enumerator of the code C is
A(X) = 1 + 759X8 + 2576X12 + 759X16 + X24.
iii.
Apply Theorem 4.22 to construct a 5-(24, 8, 1) design.
5.4 Investigate the observation that if n ≡−1 modulo 4 and ⟨g⟩is a quadratic residue code,
then the reverse of the polynomial (Xn −1)/(X −1)g(X) is g(X). Does this imply that the
extension of the code ⟨g⟩is self-dual?
5.5 Suppose that g(X) ∈Fq[X] is the polynomial of minimal degree such that
g(αj) = 0,
for j = ℓ+ 1, . . . , ℓ+ d0 −1.
Prove that the dimension of ⟨g⟩is at least n −m(d0 −1) and the minimum distance of
⟨g⟩is at least d0.
5.6 Construct the largest possible BCH code with the following parameters.
i.
A binary code of length 15 with minimum distance at least 5.
ii.
A binary code of length 31 with minimum distance at least 11.
iii.
A ternary code of length 13 with minimum distance at least 7.

5
82
Chapter 5 • Cyclic Codes
Compare the dimension of the codes with the Griesmer bound, the sphere-packing bound and
the Gilbert–Varshamov bound.
5.7
i.
Prove that X17 + 1 factorises in F2[X] as (X + 1)f (X)g(X), where
f (X) = ←−
f (X) = X8 + X7 + X6 + · · ·
and g(X) = ←−
g (X).
ii.
Construct a [17, 9, 5]2 code.
ii.
Construct a [18, 9, 6]2 code.
5.8
i.
Prove that the polynomial X11 + 1 factorises in F4[X] into two irreducible factors of
degree 5 and one of degree 1.
ii.
Using one of the factors of degree 5, construct a [11, 6, d]4 code C.
iii.
Prove that C is a [11, 6, ⩾4]4 code.
iv.
With the aid of a computer, or not, verify that C is a [11, 6, 5]4 code.
5.9
i.
Prove that the polynomial X17 + 1 factorises in F4[X] into four irreducible factors of
degree 4 and one of degree 1.
ii.
Construct a [17, 9, ⩾7]4 code.
iii.
Let g(X) = X8 + eX7 + X6 + X5 + (1 + e)X4 + X3 + X2 + eX + 1, where e is an
element of F4 such that e2 = e + 1. Prove that g divides X17 + 1.
iv.
Assuming that the code in ii. is ⟨g⟩, prove that the minimum distance of the code
constructed in ii. is 7.

83
6
Maximum Distance Separable Codes
Two codewords of a block code of length n and minimum distance d must differ on any
set of n −d + 1 coordinates, since they are at distance at least d from each other. This
observation leads to the Singleton bound, Theorem 6.1. A code whose parameters give
an equality in the Singleton bound is called a maximum distance separable code or
simply an MDS code. Therefore, an MDS code is a block code in which every possible
(n −d + 1)-tuple of elements of the alphabet occurs in a unique codeword for any set
of n −d + 1 coordinates. The focus in this chapter will be on linear MDS codes, since
not so much is known about non-linear MDS codes, and there are no known non-linear
MDS codes which outperform linear MDS codes.
The most widely implemented linear MDS codes are the Reed–Solomon codes,
whose codewords are the evaluation of polynomials of low degree. Exploiting this
algebraic structure of Reed–Solomon codes will allow us to develop a fast decoding
algorithm which corrects up to 1
2(1 −R)n errors, where R is the transmission rate of
the code. For an arbitrary received vector, we can only correct a number of errors less
than half the minimum distance. However, it may be that, although the number of errors
e ⩾1
2d, there is only one codeword that is at distance at most e from the received vector.
We will prove that there is an algorithm, which was discovered only recently, which
creates a relatively short list of possible sent codewords, when up to (1 −
√
2R)n errors
have occurred. If we are in the afore-mentioned case that there is only one codeword
close to the received vector then the list will contain only one codeword. Moreover, this
list decoding algorithm can be used simultaneously for two codes which will effectively
allow one to decode beyond the bound of half the minimum distance, albeit at a slightly
reduced rate, as explained in ▷Section 3.3.
The fundamental question concerning linear MDS codes asks if there are MDS codes
which better the Reed–Solomon code. The MDS conjecture postulates that no such
codes exist, apart from some known exceptions. The conjecture was recently proved
for codes over ﬁelds of prime order but remains open over ﬁelds of non-prime order.
We will prove that the longest three-dimensional linear MDS codes over a ﬁeld of odd
characteristic are the Reed–Solomon codes (this is not the case for even characteristic
ﬁelds) and delve a little deeper into the proof to see how the tools implemented therein
can be used to prove the MDS conjecture over prime ﬁelds.
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_6

6
84
Chapter 6 • Maximum Distance Separable Codes
6.1
Singleton Bound
Theorem 6.1 (Singleton bound)
An r-ary code C of length n and minimum distance d satisﬁes |C| ⩽rn−d+1.
Proof
Consider any set of n −(d −1) coordinates of a codeword. If two codewords agree on these
coordinates, then their distance is at most d −1. Hence, they must be different on these
n −d + 1 coordinates. There are rn−d+1 distinct (n −d + 1)-tuples, which gives the bound.
⊓⊔
The following example is a rather trivial example of a code which meets the
Singleton bound.
Example 6.2
Let A be an abelian group with r elements. Deﬁne
C = {(a1, . . . , an−1, a1 + · · · + an−1) | ai ∈A}.
If ai = bi for all but one i, then a1 + · · · + an−1 ̸= b1 + · · · + bn−1. Hence, the minimum
distance of C is 2. Since |C| = rn−1, it is an MDS code.
■
Theorem 6.3
If there is a [n, k, d]q code, then k ⩽n −d + 1 and a [n, k, d]q code is an MDS code
if and only if k = n −d + 1.
Proof
For a [n, k, d]q code, Theorem 6.1 implies that qk ⩽qn−d+1.
⊓⊔
6.2
Reed–Solomon Code
The Reed–Solomon code is the classical example of a linear MDS code. It is an example
of an evaluation code. An evaluation code is a code whose codewords are the evaluation
of certain functions. In the case of a Reed–Solomon code the functions are given by low
degree uni-variate polynomials. In ▷Chapter 7 and ▷Chapter 9, we will see other
examples of evaluation codes.

6.2 · Reed–Solomon Code
85
6
Example 6.4
Let {a1, . . . , aq} be the set of elements of Fq. The Reed–Solomon code is
C = {(f (a1), . . . , f (aq), cf ) | f ∈Fq[X], deg(f ) ⩽k −1},
where cf is the coefﬁcient of Xk−1 in f .
■
Note that the coordinate given by cf can be interpreted as the evaluation of f at ∞.
By homogenising the polynomial f (X), we get a homogeneous polynomial
h(X, Y) = Y kf (X/Y)
of degree k. The evaluation of f at x is h(x, 1) and h(1, 0) = cf .
Lemma 6.5 The Reed–Solomon code C in Example 6.4 is a [q + 1, k, q + 2 −k]q linear
MDS code.
Proof
We will ﬁrst prove that C is linear.
Let f and g be two polynomials of degree at most k −1. Then f + g is a polynomial of
degree at most k −1 and the coefﬁcient of Xk−1 in f + g is the sum of the coefﬁcients of
Xk−1 in f and g. Hence, if u, v ∈C, then u + v ∈C.
Let λ ∈Fq. Then λf is a polynomial of degree at most k −1 and the coefﬁcient of Xk−1
in λf is λ times the coefﬁcient of Xk−1 in f . Hence, if u ∈C, then λu ∈C.
Therefore, C is a k-dimensional linear code of length n = q + 1.
It follows from the fact that a non-zero polynomial of degree at most k −1 has at most
k −1 zeros that the weight of a codeword for which cf ̸= 0 is at least n −(k −1). If cf = 0
and f ̸= 0, then the polynomial f has degree at most k −2 and so the codeword has at most
k −1 zeros. Therefore, the non-zero codewords of C have weight at least n −k + 1. Thus,
by Lemma 4.1, the code C has minimum distance at least n −k + 1. Then, by the Singleton
bound from Theorem 6.1, C has minimum distance n −k + 1. Hence, C is an MDS code.
⊓⊔
To construct a generator matrix (gij) for the Reed–Solomon code, we choose
k linearly independent polynomials f1(X), . . . , fk(X) of degree at most k −1 and
index the rows with these polynomials. Then we index the columns with the elements
a1, . . . , aq of Fq. The entry gij = fi(aj) for j ⩽q and the gi,q+1 entry is the coefﬁcient
of Xk−1 in the polynomial fi(X).
For example, with fi(X) = Xi−1 the matrix
(gij) =
⎛
⎜⎜⎜⎜⎜⎝
1
1
. . .
1
0
a1
a2
. . . aq
0
.
.
. . .
.
.
ak−2
1
ak−2
2
. . . ak−2
q
0
ak−1
1
ak−1
2
. . . ak−1
q
1
⎞
⎟⎟⎟⎟⎟⎠

6
86
Chapter 6 • Maximum Distance Separable Codes
is a generator matrix for the Reed–Solomon code.
What makes Reed–Solomon codes so attractive for implementation is the availability
of fast decoding algorithms. In the following theorem, we prove that there is a decoding
algorithm that will correct up to t = ⌊(d −1)/2⌋errors, where d is the minimum
distance.
Although it is not really necessary, to make the proof of the following theorem
easier, we shall only use the shortened Reed–Solomon code in which we delete the last
coordinate. In this way every coordinate of a codeword is the evaluation of a polynomial
at an element of Fq.
Theorem 6.6
There is a decoding algorithm for a k-dimensional shortened Reed–Solomon code of
length n, which corrects up to 1
2(n−k) errors and completes in a number of operations
which is polynomial in n.
Proof
Suppose that we have received the vector (y1, . . . , yn). We want to ﬁnd the polynomial f ∈
Fq[X] of degree at most k −1 such that
(y1, . . . , yn) = (f (a1), . . . , f (an)) + e,
where e is the error vector of weight at most 1
2(n −k).
Observe that since the Reed–Solomon code is an MDS code,
1
2(n −k) = 1
2(d −1).
Let h(X) be an arbitrary polynomial of degree ⌊1
2(n −k)⌋and let g(X) be an arbitrary
polynomial of degree k + ⌈1
2(n −k)⌉−1.
We determine the coefﬁcients of g and h by solving the system of n equations,
g(aj) −h(aj)yj = 0,
for j = 1, . . . , n. This homogeneous linear system has
⌊1
2(n −k)⌋+ 1 + k + ⌈1
2(n −k)⌉= n + 1
unknowns (the coefﬁcients of g and h) and n equations. Hence, we can ﬁnd a non-trivial
solution for h(X) and g(X) in a number of operations that is polynomial in n using Gaussian
elimination.
By assumption, there is a polynomial f of degree at most k −1, such that yj = f (aj)
for at least n −⌊1
2(n −k)⌋values of j. For these values of j, aj is a zero of

6.2 · Reed–Solomon Code
87
6
g(X) −h(X)f (X).
The degree of this polynomial is at most k + ⌈1
2(n −k)⌉−1. Since
n −⌊1
2(n −k)⌋> k + ⌈1
2(n −k)⌉−1
the polynomial g(X) −h(X)f (X) has more zeros than its degree, so it is identically zero.
Therefore, h(X) divides g(X) and the quotient is f (X).
⊓⊔
In the following example we apply the algorithm in Theorem 6.6 to a concrete case.
Example 6.7
Suppose that we have sent a codeword u of the 2-dimensional shortened Reed–Solomon code
over F7 = {0, 1, 2, 3, 4, 5, 6} (ordering the elements of F7 in that order) and that we have
received
y = (1, 0, 0, 0, 6, 2, 5).
According to the algorithm in the proof of Theorem 6.6, we should ﬁnd a polynomial g(X)
of degree 4 and a polynomial h(X) of degree 2, such that
g(aj) = h(aj)yj,
for j = 1, . . . , 7 and where aj is the j-th element of F7.
The equations are
g(0) = h(0), g(1) = 0, g(2) = 0, g(3) = 0, g(4) = 6h(4),
g(5) = 2h(5), and g(6) = 5h(6).
From this we deduce that
g(X) = (X −1)(X −2)(X −3)(g1X + g0)
and
h(X) = h2X2 + h1X + h0,
for some h2, h1, h0, g1, g0 ∈F7, which are solutions of the system
g0 = h0, 6(4g1 + g0) = 6(2h2 + 4h1 + h0)
3(5g1 + g0) = 2(4h2 + 2h1 + h0), 4(6g1 + g0) = 5(h2 + 6h1 + h0).
This system of equations has a solution g1 = 0, g0 = 3, h2 = 1, h1 = 3 and h0 = 3.

6
88
Chapter 6 • Maximum Distance Separable Codes
If less than ⌊1
2(n−k)⌋= 2 errors have occurred, then the codeword u is the evaluation of
f (X) = g(X)
h(X) = 3(X −1)(X −2)(X −3)
X2 + 3X + 3
= 3X + 1.
Evaluating the polynomial f , we deduce that
u = (1, 4, 0, 3, 6, 2, 5).
■
Theorem 6.6 allows us to deduce the sent vector providing at most 1
2n(1 −R) errors
occur in transmission. Recall, that the transmission rate of a k-dimensional linear code
of length n is R = k/n. We interpreted Exercise 3.12 v. as saying that (at least for a
binary code) the number of codewords at a distance at most 1
2n(1−√1 −2(d/n)) from
a ﬁxed vector is less than 2n. If we consider the ﬁxed vector as the received vector, then
this implies that it should be feasible to construct a short list of possible sent codewords,
even if the number of errors which have occurred exceeds half the minimum distance. A
decoding algorithm which produces such a short list is called a list decoding algorithm.
It may be that although the received codeword is more than half the minimum distance
away from a codeword, it is near to only one codeword. In such a case the list decoding
algorithm may allow us to correct the errors. In other words, we may be able to decode
uniquely even when more than 1
2(d −1) errors have occurred. And if we encode the
message with two distinct codes, as we saw in ▷Section 3.3 this can be done so that
the rate is not reduced by much, then with a high probability the intersection of the two
lists will be the sent codeword.
We can list decode in a simple way by using standard array decoding. We make
a table whose rows are indexed by the error vectors and whose columns are indexed by
the codewords and whose entry is found by summing the error vector and the codeword.
To decode a received vector v, one searches through the table entries for v making a list
of the codewords u for which v appears in the column indexed by u. If there is a unique
entry in the list, then v can be uniquely decoded. One downside to this algorithm is that
for any code of reasonable size, the table requires a large amount of storage space.
The following example illustrates the main idea behind the algorithm presented in
Theorem 6.9, which is a list decoding algorithm for Reed–Solomon codes.
Example 6.8
Suppose that we have sent a codeword of the shortened 4-dimensional Reed–Solomon code
over F13, where the elements of F13 are ordered as
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}.
The unique decoding algorithm in Theorem 6.6 allows us to correct up to ⌊1
2(d −1)⌋= 4
errors. Suppose that 5 errors have occurred and that we have received
y = (4, 11, 0, 3, 0, 1, 0, 0, 0, 0, 0, 3, 12).

6.2 · Reed–Solomon Code
89
6
Let
Q(X, Y) = cY 2 + g(X)Y + h(X),
where c ∈F13,
g(X) = g4X4 + g3X3 + g2X2 + g1X + g0
is an arbitrary polynomial of degree at most 4 and h(X) is an arbitrary polynomial of degree
at most 7.
We ﬁnd g(X) and h(X), and hence Q(X, Y), by solving the set of equations,
Q(xj, yj) = 0,
for j = 1, . . . , 13, where xj is the j-th element of F13 and yj is the j-th coordinate of y.
There are 14 unknowns in this homogeneous linear system of equations, the coefﬁcients of
g and h and the constant c, and 13 equations. Hence, there is a non-trivial solution to this
system of equations.
For xj ∈{2, 4, 6, 7, 8, 9, 10}, the equation Q(xj, yj) = 0 implies h(xj) = 0, since
yj = 0 for j ∈{3, 5, 7, 8, 9, 10, 11}.
Thus,
h(X) = a(X −2)(X −4)(X −6)(X −7)(X −8)(X −9)(X −10)
for some a ∈F13.
The remaining equations imply,
4c+11(g0+g1+g2+g3+g4)+10a = 0, 9c+3(g0+3g1+9g2+g3+3g4)+11a = 0,
c+g0+5g1+12g2+8g3+g4+4a = 0, 9c+3(g0+11g1+4g2+5g3+3g4)+7a = 0,
and
c + 12(g0 + 12g1 + g2 + 12g3 + g4) + 10a = 0.
Up to scalar factor, the solution of this system implies
Q(X, Y) = 3Y 2 + (12X4 + 5X3 + 8X2 + 7X + 5)Y
+(X −2)(X −4)(X −6)(X −7)(X −8)(X −9)(X −10).
Suppose that f (X) is the polynomial of degree at most 3, whose evaluation is the sent
codeword u. Then Q(X, f (X)) is a polynomial of degree at most 7, which is zero whenever
yj = f (xj). This occurs whenever yj = uj and no error has occurred in the j-th coordinate.
Assuming that at most 5 errors have occurred, y and u agree in at least 8 coordinates. Hence,

6
90
Chapter 6 • Maximum Distance Separable Codes
Q(X, f (X)) has at least 8 zeros. Therefore, Q(X, f (X)) ≡0 which implies Y −f (X)
divides Q(X, Y).
Indeed, Q(X, Y) factorises as
Q(X, Y) = (Y −X3 −X2 −5X −4)(3Y −X4 + 8X3 + 11X2 + 9X + 4).
Since f (X) is a polynomial of degree at most 3, it must be that
f (X) = X3 + X2 + 5X + 4.
The evaluation of f (X) is
u = (4, 11, 0, 3, 0, 10, 2, 9, 1, 10, 3, 12),
which is at distance 5 from the received vector y.
■
The following theorem follows the same idea as Example 6.8 and provides an
algorithm which outputs a short list of possibilities for the sent codeword having
received a vector in which up to approximately n(1 −
√
2R) errors have occurred. As
with the algorithm described in Theorem 6.6, this algorithm also completes in a number
of operations which is polynomial in n.
Theorem 6.9
There is a decoding algorithm for a k-dimensional shortened Reed–Solomon code of
length n, that completes in a number of operations which is polynomial in n and which
outputs a list of less than √2n/(k −1) codewords, one of which is the sent vector,
provided less than n −
√
2nk errors have occurred in transmission.
Proof
Let m = ⌈√2n/(k −1)⌉−1 and deﬁne a bi-variate polynomial
Q(X, Y) =
m

i=0
⌈1
2 k⌉+i(k−1)

j=0
qijXjY m−i,
where the coefﬁcients are to be determined. Since
m

i=0
(⌈1
2k⌉+ i(k −1)) = ⌈1
2k⌉(m + 1) + 1
2m(m + 1)(k −1)
> 1
2(m + 1)2(k −1) ⩾n,
the polynomial Q(X, Y) has more than n coefﬁcients.
Let (y1, . . . , yn) be the received vector.

6.3 · Linear MDS Codes
91
6
The coordinates of a codeword of the shortened Reed–Solomon code are indexed by
{x1, . . . , xn}, the set of elements of Fq.
We make a homogeneous system of ℓequations, where for each ℓ∈{1, . . . , n}, we have
the equation
Q(xℓ, yℓ) = 0.
Since we have more than n unknowns, this homogeneous system has a non-trivial solution.
And we can ﬁnd a solution, using Gaussian elimination, in a number of operations which is
polynomial in n.
Let g(X) be a polynomial of degree at most k −1. Then the uni-variate polynomial
Q(X, g(X)) has degree at most
⌈1
2k⌉+ m(k −1) <

2n(k −1) <
√
2nk.
By hypothesis, less than n −
√
2nk errors have occurred in transmission, so there is a
polynomial f (X), of degree at most k −1, for which yℓ= f (xℓ) for more than
√
2nk values
of ℓ. Therefore, Q(X, f (X)) ≡0, since a non-zero polynomial cannot have more zeros than
its degree.
We can write
Q(X, Y) = (Y −f (X))C(X, Y) + R(X)
for some polynomials C(X, Y) and R(X) and conclude, substituting Y
= f (X), that
R(X) ≡0.
Hence, Y −f (X) divides Q(X, Y). The bi-variate polynomial Q(X, Y) can be factorised
in a number of operation that is polynomial in its degree.
Thus, if f (X) is the polynomial which the sent codeword is the evaluation of, then Y −
f (X) is a factor of Q(X, Y). Since the degree in Y of Q(X, Y) is at most m, there are at most
m possibilities for the sent codeword.
⊓⊔
6.3
Linear MDS Codes
In this section we consider the general class of linear MDS codes. For the Reed–
Solomon code, the minimum distance d = q +2−k. However, for a hypothetical linear
MDS code, the trivial upper bound, given by Exercise 6.6, is d ⩽q. Thus, the trivial
bound does not rule out the possibility that there are linear MDS codes which are much
better than Reed–Solomon codes. We will prove that linear MDS codes are equivalent to
a certain geometric object and prove that, in the case that q is odd, a three-dimensional
linear MDS code of length q + 1 is a Reed–Solomon code. This proof contains all the
ingredients needed to prove that there are no linear MDS codes over ﬁelds of prime
order better than the Reed–Solomon codes. The non-prime case remains open and, in
part, is a more difﬁcult problem since there are examples of linear MDS codes of length

6
92
Chapter 6 • Maximum Distance Separable Codes
q + 1 which are not equivalent to Reed–Solomon codes. These appear in Exercise 6.9,
Exercise 6.10 and Exercise 6.11.
Theorem 6.10
G is a generator matrix of a linear MDS code if and only if every subset of k columns
of G is a basis of Fk
q.
Proof
Let S be the multi-set of columns of the matrix G.
Suppose that G is the generator matrix of a linear MDS code. By Lemma 4.15, a
hyperplane of Fk
q contains at most n −d = k −1 vectors of S. This implies that S is a
set and not a multi-set and that any k-subset of S is a basis of Fk
q.
Suppose that every k-subset of S is a basis of Fk
q. Then uG has at most k −1 zeros for
any non-zero u ∈Fk
q. Therefore, the non-zero codewords of the code C generated by G have
weight at least n −k + 1. By Lemma 4.1, the minimum weight of a non-zero codeword is
equal to the minimum distance, so C has minimum distance at least n −k + 1. Theorem 6.1
implies that the minimum distance of C is n −k + 1 and so C is MDS.
⊓⊔
Theorem 6.11
The dual of a linear MDS code is a linear MDS code.
Proof
Let C be a k-dimensional linear MDS code of length n and let S = {s1, . . . , sn} be the set of
columns of a generator matrix for C. The dual code C⊥is a (n−k)-dimensional linear code.
Suppose (u1, . . . , un) ∈C⊥is a non-zero codeword of weight at most k. Since
u1s1 + · · · + unsn = 0,
this implies that there is a linear combination of at most k of the vectors of S which are
linearly dependent, contradicting Theorem 6.10. Therefore, the minimum non-zero weight
of C⊥is k + 1. Lemma 4.1 implies that C⊥has minimum distance at least k + 1, which
implies that C⊥is MDS since
n −(n −k) + 1 = k + 1.
⊓⊔
An arc is a set S of vectors of Fk
q with the property that every subset of S of size k
is a set of linearly independent vectors, i.e. is a basis of Fk
q. Putting the vectors of an arc

6.3 · Linear MDS Codes
93
6
of size n as the columns of a k × n matrix, one obtains a generator matrix of a linear
MDS code of length n. Vice versa, the set of columns of a generator matrix of a linear
MDS code is an arc of Fk
q of size n. One can also consider the arc as a set of points in
the projective space PG(k −1, q), since the linear independence property is unchanged
if we take non-zero scalar multiples of the vectors of S. Thus, an arc in PG(k −1, q) is
a set S of points of PG(k −1, q) with the property that every subset of size k spans the
whole space.
Theorem 6.12
If k ⩾q, then a k-dimensional linear MDS code over Fq has minimum distance at
most 2.
Proof
Suppose that the minimum distance of a linear MDS code C of length n is at least 3. By
Theorem 6.3, n = d + k −1 ⩾k + 2.
By Theorem 6.10, the set S of columns of a generator matrix of C is an arc of Fk
q.
Order the vectors in S arbitrarily and let
B′ = {e1, . . . , ek}
be the ﬁrst k vectors of S.
Consider the coordinates of the (k + 1)-st and the (k + 2)-nd vector of S with respect
to the basis B′. Suppose there is a zero in the i-th coordinate of one of them. Then the
hyperplane, deﬁned by Xi = 0, contains k vectors of S (since it contains k −1 vectors of
B′), contradicting the arc property.
Therefore, we can ﬁnd λj, non-zero elements of Fq, such that the (k + 1)-st vector in S
is the all-one vector with respect to the basis
B = {λ1e1, . . . , λkek}.
Let (s1, . . . , sk) be coordinates of the (k + 2)-nd vector of S with respect to the basis B. As
observed above si ̸= 0, for all i = 1, . . . , k. Since k > q −1, the pigeon-hole principle
implies there exists an i and j such that si = sj. Therefore, the hyperplane deﬁned by the
equation Xi = Xj contains k vectors of S, again contradicting the arc property.
⊓⊔
We can obtain a binary code from a linear code over Fq by identifying each element
of Fq with a binary string of length ⌈log2 q⌉. In this way, if we take a k-dimensional
linear code of length N over Fq, then we get a binary code of length n = N⌈log2 q⌉
with qk elements. The rate of the binary code is log2 |C|/n ≈k/N and the relative
minimum distance is approximately d/(N log2 q).
From a linear MDS code we obtain a binary code with a transmission rate R of
approximately k/N, whose relative minimum distance is approximately

6
94
Chapter 6 • Maximum Distance Separable Codes
⊡Fig. 6.1 A burst of three errors distorting only one Fq bit.
1 −R
log2 q + 1
n.
If k ⩾q, then according to Theorem 6.12 the minimum distance is at most 2, so we
assume k < q. But then Exercise 6.6 implies N < 2q −1 which implies that n <
2q log2 q. If we want n to tend to inﬁnity, we must have q going to inﬁnity, which implies
that if the rate is bounded away from zero, then the relative minimum distance will tend
to zero. Hence, we cannot hope to obtain asymptotically good codes in this way. This
does not mean that MDS codes and in particular Reed–Solomon codes are not used
in practice. Although as a binary code, a Reed–Solomon code can only guarantee the
correction of a relatively small amount of errors, in certain circumstances it can correct
many more errors.
Consider ⊡Figure 6.1. Each element of Fq is mapped to a string of ⌈log2 q⌉binary
bits. If all the errors in the transmission of the binary string occur in the same run of
bits, then when we look to decode the Fq-linear code, only one bit has been distorted.
This is particularly useful for channels in which errors tend to come in bursts. For this
reason Reed–Solomon codes, and more generally linear codes over large ﬁelds, are very
useful for burst-error correction. Furthermore, a common method in the application of
Reed–Solomon codes is to simultaneously use two codes of high rate. For example, in
a CD the data is encoded using two Reed–Solomon codes over F64 which are cross-
interleaved. Add to that the fast decoding algorithms which allow us to decode past the
half the minimum distance bound with high probability and one has a fast and efﬁcient
means of error-correction.
6.4
MDS Conjecture
Theorem 6.12 implies that we should only look for linear MDS codes of dimension
k ⩽q −1. Theorem 6.11 implies that a k-dimensional MDS code of length n exists
if and only if a (n −k)-dimensional MDS code of length n exists. By Exercise 6.6, a
2-dimensional MDS code has length at most q + 1 and a 3-dimensional MDS code has
length at most q + 2. By Theorem 6.11, the dual of a (q −1)-dimensional MDS code
of length q + 2 is a 3-dimensional MDS code of length q + 2, which by Exercise 6.7
does not exist for q odd and, by Exercise 6.9, does exist for q even. From this we can
deduce the length of the longest linear MDS codes for all dimensions not in the range

6.4 · MDS Conjecture
95
6
4 ⩽k ⩽q −2. The MDS conjecture asserts that, within this range, one cannot do
better than the Reed–Solomon code.
Conjecture 6.13 (MDS conjecture) If 4 ⩽k ⩽q −2, then a k-dimensional linear MDS
code of length n satisﬁes n ⩽q + 1.
The MDS conjecture has been veriﬁed for q prime. It follows from the following
theorem, whose proof we will sketch at the end of this section.
Theorem 6.14
Let q = ph, where p is prime. If k ⩽p, then a k-dimensional linear MDS code of
length n satisﬁes n ⩽q + 1.
We will prove the following theorem only in the case k = 3. In this case the
hypothesis k ̸=
1
2(q + 1) is not necessary. The hypothesis k ⩽p is necessary. The
following example is not equivalent to a Reed–Solomon code.
Example 6.15
Let F32 = {a1, . . . , a32} and let C be the three-dimensional linear code over F32 generated by
the matrix G whose i-the column is (1, ai, a4
i )t, for i = 1, . . . , 32, and whose 33-rd column
is (0, 0, 1)t. A generic 3 × 3 submatrix of G is of the form
⎛
⎜⎝
1
1 1
x
y
z
x4 y4 z4
⎞
⎟⎠.
The determinant of this matrix is
(z −x)4(y −x) −(y −x)4(z −x)
If this determinant is zero, then ((z −x)/(y −x))3 = 1. Since the ﬁeld F32 contains no
element e ̸= 1 such that e3 = 1, this implies that z = y. In this way we see that all 3 × 3
submatrices of G are non-singular and, by Theorem 6.10, C is an MDS code.
■

6
96
Chapter 6 • Maximum Distance Separable Codes
Theorem 6.16
Let q = ph, where p is prime. If k ⩽p and k ̸= 1
2(q +1), then a k-dimensional linear
MDS code of length q + 1 is a Reed–Solomon code.
Proof (for k = 3)
Let S be the set of columns of a generator matrix G of a 3-dimensional linear MDS code C
of length q + 1. By Theorem 6.10, a hyperplane of F3
q contains at most two vectors of S.
Let s ∈S. There are q + 1 hyperplanes containing s, q of which contain a vector of
S \ {s}. Hence, there is exactly one hyperplane of F3
q which contains s and no other vector
of S.
Let fs(X) be a linear form whose kernel is this hyperplane.
Let x, y, z ∈S. We claim that
fx(y)fy(z)fz(x) = fy(x)fz(y)fx(z).
With respect to the basis B = {x, y, z}, the hyperplane that contains s = (s1, s2, s3) and
x = (1, 0, 0) is the kernel of the linear form
X2 −(s2/s3)X3.
Note that s3 ̸= 0 since the hyperplane ker X3 is incident with x and y.
The linear form
fx(X) = a2X2 + a3X3,
for some a2, a3 ∈Fq, since the kernel of fx(X) contains x.
For distinct s ∈S \ {x, y, z}, the value of s2/s3 is distinct, since the linear form
X2 −(s2/s3)X3
is different, for different s ∈S \ {x, y, z}.
Since,
X2 −(s2/s3)X3 ̸= X2 + (a3/a2)X3,
for any s ∈S \ {x, y, z}, we have that
{s2
s3
| s ∈S \ {x, y, z}} ∪{−a3
a2
}
is the set of all non-zero elements of Fq.
In ⊡Figure 6.2, the vectors x, y and z are drawn as points in PG(2, q) and the
hyperplanes deﬁned as the kernels of fx, fy and fz are lines in the plane.
Note that a2 = fx(y) and a3 = fx(z). The product of all the non-zero elements of Fq is
−1, so we have that

6.4 · MDS Conjecture
97
6
⊡Fig. 6.2 The lines joining the basis points to s and the tangent lines.
−fx(z)
fx(y)

s∈S\B
s2
s3
= −1.
Similarly,
fy(x)
fy(z)

s∈S\B
s3
s1
= 1 and
fz(y)
fz(x)

s∈S\B
s1
s2
= 1.
Multiplying the three equations together establishes the claim.
For any s ∈S, the linear form fs(X) is determined by its value at the basis elements, so
fs(X) = fs(x)X1 + fs(y)X2 + fs(z)X3.
Evaluating at X = s, and using the fact that fs(s) = 0,
fs(x)s1 + fs(y)s2 + fs(z)s3 = 0.
By the claim,
s1 + fy(s)fx(y)
fx(s)fy(x)s2 + fz(s)fx(z)
fx(s)fz(x)s3 = 0.
Now, substituting
fx(s) = fx(y)s2 + fx(z)s3,
fy(s) = fy(x)s1 + fy(z)s3,
fz(s) = fz(x)s1 + fz(y)s2,
we get

6
98
Chapter 6 • Maximum Distance Separable Codes
2(c3s1s2 + c2s1s3 + c1s2s3) = 0,
for some c1, c2, c3 ∈Fq \ {0}.
Explicitly,
c1 = fy(z)fx(y)
fy(x), c2 = fx(z), and c3 = fx(y).
Since we are assuming that q is odd, the vectors of S are zeros of the quadratic form
c3X1X2 + c2X1X3 + c1X2X3.
The zeros of this quadratic form excluding (0, 0, 1) are parameterisable by
s = (t, c2
c2
1
(c3 −c1t), t
c1
(c1t −c3)).
Therefore, the i-th coordinate of a column of G is the evaluation of the polynomial fi(X),
where
f1(X) = X, f2(X) = −c2c−1
1 X + c−2
1 c2c3
and
f3(X) = X2 −c−1
1 c3X,
with the exception of the column (0, 0, 1), whose i-th coordinate is the coefﬁcient of X2 of
fi(X).
Hence, the codeword (u1, u2, u3)G is the evaluation of the polynomial
u1f1(X) + u2f2(X) + u3f3(X),
so C is a Reed–Solomon code.
⊓⊔
The proof of Theorem 6.14 and the general proof of Theorem 6.16 follow the same
strategy as the proof of Theorem 6.16 given here for k = 3.
Let S be the set of columns of a generator matrix of a k-dimensional linear MDS
code over Fq. Let τ = q + k −1 −|S|.
Let A be a (k −2)-subset of S. There are q + 1 hyperplanes which contain the
(k −2)-dimensional subspace of Fk
q spanned by the points of A, τ of which contain no
other vectors of S. With linear forms α1, . . . , ατ, whose kernels are these τ hyperplanes,
we deﬁne a polynomial
fA(X) =
τ
i=1
αi(X).

6.4 · MDS Conjecture
99
6
This deﬁnes fA(X) uniquely up to scalar factor. The claim is then that
fD∪{x}(y)fD∪{y}(z)fD∪{z}(x) = (−1)τ+1fD∪{y}(x)fD∪{z}(y)fD∪{x}(z),
for all k-subsets D ∪{x, y, z} of S.
To simplify matters slightly, let us assume that τ is odd. Since the polynomials
fA(X) are deﬁned up to scalar factor, we can scale them in such a way that the above
equality gives
fD∪{x}(y) = fD∪{y}(x).
This implies that for C, a (k −1)-subset of S, there is a non-zero aC ∈Fq, such that for
all e ∈C,
fC\{e}(e) = aC.
The interpolation of the linear form fs(X) in the proof for k = 3 is generalised to
interpolating the polynomial fA(X) of degree τ. Even though fA(X) is a homogeneous
polynomial in k variables, since it is the product of linear forms whose kernels contain
the subspace spanned by A = {a1, . . . , ak−2}, with respect to a basis of Fk
q containing
A, fA(X) is a homogeneous polynomial in two variables. Hence, it can be interpolated.
To be able to interpolate we ﬁx a subset E of S of size k + τ and an element x ∈E.
Writing det(X, u, A) as a shorthand for
            
X1
X2
. . .
Xk
u1
u2
. . .
uk
a11
a12
. . .
a1k
...
...
. . .
...
ak−2,1 ak−2,2 . . . ak−2,k
            
the interpolation implies
fA(X) =

e∈E\(A∪{x})
fA(e)

u∈E\(A∪{x,e})
det(X, u, A)
det(e, u, A) .
One can check that substituting X = e gives fA(e) on the right-hand side for all e ∈
E \ {x}.
Substituting X = x and rearranging terms leads to the equation

e∈E\A
fA(e)

u∈E\(A∪{e})
det(u, e, A)−1 = 0.

6
100
Chapter 6 • Maximum Distance Separable Codes
This implies that for a (k −2)-subset A of E,

C⊃A
aC

u∈E\C
det(u, C)−1 = 0,
where the sum runs over the (k −1)-subsets C of E containing A.
Thus, we get an equation for each (k −2)-subset A of E. Setting
λC = aC

u∈E\C
det(u, C)−1,
the equation is simply

C⊃A
λC = 0,
where the sum runs over the (k −1)-subsets C of E containing A.
This set of equations is enough to prove both Theorem 6.14 and, with a little more
work, Theorem 6.16. To prove Theorem 6.14, we assume that the length of the MDS
code is q + 2 and so |S| = q + 2. We can assume by Theorem 6.11 that k ⩽(q + 2)/2,
taking the dual code if necessary. Since |S| = q + 2 we have that τ = k −3 and
|E| = 2k −3. There are N =
2k−3
k−2

linear equations. For each (k −1)-subset C of
E, we have an unknown λC, so in all we have
2k−3
k−1

unknowns. Thus, we have a linear
system of N equations in N unknowns. This system of equations implies the equation
(k −1)!λC = 0,
which is a contradiction if k ⩽p, since λC ̸= 0.
6.5
Comments
The Singleton bound appears in Singleton’s paper [66] on MDS codes from 1964, the
Reed–Solomon codes having already been published some years before in [60]. In
the same paper the authors detail the decoding algorithm presented in Theorem 6.6.
An algorithm based on interpolation was proposed by Berlekamp and Welch [9]. As
mentioned in the text, the list decoding of Theorem 6.9, from Sudan [68], decodes up
to (1 −
√
2R)n errors and produces a list with a constant number of codewords. This
bound improves on the unique decoding bound of 1
2(1 −R)n when R < 3 −2
√
2. The
bound can be improved to (1 −
√
R)n by interpolating the zeros with multiplicity, see
Guruswami and Sudan [33]. The bound (1 −
√
R)n is larger than the unique decoding
bound for all R. There is a limit of (1 −R)n errors, beyond which one cannot hope to
produce a list with a constant number of codewords, see Guruswami and Rudra [32].
Indeed, one can ﬁnd received vectors for which the number of codewords of the Reed–
Solomon code, at a distance of (1 −R)n or less, is not bounded by a polynomial in n.

6.6 · Exercises
101
6
One can use a Reed–Solomon code, or more generally an MDS code, in concate-
nation codes to produce binary codes which come arbitrarily close to the Gilbert–
Varshamov bound asymptotically. This construction is due to Thommesen
[70].
Suppose we have an MDS code of length N and rate R over a ﬁeld with 2k elements.
A codeword is a vector v = (v1, . . . , vN) ∈FN
2k. For each coordinate, we randomly
choose Gi, a k ×n matrix with entries from F2. We consider each coordinate vi of v as a
vector in Fk
2. Recall that, in ▷Chapter 2, we constructed the elements of F2k as elements
of F2[X]/(f ), where f (X) is an irreducible polynomial of degree k in F2[X]. If
vi = a0 + a1X + · · · + ak−1Xk−1
in this quotient ring, then we consider vi as the vector (a0, a1, . . . , ak−1). We then make
a binary code of length nN by taking as its elements the strings
u = (v1G1, v2G2, . . . , vNGN).
With a high probability this binary code will be arbitrarily close to the Gilbert–
Varshamov bound.
For more on applications of Reed–Solomon codes, in particular the simultaneous
use of two Reed–Solomon codes in compact discs, see [76].
MDS codes are widely used in distributed storage systems to protect data from server
failures. A codeword vG of a k-dimensional linear MDS code can be recovered from
just k coordinates, since k of the coordinates uniquely determine v and therefore the
codeword. This recoverability property is the important feature of local reconstruction
codes, of which MDS codes are a special sub-class. A local reconstruction code is a
block code in which for each coordinate i, there is a subset of the coordinates Ri, such
that knowing the coordinates in Ri of a codeword, one can recover the i-th coordinate of
the codeword. It is this more general class of codes which are implemented in distributed
storage systems.
The MDS conjecture appears in [50] although it origins can be traced back to
the fundamental questions asked by Segre [64] in 1967. Proofs of Theorem 6.14 and
Theorem 6.16 can be found in Ball [5], although the original proof of Theorem 6.16 for
k = 3 is due to Segre [63].
The MDS code in Exercise 6.10 was discovered by Glynn [26], and the MDS code
in Exercise 6.11 is due to Segre [64].
6.6
Exercises
6.1 Prove that the dual of a Reed–Solomon code is a Reed–Solomon code.
6.2 Shortening the k-dimensional Reed–Solomon code over Fq, by removing the last column
and the column which is the evaluation of the polynomial at zero in Example 6.4, we get a
k-dimensional linear code C of length q −1. Prove that C is the cyclic code in Example 5.12.

6
102
Chapter 6 • Maximum Distance Separable Codes
6.3 Let C be the 4-dimensional shortened Reed–Solomon code of length 7 over F7 which is
the evaluation of polynomials of degree at most three, where the elements of F7 are ordered as
{0, 1, 2, 3, 4, 5, 6}. Decode the received vector (1, 1, 0, 4, 2, 2, 1) using the algorithm from
Theorem 6.6.
6.4 Suppose that E is the set of coordinates where an error has occurred in transmission.
Prove that in the set of equations in Theorem 6.6, h(ai) = 0 for all i ∈E.
6.5 Let C be the linear code from Exercise 4.8. Prove that C⊥is an MDS code and that
therefore C is an MDS code.
6.6 Prove that if there exists an [n, k, n −k + 1]q code, then n ⩽q + k −1.
6.7 Let S be the set of columns of a generator matrix of a 3-dimensional linear MDS code
of length n, considered as a set of points of PG(2, q).
i.
Prove that S is a set of n points, no three of which are collinear.
ii.
Prove that if q is odd then a 3-dimensional linear MDS code has length at most q + 1.
iii.
Prove that if q is even, then a 3-dimensional linear MDS code of length q + 1 is
extendable to a 3-dimensional linear MDS code of length q + 2.
6.8 Let S be the set of columns of a generator matrix of a 4-dimensional linear MDS code
of length n, considered as a set of points of PG(3, q).
i.
Prove that S is a set of n points, no 4 of which are contained in a hyperplane.
ii.
Prove that if |S| = q + 2 and q is even, then to each point x of S there is a line ℓx,
incident with x, with the property that each plane containing ℓx contains exactly one
point of S \ {x}.
iii.
Prove that there are no 4-dimensional linear MDS codes of length q + 3.
6.9 Show that if e and h are co-prime, then the matrix whose columns are {(1, t, t2e) | t ∈
Fq} ∪{(0, 1, 0), (0, 0, 1)} generates a 3-dimensional linear MDS of length q + 2 over Fq,
q = 2h.
6.10 Show that the matrix whose columns are {(1, t, t2 + ηt6, t3, t4) | t
∈
F9} ∪
{(0, 0, 0, 0, 1)}, where η4 = −1, generates a 5-dimensional MDS of length 10 over F9.
6.11 Suppose q is even and let σ be an automorphism of Fq. Let
M =
⎛
⎜⎜⎜⎝
eσ+1 eσ b ebσ bσ+1
eσ c
eσ d bσ c bσ d
cσ e
cσ b dσ e dσ b
cσ+1 cσ d cdσ dσ+1
⎞
⎟⎟⎟⎠.
Let A be the 4 × 4 matrix whose i-th column is the transpose of (1, ti, tσ
i , tσ+1
i
).

6.6 · Exercises
103
6
i.
Verify that if e, b, c, d are chosen so that c+dt1 = 0, e+bt2 = 0 and e+bt3 = c+dt3,
then
det MA = ((e + bt1)(c + dt2)(e + bt3))σ+1 det
⎛
⎜⎜⎜⎝
1 0 1 (e + bt4)σ+1
0 0 1 (e + bt4)σ (c + dt4)
0 0 1 (c + dt4)σ (e + bt4)
0 1 1 (c + dt4)σ+1
⎞
⎟⎟⎟⎠.
ii.
Prove that if there is no non-zero element a ∈Fq for which aσ = a, then the code
generated by the matrix G, whose columns are the transpose of (1, t, tσ , tσ+1), for each
t ∈Fq, is a 4-dimensional linear MDS code of length q.
iii.
Prove that by adding the column (0, 0, 0, 1)t to G, the code generated by the matrix
extends the 4-dimensional linear MDS code of length q to a 4-dimensional linear MDS
code of length q + 1.

105
7
Alternant and Algebraic Geometric
Codes
Alternant codes are subﬁeld subcodes of a generalised Reed–Solomon code over an
extension ﬁeld of Fq. This is a large class of linear codes which includes BCH codes,
one of the families of cyclic codes which appeared in ▷Chapter 5. Although BCH codes
are not asymptotically good, we will prove that there are asymptotically good alternant
codes. Not only are alternant codes linear, and so easy to encode, they also have an
algebraic structure which can be exploited in decoding algorithms. However, as with
the codes constructed in Theorem 3.7, the construction of these asymptotically good
alternant codes is probabilistic. We prove that such a code must exist without giving an
explicit construction.
Algebraic geometric codes are codes constructed from algebraic curves. We shall
cover the basic properties of these codes and prove that algebraic geometric codes can
provide examples of asymptotically good codes. In the case of r-ary codes, where r is
the square of an odd prime larger than or equal to 7, there are algebraic geometric codes
whose rate and relative minimum distance exceed the Gilbert–Varshamov bound.
7.1
Subfield Subcodes
Let C be a linear code over Fqh of length n.
The subﬁeld subcode A(C) of C is a code over Fq, deﬁned as the set of codewords
of C all of whose coordinates are elements of Fq.
Lemma 7.1 If C is a [n, k′, d]qh code, then A(C) is a [n, k, ⩾d]q code, where k′ ⩾k ⩾
n −(n −k′)h.
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_7

7
106
Chapter 7 • Alternant and Algebraic Geometric Codes
Proof
Suppose u, v ∈A(C). Then u, v ∈C and since C is linear u + v ∈C. Since u + v has
coordinates in Fq, u + v ∈A(C). Similarly λu ∈A(C), for all λ ∈Fq, so A(C) is linear
over Fq.
Let H be a (n −k′) × n check matrix for C. Then (a1, . . . , an) ∈A(C) if and only if
a1x1 + · · · anxn = 0,
for all rows (x1, . . . , xn) of H.
As we saw in ▷Chapter 2, the elements xi of Fqh are polynomials in the ring Fq[X]/(f ),
where f is an irreducible polynomial of degree h in Fq[X]. Writing the equation above over
Fq[X] gives at most h constraints (j = 1, . . . , h) on A(C) of the form
a1x1j + · · · + anxnj = 0,
where xij ∈Fq is deﬁned by
xi = xi1 + xi2X + · · · + xihXh−1.
Therefore, a check matrix for A(C) has rank at most (n −k′)h, which implies that the
dimension of A(C) is at least n −(n −k′)h.
⊓⊔
Example 7.2
Let e be a primitive element of F9 which satisﬁes e2 = e + 1.
Consider the check matrix
H =

1 1 . . . 1 1 0
e e2 . . . e8 0 1

of a [10, 8, 3]9 code C.
Writing out the elements of H as a + be, where a, b ∈F3, we get a check matrix
⎛
⎜⎜⎜⎝
1 1 1 1 1 1 1 1 1 0
0 0 0 0 0 0 0 0 0 0
0 1 1 2 0 2 2 1 0 1
1 1 2 0 2 2 1 0 0 0
⎞
⎟⎟⎟⎠
for A(C) over F3. The rank of this matrix is 3, so A(C) has dimension 7. A non-zero
codeword of C has weight at least 3, so it follows that a non-zero codeword of A(C) has
weight at least 3. Thus, A(C) is a [10, 7, ⩾3]3 code and since (0, 0, 0, 0, 0, 0, 1, 2, 2) ∈
A(C), A(C) is a [10, 7, 3]3 code.
■

7.2 · Generalised Reed–Solomon Codes
107
7
7.2
Generalised Reed–Solomon Codes
Let x1, . . . , xn be distinct elements of Fqh and let v1, . . . , vn be non-zero elements
of Fqh.
The linear code over Fqh deﬁned by
C = {(v1f (x1), . . . , vnf (xn)) | f ∈Fqh[X], deg f ⩽k′ −1},
where k′ < qh, is a generalised Reed–Solomon (GRS) code.
The subﬁeld subcode A(C) is an alternant code if C is a GRS code. Alternant codes
are useful because they allow us to construct linear codes over small ﬁelds (including
binary codes). Not only can they have good parameters, the algebraic structure, inherent
in their construction, can be exploited to develop reasonably fast decoding algorithms.
Example 7.3
Let e be a primitive element of F8, such that e3 = e + 1 and let C be the GRS code
{(f (0), f (1), f (e), e6f (e2), f (e3), e4f (e4), e5f (e5), e3f (e6)) |f ∈F8[X],
deg f ⩽5}.
The matrix

1 1 1 e 1 e3 e2 e4
0 1 e e3 e3 1 1 e3

is a check matrix for C. This can readily checked. The scalar product of the ﬁrst and second
rows of the matrix with a codeword of C is

x∈F8
f (x) and

x∈F8
xf (x)
respectively. For a polynomial f of degree at most 5 these sums are zero, see Exercise 2.8.
As in Example 7.2, we write out the elements of H as a + be + ce2, where a, b, c ∈F2.
In this way, we get a check matrix
H =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 1 1 0 1 1 0 0
0 0 0 1 0 1 0 1
0 0 0 0 0 0 1 1
0 1 0 1 1 1 1 1
0 0 1 1 1 0 0 1
0 0 0 0 0 0 0 0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
for A(C). The matrix H has rank 5, so the dimension of A(C) is 3.
By solving the system of equations Hut = 0, where u ∈F8
2, we can ﬁnd a basis for
the code A(C) and therefore a generator matrix G for A(C). One can readily check that
GHt = 0, where

7
108
Chapter 7 • Alternant and Algebraic Geometric Codes
G =
⎛
⎜⎝
0 0 1 1 0 1 0 0
1 1 1 0 1 0 0 0
1 1 0 1 0 0 1 1
⎞
⎟⎠.
The seven non-zero codewords of A(C) have weight at least 3. By Lemma 4.1, the minimum
distance of A(C) is 3, since this is minimum weight of a non-zero codeword of A(C).
Therefore, A(C) is a [8, 3, 3]2 code.
■
Lemma 7.4 If C is a k′-dimensional GRS code of length n over Fqh, then the minimum
distance of A(C) is at least n −k′ + 1.
Proof
As for Reed–Solomon codes, since a non-zero polynomial of degree at most k′−1 has at most
k′ −1 zeros, a non-zero codeword of the code C has weight at least n −(k′ −1). Therefore,
the weight of any non-zero codeword of A(C) is at least n −k′ + 1. By Lemma 4.1, the
minimum weight of a non-zero codeword is equal to the minimum distance.
⊓⊔
The following lemma is Lagrange interpolation, which will be used to prove
Lemma 7.6.
Lemma 7.5 Suppose that a1, . . . , ak are distinct elements of Fq and let b1, . . . , bk be
elements of Fq. There is a unique polynomial f ∈Fq[X] of degree at most k −1 such
that f (ai) = bi, for all i = 1, . . . , k.
Proof
For each i = 1, . . . , k, the equality f (ai) = bi is a constraint on the polynomial f (X) =
k−1
i=1 ciXi,
c0 + c1ai + · · · + ck−1ak−1
i
= bi.
In matrix form this system of equations is
⎛
⎜⎜⎜⎝
1 a1 . . . ak−1
1
. . . . .
.
. . . . .
.
1 ak . . . ak−1
k
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
c0
.
.
ck−1
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
b1
.
.
bk
⎞
⎟⎟⎟⎠.
Since the matrix is a Vandermonde matrix and the ai’s are distinct, the matrix has non-zero
determinant and so the system of equations has a unique solution.
⊓⊔
In Theorem 7.7, we are going to prove that there are alternant codes which do not
have any non-zero codewords of small weight. We do this by counting, bounding by
above the number of alternant codes containing vectors of small weight d. Then, by
proving there are more alternant codes than alternant codes containing vectors of weight
less than d, we conclude that there are alternant codes with minimum distance at least

7.3 · Alternant Codes Meeting the Gilbert–Varshamov Bound
109
7
d. Firstly, we bound the number of alternant codes containing a ﬁxed non-zero vector
of Fn
q.
Lemma 7.6 Let a be a non-zero vector of Fn
q. The number of k′-dimensional GRS codes of
length n over Fqh containing a, for a ﬁxed n-tuple (x1, . . . , xn) of distinct elements of Fqh,
is at most (qh −1)k′.
Proof
If there is a k′-dimensional GRS codes of length n over Fqh containing a, then a has at most
k′ −1 zero coordinates. After a suitable permutation of the coordinates, assume that all the
zero coordinates of a are contained in the ﬁrst k′ −1 coordinates. Choose v1, . . . , vk′ ∈
Fqh \ {0}.
So that a is contained in the GRS code with this choice of v1, . . . , vk′, there must be a
polynomial f of degree at most k′ −1 such that
f (xi) = ai
vi
,
for i = 1, . . . , k′. By Lemma 7.5, there is a unique polynomial f with this property.
For j = k′+1, . . . , n, if f (xj) = 0, then a is not contained in a GRS code for this choice
of v1, . . . , vk′, since the zeros of a all occur in the ﬁrst k′ −1 coordinates. Hence, f (xj) ̸= 0
and the elements vj are ﬁxed by
vj =
aj
f (xj).
Moreover, vj ̸= 0 since aj ̸= 0 for these values of j.
⊓⊔
7.3
Alternant Codes Meeting the Gilbert–Varshamov Bound
We now prove that there are alternant codes of rate R and minimum distance d whose
parameters approximate to the parameters of a code on the Gilbert–Varshamov curve of
⊡Figure 3.1.
Theorem 7.7
There are asymptotically good alternant codes of rate R ∈Q meeting the Gilbert–
Varshamov bound.
Proof
Choose h and n so that Rn is an integer, h divides n −Rn and
k′ = n −n(1 −R)
h
< qh.

7
110
Chapter 7 • Alternant and Algebraic Geometric Codes
By Lemma 7.6, for a ﬁxed x1, . . . , xn, the number of k′-dimensional GRS codes over Fqh of
length n containing the vectors of Fn
q of weight at most d −1 is at most
(qh −1)k′ d−1

j=0
(q −1)j
n
j

.
Since we can choose v1, . . . , vn ∈Fqh \ {0}, the total number of k′-dimensional GRS codes
for a ﬁxed x1, . . . , xn is (qh −1)n.
Therefore, if
(qh −1)k′ d−1

j=0
(q −1)j
n
j

< (qh −1)n,
then there is a k′-dimensional GRS code C with no non-zero codewords of weight less than d.
By Lemma 7.1, the alternant code A(C) has dimension k ⩾n −(n −k′)h = Rn, so the
rate of A(C) is at least R. Substituting k′ = n −n(1 −R)/h, the condition is that
(qh −1)Rn/h
d−1

j=0
(q −1)j
n
j

< (qh −1)n/h,
which is (asymptotically) the Gilbert–Varshamov bound, since (qh −1)r/h ≈qr.
⊓⊔
The following theorem says that we can decode received vectors with few errors
quickly. Observe that the minimum distance d, for the alternant code which we get from
Theorem 7.7, is at least n −k′ + 1 by Lemma 7.4.
Theorem 7.8
Let C be a k′-dimensional GRS code of length n. If the number of errors that occur in
the transmission of a codeword of the alternant code A(C) is at most 1
2(n −k′), then
there exists a polynomial time decoding algorithm which corrects the errors.
Proof
This is similar to the proof of Theorem 6.6.
We suppose g is an arbitrary polynomial of degree ⌈1
2(n + k′)⌉−1 and that h is an
arbitrary polynomial of degree ⌊1
2(n −k′)⌋.
We determine the coefﬁcients of g and h by solving the system of n equations,
g(aj) −h(aj)v−1
j yj = 0,
for j = 1, . . . , n. The homogeneous linear system has n + 1 unknowns (the coefﬁcients of g
and h) and n equations. Hence, we can ﬁnd a non-trivial solution (in polynomial time) which
determines h(X) and g(X).

7.3 · Alternant Codes Meeting the Gilbert–Varshamov Bound
111
7
By assumption, there is a polynomial f of degree at most k′ −1, such that yj = vjf (aj)
for at least n −⌊1
2(n −k′)⌋values of j. For these values of j, the evaluation at aj of
g(X) −h(X)f (X)
is zero. The degree of this polynomial is at most ⌈1
2(n + k′)⌉−1. Since
n −⌊1
2(n −k′)⌋> ⌈1
2(n + k′)⌉−1,
it has more zeros than its degree, so it is identically zero. Therefore, h(X) divides g(X) and
the quotient is f (X) = g(X)/h(X).
⊓⊔
Example 7.9
Suppose that we are using the code A(C), where C is deﬁned as in Example 7.3, and that we
have received the vector
y = (0, 0, 1, 0, 1, 0, 1, 1).
According to Theorem 7.8, we should solve the equations
vjg(aj) −h(aj)yj = 0,
where j = 1, . . . , 8, and where g(X) is a polynomial of degree at most 6 and
h(X) = h1X + h0.
The equations are
g(0) = 0, g(1) = 0, g(e) = h(e), g(e2) = 0, g(e3) = h(e3), g(e4) = 0,
g(e5)e5 = h(e3), g(e6)e3 = h(e6).
From these equations we have that
g(X) = X(X + 1)(X + e2)(X + e4)(g2X2 + g1X + g0)
for some g0, g1 and g2 and that
⎛
⎜⎜⎜⎝
e5 e4 e3 e 1
1 e4 e e3 1
e6 e e3 e5 1
e5 e6 1 e6 1
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎜⎜⎝
g2
g1
g0
h1
h0
⎞
⎟⎟⎟⎟⎟⎟⎠
= 0.

7
112
Chapter 7 • Alternant and Algebraic Geometric Codes
This homogeneous system of equations has a solution with g2 = 1 which gives
g(X) = X(X + 1)(X + e2)(X + e4)(X2 + eX + e2)
and
h(X) = eX + e3.
Therefore, by Theorem 7.8, the sent codeword is the evaluation of
g(X)
h(X) = e6X(X + 1)(X + e4)(X2 + eX + e2).
One can check that the evaluation of this polynomial is the codeword
(0, 0, 1, 1, 1, 0, 1, 1).
■
We can use the list decoding algorithm of Theorem 6.9 to correct more distorted
codewords. To do this we make a slight modiﬁcation of the interpolation and solve the
system of equations
Q(xℓ, v−1
ℓyℓ) = 0.
This will produce a short list of possibilities for the sent codeword. The list of
possibilities is further reduced since we can discard any vectors in the list which are
not codewords of A(C).
7.4
Algebraic Geometric Codes
Let φ be an absolutely irreducible homogeneous polynomial in Fq[X, Y, Z] of degree
m. Recall that absolutely irreducible means that φ is irreducible over Fq, an algebraic
closure of Fq.
Let χ be the plane curve, deﬁned as the points where φ is zero, where the points are
points of the projective plane deﬁned over Fq. It is not necessary to take χ to be a plane
curve, but it will make things simpler and more apparent if we assume for the moment
that it is. In the next section, we will consider an example of a higher dimensional curve.
We direct the reader to the comments section for references to a more general treatment
of algebraic geometric codes. Although we will deﬁne our codes over Fq, we have to
consider the curve over Fq. It is essential that when we apply Bezout’s theorem, the
number of points in the intersection of a curve deﬁned by a homogeneous polynomial g
and a curve deﬁned by a homogeneous polynomial h is (deg g)(deg h), where we have
to count the intersections with multiplicity.

7.4 · Algebraic Geometric Codes
113
7
The coordinate ring is deﬁned as the quotient ring
Fq[χ] = Fq[X, Y, Z]/(φ).
Therefore, the elements of Fq[χ] are residue classes which can be represented by
polynomials. We will only be interested in residue classes represented by homogeneous
polynomials. We will be particularly interested in the elements of
Fq(χ) = {f | f = g/h, for some homogeneous g, h ∈Fq[χ] of the same degree}.
An element f of Fq(χ) can have very different representations.
Example 7.10
Let χ be the curve deﬁned by X3 = Y 2Z. The element f of Fq(χ) represented by X2/Y 2 is
the same as the element of Fq(χ) represented by Z/X.
■
The elements of Fq(χ) do not in general deﬁne functions on the curve χ, since there
may be a point P of χ where h is zero. However, there may be another representation
of the same element of Fq(χ), where h is not zero, so the evaluation of f is deﬁned.
As in ▷Section 2.4, we denote by (x : y : z) the point of PG(2, q) with vector
representative (x, y, z).
Example 7.11
In Example 7.10, f is deﬁned at the point P = (0 : 1 : 0), even though in the representation
Z/X we have a zero in the denominator. Indeed, using the representation X2/Y 2, we deduce
that f has a zero at P . However, f is not deﬁned at the point (0 : 0 : 1), where it has a pole.
■
The elements of Fq are representatives of elements of Fq(χ), since they are
polynomials of degree 0 divided by a polynomial of degree 0. These elements deﬁne
a constant function on the curve χ.
We deﬁne a divisor as a ﬁnite sum of the points of φ with integer coefﬁcients,
D =

P∈χ
nP P.
At ﬁrst glance, this seems like an odd thing to deﬁne. But it helps us keep track of the
zeros and poles of an element of Fq(χ) and this will be of utmost importance to us.
Assume that χ is a non-singular curve.
We deﬁne the divisor of f ∈Fq(χ) to be the divisor, denoted (f ), where we sum
the zeros of g intersect φ, counted with multiplicity, and subtract the zeros of h intersect
φ, counted with multiplicity. This deﬁnition of (f ) is well-deﬁned in that it does not
depend on the representatives g and h for f = g/h.

7
114
Chapter 7 • Alternant and Algebraic Geometric Codes
The degree of a divisor D is
deg(D) =

P∈χ
nP .
Bezout’s theorem, as mentioned before, ensures that the degree of the divisor (f ), for
any f ∈Fq(χ), is zero.
Example 7.12
Let us calculate the divisor of f , from Example 7.11, using the representative Z/X. The
curve deﬁned by the equation Z = 0 and the curve deﬁned by X3 = Y 2Z intersect in the
point P2 = (0 : 1 : 0) with multiplicity three. The curve deﬁned by X = 0 and the curve
deﬁned by X3 = Y 2Z intersect in the point P2 = (0 : 1 : 0) and the point P3 = (0 : 0 : 1)
with multiplicity two. Hence,
(f ) = 3P2 −2P3 −P2 = 2P2 −2P3.
If we had used the representative X2/Y 2, we would have arrived at the same conclusion. ■
Let
D =

P∈χ
nP P.
We write D ⩾0 if and only if the coefﬁcients nP ⩾0 for all P ∈χ.
Observe that the coefﬁcients of (f ) are positive and negative unless f is represented
by a polynomial of degree zero. Hence, (f ) ⩾0 if and only if f = c, for some c ∈Fq.
If P is a point of χ whose coordinates are all in Fq, then we say that P is an Fq-
rational point of χ.
Suppose that D is a divisor in which the sum is restricted to Fq-rational points. It is
straightforward to verify that the subset
L(D) = {f ∈Fq(χ) | (f ) + D ⩾0}
is a vector space over Fq. Moreover, its dimension can be calculated from the Riemann–
Roch theorem. Precisely, if deg(D) ⩾2g −1, then
dim L(D) = deg(D) −g + 1,
where g is the genus of the curve χ. Recall that for a non-singular plane curve the genus
of χ is (m −1)(m −2)/2, where m is the degree of φ.
Observe that if deg D < 0, then L(D) = {0}.

7.4 · Algebraic Geometric Codes
115
7
We are now in a position to prove that the code whose codewords are the evaluation
of the functions in L(D) at certain points of the curve χ, will be a linear code over Fq,
whose dimension we know and whose minimum distance we can bound from below. In
other words, we have a prescribed minimum distance as we did for BCH codes.
Theorem 7.13
Suppose
D =

P ∈χ
nP P,
where the sum is over the Fq-rational points of χ.
Let {P1, . . . , Pn} be a set of n Fq-rational points of χ for which nPj = 0, for all
j ∈{1, . . . , n}.
If n > deg(D) ⩾2g −1, then
C(χ, D) = {(f (P1), . . . , f (Pn)) | f ∈L(D)}
is a
[n, deg(D) −g + 1, ⩾n −deg(D)]q
code.
Proof
Let α : L(D) →Fn
q be deﬁned by
α(f ) = (f (P1), . . . , f (Pn)).
Since L(D) is a vector space, this deﬁnes a linear map.
Let E = P1 +· · ·+Pn. If f ∈ker α, then f ∈L(D −E), since f ∈L(D) and f is zero
at P1, . . . , Pn. Since deg(D −E) < 0, this implies f = 0. Therefore, the image of α has
dimension dim L(D), which is deg(D) −g + 1, by the Riemann–Roch theorem mentioned
above.
Suppose α(f ) has weight w > 0. Then, after a suitable reordering of the points, we can
assume
f (P1) = · · · = f (Pn−w) = 0.
Since f ̸= 0 and f ∈L(D −P1 −· · · −Pn−w), this implies that
deg(D −P1 −· · · −Pn−w) ⩾0,

7
116
Chapter 7 • Alternant and Algebraic Geometric Codes
which gives deg(D) ⩾n −w. Therefore, the minimum weight of a non-zero codeword of C
is at least n −deg(D) which, by Lemma 4.1, implies that the minimum distance of the linear
code C(χ, D) is at least n −deg(D).
⊓⊔
Example 7.14
Let χ be the curve of genus 1 deﬁned as the zeros of the polynomial
X3 + Y 2Z + Z2Y ∈F4[X, Y, Z].
The line X = 0 intersects χ in the points Q = (0 : 1 : 0), P = (0 : 0 : 1) and R = (0 : 1 : 1).
The line Y = 0 intersects χ in the point P with multiplicity 3 and the line Z = 0
intersects χ in the point Q with multiplicity 3.
Suppose D = 3Q. Then, since
(X
Z ) = P + R −2Q
and
( Y
Z ) = 3P −3Q
and
dim L(D) = deg(D) −g + 1 = 3,
we have that
{X
Z , Y
Z , 1}
is a basis for L(D). A generator matrix for C(χ, 3Q) is given by
G =
⎛
⎜⎝
0 0 1 1 e e e2 e2
0 1 e e2 e e2 e e2
1 1 1 1 1 1 1 1
⎞
⎟⎠,
where e ∈F4 and e2 = e + 1. In this case we get a generator matrix whose columns are the
eight F4-rational points of χ not equal to Q.
By Theorem 7.13, the minimum distance of C(χ, 3Q) is at least 5. The codeword
(1, 1, 1)G has weight 5, so C(χ, 3Q) is an [8, 3, 5]4 code.
Now, suppose D = 5Q. By Theorem 7.13 the dimension of C(χ, 5Q) is 5. To ﬁnd a
generator matrix for C(χ, 5Q), we can extend the basis for L(3Q) to L(5Q) by observing
that

7.5 · Algebraic Geometric Codes Surpassing the Gilbert–Varshamov Bound
117
7
(X2
Z2 ) = 2P + 2R −4Q
and
(XY
Z2 ) = 4P + R −5Q.
Therefore,
{X
Z , Y
Z , X2
Z2 , XY
Z2 , 1}
is a basis for L(5Q).
Thus, C(χ, 5Q) has a generator matrix
G′ =
⎛
⎜⎜⎜⎜⎜⎜⎝
0 0 1 1 e e e2 e2
0 1 e e2 e e2 e e2
0 0 1 1 e2 e2 e e
0 0 e e2 e2 1 1 e
1 1 1 1 1 1 1 1
⎞
⎟⎟⎟⎟⎟⎟⎠
.
According to Theorem 7.13, the minimum distance of C(χ, 5Q) is at least n−deg(5Q) = 3.
The codeword (1, 1, 1, 1, 0)G′ has weight 3, so C(χ, 5Q) has minimum distance equal to 3.
Therefore, C(χ, 5Q) is an [8, 5, 3]4 code.
■
7.5
Algebraic Geometric Codes Surpassing the
Gilbert–Varshamov Bound
Algebraic geometric codes are of particular interest because they can provide examples
of asymptotically good codes which better the Gilbert–Varshamov bound for certain
large enough alphabets. We ﬁrst calculate the asymptotic Gilbert–Varshamov bound for
a general alphabet of size r, as we did in Corollary 3.8 for binary codes.
Supposing that the codes have rate R and relative minimum distance δ, the bound in
Theorem 3.7 gives
 n
δn

(r −1)δnrnR > rn,
which by Lemma 1.11 gives
2h(δ)n(r −1)δnrnR > rn.

7
118
Chapter 7 • Alternant and Algebraic Geometric Codes
Taking logarithms and dividing by n, we have that the r-ary asymptotic Gilbert–
Varshamov bound is
R > 1 −h(δ) logr 2 −δ logr(r −1).
Theorem 7.13 implies that a k-dimensional algebraic geometric code of a curve of
genus g satisﬁes
k + d ⩾n −g + 1,
where d is the minimum distance and n is the length, which is maximised when we take
as many Fq-rational points of χ as possible. Dividing by n, this gives the bound
R + δ ⩾1 −g/n + 1/n.
Therefore, to ﬁnd asymptotically good codes, we need a sequence of curves Ci (i ∈N)
of genus gi, for which gi →∞, and where gi/ni tends to a number smaller than 1.
Here, ni is the number of Fq-rational points of Ci. One way to construct such curves
is with recursive towers. These curves are constructed from an absolutely irreducible
polynomial f ∈Fq[X, Y]. The afﬁne points of Ci are deﬁned in the following way. The
curve C1 is deﬁned by f (X1, X2) = 0. The second curve C2 is deﬁned by
f (X1, X2) = f (X2, X3) = 0,
the third curve C3 is deﬁned by
f (X1, X2) = f (X2, X3) = f (X3, X4) = 0,
and so on. Observe that the curve Ci is a set of points in PG(i + 1, q), although we have
only described the afﬁne part of the curve. The points on the hyperplane at inﬁnity are
obtained by homogenising the polynomials and setting the new variable to zero.
If q is an even power of an odd prime, then the sequence of curves constructed in
this way from
f (X, Y) = (Y
√q + Y)(1 + X
√q−1) −X
√q
is a sequence of asymptotically good codes of rate R and relative minimum distance δ
for which
R + δ ⩾1 −(√q −1)−1.
For q ⩾49, this line surpasses the Gilbert–Varshamov bound, for some range of values
of δ, see ⊡Figure 7.1. This example is due to Tsfasman, Vl˘adut and Zink.

7.7 · Exercises
119
7
⊡Fig. 7.1 The asymptotic
Gilbert–Varshamov curve and
the Tsfasman–Vl˘adut–Zink
bound for q = 49.
Gilbert−Varshamov
Tsfasman−Vlădut−Zink
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
7.6
Comments
Alternant codes were introduced by Helgert [36] in 1974, Goppa having previously
considered a subclass of alternant codes [28]. It was Goppa’s later work from [29]
which led to algebraic geometric codes. The book of Tsfasman and Vl˘adut on algebraic
geometric codes [72] was published in 1991 and contains a wealth of results on such
codes. The particular bound in ⊡Figure 7.1 is from [73] and a survey of the asymptotic
bounds can be found in Tsfasman [71].
7.7
Exercises
7.1 Suppose that we have received the vector (−1, 1, 1, 1, 0, 0, 1, 1, 1), having sent a
codeword of the subﬁeld subcode in Example 7.2. Use syndrome decoding to ﬁnd and correct
the error bit.
7.2 Let M be an invertible matrix and let H be as in Exercise 7.6. Let C be the linear code
over Fqh with check matrix H and let C′ be the linear code over Fqh with check matrix MH.
Prove that A(C) = A(C′).
7.3 Let φ(X) be a polynomial in Fqh[X] of degree r and let {α1, . . . , αn} be the set of
elements of Fqh which are not zeros of φ(X). For each u = (u1, . . . , un) ∈Fqh, let

7
120
Chapter 7 • Alternant and Algebraic Geometric Codes
fu(X) =
n

i=1
ui
φ(αi) −φ(X)
X −αi

φ(αi)−1.
Prove that the subﬁeld subcode A(C), where
C = {(u1, . . . , un) ∈Fqh | fu(X) = 0},
is an alternant code.
7.4 Let C be the linear code over F8 with check matrix
H =

1 1 1 1 1 1 1 1
e e2 e3 e4 e5 e6 e7 0

,
where e is a primitive element of F8.
Prove that the binary alternant code A(C) is a [8, 4, 4]2 code.
7.5 Let e be a primitive element of F8, such that e3 = e + 1 and let C be the GRS code
{(e4f (0), f (1), e6f (e), e4f (e2), f (e3), e6f (e4), e4f (e5), f (e6)) | f ∈F8[X],
deg f ⩽4}.
Prove that
A(C) = {(0, 0, 0, 0, 0, 0, 0, 0), (1, 1, 1, 1, 0, 1, 0, 1)}.
7.6
i.
Prove that the GRS code in Lemma 7.4 has a check matrix
H =
⎛
⎜⎜⎜⎜⎜⎜⎝
1
1
. . .
1
α1
α2
. . .
αn
.
.
. . .
.
.
.
. . .
.
αn−k′−1
1
αn−k′−1
2
. . . αn−k′−1
n
⎞
⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
v−1
1
0
. . . . . .
0
0
v−1
2
0 . . .
...
0
0
... 0
0
...
. . .
0 ...
0
0
. . . . . . 0 v−1
n
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
ii.
Prove that the dual of a GRS code is a GRS code.
7.7 Let C be the GRS code deﬁned over F4 = {0, 1, e, e2} by
C = {(f (0), f (1), e−1f (e), e−2f (e2)) | f ∈F4[X], deg f ⩽1}.

7.7 · Exercises
121
7
Suppose that we have received the vector y = (1, 1, 1, 1). Use the decoding algorithm in
Theorem 7.8 to ﬁnd g and h, polynomials of degree 2 and 1, respectively. Verify that h divides
g, deduce f and correct the error in y.
7.8 Prove that, in the decoding algorithm of Theorem 7.8, if n + k′ is odd, then
deg g ⩽⌈1
2(n + k′)⌉−2.
7.9 Let C be the GRS code deﬁned over F9 by
C = {(f (0), f (1), ef (e), f (e2), e2f (e3)) | f ∈F9[X], deg f ⩽1},
where e2 = e + 1.
Suppose that we have received the vector y = (1, 2, 1 + 2e, 2 + e, 0).
Use the decoding algorithm in Theorem 7.8 to ﬁnd g and h, polynomials of degree at
most 3 and 1, respectively. Verify, as claimed in Exercise 7.8, that the degree of g is 2, that h
divides g, deduce f and correct the error in y.
7.10 Let χ be the curve deﬁned as the zeros of the polynomial
X4 + Y 3Z + Z3Y.
Let P1 = (1 : 0 : 0), P2 = (0 : 1 : 0) and P3 = (0 : 0 : 1).
i.
Calculate the divisor (Y/Z).
ii.
Find a basis for L(D), where D = 3P1 + 3P2.
iii.
The curve χ has 28 rational points over F9. Construct a [24, 4, 18]9 code.
iv.
Verify that the Griesmer bound for a [24, 4, d]9 code gives d ⩽19.

123
8
Low Density Parity Check Codes
A linear code with a check matrix in which each column has few non-zero entries is
called a low density parity check code or, for brevity, an LDPC code. These codes were
introduced in the 1960s by Gallager who proved that probabilistic constructions of such
matrices produce asymptotically good linear codes. Moreover, he observed that LDPC
codes perform well when applying the following decoding algorithm. On receiving a
vector v, one calculates the weight of the syndrome of v +e, for each vector e of weight
one. If the weight of this syndrome is less than the weight of the syndrome of v, for
some e, then we replace v by v + e and repeat the process. If at each iteration there
is such a vector e, then, since after replacing v by v + e, the weight of the syndrome
of v decreases, we will eventually ﬁnd a vector whose syndrome is zero, which must
be the syndrome of some codeword u. We then decode v as u. If at some iteration no
such e exists then the decoding breaks down. If at some iteration more than one such
vector e exists, then one could choose e so that the weight of the syndrome of v + e is
minimised. In this chapter we will prove that there are LDPC codes, constructed from
graphs with the expander property, for which the decoding algorithm will not break
down. Provided that the number of error bits is less than half the minimum distance,
the decoding algorithm will return the nearest codeword to the received vector. We will
use probabilistic arguments to construct the graphs, and from these a sequence of codes
which are asymptotically good.
8.1
Bipartite Graphs with the Expander Property
A graph is a pair (V, E), where V is a set and E is a set of 2-subsets of V . We consider
the elements of V to be vertices and the set E to be a set of edges, where an edge {v1, v2}
joins the two vertices v1 and v2. A bipartite graph is a graph in which V is the disjoint
union V1 ∪V2 and for all e = {v1, v2} ∈E, we have v1 ∈V1 and v2 ∈V2. In other
words, there are no edges joining two vertices in V1 and no edges joining two vertices in
V2. The subsets V1 and V2 are called the stable sets of the bipartite graph. The degree
of a vertex v is the number of edges which contain v and we say that u is a neighbour
of v if {u, v} is an edge. A bipartite graph with stable sets V1 and V2 is left γ -regular if
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_8

8
124
Chapter 8 • Low Density Parity Check Codes
every vertex in V1 has degree γ , i.e. has γ neighbours in V2. For a subset S of V1, denote
by N(S) the set of vertices of V2 which are neighbour to some vertex in S.
Let δ be a real number in the interval (0, 1). A left γ -regular bipartite graph with
stable sets V1 and V2 of size n and m, respectively, has the expander property with
respect to δ if for all subsets S of V1 of size less than δn, |N(S)| > 3
4γ |S|.
Example 8.1
⊡Figure 8.1 is a left 3-regular bipartite graph with the expander property with respect to
δ = 1
4. The 12 vertices of V1 are the vertices on the outer circle and the 9 vertices of V2 are
the vertices on the inner circle. One can verify that every subset S of V1 of size 1 has three
neighbours, so the graph is left 3-regular. If S is a 2-subset of V1, then N(S) has either 5 or
6 vertices. Therefore, for every subset S of V1 of size less than δn = 3, |N(S)| > 9
4|S|.
■
Lemma 8.2 Given γ > 4 and R ∈(0, 1), there is a constant δ ∈(0, 1), dependent on γ and
R, for which a left γ -regular bipartite graph with the expander property with respect to δ
exists, for all n large enough, where the left and right stable sets have size n and ⌊(1 −R)n⌋,
respectively.
Proof
Consider the set  of bipartite left γ -regular graphs with stable sets V1 and V2 of size n and
m = ⌊(1 −R)n⌋.
⊡Fig. 8.1 A left 3-regular bipartite graph with the expander property.

8.1 · Bipartite Graphs with the Expander Property
125
8
For a graph  ∈, a subset S of V1 of size s < δn and a subset T of V2 of size ⌊3
4γ s⌋,
deﬁne a random variable XS,T which takes the value 1 if all edges of  with an end-vertex
in S have an end-vertex in T and 0 otherwise. If we can prove that the probability
P(

S,T
XS,T = 0) ̸= 0,
then we can deduce that there is a graph  ∈ for which XS,T = 0 for all subsets S and T .
This implies that the graph  has the property that the union of the neighbours of the vertices
in S has more than 3
4γ |S| vertices, for all subsets S of V1 of size less than δn. Hence,  has
the expander property with respect to δ.
Since the probability that a randomly chosen edge has an end-vertex in T is ⌊3
4γ s⌋/m,

S,T
P(XS,T = 1) <
⌊δn⌋

s=1
n
s

m
⌊3
4sγ ⌋
 
⌊3
4γ s⌋
m
γ s
,
where the sum is over all subsets S of V1 of size s < δn and all subsets T of V2 of size
⌊3
4γ s⌋.
Since
ek =
∞

j=0
kj
j! > kk
k! ,
we have
n
k

⩽nk
k! < (ne
k )k,
which in the above gives

S,T
P(XS,T = 1) <
δn

s=1
en
s
s

e⌊(1 −R)n⌋
⌊3
4γ s⌋
 3
4 sγ 
⌊3
4γ s⌋
⌊(1 −R)n⌋
γ s
.
Since
s( 1
4 γ −1)s < (δn)( 1
4 γ −1)s,
this gives

S,T
P(XS,T = 1) <
δn

s=1
(Nδ
1
4 γ −1)s,
for some constant N, dependent on γ and R, but not dependent on n.
Now, if we choose δ so that Nδ
1
4 γ −1 < 1
2, then this sum is less than 1.

8
126
Chapter 8 • Low Density Parity Check Codes
Since,
P(

S,T
XS,T ̸= 0) ⩽

S,T
P (XS,T = 1) < 1,
we have that
P(

S,T
XS,T = 0) ̸= 0.
⊓⊔
8.2
Low Density Parity Check Codes
A low density parity check code is a linear code which has a check matrix H with
the property that H has few non-zero elements in each column. We will only consider
low density parity check binary codes, so H will have the property that it has few 1’s
in each column. We make this vague deﬁnition precise for sequences of codes of length
n, where n tends to inﬁnity, by insisting that the few non-zero elements is a constant
number.
Lemma 8.2 proves that sequences of bipartite expander graphs exist for n large
enough. We now prove that we can construct a matrix from a bipartite graph in this
sequence, which is a check matrix of a code in a sequence of asymptotically good linear
codes.
Fix R to be the rate of transmission we would like to achieve and δ to be the relative
minimum distance.
Lemma 8.3 Given a left γ -regular bipartite graph  with stable sets of size n and m =
⌊(1 −R)n⌋and the expander property with respect to δ, there exists a binary linear code
C() with rate at least R and relative minimum distance at least δ.
Proof
Let H be the m × n matrix whose rows are indexed by the vertices of V2 and whose columns
are indexed by the vertices of V1. A row-column entry is 1 if there is an edge joining the
vertex of V1 to the vertex of V2 and zero otherwise. Then H has γ 1’s in each column.
Let C() be the binary linear code deﬁned by the check matrix H, i.e.
C() = {u ∈Fn
2 | uHt = 0}.
Since the rank of H is at most the number of rows, the dimension of C() is at least
n −m = n −⌊(1 −R)n⌋⩾Rn, so C() will have rate at least R.
Suppose that C() has minimum distance less than δn. By Lemma 4.1, there is a non-
zero vector u of C() of weight less than δn. Let S be the support of u, the set of coordinates
where u has a 1. These coordinates correspond to vertices of V1, since the rows of Ht are

8.2 · Low Density Parity Check Codes
127
8
indexed by the vertices of V1, so we can think of S as a subset of V1. Since |S| < δn and
 has the expander property with respect to δ, the size of the set N(S), the set of vertices
neighbour to some vertex of S, is at least 3
4γ |S|.
If every vertex of N(S) has at least two edges joining it to vertices of S, then, counting
edges with an end-vertex in S,
|S|γ ⩾2|N(S)|,
(8.1)
which contradicts |N(S)| ⩾3
4|S|γ .
Therefore, there is some vertex v in N(S) which is joined to just one vertex of S. A vertex
of V2 indexes a row r of the check matrix H. Since r is a row of the check matrix, it has the
property that r · w = 0, for all codewords w ∈C().
However, v is joined to just one vertex of the support S of u, so r · u = 1, which is
a contradiction, since the scalar product of a row of the check matrix H and a codeword is
zero. Hence, each non-zero vector in C() has weight at least δn. By Lemma 4.1, C() has
minimum distance at least δn.
⊓⊔
Observe that we could relax the expander property to |N(S)| >
1
2γ |S| and
Lemma 8.3 would still hold. However, we insist upon |N(S)| >
3
4γ |S|, so that the
decoding algorithm, which we will see in the following section, works.
Example 8.4
The check matrix obtained from the bipartite graph  in Example 8.1 is
H =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 0 0 0 1 0 1 0 0 1 0 0
0 1 0 0 0 1 1 0 0 0 0 1
0 0 1 1 0 0 1 0 0 0 1 0
1 0 0 0 0 1 0 1 0 0 1 0
0 1 0 1 0 0 0 1 0 1 0 0
0 0 1 0 1 0 0 1 0 0 0 1
1 0 0 1 0 0 0 0 1 0 0 1
0 1 0 0 1 0 0 0 1 0 1 0
0 0 1 0 0 1 0 0 1 1 0 0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Lemma 8.3 implies that the rate of C() is at least 1
4 and the minimum distance is at least 3.
It is Exercise 8.1 to verify that H has rank 9 and so the dimension of C() is 3 and the rate
of C() is precisely 1
4.
Consider the graph in Example 8.1. We can deﬁne a geometry which has as points the
vertices of V2 and as lines the vertices of V1, where for each vertex u of V1, we have a line of
the geometry consisting of the points which are neighbours to u in the graph. This geometry
is AG(2, 3), the afﬁne plane of order 3, see ▷Section 2.4, and in particular ⊡Figure 2.2.
Each block of three columns corresponds to a parallel set of lines, so C() has a generator
matrix

8
128
Chapter 8 • Low Density Parity Check Codes
G =
⎛
⎜⎝
1 1 1 1 1 1 0 0 0 0 0 0
0 0 0 0 0 0 1 1 1 1 1 1
1 1 1 0 0 0 1 1 1 0 0 0
⎞
⎟⎠.
Thus we deduce that all codewords of C() have weight 0, 6 or 12, so C() is a [12, 3, 6]2
code.
The Griesmer bound from Theorem 4.18 for a [12, 3, d]2 code gives the bound d ⩽6.
■
8.3
Decoding LDPC Codes
Lemma 8.2 and Lemma 8.3 imply that we can ﬁnd asymptotically good codes using
bipartite graphs with the expander property. Moreover, these codes are linear, so fast
to encode. However, the really useful property that these codes have is that they are
also fast to decode. We will prove this in Theorem 8.7, but ﬁrst we need to prove the
following lemma.
Recall that for x ∈Fn
2, the syndrome of x is
s(x) = xH t
and wt(x) is the number of non-zero coordinates that x has.
Let ei denote the vector of weight one and length n with a 1 in the i-th coordinate.
Lemma 8.5 Suppose that x ∈Fn
2 and that d(x, u) < δn for some u ∈C(), where C()
is the binary linear code obtained from a bipartite graph  which has the expander property
with respect to δ. Then there is an i ∈{1, . . . , n} such that wt(s(x + ei)) < wt(s(x)).
Proof
Let S be the coordinates where x and u differ. By assumption, |S| < δn. As in the proof of
Lemma 8.3, the set S corresponds to a subset of the vertices V1 of the graph . As before, let
N(S) denote the set of vertices which are neighbour to some vertex of S. Then, as in the proof
of Lemma 8.3, the vertices of N(S) index rows of the matrix H which in turn correspond to
linear constraints on the code C(). Divide N(S) into T , constraints that x satisﬁes and U,
constraints that x does not satisfy. In other words, T is the subset of N(S) where s(x) has a
zero and U is the subset of N(S) where s(x) has a 1. Here, we are identifying the coordinates
of s(x) with vertices of V2.
Since |S| < δn, the expander property implies
|T | + |U| = |N(S)| > 3
4γ |S|.
Counting edges between S and N(S), we have
|U| + 2|T | ⩽γ |S|,

8.3 · Decoding LDPC Codes
129
8
since there must be at least two edges joining T to vertices in S, otherwise the constraint
would not be satisﬁed.
Combining the inequalities this implies |T | < 1
4γ |S| and therefore
|U| > 1
2γ |S|.
(8.2)
This implies that
wt(s(x)) > 1
2γ |S|.
Since,
s(x) = s(x + u) =

i∈S
s(ei),
the pigeon-hole principle implies that there is an i for which s(ei) and s(x) both have a 1 in
more than 1
2γ of the coordinates. Since wt(s(ei)) = γ , this implies that
wt(s(x + ei)) < wt(s(x)).
⊓⊔
Example 8.6
Suppose that we have sent a codeword of C(), deﬁned by the check matrix in Example 8.4,
and that we have received
x = (0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0).
By calculating xHt,
s(x) = (0, 0, 0, 1, 1, 1, 1, 1, 1).
The weights of s(x + ei) are given in the following table:
i
1
2
3
4
5
6
7
8
9
10
11
12
wt(s(x + ei))
5
5
5
5
5
5
9
3
3
5
5
5.
Assuming that less than two errors have occurred in transmission, Lemma 8.5 implies
that there is an i for which wt(s(x + ei)) < 6 and this is the case for all i ̸= 7.
We will return to this example in Example 8.8.
■
Lemma 8.5 allows us to apply the decoding algorithm described in Theorem 8.7.
This type of decoding algorithm is called belief propagation. Observe that in
Lemma 8.5 we are not claiming that by summing ei to x we are correcting an error,

8
130
Chapter 8 • Low Density Parity Check Codes
only that the weight of the syndrome decreases. It may be that in the algorithm in
Theorem 8.7, we introduce new errors at a particular iteration. However, since the
weight of the syndrome is decreasing, it will eventually have weight zero and all errors,
even those which we may have inadvertently introduced, will have been corrected.
Theorem 8.7
Let C() be the linear code of length n obtained from a bipartite graph  with the
expander property with respect to δ. There is a decoding algorithm for C() which
completes in a number of steps which is polynomial in n and which corrects up to 1
2δn
error bits.
Proof
We will provide an algorithm for decoding the received vector x ∈Fn
2, where d(x, u) < 1
2δn,
for some u ∈C.
Let S be the support of x −u, i.e. the coordinates where x and u differ.
Although we do not know what S is, by assumption |S| < 1
2δn.
By Lemma 8.5, if we test x + ei for i = 1, . . . , n, we will ﬁnd an i for which
wt(s(x + ei)) < wt(s(x)).
So, we can repeat this process with x+ei and |U|, the size of the set of unsatisﬁed constraints,
will decrease.
The maximum value of |U| (at the ﬁrst step since |U| is decreasing) is bounded by
|U| ⩽|N(S)| ⩽γ |S| < 1
2γ δn.
At each iteration, when we apply Lemma 8.5, equation (8.2) implies |S| < δn. Therefore,
the hypothesis of Lemma 8.5 is satisﬁed and can be applied at the next iteration.
The weight of s(x) in the ﬁrst iteration is less than, 1
2γ δn. In each of the following
iterations the weight of the syndrome decreases, so the number of iterations will be at most
1
2γ δn. In each iteration we have to multiply a matrix with n vectors, so the whole algorithm
completes in a number of steps which is polynomial in n.
⊓⊔
Example 8.8
Theorem 8.7 only guarantees that we can correct up to 1
2δn errors. In Example 8.4, this
implies that we can correct up to only one error bit even though the minimum distance is 6
and we would expect to be able to correct up to two error bits.
If we apply the algorithm described in Theorem 8.7 to the table of syndromes we
calculated in Example 8.6, then we have many choices for i in the ﬁrst iteration. Let us
suppose we decide to choose an i for which the weight of s(x + ei) is minimised at each
iteration. Then we would replace x by x + e8 at the ﬁrst iteration, then x + e8 by x + e8 + e9
at the second iteration and would have correctly found the codeword
(0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0)
at distance 2 to x.
■

8.5 · Exercises
131
8
8.4
Comments
This chapter has realised an important aim that of proving that there are asymptotically
good codes which we can encode and decode in an efﬁcient manner. There are
LDPC codes whose rate nears the Shannon capacity, see MacKay and Neal
[49].
For an excellent survey on expanders, see Hoory, Linial and Wigderson [39]. The
decoding algorithm for expander codes is due to Sipser and Spielman [67]. Regarding
Exercise 8.5, a list of the known hyperovals and their collineation groups can be found
in Penttila and Pinneri’s article [56]. For more on LDPC codes constructed from ﬁnite
geometries, see Kou, Lin and Fossorie [45] and Pepe [57].
LDPC codes have replaced turbo codes in 5G mobile networks. Turbo codes are not
covered here in this text, principally because they lack any algebraic structure. LDPC
codes are widely used in mobile and satellite communication and have been employed
by NASA in recent missions as an alternative to Reed–Solomon codes.
8.5
Exercises
8.1
Prove that there is no subset S of points of AG(2, 3) with the property that every line is
incident with an even number of points of S. Conclude that the check matrix from Example 8.4
has rank 9 over F2.
8.2 Let  be the bipartite graph whose vertices V1 are the lines of PG(3, 4) and whose
vertices V2 are the points of PG(3, 4) and where a point is joined to a line by an edge in  if
and only if the point and line are incident in the geometry.
i.
Apply Lemma 8.3 to prove that C() is a linear code of rate at least 272
357 with relative
minimum distance at least
4
357.
ii.
Prove that the minimum distance of C() is at least 9.
iii.
Determine up to how many error bits we can guarantee to correct when applying belief
propagation decoding.
8.3 Prove that the point-line incidence matrix of AG(2, 4) is the adjacency matrix of a left
4-regular graph  with the expander property with respect to δ = 1
5, with stable sets of size
20 and 16.
8.4 Let q be odd.
i.
Prove that there is no set S of points of AG(2, q) with the property that every line is
incident with an even number of points of S.
ii.
Let H be the matrix whose rows are indexed by the points of AG(2, q) and whose columns
are indexed by the lines of AG(2, q) and where an entry in the matrix is 1 if the point is
incident with the line and 0 otherwise.
Let C be the binary linear code with check matrix H. Prove that the dimension of C
is at least q and that the minimum distance of C is at least q + 2.

8
132
Chapter 8 • Low Density Parity Check Codes
A dual hyperoval in AG(2, q) or PG(2, q) is a subset L of q + 2 lines with the
property that every point is incident with either zero or two lines of L.
8.5
i.
Prove that if AG(2, q) has a dual hyperoval, then q is even.
ii.
Suppose that u is a codeword of weight 6 of the binary linear code C(), where  is the
expander graph in Exercise 8.3. Prove that u is the indicator vector of a dual hyperoval.
In other words, if S is the support of u, then, as a set of lines of AG(2, 4), S is a dual
hyperoval.
iii.
Deduce the minimum distance of C() by proving that there are no small sets of lines L
with the property that every point of AG(2, 4) is incident with an even number of lines
of L.
iv.
By calculating the rank of a check matrix for C() (with the aid of a computer),
determine the dimension of C().

133
9
Reed–Muller and Kerdock Codes
In ▷Chapter 6, we studied Reed–Solomon codes, codes whose codewords are the
evaluation of polynomials in one variable of degree at most k −1 at the elements of
Fq ∪{∞}. Reed–Solomon codes are short length codes, where the length n is bounded
by q + 1, and only useful when we take the ﬁeld to be large. The alternant codes which
we constructed from generalised Reed–Solomon codes in ▷Chapter 7 allowed us to
construct codes over small ﬁelds and we put this to good use. In this chapter we will
consider another generalisation of Reed–Solomon codes, codes whose codewords are
the evaluation of polynomials in many variables. This again allows us to construct
linear codes over small ﬁelds and we will restrict our attention, for the most part,
to binary linear codes. It will turn out that these codes are not asymptotically good.
Nevertheless, they are an important class of codes which are widely implemented due to
the availability of fast decoding algorithms. One example of such a decoding algorithm
is the majority-logic decoding algorithm that we will study here. We will then go on and
construct Kerdock codes which are certain subcodes of the second-order Reed–Muller
codes. These codes can give examples of non-linear codes with parameters for which no
linear code exists.
9.1
Binary Reed–Muller Codes
A Boolean function from Fm
2 to F2 is the evaluation map of a polynomial with
coefﬁcients from F2 in m variables generated by monomials in which the degree of
any particular indeterminate is at most 1.
Note that for both elements x of F2, x2 = x, so the function deﬁned by the evaluation
of the polynomial x2
1x3
2x3 at the elements of F3
2 and the polynomial x1x2x3 will be the
same. Therefore, it makes sense that when considering evaluations of polynomials in
many variables over F2, we restrict our attention to Boolean functions.
The r-th order Reed–Muller code is a binary code R(r, m) of length 2m deﬁned by
R(r, m) = {(f (a1), . . . , f (a2m)) | deg f ⩽r},
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_9

9
134
Chapter 9 • Reed–Muller and Kerdock Codes
where {a1, . . . , a2m} is the set of vectors of Fm
2 and f runs through all Boolean functions
that are deﬁned by polynomials in m indeterminates of degree at most r.
The code R(r, m) is a linear code over F2, since
(f (a1), . . . , f (a2m)) + (g(a1), . . . , g(a2m)) = ((f + g)(a1), . . . , (f + g)(a2m)).
The vector space of Boolean functions of degree at most r in m variables has a canonical
basis, which is the set of monomials of degree at most r in m variables and degree at
most one in any particular variable. Therefore, the code R(r, m) has a generator matrix
whose rows are indexed by these monomials. For example, the set of monomials
{1, x1, . . . , xm, x1x2, . . . , xm−1xm}
is a basis for the vector space of Boolean functions in m variables of degree at most 2.
Example 9.1
The 11 × 16 matrix
G =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1
0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1
0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1
0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1
0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1
0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
1
x1
x2
x3
x4
x1x2
x1x3
x1x4
x2x3
x2x4
x3x4
is a generator matrix of the code R(2, 4), with the rows being indexed by the monomials in
four variables of degree at most two.
■
We have already proved the following lemma.
Lemma 9.2 R(r, m) is a linear code of length 2m and of dimension
1 +
m
1

+
m
2

+ . . . +
m
r

.
Since R(r, m) is linear, Lemma 4.1 implies that its minimum distance is equal to the
minimum weight of a non-zero codeword. In the above example, evidently R(2, 4) has
codewords of weight 4 and this is indeed its minimum distance. In Theorem 9.3 we will

9.2 · Decoding Reed–Muller Codes
135
9
calculate the minimum distance for binary Reed–Muller codes. Later, in Theorem 9.15,
we will calculate the minimum distance for non-binary Reed–Muller codes.
Theorem 9.3
The minimum distance of R(r, m) is 2m−r.
Proof
By induction on m. If m = r, then the evaluation of the polynomial X1 · · · Xr is a codeword
of weight one.
Suppose that the minimum distance of R(r, m) is 2m−r.
Order the vectors of Fm+1
2
so that the ﬁrst 2m vectors have xm+1 = 0.
A codeword (u, u + v) of R(r, m + 1) is the evaluation of a polynomial
f (X) + Xm+1g(X),
where f (X) is a polynomial of degree at most r in m variables and g(X) is a polynomial of
degree at most r −1 in m variables. Then u ∈R(r, m), since it is the evaluation of f (X) and
v ∈R(r −1, m), since it is the evaluation of g(X).
If u = 0, then the codeword is (0, v) and by induction has non-zero weight at least
2m−(r−1).
If u + v = 0, then u = v ∈R(r −1, m) and so the codeword (u, 0) has non-zero weight
at least 2m−(r−1).
If neither u nor u + v is zero, then (u, u + v) has weight at least 2 · 2m−r = 2m−r+1,
since both u and u + v are in R(r, m).
Thus, the minimum weight of a non-zero codeword of R(r, m + 1) is 2m−r+1. By
Lemma 4.1, the minimum weight of a non-zero codeword of a linear code is equal to its
minimum distance.
⊓⊔
9.2
Decoding Reed–Muller Codes
The popularity of Reed–Muller codes in real-world applications is due in part to the fact
that there are fast decoding algorithms, the most common of which is the focus of this
section. Before we consider this decoding algorithm, we ﬁrst prove a couple of lemmas
which prove some properties of Boolean functions.
For each non-empty subset J of {1, . . . , m}, let
fJ(X) =

j∈J
Xj
and deﬁne
f∅(X) = 1.

9
136
Chapter 9 • Reed–Muller and Kerdock Codes
Then
{fJ(X) | J ⊆{1, . . . , m}, |J| ⩽r}
is a basis for the space of polynomials in m variables of degree at most r whose
evaluations deﬁne Boolean functions.
We will exploit the following lemma repeatedly.
Lemma 9.4 Let J be a subset of {1, . . . , m}. Suppose
g(X) =

L⊆{1,...,m}
aLfL(X),
for some aL ∈F2, where the sum is over all subsets L of size at most m −|J|.
Then

x∈Fm
2
fJ (x)g(x) = a{1,...,m}\J .
Proof
Let K ⊆{1, . . . , m}.
If there is an i ∈{1, . . . , m} \ K, then

{x∈Fm
2 |xi=0}
fK(x) =

{x∈Fm
2 |xi̸=0}
fK(x).
This implies

x∈Fm
2
fK(x) = 0,
(9.1)
unless K = {1, . . . , m}.
Then

x∈Fm
2
fJ (x)g(x) =

L⊆{1,...,m}

x∈Fm
2
aLfJ (x)fL(x),
where the ﬁrst sum on the right-hand side is over all subsets L of size at most m −|J|.
This expression is equal to

L⊆{1,...,m}
aL

x∈Fm
2
fJ∪L(x) = a{1,...,m}\J ,
by (9.1).
⊓⊔

9.2 · Decoding Reed–Muller Codes
137
9
Theorem 9.5
The dual of the code R(r, m) is the Reed–Muller code R(m −r −1, m).
Proof
A codeword u of R(r, m) is the evaluation of a polynomial
g(X) =

K⊆{1,...,m}
aKfK(X),
for some aK ∈F2, where the sum is over all subsets of size at most r.
A codeword v of R(m −r −1, m) is the evaluation of
h(X) =

L⊆{1,...,m}
bLfL(X),
for some bL ∈F2, where the sum is over all subsets of size at most m −r −1.
The inner product of u and v is

x∈Fm
2
h(x)g(x) =

x∈Fm
2

K

L
aKbLfK(x)fL(x)
=

K

L
aKbL

x∈Fm
2
fK∪L(x) = 0,
by Lemma 9.4.
Therefore,
R(m −r −1, m) ⊆R(r, m)⊥.
By Theorem 9.3, the sum of the dimensions of R(r, m) and R(m −r −1, m) is 2m, which is
the length of the codes.
Hence,
dim R(m −r −1, m) = dim R(r, m)⊥.
⊓⊔
The following lemma is fundamental to the decoding algorithm.
Lemma 9.6 Let
g(X) =

K⊆{1,...,m}, |K|⩽r
bKfK(X),

9
138
Chapter 9 • Reed–Muller and Kerdock Codes
where bK ∈F2 and let J be a subset of {1, . . . , m} of size r.
For all 2m−r choices of ai ∈F2, i ∈{1, . . . , m} \ J,

x∈Fn
2
g(x)

i∈{1,...,m}\J
(xi + ai) = bJ .
Proof
When we expand the product in the sum, all terms have degree less than m except those
coming from
g(x)

i∈{1,...,m}\J
xi = g(x)f{1,...,m}\J (x).
The lemma follows from Lemma 9.4.
⊓⊔
We are now in a position to describe a decoding algorithm for Reed–Muller codes,
which is an example of a majority-logic decoding algorithm. Let v be the received
vector, whose coordinates vx are indexed by the vectors x ∈Fm
2 . For each subset J of
{1, . . . , m} of size r, we perform a test. We wish to determine whether uJ is zero or one,
where the sent codeword u is the evaluation of

J⊆{1,...,m}, |J|⩽r
uJfJ(X).
For all 2m−r choices of ai ∈F2, i ∈{1, . . . , m} \ J, we calculate

x∈Fm
2
vx

i∈{1,...,m}\J
(xi + ai).
If the result of this test is 1 in the majority of cases, then we conclude that uJ = 1 and
vice versa, if it is 0 in the majority of cases, then we conclude that uJ = 0. Once we have
completed this for all subsets J of {1, . . . , m} of size r, we subtract the evaluation of

K⊆{1,...,m}, |K|=r
uKfK(X),
from the received vector and continue with the subsets of size r −1 supposing that, if
we are correctly decoding, we now have a corrupted codeword of R(r −1, m).
All that remains to be shown, to prove that this decoding algorithm will correct up to
2m−r−1 −1 error bits, is to show that an error bit will only affect one of the tests. Before
we prove this in Lemma 9.8, we consider an example.
Example 9.7
Suppose that we have encoded using R(2, 4) and have received
v = (1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0),

9.2 · Decoding Reed–Muller Codes
139
9
where the vectors of F4
2 are ordered as in the matrix G in Example 9.1.
We calculate
w = vGt = (1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0).
The coordinates are indexed by subsets of {1, 2, 3, 4} of size at most 2, as in Example 9.1.
Indexing the coordinates explicitly
J
∅{1} {2} {3} {4} {12} {13} {14} {23} {24} {34}
wJ 1 0
1
1
0
1
1
0
0
1
0
where
wJ =

x∈Fm
2
vxfJ (x)
are the coordinates of w.
We start by determining uJ for the subsets J of size r = 2.
To determine u{12}, we make 2m−r = 4 tests by calculating

x∈Fm
2
vxx3x4,

x∈Fm
2
vx(x3x4 + x3),

x∈Fm
2
vx(x3x4 + x4)
and

x∈Fm
2
vx(x3x4 + x3 + x4 + 1),
which is
w{34}, w{34} + w{3}, w{34} + w{4} and w{34} + w{3} + w{4} + w∅,
respectively.
The results of these tests are 0, 1, 0, 0, respectively, so we decode u{12} as 0, since there
are a majority of zeros.
The following table lists the results of these tests for all subsets of size 2 and indicates
the majority decision.
u{12} 0, 1, 0, 0 →0 u{13} 1, 0, 1, 1 →1 u{14} 0, 1, 1, 1 →1
u{23} 0, 0, 0, 1 →0 u{24} 1, 1, 0, 1 →1 u{34} 1, 1, 0, 1 →1
Based on the results of those tests, we subtract
(0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1)G

9
140
Chapter 9 • Reed–Muller and Kerdock Codes
from v and get
v1 = v + (0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1)G = (1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0).
If we are decoding correctly, v1 should be a (possibly) corrupted codeword of R(1, 4). To
determine uJ , where J is a subset of size 1, we repeat the above.
We calculate
w1 = v1Gt
1,
where G1 is the generator matrix of R(3, m). This vector will have coordinates
w1
K =

x∈Fm
2
fK(x)v1
x,
where K is a subset of {1, 2, 3, 4} of size at most 3.
Indexing the coordinates explicitly as before
K ∅{1} {2} {3} {4} {12} {13} {14}
w1
K 1 0
1
1
0
0
0
0
K {23} {24} {34} {123} {124} {134} {234}
w1
K
1
0
0
0
0
1
0
allows us to perform 2m−(r−1) = 8 tests for each uJ .
To determine u{1}, we make 8 tests by calculating
w1
{234}, w1
{234} + w1
{23}, w1
{234} + w1
{24}, w1
{234} + w1
{34}, w1
{234} + w1
{23} + w1
{24} + w1
{2},
w1
{234} + w1
{23} + w1
{34} + w1
{3}, w1
{234} + w1
{24} + w1
{34} + w1
{4}
and
w1
{234} + w1
{23} + w1
{24} + w1
{34} + w1
{2} + w1
{3} + w1
{4} + w1
∅.
The results of these tests are
u{1} 0, 1, 0, 0, 0, 0, 0, 0 →0 u{2} 1, 1, 1, 1, 1, 0, 1, 1 →1
u{3} 0, 0, 0, 0, 0, 1, 0, 0 →0 u{4} 0, 0, 0, 1, 0, 0, 0, 0 →0
Based on the results of the tests, we subtract
(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0)G

9.2 · Decoding Reed–Muller Codes
141
9
from v1 and get
v2 = v + (0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1)G = (1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1).
Summing
(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)G
to v2 we have that
v + (1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1)G = (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0).
Therefore, we have determined that the error is in the 7-th bit, that the uncoded string
u = (1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1)
and that the sent codeword was uG.
■
To ﬁnish this section, we prove that an error bit affects exactly one of the tests when
testing a corrupted codeword of R(r, m). Since we perform 2m−r tests, this implies that
we can correct up to 2m−r−1 −1 error bits, or in other terms 1
2d −1 error bits, since the
minimum distance d of R(r, m) is 2m−r.
Lemma 9.8 Suppose that e is a vector of F2m
2
of weight one, whose coordinates are indexed
by the vectors of Fm
2 . Let J be a subset of {1, . . . , m}. For all but one of the choices of a,
whose coordinates ai ∈F2 for i ∈{1, . . . , m} \ J,

x∈Fm
2
ex

i∈{1,...,m}\J
(xi + ai) = 0,
where ex is the coordinate of e indexed by x.
Proof
Let y be the vector of Fm
2 indexing the coordinate where the vector e has a 1.
The vector e is the evaluation of
m

i=1
(Xi + yi + 1),
since it is zero unless Xi = yi for all i = 1, . . . , m.
Hence, for all x ∈Fm
2 ,
ex

i∈{1,...,m}\J
(xi + ai) =
m

i=1
(xi + yi + 1)

i∈{1,...,m}\J
(xi + ai),

9
142
Chapter 9 • Reed–Muller and Kerdock Codes
which will contain a factor x2
i + xi (and is therefore zero) unless ai = yi + 1 for all i ∈
{1, . . . , m} \ J.
⊓⊔
To decode using this majority-logic decoding algorithm we perform at most 2m tests
k times, where k is the dimension of the code. This is less than n2 tests, where n is
the length of the code. Each test involves summing less than n terms, so the decoding
algorithm is completed in a number of steps which is polynomial in the length of the
code. This should be compared to syndrome decoding from ▷Chapter 4, which involved
searching through a look-up table with a number of entries which is exponential in
n. For this reason Reed–Muller codes and the majority-logic decoding algorithm are
widely implemented. However, they do not give a sequence of asymptotically good
codes. Although the relative minimum distance is 2−r, which we can bound away from
zero by ﬁxing r, the transmission rate of R(r, m) is less than
r
n
log n
r

which tends to zero as n tends to inﬁnity.
9.3
Kerdock Codes
A codeword of R(2, m) \ R(1, m) is the evaluation of polynomials of the form
q(X) + ℓ(X) or q(X) + ℓ(X) + 1,
where ℓ(X) is a linear form in m variables and
q(X) =

1⩽i<j⩽m
aijXiXj
is a non-zero quadratic form.
If the quadratic form q(X) has maximum rank, then we will prove that, for all the
linear forms ℓ(X), these codewords will have large weight. Therefore, if we can ﬁnd a
set of quadratic forms whose differences are quadratic forms of maximum rank, then the
distance between any two codewords will be large. In this section we will develop and
formalise this idea.
Let A = (aij) be the symmetric matrix deﬁned by the symmetric bilinear form
b(X, Y) = q(X + Y) −q(X) −q(Y) =

1⩽i<j⩽m
aij(XiYj + XjYi) = XtAY.
The rank of the bilinear form b(X, Y) is deﬁned to be the rank of A.

9.3 · Kerdock Codes
143
9
Lemma 9.9 Suppose m is even. The evaluation of
m/2

i=1
X2i−1X2i
at the vectors of Fm
2 has 2m−1 + 2m/2−1 zeros.
Proof
There are 2m/2 zeros of the form (0, x2, 0, x4, . . . , 0, xm).
If x2i−1 ̸= 0 for some i = 1, . . . , m/2, then one of the x2i is determined by
m/2

i=1
x2i−1X2i = 0,
which gives 2m/2−1(2m/2 −1) zeros of this form, 2m/2−1 zeros for each non-zero vector
(x1, x3, . . . , xm−1).
Hence, there are precisely 2m−1 + 2m/2−1 zeros when evaluated at the vectors of Fm
2 . ⊓⊔
We are going to construct codes whose codewords are the evaluation of the sum of
a quadratic form and a linear form. For this reason, we want to know the weights of the
vectors which are the evaluations of these Boolean functions.
Lemma 9.10 Suppose m is even, q(X) is a quadratic form and ℓ(X) is a linear form. If
the bilinear form associated to q(X) has rank m, then the evaluation of q(X) + ℓ(X) at the
vectors of Fm
2 has either 2m−1 + 2m/2−1 or 2m−1 −2m/2−1 zeros.
Proof
Dickson’s theorem, Exercise 9.2, implies that there is a basis of Fm
2 with respect to which
q(x) + ℓ(X) is
m/2

i=1
(X2i−1X2i + a2i−1X2i−1 + a2iX2i).
This is equal to
m/2

i=1
(X2i−1 + a2i)(X2i + a2i−1) + b
for some b ∈F2. By Lemma 9.9, the evaluation of q(X) + ℓ(X) has either 2m−1 + 2m/2−1
zeros or 2m −(2m−1 + 2m/2−1) zeros, depending on whether b = 0 or 1.
⊓⊔

9
144
Chapter 9 • Reed–Muller and Kerdock Codes
Let K be a set of symmetric m × m matrices over F2, which have zeros on the
diagonal, and which have the property that the matrix A −A′ has rank m for all distinct
A, A′ ∈K.
No two matrices in K can have the same ﬁrst row, since their difference is of rank
m. The entries on the diagonal of the matrices in K are zero, so the top-left entry of a
matrix in K is zero. Hence, we have that
|K| ⩽2m−1.
For each A = (aij) ∈K, let
qA(X) =

1⩽i<j⩽m
aijXiXj.
Let C(K) be the code whose codewords are the evaluation at the vectors of Fm
2 of
qA(X) + ℓ(X) or qA(X) + ℓ(X) + 1,
for all A ∈K and for all linear forms ℓ(X).
Theorem 9.11
Suppose that m is even. The code C(K) is a binary block code of length 2m, size
|K||R(1, m)| and minimum distance 2m−1 −2m/2−1.
Proof
The distance between the evaluation of
qA(X) + ℓ(X) + b
and
qA′(X) + ℓ′(X) + b′
is the weight of the evaluation of
qA−A′(X) + ℓ(X) −ℓ′(X) + b −b′.
Since, A −A′ has rank m, Lemma 9.10 implies that this distance is at least 2m−1 −2m/2−1.
⊓⊔
A Kerdock code is a code C(K) where |K| = 2m−1. Thus, for a Kerdock code, K
is of maximum size and the set K is called a Kerdock set. A Kerdock code is a binary

9.4 · Non-binary Reed–Muller Codes
145
9
block code of length 2m, it has minimum distance 2m−1 −2m/2−1 and size 22m, i.e. it is
a (2m, 22m, 2m−1 −2m/2−1)2 code.
There are many non-equivalent Kerdock codes. Indeed, if m −1 is not prime, then
there are at least 2
√m/2 inequivalent Kerdock codes of length 2m. However, a sequence
of Kerdock codes, whose lengths tend to inﬁnity, is asymptotically bad. Although the
relative minimum distance tends to 1
2, the transmission rate is 2m/2m, which tends to
zero.
Kerdock codes are of interest because they can be non-linear. The algebraic and
geometric nature of their construction allows for non-trivial decoding algorithms to be
implemented. The fact that Kerdock codes can be non-linear opens up the possibility of
constructing codes with parameter sets for which linear codes do not exist.
Example 9.12
Consider the set of 4 × 4 matrices over F2
{
⎛
⎜⎜⎜⎝
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 0 1 0
0 0 0 1
1 0 0 1
0 1 1 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 0 0 1
0 0 1 1
0 1 0 1
1 1 1 0
⎞
⎟⎟⎟⎠}.
This set of matrices can be extended to a set K of 8 matrices with the property that the
difference of any two matrices has rank 4, see Exercise 9.5. Thus, K is a Kerdock set and,
by Theorem 9.11, the binary Kerdock code C(K) is a (16, 256, 6)2 code. We proved in
Example 4.20 that there is no binary linear code with these parameters. This code is the
Nordstrom–Robinson code.
■
9.4
Non-binary Reed–Muller Codes
Until now we have only considered Reed–Muller codes over F2, but one can naturally
generalise the deﬁnition of a Reed–Muller code over a general ﬁnite ﬁeld Fq. The
codewords of Rq(r, m) are the evaluations at the vectors of Fm
q of polynomials of degree
at most r in m variables, where the degree in any particular variable is at most q −1. The
number of vectors in an m-dimensional vector space over Fq is qm, so the length of the
linear code Rq(r, m) is qm. Its dimension is more difﬁcult to calculate, see Exercise 9.7.
In the following examples, we calculate the dimension for some speciﬁc cases and some
low weight codewords, which we will then go on and prove are of minimum non-zero
weight.
Example 9.13
Suppose r ⩽q −1. The evaluation of any polynomial in m variables of degree at most r will
be a codeword of Rq(r, m). The set
{Xc1
1 · · · Xcm
m | c1 + · · · + cm ⩽r}

9
146
Chapter 9 • Reed–Muller and Kerdock Codes
is a basis for the space of polynomials in m variables of degree at most r. Hence, the
dimension of Rq(r, m) is
m + r
r

since this is the number of non-negative integer solutions to
c1 + · · · + cm ⩽r.
Let g(X1) be a polynomial of degree r with r distinct roots in Fq. The evaluation of g is
a codeword with precisely rqm−1 zero coordinates; a zero coordinate being indexed by a
vector of Fm
q whose ﬁrst coordinate is a root of g. Therefore, Rq(r, m) has codewords of
weight (q −r)qm−1.
■
Example 9.14
The space of polynomials of degree at most 3 in three variables in which no variable has an
exponent larger than 2 has a basis
{1, X1, X2, X3, X2
1, X2
2, X2
3, X1X2, X1X3, X2X3,
X2
1X2, X2
1X3, X2
2X1, X2
2X3, X2
3X1, X2
3X2, X1X2X3}.
Therefore, the code R3(3, 3) is a 17-dimensional ternary linear code of length 27. We could
also have arrived at this conclusion by considering a monomial basis for all polynomials of
degree at most three in three variables and deleting X3
1, X3
2 and X3
3, see Exercise 9.7.
■
Suppose that r = a(q −1) + b, where 0 ⩽b ⩽q −2. If g(X1) is a polynomial of
degree b with b distinct roots in Fq, then the evaluation of
g(X1)(Xq−1
2
−1) · · · (Xq−1
a+1 −1),
a polynomial of degree r, is non-zero only when evaluated at
x = (x1, 0, . . . , 0, xa+2, . . . , xm)
for some x1 which is not a root of g. Therefore, Rq(r, m) has a codeword of weight
(q −b)qm−a−1.
We shall prove that this is the minimum weight of a non-zero codeword in the
following theorem, the proof of which is an example of a proof using the polynomial
method. This type of proof, which one sees often in combinatorics, attempts to
obtain bounds from the fact that the number of zeros of a non-zero polynomial is
bounded. The application of the method is often something like the following. Given

9.4 · Non-binary Reed–Muller Codes
147
9
a combinatorial object, a polynomial is constructed in such a way that the properties
of the combinatorial object are translated into algebraic properties of the polynomial.
Usually we are interested in the zeros of the polynomial, often restricted to subsets of a
vector space. Here, the polynomial is directly given as the polynomial whose evaluation
is the codeword. By bounding from above the number of zeros of the polynomial, we
will bound from below the weight of the codeword.
Theorem 9.15
The minimum distance of Rq(r, m) is (q −b)qm−a−1, where r = a(q −1) + b and
0 ⩽b ⩽q −2.
Proof
By induction on m.
If m = 1, then the codewords are the evaluation of a polynomial of degree r ⩽q −1 in
one variable. The polynomial has at most r zeros, so the codeword has weight at least q −r.
Observe that if r = q −1, then a = 1 and b = 0 and
(q −b)qm−a−1 = q/q = 1 = q −r.
Suppose that the codeword u ∈Rq(r, m) is the evaluation of the polynomial
f (X) = f (X1, . . . , Xm).
We write f (X) as a polynomial in Xm, whose coefﬁcients are polynomials in X1, . . . , Xm−1.
Thus,
f (X) =
c

i=0
fi(X1, . . . , Xm−1)Xi
m,
where c is the degree of f (X) in the indeterminate Xm. Note that fc(X1, . . . , Xm−1) ̸≡0
and
deg fc ⩽deg f −c ⩽r −c.
The codeword of Rq(r −c, m −1) which is the evaluation of fc has, by induction, at least
(q −b′)qm−a′−2
non-zero coordinates, where r −c = a′(q −1) + b′ and 0 ⩽b′ ⩽q −2.
For any (x1, . . . , xm−1) such that fc(x1, . . . , xm−1) ̸= 0, there are at least q −c elements
of Fq for which f (x1, . . . , xm−1, Xm) is not zero. Hence, the codeword u has weight at least
(q −c)(q −b′)qm−a′−2.

9
148
Chapter 9 • Reed–Muller and Kerdock Codes
It remains to prove that
(q −c)(q −b′)qm−a′−2 ⩾(q −b)qm−a−1.
The theorem then follows since, by Lemma 4.1, the minimum distance of a linear code is
equal to the minimum weight of a non-zero codeword.
If a′ ⩽a −2, then this is clear, so we can assume a′ = a −1 or a.
Suppose a′ = a −1 and (q −c)(q −b′) < q −b. This inequality implies b′ > b. We
have r = a(q −1) + b and r −c = a′(q −1) + b′, so
c = (a −a′)(q −1) + b −b′ = q −1 + b −b′.
Then (q −c)(q −b′) < (q −b) implies (b′ −b + 1)(q −b′) < q −b, a contradiction.
Suppose a′ = a and (q −c)(q −b′) < q(q −b). We have r = a(q −1) + b and
r −c = a′(q −1) + b′, so c = b −b′. Then (q −c)(q −b′) < q(q −b) implies (q −b +
b′)(q −b′) < q(q −b) which implies b < b′ and c < 0, a contradiction.
⊓⊔
9.5
Comments
Reed–Muller codes were introduced by Reed [59] and Muller [53] in the 1950s.
We have taken an algebraic rather than a geometric approach to the majority-logic
decoding algorithm. For a geometric description of the algorithm, see Van Lint [74] or
MacWilliams and Sloane [50].
Dickson’s classiﬁcation of quadratic form over ﬁelds of even characteristic is from
[22].
If m is odd, then there are examples of sets K for which Exercise 9.6 is a ( 1
2(m2 +
m) + 1 −rm)-dimensional binary linear code. The 11-dimensional codes (m = 5 and
r = 2) are the codes which caused a dispute between Apple and Samsung, referred to
in James Davis’ lecture [20]. They can be found in Corollary 17 (m = 5, d = t = 2)
on page 455 of MacWilliams and Sloane [50].
Kerdock codes were ﬁrst considered by Kerdock in [44] in 1972. That there are an
exponential number of inequivalent Kerdock codes is proven by Kantor in [41]. Kantor
takes a geometric approach to Kerdock codes in the articles [42], a treatment of which
can be found in Chapter 12 of Cameron and van Lint’s book [17]. The Nordstrom–
Robinson code is from [54]. Kerdock codes have applications to quantum mechanics,
see [15] and [18].
The non-binary Reed–Muller codes were deﬁned by various authors. Theorem 9.15
is attributed to Kasami, Lin and Peterson [43] in Bishnoi [11], where the proof given
here is adapted from.

9.6 · Exercises
149
9
9.6
Exercises
9.1 Suppose that we have sent a codeword of the code R(2, 4), the coordinates ordered as
in Example 9.1, and have received the vector
(0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1).
i.
Decode the received vector using syndrome decoding.
ii.
Decode the received vector using majority-logic decoding.
9.2 Suppose that q(X) is a quadratic form of rank m of the type
q(X) =

1⩽i<j⩽m
qijXiXj.
Prove that there is a basis of Fm
2 , with respect to which, q(X) is
m/2

i=1
X2i−1X2i.
9.3
i.
Prove that we can select half the codewords of R(1, m) so that the 2m × 2m matrix H,
whose rows are the selected codewords with zeros changed to minus one, has the property
that HHt = 2mI, where I is the 2m × 2m identity matrix.
ii.
Prove that for each vector v ∈F2m
2
there is a codeword u of R(1, m) such that d(u, v) ⩽
2m−1 −2m/2−1.
9.4 Prove that the Kerdock code C(K) of length 2m is linear if and only if the Kerdock set
K is a subspace of the vector space of m × m matrices.
9.5 Complete the set of matrices to a Kerdock set K of eight matrices and prove that C(K)
is a non-linear (16, 256, 16) code.
{
⎛
⎜⎜⎜⎝
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 0 1 0
0 0 0 1
1 0 0 1
0 1 1 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 0 0 1
0 0 1 1
0 1 0 1
1 1 1 0
⎞
⎟⎟⎟⎠}
9.6
i.
Prove that the evaluation of
q(X) =
r

i=1
X2i−1X2i

9
150
Chapter 9 • Reed–Muller and Kerdock Codes
at the vectors of Fm
2 , has 2m−1 + 2m−r−1 zeros.
ii.
Prove that the evaluation of q(X) + ℓ(X), where ℓ(X) is a linear form and q(X) is a
quadratic form whose associated bilinear form is of rank 2r, at the vectors of Fm
2 , has
either 2m−1 + 2m−r−1, 2m−1 or 2m−1 −2m−r−1 zeros.
iii.
Suppose that K is a set of m×m symmetric matrices over F2 with the property that A+
A′ has rank at least 2r for all A, A′ ∈K. Construct a (2m, |K|2m+1, 2m−1 −2m−r−1)2
code.
iv.
Use iii. to construct a [32, 11, 12]2 code.
v.
Construct a linear code with the same parameters from the code of length 31 constructed
in Exercise 5.6.
9.7
i.
By ﬁnding a monomial basis for the space of polynomials in 3 variables of degree at
most 4, in which the degree of each variable is at most 2, calculate the dimension of
R3(4, 3).
ii.
Prove that if r ⩽q −1, then the dimension of Rq(r, m) is
r

i=0
m + i −1
m −1

.
iii.
Prove that the dimension of Rq(r, m) is
r

i=0
m

j=0
(−1)j
m + i −1 −jq
m −1
m
j

.

151
10
p-Adic Codes
The p-adic numbers were ﬁrst considered by Hensel in the 19th century. He observed
that the primes play an analogous role in the integers as linear polynomials do in C[X].
The Laurent expansion of a rational function led him to consider the p-adic expansion
of a rational number. In this chapter, for a ﬁxed prime p, we will construct block codes
over the rings Z/phZ simultaneously, by constructing codes over the p-adic numbers
and then considering the coordinates modulo ph. These codes will be linear over the
ring but when mapped to codes over Z/pZ will result in codes which are not equivalent
to linear codes. We start with a brief introduction to p-adic numbers, which will cover
enough background for our purposes. The classical cyclic codes, that we constructed
in ▷Chapter 5, lift to cyclic codes over the p-adic numbers. In the case of the cyclic
Hamming code, this lift extends to a code over Z/4Z which, when mapped to a binary
code, gives a non-linear code with a set of parameters for which no linear code exists.
10.1 p-Adic Numbers
Let p be a prime.
The set of p-adic integers, which is denoted by Zp, is the set of sequences,
a = (a1, a2, a3, . . .),
where ai ∈Z/piZ for all i ∈N and
aj+1 ≡aj (mod pj).
An ordinary integer n ∈Z is an element of Zp deﬁned by the sequence
aj ≡n (mod pj).
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_10

10
152
Chapter 10 • p-Adic Codes
The sequence deﬁned by
aj+1 = aj + pj,
j ∈N, is a p-adic integer which is not an ordinary integer.
For example, with a1 = 3 and p = 5, this sequence begins
(3, 8, 33, 158, 783, . . .).
We deﬁne addition and multiplication on the sequences component-wise, so
a + b = (a1, a2, a3, . . .) + (b1, b2, b3, . . .) = (a1 + b1, a2 + b2, a3 + b3, . . .).
To verify a + b ∈Zp, observe that
aj+1 + bj+1 ≡aj + bj (mod pj).
Similarly,
ab = (a1, a2, a3, . . .)(b1, b2, b3, . . .) = (a1b1, a2b2, a3b3, . . .).
To verify ab ∈Zp, observe that
aj+1bj+1 ≡ajbj (mod pj).
With these deﬁnitions multiplication is distributive with respect to addition, so Zp is a
ring and has a multiplicative identity element
1 = (1, 1, 1, . . .).
If a is a sequence for which a1 = 0, then a does not have a multiplicative inverse, so Zp
is not a ﬁeld. It is, however, an integral domain (xy = 0 implies either x = 0 or y = 0),
so it has a quotient ﬁeld. This quotient ﬁeld is called the ﬁeld of p-adic numbers and is
denoted Qp. Elements of Qp are called p-adic numbers.
All non-zero elements of Zp can be written as the product of a unit times some
non-negative power of p.
For example, the 5-adic integer
(0, 15, 40, 290, 915, . . .) = 5(3, 8, 58, 183, . . . , ),
since 15 ≡0 modulo 5, 40 ≡15 modulo 25, 290 ≡40 modulo 125, etc.
The ﬁeld Qp consists of the sequences where we allow negative powers of p as well.
For example,
5−2(2, 22, 97, 222, . . .),

10.2 · Polynomials over the p-Adic Numbers
153
10
is a 5-adic number.
The product of pα(a1, a2, a3, . . .) and pβ(b1, b2, b3, . . .) is
pα+β(a1b1, a2b2, a3b3, . . .).
Returning to the previous examples,
5(3, 8, 58, 183, . . . , )5−2(2, 22, 97, 222, . . .) = 5−1(1, 1, 1, 1, . . .).
10.2 Polynomials over the p-Adic Numbers
Let Qp denote an algebraic closure of Qp. Recall that, since Qp is an algebraic closure,
the polynomials of positive degree over Qp factorise into linear factors over Qp. The
following lemma is a straightforward application of the binomial theorem.
Lemma 10.1 If α, β ∈Qp and
α ≡β (mod pr)
then
αp ≡βp (mod pr+1).
Proof
We can write α = β + prγ , for some γ ∈Zp. Then
αp = (β + prγ )p ≡βp (mod pr+1).
⊓⊔
In ▷Chapter 2 we studied how to factorise cyclotomic polynomials over ﬁnite ﬁelds
and put this to use in ▷Chapter 5 while constructing cyclic codes. The following
theorem tells us that a factorisation over Fp “lifts” to a factorisation over the p-adic
numbers. As in ▷Chapter 5, we will exploit this factorisation to construct cyclic codes
and their extensions with some surprising results.
Theorem 10.2
Let p be a prime and let n be a positive integer which is not a multiple of p. If h is a
monic irreducible divisor of Xn−1 in (Z/pZ)[X], then there exists a monic irreducible
polynomial h∞in Zp[X] which divides Xn −1 and is congruent to h modulo p.

10
154
Chapter 10 • p-Adic Codes
Proof
By induction on r, we will ﬁnd a polynomial hr(X) ∈(Z/prZ)[X] such that hr(X) divides
Xn −1 and hr ≡h modulo p. Then h∞will be the polynomial hr as r →∞.
An element c ∈Z/prZ can be extended to an element of Zp by taking the sequence
(c1, c2, . . . , cr−1, c, c, c, . . .),
where ci = c mod pi, for i = 1, . . . r −1. Therefore, the coefﬁcients of hr(X) can be viewed
as elements of Zp and therefore as elements of Qp.
Since n is not a multiple of p, the roots of h1(X) in Qp are distinct. By induction, we
can assume that the roots of hr(X) are distinct.
For each root α of hr(X) (in Qp),
αn ≡1 (mod pr).
Let
f (X) = hr(X) + prg(X),
for some polynomial g(X) ∈Zp[X].
For each root β of f , there is a root α of hr(X) such that
β ≡α (mod pr).
Then, by Lemma 10.1,
βp ≡αp (mod pr+1).
Lemma 10.1 also implies that
αnp ≡1 (mod pr+1)
from which we deduce that
βnp ≡1 (mod pr+1).
Let
hr+1(X) =

(X −βp),
where the product runs over the roots β of f .
Then hr+1 divides Xn −1 modulo pr+1. Since
βp ≡αp ≡α (mod p),

10.3 · p-Adic Codes
155
10
hr+1 and hr have the same roots modulo p.
Thus, the roots of hr+1 are distinct and
hr+1 ≡hr (mod p).
⊓⊔
10.3 p-Adic Codes
Let R be a commutative ring with multiplicative identity 1. An R-module M is a
commutative group with a left multiplication from R × M →M satisfying λ(u + v) =
λu + λv, (λ + μ)u = λu + μu, (λμ)u = λ(μu) and 1u = u, for all u, v ∈M and all
λ, μ ∈R.
The set Zn
p of n-tuples over the p-adic integers is a commutative group with respect
to addition. We deﬁne left multiplication of an element (u1, . . . , un) ∈Zn
p by an element
λ ∈Zp as
λ(u1, . . . , un) = (λu1, . . . , λun).
This scalar multiplication satisﬁes λ(u + v) = λu + λv, (λ + μ)u = λu + μu,
(λμ)u = λ(μu) and 1u = u, for all u, v ∈Zn
p and all λ, μ ∈Zp. Thus, with this
scalar multiplication Zn
p is a Zp-module.
A submodule C of Zn
p is a non-empty subset of Zn
p which is closed under linear
combinations. In other words,
λu + μv ∈C,
for all u, v ∈C and all λ, μ ∈Zp.
We now re-deﬁne the analogous objects that we saw for linear codes over a ﬁeld for
codes over Zp. A p-adic code of length n is a subset of Zn
p. A linear code over Zp is a
submodule of Zn
p.
A generator matrix for a linear code C over Zp is a k×n matrix G with the property
that
C = {(u1, . . . , uk)G| (u1, . . . , uk) ∈Zk
p}.
We deﬁne the scalar product on Zn
p as the standard inner product
u · v = u1v1 + · · · + unvn.
The dual code of a linear code C is deﬁned, as in the case of a linear code over a
ﬁnite ﬁeld, as
C⊥= {v ∈Zn
p | u · v = 0 for all u ∈C}.

10
156
Chapter 10 • p-Adic Codes
A linear code C is cyclic if
(c1, c2, . . . , cn) ∈C
implies
(cn, c1, . . . , cn−1) ∈C.
A codeword of the cyclic code corresponds to a polynomial in the ring Zp[X]/(Xn −1)
under the correspondence
(c1, c2, . . . , cn) →c1 + c2X + · · · + cnXn−1.
As in the case of ﬁnite ﬁelds, under this correspondence, a cyclic code is an ideal ⟨g⟩,
where g is some divisor of Xn −1.
Example 10.3
The polynomial X3+X+1 divides X7−1 in (Z/2Z)[X]. Theorem 10.2 implies the existence
of a polynomial in Z2[X] which divides X7 −1. One can verify that
g(X) = X3 + λX2 + (λ −1)X −1
divides X7 −1 in Z2[X] if and only if λ2 −λ + 2 = 0 by observing that
X7 −1 = (X3 + λX2 + (λ −1)X −1)(X3 + (1 −λ)X2 −λX −1)(X −1).
To calculate λ, suppose
λ = (a1, a2, a3, . . .).
Since a1 ∈Z/2Z, we have a1 = 0 or 1.
If a1 = 0, then substituting λ ≡0 + 2a2 (mod 4) in
λ2 −λ + 2 ≡0 (mod 4)
implies
−2a2 + 2 ≡0 (mod 4),
so a2 = 1 and λ ≡2 (mod 4).
Substituting λ ≡2 + 4a3 (mod 8) in
λ2 −λ + 2 ≡0 (mod 8)

10.4 · Codes over Z/phZ
157
10
implies
4 −2 −4a3 + 2 ≡0 (mod 8),
so a3 = 1 and λ ≡6 (mod 8).
Continuing in this way we deduce that one of the roots of λ2 −λ + 2 is
λ = (0, 2, 6, 6, 6, 38, 38, 166, 422, . . .).
The cyclic code ⟨g⟩is a 2-adic linear code of length 7 with generator matrix
G =
⎛
⎜⎜⎜⎝
−1 λ −1
λ
1
0
0 0
0
−1
λ −1
λ
1
0 0
0
0
−1
λ −1
λ
1 0
0
0
0
−1
λ −1 λ 1
⎞
⎟⎟⎟⎠.
■
To make use of these p-adic codes, we will now consider the coordinates of the
codewords of a p-adic code modulo ph for some h. The resulting code will be a code
deﬁned over the ﬁnite alphabet Z/phZ. We will use the matrix G from Example 10.3 in
Example 10.9.
10.4 Codes over Z/phZ
A linear code over Z/phZ is a (Z/phZ)-submodule of (Z/phZ)n. As in the case for a
linear code over Fq, we deﬁne a generator matrix for a linear code C over (Z/phZ)n
as a r × n matrix G with the property that
C = {(u1, . . . , ur)G| (u1, . . . , ur) ∈(Z/phZ)r}.
If all the elements in the i-th row of G are divisible by pj, then we can restrict ui to
Z/ph−jZ.
Example 10.4
Let
G =
⎛
⎜⎝
1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 1 1 1 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8
0 0 3 6 3 6 3 6 3 6 3 6 3 6 3 6 3 6 3 6
⎞
⎟⎠,
where the elements of G are from Z/9Z.

10
158
Chapter 10 • p-Adic Codes
The code generated by the matrix G is
C = {(u1, u2, u3)G | u1, u2 ∈Z/9Z, u3 ∈Z/3Z}.
Thus, the code C is a 9-ary code of length 20 of size 243.
The codeword
(3, 0, 1)G = (3, 0, 3, 6, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0)
and the all-zero codeword differ in 11 coordinates, so the minimum distance is at most 11. It
is Exercise 10.3 to verify that the minimum distance is 11.
■
Theorem 10.5
After a suitable permutation of the coordinates, a linear code C over (Z/phZ)n has a
generator matrix of the form
G =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
I A01 A02
A03
· · ·
A0,h−1
A0,h
0 pI pA12 pA13 · · · pA1,h−1
pA1,h
0
0
p2I p2A23 · · · p2A2,h−1
p2A2,h
... ...
...
...
...
· · ·
...
... · · ·
...
...
...
...
...
0
.
· · ·
0
0
ph−1I
ph−1Ah−1,h
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
If the block sizes of the columns are k0, k1, . . . , kh (necessarily summing to n), then
|C| = pk,
where
k =
h−1

i=0
(h −i)ki.
Proof
Applying elementary row operations to the matrix does not change the code C generated
by the matrix. Since we are also allowed to permute the columns the only impediment to
obtaining a generator matrix of the form

I B01

,

10.4 · Codes over Z/phZ
159
10
is rows in which all elements are divisible by p. Thus, we obtain a generator matrix of the
form
G =

I B01
0 pB02

,
for some matrices B01 and B02. We continue applying row operations and column permuta-
tions. Again, the only impediment to obtaining a generator matrix of the form

I B01 B02
0 pI pB12

,
is rows in which all elements are divisible by p2.
Therefore, there is a generator matrix for C of the form
G =
⎛
⎜⎝
I B01
B02
0 pI
pB12
0
0
p2B22
⎞
⎟⎠.
The form of G follows by continuing applying row operations and column permutations.
The code generated by G is
C = {(u1, . . . , ur)G | ui ∈Z/phZ}.
If all the entries in the ℓ-th row of G are divisible by pj, then we can restrict uℓto Z/ph−jZ,
which implies that the size of the code is as claimed.
⊓⊔
Example 10.6
Consider the code over Z/8Z generated by the matrix

0 2 1 4 1 1
4 6 7 4 7 1

.
By shifting the coordinates one coordinate to the right, we obtain an equivalent code with
generator matrix

1 0 2 1 4 1
1 4 6 7 4 7

.
Subtracting the ﬁrst row from the second, we obtain a generator matrix for the same code

1 0 2 1 4 1
0 4 4 6 0 6

.

10
160
Chapter 10 • p-Adic Codes
Multiplying the second row by 3 we obtain another generator matrix for the code

1 0 2 1 4 1
0 4 4 2 0 2

.
Finally, interchanging the second and sixth column we obtain an equivalent code with
generator matrix

1 1 2 1 4 0
0 2 4 2 0 4

.
Comparing this to the claim of Theorem 10.5, the matrix A01 = (1), the matrix A02 =
(2 1 4 0) and the matrix A12 = (2 1 0 2).
Note that the code has size 32 and not 64, which is not immediately apparent from the
initial generator matrix.
■
10.5 Codes over Z/4Z
The Gray map is a map γ from Z/4Z to {0, 1}2 deﬁned by
x
0
1
2
3
γ (x)
(0, 0)
(0, 1)
(1, 1)
(1, 0) .
We extend the Gray map to a map from (Z/4Z)n to {0, 1}2n by applying γ to each
coordinate.
If C is a block code of length n over Z/4Z, then γ (C), deﬁned by
γ (C) = {γ (v) | v ∈C},
is a binary code of length 2n. It is immediate that if C has minimum distance d, then
γ (C) has minimum distance at least d.
However, there is a possibility that the minimum distance of γ (C) is larger than d.
Example 10.7
Let C be the code over Z/4Z generated by the matrix

1 0 2 1 1 1
0 2 2 2 0 0

.
The 8 codewords of C and the code γ (C) are

10.5 · Codes over Z/4Z
161
10
C
γ (C)
(0,0,0,0,0,0)
(0,0,0,0,0,0,0,0,0,0,0,0)
(1,0,2,1,1,1)
(0,1,0,0,1,1,0,1,0,1,0,1)
(0,2,2,2,0,0)
(0,0,1,1,1,1,1,1,0,0,0,0)
(1,2,0,3,1,1)
(0,1,1,1,0,0,1,0,0,1,0,1)
(2,0,0,2,2,2)
(1,1,0,0,0,0,1,1,1,1,1,1)
(2,2,2,0,2,2)
(1,1,1,1,1,1,0,0,1,1,1,1)
(3,0,2,3,3,3)
(1,0,0,0,1,1,1,0,1,0,1,0)
(3,2,0,1,3,3)
(1,0,1,1,0,0,0,1,1,0,1,0)
One readily checks that the minimum distance of C is 3 and the minimum distance of
γ (C) is 6.
■
The Lee distance between two elements u and v of (Z/4Z)n is deﬁned as the
Hamming distance between γ (u) and γ (v). The Lee weight of an element u of (Z/4Z)n
is the Lee distance between u and the all zero n-tuple.
Lemma 10.8 Let C be a linear code over Z/4Z. The minimum Lee weight of a non-zero
codeword of C is equal to the minimum distance of γ (C).
Proof
Let u = (u1, . . . , un) and v = (v1, . . . , vn) be two codewords of C.
By checking all possibilities for ui, vi ∈Z/4Z, one can verify that the distance between
γ (ui) and γ (vi) is equal to the distance between (0, 0) and γ (ui −vi).
Thus,
d(γ (u), γ (v)) =
n

i=1
d(γ (ui), γ (vi)) =
n

i=1
d(γ (ui −vi), (0, 0))
which is equal to the Lee weight of u −v.
⊓⊔
In the following example, we return to Example 10.3 and consider the entries in the
matrix modulo 4. This matrix will then generate a code over Z/4Z.
Example 10.9
By Example 10.3, we have that X3 + 2X2 + X + 3 divides X7 −1 in (Z/4Z)[X]. This
polynomial generates a cyclic code of length 7 which extends to a code of length 8 with
generator matrix
G =
⎛
⎜⎜⎜⎝
3 1 2 1 0 0 0 1
0 3 1 2 1 0 0 1
0 0 3 1 2 1 0 1
0 0 0 3 1 2 1 1
⎞
⎟⎟⎟⎠.

10
162
Chapter 10 • p-Adic Codes
Let C be the Z/4Z-linear code of length 8 with 256 codewords deﬁned by
C = {uG | u ∈(Z/4Z)4}.
The code γ (C) is a binary code of length 16 with 256 codewords. By Exercise 10.7,
the minimum distance of γ (C) is 6. This code is equivalent to the code constructed in
Example 9.12. As mentioned there, an important observation is that there is no binary linear
code with these parameters, which we proved in Example 4.20.
■
Example 10.9 suggests that codes over rings may be a good place to look for non-
linear codes which have better parameter sets than linear codes. It may be the case that
we need to consider non-linear codes to disprove Conjecture 3.18.
10.6 Comments
This chapter leans somewhat on the enlightening article by Calderbank and Sloane on
p-adic codes [14]. Carlet [19] has generalised the Gray map to a bijection from Z/2kZ
to R(1, k −1). This can be extended to (Z/2kZ)n and can therefore be used to construct
(non-linear) binary codes from Z/2kZ-linear codes.
Theorem 10.2 is a special case of Hensel’s lifting lemma. For more on p-adic
numbers, including the lifting lemma, see [30].
10.7 Exercises
10.1 Let
λ = (1, b2, b3, b4, . . .)
be the 2-adic integer which is a root of X2 −X + 2. Calculate the numbers b2, b3, b4 in the
sequence of λ.
10.2 Prove that the code generated by the 4 × 8 matrix obtained by extending the generator
matrix in Example 10.3 with the all-one vector is a self-dual code.
10.3 Check, with the aid of a computer or not, that the code in Example 10.4 has minimum
distance 11.
10.4
i.
Prove that the dual code C⊥, to the code C generated by the matrix in Theo-
rem 10.5, has a generator matrix of the form

10.7 · Exercises
163
10
G =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
B0,h
B0,h−1
· · ·
B03
B02 B01 I
pB1,h
pB1,h−1 · · · pB13 pB12 pI 0
p2B2,h
p2B2,h−1 · · · p2B23 p2I
0
0
.
.
.
...
...
.
.
.
.
...
...
.
.
.
ph−1Bh−1,h
ph−1I
0
. . .
. . .
. . . 0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
for some matrices Bij, where the blocks of columns have the same size as in Theo-
rem 10.5.
ii.
Prove that |C⊥| = pk⊥, where
k⊥=
h

i=1
iki.
10.5 Let C be a linear code over Z/phZ. Prove that (C⊥)⊥= C.
10.6
Let C be the linear code over Z/4Z from Example 10.7.
i.
Check that the minimum Lee weight of a non-zero codeword of C is 6 and verify that the
minimum Hamming distance between any two codewords of γ (C) is 6.
ii.
The code γ (C) is a non-linear binary code of length 12, minimum distance 6 and size
8. Construct a linear code with the same parameters.
iii.
The code
C = {λu + 2μv | λ ∈Z/4Z, μ ∈Z/2Z}
for some u ∈(Z/4Z)6 and v ∈(Z/2Z)6, where the weight of v is 3. Construct a code
with the same parameters as C in which the weight of v is 4.
10.7
i.
Prove, using row operations, that the code C in Example 10.9 has a generator matrix
G = G1 + 2G2,
where
G1 =
⎛
⎜⎜⎜⎝
1 0 0 0 0 1 1 1
0 1 0 0 1 0 1 1
0 0 1 0 1 1 0 1
0 0 0 1 1 1 1 0
⎞
⎟⎟⎟⎠

10
164
Chapter 10 • p-Adic Codes
and
G2 =
⎛
⎜⎜⎜⎝
0 0 0 0 1 1 0 0
0 0 0 0 0 1 1 0
0 0 0 0 0 0 1 1
0 0 0 0 1 0 0 1
⎞
⎟⎟⎟⎠.
ii.
Prove, using Lemma 10.8, that the code γ (C) in Example 10.9 has minimum distance 6.
10.8
i.
Prove that X2 + λX −1 divides X8 −1 in Zp[X], where λ is a p-adic integer satisfying
λ2 = −2.
ii.
Calculate the next few numbers in the sequences (1, 4, . . .) and (2, 5, . . .) which are both
solutions of λ2 = −2 in Z3.
10.9
i.
Prove that X5 + λX4 −X3 + X2 + (λ −1)X −1 divides X11 −1 in Zp[X], where λ is
a p-adic integer satisfying λ2 = λ −3.
ii.
Calculate the ﬁrst few numbers in the sequences which are solutions of λ2 = λ−3 in Z3.
10.10
i.
Prove that X11 + λX10 + (λ −3)X9 −4X8 −(λ + 3)X7 −(2λ + 1)X6 −(2λ −3)X5 −
(λ −4)X4 + 4X3 + (λ + 2)X2 + (λ −1)X −1 divides X23 −1 in Zp[X], where λ is a
p-adic integer satisfying λ2 = λ −6.
ii.
Calculate the ﬁrst few numbers in the sequences which are solutions of λ2 = λ−6 in Z2.

165
Hints and Answers to Selected Exercises
1.1 Hr(X) = logr(223/1835/35−5/18) ≈logr 9.6759.
1.2 H2(X) = n −log2(2n −1) + (1 −2−n)(2 −21−n −n2−n).
1.3
i.
n
j

2n−j.
ii. H2(X) = 3
2.
1.4 φhr(p).
1.5 Hr(Y) = m(1 −logr m), where m = (φr + 1 −2φ)/(r −1).
I(X, Y) = m(1 −logr m) + φ logr φ + r logr((1 −φ)/(r −1)).
1.6 φ.
1.8
i. H(Y|X) = −(1 −p)( 3
4 logr 3 −logr 4).
H(Y) = hr(p) −(1 −p)( 3
4 logr 3 −logr 4).
H(X) = hr(p).
ii. logr 2.
1.9
i.
−3
4p log 3 + 3
2p log 2 −

4−3p
4

log

4−3p
8

.
iii.
I(X, Y) evaluated at p = 4/(28/3 + 3).
2.5
i.
1, 2, 4, 4, 4.
ii.
1, 11, 11.
iii.
1, 1, 1, 1, 2, 2, 2, 2.
2.6
i.
(X −1)(X −2)(X −4)(X3 −2)(X3 −4).
ii.
(X −1)(X10 + X9 + X8 + X7 + X6 + X5 + X4 + X3 + X2 + X + 1).
iii.
(X −1)(X + 1)(X2 + 1)(X2 + 5X −1)(X2 −5X −1).
2.12 Hint: Let U be a k-dimensional subspace of Fk+1
q
, so U is a hyperplane of
PG(k, q). Let V be an (r+1)-dimensional subspace of Fk+1
q
and observe that v+(V ∩U),
where v ∈V \ U, is a coset of a subspace of U ∼= Fk
q.
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4

166
Hints and Answers to Selected Exercises
3.4 Hint: Assuming (0, 0, 0, 0, 0, 0, 0, 0, 0, 0) ∈C, the non-zero vectors all have
weight 6. Look at the proof of Lemma 3.10.
The codewords are the six rows of the matrix
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
0 0 0 0 0 0 0 0 0 0
1 1 1 1 1 1 0 0 0 0
1 1 1 0 0 0 1 1 1 0
1 0 0 1 1 0 1 1 0 1
0 1 0 1 0 1 1 0 1 1
0 0 1 0 1 1 0 1 1 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
.
3.5 Hint: By considering the proof of Lemma 3.10, observe that equality in the bound
cannot occur.
3.6 Hint: follow the same strategy as Exercise 3.5.
3.12
ii.
The vector σ(u) has n −wt(u) ones and wt(u) minus ones, where wt(u) is the
number of ones that u has.
iii.
If u and v agree on a coordinate, then that coordinate contributes 1 to the scalar
product σ(u) · σ(v). If u and v differ on a coordinate, then that coordinate
contributes −1 to the scalar product σ(u) · σ(v). They agree on n −d(u, v)
coordinates and differ on d(u, v) coordinates.
iv.
Using ii. and iii. and the fact that d(u, v) ⩾d for all u, v ∈C, the bound follows.
v.
The bound on w ensures that the scalar product in iv. is non-positive, with λ =
1
2
√1 −2(d/n). Assuming that (0, 0, . . . , 0) ∈C, we have by i. and iv. that the
number of codewords at a distance at most w from (0, 0, . . . , 0) is at most 2n.
4.3 Prove there exists a [n−k +r, r, d]q code for r = 1, . . . , k, by induction on r. Note
that there is a [n −k + 1, 1, d]q code, since the condition implies d ⩽n −k + 1. The
condition implies the condition for n replaced by n −1 and k replaced by k −1 so, by
induction, there exists a [n −1, k −1, d]q code. Let H be a (n −k) × (n −1) check
matrix for this code. The condition allows us to add a column to H so that any d −1
columns of the extended matrix are linearly independent, i.e. there is a non-zero vector
of Fn−k
q
which is not a linear combination of any subset of at most d −2 columns of H.
Apply Lemma 4.4.
4.4 Hint: Since the code is self-dual it has no codewords of odd weight.
The set of equations in matrix form is

Hints and Answers to Selected Exercises
167
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−15
1
1
1
1
8
4
0
−4
−8
28 −12 −4
4
8
56
−4
0
4
−56
70 −10 −10 −10 70
56
−4
0
4
−56
28
4
−4 −12
8
8
4
0
−4
−8
1
1
1
1
−15
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎜⎝
1
a2
a4
a6
a8
⎞
⎟⎟⎟⎟⎟⎠
= 0.
4.5 The sent vector is (1, 1, 1, 0, −1, −1, −1, 0).
4.6 A(X) = 1 + 4X3 + 3X4.
4.7 The sent vector is (1, 2, 3)G.
4.8
i. Prove that every 4 × 4 sub-matrix of H is non-singular.
ii. The sent vector is (1, 6, 3, 6, 1, 2, 2).
4.9 The sent vector is (1, 2, 1, 2, 0, 2, 0, 2, 0).
4.14 For example,
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1 1
1
0
1 1
0
1
1 0
1
1
0 1
1
1
1 1 −1 −1
1 −1 1 −1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
.
4.15 Let u and v be two codewords of weight w with the same support. For all non-zero
λ ∈Fq the vector λv is a codeword. For each non-zero coordinate ui of u there is a λ
such that ui = λvi, where vi is the i-th coordinate of v. By the pigeon-hole principle,
there is a λ such that ⌈w/(q −1)⌉of the coordinates agree with the corresponding
coordinate in λv. Then u −λv is a codeword of weight at most w −⌈w/(q −1)⌉.
4.16
i.
Hint: Make the substitution X →1 −X in the MacWilliams identities.
ii.
Hint: The subspace of polynomials spanned by
{Xj(1 + (q −1)(1 −X))n−j | j = d, . . . , n}
is equal to the subspace of polynomials spanned by
{Xj | j = d, . . . , n}.

168
Hints and Answers to Selected Exercises
iii.
Applying ii., there is a unique way to write the polynomial (1 + (q −1)(1 −X))n
as a linear combination of the n + 1 polynomials in ii.
iv.
Apply Theorem 4.13.
5.1 Hint: Since C is self-dual, all codewords have weight which are multiples of 3.
Using this observation, solve the equations given by the equality
729A(X) = (1 + 2X)12A
 1 −X
1 + 2X

,
given by Theorem 4.13.
5.2 ←−
g (X) = X11 + X10 + X6 + X5 + X4 + X2 + 1.
5.6 The largest dimensions are i. 7. ii. 11 iii. 4.
5.7
i.
f (X) = X8 + X7 + X6 + X4 + X2 + X + 1.
g(X) = X8 + X5 + X4 + X3 + 1.
ii.
Use Theorem 5.10.
iii.
Use Theorem 3.5 and prove that the extended code is also linear.
5.8
i.
The polynomial X11 + 1 factorises as
(X +1)(X5 +ϵX4 +X3 +X2 +ϵ2X +1)(X5 +ϵ2X4 +X3 +X2 +ϵX +1),
where ϵ2 = 1 + ϵ.
iii.
Hint: Use Exercise 5.5.
5.9
ii.
Apply Exercise 5.5.
iv.
Find a codeword of weight 7.
6.1 Hint: use Exercise 2.8.
6.2 Hint: (c0, . . . , cn−1) ∈C if and only if there is a polynomial h of degree at most
k −1 such that ci = h(αi). Consider the polynomial c(X) = n−1
i=0 ciXi. To prove the
exercise observe that it is sufﬁcient to prove that c(αj) = 0, for all j = 1, . . . , n −k.
6.3 The sent codeword is the evaluation of the polynomial F(X) = X3 −X + 1.
6.5 Hint: The 4 × 4 sub-matrices are Vandermonde.
6.6 Hint: use the Griesmer bound.
6.7 Hint: use i. to prove ii.

Hints and Answers to Selected Exercises
169
6.8
ii.
Use Exercise 6.7 iii.
iii.
For q odd, use Exercise 6.7 ii. For q even, use ii.
6.9 Hint: x2e−1 = y2e−1 implies x = y.
6.10 Hint: Let A be a 5 × 5 sub-matrix of the matrix. Prove that (det A)3 = v −ηv3,
where v is the determinant of a Vandermonde matrix. Apply Theorem 6.10.
6.11 Hint: Prove that det A ̸= 0 and use Theorem 6.10.
7.1 The sent codeword is (−1, 1, 1, 1, 1, 0, 1, 1, 1).
7.3 Hint: Use Exercise 7.2.
7.7 g(X) = aX2, h(X) = aX, f (X) = X.
7.9 g(X) = (X + 1)h(X), h(X) = a(2(e + 1)X + 1), f (X) = X + 1.
7.10
i. (Y/Z) = 4P3 −4P2.
ii. {1, X/Y, X/Z, (Y 2 + Z2)/X2}.
8.2
iii.
1.
8.5
iii.
The minimum distance is 6.
iv.
The dimension is 11.
9.1 The sent vector is
(0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1),
which the evaluation of the polynomial X3 + X4 + X1X2.
9.2 Hint: Start by selecting a vector e1 such that q(e1) = 0. Then choose e2 such that
q(e2) = 0 and b(e1, e2) = 0, where b(X, Y) = q(X + Y) −q(X) −q(Y).
9.5 Hint: we know the ﬁrst row in the remaining matrices. Since the matrices are
symmetric and have zeroes on the diagonal, each of the remaining matrices has only
three entries to be determined.
The four additional matrices are
{
⎛
⎜⎜⎜⎝
0 1 1 0
1 0 1 1
1 1 0 0
0 1 0 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 1 0 1
1 0 1 0
0 1 0 0
1 0 0 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 0 1 1
0 0 1 0
1 1 0 1
1 0 1 0
⎞
⎟⎟⎟⎠,
⎛
⎜⎜⎜⎝
0 1 1 1
1 0 0 1
1 0 0 0
1 1 0 0
⎞
⎟⎟⎟⎠}

170
Hints and Answers to Selected Exercises
9.6
i.
As in the proof of Lemma 9.9.
ii.
As in the proof of Lemma 9.10 but there is also the case that the q(X) + ℓ(X) is of
the form
r

i=1
X2i−1X2i + X2r+1
after a suitable change of basis.
iv.
Hint: Find a basis for a 5-dimensional subspace of ﬁve 5 × 5 matrices all of whose
non-zero matrices have rank at least 4.
9.7 The number of monomials of degree i in m indeterminates in which the degree of
each indeterminate is at most q−1 is equal to the number of solutions of x1+· · ·+xm =
i, where 0 ⩽xj ⩽q −1. Use inclusion-exclusion.
10.1 Hint: b2 = 2c2 + 1 for some c2 ∈{0, 1}. We can solve for c2 from
(2c2 + 1)2 −(2c2 + 1) + 2 = 0 (mod 4).
λ = (1, 3, 3, 11, . . .).
10.6
ii.
The linear code over F2 generated by the matrix
⎛
⎜⎝
1 0 0 0 1 1 1 1 0 0 0 1
0 1 0 1 0 1 1 0 1 0 1 0
0 0 1 1 1 0 1 0 0 1 1 1
⎞
⎟⎠
has minimum distance 6.
iii.
For example, the code generated by the matrix

1 1 2 0 1 1
0 0 2 2 2 2

.
10.8
ii. (1, 4, 22, 76, . . .) and (2, 5, 5, 59, . . .).
10.9
ii. (0, 3, 12, . . .) and (1, 7, 16, . . .).
10.10
ii. (0, 2, 2, 10, . . .) and (1, 3, 7, 7, . . .).

171
Bibliography
1. T. Alderson, A. Gács, On the maximality of linear codes. Des. Codes Cryptogr. 53, 59–68 (2009)
2. T. Alderson, A.A. Bruen, R. Silverman, Maximum distance separable codes and arcs in projective
spaces. J. Combin. Theory Ser. A 114, 1101–1117 (2007)
3. R.B. Ash, Information Theory (Dover, Mineola, 1965)
4. E.F. Assmus Jr., H.F. Mattson Jr., New 5-designs. J. Combin. Theory 6, 122–151 (1969)
5. S. Ball, On sets of vectors of a ﬁnite vector space in which every subset of basis size is a basis. J.
Eur. Math. Soc. 14, 733–748 (2012)
6. S. Ball, Finite Geometry and Combinatorial Applications. London Mathematical Society Student
Texts, vol. 82 (Cambridge University Press, Cambridge, 2015)
7. L.A. Bassalygo, New upper bounds for error-correcting codes. Probl. Inform. Transm. 1, 32–35
(1965)
8. E.R. Berlekamp, Algebraic Coding Theory (McGraw-Hill, New York, 1968)
9. E.R. Berlekamp, L.R. Welch, Error correction for algebraic block codes, U.S. Patent 4,633,470, 30
Dec 1986
10. J. Bierbrauer, Introduction to Coding Theory, 2nd edn. (Chapman and Hall/CRC Press, Boca Raton,
2016)
11. A. Bishnoi, Some contributions to incidence geometry and the polynomial method. PhD thesis,
Universiteit Gent, Gent, 2016
12. R.E. Blahut, The Gleason-Prange theorem. IEEE Trans. Inform. Theory 37, 1269–1273 (2006)
13. R.C. Bose, D.K. Ray-Chaudhuri, On a class of error correcting binary group codes. Inform. Control
3, 68–79 (1960)
14. R. Calderbank, N.J.A. Sloane, Modular and p-adic cyclic codes. Des. Codes Cryptogr. 6, 21–35
(1995)
15. R. Calderbank, E.M. Rains, P.W. Shor, N.J.A. Sloane, Quantum error correction via codes over
GF(4). IEEE Trans. Inform. Theory 44, 1369–1387 (1998)
16. P.J. Cameron, Combinatorics: Topics, Techniques, Algorithms (Cambridge University Press,
Cambridge, 1994; reprinted 1996)
17. P.J. Cameron, J.H. van Lint, Designs, Codes, Graphs and Their Links. London Mathematical
Society Student Texts, vol. 22 (Cambridge University Press, Cambridge, 1991)
18. T. Can, N. Rengaswamy, R. Calderbank, H.D. Pﬁster, Kerdock codes determine unitary 2-designs
(2019), https://arxiv.org/abs/1904.07842
19. C. Carlet, Z2k-linear codes. IEEE Trans. Inform. Theory 44, 1543–1547 (1998)
20. J. Davis, IMS Public Lecture: Apple vs Samsung: a Mathematical Battle, https://ims.nus.edu.sg/
resourcevideo.php, 18 May 2016
21. P. Delsarte, An Algebraic Approach to the Association Schemes of Coding Theory. Philips Research
Reports Supplement, vol. 10 (N.V. Philips’ Gloeilampenfabrieken, Amsterdam, 1973)
22. L.E. Dickson, Linear Groups: With an Exposition of the Galois Field Theory (Dover Publications,
New York, 1901)
23. R. Fano, Transmission of Information; A Statistical Theory of Communications (MIT Press,
Cambridge, 1961)
24. R.G. Gallager, Low Density Parity Check Codes (MIT Press, Cambridge, 1963)
25. E.N. Gilbert, A comparison of signalling alphabets. Bell Syst. Tech. J. 31, 504–522 (1952)
26. D.G. Glynn, The non-classical 10-arc of PG(4, 9). Discrete Math. 59, 43–51 (1986)
27. M.J.E. Golay, Notes on digital coding. Proc. Inst. Radio Eng. 37, 657 (1949)
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4

172
Bibliography
28. V.D. Goppa, A new class of linear error-correcting codes. Probl. Inform. Transm. 6, 207–212 (1970)
29. V.D. Goppa, Codes on algebraic curves. Soviet Math. Dokl. 24, 170–172 (1981)
30. F. Gouvea, p-Adic Numbers: An Introduction (Springer, Berlin, 1997)
31. J.H. Griesmer, A bound for error-correcting codes. IBM J. Res. Dev. 4, 532–542 (1960)
32. V. Guruswami, A. Rudra, Limits to list decoding Reed-Solomon codes. IEEE Trans. Inform.
Theory 52, 3642–3649 (2006)
33. V. Guruswami, M. Sudan, Improved decoding of Reed-Solomon and algebraic-geometry codes.
IEEE Trans. Inform. Theory 45, 1757–1767 (1999)
34. R.W. Hamming, Error detecting and error correcting codes. Bell Labs Tech. J. 29, 147–160 (1950)
35. R.V.L. Hartley, Transmission of information. Bell Syst. Tech. J. 7, 535–563 (1928)
36. H.J. Helgert, Alternant codes. Inform. Control 26, 369–380 (1974)
37. R. Hill, A First Course in Coding Theory (Oxford University Press, Oxford, 1988)
38. A. Hocquenghem, Codes correcteurs d’erreurs. Chiffres 2, 147–156 (1959)
39. S. Hoory, N. Linial, A. Wigderson, Expander graphs and their applications. Bull. Am. Math. Soc.
43, 439–561 (2006)
40. G.A. Jones, J.M. Jones, Information and Coding Theory. Springer Undergraduate Mathematics
Series (Springer, Berlin, 2000)
41. W.M. Kantor, An exponential number of generalized Kerdock codes. Inform. Control 53, 74–80
(1982)
42. W.M. Kantor, Spreads, translation planes and Kerdock sets, I, II. SIAM J. Algebraic Discrete Math.
3, 151–165 and 308–318 (1983)
43. T. Kasami, S. Lin, W.W. Peterson, Generalized Reed-Muller codes. Electron. Commun. Jpn. 51,
96–104 (1968)
44. A.M. Kerdock, A class of low-rate non-linear binary codes. Ann. Univ. Turku, Ser. A 20, 182–187
(1972)
45. Y. Kou, S. Lin, M. Fossorier, Low-density parity-check codes based on ﬁnite geometries: a
rediscovery and new results. IEEE Trans. Inform. Theory 47, 2711–2736 (2001)
46. R. Lidl, H. Niederreiter, Finite Fields. Encyclopedia of Mathematics and Its Applications, vol. 20,
2nd edn. (Cambridge University Press, Cambridge, 1997)
47. S. Lin, E.J. Welden, Long BCH codes are bad. Inform. Control 11, 445–451 (1967)
48. S. Ling, C. Xing, Coding Theory: A First Course (Cambridge University Press, Cambridge, 2004)
49. D.J.C. MacKay, R.M. Neal, Near Shannon limit performance of low density parity check codes.
Electron. Lett. 32, 1645–1646 (1996)
50. F.J. MacWilliams, N.J.A. Sloane, The Theory of Error-Correcting Codes (North-Holland, New
York, 1977)
51. R.J. McEliece, E.R. Rodemich, H. Rumsey, L.R. Welch, New upper bounds on the rate of a code
via the Delsarte-MacWilliams inequalities. IEEE Trans. Inform. Theory 23, 157–166 (1997)
52. G.L. Mullen, D. Panario (eds.), Handbook of Finite Fields. Discrete Mathematics and Its
Applications (CRC Press, Boca Raton, 2013)
53. D.E. Muller, Application of Boolean algebra to switching circuit design and to error detection.
IEEE Trans. Comput. 3, 6–12 (1954)
54. A.W. Nordstrom, J.P. Robinson, An optimum nonlinear code. Inform. Control 11, 613–616 (1967)
55. P.R.J. Östergård, On binary/ternary error-correcting codes with minimum distance 4, in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes, ed. by M. Fossorier, H. Imai, S. Lin,
A. Poli. Lecture Notes in Computer Science, vol. 1719 (Springer, Berlin, 1999), pp. 472–481
56. T. Penttila, I. Pinneri, Hyperovals. Australas. J. Combin. 19, 101–114 (1999)
57. V. Pepe, LDPC codes from the Hermitian curve. Des. Codes Cryptogr. 42, 303–315 (2007)
58. M. Plotkin, Binary codes with speciﬁed minimum distance. IRE Trans. Inform. Theory 6, 445–450
(1960)
59. I.S. Reed, A class of multiple-error-correcting codes and the decoding scheme. IEEE Trans. Inform.
Theory 4, 38–49 (1954)

Bibliography
173
60. I.S. Reed, G. Solomon, Polynomial codes over certain ﬁnite ﬁelds. J. Soc. Ind. Appl. Math. 8,
300–304 (1960)
61. S. Roman, Coding and Information Theory. Graduate Texts in Mathematics, vol. 134 (Springer,
Berlin, 1992)
62. C. Roos, A note on the existence of perfect constant weight codes. Discrete Math. 47, 121–123
(1983)
63. B. Segre, Ovals in a ﬁnite projective plane. Canad. J. Math. 7, 414–416 (1955)
64. B. Segre, Introduction to Galois geometries. Atti Accad. Naz. Lincei Mem. 8, 133–236 (1967)
65. C.E. Shannon, A Mathematical Theory of Communication (University of Illinois Press, Champaign,
1949; reprinted 1998)
66. R.C. Singleton, Maximum distance q-nary codes. IEEE Trans. Inform. Theory 10, 116–118 (1964)
67. M. Sipser, D.A. Spielman, Expander codes. IEEE Trans. Inform. Theory 42, 1710–1722 (1996)
68. M. Sudan, Decoding of Reed Solomon codes beyond the error-correction bound. J. Complexity 13,
180–193 (1997)
69. A. Ta-Shma, Explicit, almost optimal, epsilon-balanced codes, in Proceedings of the 49th Annual
ACM SIGACT Symposium on Theory of Computing (2017), pp. 238–251
70. C. Thommesen, The existence of binary linear concatenated codes with Reed-Solomon outer codes
which asymptotically meet the Gilbert-Varshamov bound. IEEE Trans. Inform. Theory 29, 850–853
(1983)
71. M.A. Tsfasman, Algebraic-Geometric codes and asymptotic problems. Discrete Applied Math. 33,
241–256 (1991)
72. M.A. Tsfasman, S.V. Vl˘adut, Algebraic-Geometric Codes (Kluwer Academic Publishers, Norwell,
1991)
73. M.A. Tsfasman, S.V. Vl˘adut, T. Zink, Modular curves, Shimura curves, and Goppa codes, better
than the Varshamov-Gilbert bound. Math. Nachr. 109, 21–28 (1982)
74. J.H. van Lint, Introduction to Coding Theory. Graduate Texts in Mathematics, vol. 86, 3rd edn.
(Springer, Berlin, 1999)
75. R.R. Varshamov, Estimate of the number of signals in error correcting codes. Dokl. Acad. Nauk
SSSR 117, 739–741 (1957)
76. S.B. Wicker, V.K. Bhargava (eds.), Reed-Solomon Codes and Their Applications (IEEE Press,
Piscataway, 1994)

175
Index
A
A(n, d, w), 38, 44
Ar(n, d), 32, 43
Afﬁne plane, 26, 131
Afﬁne space, 26
AG(k, q), 26, 131
Alderson-Bruen-Silverman model, 59
Alphabet, 29
Arc, 92
B
Belief propagation, 128
Block code, 10
Boolean function, 133
Bound
– Elias–Bassalygo, 39
– Gilbert–Varshamov, 32, 67, 109
– Griesmer, 60, 69
– linear programming, 66
– McEliece–Rodemich–Rumsey–Welch, 41
– Plotkin, 35, 37, 63
– Singleton, 84
– sphere packing, 33, 36, 44, 63
Burst errors, 94
C
Channel, 4
– binary erasure, 6, 15
– binary symmetric, 5, 11
– capacity of, 11
– information, 4
Character, 55
Check matrix, 48, 51, 106, 120, 126
Code
– algebraic geometric, 112
– alternant, 107
– asymptotically good, 36
– BCH, 78, 81
– block, 10, 29
– constant weight, 38
– cyclic, 71, 156
– dual, 54, 72, 136, 155, 162
– equivalent, 59
– evaluation, 85
– extended, 31, 67
– extension, 31
– generalised Reed–Solomon, 107, 120
– Golay, 73, 76, 80, 81
– Hamming, 49
– Kerdock, 142, 144
– LDPC, 126
– length of, 29
– linear, 47, 92, 155, 157
– local reconstruction, 101
– MDS, 83
– Nordstrom–Robinson, 145
– over a ring, 157
– over Z/4Z, 160, 163
– p-adic, 155
– perfect, 34, 43, 67, 73
– quadratic residue, 75
– Reed–Muller, 133
– Reed–Solomon, 85, 96, 101
– repetition, 30
– self-dual, 54
– subﬁeld sub, 105
– systematic, 38
– turbo, 131
Combinatorial design, 63
Conjecture
– constant weight binary codes, 42
– Gilbert–Varshamov, 42
– MDS, 95
Coordinate ring, 113
Cyclotomic polynomial, 21, 71, 81
D
Decoding, 10
– belief propagation, 128
– list, 88
– majority-logic, 138
– maximum likelihood, 10
– nearest neighbour, 30
© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4

176
Index
– Reed–Solomon code, 86
– standard array, 88
– syndrome, 51, 67
Degree
– of a divisor, 114
Design, 63
Distance
– Hamming, 11, 30
– Lee, 161
– minimum, 30
Divisor, 113
– degree of, 114
Dual code, 54, 72
E
End-vertex, 123
Entropy, 3
– conditional, 6
– input, 5
– joint, 6
– output, 5
Entropy function, 3
Expander graph, 124
F
Field, 17
– ﬁnite, 19
– subﬁeld, 26
Frobenius automorphism, 21, 55
G
Generator matrix, 48, 69, 155, 157
Genus, 114
Graph, 123
– bipartite, 123
Gray map, 160
H
Hamming distance, 11
h(p), 3, 12, 32
Hyperplane, 24
I
Information, 2
– average, 6
– mutual, 8
L
Lee distance, 161
Lee weight, 161
List decoding, 88
M
MacWilliams identities, 56
Matrix
– check, 48, 51, 106, 120, 126
– generator, 48, 69, 155, 157
Minimum distance, 47, 161
– prescribed, 78, 115
Minimum weight, 47
Module, 155
N
Neighbour, 123
[n, k, d]q, 48
(n, K, d)r, 48
P
p-adic integers, 151
p-adic numbers, 152
PG(k −1, q), 24, 92
Plotkin lemma, 34, 43
Polynomial
– cyclotomic, 21
– interpolation, 99, 108
– Krawtchouk, 66
Primitive element, 22
Probability
– backward, 4
– of a correct decoding, 10
– forward, 4
– joint, 4
Projective plane, 24
Projective space, 24, 92
R
Random variable, 1
Rate, 10, 36
Ring, 17
– coordinate, 113

Index
177
S
Shannon’s theorem, 13
Stable set, 123
Standard array decoding, 88
Submodule, 155
Support, 63
Syndrome, 51, 128
Syndrome decoding, 51, 67
T
Trace map, 55
Transmission rate, 10
W
Weight, 37, 47, 71
– Lee, 161
Weight enumerator, 54

