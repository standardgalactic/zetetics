ControlRoom3D: Room Generation using Semantic Proxy Rooms
Jonas Schult1,2‚àó
Sam Tsai1
Lukas H¬®ollein1,3‚àó
Bichen Wu1
Jialiang Wang1
Chih-Yao Ma1
Kunpeng Li1
Xiaofang Wang1
Felix Wimbauer1,3‚àó
Zijian He1
Peizhao Zhang1
Bastian Leibe2
Peter Vajda1
Ji Hou1
1Meta GenAI
2RWTH Aachen University
3Technical University of Munich
üéõÔ∏è ControlRoom3D
¬† ¬† ¬† ¬†Watertight 3D Mesh
Cyberpunk bedroom
Rustic farmhouse bedroom
¬† ¬† ¬† 3D Proxy Room
Figure 1. Textured 3D Mesh Generation from 3D Semantic Layouts and Text Prompts. Given a textual description of the overall room
style and a rough 3D room layout based on 3D semantic bounding boxes, our method called ControlRoom3D creates diverse and globally
plausible 3D room meshes which align well with the room layout.
Abstract
Manually creating 3D environments for AR/VR appli-
cations is a complex process requiring expert knowledge
in 3D modeling software. Pioneering works facilitate this
process by generating room meshes conditioned on textual
style descriptions. Yet, many of these automatically gener-
ated 3D meshes do not adhere to typical room layouts, com-
promising their plausibility, e.g., by placing several beds in
one bedroom. To address these challenges, we present Con-
trolRoom3D, a novel method to generate high-quality room
meshes. Central to our approach is a user-defined 3D se-
mantic proxy room that outlines a rough room layout based
on semantic bounding boxes and a textual description of the
overall room style. Our key insight is that when rendered
to 2D, this 3D representation provides valuable geometric
and semantic information to control powerful 2D models to
generate 3D consistent textures and geometry that aligns
well with the proxy room. Backed up by an extensive study
including quantitative metrics and qualitative user evalu-
ations, our method generates diverse and globally plausi-
ble 3D room meshes, thus empowering users to design 3D
rooms effortlessly without specialized knowledge.
‚àóWork performed during an internship at Meta GenAI.
1. Introduction
Generating high-quality mesh representations of 3D scenes
is essential for creating immersive experiences in AR/VR
applications. However, manually synthesizing these 3D en-
vironments is a complex task that requires expertise in 3D
modeling software [1, 17], posing a substantial barrier for
end-users looking to create personalized scenes. Fueled by
the recent progress in 2D generative models [13, 19, 39, 45],
an emerging line of work aims to simplify the creation pro-
cess of 3D objects [9, 26, 48, 52, 53] or even room-scale
scenes [10, 15, 20, 57].
Among them, the recent work
of Text2Room [20] creates compelling textured 3D room-
scale meshes using a text prompt describing the scene and
an off-the-shelf text-to-image model [39]. This is achieved
by iteratively inpainting missing 2D regions of the rendered
3D mesh from a novel viewpoint. Subsequently, depth is
estimated for the new content, followed by backprojection
and fusion with the current 3D mesh. Despite Text2Room‚Äôs
ability to generate locally convincing meshes with detailed
textures, it suffers from two crucial shortcomings due to its
iterative mesh inpainting strategy solely based on local con-
text: First, Text2Room does not follow conventional room
layouts, yielding unrealistic spatial arrangements of objects
and walls. Secondly, it tends to duplicate the most promi-
1

nent objects of a scene type, e.g., placing several beds into a
single bedroom, leading to semantically implausible scenes.
To address these shortcomings, we propose Control-
Room3D, which generates diverse and globally plausible
3D room meshes. A central element of our method is the
3D semantic proxy room (Fig. 1,
) which allows the user
to specify semantic bounding boxes and textual descriptions
of the desired room style that then serve as a rough outline
for the generated room layout. Our key insight is that this
3D representation, when rendered to 2D, provides valuable
geometric and semantic information. This information can
be used to control powerful 2D generative models, enabling
the creation of 3D consistent textures and geometry that
align with the proxy room. However, projecting 3D con-
trols to 2D comes with a loss of information which needs to
be properly addressed. As naively generating 2D views in-
dividually is detrimental for scene style consistency, we first
present a novel panorama generation method, which gener-
ates a comprehensive 360‚ó¶view of the room. Leveraging
homographic constraints between equirectangular patches,
we equip a latent diffusion model with correspondence-
aware attention modules [49] which establish a coherent
and seamless style, while faithfully adhering to the pro-
jected semantics of the user-provided 3D proxy room. Next,
the quality of depth estimation becomes pivotal for con-
verting this panorama into a 3D representation that aligns
with the proxy room. Building upon our insight that the 3D
proxy room offers not only semantic but also valuable ge-
ometric information, we introduce a novel geometry align-
ment approach which leverages the spatial dimensions of
3D bounding boxes in an iterative optimization strategy to
support a metric depth estimation network [6] to generate
3D structures which align with the proxy room. Thirdly,
while panoramic views are valuable, they are insufficient
for a complete 3D scene. To address issues like occlusion
and low-resolution textures, our method first eliminates low
density regions, then uses new viewpoints for inpainting,
guided by the proxy room and our geometry alignment, thus
integrating new geometry into the mesh seamlessly.
To summarize, our contributions are: (1) We propose
ControlRoom3D which generates diverse and semantically
plausible 3D room meshes aligning well with the user-
defined room layout and textual description of the room
style. (2) Leveraging our key insight that this room layout
provides both valuable semantic and geometric priors, we
present technical components including the semantic proxy
room, guided panorama generation, mesh completion, and
geometry alignment to enable coherent and seamless 3D
textures and geometry. (3) Using both quantitative and qual-
itative metrics, we validate the effectiveness of each compo-
nent introduced and show that our method generates more
plausible 3D rooms compared to competing methods.
2. Related Work
Text-to-3D. Fueled by the availability of large-scale vision-
text datasets [21, 31, 42‚Äì44, 51, 56] and powerful vision-
language models [34], recent advancements in 2D diffu-
sion models [5, 11, 30, 35, 39, 40] have significantly con-
tributed to the ongoing progress in generating 3D objects
and scenes. In particular, one prominent research direction
focuses on single object synthesis [26, 29, 33, 48, 53, 57],
wherein a differentiable 3D representation [22, 28] is com-
monly optimized through a loss signal derived from the de-
noising of rendered views [33, 52, 53].
Despite achiev-
ing impressive results in the synthesis of object-centric
scenes, the challenge of generalizing these techniques to
large and complex scenes persists [53]. Typically, existing
approaches rely on additional 3D data sources [3, 4, 12],
which are challenging to obtain at scale.
Two notable
approaches related to our work include SceneScape [15]
and Text2Room [20].
These methods leverage off-the-
shelf text-conditioned inpainting models [39] in combi-
nation with state-of-the-art monodepth estimation mod-
els [2, 36] to create a 3D mesh representation. Alternatively,
LDM3D [47] directly predicts an RGBD frame based on a
given text prompt. However, these approaches have lim-
ited control over the generation process by only being able
to change the text prompt at each inpainting step, typically
leading to globally implausible room layouts. In contrast,
ControlRoom3D allows precise control over the scene com-
position by using globally consistent 2D maps derived from
3D semantic proxy rooms to guide the generation process.
Semantically Guided 3D Scene Generation. An emerging
line of works investigates scene generation with semantic
compositing [8, 14, 16, 25, 39, 54, 55]. However, in 3D they
are typically constrained to low resolution textures and par-
tial room settings to enable inward-facing views [10, 32, 57]
or require synthetic 3D datasets and thus are restricted to
the room style present in the dataset [3]. At the same time,
current diffusion models show great capabilities to be ex-
tended by control signals such as pixel-perfect maps, key-
points or bounding boxes [8, 25, 54, 55, 58]. We follow this
line of work and show that 2D control signals derived from
a 3D representation can be used to produce 3D consistent
high-resolution images of large diversity without the need
of inward-facing views.
Mesh Texturization. A related line of work deals with re-
texturizing a given mesh representation of 3D scenes [46,
49] or objects [27] using a stylization text prompt. Unlike
ControlRoom3D, these methods focus on re-texturizing the
mesh in a novel style without (or only minor adaptations)
to the original geometry while our approach not only gener-
ates stylized textures but also creates novel geometry from
scratch which aligns with the style of the texture.
2

Panorama
¬† Generation
2D Proxy Renderings
Iterative Mesh Reconstruction
Poisson Reconstruction
Mesh Cleaning
Superimposed Depth Alignment
Mono Depth
Estimator
Completion
RGB
Depth Alignment
Mono Depth
Estimator
Partial RGB / Mask Rendering
Init
Depth
Panorama
3D Semantic Proxy Room
"Rustic farmhouse living room"
Semantic
Inpainting
Fuse
2D Proxy Renderings
Figure 2. Method Overview. The 3D semantic proxy room
contains a user-defined 3D room layout based on semantic bounding boxes
and a textual description of the overall room style. From this representation, we derive control signals that ensure 3D consistent textures
and align corresponding depth estimates with the room layout. The panorama generation module
generates a comprehensive 360‚ó¶view
of the room, the depth of which is then estimated and aligned with the proxy room
. After Poisson reconstruction and artifact cleaning
,
we update the room mesh by iteratively completing partial renderings of the scene
and estimating the depth of newly added textures
.
3. Method
Overview. (Fig. 2) Given a textual description of the over-
all room style and a rough 3D room layout based on 3D
semantic bounding boxes, ControlRoom3D creates diverse
and globally plausible 3D room meshes which align well
with the room layouts. The key technical components of
our method comprise the proxy room
, from which we
derive 2D semantic and guidance depth maps for arbitrary
viewpoints in the scene; the geometry alignment module
,
to align newly generated 2D views to the proxy room; the
panorama generation module
, to initially generate a glob-
ally style-consistent 360‚ó¶view of the scene; and a final
completion module
, which samples novel camera view-
points to inpaint occluded and unobserved regions in the
mesh with high-resolution textures. Details of each of these
steps are discussed in the following sections, and we high-
light each component with its corresponding color of Fig. 2.
3D Semantic Proxy Room.
(Fig. 2,
) At the core of
our method lies the user-defined 3D semantic proxy room,
which acts as a rough outline for the room layout and is
defined by semantic bounding boxes and a textual descrip-
tion of the desired room style.
Specifically, we define
a proxy room M
as a set of semantic bounding boxes
B = (p, s, c, i), where p ‚ààR3 is the box center, s ‚ààR3
the box size, c the semantic class id, and i the unique in-
stance id associated with each box. Our key insight is that
(a) Semantic Map
(b) Instance Map
(c) Near Depth
(d) Far Depth
3D Semantic Proxy Room
"Ancient Roman Living Room"
Figure 3. Proxy Room Renderings. We use classical rasterization
to render semantic segmentation and instance maps as well as near
and far depth maps from the proxy room.
this 3D representation, when rendered to 2D, provides valu-
able geometric and semantic information that can be used
to control powerful 2D generative models, enabling the cre-
ation of 3D consistent textures and geometry that align with
the proxy room. We render the proxy scene from a given
viewpoint V , producing multiple types of frames:
S, I, Dn, Df = r(M , V )
Here, r(¬∑) is standard rasterization without shading, the out-
puts S‚àà[1, C]H√óW , I‚ààN H√óW , Dn, Df‚ààRH√óW
+
are the
rendered 2D semantic segmentation map of C classes, the
instance segmentation map, and the near and far guidance
3

LDM
Semantic
Adapter
Depth
Adapter
shared
¬† Semantics
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Guidance Near Depth¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†
CAA
CAA
Spherical Projections with Pyramid Blending
Equirectangular Projection
shared
Figure 4. Guided Panorama Generation. We generate 360‚ó¶
panoramas guided by 3D semantic bounding boxes rendered
from the proxy room. To this end, we equip an LDM
with
correspondence-aware attention modules (CAA) [49] and seman-
tic
and depth
adapters finetuned on semantic bounding boxes.
depth map, respectively (Fig. 3). In the following, we detail
how these rendered maps are utilized in our components to
achieve room mesh generation that aligns both semantically
and geometrically with the proxy room.
Guided Panorama Generation.
(Fig. 2,
and Fig. 4)
We observe that naively generating 2D views individually or
only relying on iterative inpainting is detrimental for scene
style consistency as the overall style tends to drift with each
generated image and inaccurate loop closure [49]. Instead,
we jointly create a comprehensive 360‚ó¶view of the room
that establishes a coherent and seamless style. This pro-
cess is guided by the 3D semantic bounding boxes pro-
jected from the proxy room (Fig. 3a,c). To harness read-
ily available powerful 2D generation models trained on per-
spective images, we first divide the semantic panorama into
8 views, each encompassing a 90‚ó¶FOV and a 45‚ó¶hori-
zontal rotation using equirectangular projections, providing
pixel-perfect correspondences between adjacent frames de-
fined by 3√ó3 homography matrices. We then equip a latent
diffusion model (LDM) with 3 types of modules. Firstly,
correspondence-aware attention modules [49] use homo-
graphic correspondences between adjacent frames.
Sec-
ondly, a semantic adapter guides the generation towards the
specified room layout, while thirdly, the depth adapter con-
Proxy Room
Far Depth
Z-Buffer
Normal
Calc.
RGB Input
Pretrained
MDE
Near Depth
SAM Masks
Normal
Calc.
Figure 5. Geometry Alignment. We align generated 2D textures
with the proxy room
. To achieve this, we derive both near and
far depth maps by rendering from an identically posed camera in-
side the proxy room and align the generated objects (extracted with
SAM [23]) accordingly by fine-tuning a pre-trained depth estima-
tion network. Additionally, we preserve the overall object shape
by ensuring the integrity of the normals during optimization.
ditions the generation on the guidance depth map. We em-
ploy pyramid blending to create the final panoramic image
from the spherical projections of the generated images [49].
Adapter Fine-tuning on HyperSim Bounding Boxes.
Off-the-shelf semantic adapters gain strong semantic under-
standing through training on large datasets, but they are typ-
ically trained on pixel-perfect semantic masks. This stands
in stark contrast to our control input, which involves 3D
bounding boxes rendered in 2D views that only provide a
rough outline of the object (Fig. 3a). To bridge this gap, we
construct a dataset derived from HyperSim [38], containing
photo-realistic renderings of indoor environments with 3D
semantic bounding box annotations. We render 3D bound-
ing boxes for each scene and fine-tune the semantic and
depth adapters on this dataset. Additional details can be
found in our supplementary material.
Geometry Alignment. (Fig. 2,
and Fig. 5) After gener-
ating panoramas, the next step is to transform these 2D tex-
tures into a 3D representation by predicting per-pixel depth.
Naively estimating depth using a standard depth estimator
can yield subpar results due to scale ambiguity. However,
the proxy room offers valuable geometric information, in-
cluding the near and far guidance depth map (Fig. 3c,d). A
fundamental aspect of ControlRoom3D is to align the pre-
dicted depth with this geometric prior. Specifically, we opti-
mize the depth predicted by a pre-trained metric depth esti-
mator (MDE) [6] to fit within its respective bounding boxes
while preserving the object‚Äôs shape (Fig. 5). To achieve this,
we introduce a geometry alignment loss:
Ld =
(
0
if Dn ‚â§ÀÜd ‚â§Df,
min

‚à•ÀÜd ‚àíDn‚à•1, ‚à•ÀÜd ‚àíDf‚à•1

otherwise.
4

(a) Poisson Surface Reconstruction¬†
(b) Cleaned Mesh
Figure 6. Mesh Cleaning. After obtaining a watertight mesh with
Poisson surface reconstruction (left), we delete vertices whose
densities fall below a threshold (density is depicted with darker
color shades). After removing small connected components, we
obtain a cleaned mesh ready to be inpainted with novel content
from favourable viewpoints (right).
Here, ÀÜd is the predicted depth, and Dn, Df denote the near
and far depth values. Note that for walls, ceiling, and floor
Dn=Df. This loss penalizes depth predictions that fall out-
side the designated bounding box with an L1 loss. Addi-
tionally, we employ SAM [23] to extract pixel-perfect in-
stance masks for each generated instance by prompting it
with the bounding box extracted from the instance maps I
(Fig. 3b). For pixels within the rendered bounding box but
outside the SAM mask, we set Dn to Df. During test time,
we optimize a pre-trained MDE using this loss, thereby
aligning the depth with the proxy room while retaining the
depth estimator‚Äôs strong prior learned from large datasets.
While the depth alignment loss Ld aligns the frame with the
proxy room, it may alter the object‚Äôs surface significantly
while fitting it into its bounding box. To mitigate this, we
present a normal preservation loss: Ln = ‚à•ÀÜn ‚àín0‚à•1 where
ÀÜn are the estimated normals from the current depth predic-
tion ÀÜd and n0 are the normals from the original MDE.
Once we have predicted depth values that align with the
proxy room, our next step is to convert the 2D textures into a
3D representation. However, a straightforward unprojection
using the depth values can result in visible seams in over-
lapping regions, because the depth values for each frame are
individually optimized. To address this challenge, we first
convert the z-depth values into distance values, as they re-
main invariant to rotations. We then combine all frames into
a panoramic view using spherical projections scaled by their
corresponding distance values followed by pyramid blend-
ing, ultimately yielding the initial 3D representation.
Mesh Cleaning.
(Fig. 2,
and Fig. 6) While RGBD
panoramas offer an initial 3D representation, they are sus-
ceptible to issues like noisy depth estimates and partially
low-resolution textures, particularly in areas that are oc-
cluded or where mesh faces are stretched. To address these
challenges, our approach begins by identifying and filtering
out regions of low quality in the mesh. We then synthesize
Erosion & Dilation
Proxy Semantics¬† ¬† ¬†
Proxy Depth¬† ¬† ¬†¬†
Rendered View¬† ¬† ¬†¬†
Inpainted View
Telea Inpainting
Figure 7. Completion Pipeline. After inpainting small holes us-
ing classical Telea inpainting, we employ a 2D LDM
guided
by a semantic
and depth
adapter for larger missing areas, ef-
fectively inpainting these areas while aligning them to the control
signals from the proxy room M .
new content for these regions from a more favorable view-
ing angle. To this end, we first perform Poisson surface re-
construction, converting the 3D point cloud obtained from
the RGBD panorama into a watertight mesh representation
M
(Fig. 6a). We proceed to remove mesh vertices and
their adjacent faces if their density falls below a specified
threshold Œ¥v. Finally, we eliminate floating artifacts that are
disconnected from the main mesh by deleting components
with a vertex count below a specified threshold Œ¥c (Fig. 6b).
Mesh Completion. (Fig. 2,
and Fig. 7) After Poisson
reconstruction and artifact removal, we further refine the
room mesh by iteratively inpainting the scene from vari-
ous favorable viewpoints V , effectively filling missing ar-
eas in the mesh with novel content. To this end, we render
the current 3D mesh M
with classical rasterization r(¬∑)
to obtain the RGB frame F, depth map D
as well as the
image mask M that identifies pixels without observed con-
tent: F, D , M = r(M , V ). In order to unleash the full
potential of powerful 2D generation models, which perform
better with larger connected regions, we first inpaint small
gaps using traditional Telea inpainting [15, 20, 50]. We then
expand the remaining holes through a sequence of morpho-
logical operations, specifically erosion followed by dilation.
This modified mask, combined with the rendered 2D se-
mantic S and guidance depth map Dn from the proxy room,
guides the inpainting of the missing areas using a semantic
and depth adapter, yielding the final generated RGB frame.
We integrate this new content into the mesh using our geom-
etry alignment module (Fig. 2,
). For seamless integration
with pre-existing structures, we superimpose the rendered
depth D on the near and far guidance depth map Dn, Df
to optimize the depth of novel content to correspond to al-
ready generated 3D structures, i.e., Di,j
n = Di,j
f
= Di,j for
M i,j = 0. We repeat this process for a number of selected
viewpoints, effectively inpainting any missing region while
seamlessly integrating novel content.
5

4. Experiments
Evaluation Setup and Metrics. We use distinct room lay-
outs encompassing typical indoor rooms such as bedroom
or kitchen but also unique settings such as wellness spa
with an indoor pool as targeted room for evaluation. For
text prompting, we define 12 room styles, spanning a di-
verse range of geographical (e.g., Japanese, Scandinavian),
seasonal (e.g., Halloween, Christmas), historical (e.g., An-
cient Roman, Stone-age cave), and artistic (e.g., cyberpunk,
industrial) themes.
We ask 12 participants to rate these
scenes on a scale of 1‚Äì5 with respect to the following di-
mensions: 3D structure completeness (3DS), proxy align-
ment to the 3D guidance (PA), generated room layout plau-
sibility (LP), and overall perceptual quality based on user
preference (PQ). We render 25sec long videos with 650
frames each for each generated room showing all objects
from multiple (challenging) angles. We let users rate each
scene individually (no head-to-head comparison). In total,
we collected 1120 data points using 175 scenes from the
user study. We provide further details about the user study
in the supplementary material. Moreover, we calculate the
CLIP score (CS) [18] and Inception score (IS) [41] on 100
RGB renderings from random viewpoints for each scene.
Implementation Details.
We use a pre-trained LDM [39]
as well as semantic and depth adapter modules. We finetune
both the adapter weights on our 3D bounding box dataset
derived from the HyperSim [38] dataset using BLIPv2 cap-
tions [24].
The correspondence-aware attention module
(CAA) is trained on the Matterport3D [7] dataset. Our met-
ric depth estimator is a pre-trained ZoeDepth [6] model. We
use PyTorch3D [37] for mesh rasterization and fusion for
the 3D proxy room as well as the final generated mesh.
4.1. Comparison with State-of-the-Art Methods
Baselines.
There are no directly comparable methods
which generate 3D textured meshes of entire rooms given
a 3D room layout and a textual style description.
We
thus adapt top-performing related methods for this task.
Text2Room [20] uses a text-conditioned inpainting model
followed by mono-depth inpainting [2].
Following their
original instructions, we enforce a room layout by chang-
ing the text prompt during the camera trajectory to reflect
which objects are visible in the current frame. MVDiffu-
sion [49] creates panorama images by enforcing correspon-
dences between overlapping image crops using homogra-
phy constraints. Here, for each image crop we adjust the
text condition to reflect which objects should be visible and
use the same off-the-shelf mono-depth estimator [6] and
poisson mesh reconstruction as our method.
Results.
In our study, we provide a detailed quantitative
comparison with top-performing methods in Tab. 1 and il-
lustrate qualitative examples in Fig. 8. A critical finding is
Table 1.
Quantitative Comparison.
We report 2D metrics
and user study results, including: CLIP Score (CS), Inception
Score (IS), 3D Structure Completeness (3DS), Layout Plausibil-
ity (LP) and Perceptual Quality (PQ).
Method
2D Metrics
User Study (3D Mesh)
CS ‚Üë
IS ‚Üë
3DS ‚Üë
LP ‚Üë
PQ ‚Üë
Text2Room [20]
24.0
6.47
1.71
2.12
2.33
MVDiffusion [49]
25.6
6.37
2.48
3.01
2.89
ControlRoom3D (Ours)
28.4
5.75
4.19
4.54
4.07
Table 2. Ablation Study. (Fig. 2) We ablate the semantic proxy
room (PR), geometry alignment module (GA), panorama genera-
tion (PG) and mesh completion (MC).
PR
GA
PG
MC
2D Metrics
User Study (3D Mesh)
CS ‚Üë
IS ‚Üë
3DS ‚Üë
PA ‚Üë
PQ ‚Üë
‚úó
‚úó
‚úó
‚úó
27.0
5.76
1.78
1.14
2.33
‚úì
‚úó
‚úó
‚úó
27.6
5.60
2.81
2.02
2.95
‚úì
‚úì
‚úó
‚úó
27.8
5.64
3.34
4.34
3.45
‚úì
‚úì
‚úì
‚úó
28.4
5.90
3.68
4.55
3.95
‚úì
‚úì
‚úì
‚úì
28.4
5.75
4.22
4.76
4.23
that text-only scene conditioning is inadequate for generat-
ing realistic room layouts. Instead, our method integrates
2D semantic and geometric controls based on a 3D proxy
room
leading to more plausible layouts with an improve-
ment of +1.5 LP. For instance, Text2Room frequently but
incorrectly places multiple instances of the most prominent
object of a scene type in the room, such as 4 beds in a single
bedroom (Fig. 8, bottom), resulting in unrealistic layouts. In
contrast, our approach accurately reflects the intended lay-
out, marked by colored bounding boxes on the renderings.
This accuracy enables more efficient completion of missing
areas reducing artifacts (+1.7 3DS) and enhancing overall
perceptual quality (+1.2 PQ) while faithfully following the
text conditioning (+2.8 CS). The inception score (IS) tends
to favor methods that generate diverse scenes, even if they
do not necessarily represent plausible room layouts (LP).
Nonetheless, our method displays versatility in creating
a wide range of scene types and layouts, as shown in Fig. 8
and supplementary videos.
4.2. Analysis Experiments
The key technical components of our methods comprise
the proxy room
, the geometry alignment module
, the
panorama generation
, and the final mesh completion
.
In Tab. 2, we evaluate the influence of each component.
In the supplementary material, we provide further results
about the adapter fine-tuning on HyperSim as well as visu-
alizations of baseline methods of the ablation study.
6

Text2Room¬† [20]
MVDiffusion¬† [49]
ControlRoom3D (Ours)
Ancient Roman Bathroom
Golden Wellness Spa
Futuristic Living Room
Rustic Farmhouse Bedroom
Rendered View (+ Geometry)
Rendered View
Scene Layout
Modern Scandinavian Kitchen
Magical Winter Children's Room
texture artifacts
geometry implausible
geometry incomplete
1
2
3
4
unobserved areas
Figure 8. Qualitative Comparison with Baseline Methods. We show colored geometry renderings of our method and two baseline
methods. ControlRoom3D generates convincing geometries and textures given a user-defined room layout and a textual style description.
We provide further example images and videos in the supplementary material.
7

Figure 9. Fine-tuning with rendered bounding boxes from HyperSim. Prior to fine-tuning with HyperSim data, ControlRoom3D tends
to create box-shaped objects and struggles to accurately generate objects within their respective bounding boxes (a), whereas fine-tuning
enables ControlRoom3D to fill 3D bounding boxes with appropriate content (b).
+ 2D Guidance
only text prompt
(a) Iterative Inpainting
(b) Panorama Generation
1
2
3
style consistent¬† ¬† ¬† plausible layout
style consistent¬† ¬† ¬† plausible layout
style consistent¬† ¬† ¬† plausible layout
style consistent¬† ¬† ¬† plausible layout
Guided Panorama Generation (Ours)
‚úó
‚úì
‚úó
‚úó
‚úì
‚úó
‚úì
‚úì
Figure 10. 2D Guided Panorama Initialization. We find that only using text prompts to generate panoramas result in subpar layouts,
particularly, iterative inpainting repeats the most prominent object in the scene (top, left). Adding 2D guidance with semantic and depth
maps, results in more plausible room layouts, whereas panorama generation yields a better style consistency (bottom, right).
Adapter Fine-tuning on HyperSim Bounding Boxes.
Fig. 9 presents a side-by-side qualitative evaluation of Con-
trolRoom3D, illustrating its performance with and without
fine-tuning on our 3D bounding box dataset derived from
HyperSim.
A critical observation is that when Control-
Room3D uses adapters only trained on datasets of pixel-
precise masks, it faces challenges in generating objects ac-
curately within their designated bounding boxes. Instead,
it tends to create box-shaped objects that entirely fill the
bounding boxes (Fig. 9a).
However, when the adapters
are fine-tuned using our dataset derived from HyperSim,
ControlRoom3D demonstrates an improved ability to gen-
erate objects that appropriately fit within their correspond-
ing bounding boxes (Fig. 9b).
Consistent Style Panorama Generation.
The partici-
pants in our user study expressed a strong preference for
the panorama generation module
, noting its superiority
across all three metrics compared to the incremental in-
painting approach. Incremental inpainting involves warping
generated images to subsequent ones and autoregressively
filling in unseen areas. Since this inpainting is solely based
on local context, it leads to gradual changes in style and
challenges in achieving accurate loop closure [49] (Fig. 10a,
white box). In contrast, our panorama generation module
creates a complete 360‚ó¶panorama, conditioned on both se-
mantic and geometric 3D information in a unified manner.
This approach ensures a consistent style throughout the en-
tire scene, effectively addressing the issues associated with
incremental inpainting and resulting in a more cohesive and
visually appealing overall scene (Fig. 10b, bottom).
Geometry Alignment with 3D Boxes. Estimating the ex-
act scale of a 3D scene from a single RGB image is an
ill-posed task, often leading to severe inaccuracies in even
the most advanced metric depth estimators [6]. As shown
in Fig. 11, without iterative alignment, the estimated depth
would lead to implausible geometry (t=1). By iteratively
optimizing the 3D representation with additional geometric
information of 3D bounding boxes of the proxy room lay-
out, the final geometry is more plausible (t=600). The ge-
ometry alignment module is the core ingredient to achieve
strong alignment with the proxy room, as evident by the
substantial improvement of +2.32 PA in Tab. 2.
In Fig. 12, we present an additional qualitative ablation
study focusing on the use of SAM masks and the normal
preservation loss, both integral components of the depth
alignment module. We leverage SAM [23] to obtain pixel-
precise instance masks for each generated object. For pix-
els located within the rendered bounding box but outside the
SAM mask, we assign the depth value Dn to Df. As shown
in Fig. 12 (top), including SAM masks leads to sharper 3D
8

Figure 11. Geometry Alignment. Scale ambiguity leads to sig-
nificant inaccuracies in MDEs ( ).
In contrast, our proposed
depth alignment module optimizes the alignment loss L to achieve
strong alignment with the proxy room ( ).
object boundaries, resulting in a more seamless integration
into the 3D room mesh. Although the depth alignment loss
Ld effectively aligns the frame with the 3D proxy room, it
may occasionally distort the surface of objects to fit them
within their bounding boxes. To counter this, we introduce
the normal preservation loss Ln, retaining the original shape
of the objects (Fig. 12, bottom).
High Quality Generation with Mesh Completion. Once
the geometry alignment module has aligned the generated
geometry with the layout, we can render control signals
from the proxy room, including semantic, instance, and
guidance depth maps, for selected new viewpoints that cor-
respond with the created 3D mesh. This process enables
us to fill in the missing parts of the mesh which remained
unobserved after the panorama generation with localized in-
painting, and incorporate new content into the 3D structure,
guided by these aligned control signals. This yields notable
enhancement in structural completeness (+0.54 3DS), cul-
minating in the highest perceptual quality (4.23 PQ).
5. Conclusion
In this paper, we proposed ControlRoom3D, a novel method
to generate high-quality 3D room meshes given a textual
description of the room style and a user-defined room lay-
out outlined by 3D semantic bounding boxes. When ren-
dered to 2D, the geometric and semantic information is used
to guide generative models to create texture and geometry
aligned with the proxy room‚Äôs layout. Naive guidance does
not produce plausible meshes, thus, we introduced guided
panorama generation and geometry alignment modules to
ensure consistent style and plausible geometry. Finally, we
introduced mesh post-processing and completion methods,
improving the final generation quality substantially. We be-
lieve our method marks a significant step forward in en-
abling the creation of large-scale 3D assets through user-
defined control signals, greatly simplifying the process for
end-users without any expert knowledge in 3D modeling
software to design personalized scenes.
with SAM masks
without SAM masks
with normal loss
without normal loss
Figure 12. Ablation on Depth Alignment. Leveraging pixel-
perfect SAM masks yields crisper 3D object boundaries (top).
While the depth alignment loss aligns the frame with the 3D proxy
room, it can distort the object‚Äôs surface as it fits the depth within
its bounding box. Addressing this issue, we introduce our normal
preservation loss to maintain the object‚Äôs original shape (bottom).
Acknowledgments. We would like to thank Xiaoliang Dai,
Jia-Bin Huang, Jeff Liang and Matthew Yu for helpful feed-
back and discussions.
9

References
[1] Autodesk, Inc. Maya, 2019. 1
[2] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Iron-
Depth: Iterative Refinement of Single-View Depth using
Surface Normal and its Uncertainty.
In BMVC, 2022.
2,
6
[3] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou,
Xingguang Yan, Gordon Wetzstein, Leonidas Guibas, and
Andrea Tagliasacchi. Cc3d: Layout-conditioned generation
of compositional 3d scenes. In ICCV, 2023. 2
[4] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-
ter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent
Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin
Dehghan, and Josh Susskind. GAUDI: A Neural Architect
for Immersive 3D Scene Generation. In NeurIPS, 2022. 2
[5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,
Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,
Yunxin Jiao, and Aditya Ramesh. Improving Image Genera-
tion with Better Captions. OpenAI, 2023. 2
[6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,
and Matthias M¬®uller. ZoeDepth: Zero-shot Transfer by Com-
bining Relative and Metric Depth. arXiv:2302.12288, 2023.
2, 4, 6, 8
[7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-
D Data in Indoor Environments. In 3DV, 2017. 6
[8] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-
Free
Layout
Control
with
Cross-Attention
Guidance.
arXiv:2304.03373, 2023. 2
[9] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,
Gang Yu, Lei Yang, and Guosheng Lin.
IT3D: Im-
proved Text-to-3D Generation with Explicit View Synthesis.
arXiv:2308.11473, 2023. 1
[10] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes,
and Daniel Cohen-Or. Set-the-Scene: Global-Local Training
for Generating Controllable NeRF Scenes. In ICCVW, 2023.
1, 2
[11] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang
Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-
aofang Wang, Abhimanyu Dubey, et al.
Emu: Enhanc-
ing image generation models using photogenic needles in a
haystack. arXiv:2309.15807, 2023. 2
[12] Terrance DeVries, Miguel Angel Bautista, Nitish Srivas-
tava, Graham W. Taylor, and Joshua M. Susskind. Uncon-
strained Scene Generation with Locally Conditioned Radi-
ance Fields. In ICCV, 2021. 2
[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. In NeurIPS, 2021. 1
[14] Patrick Esser, Robin Rombach, and Bj¬®orn Ommer.
Tam-
ing Transformers for High-Resolution Image Synthesis. In
CVPR, 2021. 2
[15] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali
Dekel. SceneScape: Text-Driven Consistent Scene Gener-
ation. arXiv:2302.01133, 2023. 1, 2, 5
[16] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman.
Make-a-scene: Scene-
based text-to-image generation with human priors. In ECCV,
2022. 2
[17] Roland Hess. Blender Foundations: The Essential Guide to
Learning Blender 2.6. Focal Press, 2010. 1
[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. In EMNLP, 2021. 6
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-
fusion Probabilistic Models. In NeurIPS, 2020. 1
[20] Lukas H¬®ollein, Ang Cao, Andrew Owens, Justin Johnson,
and Matthias Nie√üner. Text2Room: Extracting Textured 3D
Meshes from 2D Text-to-Image Models. In ICCV, 2023. 1,
2, 5, 6, 12
[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML, 2021. 2
[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¬®uhler,
and George Drettakis. 3D Gaussian Splatting for Real-Time
Radiance Field Rendering. In ACM Transactions on Graph-
ics, 2023. 2
[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll¬¥ar, and
Ross Girshick. Segment Anything. arXiv:2304.02643, 2023.
4, 5, 8
[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 6
[25] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
GLIGEN: Open-Set Grounded Text-to-Image Generation.
CVPR, 2023. 2
[26] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution
Text-to-3D Content Creation. In CVPR, 2023. 1, 2
[27] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and
Rana Hanocka. Text2Mesh: Text-Driven Neural Stylization
for Meshes. In CVPR, 2022. 2
[28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing Scenes as Neural Radiance Fields for View
Synthesis. In ECCV, 2020. 2
[29] Norman
M¬®uller,
Yawar
Siddiqui,
Lorenzo
Porzi,
Samuel Rota Bulo,
Peter Kontschieder,
and Matthias
Nie√üner.
Diffrf:
Rendering-guided 3d radiance field
diffusion. In CVPR, 2023. 2
[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image genera-
tion and editing with text-guided diffusion models. In ICML,
2021. 2
10

[31] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi,
Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen,
Minh-Thang Luong, Yonghui Wu, et al. Combined scaling
for zero-shot transfer learning. Neurocomputing, 2023. 2
[32] Ryan Po and Gordon Wetzstein.
Compositional 3D
Scene Generation using Locally Conditioned Diffusion.
arXiv:2303.12218, 2023. 2
[33] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. DreamFusion: Text-to-3D using 2D Diffusion. In ICLR,
2022. 2
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 2
[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv:2204.06125, 2022. 2
[36] Ren¬¥e Ranftl,
Katrin Lasinger,
David Hafner,
Konrad
Schindler, and Vladlen Koltun. Towards Robust Monocu-
lar Depth Estimation: Mixing Datasets for Zero-shot Cross-
dataset Transfer. IEEE TPAMI, 2020. 2
[37] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-
lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia
Gkioxari. Accelerating 3D Deep Learning with PyTorch3D.
arXiv:2007.08501, 2020. 6
[38] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,
and Joshua M. Susskind. Hypersim: A Photorealistic Syn-
thetic Dataset for Holistic Indoor Scene Understanding. In
ICCV, 2021. 4, 6, 12
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 1, 2, 6
[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. In NeurIPS, 2022. 2
[41] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved
Techniques for Training GANs. In NeurIPS, 2016. 6
[42] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv:2111.02114, 2021. 2
[43] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon,
Ross Wightman,
Mehdi Cherti,
Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. In NeurIPS, 2022.
[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual Captions: A Cleaned, Hypernymed, Im-
age Alt-text Dataset For Automatic Image Captioning. In
ACL, 2018. 2
[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep Unsupervised Learning using
Nonequilibrium Thermodynamics. In ICML, 2015. 1
[46] Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang,
Junsong Tang, Feng Yuan, and Yang Zhao. RoomDreamer:
Text-Driven 3D Indoor Scene Synthesis with Coherent Ge-
ometry and Texture. arXiv:2305.11337, 2023. 2
[47] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex
Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen
Tseng, Fabio Nonato, Matthias Muller, and Vasudev Lal.
LDM3D: Latent Diffusion Model for 3D. arXiv:2305.10853,
2023. 2
[48] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang
Zeng. DreamGaussian: Generative Gaussian Splatting for
Efficient 3D Content Creation. arXiv:2309.16653, 2023. 1,
2
[49] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and
Yasutaka Furukawa. MVDiffusion: Enabling Holistic Multi-
view Image Generation with Correspondence-Aware Diffu-
sion. In NeurIPS, 2023. 2, 4, 6, 8, 12
[50] Alexandru Telea. An image inpainting technique based on
the fast marching method. In Journal of graphics tools, 2004.
5
[51] Bart Thomee, David A Shamma, Gerald Friedland, Ben-
jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and
Li-Jia Li. YFCC100M: The new data in multimedia research.
In Communications of the ACM, 2016. 2
[52] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,
and Greg Shakhnarovich. Score Jacobian Chaining: Lift-
ing Pretrained 2D Diffusion Models for 3D Generation. In
CVPR, 2023. 1, 2
[53] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity
and Diverse Text-to-3D Generation with Variational Score
Distillation. In NeurIPS, 2023. 1, 2
[54] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu,
Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou.
BoxDiff: Text-to-Image Synthesis with Training-Free Box-
Constrained Diffusion. In ICCV, 2023. 2
[55] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin
Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael
Zeng, and Lijuan Wang. ReCo: Region-Controlled Text-to-
Image Generation. In CVPR, 2023. 2
[56] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,
Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,
Brian Karrer, Shelly Sheynin, et al.
Scaling autoregres-
sive multi-modal models: Pretraining and instruction tuning.
arXiv:2309.02591, 2023. 2
[57] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing
Liao. Text2NeRF: Text-Driven 3D Scene Generation with
Neural Radiance Fields. arXiv:2305.11588, 2023. 1, 2
[58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
Conditional Control to Text-to-Image Diffusion Models. In
ICCV, 2023. 2
11

ControlRoom3D: Room Generation using Semantic Proxy Rooms
Supplementary Material
In this supplementary material, we provide mode de-
tails of our technical components and experimental setup.
Specifically, we provide further details about our user study
and conclude with additional qualitative results. For a com-
prehensive understanding, we suggest that reviewers watch
our supplemental video, which includes detailed explana-
tions and showcases videos of 3D room meshes in a variety
of room types and styles.
6. Adapter Fine-tuning on HyperSim
Dataset Preparation and Implementation Details. The
HyperSim dataset [38] offers 2D rendered images that in-
clude camera positions and aligned 3D semantic bounding
boxes. Using these camera poses, we project the semantic
bounding boxes into 2D, creating depth maps and seman-
tic maps of the bounding boxes (see Fig. 13, top). How-
ever, we observed that some images from the HyperSim
dataset do not meet our quality requirements (Fig. 13, bot-
tom). Therefore, we exclude images where the camera roll
exceeds ¬±8.6‚ó¶, as our camera setup is typically parallel to
the ground. Additionally, we discard images where a single
semantic class covers more than half of the image area, as
this often suggests the camera is too close to objects in the
3D scene or inside a bounding box. Images with a maxi-
mum depth value over 15 meters are also removed, consid-
ering our focus on bounded indoor environments. For the
selected images, we calculate BLIPv2 captions. We fine-
tune the semantic and depth adapter independently on their
respective maps. We train for 5000 iterations on 8 A100
GPUs, with a batch size of 8 and a learning rate of 1e-7.
Qualitative Comparison.
Fig. 9 presents a side-by-
side qualitative evaluation of ControlRoom3D, illustrating
its performance with and without fine-tuning on our 3D
bounding box dataset derived from HyperSim.
A criti-
cal observation is that when ControlRoom3D uses adapters
only trained on datasets of pixel-precise masks, it faces
challenges in generating objects accurately within their
designated bounding boxes.
Instead, it tends to create
box-shaped objects that entirely fill the bounding boxes
(Fig. 9a). However, when the adapters are fine-tuned us-
ing our dataset derived from HyperSim, ControlRoom3D
demonstrates an improved ability to generate objects that
appropriately fit within their bounding boxes (Fig. 9b).
7. Details on User Study
We carried out two user studies in which we ask 12 partic-
ipants to rate 3D scenes on a scale of 1‚Äì5 with respect to
three qualitative dimensions. Firstly, we compare our work
RGB Rendering
Sem Bounding Boxes
Depth Maps
Camera Roll
Unbounded Scenes
Too close to camera
Included Samples
Filtered Samples
Figure 13. HyperSim Filtering. We show samples rendered from
the HyperSim dataset which meet our quality requirements (top).
Samples with camera roll, extreme far shots and close-ups are fil-
tered out to better match ControlRoom3D‚Äôs use case (bottom).
with related work, i.e., Text2Room [20] and an adaption of
MVDiffusion [49] for 3D room generation. We ask all par-
ticipants to rate each scene individually with respect to 3D
structure (3DS), i.e., ‚Äúis the 3D mesh complete and not dis-
torted?‚Äù, layout plausibility (LP), i.e., ‚Äúdoes the room layout
resemble a typical room layout of the specified type?‚Äù and
overall perceptual quality (PQ). We show the user study in-
terface in Fig. 14. Secondly, we conduct a user study for the
ablation study in which we evaluate the influence of each
technical component to our full model. Here, we replace
the question about layout plausibility with proxy alignment
(PA), i.e., ‚Äúare objects generated within their corresponding
bounding box?‚Äù, and additionally superimpose 3D bound-
ing boxes on the video to visualize the proxy room guid-
ance. An example of this is included in the supplementary
material.
12

Figure 14. User Study Interface. We render 25sec long videos for each generated room showing all objects from multiple (challenging)
angles. We ask participants to rate these scenes on a scale of 1‚Äì5 with respect to the following dimensions: 3D structure completeness
(3DS), proxy alignment to the 3D guidance (PA), generated room layout plausibility (LP), and overall perceptual quality (PQ).
Figure 15. Qualitative Ablation on the key components of our method. We notice that 2D guidance (c) plays a crucial role in creating a
plausible room layout, cf. (a) ‚Äì (b). Nevertheless, the scene still lacks geometric alignment with the proxy room (cf. transparent bounding
box overlays). Our depth alignment module accurately aligns the generated scene with the proxy room geometry (d). To address bad
reconstruction artifacts due to unobserved areas (bottom left), we employ our semantic completion stage to inpaint these missing regions.
This results in complete 3D rooms without blurred-out sections (bottom right).
8. Further Qualitative Results
Visualization of the Ablation Study. We provide an ad-
ditional qualitative study regarding the key technical com-
ponents of ControlRoom3D in Fig. 15 and show videos of
an exemplary 3D room of each ablation study experiment
in the supplementary material.
Additional qualitative results of our main method.
In
Fig. 16, we show additional qualitative examples of Con-
trolRoom3D. Moreover, we recommend that reviewers
watch our supplemental video explaining main technical
component of ControlRoom3D as well as qualitative videos
of 3D room meshes of various room types and styles.
13

Rendered View (+ Geometry)
Rendered View
Scene Layout
Rustic Farmhouse Kitchen
Vampire's Bedroom
Icy Winter Children's Bedroom
Pink Princess Bedroom
Dark Nebula Living Room
Industrial Style Bedroom
Figure 16. Further Qualitative Examples. We show colored geometry renderings of our method. ControlRoom3D generates convincing
geometries and textures given a user-defined room layout and textual style description. More qualitative videos can be found at the end of
the explanatory video in the supplementary material.
14

