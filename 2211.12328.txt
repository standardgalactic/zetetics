A survey on knowledge-enhanced multimodal learning
Maria Lymperaiou and Giorgos Stamou
Artiﬁcial Intelligence and Learning Systems Laboratory
School of Electrical and Computer Engineering
National Technical University of Athens
marialymp@islab.ntua.gr , gstam@cs.ntua.gr
Abstract
Multimodal learning has been a ﬁeld of increasing
interest, aiming to combine various modalities in a
single joint representation.
Especially in the area
of visiolinguistic (VL) learning multiple models and
techniques have been developed, targeting a variety
of tasks that involve images and text.
VL models
have reached unprecedented performances by extend-
ing the idea of Transformers, so that both modalities
can learn from each other. Massive pre-training pro-
cedures enable VL models to acquire a certain level
of real-world understanding, although many gaps can
be identiﬁed: the limited comprehension of common-
sense, factual, temporal and other everyday knowl-
edge aspects questions the extendability of VL tasks.
Knowledge graphs and other knowledge sources can
ﬁll those gaps by explicitly providing missing infor-
mation, unlocking novel capabilities of VL models. In
the same time, knowledge graphs enhance explainabil-
ity, fairness and validity of decision making, issues
of outermost importance for such complex implemen-
tations. The current survey aims to unify the ﬁelds
of VL representation learning and knowledge graphs,
and provides a taxonomy and analysis of knowledge-
enhanced VL models.
1
Introduction
Multimodal representation learning has been an area of
machine learning that increasingly draws the attention of
the research community. Combining information from
different modalities, such as images, and text, allows more
informative representations, as they provide complemen-
tary insights for the same instances. Several works focus
on using both vision and language modalities, introduc-
ing tasks such as visual question answering (Agrawal
et al., 2016), visual reasoning (He et al., 2021a), visual
commonsense reasoning (Zellers et al., 2019), visual en-
tailment (Xie et al., 2018), image captioning (Stefanini
et al., 2021), image-text retrieval and inversely text-image
retrieval (Dubey, 2021), referring expressions (Krishna
et al., 2018), visual explanations (Hendricks et al., 2016)
and grounding (Endo et al., 2017), visual-language nav-
igation (Anderson et al., 2018), visual generation from
text (Reed et al., 2016c), visual storytelling (Huang et al.,
2016b) and its inverse task of story visualization(Li et al.,
2019c), and visual dialog (El-Nouby et al., 2019).
Some of the ﬁrst attempts that combine vision and lan-
guage face several limitations due to the restricted capac-
ity of sequential models for language, such as recurrent
neural networks (RNNs), LSTMs (Hochreiter and Schmid-
huber, 1997) and GRUs (Cho et al., 2014), which struggle
to represent long textual sequences. The area of multi-
modal learning has faced signiﬁcant advancements espe-
cially since the introduction of the Transformer (Vaswani
et al., 2017). Several powerful transformer-based variants,
such as BERT (Devlin et al., 2019) and GPT-3 (Brown
et al., 2020), set the foundations for the surge of visi-
olinguistic (VL) transformers. The extension of single-
modality pre-training requires the introduction of visual
features, which allow to infer masked linguistic compo-
nents and vice versa, enabling learning cross-modality
relationships from aligned data. In the meantime, pre-
training tasks applied independently on the visual or lin-
guistic components permit intra-modality learning. Fine-
tuning on task-speciﬁc VL datasets, addressing the tasks
of text-image retrieval (Lin et al., 2014), visual question
answering (Ren et al., 2015; Agrawal et al., 2016; Hudson
and Manning, 2019; Krishna et al., 2016; Zhu et al., 2016;
Gao et al., 2019), visual reasoning (Suhr et al., 2017),
visual commonsense reasoning (Zellers et al., 2019) and
others, follows the pre-training stage. Models such as
LXMERT (Tan and Bansal, 2019), VisualBERT (Li et al.,
2019a), ViLBERT (Lu et al., 2019, 2020), VL-BERT (Su
et al., 2020), UNITER (Chen et al., 2020), OSCAR (Li
et al., 2020c), ViLT (Kim et al., 2021), CLIP (Radford
et al., 2021), SIMVLM (Wang et al., 2021c) and many oth-
ers have demonstrated state-of-the-art results in multiple
VL tasks.
As for recent transformer-based approaches, despite
pre-training on large amounts of aligned VL data, usually
from Conceptual Captions, (Sharma et al., 2018) COCO
(Ren et al., 2015) and Visual Genome (Krishna et al.,
2016) datasets, the learned concepts remain limited and
arXiv:2211.12328v2  [cs.LG]  6 Feb 2023

lack further explicit information regarding commonsense
knowledge, abstract entities or real-world events. Rele-
vant issues in the natural language processing ﬁeld were
addressed by leveraging knowledge graphs, thus result-
ing in knowledge-enhanced approaches for several tasks,
such as Language Modeling (Logan et al., 2019; Lu et al.,
2021), Natural Language Inference (NLI) (Chen et al.,
2018b,a), Language Generation (Yu et al., 2021b), Dialog
Generation (Cui et al., 2021a), Entity Disambiguation (Ji
et al., 2020), multilinguality (Liu et al., 2021a), Contex-
tualized Language Embeddings (Sun et al., 2020), and
models such as KnowBert (Peters et al., 2019), E-BERT
(Poerner et al., 2020), ERICA (Qin et al., 2021), ERNIE
(Zhang et al., 2019b), ERNIE-NLI (Bauer et al., 2021),
LUKE (Yamada et al., 2020) and others. Therefore, the
incorporation of large-scale knowledge graphs and on-
tologies can be also critical to the quality of multimodal
representations and the success of relevant models on the
various downstream tasks.
While previous surveys (Baltrušaitis et al., 2017; Kaﬂe
et al., 2019; Guo et al., 2019; Mogadala et al., 2021; Zhang
et al., 2020; Uppal et al., 2020; Cao et al., 2020; Du et al.,
2022) provide analysis and taxonomies over models, tasks
and datasets regarding multimodal representations, they
do not analyze knowledge-enhanced approaches. In con-
trast to those works, we focus on the integration and impor-
tance of external knowledge to VL models. Even though
the current trends focus on transformer based implemen-
tations, for the sake of completeness, other techniques
that have contributed to the ﬁeld of knowledge-enhanced
VL (KVL) learning are also included. Overall, we tar-
get to bridge the gap between knowledge representation
and multimodal deep learning: we provide a broad and
comprehensive analysis of both ﬁelds, and consequently
collect models that have served the various KVL tasks.
Finally, we discuss current challenges and limitations of
existing datasets and approaches, upon which we suggest
potential future directions of this evolving ﬁeld. To the
best of our knowledge, there are no extended works cover-
ing the intersection of those two very fundamental ﬁelds
of AI, which have demonstrated promising directions in
many aspects of state-of-the-art research when combined.
The current survey consists of four main parts. The
ﬁrst part (Section 2) covers the preliminaries of multi-
modal deep learning, analyzing trends, methods, models
and tasks which set the basis for knowledge-enhanced VL
(KVL) models. An analysis regarding graph structures
and their representation follows in Section 4. Section
5 is dedicated to a taxonomy of knowledge senses and
types, as well as the presentation of popular knowledge
bases. Section 6 provides a taxonomy and analysis as
per KVL task where the usage of external knowledge has
been attempted, accompanied with datasets and evaluation
methods. Finally, some needs and possible future direc-
tions regarding the usage of knowledge in multimodal
learning are identiﬁed and analyzed.
2
Background
Multimodal learning is a large and diverse ﬁeld that in-
volves a variety of data sources, architectural approaches
and tasks. By focusing on VL tasks, which exploit text
and image data, we can identify a variety of relevant ap-
plications. The nature of each task deﬁnes the chosen
backbone architecture, upon which all consequent ap-
proaches are built. More speciﬁcally, VL tasks can be
divided in discriminative tasks, where the goal is either to
provide a matching between modalities or understanding
the one modality based on the other, and generative tasks,
which target image or text generation. Discriminative
VL tasks present a long line of research initially based
on recurrent neural networks (RNNs) for text represen-
tation, with most contemporary approaches favoring the
Transformer (Vaswani et al., 2017) framework for its indis-
putable advantages. On the other hand, generative tasks
demonstrate an interesting variability in architectural ap-
proaches: while language generation tasks conditioned on
image are addressed by architectures based on RNNs or
Transformers, image generation tasks conditioned on text
are mainly tackled by Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014), and more recently by
Transformers (Esser et al., 2020) and Diffusion models
(Dhariwal and Nichol, 2021).
Knowledge-enhanced VL models (KVL models) usu-
ally step on existing approaches for VL representation
and then employ various strategies to integrate knowledge.
One common ﬁrst step between most VL approaches is
the independent encoding of text T and images I, followed
by the interaction of these encodings in order to acquire a
joint representation. The choice of text encoding (RNN or
Transformer) heavily inﬂuences the overall architecture
of a VL model. On the other hand, image encoding adapts
to the needs deﬁned by text encoding, and variability in
chosen image encoders serves particular improvements,
such as performance boosting or reduction of trainable
parameters.
Transformer-based VL models consist of two main
stages: pre-training on large amounts of aligned image-
text data, such as images and their captions, and then
ﬁne-tuning on smaller task-speciﬁc datasets. Pre-training
learns a generic joint VL representation from independent
image and text encodings using certain pre-training tasks
(or pre-training objectives) that enforce cross-modality in-

teraction. Fine-tuning is performed independently on each
task-speciﬁc dataset, leveraging the learned representation
from the pre-training stage.
There is a variety of ways to incorporate knowledge
K, with most approaches favoring external knowledge
sources in the form of widely used knowledge graphs
(KGs). In this case, a representation based on graph neu-
ral networks (GNNs) is the most popular approach to-
wards providing an appropriate encoding for K. However,
other knowledge types can be integrated as well, either in
the form of knowledge stored in neural network weights
(implicit knowledge) or linguistic knowledge from the
web, embedded with a text encoder. Enhancements to VL
model performance can be also realized via self-acquired
(internal) knowledge: without leveraging external knowl-
edge sources, automatic extraction of speciﬁc character-
istics from either modality boosts and guides learning,
improving knowledge-free baselines. Knowledge K can
be fused either in early stages together with text T ∈W
and image I ∈V instances resulting in a KVL representa-
tion, or alternatively in later stages and independently of
the VL stream, reﬁning and correcting the predictions of
the VL model.
KVL models can either target one task at a time (single-
task models), or multiple tasks simultaneously (multi-
task models). Single-task models present a large variety
of architectural implementations so far, while multi-task
models are exclusively built upon multimodal transform-
ers, as they heavily rely on the pre-training ﬁne-tuning
scheme. Discriminative tasks are tackled by both single-
and multi-task models. On the other hand, generative
tasks can be only handled by single-task models, as they
are harder by nature. The evaluation of the overall model
performance is realized as per task, based on appropriate
task-speciﬁc evaluation metrics.
A broad overview of KVL models is provided in Figure
1. The joint representation module is shown as a black
box, as most architectural variations analyzed in following
sections are happening within this stage.
3
Multimodal representation learning
The core of multimodal deep learning revolves around
the ways the various modalities are represented indepen-
dently and interact with each other. Especially for text
and images, several representation techniques have been
developed throughout the years, due to the advancements
of image classiﬁcation (He et al., 2016; Simonyan and
Zisserman, 2015) and object detection (Girshick, 2015;
Ren et al., 2017) models for vision, as well as distributed
language representations (Mikolov et al., 2013; Penning-
ton et al., 2014), recurrent neural networks (Hochreiter
and Schmidhuber, 1997; Schuster and Paliwal, 1997; Cho
et al., 2014), and attention-based models (Vaswani et al.,
2017; Devlin et al., 2019; Liu et al., 2019b; Brown et al.,
2020) for text. Apart from vision and language, more
modalities can potentially contribute in a joint representa-
tion, such as speech, music, graphs and others.
3.1
Text representation
Text representation offers several variations over imple-
mentations, imposing signiﬁcant inﬂuences towards mul-
timodal learning in total. The two major categories of
language embeddings include recurrent architectures
(RNN/LSTM/GRU) and Transformers. The architec-
tural choices for language representation guide the taxon-
omy of models per KVL task, due to the diversity they
impose in the resulting implementations.
Distributed word representations
Traditional and
widely used representations such as Word2Vec (Mikolov
et al., 2013) have contributed in several components of
the various VL tasks, often used as initializations for
other methods. Doc2vec (Le and Mikolov, 2014) extends
word2vec, achieving a vector representation of a group of
words. GloVe (Pennington et al., 2014), a distributed word
representation model, is trained on a global word-word
co-occurrence frequency matrix and successfully captures
both local and global statistics. Fast-text (Bojanowski
et al., 2017) represents each word with a bag of charac-
ter n-grams in order to capture the internal structure of
words. This approach can effectively utilize the morphol-
ogy of words, thus it is able to represent rare formations
occurring in morphologically rich languages. Word se-
mantics cannot be successfully captured by static word
representation methods, so contextualization is needed
especially in cases of polysemy, an issue tackled in CoVe
(McCann et al., 2018). ELMo (Peters et al., 2018) is an-
other deep word contextualization model which leverages
character level information to form robust representations
based on morphological information. Universal Language
Model Fine-tuning (ULMFiT) (Howard and Ruder, 2018)
achieves robust inductive transfer learning on a variety
of downstream NLP tasks, where only ﬁne-tuning is re-
quired.
Recurrent neural networks
The basic idea behind re-
current neural architectures (RNNs) is the sequential
processing of elements belonging to a ﬁnite sequence
(x1, x2, ..., xT ), one xt at a time, while retaining context
information from the previous elements in the form of
the previous node’s hidden state ht−1. Both xt and ht−1
contribute to the calculation of the current hidden state ht,

Figure 1: A generic overview of KVL frameworks.
which consequently participates in deﬁning the current
output yt. Feed forward neural networks implement each
time step as a layer, with shared weights across layers.
Backpropagation through time (BPTT) is utilized to train
recurrent neural networks (Lipton et al., 2015). Other
recurrent neural architectures such as Long Short-Term
Memory (LSTM) (Hochreiter and Schmidhuber, 1997)
dominated the ﬁeld of language embeddings in many ap-
proaches, with more reﬁned variants such as bidirectional
LSTM (BiLSTM) (Schuster and Paliwal, 1997), Gated
Recurrent Unit (GRU) (Cho et al., 2014) and bidirectional
GRUs (BiGRUs) following in later works.
This sequential nature of RNNs ﬁts naturally to lan-
guage processing, inspiring several relevant implementa-
tions in the NLP domain, as well as in multimodal tasks.
To this end, earlier works in VL architectures heavily
rely on distributed word embeddings together with se-
quential models for language representation. However,
limitations tied with sequential processing such as vanish-
ing gradients and inability of parallel processing directed
research interest towards novel frameworks, such as the
Transformer (Vaswani et al., 2017).
Language transformers
The introduction of attention
mechanisms and especially the Transformer framework
(Vaswani et al., 2017) opened a whole new world of possi-
bilities for language embeddings and several downstream
linguistic tasks. The Transformer model relies on encoder-
decoder structure and utilizes an attention mechanism to
construct dependencies between input and output data.
The layers of the encoder and the decoder are stacked
the one upon the other, containing sub-layers with Multi-
Head self-attention and position-wise fully connected
feed-forward layers. Residual connections followed by
layer normalization are used between the sub-layers of the
encoder. The decoder has an additional encoder-decoder
multi-head attention sub-layer that helps focusing on the
appropriate parts of the encoded input sequence. More-
over, the decoder’s self-attention modules are modiﬁed so
that they force the prediction at a certain position to be
based only on the known predictions of previous positions.
Transformer architectures prove that there is no need for
convolutions or recurrent units to achieve state-of-the-art
performance in linguistic tasks. Currently, most state-
of-the-art VL architectures utilize attention mechanisms
within their implementation.
Transformer models for NLP tasks consist of a pre-
training stage on a large corpus of data, followed by ﬁne-
tuning on certain downstream tasks. Language transform-
ers can be divided in two major categories: autoregres-
sive (AR) and autoencoding (AE), depending on whether
pre-training is performed in a unidirectional or a bidirec-
tional way. AR language models attempt to estimate the
probability distribution of a text corpus, while AE models
learn to reconstruct manipulated inputs with the help of

surrounding information (Yang et al., 2019b).
BERT (Devlin et al., 2019) is a popular bidirectional
transformer-based language representation model, able to
handle a variety of natural language processing tasks by
just ﬁne-tuning one additional output layer. It uses masked
language modeling (MLM), randomly hiding some input
tokens in order to be inferred from the surrounding words.
The pre-training stage is based on unlabeled data, which
enable parameter learning. Those parameters are then ﬁne-
tuned with labelled data corresponding to some certain
tasks. RoBERTa (Liu et al., 2019b) offers an optimized
extension to BERT, suggesting that longer training, more
data and larger batch size, as well as training on longer
sequences, dynamically altering the masking patterns and
removing the next sentence prediction loss are factors that
contribute to advanced performance of the original BERT
model. XLNet (Yang et al., 2019b) combines AR and AE
language modelling in a single approach. It introduces the
pre-training objective of permuted language modelling,
which attempts to collect information from permutations
of the factorization order of text tokens with respect to AR
likelihood estimation, practically inducing bidirectional
capabilities to the learning process.
Generative NLP models have demonstrated impressive
results in recent literature. GPT-2 (Radford et al., 2019)
is pre-trained on a very large dataset (40GB of text data
from over 8 million documents) and utilizes 1.5 billion pa-
rameters in order to construct a powerful language model.
GPT-3 (Brown et al., 2020) is an AR language model of
175 billion parameters which achieves zero-shot, one-shot
and few-shot capabilities. It is able of synthesizing results
in a human-like level, so that writing articles or code in
some programming language, learning and reusing new
words, unscramble words and other tasks can be realized
indistinguishably to humans. T5 (Raffel et al., 2020) in-
troduces a uniﬁed format where both inputs and outputs
are text. Without changing the model architecture, the
hyperparameters or the loss function, T5 generates text
to address tasks such as question answering, text sum-
marization, machine translation, as well as classiﬁcation
tasks. BART (Lewis et al., 2019) utilizes a bidirectional
encoder, enabling attention from both left and right di-
rections, and an autoregressive (unidirectional) decoder,
which allows attending to past tokens only. It is able to
handle generative tasks such as question-answering, text
summarization, conditional text generation, mask ﬁlling,
and also classiﬁcation tasks.
As for text encodings for VL models, BERT (Devlin
et al., 2019) has become a golden standard for several
transformer-based approaches, while fewer implementa-
tions utilize variants such as RoBERTa (Liu et al., 2019b).
GPT2 (Radford et al., 2019), T5 (Raffel et al., 2020) and
BART (Lewis et al., 2019) have also served as language
encoders.
3.2
Visual representation
There is much less diversity in the representation of the
visual modality compared to text encoding. Most works
rely on widespread convolutional architectures without
signiﬁcant variations, and only recently some works at-
tempted encoders based on image transformers, which
however do not enforce architectural modiﬁcations.
Convolutional Neural Networks
Representation of
images can involve object level or image level features,
depending on the granularity of information that needs to
be imbued in the representation. A global image represen-
tation can be achieved by employing widely used image
classiﬁcation models as feature extractors. Many works
rely on CNN based classiﬁers such as VGG (Simonyan
and Zisserman, 2015) and ResNet (He et al., 2016), while
others prefer more ﬁne-grained local representations sup-
ported by object detectors, such as Fast-RCNN (Girshick,
2015) and Faster-RCNN (Ren et al., 2017). The ﬁxed
pre-trained models for object feature extraction somehow
limit the expressivity of VL transformers, while being
slow. Solutions to this limitation include the usage of
grid features as visual tokens (Huang et al., 2020b), or
discretized grid features (Huang et al., 2021).
Image Transformers
Recent advancements in Visual
Transformers as an extension of the aforementioned lan-
guage Transformers (Vaswani et al., 2017) have inﬂuenced
the ﬁeld of image representation. ViT (Dosovitskiy et al.,
2021) suggests patch-like parsing of images for feature
extraction, resulting in more powerful image represen-
tations. Swin Transformer (Liu et al., 2021c) is a more
efﬁcient implementation due to the usage of self-attention
in local image patches contrary to global self-attention
of other approaches, which results in quadratic compu-
tation complexity compared to image size. Swin Trans-
former achieves linear complexity by hierarchically merg-
ing larger and larger image patches across layers, with
self-attention acting only within each patch. Similar to
NLP Transformers scaling capabilities, Swin Transformer
V2 of 3 billion trainable parameters serves as the largest
dense vision model so far (Liu et al., 2022).
3.3
Sequential models for VL tasks
Even though Transformer-based approaches have domi-
nated the ﬁeld of multimodal learning, sequential mod-
els have offered a variety of interesting solutions in sev-

eral tasks, and still serve as the way to go in some of
them.
Most sequential-based techniques address the
vision-language co-operation through encoding-decoding
schemes which utilize a CNN for images I and an
RNN/LSTM/GRU structure for text T. There are differ-
ent ways for visual and language embeddings to interact,
depending on the downstream task: for example, a CNN-
based image encoding can be fed as a conditioning to an
RNN/LSTM/GRU decoder structure for tasks requiring
text generation from image. Alternatively, input text can
be embedded using an RNN/LSTM/GRU encoder, and
then a feed-forward neural network can learn the corre-
lations between text embeddings and CNN-based image
embeddings. This variant can also serve text-image match-
ing tasks. Sequential structures for VL learning remain
rather popular in tasks that require language generation,
especially in underexplored ones where more reﬁned ar-
chitectures have not been attempted yet.
3.4
Multimodal Transformers
Multimodal transformers have revolutionized the ﬁeld
of multimodal learning, with almost any state-of-the-art
model built upon them. There are some certain steps fol-
lowed from receiving the input data until the ﬁnal result
on multimodal tasks, as presented in Figure 2. Initially,
given a multimodal dataset, for example a VL dataset
D comprised of image-text pairs (I, T) which consist of
visual features v and textual features w respectively, an
appropriate encoding scheme, such as the one described
in sections 3.1 for T and 3.2 for I should be decided.
The input embedding contains a tokenized text represen-
tation, an image encoding and other special embeddings.
All transformer-based architectures, either targeting vi-
sion, language or both consist of the pre-training and
ﬁne-tuning stages. For this reason, a multimodal encod-
ing module is designed to receive the embedded input,
enabling the interaction between modalities by jointly
learning complementary information with the help of
well-designed cross-modal pre-training tasks. Finally,
a ﬁne-tuning stage adapts the pre-trained model to the
downstream task by training on a smaller labelled dataset.
3.4.1
Special input tokens and embeddings
Special tokens need to be appended to the input before
entering the VL transformer model in order to discern be-
tween different modalities, as well as the start and some-
times the end of the sequence. An input embedding is
formed by combining text representation, visual represen-
tation, special tokens and other embedding information to
guide training. Despite different VL transformers follow-
ing slightly different strategies regarding input represen-
tation, the main constituents are analyzed below. Figure
3 provides a general overview of the input tokens and
embeddings.
The input token denoted as [CLS] is a special classiﬁ-
cation token that deﬁnes the start of the input sequence.
Linguistic information from text T is appended after the
[CLS] token. Usually WordPiece (Wu et al., 2016c) tok-
enizer, a sub-word based tokenizer framework, transforms
words into tokens. Afterwards, a numerical format of
tokens is obtained by assigning a unique embedding per
subword token so that it can be further processed. The
text embedding will then be w = w1, w2, ..., wn ∈Rd,
where n corresponds to the textual sequence length and d
is the embedding dimension. A segment token [SEP] is
appended after w to separate different modalities. Follow-
ing the separator [SEP], a visual token [IMG] indicates
the visual modality I. There are different usages of [IMG],
either marking the start of visual features v, or multiple
[IMG] instances acting as masks that locate the existence
of visual elements within the token embedding. Some
architectures may even discard [IMG] tokens. A ﬂattened
visual feature vector is extracted as a result of the chosen
visual encoder, denoted as v = v1, v2, ..., vk ∈Rd, where
k is the number of visual features and d the embedding
dimension. The end token [END] marks the end of the
sequence. Many VL transformers ommit [END] token.
Segment embeddings attribute the source of each in-
put element by assigning a unique label to each of them.
For example, input textual and visual features would be
assigned with a different segment embedding sw and
sv respectively, as they come from different modalities.
Therefore, sequences sw = sw1, sw2, ..., swn are going
to be summed with the text embeddings and similarly,
sv = sv1, sv2, ..., svk will be summed with visual embed-
dings. Alternatively, if the input contains two sentences
(such as question and answer pairs), each of those will be
aligned with different segment labels. Position embed-
dings pi denote the positioning of tokens in a sequence
{w1, ..., wi, ..., wn}, as originally used in BERT (Devlin
et al., 2019).
The summation of token embeddings, segment em-
beddings, position embeddings and visual embeddings
can act as starting point towards the input representation,
followed by an appropriate transformer structure. The in-
put embedding generally follows the {[CLS], ˆw1, ˆw2,
... , ˆwn, [SEP], ˆv1, ˆv2, ... , ˆvk, [END]} format, where ˆw
and ˆv are the ﬁnal text and visual embeddings respec-
tively, after the summation of w and v with the segment
and/or position embeddings.

Figure 2: The overall workﬂow of a VL transformer.
Figure 3: A general outline of input tokens and embeddings.
3.4.2
Vision and Language joint encoding
Encoded modalities need to be projected to the same
vector space and interact in order to achieve a meaning-
ful joint representation. A concise separation of cross-
modality encoding is provided in Du et al. (2022), where
two main categories of encoding schemes are identiﬁed:
fusion encoder and dual encoder. Those two approaches
can be even combined. Fusion encoder concerns an abun-
dance of VL transformers, and can be further divided in
single-stream encoding and double-stream encoding.
Double-stream fusion encoder
utilizes two separate
transformer modules to process images and text respec-
tively. First VL approaches such as ViLBERT (Lu et al.,

2019) and LXMERT (Tan and Bansal, 2019) fall into
this category, where they naturally extend BERT to also
process images. Speciﬁcally, ViLBERT decomposes im-
ages in non-overlapping patches, similar to how tokens
serve as inputs in the case of BERT. Text is tokenized
and fed together with positional embeddings to its trans-
former stream. A co-attention module enables interac-
tion and alignment between modalities given their inter-
mediate representations. An extension of ViLBERT to
12 downstream tasks was presented in Lu et al. (2020).
LXMERT (Tan and Bansal, 2019) reﬁnes the cross-modal
part to achieve advanced performance in downstream
tasks. More recent models employing a double-stream
fusion encoder are ALBEF (Li et al., 2021), Visual Pars-
ing (Xue et al., 2021) and WenLan (Huo et al., 2021). In
general, double-stream encoders demand training of two
transformer models (one for each stream), which is com-
putationally inefﬁcient. Therefore, succeeding approaches
focus on single-stream encoders.
Single-stream fusion encoder
concerns the majority
of state-of-the-art VL models. A single transformer net-
work, usually stepping on the BERT (Devlin et al., 2019)
backbone is used to process images and text representa-
tions simultaneously, where alignments are discovered
via self-attention. Segment tokens to separate modalities,
together with position tokens to indicate aligned token
pairs are added to input data before they are concatenated
and fed into the encoder. VisualBERT (Li et al., 2019a),
VL-BERT (Su et al., 2020), PixelBERT (Huang et al.,
2020b), InterBERT (Lin et al., 2021), VLP (Zhou et al.,
2019a), Uniﬁed-VLP (Zhou et al., 2019b), B2T2 (Al-
berti et al., 2019), UNITER (Chen et al., 2020), XGPT
(Xia et al., 2020), ViLT (Kim et al., 2021), VL-T5 (Cho
et al., 2021), SOHO (Huang et al., 2021), SimVLM (Wang
et al., 2021c) belong to the single-stream encoder cate-
gory. Some of the latest models even present zero-shot
learning capabilities, enabling more out-of-the-box capa-
bilities in VL tasks.
Dual encoder
is favored by a small family of recent
VL models which exploit contrastive learning to pro-
vide image-text representations. Separate encoders em-
bed each modality independently, and their representa-
tions are projected on the same vector space with the
goal of learning similarity and dissimilarity properties.
This is why contrastive learning naturally ﬁts: paired
image-text samples are trained to stay close together,
while being apart from the rest. CLIP (Radford et al.,
2021) implements this strategy utilizing more than 400
million image-text pairs for training, achieving even zero-
shot capabilities in retrieving previously unseen matches.
ALIGN (Jia et al., 2021) follows the same recipe us-
ing over one billion image-text pairs, demonstrating that
large-scale uncurated data can compensate for the pres-
ence of noise. FLORENCE (Yuan et al., 2021) exploits
both image-to-language contrastive loss and its reverse,
language-to-image contrastive loss, actually forming a
bi-directional contrastive loss applied on image-label-
description triplets.
Fusion and dual encoder
A couple of models leverage
the beneﬁts of both encoding approaches (Du et al., 2022).
FLAVA (Singh et al., 2021) is a holistic approach that
utilizes a dual encoder to integrate visual and text rep-
resentations into a multimodal one, providing unimodal
and multimodal reasoning capabilities in the same model.
Additionally, VLMo (Wang et al., 2021a) offers a ﬂexible
format by providing a fusion encoder for classiﬁcation
task, and a dual encoder for retrieval tasks.
3.4.3
Self-supervised pre-training
VL pre-training datasets
A VL model can be pre-
trained on unimodal (text and/or visual data indepen-
dently) and multimodal data (paired image-text data). Uni-
modal data can be unlabelled, noisy and abundant, such as
sets of documents or images, while multimodal data are la-
belled and cleaner, so that each modality presents another
point of view for the same instance. Large-scale datasets
containing visual and linguistic information are necessary
for pre-training. Pre-training aspires to instill a generic
understanding of the visual world, natural language and
their in between interactions. Widely used datasets can
either be in-domain, meaning that their data distribution is
very close to task-speciﬁc datasets used in the ﬁne-tuning
stage, or out-of-domain, containing less similar data to
the downstream tasks, but usually being much larger in
size. Most VL transformers leverage a corpus consisting
of COCO (Lin et al., 2014), Visual Genome (Krishna
et al., 2016), SBU (Ordonez et al., 2011) and Conceptual
Captions (CC) (Sharma et al., 2018) for pre-training, with
fewer models either excluding some of those, or adding
more datasets to this corpus.
COCO (Lin et al., 2014) consists of around 106K im-
ages/533K captions in the train split, and 25K images/5K
captions in the test split, with 5 sentences provided per
caption from different annotators. Those sentences are
designed to provide an overall (global) understanding of
their corresponding images, which represent scenes of
multiple objects and their in-between relationships. As
most VL tasks are built atop COCO, it is considered to be

an in-domain dataset. Because of the usage of COCO for
downstream tasks, subsets of COCO used in pre-training
and ﬁne-tuning must be mutually exclusive to avoid any
data leakage.
Visual Genome (VG) (Krishna et al., 2016) is another
in-domain dataset, from which several VL tasks emerge.
It contains more than 100K images of complex scenes,
providing multiple annotations regarding per image scene
graphs, objects (3.8M instances), relationships (2.3 M), at-
tributes (2.8 M), visual questions and answers (1.7 M), re-
gion descriptions (5.4 M) and region scene graphs (3,7 M).
Region descriptions are captions grounded to image re-
gions, acting as dense captions per image. As VG images
have a partial overlap with COCO images, any COCO
image used in downstream tasks should be excluded from
the VG during pre-training.
SBU Captions (Ordonez et al., 2011) consists of 990K
images/990K captions in the train split, and 10K im-
ages/10K captions in the test spit, with each image corre-
sponding to one caption. It is an out-of-domain dataset,
larger in size than COCO and VG.
Conceptual Captions (CC) (Sharma et al., 2018) is
the largest out-of-domain dataset used for pre-training,
consisting of more than 3 million images for training and
14K for validation with 1 caption each.
Pre-training objectives
Uni-modal objectives concern-
ing either T (language objectives) or I (vision objectives)
at a time, as well as cross-modal objectives which take into
account both modalities at once are used for pre-training.
Such objectives teach the models to infer missing informa-
tion by understanding their surroundings. Self-supervised
learning is the most prevalent technique, enabling learning
with the help of a corrupted part of the input or adversari-
ally matched pairs in a contrastive fashion.
Language objectives are designed to implicitly learn
linguistic rules and patters so that a model pre-trained on
them acquires some ’understanding’ of natural language.
Some language modelling objectives are analyzed below.
Masked Language Modeling (MLM) is a pre-training
objective introduced in BERT (Devlin et al., 2019). To-
kens in the input sentence are replaced at random with
a special [MASK] token. The model needs to uncover
the actual token by learning the context from its surround-
ings, thus permitting contextualized representations. The
default probability of masking out a token is 0.15. The
MLM objective function is bidirectional, which means
that a token can be predicted from either its right ones,
or its left ones. Preﬁx Language Modeling (PreﬁxLM)
(Wang et al., 2021c) differs from standard MLM such that
it enables bi-directional attention on the preﬁx sequence.
Next Sequence Prediction (NSP) refers to the objective
that given a pair of sentences, tasks the model to predict
whether they could serve as consecutive sentences in a
corpus or not.
Vision objectives apply similar ideas on the visual
modality.
Due to the more high-dimensional nature
of images compared to text, the masking tasks are
challenging in design.
Masked Region Modeling (MRM) applies zeros over
image regions, asking the model to infer missing parts.
Semantic and pixel level information can be retrieved,
naturally leading to two sub-tasks:
• Classiﬁcation of regions can be used to obtain a
discrete signal corresponding to semantic entities.
• Regression of region features provides a more con-
tinuous, pixel-level understanding of missing parts.
Random Pixel Sampling (RPS) (Huang et al., 2020b)
tackles overﬁtting similarly to the Dropout mechanism
(Srivastava et al., 2014): in each iteration, a subset of
pixels is chosen to be inserted in the transformer network.
The corrupted image that the model receives each time en-
ables more robust representations, as relying on semantics
instead of single pixels is encouraged.
Cross-modal objectives jointly leverages information
from both text and image modalities for learning.
The design of cross-modal pre-training tasks is more
challenging compared to their unimodal counterparts, as
it is required to ensure that the model does not depend
learning on a single modality exclusively (Chen et al.,
2020; Wang et al., 2021c; Du et al., 2022).
Unidirectional
(seq2seq)
Language
Modeling
(seq2seqLM) is an autoregressive variant of MLM, which
means that a masked token can only attend to previous
tokens and not to future ones. Seq2seqLM attempts to
directly maximize the likelihood of a text, image pair
(T, I) = x ∈(W, V) from dataset D:
LLM = -E(W,V)∈DlogPθ(x))
Masked Language Modeling with Image (MLMI) at-
tempts to recover the corrupted text tokens by also con-
sulting the image apart from the linguistic part exclusively.
Inferring the masked token is achieved by minimizing the
negative log-likelihood, where wm are the masked tokens,
w\m are the unmasked ones, and (W, V) ∈D:
LMLM = -E(W,V)∈DlogPθ(wm|w\m, V)

Whole Word Masking (WWM) (Kim et al., 2021) is an
extension of MLMI which masks out entire words rather
than subword tokens. Therefore, the model is encour-
aged to consult the visual part to infer the corrupted word
instead of guessing the missing part from its unmasked
constituents. This procedure enforces a more difﬁcult task
to the transformer, achieving better cross-modal align-
ment.
Masked Region Modelling with Language (MRCL) is
the complementary task of MLMI. Instead of text tokens,
image region features are masked out with a probability
(by default 0.15). Masking does not include a special
token for the visual modality; it is implemented by ﬁlling
the corresponding image regions with zeros. The model
is tasked to reconstruct visual regions v\m based on infor-
mation provided from the unmasked features vm and the
textual modality W, targeting to optimize the objective:
LMRM = -E(W,V)∈Dfθ(vm|v\m, W)
Reconstruction of visual features can yield two tasks,
offering different insights to the high-dimensional prob-
lem of feature prediction:
• Masked Region Feature Regression (MRFR)
refers to producing features of the same dimension-
ality as the visual region. This is achieved by ap-
plying L2 regression between the predicted and the
ground truth visual vector to minimize their distance.
Considering the transformer prediction FC(hvi), ac-
quired after passing the output of the masked region
hvi through a fully connected (FC) layer, and the
region feature ˆEv(vi) of region vi, the minimization
of L2 can be written as:
LMRFR = -E(W,V)∈D
M
X
i=1
||FC(hvi) −ˆEv(vi)||2
• Masked Region Classiﬁcation (MRC) aims to pre-
dict the semantic class of the masked image region.
The transformer prediction is compared with the out-
put of an object detector, by considering the highest
conﬁdence object label which serves as the actual tar-
get. The cross-entropy (CE) loss between the object
detector label c(vi) and the transformer prediction
FC(hvi) for m regions needs to be optimized:
LMRC =
-E(W,V)∈D
M
X
i=1
CE(softmax(FC(hvi), c(vi))
An extension of MRC can consider the overall distri-
bution of the object detector label predictions instead
of exclusively focusing on the top class. In that case,
the objective function would attempt to minimize the
distance between the object detector distribution and
the transformer’s distribution of predictions, which
is actually equivalent of minimizing the KL diver-
gence between those two distributions. The objective
function for Masked Region Classiﬁcation with KL-
Divergence (MRC-KL) for a distribution of object
detector labels ˜c(vi)) instead of top object detector
label c(vi) can be written as:
LMRC-KL = -E(W,V)∈D
M
X
i=1
DKL((FC(hvi), ˜c(vi))
Image-Text Matching (ITM) enables learning visiolin-
guistic matches in a global level. It can be viewed as a
multimodal extension of next sentence prediction (NSP),
where the model needs to recognize whether a given pair
of text and image is in fact matched or not, as both positive
and negative pairs are sampled. The alignment probability
between text and image is provided by a score function sθ,
and the binary cross-entropy (CE) needs to be optimized:
LV LM =
-E(W,V)∈D[y log sθ(W, V) + (1 −y) log(1 −sθ(W, V))]
Word-Region Alignment (WRA) is a more ﬁne grained
version of ITM, where words have to be grounded to
image regions.
Contrastive objectives act upon data pairs projected
on the same semantic space, so that the model learns a
representation based on their similarity.
Cross-Modal Contrastive Learning (CMCL) learns to
place close together matching image-text pairs, while
pushing apart any mismatched ones. The contrastive loss
for the i-th and j-th pairs sampled from D, where vi refers
to the image of the i-th pair and wj refers to the text of the
j-th pair, sθ(vi, wj) = v⊤
i wj is a scoring function which
is maximized when matching pairs occur, and σ serves as
a learnable temperature parameter:
LCMCL = −1
n
M
X
i=1
log
exp(v⊤
i wi/σ)
PM
j=1exp(v⊤
i wi/σ)
3.4.4
Task-speciﬁc ﬁne-tuning
Current VL tasks are usually created by extending existing
tasks in NLP or vision domains.They may either be dis-
criminative or generative, and usually there are variants
addressing both problems (Mogadala et al., 2021).

Visual Question Answering (VQA)
Given an image
I ∈V and a natural language question q ∈W, a VQA
model is tasked to predict the correct answer a ∈W.
VQA is an extension of NLP question answering (QA)
to include the visual modality. It can be viewed as a
classiﬁcation task, where the predicted answer can be se-
lected among a set of candidate answers. Alternatively,
free-form answers can be generated, forming a gener-
ative VQA task. Widely used datasets for VQA, con-
taining complex scenes of objects and relationships, are
the original VQA (Agrawal et al., 2016), VQAv2 (Goyal
et al., 2016), GQA (Hudson and Manning, 2019), Vi-
sual7W (Zhu et al., 2016), Visual Genome QA (Krishna
et al., 2016) and COCO QA (Ren et al., 2015). Addition-
ally, datasets providing explanations for answer selections,
such as VQA-E (Li et al., 2018) have recently emerged.
Visual Entailment (VE)
extends the default task of tex-
tual entailment by answering whether an image I ∈V
acting as the premise semantically entails the given textual
hypothesis h ∈W. The hypothesis h can either entail,
contradict or remain neutral with respect to the premise,
providing an answer a to the visual premise I. SNLI-VE
(Xie et al., 2019) is a dataset used for VE. Moreover, e-
SNLI-VE (Do et al., 2020) corrects label errors present in
SNLI-VE due to its automatic assembling while providing
human-written explanations in natural language for the
corrected SNLI-VE corpus.
Visual
Referring
Expressions
(VRE)
or
Visual
Grounding
extends NLP referring expressions, attempt-
ing to ground a textual phrase s ∈W to an image object or
region for each image I ∈V. Datasets used for VRE are
CLEVR-Ref+ (Liu et al., 2019a), RefCOCO, RefCOCO+,
RefCOCO-g (Yu et al., 2016) and GuessWhat (de Vries
et al., 2016).
Visual Dialog (VD)
is the analogue of chatbots, aiming
to maintain a meaningful conversation by responding to
consecutive textual inputs q ∈W. VD is tasked to create
such a dialog upon a given image I ∈V. VisDial (Das
et al., 2016) is a dataset for VD that was proposed together
with the introduction of the task. Other datasets for multi-
modal dialog are GuessWhat (de Vries et al., 2016) and
CLEVR-Dialog (Kottur et al., 2019).
Image retrieval from text or Text-Image Retrieval
(TIR)
attempts to return the most suitable image I ∈V
within a database according to a natural language descrip-
tion c ∈W. There is also the inverse task of text re-
trieval from image or Image-Text Retrieval (ITR) that
searches the optimal c ∈W given an image I ∈V. Cross-
modal retrieval, referring to retrieving any modality from
the other one is an extension of the NLP task of document
retrieval. Common datasets used in TIR/ITR are COCO
(Lin et al., 2014) and Flickr8k/30k (Young et al., 2014)
which contain images paired with 5 captions each.
Image Captioning (IC)
is a generative task, extending
natural language generation (NLG) to describe images:
given an image I ∈V, provide a sentence c ∈W that
describes it. IC can be viewed as a generative counterpart
of text retrieval from image (ITR). Dense Captioning is
a ﬁne-grained analogue of IC that requires generation
of descriptions for image regions instead of providing a
global visual caption. Conceptual Captions (CC) (Sharma
et al., 2018), SBU (Ordonez et al., 2011), COCO (Lin
et al., 2014) and Flickr8k/30k (Young et al., 2014) are
widely used datasets for image captioning.
Visual Storytelling (VIST)
is the extension of im-
age captioning to a sequence of N related images
I1, I2, ..., IN ∈V. Generated captions c1, c2, ..., cN ∈W
should be consistent with each other throughout the se-
quence, forming a textual ’story’. Datasets related to
VIST are Visual Storytelling Dataset (VIST) (Huang
et al., 2016a) which models social language regarding
visual concepts, and New York City Storytelling (NYC-
Storytelling) (Park and Kim, 2015), which contains narra-
tives from blogs.
Multimodal Machine Translation (MMT)
is assisted
from the visual modality to translate between two lan-
guages, as an extension of the machine translation task.
Multi30K-MMT (Elliott et al., 2016) contains multilin-
gual descriptions of images in English, German, French,
and Czech languages.
Visual Reasoning (VR)
extends visual perception
tasks, such as object detection and classiﬁcation, semantic
segmentation and others. VR needs to predict meaning-
ful relationships between image entities, which is simi-
lar to creating a scene graph. Compositional reasoning
refers to the task where attributes need to be combined
so that the identity of the whole can be inferred. Popular
datasets for VR are Compositional Language and Elemen-
tary Visual Reasoning (CLEVR) (Johnson et al., 2017),
Relational and Analogical Visual rEasoNing (RAVEN)
(Zhang et al., 2019a), Natural Language Visual Reasoning
(NLVR) (Suhr et al., 2017) and Natural Language Visual

Reasoning for Real (NLVR2) (Suhr et al., 2019). In order
to test the ability of VR models in novel attribute com-
binations, CLEVR-CoGenT dataset was proposed as an
extension of CLEVR. Real world compositional questions
of GQA (Hudson and Manning, 2019) can also be utilized
for VR models. Finally, AQUA (Garcia et al., 2020) is a
visual reasoning dataset dedicated to the artistic domain.
Visual Commonsense Reasoning (VCR)
attempts to
understand an image I ∈V by incorporating common-
sense knowledge relationships to explain the answer
a ∈W derived for each question q ∈W. It can be also
viewed as an extension of the VQA task, where instead of
merely providing an answer a to a given visual question
q, a rationale r ∈W justifying the choice of the answer
needs to be returned as well. The answer a and rationale
r can be also generated, thus yielding the Visual Com-
monsense Generation (VCG) task. Widely used datasets
are VCR (Zellers et al., 2019) and Visual COMmonsense
rEasoning in Time (Visual COMET) (Park et al., 2020).
Vision-and-Language Navigation (VLN)
can be the
equivalent of either visual navigation or linguistic naviga-
tion in the multimodal domain. Some datasets for VLN
are Cooperative Vision-and-Dialog Navigation (CVDN)
(Thomason et al., 2019), Action Learning From Realistic
Environments and Directives (ALFRED) (Shridhar et al.,
2020) and others.
3.5
Image generation
There is a lot of architectural variation when the modal-
ity to be generated is the visual one, greatly diverging
from architectures employed for discriminative visual
tasks. Image generation can be performed by leveraging
Generative Adversarial Networks (GANs), Transformers,
Diffusion models or a combination of them.
3.5.1
Conditional image generation tasks
Conditional Image Generation (cIG)
addresses the
synthesis of an image I guided by textual information
s ∈W, extending uncoditional image generation. Tradi-
tionally, text to image generation is performed by condi-
tional GANs (cGANs) (Mirza and Osindero, 2014), which
attempt to generate realistic images semantically corre-
sponding to a text description. Image generation from text
can be considered as the generative counterpart of image
retrieval from text (TIR), and can be also regarded as
the inverse task of image captioning (IC). Text to image
GANs have been benchmarked on a plethora of datasets,
either containing simpler images with simple conditioning
such as ImageNet (Deng et al., 2009), Oxford Flowers
(Nilsback and Zisserman, 2008), FFHQ (Karras et al.,
2018) and CIFAR (Krizhevsky, 2009), longer condition-
ing such as captions in natural language, as in CUB (He
and Peng, 2020; Reed et al., 2016b), or even more com-
plex scenes accompanied with captions as conditioning,
such as COCO (Lin et al., 2014).
Story Visualization (SV)
refers to the synthesis of a
visual sequence I1, I2, ..., IN based on an input story
c1, c2, ..., cN ∈W, the inverse task of visual storytelling
(VIST). Once again, GAN architectures are leveraged
to produce the sequence of images, which should not
only be realistic, but also maintain the serial progression
from frame to frame while remaining relevant to their
textual description. Datasets accompanying SV research
are CLEVR-SV (Li et al., 2019c), Pororo-SV (Kim et al.,
2017), Flinstones-SV (ﬂi) and DiDeMo-SV (Hendricks
et al., 2017).
3.5.2
Generative VL architectures
Text-to-image synthesis
Adversarial text to image syn-
thesis has demonstrated a long line of impressive results
by converting textual inputs such as captions, interac-
tive dialogs, sequential story-like captions or structured
formats of textual inputs (such as scene graphs and lay-
outs) to plausible images. Some of the ﬁrst ventures
(Reed et al., 2016a,d) achieve synthesizing images, even
though their quality is fairly low. To resolve this limita-
tion, subsequent implementations synthesize images in
stages, increasing image resolution step by step. Stack-
GAN (Zhang et al., 2017) and StackGAN++ (Zhang et al.,
2018a) exploit stacked generators and discriminators, ded-
icated to coarser or ﬁner resolutions. AttnGAN focuses
on individual words rather than whole sentences to syn-
thesize ﬁner details of the image (Xu et al., 2018). This
idea is extended by SEGAN (Tan et al., 2019), which at-
tends on relevant keywords from the sentence rather than
all existing words. The most signiﬁcant parts of the sen-
tences with respect to the image contribute in synthesizing
images with reduced fuzziness in details, as reported in
DM-GAN (Zhu et al., 2019).
Sequential text-to-image synthesis
StoryGAN (Li
et al., 2019c) was the model that introduced the Story
Visualization (SV) task, which concerns synthesizing a se-
quence of related images, maintaining consistency across
story frames. The main StoryGAN architecture consists
of a generator and two discriminators (image and story
discriminators) guided by an RNN structure responsible
of encoding the textual story. Improvements on the basic
model were performed in (Zeng et al., 2019; Li et al.,

2020a). More recent models substituted the RNN en-
coding scheme with transformers (Maharana et al., 2021;
Maharana and Bansal, 2021; Tsakas et al., 2023), follow-
ing the same trend as in other multimodal tasks.
Text-to-image generative VL Transformers
Surpris-
ingly, VL transformers in their original form are not able
of generating realistic images, despite their impressive
capabilities in other visual tasks. This issue is attributed
to the fact that regression based training objectives, such
as the one used in LXMERT (Tan and Bansal, 2019), are
not able to handle feature generation in high dimensional
spaces. Getting VL transformers one step further, X-
LXMERT (Cho et al., 2020) as an extension of LXMERT,
generates caption-conditioned high ﬁdelity images consis-
tent to their descriptions. X-UNITER follows the same
extension logic as X-LXMERT, based on UNITER ar-
chitecture (Chen et al., 2020). The image generation ca-
pabilities of X-UNITER are comparable to X-LXMERT,
showcasing the general applicability of this approach.
DALL-E (Ramesh et al., 2021) uses a 12 billion pa-
rameter version of GPT-3 to generate ﬁne-grained and
highly diverse images based on corresponding textual de-
scriptions. It demonstrates a large range of conditional
synthesis capabilities, such as controlling certain visual
attributes, as well as object positioning in an accurate
way, capturing and visualizing 3D scenes, performing
several natural effects like reﬂections, inferring missing
details from descriptions, combining unrelated concepts
in one image, performing zero-shot reasoning in the vi-
sual domain, incorporating external spatial and temporal
knowledge and others. Once again, a connection between
model scale and advanced performance is observed in
terms of zero-shot generation and regarding the range of
generalization capabilities.
Text-to-image diffusion models
are setting new base-
lines for state-of-the-art conditional image generation
(Dhariwal and Nichol, 2021). They follow a process of
gradually adding Gaussian noise on an image and then
learn to reconstruct it. During the last year, the ﬁeld of
diffusion-based image synthesis has received a variety
of interesting works. Stable Diffusion (Rombach et al.,
2021) enables previously computationally demanding im-
age synthesis even within limited resources scenarios by
applying the diffusion training process in the latent space
of autoencoders rather than pixel-level operations in the
image space. DALL-E2 (Ramesh et al., 2022) extends the
high-quality result of its predecessor (Ramesh et al., 2021)
by using learned text-conditioned image embeddings ob-
tained from CLIP (Radford et al., 2021) as conditionings
to a diffusion model acting as the decoder. Synthesized
images are photorealistic and faithful to their condition-
ing, while zero-shot language-guided manipulation on a
source image is also possible. The concurrent work of
Imagen (Saharia et al., 2022) exploits large pre-trained
language models, such as T5 (Raffel et al., 2020) for
language encoding and proceeds with image synthesis
based on the diffusion process. DreamBooth (Ruiz et al.,
2022) steps upon Imagen to contextualize image synthe-
sis, given variable context described in text. Therefore,
different variations of visual subjects can be obtained,
maintaining high synthesis quality.
Combined GAN/Transformer text-to-image models
Hybrid architectures utilize existing GAN generators to-
gether with powerful VL transformers such as CLIP (Rad-
ford et al., 2021). Given an input text prompt, CLIP is
responsible of guiding image synthesis in the latent space,
based on the text-image similarities it has learned. Fuse-
Dream (Liu et al., 2021b) follows this paradigm by opti-
mizing the latent space of pre-trained GANs for efﬁcient
navigation. Similar hybrid text to image implementations
are Big Sleep1 and VQGAN+CLIP 2.
3.6
Evaluation metrics for VL models
3.6.1
Classiﬁcation metrics
Discriminative VL tasks that provide an answer chosen
among pre-deﬁned candidates can be evaluated via clas-
siﬁcation metrics. Such tasks include visual question
answering (VQA), visual referring expressions (VRE),
visual reasoning (VR), visual entailment (VE).
Accuracy@k measures the proportion of correct an-
swers over all answers, where an answer is considered to
be correct if it belongs within the top-k answers. It can
serve as a generic measure of quality for discriminative
VL tasks, irrespectively of the modality that is predicted
as the answer: for example, in the case of VQA, accuracy
refers to the linguistic modality, as a textual answer needs
to be selected. When bounding boxes have to be pre-
dicted, as in the case of VRE, Intersection over Union
(IoU) provides a measure of success regarding the over-
lap between ground truth and predicted bounding boxes.
Higher values are better, indicating a larger percentage of
overlap.
3.6.2
Ranking metrics
Ranking metrics (Manning et al., 2008) provide further
insights regarding the success of retrieving the right an-
1BigSleep notebook
2VQGAN+CLIP notebook

swer by providing information related to the position the
ground truth answer was ranked. Tasks commonly using
ranking metrics are image captioning (IC), visual story-
telling (VIST), visual dialog (VD), machine translation
(MT). Recall@k (R@k) measures the proportion of to-
tal ground truth instances that were found in the top-k
rank, without taking into account their ordering. Higher
Recall@k scores are better. Precision@k (P@k) is the
percentage of ground truth answers in the top-k over all
retrieved top-k items, without taking into account their
ordering. Higher Precision@k scores are better. When
ordering is to be considered, Mean Reciprocal Rank@k
(MRR@k) acts as a useful performance measure. As
reciprocal rank of an answer is considered the inverse of
the rank position of the -ﬁrst- right answer. By averaging
over all instances, MRR is derived, with higher values
indicating better performance. For N instances, if ranki
denotes the position of instance i, MRR can be written as:
MRR = 1
N
N
X
i=1
1
ranki
Another order-sensitive metric is Discounted cumu-
lative gain (DCG), which measures the gain an answer
offers based on its ranking by taking into account a graded
relevance scale. The graded relevance value is supposed
to reduce logarithmically with respect to the position, thus
penalizing highly relevant answers that appear lower in
the rank. For reli the graded relevance at position i and p
a particular rank position, DCG score for p is deﬁned as:
DCGp =
p
X
i=1
reli
log2(i + 1)
Normalized Discounted Cumulative Gain (NDCG)
is a normalized rank quality score that represents the ra-
tio between the DCG score of the rank returned by an
algorithm divided by the ideal order DCG (iDCG).
Median Rank (MedRank) is the median position of
ranked ground truth answers when all answers have been
considered. In a similar fashion, Mean Rank denotes the
average position of ranked ground truth answers when all
answers have been considered. Lower median and mean
rank values are better.
3.6.3
Similarity metrics
Wu-Palmer similarity (WUPS)
(Wu and Palmer,
1994) reports the degree of similarity between two words
based on their least common subsumer on WordNet (Fell-
baum, 1998) taxonomy. Various thresholds can deﬁne the
agreement between two candidates, with the most typical
being 0.0 and 0.9.
3.6.4
Language generation metrics
Language generation tasks such as IC, VIST and genera-
tive versions of VQA and VCR (VCG) evaluate produced
outputs using the following language generation metrics.
One of the oldest metrics is Bilingual Evaluation Un-
derstudy (BLEU) (Papineni et al., 2002), originally de-
veloped to evaluate machine translation. It compares how
much a machine generated linguistic output matches text
written by a human in a precision-oriented way. Therefore,
it is supposed to demonstrate high agreement with human
perception regarding the quality of generated text, with
higher BLEU score indicating higher perceived quality
and maximum BLEU score being equal to 1 or 100%.
BLEU can not only measure individual word marches, but
also n-gram (grouped words) matches, providing BLEU-
N scores for different N (most usually N = 1 for unigrams,
2 for bigrams, 3 for trigrams, 4 for quadrigrams). Never-
theless, it cannot assess linguistic diversity of generated
text and sometimes it cannot appropriately perform in
practical settings. BLEU is rather reliable for long sen-
tences, but not very helpful in short/monolectic ones.
Recall Oriented Understudy for Gisting Evaluation
(ROUGE) (Lin, 2004) is designed to evaluate the quality
of summarization compared to a human-made summary.
Similarly to BLEU, n-grams of varying N are utilized to
calculate ROUGE-N scores. ROUGE-N offers not only
recall, but also precision and F1 evaluation. Speciﬁcally,
ROUGE-N Precision measures how many overlapping
n-grams were found over the total generated n-grams:
ROUGE-Nprecision =
common n-grams in generated and reference
n-grams in generated
ROUGE-N Recall assesses how many overlapping n-
grams were found compared to the human-made refer-
ence:
ROUGE-Nrecall =
common n-grams in generated and reference
n-grams in reference
Finally, ROUGE-N F1 score takes into account both
ROUGE-N precision and recall:
ROUGE-NF1 =
2 ∗(ROUGE-Nprecision) ∗(ROUGE-Nrecall)
(ROUGE-Nprecision) + (ROUGE-Nrecall)
Additionally, there is ROUGE-L score, which mea-
sures the longest common subsequence (LCS) between
generated and ground truth text, with a longer LCS imply-
ing higher similarity. ROUGE-L can operate in sentence-
level or summary-level and demonstrates precision, recall
and F1-score variants. Speciﬁcally, ROUGE-L Precision

measures the LCS length compared to generated n-grams
count:
ROUGE-Lprecision =
LCS (common n-grams)
count of n-grams in generated
Moreover, ROUGE-L Recall measures the LCS length
compared to ground truth n-grams count:
ROUGE-Lrecall =
LCS (common n-grams)
count of n-grams in reference
ROUGE-L F1 score takes into account both ROUGE-
L precision and recall:
ROUGE-LF1 =
2 ∗(ROUGE-Lprecision) ∗(ROUGE-Lrecall)
(ROUGE-Lprecision) + (ROUGE-Lrecall)
There are also some other rarely used variants of
ROUGE score. ROUGE-W searches for the Weighted
Longest Common Sub-sequence.
ROUGE-S (Skip-
Bigram Co-Occurrences Statistics) measures overlap of
non-consecutive n-grams between generated and refer-
ence sequences. Similar to BLEU, ROUGE scores are
more effective in long sentences rather than short ones.
Metric for Evaluation of Translation with Explicit
Ordering (METEOR) (Banerjee and Lavie, 2005) tack-
les the need for exact word matching as in BLEU score,
and instead enforces semantic matching, taking into ac-
count possible synonyms and paraphrases of words in
the reference text. Semantics matches are made possible
due to the usage of WordNet (Fellbaum, 1998). Unigram
alignment between ground truth and generated sequences
contribute to the METEOR score. Multiple possible align-
ments between the two sequences are resolved by select-
ing the alignment with the fewest crosses, when ground
truth and generated unigrams are matched. METEOR
presents high agreement with human perception regarding
the similarity of generated text sequences.
Consensus-based Image Description Evaluation
(CIDEr) (Vedantam et al., 2014) is a metric inspired from
human agreement when ground truth and generated sen-
tences are compared. Such similarity can be expressed
via TF-IDF score of n-grams across reference sentences,
instructing frequently occurring n-grams in the whole
dataset to have lower weight in the ﬁnal score, as it is
more possible to be less informative (IDF term), while at
the same time increasing the weight of frequent n-grams
within a reference sentence (TF term). Similar to ME-
TEOR, CIDEr exploits semantic matching by comparing
stemmed versions of words in sentences. A cosine simi-
larity score between generated and ground truth vectors
computed from TF-IDF n-gram weights provides the ﬁnal
CIDEr score.
Semantic Propositional Image Captioning Evalua-
tion (SPICE) (Anderson et al., 2016) is an automated
evaluation metric for language generation operating over
scene graphs, which are synthesized from ground truth
captions and generated captions via dependency parsing.
WordNet is utilized for disambiguation and synonym de-
tection. Even though it resolves shortcomings of previous
methods related to n-gram overlaps, the scene graph con-
struction stage is possible to induce errors early in the
evaluation process.
BLEURT (Sellam et al., 2020) is a BERT-based
learned evaluation metric that addresses the shortcom-
ings of traditional BLEU and ROUGE metrics in order to
better correlate the resulting scores with human percep-
tion. It combines the merits of learning pure linguistic
associations, as well as human assessments over language
metrics. Therefore, pre-training on synthetic sentence
pairs, which are designed to capture semantic, syntactic
and lexical information assists in identifying possible dis-
similarities between real and generated text. Afterwards,
ﬁne-tuning adapts automatically captured disagreements
to actual human ratings.
3.6.5
Image generation metrics
Inception Score (IS)(Salimans et al., 2016) evaluates
the quality and diversity of generated images by utiliz-
ing a pre-trained image classiﬁer, such as Inception V3
(Szegedy et al., 2015). The classiﬁer is tasked to provide
a probability of whether an image is generated or real. IS
can have a lowest value of 1.0 and a highest value equal to
the number of classes the pre-trained classiﬁer has seen,
which in case of Inception V3 is 1000 (ImageNet (Deng
et al., 2009) classes). Higher IS implies better quality and
more diversity, therefore higher IS is better.
Fréchet Inception Distance (FID)(Heusel et al.,
2017) compares the distribution of generated images with
the distribution of the real ones by taking into account
the mean and the standard deviation of the distributions.
A pre-trained Inception-V3 (Szegedy et al., 2015) model
is utilized to extract summary statistics for real and gen-
erated images. Lower FID scores are better, indicating
higher similarity between generated and real images.
Learned Perceptual Image Patch Similarity LPIPS)
(Zhang et al., 2018b) is a metric designed to reﬂect the
perceptual similarity between real and generated images
using deep features of image classiﬁers. Lower LPIPS
values indicate more similar images.
R-precision is another widely used metric to evaluate
synthesis quality. Despite primarily being a retrieval met-
ric, it can serve conditional generative models: generated
images are used as queries to ground truth descriptions,

with higher scores indicating better quality of generated
images.
4
Graphs
A graph structure G = V, E consists of a set of nodes V
interconnected with weighted or unweighted edges from
a set E. Edges also can be either directed or undirected
depending on the constrains of relationships they express.
Considering two nodes vi ∈V, vj ∈V, their in between
edge is denoted as (vi, vj) ∈E. Nodes and edges can
additionally contain features. There are some distinct
subcategories of graphs mentioned below, often present
in real world data representation scenarios (Hamilton).
Different types of relationships raise the need for dis-
tinct edge representations, which is satisﬁed via Multi-
relational graphs. The edge notation needs to be altered
to contain the edge type τ, so that the multi-relational edge
notation being (vi, τ, vj) ∈E. Heterogeneous graphs
extend multi-relational graphs by introducing varying
node types, so that nodes can contain labels forming non-
overlapping node sets. Therefore, the node set V can be
expressed as a union of node labels V = V1 ∪V2 ∪...∪Vn
for n distinct and disjoint node categories. Those cate-
gories most often impose constrains on the edge types as
well in order to remain meaningful. Multipartite graphs
are heterogeneous graphs which exclusively contain edges
that connect nodes of different types.
4.1
Knowledge Graphs
A Knowledge Graph (KG) G is a structured representation
of facts F, which consist of entities E, relationships R
and semantic descriptions. Entities can describe either
abstract concepts or actual objects, relationships form
the meaningful connections between entities, and seman-
tic descriptions incorporate types and properties of those
objects and relationships. KGs are directed and hetero-
geneous structures that describe human knowledge in the
form of triplets head - relationship - tail, often denoted as
(h, r, t) ∈F, or equivalently subject - predicate - object,
denoted as (s, p, o) ∈F. (Ji et al., 2021)
While existing edges express known facts, there are two
scenarios for missing edges (Ji et al., 2021): Open World
Assumption (OWA) assumes that unobserved facts are
either missing or false; Closed World Assumption (CWA)
assumes that all unobserved facts are false.
4.2
Graph representation
Graph representation has been a ﬁeld of increasing inter-
est, as it affects numerous applications in artiﬁcial intelli-
gence. Similar to language representations, KG embed-
dings are low-dimensional mappings of the graph entities
and relationships. These vector representations capture
the semantic information contained in KGs, which can
consequently be used for a variety of downstream tasks
(Ji et al., 2021; Hamilton et al., 2018).
Node embeddings constitute a family of graph embed-
ding algorithms aiming to represent the nodes’ position
and context in a vector space. Popular methods regarding
node embeddings are based on random walks, presenting
widely used implementations such as node2vec (Grover
and Leskovec, 2016), DeepWalk (Perozzi et al., 2014),
Large-scale Information Network Embedding (LINE)
(Tang et al., 2015), Graph2vec (Narayanan et al., 2017)
and others. However, standalone node embeddings are in-
adequate for multi-relational graph representations, which
require an overall representation of nodes, edges and at-
tributes. To this end, shallow translational models exploit
geometric capabilities of the vector space to achieve multi-
relational graph representations (Hamilton).
Graph Neural Networks (GNNs) refer to a broad
framework that achieves graph structure and feature rep-
resentations using deep learning. The basic idea behind
GNNs is neural message passing, meaning the exchange
and aggregation of neighboring node information through
their connections, resulting in updating the node embed-
ding itself. The process repeats by expanding in more
distant neighborhoods (hops) in every step, integrating
more information regarding graph structure and neighbor-
ing features (Hamilton).
More speciﬁcally, each node v ∈V of a graph G is
initialized with a random embedding h0 and the following
hidden embedding states hv are computed via a local
transition function f such that:
hv = f(xv, xco[v], hNv, xNv)
(1)
where xv are the features of the node v, xco[v] are the edge
features connected to v, hNv are the embedding states of
the neighbors of v and xNv are their features. The output
is given from a local output function g:
ov = g(hv, xv)
(2)
Graph Convolutional Networks (GCNs) are a vari-
ant of CNNs operating on graphs. However, different than
images, graphs contain unordered nodes with varying
numbers of connections. Given a graph G = (V, E, X),
the GCN receives as an input the feature matrix X contain-
ing node features, and an adjacency matrix A representing
the graph structure. The convolution operation is general-
ized so that the representation of a current node is obtained
by aggregating its own features, as well as the features of
its neighbors. A non-linear transformation is then applied

on the aggregated features. Many approaches stack multi-
ple convolutional layers so that feature information from
further neighborhoods can be received (Kipf and Welling,
2016; Wu et al., 2021b).
GCNs can effectively be applied on relational data and
serve a variety of relevant downstream tasks. Relational
Graph Convolutional Networks (R-GCNs) comprise a
GCN variant for relation-speciﬁc encodings for KGs, i.e.
encodings depending on edge directivity and type. There-
fore, different weights will be assigned to different rela-
tionships (Schlichtkrull et al., 2017).
Graph Attention Networks (GATs) are based on at-
tention mechanisms, which have the ability of handling
variable sized inputs. Therefore attention mechanisms
could be applied successfully for the representations of
graphs containing nodes with different edge degrees. In-
deed, self-attention allows each node to attend to its neigh-
bors, thus computing the hidden encoding. This process
can be parallelized for neighboring pairs, independently
of graph structure (Veliˇckovi´c et al., 2018).
Despite GNNs being able to learn representation on
ﬁxed and homogeneous graphs, they are not so power-
ful on graphs containing heterogeneous information on
nodes and edges. In such cases, embedding extraction
mechanisms should be adjusted appropriately to encode
heterogeneous information. Various types of relationships
create meta-paths between nodes, containing diverse and
rich semantic information. GATs are extended by apply-
ing a hierarchical attention mechanism with node-level
and semantic-level attention, assigning different impor-
tances to nodes and meta-paths. Neighboring features
are aggregated in a hierarchical fashion forming the ﬁnal
node embeddings (Wang et al., 2021b).
Graph Transformer Networks (GTNs) can also han-
dle heterogeneity and generate reasonable meta-paths
connecting nodes, as well as effective node representa-
tion. Meta-paths may be of any length up to the number
of Graph Transformer layers and contain arbitrary edge
types. Multiple generated meta-paths can simultaneously
be considered, deﬁning multiple learned graphs. Due to
continuously generating new graph structures from ad-
jacency matrices contained in data, more powerful node
representations are learned through convolutions, as there
are higher chances of ﬁnding more useful meta-paths. A
GCN is applied on each meta-path, yielding an ensemble
of GCNs when considering all the generated meta-paths
(Yun et al., 2020).
The Heterogeneous Graph Transformer (HGT) is de-
signed to model web-scale heterogeneous graphs using
heterogeneous attention. Node and edge-level dependent
parameters are exploited in order to obtain dedicated node
and edge representations. HGT extracts all related pairs
from a sampled heterogeneous subgraph, so that each
pair contains a source node and a target node. The in-
formation from source nodes is aggregated to provide
a contextualized representation for a target node. This
process can be decomposed in three parts, namely Het-
erogeneous Mutual Attention, Heterogeneous Message
Passing and Target-Speciﬁc Aggregation. The weight ma-
trices for the heterogeneous mutual attention, message
passing, and propagation steps are parameterized using
the meta-relations of the graph. Dynamic representation
graphs are handled using the relative temporal encoding
technique, which captures arbitrary long dynamic struc-
tural dependencies (Hu et al., 2020).
4.2.1
Graph representations in KVL models
Apparently, one important aspect towards KVL models is
how knowledge is incorporated with vision and language
modalities. First attempts to incorporate knowledge stored
in KGs were based on exact matching between extracted
visual and textual concepts from the given input and ex-
isting KG nodes. However, exact matching will result in
errors even when there is a small discrepancy between
KG entities and extracted concepts, limiting the contribu-
tions of additional knowledge. In order to better exploit
semantically rich structured knowledge sources, more re-
ﬁned strategies, such as the usage of GNN representations
are explored in recent literature. The variability of GNN
implementations as described previously allows dedicated
representations for different graphs, with incorporation of
node, edge and feature information. Therefore retrieved
knowledge is more accurate and informative, ultimately
capable of boosting VL models.
5
Knowledge
In recent years, there has been an ever increasing interest
about incorporating external knowledge K in VL models.
We can identify the merits of such an approach: additional
knowledge can offer performance boosting, extendability
and potentially explainability of existing VL tasks. Most
common senses of additional knowledge offering those
beneﬁts are analyzed below.
Hierarchical knowledge refers to is-a relationships
forming a tree structure, with the root serving as the most
generic concept and parent node of all the rest, while
leaves constitute the most speciﬁc concepts. For exam-
ple, cat is-a mammal is an instance that represents such
hierarchical relationships.
Lexical knowledge serves as a structured dictionary,
offering linguistic rules, while being able to resolve issues
such as word sense disambiguation. Lexical knowledge

can be combined with hierarchical knowledge, providing
hypernym/hyponym relationships.
Named entities cover a variety of proper names as in-
stances of entities, and include names of people, locations,
companies, organizations etc (Grishman and Sundheim,
1996). For example, the sentence Joe Biden is the presi-
dent of the United States contains the named entities Joe
Biden and United States.
Factual knowledge includes encyclopedic information
of the world, such as the historical fact WW2 lasted from
1939 to 1945. Such knowledge can also refer to more
speciﬁc scientiﬁc facts, like knowledge in medical, bio-
logical, chemistry domains and many more. Facts can
also be combined with named entities, forming statements
like Zebras live in Africa (Africa is a named entity).
Commonsense knowledge is the self-evident percep-
tion of the world according to humans; sugar is sweet
and if I go out in the rain I’ll get wet are obvious com-
monsense statements. We can identify several discrete
senses of commonsense knowledge, affecting aspects of
the world a human experiences. Such subcategories re-
fer to similarity/dissimilarity concepts, like synonyms
and antonyms of words. Another commonsense variant
includes part-whole (part-of) relationships representing
concepts belonging to more generic ones or consisting
members of a group, for example the bark is a part of a
tree, the tree is part of the forest. Part-whole in terms of
lexical knowledge is expressed via meronyms (part) and
holonyms (whole). Utility relationships describe usage
scenarios, such as a fork is used for eating or capability
(wheels can rotate). Spatial information offers knowledge
about usual locations of objects in the physical world, for
example boats are situated near water, or even geographic
information, such as Italy is located at Europe, which sits
on the intersection with factual knowledge and named
entities. Comparative knowledge provides rules of com-
parison between objects, for example leopards are larger
than cats. Such statements are crucial towards learning
logical reasoning scenarios. Numerical knowledge ad-
dresses common enumerations in real life, providing facts
such as humans have two eyes. Intents, desires and plans
constitute another sense of commonsense knowledge, in-
cluding facts like hungry people want to eat and a hungry
person cooks to eat. Behavioral knowledge results from
logical reasoning over commonsense facts, forming rules
like a child cannot drink 10 liters of water in one day.
Statements like a song is created by a musician or bread
is made from wheat belong to creator knowledge.
Event/temporal knowledge contains chronological in-
formation and order of events, blending factual and com-
monsense knowledge. Events can refer to a large num-
ber of chronologically distinct time periods from widely
known events such as world wars, signiﬁcant political
events, sports, social/scientiﬁc movements and a lot more,
to more specialized events known to smaller audiences.
Temporal sequences can contain chronologically ordered
events. For example, COVID-19 started in 2019. Vaccines
for COVID-19 were developed during 2020 is a factual
sequence of events. Commonsense sequence of events
could contain information such as Spring comes after win-
ter. Sequences can also refer to causal relationships with
the cause preceding the event, such as the boy dropped
a glass of water and then the glass broke, which can
also be transformed to hypothetical if/then statements,
for example if a boy drops the glass of water, the glass
will break, or even counterfactual statements expressing
what would have happened if an alternative scenario oc-
curred, like if the boy had not dropped the glass of water,
the glass would not have been broken.
Visual knowledge contains images and possibly ad-
ditional annotations to connect visual perception with
commonsense. Attributes of objects, such as shape, color,
texture and others can be connected with their visual
counterpart, visualizing commonsense situations such as
tomatoes are red and round. Visual knowledge is ideal
for learning instances of the world about object relation-
ships and attributes, paving the way for more complex
reasoning required in several multimodal tasks. Spatial
relationships are naturally combined with images; for
example apples placed inside a bowl, bowl placed on a ta-
ble. More types of relationships can be further visualized,
including actions between visual entities such as a girl
is holding a tennis racket, object details like black and
white stripped hat, part-whole has-a relationships such
as woman has long blonde hair or scene text like a truck
with Coca-Cola logo. Those rather obvious statements
can be extended to commonsense assumptions, such as
the temperature is low, when an image of an icy landscape
is provided. More complex visual instances can provide
information about intents (a customer enters a restaurant
to eat, a person holding a suitcase and a passport plans
to travel), causes/effects (a biker cycling out in the rain
will get wet), factual instantiations (an ancient Greek
temple of the 5th century BC, girls with Japanese kimono
dresses),similarity reasoning (the dog’s toy looks similar
to a plate), similarity including named entities (A man
looking similar to Brad Pitt), creator knowledge (the
painting was created by a person holding a paintbrush),
capability (a cat can jump on the tree branch).

5.1
Types of knowledge sources
We divide external knowledge in three categories: im-
plicit knowledge, present in non-symbolic form, ex-
plicit knowledge, typically stored in structured knowl-
edge bases, and web-crawled knowledge, acquired from
various online sources, usually in unstructured form.
Moreover, we can recognize the category of internal
knowledge or self-knowledge, which does not rely on
external sources, but rather obtains extra knowledge from
the existing data.
Implicit knowledge refers to information stored in a
non-symbolic form, such as neural network weights. The
indisputable popularity of neural architectures in recent
deep learning literature has led to numerous relevant con-
tributions, even if their primary goal deviates from knowl-
edge representation. Unsupervised or self-supervised pre-
training of transformer models can provide implicit knowl-
edge in several linguistic (Safavi and Koutra, 2021) or
multimodal tasks. Therefore, incorporating large-scale
linguistic and visual data in the pre-training stage can
seemingly form unstructured knowledge bases.
Insightful studies have attempted to discover the opti-
mal pre-training regime and whether it serves as a neces-
sary prior for performance. The right dataset and design
choices are crucial for achieving successful representa-
tions, which can become a prerequisite for the overall suc-
cess of the ﬁnal task, after proper ﬁne-tuning. First, the
amount and type of pre-training data need to be examined.
Speciﬁcally, the relevance of the selected pre-training
dataset with respect to the downstream task has been
proven more inﬂuential rather than dataset size, an obser-
vation that remains valid even when generated datasets
are used over out-of-domain natural ones. Only scaling up
in-domain data seems to positively impact the model, en-
couraging the scalability of implicit knowledge bases. In
some cases, especially when selected pre-training datasets
are not diverse enough, pre-training transferability to-
wards downstream tasks is low, so that in fact pre-training
knowledge is deemed insufﬁcient. In the same fashion,
even though the pre-training scenario which demonstrates
the lowest losses can be regarded as the best prior, it can
be proven suboptimal without proper ﬁne-tuning. Data rel-
evance seems to be more signiﬁcant than model size, even
though deeper single stream models mitigate the semantic
gap between images and text in later layers. However, for
double-stream models earlier layers present more narrow
semantic gap, disagreeing with the single-stream observa-
tion (Cao et al., 2020; Singh et al., 2020).
Other investigations question if all participating modali-
ties contribute equally to the learned representations, prov-
ing that during inference, the contribution of language is
more prominent than vision in both single-stream and
double-stream encoder architectures. Nevertheless, rich
visual knowledge is encoded in pre-training, effectively
capturing visual relationships (Cao et al., 2020). Such
observations can offer valuable insights in the complex
pre-training procedure and provide enhancements on the
knowledge acquired, towards more high-quality, ﬂexible
and robust implicit knowledge bases.
Pre-training holds the advantage that it can exploit un-
labelled data for achieving a generic understanding of
language (Devlin et al., 2019; Brown et al., 2020), which
can serve as an initial point for linguistic or VL tasks. In
most multimodal cases however, a level of supervision is
required, such as the need for paired images and captions.
The current abundance of paired image-text samples re-
solves the labelling issue up to some point, at least for
general-purpose domains. The raw nature of linguistic,
visual and paired image-text data that are typically used
for pre-training alleviate the need for a strict represen-
tation, which would limit the ﬂexibility of incorporated
knowledge in many aspects, including storage, expres-
sivity and accessibility (Safavi and Koutra, 2021). Addi-
tionally, the automatic incorporation of extra information
by repeating pre-training or performing ﬁne-tuning can
help extending and reﬁning the required implicit knowl-
edge, contrary to handcrafted knowledge bases which
are hard to be extended at scale. Even though such pre-
training procedures are very expensive computationally,
luckily pre-trained transformer models are offered to the
research community in ready-to-use models. Transfer
learning can then effectively leverage existing implicit
knowledge sources achieving impressing results in vari-
ous tasks, which accounts for many advancements in VL
tasks and transformer-based implementations in general.
Nevertheless, implicit knowledge is not always sufﬁ-
cient to answer questions requiring general, factual and
commonsense knowledge, especially when rare informa-
tion is requested. Additionally, its black box nature raises
concerns about how and what a pre-trained model has
learned; for example, biased or erroneous data received
during pre-training will be reﬂected in all later stages,
resulting in decreased performance of the downstream VL
model. Tracing back the source of such a problem is not
possible due to the lack of interpretability tied to implicit
knowledge bases.
Explicit knowledge is based on clear, structured facts
in the form of a knowledge graph and it is able to explicitly
ﬁll the gaps that cannot be covered via transfer learning.
Even though most contemporary multimodal approaches,
including transformer-based ones, have acquired a cer-

tain understanding of language, visual concepts and their
in between relationships, they cannot effectively handle
concepts and relationships they have never seen during
training (Ilievski et al., 2021) (excluding implementations
that present zero-shot capabilities (Radford et al., 2021;
Wang et al., 2021c)). Consequently, even the best VL
models will fail in cases the data distribution is signiﬁ-
cantly different from the one they have been trained on.
The same discrepancy may apply even when an implicit
knowledge source is used, if the implicit distribution re-
mains rather distant. For example, a model trained on
pairs of generic images and corresponding captions will
inevitably present much lower metrics when asked to infer
from medical images accompanied by relevant captions
with scientiﬁc vocabulary. The same limitation is preva-
lent when there is a shortage of training data (Ilievski
et al., 2021). Although an intuitive scenario would sug-
gest to repeat the pre-training procedure, so that this extra
information will be reﬂected via updated neural weights,
the pre-training cost is in reality computationally unaf-
fordable (Sharir et al., 2020) for the majority of research
institutions. Even in that case, repeated occurrences of
out-of-distribution data would demand from scratch pre-
training or at least ﬁne tuning each time, preventing the
scalability of related tasks.
Another issue strongly interconnected with massive pre-
training is the lack of explainability (Kaﬂe et al., 2019), as
it is difﬁcult to track what and how a pre-trained model has
learned from data. This black box nature poses questions
regarding how rare concepts or rare combinations present
in data are handled and whether they can be represented
with equal success as the more common ones. At the same
time, possible data biases, errors and inconsistencies will
be reﬂected in learned representations, with those issues
being hard to be captured and resolved beforehand.
On the contrary, in the case of explicit knowledge bases,
the degree of contribution of the knowledge base can be
measured and evaluated, offering valuable transparent
insights regarding the role of knowledge. Such out-of-
distribution information is well represented in structured
knowledge graphs. Large scale knowledge can comple-
ment pre-trained models by extending their understanding
to previously unseen concepts, either by substituting the
need for extra training, or by enriching existing datasets
to achieve more informative, fair and high quality repre-
sentations, if (re-)training is necessary. Even in that case,
pre-training demands can be reduced, achieving similar
representation capabilities to larger models pre-trained
without additional explicit knowledge. The quality of
such representations is somehow controllable, a beneﬁt
which can be attributed to the explicit and transparent na-
ture of KGs: issues regarding biases, errors, concept drifts
and inconsistencies can be captured and resolved easily,
exploiting automatic techniques or manual interventions.
In any case, KGs should contain relevant information to
the downstream task in order to be beneﬁcial (Ilievski
et al., 2021).
There are some downsides regarding the usage of ex-
plicit knowledge in the form of KGs. First, many KGs
may require manual labor for data collection and curation.
The same disadvantage also applies on the construction
and maintenance of the graph itself. In certain cases, such
as in the medical domain, experts are necessary in order
to design and construct dedicated KGs. Moreover, there
are difﬁculties regarding alignment and co-operation be-
tween different KGs (Ilievski et al., 2021), thus sometimes
limiting in practice the improvements they offer.
Combining implicit and explicit information can of-
fer advanced capabilities to downstream tasks, as implicit
sources can fuse large-scale general knowledge to a model,
while explicit sources can ﬁx errors, enrich existing knowl-
edge and increase a model’s transparency.
Web-crawled knowledge refers to unstructured knowl-
edge obtained from the web, which is able to combine
beneﬁts present in implicit and explicit knowledge bases.
There is no need for labelled data, but also no need for
expensive pre-training, which is one major limitation of
implicit knowledge. Online sources can be accessed eas-
ily, while the amount and the content of retrieved knowl-
edge is easily controlled and customized to the task’s
needs. A questionable part of web-scrapped knowledge
is its quality, as it is hard to validate any available web
source. Low-quality data may deteriorate the ﬁnal per-
formance of the model, therefore time and effort has to
be invested in techniques that ensure high-quality data
automatically. Web knowledge can offer some amount of
transparency, as a sentence leading to the ﬁnal prediction
can be tracked, even if reasoning is not as fully explicit as
in cases of structured graphs.
Internal or self-knowledge is a knowledge type that
does not rely on any external source, as it can be obtained
from existing textual and visual data themselves. For
example, producing a scene graph can enable learning
more ﬁne-grained representations compared to just uti-
lizing VL data in their original format (Yu et al., 2021a).
Self-knowledge has demonstrated improvements in down-
stream model performance, especially when detailed dis-
ambiguation is necessary, as it enables better associations
between existing data. However, self-knowledge does not
extend the knowledge a model has already acquired from
the data it has been trained on. Furthermore, it is prone to
errors associated with the knowledge acquisition process,

such as scene graph generation errors.
An overview of the available types of knowledge
sources is provided in Figure 4.
5.2
Widely used Knowledge graphs
Widely used knowledge graphs in literature are analyzed
below. WordNet (Fellbaum, 1998) is a large-scale lexi-
cal database which provides cognitive synonyms, called
synsets, for words (nouns, verbs, adjectives and adverbs)
of the English language, representing their conceptual
and lexical relationships. In total there are 117K Word-
Net synsets that form a tree-structure through their re-
lationships. Verb and noun synsets are interlinked with
transitive hierarchical (is-a) relationships, bringing seman-
tically related synsets close together. Therefore, distance
on the WordNet graph provides a measure of concept
similarity. There is also a distinction between types and
instances, with types (common nouns) expressing more
speciﬁc meanings of a concept (cat is a type of animal),
while instances are speciﬁc persons, countries and geo-
graphic entities (Rome is an instance of a city). The root
node of the WordNet tree belongs to the {entity} synset,
the most generic concept and parent of all other synsets.
Traversing from root to leaves leads to more and more
speciﬁc concepts, with instances always being located at
the leaf level. Synsets easily offer word sense disambigua-
tion, as certain words that have many distinct meanings
are mapped to a different synset, so that ﬁnally all synsets
represent different meanings. Part-whole relationships are
also present and parts of a current node can be inherited
from parent nodes, but not vice versa. Adjectives are
linked via semantic similarity and dissimilarity links, with
antonyms and synonyms directly related.
ConceptNet (Speer et al., 2017) is a multilingual com-
monsense knowledge graph, containing a variety of re-
lationships that express many dimensions of common-
sense knowledge. In its current version of ConceptNet
5, it is comprised of ∼8 million nodes interconnected
by 21 million edges. Speciﬁcally, it contains hierarchi-
cal relationships (IsA), lexical (DerivedFrom, FormOf),
path-whole (PartOf, HasA, MadeOf), similarity (Synonym,
SimilarTo), dissimilarity/distinctness (Antonym, Distinct-
From), spatial (LocatedNear), utility (CapableOf, Used-
For), creation (CreatedBy), quality (HasProperty, Sym-
bolOf), desire/goal (CausesDesire, ObstructedBy), tem-
poral (Causes, Entails, HasSubevent) and relational-other
(RelatedTo, HasContext) (Ilievski et al., 2021). Concept-
Net data are aggregated from different sources, with ini-
tial versions based on crowdsourcing via the Open Mind
Common Sense website. Supplemental sources include
other knowledge bases, such as WordNet for lexical infor-
mation, DBPedia for Wikipedia infoboxes and OpenCyc
ontology for high-level commonsense facts, as well as
multilingual sources serving similar knowledge content.
Additionally, knowledge about intuitive linguistic associa-
tions is gathered via so-called ’games with a purpose’.
DBPedia (Auer et al., 2007) is a multi-lingual knowl-
edge base that extracts factual structured information from
Wikipedia. It covers a variety of domains and evolves to-
gether with Wikipedia, thus handling concept drifts. DB-
Pedia also provides SPARQL endpoints to enable queries.
Wikidata (Vrande˘ci`c and Krötzsch, 2014) is a free
collaborative multilingual knowledge base of more than
6.7k relationships and more than 97 million data items,
available to everyone for editing. It provides links of
entities to their sources and other databases, endorsing the
veriﬁability of contents.
WebChild (Tandon et al., 2014) is a large common-
sense knowledge base automatically collected from the
web. It presents high accuracy of statements, with ﬁne-
grained entries involving part-whole, comparative, prop-
erty, activity and spatial relationships. All entries are
disambiguated by mapping on WordNet synsets. In to-
tal, WebChild 2.0 (Tandon et al., 2017) contains more
than 2 million concepts, interconnected by 18 million
relationships.
HasPartKB (Bhakthavatsalam et al., 2020) contains
part-whole statements extracted from a large generic cor-
pus, covering numerous common terms.
Due to the
huge possible instances of part-whole relationships in real
world, salient parts are preferred, referring to instances
that are most probably useful to be stored. to all Entities
are linked with WordNet and Wikipedia.
YAGO4 (Tanon et al., 2020) is a general purpose
knowledge base storing knowledge about people, places,
movies, organizations and others. It consists of 2 bil-
lion triples and 64 million entities automatically extracted
from Wikipedia. WordNet is leveraged for entity and rela-
tionhsip disambiguation. YAGO4 actually forms a cleaner
and more human-readable version of Wikidata, satisfying
logical constrains, therefore allowing automated reason-
ing on the data.
ATOMIC (Hwang et al., 2021) is a commonsense
knowledge graph with 1.33M tuples regarding events and
entities, incorporating both social and physical senses of
everyday experiences. In total, it contains 23 relation-
ship types, with 9 types referring to social interactions, 7
types concerning physical entities and 7 types represent-
ing event relationships.
Visual Genome (Krishna et al., 2016) can be also
viewed as a knowledge base, thanks to the scene graph
annotations that explicitly showcase entities, relationships

Figure 4: Overview of knowledge sources.
and attributes present in a scene, accompanied by the ac-
tual visual information of each image. Mappings to Word-
Net synsets offer disambiguation as well as hierarchical
relationships between visual instances. In general, scene
graphs can act as spatial and visual knowledge bases.
6
Multimodal Tasks with knowledge
Recent efforts of integrating internal and external knowl-
edge sources in VL tasks are analyzed in this section,
together with datasets and evaluation methods they follow.
As mentioned in Section 2, we can divide existing KVL
approaches in single-task and multi-task models. Start-
ing from single-task models, we ﬁrst present works in the
most developed ﬁeld of discriminative KVL tasks, and
more speciﬁcally understanding tasks such as visual ques-
tion answering (VQA), visual reasoning (VR), visual com-
monsense reasoning (VCR), followed by generative tasks,
such as image captioning (IC), visual storytelling (VIST),
story visualization (SV), conditional (text-to-image) gen-
eration (cIG). Finally, multi-task models, targeting more
than one downstream tasks at once are analyzed.
In Figure 5 an overview and taxonomy of KVL tasks is
presented. Some tasks such as visual referring expressions
(VRE), visual question answering (VQA), visual com-
monsense reasoning (VCR) and visual dialog (VD) can
be either discriminative or generative, even though their
discriminative variants are more widespread, being com-
paratively easier. Single-task models are focusing either
on generative or understanding tasks, while there is no
single-task model focusing exclusively on a retrieval task
such as cross-modal retrieval (TIR/ITR) and visual refer-
ring expressions (VRE). Those two tasks are exclusively
tackled -among with others- by multi-task models. In the
same fashion, visual entailment (VE) is only addressed
by multi-task models. Finally, a couple of tasks such as
visual-language navigation (VLN) and multimodal ma-
chine translation (MMT) still lack a knowledge-enhanced
counterpart.
The upcoming sections are primarily organized start-
ing from single-task approaches as per task, followed by
multi-task approaches in the end. Datasets, methods and
evaluation metrics are provided for each independent task.
A more detailed division of methods is achieved based on
the knowledge type (external/internal) and the language
representation scheme (sequential models/transformers)
per approach.
6.1
Knowledge in Visual Question Answering
(K-VQA)
6.1.1
Datasets
The following datasets have been used in implementations
that have integrated external knowledge sources in order
to provide an answer. Many of those, including VQA
(Agrawal et al., 2016; Goyal et al., 2016), VQA-E (Li
et al., 2018), COCO-QA (Ren et al., 2015) are also used in
knowledge-free versions of VQA, and external knowledge
is not required in order for their questions to be answered.
VQA (Agrawal et al., 2016) was the ﬁrst dataset intro-
ducing the task of Visual Question Answering. It contains
approximately 204K images with diverse and complex
scenes, with at least 3 open-ended free-form questions
per image and a total of 760K questions in the whole
dataset. Many of those questions are commonsense re-
lated, such as Is this a vegeterian pizza?. For each ques-
tion 10 ground truth answers are suggested from different
annotators, with a total of almost 10 million answers in
the dataset. Most answers are short, with the majority
of them consisting of a single word. The answers can

Figure 5: A taxonomy of VL tasks with knowledge
be evaluated either in open-ended or in multiple-choice
settings. Open-ended answers should be validated by 3 an-
notators agreeing on exactly the same answer for a given
question. The multiple-choice scenario regards 18 unique
candidate answers per question. Such candidates can be
correct answers, obtained from the 10 matched answers
per question, plausible answers, i.e. possibly incorrect
answers provided by annotators without viewing the im-
age, popular answers, i.e. the most frequently appearing
answers in the dataset, and random answers sampled from
other random questions within the dataset.
VQA with Explanations (VQA-E) (Li et al., 2018)
pursues the tractability of the reasoning process leading
to an answer. In total, it contains around 108k images,
and more than 269k explanations assigned to an equal
number of QA pairs. Based on VQAv2 (Goyal et al.,
2016), it automatically constructs explanations with the
help of COCO captions (Lin et al., 2014), as they are
connected with VQAv2 images. Caption embeddings and
question/answer embeddings are coupled, forming pairs
of highest cosine similarities, thus assigning captions to
images. Resulting explanations are highly diverse, with
more than 171k unique instances, although they cannot
cover images with subjective questions, such as emotional
(Do you think this pony is cute?), commonsense (Can you
cross the street?) or behavioral (Could you eat all these
bananas by yourself?) ones. Human evaluators assess
the quality of explanations, measuring is they are ﬂuent,
correct, relevant and complementary to the answer.
DAQUAR (Malinowski and Fritz, 2014) is a dataset
of real world indoor scenes containing ﬁne-grained ob-
ject categories. Questions and answers related to the
images are very rich regarding the objects they express:
573 unique nouns are mentioned within the whole corpus
of questions and answers. Questions requiring common-
sense knowledge such as Which object on the table is
used for cutting? are included in DAQUAR, while even
spatial questions such as What is above the desk in front
of scissors? can be beneﬁted from additional knowledge.
COCO-QA (Ren et al., 2015) addresses shortcomings
of the DAQUAR (Malinowski and Fritz, 2014) datasets,
such as its small size in terms of train/test samples and
the limited number of object classes. COCO-QA contains
123,287 images, together with 78,736 train and 38,948
test questions obtained from COCO image descriptions
(Lin et al., 2014). Questions are divided in 4 types with

varying numbers of questions in each of them: Object,
Number, Color and Location questions.
KB-VQA (Wu et al., 2016b) has been constructed in or-
der to evaluate VQA models on questions that need visual
information as well as external knowledge to explicitly
infer the right answer. It includes images from COCO
(Lin et al., 2014) containing approximately 150 object
classes and 100 scene classes, question-answer pairs fol-
lowing pre-deﬁned templates and question labels. The
questions involved in KB-VQA are divided in three cat-
egories: visual questions can be answered by extracting
information from the image (such as Is there a dog in
this image?); common-sense questions rely on external
knowledge contained in commonsense knowledge bases
(How many road vehicles are in this image?); ﬁnally, KB-
knowledge questions require information form Wikipedia
or similar sources (When was the home appliance in this
image invented?).
Factual VQA (FVQA) (Wang et al., 2018a) is a
dataset addressing factual VQA, based on images sampled
from COCO (Lin et al., 2014) and ImageNet (Deng et al.,
2009) which form three types of visual content (object,
scene and action classes), together with structured visual-
related knowledge extracted from DBpedia (Auer et al.,
2007), ConceptNet (Speer et al., 2017) and WebChild
(Tandon et al., 2014). All this information is stored in a
graph of RDF triplets. Annotators construct questions and
answers which require both selected visual content and
associated facts. In total, FVQA contains 2,190 images of
326 object classes and 221 scene classes, 5,826 questions
of 32 categories, which correspond to 4,216 unique facts.
Knowledge-aware VQA (KVQA) (Shah et al., 2019)
targets world knowledge-aware VQA by ﬁlling the gap of
named entities knowledge. It relies on knowledge present
in Wikidata (Vrande˘ci`c and Krötzsch, 2014) knowledge
graph, resulting in 183K question-answer pairs which
involve more than 18K named entities and 24K images.
Outside-knowledge VQA (OK-VQA) (Marino et al.,
2019) contains more than 14K diverse and difﬁcult ques-
tions of 10 mutually exclusive categories which cannot
be answered without external knowledge. More than 14K
images were sampled and ﬁltered from COCO (Lin et al.,
2014). In contrast with previous related works, OK-VQA
does not consult a ﬁxed knowledge graph to guide answer
prediction, but dynamically recognizes what knowledge
is needed, either structured or unstructured.
Text-KVQA (Singh et al., 2019) is a very large dataset
addressing scene-text recognition for knowledge enabled
VQA. It contains images from book covers (Iwana et al.,
2017) and movie posters (mov), as well as Google scraped
images of 1000 business brands. All images are eval-
uated so that they contain for sure scene text relevant
to the content. Knowledge bases corresponding to each
of those 3 scene types were constructed based on Wiki-
data (Vrande˘ci`c and Krötzsch, 2014) for business scenes,
IMDb (imd) for movie posters and Iwana et al. (2017)
for book covers. The train/validation/test splits enable
zero-shot capabilities, as there is no entity overlap be-
tween them. The supporting facts are not tied with their
corresponding entities, but instead are dynamically mined
from the knowledge bases.
Visual7W+KB (Yu et al., 2020) is an extension of the
Visual7W (Zhu et al., 2016) test split which further con-
tains knowledge-based visual questions guided from Con-
ceptNet (Speer et al., 2017). However, the dataset is not
tied with a speciﬁc knowledge graph, even though Con-
ceptNet is indeed preferred in practice. In total it consists
of 16,850 open-domain question-answer pairs and 8,425
images from Visual Genome (Krishna et al., 2016). The
questions belong to 7 categories (what, where, when, who,
why, which and how), while the answers are provided in
multiple-choice format.
One of the major challenges in knowledge-enhanced
VQA is that questions should encourage exploitation of
all participating modalities, therefore data-related weak-
nesses arise in existing benchmarks. In the meanwhile,
information leakage between train and test set answers
often promotes guessing rather than reasoning. S3VQA
(Jain et al., 2021) is a dataset aiming to tackle those is-
sues, by containing questions that can be answered only
with the help of a knowledge graph, as well as visual and
textual information from the image.
Zero-shot Fact VQA (ZS-F-VQA) (Chen et al.,
2021c) extends F-VQA (Wang et al., 2018a) for zero-shot
learning settings. It considers the image-question-answer
triples whose answers belong among the 500 most fre-
quent ones. The ﬁltered dataset is split in train (seen)
and test (unseen) triples which contain non-overlapping
answers. In total, 5 splits of the original F-VQA dataset
are performed, yielding on average 2,732 train and 2,760
test triples.
Art QUestion Answering (AQUA) (Garcia et al.,
2020) is a visual reasoning dataset for the art domain.
There are many challenges tied with analyzing and rea-
soning over artworks. First, there are different levels of
abstraction regarding common objects and entities, as
many paintings deviate from realism. Therefore, recog-
nizing objects and reasoning about them is much harder
compared to scenes existing in most datasets. Moreover,
domain knowledge regarding artists, art movements, his-
torical periods and other cultural inﬂuences can only be
recognized with the help of a knowledge source. This

information also affects the interpretation of a painting.
QA pairs are generated automatically based on paintings
and descriptions of the SemArt (Garcia and Vogiatzis,
2018) dataset, which form the knowledge source. In total,
AQUA contains more than 69K QA training pairs after
cleansing, from which around 29K pairs are visual and
40K pairs are knowledge oriented.
6.1.2
Methods
6.1.2.1
Keyword-based explicit KG querying
First attempts target the construction of a scalable mul-
timodal knowledge base which aims to answer visual
queries that require real-world knowledge. Image classes,
attributes and actions are extracted from the images, form-
ing logical rules. The knowledge base built upon those
rules contains nodes of visual and textual entities, as well
as edges of diverse types between the entities. (Zhu et al.,
2015) However, this constructed knowledge base remains
limited to the visual information present speciﬁcally in the
SUN (Xiao et al., 2010) dataset. Most subsequent meth-
ods utilize already constructed large knowledge bases, tar-
geting a broader range of concepts, commonsense knowl-
edge and more complex questions to be answered.
Towards this direction, early approaches focus on han-
dling open ended questions regarding contents of a scene
with the assistance of provided external knowledge. At-
tributes extracted from images using a ﬁne-tuned VGG-16
(Simonyan and Zisserman, 2015) model act as SPARQL
queries to knowledge bases such as DBPedia (Auer et al.,
2007), and contribute to caption generation. Retrieved
knowledge embedded via Doc2Vec (Le and Mikolov,
2014), together with attributes and LSTM-based caption
representations are fed in another LSTM model which
generates the ﬁnal answer. (Wu et al., 2016b) An im-
provement of this version followed in (Wu et al., 2016a),
extending the framework to two more datasets, namely
DAQURA-ALL (Malinowski and Fritz, 2014) and its re-
duced version DAQUAR-REDUCED.
Even from early works in knowledge-enhanced VQA,
explainability is addressed as an important topic to de-
ﬁne how a model actually learns from visual content
and external knowledge towards concluding to an answer.
Therefore, Wang et al developed a knowledge-enhanced
VQA framework that provides the reasoning path from
which the answer is inferred. Objects are detected using
Fast-RCNN (Girshick, 2015) object detectors trained on
ImageNet (Deng et al., 2009) and MS-COCO (Lin et al.,
2014), scene classes are extracted from a VGG-16 (Si-
monyan and Zisserman, 2015) pre-trained on MIT-Places
(Zhou et al., 2014), and scene attributes are captured via
a VGG-16 pre-trained on ImageNet and ﬁne-tuned on
MS-COCO. All those visual concepts form RDF triples
and are linked with corresponding DBPedia (Auer et al.,
2007) entities. Questions are parsed so that key-phrases
are extracted and mapped to the knowledge base entities.
The same work introduced the KB-VQA dataset. (Wang
et al., 2017)
Consequent works further proceed towards avoiding
SPARQL queries on the knowledge graph, but rather fully
utilize embedding representations for fact selection and
reasoning to provide an answer.
6.1.2.2
Sequential language models for question en-
coding
Instead of SPARQL querying from plain keyword extrac-
tion, vector representation of involved modalities set the
basis for improved performance and state of the art re-
sults in knowledge-enhanced VQA. Initially, fact ranking
based on embedding similarity metrics paved the path of
successful approaches, upon which graph neural network
reasoning further advanced the contribution of external
knowledge and overall performance.
Embedding-based fact retrieval from KG
Tradi-
tional VQA models utilizing RNNs for language encod-
ing, focus on learning a question-answer mapping. Due
to the limited and opaque reasoning capabilities of this
approach over diverse answers, a more scalable solution
that involves learning the mapping between questions and
KB-queries using LSTMs was proposed. This new ap-
proach is explainable, as the fact connecting a question
and an answer reveals the reasoning procedure. (Wang
et al., 2018a)
Projecting question-image pairs and facts on a com-
mon embedding space poses advantages over previous
approaches, such as extendability to different knowledge
bases and error elimination by avoiding explicit query
generation. Images and questions are embedded using
CNNs (for objects, scenes and actions) and an LSTM
respectively, and they are projected in a common space
using a Multi Layer Perceptron. Another LSTM is used
to retrieve facts from the knowledge base, which are then
encoded in GloVe (Pennington et al., 2014) vectors. The
dot similarity between the question-image representation
and fact embeddings provides a fact ranking, from which
the ﬁnal answer is inferred. (Narasimhan and Schwing,
2018)
Narasimhan et al, building upon (Narasimhan and
Schwing, 2018), argue that considering muiltiple relevant
facts instead of a single top-ranked fact at a time leads
to better generalization. During the fact retrieval stage, a

subset of highly-relevant facts is obtained with the help of
LSTM-based question embeddings. In the answer predic-
tion stage, each node is represented by concatenating the
selected entity representation from the previous stage, vi-
sual features from the image and the question embedding.
The subgraph formed from all the relevant facts is jointly
assessed by a graph convolutional network (GCN) (Kipf
and Welling, 2016), followed by a multi-layer perceptron
that decides if each entity constitutes the ﬁnal answer or
not. (Narasimhan et al., 2018)
Based on the KVQA dataset, a memory network (mem-
Net) framework sets the baseline for VQA with knowl-
edge of named entities. Speciﬁcally, entities extracted
from the question and the image are used to obtain facts
from Wikidata (Vrande˘ci`c and Krötzsch, 2014) knowl-
edge graph. Retrieved facts together with corresponding
entity coordinates from the image are used to produce
memory embeddings via a BiLSTM network, and a sim-
ilar procedure is followed for the question embeddings.
Both representations contribute to the ﬁnal answer, which
is deﬁned by a multi-layer perceptron. (Shah et al., 2019)
Text present on an image can provide further informa-
tion towards inferring the correct answer. Extracted text
and image areas are fused together with the given question
to retrieve relevant facts during the fusion stage, and a
multi-relational graph is constructed based on all those
components. The text recognition part relies on word
proposals assisted by the knowledge graphs accompany-
ing the text-KVQA dataset. Scene proposals were created
with the help of Places dataset for scene recognition (Zhou
et al., 2018) and a ﬁne-tuned VGG-16 (Simonyan and Zis-
serman, 2015). A gated graph neural network (GGNN)
performs one-hop reasoning on this graph to derive the
ﬁnal answer. (Singh et al., 2019)
Multimodal graphs
Unexpected noise in the answer
inference process can be attributed to the absence of de-
tailed selection of information during modalities fusion.
Considering multiple views of the same image offers a
new perspective that is closer to human cognition. Multi-
ple knowledge graphs provide visual, semantic and factual
information derived from corresponding images, text and
facts respectively, while the visual and the semantic graph
can be considered as instances of the factual graph. Intra-
modal graph convolutions focus on the most relevant parts
of each modality. Consequently, cross-modal knowledge
reasoning on the fact graph iteratively aggregates informa-
tion from the visual and semantic graphs using a recurrent
module, and after multi-step reasoning the multimodal
knowledge is fused in each entity. The ﬁnal answer is
returned by applying a GCN (Kipf and Welling, 2016)
over those entities. This approach offers interpretabil-
ity by revealing the entity and the modality graph which
contributed to the answer. (Yu et al., 2020)
A similar approach to (Yu et al., 2020) utilizes visual,
semantic and factual graphs for image representation to
eliminate noise during multimodal fusion. The Multi-
Modal Heterogeneous Graph Construction stage is respon-
sible of constructing those modality graphs, followed by
the Cross-Modal Heterogeneous Graph Reasoning which
selects intra-modal knowledge and then performs cross-
modal reasoning. Information relevant to the question
is extracted from the three graphs via a modality-aware
heterogeneous graph convolutional network. Cross-modal
convolutions deﬁne complementary relevant information
transmitted from visual and semantic graphs to the fact
graph. The ﬁnal answer is returned after reasoning over
aggregated fact information. (Zhu et al., 2020)
A major challenge in knowledge-enhanced multimodal
tasks is its supervised nature, as a possible absence of
ground truth facts may hinder the inference of a proper
answer in several approaches. A local subgraph is con-
structed based on concepts present in the image and the
question, aiming to bridge the gap between question-
image context and external knowledge. Those subgraph
concepts act as anchor points to a knowledge graph,
such as ConceptNet (Speer et al., 2017) and Wikidata
(Vrande˘ci`c and Krötzsch, 2014), enabling the expansion
to their immediate neighbors. Moreover, a global sub-
graph is constructed in a similar fashion for all the candi-
date answers. In each subgraph the information of neigh-
boring nodes is aggregated to produce embeddings of the
anchor concepts, and their similarity to the query embed-
dings drive the ﬁnal answer. (Li et al., 2020b)
Multiple feature spaces
Addressing the zero-shot set-
ting of knowledge-enhanced VQA, a knowledge graph
can help capturing semantics outside training data. Mul-
tiple feature spaces are used for independent alignment
between image/question input and KG entities. The se-
mantic space focuses on the linguistic information of the
input (image, question) pair, representing a feature space
of relationships; the object space acts as a support entity
feature space, capturing visual and textual salient features;
ﬁnally, knowledge space is dedicated to answer represen-
tation. (Chen et al., 2021c)
6.1.2.3
Transformer-based models
Transformer based approaches form end-to-end architec-
tures that utilize single-modality or joint representations
rather than creating queries to knowledge bases and then

injecting the retrieved entities. Thus, we can classify the
transformer-based approach in two categories: the ﬁrst
includes architectures which use transformer architectures
for text encoding, while the second utilizes multimodal
transformers to jointly encode vision and language.
Transformer architectures for language encoding
Similarly to (Yu et al., 2020; Zhu et al., 2020), the us-
age of dedicated graphs for different modalities is also
followed in (Ziaeefard and Lécué, 2020), attempting to
represent relationships between visual objects and seman-
tic entities present in a scene graph and a knowledge graph
respectively. The scene graph is constructed from visual
and question embeddings which form the graph nodes
and relationships. In the meanwhile, joint image and
question embeddings select the most relevant knowledge
graph node embeddings to construct the concept graph.
Both image-question and knowledge representations are
obtained via pre-trained language models for sentence
similarity, such as sentence-BERT (SBERT) (Reimers and
Gurevych, 2019) and Universal Sentence Encoder (USE)
(Cer et al., 2018). The most relevant nodes of both scene
and concept graphs are selected via a Graph Attention
Network (GAT) (Veliˇckovi´c et al., 2018), which decides
the edge weights with respect to the question. A joint em-
bedding incorporates the question embeddings together
with the scene graph and knowledge graph outputs.
Even though explicit and structured knowledge bases
constitute the majority of the approaches analyzed so far,
implicit and unstructured knowledge can also boost the
VQA task. GPT-3 (Brown et al., 2020) can retrieve knowl-
edge based on text prompts and effectively reason over it
in a few-shot manner: no ﬁne-tuning is required and in-
stead only a few examples during inference are provided.
Captions are ﬁrst extracted from images using VinVL
(Zhang et al., 2021a) to form GPT-3 inputs. Regarding
sample selection for the few-shot inference stage, both
improving the quality and increasing the number of sam-
ples have been explored. The top-n most similar prompt
examples comparing to the inference-time question to be
answered are deﬁned by CLIP (Radford et al., 2021), thus
maximizing sample relevance. On the other hand, multi-
ple queries corresponding to one inference-time example
can be used to retrieve answers from GPT-3 using n ex-
ample prompts each time, and their ensembling results in
the ﬁnal answer. Contrary to most works on knowledge-
enhanced VQA, inferring the answer is a generative task
and not a discriminative one among pre-selected candidate
answers or graph nodes. (Yang et al., 2021)
Unimodal pre-trained transformers yield better gen-
eralization capabilities over multimodal approaches of
comparable size when external knowledge is necessary.
Language models employed for the knowledge-enhanced
VQA task are sufﬁcient even to compensate for the limita-
tions of image captioning models, which often fail to fully
capture visual semantics. To this end, a pre-trained image
captioning system, in this case the multi-task OSCAR
(Li et al., 2020c) transformer, is used to extract linguistic
information from an image, while a language model such
as BERT, acting as an implicit knowledge source, receives
the caption and the question to infer an answer. Moreover,
text-only and multimodal approaches have complemen-
tary capabilities, therefore their combination can yield
even more powerful models. (Salaberria et al., 2021)
VIKING is a framework accompanying AQUA dataset
(Garcia et al., 2020) for visual QA on the artistic domain.
As questions in AQUA may either be visual or knowledge
oriented, a modality selector ﬁrst decides the right cate-
gory by receiving the encoded image and question. Visual
oriented questions do not require external knowledge in
order to be answered. For knowledge-oriented questions,
a two stage fact retrieval strategy is followed, pairing the
given question with the most relevant painting description,
which corresponds to the external knowledge fact needed.
The ﬁrst stage utilizes TF-IDF to rank descriptions accord-
ing to the question, and in the second stage re-ranking
is performed using BERT. Finally, a ﬁne-tuned XLNet
(Yang et al., 2019b) model provides the ﬁnal answer.
Joint multimodal encoding with attention-based fu-
sion
A slightly different technique is followed in (Zheng
et al., 2021), where the visual modality is not captioned,
but fused with the BERT-embedded question. More specif-
ically, a knowledge graph for artistic VQA is construcyed
based on YAGO (Tanon et al., 2020) knowledge graph
and AQUA (Garcia et al., 2020) dataset. A Hierarchical-
Knowledge Embedding module is responsible of retriev-
ing relevant relationships r from the knowledge graph that
can form a (h, r, t) triple with question related entities
serving as the head h of the triple, and answer related
entities as the tail t. A Network-Based Representation
Learning module extracts visual and textual features and
fuses them together in order to obtain a VL represen-
tation. The fusion part ﬁrst applies local attention per
modality, and then global attention on both text and im-
age, where locally ’attended’ text features form the query,
and visual features the key and value. Query, key and
value are inserted in a multi-head attention unit which
further promotes the joint representation to consequent
layers, until a global representation is obtained. Then, a
Knowledge-Based Representation Learning module in-
jects hierarchical-knowledge embeddings to the network-

based representation. This representation is inserted into
a relational module, which performs meta-training: a rep-
resentation learned on the training data is transferred to
a support set of disjoint labels. Finally, the relational
module derives the answer.
Joint multimodal encoding with VL transformers
ConceptBERT is one of the ﬁrst attempts towards the end-
to-end transformer-based direction, where all modalities
are jointly exploited for learning. The ﬁrst step includes
obtaining representations for each individual modality.
Visual features are extracted using a pre-trained Faster R-
CNN network (Ren et al., 2017) and BERT (Devlin et al.,
2019) provides the question representation. ConceptNet
(Speer et al., 2017) acts as the commonsense knowledge
source, and is encoded using the ConceptNet embedding
(Malaviya et al., 2019) method, a Graph Convolutional
Network (Kipf and Welling, 2016) variant that relies on
message passing from node to node in order to obtain
the ConceptNet graph representation. Two modules re-
ceive the embedded inputs: A vision-language module
consisting of two streams in a ViLBERT (Lu et al., 2019)
fashion, and a concept-language module based on the bidi-
rectional Transformer architecture are proposed to model
the interactions between the relevant modalities. Both
outputs of these modules are joined to form a concept-
vision-language representation, which ﬁnally concludes
to the answer via a classiﬁer. (Gardères et al., 2020)
Knowledge obtained from the web according to the
given question and respective answer can act as a large ex-
ternal implicit knowledge source for the OK-VQA dataset,
covering knowledge ’gaps’ in several domains without
manual human effort. The proposed weakly-supervised
framework consists of two phases: the ﬁrst one (Retriever)
retrieves relevant knowledge, which guides answer pre-
diction in the second stage (Reader). Two different ap-
proaches are followed for representing the question-image
pair inputs: either the question and image are encoded
using an LXMERT (Tan and Bansal, 2019) transformer,
resulting in a multimodal representation, or the image cap-
tion and the question are encoded via BERT (Devlin et al.,
2019), leading to an exclusively linguistic representation.
The BERT-based linguistic encoding can contribute to
both a neural based retriever and a term based (Robert-
son and Zaragoza, 2009) retriever. A similarity score
deﬁnes the relevant knowledge for the neural based re-
triever, which is further concatenated with the question
representation, and consequently with the image using
again an LXMERT model. (Luo et al., 2021)
KRISP framework addresses the scenario where es-
sential external knowledge is absent during training, as
well as test time. Both implicit and explicit knowledge
sources are utilized: Explicit knowledge combines DBPe-
dia (Auer et al., 2007), ConceptNet (Speer et al., 2017),
VisualGenome (Krishna et al., 2016) and hasPart KB
(Bhakthavatsalam et al., 2020) in a knowledge graph of
36,000 edges and 8,000 nodes after ﬁltering out irrele-
vant concepts, while pre-training using BERT can offer
implicit knowledge. Explicit visual symbols from images
are extracted to constrain the knowledge graph entities
corresponding to image-related concepts, including ob-
jects, parts of objects, attributes and places. Likewise,
symbols are extracted from question to contribute to the
formation of a graph for all explicit symbols. A Relational
Graph Convolutional Network (RGCN) is used for graph
representation, allowing dedicated processing for differ-
ent edge types and directions. After reasoning, a symbolic
prediction vector is returned. Regarding the implicit infor-
mation stream, a multimodal BERT (MMBERT) model
incorporates visual and textual embeddings to produce an
implicit prediction vector. Finally, the top-ranked predic-
tion from both vectors deﬁnes the answer. (Marino et al.,
2021)
The presence of scene text can offer valuable informa-
tion for properly predicting the correct answer. The de-
tected text, among with the image, the relevant knowledge
from Google Knowledge Base (GKB) and the question
representation are fed into a multimodal transformer, en-
abling interaction through attention mechanisms between
the different modalities. The OCR extracted text act as
a query to GKB to retrieve candidate entities, which are
then disambiguated based on the visual context. External
knowledge not only boosts the understanding of scene-text
even in unseen instances, but also tackle biases present in
training data. (Dey et al., 2021)
While employing an abundance of knowledge sources
can cover more visual topics, a lot of noise may be
introduced, as more irrelevant information is retrieved.
MAVEx utilizes multi-granular queries to retrieve exter-
nal knowledge with the purpose of validating and correct-
ing predicted answers among suitable candidates with the
help of various knowledge sources. Speciﬁcally, a ﬁne-
tuned ViLBERT (Lu et al., 2019) model creates a pool of
candidate answers, and together with the corresponding
question, extracted keywords and phrases from the ques-
tion and the possible answers are utilized to query external
knowledge. Wikipedia, ConceptNet and Google images
act as knowledge sources regarding different views of
knowledge. Finally, retrieved knowledge instances are
matched with the queries to acquire the highest ranked
supporting fact, which returns the degree of agreement
with respect to candidate answers, guiding decision to-

wards the most trustworthy knowledge source. (Wu et al.,
2021a)
Passage retrieval can serve as an answer selection tech-
nique to VQA instead of choosing among pre-deﬁned
candidate answers. Both sparse and dense retrieval are
investigated. For sparse retrieval, given a question and
an image, visual clues such as object names and captions
are extracted from the image and BM25 is used to re-
turn the k most relevant passages. For dense retrieval,
questions and images are jointly encoded in dense vectors
using LXMERT. In any case, retrieved external knowledge
can be integrated dynamically from diverse and generic
sources, without using a ﬁxed knowledge base. As posi-
tive passages are considered the ones containing exactly
the ground truth answer. LXMERT (Tan and Bansal,
2019) is used to encode the question and the image jointly,
while BERT encodes the passage, and a dot similarity
between them deﬁnes the k most relevant passages. (Qu
et al., 2021)
Based on the E-BERT (Poerner et al., 2020) strategy
of knowledge injection without expensive re-training,
LXMERT (Tan and Bansal, 2019) language encoder in-
put is modiﬁed to incorporate factual knowledge from
Wikipedia by aligning Wikipedia2Vec embeddings with
BERT wordpiece vectors. No other change is required
within the language encoder’s architecture, while the vi-
sual encoder remains entirely intact. Only ﬁne-tuning is
required for achieving advanced accuracy due to knowl-
edge injection. In the meanwhile explainability regarding
visual and textual modalities is also enhanced. For this
purpose, BM-GAE (Chefer et al., 2021) is employed to
extract visual and token explanations that help identify
in which parts knowledge injection was helpful. (Garcia-
Olano et al., 2021)
S3, presented with the S3VQA dataset, targets to an-
swer visual question based on all the participating modal-
ities simultaneously. Entity spans from the question are
selected to be matched with objects of scene graphs cor-
responding to images. This match can be often guided
by external knowledge sources, which enables answering
more complex questions that require multi-hop reasoning.
BERT identiﬁes those appropriate question spans, while
object detectors propose the objects that most likely ﬁll
the spans. Wordnet (Fellbaum, 1998) synsets are mapped
to the objects, and their hierarchical positions are repre-
sented via structural embedding methods. Finally, Google
search is used to retrieved the top results of the enriched
question representation. Alternatively, the answer can be
provided via classiﬁcation of possible candidate answers.
(Jain et al., 2021)
6.1.3
Evaluation
Classiﬁcation/Ranking metrics are widely used in K-
VQA, following the paradigm of knowledge-free VQA,
with most works rely on the top-1 accuracy metric for
comparison. Accuracy metric is further decomposed to
explain the contribution of subcomponents in many works:
object, counting, color, and location accuracies, as well as
accuracy per question type are reported (Wu et al., 2016b;
Wang et al., 2017). Other accuracy reportings include
performance as per selected knowledge source, per vi-
sual concept and per answer source (image or knowledge)
(Wang et al., 2018a); the individual accuracies of involved
stages of the reasoning process, which together contribute
to answer prediction (Narasimhan and Schwing, 2018;
Narasimhan et al., 2018); accuracies as per question cat-
egory (Shah et al., 2019; Yu et al., 2020; Ziaeefard and
Lécué, 2020; Gardères et al., 2020). Moreover, preci-
sion@k and recall@k have appeared in fewer works, as
well as ranking metrics such as MRR. Some early works
(Wu et al., 2016b; Wang et al., 2018a) utilize WUPS with
typically used thresholds of 0.0 and 0.9.
Human evaluation provides further insights compar-
ing to solid accuracy-based scores that fail to fully de-
scribe the success or the shortcomings of a metric. There-
fore, many authors propose human evaluation experiments
to grade the model’s response comparing to human per-
ception, and count the number of agreement instances
over all results (Wang et al., 2017, 2018a; Narasimhan
et al., 2018).
Evaluation of reasoning paths is followed by employ-
ing human judgement, as being one of the most trustwor-
thy indicators (Wang et al., 2017). Due to the transpar-
ent reasoning process, failure cases can be traced down,
revealing the exact stage where the prediction deviated
from the intended one. Thus, shortcomings can be at-
tributed to architectural choices, encoding techniques, or
even incorrect data annotations. Other metrics regarding
explainable reasoning include top-k fact retrieval accu-
racy for different k values, a crucial step for returning the
correct answer in several approaches (Wang et al., 2018a;
Narasimhan and Schwing, 2018; Narasimhan et al., 2018;
Zhu et al., 2020). Fact recall can also assess the frac-
tion of relevant facts retrieved for a given question (Singh
et al., 2019).
Generally, benchmarking knowledge-enhanced VQA
approaches is not trivial. The plethora of combinations
between available knowledge based datasets and external
knowledge sources is rather large comparing of the num-
ber of available implementations in the ﬁeld. Additionally,
various choice of metrics in literature makes comparisons
of model performance even harder. Only recently imple-

mentations started becoming more consistent, focusing
on evaluating results with plain accuracy and leveraging
OK-VQA as a widely used dataset.
6.2
Knowledge in Visual Reasoning (K-VR)
6.2.1
Datasets
High-order Visual Question Reasoning (HVQR) (Cao
et al., 2019) is a knowledge-based dataset endorsing inter-
pretable visual reasoning using commonsense knowledge.
Given an image and a question, an answer is inferred, as
well as a reasoning path as explanation. Even thought this
is similar to rationales used in knowledge-free datasets
for VCR (Zellers et al., 2019) in fact the format of HVQR
explanations differ. Instead of textual rationales, rules for
the whole reasoning path are returned, combining visual
and knowledge oriented triples, derived from the scene
graph and the commonsense knowledge graph respec-
tively. HVQR contains questions that require multi-step
reasoning to infer an answer. Moreover, each knowledge
triplet appears only once per question, in order to avoid
frequency-based biases. An evaluation scheme validates
each step of the reasoning process based on the common-
sense and scene graphs provided. More than 157K QA
pairs comprise the dataset, from which 289,720 pairs are
unique, together with approximately 32K images and cor-
responding scene graphs from Visual Genome (Krishna
et al., 2016). Based on the reasoning steps required for
the answer, ﬁrst-order and second-order questions can
be recognized, corresponding to 68,448 and 88,753 ques-
tions respectively. Another split deﬁnes 87K KB-related
questions and 70K KB-not-related questions. Addition-
ally, 193,449 facts from WebChild (Tandon et al., 2014),
ConceptNet (Speer et al., 2017), and DBpedia (Auer et al.,
2007) formulate the knowledge base. Scene graphs per
image are combined with related entities from the knowl-
edge base, constituting image-speciﬁc knowledge graphs.
Compositional Language and Elementary Visual
Reasoning - CLEVR (Johnson et al., 2017) is a syn-
thetic dataset of 3D objects which contain annotations
regarding their position and attributes. Those attributes
describe the size (small, large), color (red, brown, yellow,
green, blue, cyan, purple, gray), shape (cube, cylinder,
sphere) and material (rubber, metallic) of each object. Po-
sitions can belong in 4 types, namely left, right, behind,
in front. Highly compositional questions form 5 question
categories: Exist, Count, Compare Integer (equal, less,
greater), Query Attribute (size, color, material, shape) and
Compare Attribute (size, color, material, shape). More-
over, CLEVR contains 90 question families following
different program templates, as well as text templates,
so that natural language questions can be derived. The
questions are translated in natural language by ﬁlling the
template with template parameters. CLEVR is not con-
nected to some external knowledge, although due to the
limited semantics and the nature of the task they target
CLEVR CoGenT is a benchmark derived from
CLEVR (Johnson et al., 2017) that assesses the ability to
capture unseen combinations of attributes during testing,
thus showcasing a model’s generalization capabilities.
6.2.2
Methods
6.2.2.1
Sequential language models
External knowledge
KM-net (knowledge routed mod-
ular network) is introduced in the same work with the
HVQR dataset (Cao et al., 2019), addressing multi-step
(compositional) reasoning using visual and commonsense
knowledge. Each question is decomposed into consec-
utive subqueries via LSTM encoder-decoder schemes,
passed to a visual reasoning module and a commonsense
reasoning module to extract different types of knowledge
accordingly. The subqueries form a query layout, i.e. a
tree structure revealing the relationships of subqueries,
with leaf nodes belonging to distinct words of the queries.
A bottom-up attention R-CNN provides visual features
for the image. The subqueries are processed sequentially
starting from the most speciﬁc ones, driven by the KM-
net reasoning module. First, the knowledge reasoning
module receives question entities from the query layout
and returns the most probable candidate entities from the
knowledge base. Then, the visual reasoning module re-
ceives entities from the scene graph, together with the
candidate entities of the knowledge module and fuses the
candidate entities, image features and query embedding
to derive the answer.
Internal knowledge
Self-knowledge includes the us-
age or construction of a scene graph based on the detected
objects, relationships and attributes. In the same time,
the question can be parsed in a structured program via an
LSTM, producing subqueries that form a tree structure.
Given those two graph representations, an encoding is
derived for the query. Node attention and edge attention
are calculated based on the query embedding. Combining
a node attention vector and an edge attention matrix, new
objects can inferred due to the graph structure; basically
starting from the attended node vector and traversing over
an attended edge, a new node vector will be provided.
The same procedure can be followed for all subqueries,
respecting the structure of the query tree. Logically relat-
ing subqueries, results in logical operations (such as and,
or, not) over attended scene graphs. Finally, based on the

question type and the ﬁnal scene graph, the answer can be
provided. (Shi et al., 2018)
6.2.3
Evaluation
Classiﬁcation metrics are commonly used for bench-
marking, with answer accuracy providing a general mea-
sure of performance (Cao et al., 2019; Shi et al., 2018).
Compositional commonsense reasoning heavily relies on
the evaluation of reasoning paths that provide the ﬁnal
answer (Cao et al., 2019). The accuracy score is further
decompose to present KB-related and KB-not-related ac-
curacies depending of the need for external knowledge;
those can be decomposed into ﬁrst-order and second-
order accuracies, regarding the number of reasoning steps
required; ﬁnally, a more ﬁne grained categorization pro-
vides question-type accuracy, based on the template the
query components follow.
Ranking metrics such as average recall are used to
evaluate the retrieval success of supporting facts for ex-
planations. Average recall is further decomposed to KB-
related and KB-not-related fact recall.
6.3
Knowledge in Visual Commonsense Reasoning
(K-VCR)
Various rich in information external knowledge sources
can provide insights of unseen concepts that humans
would effortlessly infer from the information provided
in a scene. This missing commonsense knowledge is able
to guide answer explanation towards the right rationale,
revealing if a more accurate reasoning process is followed
by VCR models when knowledge is added.
6.3.1
Datasets
Visual Commonsense Reasoning (VCR) (Zellers et al.,
2019) is the dataset which introduced the task and serves
both knowledge-free and knowledge-enhanced versions of
VCR. It contains 110k unique images from movie scenes,
290k multiple choice challenging questions, with 290k
correct answers and rationales. Images contain annota-
tions which are anchored over questions, answers and
rationales. The technique of adversarial matching is cho-
sen for the answers in order to minimize biases; each
correct answer appears four times in the whole dataset,
once as a positive answer and three times as negative
answer. Therefore, a VCR model will not favor more
frequently appearing answers which would endorse guess-
ing rather than reasoning. The questions are classiﬁed
in non-mutually exclusive categories according to their
purpose, with categories being Explanations (Why is [per-
son11] wearing sunglasses inside?), Activity (What are
[person1] and [person2] doing?), Temporal (What will
[person6] do after unpacking the groceries?), Mental
(What is [person3] thinking while [person5] shakes his
hand?), Role (What is [person1]’s relation to [person4]?),
Scene (Where is [person1] now?), Hypothetical (What
would happen if [person3] fell asleep?). This dataset
originally is knowledge-free, therefore not necessarily
requiring external knowledge, nor is it associated with
any knowledge base. Nevertheless, the questions existing
in the various VCR (Zellers et al., 2019) question cat-
egories can be greatly beneﬁted by the introduction of
external knowledge sources which can explicitly incorpo-
rate senses like the ones described in section 5.
Visual Commonsense Graphs (VCG) (Park et al.,
2020) is a large scale dataset that provides information
regarding temporal commonsense relationships, such as
what may have happened before, what may happen in
the near future and what are the intents of the people
present based on static images. In total it contains more
than 59K images and more than 139K textual descriptions
of event at present. Additionally, around 295K intents
at present, as well as more than 584k events before and
586k events after complete the dataset, resulting in more
than 1,4 million commonsense inferences. People and
locations appearing in the images are grounded with their
mentions in the textual descriptions.
6.3.2
Methods
6.3.2.1
Transformer-based models
External knowledge
Some of the ﬁrst knowledge-
enhanced transformer-based attempts stepped upon BERT
(Devlin et al., 2019) framework to introduce knowledge-
vision-language (KVL) learning as an instance of mul-
timodal learning. In the KVL-BERT architecture (Song
et al., 2021), ConceptNet (Speer et al., 2017) is leveraged
to enrich sentences with relevant commonsense informa-
tion. The knowledge-enriched linguistic input will be
inserted in a BERT-like multimodal transformer. The
preservation of semantic structure is achieved by using
relative position embeddings. However, injected informa-
tion should be only visible to their corresponding textual
entities of the sentence and not other tokens or visual
features, a need that is satisﬁed via a ’weakening’ visible
matrix. Moreover, it is possible that different enriched
textual tokens in the sentence share the same relative po-
sition embeddings, which would make unrelated tokens
obtain high self-attention scores, implying that they are re-
lated. This contradiction is resolved by imposing a mask-
self-attention mechanism via the visible matrix, with the
purpose to restrict the area a token can attend. After those
treatments, the input is in a form suitable to be fed in a
VL transformer, in this case VL-BERT (Su et al., 2020).
It was observed that KVL-BERT outperforms its multi-

task baselines, as well as models dedicated to the VCR
task, even though it cannot trespass the performance of
knowledge-free VL transformers that invest on additional
pre-training.
A somehow different strategy is employed in the case of
Vision–Language–Knowledge Co-Embedding (ViLaKC)
(Lee and Kim, 2020): the three modalities are ﬁrst em-
bedded independently and afterwards are fused together.
Initially, a knowledge extraction module (KEM) retrieves
relevant knowledge from ConceptNet based on concepts
appearing on the image, question and candidate answers.
The encoding of modalities is performed in the two-stage
VLKEM module: ﬁrst, the independent modality encod-
ing embeds images using ResNet (He et al., 2016), lan-
guage using BERT (Devlin et al., 2019) and knowledge
using GCN (Kipf and Welling, 2016). The second stage
consists of the co-embedding submodule which aligns
and integrates the three vectors via a multi-head self-
attention mechanism. The co-embedder is pre-trained
in two phases, the ﬁrst being task-agnostic, such as in
several VL transformer models, and the second task-
speciﬁc, utilizing signiﬁcantly less data (∼200K samples)
coming from all three modalities. The task-speciﬁc pre-
training stage introduces novel pre-training tasks, such
as masked language modeling with image and knowl-
edge (MLMIK), masked object classiﬁcation with text and
knowledge (MOCTK), and vision-language-knowledge
matching (VLKM), in order to enforce co-learning. This
joint embedding is then inserted in an answer determina-
tion module (ADM) consisting of a fully connected layer
followed by a softmax.
The CKRM framework (Wen and Peng, 2021) con-
sists of two stages, the ﬁrst used for knowledge retrieval
and the second one for reasoning. SWAG (Zellers et al.,
2018), is a dataset containing pairs of situations which de-
scribes a situation (context) and possible endings, serves
as the commonsense knowledge source, aiming to trans-
fer knowledge regarding everyday situations to the target
task of VCR. A source and a task encoder are respon-
sible of receiving (context, ending) pairs and (question,
answer) pairs respectively to perform knowledge trans-
fer in different granularity layers. The encoders ﬁrst use
BERT (Devlin et al., 2019) followed by a BiLSTM struc-
ture to model temporal interactions of words. Cell-level
knowledge transfer refers to the most ﬁne-grained infor-
mation fusion from source to target task, with layer-level
and attention-level knowledge corresponding to coarser
aspects of information. This strategy offers acquiring
knowledge from various perspectives for a more enriched
representation. The knowledge based reasoning module
incorporates the multi-level knowledge from the previ-
ous stage together with visual features in the Knowledge-
enriched visual attention module. Finally, a reasoning
composition module combines all aspects of knowledge
derived from the multi-level transfer procedure and the
enriched visual representations to derive the answer.
6.3.3
Evaluation
Classiﬁcation metrics, especially classiﬁcation accu-
racy is employed for evaluating K-VCR results that fol-
low the multiple-choice format for answers (A) and ratio-
nales (R). Accuracy is decomposed by evaluating inde-
pendently each of the following aspects:
1. Q −→A: given a question Q, choose as A one of
the 4 available answers and compare if it matches
the real answer or not.
2. QA −→R: given a question Q and the correct
answer A, select as R one out of the 4 rationales and
compare if it matches the real rationale or not.
3. Q −→AR: given a question Q select as A one of
the 4 answers, and depending on selected A choose
one of the 4 rationales. The result is regarded to be
correct if and only if both right A and R are chosen.
6.4
Knowledge in Image Captioning (K-IC)
6.4.1
Datasets
There are no dedicated knowledge-enhanced or knowl-
edge demanding datasets for K-IC. Knowledge-enhanced
models are using COCO captions (Lin et al., 2014) as
described in section 3.4.3. Moreover, Flickr30k (Young
et al., 2014), a dataset containing 31,783 scene images
accompanied by 5 human annotated sentences each is
widely employed for K-IC.
6.4.2
Methods
6.4.2.1
Sequential language models
External knowledge
First attempts for knowledge-
enhanced image captioning propose the extension of ex-
isting implementations by injecting commonsense knowl-
edge from external sources. Speciﬁcally, the backbone im-
age captioning architecture extracts visual features from
images via a CNN, which are then inserted in an LSTM
to generate a knowledge-free answer. To enhance this
baseline with knowledge, objects extracted from the im-
age are used as queries to ConceptNet (Speer et al., 2017).
Related ConceptNet entities, either regarding individual
objects (direct terms) or the remaining image areas (indi-
rect terms), are fed to a pre-trained LSTM which provides
semantic representations for each of those two points of
view. Then, visual features, direct terms representations

and indirect terms representations are concatenated to
form the initial state of another LSTM model, which ﬁ-
nally generates the knowledge-enhanced caption (Zhou
et al., 2019c).
Both visual and commonsense knowledge for image
captioning are used in (Hou et al., 2019). The ﬁrst step
includes dense region sampling from images in order to
acquire visual and knowledge mappings. Dense visual fea-
ture extraction includes the deﬁnition of candidate regions,
which are clustered together to provide a more concrete
representation: the cluster center points for each dense
region cluster serve as the corresponding visual feature.
Consequently, the knowledge mapping receives visual
features and knowledge embedding vectors from Visual
Genome (Krishna et al., 2016) and returns a knowledge-
related representation per region cluster. Both visual and
knowledge embeddings resulting from the two mapping
procedures are concatenated and then inserted in a com-
monsense reasoning module. This module projects the
two inputs in the same semantic space, from which a
semantic graph is constructed under the guidance of com-
monsense knowledge. In the relational reasoning module
a GCN (Kipf and Welling, 2016) operates on the seman-
tic graph to obtain relation-aware node features. Finally,
a LSTM receiving as inputs the knowledge-aware node
embeddings generates the caption.
Inferring words not appearing in the image remains a
challenge in image captioning, as there is no guidance re-
garding how those unseen words should be inferred to be
used in captions. Such unmatched elements can be solved
with internal self-knowledge based on more ﬁne-grained
alignments between individual words and image regions,
which is achieved by attention mechanisms, and external
commonsense knowledge to capture implicit information
that cannot be derived from the existing data. Objects de-
tected on the image are used to retrieve knowledge from
ConceptNet (Speer et al., 2017). Region features extracted
from the image via a region proposal network and word-
level attention on the sentence part co-operate towards
attending to the most salient features of the image. This
visual attention guided by language attention, together
with the corresponding word embedding are inserted in a
LSTM, which feedbacks each previous hidden state to up-
date the word-level attention signal that contributes to the
visual attention in every round. The external knowledge is
incorporated in a later stage, when the answer is generated;
therefore, it can tune the probabilities of LSTM-generated
words to be added in the sentence towards more mean-
ingful results. A reinforcement learning training strategy
is followed by setting the LSTM as an agent, the words
and visual features as the environment, and the generation
of the best next word from the captioning model as the
policy. (Huang et al., 2020a)
Even though local information is well-represented
based on detected objects, image captioning is gener-
ally not interpretable and therefore not explicitly con-
trollable.
An external knowledge source can help in
grounding detected objects with semantic entities from the
graph, which in turn provides enriched semantic labels
for the objects present in the image. In order to con-
trol objects appearing in the caption, an attention-based
human-interpretable mask is introduced, which assists in
diverse caption generation. This masked can be dynami-
cally tuned by a human to inﬂuence the resulting caption.
(Aditya Mogadala, 2020)
Off-the-self object detectors have served several image
captioning architectures. However, some tough situations
such as very small objects, occlusion or rare object classes
can result in error propagation and negatively impact all
consequent components until the ﬁnal caption generation.
Commonsense constrains and semantic correlations ex-
tracted from Visual Genome (Krishna et al., 2016) can act
as priors to guide a more accurate representation. A se-
mantic graph is constructed upon extracted image regions,
allowing GCN-based (Kipf and Welling, 2016) reason-
ing which. Speciﬁcally, visual semantics such as objects,
attributes, relationships are captured by extracting candi-
date region proposals. CNN-based region features can
satisfy object and attribute representations, while features
from regions union areas provide relationship representa-
tion. Visual features are projected in the same high-level
semantic space as Knowledge embedding derived from
Visual Genome. Therefore, knowledge-enhanced visual
triplets are formed, respecting rules imposed by knowl-
edge. The semantic graph is built upon those triples. Then,
relational reasoning is performed on the semantic graph
using a GCN, the output of which is inserted in the LSTM
module that generates the answer. (Hou et al., 2020)
Internal knowledge
Visiolinguistic priors are naturally
connected with describing images, in the sense that hu-
mans logically infer unseen entities given a partial de-
scription of a visual situation. Obtaining such priors from
existing images and captions is a way of ’creating’ knowl-
edge and facilitate reasoning of image caption models
without adding external sources.
Scene graph generation is a widely used technique for
self-augmentation of information present in the dataset.
Both images and text need to be represented in graph
structures to bridge the two modalities. The Scene Graph
Auto-Encoder (SGAE) framework utilizes this graph con-
version to instill language priors into the encoder-decoder

image captioning structure. More speciﬁcally, a learnable
dictionary maps the relationships between a sentence and
its corresponding scene graph iteratively, reconstructing
the initial text from the generated graph in each round.
For scene graph generation from text, a pre-trained scene
graph parser is utilized, while for the reverse procedure,
a trainable RNN decoder converts the dictionary back to
text. During this procedure, the dictionary achieves to
capture the necessary language prior to be transferred for
captioning. The learned dictionary can then be inserted in
the image-involving pipeline: a scene graph parser con-
verts the image to a scene graph, which is then passed
to the dictionary encoded by a GCN (Kipf and Welling,
2016). Finally, the decoding of the dictionary provides
the ﬁnal caption. (Yang et al., 2018)
Attention mechanisms are able to identify such struc-
tured visiolinguistic priors and highlight connections be-
tween text and images, therefore augmenting image cap-
tioning implementations. Conditional Latent Topic Atten-
tion (CLTA) in combination with sentence prior are able to
fuse the model with prior knowledge without the need of
constructing scene graphs. Latent topic models are able
to recognize semantically signiﬁcant topics which are
driving attention mechanisms to capture local and global
dependencies in images. Thus, salient visual features
emerge through words, and also more candidate salient
regions are discovered and re-weighted accordingly, if
they are associated with a topic contributing to an existing
salient region. CLTA implements this re-weighting proce-
dure to construct a context vector. Moreover, a sentence
autoencoder acting as the sentence prior encourages the
extraction of more context information and enhances gen-
eralization. Both the context vector and the sentence prior
are inserted in a LSTM that generates the answer. (Goel
et al., 2020)
6.4.2.2
Transformer-based models
Recent knowledge-enhanced image captioning models
are implemented based on Transformers as an expected
substitution of sequential models.
External knowledge
Named entities and event knowl-
edge has not been studied in previous image captioning
works. This type of information is widely available in
news articles, with raw sources being too complicated
for language models to infer the right semantics. Special
datasets are crafted for this purpose, providing an appro-
priate form of information for named entity/event-aware
image captioning. The heart of the proposed method is
the cross-modal entity matching, which incorporates in-
formation from various sources. Sub-graphs are extracted
from the image and the article text descriptions forming
structure representations for the input. The nodes of the
text sub-graph belong to named entities, and the edges to
their in-between relationships, while the image sub-graph
is more generic, by representing objects present on the
image. The two sub-graphs are linked via similarity be-
tween image sub-graph objects and text sub-graph named
entities in the cross-modal entity matching module. This
module is trained with the help of multimodal external
knowledge from Wikipedia. As a result, a multimodal
knowledge graph is produced containing visual, textual
and knowledge information. Embedding representations
are obtained for each modality: a GAT (Veliˇckovi´c et al.,
2018) produces a multimodal knowledge graph embed-
ding, RoBERTa (Liu et al., 2019b) encodes news captions
and image features are derived from a pre-trained ResNet-
152 (He et al., 2016). An entity-aware captioning model
receives the visual, textual and multimodal knowledge
graph representations, feeding them to a Transformer
(Vaswani et al., 2017) decoder to produce the caption.
(Zhao et al., 2021)
BART transformer (Lewis et al., 2019) can provide
further advancements towards the reﬁned task of Visual
Commonsense Generation (VCG) (Park et al., 2020) ly-
ing on the intersection of the generative image caption-
ing task and the non-generative visual commonsense rea-
soning task. To this end, knowledge-enhanced Multi-
modal BART (KM-BART) was developed, able to incor-
porate both visual and linguistic information with the
help of modality and task-relevant tokens in the trans-
former input. More speciﬁcally, task-relevant tokens are
added in the beginning of the input sequence denoting
the task type. For example, for VCG <before>, <after>,
or <intent> tokens, representing temporal sequence of
events (what happened before, what may happen next)
and intents of people present in the image. Furthermore,
the pre-training task of Knowledge-based Commonsense
Generation (KCG) fuses commonsense knowledge from
structured sources early in the pipeline, actually achiev-
ing in implicitly integrating explicit knowledge. Also
Attribution Prediction (AP) and Relation Prediction (RP)
pre-training tasks are used for the ﬁrst time in knowledge-
enhanced VL learning. COMET (Bosselut et al., 2019)
is a transformer model trained on knowledge bases such
as ATOMIC (Hwang et al., 2021) and ConceptNet (Speer
et al., 2017) that generates commonsense descriptions,
and acts as a knowledge source for KM-BART. Two possi-
ble settings are examined for KM-BART, one containing
the image and the event description (i.e. some textual
information about the image that provides context of the

depicted situation) and a harder one that omits the event
description. (Xing et al., 2021)
Internal knowledge
Transformer based captioning
poses some challenges, one of those attributed to the AR
training procedure which is based on the maximum likeli-
hood estimation (MLE). The main issue stemming from
MLE is that when the generated sequence does not match
the ground truth one, there is no discrimination between
different ’failed’ predictions. Therefore, words that are
totally unrelated to the ground truth match are treated the
same as semantically similar words. For this reason, a KL
divergence term is added to weight semantic relationships
between generated words, with respect to their ground
truth match. Moreover, a knowledge graph is used to en-
rich the transformer input embeddings infusing contextual
information from neighboring entities in the graph. This
knowledge graph is constructed from the linguistic in-
formation itself, by leveraging cosine similarity between
embedded words to position them within a vector space.
The original Transformer (Vaswani et al., 2017) architec-
ture is leveraged for the task, with image features word
embeddings representing the visual modality. (Zhang
et al., 2021b)
6.4.3
Evaluation
Language metrics such as BLEU, ROUGE, METEOR,
CIDEr are used for evaluation as in most language gen-
eration tasks. (Zhou et al., 2019c; Hou et al., 2019; Goel
et al., 2020; Huang et al., 2020a) SPICE is also used in
(Aditya Mogadala, 2020; Zhang et al., 2021b).
Human Evaluation can qualitatively evaluate gener-
ated sentences. The human evaluation experiment in
(Yang et al., 2018) compares the quality of generated
captions from different models according to the percep-
tion of 30 evaluators. Even though such an experiment is
rather subjective, it indicates the importance of language
priors. In (Zhao et al., 2021) human preference is mea-
sured comparing with the previous best-performer in the
before, after, intent generated sentences.
6.5
Knowledge in Visual Dialog (K-VD)
6.5.1
Datasets
VisDial (Das et al., 2016) is a dataset used in both
knowledge-free and knowledge-enhanced versions of VD.
It consists of 133k dialogs and an equal number of im-
ages from COCO, with train and validation splits (125k
dialogs) assigning 10-round dialogs -QA pairs- per image.
In the test split (8k dialogs) random rounds are paired with
each image. Some important aspects of this dataset is the
presence of coreferences, endorsing the coherence of the
conversation in linguistic level, and temporal continuity
in topics, which supports the preservation and consistency
of semantic meaning across the dialogs. The questions
mostly follow a concrete and rather exploratory pattern:
starting from asking about entities involved in COCO
captions, then diving into details, trying to deﬁne a cat-
egorization of the whole scene or the most appropriate
setting description, questioning about the weather of the
scene, exploring key semantics not mentioned previously
and ﬁnally validating and expanding the understanding of
elements provided in the answers.
VisDialCK (Zhang et al., 2022) is an extension of Vis-
Dial containing 940 history-required and commonsense-
required dialogs.
6.5.2
Methods
6.5.2.1
Transformer-based models
Internal
knowledge
Very
recently
a
knowledge-
enhanced implementation for visual dialog was intro-
duced, inspired from the fact that commonsense related
questions are ignored. A visual dialog model requires
two necessary inputs: an image and dialog history. Vi-
sual graphs have assisted the task by providing object
relationships explicitly, even though this knowledge is
not adequate for commonsense inferences. The integra-
tion of commonsense knowledge can be well-represented
with graph-level facts and sentence-level facts. Then,
facts from a commonsense knowledge graph such as Con-
ceptNet (Speer et al., 2017) can be extracted based on
the calculation of cosine similarity between their word
embedding representation comparing to embedding repre-
sentations of the words in the sentences and the detected
objects. Those graph level facts can complement entities
from the visual graph. Therefore, an enriched vision-fact
graph can be produced after individual graphs are puriﬁed
by removing redundant information. The sentence-level
facts are derived from the dialog sentences in the form of
(subject, relation, object) triples, forming a graph struc-
ture. Similarly to the visual stream, the sentence graph
is cleaned and enriched with commonsense knowledge.
Finally, a transformer-based fusion module receives the
enriched graphs, as well as the question embedding to
provide the answer, exploiting a generative and a discrim-
inative decoder. (Zhang et al., 2022)
6.5.3
Evaluation
Ranking metrics such as NDCG, MRR, R@1, R@5,
R@10, Mean position provide the quality of answer re-
trieval for visual dialog for both generative and discrimi-
native answer prediction. (Zhang et al., 2022)
Human evaluation is used in the generative setting of

(Zhang et al., 2022). Speciﬁcally, two metrics are pro-
vided: the ﬁrst one indicates the percentage of responses
passing the Turing test, thus providing the amount of gen-
erated sentences that could be perceived as human-written;
the second metric measures the number of generated re-
sponses that were perceived as of equal or better quality
compared to speciﬁc human responses as baselines.
6.6
Knowledge in Visual Storytelling (K-VIST)
Visual Storytelling presents many situations where hypo-
thetical concepts can be driven from commonsense and
temporal reasoning. Unseen events can enrich or even
be necessary for appropriate and coherent textual stories.
For example, some sequential inferences were presented
in the event/temporal knowledge analysis 5, providing
knowledge such as the boy dropped a glass of water and
then the glass broke. Not all concepts mentioned in this
sentence may be explicitly apparent on a frame of the
visual sequence. However, a knowledge graph can guide
inference by searching for possible connections between
concepts appearing on images, and thus acquire imaginary
concepts.
6.6.1
Datasets
There are no dedicated datasets for K-VIST. On the con-
trary, relevant literature relies on datasets used for the
knowledge-free version of the task, such as VIST (Huang
et al., 2016a). This dataset contains more than 81K unique
photos in around 20K sequences with corresponding tex-
tual stories. Textual stories are following a narrative style
imposing more high level inference capabilities compar-
ing to literal visual descriptions. This requirement is an
extension against the majority of visual description tasks,
which do not directly focus on sequential coherence and
even abstract meanings. Two extra descriptions are pro-
vided per frame in order to bridge literal description with
narratives: descriptions of images-in-isolation (DII) and
images-in-sequence (DIS).
6.6.2
Methods
6.6.2.1
Sequential language models
External knowledge
A two-stage structure was pro-
posed in (Yang et al., 2019a), consisting of a reasoning
and a generation module. The vision aware commonsense
reasoning module is responsible of extracting the most
relevant knowledge from an external knowledge base. Ob-
jects detected on all images of a sequence are fed in a
GRU which provides a semantic and temporal representa-
tion. In the same time, candidate ConceptNet entities are
fetched based on the detected objects. Attention modules
ﬁnally select the most relevant ConceptNet candidates,
which after passing through a GRU provides the ﬁnal com-
monsense representation. The knowledge augumented
generation module receives the extracted commonsense
knowledge together with the visual information, as well
as the previously generated sentences.
A prevalent issue in VIST is the monotonous and repeti-
tive generated stories. This can be attributed to the limited
vocabulary of the VIST dataset. In KG-Story (Hsu et al.,
2019) the ﬁrst stage (distill) gathers words from images
using object detection and GRUs for word prediction. Po-
tential relationships between pairs of concepts throughout
images are searched on external knowledge graphs, and
if multiple candidates occur, a scoring function is used
to rank their relevancy. This is the enrich stage. Finally,
the generate stage utilizes a Transformer which imposes
a repetition penalty to mitigate redundant narration. Fur-
ther modiﬁcations in the default Transformer structure
is the introduction of an anaphoric expressions genera-
tor to enhance coreferences and usage of pronouns, as
well as positional encodings of variable length to enable
representing stories of different lengths.
Addressing again the coherence and novelty of gener-
ated stories, authors of (Xu et al., 2021) propose a three-
stage structure corresponding to imagination, reasoning
and writing capabilities of human. The ﬁrst stage (imag-
ine) focuses on the sequential consistence by extracting
the visual topic of a frame through the combination of
the current visual features and the sentence generated in
the previous step. The knowledge part targets the content
of narratives and consists of three graph types: a general
commonsense knowledge graph, a scene graph and an
event graph. A GCN applied on each graph selects the
most suitable knowledge parts, which are combine to form
the second stage (reason). Both imagine and reason out-
puts are fed to the third stage (write), which is responsible
for generating the story.
6.6.2.2
Transformer-based models
External knowledge
Towards informative and more di-
verse stories, (Chen et al., 2021a) is the ﬁrst knowledge-
enhanced approach that utilizes a generative transformer
to produce the story output. The concept enrichment stage
connects concepts present in images with ConceptNet.
Then, a graph attention network (GAT) operates on the
graph and image features in order to integrate information
of the most appropriate candidate concept nodes, which
will be passed in the next selection module. The concept
selection module utilizes two different selection methods:
a Sequential Selection Module (SSM) that operates in
an encoder-decoder fashion, outputting selected concepts

after encoding the embedded candidate concepts; a Maxi-
mal Clique Selection Module (MCSM) outputs a maximal
clique containing all appropriate concepts for story gen-
eration given the concept graph. Finally, the concept to
story module uses either an RNN structure or a BART
language model, with BART demonstrating more diverse
stories while preserving quality.
6.6.3
Evaluation
Language generation metrics BLUE, ROUGE, ME-
TEOR and CIDEr are widely used automatic metrics that
evaluate the linguistic quality of generated stories.
Diversity of generated stories is measured via the
Distinct-n (Dist-n) score (Li et al., 2015). This metric
indicates the originality of generated text by calculating
the frequency of n-grams throughout the whole corpus of
generated stories. Higher Dist-n scores represent more
diverse stories. (Yang et al., 2019a; Chen et al., 2021a)
Human Evaluation is very important for generative
tasks, as automatic evaluation metrics cannot assess the
full range of linguistic capabilities, especially when it
comes to evaluating sequential quality. However, differ-
ent implementations perform varying human evaluation
experiments, which somehow impedes the direct compar-
ison of models.
In (Yang et al., 2019a) four aspects were examined:
Fluency checks the linguistic quality, relevance measures
the success of textual description in describing visual con-
cepts, informativeness measures the diversity of produced
stories and coherence evaluates the semantic continuity
of stories in a sequence. Each aspect receives a score
from 1 (worse) to 5 (best) from three evaluators, and their
average values serve as the ﬁnal results. Similarly, in
(Xu et al., 2021) relevance, coherence and informative-
ness are regarded, receiving scores from 0 (worse) to 2
(best) from ﬁve evaluators. A different human evaluation
strategy is followed in (Hsu et al., 2019): comparative
experiments between VIST models are performed, asking
users to rank generated stories from different models ei-
ther with or without revealing the corresponding images.
This is an indirect evaluation of linguistic quality and co-
herence, when only text is regarded, and also semantic
relevance, when corresponding images are provided. The
comparative approach is also used in (Chen et al., 2021a),
with two evaluators declaring their preference (or tie) be-
tween two models regarding three aspects: relevance and
informativeness similar to (Yang et al., 2019a; Xu et al.,
2021), together with logicality which measures the logical
coherence over story sequences. Additionally, overall
indicates the evaluator’s preference in general between
the two models.
6.7
Knowledge in Image Generation
6.7.1
Datasets
A variety of datasets have been used in visual genera-
tion, which however do not contain some certain sense
of knowledge, and are widely used in knowledge-free
settings. Datasets used in conditional image synthesis are
ImageNet (Deng et al., 2009), CIFAR (Krizhevsky, 2009),
FFHQ (Karras et al., 2018), Oxford Flowers (Nilsback
and Zisserman, 2008), CUB (He and Peng, 2020) and
many others.
Sequential synthesis (story visualization) greatly uti-
lizes Pororo-SV cartoon dataset (Kim et al., 2017). It con-
tains more than 16k pairs of scenes and dialogs extracted
from 20 hours of video, 27,328 ﬁne-grained scene descrip-
tions in natural language provided by human annotators,
and 8,913 QA multiple-choice pairs related to the story. In
total, 10 main characters appear in the frames. Questions
are divided in 11 types: Action, Person, Abstract, Detail,
Method, Reason, Location, Statement, Causality, Yes/No,
Time. FlinstonesSV (ﬂi) is also based on cartoon frames.
It is composed of 25184 densely annotated videos, each
of which containing 75 frames. The annotations include
bounding boxes with labels for characters and items of the
frames, as well as segmentation masks. Another emerging
dataset for Story Visualization is DiDeMo-SV (Maharana
et al., 2022; Hendricks et al., 2017), a dataset based on
video captions that contains 10,000 with more than 40,000
temporally localized textual descriptions.
6.7.2
Methods
6.7.2.1
Knowledge in Conditional Image Genera-
tion (K-cIG)
Internal knowledge
Even though GANs have been
powerful in synthesizing novel images, they cannot han-
dle combinations of attributes they have not encountered
in the training data. Therefore, if the textual condition
refers to such unseen combinations, the synthesized im-
age has sacriﬁced some of the semantics, in order to pro-
duce a result that remains within the learned distribution.
The insertion of additional knowledge can expand the
generated distribution to enhance the consistency on the
condition without sacriﬁcing resulting ﬁdelity. This can
be translated in two needs regarding a GAN model: the
generator should become more ﬂexible, and the genera-
tor more tolerant. KG-GAN meets those requirements
by introducing a second generator, trained on domain
knowledge by utilizing a novel knowledge loss. This sec-
ond generator shares parameters with the original one,
which is responsible of synthesizing images conditioned
on text. A regression network receives the synthesized

images from the seen-image generator and the ones from
the knowledge generator, imposing constrains regarding
the plausibility of unseen combinations. The semantic
vector produced by the knowledge generator is redirected
to the seen-image generator to guide generation outside
the predeﬁned classes. KG-GAN does not exploit exter-
nal knowledge sources, but with this simple distribution
enhancement it achieves some preliminary zero-shot ca-
pabilities. (Chang et al., 2019)
6.7.2.2
Knowledge in Story Visualization (K-SV)
External knowledge
Story Visualization is another task
with limited contributions in knowledge-enhanced set-
tings. Structured information from text can be obtained
via parse trees which can permit hierarchical encoding
of longer phrases. Missing information regarding visual
details in text can be ﬁlled out with external knowledge
from ConceptNet (Speer et al., 2017). Moreover, con-
ceptually similar sentences that are phrased in a different
way need to be placed closed in an embedding space, an
issue that external knowledge can again effectively re-
solve. Spatial knowledge is also underrepresented in most
sentences, even though scene synthesis needs detailed
information of object positions. Dense captioning as a
form of self-augmenting knowledge provided detailed po-
sitioning information due to the usage of region bounding
boxes. The combination of internal spatial and exter-
nal semantic knowledge is able to better guide sequen-
tial synthesis, resolving all involved aspects such as text-
image consistency, visual quality and sequential continu-
ity. A Memory-Augmented Recurrent Tree-Transformer
(MARTT) encodes the parse trees for the text, while a
Graph Transformer (Yun et al., 2020) embeds the com-
monsense knowledge. Both embeddings are inserted in
the story encoder, which outputs contextualized embed-
dings for the image generator. The generated images are
passed to image and story discriminators, which redirect
synthesis based on individual and sequential aspects. Spa-
tial knowledge from dense captioning enforces additional
loss functions while training, to provide more explicit
information about positions and detailed grounding of
characters on the images with respect to their descriptions
in the text. (Maharana and Bansal, 2021)
The groundbreaking success of DALL-E (Ramesh et al.,
2021, 2022) inspired the usage of massive zero-shot
transformer-based generative models in Story Visualiza-
tion; StoryDALL-E (Maharana et al., 2022) achieves gen-
eralization of visual synthesis to unseen textual stories,
also extending the task to Story Continuation: in this case,
a source image is included in the conditioning, requesting
from the model to continue the visual story in a consistent
way. Story Visualization has been a task lacking sufﬁcient
datasets, due to the increased effort needed to construct
appropriate ones, either manually or automatically. To
this end, external unstructured knowledge obtained from
pre-trained DALL-E (Ramesh et al., 2021) enables even
zero-shot sequential synthesis based on input ’story’ text.
6.7.3
Metrics
Image generation metrics (section 3.6.5) such as FID
for seen and unseen classes were used in KG-GAN
(Chang et al., 2019). FID is also used to evaluate qual-
ity of generated frames independently in (Maharana and
Bansal, 2021; Maharana et al., 2022). R-precision indi-
cates quality by measuring the retrieval capabilities of
generated frames over ground truth captions comparing
to retrieval using the real frames. (Maharana and Bansal,
2021)
Classiﬁcation metrics, such as Character F1 score
measure the quality of generated characters in predicted
images. Also, frame accuracy checks the exact match be-
tween semantics of the ground truth and generated frames.
(Maharana and Bansal, 2021; Maharana et al., 2022)
Language metrics are also relevant:
viewing SV
frames as a video, captions for generated frames can
be produced using video captioning techniques. BLEU
scores evaluate the quality of captions as an indirect mea-
sure of visual quality, based on the idea that well designed
semantics will be captured in captions better than low
quality concepts. (Maharana and Bansal, 2021)
Human Evaluation can reveal the human perception
over quality, as in most generative tasks. Speciﬁcally for
SV, evaluators need to assess results over visual quality,
consistence and relevance comparing to the previous state-
of-the-art model on the same task. (Maharana and Bansal,
2021; Maharana et al., 2022)
6.8
Multi-task transformers with knowledge
6.8.1
Methods
Multi-task models can easily be built using multimodal
transformer backbones.
Instead of utilizing external
knowledge graphs as in previous methods, many imple-
mentations employ self-knowledge exclusively, by ob-
taining more structured representations from the existing
visual and textual data.
External knowledge
A natural uniﬁcation of multiple
tasks under the same model would incorporate tasks mov-
ing in the same direction such as cross-modal reason-
ing tasks or cross-modal-retrieval tasks. Indeed, VQA,
VCR and VE were uniﬁed in Rationale VT transformer

(Marasovi´c et al., 2020), a framework that utilizes visual
and linguistic clues to generate free-text rationales. Two
knowledge sources attempt to provide reasoning informa-
tion regarding scenes: a grounded situation recognizer
(Pratt et al., 2020) describes activities on scenes, entities
involved and draws bounding boxes for entities to visu-
ally ground them; and Visual Commonsense Graphs (Park
et al., 2020) to fuse commonsense inferences about events
and intents so that a temporal perspective of a scene is also
considered. Rationales are generated for VQA-E (visual
question answering) (Li et al., 2018), E-SNLI-VE (visual
entailment) (Do et al., 2020) and VCR (visual common-
sense reasoning) (Zellers et al., 2019) datasets. Visual
recognition of objects is the ﬁrst step for visual under-
standing, followed by capturing their in-between relation-
ships utilizing the knowledge provided by the grounded
situation recognizer (Pratt et al., 2020). Higher-level cog-
nition is achieved using knowledge from VisualCOMET
(Park et al., 2020), which receives the knowledge stored in
Visual Commonsense Graphs to generate commonsense
inferences. VisualCOMET is built upon GPT-2 (Rad-
ford et al., 2019), therefore a unimodal purely linguistic
input can be provided, utilizing object labels, textual ques-
tion/answers and inferences. Alternatively, GPT-2 can be
adapted, resulting in a hybrid implementation: visual fea-
tures and bounding box coordinates act as visual embed-
dings, combined with VisualCOMET token embeddings
indicating the beginning of before, after, intent inferences.
Targeting again reasoning tasks, (Shevchenko et al.,
2021) builds on top of LXMERT (Cho et al., 2020) to
address the knowledge-enhanced versions of the VQA,
VCR and VE tasks on the OK-VQA (Marino et al.,
2019), FVQA (Wang et al., 2018a), NLVR2 (Suhr et al.,
2019), SNLI-VE (Xie et al., 2019) datasets. External
knowledge is provided from ConceptNet (Speer et al.,
2017) and Wikidata (Vrande˘ci`c and Krötzsch, 2014).
Knowledge-rich expressions are created by matching
embedded knowledge with training sentences from the
datasets. Moreover, a training objective targeting the
alignment of knowledge embeddings and knowledge-rich
expressions encourages learning a global representation
structure. Utilizing this objective is proven beneﬁcial dur-
ing both pre-training and ﬁne-tuning. It is also observed
that the introduction of this knowledge-oriented objective
smooths the embedding space, which facilitates similarity
matching between words.
KB-VLP (Chen et al., 2021b) utilizes knowledge em-
beddings based on Wikidata (Vrande˘ci`c and Krötzsch,
2014) entities, which are concatenated with the visiolin-
guistic instances as inputs of a VL transformer. Specif-
ically, entity recognition on text is performed to ex-
tract relevant Wikidata entries, which are embedded via
Wikipedia2vec to form text-related knowledge embed-
dings. Object tags obtained from the image are used to ob-
tain image-related knowledge embeddings from relevant
Wikidata entities. The input vector consists of 5 compo-
nents: word embeddings for text, text-related knowledge
embeddings, word embeddings sequences for object tags
per image, visual features and image-related knowledge
embeddings. Two specialized pre-training objectives are
used: sentence-level objective substitutes elements from
the input vector with other random elements, while token-
level objective extends text - image masking to text-related
knowledge embedding - image-related knowledge embed-
ding masking. Task speciﬁc datasets for KB-VLP are
VQA (Agrawal et al., 2016), GQA (Hudson and Manning,
2019) and OK-VQA (Marino et al., 2019) for visual ques-
tion answering, and NLVR2 (Suhr et al., 2019) for visual
reasoning.
Internal knowledge
OSCAR (Li et al., 2020c) is one
of the models that effortlessly transit from knowledge-free
to knowledge-enhanced learning utilizing self-acquired
knowledge in its simplest form. Instead of -rather naively-
letting the model infer the correct image-text alignments
in an exhaustive way, OSCAR facilitates the procedure
with the usage of object tags, as intermediaries between
text and image instances. This procedure is endorsed
from the observation that salient objects in the image will
most probably also appear in text. The input to the VL
transformer module consists of word tokens, object tag
embeddings and visual features. The intermediary object
tags form separate semantic spaces, depending on whether
they are paired with text or image, yielding two dedicated
pre-training objectives. The masked token loss objective
views text and tag word representations in the same space,
randomly masking each of them and letting reconstruct
the missing parts through the visual modality. Conversely,
contrastive loss views tags paired with visual features,
and randomly replaces the real tag sequence with another
one sampled from the dataset, learning to pull apart mis-
matched tag sequences and bring close together the match-
ing ones. OSCAR succeeds in both understanding tasks,
such as cross-modal retrieval (ITR/TIR), visual question
answering (on VQA (Agrawal et al., 2016) and GQA
(Hudson and Manning, 2019)) and visual reasoning (on
NLVR2 (Suhr et al., 2019)), as well as in generation tasks,
such as image captioning and novel object captioning.
ERNIE-ViL (Yu et al., 2021a) leverages structured vi-
sual knowledge from scene graphs to bridge detailed se-
mantics across vision and language. Such ﬁne-grained
representations are important to differentiate between con-

ceptually similar scenes. Scene graph prediction tasks
(object, attribute and relationships prediction) encourage
learning those ﬁne-grained differences. Even though not
using external knowledge, ERNIE-ViL internally con-
structs structured knowledge during the cross-modal pre-
training. Nevertheless, this self-knowledge is sufﬁcient
to boost performance in 5 VL tasks, especially in those
that ﬁne-grained associations are required, such as visual
referring expressions (VRE). Other tasks beneﬁted from
this approach are VCR, VQA and cross-modal retrieval
(ITR/TIR).
ROSITA (Cui et al., 2021b) extends the self-knowledge
idea by employing both cross-modal and intra-modal
knowledge in the same time. Given an image-text pair,
the ﬁrst step is to construct intra-modal graphs, i.e an im-
age graph and a text graph. The image graph consists of
regions (deﬁned by a pre-trained object detector) as nodes,
with IoU scores of paired regions acting as edge weights
between those regions. Similarly for the text graph, ob-
jects, attributes and relationships are extracted from text
to ﬁll the nodes of the text graph, while edge weights
are deﬁned by the co-occurrence frequency between pairs
of nodes. In both graphs, zero similarity scores between
nodes indicate absence of edge. A cross-modal scene
graph is derived from the image and text graph by align-
ing predicted region tags from the image side and words
from the text side by comparing their textual semantic sim-
ilarity. By calculating this similarity score for all possible
tag-word pair, edge weights between cross-modal nodes
are deﬁned. Nodes connected via cross-modal edges,
named anchor nodes, form subgraphs which maintain
intra-modal and cross-modal edges, as well as two-hop
connections that contain paths of cross-modal followed
by intra-modal edges. ROSITA leverages this enhanced
representation to boost three downstream tasks: VQA,
VRE and ITR.
6.8.2
Evaluation
Human Evaluation is useful in cases of language genera-
tion tasks, such as the rationales generation of (Marasovi´c
et al., 2020). In this case, the need for human evaluation
arises from the observation that certain discrete rationales,
even though not being paraphrases of each other, can be
suitable. The following aspects were evaluated: visual
plausibility referring to how well the generated rationales
support the answer (in VQA and VCR) or the entailment
(VE) given the image, and visual ﬁdelity measuring the ap-
pearance of irrelevant information within more plausible
generated rationales. By excluding images, textual Plausi-
bility evaluates generated rationales based on their support
on the answer (in VQA and VCR) or the entailment (VE)
exclusively.
Classiﬁcation metrics such as accuracy serve as the
golden standard for non-generative models on cross-
modal reasoning tasks. In (Shevchenko et al., 2021). OK-
VQA accuracies per question types are also reported, in
order to validate improvements in commonsense-oriented
categories attributed to the injection of commonsense
knowledge.
Ranking metrics provide valuable insights in cases
when retrieval tasks are performed (Yu et al., 2021a; Cui
et al., 2021b), Recall@k for k=1, 5, 10 is reported.
Language metrics such as BLEU (Papineni et al.,
2002), CIDEr (Vedantam et al., 2014), SPICE are used
for language generation tasks, such as image captioning
and novel object captioning (Li et al., 2020c).
7
The future of knowledge in VL
7.1
Explainability and biases
Some early works in KVL tasks widely addressed the need
for boosting explainability via knowledge graphs. The
complex opaque reasoning accompanying many state-of-
the-art VL models indeed renders explainability a signiﬁ-
cant aspect. However, as development of more advanced
models emerged, interpretability and fairness became in-
cidental to performance. The pursue of impressive results
often promotes models with more vulnerabilities, which
have been signiﬁcantly underexplored. For example, the
leading ﬁeld of NLP has experienced some non-negligible
failures, such as producing completely wrong statements
based on a false conditioning3. It is unknown how many
such vulnerabilities exist in state-of-the-art models, as
there is no systematic way to capture and recognize them,
nor is there any guarantee they will not occur. Those is-
sues question the trust of humans over black-box models
and even make such models susceptible to misuse. Ex-
plainability and robustness of VL models are even less
explored comparing to NLP, leaving lots of space for the
development of transparent models and post-hoc explain-
ability methods. Generally, given the interwoven nature
of knowledge graphs and explainability, it is expected that
sooner or later research interests will resume towards this
direction.
7.2
Zero-shot learning
Previous works regarding zero-shot classiﬁcation tasks
(Wang et al., 2018b; Nayak and Bach, 2020; Geng et al.,
2020, 2021) leverage knowledge graphs to fuse feature
information from seen to unseen classes. Little work has
3Aligning Language Models to Follow Instructions

been done so far towards the more complex task of multi-
modal zero-shot learning with external knowledge (Chen
et al., 2021c), leaving numerous unexplored directions
open for future research.
7.3
Exploitation and integration of more knowledge
senses
Despite the increasing interest towards knowledge-
enhanced multimodal learning, there are some signiﬁ-
cantly underexplored knowledge aspects that could be
leveraged in various tasks. For example, factual knowl-
edge does not have a noticeable presence outside of VQA
applications.
Named entities and events are only ad-
dressed in a couple of applications. Temporal knowledge
or even hypotheses and counterfactual thinking could re-
veal new aspects of existing tasks with the potential of
interesting implementations.
7.4
Datasets
Even though dedicated knowledge-based datasets have
been developed for VQA (Shah et al., 2019; Wang et al.,
2018a; Marino et al., 2019; Singh et al., 2019; Yu et al.,
2020; Jain et al., 2021), lack of corresponding ventures
have been observed in other VL tasks. Such datasets could
either incorporate external knowledge from certain knowl-
edge bases in the ﬁrst place, or be more ﬂexible and dy-
namically retrieve knowledge to satisfy more challenging
inputs. Suitable and high-quality datasets are the ﬁrst step
towards the evolution of the knowledge-enhanced mul-
timodal learning ﬁeld. Furthermore, knowledge-based
datasets could be combined with explanation-oriented
datasets, like VQA-E (Li et al., 2018) and e-SNLI-VE
(Do et al., 2020) to address the issues mentioned in sec-
tion 7.1.
7.5
Knowledge-enhanced generative tasks
The ﬁeld sitting on the intersection of knowledge graphs
and generative models has been signiﬁcantly underex-
plored, despite the major success both ﬁelds have experi-
enced in recent years. Until now, visual knowledge has
been attempted in image synthesis, forming the task of im-
age generation from scene graphs and layouts with inter-
esting results and improvements on complex scene synthe-
sis (Johnson et al., 2018; Li et al., 2019b; He et al., 2021b).
Also domain knowledge (Chang et al., 2019) in GANs has
demonstrated some insightful preliminary observations re-
garding knowledge-guided synthesis of unseen attributes,
without the need of massive pre-training. A handful of
aforementioned generative multimodal approaches (Yang
et al., 2019a) incorporate knowledge graphs for the tasks
of visual storytelling and story visualization. More com-
pelling results could unfold from the combination of vari-
ous knowledge graphs with multimodal generative models,
enabling conditioning on commonsense, hierarchical, fac-
tual knowledge, and also enforcing interpretable insights
into the generation process.
7.6
The need for multi-task learners
An abundance of multi-task knowledge-free transformer-
based VL models have emerged in recent literature, pre-
senting impressive results on a variety of downstream
tasks by utilizing the same pre-trained body each time. On
the other hand, knowledge-enhanced VL models usually
target a single task, and only a few knowledge-enhanced
multi-task models (Marasovi´c et al., 2020; Shevchenko
et al., 2021; Yu et al., 2021a; Cui et al., 2021b; Chen
et al., 2021b) have been developed. Even in the case
of multi-task transformers, almost half of them utilize
only self-knowledge without exploiting the beneﬁts of
additional external sources. In the same time, the harder
venture of integrating external knowledge limits the range
of tasks that multi-task models target. Speciﬁcally, cur-
rent implementations have covered only reasoning tasks.
Therefore, as a ﬁrst future direction, retrieval tasks can
also be attempted. Going one step further, some VL tasks
addressed in multi-task knowledge-free VL transform-
ers have never been explored in literature; hence, uni-
ﬁed multi-task architectures could possibly explore their
knowledge-enhanced capabilities, without the need of
developing individual non-reusable approaches. In any
case, multi-task knowledge-enhanced models would un-
lock the full potential of the contributions of knowledge,
with competitive architectures pushing the state-of-the-art
even further.
8
Conclusion
Introducing external knowledge in multimodal learning
has demonstrated promising research directions, targeting
performance, explainability and extendability of existing
tasks. In this survey paper, we analyzed the meeting point
of visiolinguistic representation learning and knowledge
assisted learning, focusing on the contribution of existing
knowledge graphs and unstructured knowledge sources.
The presented taxonomy of knowledge-enhanced datasets,
tasks and models provides one of the ﬁrst attempts to-
wards structuring the ﬁeld of knowledge-enhanced VL
learning, with the aim to guide future research and ad-
dress prospects and challenges of this upcoming ﬁeld.
References
Imdb. https://www.imdb.com/.

Meet
the
ﬂintstones
dataset.
http:
//flintstones-dataset-dev-rev.
s3-website-us-west-2.amazonaws.com/.
Movie
genre
from
its
poster.
https:
//www.kaggle.com/neha1703/
movie-genre-from-its-poster.
Dietrich Klakow Aditya Mogadala, Xiaoyu Shen. 2020. Inte-
grating rule-based entity masking into image captioning.
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret
Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi
Parikh. 2016. Vqa: Visual question answering.
Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter.
2019. Fusion of detected objects in text for visual question
answering.
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
Gould. 2016. Spice: Semantic propositional image caption
evaluation.
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and
Anton van den Hengel. 2018. Vision-and-language naviga-
tion: Interpreting visually-grounded navigation instructions
in real environments.
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann,
Richard Cyganiak, and Zachary G. Ives. 2007. Dbpedia: A
nucleus for a web of open data. In ISWC/ASWC.
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe
Morency. 2017. Multimodal machine learning: A survey
and taxonomy.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved correlation
with human judgments. In Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages 65–72,
Ann Arbor, Michigan. Association for Computational Lin-
guistics.
Lisa Bauer, Lingjia Deng, and Mohit Bansal. 2021. Ernie-nli:
Analyzing the impact of domain-speciﬁc external knowl-
edge on enhanced representations for nli. In DEELIO.
Sumithra Bhakthavatsalam, Kyle Richardson, Niket Tandon,
and Peter Clark. 2020.
Do dogs have whiskers? a new
knowledge base of haspart relations.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas
Mikolov. 2017. Enriching word vectors with subword infor-
mation. Transactions of the Association for Computational
Linguistics, 5:135–146.
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya
Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. Comet:
Commonsense transformers for automatic knowledge graph
construction.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020. Language models
are few-shot learners. In Advances in Neural Information
Processing Systems, volume 33, pages 1877–1901. Curran
Associates, Inc.
Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen,
and Jingjing Liu. 2020. Behind the scene: Revealing the se-
crets of pre-trained vision-and-language models. In ECCV.
Qingxing Cao, Bailin Li, Xiaodan Liang, and Liang Lin. 2019.
Explainable high-order visual question reasoning: A new
benchmark and knowledge-routed network.
Daniel Cer,
Yinfei Yang,
Sheng yi Kong,
Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario
Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan
Sung, Brian Strope, and Ray Kurzweil. 2018.
Universal
sentence encoder.
Che-Han Chang, Chun-Hsien Yu, Szu-Ying Chen, and Ed-
ward Y. Chang. 2019. Kg-gan: Knowledge-guided gener-
ative adversarial networks.
Hila Chefer, Shir Gur, and Lior Wolf. 2021. Generic attention-
model explainability for interpreting bi-modal and encoder-
decoder transformers. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), pages
397–406.
Hong Chen, Yifei Huang, Hiroya Takamura, and Hideki
Nakayama. 2021a. Commonsense knowledge aware con-
cept selection for diverse and informative visual storytelling.
In AAAI.
Kezhen Chen, Qiuyuan Huang, Yonatan Bisk, Daniel J. Mc-
Duff, and Jianfeng Gao. 2021b. Kb-vlp: Knowledge based
vision and language pretraining.
Qian Chen, Xiao-Dan Zhu, Zhenhua Ling, Diana Inkpen, and
Si Wei. 2018a. Neural natural language inference models
enhanced with external knowledge. In ACL.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen, and
Si Wei. 2018b. Neural natural language inference models
enhanced with external knowledge. In Proceedings of the
56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 2406–2417,
Melbourne, Australia. Association for Computational Lin-
guistics.
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.
Uniter: Universal image-text representation learning.
Zhuo Chen, Jiaoyan Chen, Yuxia Geng, Jeff Z. Pan, Zonggang
Yuan, and Huajun Chen. 2021c. Zero-shot visual question
answering using knowledge graph. In SEMWEB.

Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Uni-
fying vision-and-language tasks via text generation.
Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi,
and Aniruddha Kembhavi. 2020. X-lxmert: Paint, caption
and answer questions with multi-modal transformers.
In
EMNLP.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau,
and Yoshua Bengio. 2014. On the properties of neural ma-
chine translation: Encoder-decoder approaches.
Leyang Cui, Yu Wu, Shujie Liu, and Yue Zhang. 2021a.
Knowledge enhanced ﬁne-tuning for better handling unseen
entities in dialogue generation. ArXiv, abs/2109.05487.
Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao,
Ji Zhang, Meng Wang, and Jun Yu. 2021b. Rosita: Enhanc-
ing vision-and-language semantic alignments via cross- and
intra-modal knowledge integration. Proceedings of the 29th
ACM International Conference on Multimedia.
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,
Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv
Batra. 2016. Visual dialog.
Harm de Vries, Florian Strub, Sarath Chandar, Olivier
Pietquin, Hugo Larochelle, and Aaron Courville. 2016.
Guesswhat?! visual object discovery through multi-modal
dialogue.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical im-
age database. In 2009 IEEE conference on computer vision
and pattern recognition, pages 248–255. Ieee.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019.
Bert:
Pre-training of deep bidirec-
tional transformers for language understanding.
ArXiv,
abs/1810.04805.
Arka Ujjal Dey, Ernest Valveny, and Gaurav Harit. 2021. Ex-
ternal knowledge enabled text visual question answering.
Prafulla Dhariwal and Alex Nichol. 2021. Diffusion models
beat gans on image synthesis.
Virginie Do, Oana-Maria Camburu, Zeynep Akata, and
Thomas Lukasiewicz. 2020.
e-snli-ve: Corrected visual-
textual entailment with natural language explanations.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa
Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image
is worth 16x16 words: Transformers for image recognition
at scale. In International Conference on Learning Repre-
sentations.
Yifan Du, Zikang Liu, Junyi Li, and Wayne Zhao. 2022. A
survey of vision-language pre-trained models.
Shiv Ram Dubey. 2021. A decade survey of content based
image retrieval using deep learning. IEEE Transactions on
Circuits and Systems for Video Technology, page 1–1.
Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon
Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Ben-
gio, and Graham W. Taylor. 2019. Tell, draw, and repeat:
Generating and modifying images based on continual lin-
guistic instruction.
D. Elliott, S. Frank, K. Sima’an, and L. Specia. 2016.
Multi30k: Multilingual english-german image descriptions.
In Proceedings of the 5th Workshop on Vision and Lan-
guage, pages 70–74.
Ko Endo, Masaki Aono, Eric Nichols, and Kotaro Funakoshi.
2017. An attention-based regression model for grounding
textual phrases in images. In IJCAI.
Patrick Esser, Robin Rombach, and Björn Ommer. 2020. Tam-
ing transformers for high-resolution image synthesis.
Christiane Fellbaum. 1998. Wordnet: An electronic lexical
database.
Difei Gao, Ruiping Wang, S. Shan, and Xilin Chen. 2019.
Cric: A vqa dataset for compositional reasoning on vision
and commonsense.
Noa Garcia and George Vogiatzis. 2018. How to read paint-
ings:
Semantic art understanding with multi-modal re-
trieval.
Noa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu
Otani, Chenhui Chu, Yuta Nakashima, and Teruko Mita-
mura. 2020.
A dataset and baselines for visual question
answering on art.
Diego Garcia-Olano, Yasumasa Onoe, and Joydeep Ghosh.
2021. Improving and diagnosing knowledge-based visual
question answering via entity enhanced knowledge injec-
tion.
François Gardères, Maryam Ziaeefard, Baptiste Abeloos, and
Freddy Lécué. 2020. Conceptbert: Concept-aware repre-
sentation for visual question answering. In FINDINGS.
Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Jeff Z. Pan, Zhiquan
Ye, Zonggang Yuan, Yantao Jia, and Huajun Chen. 2021.
Ontozsl: Ontology-enhanced zero-shot learning. Proceed-
ings of the Web Conference 2021.
Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Zhiquan Ye, Zong-
gang Yuan, Yantao Jia, and Huajun Chen. 2020. Generative
adversarial zero-shot learning via knowledge graphs.
Ross Girshick. 2015. Fast r-cnn.
Arushi Goel, Basura Fernando, Thanh-Son Nguyen, and
Hakan Bilen. 2020. Injecting prior knowledge into image
caption generation. In ECCV Workshops.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. 2014.
Generative adversarial nets.
In
Advances in Neural Information Processing Systems, vol-
ume 27.

Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh. 2016. Making the v in vqa matter: El-
evating the role of image understanding in visual question
answering.
Ralph Grishman and Beth Sundheim. 1996.
Design of the
muc-6 evaluation. In Proceedings of a Workshop on Held
at Vienna, Virginia: May 6-8, 1996, TIPSTER ’96, page
413–422, USA. Association for Computational Linguistics.
Aditya Grover and Jure Leskovec. 2016.
node2vec: Scal-
able feature learning for networks. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Wenzhong Guo, Jianwen Wang, and Shiping Wang. 2019.
Deep multimodal representation learning: A survey. IEEE
Access, 7:63373–63394.
William L. Hamilton. Graph representation learning. Synthe-
sis Lectures on Artiﬁcial Intelligence and Machine Learn-
ing, 14(3):1–159.
William L. Hamilton, Rex Ying, and Jure Leskovec. 2018.
Representation learning on graphs: Methods and applica-
tions.
Feijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun. 2021a.
Interpretable visual reasoning: A survey. Image and Vision
Computing, 112:104194.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
2016.
Deep residual learning for image recognition.
In
2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778.
Sen He, Wentong Liao, Michael Yang, Yongxin Yang, Yi-Zhe
Song, Bodo Rosenhahn, and Tao Xiang. 2021b. Context-
aware layout to image generation with enhanced object ap-
pearance. In CVPR.
Xiangteng He and Yuxin Peng. 2020.
Fine-grained visual-
textual representation learning. IEEE Transactions on Cir-
cuits and Systems for Video Technology, 30(2):520–531.
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff
Donahue, Bernt Schiele, and Trevor Darrell. 2016. Gener-
ating visual explanations.
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef
Sivic, Trevor Darrell, and Bryan Russell. 2017. Localizing
moments in video with natural language.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. 2017. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-
term memory. Neural computation, 9:1735–80.
Jingyi Hou, Xinxiao Wu, Yayun Qi, Wentian Zhao, Jiebo
Luo, and Yunde Jia. 2019. Relational reasoning using prior
knowledge for visual captioning. ArXiv, abs/1906.01290.
Jingyi Hou, Xinxiao Wu, Xiaoxun Zhang, Yayun Qi, Yunde
Jia, and Jiebo Luo. 2020. Joint commonsense and relation
reasoning for image and video captioning. In AAAI.
Jeremy Howard and Sebastian Ruder. 2018.
Universal lan-
guage model ﬁne-tuning for text classiﬁcation.
Chao-Chun Hsu, Zi-Yuan Chen, Chi-Yang Hsu, Chih-Chia Li,
Tzu-Yuan Lin, Ting-Hao ’Kenneth’ Huang, and Lun-Wei
Ku. 2019. Knowledge-enriched visual storytelling.
Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.
2020. Heterogeneous graph transformer. In Proceedings
of The Web Conference 2020, WWW ’20, page 2704–2710,
New York, NY, USA. Association for Computing Machin-
ery.
Feicheng Huang, Zhixin Li, Shengjia Chen, Canlong Zhang,
and Huifang Ma. 2020a.
Image captioning with internal
and external knowledge. Proceedings of the 29th ACM In-
ternational Conference on Information & Knowledge Man-
agement.
Ting-Hao K. Huang, Francis Ferraro, Nasrin Mostafazadeh,
Ishan Misra, Jacob Devlin, Aishwarya Agrawal, Ross Gir-
shick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al.
2016a. Visual storytelling. In 15th Annual Conference of
the North American Chapter of the Association for Compu-
tational Linguistics (NAACL 2016).
Ting-Hao
Kenneth
Huang,
Francis
Ferraro,
Nasrin
Mostafazadeh,
Ishan Misra,
Aishwarya Agrawal,
Ja-
cob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli,
Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy
Vanderwende,
Michel Galley,
and Margaret Mitchell.
2016b.
Visual storytelling.
In Proceedings of the 2016
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics:
Human Language
Technologies, pages 1233–1239, San Diego, California.
Association for Computational Linguistics.
Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu,
Dongmei Fu, and Jianlong Fu. 2021. Seeing out of the box:
End-to-end pre-training for vision-language representation
learning.
Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and
Jianlong Fu. 2020b. Pixel-bert: Aligning image pixels with
text by deep multi-modal transformers.
Drew A. Hudson and Christopher D. Manning. 2019. Gqa:
A new dataset for real-world visual reasoning and composi-
tional question answering.
Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao
Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui
Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, An-
wen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang
Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang
Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao
Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou,
Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, and
Ji-Rong Wen. 2021. Wenlan: Bridging vision and language
by large-scale multi-modal pre-training.

Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff
Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi.
2021. Comet-atomic 2020: On symbolic and neural com-
monsense knowledge graphs. In AAAI.
Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang,
Deborah L. McGuinness, and Pedro Szekely. 2021. Dimen-
sions of commonsense knowledge.
Brian Kenji Iwana, Syed Tahseen Raza Rizvi, Sheraz Ahmed,
Andreas Dengel, and Seiichi Uchida. 2017. Judging a book
by its cover.
Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi
Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti.
2021.
Select, substitute, search: A new benchmark for
knowledge-augmented visual question answering. Proceed-
ings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval.
Shaoxiong Ji, Shirui Pan, E. Cambria, P. Marttinen, and
Philip S. Yu. 2021. A survey on knowledge graphs: Repre-
sentation, acquisition and applications. IEEE transactions
on neural networks and learning systems, PP.
Zizheng Ji, Lin Dai, Jin Pang, and Tingting Shen. 2020. Lever-
aging concept-enhanced pre-training model and masked-
entity language model for named entity disambiguation.
IEEE Access, 8:100469–100484.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom
Duerig. 2021. Scaling up visual and vision-language repre-
sentation learning with noisy text supervision.
Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018.
Im-
age generation from scene graphs. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1219–1228.
Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017.
Clevr: A diagnostic dataset for compositional language and
elementary visual reasoning. In CVPR.
Kushal Kaﬂe, Robik Shrestha, and Christopher Kanan. 2019.
Challenges and prospects in vision and language research.
Tero Karras, Samuli Laine, and Timo Aila. 2018.
A style-
based generator architecture for generative adversarial net-
works.
Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-
Tak Zhang. 2017. Deepstory: Video story qa by deep em-
bedded memory networks.
Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021.
Vilt:
Vision-and-language transformer without convolution or re-
gion supervision.
Thomas N Kipf and Max Welling. 2016.
Semi-supervised
classiﬁcation with graph convolutional networks.
arXiv
preprint arXiv:1609.02907.
Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra,
and Marcus Rohrbach. 2019. Clevr-dialog: A diagnostic
dataset for multi-round reasoning in visual dialog.
Ranjay Krishna, Ines Chami, Michael Bernstein, and Li Fei-
Fei. 2018. Referring relationships.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein,
and Fei-Fei Li. 2016. Visual genome: Connecting language
and vision using crowdsourced dense image annotations.
Alex Krizhevsky. 2009. Learning multiple layers of features
from tiny images.
Quoc V. Le and Tomas Mikolov. 2014. Distributed representa-
tions of sentences and documents.
JaeYun
Lee
and
Incheol
Kim.
2020.
Vi-
sion–language–knowledge
co-embedding
for
visual
commonsense reasoning.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-
jad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,
and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-
sequence pre-training for natural language generation, trans-
lation, and comprehension.
Chunye Li, Liya Kong, and Zhiping Zhou. 2020a. Improved-
storygan for sequential images visualization.
Jour-
nal of Visual Communication and Image Representation,
73:102956.
Guohao Li, Xin Wang, and Wenwu Zhu. 2020b. Boosting vi-
sual question answering with context-aware knowledge ag-
gregation. Proceedings of the 28th ACM International Con-
ference on Multimedia.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill
Dolan. 2015. A diversity-promoting objective function for
neural conversation models.
Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Got-
mare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi. 2021.
Align before fuse: Vision and language representation learn-
ing with momentum distillation.
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
Kai-Wei Chang. 2019a. Visualbert: A simple and perfor-
mant baseline for vision and language.
Qing Li, Qingyi Tao, Shaﬁq Joty, Jianfei Cai, and Jiebo Luo.
2018. Vqa-e: Explaining, elaborating, and enhancing your
answers for visual questions.
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei
Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu
Wei, Yejin Choi, and Jianfeng Gao. 2020c. Oscar: Object-
semantics aligned pre-training for vision-language tasks.
Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xi-
aogang Wang. 2019b. Pastegan: A semi-parametric method
to generate image from scene graph. NeurIPS.

Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng,
Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng
Gao. 2019c.
Storygan: A sequential conditional gan for
story visualization. pages 6322–6331.
Chin-Yew Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Text Summarization Branches Out,
pages 74–81, Barcelona, Spain. Association for Computa-
tional Linguistics.
Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou,
and Hongxia Yang. 2021. Interbert: Vision-and-language
interaction for multi-modal pretraining.
Tsung-Yi Lin,
Michael Maire,
Serge Belongie,
James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C. Lawrence Zitnick. 2014. Microsoft coco: Common ob-
jects in context. In Computer Vision – ECCV 2014, pages
740–755, Cham. Springer International Publishing.
Zachary C. Lipton, John Berkowitz, and Charles Elkan. 2015.
A critical review of recurrent neural networks for sequence
learning.
Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shaﬁq R. Joty,
and Luo Si. 2021a. Knowledge based multilingual language
model. ArXiv, abs/2111.10962.
Runtao Liu, Chenxi Liu, Yutong Bai, and Alan Yuille. 2019a.
Clevr-ref+: Diagnosing visual reasoning with referring ex-
pressions.
Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang,
Hao Su, and Qiang Liu. 2021b. Fusedream: Training-free
text-to-image generation with improved clip+gan space op-
timization.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. 2019b. Roberta: A robustly
optimized bert pretraining approach.
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yix-
uan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu
Wei, and Baining Guo. 2022. Swin transformer v2: Scaling
up capacity and resolution. In International Conference on
Computer Vision and Pattern Recognition (CVPR).
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. 2021c. Swin trans-
former: Hierarchical vision transformer using shifted win-
dows. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV).
Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gard-
ner, and Sameer Singh. 2019. Barack’s wife hillary: Us-
ing knowledge graphs for fact-aware language modeling. In
Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 5962–5971, Florence,
Italy. Association for Computational Linguistics.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
Vilbert: Pretraining task-agnostic visiolinguistic representa-
tions for vision-and-language tasks.
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh,
and Stefan Lee. 2020. 12-in-1: Multi-task vision and lan-
guage representation learning.
Yinquan Lu, H. Lu, Guirong Fu, and Qun Liu. 2021. Kelm:
Knowledge enhanced pre-trained language representations
with message passing on hierarchical relational graphs.
ArXiv, abs/2109.04223.
Man Luo,
Yankai Zeng,
Pratyay Banerjee,
and Chitta
Baral. 2021. Weakly-supervised visual-retriever-reader for
knowledge-based question answering. In EMNLP.
Adyasha Maharana and Mohit Bansal. 2021. Integrating vi-
suospatial, linguistic, and commonsense structure into story
visualization. ArXiv, abs/2110.10834.
Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2021.
Improving generation and evaluation of visual stories via
semantic consistency. ArXiv, abs/2105.10026.
Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022.
Storydall-e: Adapting pretrained text-to-image transform-
ers for story continuation.
Chaitanya Malaviya, Chandra Bhagavatula, Antoine Bosselut,
and Yejin Choi. 2019. Commonsense knowledge base com-
pletion with structural and semantic context.
Mateusz Malinowski and Mario Fritz. 2014. Towards a visual
turing challenge.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich
Schütze. 2008. Introduction to Information Retrieval. Cam-
bridge University Press, USA.
Ana Marasovi´c, Chandra Bhagavatula, Jae Sung Park, Ro-
nan Le Bras, Noah A. Smith, and Yejin Choi. 2020. Nat-
ural language rationales with full-stack visual reasoning:
From pixels to semantic frames to commonsense graphs. In
FINDINGS.
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Kumar
Gupta, and Marcus Rohrbach. 2021. Krisp: Integrating im-
plicit and symbolic knowledge for open-domain knowledge-
based vqa. 2021 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 14106–14116.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. 2019. Ok-vqa: A visual question an-
swering benchmark requiring external knowledge.
2019
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 3190–3199.
Bryan McCann, James Bradbury, Caiming Xiong, and Richard
Socher. 2018. Learned in translation: Contextualized word
vectors.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and
Jeffrey Dean. 2013. Distributed representations of words
and phrases and their compositionality.
Mehdi Mirza and Simon Osindero. 2014. Conditional genera-
tive adversarial nets.

Aditya Mogadala,
Marimuthu Kalimuthu,
and Dietrich
Klakow. 2021. Trends in integration of vision and language
research: A survey of tasks, datasets, and methods. Journal
of Artiﬁcial Intelligence Research, 71:1183–1317.
Medhini Narasimhan, Svetlana Lazebnik, and Alexander G.
Schwing. 2018. Out of the box: Reasoning with graph con-
volution nets for factual visual question answering.
Medhini Narasimhan and Alexander G. Schwing. 2018.
Straight to the facts: Learning knowledge base retrieval for
factual visual question answering. ArXiv, abs/1809.01124.
Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar
Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal.
2017. graph2vec: Learning distributed representations of
graphs.
Nihal V. Nayak and Stephen H. Bach. 2020.
Zero-
shot learning with common sense knowledge graphs.
arXiv:2006.10713 [cs.LG].
Maria-Elena Nilsback and Andrew Zisserman. 2008.
Auto-
mated ﬂower classiﬁcation over a large number of classes.
In 2008 Sixth Indian Conference on Computer Vision,
Graphics & Image Processing, pages 722–729.
Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011.
Im2text: Describing images using 1 million captioned pho-
tographs. In Advances in Neural Information Processing
Systems, volume 24. Curran Associates, Inc.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics, pages
311–318, Philadelphia, Pennsylvania, USA. Association for
Computational Linguistics.
Cesc Chunseong Park and Gunhee Kim. 2015. Expressing an
Image Stream with a Sequence of Natural Sentences. In
NIPS.
Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali
Farhadi, and Yejin Choi. 2020. Visualcomet: Reasoning
about the dynamic context of a still image.
In In Pro-
ceedings of the European Conference on Computer Vision
(ECCV).
Jeffrey Pennington, Richard Socher, and Christopher Manning.
2014. GloVe: Global vectors for word representation. In
Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1532–
1543, Doha, Qatar. Association for Computational Linguis-
tics.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deep-
walk: Online learning of social representations. In Proceed-
ings of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’14, pages
701–710, New York, NY, USA. ACM.
Matthew E. Peters, Mark Neumann, Robert L. Logan IV au2,
Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A.
Smith. 2019. Knowledge enhanced contextual word repre-
sentations.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard-
ner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.
2018. Deep contextualized word representations. In Proc.
of NAACL.
Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020. E-
BERT: Efﬁcient-yet-effective entity embeddings for BERT.
In Findings of the Association for Computational Linguis-
tics: EMNLP 2020, pages 803–818, Online. Association for
Computational Linguistics.
Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and
Aniruddha Kembhavi. 2020. Grounded situation recogni-
tion.
Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng
Li, Heng Ji, Minlie Huang, Maosong Sun, and Jie Zhou.
2021. Erica: Improving entity and relation understanding
for pre-trained language models via contrastive learning. In
ACL/IJCNLP.
Chen Qu, Hamed Zamani, Liu Yang, William Bruce Croft, and
Erik G. Learned-Miller. 2021. Passage retrieval for outside-
knowledge visual question answering. Proceedings of the
44th International ACM SIGIR Conference on Research and
Development in Information Retrieval.
A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei,
and Ilya Sutskever. 2019. Language models are unsuper-
vised multitask learners.
Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and
Ilya Sutskever. 2021. Learning transferable visual models
from natural language supervision.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. Journal of Machine
Learning Research, 21(140):1–67.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. 2022. Hierarchical text-conditional image
generation with clip latents.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
2021. Zero-shot text-to-image generation.
Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka,
Bernt Schiele, and Honglak Lee. 2016a. Learning what and
where to draw.
Scott Reed, Zeynep Akata, Bernt Schiele, and Honglak Lee.
2016b. Learning deep representations of ﬁne-grained visual
descriptions.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. 2016c. Gen-
erative adversarial text to image synthesis. In Proceedings
of The 33rd International Conference on Machine Learning,
volume 48 of Proceedings of Machine Learning Research,
pages 1060–1069, New York, New York, USA. PMLR.

Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. 2016d. Gen-
erative adversarial text to image synthesis.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sen-
tence embeddings using siamese bert-networks.
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. Explor-
ing models and data for image question answering.
S. Ren, K. He, R. Girshick, and J. Sun. 2017. Faster r-cnn:
Towards real-time object detection with region proposal net-
works. IEEE Transactions on Pattern Analysis & Machine
Intelligence, 39(06):1137–1149.
Stephen Robertson and Hugo Zaragoza. 2009. The probabilis-
tic relevance framework: Bm25 and beyond. Foundations
and Trends in Information Retrieval, 3:333–389.
Robin Rombach,
Andreas Blattmann,
Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2021. High-resolution im-
age synthesis with latent diffusion models.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. 2022. Dreambooth:
Fine tuning text-to-image diffusion models for subject-
driven generation.
Tara Safavi and Danai Koutra. 2021. Relational world knowl-
edge representation in contextual language models: A re-
view. ArXiv, abs/2104.05837.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay
Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,
Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo
Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mo-
hammad Norouzi. 2022. Photorealistic text-to-image diffu-
sion models with deep language understanding.
Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle,
Aitor Soroa Etxabe, and Eneko Agirre. 2021. Image cap-
tioning for effective use of language models in knowledge-
based visual question answering. ArXiv, abs/2109.08029.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Che-
ung, Alec Radford, and Xi Chen. 2016.
Improved tech-
niques for training gans.
Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne
van den Berg, Ivan Titov, and Max Welling. 2017. Model-
ing relational data with graph convolutional networks.
Mike Schuster and Kuldip K. Paliwal. 1997.
Bidirectional
recurrent neural networks.
IEEE Trans. Signal Process.,
45:2673–2681.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.
BLEURT: Learning robust metrics for text generation. In
Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 7881–7892, Online.
Association for Computational Linguistics.
Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pra-
tim Talukdar. 2019. Kvqa: Knowledge-aware visual ques-
tion answering. In AAAI.
Or Sharir, Barak Peleg, and Yoav Shoham. 2020. The cost of
training nlp models: A concise overview.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. 2018.
Conceptual captions: A cleaned, hyper-
nymed, image alt-text dataset for automatic image caption-
ing. In ACL.
Violetta Shevchenko, Damien Teney, Anthony R. Dick, and
Anton van den Hengel. 2021. Reasoning over vision and
language: Exploring the beneﬁts of supplemental knowl-
edge. ArXiv, abs/2101.06013.
Jiaxin Shi, Hanwang Zhang, and Juanzi Li. 2018. Explainable
and explicit visual reasoning over scene graphs.
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer,
and Dieter Fox. 2020. ALFRED: A Benchmark for Inter-
preting Grounded Instructions for Everyday Tasks. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR).
Karen Simonyan and Andrew Zisserman. 2015.
Very deep
convolutional networks for large-scale image recognition.
Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar, and
Anirban Chakraborty. 2019.
From strings to things:
Knowledge-enabled vqa model that can read and reason.
2019 IEEE/CVF International Conference on Computer Vi-
sion (ICCV), pages 4601–4611.
Amanpreet Singh, Vedanuj Goswami, and Devi Parikh. 2020.
Are we pretraining it right?
digging deeper into visio-
linguistic pretraining.
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-
laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Douwe Kiela. 2021. Flava: A foundational language and
vision alignment model.
Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, and
Lejian Liao. 2021.
Kvl-bert:
Knowledge enhanced
visual-and-linguistic bert for visual commonsense reason-
ing. Knowl. Based Syst., 230:107408.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Con-
ceptnet 5.5: An open multilingual graph of general knowl-
edge. In AAAI.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A
simple way to prevent neural networks from overﬁtting.
Journal of Machine Learning Research, 15(56):1929–1958.
Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia
Cascianelli, Giuseppe Fiameni, and Rita Cucchiara. 2021.
From show to tell: A survey on deep learning-based image
captioning.
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai. 2020. Vl-bert: Pre-training of generic
visual-linguistic representations.

Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017.
A corpus of natural language for visual reasoning. In Pro-
ceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers),
pages 217–223, Vancouver, Canada. Association for Com-
putational Linguistics.
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun
Bai, and Yoav Artzi. 2019. A corpus for reasoning about
natural language grounded in photographs. In Proceedings
of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, pages 6418–6428, Florence, Italy. Asso-
ciation for Computational Linguistics.
Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru
Hu, Xuanjing Huang, and Zheng Zhang. 2020.
Colake:
Contextualized language and knowledge embedding.
In
COLING.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon
Shlens, and Zbigniew Wojna. 2015. Rethinking the incep-
tion architecture for computer vision.
Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-
modality encoder representations from transformers.
Hongchen Tan, Xiuping Liu, Xin Li, Yi Zhang, and Baocai
Yin. 2019. Semantics-enhanced adversarial nets for text-to-
image synthesis. In 2019 IEEE/CVF International Confer-
ence on Computer Vision (ICCV), pages 10500–10509.
N. Tandon, Gerard de Melo, and G. Weikum. 2014. Acquir-
ing comparative commonsense knowledge from the web.
Proceedings of the National Conference on Artiﬁcial Intelli-
gence, 1:166–172.
Niket Tandon, Gerard de Melo, and Gerhard Weikum. 2017.
WebChild 2.0 : Fine-grained commonsense knowledge dis-
tillation. In Proceedings of ACL 2017, System Demonstra-
tions, pages 115–120, Vancouver, Canada. Association for
Computational Linguistics.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan,
and Qiaozhu Mei. 2015. Line: Large-scale information net-
work embedding.
Proceedings of the 24th International
Conference on World Wide Web.
Thomas Tanon, Gerhard Weikum, and Fabian Suchanek. 2020.
Yago 4: A reason-able knowledge base. pages 583–596.
Jesse Thomason, Michael Murray, Maya Cakmak, and Luke
Zettlemoyer. 2019. Vision-and-dialog navigation. In Con-
ference on Robot Learning (CoRL).
Nikolaos Tsakas, Maria Lymperaiou, Giorgos Filandrianos,
and Giorgos Stamou. 2023. An impartial transformer for
story visualization.
Shagun
Uppal,
Sarthak
Bhagat,
Devamanyu
Hazarika,
Navonil Majumdar, Soujanya Poria, Roger Zimmermann,
and Amir Zadeh. 2020. Multimodal research in vision and
language: A review of current and emerging trends.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-
eit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. In Advances in
Neural Information Processing Systems, volume 30. Curran
Associates, Inc.
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
Parikh. 2014. Cider: Consensus-based image description
evaluation.
Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adri-
ana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph
attention networks.
Denny Vrande˘ci`c and Markus Krötzsch. 2014. Wikidata: a
free collaborative knowledgebase. Commun. ACM, 57:78–
85.
Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and An-
ton van den Hengel. 2017. Explicit knowledge-based rea-
soning for visual question answering. In IJCAI.
Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and An-
ton van den Hengel. 2018a. Fvqa: Fact-based visual ques-
tion answering. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 40:2413–2427.
Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. 2021a.
Vlmo: Uniﬁed vision-language pre-training with mixture-
of-modality-experts.
Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu,
and Yanfang Ye. 2021b. Heterogeneous graph attention net-
work.
Xiaolong Wang, Yufei Ye, and Abhinav Gupta. 2018b. Zero-
shot recognition via semantic embeddings and knowledge
graphs.
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. 2021c. Simvlm: Simple visual
language model pretraining with weak supervision.
Zhang Wen and Yuxin Peng. 2021.
Multi-level knowledge
injecting for visual commonsense reasoning. IEEE Trans-
actions on Circuits and Systems for Video Technology,
31:1042–1054.
Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh
Mottaghi. 2021a.
Multi-modal answer validation for
knowledge-based vqa. ArXiv, abs/2103.12248.
Qi Wu, Chunhua Shen, Anton Hengel, Peng Wang, and An-
thony Dick. 2016a.
Image captioning and visual ques-
tion answering based on attributes and their related exter-
nal knowledge. IEEE Transactions on Pattern Analysis and
Machine Intelligence, PP.
Qi Wu, Peng Wang, Chunhua Shen, Anthony R. Dick, and An-
ton van den Hengel. 2016b. Ask me anything: Free-form
visual question answering based on knowledge from exter-
nal sources. 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 4622–4630.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,
Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva
Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei
Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rud-
nick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and
Jeffrey Dean. 2016c. Google’s neural machine translation
system: Bridging the gap between human and machine
translation.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and
lexical selection. In Proceedings of the 32nd Annual Meet-
ing on Association for Computational Linguistics, ACL ’94,
page 133–138, USA. Association for Computational Lin-
guistics.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long,
Chengqi Zhang, and Philip S. Yu. 2021b. A comprehen-
sive survey on graph neural networks. IEEE Transactions
on Neural Networks and Learning Systems, 32(1):4–24.
Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang,
Lei Ji, Zhifang Sui, Edward Cui, Taroon Bharti, Xin Liu,
and Ming Zhou. 2020. Xgpt: Cross-modal generative pre-
training for image captioning.
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva,
and Antonio Torralba. 2010.
Sun database: Large-scale
scene recognition from abbey to zoo. In 2010 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition, pages 3485–3492.
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2018.
Visual entailment task for visually-grounded language learn-
ing. arXiv preprint arXiv:1811.10582.
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019.
Visual entailment: A novel task for ﬁne-grained image un-
derstanding. arXiv preprint arXiv:1901.06706.
Yiran Xing, Z. Shi, Zhao Meng, Yunpu Ma, and Roger Wat-
tenhofer. 2021. Km-bart: Knowledge enhanced multimodal
bart for visual commonsense generation. In ACL/IJCNLP.
Chunpu Xu, Min Yang, Chengming Li, Ying Shen, Xiang Ao,
and Ruifeng Xu. 2021. Imagine, reason and write: Visual
storytelling with graph knowledge and relational reasoning.
In AAAI.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe
Gan, Xiaolei Huang, and Xiaodong He. 2018.
Attngan:
Fine-grained text to image generation with attentional gen-
erative adversarial networks. In CVPR 2018.
Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jian-
long Fu, Houqiang Li, and Jiebo Luo. 2021.
Probing
inter-modality: Visual parsing with self-attention for vision-
language pre-training.
Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda,
and Yuji Matsumoto. 2020.
Luke: Deep contextualized
entity representations with entity-aware self-attention. In
EMNLP.
Pengcheng Yang, Fuli Luo, Peng Chen, Lei Li, Zhiyi Yin, Xi-
aodong He, and Xu Sun. 2019a. Knowledgeable storyteller:
A commonsense-driven generative model for visual story-
telling. In Proceedings of the Twenty-Eighth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-19, pages
5356–5362. International Joint Conferences on Artiﬁcial In-
telligence Organization.
Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai.
2018. Auto-encoding scene graphs for image captioning.
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu-
mao Lu, Zicheng Liu, and Lijuan Wang. 2021.
An em-
pirical study of gpt-3 for few-shot knowledge-based vqa.
ArXiv, abs/2109.05014.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,
Russ R Salakhutdinov, and Quoc V Le. 2019b. Xlnet: Gen-
eralized autoregressive pretraining for language understand-
ing.
In Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-
maier. 2014. From image descriptions to visual denotations:
New similarity metrics for semantic inference over event
descriptions. Transactions of the Association for Computa-
tional Linguistics, 2:67–78.
Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu,
and Haifeng Wang. 2021a. Ernie-vil: Knowledge enhanced
vision-language representations through scene graph.
In
AAAI.
J. Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu,
and Jianlong Tan. 2020. Cross-modal knowledge reason-
ing for knowledge-based visual question answering. ArXiv,
abs/2009.00145.
Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg,
and Tamara L. Berg. 2016. Modeling context in referring
expressions. In Computer Vision – ECCV 2016, pages 69–
85, Cham. Springer International Publishing.
Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun
Wang, Heng Ji, and Meng Jiang. 2021b.
A survey of
knowledge-enhanced text generation.
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng
Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin
Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei
Zhou, and Pengchuan Zhang. 2021. Florence: A new foun-
dation model for computer vision.
Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang,
and Hyunwoo J. Kim. 2020. Graph transformer networks.
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
2019. From recognition to cognition: Visual commonsense
reasoning.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi.
2018. Swag: A large-scale adversarial dataset for grounded
commonsense inference.

Gangyan Zeng, Zhaohui Li, and Yuan Zhang. 2019.
Poro-
rogan: An improved story visualization model on pororo-
sv dataset. In Proceedings of the 2019 3rd International
Conference on Computer Science and Artiﬁcial Intelligence,
CSAI2019, page 155–159, New York, NY, USA. Associa-
tion for Computing Machinery.
Chao Zhang, Zichao Yang, Xiaodong He, and Li Deng. 2020.
Multimodal intelligence: Representation learning, informa-
tion fusion, and applications. IEEE Journal of Selected Top-
ics in Signal Processing, 14(3):478–493.
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-
Chun Zhu. 2019a. Raven: A dataset for relational and ana-
logical visual reasoning.
Han Zhang, Tao Xu, and Hongsheng Li. 2017. Stackgan: Text
to photo-realistic image synthesis with stacked generative
adversarial networks. pages 5908–5916.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. 2018a.
Stackgan++: Realistic image synthesis with stacked gener-
ative adversarial networks.
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei
Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021a.
Vinvl: Revisiting visual representations in vision-language
models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages
5579–5588.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. 2018b. The unreasonable effectiveness of
deep features as a perceptual metric. In CVPR.
Shunyu Zhang, Xiaoze Jiang, Zequn Yang, Tao Wan, and
Zengchang Qin. 2022. Reasoning with multi-structure com-
monsense knowledge in visual dialog.
Yu Zhang, Xinyu Shi, Siya Mi, and Xu Yang. 2021b. Image
captioning with transformer and knowledge graph. Pattern
Recognition Letters, 143:43–49.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong
Sun, and Qun Liu. 2019b. Ernie: Enhanced language repre-
sentation with informative entities. In ACL.
Wentian Zhao, Yao Hu, Heda Wang, Xinxiao Wu, and Jiebo
Luo. 2021. Boosting entity-aware image captioning with
multi-modal knowledge graph.
Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang. 2021.
Knowledge is power: Hierarchical-knowledge embedded
meta-learning for visual reasoning in artistic domains. Pro-
ceedings of the 27th ACM SIGKDD Conference on Knowl-
edge Discovery & Data Mining.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. 2018. Places: A 10 million image
database for scene recognition. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 40(6):1452–1464.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-
ralba, and Aude Oliva. 2014. Learning deep features for
scene recognition using places database.
In Advances in
Neural Information Processing Systems, volume 27. Curran
Associates, Inc.
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-
son J. Corso, and Jianfeng Gao. 2019a.
Uniﬁed vision-
language pre-training for image captioning and vqa.
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-
son J. Corso, and Jianfeng Gao. 2019b.
Uniﬁed vision-
language pre-training for image captioning and vqa.
Yimin Zhou, Yiwei Sun, and Vasant G Honavar. 2019c. Im-
proving image captioning by leveraging knowledge graphs.
2019 IEEE Winter Conference on Applications of Computer
Vision (WACV), pages 283–293.
Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. DM-
GAN: dynamic memory generative adversarial networks for
text-to-image synthesis. CoRR, abs/1904.01310.
Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei.
2016. Visual7w: Grounded question answering in images.
2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4995–5004.
Yuke Zhu, Ce Zhang, Christopher Ré, and Li Fei-Fei. 2015.
Building a large-scale multimodal knowledge base system
for answering visual queries.
Zihao Zhu, J. Yu, Yujing Wang, Yajing Sun, Yue Hu, and
Qi Wu. 2020. Mucko: Multi-layer cross-modal knowledge
reasoning for fact-based visual question answering. In IJ-
CAI.
Maryam Ziaeefard and Freddy Lécué. 2020.
Towards
knowledge-augmented visual question answering. In COL-
ING.

