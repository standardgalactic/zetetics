CODE LISTINGS AND FIGURES

CHAPTER 1
Figure 1.1. Two 
 to 
: parallel 
 on a dual-core machine
versus 
 switching on a single-core machine
approaches
concurrency
execution
task
Figure 1.2. Task switching of four tasks on two cores
Figure 1.3. Communication between a pair of processes running concurrently

Figure 1.4. Communication between a pair of 
 running concurrently in a single
process
threads
1
2
3
4
5
#include <iostream> 
int main() 
{ 
    std::cout<<"Hello World\n"; 
}
Listing 1.1. A simple Hello Concurrent World program
1
2
#include <iostream> 
#include <thread> 

3
4
5
6
7
8
9
10
11
void hello() 
{ 
    std::cout<<"Hello Concurrent World\n"; 
} 
int main() 
{ 
    std::thread t(hello);   3 
    t.join(); 
}

CHAPTER 2
1
2
void do_some_work(); 
std::thread my_thread(do_some_work);
1
2
3
4
5
6
7
8
9
10
11
class background_task 
{ 
public: 
    void operator()() const 
    { 
        do_something(); 
        do_something_else(); 
    } 
}; 
background_task f; 
std::thread my_thread(f);
1 std::thread my_thread(background_task());
1
2
std::thread my_thread((background_task())); 
std::thread my_thread{background_task()};
1
2
3
4
std::thread my_thread([]{ 
    do_something(); 
    do_something_else(); 
});
Listing 2.1. A function that returns while a thread still has access to local variables

Table 2.1. Accessing a local variable with a detached 
 after it has been destroyed
(view table Øgure)
Main thread
New thread
Constructs my_func with reference to
some_local_state
 
Starts new thread my_thread
 
 
Started
 
Calls func::operator()
Detaches my_thread
Running func::operator(); may call
do_something with reference to
some_local_state
Destroys some_local_state
Still running
Exits oops
Still running func::operator(); may call
do_something with reference to
some_local_state => undened
behavior
thread
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
struct func 
{ 
    int& i; 
    func(int& i_):i(i_){} 
    void operator()() 
    { 
        for(unsigned j=0;j<1000000;++j) 
        { 
            do_something(i); 
        } 
    } 
}; 
void oops() 
{ 
    int some_local_state=0; 
    func my_func(some_local_state); 
    std::thread my_thread(my_func); 
    my_thread.detach(); 
}
1
2
3
Listing 2.2. Waiting for a thread to Ønish
1
2
3
4
5
struct func; 
void f() 
{ 
    int some_local_state=0; 
    func my_func(some_local_state); 
1

6
7
8
9
10
11
12
13
14
15
16
17
    std::thread t(my_func); 
    try 
    { 
        do_something_in_current_thread(); 
    } 
    catch(...) 
    { 
        t.join(); 
        throw; 
    } 
    t.join(); 
}
Listing 2.3. Using RAII to wait for a thread to complete
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
class thread_guard 
{ 
    std::thread& t; 
public: 
    explicit thread_guard(std::thread& t_): 
        t(t_) 
    {} 
    ~thread_guard() 
    { 
        if(t.joinable()) 
        { 
            t.join(); 
        } 
    } 
    thread_guard(thread_guard const&)=delete; 
    thread_guard& operator=(thread_guard const&)=delete; 
}; 
struct func; 
void f() 
{ 
    int some_local_state=0; 
    func my_func(some_local_state); 
    std::thread t(my_func); 
    thread_guard g(t); 
    do_something_in_current_thread(); 
}
1
1
2
3
std::thread t(do_background_work); 
t.detach(); 
assert(!t.joinable());
Listing 2.4. Detaching a thread to handle other documents

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
void edit_document(std::string const& filename) 
{ 
    open_document_and_display_gui(filename); 
    while(!done_editing()) 
    { 
        user_command cmd=get_user_input(); 
        if(cmd.type==open_new_document) 
        { 
            std::string const new_name=get_filename_from_user(); 
            std::thread t(edit_document,new_name); 
            t.detach(); 
        } 
        else 
        { 
            process_user_input(cmd); 
        } 
    } 
}
1
2
void f(int i,std::string const& s); 
std::thread t(f,3,"hello");
1
2
3
4
5
6
7
8
void f(int i,std::string const& s); 
void oops(int some_param) 
{ 
    char buffer[1024]; 
    sprintf(buffer, "%i",some_param); 
    std::thread t(f,3,buffer); 
    t.detach(); 
}
1
2
3
4
5
6
7
8
void f(int i,std::string const& s); 
void not_oops(int some_param) 
{ 
    char buffer[1024]; 
    sprintf(buffer,"%i",some_param); 
    std::thread t(f,3,std::string(buffer)); 
    t.detach(); 
}
1
1
2
3
void update_data_for_widget(widget_id w,widget_data& data); 
void oops_again(widget_id w) 
{ 

4
5
6
7
8
9
    widget_data data; 
    std::thread t(update_data_for_widget,w,data); 
    display_status(); 
    t.join(); 
    process_widget_data(data); 
}
1 std::thread t(update_data_for_widget,w,std::ref(data));
1
2
3
4
5
6
7
class X 
{ 
public: 
    void do_lengthy_work(); 
}; 
X my_x; 
std::thread t(&X::do_lengthy_work,&my_x);
1
2
3
4
void process_big_object(std::unique_ptr<big_object>); 
std::unique_ptr<big_object> p(new big_object); 
p->prepare_data(42); 
std::thread t(process_big_object,std::move(p));
1
2
3
4
5
6
7
8
void some_function(); 
void some_other_function(); 
std::thread t1(some_function); 
std::thread t2=std::move(t1); 
t1=std::thread(some_other_function); 
std::thread t3; 
t3=std::move(t2); 
t1=std::move(t3);
1
Listing 2.5. Returning a std::thread  from a function
1
2
3
std::thread f() 
{ 
    void some_function(); 

4
5
6
7
8
9
10
11
    return std::thread(some_function); 
} 
std::thread g() 
{ 
    void some_other_function(int); 
    std::thread t(some_other_function,42); 
    return t; 
}
1
2
3
4
5
6
7
8
void f(std::thread t); 
void g() 
{ 
    void some_function(); 
    f(std::thread(some_function)); 
    std::thread t(some_function); 
    f(std::move(t)); 
}
Listing 2.6. scoped_thread  and example usage
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
class scoped_thread 
{ 
    std::thread t; 
public: 
    explicit scoped_thread(std::thread t_): 
        t(std::move(t_)) 
    { 
        if(!t.joinable()) 
            throw std::logic_error("No thread"); 
    } 
    ~scoped_thread() 
    { 
        t.join(); 
    } 
    scoped_thread(scoped_thread const&)=delete; 
    scoped_thread& operator=(scoped_thread const&)=delete; 
}; 
struct func; 
void f() 
{ 
    int some_local_state; 
    scoped_thread t{std::thread(func(some_local_state))}; 
    do_something_in_current_thread(); 
}
1
Listing 2.7. A joining_thread  class
1
2
class joining_thread 
{ 

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
    std::thread t; 
public: 
    joining_thread() noexcept=default; 
    template<typename Callable,typename ... Args> 
    explicit joining_thread(Callable&& func,Args&& ... args): 
        t(std::forward<Callable>(func),std::forward<Args>(args)...) 
    {} 
    explicit joining_thread(std::thread t_) noexcept: 
        t(std::move(t_)) 
    {} 
    joining_thread(joining_thread&& other) noexcept: 
        t(std::move(other.t)) 
    {} 
    joining_thread& operator=(joining_thread&& other) noexcept 
    { 
        if(joinable()) 
            join(); 
        t=std::move(other.t); 
        return *this; 
    } 
    joining_thread& operator=(std::thread other) noexcept 
    { 
        if(joinable()) 
            join(); 
        t=std::move(other); 
        return *this; 
    } 
    ~joining_thread() noexcept 
    { 
        if(joinable()) 
            join(); 
    } 
    void swap(joining_thread& other) noexcept 
    { 
        t.swap(other.t); 
    } 
    std::thread::id get_id() const noexcept{ 
        return t.get_id(); 
    } 
    bool joinable() const noexcept 
    { 
        return t.joinable(); 
    } 
    void join() 
    { 
        t.join(); 
    } 
    void detach() 
    { 
        t.detach(); 
    } 
    std::thread& as_thread() noexcept 
    { 
        return t; 
    } 
    const std::thread& as_thread() const noexcept 
    { 
        return t; 
    } 
};
Listing 2.8. Spawns some threads and waits for them to Ønish

1
2
3
4
5
6
7
8
9
10
11
void do_work(unsigned id); 
void f() 
{ 
    std::vector<std::thread> threads; 
    for(unsigned i=0;i<20;++i) 
    { 
        threads.emplace_back(do_work,i); 
    } 
    for(auto& entry: threads) 
        entry.join(); 
}
1
2
Listing 2.9. A naïve parallel version of std::accumulate
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
template<typename Iterator,typename T> 
struct accumulate_block 
{ 
    void operator()(Iterator first,Iterator last,T& result) 
    { 
        result=std::accumulate(first,last,result); 
    } 
}; 
template<typename Iterator,typename T> 
T parallel_accumulate(Iterator first,Iterator last,T init) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return init; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    std::vector<T> results(num_threads); 
    std::vector<std::thread>  threads(num_threads-1); 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        Iterator block_end=block_start; 
        std::advance(block_end,block_size); 
        threads[i]=std::thread( 
            accumulate_block<Iterator,T>(), 
            block_start,block_end,std::ref(results[i])); 
        block_start=block_end; 
    } 
    accumulate_block<Iterator,T>()( 
        block_start,last,results[num_threads-1]); 
 
    for(auto& entry: threads) 
           entry.join(); 
    return std::accumulate(results.begin(),results.end(),init); 
}

1
2
3
4
5
6
7
8
9
std::thread::id master_thread; 
void some_core_part_of_algorithm() 
{ 
    if(std::this_thread::get_id()==master_thread) 
    { 
        do_master_thread_work(); 
    } 
    do_common_work(); 
}
1 std::cout<<std::this_thread::get_id();

CHAPTER 3
Figure 3.1. Deleting a 
 from a doubly linked list
node
Listing 3.1. Protecting a list with a mutex
1
2
3
#include <list> 
#include <mutex> 
#include <algorithm> 

4
5
6
7
8
9
10
11
12
13
14
15
16
std::list<int> some_list; 
std::mutex some_mutex; 
void add_to_list(int new_value) 
{ 
    std::lock_guard<std::mutex> guard(some_mutex); 
    some_list.push_back(new_value); 
} 
bool list_contains(int value_to_find) 
{ 
    std::lock_guard<std::mutex> guard(some_mutex); 
    return std::find(some_list.begin(),some_list.end(),value_to_find) 
        != some_list.end(); 
}
12
3
4
1 std::lock_guard guard(some_mutex);
1 std::scoped_lock guard(some_mutex);
Listing 3.2. Accidentally passing out a reference to protected data
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
class some_data 
{ 
    int a; 
    std::string b; 
public: 
    void do_something(); 
}; 
class data_wrapper 
{ 
private: 
    some_data data; 
    std::mutex m; 
public: 
    template<typename Function> 
    void process_data(Function func) 
    { 
        std::lock_guard<std::mutex> l(m); 
        func(data); 
    } 
}; 
some_data* unprotected; 
void malicious_function(some_data& protected_data) 
{ 
    unprotected=&protected_data; 
} 
data_wrapper x; 
void foo() 
{ 
    x.process_data(malicious_function); 
1
2
3

Table 3.1. A possible 
ing of 
 on a stack from two 
 (view table
Øgure)
Thread A
Thread B
if(!s.empty())
 
 
 
 
if(!s.empty())
 
int const value=s.top();
 
 
 
 
 
int const value=s.top();
order
operations
threads
30
31
    unprotected->do_something(); 
}
Listing 3.3. The interface to the std::stack container adapter
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
template<typename T,typename Container=std::deque<T> > 
class stack 
{ 
public: 
    explicit stack(const Container&); 
    explicit stack(Container&& = Container()); 
    template <class Alloc> explicit stack(const Alloc&); 
    template <class Alloc> stack(const Container&, const Alloc&); 
    template <class Alloc> stack(Container&&, const Alloc&); 
    template <class Alloc> stack(stack&&, const Alloc&); 
    bool empty() const; 
    size_t size() const; 
    T& top(); 
    T const& top() const; 
    void push(T const&); 
    void push(T&&); 
    void pop(); 
    void swap(stack&&); 
    template <class... Args> void emplace(Args&&... args); 
};
1
1
2
3
4
5
6
7
stack<int> s; 
if(!s.empty()) 
{ 
    int const value=s.top(); 
    s.pop(); 
    do_something(value); 
}
1
2
3

Thread A
Thread B
 
s.pop();
 
 
 
do_something(value);
 
s.pop();
 
 
 
do_something(value);
1
2
std::vector<int> result; 
some_stack.pop(result);
Listing 3.4. An outline class deØnition for a thread-safe stack
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
#include <exception> 
#include <memory> 
struct empty_stack: std::exception 
{ 
    const char* what() const noexcept; 
}; 
template<typename T> 
class threadsafe_stack 
{ 
public: 
    threadsafe_stack(); 
    threadsafe_stack(const threadsafe_stack&); 
    threadsafe_stack& operator=(const threadsafe_stack&) = delete; 
    void push(T new_value); 
    std::shared_ptr<T> pop(); 
    void pop(T& value); 
    bool empty() const; 
};
1
2
Listing 3.5. A Ùeshed-out class deØnition for a thread-safe stack
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
#include <exception> 
#include <memory> 
#include <mutex> 
#include <stack> 
struct empty_stack: std::exception 
{ 
    const char* what() const throw(); 
}; 
template<typename T> 
class threadsafe_stack 
{ 
private: 
    std::stack<T> data; 
    mutable std::mutex m; 
public: 
    threadsafe_stack(){} 
    threadsafe_stack(const threadsafe_stack& other) 

18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
    { 
        std::lock_guard<std::mutex> lock(other.m); 
        data=other.data; 
    } 
    threadsafe_stack& operator=(const threadsafe_stack&) = delete; 
    void push(T new_value) 
    { 
        std::lock_guard<std::mutex> lock(m); 
        data.push(std::move(new_value)); 
    } 
    std::shared_ptr<T> pop() 
    { 
        std::lock_guard<std::mutex> lock(m); 
        if(data.empty()) throw empty_stack(); 
        std::shared_ptr<T> const res(std::make_shared<T>(data.top())); 
        data.pop(); 
        return res; 
    } 
    void pop(T& value) 
    { 
        std::lock_guard<std::mutex> lock(m); 
        if(data.empty()) throw empty_stack(); 
        value=data.top(); 
        data.pop(); 
    } 
    bool empty() const 
    { 
        std::lock_guard<std::mutex> lock(m); 
        return data.empty(); 
    } 
};
1
2
3
Listing 3.6. Using std::lock()  and std::lock_guard  in a swap operation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
class some_big_object; 
void swap(some_big_object& lhs,some_big_object& rhs); 
class X 
{ 
private: 
    some_big_object some_detail; 
    std::mutex m; 
public: 
    X(some_big_object const& sd):some_detail(sd){} 
    friend void swap(X& lhs, X& rhs) 
    { 
        if(&lhs==&rhs) 
            return; 
        std::lock(lhs.m,rhs.m); 
        std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock); 
        std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock); 
        swap(lhs.some_detail,rhs.some_detail); 
    } 
};
1
2
3
1
2
void swap(X& lhs, X& rhs) 
    { 

Figure 3.2. 
 with 
 traversing a list in opposite 
Deadlock
threads
orders
3
4
5
6
7
        if(&lhs==&rhs) 
            return; 
        std::scoped_lock guard(lhs.m,rhs.m); 
        swap(lhs.some_detail,rhs.some_detail); 
    }
1
1 std::scoped_lock<std::mutex,std::mutex> guard(lhs.m,rhs.m);

Listing 3.7. Using a lock hierarchy to prevent deadlock
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
hierarchical_mutex high_level_mutex(10000); 
hierarchical_mutex low_level_mutex(5000); 
hierarchical_mutex other_mutex(6000); 
int do_low_level_stuff(); 
int low_level_func() 
{ 
    std::lock_guard<hierarchical_mutex> lk(low_level_mutex); 
    return do_low_level_stuff(); 
} 
void high_level_stuff(int some_param); 
void high_level_func() 
{ 
    std::lock_guard<hierarchical_mutex> lk(high_level_mutex); 
    high_level_stuff(low_level_func()); 
} 
void thread_a() 
{ 
    high_level_func(); 
} 
 
void do_other_stuff(); 
void other_stuff() 
{ 
    high_level_func(); 
    do_other_stuff(); 
} 
void thread_b() 
{ 
    std::lock_guard<hierarchical_mutex> lk(other_mutex); 
    other_stuff(); 
}
1
2
3
4
5
6
7
8
9
10
Listing 3.8. A simple hierarchical mutex
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
class hierarchical_mutex 
{ 
    std::mutex internal_mutex; 
    unsigned long const hierarchy_value; 
    unsigned long previous_hierarchy_value; 
    static thread_local unsigned long this_thread_hierarchy_value; 
    void check_for_hierarchy_violation() 
    { 
        if(this_thread_hierarchy_value <= hierarchy_value) 
        { 
            throw std::logic_error("mutex hierarchy violated"); 
        } 
    } 
    void update_hierarchy_value() 
    { 
        previous_hierarchy_value=this_thread_hierarchy_value; 
        this_thread_hierarchy_value=hierarchy_value; 
    } 
public: 
    explicit hierarchical_mutex(unsigned long value): 
        hierarchy_value(value), 
        previous_hierarchy_value(0) 
    {} 
    void lock() 
    { 
        check_for_hierarchy_violation(); 
1
2
3

27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
        internal_mutex.lock(); 
        update_hierarchy_value(); 
    } 
    void unlock() 
    { 
        if(this_thread_hierarchy_value!=hierarchy_value) 
            throw std::logic_error("mutex hierarchy violated"); 
        this_thread_hierarchy_value=previous_hierarchy_value; 
        internal_mutex.unlock(); 
    } 
    bool try_lock() 
    { 
        check_for_hierarchy_violation(); 
        if(!internal_mutex.try_lock()) 
            return false; 
        update_hierarchy_value(); 
        return true; 
    } 
}; 
thread_local unsigned long 
    hierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX);
45
6
7
8
9
Listing 3.9. Using std::lock()  and std::unique_lock  in a swap operation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
class some_big_object; 
void swap(some_big_object& lhs,some_big_object& rhs); 
class X 
{ 
private: 
    some_big_object some_detail; 
    std::mutex m; 
public: 
    X(some_big_object const& sd):some_detail(sd){} 
    friend void swap(X& lhs, X& rhs) 
    { 
        if(&lhs==&rhs) 
            return; 
        std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock); 
        std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock); 
        std::lock(lock_a,lock_b); 
        swap(lhs.some_detail,rhs.some_detail); 
    } 
};
1
2
1
2
3
4
5
6
7
8
9
10
11
12
std::unique_lock<std::mutex> get_lock() 
{ 
    extern std::mutex some_mutex; 
    std::unique_lock<std::mutex> lk(some_mutex); 
    prepare_data(); 
    return lk; 
} 
void process_data() 
{ 
    std::unique_lock<std::mutex> lk(get_lock()); 
    do_something(); 
}
1
2

1
2
3
4
5
6
7
8
9
void get_and_process_data() 
{ 
    std::unique_lock<std::mutex> my_lock(the_mutex); 
    some_class data_to_process=get_next_data_chunk(); 
    my_lock.unlock(); 
    result_type result=process(data_to_process); 
    my_lock.lock(); 
    write_result(data_to_process,result); 
}
1
2
Listing 3.10. Locking one mutex at a time in a comparison operator
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
class Y 
{ 
private: 
    int some_detail; 
    mutable std::mutex m; 
    int get_detail() const 
    { 
        std::lock_guard<std::mutex> lock_a(m); 
        return some_detail; 
    } 
public: 
    Y(int sd):some_detail(sd){} 
    friend bool operator==(Y const& lhs, Y const& rhs) 
    { 
        if(&lhs==&rhs) 
            return true; 
        int const lhs_value=lhs.get_detail(); 
        int const rhs_value=rhs.get_detail(); 
        return lhs_value==rhs_value; 
    } 
};
1
2
3
4
1
2
3
4
5
6
7
8
9
std::shared_ptr<some_resource> resource_ptr; 
void foo() 
{ 
    if(!resource_ptr) 
    { 
        resource_ptr.reset(new some_resource); 
    } 
    resource_ptr->do_something(); 
}
1

Listing 3.11. Thread-safe lazy initialization using a mutex
1
2
3
4
5
6
7
8
9
10
11
12
std::shared_ptr<some_resource> resource_ptr; 
std::mutex resource_mutex; 
void foo() 
{ 
    std::unique_lock<std::mutex> lk(resource_mutex); 
    if(!resource_ptr) 
    { 
        resource_ptr.reset(new some_resource); 
    } 
    lk.unlock(); 
    resource_ptr->do_something(); 
}
1
2
1
2
3
4
5
6
7
8
9
10
11
12
void undefined_behaviour_with_double_checked_locking() 
{ 
    if(!resource_ptr) 
    { 
        std::lock_guard<std::mutex> lk(resource_mutex); 
        if(!resource_ptr) 
        { 
            resource_ptr.reset(new some_resource); 
        } 
    } 
    resource_ptr->do_something(); 
}
1
2
3
4
1
2
3
4
5
6
7
8
9
10
11
std::shared_ptr<some_resource> resource_ptr; 
std::once_flag resource_flag; 
void init_resource() 
{ 
    resource_ptr.reset(new some_resource); 
} 
void foo() 
{ 
    std::call_once(resource_flag,init_resource); 
    resource_ptr->do_something(); 
}
1
2
Listing 3.12. Thread-safe lazy initialization of a class member using std::call_once
1
2
3
4
5
6
class X 
{ 
private: 
    connection_info connection_details; 
    connection_handle connection; 
    std::once_flag connection_init_flag; 

7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
    void open_connection() 
    { 
        connection=connection_manager.open(connection_details); 
    } 
public: 
    X(connection_info const& connection_details_): 
        connection_details(connection_details_) 
    {} 
    void send_data(data_packet const& data) 
    { 
        std::call_once(connection_init_flag,&X::open_connection,this); 
        connection.send_data(data); 
    } 
    data_packet receive_data() 
    { 
        std::call_once(connection_init_flag,&X::open_connection,this); 
        return connection.receive_data(); 
    } 
};
1
2
2
3
1
2
3
4
5
6
class my_class; 
my_class& get_my_class_instance() 
{ 
    static my_class instance; 
    return instance; 
}
1
Listing 3.13. Protecting a data structure with std::shared_mutex
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
#include <map> 
#include <string> 
#include <mutex> 
#include <shared_mutex> 
class dns_entry; 
class dns_cache 
{ 
    std::map<std::string,dns_entry> entries; 
    mutable std::shared_mutex entry_mutex; 
public: 
    dns_entry find_entry(std::string const& domain) const 
    { 
        std::shared_lock<std::shared_mutex> lk(entry_mutex); 
        std::map<std::string,dns_entry>::const_iterator const it= 
            entries.find(domain); 
        return (it==entries.end())?dns_entry():it->second; 
    } 
    void update_or_add_entry(std::string const& domain, 
                             dns_entry const& dns_details) 
    { 
        std::lock_guard<std::shared_mutex> lk(entry_mutex); 
        entries[domain]=dns_details; 
    } 
};
1
2


CHAPTER 4
1
2
3
4
5
6
7
8
9
10
11
12
bool flag; 
std::mutex m; 
void wait_for_flag() 
{ 
    std::unique_lock<std::mutex> lk(m); 
    while(!flag) 
    { 
        lk.unlock(); 
        std::this_thread::sleep_for(std::chrono::milliseconds(100)); 
        lk.lock(); 
    } 
}
1
2
3
Listing 4.1. Waiting for data to process with std::condition_variable
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
std::mutex mut; 
std::queue<data_chunk> data_queue; 
std::condition_variable data_cond; 
void data_preparation_thread() 
{ 
    while(more_data_to_prepare()) 
    { 
        data_chunk const data=prepare_data(); 
        { 
            std::lock_guard<std::mutex> lk(mut); 
            data_queue.push(data); 
        } 
        data_cond.notify_one(); 
    } 
} 
void data_processing_thread() 
{ 
    while(true) 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait( 
            lk,[]{return !data_queue.empty();}); 
        data_chunk data=data_queue.front(); 
        data_queue.pop(); 
        lk.unlock(); 
        process(data); 
        if(is_last_chunk(data)) 
            break; 
    } 
}
1
2
3
4
5
6

1
2
3
4
5
6
7
template<typename Predicate> 
void minimal_wait(std::unique_lock<std::mutex>& lk,Predicate pred){ 
    while(!pred()){ 
        lk.unlock(); 
        lk.lock(); 
    } 
}
Listing 4.2. std::queue  interface
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
template <class T, class Container = std::deque<T> > 
class queue { 
public: 
    explicit queue(const Container&); 
    explicit queue(Container&& = Container()); 
    template <class Alloc> explicit queue(const Alloc&); 
    template <class Alloc> queue(const Container&, const Alloc&); 
    template <class Alloc> queue(Container&&, const Alloc&); 
    template <class Alloc> queue(queue&&, const Alloc&); 
    void swap(queue& q); 
    bool empty() const; 
    size_type size() const; 
    T& front(); 
    const T& front() const; 
    T& back(); 
    const T& back() const; 
    void push(const T& x); 
    void push(T&& x); 
    void pop(); 
    template <class... Args> void emplace(Args&&... args); 
};
Listing 4.3. The interface of your threadsafe_queue
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
#include <memory> 
template<typename T> 
class threadsafe_queue 
{ 
public: 
    threadsafe_queue(); 
    threadsafe_queue(const threadsafe_queue&); 
    threadsafe_queue& operator=( 
        const threadsafe_queue&) = delete; 
    void push(T new_value); 
    bool try_pop(T& value); 
    std::shared_ptr<T> try_pop(); 
    void wait_and_pop(T& value); 
    std::shared_ptr<T> wait_and_pop(); 
    bool empty() const; 
};
1
2
3
4

Listing 4.4. Extracting push()  and wait_and_pop()  from listing 4.1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
#include <queue> 
#include <mutex> 
#include <condition_variable> 
template<typename T> 
class threadsafe_queue 
{ 
private: 
    std::mutex mut; 
    std::queue<T> data_queue; 
    std::condition_variable data_cond; 
public: 
    void push(T new_value) 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        data_queue.push(new_value); 
        data_cond.notify_one(); 
    } 
    void wait_and_pop(T& value) 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        value=data_queue.front(); 
        data_queue.pop(); 
    } 
}; 
threadsafe_queue<data_chunk> data_queue; 
void data_preparation_thread() 
{ 
    while(more_data_to_prepare()) 
    { 
        data_chunk const data=prepare_data(); 
        data_queue.push(data); 
    } 
} 
void data_processing_thread() 
{ 
    while(true) 
    { 
        data_chunk data; 
        data_queue.wait_and_pop(data); 
        process(data); 
        if(is_last_chunk(data)) 
            break; 
    } 
}
1
2
3
Listing 4.5. Full class deØnition of a thread-safe queue using condition variables
1
2
3
4
5
6
7
8
9
10
11
#include <queue> 
#include <memory> 
#include <mutex> 
#include <condition_variable> 
template<typename T> 
class threadsafe_queue 
{ 
private: 
    mutable std::mutex mut; 
    std::queue<T> data_queue; 
    std::condition_variable data_cond; 
1

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
public: 
    threadsafe_queue() 
    {} 
    threadsafe_queue(threadsafe_queue const& other) 
    { 
        std::lock_guard<std::mutex> lk(other.mut); 
        data_queue=other.data_queue; 
    } 
    void push(T new_value) 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        data_queue.push(new_value); 
        data_cond.notify_one(); 
    } 
    void wait_and_pop(T& value) 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        value=data_queue.front(); 
        data_queue.pop(); 
    } 
    std::shared_ptr<T> wait_and_pop() 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front())); 
        data_queue.pop(); 
        return res; 
    } 
    bool try_pop(T& value) 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        if(data_queue.empty()) 
            return false; 
        value=data_queue.front(); 
        data_queue.pop(); 
        return true; 
    } 
    std::shared_ptr<T> try_pop() 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        if(data_queue.empty()) 
            return std::shared_ptr<T>(); 
        std::shared_ptr<T> res(std::make_shared<T>(data_queue.front())); 
        data_queue.pop(); 
        return res; 
    } 
    bool empty() const 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        return data_queue.empty(); 
    } 
};
Listing 4.6. Using std::future  to get the return value of an asynchronous task
1
2
3
4
5
6
7
#include <future> 
#include <iostream> 
int find_the_answer_to_ltuae(); 
void do_other_stuff(); 
int main() 
{ 
    std::future<int> the_answer=std::async(find_the_answer_to_ltuae); 

8
9
10
    do_other_stuff(); 
    std::cout<<"The answer is "<<the_answer.get()<<std::endl; 
}
Listing 4.7. Passing arguments to a function with std::async
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
#include <string> 
#include <future> 
struct X 
{ 
    void foo(int,std::string const&); 
    std::string bar(std::string const&); 
}; 
X x; 
auto f1=std::async(&X::foo,&x,42,"hello"); 
auto f2=std::async(&X::bar,x,"goodbye"); 
struct Y 
{ 
    double operator()(double); 
}; 
Y y; 
auto f3=std::async(Y(),3.141); 
auto f4=std::async(std::ref(y),2.718); 
X baz(X&); 
std::async(baz,std::ref(x)); 
class move_only 
{ 
public: 
    move_only(); 
    move_only(move_only&&) 
    move_only(move_only const&) = delete; 
    move_only& operator=(move_only&&); 
    move_only& operator=(move_only const&) = delete; 
    void operator()(); 
}; 
auto f5=std::async(move_only());
1
2
3
4
5
6
1
2
3
4
5
6
7
auto f6=std::async(std::launch::async,Y(),1.2); 
auto f7=std::async(std::launch::deferred,baz,std::ref(x)); 
auto f8=std::async( 
   std::launch::deferred | std::launch::async, 
   baz,std::ref(x)); 
auto f9=std::async(baz,std::ref(x)); 
f7.wait();
1
2
3
4
Listing 4.8. Partial class deØnition for a specialization of std::packaged_task< >
1
2
template<> 
class packaged_task<std::string(std::vector<char>*,int)> 

3
4
5
6
7
8
9
{ 
public: 
    template<typename Callable> 
    explicit packaged_task(Callable&& f); 
    std::future<std::string> get_future(); 
    void operator()(std::vector<char>*,int); 
};
Listing 4.9. Running code on a GUI thread using std::packaged_task
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
#include <deque> 
#include <mutex> 
#include <future> 
#include <thread> 
#include <utility> 
std::mutex m; 
std::deque<std::packaged_task<void()> > tasks; 
bool gui_shutdown_message_received(); 
void get_and_process_gui_message(); 
void gui_thread() 
{ 
    while(!gui_shutdown_message_received()) 
    { 
        get_and_process_gui_message(); 
        std::packaged_task<void()> task; 
        { 
            std::lock_guard<std::mutex> lk(m); 
            if(tasks.empty()) 
                continue; 
            task=std::move(tasks.front()); 
            tasks.pop_front(); 
        } 
        task(); 
    } 
} 
std::thread gui_bg_thread(gui_thread); 
template<typename Func> 
std::future<void> post_task_for_gui_thread(Func f) 
{ 
    std::packaged_task<void()> task(f); 
    std::future<void> res=task.get_future(); 
    std::lock_guard<std::mutex> lk(m); 
    tasks.push_back(std::move(task)); 
    return res; 
}
1
2
3
4
5
6
7
8
9
10
Listing 4.10. Handling multiple connections from a single thread using promises
1
2
3
4
5
6
7
8
#include <future> 
void process_connections(connection_set& connections) 
{ 
    while(!done(connections)) 
    { 
        for(connection_iterator 
                connection=connections.begin(),end=connections.end(); 
            connection!=end; 
1
2

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
            ++connection) 
        { 
            if(connection->has_incoming_data()) 
            { 
                data_packet data=connection->incoming(); 
                std::promise<payload_type>& p= 
                    connection->get_promise(data.id); 
                p.set_value(data.payload); 
            } 
            if(connection->has_outgoing_data()) 
            { 
                outgoing_packet data= 
                    connection->top_of_outgoing_queue(); 
                connection->send(data.payload); 
                data.promise.set_value(true); 
            } 
        } 
    } 
}
3
4
5
6
1
2
3
4
5
6
7
8
double square_root(double x) 
{ 
    if(x<0) 
    { 
        throw std::out_of_range("x<0"); 
    } 
    return sqrt(x); 
}
1 double y=square_root(-1);
1
2
std::future<double> f=std::async(square_root,-1); 
double y=f.get();
1
2
3
4
5
6
7
8
9
extern std::promise<double> some_promise; 
try 
{ 
    some_promise.set_value(calculate_value()); 
} 
catch(...) 
{ 
    some_promise.set_exception(std::current_exception()); 
}

Figure 4.1. Using multiple 
:shared_
 
 to avoid 
 races
std:
future objects
data
1 some_promise.set_exception(std::make_exception_ptr(std::logic_error("foo ")));
1
2
3
4
std::promise<int> p; 
std::future<int> f(p.get_future()); 
assert(f.valid()); 
std::shared_future<int> sf(std::move(f)); 
1

5
6
assert(!f.valid()); 
assert(sf.valid());
2
3
1
2
std::promise<std::string> p; 
std::shared_future<std::string> sf(p.get_future());
1
1
2
3
std::promise< std::map< SomeIndexType, SomeDataType, SomeComparator, 
    SomeAllocator>::iterator> p; 
auto sf=p.get_future().share();
1
2
3
4
using namespace std::chrono_literals; 
auto one_day=24h; 
auto half_an_hour=30min; 
auto max_time_between_messages=30ms;
1
2
3
std::chrono::milliseconds ms(54802); 
std::chrono::seconds s= 
    std::chrono::duration_cast<std::chrono::seconds>(ms);
1
2
3
std::future<int> f=std::async(some_task); 
if(f.wait_for(std::chrono::milliseconds(35))==std::future_status::ready) 
    do_something_with(f.get());
1
2
3
4
5
6
auto start=std::chrono::high_resolution_clock::now(); 
do_something(); 
auto stop=std::chrono::high_resolution_clock::now(); 
std::cout<<"do_something() took " 
  <<std::chrono::duration<double,std::chrono::seconds>(stop-start).count() 
  <<" seconds"<<std::endl;

Table 4.1. (view table Øgure)
Class/Namespace
Return Values
:this_
 namespace
sleep_for(
) sleep_until(time_point)
N/A
std::
_variable or std::condition_variable_any
_for(
,duration)
wait_until(lock,time_point)
std::cv_status::
 or
std::cv_status::no_timeout
 
wait_for(lock,duration, predicate)
wait_until(lock,time_point, predicate)
bool—the return value of the
predicate when woken
std::timed_
, std::
_timed_mutex or
std::shared_timed_mutextry_lock_for(duration) try_lock_until(time_point)
bool—true if the lock was acquired, 
otherwise
 
std::shared_timed_mutex
try_lock_shared_for(duration)
try_lock_shared_until(time_point)
bool—true if the lock was
acquired, false otherwise
std::unique_lock<TimedLockable>unique_lock(lockable,duration)
unique_lock(lockable,time_point)
N/A—owns_lock() on the newly-constructed
 returns true if the lock was acquired, false
otherwise
 
try_lock_for(duration)
try_lock_until(time_point)
bool—true if the lock was
acquired, false otherwise
std::shared_lock<SharedTimedLockable>shared_lock (lockable,duration)
shared_lock(lockable,time_point)
N/A—owns_lock() on the newly-constructed
object returns true if the lock was acquired, false
otherwise
 
Functions
std:
thread
duration
condition
wait
lock
timeout
mutex
recursive
false
object
Listing 4.11. Waiting for a condition variable with a timeout
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
#include <condition_variable> 
#include <mutex> 
#include <chrono> 
std::condition_variable cv; 
bool done; 
std::mutex m; 
bool wait_loop() 
{ 
    auto const timeout= std::chrono::steady_clock::now()+ 
        std::chrono::milliseconds(500); 
    std::unique_lock<std::mutex> lk(m); 
    while(!done) 
    { 
        if(cv.wait_until(lk,timeout)==std::cv_status::timeout) 
            break; 
    } 
    return done; 
}

Class/Namespace
Return Values
try_lock_for(duration)
try_lock_until(time_point)
bool—true if the lock was
acquired, false otherwise
std::
<ValueType> or std::shared_future<ValueType>wait_for(duration)
wait_until(time_point)
std::future_status::timeout if the wait timed
out, std::future_status::ready if the future is
ready, or std::future_status::deferred if the
future holds a deferred function that hasn’t yet
started
 
Functions
future
Figure 4.2. FP-style 
 sorting
recursive
Listing 4.12. A sequential implementation of Quicksort
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
template<typename T> 
std::list<T> sequential_quick_sort(std::list<T> input) 
{ 
    if(input.empty()) 
    { 
        return input; 
    } 
    std::list<T> result; 
    result.splice(result.begin(),input,input.begin()); 
    T const& pivot=*result.begin(); 
 
    auto divide_point=std::partition(input.begin(),input.end(), 
            [&](T const& t){return t<pivot;}); 
    std::list<T> lower_part; 
    lower_part.splice(lower_part.end(),input,input.begin(), 
        divide_point); 
    auto new_lower( 
        sequential_quick_sort(std::move(lower_part))); 
    auto new_higher( 
        sequential_quick_sort(std::move(input))); 
    result.splice(result.end(),new_higher); 
    result.splice(result.begin(),new_lower); 
1
2
3
4
5
6
7
8

Figure 4.3. A simple state machine model for an ATM
24     return result; 
}
Listing 4.13. Parallel Quicksort using futures
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
template<typename T> 
std::list<T> parallel_quick_sort(std::list<T> input) 
{ 
    if(input.empty()) 
    { 
        return input; 
    } 
    std::list<T> result; 
    result.splice(result.begin(),input,input.begin()); 
    T const& pivot=*result.begin(); 
    auto divide_point=std::partition(input.begin(),input.end(), 
            [&](T const& t){return t<pivot;}); 
    std::list<T> lower_part; 
    lower_part.splice(lower_part.end(),input,input.begin(), 
        divide_point); 
    std::future<std::list<T> > new_lower( 
        std::async(&parallel_quick_sort<T>,std::move(lower_part))); 
    auto new_higher( 
        parallel_quick_sort(std::move(input))); 
    result.splice(result.end(),new_higher); 
    result.splice(result.begin(),new_lower.get()); 
    return result; 
}
1
2
3
4
Listing 4.14. A sample implementation of spawn_task
1
2
3
4
5
6
7
8
9
10
11
12
template<typename F,typename A> 
std::future<std::result_of<F(A&&)>::type> 
    spawn_task(F&& f,A&& a) 
{ 
    typedef std::result_of<F(A&&)>::type result_type; 
    std::packaged_task<result_type(A&&)> 
        task(std::move(f))); 
    std::future<result_type> res(task.get_future()); 
    std::thread t(std::move(task),std::move(a)); 
    t.detach(); 
    return res; 
}

Listing 4.15. A simple implementation of an ATM logic class
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
struct card_inserted 
{ 
    std::string account; 
}; 
class atm 
{ 
    messaging::receiver incoming; 
    messaging::sender bank; 
    messaging::sender interface_hardware; 
    void (atm::*state)(); 
    std::string account; 
    std::string pin; 
    void waiting_for_card() 
    { 
        interface_hardware.send(display_enter_card()); 
        incoming.wait() 
            .handle<card_inserted>( 
                [&](card_inserted const& msg) 
                { 
                    account=msg.account; 
                    pin=""; 
                    interface_hardware.send(display_enter_pin()); 
                    state=&atm::getting_pin; 
                } 
                ); 
    } 
    void getting_pin(); 
public: 
    void run() 
    { 
        state=&atm::waiting_for_card; 
        try 
        { 
            for(;;) 
            { 
1
2
3
4
5
6

36
37
38
39
40
41
42
43
                (this->*state)(); 
            } 
        } 
        catch(messaging::close_queue const&) 
        { 
        } 
    } 
};
7
Listing 4.16. The getting_pin  state function for the simple ATM implementation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
void atm::getting_pin() 
{ 
    incoming.wait() 
        .handle<digit_pressed>( 
            [&](digit_pressed const& msg) 
            { 
                unsigned const pin_length=4; 
                pin+=msg.digit; 
                if(pin.length()==pin_length) 
                { 
                    bank.send(verify_pin(account,pin,incoming)); 
                    state=&atm::verifying_pin; 
                } 
            } 
            ) 
        .handle<clear_last_pressed>( 
            [&](clear_last_pressed const& msg) 
            { 
                if(!pin.empty()) 
                { 
                    pin.resize(pin.length()-1); 
                } 
            } 
            ) 
        .handle<cancel_pressed>( 
            [&](cancel_pressed const& msg) 
            { 
                state=&atm::done_processing; 
            } 
            ); 
}
1
2
3
1
2
3
4
5
std::experimental::future<int> find_the_answer; 
auto fut=find_the_answer(); 
auto fut2=fut.then(find_the_question); 
assert(!fut.valid()); 
assert(fut2.valid());

1 std::string find_the_question(std::experimental::future<int> the_answer);
Listing 4.17. A simple equivalent to std::async  for Concurrency TS futures
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
template<typename Func> 
std::experimental::future<decltype(std::declval<Func>()())> 
spawn_async(Func&& func){ 
    std::experimental::promise< 
        decltype(std::declval<Func>()())> p; 
    auto res=p.get_future(); 
    std::thread t( 
        [p=std::move(p),f=std::decay_t<Func>(func)]() 
            mutable{ 
            try{ 
                p.set_value_at_thread_exit(f()); 
            } catch(...){ 
                p.set_exception_at_thread_exit(std::current_exception()); 
            } 
    }); 
    t.detach(); 
    return res; 
}
Listing 4.18. A simple sequential function to process user login
1
2
3
4
5
6
7
8
9
10
void process_login(std::string const& username,std::string const& password) 
{ 
    try { 
        user_id const id=backend.authenticate_user(username,password); 
        user_data const info_to_display=backend.request_current_info(id); 
        update_display(info_to_display); 
    } catch(std::exception& e){ 
        display_error(e); 
    } 
}
Listing 4.19. Processing user login with a single async task
1
2
3
4
5
6
7
8
9
10
std::future<void> process_login( 
    std::string const& username,std::string const& password) 
{ 
    return std::async(std::launch::async,[=](){ 
        try { 
            user_id const id=backend.authenticate_user(username,password); 
            user_data const info_to_display= 
                backend.request_current_info(id); 
            update_display(info_to_display); 
        } catch(std::exception& e){ 

11
12
13
14
            display_error(e); 
        } 
    }); 
}
Listing 4.20. A function to process user login with continuations
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
std::experimental::future<void> process_login( 
    std::string const& username,std::string const& password) 
{ 
    return spawn_async([=](){ 
        return backend.authenticate_user(username,password); 
    }).then([](std::experimental::future<user_id> id){ 
        return backend.request_current_info(id.get()); 
    }).then([](std::experimental::future<user_data> info_to_display){ 
        try{ 
            update_display(info_to_display.get()); 
        } catch(std::exception& e){ 
            display_error(e); 
        } 
    }); 
}
Listing 4.21. A function to process user login with fully asynchronous operations
1
2
3
4
5
6
7
8
9
10
11
12
13
14
std::experimental::future<void> process_login( 
    std::string const& username,std::string const& password) 
{ 
    return backend.async_authenticate_user(username,password).then( 
        [](std::experimental::future<user_id> id){ 
            return backend.async_request_current_info(id.get()); 
        }).then([](std::experimental::future<user_data> info_to_display){ 
            try{ 
                update_display(info_to_display.get()); 
            } catch(std::exception& e){ 
                display_error(e); 
            } 
        }); 
}
1
2
3
4
return backend.async_authenticate_user(username,password).then( 
        [](auto id){ 
            return backend.async_request_current_info(id.get()); 
        });

1
2
3
4
5
6
7
auto fut=spawn_async(some_function).share(); 
auto fut2=fut.then([](std::experimental::shared_future<some_data> data){ 
    do_stuff(data); 
    }); 
auto fut3=fut.then([](std::experimental::shared_future<some_data> data){ 
    return do_other_stuff(data); 
    });
Listing 4.22. Gathering results from futures using std::async
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
std::future<FinalResult> process_data(std::vector<MyData>& vec) 
{ 
    size_t const chunk_size=whatever; 
    std::vector<std::future<ChunkResult>> results; 
    for(auto begin=vec.begin(),end=vec.end();beg!=end;){ 
        size_t const remaining_size=end-begin; 
        size_t const this_chunk_size=std::min(remaining_size,chunk_size); 
        results.push_back( 
            std::async(process_chunk,begin,begin+this_chunk_size)); 
        begin+=this_chunk_size; 
    } 
    return std::async([all_results=std::move(results)](){ 
        std::vector<ChunkResult> v; 
        v.reserve(all_results.size()); 
        for(auto& f: all_results) 
        { 
            v.push_back(f.get()); 
        } 
        return gather_results(v); 
    }); 
}
1
Listing 4.23. Gathering results from futures using std::experimental::when_all
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
std::experimental::future<FinalResult> process_data( 
    std::vector<MyData>& vec) 
{ 
    size_t const chunk_size=whatever; 
    std::vector<std::experimental::future<ChunkResult>> results; 
    for(auto begin=vec.begin(),end=vec.end();beg!=end;){ 
        size_t const remaining_size=end-begin; 
        size_t const this_chunk_size=std::min(remaining_size,chunk_size); 
        results.push_back( 
            spawn_async( 
            process_chunk,begin,begin+this_chunk_size)); 
        begin+=this_chunk_size; 
    } 
    return std::experimental::when_all( 
        results.begin(),results.end()).then( 
        [](std::future<std::vector< 
             std::experimental::future<ChunkResult>>> ready_results) 
        { 
            std::vector<std::experimental::future<ChunkResult>> 
                all_results=ready_results .get(); 
            std::vector<ChunkResult> v; 
1

22
23
24
25
26
27
28
29
            v.reserve(all_results.size()); 
            for(auto& f: all_results) 
            { 
                v.push_back(f.get()); 
            } 
            return gather_results(v); 
        }); 
}
2
Listing 4.24. Using std::experimental::when_any  to process the Ørst value
found
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
std::experimental::future<FinalResult> 
find_and_process_value(std::vector<MyData> &data) 
{ 
    unsigned const concurrency = std::thread::hardware_concurrency(); 
    unsigned const num_tasks = (concurrency > 0) ? concurrency : 2; 
    std::vector<std::experimental::future<MyData *>> results; 
    auto const chunk_size = (data.size() + num_tasks - 1) / num_tasks; 
    auto chunk_begin = data.begin(); 
    std::shared_ptr<std::atomic<bool>> done_flag = 
        std::make_shared<std::atomic<bool>>(false); 
    for (unsigned i = 0; i < num_tasks; ++i) { 
        auto chunk_end = 
            (i < (num_tasks - 1)) ? chunk_begin + chunk_size : data.end(); 
        results.push_back(spawn_async([=] { 
            for (auto entry = chunk_begin; 
                !*done_flag && (entry != chunk_end); 
                 ++entry) { 
                if (matches_find_criteria(*entry)) { 
                    *done_flag = true; 
                    return &*entry; 
                } 
            } 
            return (MyData *)nullptr; 
        })); 
        chunk_begin = chunk_end; 
    } 
    std::shared_ptr<std::experimental::promise<FinalResult>> final_result = 
        std::make_shared<std::experimental::promise<FinalResult>>(); 
    struct DoneCheck { 
        std::shared_ptr<std::experimental::promise<FinalResult>> 
            final_result; 
 
        DoneCheck( 
            std::shared_ptr<std::experimental::promise<FinalResult>> 
                final_result_) 
            : final_result(std::move(final_result_)) {} 
 
        void operator()( 
            std::experimental::future<std::experimental::when_any_result< 
                std::vector<std::experimental::future<MyData *>>>> 
                results_param) { 
            auto results = results_param.get(); 
            MyData *const ready_result = 
                results.futures[results.index].get(); 
            if (ready_result) 
                final_result->set_value( 
                    process_found_value(*ready_result)); 
            else { 
                results.futures.erase( 
                    results.futures.begin() + results.index); 
1
2
4
5
6
7

51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
                if (!results.futures.empty()) { 
                    std::experimental::when_any( 
                        results.futures.begin(), results.futures.end()) 
                        .then(std::move(*this)); 
            } else { 
                final_result->set_exception( 
                    std::make_exception_ptr( 
                        std::runtime_error("Not found"))); 
            } 
        } 
    }; 
 
    std::experimental::when_any(results.begin(), results.end()) 
        .then(DoneCheck(final_result)); 
    return final_result->get_future(); 
}
3
8
9
10
1
2
3
4
5
6
7
8
9
std::experimental::future<int> f1=spawn_async(func1); 
std::experimental::future<std::string> f2=spawn_async(func2); 
std::experimental::future<double> f3=spawn_async(func3); 
std::experimental::future< 
    std::tuple< 
        std::experimental::future<int>, 
        std::experimental::future<std::string>, 
        std::experimental::future<double>>> result= 
    std::experimental::when_all(std::move(f1),std::move(f2),std::move(f3));
Listing 4.25. Waiting for events with std::experimental::latch
1
2
3
4
5
6
7
8
9
10
11
12
13
14
void foo(){ 
    unsigned const thread_count=...; 
    latch done(thread_count); 
    my_data data[thread_count]; 
    std::vector<std::future<void> > threads; 
    for(unsigned i=0;i<thread_count;++i) 
        threads.push_back(std::async(std::launch::async,[&,i]{ 
            data[i]=make_data(i); 
            done.count_down(); 
            do_more_stuff(); 
        })); 
    done.wait(); 
    process_data(data,thread_count); 
}
1
2
3
4
5
6
7
Listing 4.26. Using std::experimental::barrier
1
2
3
result_chunk process(data_chunk); 
std::vector<data_chunk> 
divide_into_chunks(data_block data, unsigned num_threads); 

4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
 
void process_data(data_source &source, data_sink &sink) { 
    unsigned const concurrency = std::thread::hardware_concurrency(); 
    unsigned const num_threads = (concurrency > 0) ? concurrency : 2; 
 
    std::experimental::barrier sync(num_threads); 
    std::vector<joining_thread> threads(num_threads); 
 
    std::vector<data_chunk> chunks; 
    result_block result; 
 
    for (unsigned i = 0; i < num_threads; ++i) { 
        threads[i] = joining_thread([&, i] { 
            while (!source.done()) { 
                if (!i) { 
                    data_block current_block = 
                        source.get_next_data_block(); 
                    chunks = divide_into_chunks( 
                        current_block, num_threads); 
                } 
                sync.arrive_and_wait(); 
                result.set_chunk(i, num_threads, process(chunks[i])); 
                sync.arrive_and_wait(); 
                if (!i) { 
                    sink.write_data(std::move(result)); 
                } 
            } 
        }); 
    } 
}
1
2
3
4
5
6
7
Listing 4.27. Using std::flex_barrier  to provide a serial region
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
void process_data(data_source &source, data_sink &sink) { 
    unsigned const concurrency = std::thread::hardware_concurrency(); 
    unsigned const num_threads = (concurrency > 0) ? concurrency : 2; 
 
    std::vector<data_chunk> chunks; 
 
    auto split_source = [&] { 
        if (!source.done()) { 
            data_block current_block = source.get_next_data_block(); 
            chunks = divide_into_chunks(current_block, num_threads); 
        } 
    }; 
 
    split_source(); 
 
    result_block result; 
 
    std::experimental::flex_barrier sync(num_threads, [&] { 
        sink.write_data(std::move(result)); 
        split_source(); 
        return -1; 
    }); 
    std::vector<joining_thread> threads(num_threads); 
 
    for (unsigned i = 0; i < num_threads; ++i) { 
        threads[i] = joining_thread([&, i] { 
            while (!source.done()) { 
                result.set_chunk(i, num_threads, process(chunks[i])); 
                sync.arrive_and_wait(); 
            } 
1
2
3
4
5
6
7

31
32
33
        }); 
    } 
}

CHAPTER 5
Figure 5.1. The division of a struct  into 
 and 
objects
memory locations
Table 5.1. The alternative names for the 
 and their corresponding
:atomic<>  specializations (view table Øgure)
Atomic type
Corresponding specialization
atomic_bool
std::
atomic_char
std::atomic<char>
atomic_schar
std::atomic<signed char>
atomic_uchar
std::atomic<unsigned char>
atomic_int
std::atomic<int>
atomic_uint
std::atomic<unsigned>
atomic_short
std::atomic<short>
atomic_ushort
std::atomic<unsigned short>
atomic_long
std::atomic<long>
standard atomic types
std:
atomic<bool>

Atomic type
Corresponding specialization
atomic_ulong
std::atomic<unsigned long>
atomic_llong
std::atomic<long long>
atomic_ullong
std::atomic<unsigned long long>
atomic_char16_t
std::atomic<char16_t>
atomic_char32_t
std::atomic<char32_t>
atomic_wchar_t
std::atomic<wchar_t>
Table 5.2. The standard 
 typedef s and their corresponding built-in
typedef s (view table Øgure)
Atomic typedef
Corresponding Standard Library typedef
atomic_int_least8_t
int_least8_t
atomic_uint_least8_t
uint_least8_t
atomic_int_least16_t
int_least16_t
atomic_uint_least16_t
uint_least16_t
atomic_int_least32_t
int_least32_t
atomic_uint_least32_t
uint_least32_t
atomic_int_least64_t
int_least64_t
atomic_uint_least64_t
uint_least64_t
atomic_int_fast8_t
int_fast8_t
atomic_uint_fast8_t
uint_fast8_t
atomic_int_fast16_t
int_fast16_t
atomic_uint_fast16_t
uint_fast16_t
atomic_int_fast32_t
int_fast32_t
atomic_uint_fast32_t
uint_fast32_t
atomic_int_fast64_t
int_fast64_t
atomic_uint_fast64_t
uint_fast64_t
atomic_intptr_t
intptr_t
atomic_uintptr_t
uintptr_t
atomic_size_t
size_t
atomic_ptrdi|_t
ptrdi|_t
atomic_intmax_t
intmax_t
atomic_uintmax_t
uintmax_t
atomic

1 std::atomic_flag f=ATOMIC_FLAG_INIT;
1
2
f.clear(std::memory_order_release); 
bool x=f.test_and_set();
1
2
Listing 5.1. Implementation of a spinlock mutex using std::atomic_Ùag
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
class spinlock_mutex 
{ 
    std::atomic_flag flag; 
public: 
    spinlock_mutex(): 
        flag(ATOMIC_FLAG_INIT) 
    {} 
    void lock() 
    { 
        while(flag.test_and_set(std::memory_order_acquire)); 
    } 
    void unlock() 
    { 
        flag.clear(std::memory_order_release); 
    } 
};
1
2
std::atomic<bool> b(true); 
b=false;
1
2
3
4
std::atomic<bool> b; 
bool x=b.load(std::memory_order_acquire); 
b.store(true); 
x=b.exchange(false,std::memory_order_acq_rel);
1
2
3
bool expected=false; 
extern atomic<bool> b; // set somewhere else 
while(!b.compare_exchange_weak(expected,true) && !expected);

Table 5.3. The 
 available on 
 (view table Øgure)
Operation
atomic<integral-type>
atomic<other-type>
test_and_set
Y
 
 
 
 
clear
Y
 
 
 
 
is_
_free
 
Y
Y
Y
Y
load
 
Y
Y
Y
Y
store
 
Y
Y
Y
Y
exchange
 
Y
Y
Y
Y
compare_exchange_weak,
compare_exchange_strong
 
Y
Y
Y
Y
fetch_add, +=
 
 
Y
Y
 
fetch_sub, -=
 
 
Y
Y
 
fetch_or, |=
 
 
 
Y
 
operations
atomic types
atomic_ag
atomic<bool>
atomic<T*>
lock
1
2
3
4
5
std::atomic<bool> b; 
bool expected; 
b.compare_exchange_weak(expected,true, 
    memory_order_acq_rel,memory_order_acquire); 
b.compare_exchange_weak(expected,true,memory_order_acq_rel);
1
2
3
4
5
6
7
8
9
class Foo{}; 
Foo some_array[5]; 
std::atomic<Foo*> p(some_array); 
Foo* x=p.fetch_add(2); 
assert(x==some_array); 
assert(p.load()==&some_array[2]); 
x=(p-=1); 
assert(x==&some_array[1]); 
assert(p.load()==&some_array[1]);
1
2
1 p.fetch_add(3,std::memory_order_release);

Operation
atomic<integral-type>
atomic<other-type>
fetch_and, &=
 
 
 
Y
 
fetch_xor, ^=
 
 
 
Y
 
++, --
 
 
Y
Y
 
atomic_ag
atomic<bool>
atomic<T*>
Figure 5.2. Enforcing an 
ing between 
 using atomic
operations
order
non-atomic operations
1
2
3
4
5
6
7
8
9
10
11
std::shared_ptr<my_data> p; 
void process_global_data() 
{ 
    std::shared_ptr<my_data> local=std::atomic_load(&p); 
    process_data(local); 
} 
void update_global_data() 
{ 
    std::shared_ptr<my_data> local(new my_data); 
    std::atomic_store(&p,local); 
}
Listing 5.2. Reading and writing variables from different threads
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
#include <vector> 
#include <atomic> 
#include <iostream> 
std::vector<int> data; 
std::atomic<bool> data_ready(false); 
void reader_thread() 
{ 
    while(!data_ready.load()) 
    { 
        std::this_thread::sleep(std::chrono::milliseconds(1)); 
    } 
    std::cout<<"The answer="<<data[0]<<"\n"; 
} 
void writer_thread() 
{ 
    data.push_back(42); 
    data_ready=true; 
}
1
2
3
4

Listing 5.3. Order of evaluation of arguments to a function call is unspeciØed
1
2
3
4
5
6
7
8
9
10
11
12
13
14
#include <iostream> 
void foo(int a,int b) 
{ 
    std::cout<<a<<","<<b<<std::endl; 
} 
int get_num() 
{ 
    static int i=0; 
    return ++i; 
} 
int main() 
{ 
    foo(get_num(),get_num()); 
}
1
Listing 5.4. Sequential consistency implies a total ordering

Figure 5.3. Sequential consistency and happens-before
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
#include <atomic> 
#include <thread> 
#include <assert.h> 
std::atomic<bool> x,y; 
std::atomic<int> z; 
void write_x() 
{ 
    x.store(true,std::memory_order_seq_cst); 
} 
void write_y() 
{ 
    y.store(true,std::memory_order_seq_cst); 
} 
void read_x_then_y() 
{ 
    while(!x.load(std::memory_order_seq_cst)); 
    if(y.load(std::memory_order_seq_cst)) 
        ++z; 
} 
void read_y_then_x() 
{ 
    while(!y.load(std::memory_order_seq_cst)); 
    if(x.load(std::memory_order_seq_cst)) 
        ++z; 
} 
int main() 
{ 
    x=false; 
    y=false; 
    z=0; 
    std::thread a(write_x); 
    std::thread b(write_y); 
    std::thread c(read_x_then_y); 
    std::thread d(read_y_then_x); 
    a.join(); 
    b.join(); 
    c.join(); 
    d.join(); 
    assert(z.load()!=0); 
}
1
2
3
4
5

Figure 5.4. Relaxed 
 and happens-before
atomics
Listing 5.5. Relaxed operations have few ordering requirements
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
#include <atomic> 
#include <thread> 
#include <assert.h> 
std::atomic<bool> x,y; 
std::atomic<int> z; 
void write_x_then_y() 
{ 
    x.store(true,std::memory_order_relaxed); 
    y.store(true,std::memory_order_relaxed); 
} 
void read_y_then_x() 
{ 
    while(!y.load(std::memory_order_relaxed)); 
    if(x.load(std::memory_order_relaxed)) 
        ++z; 
} 
int main() 
{ 
    x=false; 
    y=false; 
    z=0; 
    std::thread a(write_x_then_y); 
    std::thread b(read_y_then_x); 
    a.join(); 
    b.join(); 
    assert(z.load()!=0); 
}
1
2
3
4
5

Listing 5.6. Relaxed operations on multiple threads
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
#include <thread> 
#include <atomic> 
#include <iostream> 
std::atomic<int> x(0),y(0),z(0); 
std::atomic<bool> go(false); 
unsigned const loop_count=10; 
struct read_values 
{ 
    int x,y,z; 
}; 
read_values values1[loop_count]; 
read_values values2[loop_count]; 
read_values values3[loop_count]; 
read_values values4[loop_count]; 
read_values values5[loop_count]; 
void increment(std::atomic<int>* var_to_inc,read_values* values) 
{ 
    while(!go) 
        std::this_thread::yield(); 
    for(unsigned i=0;i<loop_count;++i) 
    { 
        values[i].x=x.load(std::memory_order_relaxed); 
        values[i].y=y.load(std::memory_order_relaxed); 
        values[i].z=z.load(std::memory_order_relaxed); 
        var_to_inc->store(i+1,std::memory_order_relaxed); 
        std::this_thread::yield(); 
    } 
} 
void read_vals(read_values* values) 
{ 
    while(!go) 
        std::this_thread::yield(); 
    for(unsigned i=0;i<loop_count;++i) 
    { 
        values[i].x=x.load(std::memory_order_relaxed); 
        values[i].y=y.load(std::memory_order_relaxed); 
        values[i].z=z.load(std::memory_order_relaxed); 
        std::this_thread::yield(); 
    } 
} 
void print(read_values* v) 
{ 
    for(unsigned i=0;i<loop_count;++i) 
    { 
        if(i) 
            std::cout<<","; 
        std::cout<<"("<<v[i].x<<","<<v[i].y<<","<<v[i].z<<")"; 
    } 
    std::cout<<std::endl; 
} 
int main() 
{ 
    std::thread t1(increment,&x,values1); 
    std::thread t2(increment,&y,values2); 
    std::thread t3(increment,&z,values3); 
    std::thread t4(read_vals,values4); 
    std::thread t5(read_vals,values5); 
    go=true; 
    t5.join(); 
    t4.join(); 
    t3.join(); 
    t2.join(); 
    t1.join(); 
    print(values1); 
    print(values2); 
    print(values3); 
1
2
3
4
5
6
7

Figure 5.5. The notebook for the man in the cubicle
67
68
69
    print(values4); 
    print(values5); 
}
1
2
3
4
5
6
7
8
9
10
(0,0,0),(1,0,0),(2,0,0),(3,0,0),(4,0,0),(5,7,0),(6,7,8),(7,9,8),(8,9,8), 
(9,9,10) 
(0,0,0),(0,1,0),(0,2,0),(1,3,5),(8,4,5),(8,5,5),(8,6,6),(8,7,9),(10,8,9), 
(10,9,10) 
(0,0,0),(0,0,1),(0,0,2),(0,0,3),(0,0,4),(0,0,5),(0,0,6),(0,0,7),(0,0,8), 
(0,0,9) 
(1,3,0),(2,3,0),(2,4,1),(3,6,4),(3,9,5),(5,10,6),(5,10,8),(5,10,10), 
(9,10,10),(10,10,10) 
(0,0,0),(0,0,0),(0,0,0),(6,3,7),(6,5,7),(7,7,7),(7,8,7),(8,8,7),(8,8,9), 
(8,8,9)
Listing 5.7. Acquire-release doesn’t imply a total ordering
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
#include <atomic> 
#include <thread> 
#include <assert.h> 
std::atomic<bool> x,y; 
std::atomic<int> z; 
void write_x() 
{ 
    x.store(true,std::memory_order_release); 
} 
void write_y() 
{ 
    y.store(true,std::memory_order_release); 
} 
void read_x_then_y() 
{ 
    while(!x.load(std::memory_order_acquire)); 
    if(y.load(std::memory_order_acquire)) 
        ++z; 
} 
1

Figure 5.6. Acquire-release and happens-before
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
void read_y_then_x() 
{ 
    while(!y.load(std::memory_order_acquire)); 
    if(x.load(std::memory_order_acquire)) 
        ++z; 
} 
int main() 
{ 
    x=false; 
    y=false; 
    z=0; 
    std::thread a(write_x); 
    std::thread b(write_y); 
    std::thread c(read_x_then_y); 
    std::thread d(read_y_then_x); 
    a.join(); 
    b.join(); 
    c.join(); 
    d.join(); 
    assert(z.load()!=0); 
}
2
3
Listing 5.8. Acquire-release operations can impose ordering on relaxed operations
1
2
3
4
5
6
7
8
9
10
11
12
13
#include <atomic> 
#include <thread> 
#include <assert.h> 
std::atomic<bool> x,y; 
std::atomic<int> z; 
void write_x_then_y() 
{ 
    x.store(true,std::memory_order_relaxed); 
    y.store(true,std::memory_order_release); 
} 
void read_y_then_x() 
{ 
    while(!y.load(std::memory_order_acquire)); 
1
2
3

14
15
16
17
18
19
20
21
22
23
24
25
26
27
    if(x.load(std::memory_order_relaxed)) 
        ++z; 
} 
int main() 
{ 
    x=false; 
    y=false; 
    z=0; 
    std::thread a(write_x_then_y); 
    std::thread b(read_y_then_x); 
    a.join(); 
    b.join(); 
    assert(z.load()!=0); 
}
4
5
Listing 5.9. Transitive synchronization using acquire and release ordering
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
std::atomic<int> data[5]; 
std::atomic<bool> sync1(false),sync2(false); 
void thread_1() 
{ 
    data[0].store(42,std::memory_order_relaxed); 
    data[1].store(97,std::memory_order_relaxed); 
    data[2].store(17,std::memory_order_relaxed); 
    data[3].store(-141,std::memory_order_relaxed); 
    data[4].store(2003,std::memory_order_relaxed); 
    sync1.store(true,std::memory_order_release); 
} 
void thread_2() 
{ 
    while(!sync1.load(std::memory_order_acquire)); 
    sync2.store(true,std::memory_order_release); 
} 
void thread_3() 
{ 
    while(!sync2.load(std::memory_order_acquire)); 
    assert(data[0].load(std::memory_order_relaxed)==42); 
    assert(data[1].load(std::memory_order_relaxed)==97); 
    assert(data[2].load(std::memory_order_relaxed)==17); 
    assert(data[3].load(std::memory_order_relaxed)==-141); 
    assert(data[4].load(std::memory_order_relaxed)==2003); 
}
1
2
3
4
1
2
3
4
5
6
7
8
9
10
11
12
13
std::atomic<int> sync(0); 
void thread_1() 
{ 
    // ... 
    sync.store(1,std::memory_order_release); 
} 
void thread_2() 
{ 
    int expected=1; 
    while(!sync.compare_exchange_strong(expected,2, 
                                        std::memory_order_acq_rel)) 
        expected=1; 
} 

14
15
16
17
18
void thread_3() 
{ 
    while(sync.load(std::memory_order_acquire)<2); 
    // ... 
}
Listing 5.10. Using std::memory_order_consume  to synchronize data
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
struct X 
{ 
    int i; 
    std::string s; 
}; 
std::atomic<X*> p; 
std::atomic<int> a; 
void create_x() 
{ 
    X* x=new X; 
    x->i=42; 
    x->s="hello"; 
    a.store(99,std::memory_order_relaxed); 
    p.store(x,std::memory_order_release); 
} 
void use_x() 
{ 
    X* x; 
    while(!(x=p.load(std::memory_order_consume))) 
        std::this_thread::sleep(std::chrono::microseconds(1)); 
    assert(x->i==42); 
    assert(x->s=="hello"); 
    assert(a.load(std::memory_order_relaxed)==99); 
} 
int main() 
{ 
    std::thread t1(create_x); 
    std::thread t2(use_x); 
    t1.join(); 
    t2.join(); 
}
1
2
3
4
5
6
1
2
3
4
5
6
7
int global_data[]={ ... }; 
std::atomic<int> index; 
void f() 
{ 
    int i=index.load(std::memory_order_consume); 
    do_something_with(global_data[std::kill_dependency(i)]); 
}
Listing 5.11. Reading values from a queue with atomic operations

Figure 5.7. The 
 sequence for the queue 
 from listing 5.11
release
operations
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
#include <atomic> 
#include <thread> 
std::vector<int> queue_data; 
std::atomic<int> count; 
void populate_queue() 
{ 
    unsigned const number_of_items=20; 
    queue_data.clear(); 
    for(unsigned i=0;i<number_of_items;++i) 
    { 
        queue_data.push_back(i); 
    } 
 
    count.store(number_of_items,std::memory_order_release); 
} 
void consume_queue_items() 
{ 
    while(true) 
    { 
        int item_index; 
        if((item_index=count.fetch_sub(1,std::memory_order_acquire))<=0) 
        { 
            wait_for_more_items(); 
            continue; 
        } 
        process(queue_data[item_index-1]); 
    } 
} 
int main() 
{ 
    std::thread a(populate_queue); 
    std::thread b(consume_queue_items); 
    std::thread c(consume_queue_items); 
    a.join(); 
    b.join(); 
    c.join(); 
}
1
2
3
4

Listing 5.12. Relaxed operations can be ordered with fences
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
#include <atomic> 
#include <thread> 
#include <assert.h> 
std::atomic<bool> x,y; 
std::atomic<int> z; 
void write_x_then_y() 
{ 
    x.store(true,std::memory_order_relaxed); 
    std::atomic_thread_fence(std::memory_order_release); 
    y.store(true,std::memory_order_relaxed); 
} 
void read_y_then_x() 
{ 
    while(!y.load(std::memory_order_relaxed)); 
    std::atomic_thread_fence(std::memory_order_acquire); 
    if(x.load(std::memory_order_relaxed)) 
        ++z; 
} 
int main() 
{ 
    x=false; 
    y=false; 
    z=0; 
    std::thread a(write_x_then_y); 
    std::thread b(read_y_then_x); 
    a.join(); 
    b.join(); 
    assert(z.load()!=0); 
}
1
2
3
4
5
6
7

1
2
3
4
5
6
void write_x_then_y() 
{ 
    std::atomic_thread_fence(std::memory_order_release); 
    x.store(true,std::memory_order_relaxed); 
    y.store(true,std::memory_order_relaxed); 
}
Listing 5.13. Enforcing ordering on non-atomic operations
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
#include <atomic> 
#include <thread> 
#include <assert.h> 
bool x=false; 
std::atomic<bool> y; 
std::atomic<int> z; 
void write_x_then_y() 
{ 
    x=true; 
    std::atomic_thread_fence(std::memory_order_release); 
    y.store(true,std::memory_order_relaxed); 
} 
void read_y_then_x() 
{ 
    while(!y.load(std::memory_order_relaxed)); 
    std::atomic_thread_fence(std::memory_order_acquire); 
    if(x) 
        ++z; 
} 
int main() 
{ 
    x=false; 
    y=false; 
    z=0; 
    std::thread a(write_x_then_y); 
    std::thread b(read_y_then_x); 
    a.join(); 
    b.join(); 
    assert(z.load()!=0); 
}
1
2
3
4
5
6

CHAPTER 6
Listing 6.1. A class deØnition for a thread-safe stack
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
#include <exception> 
struct empty_stack: std::exception 
{ 
    const char* what() const throw(); 
}; 
template<typename T> 
class threadsafe_stack 
{ 
private: 
    std::stack<T> data; 
    mutable std::mutex m; 
public: 
    threadsafe_stack(){} 
    threadsafe_stack(const threadsafe_stack& other) 
    { 
        std::lock_guard<std::mutex> lock(other.m); 
        data=other.data; 
    } 
    threadsafe_stack& operator=(const threadsafe_stack&) = delete; 
    void push(T new_value) 
    { 
        std::lock_guard<std::mutex> lock(m); 
        data.push(std::move(new_value)); 
    } 
    std::shared_ptr<T> pop() 
    { 
        std::lock_guard<std::mutex> lock(m); 
        if(data.empty()) throw empty_stack(); 
        std::shared_ptr<T> const res( 
           std::make_shared<T>(std::move(data.top()))); 
        data.pop(); 
        return res; 
    } 
    void pop(T& value) 
    { 
        std::lock_guard<std::mutex> lock(m); 
        if(data.empty()) throw empty_stack(); 
        value=std::move(data.top()); 
        data.pop(); 
    } 
    bool empty() const 
    { 
        std::lock_guard<std::mutex> lock(m); 
        return data.empty(); 
    } 
};
1
2
3
4
5
6
Listing 6.2. The full class deØnition for a thread-safe queue using condition variables
1
2
template<typename T> 
class threadsafe_queue 

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
{ 
private: 
    mutable std::mutex mut; 
    std::queue<T> data_queue; 
    std::condition_variable data_cond; 
public: 
    threadsafe_queue() 
    {} 
    void push(T new_value) 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        data_queue.push(std::move(new_value)); 
        data_cond.notify_one(); 
    } 
    void wait_and_pop(T& value) 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        value=std::move(data_queue.front()); 
        data_queue.pop(); 
    } 
    std::shared_ptr<T> wait_and_pop() 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        std::shared_ptr<T> res( 
            std::make_shared<T>(std::move(data_queue.front()))); 
        data_queue.pop(); 
        return res; 
    } 
    bool try_pop(T& value) 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        if(data_queue.empty()) 
            return false; 
        value=std::move(data_queue.front()); 
        data_queue.pop(); 
        return true; 
    } 
    std::shared_ptr<T> try_pop() 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        if(data_queue.empty()) 
            return std::shared_ptr<T>(); 
        std::shared_ptr<T> res( 
            std::make_shared<T>(std::move(data_queue.front()))); 
        data_queue.pop(); 
        return res; 
    } 
    bool empty() const 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        return data_queue.empty(); 
    } 
};
1
2
3
4
5
Listing 6.3. A thread-safe queue holding std::shared_ptr<>  instances
1
2
3
4
5
template<typename T> 
class threadsafe_queue 
{ 
private: 
    mutable std::mutex mut; 

Figure 6.1. A queue represented using a single-linked list
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
    std::queue<std::shared_ptr<T> > data_queue; 
    std::condition_variable data_cond; 
public: 
    threadsafe_queue() 
    {} 
    void wait_and_pop(T& value) 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        value=std::move(*data_queue.front()); 
        data_queue.pop(); 
    } 
    bool try_pop(T& value) 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        if(data_queue.empty()) 
            return false; 
        value=std::move(*data_queue.front()); 
        data_queue.pop(); 
        return true; 
    } 
    std::shared_ptr<T> wait_and_pop() 
    { 
        std::unique_lock<std::mutex> lk(mut); 
        data_cond.wait(lk,[this]{return !data_queue.empty();}); 
        std::shared_ptr<T> res=data_queue.front(); 
        data_queue.pop(); 
        return res; 
    } 
    std::shared_ptr<T> try_pop() 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        if(data_queue.empty()) 
            return std::shared_ptr<T>(); 
        std::shared_ptr<T> res=data_queue.front(); 
        data_queue.pop(); 
        return res; 
    } 
    void push(T new_value) 
    { 
        std::shared_ptr<T> data( 
            std::make_shared<T>(std::move(new_value))); 
        std::lock_guard<std::mutex> lk(mut); 
        data_queue.push(data); 
        data_cond.notify_one(); 
    } 
    bool empty() const 
    { 
        std::lock_guard<std::mutex> lk(mut); 
        return data_queue.empty(); 
    } 
};
1
2
3
4
5

Listing 6.4. A simple single-threaded queue implementation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
template<typename T> 
class queue 
{ 
private: 
    struct node 
    { 
        T data; 
        std::unique_ptr<node> next; 
        node(T data_): 
            data(std::move(data_)) 
        {} 
    }; 
    std::unique_ptr<node> head; 
    node* tail; 
public: 
    queue(): tail(nullptr) 
    {} 
    queue(const queue& other)=delete; 
    queue& operator=(const queue& other)=delete; 
    std::shared_ptr<T> try_pop() 
    { 
        if(!head) 
        { 
            return std::shared_ptr<T>(); 
        } 
        std::shared_ptr<T> const res( 
            std::make_shared<T>(std::move(head->data))); 
        std::unique_ptr<node> const old_head=std::move(head); 
        head=std::move(old_head->next); 
        if(!head) 
            tail=nullptr; 
        return res; 
    } 
    void push(T new_value) 
    { 
        std::unique_ptr<node> p(new node(std::move(new_value))); 
        node* const new_tail=p.get(); 
        if(tail) 
        { 
            tail->next=std::move(p); 
        } 
        else 
        { 
            head=std::move(p); 
        } 
        tail=new_tail; 
    } 
};
1
2
3
4
5
6

Listing 6.5. A simple queue with a dummy node
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
template<typename T> 
class queue 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::unique_ptr<node> next; 
    }; 
    std::unique_ptr<node> head; 
    node* tail; 
public: 
    queue(): 
        head(new node),tail(head.get()) 
    {} 
    queue(const queue& other)=delete; 
    queue& operator=(const queue& other)=delete; 
    std::shared_ptr<T> try_pop() 
    { 
        if(head.get()==tail) 
        { 
            return std::shared_ptr<T>(); 
        } 
        std::shared_ptr<T> const res(head->data); 
        std::unique_ptr<node> old_head=std::move(head); 
        head=std::move(old_head->next); 
        return res; 
    } 
    void push(T new_value) 
    { 
        std::shared_ptr<T> new_data( 
            std::make_shared<T>(std::move(new_value))); 
        std::unique_ptr<node> p(new node); 
        tail->data=new_data; 
        node* const new_tail=p.get(); 
        tail->next=std::move(p); 
        tail=new_tail; 
    } 
};
1
2
3
4
5
6
7
8
9
Listing 6.6. A thread-safe queue with Øne-grained locking
1
2
3
4
5
6
7
8
9
10
11
template<typename T> 
class threadsafe_queue 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::unique_ptr<node> next; 
    }; 
    std::mutex head_mutex; 
    std::unique_ptr<node> head; 

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
    std::mutex tail_mutex; 
    node* tail; 
    node* get_tail() 
    { 
        std::lock_guard<std::mutex> tail_lock(tail_mutex); 
        return tail; 
    } 
    std::unique_ptr<node> pop_head() 
    { 
        std::lock_guard<std::mutex> head_lock(head_mutex); 
 
        if(head.get()==get_tail()) 
        { 
            return nullptr; 
        } 
        std::unique_ptr<node> old_head=std::move(head); 
        head=std::move(old_head->next); 
        return old_head; 
    } 
public: 
    threadsafe_queue(): 
        head(new node),tail(head.get()) 
    {} 
    threadsafe_queue(const threadsafe_queue& other)=delete; 
    threadsafe_queue& operator=(const threadsafe_queue& other)=delete; 
    std::shared_ptr<T> try_pop() 
    { 
        std::unique_ptr<node> old_head=pop_head(); 
        return old_head?old_head->data:std::shared_ptr<T>(); 
    } 
    void push(T new_value) 
    { 
        std::shared_ptr<T> new_data( 
            std::make_shared<T>(std::move(new_value))); 
        std::unique_ptr<node> p(new node); 
        node* const new_tail=p.get(); 
        std::lock_guard<std::mutex> tail_lock(tail_mutex); 
        tail->data=new_data; 
        tail->next=std::move(p); 
        tail=new_tail; 
    } 
};
1
2
3
4
5
6
7
8
9
10
11
12
13
std::unique_ptr<node> pop_head() 
    { 
        node* const old_tail=get_tail(); 
        std::lock_guard<std::mutex> head_lock(head_mutex); 
 
        if(head.get()==old_tail) 
        { 
            return nullptr; 
        } 
        std::unique_ptr<node> old_head=std::move(head); 
        head=std::move(old_head->next); 
        return old_head; 
    }
1
2
3
4

Listing 6.7. A thread-safe queue with locking and waiting: internals and interface
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
template<typename T> 
class threadsafe_queue 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::unique_ptr<node> next; 
    }; 
    std::mutex head_mutex; 
    std::unique_ptr<node> head; 
    std::mutex tail_mutex; 
    node* tail; 
    std::condition_variable data_cond; 
public: 
    threadsafe_queue(): 
        head(new node),tail(head.get()) 
    {} 
    threadsafe_queue(const threadsafe_queue& other)=delete; 
    threadsafe_queue& operator=(const threadsafe_queue& other)=delete; 
    std::shared_ptr<T> try_pop(); 
    bool try_pop(T& value); 
    std::shared_ptr<T> wait_and_pop(); 
    void wait_and_pop(T& value); 
    void push(T new_value); 
    bool empty(); 
};
Listing 6.8. A thread-safe queue with locking and waiting: pushing new values
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
template<typename T> 
void threadsafe_queue<T>::push(T new_value) 
{ 
    std::shared_ptr<T> new_data( 
        std::make_shared<T>(std::move(new_value))); 
    std::unique_ptr<node> p(new node); 
    { 
        std::lock_guard<std::mutex> tail_lock(tail_mutex); 
        tail->data=new_data; 
        node* const new_tail=p.get(); 
        tail->next=std::move(p); 
        tail=new_tail; 
    } 
    data_cond.notify_one(); 
}
Listing 6.9. A thread-safe queue with locking and waiting: wait_and_pop()
1
2
3
4
5
template<typename T> 
class threadsafe_queue 
{ 
private: 
    node* get_tail() 

6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
    { 
        std::lock_guard<std::mutex> tail_lock(tail_mutex); 
        return tail; 
    } 
    std::unique_ptr<node> pop_head() 
    { 
        std::unique_ptr<node> old_head=std::move(head); 
        head=std::move(old_head->next); 
        return old_head; 
    } 
    std::unique_lock<std::mutex> wait_for_data() 
    { 
        std::unique_lock<std::mutex> head_lock(head_mutex); 
        data_cond.wait(head_lock,[&]{return head.get()!=get_tail();}); 
        return std::move(head_lock); 
    } 
    std::unique_ptr<node> wait_pop_head() 
    { 
        std::unique_lock<std::mutex> head_lock(wait_for_data()); 
        return pop_head(); 
    } 
    std::unique_ptr<node> wait_pop_head(T& value) 
    { 
        std::unique_lock<std::mutex> head_lock(wait_for_data()); 
        value=std::move(*head->data); 
        return pop_head(); 
    } 
public: 
    std::shared_ptr<T> wait_and_pop() 
    { 
        std::unique_ptr<node> const old_head=wait_pop_head(); 
        return old_head->data; 
    } 
    void wait_and_pop(T& value) 
    { 
        std::unique_ptr<node> const old_head=wait_pop_head(value); 
    } 
};
1
2
3
4
5
Listing 6.10. A thread-safe queue with locking and waiting: try_pop()  and
empty()
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
template<typename T> 
class threadsafe_queue 
{ 
private: 
    std::unique_ptr<node> try_pop_head() 
    { 
        std::lock_guard<std::mutex> head_lock(head_mutex); 
        if(head.get()==get_tail()) 
        { 
            return std::unique_ptr<node>(); 
        } 
        return pop_head(); 
    } 
    std::unique_ptr<node> try_pop_head(T& value) 
    { 
        std::lock_guard<std::mutex> head_lock(head_mutex); 
        if(head.get()==get_tail()) 
        { 
            return std::unique_ptr<node>(); 
        } 

21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
        value=std::move(*head->data); 
        return pop_head(); 
    } 
public: 
    std::shared_ptr<T> try_pop() 
    { 
        std::unique_ptr<node> old_head=try_pop_head(); 
        return old_head?old_head->data:std::shared_ptr<T>(); 
    } 
    bool try_pop(T& value) 
    { 
        std::unique_ptr<node> const old_head=try_pop_head(value); 
        return old_head; 
    } 
    bool empty() 
    { 
        std::lock_guard<std::mutex> head_lock(head_mutex); 
        return (head.get()==get_tail()); 
    } 
};
1 mapped_type get_value(key_type const& key, mapped_type default_value);
Listing 6.11. A thread-safe lookup table
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
template<typename Key,typename Value,typename Hash=std::hash<Key> > 
class threadsafe_lookup_table 
{ 
private: 
    class bucket_type 
    { 
    private: 
        typedef std::pair<Key,Value> bucket_value; 
        typedef std::list<bucket_value> bucket_data; 
        typedef typename bucket_data::iterator bucket_iterator; 
        bucket_data data; 
        mutable std::shared_mutex mutex; 
 
        bucket_iterator find_entry_for(Key const& key) const 
        { 
            return std::find_if(data.begin(),data.end(), 
                                [&](bucket_value const& item) 
                                {return item.first==key;}); 
        } 
    public: 
        Value value_for(Key const& key,Value const& default_value) const 
        { 
            std::shared_lock<std::shared_mutex> lock(mutex); 
            bucket_iterator const found_entry=find_entry_for(key); 
            return (found_entry==data.end())? 
                default_value:found_entry->second; 
        } 
        void add_or_update_mapping(Key const& key,Value const& value) 
        { 
            std::unique_lock<std::shared_mutex> lock(mutex); 
            bucket_iterator const found_entry=find_entry_for(key); 
1
2
3
4

32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
            if(found_entry==data.end()) 
            { 
                data.push_back(bucket_value(key,value)); 
            } 
            else 
            { 
                found_entry->second=value; 
            } 
        } 
        void remove_mapping(Key const& key) 
        { 
            std::unique_lock<std::shared_mutex> lock(mutex); 
            bucket_iterator const found_entry=find_entry_for(key); 
            if(found_entry!=data.end()) 
            { 
                data.erase(found_entry); 
            } 
        } 
    }; 
    std::vector<std::unique_ptr<bucket_type> > buckets; 
    Hash hasher; 
    bucket_type& get_bucket(Key const& key) const 
    { 
        std::size_t const bucket_index=hasher(key)%buckets.size(); 
        return *buckets[bucket_index]; 
    } 
public: 
    typedef Key key_type; 
    typedef Value mapped_type; 
    typedef Hash hash_type; 
    threadsafe_lookup_table( 
        unsigned num_buckets=19,Hash const& hasher_=Hash()): 
        buckets(num_buckets),hasher(hasher_) 
    { 
        for(unsigned i=0;i<num_buckets;++i) 
        { 
            buckets[i].reset(new bucket_type); 
        } 
    } 
    threadsafe_lookup_table(threadsafe_lookup_table const& other)=delete; 
    threadsafe_lookup_table& operator=( 
        threadsafe_lookup_table const& other)=delete; 
    Value value_for(Key const& key, 
                    Value const& default_value=Value()) const 
    { 
        return get_bucket(key).value_for(key,default_value); 
    } 
    void add_or_update_mapping(Key const& key,Value const& value) 
    { 
        get_bucket(key).add_or_update_mapping(key,value); 
    } 
    void remove_mapping(Key const& key) 
    { 
        get_bucket(key).remove_mapping(key); 
    } 
};
5
6
7
8
9
10
Listing 6.12. Obtaining contents of a threadsafe_lookup_table  as std::map<>
1
2
3
4
std::map<Key,Value> threadsafe_lookup_table::get_map() const 
{ 
    std::vector<std::unique_lock<std::shared_mutex> > locks; 
    for(unsigned i=0;i<buckets.size();++i) 

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
    { 
        locks.push_back( 
            std::unique_lock<std::shared_mutex>(buckets[i].mutex)); 
    } 
    std::map<Key,Value> res; 
    for(unsigned i=0;i<buckets.size();++i) 
    { 
        for(bucket_iterator it=buckets[i].data.begin(); 
            it!=buckets[i].data.end(); 
            ++it) 
        { 
            res.insert(*it); 
        } 
    } 
    return res; 
}
Listing 6.13. A thread-safe list with iteration support
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
template<typename T> 
class threadsafe_list 
{ 
    struct node 
    { 
        std::mutex m; 
        std::shared_ptr<T> data; 
        std::unique_ptr<node> next; 
        node(): 
            next() 
        {} 
        node(T const& value): 
            data(std::make_shared<T>(value)) 
        {} 
    }; 
    node head; 
public: 
    threadsafe_list() 
    {} 
    ~threadsafe_list() 
    { 
        remove_if([](node const&){return true;}); 
    } 
    threadsafe_list(threadsafe_list const& other)=delete; 
    threadsafe_list& operator=(threadsafe_list const& other)=delete; 
    void push_front(T const& value) 
    { 
        std::unique_ptr<node> new_node(new node(value)); 
        std::lock_guard<std::mutex> lk(head.m); 
        new_node->next=std::move(head.next); 
        head.next=std::move(new_node); 
    } 
    template<typename Function> 
    void for_each(Function f) 
    { 
        node* current=&head; 
        std::unique_lock<std::mutex> lk(head.m); 
        while(node* const next=current->next.get()) 
        { 
            std::unique_lock<std::mutex> next_lk(next->m); 
            lk.unlock(); 
            f(*next->data); 
            current=next; 
            lk=std::move(next_lk); 
1
2
3
4
5
6
7
8
9
10
11
12
13

45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
        } 
    } 
    template<typename Predicate> 
    std::shared_ptr<T> find_first_if(Predicate p) 
    { 
        node* current=&head; 
        std::unique_lock<std::mutex> lk(head.m); 
        while(node* const next=current->next.get()) 
        { 
            std::unique_lock<std::mutex> next_lk(next->m); 
            lk.unlock(); 
            if(p(*next->data)) 
            { 
                return next->data; 
            } 
            current=next; 
            lk=std::move(next_lk); 
        } 
        return std::shared_ptr<T>(); 
    } 
    template<typename Predicate> 
    void remove_if(Predicate p) 
    { 
        node* current=&head; 
        std::unique_lock<std::mutex> lk(head.m); 
        while(node* const next=current->next.get()) 
        { 
            std::unique_lock<std::mutex> next_lk(next->m); 
            if(p(*next->data)) 
            { 
                std::unique_ptr<node> old_next=std::move(current->next); 
                current->next=std::move(next->next); 
                next_lk.unlock(); 
            } 
            else 
            { 
                lk.unlock(); 
                current=next; 
                lk=std::move(next_lk); 
            } 
        } 
    } 
};
14
15
16
17
18
19
20
21

CHAPTER 7
Listing 7.1. Implementation of a spin-lock mutex using std::atomic_flag
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
class spinlock_mutex 
{ 
    std::atomic_flag flag; 
public: 
    spinlock_mutex(): 
        flag(ATOMIC_FLAG_INIT) 
    {} 
    void lock() 
    { 
        while(flag.test_and_set(std::memory_order_acquire)); 
    } 
    void unlock() 
    { 
        flag.clear(std::memory_order_release); 
    } 
};
Listing 7.2. Implementing push()  without locks
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
template<typename T> 
class lock_free_stack 
{ 
private: 
    struct node 
    { 
        T data; 
        node* next; 
        node(T const& data_): 
            data(data_) 
        {} 
    }; 
    std::atomic<node*> head; 
public: 
    void push(T const& data) 
    { 
        node* const new_node=new node(data); 
        new_node->next=head.load(); 
        while(!head.compare_exchange_weak(new_node->next,new_node)); 
    } 
};
1
2
3
4
1
2
3
template<typename T> 
class lock_free_stack 
{ 

4
5
6
7
8
9
10
11
public: 
    void pop(T& result) 
    { 
        node* old_head=head.load(); 
        while(!head.compare_exchange_weak(old_head,old_head->next)); 
        result=old_head->data; 
    } 
};
Listing 7.3. A lock-free stack that leaks nodes
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
template<typename T> 
class lock_free_stack 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        node* next; 
        node(T const& data_): 
            data(std::make_shared<T>(data_)) 
        {} 
    }; 
    std::atomic<node*> head; 
public: 
    void push(T const& data) 
    { 
        node* const new_node=new node(data); 
        new_node->next=head.load(); 
        while(!head.compare_exchange_weak(new_node->next,new_node)); 
    } 
    std::shared_ptr<T> pop() 
    { 
        node* old_head=head.load(); 
        while(old_head && 
            !head.compare_exchange_weak(old_head,old_head->next)); 
        return old_head ? old_head->data : std::shared_ptr<T>(); 
    } 
};
1
2
3
4
Listing 7.4. Reclaiming nodes when no threads are in pop()
1
2
3
4
5
6
7
8
9
10
11
12
13
14
template<typename T> 
class lock_free_stack 
{ 
private: 
    std::atomic<unsigned> threads_in_pop; 
    void try_reclaim(node* old_head); 
public: 
    std::shared_ptr<T> pop() 
    { 
        ++threads_in_pop; 
        node* old_head=head.load(); 
        while(old_head && 
              !head.compare_exchange_weak(old_head,old_head->next)); 
        std::shared_ptr<T> res; 
1
2

15
16
17
18
19
20
21
22
        if(old_head) 
        { 
            res.swap(old_head->data); 
        } 
        try_reclaim(old_head); 
        return res; 
    } 
};
3
4
Listing 7.5. The reference-counted reclamation machinery
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
template<typename T> 
class lock_free_stack 
{ 
private: 
    std::atomic<node*> to_be_deleted; 
    static void delete_nodes(node* nodes) 
    { 
        while(nodes) 
        { 
            node* next=nodes->next; 
            delete nodes; 
            nodes=next; 
         } 
    } 
    void try_reclaim(node* old_head) 
    { 
        if(threads_in_pop==1) 
        { 
            node* nodes_to_delete=to_be_deleted.exchange(nullptr); 
            if(!--threads_in_pop) 
            { 
                delete_nodes(nodes_to_delete); 
            } 
            else if(nodes_to_delete) 
            { 
                chain_pending_nodes(nodes_to_delete); 
            } 
            delete old_head; 
        } 
        else 
        { 
            chain_pending_node(old_head); 
            --threads_in_pop; 
        } 
    } 
    void chain_pending_nodes(node* nodes) 
    { 
        node* last=nodes; 
        while(node* const next=last->next) 
        { 
            last=next; 
        } 
        chain_pending_nodes(nodes,last); 
    } 
    void chain_pending_nodes(node* first,node* last) 
    { 
        last->next=to_be_deleted; 
        while(!to_be_deleted.compare_exchange_weak( 
                  last->next,first)); 
    } 
    void chain_pending_node(node* n) 
    { 
1
2
3
4
5
6
7
8
9
10
11

Figure 7.1. Three 
 call pop()  concurrently, showing why you must check
threads_in_pop  after claiming the 
 to be deleted in try_reclaim() .
threads
nodes
53
54
55
        chain_pending_nodes(n,n); 
    } 
};
12

1
2
3
4
5
6
7
8
9
10
11
12
13
std::shared_ptr<T> pop() 
{ 
    std::atomic<void*>& hp=get_hazard_pointer_for_current_thread(); 
    node* old_head=head.load(); 
    node* temp; 
    do 
    { 
        temp=old_head; 
        hp.store(old_head); 
        old_head=head.load(); 
    } while(old_head!=temp); 
    // ... 
}
1
2
3
Listing 7.6. An implementation of pop()  using hazard pointers
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
std::shared_ptr<T> pop() 
{ 
    std::atomic<void*>& hp=get_hazard_pointer_for_current_thread(); 
    node* old_head=head.load(); 
    do 
    { 
        node* temp; 
        do 
        { 
            temp=old_head; 
            hp.store(old_head); 
            old_head=head.load(); 
        } while(old_head!=temp); 
    } 
    while(old_head && 
          !head.compare_exchange_strong(old_head,old_head->next)); 
    hp.store(nullptr); 
    std::shared_ptr<T> res; 
    if(old_head) 
    { 
        res.swap(old_head->data); 
        if(outstanding_hazard_pointers_for(old_head)) 
        { 
            reclaim_later(old_head); 
        } 
        else 
        { 
1
2
3
4

28
29
30
31
32
33
            delete old_head; 
        } 
        delete_nodes_with_no_hazards(); 
    } 
    return res; 
}
5
6
Listing 7.7. A simple implementation of
get_hazard_pointer_for_current_thread()
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
unsigned const max_hazard_pointers=100; 
struct hazard_pointer 
{ 
    std::atomic<std::thread::id> id; 
    std::atomic<void*> pointer; 
}; 
hazard_pointer hazard_pointers[max_hazard_pointers]; 
class hp_owner 
{ 
    hazard_pointer* hp; 
 
public: 
    hp_owner(hp_owner const&)=delete; 
    hp_owner operator=(hp_owner const&)=delete; 
    hp_owner(): 
        hp(nullptr) 
    { 
        for(unsigned i=0;i<max_hazard_pointers;++i) 
        { 
            std::thread::id old_id; 
            if(hazard_pointers[i].id.compare_exchange_strong( 
                old_id,std::this_thread::get_id())) 
            { 
                hp=&hazard_pointers[i]; 
                break; 
            } 
        } 
        if(!hp) 
        { 
            throw std::runtime_error("No hazard pointers available"); 
        } 
    } 
    std::atomic<void*>& get_pointer() 
    { 
        return hp->pointer; 
    } 
    ~hp_owner() 
    { 
        hp->pointer.store(nullptr); 
        hp->id.store(std::thread::id()); 
    } 
}; 
std::atomic<void*>& get_hazard_pointer_for_current_thread() 
{ 
    thread_local static hp_owner hazard; 
    return hazard.get_pointer(); 
}
1
2
3
4
5
6

1
2
3
4
5
6
7
8
9
10
11
bool outstanding_hazard_pointers_for(void* p) 
{ 
    for(unsigned i=0;i<max_hazard_pointers;++i) 
    { 
        if(hazard_pointers[i].pointer.load()==p) 
        { 
            return true; 
        } 
    } 
    return false; 
}
Listing 7.8. A simple implementation of the reclaim functions
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
template<typename T> 
void do_delete(void* p) 
{ 
    delete static_cast<T*>(p); 
} 
struct data_to_reclaim 
{ 
    void* data; 
    std::function<void(void*)> deleter; 
    data_to_reclaim* next; 
    template<typename T> 
    data_to_reclaim(T* p): 
        data(p), 
        deleter(&do_delete<T>), 
        next(0) 
    {} 
    ~data_to_reclaim() 
    { 
        deleter(data); 
    } 
}; 
std::atomic<data_to_reclaim*> nodes_to_reclaim; 
void add_to_reclaim_list(data_to_reclaim* node) 
{ 
    node->next=nodes_to_reclaim.load(); 
    while(!nodes_to_reclaim.compare_exchange_weak(node->next,node)); 
} 
template<typename T> 
void reclaim_later(T* data) 
{ 
    add_to_reclaim_list(new data_to_reclaim(data)); 
} 
void delete_nodes_with_no_hazards() 
{ 
    data_to_reclaim* current=nodes_to_reclaim.exchange(nullptr); 
    while(current) 
    { 
        data_to_reclaim* const next=current->next; 
        if(!outstanding_hazard_pointers_for(current->data)) 
        { 
            delete current; 
        } 
        else 
        { 
            add_to_reclaim_list(current); 
        } 
        current=next; 
1
2
3
4
5
6
7
8
9

49     } 
}
Listing 7.9. A lock-free stack using a lock-free std::shared_ptr<>
implementation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
template<typename T> 
class lock_free_stack 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::shared_ptr<node> next; 
        node(T const& data_): 
            data(std::make_shared<T>(data_)) 
        {} 
    }; 
    std::shared_ptr<node> head; 
public: 
    void push(T const& data) 
    { 
        std::shared_ptr<node> const new_node=std::make_shared<node>(data); 
        new_node->next=std::atomic_load(&head); 
        while(!std::atomic_compare_exchange_weak(&head, 
                  &new_node->next,new_node)); 
    } 
    std::shared_ptr<T> pop() 
    { 
        std::shared_ptr<node> old_head=std::atomic_load(&head); 
        while(old_head && !std::atomic_compare_exchange_weak(&head, 
                  &old_head,std::atomic_load(&old_head->next))); 
        if(old_head) { 
            std::atomic_store(&old_head->next,std::shared_ptr<node>()); 
            return old_head->data; 
        } 
        return std::shared_ptr<T>(); 
    } 
    ~lock_free_stack(){ 
        while(pop()); 
    } 
};
Listing 7.10. Stack implementation using
std::experimental::atomic_shared_ptr<>
1
2
3
4
5
6
7
8
template<typename T> 
class lock_free_stack 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::experimental::atomic_shared_ptr<node> next; 

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
        node(T const& data_): 
            data(std::make_shared<T>(data_)) 
        {} 
    }; 
    std::experimental::atomic_shared_ptr<node> head; 
public: 
    void push(T const& data) 
    { 
        std::shared_ptr<node> const new_node=std::make_shared<node>(data); 
        new_node->next=head.load(); 
        while(!head.compare_exchange_weak(new_node->next,new_node)); 
    } 
    std::shared_ptr<T> pop() 
    { 
        std::shared_ptr<node> old_head=head.load(); 
        while(old_head && !head.compare_exchange_weak( 
                  old_head,old_head->next.load())); 
        if(old_head) { 
            old_head->next=std::shared_ptr<node>(); 
            return old_head->data; 
        } 
        return std::shared_ptr<T>(); 
    } 
    ~lock_free_stack(){ 
        while(pop()); 
    } 
};
Listing 7.11. Pushing a node on a lock-free stack using split reference counts
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
template<typename T> 
class lock_free_stack 
{ 
private: 
    struct node; 
    struct counted_node_ptr 
    { 
        int external_count; 
        node* ptr; 
    }; 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::atomic<int> internal_count; 
        counted_node_ptr next; 
        node(T const& data_): 
            data(std::make_shared<T>(data_)), 
            internal_count(0) 
        {} 
    }; 
    std::atomic<counted_node_ptr> head; 
public: 
    ~lock_free_stack() 
    { 
        while(pop()); 
    } 
    void push(T const& data) 
    { 
        counted_node_ptr new_node; 
        new_node.ptr=new node(data); 
        new_node.external_count=1; 
        new_node.ptr->next=head.load(); 
        while(!head.compare_exchange_weak(new_node.ptr->next,new_node)); 
1
2
3
4
5

34
35
    } 
};
Listing 7.12. Popping a node from a lock-free stack using split reference counts
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
template<typename T> 
class lock_free_stack 
{ 
private: 
    // other parts as in listing 7.11 
    void increase_head_count(counted_node_ptr& old_counter) 
    { 
        counted_node_ptr new_counter; 
        do 
        { 
            new_counter=old_counter; 
            ++new_counter.external_count; 
        } 
        while(!head.compare_exchange_strong(old_counter,new_counter)); 
        old_counter.external_count=new_counter.external_count; 
    } 
public: 
    std::shared_ptr<T> pop()# 
    { 
        counted_node_ptr old_head=head.load(); 
        for(;;) 
        { 
            increase_head_count(old_head); 
            node* const ptr=old_head.ptr; 
            if(!ptr) 
            { 
                return std::shared_ptr<T>(); 
            } 
            if(head.compare_exchange_strong(old_head,ptr->next)) 
            { 
                std::shared_ptr<T> res; 
                res.swap(ptr->data); 
                int const count_increase=old_head.external_count-2; 
                if(ptr->internal_count.fetch_add(count_increase)== 
                   -count_increase) 
                { 
                    delete ptr; 
                } 
                return res; 
            } 
            else if(ptr->internal_count.fetch_sub(1)==1) 
            { 
                delete ptr; 
            } 
        } 
    } 
};
1
2
3
4
5
6
7
8
1
2
3
void push(T const& data) 
{ 
    counted_node_ptr new_node; 

4
5
6
7
8
9
    new_node.ptr=new node(data); 
    new_node.external_count=1; 
    new_node.ptr->next=head.load(std::memory_order_relaxed) 
    while(!head.compare_exchange_weak(new_node.ptr->next,new_node, 
        std::memory_order_release,std::memory_order_relaxed)); 
}
1
2
3
4
5
6
7
8
9
10
11
12
void increase_head_count(counted_node_ptr& old_counter) 
{ 
    counted_node_ptr new_counter; 
    do 
    { 
        new_counter=old_counter; 
        ++new_counter.external_count; 
    } 
    while(!head.compare_exchange_strong(old_counter,new_counter, 
        std::memory_order_acquire,std::memory_order_relaxed)); 
    old_counter.external_count=new_counter.external_count; 
}
Listing 7.13. A lock-free stack with reference counting and relaxed atomic operations
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
template<typename T> 
class lock_free_stack 
{ 
private: 
    struct node; 
    struct counted_node_ptr 
    { 
        int external_count; 
        node* ptr; 
    }; 
    struct node 
    { 
        std::shared_ptr<T> data; 
        std::atomic<int> internal_count; 
        counted_node_ptr next; 
        node(T const& data_): 
            data(std::make_shared<T>(data_)), 
            internal_count(0) 
        {} 
    }; 
    std::atomic<counted_node_ptr> head; 
    void increase_head_count(counted_node_ptr& old_counter) 
    { 
        counted_node_ptr new_counter; 
        do 
        { 
            new_counter=old_counter; 
            ++new_counter.external_count; 
        } 
        while(!head.compare_exchange_strong(old_counter,new_counter, 
                                            std::memory_order_acquire, 
                                            std::memory_order_relaxed)); 
        old_counter.external_count=new_counter.external_count; 
    } 

35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
public: 
    ~lock_free_stack() 
    { 
        while(pop()); 
    } 
    void push(T const& data) 
    { 
        counted_node_ptr new_node; 
        new_node.ptr=new node(data); 
        new_node.external_count=1; 
        new_node.ptr->next=head.load(std::memory_order_relaxed) 
        while(!head.compare_exchange_weak(new_node.ptr->next,new_node, 
                                          std::memory_order_release, 
                                          std::memory_order_relaxed)); 
    } 
    std::shared_ptr<T> pop() 
    { 
        counted_node_ptr old_head= 
            head.load(std::memory_order_relaxed); 
        for(;;) 
        { 
            increase_head_count(old_head); 
            node* const ptr=old_head.ptr; 
            if(!ptr) 
            { 
                return std::shared_ptr<T>(); 
            } 
            if(head.compare_exchange_strong(old_head,ptr->next, 
                                            std::memory_order_relaxed)) 
            { 
                std::shared_ptr<T> res; 
                res.swap(ptr->data); 
                int const count_increase=old_head.external_count-2; 
                if(ptr->internal_count.fetch_add(count_increase, 
                       std::memory_order_release)==-count_increase) 
                { 
                    delete ptr; 
                } 
                return res; 
            } 
            else if(ptr->internal_count.fetch_add(-1, 
                        std::memory_order_relaxed)==1) 
            { 
                ptr->internal_count.load(std::memory_order_acquire); 
                delete ptr; 
            } 
        } 
    } 
};
Listing 7.14. A single-producer, single-consumer lock-free queue
1
2
3
4
5
6
7
8
9
10
11
template<typename T> 
class lock_free_queue 
{ 
private: 
    struct node 
    { 
        std::shared_ptr<T> data; 
        node* next; 
        node(): 
            next(nullptr) 
        {} 

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
    }; 
    std::atomic<node*> head; 
    std::atomic<node*> tail; 
    node* pop_head() 
    { 
        node* const old_head=head.load(); 
        if(old_head==tail.load()) 
        { 
            return nullptr; 
        } 
        head.store(old_head->next); 
        return old_head; 
    } 
public: 
    lock_free_queue(): 
        head(new node),tail(head.load()) 
    {} 
    lock_free_queue(const lock_free_queue& other)=delete; 
    lock_free_queue& operator=(const lock_free_queue& other)=delete; 
    ~lock_free_queue() 
    { 
        while(node* const old_head=head.load()) 
        { 
            head.store(old_head->next); 
            delete old_head; 
        } 
    } 
    std::shared_ptr<T> pop() 
    { 
        node* old_head=pop_head(); 
        if(!old_head) 
        { 
            return std::shared_ptr<T>(); 
        } 
        std::shared_ptr<T> const res(old_head->data); 
        delete old_head; 
        return res; 
    } 
    void push(T new_value) 
    { 
        std::shared_ptr<T> new_data(std::make_shared<T>(new_value)); 
        node* p=new node; 
        node* const old_tail=tail.load(); 
        old_tail->data.swap(new_data); 
        old_tail->next=p; 
        tail.store(p); 
    } 
};
1
2
3
4
5
6
7
Listing 7.15. A (broken) Ørst attempt at revising push()
1
2
3
4
5
6
7
8
9
10
11
12
void push(T new_value) 
{ 
    std::unique_ptr<T> new_data(new T(new_value)); 
    counted_node_ptr new_next; 
    new_next.ptr=new node; 
    new_next.external_count=1; 
    for(;;) 
    { 
        node* const old_tail=tail.load(); 
        T* old_data=nullptr; 
        if(old_tail->data.compare_exchange_strong( 
            old_data,new_data.get())) 
1
2

13
14
15
16
17
18
19
20
        { 
            old_tail->next=new_next; 
            tail.store(new_next.ptr); 
            new_data.release(); 
            break; 
        } 
    } 
}
3
Listing 7.16. Implementing push()  for a lock-free queue with a reference-counted
tail
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
template<typename T> 
class lock_free_queue 
{ 
private: 
    struct node; 
    struct counted_node_ptr 
    { 
        int external_count; 
        node* ptr; 
    }; 
    std::atomic<counted_node_ptr> head; 
    std::atomic<counted_node_ptr> tail; 
    struct node_counter 
    { 
        unsigned internal_count:30; 
        unsigned external_counters:2; 
    }; 
    struct node 
    { 
        std::atomic<T*> data; 
        std::atomic<node_counter> count; 
        counted_node_ptr next; 
        node() 
        { 
            node_counter new_count; 
            new_count.internal_count=0; 
            new_count.external_counters=2; 
            count.store(new_count); 
 
            next.ptr=nullptr; 
            next.external_count=0; 
        } 
    }; 
public: 
    void push(T new_value) 
    { 
        std::unique_ptr<T> new_data(new T(new_value)); 
        counted_node_ptr new_next; 
        new_next.ptr=new node; 
        new_next.external_count=1; 
        counted_node_ptr old_tail=tail.load(); 
        for(;;) 
        { 
            increase_external_count(tail,old_tail); 
            T* old_data=nullptr; 
            if(old_tail.ptr->data.compare_exchange_strong( 
               old_data,new_data.get())) 
            { 
                old_tail.ptr->next=new_next; 
                old_tail=tail.exchange(new_next); 
1
2
3
4
5
6

51
52
53
54
55
56
57
58
                free_external_counter(old_tail); 
                new_data.release(); 
                break; 
            } 
            old_tail.ptr->release_ref(); 
        } 
    } 
};
7
Listing 7.17. Popping a node from a lock-free queue with a reference-counted tail
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
template<typename T> 
class lock_free_queue 
{ 
private: 
    struct node 
    { 
        void release_ref(); 
    }; 
public: 
    std::unique_ptr<T> pop() 
    { 
        counted_node_ptr old_head=head.load(std::memory_order_relaxed); 
        for(;;) 
        { 
            increase_external_count(head,old_head); 
            node* const ptr=old_head.ptr; 
            if(ptr==tail.load().ptr) 
            { 
                ptr->release_ref(); 
                return std::unique_ptr<T>(); 
            } 
            if(head.compare_exchange_strong(old_head,ptr->next)) 
            { 
                T* const res=ptr->data.exchange(nullptr); 
                free_external_counter(old_head); 
                return std::unique_ptr<T>(res); 
            } 
            ptr->release_ref(); 
        } 
    } 
};
1
2
3
4
5
6
Listing 7.18. Releasing a node reference in a lock-free queue
1
2
3
4
5
6
7
8
9
10
11
template<typename T> 
class lock_free_queue 
{ 
private: 
    struct node 
    { 
        void release_ref() 
        { 
            node_counter old_counter= 
                count.load(std::memory_order_relaxed); 
            node_counter new_counter; 

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
            do 
            { 
                new_counter=old_counter; 
                --new_counter.internal_count; 
            } 
            while(!count.compare_exchange_strong( 
                  old_counter,new_counter, 
                  std::memory_order_acquire,std::memory_order_relaxed)); 
            if(!new_counter.internal_count && 
               !new_counter.external_counters) 
            { 
                delete this; 
            } 
        } 
    }; 
};
1
2
3
Listing 7.19. Obtaining a new reference to a node in a lock-free queue
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
template<typename T> 
class lock_free_queue 
{ 
private: 
    static void increase_external_count( 
        std::atomic<counted_node_ptr>& counter, 
        counted_node_ptr& old_counter) 
    { 
        counted_node_ptr new_counter; 
        do 
        { 
            new_counter=old_counter; 
            ++new_counter.external_count; 
        } 
        while(!counter.compare_exchange_strong( 
              old_counter,new_counter, 
              std::memory_order_acquire,std::memory_order_relaxed)); 
        old_counter.external_count=new_counter.external_count; 
    } 
};
Listing 7.20. Freeing an external counter to a node in a lock-free queue
1
2
3
4
5
6
7
8
9
10
11
12
13
14
template<typename T> 
class lock_free_queue 
{ 
private: 
    static void free_external_counter(counted_node_ptr &old_node_ptr) 
    { 
        node* const ptr=old_node_ptr.ptr; 
        int const count_increase=old_node_ptr.external_count-2; 
        node_counter old_counter= 
            ptr->count.load(std::memory_order_relaxed); 
        node_counter new_counter; 
        do 
        { 
            new_counter=old_counter; 

15
16
17
18
19
20
21
22
23
24
25
26
27
            --new_counter.external_counters; 
            new_counter.internal_count+=count_increase; 
        } 
        while(!ptr->count.compare_exchange_strong( 
              old_counter,new_counter, 
              std::memory_order_acquire,std::memory_order_relaxed)); 
        if(!new_counter.internal_count && 
           !new_counter.external_counters) 
        { 
            delete ptr; 
        } 
    } 
};
1
2
3
4
Listing 7.21. pop()  modiØed to allow helping on the push()  side
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
template<typename T> 
class lock_free_queue 
{ 
private: 
    struct node 
    { 
        std::atomic<T*> data; 
        std::atomic<node_counter> count; 
        std::atomic<counted_node_ptr> next; 
    }; 
public: 
    std::unique_ptr<T> pop() 
    { 
        counted_node_ptr old_head=head.load(std::memory_order_relaxed); 
        for(;;) 
        { 
            increase_external_count(head,old_head); 
            node* const ptr=old_head.ptr; 
            if(ptr==tail.load().ptr) 
            { 
                return std::unique_ptr<T>(); 
            } 
            counted_node_ptr next=ptr->next.load(); 
            if(head.compare_exchange_strong(old_head,next)) 
            { 
                T* const res=ptr->data.exchange(nullptr); 
                free_external_counter(old_head); 
                return std::unique_ptr<T>(res); 
            } 
            ptr->release_ref(); 
        } 
    } 
};
1
2
Listing 7.22. A sample push()  with helping for a lock-free queue
1
2
3
4
template<typename T> 
class lock_free_queue 
{ 
private: 

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
    void set_new_tail(counted_node_ptr &old_tail, 
                      counted_node_ptr const &new_tail) 
    { 
        node* const current_tail_ptr=old_tail.ptr; 
        while(!tail.compare_exchange_weak(old_tail,new_tail) && 
              old_tail.ptr==current_tail_ptr); 
        if(old_tail.ptr==current_tail_ptr) 
            free_external_counter(old_tail); 
        else 
            current_tail_ptr->release_ref(); 
    } 
public: 
    void push(T new_value) 
    { 
        std::unique_ptr<T> new_data(new T(new_value)); 
        counted_node_ptr new_next; 
        new_next.ptr=new node; 
        new_next.external_count=1; 
        counted_node_ptr old_tail=tail.load(); 
        for(;;) 
        { 
            increase_external_count(tail,old_tail); 
            T* old_data=nullptr; 
            if(old_tail.ptr->data.compare_exchange_strong( 
                   old_data,new_data.get())) 
            { 
                counted_node_ptr old_next={0}; 
                if(!old_tail.ptr->next.compare_exchange_strong( 
                       old_next,new_next)) 
                { 
                    delete new_next.ptr; 
                    new_next=old_next; 
                } 
                set_new_tail(old_tail, new_next); 
                new_data.release(); 
                break; 
            } 
            else 
            { 
                counted_node_ptr old_next={0}; 
                if(old_tail.ptr->next.compare_exchange_strong( 
                       old_next,new_next)) 
                { 
                    old_next=new_next; 
                    new_next.ptr=new node; 
                } 
                set_new_tail(old_tail, old_next); 
            } 
        } 
    } 
};
1
2
3
4
5
6
7
8
9
10
11
12
13
14

CHAPTER 8
Figure 8.1. Distributing consecutive chunks of 
 between 
data
threads
Figure 8.2. Recursively 
 
dividingdata
Listing 8.1. Parallel Quicksort using a stack of pending chunks to sort
1
2
3
4
5
6
template<typename T> 
struct sorter 
{ 
    struct chunk_to_sort 
    { 
        std::list<T> data; 
1

7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
        std::promise<std::list<T> > promise; 
    }; 
    thread_safe_stack<chunk_to_sort> chunks; 
    std::vector<std::thread> threads; 
    unsigned const max_thread_count; 
    std::atomic<bool> end_of_data; 
    sorter(): 
        max_thread_count(std::thread::hardware_concurrency()-1), 
        end_of_data(false) 
    {} 
    ~sorter() 
    { 
        end_of_data=true; 
        for(unsigned i=0;i<threads.size();++i) 
        { 
            threads[i].join(); 
        } 
    } 
    void try_sort_chunk() 
    { 
        boost::shared_ptr<chunk_to_sort > chunk=chunks.pop(); 
        if(chunk) 
        { 
            sort_chunk(chunk); 
        } 
    } 
    std::list<T> do_sort(std::list<T>& chunk_data) 
    { 
        if(chunk_data.empty()) 
        { 
            return chunk_data; 
        } 
        std::list<T> result; 
        result.splice(result.begin(),chunk_data,chunk_data.begin()); 
        T const& partition_val=*result.begin(); 
        typename std::list<T>::iterator divide_point= 
            std::partition(chunk_data.begin(),chunk_data.end(), 
                           [&](T const& val){return val<partition_val;}); 
        chunk_to_sort new_lower_chunk; 
        new_lower_chunk.data.splice(new_lower_chunk.data.end(), 
                                    chunk_data,chunk_data.begin(), 
                                    divide_point); 
        std::future<std::list<T> > new_lower= 
            new_lower_chunk.promise.get_future(); 
        chunks.push(std::move(new_lower_chunk)); 
        if(threads.size()<max_thread_count) 
        { 
            threads.push_back(std::thread(&sorter<T>::sort_thread,this)); 
        } 
        std::list<T> new_higher(do_sort(chunk_data)); 
        result.splice(result.end(),new_higher); 
        while(new_lower.wait_for(std::chrono::seconds(0)) != 
              std::future_status::ready) 
        { 
            try_sort_chunk(); 
        } 
        result.splice(result.begin(),new_lower.get()); 
        return result; 
    } 
    void sort_chunk(boost::shared_ptr<chunk_to_sort > const& chunk) 
    { 
        chunk->promise.set_value(do_sort(chunk->data)); 
    } 
    void sort_thread() 
    { 
        while(!end_of_data) 
        { 
            try_sort_chunk(); 
            std::this_thread::yield(); 
        } 
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Figure 8.3. Matrix multiplication
77
78
79
80
81
82
83
84
85
86
87
88
    } 
}; 
template<typename T> 
std::list<T> parallel_quick_sort(std::list<T> input) 
{ 
    if(input.empty()) 
    { 
        return input; 
    } 
    sorter<T> s; 
    return s.do_sort(input); 
}
19
20
1
2
3
4
5
6
7
8
std::atomic<unsigned long> counter(0); 
void processing_loop() 
{ 
    while(counter.fetch_add(1,std::memory_order_relaxed)<100000000) 
    { 
        do_something(); 
    } 
}
1
2
3
4
5
6
7
8
9
10
std::mutex m; 
my_data data; 
void processing_loop_with_mutex() 
{ 
    while(true) 
    { 
        std::lock_guard<std::mutex> lk(m); 
        if(done_processing(data)) break; 
    } 
}

1
2
3
4
5
6
struct protected_data 
{ 
    std::mutex m; 
    char padding[std::hardware_destructive_interference_size]; 
    my_data data_to_protect; 
};
1
1
2
3
4
5
6
7
struct my_data 
{ 
    data_item1 d1; 
    data_item2 d2; 
    char padding[std::hardware_destructive_interference_size]; 
}; 
my_data some_array[256];
Listing 8.2. A naive parallel version of std::accumulate  (from listing 2.9)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
template<typename Iterator,typename T> 
struct accumulate_block 
{ 
    void operator()(Iterator first,Iterator last,T& result) 
    { 
        result=std::accumulate(first,last,result); 
    } 
}; 
template<typename Iterator,typename T> 
T parallel_accumulate(Iterator first,Iterator last,T init) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return init; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    std::vector<T> results(num_threads); 
    std::vector<std::thread>  threads(num_threads-1); 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        Iterator block_end=block_start; 
        std::advance(block_end,block_size); 
        threads[i]=std::thread( 
            accumulate_block<Iterator,T>(), 
            block_start,block_end,std::ref(results[i])); 
        block_start=block_end; 
    } 
    accumulate_block<Iterator,T>()( 
        block_start,last,results[num_threads-1]); 
    std::for_each(threads.begin(),threads.end(), 
        std::mem_fn(&std::thread::join)); 
1
2
3
4
5
6
7
8
9

39
40
    return std::accumulate(results.begin(),results.end(),init); 
}
10
Listing 8.3. A parallel version of std::accumulate  using std::packaged_task
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
template<typename Iterator,typename T> 
struct accumulate_block 
{ 
    T operator()(Iterator first,Iterator last) 
    { 
        return std::accumulate(first,last,T()); 
    } 
}; 
template<typename Iterator,typename T> 
T parallel_accumulate(Iterator first,Iterator last,T init) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return init; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    std::vector<std::future<T> > futures(num_threads-1); 
    std::vector<std::thread> threads(num_threads-1); 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        Iterator block_end=block_start; 
        std::advance(block_end,block_size); 
        std::packaged_task<T(Iterator,Iterator)> task( 
            accumulate_block<Iterator,T>()); 
        futures[i]=task.get_future(); 
        threads[i]=std::thread(std::move(task),block_start,block_end); 
        block_start=block_end; 
    } 
    T last_result=accumulate_block<Iterator,T>()(block_start,last); 
    std::for_each(threads.begin(),threads.end(), 
        std::mem_fn(&std::thread::join)); 
    T result=init; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        result+=futures[i].get(); 
    } 
    result += last_result; 
    return result; 
}
1
2
3
4
5
6
7
8
9
10
1
2
3
4
try 
{ 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
        // ... as before 
    } 
    T last_result=accumulate_block<Iterator,T>()(block_start,last); 
    std::for_each(threads.begin(),threads.end(), 
        std::mem_fn(&std::thread::join)); 
} 
catch(...) 
{ 
    for(unsigned long i=0;i<(num_thread-1);++i) 
    { 
        if(threads[i].joinable()) 
            thread[i].join(); 
    } 
    throw; 
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
class join_threads 
{ 
    std::vector<std::thread>& threads; 
public: 
    explicit join_threads(std::vector<std::thread>& threads_): 
        threads(threads_) 
    {} 
    ~join_threads() 
    { 
        for(unsigned long i=0;i<threads.size();++i) 
        { 
            if(threads[i].joinable()) 
                threads[i].join(); 
        } 
    } 
};
Listing 8.4. An exception-safe parallel version of std::accumulate
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
template<typename Iterator,typename T> 
T parallel_accumulate(Iterator first,Iterator last,T init) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return init; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    std::vector<std::future<T> > futures(num_threads-1); 
    std::vector<std::thread> threads(num_threads-1); 
    join_threads joiner(threads); 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        Iterator block_end=block_start; 
1

22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
        std::advance(block_end,block_size); 
        std::packaged_task<T(Iterator,Iterator)> task( 
            accumulate_block<Iterator,T>()); 
        futures[i]=task.get_future(); 
        threads[i]=std::thread(std::move(task),block_start,block_end); 
        block_start=block_end; 
    } 
    T last_result=accumulate_block<Iterator,T>()(block_start,last); 
    T result=init; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        result+=futures[i].get(); 
    } 
    result += last_result; 
    return result; 
}
2
Listing 8.5. An exception-safe parallel version of std::accumulate  using
std::async
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
template<typename Iterator,typename T> 
T parallel_accumulate(Iterator first,Iterator last,T init) 
{ 
    unsigned long const length=std::distance(first,last); 
    unsigned long const max_chunk_size=25; 
    if(length<=max_chunk_size) 
    { 
        return std::accumulate(first,last,init); 
    } 
    else 
    { 
    Iterator mid_point=first; 
        std::advance(mid_point,length/2); 
        std::future<T> first_half_result= 
            std::async(parallel_accumulate<Iterator,T>, 
                       first,mid_point,init); 
        T second_half_result=parallel_accumulate(mid_point,last,T()); 
        return first_half_result.get()+second_half_result; 
    } 
}
1
2
3
4
5
6
1
2
3
4
5
while(true) 
{ 
    event_data event=get_event(); 
    if(event.type==quit) 
        break; 

6
7
    process(event); 
}
Listing 8.6. Separating GUI thread from task thread
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
std::thread task_thread; 
std::atomic<bool> task_cancelled(false); 
void gui_thread() 
{ 
    while(true) 
    { 
        event_data event=get_event(); 
        if(event.type==quit) 
            break; 
        process(event); 
    } 
} 
void task() 
{ 
    while(!task_complete() && !task_cancelled) 
    { 
        do_next_operation(); 
    } 
    if(task_cancelled) 
    { 
        perform_cleanup(); 
    } 
    else 
    { 
        post_gui_event(task_complete); 
    } 
} 
void process(event_data const& event) 
{ 
    switch(event.type) 
    { 
    case start_task: 
        task_cancelled=false; 
        task_thread=std::thread(task); 
        break; 
    case stop_task: 
        task_cancelled=true; 
        task_thread.join(); 
        break; 
    case task_complete: 
        task_thread.join(); 
        display_results(); 
        break; 
    default: 
        //... 
    } 
}
Listing 8.7. A parallel version of std::for_each

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
template<typename Iterator,typename Func> 
void parallel_for_each(Iterator first,Iterator last,Func f) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    std::vector<std::future<void> > futures(num_threads-1); 
    std::vector<std::thread> threads(num_threads-1); 
    join_threads joiner(threads); 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        Iterator block_end=block_start; 
        std::advance(block_end,block_size); 
        std::packaged_task<void(void)> task( 
            [=]() 
            { 
                std::for_each(block_start,block_end,f); 
            }); 
        futures[i]=task.get_future(); 
        threads[i]=std::thread(std::move(task)); 
        block_start=block_end; 
    } 
    std::for_each(block_start,last,f); 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        futures[i].get(); 
    } 
}
1
2
3
4
Listing 8.8. A parallel version of std::for_each  using std::async
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
template<typename Iterator,typename Func> 
void parallel_for_each(Iterator first,Iterator last,Func f) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return; 
    unsigned long const min_per_thread=25; 
    if(length<(2*min_per_thread)) 
    { 
        std::for_each(first,last,f); 
    } 
    else 
    { 
        Iterator const mid_point=first+length/2; 
        std::future<void> first_half= 
            std::async(&parallel_for_each<Iterator,Func>, 
                       first,mid_point,f); 
        parallel_for_each(mid_point,last,f); 
        first_half.get(); 
    } 
}
1
2
3
4

Listing 8.9. An implementation of a parallel Ønd algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
template<typename Iterator,typename MatchType> 
Iterator parallel_find(Iterator first,Iterator last,MatchType match) 
{ 
    struct find_element 
    { 
        void operator()(Iterator begin,Iterator end, 
                        MatchType match, 
                        std::promise<Iterator>* result, 
                        std::atomic<bool>* done_flag) 
        { 
            try 
            { 
                for(;(begin!=end) && !done_flag->load();++begin) 
                { 
                    if(*begin==match) 
                    { 
                        result->set_value(begin); 
                        done_flag->store(true); 
                        return; 
                    } 
                } 
            } 
            catch(...) 
            { 
                try 
                { 
                    result->set_exception(std::current_exception()); 
                    done_flag->store(true); 
                } 
                catch(...) 
                {} 
            } 
        } 
    }; 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return last; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    std::promise<Iterator> result; 
    std::atomic<bool> done_flag(false); 
    std::vector<std::thread> threads(num_threads-1); 
    { 
        join_threads joiner(threads); 
        Iterator block_start=first; 
        for(unsigned long i=0;i<(num_threads-1);++i) 
        { 
            Iterator block_end=block_start; 
            std::advance(block_end,block_size); 
            threads[i]=std::thread(find_element(), 
                                   block_start,block_end,match, 
                                   &result,&done_flag); 
            block_start=block_end; 
        } 
1
2
3
4
5
6
7
8
9
10
11

61
62
63
64
65
66
67
68
        find_element()(block_start,last,match,&result,&done_flag); 
    } 
    if(!done_flag.load()) 
    { 
        return last; 
    } 
    return result.get_future().get(); 
}
12
13
14
Listing 8.10. An implementation of a parallel Ønd algorithm using std::async
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
template<typename Iterator,typename MatchType> 
Iterator parallel_find_impl(Iterator first,Iterator last,MatchType match, 
                            std::atomic<bool>& done) 
{ 
    try 
    { 
        unsigned long const length=std::distance(first,last); 
        unsigned long const min_per_thread=25; 
        if(length<(2*min_per_thread)) 
        { 
            for(;(first!=last) && !done.load();++first) 
            { 
                if(*first==match) 
                { 
                    done=true; 
                    return first; 
                } 
            } 
            return last; 
        } 
        else 
        { 
            Iterator const mid_point=first+(length/2); 
            std::future<Iterator> async_result= 
                std::async(&parallel_find_impl<Iterator,MatchType>, 
                           mid_point,last,match,std::ref(done)); 
            Iterator const direct_result= 
                    parallel_find_impl(first,mid_point,match,done); 
            return (direct_result==mid_point)? 
                async_result.get():direct_result; 
        } 
    } 
    catch(...) 
    { 
        done=true; 
        throw; 
    } 
} 
template<typename Iterator,typename MatchType> 
Iterator parallel_find(Iterator first,Iterator last,MatchType match) 
{ 
    std::atomic<bool> done(false); 
    return parallel_find_impl(first,last,match,done); 
}
1
2
3
4
5
6
7
8
9
10
11
12

Listing 8.11. Calculating partial sums in parallel by dividing the problem
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
template<typename Iterator> 
void parallel_partial_sum(Iterator first,Iterator last) 
{ 
    typedef typename Iterator::value_type value_type; 
 
    struct process_chunk 
    { 
        void operator()(Iterator begin,Iterator last, 
                        std::future<value_type>* previous_end_value, 
                        std::promise<value_type>* end_value) 
        { 
            try 
            { 
                Iterator end=last; 
                ++end; 
                std::partial_sum(begin,end,begin); 
                if(previous_end_value) 
                { 
                    value_type& addend=previous_end_value->get(); 
                    *last+=addend; 
                    if(end_value) 
                    { 
                        end_value->set_value(*last); 
                    } 
                    std::for_each(begin,last,[addend](value_type& item) 
                                  { 
                                      item+=addend; 
                                  }); 
                } 
                else if(end_value) 
                { 
                    end_value->set_value(*last); 
                } 
            } 
            catch(...) 
            { 
                if(end_value) 
                { 
                    end_value->set_exception(std::current_exception()); 
                } 
                else 
                { 
                    throw; 
                } 
            } 
        } 
    }; 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return; 
    unsigned long const min_per_thread=25; 
    unsigned long const max_threads= 
        (length+min_per_thread-1)/min_per_thread; 
    unsigned long const hardware_threads= 
        std::thread::hardware_concurrency(); 
    unsigned long const num_threads= 
        std::min(hardware_threads!=0?hardware_threads:2,max_threads); 
    unsigned long const block_size=length/num_threads; 
    typedef typename Iterator::value_type value_type; 
    std::vector<std::thread> threads(num_threads-1); 
    std::vector<std::promise<value_type> > 
         end_values(num_threads-1); 
    std::vector<std::future<value_type> > 
         previous_end_values; 
    previous_end_values.reserve(num_threads-1); 
    join_threads joiner(threads); 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_threads-1);++i) 
    { 
        Iterator block_last=block_start; 
        std::advance(block_last,block_size-1); 
        threads[i]=std::thread(process_chunk(), 
                               block_start,block_last, 
                               (i!=0)?&previous_end_values[i-1]:0, 
                               &end_values[i]); 
        block_start=block_last; 
        ++block_start; 
        previous_end_values.push_back(end_values[i].get_future()); 
    } 
    Iterator final_element=block_start; 
    std::advance(final_element,std::distance(block_start,last)-1); 
    process_chunk()(block_start,final_element, 
                    (num_threads>1)?&previous_end_values.back():0, 
                    0); 
}
17
18
19
20
21
22
Listing 8.12. A simple barrier class
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
class barrier 
{ 
    unsigned const count; 
    std::atomic<unsigned> spaces; 
    std::atomic<unsigned> generation; 
public: 
    explicit barrier(unsigned count_): 
        count(count_),spaces(count),generation(0) 
    {} 
    void wait() 
    { 
        unsigned const my_generation=generation; 
        if(!--spaces) 
        { 
            spaces=count; 
            ++generation; 
        } 
        else 
        { 
            while(generation==my_generation) 
                std::this_thread::yield(); 
        } 
    } 
};
1
2
3
4
5
6
7
1 std::atomic<unsigned> count;

1 spaces=count.load();
1
2
3
4
5
6
7
8
9
void done_waiting() 
{ 
    --count; 
    if(!--spaces) 
    { 
        spaces=count.load(); 
        ++generation; 
    } 
}
1
2
3
Listing 8.13. A parallel implementation of partial_sum  by pairwise updates
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
struct barrier 
{ 
    std::atomic<unsigned> count; 
    std::atomic<unsigned> spaces; 
    std::atomic<unsigned> generation; 
    barrier(unsigned count_): 
        count(count_),spaces(count_),generation(0) 
    {} 
    void wait() 
    { 
        unsigned const gen=generation.load(); 
        if(!--spaces) 
        { 
            spaces=count.load(); 
            ++generation; 
        } 
        else 
        { 
            while(generation.load()==gen) 
            { 
                std::this_thread::yield(); 
            } 
        } 
    } 
    void done_waiting() 
    { 
        --count; 
        if(!--spaces) 
        { 
            spaces=count.load(); 
            ++generation; 
        } 
    } 
}; 
template<typename Iterator> 
void parallel_partial_sum(Iterator first,Iterator last) 
{ 
    typedef typename Iterator::value_type value_type; 
    struct process_element 
    { 
        void operator()(Iterator first,Iterator last, 
1

42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
                        std::vector<value_type>& buffer, 
                        unsigned i,barrier& b) 
        { 
            value_type& ith_element=*(first+i); 
            bool update_source=false; 
 
            for(unsigned step=0,stride=1;stride<=i;++step,stride*=2) 
            { 
                value_type const& source=(step%2)? 
                    buffer[i]:ith_element; 
                value_type& dest=(step%2)? 
                    ith_element:buffer[i]; 
                value_type const& addend=(step%2)? 
                    buffer[i-stride]:*(first+i-stride); 
                dest=source+addend; 
                update_source=!(step%2); 
                b.wait(); 
            } 
            if(update_source) 
            { 
                ith_element=buffer[i]; 
            } 
            b.done_waiting(); 
        } 
    }; 
    unsigned long const length=std::distance(first,last); 
    if(length<=1) 
        return; 
    std::vector<value_type> buffer(length); 
    barrier b(length); 
    std::vector<std::thread> threads(length-1); 
    join_threads joiner(threads); 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(length-1);++i) 
    { 
        threads[i]=std::thread(process_element(),first,last, 
                               std::ref(buffer),i,std::ref(b)); 
    } 
    process_element()(first,last,buffer,length-1,b); 
}
2
3
4
5
6
7
8
9
10

CHAPTER 9
Listing 9.1. Simple thread pool
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
class thread_pool 
{ 
    std::atomic_bool done; 
    threadsafe_queue<std::function<void()> > work_queue; 
    std::vector<std::thread> threads; 
    join_threads joiner; 
    void worker_thread() 
    { 
        while(!done) 
        { 
            std::function<void()> task; 
            if(work_queue.try_pop(task)) 
            { 
                task(); 
            } 
            else 
            { 
                std::this_thread::yield(); 
            } 
        } 
    } 
public: 
    thread_pool(): 
        done(false),joiner(threads) 
    { 
        unsigned const thread_count=std::thread::hardware_concurrency(); 
        try 
        { 
            for(unsigned i=0;i<thread_count;++i) 
            { 
                threads.push_back( 
                    std::thread(&thread_pool::worker_thread,this)); 
            } 
        } 
        catch(...) 
        { 
            done=true; 
            throw; 
        } 
    } 
    ~thread_pool() 
    { 
        done=true; 
    } 
    template<typename FunctionType> 
    void submit(FunctionType f) 
    { 
        work_queue.push(std::function<void()>(f)); 
    } 
};
1
2
3
4
5
6
7
8
9
10
11
12

Listing 9.2. A thread pool with waitable tasks
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
class function_wrapper 
{ 
    struct impl_base { 
        virtual void call()=0; 
        virtual ~impl_base() {} 
    }; 
    std::unique_ptr<impl_base> impl; 
    template<typename F> 
    struct impl_type: impl_base 
    { 
        F f; 
        impl_type(F&& f_): f(std::move(f_)) {} 
        void call() { f(); } 
    }; 
public: 
    template<typename F> 
    function_wrapper(F&& f): 
        impl(new impl_type<F>(std::move(f))) 
    {} 
    void operator()() { impl->call(); } 
    function_wrapper() = default; 
    function_wrapper(function_wrapper&& other): 
        impl(std::move(other.impl)) 
    {} 
    function_wrapper& operator=(function_wrapper&& other) 
    { 
        impl=std::move(other.impl); 
        return *this; 
    } 
    function_wrapper(const function_wrapper&)=delete; 
    function_wrapper(function_wrapper&)=delete; 
    function_wrapper& operator=(const function_wrapper&)=delete; 
}; 
class thread_pool 
{ 
    thread_safe_queue<function_wrapper> work_queue; 
    void worker_thread() 
    { 
        while(!done) 
        { 
            function_wrapper task; 
            if(work_queue.try_pop(task)) 
            { 
                task(); 
            } 
            else 
            { 
                std::this_thread::yield(); 
            } 
        } 
    } 
public: 
    template<typename FunctionType> 
    std::future<typename std::result_of<FunctionType()>::type> 
        submit(FunctionType f) 
    { 
        typedef typename std::result_of<FunctionType()>::type 
            result_type; 
        std::packaged_task<result_type()> task(std::move(f)); 
        std::future<result_type> res(task.get_future()); 
        work_queue.push(std::move(task)); 
        return res; 
    } 
    // rest as before 
};
1
2
3
4
5
6
7

Listing 9.3. parallel_accumulate  using a thread pool with waitable tasks
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
template<typename Iterator,typename T> 
T parallel_accumulate(Iterator first,Iterator last,T init) 
{ 
    unsigned long const length=std::distance(first,last); 
    if(!length) 
        return init; 
    unsigned long const block_size=25; 
    unsigned long const num_blocks=(length+block_size-1)/block_size; 
    std::vector<std::future<T> > futures(num_blocks-1); 
    thread_pool pool; 
    Iterator block_start=first; 
    for(unsigned long i=0;i<(num_blocks-1);++i) 
    { 
        Iterator block_end=block_start; 
        std::advance(block_end,block_size); 
        futures[i]=pool.submit([=]{ 
            accumulate_block<Iterator,T>()(block_start,block_end); 
        }); 
        block_start=block_end; 
    } 
    T last_result=accumulate_block<Iterator,T>()(block_start,last); 
    T result=init; 
    for(unsigned long i=0;i<(num_blocks-1);++i) 
    { 
        result+=futures[i].get(); 
    } 
    result += last_result; 
    return result; 
}
1
2
Listing 9.4. An implementation of run_pending_task()
1
2
3
4
5
6
7
8
9
10
11
12
void thread_pool::run_pending_task() 
{ 
    function_wrapper task; 
    if(work_queue.try_pop(task)) 
    { 
        task(); 
    } 
    else 
    { 
        std::this_thread::yield(); 
    } 
}
Listing 9.5. A thread-pool–based implementation of Quicksort

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
template<typename T> 
struct sorter 
{ 
    thread_pool pool; 
 
    std::list<T> do_sort(std::list<T>& chunk_data) 
    { 
        if(chunk_data.empty()) 
        { 
            return chunk_data; 
        } 
        std::list<T> result; 
        result.splice(result.begin(),chunk_data,chunk_data.begin()); 
        T const& partition_val=*result.begin(); 
        typename std::list<T>::iterator divide_point= 
            std::partition(chunk_data.begin(),chunk_data.end(), 
                           [&](T const& val){return val<partition_val;}); 
        std::list<T> new_lower_chunk; 
        new_lower_chunk.splice(new_lower_chunk.end(), 
                               chunk_data,chunk_data.begin(), 
                               divide_point); 
        std::future<std::list<T> > new_lower= 
            pool.submit(std::bind(&sorter::do_sort,this, 
                                  std::move(new_lower_chunk))); 
        std::list<T> new_higher(do_sort(chunk_data)); 
        result.splice(result.end(),new_higher); 
        while(new_lower.wait_for(std::chrono::seconds(0)) == 
            std::future_status::timeout) 
        { 
            pool.run_pending_task(); 
        } 
        result.splice(result.begin(),new_lower.get()); 
        return result; 
    } 
}; 
template<typename T> 
std::list<T> parallel_quick_sort(std::list<T> input) 
{ 
    if(input.empty()) 
    { 
        return input; 
    } 
    sorter<T> s; 
    return s.do_sort(input); 
}
1
2
3
4
Listing 9.6. A thread pool with thread-local work queues
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
class thread_pool 
{ 
    threadsafe_queue<function_wrapper> pool_work_queue; 
    typedef std::queue<function_wrapper> local_queue_type; 
    static thread_local std::unique_ptr<local_queue_type> 
        local_work_queue; 
    void worker_thread() 
    { 
        local_work_queue.reset(new local_queue_type); 
 
        while(!done) 
        { 
            run_pending_task(); 
        } 
    } 
1
2
3

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
public: 
    template<typename FunctionType> 
    std::future<typename std::result_of<FunctionType()>::type> 
        submit(FunctionType f) 
    { 
        typedef typename std::result_of<FunctionType()>::type result_type; 
        std::packaged_task<result_type()> task(f); 
        std::future<result_type> res(task.get_future()); 
        if(local_work_queue) 
        { 
            local_work_queue->push(std::move(task)); 
        } 
        else 
        { 
            pool_work_queue.push(std::move(task)); 
        } 
        return res; 
    } 
    void run_pending_task() 
    { 
        function_wrapper task; 
        if(local_work_queue && !local_work_queue->empty()) 
        { 
            task=std::move(local_work_queue->front()); 
            local_work_queue->pop(); 
            task(); 
        } 
        else if(pool_work_queue.try_pop(task)) 
        { 
            task(); 
        } 
        else 
        { 
            std::this_thread::yield(); 
        } 
    } 
    // rest as before 
};
4
5
6
7
Listing 9.7. Lock-based queue for work stealing
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
class work_stealing_queue 
{ 
private: 
    typedef function_wrapper data_type; 
    std::deque<data_type> the_queue; 
    mutable std::mutex the_mutex; 
public: 
    work_stealing_queue() 
    {} 
    work_stealing_queue(const work_stealing_queue& other)=delete; 
    work_stealing_queue& operator=( 
        const work_stealing_queue& other)=delete; 
    void push(data_type data) 
    { 
        std::lock_guard<std::mutex> lock(the_mutex); 
        the_queue.push_front(std::move(data)); 
    } 
    bool empty() const 
    { 
        std::lock_guard<std::mutex> lock(the_mutex); 
        return the_queue.empty(); 
    } 
1
2

23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
    bool try_pop(data_type& res) 
    { 
        std::lock_guard<std::mutex> lock(the_mutex); 
        if(the_queue.empty()) 
        { 
            return false; 
        } 
        res=std::move(the_queue.front()); 
        the_queue.pop_front(); 
        return true; 
    } 
    bool try_steal(data_type& res) 
    { 
        std::lock_guard<std::mutex> lock(the_mutex); 
        if(the_queue.empty()) 
        { 
            return false; 
        } 
        res=std::move(the_queue.back()); 
        the_queue.pop_back(); 
        return true; 
    } 
};
3
4
Listing 9.8. A thread pool that uses work stealing
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
class thread_pool 
{ 
    typedef function_wrapper task_type; 
    std::atomic_bool done; 
    threadsafe_queue<task_type> pool_work_queue; 
    std::vector<std::unique_ptr<work_stealing_queue> > queues; 
    std::vector<std::thread> threads; 
    join_threads joiner; 
    static thread_local work_stealing_queue* local_work_queue; 
    static thread_local unsigned my_index; 
    void worker_thread(unsigned my_index_) 
    { 
        my_index=my_index_; 
        local_work_queue=queues[my_index].get(); 
        while(!done) 
        { 
            run_pending_task(); 
        } 
    } 
    bool pop_task_from_local_queue(task_type& task) 
    { 
        return local_work_queue && local_work_queue->try_pop(task); 
    } 
    bool pop_task_from_pool_queue(task_type& task) 
    { 
        return pool_work_queue.try_pop(task); 
    } 
    bool pop_task_from_other_thread_queue(task_type& task) 
    { 
        for(unsigned i=0;i<queues.size();++i) 
        { 
            unsigned const index=(my_index+i+1)%queues.size(); 
            if(queues[index]->try_steal(task)) 
            { 
                return true; 
            } 
        } 
1
2
3
4
5

38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
        return false; 
    } 
public: 
    thread_pool(): 
        done(false),joiner(threads) 
    { 
        unsigned const thread_count=std::thread::hardware_concurrency(); 
        try 
        { 
            for(unsigned i=0;i<thread_count;++i) 
            { 
                queues.push_back(std::unique_ptr<work_stealing_queue>( 
                                     new work_stealing_queue)); 
            } 
            for(unsigned i=0;i<thread_count;++i) 
            { 
                threads.push_back( 
                    std::thread(&thread_pool::worker_thread,this,i)); 
            } 
        } 
        catch(...) 
        { 
            done=true; 
            throw; 
        } 
    } 
    ~thread_pool() 
    { 
        done=true; 
    } 
    template<typename FunctionType> 
    std::future<typename std::result_of<FunctionType()>::type> submit( 
        FunctionType f) 
    { 
        typedef typename std::result_of<FunctionType()>::type result_type; 
        std::packaged_task<result_type()> task(f); 
        std::future<result_type> res(task.get_future()); 
        if(local_work_queue) 
        { 
            local_work_queue->push(std::move(task)); 
        } 
        else 
        { 
            pool_work_queue.push(std::move(task)); 
        } 
        return res; 
    } 
    void run_pending_task() 
    { 
        task_type task; 
        if(pop_task_from_local_queue(task) || 
           pop_task_from_pool_queue(task) || 
           pop_task_from_other_thread_queue(task)) 
        { 
            task(); 
        } 
        else 
        { 
            std::this_thread::yield(); 
        } 
    } 
};
6
7
8
9

1
2
3
4
5
6
7
8
9
10
class interruptible_thread 
{ 
public: 
    template<typename FunctionType> 
    interruptible_thread(FunctionType f); 
    void join(); 
    void detach(); 
    bool joinable() const; 
    void interrupt(); 
};
Listing 9.9. Basic implementation of interruptible_thread
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
class interrupt_flag 
{ 
public: 
    void set(); 
    bool is_set() const; 
}; 
thread_local interrupt_flag this_thread_interrupt_flag; 
class interruptible_thread 
{ 
    std::thread internal_thread; 
    interrupt_flag* flag; 
public: 
    template<typename FunctionType> 
    interruptible_thread(FunctionType f) 
    { 
        std::promise<interrupt_flag*> p; 
        internal_thread=std::thread([f,&p]{ 
                p.set_value(&this_thread_interrupt_flag); 
                f(); 
            }); 
        flag=p.get_future().get(); 
    } 
    void interrupt() 
    { 
        if(flag) 
        { 
            flag->set(); 
        } 
    } 
};
1
2
3
4
5
6
1
2
3
4
5
6
7
void interruption_point() 
{ 
    if(this_thread_interrupt_flag.is_set()) 
    { 
        throw thread_interrupted(); 
    } 
}

1
2
3
4
5
6
7
8
void foo() 
{ 
    while(!done) 
    { 
        interruption_point(); 
        process_next_item(); 
    } 
}
Listing 9.10. A broken version of interruptible_wait  for
std::condition_variable
1
2
3
4
5
6
7
8
9
void interruptible_wait(std::condition_variable& cv, 
                        std::unique_lock<std::mutex>& lk) 
{ 
    interruption_point(); 
    this_thread_interrupt_flag.set_condition_variable(cv); 
    cv.wait(lk); 
    this_thread_interrupt_flag.clear_condition_variable(); 
    interruption_point(); 
}
1
2
3
Listing 9.11. Using a timeout in interruptible_wait  for
std::condition_variable
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
class interrupt_flag 
{ 
    std::atomic<bool> flag; 
    std::condition_variable* thread_cond; 
    std::mutex set_clear_mutex; 
public: 
    interrupt_flag(): 
        thread_cond(0) 
    {} 
    void set() 
    { 
        flag.store(true,std::memory_order_relaxed); 
        std::lock_guard<std::mutex> lk(set_clear_mutex); 
        if(thread_cond) 
        { 
            thread_cond->notify_all(); 
        } 
    } 
    bool is_set() const 
    { 
        return flag.load(std::memory_order_relaxed); 
    } 
    void set_condition_variable(std::condition_variable& cv) 
    { 
        std::lock_guard<std::mutex> lk(set_clear_mutex); 
        thread_cond=&cv; 

27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
    } 
    void clear_condition_variable() 
    { 
        std::lock_guard<std::mutex> lk(set_clear_mutex); 
        thread_cond=0; 
    } 
    struct clear_cv_on_destruct 
    { 
        ~clear_cv_on_destruct() 
        { 
            this_thread_interrupt_flag.clear_condition_variable(); 
        } 
    }; 
}; 
void interruptible_wait(std::condition_variable& cv, 
                        std::unique_lock<std::mutex>& lk) 
{ 
    interruption_point(); 
    this_thread_interrupt_flag.set_condition_variable(cv); 
    interrupt_flag::clear_cv_on_destruct guard; 
    interruption_point(); 
    cv.wait_for(lk,std::chrono::milliseconds(1)); 
    interruption_point(); 
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
template<typename Predicate> 
void interruptible_wait(std::condition_variable& cv, 
                        std::unique_lock<std::mutex>& lk, 
                        Predicate pred) 
{ 
    interruption_point(); 
    this_thread_interrupt_flag.set_condition_variable(cv); 
    interrupt_flag::clear_cv_on_destruct guard; 
    while(!this_thread_interrupt_flag.is_set() && !pred()) 
    { 
        cv.wait_for(lk,std::chrono::milliseconds(1)); 
    } 
    interruption_point(); 
}
Listing 9.12. interruptible_wait  for std::condition_variable_any
1
2
3
4
5
6
7
8
9
10
11
12
13
14
class interrupt_flag 
{ 
    std::atomic<bool> flag; 
    std::condition_variable* thread_cond; 
    std::condition_variable_any* thread_cond_any; 
    std::mutex set_clear_mutex; 
public: 
    interrupt_flag(): 
        thread_cond(0),thread_cond_any(0) 
    {} 
    void set() 
    { 
        flag.store(true,std::memory_order_relaxed); 
        std::lock_guard<std::mutex> lk(set_clear_mutex); 

15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
        if(thread_cond) 
        { 
            thread_cond->notify_all(); 
        } 
        else if(thread_cond_any) 
        { 
            thread_cond_any->notify_all(); 
        } 
    } 
    template<typename Lockable> 
    void wait(std::condition_variable_any& cv,Lockable& lk) 
    { 
        struct custom_lock 
        { 
            interrupt_flag* self; 
            Lockable& lk; 
            custom_lock(interrupt_flag* self_, 
                        std::condition_variable_any& cond, 
                        Lockable& lk_): 
                self(self_),lk(lk_) 
            { 
                self->set_clear_mutex.lock(); 
                self->thread_cond_any=&cond; 
            } 
            void unlock() 
            { 
                lk.unlock(); 
                self->set_clear_mutex.unlock(); 
            } 
            void lock() 
            { 
                std::lock(self->set_clear_mutex,lk); 
            } 
            ~custom_lock() 
            { 
                self->thread_cond_any=0; 
                self->set_clear_mutex.unlock(); 
            } 
        }; 
        custom_lock cl(this,cv,lk); 
        interruption_point(); 
        cv.wait(cl); 
        interruption_point(); 
    } 
    // rest as before 
}; 
template<typename Lockable> 
void interruptible_wait(std::condition_variable_any& cv, 
                        Lockable& lk) 
{ 
    this_thread_interrupt_flag.wait(cv,lk); 
}
1
2
3
4
5
1
2
3
4
5
6
7
8
9
10
template<typename T> 
void interruptible_wait(std::future<T>& uf) 
{ 
   while(!this_thread_interrupt_flag.is_set()) 
   { 
       if(uf.wait_for(lk,std::chrono::milliseconds(1))== 
           std::future_status::ready) 
           break; 
   } 

11    interruption_point(); 
}
1
2
3
4
5
6
7
8
try 
{ 
    do_something(); 
} 
catch(thread_interrupted&) 
{ 
    handle_interruption(); 
}
1
2
3
4
5
6
7
8
9
internal_thread=std::thread([f,&p]{ 
        p.set_value(&this_thread_interrupt_flag); 
        try 
        { 
            f(); 
        } 
        catch(thread_interrupted const&) 
        {} 
    });
Listing 9.13. Monitoring the Ølesystem in the background
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
std::mutex config_mutex; 
std::vector<interruptible_thread> background_threads; 
void background_thread(int disk_id) 
{ 
    while(true) 
    { 
        interruption_point(); 
        fs_change fsc=get_fs_changes(disk_id); 
        if(fsc.has_changes()) 
        { 
            update_index(fsc); 
        } 
    } 
} 
void start_background_processing() 
{ 
    background_threads.push_back( 
        interruptible_thread(background_thread,disk_1)); 
    background_threads.push_back( 
        interruptible_thread(background_thread,disk_2)); 
} 
int main() 
{ 
    start_background_processing(); 
    process_gui_until_exit(); 
1
2
3
4
5

26
27
28
29
30
31
32
33
34
35
    std::unique_lock<std::mutex> lk(config_mutex); 
    for(unsigned i=0;i<background_threads.size();++i) 
    { 
        background_threads[i].interrupt(); 
    } 
    for(unsigned i=0;i<background_threads.size();++i) 
    { 
        background_threads[i].join(); 
    } 
}
6
7

CHAPTER 10
1
2
std::vector<int> my_data; 
std::sort(std::execution::par,my_data.begin(),my_data.end());
1 std::for_each(v.begin(),v.end(),[](auto x){ throw my_exception(); });
1
2
3
std::for_each( 
    std::execution::seq,v.begin(),v.end(), 
    [](auto x){ throw my_exception(); });
1
2
3
4
std::vector<int> v(1000); 
int count=0; 
std::for_each(std::execution::seq,v.begin(),v.end(), 
    [&](int& x){ x=++count; });
1 std::for_each(std::execution::par,v.begin(),v.end(),[](auto& x){++x;});
1
2
std::for_each(std::execution::par,v.begin(),v.end(), 
    [&](int& x){ x=++count; });

1
2
3
4
5
6
template<class RandomAccessIterator> 
void sort(RandomAccessIterator first, RandomAccessIterator last); 
 
template<class RandomAccessIterator, class Compare> 
void sort( 
    RandomAccessIterator first, RandomAccessIterator last, Compare comp);
1
2
3
4
5
6
7
8
9
template<class ExecutionPolicy, class RandomAccessIterator> 
void sort( 
    ExecutionPolicy&& exec, 
    RandomAccessIterator first, RandomAccessIterator last); 
 
template<class ExecutionPolicy, class RandomAccessIterator, class Compare> 
void sort( 
    ExecutionPolicy&& exec, 
    RandomAccessIterator first, RandomAccessIterator last, Compare comp);
1
2
3
template<class InputIterator, class OutputIterator> 
OutputIterator copy( 
    InputIterator first, InputIterator last, OutputIterator result);
1
2
3
4
5
6
template<class ExecutionPolicy, 
    class ForwardIterator1, class ForwardIterator2> 
ForwardIterator2 copy( 
    ExecutionPolicy&& policy, 
    ForwardIterator1 first, ForwardIterator1 last, 
    ForwardIterator2 result);
1
2
3
4
#pragma omp parallel for 
for(unsigned i=0;i<v.size();++i){ 
    do_stuff(v[i]); 
}
1 std::for_each(std::execution::par,v.begin(),v.end(),do_stuff);

Listing 10.1. Parallel algorithms on a class with internal synchronization
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
class X{ 
    mutable std::mutex m; 
    int data; 
public: 
    X():data(0){} 
    int get_value() const{ 
        std::lock_guard guard(m); 
        return data; 
    } 
    void increment(){ 
        std::lock_guard guard(m); 
        ++data; 
    } 
}; 
void increment_all(std::vector<X>& v){ 
    std::for_each(std::execution::par,v.begin(),v.end(), 
        [](X& x){ 
            x.increment(); 
        }); 
}
Listing 10.2. Parallel algorithms on a class without internal synchronization
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
class Y{ 
    int data; 
public: 
    Y():data(0){} 
    int get_value() const{ 
        return data; 
    } 
    void increment(){ 
        ++data; 
    } 
}; 
class ProtectedY{ 
    std::mutex m; 
    std::vector<Y> v; 
public: 
   void lock(){ 
         m.lock(); 
     } 
   void unlock(){ 
         m.unlock(); 
     } 
     std::vector<Y>& get_vec(){ 
         return v; 
     } 
}; 
void increment_all(ProtectedY& data){ 
    std::lock_guard guard(data); 
    auto& v=data.get_vec(); 
    std::for_each(std::execution::par_unseq,v.begin(),v.end(), 
        [](Y& y){ 

31
32
33
            y.increment(); 
        }); 
}
Listing 10.3. Using transform_reduce  to count visits to pages of a website
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
#include <vector> 
#include <string> 
#include <unordered_map> 
#include <numeric> 
 
struct log_info { 
    std::string page; 
    time_t visit_time; 
    std::string browser; 
    // any other fields 
}; 
 
extern log_info parse_log_line(std::string const &line); 
 
using visit_map_type= std::unordered_map<std::string, unsigned long long>; 
 
visit_map_type 
count_visits_per_page(std::vector<std::string> const &log_lines) { 
 
    struct combine_visits { 
        visit_map_type 
        operator()(visit_map_type lhs, visit_map_type rhs) const { 
            if(lhs.size() < rhs.size()) 
                std::swap(lhs, rhs); 
            for(auto const &entry : rhs) { 
                lhs[entry.first]+= entry.second; 
            } 
            return lhs; 
        } 
 
        visit_map_type operator()(log_info log,visit_map_type map) const{ 
            ++map[log.page]; 
            return map; 
        } 
        visit_map_type operator()(visit_map_type map,log_info log) const{ 
            ++map[log.page]; 
            return map; 
        } 
        visit_map_type operator()(log_info log1,log_info log2) const{ 
            visit_map_type map; 
            ++map[log1.page]; 
            ++map[log2.page]; 
            return map; 
        } 
    }; 
 
    return std::transform_reduce( 
        std::execution::par, log_lines.begin(), log_lines.end(), 
        visit_map_type(), combine_visits(), parse_log_line); 
}
1
2
3
4
5
6


CHAPTER 11
Listing 11.1. An example test for concurrent push()  and pop()  calls on a queue
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
void test_concurrent_push_and_pop_on_empty_queue() 
{ 
    threadsafe_queue<int> q; 
    std::promise<void> go,push_ready,pop_ready; 
    std::shared_future<void> ready(go.get_future()); 
    std::future<void> push_done; 
    std::future<int> pop_done; 
    try 
    { 
        push_done=std::async(std::launch::async, 
                             [&q,ready,&push_ready]() 
                             { 
                                 push_ready.set_value(); 
                                 ready.wait(); 
                                 q.push(42); 
                             } 
            ); 
        pop_done=std::async(std::launch::async, 
                            [&q,ready,&pop_ready]() 
                            { 
                                pop_ready.set_value(); 
                                ready.wait(); 
                                return q.pop(); 
                            } 
            ); 
        push_ready.get_future().wait(); 
        pop_ready.get_future().wait(); 
        go.set_value(); 
        push_done.get(); 
        assert(pop_done.get()==42); 
        assert(q.empty()); 
    } 
    catch(...) 
    { 
        go.set_value(); 
        throw; 
    } 
}
1
2
3
4
5
6
7
8
9
10
11
12

