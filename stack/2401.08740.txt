SiT: Exploring Flow and Diffusion-based Generative Models
with Scalable Interpolant Transformers
Nanye Ma
Mark Goldstein
Michael S. Albergo
Nicholas M. Boffi
Eric Vanden-Eijnden†
Saining Xie†
New York University
Code: https://github.com/willisma/SiT
Abstract
We present Scalable Interpolant Transformers (SiT), a
family of generative models built on the backbone of Diffu-
sion Transformers (DiT). The interpolant framework, which
allows for connecting two distributions in a more flexible
way than standard diffusion models, makes possible a mod-
ular study of various design choices impacting generative
models built on dynamical transport: using discrete vs. con-
tinuous time learning, deciding the objective for the model to
learn, choosing the interpolant connecting the distributions,
and deploying a deterministic or stochastic sampler. By care-
fully introducing the above ingredients, SiT surpasses DiT
uniformly across model sizes on the conditional ImageNet
256x256 benchmark using the exact same backbone, number
of parameters, and GFLOPs. By exploring various diffusion
coefficients, which can be tuned separately from learning,
SiT achieves an FID-50K score of 2.06.
1. Introduction
Contemporary success in image generation has come from a
combination of algorithmic advances and improvements in
model architecture and progress in scaling neural network
models and data. State-of-the-art diffusion models [25, 51]
proceed by incrementally transforming data into Gaussian
noise as prescribed by an iterative stochastic process, which
can be specified either in discrete or continuous time. At an
abstract level, this corruption process can be viewed as defin-
ing a time-dependent distribution that is iteratively smoothed
from the original data distribution into a standard normal
distribution. Diffusion models learn to reverse this corrup-
tion process and push Gaussian noise backwards along this
connection to obtain data samples. The objects learned to
perform this transformation are conventionally either predict-
ing the noise in the corruption process [25] or predicting the
score of the distribution connecting the data and the Gaus-
†Equal advising.
Model
Params(M)
Training Steps
FID ↓
DiT-S
33
400K
68.4
SiT-S
33
400K
57.6
DiT-B
130
400K
43.5
SiT-B
130
400K
33.5
DiT-L
458
400K
23.3
SiT-L
458
400K
18.8
DiT-XL
675
400K
19.5
SiT-XL
675
400K
17.2
DiT-XL
675
7M
9.6
SiT-XL
675
7M
8.6
DiT-XL (cfg=1.5)
675
7M
2.27
SiT-XL (cfg=1.5)
675
7M
2.06
Table 1. Scalable Interpolant Transformers. We systematically
vary the following aspects of a generative model: discrete vs. con-
tinuous time, model prediction, interpolant, choice of sampler.
The resulting Scalable Interpolant Transformer (SiT) model, under
identical training compute, consistently outperforms the Diffusion
Transformer (DiT) in generating 256×256 ImageNet images. All
models employ a patch size of 2. In this work, we ask the question:
What is the source of the performance gain?
sian [62], though alternatives of these choices exist [27, 54].
The neural network architectures used to represent these
objects have been shown to perform well on a variety of tasks.
While diffusion models were originally built upon a U-Net
backbone [25, 52], recent work has highlighted that archi-
tectural advances in vision such as the Vision Transformer
(ViT) [21] can be incorporated into the standard diffusion
model pipeline to improve performance [48]. The aims of
[48] were to push improvements on the model side of the
duality of algorithm and model.
Orthogonally, significant research effort has gone into ex-
ploring the structure of the noising process, which has been
shown to lead to performance benefits [32, 35, 36, 58]. Yet,
many of these efforts do not move past the notion of passing
1
arXiv:2401.08740v1  [cs.CV]  16 Jan 2024

Figure 1. Selected samples from SiT-XL models trained on ImageNet [53] at 512×512 and 256×256 resolution with cfg = 4.0, respectively.
data through a diffusion process with an equilibrium distri-
bution, which is a restricted type of connection between the
data and the Gaussian. The recently-introduced stochastic
interpolants [3] lift such constraints and introduce more flex-
ibility in the noise-data connection. In this paper, we further
explore its performance in large scale image generation.
Intuitively, we expect that the difficulty of the learning
problem can be related to both the specific connection cho-
sen and the object that is learned. Our aim is to clarify these
design choices, in order to simplify the learning problem
and improve performance. To glean where potential benefits
arise in the learning problem, we start with Denoising Dif-
fusion Probabilistic Models (DDPMs) and sweep through
adaptations of: (i) which object to learn, and (ii) which
interpolant to choose to reveal best practices.
In addition to the learning problem, there is a sampling
problem that must be solved at inference time. It has been
acknowledged for diffusion models that sampling can be
either deterministic or stochastic [61], and the choice of
sampling method can be made after the learning process.
Yet, the diffusion coefficients used for stochastic sampling
are typically presented as intrinsically tied to the forward
noising process, which need not be the case in general.
Throughout this paper, we explore how the design of the
interpolant and the use of the resulting model as either a de-
terministic or a stochastic sampler impact performance. We
gradually transition from a typical denoising diffusion model
to an interpolant model by taking a series of orthogonal steps
in the design space. As we progress, we carefully evaluate
how each move away from the diffusion model impacts the
performance. In summary, our main contributions are:
• By moving from discrete to continuous time, changing
the model prediction, interpolant, and the choice of sam-
pler, we observe a consistent performance improvement
over the Diffusion Transformer (DiT).
• We systematically study where these improvements come
from by addressing these factors one by one: learning
in continuous time; learning a velocity as compared to
a score; changing the interpolant connecting the the two
distributions; and using the velocity in an SDE sampler
with particular choices of diffusion coefficients.
• We show that the SDE for the interpolant can be instanti-
ated using just a velocity model, which we use to push the
performance of these methods beyond previous results.
2

200K
300K
400K
500K
600K
700K
Training Steps
45
50
55
60
65
70
75
80
FID-50K
SiT-S
DiT-S
200K
300K
400K
500K
600K
Training Steps
25
30
35
40
45
50
55
FID-50K
SiT-B
DiT-B
200K
300K
400K
500K
600K
700K
800K
Training Steps
10
15
20
25
30
35
FID-50K
SiT-L
DiT-L
200K
300K
400K
500K
600K
700K
800K
Training Steps
12.5
15.0
17.5
20.0
22.5
25.0
FID-50K
SiT-XL
DiT-XL
Figure 2. SiT observes improvement in FID across all model sizes. We show FID-50K over training iterations for both DiT and SiT. All
results are produced by a Euler-Maruyama sampler using 250 integration steps. Across all model sizes, SiT converges much faster.
2. SiT: Scalable Interpolant Transformers
We begin by recalling the main ingredients for building flow-
based and diffusion-based generative models.
2.1. Flows and diffusions
In recent years, a flexible class of generative models based
on turning noise ε ∼N(0, I) into data x∗∼p(x) have been
introduced. These models use the time-dependent process
xt = αtx∗+ σtε,
(1)
where αt is a decreasing function of t and σt is an increas-
ing function of t. Stochastic interpolants and other flow
matching methods [1, 3, 39, 42] restrict the process (1) on
t ∈[0, 1], and set α0 = σ1 = 1, α1 = σ0 = 0, so that xt
interpolates exactly between x∗at time t = 0 and ε and time
t = 1. By contrast, score-based diffusion models [32, 36, 62]
set both αt and σt indirectly through different formulations
of a stochastic differential equation (SDE) with N(0, I) as
its equilibrium distribution. Moreover, they consider the
process xt on an interval [0, T] with T large enough that xT
approximates a Gaussian distribution.
Probability flow.
Common to both stochastic interpolants
and score-based diffusion models is the observation that the
process xt can be sampled dynamically using either an SDE
or a probability flow ordinary differential equation (ODE).
More precisely, the marginal probability distribution pt(x)
of xt in (1) coincides with the distribution of the probability
flow ODE with a velocity field
˙Xt = v(Xt, t),
(2)
where v(x, t) is given by the conditional expectation
v(x, t) = E[ ˙xt|xt = x],
= ˙αtE[x∗|xt = x] + ˙σtE[ε|xt = x].
(3)
Equation (3) is derived in Appendix A.1. By solving the
probability flow ODE (2) backwards in time from XT =
ε ∼N(0, I), we can generate samples from p0(x), which
approximates the ground-truth data distribution p(x). We
refer to (2) as a flow-based generative model.
Reverse-time SDE.
The time-dependent probability distri-
bution pt(x) of xt also coincides with the distribution of the
reverse-time SDE [5]
dXt = v(Xt, t)dt + 1
2wts(Xt, t)dt + √wtd ¯
Wt,
(4)
where ¯
Wt is a reverse-time Wiener process, wt > 0 is an
arbitrary time-dependent diffusion coefficient, v(x, t) is the
velocity defined in (3), and where s(x, t) = ∇log pt(x) is
the score. Similar to v, this score is given by the conditional
expectation
s(x, t) = −σ−1
t
E[ε|xt = x].
(5)
This equation is derived in Appendix A.3. Similarly, solving
the reverse SDE (4) backwards in time from XT = ε ∼
N(0, I) enables generating samples from the approximated
data distribution p0(x) ∼p(x). We refer to (2) as a diffusion-
based generative model.
Design choices.
Score-based diffusion models typically
tie the choice of αt, σt, and wt in (4) to the drift and dif-
fusion coefficients used in the forward SDE that generates
xt (see (10) below). The stochastic interpolant framework
decouples the formulation of xt from the forward SDE and
shows that there is more flexibility in the choices of αt, σt,
and wt. Below, we will exploit this flexibility to construct
generative models that outperform score-based diffusion
models on standard benchmarks in image generation task.
2.2. Estimating the score and the velocity
Practical use of the probability flow ODE (2) and the reverse-
time SDE (4) as generative models relies on our ability to
estimate the velocity v(x, t) and/or score s(x, t) fields that
enter these equations. The key observation made in score-
based diffusion models is that the score can be estimated
parametrically as sθ(x, t) using the loss
Ls(θ) =
Z T
0
E[∥σtsθ(xt, t) + ε∥2]dt.
(6)
This loss can be derived by using (5) along with standard
properties of the conditional expectation. Similarly, the
3

velocity in (3) can be estimated parametrically as vθ(x, t)
via the loss
Lv(θ) =
Z T
0
E[∥vθ(xt, t) −˙αtx∗−˙σtε∥2]dt.
(7)
We note that any time-dependent weight can be included un-
der the integrals in both (6) and (7). These weight factors are
key in the context of score-based models when T becomes
large [35]; in contrast, with stochastic interpolants where
T = 1 without any bias, these weights are less important and
might impose numerical stability issue (see Appendix B).
Model prediction.
We observed that only one of the two
quantities sθ(x, t) and vθ(x, t) needs to be estimated in
practice. This follows directly from the constraint
x = E[xt|xt = x],
= αtE[x∗|xt = x] + σtE[ε|xt = x],
(8)
which can be used to re-express the score (5) in terms of the
velocity (3) as
s(x, t) = σ−1
t
αtv(x, t) −˙αtx
˙αtσt −αt ˙σt
.
(9)
We will use this relation to specify our model prediction.
Conversely, we can also express v(x, t) in terms of s(x, t).
In our experiments, we typically learn the velocity field
v(x, t) and use it to express the score s(x, t) when using
an SDE for sampling. We include a detailed derivation
in Appendix A.4.
Note that by our definitions ˙αt < 0 and ˙σt > 0, so that
the denominator of (9) is never zero. Yet, σt vanishes at
t = 0, making the σ−1
t
in (9) appear to cause a singularity
there1. This suggests the choice wt = σt in (4) to cancel this
singularity (see Appendix A.3), for which we will explore
the performance in the numerical experiments.
2.3. Specifying the interpolating process
Score-based diffusion.
In Score-Based Diffusion Models
(SBDM), the choice of αt and σt in (1) is typically deter-
mined by the choice of the forward SDE used to define
this process, though recent work has tried to reconsider this
[32, 35]. For example, if we use the standard variance-
preserving (VP) SDE [62]
dXt = −1
2βtXtdt +
p
βtdWt
(10)
for some βt > 0, it can be shown (see Appendix B) that the
solution to (10) has the same probability distribution pt(x)
1We remark that s(x, t) can be shown to be non-singular at t = 0
analytically if the data distribution p(x) has a smooth density [3], though
this singularity appears in numerical implementations and losses in general.
as the process xt defined in (1) for the choice
VP: αt = e−1
2
R t
0 βsds,
σt =
q
1 −e−
R t
0 βsds. (11)
The only design flexibility in (11) comes from the choice
of βt, because it determines both αt and σt2. For example,
setting βt = 1 leads to αt = e−t and σt =
√
1 −e−2t.
This choice necessitates taking T sufficiently large [25] or
searching for more appropriate choices of βt [16, 58, 62] to
reduce the bias induced by the fact that the solution to the
SDE (10) only converges to ε ∼N(0, I) as t →∞.
General interpolants.
In the stochastic interpolant frame-
work, the process (1) is defined explicitly and without any
reference to a forward SDE, creating more flexibility in the
choice of αt and σt. Specifically, any choice satisfying:
(i) α2
t + σ2
t > 0 for all t ∈[0, 1];
(ii) αt and σt are differentiable for all t ∈[0, 1];
(iii) α1 = σ0 = 0, α0 = σ1 = 1;
gives a process that interpolates without bias between
xt=0 = x∗and xt=1 = ε. In our numerical experiments, we
exploit this design flexibility to test, in particular, the choices
Linear:
αt = 1 −t,
σt = t,
GVP:
αt = cos( 1
2πt),
σt = sin( 1
2πt),
(12)
where GVP refers to a generalized VP which has constant
variance across time for any endpoint distributions with the
same variance. We note that the fields v(x, t) and s(x, t)
entering (2) and (4) depend on the choice of αt and σt,
and typically must be specified before learning3. This is in
contrast to the diffusion coefficient w(t), as we now describe.
2.4. Specifying the diffusion coefficient
As stated earlier, the SBDM diffusion coefficient used in the
reverse SDE (4) is usually taken to match that of the forward
SDE (10). That is, one sets wt = βt. In the stochastic
interpolant framework, this choice is again subject to greater
flexibility: any wt > 0 can be used. Interestingly, this choice
can be made after learning, as it does not affect the velocity
v(x, t) or the score s(x, t). In our experiments, we exploit
this flexibility by considering the choices listed in Table 2.
2.5. Time-discretization and link with DDPM
During inference, continuous time models must be dis-
cretized when solving the probability flow ODE (2) and
the reverse-time SDE (4). This allows us to make a link with
DDPMs [25].
2VP is the only linear scalar SDE with an equilibrium distribution [58];
interpolants extend beyond α2
t + σ2
t = 1 by foregoing the requirement of
an equilibrium distribution.
3The requirement to learn and sample under one choice of path specified
by αt, σt, at training time may be relaxed and is explored in [2].
4

Increasing transformer sizes
−−−−−−−−−−−−−−−−−−−−→
Figure 3. Increasing transformer size increases sample quality. Best viewed zoomed-in. We sample from all 4 of our SiT model (SiT-S,
SiT-B, SiT-L and SiT-XL) after 400K training steps using the same latent noise and class label.
Expression for wt
βt = −2σt( ˙σt −σt ˙αt
αt )
σt
1 −t
sin2(πt)
(cos(πt) + 1)2
(cos(πt) −1)2
Table 2. Diffusion coefficients. They can be specified after learning
to maximize performance. wt = βt in the first row corresponds
to SBDM (see Eq. (11)), which is coupled to the forward process.
A detailed derivation is provided in Appendix B. wt = σt in the
second row is used to eliminate the singularity at t = 0 following
the explanation at the end of Sec. 2.2. In other choices we decouple
wt from the interpolant and experiment with removing diffusivity
at different times in sampling.
Assuming that we discretize time using a grid 0 = t0 <
t1 < t2 < . . . < tN = T, the process (1) can be evaluated
at each grid point, xti = αtix∗+σtiε, and both the velocity
and the score can be estimated on these points via the losses
LN
s (θ) =
N
X
i=0
E[∥σtisθ(xti, ti) + ε∥2],
(13)
LN
v (θ) =
N
X
i=0
E[∥vθ(xti, ti) −˙αtix∗−˙σtiε∥2].
(14)
Moreover, only the learned sθ(x, ti) or vθ(x, ti) is needed
to integrate the probability flow ODE (2) and the reverse-
time SDE (4) on the same grid. The resulting procedure, in
which we define xt iteratively on the grid, is a generalization
of DDPM. Starting from xt0 = x∗, we set for i ≥0,
xti+1 =
p
1 −hβtixti +
p
hβtiεti,
(15)
where h = ti+1 −ti and where we assume that the grid
is uniform. Because
p
1 −hβti = 1 −1
2hβti + o(h), it is
easy to see that (15) is a consistent time-discretization of the
forward SDE (10). Our results show that it is not necessary
to specify the time discretized process xti using (15), but
instead we can directly use (1) on the time grid.
2.6. Interpolant Transformer Architecture
The backbone architecture and capacity of generative models
are also crucial for producing high-quality samples. In order
to eliminate any confounding factors and focus on our explo-
ration, we strictly follow the standard Diffusion Transformer
(DiT) [48] and its configurations. This way, we can also test
the scalability of our model across various model sizes.
Here we briefly introduce the model design. Generating
high-resolution images with diffusion models can be compu-
tationally expensive. Latent diffusion models (LDMs) [51]
address this by first downsampling images into a smaller la-
tent embedding space using an encoder E, and then training
a diffusion model on z = E(x). New images are created by
sampling z from the model and decoding it back to images
using a decoder x = D(z).
Similarly, SiT is also a latent generative model and we
use the same pre-trained VAE encoder and decoder models
originally used in Stable Diffusion [51]. SiT processes a
spatial input z (shape 32×32×4 for 256×256×3 images)
by first ‘patchifying’ it into T linearly embedded tokens
of dimension d. We always use a patch size of 2 in these
models as they achieve the best sample quality. We then
5

Model
Layers N
Hidden size d
Heads
SiT-S
12
384
6
SiT-B
12
768
12
SiT-L
24
1024
16
SiT-XL
28
1152
16
Table 3. Details of SiT models. We follow DiT [48] for the Small
(S), Base (B), Large (L) and XLarge (XL) model configurations.
apply standard ViT [21] sinusoidal positional embeddings to
these tokens. We use a series of N SiT transformer blocks,
each with hidden dimension d.
Our model configurations—SiT-{S,B,L,XL}—vary in
model size (parameters) and compute (flops), allowing for
a model scaling analysis. For class-conditional generation
on ImageNet, we use the AdaLN-Zero block [48] to process
additional conditional information (times and class labels).
SiT architectural details are listed in Table 3.
3. Experiments
To provide a more detailed answer to the question raised in
Tab. 1 and make a fair comparison between DiT and SiT, we
gradually transition from a DiT model (discretized, score pre-
diction, VP interpolant) to a SiT model (continuous, velocity
prediction, Linear interpolant) in the following four subsec-
tions, and present the impacts on performance. Throughout
our experiments in each subsection, we uses a DiT-B model
at 400K training steps as our backbone. For solving the
ODE (2), we adopt a fixed Heun integrator; for solving the
SDE (4), we used an Euler-Maruyama integrator. With both
solver choices we limit the number of function evaluations
(NFE) to be 250 to match the number of sampling steps used
in DiT. All numbers presented in the following sections are
FID-50K scores evaluated on the ImageNet256 training set.
3.1. Discrete to Continuous Time
To understand the role of continuous-time versus discrete-
time models, we study discrete-time DDPM against
continuous-time SBDM-VP with estimation of the score.
The results are presented in Table 4, where we find a
marginal improvement in FID scores when going from a
discrete-time denoiser to a continuous-time score field.
Model
Objective
FID
DDPM
Noise
LN
s
44.2
SBDM-VP
Score
Ls
43.6
Table 4. DDPM vs. SBDM.
100K
200K
300K
400K
500K
600K
700K
Training Steps
0.2
0.3
0.4
0.5
0.6
0.7
R
|vθ(x, t)|2pt(x)dxdt
SBDM-VP
Linear
GVP
Figure 4.
Path length.
The path length
C(v)
=
R 1
0 E[|v(xt, t)|2]dt arising from the velocity field at different train-
ing steps for the various models considered: SBDM (VP), Linear,
and GVP; each curve is approximated by 10000 datapoints.
3.2. Model parameterizations
To clarify the role of the model parameterization in the con-
text of SBDM-VP, we now compare learning (i) a score
model using (6), (ii) a weighted score model (see Ap-
pendix A.3), or (iii) a velocity model using (7). The results
are shown in Table 5, where we find that we obtain a signifi-
cant performance improvement by learning a weighted score
model or a velocity model.
Interpolant
Model
Objective
ODE
SBDM-VP
Score
Ls
43.6
SBDM-VP
Score
Lsλ
39.1
SBDM-VP
Velocity
Lv
39.8
Table 5. Effect of model parameterization.
3.3. Choice of interpolant
Section 2 highlights that there are many possible ways to
build a connection between the data distribution and a Gaus-
sian by varying the choice of αt and σt in the definition of
the interpolant (1). To understand the role of this choice, we
now study the benefits of moving away from the commonly-
used SBDM-VP setup. We consider learning a velocity
model v(x, t) with the Linear and GVP interpolants pre-
sented in (12), which make the interpolation between the
Gaussian and the data distribution exact on [0, 1]. We bench-
mark these models against the SBDM-VP in Table 6, where
we find that both the GVP and Linear interpolants obtain
significantly improved performance.
One possible explanation for this observation is given
in Fig. 4, where we see that the path length (transport
cost) is reduced when changing from SBDM-VP to GVP
or Linear. Numerically, we also note that for SBDM-VP,
˙σt = βte−
R t
0 βsds/(2σt) becomes singular at t = 0: this
6

can pose numerical difficulties inside Lv, leading to diffi-
culty in learning near the data distribution. This issue does
not appear with the GVP and Linear interpolants.
Interpolant
Model
Objective
FID
SBDM-VP
Velocity
Lv
39.8
Linear
Velocity
Lv
34.8
GVP
Velocity
Lv
34.6
Table 6. Effect of interpolant.
3.4. Deterministic vs stochastic sampling
Using the SBDM diffusion coefficient.
As shown
in Sec. 2, given a learned model, we can sample using either
the probability flow equation (2) or an SDE (4). In SBDM
we conventionally take as diffusion coefficient wt = βt.
For Linear and GVP interpolant, we follow the derivation
in Appendix B to express βt in terms of αt and σt.
Our results are shown in Tab. 7, where we find perfor-
mance improvements by sampling with an SDE over the
ODE, which is in line with the bounds given in [3]: the
SDE has better control over the KL divergence between the
time-dependent density at t = 0 and the ground truth data
distribution. We note that the performance of ODE and SDE
integrators may differ under different computation budgets.
As shown in Fig. 5, the ODE converges faster with fewer
NFE, while the SDE is capable of reaching a much lower
final FID score when given a larger computational budget.
Interpolant
Model
Objective
ODE
SDE
SBDM-VP
Velocity
Lv
39.8
37.8
Linear
Velocity
Lv
34.8
33.6
GVP
Velocity
Lv
34.6
32.9
Table 7. ODE vs. SDE, SBDM diffusion.
Tunable diffusion coefficient.
Motivated by the improved
performance of SDE sampling, we now consider the effect
of tuning the diffusion coefficient in a manner that is distinct
from the choices made in SBDM, as detailed in Tab. 2. As
shown in Tab. 8, we find that the optimal choice for sam-
pling is both model prediction and interpolant dependent.
We picked the three functions with the best performance
and sweep through all different combinations of our model
prediction and interpolant, and present the result in Tab. 8.
We also note that the influences of different diffusion co-
efficients can vary across different model sizes. Empirically,
we observe the best choice for our SiT-XL is a velocity model
with Linear interpolant and sampled with σt coefficient.
3.5. Classifier-free guidance
Classifier-free guidance (CFG) [24] often leads to improved
performance for score-based models. In this section, we
8
16
32
64
128
256
512
1024
NFE
34
36
38
40
42
44
FID-10K
ODE
SDE: σt
SDE: βt
SDE: sin(πt)2
Figure 5. Comparison of ODE and SDE w/ choices of diffusion
coefficients. We evaluate each sampler using a 400K steps trained
SiT-B model with Linear interpolant and learning the v(x, t).
give a concise justification for adopting it on the velocity
model, and then empirically show that the drastic gains in
performance for DiT case carry across to SiT.
Guidance for a velocity field means that: (i) that the
velocity model vθ(x, t; y) takes class labels y during train-
ing, where y is occasionally masked with a null token ∅;
and (ii) during sampling the velocity used is vζ
θ(x, t; y) =
ζvθ(x, t; y) + (1 −ζ)vθ(x, t; ∅) for a fixed ζ > 0. In Ap-
pendix C, we show that this indeed corresponds to sampling
the tempered density p(xt)p(y|xt)ζ as proposed in [46].
Given this observation, one can leverage the usual argument
for classifier-free guidance of score-based models.
For a CFG scale of ζ = 1.5, DiT-XL sees an improve-
ment in FID from 9.6 (non-CFG) down to 2.27 (CFG). We
observed similar performance improvement with our largest
SiT-XL model under identical computation budget and CFG
scale. Sampled with an ODE, the FID-50K score improves
from 9.4 to 2.15 (Tab. 9 and Appendix D); with an SDE, the
FID improves from 8.6 to 2.06 (Tab. 1 and Tab. 9). This
shows that SiT benefits from the same training and sam-
pling choices explored previously, and can surpass DiTs
performance in each training setting, not only with respect
to model size, but also with respect to sampling choices.
4. Related Work
Transformers.
The transformer architecture [66] has
emerged as a powerful tool for application domains as di-
verse as vision [21, 47], language [68, 69], quantum chem-
istry [67], active matter systems [9], and biology [11]. Sev-
eral works have built on DiT and have made improvements
by modifying the architecture to internally include masked
prediction layers [22, 70]; these choices are orthogonal to
the transition from DiT to SiT studied in this work; they may
be fruitfully combined in future work.
7

Interpolant
Model
Objective
wt = βt
wt = σt
wt = sin2(πt)
SBDM-VP
velocity
Lv
37.8
38.7
39.2
score
Lsλ
35.7
37.1
37.7
GVP
velocity
Lv
32.9
33.4
33.6
score
Ls
38.0
33.5
33.2
Linear
velocity
Lv
33.6
33.5
33.3
score
Ls
41.0
35.3
34.4
Table 8. Evaluation of our SDE samplers. All results in the table are FID-50K scores produced by an SiT-B model at 400K training steps.
The last three columns specify different diffusion coefficients wt detailed in Tab. 2. To make the SBDM-VP competitive when learning a
score, we use a weighted score given in Appendix A.4, as per the remarks below (7).
Class-Conditional ImageNet 256x256
Model
FID↓sFID↓
IS↑
Precision↑Recall↑
BigGAN-deep[10]
6.95
7.36
171.4
0.87
0.28
StyleGAN-XL[55]
2.30
4.02 265.12
0.78
0.53
Mask-GIT[12]
6.18
-
182.1
-
-
ADM[19]
10.94 6.02 100.98
0.69
0.63
ADM-G, ADM-U
3.94
6.14 215.84
0.83
0.53
CDM[26]
4.88
-
158.71
-
-
RIN[30]
3.42
-
182.0
-
-
Simple Diffusion(U-Net)[27] 3.76
-
171.6
-
-
Simple Diffusion(U-ViT, L)
2.77
-
211.8
-
-
VDM++[35]
2.12
-
267.7
-
-
DiT-XL(cfg = 1.5)[48]
2.27
4.60 278.24
0.83
0.57
SiT-XL(cfg = 1.5, ODE)
2.15
4.60 258.09
0.81
0.60
SiT-XL(cfg = 1.5, SDE:σt)
2.06
4.50 270.27
0.82
0.59
Table 9. Benchmarking class-conditional image generation on
ImageNet 256x256. SiT-XL surpasses DiT-XL in FID when either
of the samplers, ODE or SDE-based.
Training and Sampling in Diffusions.
Diffusion models
arose from [25, 59, 62] and have close historical relationship
with denoising methods [28, 29, 57]. Various efforts have
gone into improving the sampling algorithms behind these
methods in the context of DDPM [60] and SBDM [32, 61];
these are also orthogonal to our studies and may be combined
to push for better performance in future work. Improved Dif-
fusion ODE [71] also studies several combinations of model
parameterizations (velocity versus noise) and paths (VP ver-
sus Linear) for sampling an ODE; they report best results for
velocity model with smoother probability flow; they focus
on lower dimensional experiments, benchmark with likeli-
hoods, and do not consider SDE sampling. In our work,
we explore the effects of changing between VP, Linear, and
GVP interpolants, as well as score and velocity parameter-
izations in depth and show how these choices individually
improve performance on the larger scale ImageNet256. We
also document how FIDs change with respect to a family of
sampling algorithms including black-box ODEs and SDEs
indexed by a choice of diffusion coefficients, and show that
the best coefficient choice may depend on the model and
interpolant. This brings the observations about the flexibility
and trade-offs of sampling from [3] into practice.
Interpolants and flow matching.
Velocity field parame-
terizations using the Linear interpolant were also studied in
[39, 42], and were generalized to the manifold setting in [6].
A trade-off in bounds on the KL divergence between the tar-
get distribution and the model arises when considering sam-
pling with SDEs versus ODE; [3] shows that minimizing the
objectives presented in this work controls KL for SDEs, but
not for ODEs. Error bounds for SDE-based sampling with
score-based diffusion models are studied in [13, 14, 37, 38].
Error bounds on the ODE are also explored in [7, 15], in
addition to the Wasserstein bounds provided in [1].
Other related works make improvements by changing
how noise and data are sampled during training. [50, 64]
compute mini-batch optimal couplings between the Gaus-
sian and data distribution to reduce the transport cost and
gradient variance; [4] instead build the coupling by flowing
directly from the conditioning variable to the data for image-
conditional tasks such as super-resolution and in-painting.
Finally, various work considers learning a stochastic bridge
connecting two arbitrary distributions [18, 41, 49, 56]. These
directions are compatible with our investigations; they spec-
ify the learning problem for which one can then vary the
choices of model parameterizations, interpolant schedules,
and sampling algorithms.
Diffusion in Latent Space.
Generative modeling in latent
space [51, 65] is a tractable approach for modeling high-
dimensional data. The approach has been applied beyond
images to video generation [8], which is a yet-to-be explored
and promising application area for velocity trained models.
[17] also train velocity models in the latent space of the pre-
trained Stable Diffusion VAE. They demonstrate promising
results for the DiT-B backbone with a final FID-50K of 4.46;
their study was one motivation for the investigation in this
work regarding which aspects of these models contribute to
the gains in performance over DiT.
8

5. Conclusion
In this work, we have presented Scalable Interpolant Trans-
formers, a simple and powerful framework for image genera-
tion tasks. Within the framework, we explored the tradeoffs
between a number of key design choices: the choice of a con-
tinuous or discrete-time model, the choice of interpolant, the
choice of model prediction, and the choice of diffusion coef-
ficient. We highlighted the advantages and disadvantages of
each choice and demonstrated how careful decisions can lead
to significant performance improvements. Many concurrent
works [23, 31, 40, 45] explore similar approaches in a wide
variety of downstream tasks, and we leave the application of
SiT to these tasks for future works.
Acknowledgements.
We would like to thank Adithya
Iyer, Sai Charitha Akula, Fred Lu, Jitao Gu, and Edwin
P. Gerber for helpful discussions and feedback.
The
research is partly supported by the Google TRC program.
References
[1] Michael S Albergo and Eric Vanden-Eijnden. Building nor-
malizing flows with stochastic interpolants. In ICLR, 2023. 3,
8
[2] Michael S Albergo, Nicholas M Boffi, Michael Lindsey, and
Eric Vanden-Eijnden. Multimarginal generative modeling
with stochastic interpolants. arXiv preprint arXiv:2310.03695,
2023. 4
[3] Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-
Eijnden. Stochastic interpolants: A unifying framework for
flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.
2, 3, 4, 7, 8, 1, 5, 6
[4] Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Ra-
jesh Ranganath, and Eric Vanden-Eijnden. Stochastic in-
terpolants with data-dependent couplings. arXiv preprint
arXiv:2310.03725, 2023. 8
[5] Brian D.O. Anderson. Reverse-time diffusion equation mod-
els. Stochastic Processes and their Applications, 1982. 3
[6] Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos,
Aditya Grover, Maximilian Nickel, Ricky TQ Chen, and
Yaron Lipman. Matching normalizing flows and probabil-
ity paths on manifolds. In ICML, 2022. 8
[7] Joe Benton, George Deligiannidis, and Arnaud Doucet. Er-
ror bounds for flow matching methods.
arXiv preprint
arXiv:2305.16860, 2023. 8
[8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In CVPR, 2023. 8
[9] Nicholas M Boffi and Eric Vanden-Eijnden. Deep learning
probability flows and entropy production rates in active matter.
arXiv preprint arXiv:2309.12991, 2023. 7
[10] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis. In
ICLR, 2019. 8
[11] Abel Chandra, Laura T¨unnermann, Tommy L¨ofstedt, and
Regina Gratz. Transformer-based deep learning for predicting
protein properties in the life sciences. Elife, 12:e82819, 2023.
7
[12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T.
Freeman. Maskgit: Masked generative image transformer. In
CVPR, 2022. 8
[13] Hongrui Chen, Holden Lee, and Jianfeng Lu.
Improved
analysis of score-based generative modeling: User-friendly
bounds under minimal smoothness assumptions. In ICML,
2023. 8
[14] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim,
and Anru Zhang. Sampling is as easy as learning the score:
theory for diffusion models with minimal data assumptions.
In ICLR, 2023. 8
[15] Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-
degradation beyond linear diffusions: A non-asymptotic anal-
ysis for DDIM-type samplers. In ICML, 2023. 8
[16] Ting Chen. On the importance of noise scheduling for diffu-
sion models. arXiv preprint arXiv:2301.10972, 2023. 4
[17] Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow
matching in latent space. arXiv preprint arXiv:2307.08698,
2023. 8
[18] Valentin De Bortoli, James Thornton, Jeremy Heng, and Ar-
naud Doucet. Diffusion schr¨odinger bridge with applications
to score-based generative modeling. In NeurIPS, 2021. 8
[19] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis. In NeurIPS, 2021. 8, 4, 7
[20] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-
based generative modeling with critically-damped langevin
diffusion. In ICLR, 2022. 6
[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR, 2021. 1, 6, 7
[22] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng
Yan. Masked diffusion transformer is a strong image synthe-
sizer. arXiv preprint arXiv:2303.14389, 2023. 7
[23] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera
Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos´e Lezama.
Photorealistic video generation with diffusion models. arXiv
preprint arXiv:2312.06662, 2023. 9
[24] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022. 7, 4
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS, 2020. 1, 4, 8
[26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. arXiv preprint
arXiv:2106.15282, 2021. 8
[27] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple
diffusion: End-to-end diffusion for high resolution images. In
ICML, 2023. 1, 8
[28] Aapo Hyv¨arinen. Estimation of non-normalized statistical
models by score matching. JMLR, 2005. 8
9

[29] Aapo Hyv¨arinen. Sparse code shrinkage: Denoising of non-
gaussian data by maximum likelihood estimation. Neural
Computation, 1999. 8
[30] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive
computation for iterative generation. In ICML, 2023. 8
[31] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht,
and Andrea Vedaldi. Farm3D: Learning articulated 3d ani-
mals by distilling 2d diffusion. In 3DV, 2024. 9
[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. In NeurIPS, 2022. 1, 3, 4, 8, 6
[33] Patrick Kidger. On Neural Differential Equations. PhD thesis,
University of Oxford, 2021. 6
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR, 2015. 7
[35] Diederik P Kingma and Ruiqi Gao. Understanding the diffu-
sion objective as a weighted integral of elbos. arXiv preprint
arXiv:2303.00848, 2023. 1, 4, 8, 3
[36] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. In NeurIPS, 2021. 1, 3, 6
[37] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for
score-based generative modeling with polynomial complexity.
In NeurIPS, 2022. 8
[38] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of
score-based generative modeling for general data distributions.
In ALT, 2023. 8
[39] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxi-
milian Nickel, and Matt Le. Flow matching for generative
modeling. In ICLR, 2023. 3, 8
[40] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,
Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot
one image to 3d object. In ICCV, 2023. 9
[41] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us
build bridges: Understanding and extending diffusion genera-
tive models. arXiv preprint arXiv:2208.14699, 2022. 8
[42] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight
and fast: Learning to generate and transfer data with rectified
flow. In ICLR, 2023. 3, 8
[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR, 2019. 7
[44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion
probabilistic model sampling in around 10 steps. In NeurIPS,
2022. 6
[45] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun
Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image
synthesis and editing with stochastic differential equations.
In ICLR, 2022. 9
[46] Alex Nichol and Prafulla Dhariwal. Improved denoising
diffusion probabilistic models. In ICML, 2021. 7
[47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age Transformer. In ICML, 2018. 7
[48] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In ICCV, 2023. 1, 5, 6, 8, 7
[49] Stefano Peluchetti. Non-denoising forward-time diffusions.
In ICLR, 2022. 8
[50] Aram-Alexandre
Pooladian,
Heli
Ben-Hamu,
Carles
Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky
T. Q. Chen. Multisample flow matching: Straightening flows
with minibatch couplings. In ICML, 2023. 8
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In CVPR, 2022. 1, 5,
8
[52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 1
[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large scale
visual recognition challenge. IJCV, 2015. 2
[54] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In ICLR, 2022. 1, 6
[55] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl:
Scaling stylegan to large diverse datasets. In SIGGRAPH,
2022. 8
[56] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and
Arnaud Doucet. Diffusion schr¨odinger bridge matching. In
NeurIPS, 2023. 8
[57] Eero P. Simoncelli and Edward H. Adelson. Noise removal
via bayesian wavelet coring. In ICIP, 1996. 8
[58] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath.
Where to diffuse, how to diffuse, and how to get back: Auto-
mated learning for multivariate diffusions. In ICLR, 2023. 1,
4
[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML, 2015. 8
[60] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. In ICLR, 2021. 8
[61] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
Maximum likelihood training of score-based diffusion models.
In NeurIPS, 2021. 2, 8, 3
[62] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equations.
In ICLR, 2021. 1, 3, 4, 8, 6
[63] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
Consistency models. In ICML, 2023. 6
[64] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei
Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and
Yoshua Bengio. Improving and generalizing flow-based gen-
erative models with minibatch optimal transport. In ICML
Workshop on New Frontiers in Learning, Control, and Dy-
namical Systems, 2023. 8
[65] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based
generative modeling in latent space. In NeurIPS, 2021. 8, 3
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017. 7
[67] Ingrid von Glehn, James S. Spencer, and David Pfau. A Self-
Attention Ansatz for Ab-initio Quantum Chemistry. In ICLR,
2023. 7
10

[68] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li,
Derek F Wong, and Lidia S Chao. Learning deep transformer
models for machine translation. In ACL, 2019. 7
[69] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua
Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham,
Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed.
Big Bird: Transformers for Longer Sequences. In NeurIPS,
2020. 7
[70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anand-
kumar. Fast training of diffusion models with masked trans-
formers. arXiv preprint arXiv:2306.09305, 2023. 7
[71] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Im-
proved techniques for maximum likelihood estimation for
diffusion odes. In ICML, 2023. 8
11

Appendix
A. Proofs
In all proofs below, we use · for dot product and assume all bold notations (x, ε, etc.) are real-valued vectors in Rd. Most
proofs are derived from Albergo et al. [3].
A.1. Proof of the probability flow ODE (2) with the velocity in Eq. (3).
Consider the time-dependent probability density function (PDF) pt(x) of xt = αtx∗+ σtε defined in Eq. (1). By definition,
its characteristic function ˆpt(k) =
R
Rd eik·xpt(x)dx is given by
ˆpt(k) = E[eik·xt]
(16)
where E denotes expectation over x∗and ε. Taking time derivative on both sides, and using the tower property of conditional
expectation, we have
∂tˆpt(k) = ik · E[ ˙xteik·xt]
(17)
= ik · Ex∼pt[E[ ˙xteik·xt|xt = x]]
(18)
= ik · Ex∼pt[E[( ˙αtx∗+ ˙σtε)eik·xt|xt = x]]
(19)
= ik · Ex∼pt[E[( ˙αtx∗+ ˙σtε)|xt = x]eik·x]
(20)
= ik · Ex∼pt[v(x, t)eik·x]
(21)
where v(x, t) = E[( ˙αtx∗+ ˙σtε)|xt = x] = ˙αtE[x∗|xt = x] + ˙σtE[ε|xt = x] is the velocity defined in Eq. (3). Explicitly,
Eq. (21) reads
∂t
Z
Rd eik·xpt(x)dx = ik ·
Z
Rd v(x, t)eik·xpt(x)dx
(22)
from which we deduce
Z
Rd eik·x∂tpt(x)dx =
Z
Rd v(x, t) · ∇x[eik·x]pt(x)dx = −
Z
Rd ∇x · [v(x, t)pt(x)]eik·xdx
(23)
where ∇x · [vpt] = Pd
i=1
∂
∂xi [vipt] is the divergence operator and we take advantage of the divergence theorem and used
integration by parts to get the second equality. By the properties of Fourier transform, Eq. (23) implies that pt(x) satisfies the
transport equation
∂tpt(x) + ∇x · (v(x, t)pt(x)) = 0.
(24)
Solving this equation by the method of characteristic leads to probability flow ODE (2).
A.2. Proof of the SDE (4)
We show that the SDE (4) has marginal density pt(x) with any choice of wt ≥0. To this end, recall that solution to the SDE
dXt = [v(Xt, t) + 1
2wts(Xt, t)]dt + √wtd ¯
Wt
has a PDF that satisfies the Fokker-Planck equation
∂tpt(x) = −∇x ·
 [v(x, t) + 1
2wts(x, t)]pt(x)

+ 1
2wt∆xpt(x)
(25)
where ∆x is the Laplace operator defined as ∆x = ∇x · ∇x = Pd
i=0
∂2
∂x2
i . Reorganizing the equation and usng the definition
of the score s(x, t) = ∇x log pt(x) = p−1
t (x)∇xpt(x), we have
∂tpt(x) = −∇x · [v(x, t)pt(x)]
|
{z
}
= ∂tpt(x) by Eq. (24)
−1
2wt∇x · [∇x log pt(x)pt(x)
|
{z
}
= ∇xpt(x)
] + 1
2wt∆xpt(x)
(26)
=⇒
0 = −1
2wt∇x · ∇xpt(x) + 1
2wt∆xpt(x)
(27)
1

By definition of Laplace operator, the last equation holds for any wt ≥0. When wt = 0, the Fokker-Planck equation reduces
to a continuity equation, and the SDE reduces to an ODE, so the connection trivially holds.
A.3. Proof of the expression for the score in Eq. (5)
We show that s(x, t) = −σ−1
t
E[ε|xt = x]. Letting ˆf(k, t) = E[εeiσtk·ε], we have
ˆf(k, t) = −i
σt
∇kE[eiσtk·ε]
(28)
Since ε ∼N(0, I), we can compute the expectation explicitly to obtain
ˆf(k, t) = −i
σt
(∇ke−1
2 σ2
t |k|2)
(29)
= iσtke−1
2 σ2
t |k|2
(30)
Since x∗and ε are independent random variable, we have
E[εeik·xt] = ˆf(k, t)E[eiαtk·x∗] = iσtk e−1
2 σ2
t |k|2E[eiαtk·x∗]
|
{z
}
combine this
= iσtkˆpt(k)
(31)
where ˆpt(k) is the characteristic function of xt = αtx∗+ σtε defined in Eq. (16). The left hand-side of this equation can also
be written as:
E[εeik·xt] =
Z
Rd E[εeik·xt|xt = x]pt(x)dx
(32)
=
Z
Rd E[ε|xt = x]eik·xpt(x)dx,
(33)
whereas the right hand-side is
iσtkˆpt(k) = iσtk
Z
Rd eik·xpt(x)dx
(34)
= σt
Z
Rd ∇x[eik·x]pt(x)dx
(35)
= −σt
Z
Rd eik·x∇xpt(x)dx
(36)
= −σt
Z
Rd eik·xs(x, t)pt(x)dx
(37)
where we again used divergence theorem and integration by parts to get the third equality, and again the definition of the score
to get the last. Comparing Eq. (33) and Eq. (37) we deduce that, when σt ̸= 0,
s(x, t) = ∇x log pt(x) = −σ−1
t
E[ε|xt = x]
(38)
Further, setting wt to σt in Eq. (4) gives
1
2wts(xt, t) = −1
2E[ε|xt = x]
(39)
for all t ∈[0, 1]. This bypass the constraint of σt ̸= 0 and effectively eliminate the singularity at t = 0.
2

A.4. Proof of Eq. (9)
We note that there exists a straightforward connection between v(x, t) and s(x, t). From Eq. (1), we have
v(x, t) = ˙αtE[x∗|xt = x] + ˙σtE[ε|xt = x]
(40)
= ˙αtE[xt −σtε
αt
|xt = x] + ˙σtE[ε|xt = x]
(41)
= ˙αt
αt
x + ( ˙σt −˙αtσt
αt
)E[ε|xt = x]
(42)
= ˙αt
αt
x + ( ˙σt −˙αtσt
αt
)(−σts(x, t))
(43)
= ˙αt
αt
x −λtσts(x, t)
(44)
where we defined
λt = ˙σt −˙αtσt
αt
(45)
Given Eq. (44) is linear in terms of s, reverting it will lead to Eq. (9).
Note that we can also plug Eq. (44) into the loss Lv in Eq. (7) to deduce that
Lv(θ) =
Z T
0
E[∥˙αt
αt
x
|{z}
Expand to xt = αtx∗+ σtε
+λt(−σtsθ(xt, t)) −˙αtx∗−˙σtε∥2]dt
(46)
=
Z T
0
E[∥˙αtx∗+ ˙αtσt
αt
ε + λt(−σtsθ(xt, t)) −˙αtx∗−˙σtε∥2]dt
(47)
=
Z T
0
E[∥λt(−σtsθ(xt, t)) −λtε∥2]dt
(48)
=
Z T
0
λ2
tE[∥σtsθ(xt, t) + ε∥2]dt
(49)
≡Lsλ(θ)
(50)
which defines the weighted score objective Lsλ(θ). This observation is consistent with the claim made in Kingma and
Gao [35] that the score objective with different monotonic weighting functions coincides with losses for different model
parameterizations. In Appendix B we show that λt corresponds to the square of the maximum likelihood weighting proposed
in Song et al. [61] and Vahdat et al. [65].
B. Connection with Score-based Diffusion
As shown in Song et al. [62], the reverse-time SDE from Eq. (10) is
dXt = [−1
2βtXt −βts(Xt, t)]dt +
p
βtd ¯
Wt
(51)
Let us show this SDE is Eq. (4) for the specific choice wt = βt. To this end, notice that the solution Xt to Eq. (51) for the
initial condition Xt=0 = x∗with x∗fixed is Gaussian distributed with mean and variance given respectively by
E[Xt] = e−1
2
R t
0 βsdsx∗≡αtx∗
(52)
var[Xt] = 1 −e−
R t
0 βsds ≡σ2
t
(53)
Using Eq. (44), the velocity of the score-based diffusion model can therefore be expressed as
v(x, t) = −1
2βtx + (−1
2βt(1 −e−
R t
0 βsds) −1
2βte−
R t
0 βsds)s(x, t)
(54)
= −1
2βtx −1
2βts(x, t)
(55)
we see that 2λtσt is precisely βt, making λt correspond to the square of maximum likelihood weighting proposed in Song
et al. [62]. Further, if we plug Eq. (55) into Eq. (4) with wt = βt, we arrive at Eq. (51).
3

A useful observation for choosing velocity versus noise model.
We see that in the velocity model, all of the path-dependent
terms (αt, σt) are inside the squared loss, and in the score model, the terms are pulled out (apart from the necessary σt in score
matching loss) and get squared due to coming out of the norm. So which is more stable depends on the interpolant. In the
paper we see that for SBDM-VP, due to the blowing up behavior of ˙σt near t = 0, both Lv and Lsλ are unstable.
Yet, shown in Tab. 5, we observed better performance with Lsλ for SBDM-VP, as the blowing up λt near t = 0 will
compensate for the diminishing gradient inside the squared norm, where Lv would simply experience gradient explosion
resulted from ˙σt. The behavior is different for the Linear and GVP interpolant, where the source of instability is α−1
t
near
t = 1. We note Lv is stable since α−1
t
gets cancelled out inside the squared norm, while in Lsλ it remains in λt outside the
norm.
C. Sampling with Guidance
Let pt(x|y) be the density of xt = αtx∗+ σtε conditioned on some extra variable y. By argument similar to the one given in
Appendix A.1, it is easy to see that pt(x|y) satisfies the transport equation (compare Eq. (24))
∂tpt(x|y) + ∇x · (v(x, t|y)pt(x, |y)) = 0,
(56)
where (compare Eq. (3))
v(x, t|y) = E[ ˙xt|xt = x, y] = ˙αtE[x∗|xt = x, y] + ˙σtE[ε|xt = x, y]
(57)
Proceeding as in Appendix A.3 and Appendix A.4, it is also easy to see that the score s(x, t|y) = ∇x log pt(x|y) is given by
(compare Eq. (5))
s(x, t|y) = −σ−1
t
E[ε|xt = x, y]
(58)
and that v(x, t|y) and s(x, t|y) are related via (compare Eq. (44))
v(x, t|y) = ˙αt
αt
x −λtσts(x, t|y)
(59)
Consider now
sζ(x, t|y) ≡(1 −ζ)s(x, t) + ζs(x, t|y)
(60)
= ∇log pt(x) −ζ∇log pt(x) + ζ∇log pt(x|y)
(61)
= ∇log pt(x) −ζ∇log pt(x) +

ζ∇log pt(y|x) + ζ∇log pt(x)

(62)
= ∇log pt(x) + ζ∇log pt(y|x)
(63)
= ∇log[pt(x)pζ
t (y|x)]
(64)
where we have used the fact ∇x log pt(x|y) = ∇x log pt(y|x)+∇x log pt(x) that follows from pt(x|y)p(y) = pt(y|x)pt(x),
and ζ to be some constant greater than 1. Eq. (64) shows that using the score mixture sζ(x, t|y) = (1 −ζ)s(x, t) + ζs(x, t|y),
and the velocity mixture associated with it, namely,
vζ(x, t|y) = (1 −ζ)v(x, t) + ζv(x, t|y)
(65)
= ˙αt
αt
x −λtσt[(1 −ζ)s(x, t) + ζs(x, t|y)]
(66)
= ˙αt
αt
x −λtσtsζ(x, t|y),
(67)
allows one to to construct generative models that sample the tempered distribution pt(xt)pζ
t (y|xt) following classifier guidance
[19]. Note that pt(x)pζ
t (y|x) ∝pζ
t (x|y)p1−ζ
t
(x), so we can also perform classifier free guidance sampling [24]. Empirically,
we observe significant performance boost by applying classifier free guidance, as showed in Tab. 1 and Tab. 9.
4

Model
Training Steps(K)
FID↓
sFID↓
IS↑
Precision↑
Recall↑
SiT-S
400
58.97 / 57.64
8.95 / 9.05
23.34 / 24.78
0.40 / 0.41
0.59 / 0.60
SiT-B
400
34.84 / 33.45
6.59 / 6.46
41.53 / 43.71
0.52 / 0.53
0.64 / 0.63
SiT-L
400
20.01 / 18.79
5.31 / 5.29
67.76 / 72.02
0.62 / 0.64
0.64 / 0.64
SiT-XL
400
18.04 / 17.19
5.17 / 5.07
73.90 / 76.52
0.63 / 0.65
0.64 / 0.63
SiT-XL
7000
9.35 / 8.61
6.38 / 6.32
126.06 / 131.65
0.67 / 0.68
0.68 / 0.67
Table 1. FID-50K scores produced by ODE and SDE. We demonstrate the comparison between ODE and SDE across all of our model
sizes. All statistics are produced without classifier free guidance. Each cell in the table is showing [ODE results] / [SDE results]. We note
the better performances of SDE observed in all model sizes are in line with the bounds given in [3], and that ODE has its advantage in lower
NFE region, as shown in Fig. 5
D. Sampling with ODE and SDE
In the main body of the paper, we used a Heun integrator for solving the ODE in Eq. (2) and an Euler-Maruyama integrator for
solving the SDE in Eq. (4). We summarize all results in Tab. 1, and present the implementations below.
Algorithm 1 Deterministic Heun Sampler
procedure HEUNSAMPLER(vθ(x, t, y), ti∈{0,··· ,N}, αt, σt)
sample x0 ∼N(0, I)
▷Generate initial sample
∆t ←t1 −t0
▷Determine fixed step size
for i ∈{0, · · · , N −1} do
di ←vθ(xi, ti, y)
˜xi+1 ←xi + ∆tdi
▷Euler Step at ti
di+1 ←vθ(˜xi+1, ti+1, y)
xi+1 ←xi + ∆t
2 [di + di+1]
▷Explicit trapezoidal rule at ti+1
end for
return xN
end procedure
Algorithm 2 Stochastic Euler-Maruyama Sampler
procedure EULERSAMPLER(vθ(x, t, y), wt, ti∈{0,··· ,N}, T, αt, σt)
sample x0 ∼N(0, I)
▷Generate initial sample
sθ ←convert from vθ following Appendix A.4 ▷Obtain ∇x log pt(x) in Eq. (4)
∆t ←t1 −t0
▷Determine fixed step size
for i ∈{0, · · · , N −1} do
sample εi ∼N(0, I)
dεi ←εi ∗
√
∆t
di ←vθ(xi, ti, y) + 1
2wtisθ(xi, ti, y)
▷Evaluate drift term at ti
¯xi+1 ←xi + ∆tdi
xi+1 ←¯xi+1 + √wtidεi
▷Evaluate diffusion term at ti
end for
h ←T −tN
▷Last step size; T denotes the time where xT = x∗
d ←vθ(xN, tN, y) + 1
2wtN sθ(xN, tN, y)
x ←xN + h ∗d
▷Last step; output noiseless sample without diffusion
return x
end procedure
It is feasible to use either a velocity model vθ or a score model sθ in applying the above two samplers. If learning the score
5

200K
300K
400K
500K
600K
700K
Training Steps
45
50
55
60
65
70
75
80
FID-50K
SiT-S (ODE)
DiT-S (DDIM)
200K
300K
400K
500K
600K
700K
Training Steps
25
30
35
40
45
50
55
FID-50K
SiT-B (ODE)
DiT-B (DDIM)
200K
300K
400K
500K
600K
700K
Training Steps
10
15
20
25
30
35
FID-50K
SiT-L (ODE)
DiT-L (DDIM)
200K
300K
400K
500K
600K
700K
Training Steps
15
20
25
30
FID-50K
SiT-XL (ODE)
DiT-XL (DDIM)
Figure 1. SiT observes improvement in FID across all model sizes. We show FID-50K over training iterations for both DiT and SiT
models. Across all model sizes, SiT converges faster. We acknowledge this is not directly an apples-to-apples comparison. This is because
DDIM is essentially a discrete form of the first-order Euler’s method, whereas in sampling SiT, we employ the second-order Heun’s method.
Nevertheless, both the SiT and DiT results are produced by a deterministic sampler with a 250 NFE.
for the deterministic Heun sampler, we could always convert the learned sθ to vθ following Appendix A.4. However, as there
exists potential numerical instability (depending on interpolants) in ˙σt, α−1
t
and λt, it’s recommended to learn vθ in sampling
with deterministic sampler instead of sθ. For the stochastic sampler, it’s required to have both vθ and sθ in integration, so we
always need to convert from one (either learning velocity or score) to obtain the other. Under this scenario, the numerical issue
from Appendix A.4 can only be avoided by clipping the time interval near t = 0. Empirically we found clipping the interval
by h = 0.04 and doing a long last step from t = 0.04 to 0 can greatly benefit the performance. A detailed summary of sampler
configuration is provided in Appendix E.
Additionally, we could replace vθ and sθ by vζ
θ and sζ
θ presented in Appendix C as inputs of the two samplers and enjoy the
performance improvements coming along with guidance. As guidance requires evaluating both conditional and unconditional
model output in a single step, it will impose twice the computational cost when sampling.
Comparison between DDPM and Euler-Maruyama
We primarily investigate and report the performance comparison
between DDPM and Euler-Maruyama samplers. We set our Euler sampler’s number of steps to be 250 to match that of DDPM
during evaluation. This comparison is made direct and fair, as the DDPM method is equivalent to a discretized Euler’s method.
Comparison between DDIM and Heun
We also investigate the performance difference produced by deterministic samplers
between DiT and our models. In Fig. 1, we show the FID-50K results for both DiT models sampled with DDIM and SiT
models sampled with Heun. We note that this is not directly an apples-to-apples comparison, as DDIM can be viewed as a
discretized version of the first order Euler’s method, while we use the second order Heun’s method in sampling SiT models,
due to the large discretization error with Euler’s method in continuous time. Nevertheless, we control the NFEs for both DDIM
(250 sampling steps) and Heun (250 NFE).
Higher order solvers
The performances of an adaptive deterministics dopri5 solver and a second order stochastic Heun
Sampler [32] are also tested. For dopri5, we set atol and rtol to 1e-6 and 1e-3, respectively; for Heun, we again
maintain the NFE to be 250 to match that of DDPM. In both solvers we do not observe performance increment; under the CFG
scale of ζ = 1.5, dopri5 and stochastic Heun gives FID-50K of 2.15 and 2.07, respectively.
We also note that our models are compatible with other samplers [36, 44] specifically tuned to diffusion models as well as
sampling distillation [54, 63]. We do not include the evaluations of those methods in our work for the sake of apples-to-apples
comparison with the DDPM model, and we leave the investigation of potential performance improvements to future work.
E. Additional Implementation Details
We implemented our models in JAX following the DiT PyTorch codebase by Peebles and Xie [48]4, and referred to Albergo
et al. [3]5, Song et al. [62]6, and Dockhorn et al. [20]7 for our implementation of the Euler-Maruyama sampler. For the Heun
sampler, we directly used the one from diffrax [33]8, a JAX-based numerical differential equation solver library.
4https://github.com/facebookresearch/DiT
5https://github.com/malbergo/stochastic-interpolants
6https://github.com/yang-song/score sde
7https://github.com/nv-tlabs/CLD-SGM
8https://github.com/patrick-kidger/diffrax
6

Training configurations
We trained all of our models following identical structure and hyperparameters retained from DiT
[48]. We used AdamW [34, 43] as optimizer for all models. We use a constant learning rate of 1 × 10−4 and a batch size
of 256. We used random horizontal flip with probability of 0.5 in data augmentation. We did not tune the learning rates,
decay/warm up schedules, AdamW parameters, nor use any extra data augmentation or gradient clipping during training. Our
largest model, SiT-XL, trains at approximately 6.8 iters/sec on a TPU v4-64 pod following the above configurations. This
speed is slightly faster compared to DiT-XL, which trains at 6.4 iters/sec under identical settings. We also gather the training
speed of other model sizes and summarize them below.
S
B
L
XL
DiT
20.0
19.8
9.3
6.4
SiT
19.7
20.8
9.3
6.8
Table 2. Training speed (iters/sec) across all model sizes. All training speeds are measured on a TPU v4-64 pod. We note that the training
speed is largely influenced by the hardware state.
Sampling configurations
We maintain an exponential moving average (EMA) of all models weights over training with a
decay of 0.9999. All results are sampled from the EMA checkpoints, which is empirically observed to yield better performance.
We summarize the start and end points of our deterministic and stochastic samplers with different interpolants below, where
each t0 and tN are carefully tuned to optimize performance and avoid numerical instability during integration.
Interpolant
Model
Objective
Heun
Euler-Maruyama
t0
tN
t0
tN
SBDM-VP
velocity
Lv
1
1e-5
1
4e-2
score
Lsλ
1
1e-5
1
4e-2
GVP
velocity
Lv
1
0
1
4e-2
score
Ls
1 - 1e-5
0
1 - 1e-3
4e-2
Linear
velocity
Lv
1
0
1
4e-2
score
Ls
1 - 1e-5
0
1 - 1e-3
4e-2
Table 3. Sampler configurations
FID calculation
We calculate FID scores between generated images (10K or 50K) and all available real images in ImageNet
training dataset. We observe small performance variations between TPU-based FID evaluation and GPU-based FID evaluation
(ADM’s TensorFlow evaluation suite [19]9). To ensure consistency with the basline DiT, we sample all of our models on GPU
and obtain FID scores using the ADM evaluation suite.
9https://github.com/openai/guided-diffusion/tree/main/evaluations
7

F. Additional Visual results
Figure 2. Uncurated 512 × 512 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”volcano”(980)
Figure 3. Uncurated 512 × 512 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”arctic fox”(279)
8

Figure 4. Uncurated 512 × 512 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”loggerhead turtle”(33)
Figure 5. Uncurated 512 × 512 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”balloon”(417)
9

Figure 6. Uncurated 512 × 512 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”red panda”(387)
Figure 7. Uncurated 512 × 512 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”geyser”(974)
10

Figure 8. Uncurated 256 × 256 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”macaw”(88)
Figure 9. Uncurated 256 × 256 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”golden retriever”(207)
11

Figure 10. Uncurated 256 × 256 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ” ice cream”(928)
Figure 11. Uncurated 256 × 256 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”cliff”(972)
12

Figure 12. Uncurated 256 × 256 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”husky”(250)
Figure 13. Uncurated 256 × 256 SiT-XL samples.
Classifier-free guidance scale = 4.0
Class label = ”valley”(979)
13

