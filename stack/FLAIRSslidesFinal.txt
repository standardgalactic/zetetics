See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/370838263
A Tutorial on Topological Data Analysis in Natural Language Processing
Presentation · May 2023
CITATIONS
0
READS
447
1 author:
Wlodek Zadrozny
University of North Carolina at Charlotte
126 PUBLICATIONS   1,078 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Wlodek Zadrozny on 17 May 2023.
The user has requested enhancement of the downloaded file.

A Tutorial on
Topological Data Analysis
in
Natural Language Processing
Presented at FLAIRS’36 on May 14 2023
Wlodek Zadrozny 1
1College of Computing and School of Data Science
University of North Carolina at Charlotte
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
1

Topological Data Analysis: The Main Idea
Finding Shapes in Discrete Collections of Points or Vectors
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
2

Topological Data Analysis: The Main Idea
Finding Shapes in Discrete Collections of Points or Vectors
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
3

Topological Data Analysis: The Main Idea
Finding Shapes in Discrete Collections of Points or Vectors
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
4

Agenda
Table: Approximate Time-line
No.
Segment
Time-line (90 minutes)
A
Introduction: TDA background and available software
30 minutes
B
Positioning TDA in NLP
15 minutes
C
Recent applications of TDA methods in NLP (incl. LLM)
30 minutes
D
Limitations, opportunities, and conclusion/discussion
15 minutes
Acknowledgment. This tutorial is partly based on the work of Dr. S.Gholizadeh, and our earlier
tutorials on TDA for text classiﬁcation. It has a (somewhat) diﬀerent focus and covers relevant
work up to May 10 2023.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
5

Introduction:
Objective, TDA background and available software
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
6

Objective: Improving our Understanding of Text
Representations
This tutorial targets text representations and models by adding
topological data analysis
– to produce additional features (e.g. to aid classiﬁcation)
– to improve our analysis (e.g. of large language models)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
7

Before we get to text:
– Let’s look at a big picture,
– get some intuitive understanding of the methods of TDA,
– and see a few code examples
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
8

Topological Data Analysis (TDA) Publications
TDA is a popular research area;
2120 Google Scholar results for 2022
Distribution of Domains in Publications on TDA
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
9

Applications of TDA 1/3 (Static)
Clustering
Dimensionality Reduction
Descriptive modeling: Biology (brain, genes, ...); Social Science
(crime, ...); Chemistry (structure of materials, ...)
Sensor Network Coverage
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
10

Appplications of TDA 2/3 (Dynamic)
Time Series Analysis
Signal Processing (ﬁnancial, mechanical, ...)
Dynamical Systems Analysis
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
11

Applications of TDA 3/3 (Neural)
Brain Structures
e.g. ﬁnding 5 dimensional functional substructures; predicting task
completion based on FMRI data
Artiﬁcial Neural Networks
e.g. predicting the accuracy of a language model on unseen data
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
12

What Is TDA?
TDA: A collection of methods that ﬁnd structure of shapes in data.
What’s the Ancestry of TDA?
Computational Geometry ⇒
Computational Topology ⇒
Topological Data Analysis ■
Common Approach in TDA is to:
(1) Capture the shapes as the main characteristics.
(2) Dismiss the rest as noise or irrelevant information.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
13

Betti Numbers Capture Topological Structure.
The ith Betti number: number of i-dimensional holes a in a shape.
β0: Number of connected
components
β1: Number of 1-D holes
β2: Number of 2-D voids
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
14

Betti Numbers Capture Topological Structure.
The ith Betti number: number of i-dimensional holes a in a shape.
β0: Number of connected
components
β1: Number of 1-D holes
β2: Number of 2-D voids
βs are robust under stretching or shrinking.
βs simplify complex information.
Homology studies βs.
To ﬁnd the structure of shapes:
Capture the shapes as the main characteristics.
Dismiss the rest as noise or irrelevant information.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
14

What Is Persistent Homology?
High dimensional data sets are:
“huge number of discrete points”
⇒There are no continuous shapes!
⇒How to deﬁne/compute β’s?
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
15

What Is Persistent Homology?
Decreasing resolution ⇒Data points get closer to each other.
Any k points that get close
enough ⇒Connect them.
Increasing radius gradually ⇒
Components and Holes
(Loops) appear and disappear.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
16

Persistence Diagram Captures Birth and Death Diameters of Holes.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
17

Persistence Diagram Captures Birth and Death Diameters of Holes.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
17

What Is Persistent Homology?
Demo of Ripser
Demo: Introduction to Ripser 2023.ipynb
The two Python notebooks used in this tutorial are currently available at
https://drive.google.com/drive/folders/1dezYDjm-OIyq0ceUG4Zy3rkuctLwQfry?usp=
share_link
and have been archived at
https://www.researchgate.net/publication/370838204_
FLAIRS2023TutorialCodeShared-20230517T211617Z-001
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
18

Persistence Diagrams Vs. Barcodes
Persistence diagram: Birth and death of holes shown in 2 dimensions.
Barcodes [Collins et al., 2004][Ghrist, 2008]: Birth and death of holes
shown in 1 dimension.
We will look at the numerical value of barcodes in our work.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
19

Persistence Diagrams Vs. Barcodes
Persistence diagram: Birth and death of holes shown in 2 dimensions.
Barcodes [Collins et al., 2004][Ghrist, 2008]: Birth and death of holes
shown in 1 dimension.
The dots further from the diagonal are more informative
We will look at the numerical value of barcodes in our work.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
20

Available Software
Ripser (used in this tutorial)
Bauer, Ulrich. ”Ripser: eﬃcient computation of Vietoris–Rips persistence barcodes.”
Journal of Applied and Computational Topology 5.3 (2021): 391-423.
Tralie C, Saul N, Bar-On R. Ripser.py: A lean persistent homology library for Python.
Journal of Open Source Software. 2018 Sep 13;3(29):925.
https://pypi.org/project/ripser/
https://ripser.scikit-tda.org/en/latest/
R implementation of both Ripser and Cubical Ripser
https://cran.rstudio.com/web/packages/ripserr/index.html (used for some work
in the previous tutorials w. Dr.Gholizadeh).
Ripser (other versions)
Kaji, S, Takeki S, and Kazushi A. ”Cubical Ripser: Software for computing persistent
homology of image and volume data.” https://arxiv.org/pdf/2005.
12692(claimedtobethefastestimplementationofpersistenthomology)
https://github.com/shizuo-kaji/CubicalRipser_3dim
https://github.com/simonzhang00/ripser-plusplus
R implementation of both Ripser and Cubical Ripser
https://cran.rstudio.com/web/packages/ripserr/index.html
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
21

Available Software
Other software:
Mapper (several implementations)
To experiment with several packages: https://github.com/scikit-tda
Giotto tda
https://giotto-ai.github.io/gtda-docs/0.5.1/notebooks/classifying_shapes.html
Repositories of information about TDA software
https://github.com/topics/topological-data-analysis
https://github.com/FatemehTarashi/awesome-tda
https://gist.github.com/calstad/01e174faff2cdca7faf9
https://github.com/tdaverse/tdaverse
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
22

Where are we?
Where are we in this tutorial
Done: Introduction to TDA
NEXT: TDA in NLP
Later: Recent work on TDA in NLP (inlc. LLM)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
23

TDA in Natural Language Processing
Main Questions:
Can topological features capture valuable information?
(by itself, or as an addition to other features)
If so, what exactly is being captured?
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
24

First example: 2013
Diﬀerences between child and adolescent writing
Finding signs of “tie-back” in
documents [Zhu, 2013]
Divide document to a ﬁxed
number of blocks.
SIF/SIFTS – Similarity Filtration
with Time Skeleton
Apply persistent homology on
TF-IDF space of diﬀerent blocks.
Holes in child writings vs. adolescent writings [Zhu, 2013]
Child Writing
Adolescent Writing
Adolescent (truncated)
Holes Existence
87%
100%
98%
Total Holes
3.0 ± 0.2
17.6 ± 0.9
3.9 ± 0.2
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
25

Demo: Using Ripser with Text
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
26

Early History: mixed results
[Wagner et al., 2012] is the ﬁrst mention of computational topology and text.
[Zhu, 2013] is a ﬁrst application. Since then ...
Negative results (examples):
[Michel et al., 2017] : sentiment classiﬁcation on the Cornell Sentence
Polarity corpus and IMDB movie reviews. They conclude:
using persistence diagrams for text representation does not
seem to positively contribute to document clustering and
sentiment classiﬁcation tasks.
They leave open the possibility of topological features contributing to other
NLP tasks such a parsing.
More recent: TDA talks ratings [Das et al., 2021] using doc2vec+ripser
Positive result: (there will be more :-)
[Guan et al., 2016] show that topological features can improve extraction of
multiword expressions and in document summarization.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
27

How informative are topological features for NLP?
Movie genre detection on the IMDB data set of movie plot summaries
[Doshi and Zadrozny, 2018]
[Zhu, 2013]’s methods and software
Features: TDA + genre speciﬁc words (1K words/genre)
Improvement: +4.7% (Jaccard score) over the state of the art
NN/GRU-based results of [Hoang, 2018]
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
28

TDA in NL inference
[Savle et al., 2019] used TDA witf TF/IDF for a text entailment
problem. On the task to identify paragraphs from the list that entail, with
the decision, a base case (legal).
+6.4% from TDA over random forest on COLIEE 2018 data set
(also +17% over prior art).
We don’t know what exactly was captured by persistence! I.e.
why TDA helped.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
29

What about the word order?
— We can use distances between words
— We can convert text data into time series
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
30

(1) Topological Signature of 19th Century Novelists
75 Novels form Gutenberg.org by 6 novelists
Predict author solely based on graph of main characters (=persons).
Average accuracy in binary classiﬁcation: 77%
For each Novel:
Find positions of each character (person) in the novel.
Use Stanford CoreNLP APIs →named entity recognizer (NER)
Find entities tagged as PERSON.
Save place that they appeared (Token Indices).
Use only 10 most frequent (important?) characters (persons).
Measure the distance between character A and character B.
Using Persistence Diagrams w. Ripser in R [Gholizadeh et al., 2018]
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
31

(2) Topological Signature of 19th Century Novelists
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
32

(2) Topological Signature of 19th Century Novelists
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
32

(2)Topological Signature of 19th Century Novelists
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
32

(3) Topological Signature of 19th Century Novelists
Predicting the author
Binary Classiﬁcation (balanced sub-samples)
Using a 5-NN algorithm
Using Wasserstein distance of persistence diagrams
250 times 10-fold cross validation
60’000 total predictions
Evaluations (Accuracy)
Dickens
Zola
Dostoyevsky
Austen
Twain
Scott
(17)
(18)
(8)
(6)
(8)
(18)
C. Dickens
-
87.0
72.2
100.0
74.6
73.9
´E. Zola
87.0
-
65.0
64.2
68.8
83.3
Dostoyevsky
72.2
65.0
-
90.2
73.3
55.8
J. Austen
100.0
64.2
90.2
-
82.9
94.7
M. Twain
74.6
68.8
73.3
82.9
-
68.5
W. Scott
73.9
83.3
55.8
94.7
68.5
-
Average
81.5
73.7
71.3
86.4
73.6
75.2
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
33

TDA with Word Embeddings
Borrowing Ideas from Time-Series Analysis
Consider text as n-dimensional time-series of n entities.
Designing Order-Preserving Text Processing
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
34

Word Embedding Representation of a Document
Using word embedding with D = 300 dimensions,
→a document with T words: < Word1, Word2, · · · , WordT >
→can be represented by:
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
35

TDA with Word Embeddings
Borrowing Ideas from Time-Series Analysis
Consider text as n-dimensional time-series of n entities.
Designing Order-Preserving Text Processing
Details in [Gholizadeh et al., 2020, Gholizadeh, 2020]
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
36

Evaluation on Wikipedia Movie Plots from Kaggle
Used the topological features with XGBoost to predict the genres (the
four major).
Used BiLSTM to benchmark the results.
Classiﬁer
Pre.
Rec.
F1
Acc.
1
BiLSTM
68.0
59.7
0.608
76.2
2
XGBoost on TP1
59.6
53.2
0.560
71.1
3
BiLSTM + XGBoost on TP1
67.8
64.8
0.656
77.3
TP1: Topological features from Word embeddings.
Similar results on classiﬁcation of 4k arxiv papers
Details: [Gholizadeh et al., 2020])
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
37

Two techniques from [Tymochko et al., 2021]: time series embeddings and
persistent images
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
38

Identifying fraudulent papers from their titles and abstracts
[Tymochko et al., 2021]
— 9 dimensional time series representing word frequencies
— Persistence images, derived from persistence diagrams, used for
classiﬁcation
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
39

Partial Summary
‘Topological Features’ from text can carry useful information
TDA features from TF-IDF space (movie genres, inference, ...)
TDA features from word embeddings space (movie plot summaries, arxiv
articles, ...)
TDA features based on text structures (novels)
TDA features based on times series embeddings (fraud detection )
TDA features carry exclusive information that is not reﬂected in
conventional features.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
40

Where are we?
Where are we in this tutorial
Done: Introduction to TDA
Done: TDA in NLP
NEXT: Recent work on TDA in NLP (inlc. LLM)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
41

Current Directions
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
42

Before we get to Current Directions: LLMs
Obvious question: With GPT-4 and other LLMs, is there
a place for TDA in NLP?
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
43

Before we get to Current Directions: LLMs
As of April 2023 the number of research articles about GPT-[ ] exceeds number of articles
about TDA (my estimate)
”In spite of the potential for ground-breaking achievements oﬀered by large language
models (LLMs) (e.g., GPT-3), they still lag signiﬁcantly behind fully-supervised baselines
e.g., ﬁne-tuned BERT) in [TASK]”
Important points to keep in mind about GPT-4:
GPT-4 weights are not available for inspection
The number of research articles about GPT-4 published in 2023 (until May 10) is over
1,000.
Many (most?) report large gains in performance on NLP tasks compared to GPT-3.5.
Nevertheless, GPT-4 has limitations (depth of reasoning, counting, knowledge)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
44

Understanding LLMs with Mapper
Mapper is an important TDA algorithm. It projects data points into a
graph representation on a low dimensional space but then maps back to
ﬁnd substructures in the original space. [Singh et al., 2007]. There are
several implementations and variants. Some helpful links:
https://github.com/scikit-tda/kepler-mapper
https://www.quantmetry.com/blog/topological-data-analysis-with-mapper/
Figure: Source:[Imoto and Hiraoka, 2023]
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
45

TopoBERT: Understanding LLMs w. Mapper
”TopoBERT: Exploring the topology
of ﬁne-tuned word representations”
Insight 1. Fine-tuning changes the topo-
logical structures of embeddings in higher
layers more than in lower layers
Insight 3.
The average topological
neighborhood purity is correlated with
model performance on unseen data.
[Rathore et al., 2023]
https://github.com/
tdavislab/TopoBERT (May 2023, Information
Visualization, 1–23)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
46

LLMs: Adding topological features to improve performance
— ”TDA-based classiﬁers boost the performance of pre-trained language models.”
— ” TDA-based classiﬁers have advantages over LM ﬁne-tuning because they are more
interpretable and help to introspect the inner workings of LMs”
— ”we ﬁnd a distinct pattern that is frequently present in the attention maps of unacceptable
sentences in English and Russian”
On CoLA, with Ripser, w. BERT and RoBERTa (April 2023) [Proskurina et al., 2023]
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
47

Earlier work showing the value of TDA for LLMs
”We empirically show that the [TDA] features derived from the BERT model outperform count-
and neural-based baselines up to 10% on three common datasets” [Kushnareva et al., 2021]
https://github.com/danchern97/tda4atd
”Topological features enhance the BERT-based acceptability classiﬁer scores by 8%-24% on
CoLA in three languages (English, Italian, and Swedish). By revealing the topological
discrepancy between attention maps of minimal pairs, we achieve the human-level performance
on the BLiMP benchmark, outperforming nine statistical and Transformer LM
baselines.”[Cherniavskii et al., 2022] BERT,RoBERTa + Ripser(++)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
48

Dialogue and TDA w. LLMs
”the ﬁrst application of topological features in dialogue term extraction”
[Vukovic et al., 2022] Ripser+RoBERTa
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
49

Other related work: Topology of word embeddings
Looking at embeddings or documents as high dimensional manifolds
(ﬁgure from [Jakubowski et al., 2020]; but see also [Fitz, 2022]).
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
50

More in Current Directions (2021-2023)
Example works and their topics:
Improve Accuracy of Deep Learning Models for Fake News Detection
Trained on Very Small Training Sets [Deng and Duzhin, 2022]
Word Sense Disambiguation [Rawson et al., 2022, Temˇcinas, 2018]
Comparisons and visualizations of words, sentences and relations
[Meirom and Bobrowski, 2022]
Surprisingly little about summarization, TDA and LLMs
[Haghighatkhah et al., 2022].
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
51

Inference. An opportunity?
Example works and their topics:
Topological Analysis of Contradictions in Text[Wu et al., 2022] : ”We have
seen the improvement for each model with the TDA representation in almost
all the genres in terms of accuracy and precision, with a little sacriﬁce of
recall.” No followup on [Savle et al., 2019], yet.
Attempt to model circular reasoning [Tymochko et al., 2020]
Enhancing very large LLMs?
Pro: It works for classiﬁcation w. BERT; GPT-4’s knowledge and
understanding are both limited; TDA has been applied to dialogue and long
text.
Con: GPT-4 is really good, and with proper prompting many complex tasks
have been accomplished. In principle, NN can ﬁnd topological features of
any data (universality)
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
52

What I would like to know
Mapping back from topology to text is an unsolved
problem. What are the reasons for TDA-based improvements?
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
53

Questions
Time for Questions
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
54

Bibliography I
Cherniavskii, D., Tulchinskii, E., Mikhailov, V., Proskurina, I., Kushnareva, L., Artemova, E., Barannikov, S.,
Piontkovskaya, I., Piontkovski, D., and Burnaev, E. (2022).
Acceptability judgements via examining the topology of attention maps.
In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 88–107, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
Collins, A., Zomorodian, A., Carlsson, G., and Guibas, L. J. (2004).
A barcode shape descriptor for curve point cloud data.
Computers & Graphics, 28(6):881–894.
Das, S., Haque, S. A., Tanveer, M., et al. (2021).
Persistence homology of tedtalk: Do sentence embeddings have a topological shape?
arXiv preprint arXiv:2103.14131.
Deng, R. and Duzhin, F. (2022).
Topological data analysis helps to improve accuracy of deep learning models for fake news detection trained on very small
training sets.
Big Data and Cognitive Computing, 6(3):74.
Doshi, P. and Zadrozny, W. (2018).
Movie genre detection using topological data analysis.
In International Conference on Statistical Language and Speech Processing, pages 117–128. Springer.
Fitz, S. (2022).
The shape of words-topological structure in natural language data.
In Topological, Algebraic and Geometric Learning Workshops 2022, pages 116–123. PMLR.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
55

Bibliography II
Gholizadeh, S. (2020).
Topological data analysis in text processing.
PhD thesis, The University of North Carolina at Charlotte.
Gholizadeh, S., Seyeditabari, A., and Zadrozny, W. (2018).
Topological signature of 19th century novelists: Persistent homology in text mining.
big data and cognitive computing, 2(4):33.
Gholizadeh, S., Seyeditabari, A., and Zadrozny, W. (2020).
A novel method of extracting topological features from word embeddings.
arXiv preprint arXiv:2003.13074.
Ghrist, R. (2008).
Barcodes: the persistent topology of data.
Bulletin of the American Mathematical Society, 45(1):61–75.
Guan, H., Tang, W., Krim, H., Keiser, J., Rindos, A., and Sazdanovic, R. (2016).
A topological collapse for document summarization.
In Signal Processing Advances in Wireless Communications (SPAWC), 2016 IEEE 17th International Workshop on, pages
1–5. IEEE.
Haghighatkhah, P., Fokkens, A., Sommerauer, P., Speckmann, B., and Verbeek, K. (2022).
Story trees: Representing documents using topological persistence.
In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2413–2429.
Hoang, Q. (2018).
Predicting movie genres based on plot summaries.
arXiv preprint arXiv:1801.04813.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
56

Bibliography III
Imoto, Y. and Hiraoka, Y. (2023).
V-mapper: topological data analysis for high-dimensional data with velocity.
Nonlinear Theory and Its Applications, IEICE, 14(2):92–105.
Jakubowski, A., Gaˇsi´c, M., and Zibrowius, M. (2020).
Topology of word embeddings: Singularities reﬂect polysemy.
arXiv preprint arXiv:2011.09413.
Kushnareva, L., Cherniavskii, D., Mikhailov, V., Artemova, E., Barannikov, S., Bernstein, A., Piontkovskaya, I.,
Piontkovski, D., and Burnaev, E. (2021).
Artiﬁcial text detection via examining the topology of attention maps.
arXiv preprint arXiv:2109.04825.
Meirom, S. H. and Bobrowski, O. (2022).
Unsupervised geometric and topological approaches for cross-lingual sentence representation and comparison.
In Proceedings of the 7th Workshop on Representation Learning for NLP, pages 173–183.
Michel, P., Ravichander, A., and Rijhwani, S. (2017).
Does the geometry of word embeddings help document classiﬁcation? a case study on persistent homology based
representations.
arXiv preprint arXiv:1705.10900.
Proskurina, I., Piontkovskaya, I., and Artemova, E. (2023).
Can bert eat rucola? topological data analysis to explain.
arXiv preprint arXiv:2304.01680.
Rathore, A., Zhou, Y., Srikumar, V., and Wang, B. (2023).
Topobert: Exploring the topology of ﬁne-tuned word representations.
Information Visualization, page 14738716231168671.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
57

Bibliography IV
Rawson, M., Dooley, S., Bharadwaj, M., and Choudhary, R. (2022).
Topological data analysis for word sense disambiguation.
Savle, K., Zadrozny, W., and Lee, M. (2019).
Topological data analysis for discourse semantics?
In Proceedings of the 13th International Conference on Computational Semantics-Student Papers, pages 34–43.
Singh, G., M´emoli, F., Carlsson, G. E., et al. (2007).
Topological methods for the analysis of high dimensional data sets and 3d object recognition.
PBG@ Eurographics, 2:091–100.
Temˇcinas, T. (2018).
Local homology of word embeddings.
arXiv preprint arXiv:1810.10136.
Tymochko, S., Chaput, J., Doster, T., Purvine, E., Warley, J., and Emerson, T. (2021).
Con connections: Detecting fraud from abstracts using topological data analysis.
In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), pages 403–408. IEEE.
Tymochko, S., New, Z., Bynum, L., Purvine, E., Doster, T., Chaput, J., and Emerson, T. (2020).
Argumentative topology: Finding loop (holes) in logic.
arXiv preprint arXiv:2011.08952.
Vukovic, R., Heck, M., Ruppik, B., van Niekerk, C., Zibrowius, M., and Gasic, M. (2022).
Dialogue term extraction using transfer learning and topological data analysis.
In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 564–581.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
58

Bibliography V
Wagner, H., D lotko, P., and Mrozek, M. (2012).
Computational topology in text mining.
In Computational Topology in Image Context, pages 68–78. Springer.
Wu, X., Niu, X., and Rahman, R. (2022).
Topological analysis of contradictions in text.
In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 2478–2483.
Zhu, X. (2013).
Persistent homology: An introduction and a new text representation for natural language processing.
In IJCAI, pages 1953–1959.
W.Zadrozny
A Tutorial on TDA in NLP
May 2023
59
View publication stats

