HeMIS:
Hetero-Modal Image Segmentation ⋆
Mohammad Havaei12, Nicolas Guizard1, Nicolas Chapados1, and
Yoshua Bengio3
1 Imagia Inc., Montreal, Qc, Canada
2 Universit´e de Sherbrooke, Qc, Canada
3 Universit´e de Montr´eal, Montr´eal, Canada
{mohammad.havaei,nicolas.guizard,nicolas.chapados}@imagia.com
seyed.mohammad.havaei@usherbrooke.ca
yoshua.bengio@umontreal.ca
http://www.imagia.com
Abstract. We introduce a deep learning image segmentation frame-
work that is extremely robust to missing imaging modalities. Instead of
attempting to impute or synthesize missing data, the proposed approach
learns, for each modality, an embedding of the input image into a single
latent vector space for which arithmetic operations (such as taking the
mean) are well deﬁned. Points in that space, which are averaged over
modalities available at inference time, can then be further processed to
yield the desired segmentation. As such, any combinatorial subset of
available modalities can be provided as input, without having to learn
a combinatorial number of imputation models. Evaluated on two neuro-
logical MRI datasets (brain tumors and MS lesions), the approach yields
state-of-the-art segmentation results when provided with all modalities;
moreover, its performance degrades remarkably gracefully when modal-
ities are removed, signiﬁcantly more so than alternative mean-ﬁlling or
other synthesis approaches.
Keywords: Segmentation, multi-modal, deep learning, convolutional neu-
ral networks, data abstraction, data imputation
1
Introduction
In medical image analysis, image segmentation is an important task and is pri-
mordial to visualize and quantify the severity of the pathology in clinical prac-
tice. Multi-modality imaging provides complementary information to discrim-
inate speciﬁc tissues, anatomies and pathologies. However, manual segmenta-
tion is long, painstaking and subject to human variability. In the last decades,
numerous automatic approaches have been developed to speed up medical im-
age segmentation. These methods can be grouped into two categories: The ﬁrst,
multi-atlas approaches estimate on-line intensity similarities between the subject
⋆Accepted as oral presentation at MICCAI 2016
arXiv:1607.05194v1  [cs.CV]  18 Jul 2016

2
Havaei, Guizard, Chapados, Bengio
being segmented and multi-atlases (images with expert labels). These techniques
have shown excellent results in structural segmentation when using non-linear
registration [11]; when combined with non-local approaches they have proven ef-
fective in segmenting diﬀuse and sparse pathologies (ie. multiple sclerosis (MS)
lesions [7]) as well as more complex multi-label pathology (ie. Glioblastoma [4]).
In contrast, model-based approaches are typically trained oﬄine to identify a dis-
criminative model of image intensity features. These features can be predeﬁned
by the user (e.g. with random forests [5]) or extracted and learned hierarchically
directly from the images [2].
Both strategies are typically optimized for a speciﬁc set of multi-modal im-
ages and usually require these modalities to be available. In clinical settings,
image acquisition and patient artifacts, among other hurdles, make it diﬃcult
to fully exploit all the modalities; as such, it is common to have one or more
modalities to be missing for a given instance. This problem is not new, and the
subject of missing data analysis has spawned an immense literature in statistics
(e.g. [22]). In medical imaging, a number of approaches have been proposed,
some of which require to re-train a speciﬁc model with the missing modalities
or to synthesize them [9]. Synthesis can improve multi-modal classiﬁcation by
adding information of the missing modalities in the context of a simple clas-
siﬁer such as random forests [20]. Approaches to imitate with fewer features a
classiﬁer trained with a complete set of features have also been proposed [10].
Nevertheless, it should stand to reason that a more complex model should be
capable of extracting relevant features from just the available modalities without
relying on artiﬁcial intermediate steps such as imputation or synthesis.
This paper proposes a deep learning framework (HeMIS) that can segment
medical images from incomplete multi-modal datasets. Deep learning [6] has
shown an increasing popularity in medical image processing for segmenting but
also to synthesize missing modalities [20]. Here, the proposed approach learns,
separately for each modality, an embedding of the input image into a latent
space. In this space, arithmetic operations (such as computing ﬁrst and second
moments of a collection of vectors) are well deﬁned and can be taken over the
diﬀerent modalities available at inference time. These computed moments can
then be further processed to estimate the ﬁnal segmentation. This approach
presents the advantage of being robust to any combinatorial subset of available
modalities provided as input, without the need to learn a combinatorial number
of imputation models. We start by describing the method (§2), follow with a
description of the datasets (§3) and experiments (§4) and ﬁnally oﬀer concluding
remarks (§5).
2
Method
2.1
Hetero-Modal Image Segmentation
Typical convolutional neural network (CNN) architectures take a multiplane
image as input and process it through a sequence of convolutional layers (followed
by nonlinearities such as ReLU(·) ≡max(0, ·)), alternating with optional pooling
layers, to yield a per-pixel or per-image output [6]. In such networks every input

Hetero-Modal Image Segmentation
3
Fig. 1. Illustration of the Hetero-Modal Image Segmentation architecture. Modalities
available at inference time, Mk, are provided to independent modality-speciﬁc con-
volutional layers in the back end. Feature maps statistics (ﬁrst & second moments)
are computed in the abstraction layer, which after concatenation are processed by
further convolutional layers in the front end, yielding pixelwise classiﬁcations outputs.
plane is assumed to be present within a given instance: since the very ﬁrst
convolutional layer mixes input values coming from all planes, any missing plane
introduces a bias in the computation that the network is not equipped to deal
with.
We propose an approach wherein each modality is initially processed by its
own convolutional pipeline, independently of all others. After a few independent
stages, feature maps from all available modalities are merged by computing map-
wise statistics such as the mean and the variance, quantities whose expectation
does not depend on the number of terms (i.e. modalities) that are provided. Af-
ter merging, the mean and variance feature maps are concatenated and fed into
a ﬁnal set of convolutional stages to obtain network output. This is illustrated
in Fig. 1. In this procedure, each modality contributes a separate term to the
mean and variance; in contrast to a vanilla CNN architecture, a missing modality
does not throw this computation oﬀ: the mean and variance terms simply be-
come estimated with larger uncertainty. In seeking to be robust to any subset of
missing modalities, we call this approach hetero-modal rather than multi-modal,
recognizing that in addition to taking advantage of several modalities, it can
take advantage of a diverse, instance-varying, set of modalities. In particular, it
does not require that a “least common denominator” modality be present for
every instance, as sometimes needed by common imputation methods.
Let k ∈K ⊆{1, . . . , N} denote a modality within the set of available modal-
ities for a given instance, and Mk represent the image of the k-th modality. For

4
Havaei, Guizard, Chapados, Bengio
simplicity, in this work we assume 2D data (e.g. a single slice of a tomographic
image), but it can be extended in an obvious way to full 3D sections. As shown
on Fig. 1, HeMIS proceeds in three stages:
1. Back End: In our implementation, this consists of two convolutional layers
with ReLU, the second followed with a (2, 2) max-pooling layer, denoted re-
spectively C(1)
k
and C(2)
k . To ensure that the output layer consists of the same
number of pixels as the input image, the convolutions are zero-padded and the
stride for all operations (including max-pooling) is 1. In particular, pooling with
a stride of 1 does not downsample, but simply “thickens” the feature maps; this
is found to add some robustness to the results. The number of feature maps in
each layer is given in Fig. 1. Let C(j)
k,ℓbe the the ℓ-th feature map of C(j)
k .
2. Abstraction Layer: Modality fusion is computed here, as ﬁrst and second
moments across available modalities in C(2), separately for each feature map ℓ,
bEℓ
h
C(2)i
=
1
|K|
X
k∈K
C(2)
k,ℓ
and
d
Varℓ
h
C(2)i
=
1
|K| −1
X
k∈K

C(2)
k,ℓ−bEℓ
h
C(2)i2
,
with d
Varℓ[C(2)] deﬁned to be zero if |K| = 1 (a single available modality).
3. Front End: Finally the front end combines the merged modalities to produce
the ﬁnal model output. In our implementation, we concatenate all bE

C(2)
and
d
Var

C(2)
feature maps, pass them through a convolutional layer C(3) with
ReLU activation, to ﬁnish with a ﬁnal layer C(4) that has as many feature
maps as there are target segmentation classes. The pixelwise posterior class
probabilities are given by applying a softmax function across the C(4) feature
maps, and a full image segmentation is obtained by taking the pixelwise most
likely posterior class. No further postprocessing on the resulting segment classes
(such as smoothing) is done.
Pseudo-Curriculum Training Procedure
To carry out segmentation eﬃ-
ciently, the model is trained fully convolutionnally to minimize a pixelwise class
cross-entropy loss, in the spirit of [12]. It has long been known that noise in-
jection during training is a powerful technique to make neural networks more
robust, as shown among others with denoising autoencoders [23], and dropout
and related procedures [17]. Here, we make the HeMIS architecture robust to
missing modalities by randomly dropping any number for a given training ex-
ample. Inspired by previous works on curriculum learning [1]—where the model
starts learning from easy scenarios before turning to more diﬃcult ones—we used
a pseudo-curriculum learning scheme where after few warmup epochs where all
modalities are shown to the model, we start randomly dropping modalities, en-
suring a higher probability of dropping zero or one modality only.
Interpretation as an Embedding
An embedding is a mapping from an
arbitrary source space to a target real-valued vector space of ﬁxed dimension-
ality. In recent years, embeddings have been shown to yield unexpectedly pow-
erful representations for a wide array of data types, including single words [14],
variable-length word sequences and images [24], and more.

Hetero-Modal Image Segmentation
5
In the context of HeMIS, the back end can be interpreted as learning to sep-
arately map each modality into an embedding common to all modalities, within
which vector algebra operations carry well-deﬁned semantics. As such, comput-
ing empirical moments to carry out modality fusion is sensible. Since the model
is trained entirely end-to-end with backpropagation, the key aspect of the ar-
chitecture is that this embedding only needs be deﬁned implicitly as that which
minimizes the overall training loss. Cross-modality interactions can be captured
within speciﬁc embedding dimensions, as long as there are a suﬃcient number of
them (i.e. enough feature maps within C(2)), as they can be combined by C(3).
With this interpretation, the back end consists of a modular assembly of
operators, viewed as reusable building blocks that may or may not be needed for
a given instance, each computing the embedding from its own input modality.
These projections are summarized in the abstraction layer (with a mean and
variance, although additional summary statistics are simple to entertain), and
this summary further processed in the front end to yield ﬁnal model output.
3
Data and Implementation details
We studied the HeMIS framework on two neurological pathologies: Multiple
Sclerosis (MS) with the MS Grand Challenge (MSGC) and a large Relapsing
Remitting MS (RRMS) cohort, as well as glioma with the Brain Tumor Segmen-
tation (BRATS) dataset [13].
MS MSGC: The MSGC dataset [18] provides 20 training MR cases with manual
ground truth lesion segmentation and 23 testing cases from the Boston Children’s
Hospital (CHB) and the University of North Carolina (UNC). We downloaded4
the co-registered T1W, T2W, FLAIR images for all 43 cases as well as the ground
truth lesion mask images for the 20 training cases. While lesions masks for the
23 testing cases are not available for download, an automated system is available
to evaluate the output of a given segmentation algorithm.
RRMS: This dataset is obtained from a multi-site clinical study with 300
relapsing-remitting MS (RRMS) patients (mean age 37.5 yrs, SD 10.0 yrs). Each
patient underwent an MRI that included sagittal T1W , T2W and T1 post-
contrast (T1C) images. The MRI data were acquired on 1.5T scanners from
diﬀerent manufacturers (GE, Philips and Siemens).
BRATS The BRATS-2015 dataset contains 220 subjects with high grade and
54 subjects with low grade tumors. Each subject contains four MR modalities
(FLAIR, T1W, T1C and T2) and comes with a voxel level segmentation ground
truth of 5 labels: healthy, necrosis, edema, non-enhancing tumor and enhancing
tumor. As done in [13], we transform each segmentation map into 3 binary maps
which correspond to 3 tumor categories, namely; Complete (which contains all
tumor classes), Core (which contains all tumor subclasses except “edema”) and
Enhancing (which includes the “enhanced tumor” subclass). For each binary
map, the Dice Similarity Coeﬃcient (DSC) is calculated [13].
BRATS-2013 contains two test datasets; Challenge and Leaderboard. The
Challenge dataset contains 10 subjects with high grade tumors while the Leader-
4 http://www.nitrc.org/projects/msseg/

6
Havaei, Guizard, Chapados, Bengio
board dataset contains 15 subjects with high grade tumors and 10 subject with
low grade tumors. There are no ground truth provided for these datasets and
thus quantitative evaluation can be achieved via an online evaluation system [13].
In our experiments we used Challenge and Leaderboard datasets to compare the
HeMIS segmentation performance to the state-of-the-art, when trained on all
modalities.
Pre-processing and implementation details
Before being provided to the
network, bias ﬁeld correction [15] and intensity normalization with a zero mean,
truncation of 0.001 quantile and unit variance is applied to the image intensity.
The multi-modal images are co-registered to the T1W and interpolated to 1mm
isotropic resolution.
We used Keras library [3] for our implementation. To deal with class im-
balance, we adopt the patch-wise training procedure mentioned in [8]. We ﬁrst
train the model with a balanced dataset which allows learning features that are
agnostic to the class distribution. In a second phase, we train only the ﬁnal
classiﬁcation layer with a distribution close to the ground truth. This ensures
that we learn good features yet keep the correct class priors. The method was
trained using an Nvidia TitanX GPU, with a stochastic gradient learning rate
of 0.001, decay constant of 0.0001 and Nesterov momentum coeﬃcient of 0.9
[19]. For both BRATS-2015 and MS, we split the dataset into three separate
subsets—train, valid and test—with ratios of 70%, 10% and 20% respectively.
To avoid over-ﬁtting we used early stopping on the validation set.
4
Experiments and Results
We ﬁrst validate HeMIS performance against state-of-the-art segmentation meth-
ods on the two challenge datasets: MSGC and BRATS. Since the test data and
the ranking table for BRATS 2015 are not available, we submitted results to
BRATS 2013 challenge and leaderboard. These results are presented in Table
1.5 As we observe, HeMIS outperforms Tustison et al. [21], the winner of the
BRATS 2013 challenge, on most tumor region categories.
The MSGC dataset illustrates a direct application of HeMIS ﬂexibility as only
three modalities (T1W, T2W and FLAIR) are provided for a small training set.
Therefore, given the small number of subjects, we ﬁrst trained HeMIS on RRMS
dataset with four modalities and ﬁne-tuned on MSGC. Our results were submit-
ted to the MSGC website6, with a resuts summary appearing in Table 2. The
MSGC segmentation results include three other supervised approaches; when
compared to them, HeMIS obtains highly competitive results with a combined
score of 83.2%, where 90.0% would represent human performance given inter-
rater variability.
5 Note
that
the
results
mentioned
in
Table
1
are
from
methods
compet-
ing in the BRATS 2013 challenge for which a static table is provided at
https://www.virtualskeleton.ch/BraTS/StaticResults2013. Since then, other meth-
ods have been added to the scoreboard but for which no reference is available.
6 http://www.ia.unc.edu/MSseg

Hetero-Modal Image Segmentation
7
Table 1. Comparison of HeMIS when trained on all modalities against BRATS-2013
Leaderboard and Challenge winners, in terms of Dice Similarity (scores from [13]).
Leaderboard
Method
Complete Core
Enhancing
Tustison [21] 79
65
53
Zhao [25]
79
59
47
Meier [13]
72
60
53
HeMIS
83
67
57
Challenge
Complete Core
Enhancing
87
78
74
84
70
65
82
73
69
88
75
74
Fig. 2. MLP-imputed FLAIR for an MS patient. The ﬁgure shows from left to right
the original modality and the predicted FLAIR given other modalities.
Table 2. Results of the full dataset training on the MSGC. For each rater (CHB and
UNC), we provide the volume diﬀerence (VD), surface distance (SD), true positive rate
(TPR), false positive rate (FPR) and the method’s score as in [18].
Method
Rater
VD (%)
SD (mm) TPR (%) FPR (%) Score
CHB
86.4
8.4
58.2
70.6
80.0
Souplet et al. [16]UNC
57.9
7.5
49.1
76.3
CHB
52.4
5.4
59.0
71.5
82.1
Geremia et al. [5]UNC
45.0
5.7
51.2
76.7
CHB
63.5
7.4
47.1
52.7
84.0
Brosch et al. [2] UNC
52.0
6.4
56.0
49.8
CHB
127.4
7.5
66.1
55.3
83.2
HeMIS
UNC
68.2
6.6
52.3
61.3
The main advantage of HeMIS lies in its ability to deal with missing modal-
ities, speciﬁcally when diﬀerent subjects are missing diﬀerent modalities. To
illustrate the model’s ﬂexibility in such circumstances, we compare HeMIS per-
formance to two common approaches to deal with random missing modalities.
The ﬁrst, mean-ﬁlling, is to replace a missing modality by the modality’s mean
value. In our case since all means are zero by construction, replacing a missing
modality by zeros can be viewed as imputing with the mean. The second ap-
proach is to train a multi-layer perceptron (MLP) to predict the expected value

8
Havaei, Guizard, Chapados, Bengio
Table 3. Dice similarity coeﬃcient (DSC) results on the RRMS and BRATS test sets
(%) when modalities are dropped. The table shows the DSC for all possible conﬁgura-
tions of MRI modalities being either absent (◦) or present (•), in order of FLAIR (F),
T1W (T1), T1C (T1c), T2W (T2). Results are reported for HeMIS, Mean (mean-ﬁlling)
and the imputation MLP (MLP).
RRMS
BRATS
Modalities
Lesion
Complete
Core
Enhancing
F T1 T1c T2 HeMIS Mean
MLP
HeMIS Mean
MLP
HeMIS Mean
MLP
HeMIS Mean
MLP
◦
◦
◦
• 1.74
2.66
12.77
58.48
2.70
61.50
40.18 4.00
37.32
20.31 6.25
18.62
◦
◦
•
◦2.67
0.00
3.51
33.46 23.11
2.04
44.55 23.90
17.70
49.93 30.02
32.92
◦
•
◦
◦3.89
0.00
6.64
33.22 0.00
2.07
17.42 0.00
10.52
4.67
6.25
10.78
•
◦
◦
◦34.48
9.77
38.46
71.26
72.30 63.81
37.45 0.00
34.26
5.57
6.25
15.90
◦
◦
•
• 27.52 4.31
25.83
67.59 35.01
64.97
63.39 30.92
49.38
65.38 39.00
60.30
◦
•
•
◦8.21
0.00
8.26
45.93 23.63
1.99
55.06 41.89
26.55
62.40 43.80
40.93
•
•
◦
◦38.81
11.62
39.15
80.28 75.58
78.13
49.52 0.00
48.97
22.26
6.25
25.18
◦
•
◦
• 31.25 8.31
29.39
69.56 1.77
66.88
47.26 2.63
43.66
23.56
6.25
26.37
•
◦
◦
• 39.64 33.31
38.55
82.1
81.01
81.35
53.42 25.94
52.41
23.19
6.25
25.01
•
◦
•
◦41.38 6.42
39.33
79.8
45.97
81.13
66.12 29.85
65.51
67.12 35.14
66.19
•
•
•
◦41.97 9.00
40.63
80.88
81.57
82.19
69.26
62.13
69.34
71.30 67.13
70.93
•
•
◦
• 46.6
41.12
41.83
83.87 77.84
80.40
57.76 20.66
53.46
28.46 6.25
28.34
•
◦
•
• 41.90 38.95
41.47
82.78
64.19
83.37
70.62 42.36
70.45
70.52
49.62
70.56
◦
•
•
• 34.98 5.78
29.46
70.98 30.86
67.85
66.60 45.79
55.40
67.84 50.21
64.81
•
•
•
• 48.66 43.48
43.48
83.15 82.43
82.43
72.5
71.46
71.46
75.37 72.08
72.08
# Wins / 15
9
0
6
10
1
4
14
0
1
9
0
6
of speciﬁc missing modality given the available ones. Since neural networks are
generally trained for a unique task, we need to train 28 diﬀerent MLPs (one
for each ◦in Table 3 for a given dataset) to account for diﬀerent possibilities
of missing modalities. We used the same MLP architecture for all these models,
which consists of 2 hidden layers with 100 hidden units each, trained to minimize
the mean squared error. Fig. 2 shows an example of predicted modalities for an
MS patient.
Table 3 shows the DSC for this experiment on the test set. On the BRATS
dataset, for the Core category, HeMIS achieves the best segmentation in al-
most all cases (14 out of 15) and for the Complete and Enhancing categories
it leads in most cases (10 and 9 cases out of 15 respectively). Also, the mean-
ﬁlling approach hardly outperforms HeMIS or MLP-imputation. These results
are consistent with the MS lesion segmentation dataset, where HeMIS outper-
forms other imputation approaches in 9 out of 15 cases. In scenarios where only
one or two modalities are missing, while both HeMIS and MLP-imputation ob-
tain good results, HeMIS outperforms the latter in most cases on both datasets.
On BRATS, when missing 3 out of 4 modalities, HeMIS outperforms the MLP
in a majority of cases. Moreover, whereas the HeMIS performance only grad-
ually drops as additional modalities become missing, the performance drop for
MLP-imputation and mean-ﬁlling is much more severe. On the RRMS cohort,
the MLP-imputation appears to obtain slightly better segmentations when only
one modality is available.

Hetero-Modal Image Segmentation
9
Fig. 3. Example of HeMIS segmentation results on BRATS and MS subjects for dif-
ferent combinations of input modalities. For both cohorts, an axial FLAIR slice of a
subject is overlaid with the results where for BRATS (ﬁrst row) the segmentation col-
ors describe necrosis (blue), non-enhancing (yellow), active core (orange) and edema
(green). For the MS case, the lesions are highlighted in red. The columns present the
results for diﬀerent combinations of input modalities, with ground truth in the last
column.
Although it is expected that tumor sub-label segmentations should be less
accurate with fewer modalities, we should still hope for the model to report a
sensible characterization of the tumor “footprint”. While MLP and mean-ﬁlling
fail in this respect, HeMIS quite well achieves this goal by outperforming in
almost all cases of the Complete and Core tumor categories. This can also be
seen in Fig. 3 where we show how adding modalities to HeMIS improves its ability
to achieve a more accurate segmentation. From Table 3, we can also infer that the
FLAIR modality is the most relevant for identifying the Complete tumor while
T1C is the most relevant for identifying Core and Enhancing tumor categories.
On the RRMS dataset, HeMIS results are also seen to degrade slower than the
other imputation approaches, preserving good segmentation when modalities go
missing. Indeed, as seen in Fig. 3, even though with FLAIR alone HeMIS already
produces good segmentations, it is capable of further reﬁning its results when
adding modalities, by removing false positives and improving outlines of the
correctly identiﬁed lesions or tumor.
5
Conclusion
We have proposed a new fully automatic segmentation framework for heteroge-
nous multi-modal MRI using a specialized convolutional deep neural network.
The embedding of the multi-modal CNN back-end allows to train and segment
datasets with missing modalities. We carried out an extensive validation on MS
and glioma and achieved state-of-the art segmentation results on two challeng-
ing neurological pathology image processing tasks. Importantly, we contrasted
the graceful performance degradation of the proposed approach as modalities go

10
Havaei, Guizard, Chapados, Bengio
missing, compared with other popular imputation approaches, which it achieves
without requiring training speciﬁc models for every potential missing modal-
ity combination. Future work should concentrate on extending the approach to
broader modalities outside of MRI, such as CT, PET and ultrasound.
References
1. Bengio, Y., Louradour, J., Collobert, R., Weston, J.: Curriculum learning. In: Pro-
ceedings of the 26th annual international conference on machine learning. pp. 41–
48. ACM (2009)
2. Brosch, T., Yoo, Y., Tang, L.Y.W., Li, D.K.B., Traboulsee, A., Tam, R.: MICCAI
Proceedings, chap. ”Deep Convolutional Encoder Networks for Multiple Sclerosis
Lesion Segmentation”, pp. 3–11. Springer (2015)
3. Chollet, F.: Keras (2015)
4. Cordier, N., Delingette, H., Ayache, N.: A patch-based approach for the segmenta-
tion of pathologies: Application to glioma labelling. IEEE TMI PP(99), 1–1 (2016)
5. Geremia, E., Menze, B.H., Ayache, N.: Spatially adaptive random forests pp. 1344–
1347 (2013)
6. Goodfellow, I., Bengio, Y., Courville, A.: Deep learning (2016), book in preparation
for MIT Press
7. Guizard, N., Coup´e, P., Fonov, V.S., Manj´on, J.V., Arnold, D.L., Collins, D.L.:
Rotation-invariant multi-contrast non-local means for ms lesion segmentation. Neu-
roImage: Clinical 8, 376–389 (2015)
8. Havaei, M., Davy, A., Warde-Farley, D., Biard, A., Courville, A., Bengio, Y., Pal,
C., Jodoin, P.M., Larochelle, H.: Brain tumor segmentation with deep neural net-
works. arXiv preprint arXiv:1505.03540 (2015)
9. Hofmann, M., Steinke, F., Scheel, V., Charpiat, G., et al.: MRI-based attenuation
correction for PET/MRI: a novel approach combining pattern recognition and atlas
registration. Journal of Nuclear Medicine 49(11), 1875–1883 (2008)
10. Hor, S., Moradi, M.: Scandent tree: A random forest learning method for incom-
plete multimodal datasets. In: MICCAI 2015, pp. 694–701. Springer (2015)
11. Iglesias, J.E., Sabuncu, M.R.: Multi-atlas segmentation of biomedical images: A
survey. Medical image analysis 24(1), 205–219 (2015)
12. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE CVPRConference on Computer Vision
and Pattern Recognition. pp. 3431–3440 (2015)
13. Menze, B., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.e.a.:
The multimodal brain tumor image segmentation benchmark (BRATS). IEEE TMI
34(10), 1993–2024 (Oct 2015)
14. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed represen-
tations of words and phrases and their compositionality. In: NIPS. pp. 3111–3119
(2013)
15. Sled, J.G., Zijdenbos, A.P., Evans, A.C.: A nonparametric method for automatic
correction of intensity nonuniformity in MRI data. IEEE TMI 17(1), 87–97 (1998)
16. Souplet, J., Lebrun, C., Ayache, N., Malandain, G.: An automatic segmentation
of T2-FLAIR multiple sclerosis lesions (07 2008)
17. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: A simple way to prevent neural networks from overﬁtting. J.The Journal
of Mach.ine Learninging Researchearch 15(1), 1929–1958 (2014)

Hetero-Modal Image Segmentation
11
18. Styner, M., Lee, J., Chin, B., Chin, M., Commowick, O., Tran, H., Markovic-Plese,
S., Jewells, V., Warﬁeld, S.: 3D segmentation in the clinic: A grand challenge ii:
Ms lesion segmentation. MIDAS 2008, 1–6 (2008)
19. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization
and momentum in deep learning. In: ICML-13. pp. 1139–1147 (2013)
20. Tulder, G., Bruijne, M.: MICCAI Proceedings, chap. Why Does Synthesized Data
Improve Multi-sequence Classiﬁcation?, pp. 531–538. Springer (2015)
21. Tustison, N.J., Shrinidhi, K., Wintermark, M., Durst, C.R., Kandel, B.M., Gee,
J.C., Grossman, M.C., Avants, B.B.: Optimal symmetric multimodal templates
and concatenated random forests for supervised brain tumor segmentation (sim-
pliﬁed) with ANTsR. Neuroinformatics 13(2), 209–225 (2015)
22. Van Buuren, S.: Flexible imputation of missing data. CRC press (2012)
23. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and com-
posing robust features with denoising autoencoders. In: Proc.eedings of the 25th
int’ernational conf.erence on mMachine learning. pp. 1096–1103. ACM (2008)
24. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,
Bengio, Yoshuaand Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show,
attend and tell: Neural image caption generation with visual attention. In: Blei,
D., Bach, F. (eds.) Proc. Proceedings of the 32nd International Conference on
Machine Learning (ICML-15). pp. ”2048–2057”. JMLR Workshop and Conference
Proceedings (2015)
25. Zhao, L., Wu, W., Corso, J.J.: MICCAI Proceedings, chap. Semi-automatic Brain
Tumor Segmentation by Constrained MRFs Using Structural Trajectories, pp. 567–
575. Springer (2013)

