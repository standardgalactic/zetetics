Foundation 
Mathematics 
for Computer 
Science
John Vince
A Visual Approach

Foundation Mathematics for Computer Science

John Vince
Foundation Mathematics
for Computer Science
A Visual Approach
123

Professor Emeritus John Vince, M.Tech, Ph.D., D.Sc., C.Eng., FBCS
Bournemouth University
Bournemouth
UK
ISBN 978-3-319-21436-8
ISBN 978-3-319-21437-5
(eBook)
DOI 10.1007/978-3-319-21437-5
Library of Congress Control Number: 2015943835
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)

This book is dedicated to my wife, Heidi

Preface
This book is an introductory text and aimed at students studying for an under-
graduate degree in Computer Science. The chapters cover number systems, algebra,
logic, trigonometry, coordinate systems, determinants, vectors, matrices, geometric
matrix transforms, differential and integral calculus, which should provide the
reader with a solid foundation, upon which more advanced topics of mathematics
can be studied.
Computer science is a very large subject, and graduates are able to pursue a wide
variety of careers, including programming, systems design, cryptography, website
design, real-time systems, computer animation, computer games, data visualisation,
etc. Consequently, it is virtually impossible to write a mathematics book that caters
for all of these potential career paths. Nevertheless, I have attempted to describe a
range of mathematical topics that I believe are relevant, and have helped me during
my own career in computer science. The book’s subtitle “A Visual Approach”
reﬂects the importance I place on coloured illustrations and function graphs, of
which there are over 140. Each chapter contains a variety of worked examples.
Throughout the book I have referenced the key people behind the various
mathematical discoveries covered, which I hope adds a human dimension to the
subject. I have found it very interesting and entertaining to discover how some
mathematicians ridiculed their fellow peers, when they could not comprehend the
signiﬁcance of a new invention—Cantor’s Set Theory, being an excellent example.
There is no way I could have written this book without the assistance of the
Internet and my books previously published by Springer Verlag. In particular,
I would like to acknowledge Wikipedia and Mathematics: From the Birth of
Numbers by Jan Gullberg. I prepared this book on an Apple iMac, using LATEX 2ε,
Pages and the Grapher package, and would recommend this combination to anyone
considering writing a book on mathematics. I do hope you enjoy reading this book,
and that you are tempted to study mathematics to a deeper level.
Ashtead
John Vince
May 2015
vii

Contents
1
Visual Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Visual Brains Versus Analytic Brains . . . . . . . . . . . . . . . . .
1
1.2
Learning Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
What Makes Mathematics Difficult? . . . . . . . . . . . . . . . . . .
2
1.4
Does Mathematics Exist Outside Our Brains? . . . . . . . . . . . .
3
1.5
Symbols and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Sets of Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.3
Zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.4
Negative Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.4.1
The Arithmetic of Positive and Negative
Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.5
Observations and Axioms. . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.5.1
Commutative Law . . . . . . . . . . . . . . . . . . . . . . .
9
2.5.2
Associative Law . . . . . . . . . . . . . . . . . . . . . . . .
10
2.5.3
Distributive Law . . . . . . . . . . . . . . . . . . . . . . . .
10
2.6
The Base of a Number System . . . . . . . . . . . . . . . . . . . . . .
11
2.6.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.6.2
Octal Numbers . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.6.3
Binary Numbers . . . . . . . . . . . . . . . . . . . . . . . .
12
2.6.4
Hexadecimal Numbers . . . . . . . . . . . . . . . . . . . .
13
2.6.5
Adding Binary Numbers. . . . . . . . . . . . . . . . . . .
17
2.6.6
Subtracting Binary Numbers . . . . . . . . . . . . . . . .
18
2.7
Types of Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.7.1
Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . .
18
2.7.2
Integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.7.3
Rational Numbers . . . . . . . . . . . . . . . . . . . . . . .
19
2.7.4
Irrational Numbers. . . . . . . . . . . . . . . . . . . . . . .
19
2.7.5
Real Numbers . . . . . . . . . . . . . . . . . . . . . . . . . .
20
ix

2.7.6
Algebraic and Transcendental Numbers . . . . . . . .
20
2.7.7
Imaginary Numbers . . . . . . . . . . . . . . . . . . . . . .
20
2.7.8
Complex Numbers . . . . . . . . . . . . . . . . . . . . . . .
24
2.7.9
Quaternions and Octonions . . . . . . . . . . . . . . . . .
27
2.8
Prime Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.8.1
The Fundamental Theorem of Arithmetic . . . . . . .
30
2.8.2
Is 1 a Prime?. . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.8.3
Prime Number Distribution . . . . . . . . . . . . . . . . .
31
2.8.4
Perfect Numbers . . . . . . . . . . . . . . . . . . . . . . . .
32
2.8.5
Mersenne Numbers . . . . . . . . . . . . . . . . . . . . . .
32
2.8.6
Transcendental and Algebraic Numbers . . . . . . . .
32
2.8.7
Infinity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.9
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.9.1
Algebraic Expansion . . . . . . . . . . . . . . . . . . . . .
34
2.9.2
Binary Subtraction . . . . . . . . . . . . . . . . . . . . . . .
35
2.9.3
Complex Numbers . . . . . . . . . . . . . . . . . . . . . . .
35
2.9.4
Complex Rotation . . . . . . . . . . . . . . . . . . . . . . .
35
2.9.5
Quaternions. . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3
Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.2
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.2.1
Solving the Roots of a Quadratic Equation . . . . . .
39
3.3
Indices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.3.1
Laws of Indices. . . . . . . . . . . . . . . . . . . . . . . . .
44
3.4
Logarithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.5
Further Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.6
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.6.1
Explicit and Implicit Equations . . . . . . . . . . . . . .
47
3.6.2
Function Notation . . . . . . . . . . . . . . . . . . . . . . .
47
3.6.3
Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.6.4
Function Domains and Ranges . . . . . . . . . . . . . .
49
3.6.5
Odd and Even Functions . . . . . . . . . . . . . . . . . .
50
3.6.6
Power Functions . . . . . . . . . . . . . . . . . . . . . . . .
52
3.7
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.7.1
Algebraic Manipulation . . . . . . . . . . . . . . . . . . .
52
3.7.2
Solving a Quadratic Equation . . . . . . . . . . . . . . .
53
3.7.3
Factorising . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4
Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.2
Truth Tables. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.2.1
Logical Connectives. . . . . . . . . . . . . . . . . . . . . .
58
x
Contents

4.3
Logical Premises. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.3.1
Material Equivalence . . . . . . . . . . . . . . . . . . . . .
59
4.3.2
Implication . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.3.3
Negation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.3.4
Conjunction . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.3.5
Inclusive Disjunction . . . . . . . . . . . . . . . . . . . . .
61
4.3.6
Exclusive Disjunction. . . . . . . . . . . . . . . . . . . . .
61
4.3.7
Idempotence . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.3.8
Commutativity. . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.3.9
Associativity . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.3.10
Distributivity. . . . . . . . . . . . . . . . . . . . . . . . . . .
64
4.3.11
de Morgan’s Laws . . . . . . . . . . . . . . . . . . . . . . .
64
4.3.12
Simplification . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.3.13
Excluded Middle . . . . . . . . . . . . . . . . . . . . . . . .
66
4.3.14
Contradiction . . . . . . . . . . . . . . . . . . . . . . . . . .
66
4.3.15
Double Negation . . . . . . . . . . . . . . . . . . . . . . . .
67
4.3.16
Implication and Equivalence . . . . . . . . . . . . . . . .
67
4.3.17
Exportation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3.18
Contrapositive . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3.19
Reductio Ad Absurdum . . . . . . . . . . . . . . . . . . .
68
4.3.20
Modus Ponens. . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.3.21
Proof by Cases . . . . . . . . . . . . . . . . . . . . . . . . .
70
4.4
Set Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.4.1
Empty Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4.4.2
Membership and Cardinality of a Set . . . . . . . . . .
72
4.4.3
Subsets, Supersets and the Universal Set. . . . . . . .
73
4.4.4
Set Building . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.4.5
Union. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.4.6
Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.4.7
Relative Complement . . . . . . . . . . . . . . . . . . . . .
75
4.4.8
Absolute Complement . . . . . . . . . . . . . . . . . . . .
76
4.4.9
Power Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.5
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.5.1
Truth Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.5.2
Set Building . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.5.3
Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.5.4
Power Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5
Trigonometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.2
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.3
Units of Angular Measurement . . . . . . . . . . . . . . . . . . . . . .
81
5.4
The Trigonometric Ratios. . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.4.1
Domains and Ranges . . . . . . . . . . . . . . . . . . . . .
85
Contents
xi

5.5
Inverse Trigonometric Ratios . . . . . . . . . . . . . . . . . . . . . . .
85
5.6
Trigonometric Identities . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.7
The Sine Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
5.8
The Cosine Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
5.9
Compound-Angle Identities . . . . . . . . . . . . . . . . . . . . . . . .
89
5.9.1
Double-Angle Identities . . . . . . . . . . . . . . . . . . .
91
5.9.2
Multiple-Angle Identities . . . . . . . . . . . . . . . . . .
91
5.9.3
Half-Angle Identities . . . . . . . . . . . . . . . . . . . . .
92
5.10
Perimeter Relationships . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
6
Coordinate Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.2
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.3
The Cartesian Plane. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
6.4
Function Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
6.5
Shape Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.5.1
2D Polygons . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.5.2
Areas of Shapes . . . . . . . . . . . . . . . . . . . . . . . .
98
6.6
Theorem of Pythagoras in 2D. . . . . . . . . . . . . . . . . . . . . . .
99
6.7
3D Cartesian Coordinates. . . . . . . . . . . . . . . . . . . . . . . . . .
99
6.7.1
Theorem of Pythagoras in 3D . . . . . . . . . . . . . . .
100
6.8
Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
6.9
Spherical Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . . .
102
6.10
Cylindrical Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
6.11
Barycentric Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
6.12
Homogeneous Coordinates . . . . . . . . . . . . . . . . . . . . . . . . .
104
6.13
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
6.13.1
Area of a Shape. . . . . . . . . . . . . . . . . . . . . . . . .
105
6.13.2
Distance Between Two Points . . . . . . . . . . . . . . .
105
6.13.3
Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . .
105
6.13.4
Spherical Polar Coordinates . . . . . . . . . . . . . . . .
106
6.13.5
Cylindrical Coordinates . . . . . . . . . . . . . . . . . . .
107
6.13.6
Barycentric Coordinates . . . . . . . . . . . . . . . . . . .
107
7
Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
7.2
Linear Equations with Two Variables . . . . . . . . . . . . . . . . .
110
7.3
Linear Equations with Three Variables. . . . . . . . . . . . . . . . .
113
7.3.1
Sarrus’s Rule. . . . . . . . . . . . . . . . . . . . . . . . . . .
120
7.4
Mathematical Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
7.4.1
Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
7.4.2
Order of a Determinant. . . . . . . . . . . . . . . . . . . .
121
7.4.3
Value of a Determinant . . . . . . . . . . . . . . . . . . .
121
7.4.4
Properties of Determinants . . . . . . . . . . . . . . . . .
122
xii
Contents

7.5
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
7.5.1
Determinant Expansion. . . . . . . . . . . . . . . . . . . .
123
7.5.2
Complex Determinant. . . . . . . . . . . . . . . . . . . . .
124
7.5.3
Simple Expansion . . . . . . . . . . . . . . . . . . . . . . .
124
7.5.4
Simultaneous Equations . . . . . . . . . . . . . . . . . . .
125
8
Vectors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
8.2
2D Vectors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
8.2.1
Vector Notation. . . . . . . . . . . . . . . . . . . . . . . . .
128
8.2.2
Graphical Representation of Vectors. . . . . . . . . . .
129
8.2.3
Magnitude of a Vector . . . . . . . . . . . . . . . . . . . .
130
8.3
3D Vectors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
8.3.1
Vector Manipulation . . . . . . . . . . . . . . . . . . . . .
132
8.3.2
Scaling a Vector . . . . . . . . . . . . . . . . . . . . . . . .
132
8.3.3
Vector Addition and Subtraction . . . . . . . . . . . . .
132
8.3.4
Position Vectors . . . . . . . . . . . . . . . . . . . . . . . .
133
8.3.5
Unit Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
8.3.6
Cartesian Vectors. . . . . . . . . . . . . . . . . . . . . . . .
135
8.3.7
Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
8.3.8
Scalar Product. . . . . . . . . . . . . . . . . . . . . . . . . .
136
8.3.9
The Vector Product . . . . . . . . . . . . . . . . . . . . . .
137
8.3.10
The Right-Hand Rule . . . . . . . . . . . . . . . . . . . . .
142
8.4
Deriving a Unit Normal Vector for a Triangle . . . . . . . . . . .
142
8.5
Surface Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
8.5.1
Calculating 2D Areas . . . . . . . . . . . . . . . . . . . . .
144
8.6
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
8.6.1
Position Vector . . . . . . . . . . . . . . . . . . . . . . . . .
145
8.6.2
Unit Vector. . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
8.6.3
Vector Magnitude . . . . . . . . . . . . . . . . . . . . . . .
145
8.6.4
Angle Between Two Vectors. . . . . . . . . . . . . . . .
146
8.6.5
Vector Product . . . . . . . . . . . . . . . . . . . . . . . . .
146
9
Matrices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
9.2
Geometric Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
9.3
Transforms and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . .
151
9.4
Matrix Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
9.4.1
Matrix Dimension or Order. . . . . . . . . . . . . . . . .
154
9.4.2
Square Matrix . . . . . . . . . . . . . . . . . . . . . . . . . .
154
9.4.3
Column Vector . . . . . . . . . . . . . . . . . . . . . . . . .
155
9.4.4
Row Vector . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
9.4.5
Null Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
9.4.6
Unit Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Contents
xiii

9.4.7
Trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
9.4.8
Determinant of a Matrix . . . . . . . . . . . . . . . . . . .
157
9.4.9
Transpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
9.4.10
Symmetric Matrix . . . . . . . . . . . . . . . . . . . . . . .
159
9.4.11
Antisymmetric Matrix . . . . . . . . . . . . . . . . . . . .
160
9.5
Matrix Addition and Subtraction . . . . . . . . . . . . . . . . . . . . .
162
9.5.1
Scalar Multiplication . . . . . . . . . . . . . . . . . . . . .
163
9.6
Matrix Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
9.6.1
Row and Column Vectors. . . . . . . . . . . . . . . . . .
163
9.6.2
Row Vector and a Matrix . . . . . . . . . . . . . . . . . .
164
9.6.3
Matrix and a Column Vector. . . . . . . . . . . . . . . .
165
9.6.4
Square Matrices. . . . . . . . . . . . . . . . . . . . . . . . .
166
9.6.5
Rectangular Matrices . . . . . . . . . . . . . . . . . . . . .
167
9.7
Inverse Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
9.7.1
Inverting a Pair of Matrices. . . . . . . . . . . . . . . . .
174
9.8
Orthogonal Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
9.9
Diagonal Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
9.10
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.10.1
Matrix Inversion . . . . . . . . . . . . . . . . . . . . . . . .
177
9.10.2
Identity Matrix . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.10.3
Solving Two Equations Using Matrices . . . . . . . .
178
9.10.4
Solving Three Equations Using Matrices . . . . . . .
179
9.10.5
Solving Two Complex Equations. . . . . . . . . . . . .
180
9.10.6
Solving Three Complex Equations . . . . . . . . . . . .
181
9.10.7
Solving Two Complex Equations. . . . . . . . . . . . .
182
9.10.8
Solving Three Complex Equations . . . . . . . . . . . .
182
10
Geometric Matrix Transforms. . . . . . . . . . . . . . . . . . . . . . . . . . .
185
10.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
10.2
Matrix Transforms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
10.2.1
2D Translation . . . . . . . . . . . . . . . . . . . . . . . . .
186
10.2.2
2D Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
10.2.3
2D Reflections . . . . . . . . . . . . . . . . . . . . . . . . .
189
10.2.4
2D Shearing . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
10.2.5
2D Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
10.2.6
2D Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
10.2.7
2D Reflection . . . . . . . . . . . . . . . . . . . . . . . . . .
195
10.2.8
2D Rotation About an Arbitrary Point . . . . . . . . .
195
10.3
3D Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
10.3.1
3D Translation . . . . . . . . . . . . . . . . . . . . . . . . .
196
10.3.2
3D Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
10.3.3
3D Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
10.3.4
Rotating About an Axis . . . . . . . . . . . . . . . . . . .
201
10.3.5
3D Reflections . . . . . . . . . . . . . . . . . . . . . . . . .
202
xiv
Contents

10.4
Rotating a Point About an Arbitrary Axis. . . . . . . . . . . . . . .
202
10.4.1
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
10.5
Determinant of a Transform . . . . . . . . . . . . . . . . . . . . . . . .
206
10.6
Perspective Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
10.7
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
10.7.1
2D Scale and Translate. . . . . . . . . . . . . . . . . . . .
209
10.7.2
2D Rotation . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
10.7.3
Determinant of the Rotate Transform . . . . . . . . . .
211
10.7.4
Determinant of the Shear Transform. . . . . . . . . . .
211
10.7.5
Yaw, Pitch and Roll Transforms . . . . . . . . . . . . .
211
10.7.6
Rotation About an Arbitrary Axis . . . . . . . . . . . .
212
10.7.7
3D Rotation Transform Matrix . . . . . . . . . . . . . .
213
10.7.8
Perspective Projection . . . . . . . . . . . . . . . . . . . .
213
11
Calculus: Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
11.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
11.2
Small Numerical Quantities . . . . . . . . . . . . . . . . . . . . . . . .
215
11.3
Equations and Limits. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
11.3.1
Quadratic Function. . . . . . . . . . . . . . . . . . . . . . .
217
11.3.2
Cubic Equation . . . . . . . . . . . . . . . . . . . . . . . . .
218
11.3.3
Functions and Limits . . . . . . . . . . . . . . . . . . . . .
220
11.3.4
Graphical Interpretation of the Derivative . . . . . . .
222
11.3.5
Derivatives and Differentials . . . . . . . . . . . . . . . .
223
11.3.6
Integration and Antiderivatives . . . . . . . . . . . . . .
224
11.4
Function Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
11.5
Differentiating Groups of Functions. . . . . . . . . . . . . . . . . . .
226
11.5.1
Sums of Functions . . . . . . . . . . . . . . . . . . . . . . .
226
11.5.2
Function of a Function . . . . . . . . . . . . . . . . . . . .
228
11.5.3
Function Products . . . . . . . . . . . . . . . . . . . . . . .
232
11.5.4
Function Quotients. . . . . . . . . . . . . . . . . . . . . . .
235
11.6
Differentiating Implicit Functions . . . . . . . . . . . . . . . . . . . .
237
11.7
Differentiating Exponential and Logarithmic Functions . . . . .
240
11.7.1
Exponential Functions . . . . . . . . . . . . . . . . . . . .
240
11.7.2
Logarithmic Functions . . . . . . . . . . . . . . . . . . . .
242
11.8
Differentiating Trigonometric Functions . . . . . . . . . . . . . . . .
244
11.8.1
Differentiating Tan. . . . . . . . . . . . . . . . . . . . . . .
244
11.8.2
Differentiating Csc. . . . . . . . . . . . . . . . . . . . . . .
245
11.8.3
Differentiating Sec . . . . . . . . . . . . . . . . . . . . . . .
246
11.8.4
Differentiating Cot . . . . . . . . . . . . . . . . . . . . . . .
247
11.8.5
Differentiating Arcsin, Arccos and Arctan. . . . . . .
248
11.8.6
Differentiating Arccsc, Arcsec and Arccot. . . . . . .
249
11.9
Differentiating Hyperbolic Functions . . . . . . . . . . . . . . . . . .
249
11.9.1
Differentiating Sinh, Cosh and Tanh . . . . . . . . . .
251
Contents
xv

11.10
Higher Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
11.11
Higher Derivatives of a Polynomial. . . . . . . . . . . . . . . . . . .
252
11.12
Identifying a Local Maximum or Minimum . . . . . . . . . . . . .
255
11.13
Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
11.13.1
Visualising Partial Derivatives. . . . . . . . . . . . . . .
259
11.13.2
Mixed Partial Derivatives . . . . . . . . . . . . . . . . . .
262
11.14
Chain Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
11.15
Total Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
11.16
Power Series. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
266
11.17
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
11.17.1
Antiderivative 1. . . . . . . . . . . . . . . . . . . . . . . . .
271
11.17.2
Antiderivative 2. . . . . . . . . . . . . . . . . . . . . . . . .
272
11.17.3
Differentiating Sums of Functions . . . . . . . . . . . .
272
11.17.4
Differentiating a Function Product . . . . . . . . . . . .
272
11.17.5
Differentiating an Implicit Function . . . . . . . . . . .
273
11.17.6
Differentiating a General Implicit Function . . . . . .
273
11.17.7
Local Maximum or Minimum . . . . . . . . . . . . . . .
274
11.17.8
Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . .
275
11.17.9
Mixed Partial Derivative 1 . . . . . . . . . . . . . . . . .
275
11.17.10
Mixed Partial Derivative 2 . . . . . . . . . . . . . . . . .
276
11.17.11
Total Derivative. . . . . . . . . . . . . . . . . . . . . . . . .
276
12
Calculus: Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
12.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
12.2
Indefinite Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
12.3
Integration Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . .
278
12.3.1
Continuous Functions. . . . . . . . . . . . . . . . . . . . .
278
12.3.2
Difficult Functions. . . . . . . . . . . . . . . . . . . . . . .
279
12.4
Trigonometric Identities . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
12.4.1
Exponent Notation . . . . . . . . . . . . . . . . . . . . . . .
282
12.4.2
Completing the Square . . . . . . . . . . . . . . . . . . . .
283
12.4.3
The Integrand Contains a Derivative . . . . . . . . . .
284
12.4.4
Converting the Integrand into a Series
of Fractions. . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
12.4.5
Integration by Parts . . . . . . . . . . . . . . . . . . . . . .
287
12.4.6
Integration by Substitution . . . . . . . . . . . . . . . . .
292
12.4.7
Partial Fractions. . . . . . . . . . . . . . . . . . . . . . . . .
294
12.5
Area Under a Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
12.6
Calculating Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
12.7
Positive and Negative Areas . . . . . . . . . . . . . . . . . . . . . . . .
305
12.8
Area Between Two Functions. . . . . . . . . . . . . . . . . . . . . . .
307
12.9
Areas with the Y-Axis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
12.10
Area with Parametric Functions . . . . . . . . . . . . . . . . . . . . .
310
12.11
The Riemann Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
xvi
Contents

12.12
Worked Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
12.12.1
Integrating a Function Containing Its Own
Derivative. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
12.12.2
Dividing an Integral into Several Integrals . . . . . .
314
12.12.3
Integrating by Parts 1. . . . . . . . . . . . . . . . . . . . .
315
12.12.4
Integrating by Parts 2. . . . . . . . . . . . . . . . . . . . .
316
12.12.5
Integrating by Substitution 1 . . . . . . . . . . . . . . . .
317
12.12.6
Integrating by Substitution 2 . . . . . . . . . . . . . . . .
318
12.12.7
Integrating by Substitution 3 . . . . . . . . . . . . . . . .
319
12.12.8
Integrating with Partial Fractions . . . . . . . . . . . . .
319
Appendix A: Limit of (Sin θ)/θ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
Appendix B: Integrating Cosnθ . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
Contents
xvii

Chapter 1
Visual Mathematics
1.1 Visual Brains Versus Analytic Brains
I consider myself a “visual” person, as pictures help me understand complex
problems. I also don’t ﬁnd it too difﬁcult to visualise objects from different view
points. I remember learning about electrons, neutrons and protons for the ﬁrst time,
where our planetary system provided a simple model to visualise the hidden structure
of matter. My mental image of electrons was one of small orange spheres, spinning
around a small, central nucleus containing blue protons and grey neutrons. I had still
to discover the mechanism of colour, and although this visual model was seriously
ﬂawed, it provided a ﬁrst step towards understanding the structure of matter.
As my knowledge of mathematics grew, this too, was image based. Equations
were curves and surfaces, simultaneous equations were intersecting or parallel lines,
etc., and when I embarked upon computer science, I found a natural application
for mathematics. For me, mathematics is a visual science, although I do appreciate
that many professional mathematicians need only a formal, symbolic notation for
constructing their world. Such people do not require visual scaffolding—they seem
to be able to manipulate abstract mathematical concepts at a symbolic level. Their
books do not require illustrations or diagrams—Greek symbols, upside-down and
back-to-front Latin fonts are sufﬁcient to annotate their ideas.
Today, when reading popular science books on quantum theory, I still try to
form images of 3D waves of energy and probability oscillating in space—to no
avail—and I have accepted that human knowledge of such phenomena is best left
to a mathematical description. Nevertheless, mathematicians such as Roger Penrose
know the importance of visual models in communicating complex mathematical
ideas. His book The Road to Reality: A Complete Guide to the Laws of the Universe
is decorated with beautiful, informative, hand-drawn illustrations, which help readers
understand the mathematics of science. In this book, I too, rely heavily on images
to communicate an idea. They are simple, and are the ﬁrst step on a ladder towards
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_1
1

2
1
Visual Mathematics
understanding a difﬁcult idea. Eventually, when that “Eureka!” moment arrives,
that moment when “I understand what you are saying,” the image becomes closely
associated with the mathematical notation.
1.2 Learning Mathematics
I was fortunate in my studies in that I was taught by people interested in mathematics,
and their interest rubbed off on me. I feel sorry for children who have given up
on mathematics, simply because they are being taught by teachers whose primary
subject is not mathematics. I was never too concerned about the uses of mathematics,
although applied mathematics is of special interest.
One of the problems with mathematics is its incredible breadth and depth. It
embraces everything from 2D geometry, calculus, topology, statistics, complex func-
tions to number theory and propositional calculus. All of these subjects can be stud-
ied superﬁcially or to a mind-numbing complexity. Fortunately, no one is required
to understand everything, which is why mathematicians tend to specialise in one or
two areas and develop a specialist knowledge.
1.3 What Makes Mathematics Difﬁcult?
“What makes mathematics difﬁcult?” is also a difﬁcult question to answer, but one
that has to be asked and answered. There are many answers to this question, and
I believe that problems begin with mathematical notation and how to read it; how
to analyse a problem and express a solution using mathematical statements. Unlike
learning a foreign language—which I ﬁnd very difﬁcult—mathematics is a language
that needs to be learned by discovering facts and building upon them to discover new
facts. Consequently, a good memory is always an advantage, as well as a sense of
logic.
Mathematics can be difﬁcult for anyone, including mathematicians. For example,
when the idea of √−1 was originally proposed, it was criticised and looked down
upon by mathematicians, mainly because its purpose was not fully understood. Even-
tually, it transformed the entire mathematical landscape, including physics. Similarly,
when the German mathematician Georg Cantor (1845–1919), published his papers
on set theory and transﬁnite sets, some mathematicians hounded him in a disgraceful
manner. The German mathematician Leopold Kronecker (1823–1891), called Cantor
a “scientiﬁc charlatan”, a “renegade”, and a “corrupter of youth”, and did everything
to hinder Cantor’s academic career. Similarly, the French mathematician and physi-
cist Henri Poincaré (1854–1912), called Cantor’s ideas a “grave disease”, whilst the
Austrian-British philosopher and logician Ludwig Wittgenstein (1889–1951) com-
plained that mathematics is “ridden through and through with the pernicious idioms
of set theory.” How wrong they all were. Today, set theory is a major branch of math-

1.3 What Makes Mathematics Difﬁcult?
3
ematics and has found its way into every math curriculum. So don’t be surprised to
discover that some mathematical ideas are initially difﬁcult to understand—you are
in good company.
1.4 Does Mathematics Exist Outside Our Brains?
Many people have considered the question: “What is mathematics?” Some math-
ematicians and philosophers argue that numbers and mathematical formulae have
some sort of external existence and are waiting to be discovered by us. Personally,
I don’t accept this idea. I believe that we enjoy searching for patterns and structure
in anything that ﬁnds its way into our brains, which is why we love poetry, music,
storytelling, art, singing, architecture, science, as well as mathematics. The piano,
for example, is an instrument for playing music using different patterns of notes.
When the piano was invented—a few hundred years ago—the music of Chopin,
Liszt and Rachmaninoff did not exist in any form—it had to be composed by them.
Similarly, by building a system for counting using numbers, we have an amazing tool
for composing mathematical systems that help us measure quantity, structure, space
and change. Such systems have been applied to topics such as ﬂuid dynamics, opti-
misation, statistics, cryptography, game theory probability theory, and many more.
I will attempt to develop this same idea by showing how the concept of number,
and the visual representation of number reveals all sorts of patterns, that give rise to
number systems, algebra, trigonometry, geometry, analytic geometry and calculus.
The universe does not need any of these mathematical ideas to run its machinery, but
we need these ideas to understand its operation.
1.5 Symbols and Notation
One of the reasons why many people ﬁnd mathematics inaccessible is due to its
symbols and notation. Let’s look at symbols ﬁrst. The English alphabet possesses a
reasonable range of familiar character shapes:
a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z
A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z
which ﬁnd there way into every branch of mathematics and physics, and permit us
to write equations such as
e = mc2
and
A = πr2.

4
1
Visual Mathematics
It is important that when we see an equation, we are able to read it as part of the text.
In the case of e = mc2, this is read as “e equals m, c squared”, where e stands for
energy, m for mass, and c the speed of light. In the case of A = πr2, this is read as “A
equals pi, r squared”, where A stands for area, π the ratio of a circle’s circumference
to its diameter, and r the circle’s radius. Greek symbols, which happen to look nice
and impressive, have also found their way into many equations, and often disrupt the
ﬂow of reading, simply because we don’t know their English names. For example,
the English theoretical physicist Paul Dirac (1902–1984) derived an equation for a
moving electron which the symbols αi and β, which are 4 × 4 matrices, where
αiβ + βαi = 0
and is read as
“the sum of the products alpha-i beta, and beta alpha-i, equals zero.”
Although we will not come across moving electrons in this book, we will have to be
familiar with the following Greek symbols:
α
alpha
ν
nu
β
beta
ξ
xi
γ
gamma
o
o
δ
delta
π
pi
ϵ
epsilon
ρ
rho
ζ
zeta
σ
sigma
η
eta
τ
tau
θ
theta
υ
upsilon
ι
iota
φ
phi
κ
kappa
χ
chi
λ
lambda
ψ psi
μ
mu
ω
omega
and some upper-case symbols:
Γ
Gamma
Σ Sigma
Δ Delta
Υ
Upsilon
Θ Theta
Φ Phi
Λ Lambda
Ψ
Psi
Ξ Xi
Ω Omega
Π Pi
Being able to read an equation does not mean that we understand it—but we are a
little closer than just being able to stare at a jumble of symbols! Therefore, in future,
when I introduce a new mathematical object, I will tell you how it should be read.

Chapter 2
Numbers
2.1 Introduction
Our brain’s visual cortex possesses some incredible image processing features. For
example, children know instinctively when they are given less sweets than another
child, and adults know instinctively when they are short-changed by a Parisian taxi
driver, or driven around the Arc de Triumph several times, on the way to the airport!
Intuitively, we can assess how many donkeys are in a ﬁeld without counting them,
and generally, we seem to know within a second or two, whether there are just a few,
dozens, or hundreds of something. But when accuracy is required, one can’t beat
counting. But what is counting?
Well normally, we are taught to count by our parents by memorising ﬁrst, the
counting words “one, two, three, four, ﬁve, six, seven, eight, nine, ten, …” and
second, associating them with our ﬁngers, so that when asked to count the number of
donkeysinapicturebook,eachdonkeyisassociatedwithacountingword.Wheneach
donkey has been identiﬁed, the number of donkeys equals the last word mentioned.
However, this still assumes that we know the meaning of “one, two, three, four, …”
etc. Memorising these counting words is only part of the problem—getting them in
the correct sequence is the real challenge. The incorrect sequence “one, two, ﬁve,
three, nine, four, …” etc., introduces an element of randomness into any calculation,
but practice makes perfect, and it’s useful to master the correct sequence before going
to university!
2.2 Sets of Numbers
A set is a collection of random objects called its elements or members. For example,
each system of number belongs to a set with given a name, such as N for the natural
numbers, R for real numbers, and Q for rational numbers. When we want to indicate
that something is whole, real or rational, etc., we use the notation:
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_2
5

6
2
Numbers
n ∈N
which reads “n is a member of (∈) the set N”, i.e. n is a whole number. Similarly:
x ∈R
stands for “x is a real number.”
A well-ordered set possesses a unique order, such as the natural numbers N.
Therefore, if P is the well-ordered set of prime numbers and N is the well-ordered
set of natural numbers, we can write:
P = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, . . . }
N = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, . . . }.
By pairing the prime numbers in P with the numbers in N, we have:
{2, 1}, {3, 2}, {5, 3}, {7, 4}, {11, 5}, {13, 6}, {17, 7}, {19, 8}, {23, 9}, . . . }
and we can reason that 2 is the 1st prime, and 3 is the 2nd prime, etc. However, we
still have to declare what we mean by 1, 2, 3, 4, 5, . . . etc., and without getting too
philosophical, I like the idea of deﬁning them as follows. The word “one”, represented
by 1, stands for “oneness” of anything: one ﬁnger, one house, one tree, one donkey,
etc. The word “two”, represented by 2, is “one more than one”. The word “three”,
represented by 3, is “one more than two”, and so on.
We are now in a position to associate some mathematical notation with our num-
bers by introducing the + and = signs. We know that + means add, but it also can
stand for “more”. We also know that = means equal, and it can also stand for “is the
same as”. Thus the statement:
2 = 1 + 1
is read as “two is the same as one more than one.”
We can also write:
3 = 1 + 2
which is read as “three is the same as one more than two.” But as we already have a
deﬁnition for 2, we can write
3 = 1 + 2
= 1 + 1 + 1.
Developing this idea, and including some extra combinations, we have:
2 = 1 + 1
3 = 1 + 2

2.2 Sets of Numbers
7
4 = 1 + 3 = 2 + 2
5 = 1 + 4 = 2 + 3
6 = 1 + 5 = 2 + 4 = 3 + 3
7 = 1 + 6 = 2 + 5 = 3 + 4
etc.
and can be continued without limit. These numbers, 1, 2, 3, 4, 5, 6, etc., are called
natural numbers, and are the set N.
2.3 Zero
The concept of zero has a well-documented history, which shows that it has been used
by different cultures over a period of two-thousand years or more. It was the Indian
mathematician and astronomer Brahmagupta (598-c.–670) who argued that zero
was just as valid as any natural number, with the deﬁnition: the result of subtracting
any number from itself. However, even today, there is no universal agreement as to
whether zero belongs to the set N, consequently, the set N0 stands for the set of
natural numbers including zero.
In today’s positional decimal system, which is a place value system, the digit
0 is a placeholder. For example, 203 stands for: two hundreds, no tens and three
units. Although 0 ∈N0, it does have special properties that distinguish it from other
members of the set, and Brahmagupta also gave rules showing this interaction.
If x ∈N0, then the following rules apply:
addition:
x + 0 = x
subtraction:
x −0 = x
multiplication:
x × 0 = 0 × x = 0
division: 0/x
= 0
undeﬁned division:
x/0.
The expression 0/0 is called an indeterminate form, as it is possible to show that
under different conditions, especially limiting conditions, it can equal anything. So
for the moment, we will avoid using it until we cover calculus.
2.4 Negative Numbers
When negative numbers were ﬁrst proposed, they were not accepted with open arms,
as it was difﬁcult to visualise −5 of something. For instance, if there are 5 donkeys
in a ﬁeld, and they are all stolen to make salami, the ﬁeld is now empty, and there

8
2
Numbers
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
Fig. 2.1 The number line showing negative and positive numbers
is nothing we can do in the arithmetic of donkeys to create a ﬁeld of −5 donkeys.
However, in applied mathematics, numbers have to represent all sorts of quantities
such as temperature, displacement, angular rotation, speed, acceleration, etc., and
we also need to incorporate ideas such as left and right, up and down, before and
after, forwards and backwards, etc. Fortunately, negative numbers are perfect for
representing all of the above quantities and ideas.
Consider the expression 4 −x, where x ∈N0. When x takes on certain values,
we have
4 −1 = 3
4 −2 = 2
4 −3 = 1
4 −4 = 0
and unless we introduce negative numbers, we are unable to express the result of
4−5. Consequently, negative numbers are visualised as shown in Fig.2.1, where the
number line shows negative numbers to the left of the natural numbers, which are
positive, although the + sign is omitted for clarity.
Moving from left to right, the number line provides a numerical continuum from
large negative numbers, through zero, towards large positive numbers. In any calcu-
lations, we could agree that angles above the horizon are positive, and angles below
the horizon, negative. Similarly, a movement forwards is positive, and a movement
backwards is negative. So now we are able to write:
4 −5 = −1
4 −6 = −2
4 −7 = −3
etc.,
without worrying about creating impossible conditions.
2.4.1 The Arithmetic of Positive and Negative Numbers
Once again, Brahmagupta compiled all the rules, Tables2.1 and 2.2, supporting the
addition, subtraction, multiplication and division of positive and negative numbers.
The real ﬂy in the ointment, being negative numbers, which cause problems for

2.4 Negative Numbers
9
Table 2.1 Rules for adding and subtracting positive and negative numbers
+
b
−b
−
b
−b
a
a + b
a −b
a
a −b
a + b
−a
b −a
−(a + b)
−a
−(a + b)
b −a
Table 2.2 Rules for multiplying and dividing positive and negative numbers
×
b
−b
/
b
−b
a
ab
−ab
a
a/b
−a/b
−a
−ab
ab
−a
−a/b
a/b
children, math teachers and occasional accidents for mathematicians. Perhaps, the
one rule we all remember from our school days is that two negatives make a positive.
Another problem with negative numbers arises when we employ the square-root
function. As the product of two positive or negative numbers results in a positive
result, the square-root of a positive number gives rise to a positive and a negative
answer. For example,
√
4 = ±2. This means that the square-root function only
applies to positive numbers. Nevertheless, it did not stop the invention of the imagi-
nary object i, where i2 = −1. However, i is not a number, but an operator, which is
described later.
2.5 Observations and Axioms
The following axioms or laws provide a formal basis for mathematics, and in the fol-
lowing descriptions a binary operation is an arithmetic operation such as +, −, ×, /
which operates on two operands.
2.5.1 Commutative Law
The commutative law in algebra states that when two elements are linked through
some binary operation, the result is independent of the order of the elements. The
commutative law of addition is
a + b = b + a
e.g. 1 + 2 = 2 + 1.

10
2
Numbers
The commutative law of multiplication is
a × b = b × a
e.g. 1 × 2 = 2 × 1.
Note that subtraction is not commutative:
a −b ̸= b −a
e.g. 1 −2 ̸= 2 −1.
2.5.2 Associative Law
The associative law in algebra states that when three or more elements are linked
together through a binary operation, the result is independent of how each pair of
elements is grouped. The associative law of addition is
a + (b + c) = (a + b) + c
e.g. 1 + (2 + 3) = (1 + 2) + 3.
The associative law of multiplication is
a × (b × c) = (a × b) × c
e.g. 1 × (2 × 3) = (1 × 2) × 3.
However, note that subtraction is not associative:
a −(b −c) ̸= (a −b) −c
e.g. 1 −(2 −3) ̸= (1 −2) −3.
which may seem surprising, but at the same time conﬁrms the need for clear axioms.
2.5.3 Distributive Law
The distributive law in algebra describes an operation which when performed on a
combination of elements is the same as performing the operation on the individual
elements. The distributive law does not work in all cases of arithmetic. For example,
multiplication over addition holds:
a(b + c) = ab + ac
e.g. 2(3 + 4) = 6 + 8,

2.5 Observations and Axioms
11
whereas addition over multiplication does not:
a + (b × c) ̸= (a + b) × (a + c)
e.g. 3 + (4 × 5) ̸= (3 + 4) × (3 + 5).
Although these laws are natural for numbers, they do not necessarily apply to all
mathematical objects. For instance, the vector product, which multiplies two vectors
together, is not commutative. The same applies for matrix multiplication.
2.6 The Base of a Number System
2.6.1 Background
Over recent millennia, mankind has invented and discarded many systems for repre-
senting number. People have counted on their ﬁngers and toes, used pictures (hiero-
glyphics), cut marks on clay tablets (cuneiform symbols), employed Greek symbols
(Ionic system) and struggled with, and abandoned Roman numerals (I, V, X, L, C,
D, M, etc.), until we reach today’s decimal place system, which has Hindu-Arabic
and Chinese origins. And since the invention of computers, we have witnessed the
emergence of binary, octal and hexadecimal number systems, where 2, 8 and 16
respectively, replace the 10 in our decimal system.
The decimal number 23 means “two tens and three units”, and in English
is written “twenty-three”, in French “vingt-trois” (twenty-three), and in German
“dreiundzwanzig” (three and twenty). Let’s investigate the algebra behind the dec-
imal system and see how it can be used to represent numbers to any base. The
expression:
a × 1000 + b × 100 + c × 10 + d × 1
where a, b, c, d take on any value between 0 and 9, describes any whole number
between 0 and 9999. By including
e × 0.1 + f × 0.01 + g × 0.001 + h × 0.0001
where e, f, g, h take on any value between 0 and 9, any decimal number between 0
and 9999.9999 can be represented.
Indices bring the notation alive and reveal the true underlying pattern:
. . . a103 + b102 + c101 + d100 + e10−1 + f 10−2 + g10−3 + h10−4 . . . .
Remember that any number raised to the power 0 equals 1. By adding extra terms
both left and right, any number can be accommodated.

12
2
Numbers
In this example, 10 is the base, which means that the values of a to h range between
0 and 9, 1 less than the base. Therefore, by substituting B for the base we have
. . . aB3 + bB2 + cB1 + dB0 + eB−1 + f B−2 + gB−3 + hB−4 . . .
where the values of a to h range between 0 and B −1.
2.6.2 Octal Numbers
The octal number system has B = 8, and a to h range between 0 and 7:
. . . a83 + b82 + c81 + d80 + e8−1 + f 8−2 + g8−3 + h8−4 . . .
and the ﬁrst 17 octal numbers are:
18, 28, 38, 48, 58, 68, 78, 108, 118, 128, 138, 148, 158, 168, 178, 208, 218.
The subscript 8, reminds us that although we may continue to use the words “twenty-
one”, it is an octal number, and not a decimal. But what is 148 in decimal? Well, it
stands for:
1 × 81 + 4 × 80 = 12.
Thus 356.48 in decimal, equals:
(3 × 82)+(5 × 81) + (6 × 80) + (4 × 8−1)
(3 × 64)+(5 × 8) + (6 × 1) + (4 × 0.125)
(192 + 40 + 6) + (0.5)
238.5.
Counting in octal appears difﬁcult, simply because we have never been exposed to
it, like the decimal system. If we had evolved with 8 ﬁngers, instead of 10, we would
be counting in octal!
2.6.3 Binary Numbers
The binary number system has B = 2, and a to h are 0 or 1:
. . . a23 + b22 + c21 + d20 + e2−1 + f 2−2 + g2−3 + h2−4 . . .

2.6 The Base of a Number System
13
and the ﬁrst 13 binary numbers are:
12, 102, 112, 1002, 1012, 1102, 1112, 10002, 10012, 10102, 10112, 11002, 11012.
Thus 11011.112 in decimal, equals:
(1 × 24) + (1 × 23) + (0 × 22) + (1 × 21) + (1 × 20) + (1 × 2−1) + (1 × 2−2)
(1 × 16) + (1 × 8) + (0 × 4) + (1 × 2) + (1 × 0.5) + (1 × 0.25)
(16 + 8 + 2) + (0.5 + 0.25)
26.75.
The reason why computers work with binary numbers—rather than decimal—is due
to the difﬁculty of designing electrical circuits that can store decimal numbers in
a stable fashion. A switch, where the open state represents 0, and the closed state
represents 1, is the simplest electrical component to emulate. No matter how often
it is used, or how old it becomes, it will always behave like a switch. The main
advantage of electrical circuits is that they can be switched on and off trillions of
times a second, and the only disadvantage is that the encoded binary numbers and
characters contain a large number of bits, and humans are not familiar with binary.
2.6.4 Hexadecimal Numbers
The hexadecimal number system has B = 16, anda to h can be 0 to 15, which presents
a slight problem, as we don’t have 15 different numerical characters. Consequently,
we use 0 to 9, and the letters A, B, C, D, E, F to represent 10, 11, 12, 13, 14, 15
respectively:
. . . a163 + b162 + c161 + d160 + e16−1 + f 16−2 + g16−3 + h16−4 . . .
and the ﬁrst 17 hexadecimal numbers are:
116, 216, 316, 416, 516, 616, 716, 816, 916, A16, B16, C16, D16, E16, F16, 1016, 1116.
Thus 1E.816 in decimal, equals
(1 × 16) + (E × 1) + (8 × 16−1)
(16 + 14) + (8/16)
30.5.
Although it is not obvious, binary, octal and hexadecimal numbers are closely related,
which is why they are part of a programmer’s toolkit. Even though computers work

14
2
Numbers
with binary, it’s the last thing a programmer wants to use. So to simplify the man-
machine interface, binary is converted into octal or hexadecimal. To illustrate this,
let’s convert the 16-bit binary code 1101011000110001 into octal.
Using the following general binary integer
a28 + b27 + c26 + d25 + e24 + f 23 + g22 + h21 + i20
we group the terms into threes, starting from the right, because 23 = 8:
(a28 + b27 + c26) + (d25 + e24 + f 23) + (g22 + h21 + i20).
Simplifying:
26(a22 + b21 + c20) + 23(d22 + e21 + f 20) + 20(g22 + h21 + i20)
82(a22 + b21 + c21) + 81(d22 + e21 + f 20) + 80(g22 + h21 + i20)
82R + 81S + 80T
where
R = a22 + b21 + c
S = d22 + e21 + f
T = g22 + h21 + i
andthevaluesof R, S, T varybetween0and7.Therefore,given1101011000110001,
we divide the binary code into groups of three, starting at the right, and adding two
leading zeros:
(001)(101)(011)(000)(110)(001).
For each group, multiply the zeros and ones by 4, 2, 1, right to left:
(0 + 0 + 1)(4 + 0 + 1)(0 + 2 + 1)(0 + 0 + 0)(4 + 2 + 0)(0 + 0 + 1)
(1)(5)(3)(0)(6)(1)
1530618.
Therefore, 11010110001100012 ≡1530618, (≡stands for “equivalent to”) which
is much more compact. The secret of this technique is to memorise the patterns:
0002 ≡08
0012 ≡18
0102 ≡28
0112 ≡38
1002 ≡48

2.6 The Base of a Number System
15
1012 ≡58
1102 ≡68
1112 ≡78.
Here are a few more examples, with the binary digits grouped in threes:
1112 ≡78
101 1012 ≡558
100 0002 ≡408
111 000 111 000 1112 ≡707078.
It’s just as easy to reverse the process, and convert octal into binary. Here are some
examples:
5678 ≡101 110 1112
238 ≡010 0112
17418 ≡001 111 100 0012.
A similar technique is used to convert binary to hexadecimal, but this time we
divide the binary code into groups of four, because 24 = 16, starting at the right, and
adding leading zeros, if necessary. To illustrate this, let’s convert the 16-bit binary
code 1101 0110 0011 0001 into hexadecimal.
Using the following general binary integer number
a211 + b210 + c29 + d28 + e27 + f 26 + g25 + h24 + i23 + j22 + k21 + l20
from the right, we divide the binary code into groups of four:
(a211 + b210 + c29 + d28) + (e27 + f 26 + g25 + h24) + (i23 + j22 + k21 + l20).
Simplifying:
28(a23 + b22 + c21 + d20) + 24(e23 + f 22 + g21 + h20) + 20(i23 + j22 + k21 + l20)
162(a23 + b22 + c21 + d) + 161(e23 + f 22 + g21 + h) + 160(i23 + j22 + k21 + l)
162R + 161S + 160T
where
R = a23 + b22 + c21 + d
S = e23 + f 22 + g21 + h
T = i23 + j22 + k21 + l

16
2
Numbers
and
the
values
of
R, S, T
vary
between
0
and
15.
Therefore,
given
11010110001100012, we divide the binary code into groups of fours, starting at
the right:
(1101)(0110)(0011)(0001)
For each group, multiply the zeros and ones by 8, 4, 2, 1 respectively, right to left:
(8 + 4 + 0 + 1)(0 + 4 + 2 + 0)(0 + 0 + 2 + 1)(0 + 0 + 0 + 1)
(13)(6)(3)(1)
D63116.
Therefore, 1101 0110 0011 00012 ≡D63116, which is even more compact than its
octal value 1530618.
I have deliberately used whole numbers in the above examples, but they can all be
extended to include a fractional part. For example, when converting a binary number
such as 11.11012 to octal, the groups are formed about the binary point:
(011).(110)(100) ≡3.648.
Similarly,whenconvertingabinarynumbersuchas101010.1001102 tohexadecimal,
the groups are also formed about the binary point:
(0010)(1010).(1001)(1000) ≡2A.9816.
Table2.3 shows the ﬁrst twenty decimal, binary, octal and hexadecimal numbers.
Table 2.3 The ﬁrst twenty decimal, binary, octal, and hexadecimal numbers
Decimal
Binary
Octal
Hex
Decimal
Binary
Octal
Hex
1
1
1
1
11
1011
13
B
2
10
2
2
12
1100
14
C
3
11
3
3
13
1101
15
D
4
100
4
4
14
1110
16
E
5
101
5
5
15
1111
17
F
6
110
6
6
16
10000
20
10
7
111
7
7
17
10001
21
11
8
1000
10
8
18
10010
22
12
9
1001
11
9
19
10011
23
13
10
1010
12
A
20
10100
24
14

2.6 The Base of a Number System
17
Table 2.4 Addition of two decimal integers showing the carry
+
0
1
2
3
4
5
6
7
8
9
0
0
1
2
3
4
5
6
7
8
9
1
1
2
3
4
5
6
7
8
9
10
2
2
3
4
5
6
7
8
9
10
11
3
3
4
5
6
7
8
9
10
11
12
4
4
5
6
7
8
9
10
11
12
13
5
5
6
7
8
9
10
11
12
13
14
6
6
7
8
9
10
11
12
13
14
15
7
7
8
9
10
11
12
13
14
15
16
8
8
9
10
11
12
13
14
15
16
17
9
9
10
11
12
13
14
15
16
17
18
Table 2.5 Addition of two
binary integers showing the
carry
+
0
1
0
0
1
1
1
10
2.6.5 Adding Binary Numbers
When we are ﬁrst taught the addition of integers containing several digits, we are
advised to solve the problem digit by digit, working from right to left. For example,
to add 254 to 561 we write:
561
254
815
where 4 + 1 = 5, 5 + 6 = 1 with a carry = 1, 2 + 5 + carry = 8.
Table2.4 shows all the arrangements for adding two digits with the carry shown
as carryn. However, when adding binary numbers, the possible arrangements collapse
to the four shown in Table2.5, which greatly simpliﬁes the process.
For example, to add 124 to 188 as two 16-bit binary integers, we write, showing
the status of the carry bit:
0000000011111000 carry
0000000010111100 = 188
0000000001111100 = 124
0000000100111000 = 312
Such addition is easily undertaken by digital electronic circuits, and instead of having
separate circuitry for subtraction, it is possible to perform subtraction using the
technique of two’s complement.

18
2
Numbers
2.6.6 Subtracting Binary Numbers
Two’s complement is a technique for converting a binary number into a form such
that when it is added to another binary number, it results in a subtraction. There are
two stages to the conversion: inversion, followed by the addition of 1. For example,
24 in binary is 0000000000110000, and is inverted by switching every 1 to 0, and
vice versa: 1111111111100111. Next, we add 1: 1111111111101000, which now
represents −24. If this is added to binary 36: 0000000000100100, we have
0000000000100100 = +36
1111111111101000 = −24
0000000000001100 = +12
Note that the last high-order addition creates a carry of 1, which is ignored. Here is
another example, 100 −30:
0000000000011110 = +30
inversion
1111111111100001
add 1
0000000000000001
1111111111100010 = −30
add 100
0000000001100100 = +100
0000000001000110 = +70
2.7 Types of Numbers
As mathematics evolved, mathematicians introduced different types of numbers to
help classify equations and simplify the language employed to describe their work.
These are the various types and their set names.
2.7.1 Natural Numbers
The natural numbers {1, 2, 3, 4, . . .} are used for counting, ordering and labelling
and represented by the set N. When zero is included, N0 or N0 is used:
N0 = N0 = {0, 1, 2, . . .}.
Note that negative numbers are not included. Natural numbers are used to subscript
a quantity to distinguish one element from another, e.g. x1, x2, x3, x4, . . .

2.7 Types of Numbers
19
2.7.2 Integers
Integer numbers include the natural numbers, both positive and negative, and zero,
and are represented by the set Z:
Z = {. . . , −2, −1, 0, 1, 2, 3, . . .}.
The reason for using Z is because the German for whole number is ganzen Zahlen.
Leopold Kronecker apparently criticised Georg Cantor for his work on set theory
with the jibe: “Die ganzen Zahlen hat der liebe Gott gemacht, alles andere ist Men-
schenwerk”, which translates: “God made the integers, and all the rest is man’s
work”, implying that the rest are artiﬁcial. However, Cantor’s work on set theory and
transﬁnite numbers proved to be far from artiﬁcial.
2.7.3 Rational Numbers
Any number that equals the quotient of one integer divided by another non-zero
integer, is a rational number, and represented by the set Q. For example, 2,
√
16,
0.25 are rational numbers because
2 = 4/2
√
16 = 4 = 8/2
0.25 = 1/4.
Some rational numbers can be stored accurately inside a computer, but many others
can only be stored approximately. For example, 4/3 produces an inﬁnite sequence
of threes 1.333333 . . . and is truncated when stored as a binary number.
2.7.4 Irrational Numbers
An irrational number cannot be expressed as the quotient of two integers. Irrational
numbers never terminate, nor contain repeated sequences of digits, consequently,
they are always subject to a small error when stored within a computer. Examples
are:
√
2 = 1.41421356 . . .
φ = 1.61803398 . . . (golden section)
e = 2.71828182 . . .
π = 3.14159265 . . .

20
2
Numbers
2.7.5 Real Numbers
Rational and irrational numbers comprise the set of real numbers R. Examples are
1.5, 0.004, 12.999 and 23.0.
2.7.6 Algebraic and Transcendental Numbers
Polynomial equations with rational coefﬁcients have the form:
f (x) = axn + bxn−1 + cxn−2 . . . + C
such as
y = 3x2 + 2x −1
and their roots belong to the set of algebraic numbers A. A consequence of this
deﬁnition implies that all rational numbers are algebraic, since if
x = p
q
then
qx −p = 0
which is a polynomial. Numbers that are not roots to polynomial equations are
transcendental numbers and include most irrational numbers, but not
√
2, since if
x =
√
2
then
x2 −2 = 0
which is a polynomial.
2.7.7 Imaginary Numbers
Imaginary numbers were invented to resolve problems where an equation such as
x2 + 16 = 0, has no real solution (roots). The simple idea of declaring the existence
of a quantity i, such that i2 = −1, permits the solution to be expressed as
x = ±4i.

2.7 Types of Numbers
21
For example, if x = 4i we have
x2 + 16 = 16i2 + 16
= −16 + 16
= 0
and if x = −4i we have
x2 + 16 = 16i2 + 16
= −16 + 16
= 0.
But what is i? In 1637, the French mathematician René Descartes (1596–1650),
published La Géométrie, in which he stated that numbers incorporating √−1 were
“imaginary”, and for centuries this label has stuck. Unfortunately, it was a derogatory
remark, as there is nothing “imaginary” about i—it simply is an object that when
introduced into various algebraic expressions, reveals some amazing underlying pat-
terns. i is not a number in the accepted sense, it is a mathematical object or construct
that squares to −1. However, it does lose its mystery when interpreted as a rotational
operator, which we investigate below.
The set of imaginary numbers is represented by I, which permits us to deﬁne an
imaginary number bi as
bi ∈I,
b ∈R,
i2 = −1.
As i2 = −1 then it must be possible to raise i to other powers. For example,
i4 = i2i2 = 1
and
i5 = ii4 = i.
Table2.6 shows the sequence up to i6.
This cyclic pattern is quite striking, and reminds one of a similar pattern:
(x, y, −x, −y, x, . . .)
Table 2.6 Increasing powers of i
i0
i1
i2
i3
i4
i5
i6
1
i
−1
−i
1
i
−1

22
2
Numbers
Fig. 2.2 The complex plane
Real
1
2
3
4
5
2i
1i
3i
4i
5i
Imaginary
-1
-2
-3
-4
-5
0
-1i
-3i
-2i
-4i
-5i
that arises when rotating around the Cartesian axes in a anticlockwise direction.
Such a similarity cannot be ignored, for when the real number line is combined with
a vertical imaginary axis, it creates the complex plane, as shown in Fig.2.2.
The above sequence is summarised as
i4n = 1
i4n+1 = i
i4n+2 = −1
i4n+3 = −i
where n ∈N0.
But what about negative powers? Well they, too, are also possible. Consider i−1,
which is evaluated as follows:
i−1 = 1
i = 1(−i)
i(−i) = −i
1 = −i.
Similarly,
i−2 = 1
i2 = 1
−1 = −1
and
i−3 = i−1i−2 = −i(−1) = i.

2.7 Types of Numbers
23
Table 2.7 Decreasing powers of i
i0
i−1
i−2
i−3
i−4
i−5
i−6
1
−i
−1
i
1
−i
−1
Table2.7 shows the sequence down to i−6.
This time the cyclic pattern is reversed and is similar to the pattern
(x, −y, −x, y, x, . . .)
that arises when rotating around the Cartesian axes in a clockwise direction.
Perhaps the strangest power of all is ii, which happens to equal e−π/2 =
0.207879576 . . ., and is explained in Chap.11.
Now let’s investigate how a real number behaves when it is repeatedly multiplied
by i. Starting with the number 3, we have:
i × 3 = 3i
i × 3i = −3
i × (−3) = −3i
i × (−3)i = 3.
So the cycle is (3, 3i, −3, −3i, 3, 3i, −3, −3i, 3, . . .), which has four steps, as shown
in Fig.2.3.
Fig. 2.3 The cycle of points
created by repeatedly
multiplying 3 by i
Real
1
2
3
4
5
2i
1i
3i
4i
5i
Imaginary
-1
-2
-3
-4
-5
0
-1i
-3i
-2i
-4i
-5i

24
2
Numbers
Imaginary objects occur for all sorts of reasons. For example, consider the state-
ments
AB = −BA
BA = −AB
where A and B are two undeﬁned objects that obey the associative law, but not the
commutative law, and A2 = B2 = 1. The operation (AB)2 reveals
(AB)(AB) = A(BA)B
= −A(AB)B
= −(A2)(B2)
= −1
which means that the product AB is imaginary. Such objects, which can be matrices,
are useful in describing the behaviour of sub-atomic particles.
2.7.8 Complex Numbers
A complex number has a real and imaginary part: z = a + ib, and represented by
the set C:
z = a + bi
z ∈C,
a, b ∈R,
i2 = −1.
Some examples are
z = 1 + i
z = 3 −2i
z = −23 +
√
23i.
Complex numbers obey all the normal laws of algebra. For example, if we multiply
(a + bi) by (c + di) we have
(a + bi)(c + di) = ac + adi + bci + bdi2.
Collecting up like terms and substituting −1 for i2 we get
(a + bi)(c + di) = ac + (ad + bc)i −bd

2.7 Types of Numbers
25
Fig. 2.4 The complex plane
showing four complex
numbers
Real
1
2
3
4
5
2i
1i
3i
4i
5i
Imaginary
-1
-2
-3
-4
-5
0
-1i
-3i
-2i
-4i
-5i
P
Q
R
S
which simpliﬁes to
(a + bi)(c + di) = ac −bd + (ad + bc)i
which is another complex number.
Something interesting happens when we multiply a complex number by its com-
plex conjugate, which is the same complex number but with the sign of the imaginary
part reversed:
(a + bi)(a −bi) = a2 −abi + bai −b2i2.
Collecting up like terms and simplifying we obtain
(a + bi)(a −bi) = a2 + b2
which is a real number, as the imaginary part has been cancelled out by the action of
the complex conjugate.
Figure2.4 shows how complex numbers are represented graphically using the
complex plane.
For example, the complex number P = 4 + 3i in Fig.2.5 is rotated 90◦to Q by
multiplying it by i. Let’s do this, and remember that i2 = −1:
i(4 + 3i) = 4i + 3i2
= 4i −3
= −3 + 4i.

26
2
Numbers
Fig. 2.5 The nested sets of numbers
The point Q = −3 + 4i is rotated 90◦to R by multiplying it by i:
i(−3 + 4i) = −3i + 4i2
= −3i −4
= −4 −3i.
The point R = −4 −3i is rotated 90◦to S by multiplying it by i:
i(−4 −3i) = −4i −3i2
= −4i + 3
= 3 −4i.
Finally, the point S = 3 −4i is rotated 90◦back to P by multiplying it by i:
i(3 −4i) = 3i −4i2
= 3i + 4
= 4 + 3i.
As you can see, complex numbers are intimately related to Cartesian coordinates,
in that the ordered pair (x, y) ≡(x + yi).

2.7 Types of Numbers
27
2.7.9 Quaternions and Octonions
When it was noted that complex numbers could rotate points on the 2D plane, it
was inevitable that someone would look for a 3D equivalent—that person was the
Irish mathematician Sir William Rowan Hamilton (1805–1865). Initially, he added
an extra imaginary term cj to a complex number:
z = a + bi + cj
a, b, c ∈R,
i, j ∈I,
i2, j2 = −1.
The problem with such objects arises in their product. For example, starting with:
x = a + bi + cj
y = d + ei + f j
then
xy = (a + bi + cj)(d + ei + f j)
= ad + aei + af j + dbi + bei2 + bf i j + cdj + ceji + cf j2
= (ad −be −cf ) + (ae + db)i + (af + cd) j + bf i j + ceji
which creates a similar object, but includes two extra terms bf i j and ceji which
cannot be resolved. Hamilton thought about this for over a decade, and considered
adding a third imaginary term dk:
z = a + bi + cj + dk
a, b, c, d ∈R,
i, j, k ∈I,
i2, j2, k2 = −1.
The product xy equals:
xy = (a + bi + cj + dk)(e + f i + gj + hk)
= ae + af i + agj + ahk + bei + bf i2 + bgi j + bhik + cej + cf ji + cgj2
+ chjk + dek + d f ki + dgkj + dhk2
= (ae −bf −cg −dh) + (af + be)i + (ag + ce) j + (ah + de)k
+ bgi j + bhik + cf ji + chjk + d f ki + dgkj
which again creates a similar object, but this time, with six extra terms:
bgi j + bhik + cf ji + chjk + d f ki + dgkj.

28
2
Numbers
Table 2.8 The quaternion’s
imaginary products
×
i
j
k
i
−1
k
−j
j
−k
−1
i
k
j
−i
−1
In 1843, Hamilton realised how to resolve the six extra terms, which was to let
i j = k,
ji = −k,
jk = i,
kj = −i,
ki = j,
ik = −j
shown in Table2.8, and completed the above product as follows:
xy = (ae −bf −cg −dh) + (af + be)i + (ag + ce) j + (ah + de)k
+ bgi j + bhik + cf ji + chjk + d f ki + dgkj
= (ae −bf −cg −dh) + (af + be)i + (ag + ce) j + (ah + de)k
+ bgk −bhj −cf k + chi + d f j −dgi
= (ae −bf −cg −dh) + (af + be + ch −dg)i + (ag + ce + d f −bh) j
+ (ah + de + bg −cf )k.
The product xy is now an identical object to the original objects x and y, and Hamilton
called them quaternions. He then showed how quaternions could rotate points about
an arbitrary 3D axis, but mathematicians were not keen to embrace an object with
so many imaginary terms. Fortuitously, the American mathematician Josiah Willard
Gibbs (1839–1903), realised that a quaternion’s imaginary part could be isolated and
represent quantities with magnitude and direction, and 3D vectors were born:
v = ai + bj + ck.
The set of quaternions called H, in honour of its inventor, became the ﬁrst non-
commutative algebra.
Almost immediately quaternions were invented, the hunt began for the next com-
plex algebra, which was discovered simultaneously in 1843 by a colleague of Hamil-
ton, John Thomas Graves (1806–1870), who called them octaves, and by the young
English mathematician Arthur Cayley (1821–1895), who called them Cayley Num-
bers:
z = a + bi + cj + dk + ep + f q + gr + hs
a, b, c, d, e, f, g, h ∈R,
i, j, k, p, q,r, s ∈I i2, j2, k2, p2, q2,r2, s2 = −1.

2.7 Types of Numbers
29
They are now called octonions, and are not only non-commutative, but non-
associative, which means that in general, given three octonions A, B, C, then
(AB)C ̸= A(BC). In 1898, the German mathematician Adolf Hurwitz (1859–1919),
proved that there are only four algebras where it is possible to multiply and divide in
the accepted sense: R, C, H, O. Figure2.5 shows the sets of numbers diagrammati-
cally.
2.8 Prime Numbers
A prime number is deﬁned as a positive integer that can only be divided by 1 and
itself, without leaving a remainder. The ﬁrst ﬁve prime numbers are 2, 3, 5, 7, 11.
We can prove that any positive integer must either be a prime, or the product of two
or more primes, using the following reasoning.
The set of natural numbers comprises two sets: primes and non-primes. A prime,
by deﬁnition, has no factors, apart from 1 and itself. A non-prime has factors and is
called composite. However, these factors are natural numbers, which must either be
prime or non-prime. Eventually, the composite factors must decompose into com-
posite primes.
For example, 72 = 8 × 9, but 8 = 23 and 9 = 32, therefore, 72 = 23 × 32. Even
starting with 72 = 6 × 12, but 6 = 2 × 3 and 12 = 22 × 3, therefore, 72 = 23 × 32.
Table2.9 shows the prime factors for the ﬁrst 30 numbers.
Table 2.9 The prime factors for the ﬁrst 30 numbers
Number
Factors
Number
Factors
Number
Factors
1
11
11
21
3 × 7
2
2
12
22 × 3
22
2 × 11
3
3
13
13
23
23
4
22
14
2 × 7
24
23 × 3
5
5
15
3 × 5
25
52
6
2 × 3
16
24
26
2 × 13
7
7
17
17
27
33
8
23
18
2 × 32
28
22 × 7
9
32
19
19
29
29
10
2 × 5
20
22 × 5
30
2 × 3 × 5

30
2
Numbers
2.8.1 The Fundamental Theorem of Arithmetic
Original work by the Greek mathematician Euclid (Mid-4th to mid-3rd century BC),
revealed the fundamental theorem of arithmetic (FTAr), also called the unique fac-
torisation theorem, which states that every integer greater than 1, is either prime
or the unique product of primes, and is expressed symbolically as follows. Let
p1, p2, p3 . . . pk be prime numbers, and α1, α2, α3 . . . αk be their associated pos-
itive integer powers: pα1
1 , pα2
2 , pα3
3 . . . pαk
k . We now use the product function Pi, 
to create the product: pα1
1 pα2
2 pα3
3 . . . pαk
k , and introduce the variable i with a range
of 1 to k, which permits the FTAr to be written as
n = pα1
1 pα2
2 pα3
3 · · · pαk
k
=
k
i=1
pαi
i
where  is shorthand for “multiply together the associated terms”.
For example, 2250 equals the unique product: 213253, and 245 = 5172. To prove
that these prime products are unique, let’s ﬁrst assume that they are not, and show
that this leads to a contradiction.
Let n > 1 and equals the product of two prime numbers: n = p1 p2. Now let’s
assume that n also equals the product of two other prime numbers: q1q2. Therefore,
p1 p2 = q1q2
and
p1 = q1q2
p2
which implies that either q1/p2 or q2/p2 factorises. However, this is impossible
as q1 and q2 are prime, therefore, the original assumption was incorrect. The same
reasoning may be generalised to any number of prime factors.
2.8.2 Is 1 a Prime?
You may notice in the above table of factors that 1 is not a prime, which has not
always been the case. The reason is due to maintaining the logical integrity of the
FTAr, which emphasises the uniqueness of the product of primes. If 1 were a prime,
we would have the following non-unique products
24 = 23 × 3
24 = 1 × 23 × 3

2.8 Prime Numbers
31
and it doesn’t seem satisfying to make 1 a prime, and then qualify the FTAr with the
rider: “This only applies for primes greater than 1.”
In 1742, the German mathematician Christian Goldbach (1690–1764) conjectured
that every even integer greater than 2 could be written as the sum of two primes:
4 = 2 + 2
14 = 11 + 3
18 = 11 + 7, etc.
No one has ever found an exception to this conjecture, and no one has ever con-
ﬁrmed it.
2.8.3 Prime Number Distribution
As one moves higher through the set of natural numbers, new primes are uncovered.
But every prime discovered increases the possibility for more composite numbers,
which overall, creates a falling distribution for primes. Table2.7 shows that there are
10 primes in the ﬁrst 30 numbers, and further analysis reveals 26 primes in the ﬁrst
100 numbers, after which, they slowly decline, but never disappear.
The German mathematician Carl Gauss (1777–1855), proved, at the age of four-
teen, that as x →∞, (x moves towards inﬁnity), the function π(x), which estimates
the number of primes up to x, is given by
π(x) ∼
x
ln x
(where ∼stands for “similar to”.)
Testing this for x = 100:
π(100) ∼
100
ln 100 ≈
100
4.60517 ≈22.
which is lower than the actual value of 26. However, the French mathematician
Adrien-Marie Legendre (1752–1833), conjectured the following relationship:
π(x) ∼
x
ln x −B
where B = 1.08366. But it appears that the best result is when B = 1. Testing this
for x = 100:
π(100) ∼
100
ln 100 −1 ≈100
3.605 ≈28.
which is higher than the actual value of 26.

32
2
Numbers
ReadersinterestedinlearningmoreaboutprimenumbersshouldinvestigatePrime
Numbers: A Computational Perspective (2nd edition) by Richard Crandall and Carl
Pomerance.
2.8.4 Perfect Numbers
A perfect number equals the sum of its factors. For example, the factors of 6 are 1, 2
and 3, whose sum is also 6. One would imagine that there would be a large quantity of
small perfect numbers, but the ﬁrst ﬁve are: 6, 28, 496, 8128 and 33,550,336, which
are all even. And as the search continues to discover higher values, using computers,
no odd perfect number has emerged. Euclid proved that if m is prime, and of the
form 2k −1, then m(m + 1)/2 is an even perfect number. For example, 3 is prime
and
3 = 22 −1 and
3 × 4
2
= 6.
Similarly, 7 is prime and
7 = 23 −1 and
7 × 8
2
= 28.
2.8.5 Mersenne Numbers
Numbers of the form 2k −1 are called Mersenne numbers, some of which, are also
prime. The French theologian and mathematician Marin Mersenne (1588–1648)
became interested in them towards the end of his life, and today they are known as
Mersenne primes.
By the end of the 16th-century, the highest Mersenne prime was 524,287 which
equals 219 −1. At the start of the 21st-century, 243,112,609 −1 was the highest,
containing approximately 13 million digits!
Apart from the fact that prime numbers are so mysterious, they are very important
in public key cryptography, which is central to today’s internet security systems.
2.8.6 Transcendental and Algebraic Numbers
Given a polynomial built from integers, for example
y = 3x3 −4x2 + x + 23,

2.8 Prime Numbers
33
if the result is an integer, it is called an algebraic number, otherwise it is a transcen-
dental number. Familiar examples of the latter being π = 3.141 592 653 . . . , and
e = 2.718 281 828 . . . , which can be represented as various continued fractions:
π =
4
1 +
12
2 +
32
2 +
52
2 +
72
2 + . . .
e = 2 +
1
1 +
1
2 +
1
1 +
1
1 +
1
4 + . . .
2.8.7 Inﬁnity
The term inﬁnity is used to describe the size of unbounded systems. For example,
there is no end to prime numbers: i.e. they are inﬁnite; so too, are the sets of other
numbers. Consequently, no matter how we try, it is impossible to visualise the size of
inﬁnity. Nevertheless, this did not stop Georg Cantor from showing that one inﬁnite
set could be inﬁnitely larger than another.
Cantor distinguished between those inﬁnite number sets that could be “counted”,
and those that could not. For Cantor, counting meant the one-to-one correspondence
of a natural number with the members of another inﬁnite set. If there was a clear
correspondence, without leaving any gaps, then the two sets shared a common inﬁnite
size, called its cardinality using the ﬁrst letter of the Hebrew alphabet aleph: ℵ. The
cardinality of the natural numbers N is ℵ0, called aleph-zero.
Cantor discovered a way of representing the rational numbers as a grid, which
is traversed diagonally, back and forth, as shown in Fig.2.6. Some ratios appear
several times, such as 2
2, 3
3 etc., which are not counted. Nevertheless, the one-to-
one correspondence with the natural numbers means that the cardinality of rational
numbers is also ℵ0.

34
2
Numbers
Fig. 2.6 Rational number
grid
1
1
1
1
1
2
2
1
3
1
2
2
1
3
1
4
2
3
3
2
4
1
3
3
5
1
4
2
5
2
4
3
2
4
1
5
3
4
2
5
3
5
4
4
5
3
5
4
4
5
5
5
A real surprise was that there are inﬁnitely more transcendental numbers than
natural numbers. Furthermore, there are an inﬁnite number of cardinalities rising to
ℵℵ. Cantor had been alone working in this esoteric area, and as he published his
results, he shook the very foundations of mathematics, which is why he was treated
so badly by his fellow mathematicians.
2.9 Worked Examples
2.9.1 Algebraic Expansion
Expand (a + b)(c + d), (a −b)(c + d), and (a −b)(c −d).
(a + b)(c + d) = a(c + d) + b(c + d)
= ac + ad + bc + bd.
(a −b)(c + d) = a(c + d) −b(c + d)
= ac + ad −bc −bd.
(a −b)(c −d) = a(c −d) −b(c −d)
= ac −ad −bc + bd.

2.9 Worked Examples
35
2.9.2 Binary Subtraction
Using two’s complement, subtract 12 from 50.
0000000000001100 = +12
inversion
1111111111110011
add 1
0000000000000001
1111111111110100 = −12
add 50
0000000000110010 = +50
0000000000100110 = +38
2.9.3 Complex Numbers
Compute (3 + 2i) + (2 + 2i) + (5 −3i) and (3 + 2i)(2 + 2i)(5 −3i).
(3 + 2i) + (2 + 2i) + (5 −3i) = 10 + i.
(3 + 2i)(2 + 2i)(5 −3i) = (3 + 2i)(10 −6i + 10i + 6)
= (3 + 2i)(16 + 4i)
= 48 + 12i + 32i −8
= 40 + 44i.
2.9.4 Complex Rotation
Rotate the complex point (3 + 2i) by ±90◦and ±180◦.
To rotate +90◦(anticlockwise) multiply by i.
i(3 + 2i) = (3i −2) = (−2 + 3i).
To rotate −90◦(clockwise) multiply by −i.
−i(3 + 2i) = (−3i + 2) = (2 −3i).
To rotate +180◦(anticlockwise) multiply by −1.
−1(3 + 2i) = (−3 −2i).

36
2
Numbers
To rotate −180◦(clockwise) multiply by −1.
−1(3 + 2i) = (−3 −2i).
2.9.5 Quaternions
Compute (2+3i +4 j +k)+(6+2i + j +2k) and (2+3i +4 j +k)(6+2i + j +2k).
(2 + 3i + 4 j + k) + (6 + 2i + j + 2k) = (8 + 5i + 5 j + 3k).
(2 + 3i + 4 j + k)(6 + 2i + j + 2k) = 12 + 4i + 2 j + 4k
+ 18i + 6i2 + 3i j + 6ik
+ 24 j + 8 ji + 4 j2 + 8 jk
+ 6k + 2ki + kj + 2k2
= 12 + 4i + 2 j + 4k
+ 18i −6 + 3k −6 j
+ 24 j −8k −4 + 8i
+ 6k + 2 j −i −2
= (0 + 29i + 22 j + 5k).

Chapter 3
Algebra
3.1 Introduction
Some people, including me, ﬁnd learning a foreign language a real challenge; one of
the reasons being the inconsistent rules associated with its syntax. For example, why
is a table feminine in French, “la table”, and a bed masculine, “le lit”? They both
have four legs! The rules governing natural language are continuously being changed
by each generation, whereas mathematics appears to be logical and consistent. The
reason for this consistency is due to the rules associated with numbers and the way
they are combined together and manipulated at an abstract level. Such rules, or
axioms, generally make our life easy, however, as we saw with the invention of
negative numbers, extra rules have to be introduced, such as “two negatives make
a positive”, which is easily remembered. However, as we explore mathematics, we
discover all sorts of inconsistencies, such as there is no real value associated with
the square-root of a negative number. It’s forbidden to divide a number by zero. Zero
divided by zero gives inconsistent results. Nevertheless, such conditions are easy
to recognise and avoided. At least in mathematics, we don’t have to worry about
masculine and feminine numbers!
As a student, I discovered Principia Mathematica, a three-volume work written
by the British philosopher, logician, mathematician and historian Bertrand Russell
(1872–1970), and the British mathematician and philosopher Alfred North White-
head (1861–1947), in which the authors attempted to deduce all of mathematics
using the axiomatic system developed by the Italian mathematician Giuseppe Peano
(1858–1932). The ﬁrst volume established type theory, the second was devoted to
numbers, and the third to higher mathematics. The authors did intend a fourth vol-
ume on geometry, but it was too much effort to complete. It made extremely intense
reading. In fact, I never managed to get pass the ﬁrst page! It took the authors almost
100 pages of deep logical analysis in the second volume to prove that 1 + 1 = 2!
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_3
37

38
3
Algebra
Russell wrote in his Principles of Mathematics (1903):
“The fact that all Mathematics is Symbolic Logic
is one of the greatest discoveries of our age;
and when this fact has been established,
the remainder of the principles of mathematics
consists in the analysis of Symbolic Logic itself.”
Unfortunately, this dream cannot be realised, for in 1931, the Austrian-born, and
later American logician and mathematician Kurt Gödel (1906–1978), showed that
even though mathematics is based upon a formal set of axioms, there will always be
statements involving natural numbers that cannot be proved or disproved. Further-
more, a consistent axiomatic system cannot demonstrate its own consistency. These
theorems are known as Gödel’s incompleteness theorems.
Eventhoughwestartoffwithsomesimpleaxioms,itdoesnotmeanthateverything
discovered in mathematics is provable, which does not mean that we cannot continue
our every-day studies using algebra to solve problems. So let’s examine the basic
rules of algebra and prepare ourselves for the following chapters.
3.2 Background
Modern algebraic notation has evolved over thousands of years where different
civilisations developed ways of annotating mathematical and logical problems. The
word “algebra” comes from the Arabic “al-jabr w’al-muqabal” meaning “restora-
tion and reduction”. In retrospect, it does seem strange that centuries passed before
the “equals” sign (=) was invented, and concepts such as “zero” (CE 876) were
introduced, especially as they now seem so important. But we are not at the end of
this evolution, because new forms of annotation and manipulation will continue to
emerge as new mathematical objects are invented.
One fundamental concept of algebra is the idea of giving a name to an unknown
quantity. For example, m is often used to represent the slope of a 2D line, and c is the
line’s y-coordinate where it intersects the y-axis. René Descartes formalised the idea
of usingletters fromthebeginningof thealphabet (a, b, c, . . . ) torepresent arbitrary
quantities, and letters at the end of the alphabet (p, q, r, s, t, . . . , x, y, z) to
represent quantities such as pressure (p), time (t) and coordinates (x, y, z).
With the aid of the basic arithmetic operators: +, −, ×, / we can develop expres-
sions that describe the behaviour of a physical process or a logical computation. For
example, the expression ax + by −d equals zero for a straight line. The variables x
and y are the coordinates of any point on the line and the values of a, b and d deter-
mine the position and orientation of the line. The = sign permits the line equation to
be expressed as a self-evident statement:
0 = ax + by −d.

3.2 Background
39
Such a statement implies that the expressions on the left- and right-hand sides of
the = sign are “equal” or “balanced”, and in order to maintain equality or balance,
whatever is done to one side, must also be done to the other. For example, adding d
to both sides, the straight-line equation becomes
d = ax + by.
Similarly, we could double or treble both expressions, divide them by 4, or add 6,
without disturbing the underlying relationship. When we are ﬁrst taught algebra, we
are often given the task of rearranging a statement to make different variables the
subject. For example, (3.1) can be rearranged such that x is the subject:
y = x + 4
2 −1
z
(3.1)
y

2 −1
z

= x + 4
x = y

2 −1
z

−4.
Making z the subject requires more effort:
y = x + 4
2 −1
z
y

2 −1
z

= x + 4
2y −y
z = x + 4
2y −x −4 = y
z
z =
y
2y −x −4.
Parentheses are used to isolate part of an expression in order to select a sub-
expression that is manipulated in a particular way. For example, the parentheses
in c(a + b) + d ensure that the variables a and b are added together before being
multiplied by c, and ﬁnally added to d.
3.2.1 Solving the Roots of a Quadratic Equation
Problem solving is greatly simpliﬁed if one has solved it before, and having a good
memory is always an advantage. In mathematics, we keep coming across prob-
lems that have been encountered before, apart from different numbers. For example,

40
3
Algebra
(a + b)(a −b) always equals a2 −b2, therefore factorising the following is a trivial
exercise:
a2 −16 = (a + 4)(a −4)
x2 −49 = (x + 7)(x −7)
x2 −2 = (x +
√
2)(x −
√
2).
A perfect square has the form:
a2 + 2ab + b2 = (a + b)2.
Consequently, factorising the following is also a trivial exercise:
a2 + 4ab + 4b2 = (a + 2b)2
x2 + 14x + 49 = (x + 7)2
x2 −20x + 100 = (x −10)2.
Now let’s solve the roots of the quadratic equation ax2 + bx + c = 0, i.e. those
values of x that make the equation equal zero. As the equation involves an x2 term,
we will exploit any opportunity to factorise it. We begin with the quadratic where
a ̸= 0:
ax2 + bx + c = 0.
Step 1: Subtract c from both sides to begin the process of creating a perfect square:
ax2 + bx = −c.
Step 2: Divide both sides by a to create an x2 term:
x2 + b
a x = −c
a .
Step 3: Add b2/4a2 to both sides to create a perfect square on the left side:
x2 + b
a x + b2
4a2 = b2
4a2 −c
a .
Step 4: Factorise the left side:

x + b
2a
2
= b2
4a2 −c
a .

3.2 Background
41
Step 5: Make 4a2 the common denominator for the right side:

x + b
2a
2
= b2 −4ac
4a2
.
Step 6: Take the square root of both sides:
x + b
2a = ±
√
b2 −4ac
2a
.
Step 7: Subtract b/2a from both sides:
x = ±
√
b2 −4ac
2a
−b
2a .
Step 8: Rearrange the right side:
x = −b ±
√
b2 −4ac
2a
which provides the roots for any quadratic equation.
The discriminant
√
b2 −4ac may be positive, negative or zero. A positive value
reveals two real roots:
x1 = −b +
√
b2 −4ac
2a
,
x2 = −b −
√
b2 −4ac
2a
.
(3.2)
A negative value reveals two complex roots:
x1 = −b + i

|b2 −4ac|
2a
,
x2 = −b −i

|b2 −4ac|
2a
.
And a zero value reveals a single root:
x = −b
2a .
For example, Fig.3.1 shows the graph of y = x2 + x −2, where we can see that
y = 0 at two points: x = −2 and x = 1. In this equation
a = 1
b = 1
c = −2

42
3
Algebra
Fig. 3.1 Graph of
y = x2 + x −2
-3
-2
-1
0
1
2
3
-2
-1
1
2
which when plugged into (3.2) conﬁrms the graph:
x1 = −1 + √1 + 8
2
= 1
x2 = −1 −√1 + 8
2
= −2.
Figure3.2 shows the graph of y = x2 + x + 1, where at no point does y = 0. In
this equation
a = 1
b = 1
c = 1
which when plugged into (3.2) conﬁrms the graph by giving complex roots:
x1 = −1 + √1 −4
2
= −1
2 + i
√
3
2
Fig. 3.2 Graph of
y = x2 + x + 1
-3
-2
-1
0
1
2
3
-2
-1
1
2

3.2 Background
43
x2 = −1 −√1 −4
2
= −1
2 −i
√
3
2 .
Let’s show that x1 satisﬁes the original equation:
y = x2
1 + x1 + 1
=

−1
2 + i
√
3
2
2
−1
2 + i
√
3
2 + 1
= 1
4 −i
√
3
2 −3
4 −1
2 + i
√
3
2 + 1
= 0.
x2 also satisﬁes the same equation.
Algebraic expressions also contain a wide variety of functions, such as
√x = square root of x
n√x = nth root of x
xn = x to the power n
sin x = sine of x
cos x = cosine of x
tan x = tangent of x
log x = logarithm of x
ln x = natural logarithm of x.
Trigonometric functions are factorised as follows:
sin2 x −cos2 x = (sin x + cos x)(sin x −cos x)
sin2 x −tan2 x = (sin x + tan x)(sin x −tan x)
sin2 x + 4 sin x cos x + 4 cos2 x = (sin x + 2 cos x)2
sin2 x −6 sin x cos x + 9 cos2 x = (sin x −3 cos x)2.
3.3 Indices
Indices are used to imply repeated multiplication and create a variety of situations
where laws are required to explain how the result is to be computed.

44
3
Algebra
3.3.1 Laws of Indices
The laws of indices are expressed as follows:
am × an = am+n
am
an = am−n
(am)n = amn
and are veriﬁed using some simple examples:
23 × 22 = 25 = 32
24
22 = 22 = 4
(22)3 = 26 = 64.
From the above laws, it is evident that
a0 = 1
a−p = 1
a p
a
1
q =
q√a
a
p
q =
q√
a p.
3.4 Logarithms
Two people are associated with the invention of logarithms: the Scottish theolo-
gian and mathematician John Napier (1550–1617) and the Swiss clockmaker and
mathematician Joost Bürgi (1552–1632). Both men were frustrated by the time they
spent multiplying numbers together, and both realised that multiplication could be
replaced by addition using logarithms. Logarithms exploit the addition and subtrac-
tion of indices shown above, and are always associated with a base. For example,
if ax = n, then loga n = x, where a is the base. Where no base is indicated, it is
assumed to be 10. Two examples bring the idea to life:
102 = 100 then
log 100 = 2
103 = 1000 then
log 1000 = 3

3.4 Logarithms
45
Fig. 3.3 Graph of log x
0
10
20
30
40
50
60
70
80
90 100
-1
1
2
which is interpreted as “10 has to be raised to the power (index) 2 to equal 100.” The
log operation ﬁnds the power of the base for a given number. Thus a multiplication
is translated into an addition using logs. Figure3.3 shows the graph of log x, up to
x = 100, where we see that log 20 ≈1.3 and log 50 ≈1.7. Therefore, given suitable
software, logarithm tables, or a calculator with a log function, we can compute the
product 20 × 50 as follows:
20 × 50 = log 20 + log 50 ≈1.3 + 1.7 = 3
103 = 1000.
In general, the two bases used in calculators and software are 10 and e =
2.718 281 846 . . . . To distinguish one type of logarithm from the other, a loga-
rithm to the base 10 is written as log, and a natural logarithm to the base e is written
ln.
Figure3.4 shows the graph of ln x, up to x = 100, where we see that ln 20 ≈3
and ln 50 ≈3.9. Therefore, given suitable software, a set of natural logarithm tables
or a calculator with a ln function, we can compute the product 20 × 50 as follows:
20 × 50 = ln 20 + ln 50 ≈3 + 3.9 = 6.9
e6.9 ≈1000.
Fig. 3.4 Graph of ln x
0
10
20
30
40
50
60
70
80
90 100
-1
1
2
3
4
x

46
3
Algebra
From the above notation, it is evident that
log(ab) = log a + log b
log
a
b

= log a −log b
log(an) = n log a.
3.5 Further Notation
All sorts of symbols are used to stand in for natural language expressions; here are
some examples:
< less than
> greater than
≤less than or equal to
≥greater than or equal to
≈approximately equal to
≡equivalent to
̸= not equal to
|x| absolute value of x.
For example, 0 ≤t ≤1 is interpreted as: t is greater than or equal to 0, and is less
than or equal to 1. Basically, this means t varies between 0 and 1.
3.6 Functions
The theory of functions is a large subject, and at this point in the book, I will only
touch upon some introductory ideas that will help you understand the following
chapters.
The German mathematician Gottfried von Leibniz (1646–1716) is credited with
an early deﬁnition of a function, based upon the slope of a graph. However, it was
the Swiss mathematician Leonhard Euler (1707–1783) who provided a deﬁnition
along the lines: “A function is a variable quantity, whose value depends upon one or
more independent variables.” Other mathematicians have introduced more rigorous
deﬁnitions, which are examined later on in the chapter on calculus.

3.6 Functions
47
3.6.1 Explicit and Implicit Equations
The equation
y = 3x2 + 2x + 4
associates the value of y with different values of x. The directness of the equation:
“y =”, is why it is called an explicit equation, and their explicit nature is extremely
useful. However, simply by rearranging the terms, creates an implicit equation:
4 = y −3x2 −2x
which implies that certain values of x and y combine to produce the result 4. Another
implicit form is
0 = y −3x2 −2x −4
which means the same thing, but expresses the relationship in a slightly different
way.
An implicit equation can be turned into an explicit equation using algebra. For
example, the implicit equation
4x + 2y = 12
has the explicit form:
y = 6 −2x
where it is clear what y equals.
3.6.2 Function Notation
The explicit equation
y = 3x2 + 2x + 4
tells us that the value of y depends on the value of x, and not the other way around.
For example, when x = 1, y = 9; and when x = 2, y = 20. As y depends upon
the value of x, it is called the dependent variable; and as x is independent of y, it is
called the independent variable.
We can also say that y is a function of x, which can be written as
y = f (x)
where the letter “ f ” is the name of the function, and the independent variable is
enclosed in brackets. We could have also written y = g(x), y = h(x), etc.

48
3
Algebra
Eventually, we have to identify the nature of the function, which in this case is
f (x) = 3x2 + 2x + 4.
Nothing prevents us from writing
y = f (x) = 3x2 + 2x + 4
which means: y equals the value of the function f (x), which is determined by the
independent variable x using the expression 3x2 + 2x + 4.
An equation may involve more than one independent variable, such as the volume
of a cylinder:
V = πr2h
where r is the radius, and h, the height, and is written:
V (r, h) = πr2h.
3.6.3 Intervals
An interval is a continuous range of numerical values associated with a variable,
which can include or exclude the upper and lower values. For example, a variable
such as x is often subject to inequalities like x ≥a and x ≤b, which can also be
written as
a ≤x ≤b
and implies that x is located in the closed interval [a, b], where the square brackets
indicate that the interval includes a and b. For example,
1 ≤x ≤10
means that x is located in the closed interval [1, 10], which includes 1 and 10.
When the boundaries of the interval are not included, then we would state x > a
and x < b, which is written
a < x < b
and means that x is located in the open interval ]a, b[, where the reverse square
brackets indicate that the interval excludes a and b. For example,
1 < x < 10
means that x is located in the open interval ]1, 10[, which excludes 1 and 10.

3.6 Functions
49
Fig. 3.5 Closed, open and
half-open intervals. The ﬁlled
circles indicate that a or b
are included in the interval
a
b
closed interval
open interval
half-open interval
half-open interval
[a,b]
]a,b[
]a,b]
[a,b[
Closed and open intervals may be combined as follows. If x ≥a and x < b then
a ≤x < b
and means that x is located in the half-open interval [a, b[. For example,
1 ≤x < 10
means that x is located in the half-open interval [1, 10[, which includes 1, but
not 10.
Similarly, if
1 < x ≤b
means that x is located in the half-open interval ]1, 10], which includes 10, but
not 1.
An alternative notation employs parentheses instead of reversed brackets:
]a, b[ = (a, b)
[a, b[ = [a, b)
]a, b] = (a, b].
Figure3.5 shows open, closed and half-open intervals diagrammatically.
3.6.4 Function Domains and Ranges
The following descriptions of domains and ranges only apply to functions with one
independent variable: f (x).
Returning to the above function:
y = f (x) = 3x2 + 2x + 4

50
3
Algebra
the independent variable x, can take on any value from −∞to +∞, which is called
the domain of the function. In this case, the domain of f (x) is the set of real numbers
R. The notation used for intervals, is also used for domains, which in this case is
] −∞, +∞[
and is open, as there are no precise values for −∞and +∞.
As the independent variable takes on different values from its domain, so the
dependent variable, y or f (x), takes on different values from its range. Therefore,
the range of y = f (x) = 3x2 + 2x + 4 is also the set of real numbers R.
The domain of log x is
]0, +∞[
which is open, because x ̸= 0. Whereas, the range of log x is
] −∞, +∞[.
The domain of √x is
[0, +∞[
which is half-open, because
√
0 = 0, and +∞has no precise value. Similarly, the
range of √x is
[0, +∞[.
Sometimes,afunctionissensitivetoonespeciﬁcnumber.Forexample,inthefunction
y = f (x) =
1
x −1,
when x = 1, there is a divide by zero, which is meaningless. Consequently, the
domain of f (x) is the set of real numbers R, apart from 1.
3.6.5 Odd and Even Functions
An odd function satisﬁes the condition:
f (−x) = −f (x)
where x is located in a valid domain. Consequently, the graph of an odd function is
symmetrical relative to the x-axis, relative to the origin. For example, sin(α) is odd
because
sin(−α) = −sin(α)

3.6 Functions
51
Fig. 3.6 The sine function is
an odd function
as illustrated in Fig.3.6. Other odd functions include:
f (x) = ax
f (x) = ax3.
An even function satisﬁes the condition:
f (−x) = f (x)
where x is located in a valid domain. Consequently, the graph of an even function is
symmetrical relative to the f (x) axis. For example, cos(α) is even because
cos(−α) = cos(α)
as illustrated in Fig.3.7. Other even functions include:
f (x) = ax2
f (x) = ax4.
Fig. 3.7 The cosine function
is an even function

52
3
Algebra
3.6.6 Power Functions
Functions of the form f (x) = xn are called power functions of degree n and are
either odd or even. If n is an odd natural number, then the power function is odd, else
if n is an even natural number, then the power function is even.
3.7 Worked Examples
3.7.1 Algebraic Manipulation
Rearrange the following equations to make y the subject.
7 = x + 4
3 −y ,
23 = x + 68
3 + 1
ey
,
23 =
x + 68
3 −sin y .
7 = x + 4
3 −y
3 −y = x + 4
7
y = 3 −x + 4
7
= 17 −x
7
.
23 = x + 68
3 + 1
ey
3 + 1
ey = x + 68
23
1
ey = x + 68
23
−3
= x −1
23
ey =
23
x −1
y = ln
 23
x −1

.

3.7 Worked Examples
53
23 =
x + 68
3 −sin y
3 −sin y = x + 68
23
sin y = 3 −x + 68
23
= 1 −x
23
y = arcsin
1 −x
23

.
3.7.2 Solving a Quadratic Equation
Solve the following quadratic equations, and test the answers.
0 = x2 + 4x + 1,
0 = 2x2 + 4x + 2,
0 = 2x2 + 4x + 4.
0 = x2 + 4x + 1
x = −b ±
√
b2 −4ac
2a
= −4 ± √16 −4
2
= −4 ±
√
12
2
= −2 ±
√
3.
Test with x = −2 +
√
3.
x2 + 4x + 1 = (−2 +
√
3)2 + 4(−2 +
√
3) + 1
= 4 −4
√
3 + 3 −8 + 4
√
3 + 1
= 0.
Test with x = −2 −
√
3.
x2 + 4x + 1 = (−2 −
√
3)2 + 4(−2 −
√
3) + 1
= 4 + 4
√
3 + 3 −8 −4
√
3 + 1
= 0.
0 = 2x2 + 4x + 2

54
3
Algebra
x = −b ±
√
b2 −4ac
2a
= −4 ± √16 −16
4
= −4
4
= −1.
Test with x = −1.
2x2 + 4x + 2 = 2 −4 + 2
= 0.
0 = 2x2 + 4x + 4
x = −b ±
√
b2 −4ac
2a
= −4 ± √16 −32
4
= −4 ± √−16
4
= −1 ±
√
−1
= −1 ± i.
Test with x = −1 + i.
2x2 + 4x + 4 = 2(−1 + i)2 + 4(−1 + i) + 4
= 2(1 −2i −1) −4 + 4i + 4
= −4i + 4i
= 0.
Test with x = −1 −i.
2x2 + 4x + 4 = 2(−1 −i)2 + 4(−1 −i) + 4
= 2(1 + 2i −1) −4 −4i + 4
= 4i −4i
= 0.

3.7 Worked Examples
55
3.7.3 Factorising
Factorise the following equations:
4 sin2 x −4 cos2 x
9 sin2 x + 6 sin x cos x + cos2 x
25 sin2 x + 10 sin x cos x + cos2 x.
4 sin2 x −4 cos2 x = (2 sin x + 2 cos x)(2 sin x −2 cos x)
9 sin2 x + 6 sin x cos x + cos2 x = (3 sin x + cos x)2
25 sin2 x + 10 sin x cos x + cos2 x = (5 sin x + cos x)2.

Chapter 4
Logic
4.1 Introduction
The English mathematician George Boole (1815–1864) is regarded as the “father”
of symbolic logic, which is why it bears his name. He did not associate logic with
mathematics, but wanted to devise a logical framework for expressing and analysing
logical statements. A logical statement contains one or more premises (or proposi-
tions), that form the basis of an argument. However, not all premises are true, and
starting from an incorrect premise is not a good strategy for winning an argument,
therefore one must anticipate the existence of valid and invalid premises. Complex
arguments often combine individual premises using the logical connectives negation
(NOT), conjunction (AND), inclusive disjunction (OR) and the exclusive disjunction
(XOR). Today, logic is considered to play a central role in mathematics, and Russell
believed that mathematics could be derived entirely from logic.
4.2 Truth Tables
In algebra, variables can assume an inﬁnite range of numerical values, and equations
often require careful analysis to discover their roots. However, in logic, variables
possess two states: true (T) or false (F), or the binary states: 1 and 0, respectively.
Consequently, when two or more logical variables are combined, the number of pos-
sible combinations is ﬁnite, and often restricted to a dozen or so. These combinations
are easily summarised in the form of a truth table, which automatically reveals the
logical “roots” of the statement being investigated. For example, a single variable p
can only assume two possible logic states T or F, or using binary 1 or 0, as shown
in Table4.1. With two variables p and q, the number of combinations increases
to 4 = 22, as shown in Table4.2. With three variables p, q and r, the number of
combinations increases to 8 = 23, as shown in Table4.3.
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_4
57

58
4
Logic
Table 4.1 Truth table for one logic variable
p
T
F
Table 4.2 Truth table for two logic variables
p
q
T
T
T
F
F
T
F
F
Table 4.3 Truth table for three logic variables
p
q
r
T
T
T
T
T
F
T
F
T
T
F
F
F
T
T
F
T
F
F
F
T
F
F
F
4.2.1 Logical Connectives
Over the past century, logicians have developed a framework for describing and
analysing logical statements using propositions and logical connectives. Unfortu-
nately, the logical connectives developed by logicians are not found on a computer
keyboard, and the computer science community has replaced them by other charac-
ters. Table4.4 shows the correspondence between the logical connectives used by
logicians and computer scientists. It also shows the priority of each connective to
control the sequence of evaluation.

4.3 Logical Premises
59
Table 4.4 Correspondence of logical connectives
Operation
Meaning
Logic
Computer science Priority
Negation
NOT
¬
! or ∼
1
Conjunction
AND
∧
&
2
Inclusive
disjunction
OR
∨
|
3
Exclusive
disjunction
XOR
⊕
X
3
Implication
IMPLIES
⇒
→
4
Equivalence
EQUIVALENCE
⇔
↔
4
4.3 Logical Premises
4.3.1 Material Equivalence
The premises “Descartes is human” and “Descartes is mortal” are both true, but they
are not equivalent. For example, you don’t have to be human in order to be mortal,
but you do have to be mortal in order to be human. Therefore, being mortal is a
necessary condition for being human, but is not sufﬁcient, because Descartes could
be the pet name for a donkey, which although mortal, is not human.
Now consider the premises “Descartes is human” and “Descartes is a man”, where
once again they are true, but not equivalent. For example, you don’t have to be a man
to be human, but you do have to be human to be a man. Therefore, being a man is a
sufﬁcient condition for being human, but is not necessary, because Descartes could
refer to Madame Descartes, who is not a man, but still human.
Speciﬁcally, if p and q are premises such that p implies q, then p is a sufﬁcient
condition for q, and q is a necessary condition for p. If p is both a necessary and
sufﬁcient condition for q to be true, then p and q are logically equivalent. Such a
condition is usually expressed using “if and only if ”. For example, “Descartes is
married if and only if he has a wife”. This is also expressed as “Descartes is married
iff he has a wife”. Therefore p and q are equivalent if p is true and q is true, and
p is false and q is false. This material equivalence is written p ↔q as shown in
Table4.5.
Table 4.5 Equivalence: p ↔q
p
q
p ↔q
T
T
T
T
F
F
F
T
F
F
F
T

60
4
Logic
Table 4.6 Implication p →q
p
q
p →q
T
T
T
T
F
F
F
T
T
F
F
T
4.3.2 Implication
One observation in logical reasoning is that a false premise can lead to a false or true
conclusion. Furthermore, it is false that a true premise leads to a false conclusion.
However, a true premise must always lead to a true conclusion. Such a relationship
is called implication. For example, if n ∈N then n2 ∈N, and if n /∈N then n2 /∈N.
However, starting with a false value of n, such as n = 2 + 2i then it is false that
n2 ∈N. But starting with n = −i, which is also false, leads to n2 = 1 which satisﬁes
n2 ∈N. Therefore, given two premises p and q, where p implies q, this is written
p →q, as shown in Table4.6.
Let’s examine the action of the other connectives using truth tables. In some cases,
different combinations of T and F produce a true or false result, whilst others always
produce a true result. This latter condition is called a tautology.
4.3.3 Negation
Negation is the act of reversing a logical state. For example, if p is false, then ¬p is
true, and vice versa, as shown in Table4.7.
4.3.4 Conjunction
Conjunction is the linking together of two or more premises using the ∧connective.
Table4.8 shows the action of conjunction with two premises p and q, where p ∧q is
true only when both p and q are true, otherwise it is false. Table4.9 shows the result
with three premises.
Table 4.7 Negation: ¬p
p
¬p
T
F
F
T

4.3 Logical Premises
61
Table 4.8 Conjunction: p ∧q
p
q
p∧q
T
T
T
T
F
F
F
T
F
F
F
F
Table 4.9 Conjunction: p ∧q ∧r
p
q
r
p∧q∧r
T
T
T
T
T
T
F
F
T
F
T
F
T
F
F
F
F
T
T
F
F
T
F
F
F
F
T
F
F
F
F
F
Table 4.10 Inclusive disjunction: p ∨q
p
q
p∨q
T
T
T
T
F
T
F
T
T
F
F
F
4.3.5 Inclusive Disjunction
Table4.10 shows the action of inclusive disjunction, where p ∨q is true when either
p or q, or both are true, otherwise it is false.
4.3.6 Exclusive Disjunction
Table4.11 shows the action of exclusive disjunction, where p⊕q is true when either
p or q is true, but not both, otherwise it is false.

62
4
Logic
Table 4.11 Exclusive disjunction: p ⊕q
p
q
p⊕q
T
T
F
T
F
T
F
T
T
F
F
F
Table 4.12 Idempotence of ∨: p ↔p ∨p
p
p∨p
↔p∨p
T
T
T
F
F
T
p
Table 4.13 Idempotence of ∧: p ↔p ∧p
p
p∧p
p ↔p∧p
T
T
T
F
F
T
4.3.7 Idempotence
Tables4.12 and 4.13 contain the unusual term: idempotence, which was introduced
by the American mathematician Benjamin Peirce to clarify certain mathematical or
logical operations, and literally means “having the same power” or is “not affected
by”. For example, in algebra, 1 × 1 = 1, where the number 1 is idempotent (not
affected by) multiplication by itself. In logic, p ∨p = p, where ∨is idempotent
when associated with two equal premises, and gives the same result p. Table4.12
shows the idempotence of ∨, and Table4.13 shows the idempotence of ∧.
4.3.8 Commutativity
In simple arithmetic statements, the order of elements does not affect the numerical
result. For example, 4 + 6 = 6 + 4 and xy = yx. This is called commutativity. In
logic, something similar exists when two premises are linked together with ∨or ∧.
For example, p ∨q is identical to q ∨p, and p ∧q is identical to q ∧p. Table4.14
shows the commutativity of ∨, and Table4.15 shows the commutativity of ∧.

4.3 Logical Premises
63
Table 4.14 Commutativity of ∨: p ∨q ↔q ∨p
p
q
p∨q
q∨p
p∨q ↔q∨p
T
T
T
T
T
T
F
T
T
T
F
T
T
T
T
F
F
F
F
T
Table 4.15 Commutativity of ∧: p ∧q ↔q ∧p
p
q
p∧q
q∧p
p∧q ↔q∧p
T
T
T
T
T
T
F
F
F
T
T
T
F
F
T
F
F
F
F
T
Table 4.16 Associativity of ∨: (p ∨q) ∨r ↔p ∨(q ∨r)
p
q
r
(p∨q)∨r
p∨(q∨r)
(p∨q)∨r ↔p∨(q∨r)
T
T
T
T
T
T
T
T
F
T
T
T
T
F
T
T
T
T
T
F
F
T
T
T
F
T
T
T
T
T
F
T
F
T
T
T
F
F
T
T
T
T
F
F
F
F
F
T
4.3.9 Associativity
In simple arithmetic statements, the grouping of elements does not affect the numer-
ical result. For example, 2 + (3 + 4) = (2 + 3) + 4 and x(yz) = (xy)z. This is
called associativity. In logic, something similar exists when two or more premises
are linked together with ∨or ∧. For example, (p ∨q) ∨r is identical to p ∨(q ∨r),
and (p ∧q) ∧r is identical to p ∧(q ∧r). Table4.16 shows the associativity of ∨,
and Table4.17 shows the associativity of ∧.

64
4
Logic
Table 4.17 Associativity of ∧: (p ∧q) ∧r ↔p ∧(q ∧r)
p
q
r
(p∧q)∧r
p∧(q∧r)
(p∧q)∧r ↔p∧(q∧r)
T
T
T
T
T
T
T
T
F
F
F
T
T
F
T
F
F
T
T
F
F
F
F
T
F
T
T
F
F
T
F
T
F
F
F
T
F
F
T
F
F
T
F
F
F
F
F
T
Table 4.18 Distributivity of ∧over ∨: p ∧(q ∨r) ↔(p ∧q) ∨(p ∧r)
p
q
r
p∧(q∨r)
(p∧q)∨(p∧r)
p∧(q∨r) ↔(p∧q)∨(p∧r)
T
T
T
T
T
T
T
T
F
T
T
T
T
F
T
T
T
T
T
F
F
F
F
T
F
T
T
F
F
T
F
T
F
F
F
T
F
F
T
F
F
T
F
F
F
F
F
T
4.3.10 Distributivity
The distributive law of algebra permits us to expand x(y + z) into xy + xz. Similarly,
in logic, the laws of distributivity permit us to write p ∧(q ∨r) ↔(p ∧q) ∨(p ∧r)
and p ∨(q ∧r) ↔(p ∨q) ∧(p ∨r), as shown in Tables4.18 and 4.19.
4.3.11 de Morgan’s Laws
The British mathematician and logician Augustus de Morgan (1806–1871) formu-
lated what are now known as de Morgan’s Laws, as shown in Tables4.20 and 4.21.

4.3 Logical Premises
65
Table 4.19 Distributivity of ∨over ∧: p ∨(q ∧r) ↔(p ∨q) ∧(p ∨r)
p
q
r
p∨(q∧r)
(p∨q)∧(p∨r)
p∨(q∧r) ↔(p∨q)∧(p∨r)
T
T
T
T
T
T
T
T
F
T
T
T
T
F
T
T
T
T
T
F
F
T
T
T
F
T
T
F
F
T
F
T
F
F
F
T
F
F
T
F
F
T
F
F
F
F
F
T
Table 4.20 de Morgan’s Law: ¬(p ∨q) ↔¬p ∧¬q
p
q
¬(p∨q)
¬p∧¬q
¬(p∨q) ↔¬p∧¬q
T
T
F
F
T
T
F
F
F
T
F
T
F
F
T
F
F
T
T
T
Table 4.21 de Morgan’s Law: ¬(p ∧q) ↔¬p ∨¬q
p
q
¬(p∧q)
¬p∨¬q
¬(p∧q) ↔¬p∨¬q
T
T
F
F
T
T
F
T
T
T
F
T
T
T
T
F
F
T
T
T
4.3.12 Simpliﬁcation
Some statements are so obvious it seems unnecessary to consider them. Nevertheless,
their existence should be recognised, as they can help simplify complex logical
statements. For example, p ∨T must always be true, irrespective of p. Similarly,
p ∧F must always be false. Table4.22 shows the equivalence p ∨T ↔T, and
Table4.23 shows the equivalence p ∧F ↔F. Another form of simpliﬁcation arises
with p ∨F and p ∧T, which both equal p, as shown in Tables4.24 and 4.25.

66
4
Logic
Table 4.22 Simpliﬁcation: p ∨T ↔T
p
p∨T
p∨T ↔T
T
T
T
F
T
T
Table 4.23 Simpliﬁcation: p ∧F ↔F
p
p∧F
p∧F ↔F
T
F
T
F
F
T
Table 4.24 Simpliﬁcation: p ∨F ↔p
p
p∨F
p∨F ↔p
T
T
T
F
F
T
Table 4.25 Simpliﬁcation: p ∧T ↔p
p
p∧T
(p∧T) ↔p
T
T
T
F
F
T
4.3.13 Excluded Middle
A condition known as the excluded middle arises with the choice p ∨¬p, which,
after a little thought, must always be true. For when p is true, its negation is false,
and vice versa, which guarantees either scenario being true, as shown in Table4.26.
4.3.14 Contradiction
A condition known as contradiction arises with the combination p∧¬p, which after
a little more thought, must always be false. For p and ¬p can never be equivalent,
making a conjunction impossible, as shown in Table4.27.

4.3 Logical Premises
67
Table 4.26 Excluded middle: (p ∨¬p) ↔T
p
¬p
p∨¬p
T
F
T
F
T
T
Table 4.27 Contradiction: (p ∧¬p) ↔F
p
¬p
p∧¬p
T
F
F
F
T
F
Table 4.28 Double negation: ¬(¬p) ↔p
p
¬(¬p)
¬(¬p) ↔p
T
T
T
F
F
T
Table 4.29 Implication: p →q ↔¬p ∨q
p
q
p →q
¬p∨q
p →q ↔¬p∨q
T
T
T
T
T
T
F
F
F
T
F
T
T
T
T
F
F
T
T
T
4.3.15 Double Negation
Knowingthattwonegativesmakeapositive,itwillcomeasnosurprisethat¬(¬p) ↔
p, as shown in Table4.28.
4.3.16 Implication and Equivalence
The truth table for implication has already been covered in Table4.6, however, impli-
cation is also expressed by p →q ↔¬p ∨q, as shown in Table4.29. Similarly, the
truth table for equivalence (Table4.5) is also expressed by (p ↔q) ↔(p →q)
∧(q →p), as shown in Table4.30.

68
4
Logic
Table 4.30 Equivalence: (p ↔q) ↔(p →q) ∧(q →p)
p
q
p ↔q
p →q
q →p
A ≡(p →q)∧(q →p)
(p ↔q) ↔A
T
T
T
T
T
T
T
T
F
F
F
T
F
T
F
T
F
T
F
F
T
F
F
T
T
T
T
T
Table 4.31 Exportation: (p ∧q) →r ↔p →(q →r)
p
q
r
p∧q
A ≡(p∧q) →r
q →r
B ≡p →(q →r)
A ↔B
T
T
T
T
T
T
T
T
T
T
F
T
F
F
F
T
T
F
T
F
T
T
T
T
T
F
F
F
T
T
T
T
F
T
T
F
T
T
T
T
F
T
F
F
T
F
T
T
F
F
T
F
T
T
T
T
F
F
F
F
T
T
T
T
4.3.17 Exportation
Exportation covers the equivalence (p ∧q) →r ↔p →(q →r), as shown in
Table4.31.
4.3.18 Contrapositive
The law of contrapositive, is also known as modus tollens, and acknowledges the
negative form of p →q, i.e. ¬q →¬p, as shown in Table4.32.
4.3.19 Reductio Ad Absurdum
Reductio Ad Absurdum is Latin for “reduction to absurdity” and describes a form
of argument that proposes a statement is true, by claiming that its opposite leads to
an impossible or absurd result. For example, “men must have knees, otherwise they
wouldn’t be able to kneel down and propose!” There is a wonderful story concerning

4.3 Logical Premises
69
Table 4.32 Contrapositive: p →q ↔¬q →¬p
p
q
¬p
¬q
p →q
¬q →¬p
p →q ↔¬q →¬p
T
T
F
F
T
T
T
T
F
F
T
F
F
T
F
T
T
F
T
T
T
F
F
T
T
T
T
T
Table 4.33 Reductio ad absurdum: (p →q) ∧(p →¬q) →¬p
p
q
p →q
p →¬q
(p →q)∧(p →¬q)
(p →q)∧(p →¬q) →¬p
T
T
F
F
F
T
T
F
F
T
F
T
F
T
T
T
T
T
F
F
T
T
T
T
Table 4.34 Reductio ad absurdum: (¬p →q) ∧(¬p →¬q) →p
p
q
¬p →q
¬p →¬q
(¬p →q)∧(¬p →¬q)
(¬p →q)∧(¬p →¬q) →p
T
T
T
T
T
T
T
F
T
T
T
T
F
T
F
F
F
T
F
F
F
T
F
T
Bertrand Russell, who, during a lecture on logic, mentioned that in the sense of
material implication, a false proposition implies any proposition. A bright student
raised his hand and said “In that case, given that 1 = 0, prove that you are the Pope”.
Russell immediately replied, “Add 1 to both sides of the equation: then we have
2 = 1. The set containing just me and the Pope has 2 members. But 2 = 1, so it has
only 1 member; therefore, I am the Pope.”
We can express this fallacious form of argument by examining the conjunction:
(p →q) ∧(p →¬q), which posits that p implies q and ¬q, which is absurd.
Table4.33 reveals that (p →q) ∧(p →¬q) →¬p, which is a useless result, and
Table4.34 shows its negative form.
4.3.20 Modus Ponens
Modus ponens is Latin for afﬁrming mode and describes an argument containing
three parts: a major premise, a minor premise, and a conclusion. For example, “If

70
4
Logic
Table 4.35 Modus Ponens: (p →q) ∧p →q
p
q
p →q
(p →q)∧p
(p →q)∧p →q
T
T
T
T
T
T
F
F
T
T
F
T
T
F
T
F
F
T
F
T
Table 4.36 Proof by Cases: [(p ∨q) ∧(p →r) ∧(q →r)] →r
p
q
r
p∨q
p →r
q →r
A ≡[(p∨q)∧(p →r)∧(q →r)]
A →r
T
T
T
T
T
T
T
T
T
T
F
T
F
F
F
T
T
F
T
T
T
T
T
T
T
F
F
T
F
T
F
T
F
T
T
T
T
T
T
T
F
T
F
T
T
F
F
T
F
F
T
F
T
T
F
T
F
F
F
F
T
T
F
T
an integer is positive, then it is a natural number”, is the major proposition. “23 is
a positive integer”, is the minor proposition. From which, we conclude that 23 is a
natural number. By letting p stand for a positive integer, and q stand for a natural
number,themajorpropositiontakestheformp →q.Theminorpropositionissimply
p, which makes (p →q) ∧p, which in turn, implies q, as shown in Table4.35.
4.3.21 Proof by Cases
Table4.36 shows the principle known as proof by cases where if at least one of p or
q is true, and each implies r, then r must be true as well: [(p∨q)∧(p →r)∧(q →
r)] →r.
Truth tables are extremely useful in the design of microelectronic logic gates,
whose elements contain AND, OR, NOT, NAND, NOR, XOR and XNOR. They
also play a role in clarifying logical outcomes in programming, but their principal
weakness is their size, which is proportional to 2n, where n is the number of logical
premises. The above truth tables are summarised in Table4.37.

4.4 Set Theory
71
Table 4.37 Logical identities
Identity
Law
1
¬(¬p) ↔p
Double negation
2
p∨T ↔T
Simpliﬁcation
3
p∧F ↔F
Simpliﬁcation
4
p∨F ↔p
Simpliﬁcation
5
p∧T ↔p
Simpliﬁcation
6
p∨¬p ↔T
Excluded middle
7
p∧¬p ↔F
Contradiction
8
p∨p ↔p
Idempotence of ∨
9
p∧p ↔p
Idempotence of ∧
10
p∨q ↔q∨p
Commutativity of ∨
11
p∧q ↔q∧p
Commutativity of ∧
12
(p∨q)∨r ↔p∨(q∨r)
Associativity of ∨
13
(p∧q)∧r ↔p∧(q∧r)
Associativity of ∧
14
¬(p∨q) ↔¬p∧¬q
de Morgan’s Law
15
¬(p∧q) ↔¬p∨¬q
de Morgan’s Law
16
p∧(q∨r) ↔(p∧q)∨(p∧r)
Distributivity of ∧over ∨
17
p∨(q∧r) ↔(p∨q)∧(p∨r)
Distributivity of ∨over ∧
18
p →q ↔¬p∨q
Implication
19
p ↔q ↔(p →q)∧(q →p)
Equivalence
20
(p∧q) →r ↔p →(q →r)
Exportation
21
p →q ↔¬q →¬p
Contrapositive
22
(p →q)∧(p →¬q) →¬p
Reductio ad absurdum
23
(¬p →q)∧(¬p →¬q) →p
Reductio ad absurdum
24
(p →q)∧q →q
Modus ponens
25
(p∨q)∧(p →r)∧(q →r) →r
Proof by cases
4.4 Set Theory
We have already covered some of the ideas behind set theory, especially Cantor’s
work in classifying inﬁnite sets. Bertrand Russell’s paradox showed that the set of all
sets could never be a set referenced by another set, which kept alive the search for an
alternative system. In 1902, the German logician and mathematician Ernst Zermelo
(1871–1953) published a paper on adding transﬁnite cardinals, and in 1908 published
another paper on Zermelo Set Theory. This was developed by the Norwegian math-
ematician Thoralf Skolem (1887–1963) and the German-born Israeli mathematician

72
4
Logic
Abraham Fraenkel (1891–1965), which resulted in the Zermelo-Fraenkel set theory,
abbreviated to ZF. This system builds upon the empty set and Cantor’s power set, but
does not accept that all collections of sets constitute a set, which prevents recursive
scenarios.
In the late 19th-century, the English logician and philosopher John Venn (1834–
1923) introduced a graphical technique using circles to represent sets, whose rela-
tionships are reﬂected in the nesting or intersection of the circles. He referred to them
as Euler diagrams, as Euler had previously used them. However, Venn popularised
their usage, which is why now they bear his name: Venn Diagrams.
Developing the deﬁnition that a set is a collection of objects, let’s examine empty
sets, set building, combining sets, and Cantor’s power set, using Venn diagrams.
4.4.1 Empty Set
An empty set is a set with no members. For example, if a farmer owns a set comprising
a ﬁeld of ﬁve donkeys {Betty, George, Albert, Descartes, Mary} and one night they
are all stolen, the farmer now owns an empty set in the form of an empty ﬁeld,
represented by ∅, or {}. One is tempted to question whether the ﬁeld really belongs
to the set of donkeys, to which the answer is no, but it does provide a mental device
for visualising the concept of emptiness. Although the concept of an empty set is
fundamental to ZF, and can be manipulated constructively, it is an abstract idea and
remains a topic for continued discussion.
4.4.2 Membership and Cardinality of a Set
We have already discovered that the symbol ∈means “member of” or “belongs to”
which permits us to write:
if
S = {a, e, i, o, u} then a ∈S, e ∈S, i ∈S, o ∈S, u ∈S.
In this example, the set S contains 5 elements, written |S| = 5, and is called the
cardinality of S.
A set element may also be another set. For example,
if
S = {a, b, c, {d, e}} then a ∈S, b ∈S, c ∈S, {d, e} ∈S.
Therefore, the set’s cardinality is |S| = 4.

4.4 Set Theory
73
4.4.3 Subsets, Supersets and the Universal Set
The set of natural numbers N can be divided into various subsets. For example, if E
is the set of all even natural numbers, and O is the set of all odd natural numbers,
then E and O are subsets of N, written E ⊂N, and O ⊂N. (The symbol ⊆is
also used in place of ⊂.) For any problem associated with sets, there is an associated
universal set to which the sets belong. In this case, N is the universal set, as shown
in the Venn diagram in Fig.4.1.
Conversely, if E ⊂N and O ⊂N, then N is a superset of E and O, written
N ⊃E and N ⊃O. (The symbol ⊇is also used in place of ⊃.)
The two sets E and O can also be divided into two subsets PO for odd primes,
and PE for even primes, as shown in Fig.4.2. We could have also added the subset
of primes P, as shown in Fig.4.3.
Fig. 4.1 Odd and even
numbers as subsets of N
Fig. 4.2 PO and PE as
subsets of O and E
Fig. 4.3 P as a subset of
both O and E

74
4
Logic
4.4.4 Set Building
A set is constructed using the notation {< variable > | < predicate >} or
{< variable >:< predicate >}, where <variable> is an arbitrary name given to
the set’s elements, and <predicate> is a logical ﬁlter associated with the variable.
For example, S = {n | n ∈N ∧n ≥23}, reads “The elements of set S com-
prise n, such that n is a natural number and is greater than, or equal to 23”, i.e.
S = {23, 24, 25, 26, 27, . . . }.
Here are some more examples, where the existential quantiﬁer ∃is introduced,
which stands for “there exists”. For example, ∃m ∈N n = 2m, reads “there exists
an m belonging to N where n = 2m”.
S = {n | n ∈N ∧1 ≤n ≤5}
= {1, 2, 3, 4, 5}
S = {n | ∃m ∈N n = 2m}
= {2, 4, 6, 8, 10, . . . }
S = {n | ∃m ∈N n = 2m −1} = {1, 3, 5, 7, 9, . . . }
S = {n | ∃m ∈N n = m2}
= {1, 4, 9, 16, 25, . . . }.
4.4.5 Union
The union of two sets A and B, is another set combining their respective elements
without duplications, and is written A ∪B, which reads “A union B”, or “the union
of A and B”.
Figure4.4 shows the Venn diagram for two sets A and B, which are subsets of
the universal set U, and their union is represented by the complete shaded area. For
example, if
A = {Euler, Newton, Russell, Cantor}
B = {Gauss, Peano, Russell, Cantor}
A ∪B = {Euler, Newton, Russell, Cantor, Gauss, Peano}.
Fig. 4.4 The union of A and
B: A ∪B
B
U
A

4.4 Set Theory
75
Fig. 4.5 The intersection of
A and B: A ∩B
B
U
A
Similarly,
A = {1, 4, 6, 8, {23, 41}}
B = {{23, 41}, 3, 5, 8}
A ∪B = {{23, 41}, 1, 3, 4, 5, 6, 8}.
4.4.6 Intersection
The intersection of two sets A and B, is another set containing their common ele-
ments, and is written A ∩B, which reads “A intersection B”, or “the intersection of
A and B”.
Figure4.5 shows the Venn diagram for two sets A and B, which are subsets of the
universal set U, and their intersection is represented by the shaded area. For example,
A = {Euler, Newton, Russell, Cantor}
B = {Gauss, Peano, Russell, Cantor}
A ∩B = {Russell, Cantor}.
4.4.7 Relative Complement
The relative complement between two sets is the set of elements belonging to one,
but not the other. For example, the relative complement of B in A is written A \ B,
and represents all the elements belonging to A, and not B. For example,
A = {Euler, Newton, Russell, Cantor}
B = {Gauss, Peano, Russell, Cantor}
A \ B = {Euler, Newton}
B \ A = {Gauss, Peano}.

76
4
Logic
Fig. 4.6 The relative
complement of B in A:
A \ B
B
U
A
Fig. 4.7 The relative
complement of A in B:
B \ A
B
U
A
Figure4.6 shows the relationship A \ B, and Fig.4.7 shows the relationship B \ A.
As this can be a difﬁcult relationship to grasp, let’s give a formal deﬁnition of A
in B, and B in A:
B \ A = {x | x ∈B ∧x /∈A} which reads “an element x is in B but not A”.
A \ B = {x | x ∈A ∧x /∈B} which reads “an element x is in A but not B”.
Examples:
{a, b, c, d} \ {b, c, d, e} = {a}
{john, heidi, edwin, marie} \ {edwin, marie} = {john, heidi}
{23, 5, 41, 27, 3, 29, 2} \ {23, 5, 19, 41, 44, 29, 2} = {27, 3}.
4.4.8 Absolute Complement
The absolute complement (or complement) of a set A is the difference between the
associated universal set U and A, and written Ac = U \ A, as illustrated in Fig.4.8.
For example, if the universal set is N, and A = {n | n ∈N ∧n > 1} = {1}, then
Ac = N \ A} = 1. It follows that U c = ∅and ∅c = U.

4.4 Set Theory
77
U
A
Fig. 4.8 The absolute complement Ac = U \ A
Table 4.38 Power sets for different sets
S
P(S)
|S|
{a}
{∅, a}
2
{a,b}
{∅, a, b, {a, b}}
4
{a,b,c}
{∅, a, b, c, {a, b}, {b, c}, {a, c}, {a, b, c}}
8
4.4.9 Power Set
Given a set S, Cantor’s power set is the set of all subsets of S, which includes S and
the empty set ∅, and is written P(S). Therefore, if S = {a}, then P(S) = {∅, a}.
Table4.38 shows the power sets for sets with 1, 2 and 3 elements, where it become
clear that if a set has n elements, its power set contains 2n elements. Cantor used the
power set and the idea of one-to-one correspondence to reveal an inﬁnite hierarchy
of inﬁnities.
4.5 Worked Examples
4.5.1 Truth Tables
Design the truth tables for:
(p ∧q) ∨¬q
(p ∧q) ∨¬p
(p ∨q) ∧¬q
(p ∨q) ∧¬p

78
4
Logic
p
q
p∧q
¬q
(p∧q)∨¬q
T
T
T
F
T
T
F
F
T
T
F
T
F
F
F
F
F
F
T
T
p
q
p∧q
¬p
(p∧q)∨¬p
T
T
T
F
T
T
F
F
F
F
F
T
F
T
T
F
F
F
T
T
p
q
p∨q
¬q
(p∨q)∧¬q
T
T
T
F
F
T
F
T
T
T
F
T
T
F
F
F
F
F
T
F
p
q
p∨q
¬p
(p∨q)∧¬p
T
T
T
F
F
T
F
T
F
F
F
T
T
T
T
F
F
F
T
F
4.5.2 Set Building
State the sets or the positive real numbers, and the positive integers greater than 100.
S = {x | x ∈R ∧x > 0}
S = {n | n ∈Z ∧n > 100} = {101, 102, 103, 104, . . . }.

4.5 Worked Examples
79
4.5.3 Sets
Given
A = {1, 2, 3, 4, 5}
B = {2, 4, 6}
ﬁnd A ∪B, A ∩B, A \ B, B \ A.
A ∪B = {1, 2, 3, 4, 5, 6}
A ∩B = {2, 4}
A \ B = {1, 3, 5}
B \ A = {6}.
4.5.4 Power Set
Specify the power set for S = {1, 2, 3, 4}.
P(S) = {∅, 1, 2, 3, 4, {1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4},
{1, 2, 3}, {1, 2, 4}, {1, 3, 4}, {2, 3, 4}, {1, 2, 3, 4}}.

Chapter 5
Trigonometry
5.1 Introduction
This chapter covers some basic features of trigonometry such as angular measure,
trigonometric ratios, inverse ratios, trigonometric identities and various rules, with
which the reader should be familiar.
5.2 Background
The word “trigonometry” divides into three parts: “tri”, “gon”, “metry”, which means
the measurement of three-sided polygons, i.e. triangles. It is an ancient subject and
is used across all branches of mathematics.
5.3 Units of Angular Measurement
The measurement of angles is at the heart of trigonometry, and today two units of
angular measurement have survived into modern usage: degrees and radians. The
degree (or sexagesimal) unit of measure derives from deﬁning one complete rotation
as 360◦. Each degree divides into 60min, and each minute divides into 60s. The
number 60 has survived from Mesopotamian days and is rather incongruous when
used alongside today’s decimal system—which is why the radian has secured a strong
foothold in modern mathematics.
The radian of angular measure does not depend upon any arbitrary constant—it
is the angle created by a circular arc whose length is equal to the circle’s radius.
And because the perimeter of a circle is 2πr, 2π radians correspond to one com-
plete rotation. As 360◦correspond to 2π radians, 1rad equals 180◦/π, which is
approximately 57.3◦. The following relationships between radians and degrees are
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_5
81

82
5
Trigonometry
worth remembering:
π
2 [rad] ≡90◦,
π [rad] ≡180◦
3π
2 [rad] ≡270◦,
2π [rad] ≡360◦.
To convert x◦to radians:
πx◦
180 [rad].
To convert x [rad] to degrees:
180x
π
[degrees].
5.4 The Trigonometric Ratios
Ancient civilisations knew that triangles—whatever their size—possessed some
inherent properties, especially the ratios of sides and their associated angles. This
means that if these ratios are known in advance, problems involving triangles with
unknown lengths and angles, can be discovered using these ratios.
Figure5.1 shows a point P with coordinates (base, height), on a unit-radius circle
rotated through an angle θ. As P is rotated, it moves into the 2nd quadrant, 3rd
quadrant, 4th quadrant and returns back to the ﬁrst quadrant. During the rotation, the
sign of height and base change as follows:
1st
quadrant:
height (+), base (+)
2nd quadrant:
height (+), base (−)
3rd quadrant:
height (−), base (−)
4th quadrant:
height (−), base (+).
Fig. 5.1 The four quadrants
for the trigonometric ratios
height
radius
base
+
+
_
_
1st quadrant
2nd quadrant
4th quadrant
P
3rd quadrant

5.4
The Trigonometric Ratios
83
Fig. 5.2 The graph of
height over the four
quadrants
height
1
1st
quadrant 
2nd
quadrant 
3rd
quadrant 
4th
quadrant 
 = 2
-1
Fig. 5.3 The graph of base
over the four quadrants
base
1
1st
quadrant 
2nd
quadrant 
3rd
quadrant 
4th
quadrant 
 = 2
-1
Figures5.2 and 5.3 plot the changing values of height and base over the four
quadrants, respectively. When radius = 1, the curves vary between 1 and −1. In the
context of triangles, the sides are labelled as follows:
hypotenuse = radius
opposite = height
adjacent = base.
Thus, using the right-angle triangle shown in Fig.5.4, the trigonometric ratios: sine,
cosine and tangent are deﬁned as
sin θ =
opposite
hypotenuse,
cos θ =
adjacent
hypotenuse,
tan θ = opposite
adjacent .

84
5
Trigonometry
Fig. 5.4 Sides of a
right-angle triangle
hypotenuse
opposite
adjacent
The reciprocals of these functions, cosecant, secant and cotangent are also useful:
csc θ =
1
sin θ ,
sec θ =
1
cos θ ,
cot θ =
1
tan θ .
As an example, Fig.5.5 shows a triangle where the hypotenuse and an angle are
known. The other sides are calculated as follows:
opposite
10
= sin 40◦
opposite = 10 sin 40◦≈10 × 0.64278 = 6.4278
adjacent
10
= cos 40◦
adjacent = 10 cos 40◦≈10 × 0.7660 = 7.660.
The theorem of Pythagoras conﬁrms that these lengths are correct:
6.42782 + 7.6602 ≈102.
Figure5.6 shows the graph of the tangent function, which, like the sine and cosine
functions, is periodic, but with only a period of π radians.
Fig. 5.5 A right-angle
triangle with two unknown
sides
40o
10
opposite
adjacent

5.4
The Trigonometric Ratios
85
Fig. 5.6 Graph of the
tangent function
-2
-
2
-4
-3
-2
-1
1
2
3
4
tan
5.4.1 Domains and Ranges
The periodic nature of sin θ, cos θ and tan θ, means that their domains are inﬁnitely
large. Consequently, it is customary to conﬁne the domain of sin θ to

−π
2 , π
2

and cos θ to
[0, π].
The range for both sin θ and cos θ is
[−1, 1].
The domain for tan θ is the open interval

−π
2 , π
2

and its range is the open interval:

−∞, ∞

.
5.5 Inverse Trigonometric Ratios
The functions sin θ, cos θ, tan θ, csc θ, sec θ and cot θ provide different ratios for
the angle θ, and the inverse trigonometric functions convert a ratio back into an
angle. These are arcsin, arccos, arctan, arccsc, arcsec and arccot, and are sometimes
written as sin−1, cos−1, tan−1, csc−1, sec−1 and cot−1. For example, sin 30◦= 0.5,

86
5
Trigonometry
therefore, arcsin(0.5) = 30◦. Consequently, the domain for arcsin is the range for
sin:
[−1, 1]
and the range for arcsin is the domain for sin:

−π
2 , π
2

as shown in Fig.5.7. Similarly, the domain for arccos is the range for cos:
[−1, 1]
and the range for arccos is the domain for cos:
[0, π]
as shown in Fig.5.8.
The domain for arctan is the range for tan:
] −∞, ∞[
and the range for arctan is the domain for tan:

−π
2 , π
2

as shown in Fig.5.9.
Various programming languages include the atan2 function, which is an arctan
function with two arguments: atan2(y, x). The signs of x and y provide sufﬁcient
Fig. 5.7 Graph of the arcsin
function
1
-1
/2
- /2
x
arcsin x

5.5
Inverse Trigonometric Ratios
87
Fig. 5.8 Graph of the arccos
function
1
-1
0
x
arccos x
Fig. 5.9 Graph of the arctan
function
/2
- /2
x
arctan x
information to locate the quadrant containing the angle, and gives the atan2 function
a range of [0, 2π].
5.6 Trigonometric Identities
The sin and cos curves are identical, apart from being displaced by 90◦, and are
related by
cos θ = sin(θ + π/2).
Also, simple algebra and the theorem of Pythagoras can be used to derive other
formulae such as

88
5
Trigonometry
sin θ
cos θ = tan θ
sin2 θ + cos2 θ = 1
1 + tan2 θ = sec2 θ
1 + cot2 θ = csc2 θ.
5.7 The Sine Rule
Figure5.10 shows a triangle labeled such that side a is opposite angle A, side b is
opposite angle B, etc. The sine rule states:
a
sin A =
b
sin B =
c
sin C
which can be used to compute the length of an unknown length or angle. For example,
if A = 60◦, B = 40◦, C = 80◦, and b = 10, then
a
sin 60◦=
10
sin 40◦
rearranging, we have
a = 10 sin 60◦
sin 40◦
≈13.47.
Similarly:
c
sin 80◦=
10
sin 40◦
Fig. 5.10 An arbitrary
triangle
a
A
b
B
c
C

5.7 The Sine Rule
89
therefore
c = 10 sin 80◦
sin 40◦
≈15.32.
5.8 The Cosine Rule
The cosine rule expresses the sin2 θ + cos2 θ = 1 identity for the arbitrary triangle
shown in Fig.5.10. In fact, there are three versions:
a2 = b2 + c2 −2bc cos A
b2 = c2 + a2 −2ca cos B
c2 = a2 + b2 −2ab cos C.
Three further relationships also hold:
a = b cos C + c cos B
b = c cos A + a cos C
c = a cos B + b cos A.
5.9 Compound-Angle Identities
Trigonometric identities are useful for solving various mathematical problems, but
apart from this, their proof often contains a strategy that can be used else where. In
the ﬁrst example, watch out for the technique of multiplying by 1 in the form of a
ratio, and swapping denominators. The technique is rather elegant and suggests that
the result was known in advance, which probably was the case. Let’s begin by ﬁnding
a way of representing sin(α + β) in terms of sin α, cos α, sin β, cos β.
With reference to Fig.5.11:
sin(α + β) = FD
AD = BC + ED
AD
= BC
AD
AC
AC + ED
AD
CD
CD
= BC
AC
AC
AD + ED
CD
CD
AD
sin(α + β) = sin α cos β + cos α sin β.
(5.1)
To ﬁnd sin(α −β), reverse the sign of β in (5.1):
sin(α −β) = sin α cos β −cos α sin β.
(5.2)

90
5
Trigonometry
Fig. 5.11 The geometry to
expand sin(α + β)
A
B
C
D
E
F
Now let’s expand cos(α + β) with reference to Fig.5.11:
cos(α + β) = AE
AD = AB −EC
AD
= AB
AD
AC
AC −EC
AD
CD
CD
= AB
AC
AC
AD −EC
CD
CD
AD
cos(α + β) = cos α cos β −sin α sin β.
(5.3)
To ﬁnd cos(α −β), reverse the sign of β in (5.3):
cos(α −β) = cos α cos β + sin α sin β.
To expand tan(α + β), divide (5.1) by (5.3):
sin(α + β)
cos(α + β) = sin α cos β + cos α sin β
cos α cos β −sin α sin β
=
sin α cos β
cos α cos β + cos α sin β
cos α cos β
cos α cos β
cos α cos β −sin α sin β
cos α cos β
tan(α + β) = tan α + tan β
1 −tan α tan β .
(5.4)
To ﬁnd tan(α −β), reverse the sign of β in (5.4):
tan(α −β) = tan α −tan β
1 + tan α tan β .

5.9
Compound-Angle Identities
91
5.9.1 Double-Angle Identities
By making β = α, the three compound-angle identities
sin(α ± β) = sin α cos β ± cos α sin β
cos(α ± β) = cos α cos β ∓sin α sin β
tan(α ± β) = tan α ± tan β
1 ∓tan α tan β
provide the starting point for deriving three corresponding double-angle identities:
sin(α ± α) = sin α cos α ± cos α sin α
sin 2α = 2 sin α cos α.
Similarly,
cos(α ± α) = cos α cos α ∓sin α sin α
cos 2α = cos2 α −sin2 α
which can be further simpliﬁed using sin2 α + cos2 α = 1:
cos 2α = cos2 α −sin2 α
cos 2α = 2 cos2 α −1
cos 2α = 1 −2 sin2 α.
And for tan 2α, we have:,
tan(α ± α) = tan α ± tan α
1 ∓tan α tan α
tan 2α =
2 tan α
1 −tan2 α .
5.9.2 Multiple-Angle Identities
In Chap.11, a technique employing power series is given, which shows how the
following multiple-angle identities are computed:
sin 3α = 3 sin α −4 sin3 α
cos 3α = 4 cos3 α −3 cos α

92
5
Trigonometry
tan 3α = 3 tan α −tan3 α
1 −3 tan2 α
sin 4α = 4 sin α cos α −8 sin3 α cos α
cos 4α = 8 cos4 α −8 cos2 α + 1
tan 4α =
4 tan α −4 tan3 α
1 −6 tan2 α + tan4 α
sin 5α = 16 sin5 α −20 sin3 α + 5 sin α
cos 5α = 16 cos5 α −20 cos3 α + 5 cos α
tan 5α = 5 tan α −10 tan3 α + tan5 α
1 −10 tan2 α + 5 tan4 α
.
5.9.3 Half-Angle Identities
Every now and then, it is necessary to compute the sine, cosine or tangent of a half-
angle from the corresponding whole-angle functions. To do this, we rearrange the
double-angle identities as follows.
cos 2α = 1 −2 sin2 α
sin2 α = 1 −cos 2α
2
sin2 α
2 = 1 −cos α
2
sin α
2 = ±

1 −cos α
2
.
(5.5)
Similarly,
cos2 α = 1 + cos 2α
2
cos2 α
2 = 1 + cos α
2
cos α
2 = ±

1 + cos α
2
.
(5.6)
Dividing (5.5) by (5.6) we have
tan α
2 =

1 −cos α
1 + cos α .

5.10
Perimeter Relationships
93
5.10 Perimeter Relationships
Finally, with reference to Fig.5.10, we come to the relationships that integrate angles
with the perimeter of a triangle:
s = 1
2(a + b + c)
sin A
2 =

(s −b)(s −c)
bc
sin B
2 =

(s −c)(s −a)
ca
sin C
2 =

(s −a)(s −b)
ab
cos A
2 =

s(s −a)
bc
cos B
2 =

s(s −b)
ca
cos C
2 =

s(s −c)
ab
sin A = 2
bc

s(s −a)(s −b)(s −c)
sin B = 2
ca

s(s −a)(s −b)(s −c)
sin C = 2
ab

s(s −a)(s −b)(s −c).

Chapter 6
Coordinate Systems
6.1 Introduction
In this chapter we revise Cartesian coordinates, axial systems, the distance between
two points in space, and the area of simple 2D shapes. It also covers polar, spherical
polar and cylindrical coordinate systems.
6.2 Background
René Descartes is often credited with the invention of the xy-plane, but the French
lawyer and mathematician Pierre de Fermat (1601–1665) was probably the ﬁrst
inventor. In 1636 Fermat was working on a treatise titled Ad locus planos et soli-
dos isagoge, which outlined what we now call “analytic geometry”. Unfortunately,
Fermat never published his treatise, although he shared his ideas with other mathe-
maticians such as Blaise Pascal (1623–1662). At the same time, Descartes devised
his own system of analytic geometry and in 1637 published his results in the presti-
gious journal Géométrie. In the eyes of the scientiﬁc world, the publication date of
a technical paper determines when a new idea or invention is released into the pub-
lic domain. Consequently, ever since this publication Descartes has been associated
with the xy-plane, which is why it is called the Cartesian plane.
The Cartesian plane is such a simple idea that it is strange that it took so long
to be discovered. However, although it is true that René Descartes showed how
an orthogonal coordinate system could be used for graphs and coordinate geometry,
coordinates had been used by ancient Egyptians, almost 2000years earlier! If Fermat
had been more efﬁcient in publishing his research results, the xy-plane could have
been called the Fermatian plane! (Boyer and Merzbach, 1989).
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_6
95

96
6
Coordinate Systems
Fig. 6.1 The Cartesian plane
-5
-4
-3
-2
-1
0
1
2
3
4
5
-3
-2
-1
1
2
3
P(3, 2)
Q (-4, -2)
6.3 The Cartesian Plane
The Cartesian plane provides a mechanism for locating points with a unique, ordered
pair of numbers (x, y) as shown in Fig.6.1, where P has coordinates (3, 2) and Q has
coordinates (−4, −2). The point (0, 0) is called the origin. As previously mentioned,
Descartes suggested that the letters x and y should be used to represent variables,
and letters at the other end of the alphabet should stand for numbers. Which is why
equations such as y = ax2 + bx + c, are written this way.
The axes are said to be oriented as the x-axis rotates anticlockwise towards the
y-axis. They could have been oriented in the opposite sense, with the y-axis rotating
anticlockwise towards the x-axis.
6.4 Function Graphs
When functions such as
linear: y = mx + c,
quadratic: y = ax2 + bx + c,
cubic: y = ax3 + bx2 + cx + d,
trigonometric: y = a sin x,
are drawn as graphs, they create familiar shapes that permit the function to be easily
identiﬁed. Linear functions are straight lines; quadratics are parabolas; cubics have
an “S” shape; and trigonometric functions often possess a wave-like trace. Figure6.2
shows examples of each type of function.

6.5 Shape Representation
97
Fig. 6.2 Graphs of four
function types
quadratic
linear
cubic
sinusoid
6.5 Shape Representation
The Cartesian plane also provides a way to represent 2D shapes numerically, which
permits them to be manipulated mathematically. Let’s begin with 2D polygons and
show how their internal area can be calculated.
6.5.1 2D Polygons
A polygon is formed from a chain of vertices (points) as shown in Fig.6.3. A straight
line is assumed to connect each pair of neighbouring vertices; intermediate points
on the line are not explicitly stored. There is no convention for starting a chain
of vertices, but software will often dictate whether polygons have a clockwise or
anticlockwise vertex sequence.
Fig. 6.3 A simple polygon
created by a chain of vertices
1
2
3
4
5
1
2
3
(1, 1)
(1, 3)
(3, 2)
(3, 1)
x
y

98
6
Coordinate Systems
We can now subject this list of coordinates to a variety of arithmetic and mathe-
matical operations. For example, if we double the values of x and y and redraw the
vertices, we discover that the shape’s geometric integrity is preserved, but its size
is doubled relative to the origin. Similarly, if we divide the values of x and y by 2,
the shape is still preserved, but its size is halved relative to the origin. On the other
hand, if we add 1 to every x-coordinate, and 2 to every y-coordinate, and redraw the
vertices, the shape’s size remains the same but is displaced 1 unit horizontally and 2
units vertically.
6.5.2 Areas of Shapes
The area of a polygonal shape is readily calculated from its list of coordinates. For
example, using the list of coordinates shown in Table6.1: the area is computed by
area = 1
2[(x0y1 −x1y0) + (x1y2 −x2y1) + (x2y3 −x3y2) + (x3y0 −x0y3)].
You will observe that the calculation sums the results of multiplying an x by the
next y, minus the next x by the previous y. When the last vertex is selected, it is
paired with the ﬁrst vertex to complete the process. The result is then halved to reveal
the area. As a simple test, let’s apply this formula to the shape described in Fig.6.3:
area = 1
2[(1 × 1 −3 × 1) + (3 × 2 −3 × 1) + (3 × 3 −1 × 2) + (1 × 1 −1 × 3)]
area = 1
2[−2 + 3 + 7 −2] = 3.
which, by inspection, is the true area. The beauty of this technique is that it works
with any number of vertices and any arbitrary shape.
Another feature of the technique is that if the set of coordinates is clockwise,
the area is negative, which means that the calculation computes vertex orientation as
well as area. To illustrate this feature, the original vertices are reversed to a clockwise
sequence as follows:
Table 6.1 A polygon’s
coordinates
x
y
x0
y0
x1
y1
x2
y2
x3
y3

6.5 Shape Representation
99
area = 1
2[(1 × 3 −1 × 1) + (1 × 2 −3 × 3) + (3 × 1 −3 × 2) + (3 × 1 −1 × 1)]
area = 1
2[2 −7 −3 + 2] = −3.
The minus sign conﬁrms that the vertices are in a clockwise sequence.
6.6 Theorem of Pythagoras in 2D
The theorem of Pythagoras is used to calculate the distance between two points.
Figure6.4 shows two arbitrary points P1(x1, y1) and P2(x2, y2). The distance x =
x2 −x1 and y = y2 −y1. Therefore, the distance d between P1 and P2 is given by
d =

(x)2 + (y)2.
For example, given P1(1, 1), P2(4, 5), then d =
√
32 + 42 = 5.
6.7 3D Cartesian Coordinates
Two coordinates are required to locate a point on the 2D Cartesian plane, and three
coordinates are required for 3D space. The corresponding axial system requires three
mutually perpendicular axes; however, there are two ways to add the extra z-axis.
Figure6.5 shows the two orientations, which are described as left- and right-handed
axial systems. The left-handed system permits us to align our left hand with the axes
such that the thumb aligns with the x-axis, the ﬁrst ﬁnger aligns with the y-axis,
and the middle ﬁnger aligns with the z-axis. The right-handed system permits the
same system of alignment, but using our right hand. The choice between these axial
Fig. 6.4 Calculating the
distance between two points
P
P
x
y
y2
2
1
y1
x
x
1
2
d
y
x

100
6
Coordinate Systems
x
y
z
x
y
z
(a)
(b)
Fig. 6.5 a A left-handed system. b A right-handed system
Fig. 6.6 A right-handed
axial system showing the
coordinates of a point P
P
x
y
z
x
y
z
systems is arbitrary, but one should be aware of the system employed by commercial
computer graphics packages. The main problem arises when projecting 3D points
onto a 2D plane, which has an oriented axial system. A right-handed system is
employed throughout this book, as shown in Fig.6.6, which also shows a point P
with its coordinates. It also worth noting that handedness has no meaning in spaces
with 4 dimensions or more.
6.7.1 Theorem of Pythagoras in 3D
The theorem of Pythagoras in 3D is a natural extension of the 2D rule. In fact,
it even works in higher dimensions. Given two arbitrary points P1(x1, y1, z1) and
P2(x2, y2, z2), we compute x = x2 −x1, y = y2 −y1 and z = z2 −z1, from

6.7 3D Cartesian Coordinates
101
which the distance d between P1 and P2 is given by
d =

(x)2 + (y)2 + (z)2
and the distance from the origin to a point P(x, y, z) is simply
d =

x2 + y2 + z2.
Therefore, the point (3, 4, 5) is
√
32 + 42 + 52 ≈7.07 from the origin.
6.8 Polar Coordinates
Polar coordinates are used for handling data containing angles, rather than linear
offsets. Figure6.7 shows the convention used for 2D polar coordinates, where the
point P(x, y) has equivalent polar coordinates P(ρ, θ), where:
x = ρ cos θ
y = ρ sin θ
ρ =

x2 + y2
θ = arctan(y/x).
For example, the point Q(4, 0.8π) in Fig.6.7 has Cartesian coordinates:
x = 4 cos(0.8π) ≈−3.24
y = 4 sin(0.8π) ≈2.35
Fig. 6.7 2D polar
coordinates
-4
-3
-2
-1 0
1
2
3
4
-3
-2
-1
1
2
3
Q(4, 0.8 )
P( , ) 
P(x, y)

102
6
Coordinate Systems
and the point (3, 4) has polar coordinates:
ρ =

32 + 42 = 5
θ = arctan(4/3) ≈53.13◦.
These conversion formulae work only for the ﬁrst quadrant. The atan2 function
should be used in a software environment, as it works with all four quadrants.
6.9 Spherical Polar Coordinates
Figure6.8 shows one convention used for spherical polar coordinates, where the
point P(x, y, z) has equivalent polar coordinates P(ρ, φ, θ), where:
x = ρ sin φ cos θ
y = ρ sin φ sin θ
z = ρ cos φ
ρ =

x2 + y2 + z2
φ = arccos(z/ρ)
θ = arctan(y/x).
For example, the point (3, 4, 0) has spherical polar coordinates (5, 90◦, 53.13◦):
ρ =

32 + 42 + 02 = 5
φ = arccos(0/5) = 90◦
θ = arctan(4/3) ≈53.13◦.
Fig. 6.8 Spherical polar
coordinates

6.9 Spherical Polar Coordinates
103
Take great care when using spherical coordinates, as authors often swap φ with θ, as
well as the alignment of the Cartesian axes; not to mention using a left-handed axial
system in preference to a right-handed system!
6.10 Cylindrical Coordinates
Figure6.9 shows one convention used for cylindrical coordinates, where the point
P(x, y, z) has equivalent cylindrical coordinates P(ρ, θ, z), where
x = ρ cos θ
y = ρ sin θ
z = z
ρ =

x2 + y2
θ = arctan
 y
x

.
For example, the point (3, 4, 6) has cylindrical coordinates (5, 53.13◦, 6):
ρ =

32 + 42 = 5
θ = arctan
4
3

≈53.13◦
z = 6.
Again, be careful when using cylindrical coordinates to ensure compatibility.
Fig. 6.9 Cylindrical
coordinates
x
x
y
y
z
P( , , z) P(x, y, z )
z

104
6
Coordinate Systems
6.11 Barycentric Coordinates
Barycentric coordinates locate a point in space relative to existing points, rather than
to an origin, and are also known as local coordinates. The German mathematician
August Möbius (1790–1868) is credited with their invention.
Given two points in 2D space A(xa, ya) and B(xb, yb), an intermediate point
P(x p, yp) has barycentric coordinates:
x p = sxa + txb
yp = sya + tyb
1 = s + t.
For example, when s = t = 0.5, P is halfway between A and B. In 3D, we simply
add a z-coordinate.
Given three points in 3D space: A(xa, ya, za), B(xb, yb, zb) and C(xc, yc, zc),
then any point P(x p, yp, z p) inside the triangle ABC has barycentric coordinates:
x p = rxa + sxb + txc
yp = rya + syb + tyc
z p = rza + szb + tzc
1 = r + s + t.
Forexample,whenr = 1, s = t = 0,then P = A.Similarly,whens = 1,r = t = 0,
then P = B. When r = s = t = 1/3, then P is located at the triangle’s centroid.
6.12 Homogeneous Coordinates
Homogeneous coordinates surfaced in the early 19th century where they were inde-
pendently proposed by Möbius, Feuerbach, Bobillier, and Plücker. Homogeneous
coordinates, deﬁne a point in a plane using three coordinates instead of two. This
means that for a point (x, y) there exists a homogeneous point (xt, yt, t) where
t is an arbitrary number. For example, the point (3, 4) has homogeneous coordi-
nates (6, 8, 2), because 3 = 6/2 and 4 = 8/2. But the homogeneous point (6, 8, 2)
is not unique to (3, 4); (12, 16, 4), (15, 20, 5) and (300, 400, 100) are all possible
homogeneous coordinates for (3, 4).
The reason why this coordinate system is called “homogeneous” is because it is
possible to transform functions such as f (x, y) into the form f (x/t, y/t) without
disturbing the degree of the curve. To the non-mathematician this may not seem
anything to get excited about, but in the ﬁeld of projective geometry it is a very
powerful concept.

6.12 Homogeneous Coordinates
105
In 3D, a point (x, y, z) becomes (xt, yt, zt, t) and for may applications t = 1,
which seems a futile operation, but in matrix theory it is very useful, as we will
discover.
6.13 Worked Examples
6.13.1 Area of a Shape
Compute the area and orientation of the shape deﬁned by the coordinates in Table6.2.
area = 1
2[(2 × 2 −0 × 2) + (2 × 2 −2 × 1) + (1 × 1 −2 × 1) + (1 × 1 −1 × 0)]
= 1
2(4 + 2 −1 + 1)
= 3.
The shape is oriented anticlockwise, as the area is positive.
6.13.2 Distance Between Two Points
Find the distance d12 between P1(1, 1) and P2(6, 7), and d34 between P3(1, 1, 1)
and P4(7, 8, 9).
d12 =

(6 −1)2 + (7 −1)2 =
√
61 ≈7.81
d34 =

(7 −1)2 + (8 −1)2 + (9 −1)2 =
√
149 ≈12.21.
6.13.3 Polar Coordinates
Convert the 2D polar coordinates (3, π/2) to Cartesian form, and the point (4, 5) to
polar form.
Table 6.2 Shape coordinates
x
0
2
2
1
1
0
y
0
0
2
2
1
1

106
6
Coordinate Systems
ρ = 3
θ = π/2
x = ρ cos θ = 3 cos(π/2) = 0
y = ρ sin θ = 3 sin(π/2) = 3
therefore, (3, π/2) ≡(0, 3).
x = 4
y = 5
ρ =

x2 + y2 =

42 + 52 ≈6.4
θ = arctan(y/x) = arctan(5/4) ≈51.34◦
therefore, (4, 5) ≈(6.4, 51.34◦).
6.13.4 Spherical Polar Coordinates
Convert the spherical polar coordinates (10, π/2, 45◦) to Cartesian form, and the
point (3, 4, 5) to spherical form.
ρ = 10
φ = π/2
θ = 45◦
x = ρ sin φ cos θ = 10 sin(π/2) cos 45◦= 10
√
2/2 ≈7.07
y = ρ sin φ sin θ = 10 sin(π/2) sin 45◦= 10
√
2/2 ≈7.07
z = ρ cos φ = 10 cos(π/2) = 0
therefore, (10, π/2, 45◦) ≈(7.07, 7.07, 0).
x = 3
y = 4
z = 5
ρ =

x2 + y2 + z2 =

32 + 42 + 52 ≈7.07
φ = arccos(z/ρ) ≈arccos(5/7.07) = 45◦
θ = arctan(y/x) = arctan(4/3) ≈53.13◦
therefore, (3, 4, 5) ≈(7.07, 45◦, 53.13◦).

6.13 Worked Examples
107
6.13.5 Cylindrical Coordinates
Convert the 3D cylindrical coordinates (10, π/2, 5) to Cartesian form, and the point
(3, 4, 5) to cylindrical form.
ρ = 10
θ = π/2
z = 5
x = ρ cos θ = 10 cos(π/2) = 0
y = ρ sin θ = 10 sin(π/2) = 10
z = 5
therefore, (10, π/2, 5) ≡(0, 10, 5).
6.13.6 Barycentric Coordinates
Given A(xa, ya, za), B(xb, yb, zb) and C(xc, yc, zc), state the barycentric coordi-
nates for A, B, C the points Pab, Pbc and Pca mid-way between AB, BC and C A
respectively, and the centroid Pc.
Table6.3 shows the barycentric coordinates for ABC.
Table 6.3 Barycentric
coordinates
Point
r
s
t
A
1
0
0
B
0
1
0
C
0
0
1
Pab
0.5
0.5
0
Pbc
0
0.5
0.5
Pca
0.5
0
0.5
Pc
1/3
1/3
1/3

Chapter 7
Determinants
7.1 Introduction
When patterns of numbers or symbols occur over and over again, mathematicians
often devise a way to simplify their description and assign a name to them. For
example,
4

i=1
pαi
i
is shorthand for
pα1
1 pα2
2 pα3
3 pα4
4
and
4

i=1
pαi
i
is shorthand for
pα1
1 + pα2
2 + pα3
3 + pα4
4 .
A determinant is another example of this process, and is a value derived from a
square matrix of terms, often associated with sets of equations. Such problems were
studied by the Babylonians around 300 BC and by the Chinese, between 200 BC
and 100 BC. Since then many mathematicians have been associated with the evolu-
tion of determinants and matrices, including Girolamo Cardano (1501–1576), Jan de
Witt (1625–1672), Takakazu Seki (1642–1708), Gottfried von Leibniz, Guillaume
de L’Hôpital (1661–1704), Augustin-Louis Cauchy (1789–1857), Pierre Laplace
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_7
109

110
7
Determinants
(1749–1827) and Arthur Cayley (1821–1895). To understand the rules used to com-
pute a determinant’s value, we need to understand their origin, which is in the solution
of sets of linear equations.
7.2 Linear Equations with Two Variables
Consider the following linear equations where we want to ﬁnd values of x and y that
satisfy both equations:
7 = 3x + 2y
(7.1)
10 = 2x + 4y.
(7.2)
A standard way to resolve this problem is to multiply (7.1) by 2 and subtract (7.2)
from (7.1), which removes the y-terms:
14 = 6x + 2y
10 = 2x + 4y
4 = 4x
x = 1.
Substituting x = 1 in (7.1) reveals the value of y:
7 = 3 + 2y
4 = 2y
y = 2.
Therefore, x = 1 and y = 2, solves (7.1) and (7.2).
The equations must be linearly independent, otherwise we only have one equation.
For example, starting with
7 = 3x + 2y
14 = 6x + 4y
is a futile exercise, as the second equation is double the ﬁrst, and does not provide
any extra information.
To ﬁnd a general solution to this problem, we start with
d1 = a1x + b1y
(7.3)
d2 = a2x + b2y.
(7.4)

7.2 Linear Equations with Two Variables
111
Multiply (7.3) by b2 and (7.4) by b1:
d1b2 = a1b2x + b1b2y
(7.5)
b1d2 = b1a2x + b1b2y.
(7.6)
Subtract (7.6) from (7.5):
d1b2 −b1d2 = a1b2x −b1a2x
= (a1b2 −b1a2)x
x = d1b2 −b1d2
a1b2 −b1a2
.
(7.7)
To ﬁnd y, multiply (7.3) by a2 and (7.4) by a1:
d1a2 = a2a1x + b1a2y
(7.8)
a1d2 = a2a1x + a1b2y.
(7.9)
Subtract (7.8) from (7.9):
a1d2 −d1a2 = a1b2y −b1a2y
= (a1b2 −b1a2)y
y = a1d2 −d1a2
a1b2 −b1a2
.
(7.10)
Observe that both (7.7) and (7.10) share the common denominator: a1b2 −b1a2.
Furthermore, note the positions of a1, b1, a2 and b2 in the original equations:
a1
b1
a2
b2
and the denominator is formed by cross-multiplying the diagonal terms a1b2 and
subtracting the other cross-multiplied terms b1a2. Placing the four terms between
two vertical lines creates a second-order determinant whose value equals:

a1 b1
a2 b2
 = a1b2 −b1a2.
Although the name was originally given by Johann Gauss, it was the French mathe-
matician Augustin-Louis Cauchy who clariﬁed its current modern identity.
If the original equations were linearly related by a factor λ, the determinant equals
zero:

a1
b1
λa1 λb1
 = a1λb1 −b1λa1 = 0.

112
7
Determinants
Observe that the numerators of (7.7) and (7.10) are also second-order determinants:

d1 b1
d2 b2
 = d1b2 −b1d2
and

a1 d1
a2 d2
 = a1d2 −d1a2
which means that Eqs.(7.7) and (7.10) can be written using determinants:
x =

d1 b1
d2 b2


a1 b1
a2 b2

,
y =

a1 d1
a2 d2


a1 b1
a2 b2

.
And one ﬁnal piece of algebra permits the solution to be written as
x

d1 b1
d2 b2

=
y

a1 d1
a2 d2

=
1

a1 b1
a2 b2

.
(7.11)
Observe another pattern in (7.11) where the determinant is

a1 b1
a2 b2

but the d-terms replace the x-coefﬁcients:

d1 b1
d2 b2

and then the y-coefﬁcients

a1 d1
a2 d2
 .
Returning to the original equations:
7 = 3x + 2y
10 = 2x + 4y

7.2 Linear Equations with Two Variables
113
and substituting the constants in (7.11), we have
x

7 2
10 4

=
y

3 7
2 10

=
1

3 2
2 4

which, when expanded reveals
x
28 −20 =
y
30 −14 =
1
12 −4
x
8 = y
16 = 1
8
making x = 1 and y = 2.
Let’s try another example:
11 = 4x + y
5 = x + y
and substituting the constants in (7.11), we have
x

11 1
5 1

=
y

4 11
1 5

=
1

4 1
1 1

which, when expanded reveals
x
11 −5 =
y
20 −11 =
1
4 −1
x
6 = y
9 = 1
3
making x = 2 and y = 3.
Now let’s see how a third-order determinant arises from the coefﬁcients of three
equations in three unknowns.
7.3 Linear Equations with Three Variables
Consider the following set of three linear equations:
13 = 3x + 2y + 2z
(7.12)
20 = 2x + 3y + 4z
(7.13)
7 = 2x + y + z.
(7.14)

114
7
Determinants
A standard way to resolve this problem is to multiply (7.12) by 2 and subtract (7.13),
which removes the z-terms:
26 = 6x + 4y + 4z
20 = 2x + 3y + 4z
6 = 4x + y
(7.15)
leaving (7.15) with two unknowns.
Next, we take (7.13) and (7.14) and remove the z-term by multiplying (7.14) by
4 and subtract (7.13):
28 = 8x + 4y + 4z
20 = 2x + 3y + 4z
8 = 6x + y
(7.16)
leaving (7.16) with two unknowns. We are now left with (7.15) and (7.16):
6 = 4x + y
8 = 6x + y
which can be solved using (7.11):
x

6 1
8 1

=
y

4 6
6 8

=
1

4 1
6 1

therefore,
x = 6 −8
4 −6 = 1
y = 32 −36
4 −6
= 2.
Substituting x = 1 and y = 2 in (7.12) reveals that z = 3.
We can generalise (7.11) for three equations using third-order determinants:
x

d1 b1 c1
d2 b2 c2
d3 b3 c3

=
y

a1 d1 c1
a2 d2 c2
a3 d3 c3

=
z

a1 b1 d1
a2 b2 d2
a3 b3 d3

=
1

a1 b1 c1
a2 b2 c2
a3 b3 c3

.
(7.17)

7.3 Linear Equations with Three Variables
115
Onceagain,thereisanimportantpatternin (7.17)wheretheunderlyingdeterminantis

a1 b1 c1
a2 b2 c2
a3 b3 c3

but the d-terms replace the x-coefﬁcients:

d1 b1 c1
d2 b2 c2
d3 b3 c3

the d-terms replace the y-coefﬁcients:

a1 d1 c1
a2 d2 c2
a3 d3 c3

and the d-terms replace the z-coefﬁcients:

a1 b1 d1
a2 b2 d2
a3 b3 d3

.
We must now ﬁnd a way of computing the value of a third-order determinant, which
requires the following algebraic analysis of three equations in three unknowns. We
start with three linear equations:
d1 = a1x + b1y + c1z
(7.18)
d2 = a2x + b2y + c2z
(7.19)
d3 = a3x + b3y + c3z
(7.20)
and derive one equation in two unknowns from (7.18) and (7.19), and another from
(7.19) and (7.20).
We multiply (7.18) by c2, (7.19) by c1 and subtract them:
c2d1 = a1c2x + b1c2y + c1c2z
c1d2 = c1a2x + b2c1y + c1c2z
c2d1 −c1d2 = (a1c2 −c1a2)x + (b1c2 −b2c1)y.
(7.21)
Next, we multiply (7.19) by c3, (7.20) by c2 and subtract them:
c3d2 = a2c3x + b2c3y + c2c3z
c2d3 = a3c2x + b3c2y + c2c3z
c3d2 −c2d3 = (a2c3 −a3c2)x + (b2c3 −b3c2)y.
(7.22)

116
7
Determinants
Simplify (7.21) by letting
e1 = c2d1 −c1d2
f1 = a1c2 −c1a2
g1 = b1c2 −b2c1
therefore,
e1 = f1x + g1y.
(7.23)
Simplify (7.22) by letting
e2 = c3d2 −c2d3
f2 = a2c3 −a3c2
g2 = b2c3 −b3c2
therefore,
e2 = f2x + g2y.
(7.24)
Now we have two equations in two unknowns:
e1 = f1x + g1y
e2 = f2x + g2y
which are solved using
x
A = y
B = 1
C
(7.25)
where
A =

e1 g1
e2 g2
 =

c2d1 −c1d2 b1c2 −b2c1
c3d2 −c2d3 b2c3 −b3c2

(7.26)
B =

f1 e1
f2 e2
 =

a1c2 −c1a2 c2d1 −c1d2
a2c3 −a3c2 c3d2 −c2d3

(7.27)
C =

f1 g1
f2 g2
 =

a1c2 −c1a2 b1c2 −b2c1
a2c3 −a3c2 b2c3 −b3c2

(7.28)
Weﬁrstcompute A,fromwhichwecanderive B,becausetheonlydifferencebetween
(7.26) and (7.27) is that d1, d2, d3 become a1, a2, a3 respectively, and b1, b2, b3
become d1, d2, d3 respectively.
We can derive C from A, as the only difference between (7.26) and (7.28) is that
d1, d2, d3 become a1, a2, a3 respectively. Starting with A:

7.3 Linear Equations with Three Variables
117
A = (c2d1 −c1d2)(b2c3 −b3c2) −(b1c2 −b2c1)(c3d2 −c2d3)
= b2c2c3d1 −b3c2
2d1 −b2c1c3d2 + b3c1c2d2
−b1c2c3d2 + b1c2
2d3 + b2c1c3d2 −b2c1c2d3
= b2c2c3d1 −b3c2
2d1 + b3c1c2d2 −b1c2c3d2 + b1c2
2d3 −b2c1c2d3
= c2(b2c3d1 −b3c2d1 + b3c1d2 −b1c3d2 + b1c2d3 −b2c1d3)
A = c2

d1(b2c3 −c2b3) −b1(d2c3 −c2d3) + c1(d2b3 −b2d3)

.
(7.29)
Using the substitutions described above we can derive B and C from (7.29):
B = c2

a1(d2c3 −c2d3) −b1(a2c3 −c2a3) + c1(a2d3 −d2a3)

(7.30)
C = c2

a1(b2c3 −c2b3) −b1(a2c3 −c2a3) + c1(a2b3 −b2a3)

.
(7.31)
We can now rewrite (7.29)–(7.31) using determinant notation. At the same time, we
can drop the c2 terms as they cancel out when computing x, y and z:
A = d1

b2 c2
b3 c3
 −b1

d2 c2
d3 c3
 + c1

d2 b2
d3 b3

(7.32)
B = a1

d2 c2
d3 c3
 −d1

a2 c2
a3 c3
 + c1

a2 d2
a3 d3

(7.33)
C = a1

b2 c2
b3 c3
 −b1

a2 c2
a3 c3
 + c1

a2 b2
a3 b3
 .
(7.34)
As (7.17) and (7.25) refer to the same x and y, then

d1 b1 c1
d2 b2 c2
d3 b3 c3

= d1

b2 c2
b3 c3
 −b1

d2 c2
d3 c3
 + c1

d2 b2
d3 b3

(7.35)

a1 d1 c1
a2 d2 c2
a3 d3 c3

= a1

d2 c2
d3 c3
 −d1

a2 c2
a3 c3
 + c1

a2 d2
a3 d3

(7.36)

a1 b1 c1
a2 b2 c2
a3 b3 c3

= a1

b2 c2
b3 c3
 −b1

a2 c2
a3 c3
 + c1

a2 b2
a3 b3
 .
(7.37)
As a consistent algebraic analysis has been pursued to derive (7.35)–(7.37), a con-
sistent pattern has surfaced in Fig.7.1 which shows how the three determinants are
evaluated. This pattern comprises taking each entry in the top row, called a cofactor,
and multiplying it by the determinant of entries in rows 2 and 3, whilst ignoring the
column containing the original term, called a ﬁrst minor. Observe that the second
term of the top row is switched negative, called an inversion correction factor.

118
7
Determinants
d1
b1
c1
d1
b1
c1
d1
b1
c1
d1
b1
c1
d2
b2
c2
=
d2
b2
c2
-
d2
b2
c2
+
d2
b2
c2
d3
b3
c3
d3
b3
c3
d3
b3
c3
d3
b3
c3
a1
d1
c1
a1
d1
c1
a1
d1
c1
a1
d1
c1
a2
d2
c2
=
a2
d2
c2
-
a2
d2
c2
+
a2
d2
c2
a3
d3
c3
a3
d3
c3
a3
d3
c3
a3
d3
c3
a1
b1
c1
a1
b1
c1
a1
b1
c1
a1
b1
c1
a2
b2
c2
=
a2
b2
c2
-
a2
b2
c2
+
a2
b2
c2
a3
b3
c3
a3
b3
c3
a3
b3
c3
a3
b3
c3
Fig. 7.1 Evaluating the determinants shown in (7.35)–(7.37)
Let’s repeat (7.31) again without the c2 term, as it has nothing to do with the
calculation of the determinant.
C = a1(b2c3 −c2b3) −b1(a2c3 −c2a3) + c1(a2b3 −b2a3).
(7.38)
It is possible to arrange the terms of (7.38) as a square matrix such that each row and
column sums to C:
a1(b2c3 −c2b3) −b1(a2c3 −c2a3) + c1(a2b3 −b2a3)
−a2(b1c3 −c1b3) + b2(a1c3 −c1a3) −c2(a1b3 −b1a3)
a3(b1c2 −c1b2) −b3(a1c2 −c1a2) + c3(a1b2 −b1a2)
which means that there are six ways to evaluate the determinant C: summing the rows,
or summing the columns. Figure7.2 shows this arrangement with the cofactors in
blue, and the ﬁrst minor determinants in green. Observe how the signs alternate
between the terms.
Having discovered the origins of these patterns, let’s evaluate the original equa-
tions declared at the start of this section using (7.11)
13 = 3x + 2y + 2z
20 = 2x + 3y + 4z
7 = 2x + y + z.

7.3 Linear Equations with Three Variables
119
a1
b1
c1
C
C
C
C =
a2
b2
c2
=
=
=
a3
b3
c3
-
a1
b1
c1
a1
b1
c1
a1
b1
c1
C =
a2
b2
c2
-
a2
b2
c2
+
a2
b2
c2
a3
b3
c3
a3
b3
c3
a3
b3
c3
-
+
-
a1
b1
c1
a1
b1
c1
a1
b1
c1
C = -
a2
b2
c2
+
a2
b2
c2
-
a2
b2
c2
a3
b3
c3
a3
b3
c3
a3
b3
c3
+
-
+
a1
b1
c1
a1
b1
c1
a1
b1
c1
C =
a2
b2
c2
-
a2
b2
c2
+
a2
b2
c2
a3
b3
c3
a3
b3
c3
a3
b3
c3
Fig. 7.2 The patterns of multipliers with their respective second-order determinants
x

d1 b1 c1
d2 b2 c2
d3 b3 c3

=
y

a1 d1 c1
a2 d2 c2
a3 d3 c3

=
z

a1 b1 d1
a2 b2 d2
a3 b3 d3

=
1

a1 b1 c1
a2 b2 c2
a3 b3 c3

therefore,
x

13 2 2
20 3 4
7 1 1

=
y

3 13 2
2 20 4
2 7 1

=
z

3 2 13
2 3 20
2 1 7

=
1

3 2 2
2 3 4
2 1 1

computing the determinants using the top row entries as cofactors:
x
−13 + 16 −2 =
y
−24 + 78 −52 =
z
3 + 52 −52 =
1
−3 + 12 −8
x
1 = y
2 = z
3 = 1
1
therefore, x = 1, y = 2 and z = 3.

120
7
Determinants
Fig. 7.3 The pattern behind
Sarrus’s rule
a1 b1 c1
a1 b1 c1 a1 b1
b1 c1 a1 b1 c1
a2 b2 c2
=
a2 b2 c2 a2 b2
-
b2 c2 a2 b2 c2
a3 b3 c3
a3 b3 c3 a3 b3
b3 c3 a3 b3 c3
7.3.1 Sarrus’s Rule
The French mathematician Pierre Sarrus (1798–1861) discovered another way to
compute the value of a third-order determinant, that arises from (7.38):
C = a1(b2c3 −c2b3) −b1(a2c3 −c2a3) + c1(a2b3 −b2a3)
= a1b2c3 −a1c2b3 −b1a2c3 + b1c2a3 + c1a2b3 −c1b2a3
= a1b2c3 + b1c2a3 + c1a2b3 −a1c2b3 −b1a2c3 −c1b2a3.
(7.39)
The pattern in (7.39) becomes clear in Fig.7.3, where the ﬁrst two columns of the
matrix are repeated, and comprises two diagonal sets of terms: on the left in blue,
we have the products a1b2c3, b1c2a3, c1a2b3, and on the right in red and orange,
the products a1c2b3, b1a2c3, c1b2a3. These diagonal patterns provide a useful aide-
mémoire when computing the determinant. Unfortunately, this rule only applies to
third-order determinants.
7.4 Mathematical Notation
Having discovered the background of determinants, now let’s explore a formal
description of their structure and characteristics.
7.4.1 Matrix
In the following deﬁnitions, a matrix is a square array of entries, with an equal number
of rows and columns. The entries may be numbers, vectors, complex numbers or even
partial differentials, in the case of a Jacobian. In general, each entry is identiﬁed by
two subscripts row col:
arow col.
A matrix with n rows and m columns has the following entries:
a11 a12 . . . a1m
a21 a22 . . . a2m
...
...
...
...
an1 an2 . . . anm

7.4 Mathematical Notation
121
The entries lying on the two diagonals are identiﬁed as follows: a11 and anm lie on
the main diagonal, and a1m and an1 lie on the secondary diagonal.
7.4.2 Order of a Determinant
The order of a square determinant equals the number of rows or columns. For exam-
ple, a ﬁrst-order determinant contains a single entry; a second-order determinant has
two rows and two columns; and a third-order determinant has three rows and three
columns.
7.4.3 Value of a Determinant
A determinant posses a unique, single value derived from its entries. The algorithms
used to compute this value must respect the algebra associated with solving sets of
linear equations, as discussed above.
The French mathematician, astronomer, and physicist Pierre-Simon Laplace
(1749–1827) developed a way to expand the determinant of any order. The Laplace
expansion is the idea described above and shown in Fig.7.1, where cofactors and
ﬁrst minors or principal minors are used. For example, starting with a fourth-order
determinant, when any row and column are removed, the remaining entries create a
third-order determinant, called the ﬁrst minor of the original determinant.
The following equation is used to control the sign of each cofactor:
(−1)row+col
which, for a fourth-order determinant creates:

+ −+ −
−+ −+
+ −+ −
−+ −+

.
The Laplace expansion begins by choosing a convenient row or column as the source
of cofactors. Any zeros are particularly useful, as they cancel out any contribution by
the ﬁrst minor determinant. It then sums the products of every cofactor in the chosen
row or column, with its associated ﬁrst minor, including an appropriate inversion
correction factor to adjust the sign changes. The ﬁnal result is the determinant’s
value.
A ﬁrst-order determinant:
a11
 = a11.

122
7
Determinants
A second-order determinant:

a11 a12
a21 a22
 = a11a22 −a12a21.
A third-order determinant using the Laplace expansion with cofactors from the
ﬁrst row:

a11 a12 a13
a21 a22 a23
a31 a32 a33

= a11

a22 a23
a32 a33
 −a12

a21 a23
a31 a33
 + a13

a21 a22
a31 a32
 .
A fourth-order determinant using the Laplace expansion with cofactors from the
ﬁrst row:
a11

a22 a23 a24
a32 a33 a34
a42 a43 a44

−a12

a21 a23 a24
a31 a33 a34
a41 a43 a44

+
a13

a21 a22 a24
a31 a32 a34
a41 a42 a44

−a14

a21 a22 a23
a31 a32 a33
a41 a42 a43

=

a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a42 a43 a44

Sarrus’s rule is useful to compute a third-order determinant:

a11 a12 a13
a21 a22 a23
a31 a32 a33

= a11a22a33 + a12a23a31 + a13a21a32−
a11a23a32 −a12a21a33 + a13a22a31
The Laplace expansion works with higher-order determinants, as any ﬁrst minor
can itself be expanded using the same expansion.
7.4.4 Properties of Determinants
If a determinant contains a row or column of zeros, the Laplace expansion implies
that the value of the determinant is zero.

3 0 2
2 0 4
2 0 1

= 0.

7.4 Mathematical Notation
123
If a determinant’s rows and columns are interchanged, the Laplace expansion also
implies that the value of the determinant is unchanged.

3 12 2
2 10 4
2 8 1

=

3
2 2
12 10 8
2
4 1

= −2.
If any two rows, or columns, are interchanged, without changing the order of their
entries, the determinant’s numerical value is unchanged, but its sign is reversed.

3 12 2
2 10 4
2 8 1

= −2

12 3 2
10 2 4
8 2 1

= 2.
If the entries of a row or column share a common factor, the entries may be adjusted,
and the factor placed outside.

3 12 2
2 10 4
2 8 1

= 2

3 6 2
2 5 4
2 4 1

= −2.
7.5 Worked Examples
7.5.1 Determinant Expansion
Evaluate this determinant using the Laplace expansion and Sarrus’s rule.

1 4 7
2 5 8
3 6 9

.
Using the Laplace expansion:

1 4 7
2 5 8
3 6 9

= 1

5 8
6 9
 −2

4 7
6 9
 + 3

4 7
5 8

= 1(45 −48) −2(36 −42) + 3(32 −35)
= −3 + 12 −9
= 0.

124
7
Determinants
Using Sarrus’s rule:

1 4 7
2 5 8
3 6 9

= 1 × 5 × 9 + 4 × 8 × 3 + 7 × 2 × 6 −7 × 5 × 3 −1 × 8 × 6 −4 × 2 × 9
= 45 + 96 + 84 −105 −48 −72
= 0.
7.5.2 Complex Determinant
Evaluate the complex determinant

4 + i2 1 + i
2 −i3 3 + i3
 .
Using the Laplace expansion:

4 + i2 1 + i
2 −i3 3 + i3
 = (4 + i2)(3 + i3) −(1 + i)(2 −i3)
= (12 + i18 −6) −(2 −i + 3)
= 6 + i18 −5 + i
= 1 + i19.
7.5.3 Simple Expansion
Write down the simplest expansion of this determinant with its value:

1 2 3
4 5 0
6 7 0

.
Using the Laplace expansion with cofactors from the third column:

1 2 3
4 5 0
6 7 0

= 3

4 5
6 7
 = −6.

7.5 Worked Examples
125
7.5.4 Simultaneous Equations
Solve the following equations using determinants:
3 = 2x + y −z
12 = x + 2y + z
8 = 3x −2y + 2z.
Using (7.17):
x

3
1 −1
12 2
1
8 −2 2

=
y

2 3 −1
1 12 1
3 8
2

=
z

2 1
3
1 2 12
3 −2 8

=
1

2 1 −1
1 2
1
3 −2 2

.
Therefore,
x =

3
1 −1
12 2
1
8 −2 2


2 1 −1
1 2
1
3 −2 2

= 18 −16 + 40
12 + 1 + 8
= 42
21 = 2,
y =

2 3 −1
1 12 1
3 8
2


2 1 −1
1 2
1
3 −2 2

= 32 + 3 + 28
12 + 1 + 8 = 63
21 = 3,
z =

2 1
3
1 2 12
3 −2 8


2 1 −1
1 2
1
3 −2 2

= 80 + 28 −24
24 + 1 + 8
= 84
21 = 4.

Chapter 8
Vectors
8.1 Introduction
Vectors are a relative new invention in the world of mathematics, dating only from
the 19th century. They enable us to solve complex geometric problems, the dynamics
of moving objects, and problems involving forces and ﬁelds.
We often only require a single number to represent quantities used in our daily
lives such as height, age, shoe size, waist and chest measurement. The magnitude of
these numbers depends on our age and whether we use metric or imperial units. Such
quantities are called scalars. On the other hand, there are some things that require
more than one number to represent them: wind, force, weight, velocity and sound
are just a few examples. For example, any sailor knows that wind has a magnitude
and a direction. The force we use to lift an object also has a value and a direction.
Similarly, the velocity of a moving object is measured in terms of its speed (e.g.
miles per hour), and a direction such as north-west. Sound, too, has intensity and a
direction. Such quantities are called vectors.
Complex numbers seemed to be a likely candidate for representing forces, and
were being investigated by the Norwegian-Danish mathematician Caspar Wessel
(1745–1818), the French amateur mathematician Jean-Robert Argand (1768–1822)
and the English mathematician John Warren (1796–1852). At the time, complex num-
bers were two-dimensional, and their 3D form was being investigated by Sir William
Rowan Hamilton who invented them in 1843, calling them quaternions. In 1853,
Hamilton published his book Lectures on Quaternions in which he described terms
such as “vector”, “transvector” and “provector”. Hamilton’s work was not widely
accepted until in 1881, when Josiah Gibbs published his treatise Vector Analysis,
describing modern vector analysis.
Gibbs was not a fan of the imaginary quantities associated with Hamilton’s quater-
nions, but saw the potential of creating a vectorial system from the imaginary i, j and
k into the unit basis vectors i, j and k, which is what we use today.
Some mathematicians were not happy with the direction mathematics had taken.
The German mathematician Hermann Gunther Grassmann (1809–1877), believed
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_8
127

128
8
Vectors
that his own geometric calculus was far superior to Hamilton’s quaternions, but
he died without managing to convince any of his fellow mathematicians. For-
tunately, the English mathematician and philosopher William Kingdon Clifford
(1845–1879) recognised the brilliance of Grassmann’s ideas, and formalised what
today has become known as geometric algebra.
With the success of Gibbs’ vector analysis, quaternions faded into obscurity, only
to be rediscovered in the 1970s when they were employed by the ﬂight simulation
community to control the dynamic behaviour of a simulator’s motion platform. A
decade later they found their way into computer graphics where they are used for
rotations about an arbitrary axis.
Now this does not mean that vector analysis is dead—far from it. Vast quantities
of scientiﬁc software depends upon the vector mathematics developed over a century
ago, and will continue to employ it for many years to come. Nevertheless, geometric
algebra is destined to emerge as a powerful mathematical framework that could
eventually replace vector analysis one day.
8.2 2D Vectors
8.2.1 Vector Notation
A scalar such as x represents a single numeric quantity. However, as a vector contains
two or more numbers, its symbolic name is printed using a bold font to distinguish
it from a scalar variable. Examples being n, i and q.
When a scalar variable is assigned a value, we use the standard algebraic notation:
x = 3.
However, a vector has one or more numbers enclosed in brackets, written as a column
or as a row—in this text column vectors are used:
n =
3
4

.
The numbers 3 and 4 are the components of n, and their sequence within the brackets
is important. A row vector places the components horizontally:
n = [3 4].
The difference between the two, is only appreciated in the context of matrices. Some-
times it is convenient—for presentation purposes—to write a column vector as a row
vector, in which case, it is written

8.2 2D Vectors
129
n = [3 4]T ,
where the superscript T reminds us that n is really a transposed column vector.
8.2.2 Graphical Representation of Vectors
An arrow is used to represent a vector as it possesses length and direction, as shown
in Fig.8.1. By assigning coordinates to the arrow it is possible to translate the arrow’s
length and direction into two numbers. For example, in Fig.8.2 the vector r has its
tail deﬁned by (x1, y1) = (1, 2), and its head by (x2, y2) = (3, 4). Vector s has
its tail deﬁned by (x3, y3) = (5, 3), and its head by (x4, y4) = (3, 1). The x- and
y-components for r are computed as follows
xr = x2 −x1 = 3 −1 = 2
yr = y2 −y1 = 4 −2 = 2
and the components for s are computed as follows
xs = x4 −x3 = 3 −5 = −2
ys = y4 −y3 = 1 −3 = −2.
It is the negative value of xs and ys that encode the vector’s direction. In general, if
the coordinates of a vector’s head and tail are (xh, yh) and (xt, yt) respectively, its
components Δx and Δy are given by
Δx = xh −xt
Δy = yh −yt.
Fig. 8.1 An arrow with
magnitude and direction
1
2
3
4
5
1
2
3
x
y

130
8
Vectors
Fig. 8.2 Two vectors r and s
have the same magnitude but
opposite directions
1
2
3
4
5
6
1
2
3
4
x
y
r
s
(x4, y4)
(x1, y1)
(x3, y3)
(x2, y2)
One can readily see from this notation that a vector does not have an absolute position.
It does not matter where we place a vector, so long as we preserve its length and
orientation, its components are unaltered.
8.2.3 Magnitude of a Vector
The magnitude or length of a vector r is written |r| and computed using the theorem
of Pythagoras:
|r| =

(Δx)2 + (Δy)2
and used as follows. Consider a vector deﬁned by
(xh, yh) = (4, 5)
(xt, yt) = (1, 1)
where the x- and y-components are 3 and 4 respectively. Therefore its magnitude
equals
√
32 + 42 = 5.
Figure8.3 shows eight vectors, and their geometric properties are listed in
Table8.1.
Fig. 8.3 Eight vectors
whose coordinates are shown
in Table8.1
-3
-2
-1
1
2
3
-2
-1
1
2
x
y

8.3 3D Vectors
131
Table 8.1 Values associated with the eight vectors in Fig.8.3
xh
yh
xt
yt
Δx
Δy
|vector|
2
0
0
0
2
0
2
0
2
0
0
0
2
2
−2
0
0
0
−2
0
2
0
−2
0
0
0
−2
2
1
1
0
0
1
1
√
2
−1
1
0
0
−1
1
√
2
−1
−1
0
0
−1
−1
√
2
1
−1
0
0
1
−1
√
2
8.3 3D Vectors
The above vector examples are in 2D, but it is easy to extend this notation to embrace
an extra dimension. Figure8.4 shows a 3D vector r with its head, tail, components
and magnitude annotated. The vector, its components and magnitude are given by
r = [Δx
Δy
Δz]T
Δx = xh −xt
Δy = yh −yt
Δz = zh −zt
|r| =

(Δx)2 + (Δy)2 + (Δz)2.
All future examples are three-dimensional.
Fig. 8.4 The vector r has
components Δx, Δy, Δz
x
x
y
y
z
Ph
Pt
r

132
8
Vectors
8.3.1 Vector Manipulation
As vectors are different to scalars, there are rules to control how the two mathematical
entities interact with one another. For instance, we need to consider vector addition,
subtraction and products, and how a vector is scaled.
8.3.2 Scaling a Vector
Given a vector n, 2n means that the vectors components are scaled by a factor of 2.
For example, given
n =
⎡
⎣
3
4
5
⎤
⎦,
then 2n =
⎡
⎣
6
8
10
⎤
⎦
which seems logical. Similarly, if we divide n by 2, its components are halved. Note
that the vector’s direction remains unchanged—only its magnitude changes.
In general, given
n =
⎡
⎣
n1
n2
n3
⎤
⎦,
then λn =
⎡
⎣
λn1
λn2
λn3
⎤
⎦, where λ ∈R.
There is no obvious way we can resolve the expression 2 + n, for it is not clear
which component of n is to be increased by 2. However, if we can add a scalar to an
imaginary (e.g. 2 + 3i), why can’t we add a scalar to a vector (e.g. 2 + n)? Well, the
answer to this question is two-fold: First, if we change the meaning of “add” to mean
“associated with”, then there is nothing to stop us from “associating” a scalar with a
vector, like complex numbers. Second, the axioms controlling our algebra must be
clear on this matter. Unfortunately, the axioms of traditional vector analysis do not
support the “association” of scalars with vectors in this way. However, geometric
algebra does! Furthermore, geometric algebra even permits division by a vector,
which does sound strange. Consequently, whilst reading the rest of this chapter keep
an open mind about what is permitted, and what is not permitted. At the end of the
day, virtually anything is possible, so long as we have a well-behaved axiomatic
system.
8.3.3 Vector Addition and Subtraction
Given vectors r and s, r ± s is deﬁned as

8.3 3D Vectors
133
r =
⎡
⎣
xr
yr
zr
⎤
⎦,
s =
⎡
⎣
xs
ys
zs
⎤
⎦,
then r ± s =
⎡
⎣
xr ± xs
yr ± ys
zr ± zs
⎤
⎦.
Vector addition is commutative:
a + b = b + a
e.g.
⎡
⎣
1
2
3
⎤
⎦+
⎡
⎣
4
5
6
⎤
⎦
=
⎡
⎣
4
5
6
⎤
⎦+
⎡
⎣
1
2
3
⎤
⎦.
However, like scalar subtraction, vector subtraction is not commutative:
a −b ̸= b −a
e.g.
⎡
⎣
4
5
6
⎤
⎦−
⎡
⎣
1
2
3
⎤
⎦
̸=
⎡
⎣
1
2
3
⎤
⎦−
⎡
⎣
4
5
6
⎤
⎦.
Let’s illustrate vector addition and subtraction with two examples. Figure8.5 shows
the graphical interpretation of adding two vectors r and s. Note that the tail of vector
s is attached to the head of vector r. The resultant vector t = r + s is deﬁned
by adding the corresponding components of r and s together. Figure8.6 shows a
graphical interpretation for r −s. This time, the components of vector s are reversed
toproduceanequalandoppositevector.Thenitisattachedtor andaddedasdescribed
above.
8.3.4 Position Vectors
Given any point P(x, y, z), a position vector p is created by assuming that P is
the vector’s head and the origin is its tail. As the tail coordinates are (0, 0, 0) the
Fig. 8.5 Vector addition
r + s
x
z
y
r
r+s
s

134
8
Vectors
Fig. 8.6 Vector subtraction
r −s
x
y
r
r+s
s
-s
r - s
vector’s components are x, y, z. Consequently, the vector’s magnitude |p| equals
	
x2 + y2 + z2.
8.3.5 Unit Vectors
By deﬁnition, a unit vector has a magnitude of 1.A simple example is i, where
i = [1 0 0]T ,
where |i| = 1.
Unit vectors are extremely useful in the product of two vectors, where their magni-
tudes are required; and if these are unit vectors, the computation is greatly simpliﬁed.
Converting a vector into a unit form is called normalising, and is achieved by
dividing its components by the vector’s magnitude. To formalise this process, con-
sider a vector r = [x
y
z]T , with magnitude |r| =
	
x2 + y2 + z2. The unit form
of r is given by
ˆr = 1
|r|[x
y
z]T
This is conﬁrmed by showing that the magnitude of ˆr is 1:
|ˆr| =

 x
|ˆr|
2
+
 y
|ˆr|
2
+
 z
|ˆr|
2
= 1
|ˆr|

x2 + y2 + z2
|ˆr| = 1.

8.3 3D Vectors
135
8.3.6 Cartesian Vectors
A Cartesian vector is constructed from three unit vectors: i, j and k, aligned with
the x-, y- and z-axis, respectively:
i = [1 0 0]T ,
j = [0 1 0]T ,
k = [0 0 1]T .
Therefore, any vector aligned with the x-, y- or z-axis is a scalar multiple of the
associated unit vector. For example, 10i is aligned with the x-axis, with a magnitude
of 10. 20k is aligned with the z-axis, with a magnitude of 20. By employing the rules
of vector addition and subtraction, we can compose a vector r by summing three
scaled Cartesian unit vectors as follows
r = ai + bj + ck
which is equivalent to
r = [a
b c]T
where the magnitude of r is
|r| =
	
a2 + b2 + c2.
Any pair of Cartesian vectors, such as r and s, can be combined as follows
r = ai + bj + ck
s = di + ej + f k
r ± s = (a ± d)i + (b ± e)j + (c ± f )k.
8.3.7 Products
The product of two scalars is very familiar: for example, 6×7 or 7×6 = 42. We often
visualise this operation as a rectangular area, where 6 and 7 are the dimensions of a
rectangle’s sides, and 42 is the area. However, a vector’s qualities are its length and
orientation, which means that any product must include them in any calculation. The
length is easily calculated, but we must know the angle between the two vectors as
this reﬂects their relative orientation. Although the angle can be incorporated within
the product in various ways, two particular ways lead to useful results. For example,
the product of r and s, separated by an angle θ could be |r||s| cos θ or |r||s| sin θ.
It just so happens that cos θ forces the product to result in a scalar quantity, and
sin θ creates a vector. Consequently, there are two products to consider: the scalar
product, and the vector product, which are written as r · s and r × s respectively.

136
8
Vectors
8.3.8 Scalar Product
Figure8.7 shows two vectors r and s that have been drawn, for convenience, with their
tails touching. Taking s as the reference vector—which is an arbitrary choice—we
compute the projection of r on s, which takes into account their relative orientation.
The length of r on s is |r| cos β. We can now multiply the magnitude of s by the
projected length of r: |s||r| cos β. This scalar product is written
r · s = |r||s| cos β.
(8.1)
Because of the dot symbol “·”, the scalar product is also called the dot product.
Fortunately, everything is in place to perform this task. To begin with, we deﬁne
two Cartesian vectors r and s, and proceed to multiply them together using (8.1):
r = ai + bj + ck
s = di + ej + f k
r · s = (ai + bj + ck) · (di + ej + f k)
= ai · (di + ej + f k)
+ bj · (di + ej + f k)
+ ck · (di + ej + f k)
= adi · i + aei · j + af i · k
+ bdj · i + bej · j + bf j · k
+ cdk · i + cek · j + cf k · k.
Before we proceed any further, we can see that we have created various dot product
terms such as i · i, i · j, i · k, etc. These terms can be divided into two groups: those
that reference the same unit vector, and those that reference different unit vectors.
Using the deﬁnition of the dot product (8.1), terms such as i · i, j · j and k · k = 1,
because the angle between i and i, j and j, or k and k, is 0◦; and cos 0◦= 1. But as
the other vector combinations are separated by 90◦, and cos 90◦= 0, all remaining
terms collapse to zero, and we are left with
Fig. 8.7 The projection of r
on s
x
y
r
s

8.3 3D Vectors
137
r · s = adi · i + aei · j + af i · k.
But as the the magnitude of a unit vector is 1, we can write
r · s = |r||s| cos θ = ad + be + cf
which conﬁrms that the dot product is indeed a scalar quantity.
It is worth pointing out that the angle returned by the dot product ranges between
0◦and 180◦. This is because, as the angle between two vectors increases beyond
180◦the returned angle θ is always the smallest angle associated with the geometry.
8.3.9 The Vector Product
As mentioned above, the vector product r × s creates a third vector whose magni-
tude equals |r||s| sin θ, where θ is the angle between the original vectors. Figure8.8
reminds us that the area of a parallelogram formed by r and s equals |r||s| sin θ.
Because of the cross symbol “×”, the vector product is also called the cross product.
r × s = t
(8.2)
|t| = |r||s| sin θ.
We will discover that the vector t is normal (90◦) to the plane containing the vectors
r and s, as shown in Fig.8.9, which makes it an ideal way of computing the vector
normal to a surface. Once again, let’s deﬁne two vectors and this time multiply them
together using (8.2):
Fig. 8.8 The area of the
parallelogram formed by r
and s
r
s
|s|sin
s
r

138
8
Vectors
Fig. 8.9 The vector product
x
y
r
s
|r||s|sin
s
r
t
r = ai + bj + ck
s = di + ej + f k
r × s = (ai + bj + ck) × (di + ej + f k)
= ai × (di + ej + f k)
+ bj × (di + ej + f k)
+ ck × (di + ej + f k)
= adi × i + aei × j + af i × k
+ bdj × i + bej × j + bf j × k
+ cdk × i + cek × j + cf k × k.
As we found with the dot product, there are two groups of vector terms: those that
reference the same unit vector, and those that reference different unit vectors.
Using the deﬁnition for the cross product (8.2), operations such as i × i, j × j and
k × k result in a vector whose magnitude is 0. This is because the angle between the
vectors is 0◦, and sin 0◦= 0. Consequently these terms disappear and we are left
with
r × s = aei × j + af i × k + bdj × i + bf j × k + cdk × i + cek × j.
(8.3)
Sir William Rowan Hamilton struggled for many years when working on quaternions
to resolve the meaning of a similar result. At the time, he was not using vectors, as
they had yet to be deﬁned, but the imaginary terms i, j and k. Hamilton’s problem
was to resolve the products i j, jk, ki and their opposites ji, kj and ik. What did the
products mean? He reasoned that i j = k, jk = i and ki = j, but could not resolve
their opposites. One day in 1843, when he was out walking, thinking about this
problem, he thought the impossible: i j = k, but ji = −k, jk = i, but kj = −i, and
ki = j, but ik = −j. To his surprise, this worked, but it contradicted the commutative
multiplicationlawofscalarswhere6×7 = 7×6.Wenowacceptthatthecommutative
multiplication law is there to be broken!
Although Hamilton had invented 3D complex numbers, to which he gave the
name quaternions, they were not popular with everyone. And as mentioned earlier,
Josiah Gibbs saw that converting the imaginary i, j and k terms into the unit vectors

8.3 3D Vectors
139
i, j and k created a stable algebra for manipulating vectors, and for over a century
we have been using Gibbs’ vector notation.
The question we must ask is “Was Gibbs right?” to which the answer is probably
“no!” The reason for this is that although the scalar product works in space of any
number of dimensions, the vector (cross) product does not. It obviously does not
work in 2D as there is no direction for the resultant vector. It obviously works in 3D,
but in 4D and above there is no automatic spatial direction for the resultant vector. So,
the vector product is possibly a special condition of some other structure. Hermann
Grassmann knew this but did not have the mathematical reputation to convince his
fellow mathematicians.
Let’s continue with Hamilton’s rules and reduce the cross product terms of
(8.3) to
r × s = aek −af j −bdk + bf i + cdj −cei.
(8.4)
Equation (8.4) can be tidied up to bring like terms together:
r × s = (bf −ce)i + (cd −af )j + (ae −bd)k.
(8.5)
Now let’s repeat the original vector equations to see how equation (8.5) is computed:
r = ai + bj + ck
s = di + ej + f k
r × s = (bf −ce)i + (cd −af )j + (ae −bd)k.
(8.6)
To compute the i scalar term we consider the scalars associated with the other
two unit vectors, i.e. b, c, e, and f , and cross-multiply and subtract them to form
(bf −ce).
To compute the j scalar term we consider the scalars associated with the other
two unit vectors, i.e. a, c, d, and f , and cross-multiply and subtract them to form
(cd −af ).
To compute the k scalar term we consider the scalars associated with the other
two unit vectors, i.e. a, b, d, and e, and cross-multiply and subtract them to form
(ae −bd).
The middle operation seems out of step with the other two, but in fact it pre-
serves a cyclic symmetry often found in mathematics. Nevertheless, some authors
reverse the sign of the j scalar term and cross-multiply and subtract the terms to
produce −(af −cd) which maintains a visual pattern for remembering the cross-
multiplication. Equation (8.6) now becomes
r × s = (bf −ce)i −(af −cd)j + (ae −bd)k.
(8.7)
However, we now have to remember to introduce a negative sign for the j scalar term!

140
8
Vectors
We can write (8.7) using determinants as follows:
r × s =

b c
e f
 i −

a c
d f
 j +

a b
d e
 k.
or
r × s =

b c
e f
 i +

c a
f d
 j +

a b
d e
 k.
Therefore, to derive the cross product of two vectors we ﬁrst write the vectors in the
correct sequence. Remembering that r ×s does not equal s×r. Second, we compute
the three scalar terms and form the resultant vector, which is perpendicular to the
plane containing the original vectors.
So far, we have assumed that
r × s = t
|t| = |r||s| sin θ
where θ is the angle between r and s, and t is perpendicular to the plane containing
r and s. Now let’s prove that this is the case:
r · s = |r||s| cos θ = xr xs + yr ys + zrzs
cos2 θ = (xr xs + yr ys + zrzs)2
|r|2|s|2
|t| = |r||s| sin θ
|t|2 = |r|2|s|2 sin2 θ
= |r|2|s|2(1 −cos2 θ)
= |r|2|s|2
1 −(xr xs + yr ys + zrzs)2
|r|2|s|2

= |r|2|s|2 −(xr xs + yr ys + zrzs)2
= (x2
r + y2
r + z2
r )(x2
s + y2
s + z2
s) −(xr xs + yr ys + zrzs)2
= x2
r (y2
s + z2
s) + y2
r (x2
s + z2
s) + z2
r (x2
s + y2
s )
−2xr xs yr ys −2xr xszrzs −2yr yszrzs
= x2
r y2
s + x2
r z2
s + y2
r x2
s + y2
r z2
s + z2
r x2
s + z2
r y2
s
−2xr xs yr ys −2xr xszrzs −2yr yszrzs
= (yrzs −zr ys)2 + (zr xs −xrzs)2 + (xr ys −yr xs)2

8.3 3D Vectors
141
which in determinant form is
|t|2 =

yr zr
ys zs

2
+

zr xr
zs xs

2
+

xr yr
xs ys

2
and conﬁrms that t could be the vector
t =

yr zr
ys zs
 i +

zr xr
zs xs
 j +

xr yr
xs ys
 k.
All that remains is to prove that t is orthogonal (perpendicular) to r and s, which is
achieved by showing that r · t = s · t = 0:
r = xri + yrj + zrk
s = xsi + ysj + zsk
t = (yrzs −zr ys)i + (zr xs −xrzs)j + (xr ys −yr xs)k
r · t = xr(yrzs −zr ys) + yr(zr xs −xrzs) + zr(xr ys −yr xs)
= xr yrzs −xr yszr + xs yrzr −xr yrzs + xr yszr −xs yrzr = 0
s · t = xs(yrzs −zr ys) + ys(zr xs −xrzs) + zs(xr ys −yr xs)
= xs yrzs −xs yszr + xs yszr −xr yszs + xr yszs −xs yrzs = 0
and we have proved that r × s = t, where |t| = |r||s| sin θ and t is orthogonal to the
plane containing r and s.
Let’s now consider two vectors r and s and compute the normal vector t. The
vectors are chosen so that we can anticipate approximately the answer. For the sake
of clarity, the vector equations include the scalar multipliers 0 and 1. Normally,
these are omitted. Figure8.10 shows the vectors r and s and the normal vector t,
and Table8.2 contains the coordinates of the vertices forming the two vectors which
conﬁrms what we expected from Fig.8.10.
r = [(x3 −x2) (y3 −y2) (z3 −z2)]T
s = [(x1 −x2) (y1 −y2) (z1 −z2)]T
P1 = (0, 0, 1)
P2 = (1, 0, 0)
P3 = (0, 1, 0)
r = −1i + 1j + 0k
s = −1i + 0j + 1k
r × s = [1 × 1 −0 × 0]i
−[−1 × 1 −(−1) × 0]j
+ [−1 × 0 −(−1) × 1]k
t = i + j + k

142
8
Vectors
Fig. 8.10 Vector t is normal
to the vectors r and s
x
y
r
s
P1
t
P2
P3
Table 8.2 Coordinates of the
vertices used in Fig.8.10
V ertex
x
y
z
P1
0
0
1
P2
1
0
0
P3
0
1
0
Now let’s reverse the vectors to illustrate the importance of vector sequence.
s = −1i + 0j + 1k
r = −1i + 1j + 0k
s × r = [0 × 0 −1 × 1]i
−[−1 × 0 −(−1) × 1]j
+ [−1 × 1 −(−1) × 0]k
t = −i −j −k
which is in the opposite direction to r × s and conﬁrms that the vector product is
non-commutative.
8.3.10 The Right-Hand Rule
The right-hand rule is an aide mémoire for working out the orientation of the cross
product vector. Given the operation r × s, if the right-hand thumb is aligned with r,
the ﬁrst ﬁnger with s, and the middle ﬁnger points in the direction of t. However, we
must remember that this only holds in 3D. In 4D and above, it makes no sense.
8.4 Deriving a Unit Normal Vector for a Triangle
Figure8.11 shows a triangle with vertices deﬁned in an anticlockwise sequence from
its visible side. This is the side from which we want the surface normal to point.
Using the following information we will compute the surface normal using the cross
product and then convert it to a unit normal vector.

8.4 Deriving a Unit Normal Vector for a Triangle
143
Fig. 8.11 The normal vector
t is derived from the cross
product r × s
x
y
r
s
P1
t
P2
P3
Create vector r between P3 and P1, and vector s between P3 and P2:
r = −1i + 1j + 0k
s = −1i + 0j + 2k
r × s = (1 × 2 −0 × 0)i
−(−1 × 2 −0 × −1)j
+ (−1 × 0 −1 × −1)k
t = 2i + 2j + k
|t| =
	
22 + 22 + 12 = 3
ˆtu = 2
3i + 2
3j + 1
3k.
The unit vector ˆtu can now be used for illumination calculations in computer graphics,
and as it has unit length, dot product calculations are simpliﬁed.
8.5 Surface Areas
Figure8.12 shows two vectors r and s, where the height h = |s| sin θ. Therefore the
area of the associated parallelogram is
area = |r| h = |r||s| sin θ.
But this is the magnitude of the cross product vector t. Thus when we calculate r×s,
the length of the normal vector t equals the area of the parallelogram formed by r
and s; which means that the triangle formed by halving the parallelogram is half the
area.
area of parallelogram = |t|
area of triangle = 1
2|t|.

144
8
Vectors
Fig. 8.12 The area of the
parallelogram formed by two
vectors r and s
x
y
r
s
h
This makes it relatively easy to calculate the surface area of an object constructed
from triangles or parallelograms. In the case of a triangulated surface, we simply
sum the magnitudes of the normals and halve the result.
8.5.1 Calculating 2D Areas
Figure8.13 shows a triangle with vertices P0(x0, y0), P1(x1, y1) and P2(x2, y2)
formed in an anti-clockwise sequence. The vectors r and s are computed as fol-
lows:
r = (x1 −x0)i + (y1 −y0)j
s = (x2 −x0)i + (y2 −y0)j
|r × s| = (x1 −x0)(y2 −y0) −(x2 −x0)(y1 −y0)
= x1(y2 −y0) −x0(y2 −y0) −x2(y1 −y0) + x0(y1 −y0)
= x1y2 −x1y0 −x0y2 + x0y0 −x2y1 + x2y0 + x0y1 −x0y0
= x1y2 −x1y0 −x0y2 −x2y1 + x2y0 + x0y1
= (x0y1 −x1y0) + (x1y2 −x2y1) + (x2y0 −x0y2).
Fig. 8.13 The area of the
triangle formed by the
vectors r and s
x
y
r
s
P1
P2
P0

8.5 Surface Areas
145
But the area of the triangle formed by the three vertices is 1
2|r × s|. Therefore
area = 1
2[(x0y1 −x1y0) + (x1y2 −x2y1) + (x2y0 −x0y2)]
which is the formula disclosed in Chap.5!
8.6 Worked Examples
8.6.1 Position Vector
Calculate the magnitude of the position vector p, for the point P(4, 5, 6):
p = [4 5 6]T ,
therefore, |p| =
	
42 + 52 + 62 ≈20.88.
8.6.2 Unit Vector
Convert r to a unit vector.
r = [1 2 3]T
|r| =
	
12 + 22 + 32 =
√
14
ˆr =
1
√
14
[1 2 3]T ≈[0.267 0.535 0.802]T .
8.6.3 Vector Magnitude
Compute the magnitude of r + s.
r = 2i + 3j + 4k
s = 5i + 6j + 7k
r + s = 7i + 9j + 11k
|r + s| =
	
72 + 92 + 112 ≈15.84.

146
8
Vectors
8.6.4 Angle Between Two Vectors
Find the angle between r and s.
r = [2 0 4]T
s = [5 6 10]T
|r| =
	
22 + 02 + 42 ≈4.472
|s| =
	
52 + 62 + 102 ≈12.689.
Therefore,
|r||s| cos θ = 2 × 5 + 0 × 6 + 4 × 10 = 50
12.689 × 4.472 × cos θ = 50
cos θ =
50
12.689 × 4.472 ≈0.8811
θ = arccos 0.8811 ≈28.22◦.
The angle between the two vectors is approximately 28.22◦.
8.6.5 Vector Product
To show that the vector product works with the unit vectors i, j and k. We start with
r = 1i + 0j + 0k
s = 0i + 1j + 0k
and then compute (8.7):
r × s = (0 × 0 −0 × 1)i −(1 × 0 −0 × 0)j + (1 × 1 −0 × 0)k.
The i scalar and j scalar terms are both zero, but the k scalar term is 1, which makes
i × j = k.
Let’s see what happens when we reverse the vectors. This time we start with
r = 0i + 1j + 0k
s = 1i + 0j + 0k

8.6 Worked Examples
147
and then compute (8.7)
r × s = (1 × 0 −0 × 0)i −(0 × 0 −0 × 1)j + (0 × 0 −1 × 1)k.
The i scalar and j scalar terms are both zero, but the k scalar term is −1, which makes
j × i = −k. So we see that the vector product is antisymmetric, i.e. there is a sign
reversal when the vectors are reversed. Similarly, it can be shown that
j × k = i
k × i = j
k × j = −i
i × k = −j.

Chapter 9
Matrices
9.1 Introduction
Matrices, like determinants, have their background in algebra and offer another way
to represent and manipulate equations. Matrices can be added, subtracted and multi-
plied together, and even inverted, however, they must give the same result obtained
through traditional algebraic techniques. A useful way to introduce the subject is via
geometric transforms, which we examine ﬁrst.
9.2 Geometric Transforms
Let P(x, y) be a vertex on a 2D shape, then we can devise a geometric transform
where P(x, y) becomes P′(x′, y′) on a second shape. For example, when the fol-
lowing transform is applied to every point on a shape, it is halved in size, relative to
the origin:
x′ = 0.5x
y′ = 0.5y
and this transform translates a shape horizontally by 4 units:
x′ = x + 4
y′ = y.
Figure9.1 illustrates two successive transforms applied to the large green star cen-
tered at the origin. The ﬁrst transform scales the star by a factor of 0.5 creating the
smaller yellow star, which in turn is subjected to a horizontal translation of 4 units,
creating the blue star.
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_9
149

150
9
Matrices
Fig. 9.1 A scale transform
followed by a translate
transform
x
y
Fig. 9.2 A translate
transform followed by a
scale transform
x
y
Figure9.2 starts with the same green star, but this time it is translated before being
scaled. The ﬁnal blue star ends up in a different position to the one shown in Fig.9.1,
which demonstrates the importance of transform order.
The algebra supporting the transforms in Fig.9.1 comprises:
x′ = 0.5x
y′ = 0.5y
x′′ = x′ + 4
y′′ = y′
which simpliﬁes to
x′′ = 0.5x + 4
y′′ = 0.5y

9.2 Geometric Transforms
151
whereas, the algebra supporting the transforms in Fig.9.2 comprises:
x′ = x + 4
y′ = y
x′′ = 0.5x′
y′′ = 0.5y′
which simpliﬁes to
x′′ = 0.5(x + 4)
y′′ = 0.5y
and reveals the difference between the two transform sequences.
9.3 Transforms and Matrices
Matrix notation was researched by the British mathematician Arthur Cayley around
1858. Cayley formalised matrix algebra, along with the American mathematicians
Charles Peirce (1839–1914) and his father, Benjamin Peirce (1809–1880). Previ-
ously, Carl Gauss had shown that transforms were not commutative, i.e. T1T2 ̸=
T2T1, (where T1 and T2 are transforms) and matrix notation clariﬁed such observa-
tions.
Consider the transform T1, where x and y are transformed into x′ and y′ respec-
tively:
T1 =
 x′ = ax + by
y′ = cx + dy
(9.1)
and a second transform T2, where x′ and y′ are transformed into x′′ and y′′ respec-
tively:
T2 =
 x′′ = Ax′ + By′
y′′ = Cx′ + Dy′ .
(9.2)
Substituting (9.1) in (9.2) we get
T3 =
 x′′ = A(ax + by) + B(cx + dy)
y′′ = C(ax + by) + D(cx + dy)
which simpliﬁes to
T3 =
 x′′ = (Aa + Bc)x + (Ab + Bd)y
y′′ = (Ca + Dc)x + (Cb + Dd)y .
(9.3)

152
9
Matrices
Having derived the algebra for T3, let’s examine matrix notation, where constants
are separated from the variables. For example, the transform (9.4)
x′ = ax + by
y′ = cx + dy
(9.4)
can be written in matrix form as:
 x′
y′

=
a b
c d
  x
y

(9.5)
where (9.5) contains two different structures: two single-column matrices or column
vectors
 x′
y′

and
 x
y

,
and a 2 × 2 matrix:
a b
c d

.
Algebraically, (9.4) and (9.5) are identical, which dictates the way (9.5) is converted
to (9.4). Therefore, using (9.5) we have x′ followed by the “=” sign, and the sum of
the products of the top row of constants a and b with the x and y in the last column
vector:
x′ = ax + by.
Next, we have y′ followed by the “=” sign, and the sum of the products of the
bottom row of constants c and d with the x and y in the last column vector:
y′ = cx + dy.
As an example,
 x′
y′

=
 3 4
5 6
  x
y

is equivalent to
x′ = 3x + 4y
y′ = 5x + 6y.

9.3 Transforms and Matrices
153
We can now write T1 and T2 using matrix notation:
T1 =
 x′
y′

=
a b
c d
  x
y

(9.6)
T2 =
 x′′
y′′

=
 A B
C D
  x′
y′

(9.7)
and substituting (9.6) in (9.7) we have
T3 =
 x′′
y′′

=
 A B
C D
 a b
c d
  x
y

.
(9.8)
But we have already computed T3 (9.3), which in matrix form is:
T3 =
 x′′
y′′

=
 Aa + Bc Ab + Bd
Ca + Dc Cb + Dd
  x
y

(9.9)
which implies that
 A B
C D
 a b
c d

=
 Aa + Bc Ab + Bd
Ca + Dc Cb + Dd

and demonstrates how matrices must be multiplied. Here are the rules for matrix
multiplication:
 A
B
· · · · · ·
 a · · ·
c · · ·

=
 Aa + Bc · · ·
· · ·
· · ·

.
1: The top left-hand corner element Aa + Bc is the product of the top row of the
ﬁrst matrix by the left column of the second matrix.
 A
B
· · · · · ·
 · · · b
· · · d

=
· · · Ab + Bd
· · ·
· · ·

.
2: The top right-hand element Ab + Bd is the product of the top row of the ﬁrst
matrix by the right column of the second matrix.
· · · · · ·
C
D
 a · · ·
c · · ·

=

· · ·
· · ·
Ca + Dc · · ·

.
3: The bottom left-hand element Ca + Dc is the product of the bottom row of the
ﬁrst matrix by the left column of the second matrix.
· · · · · ·
C
D
 · · · b
· · · d

=
· · ·
· · ·
· · · Cb + Dd

.

154
9
Matrices
4: The bottom right-hand element Cb + Dd is the product of the bottom row of the
ﬁrst matrix by the right column of the second matrix.
Let’s multiply the following matrices together:
 2 4
6 8
  3 5
7 9

=
 (2 × 3 + 4 × 7) (2 × 5 + 4 × 9)
(6 × 3 + 8 × 7) (6 × 5 + 8 × 9)

=
34
46
74 102

.
9.4 Matrix Notation
Having examined the background to matrices, we can now formalise their notation.
A matrix is an array of numbers (real, imaginary, complex, etc.) organised in m
rows and n columns, where each entry ai j belongs to the ith row and jth column:
A =
⎡
⎢⎢⎢⎢⎢⎣
a11 a12 a13 · · · a1n
a21 a22 a23 · · · a2n
a31 a32 a33 · · · a3n
...
...
...
...
...
am1 am2 am3 · · · amn
⎤
⎥⎥⎥⎥⎥⎦
.
It is also convenient to express the above deﬁnition as
A = [ai j]m n.
9.4.1 Matrix Dimension or Order
The dimension or order of a matrix is the expression m × n where m is the number
of rows, and n is the number of columns.
9.4.2 Square Matrix
A square matrix has the same number of rows as columns:
A = [ai j]n n =
⎡
⎢⎢⎢⎣
a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
...
an1 an2 . . . ann
⎤
⎥⎥⎥⎦,
e.g.,
⎡
⎣
1 −2 4
6
5 7
4
3 1
⎤
⎦.

9.4 Matrix Notation
155
9.4.3 Column Vector
A column vector is a matrix with a single column:
⎡
⎢⎢⎢⎣
a11
a21
...
am1
⎤
⎥⎥⎥⎦,
e.g.,
⎡
⎣
2
3
23
⎤
⎦.
9.4.4 Row Vector
A row vector is a matrix with a single row:
a11 a12 · · · a1n

,
e.g.,
2 3 5 
.
9.4.5 Null Matrix
A null matrix has all its elements equal to zero:
θn = [ai j]n n =
⎡
⎢⎢⎢⎣
0 0 · · · 0
0 0 · · · 0
...
... ... ...
0 0 · · · 0
⎤
⎥⎥⎥⎦,
e.g., θ3 =
⎡
⎣
0 0 0
0 0 0
0 0 0
⎤
⎦.
The null matrix behaves like zero when used with numbers, where we have, 0 + n =
n + 0 = n and 0 × n = n × 0 = 0, and similarly, θ + A = A + θ = A and
θA = Aθ = θ. For example,
⎡
⎣
0 0 0
0 0 0
0 0 0
⎤
⎦
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦=
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦
⎡
⎣
0 0 0
0 0 0
0 0 0
⎤
⎦=
⎡
⎣
0 0 0
0 0 0
0 0 0
⎤
⎦.
9.4.6 Unit Matrix
A unit matrix In, is a square matrix with the elements on its diagonal a11 to ann equal
to 1:

156
9
Matrices
In = [ai j]n n =
⎡
⎢⎢⎢⎣
1 0 · · · 0
0 1 · · · 0
... ... ... ...
0 0 · · · 1
⎤
⎥⎥⎥⎦,
e.g., I3 =
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦.
The unit matrix behaves like the number 1 in a conventional product, where we have,
1 × n = n × 1 = n, and similarly, IA = AI = A. For example,
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦=
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦=
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦.
9.4.7 Trace
The trace of a square matrix is the sum of the elements on its diagonal a11 to ann:
Tr(A) =
n

i=1
aii.
For example, given
A =
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦,
then Tr(A) = 1 + 5 + 9 = 15.
The trace of a rotation matrix can be used to compute the angle of rotation. For
example, the matrix to rotate a point about the origin is
A =
 cos θ −sin θ
sin θ
cos θ

where
Tr(A) = 2 cos θ
which means that
θ = arccos
Tr(A)
2

.

9.4 Matrix Notation
157
The three matrices for rotating points about the x-, y- and z-axis are respectively:
Rα,x =
⎡
⎣
1
0
0
0 cos α −sin α
0 sin α cos α
⎤
⎦
Rα,y =
⎡
⎣
cos α 0 sin α
0
1
0
−sin α 0 cos α
⎤
⎦
Rα,z =
⎡
⎣
cos α −sin α 0
sin α cos α 0
0
0
1
⎤
⎦
and it is clear that
Tr(Rα,x) = Tr(Rα,y) = Tr(Rα,z) = 1 + 2 cos α
therefore,
α = arccos
Tr(Rα,x) −1
2

.
9.4.8 Determinant of a Matrix
The determinant of a matrix is a scalar value computed from the elements of the
matrix.ThedifferentmethodsforcomputingthedeterminantaredescribedinChap.6.
For example, using Sarrus’s rule:
A =
⎡
⎣
1 2 3
4 5 6
7 8 9
⎤
⎦
then, det A = 45 + 84 + 96 −105 −48 −72 = 0.
9.4.9 Transpose
The transpose of a matrix exchanges all row elements for column elements. The
transposition is indicated by the letter ‘T’ outside the right-hand bracket.
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦
T
=
⎡
⎣
a11 a21 a31
a12 a22 a32
a13 a23 a33
⎤
⎦.

158
9
Matrices
For example,
⎡
⎣
1 2 4
6 5 7
4 3 1
⎤
⎦
T
=
⎡
⎣
1 6 4
2 5 3
4 7 1
⎤
⎦,
and
⎡
⎣
2
3
5
⎤
⎦
T
=
2 3 5
.
To prove that (AB)T = BTAT, we could develop a general proof using n×n matrices,
but for simplicity, let’s employ 3 × 3 matrices and assume the result generalises to
higher dimensions. Given
A =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦,
AT =
⎡
⎣
a11 a21 a31
a12 a22 a32
a13 a23 a33
⎤
⎦
and
B =
⎡
⎣
b11 b12 b13
b21 b22 b23
b31 b32 b33
⎤
⎦,
BT =
⎡
⎣
b11 b21 b31
b12 b22 b32
b13 b23 b33
⎤
⎦
then,
AB =
⎡
⎣
a11b11 + a12b21 + a13b31 a11b12 + a12b22 + a13b32 a11b13 + a12b23 + a13b33
a21b11 + a22b21 + a23b31 a21b12 + a22b22 + a23b32 a21b13 + a22b23 + a23b33
a31b11 + a32b21 + a33b31 a31b12 + a32b22 + a33b32 a31b13 + a32b23 + a33b33
⎤
⎦
(AB)T =
⎡
⎣
a11b11 + a12b21 + a13b31 a21b11 + a22b21 + a23b31 a31b11 + a32b21 + a33b31
a11b12 + a12b22 + a13b32 a21b12 + a22b22 + a23b32 a31b12 + a32b22 + a33b32
a11b13 + a12b23 + a13b33 a21b13 + a22b23 + a23b33 a31b13 + a32b23 + a33b33
⎤
⎦
and
BTAT =
⎡
⎣
b11a11 + b21a12 + b31a13 b11a21 + b21a22 + b31a23 b11a31 + b21a32 + b31a33
b12a11 + b22a12 + b32a13 b12a21 + b22a22 + b32a23 b12a31 + b22a32 + b32a33
b13a11 + b23a12 + b33a13 b13a21 + b23a22 + b33a23 b13a31 + b23a32 + b33a33
⎤
⎦
which conﬁrms that (AB)T = BTAT.

9.4 Matrix Notation
159
9.4.10 Symmetric Matrix
A symmetric matrix is a square matrix that equals its transpose: i.e., A = AT. For
example, A is a symmetric matrix:
A =
⎡
⎣
1 2 4
2 5 3
4 3 6
⎤
⎦=
⎡
⎣
1 2 4
2 5 3
4 3 6
⎤
⎦
T
.
In general, a square matrix A = S + Q, where S is a symmetric matrix, and Q is an
antisymmetric matrix. The symmetric matrix is computed as follows. Given a matrix
A and its transpose AT
A =
⎡
⎢⎢⎢⎣
a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
...
an1 an2 . . . ann
⎤
⎥⎥⎥⎦,
AT =
⎡
⎢⎢⎢⎣
a11 a21 . . . an1
a12 a22 . . . an2
...
...
...
...
a1n a2n . . . ann
⎤
⎥⎥⎥⎦
their sum is
A + AT =
⎡
⎢⎢⎢⎣
2a11
a12 + a21 . . . a1n + an1
a12 + a21
2a22
. . . a2n + an2
...
...
...
...
a1n + an1 a2n + an2 . . .
2ann
⎤
⎥⎥⎥⎦.
By inspection, A + AT is symmetric, and if we divide throughout by 2 we have
S = 1
2

A + AT
which is deﬁned as the symmetric part of A. For example, given
A =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦,
AT =
⎡
⎣
a11 a21 a31
a12 a22 a32
a13 a23 a33
⎤
⎦

160
9
Matrices
then
S = 1
2

A + AT
=
⎡
⎣
a11
(a12 + a21)/2 (a13 + a31)/2
(a12 + a21)/2
a22
a23 + a32
(a13 + a31)/2 (a23 + a32)/2
a33
⎤
⎦
=
⎡
⎣
a11 s3/2 s2/2
s3/2 a22 s1/2
s2/2 s1/2 a33
⎤
⎦
where
s1 = a23 + a32
s2 = a13 + a31
s3 = a12 + a21.
Using a real example:
A =
⎡
⎣
0 1 4
3 1 4
4 2 6
⎤
⎦,
AT =
⎡
⎣
0 3 4
1 1 2
4 4 6
⎤
⎦
S =
⎡
⎣
0 2 4
2 1 3
4 3 6
⎤
⎦
which equals its own transpose.
9.4.11 Antisymmetric Matrix
An antisymmetric matrix is a matrix whose transpose is its own negative:
AT = −A
and is also known as a skew-symmetric matrix.
As the elements of A and AT are related by
arow,col = −acol,row.
When k = row = col:
ak,k = −ak,k

9.4 Matrix Notation
161
which implies that the diagonal elements must be zero. For example, this is an
antisymmetric matrix
A =
⎡
⎣
0 −2
4
2
0 −3
−4
3
0
⎤
⎦= −
⎡
⎣
0 −2
4
2
0 −3
−4
3
0
⎤
⎦
T
.
The antisymmetric part is computed as follows. Given a matrix A and its
transpose AT
A =
⎡
⎢⎢⎢⎣
a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
...
an1 an2 . . . ann
⎤
⎥⎥⎥⎦,
AT =
⎡
⎢⎢⎢⎣
a11 a21 . . . an1
a12 a22 . . . an2
...
...
...
...
a1n a2n . . . ann
⎤
⎥⎥⎥⎦
their difference is
A −AT =
⎡
⎢⎢⎢⎣
0
a12 −a21
. . . a1n −an1
−

a12 −a21

0
. . . a2n −an2
...
...
...
...
−

a1n −an1

−

a2n −an2

. . .
0
⎤
⎥⎥⎥⎦.
It is clear that A −AT is antisymmetric, and if we divide throughout by 2 we have
Q = 1
2

A −AT
.
For example:
A =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦,
AT =
⎡
⎣
a11 a21 a31
a12 a22 a32
a13 a23 a33
⎤
⎦
Q =
⎡
⎣
0

a12 −a21

/2

a13 −a31

/2

a21 −a12

/2
0

a23 −a32

/2

a31 −a13

/2

a32 −a23

/2
0
⎤
⎦
and if we maintain some symmetry with the subscripts, we have
Q =
⎡
⎣
0

a12 −a21

/2 −

a31 −a13

/2
−

a12 −a21

/2
0

a23 −a32

/2

a31 −a13

/2 −

a23 −a32

/2
0
⎤
⎦
=
⎡
⎣
0
q3/2 −q2/2
−q3/2
0
q1/2
q2/2 −q1/2
0
⎤
⎦

162
9
Matrices
where
q1 = a23 −a32
q2 = a31 −a13
q3 = a12 −a21.
Using a real example:
A =
⎡
⎣
0 1 4
3 1 4
4 2 6
⎤
⎦,
AT =
⎡
⎣
0 3 4
1 1 2
4 4 6
⎤
⎦
Q =
⎡
⎣
0 −1 0
1
0 1
0 −1 0
⎤
⎦.
Furthermore, we have already computed
S =
⎡
⎣
0 2 4
2 1 3
4 3 6
⎤
⎦
and
S + Q =
⎡
⎣
0 1 4
3 1 4
4 2 6
⎤
⎦= A.
9.5 Matrix Addition and Subtraction
As equations can be added and subtracted together, it follows that matrices can also
be added and subtracted, as long as they have the same dimension. For example,
given
A =
⎡
⎣
11
22
14 −15
27
28
⎤
⎦
and B =
⎡
⎣
2 1
−4 5
1 8
⎤
⎦
then
A + B =
⎡
⎣
13
23
10 −10
28
36
⎤
⎦,
A −B =
⎡
⎣
9
21
18 −20
26
20
⎤
⎦.

9.5 Matrix Addition and Subtraction
163
9.5.1 Scalar Multiplication
As equations can be scaled and factorised, it follows that matrixes can also be scaled
and factorised.
λA = λ
⎡
⎢⎢⎢⎣
a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
...
am1 am2 . . . amn
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
λa11 λa12 . . . λa13
λa21 λa22 . . . λa23
...
...
...
...
λam1 λam2 . . . λamn
⎤
⎥⎥⎥⎦.
For example,
2
 1 2 3
4 5 6

=
2
4
6
8 10 12

.
9.6 Matrix Products
We have already seen that matrices can be multiplied together employing rules that
maintain the algebraic integrity of the equations they represent. And as matrices
may be vectors, rectangular or square, we need to examine the products that are
permitted. To keep the notation simple, the deﬁnitions and examples are restricted
to a dimension of 3 or 3 × 3.
We begin with row and column vectors.
9.6.1 Row and Column Vectors
Given
A =

a
b c

and B =
⎡
⎣
α
β
γ
⎤
⎦
then
AB =
a b c 
⎡
⎣
α
β
γ
⎤
⎦= aα + bβ + cγ
which is a scalar and equivalent to the dot or scalar product of two vectors.

164
9
Matrices
For example, given
A =
2 3 4 
and B =
⎡
⎣
10
30
20
⎤
⎦
then
AB =
2 3 4 
⎡
⎣
10
30
20
⎤
⎦= 20 + 90 + 80 = 190.
Whereas,
BA =
⎡
⎣
b11
b21
b31
⎤
⎦a11 a12 a13

=
⎡
⎣
b11a11 b11a12 b11a13
b21a11 b21a12 b21a13
b31a11 b31a12 b31a13
⎤
⎦.
For example,
BA =
⎡
⎣
10
30
20
⎤
⎦2 3 4 
=
⎡
⎣
20 30 40
60 90 120
40 60 80
⎤
⎦.
The products AA and BB are not permitted.
9.6.2 Row Vector and a Matrix
Given
A =
a11 a12 a13

and B =
⎡
⎣
b11 b12 b13
b21 b22 b23
bm1 bm2 b33
⎤
⎦
then
AB =

a11 a12 a13

⎡
⎣
b11 b12 b13
b21 b22 b23
bm1 bm2 b33
⎤
⎦
=

(a11b11 + a12b21 + a13b31) (a11b12 + a12b22 + a13b32) (a11b13 + a12b23 + a13b33)

.
The product BA is not permitted.

9.6 Matrix Products
165
For example, given
A =

2 3 4

and B =
⎡
⎣
1 2 3
3 4 5
4 5 6
⎤
⎦
then
AB =

2 3 4

⎡
⎣
1 2 3
3 4 5
4 5 6
⎤
⎦
=

(2 + 9 + 16) (4 + 12 + 20) (6 + 15 + 24)

=

27 36 45

.
9.6.3 Matrix and a Column Vector
Given
A =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦
and B =
⎡
⎣
b11
b21
b31
⎤
⎦
then
AB =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦
⎡
⎣
b11
b21
b31
⎤
⎦=
⎡
⎣
a11b11 + a12b21 + a13b31
a21b11 + a22b21 + a23b31
a31b11 + a32b21 + a33b31
⎤
⎦.
The product BA is not permitted.
For example, given
A =
⎡
⎣
1 2 3
3 4 5
4 5 6
⎤
⎦,
and B =
⎡
⎣
2
3
4
⎤
⎦
then
AB =
⎡
⎣
1 2 3
3 4 5
4 5 6
⎤
⎦
⎡
⎣
2
3
4
⎤
⎦=
⎡
⎣
2 + 6 + 12
6 + 12 + 20
8 + 15 + 24
⎤
⎦=
⎡
⎣
20
38
47
⎤
⎦.

166
9
Matrices
9.6.4 Square Matrices
To clarify the products, lower-case Greek symbols are used with lower-case letters.
Here are their names:
α = alpha,
β = beta,
γ = gamma,
λ = lambda,
μ = mu,
ν = nu,
ρ = rho,
σ = sigma,
τ = tau.
Given
A =
⎡
⎣
a b c
p q r
u v w
⎤
⎦
and B =
⎡
⎣
α β γ
λ μ ν
ρ σ τ
⎤
⎦
then
AB =
⎡
⎣
a b c
p q r
u v w
⎤
⎦
⎡
⎣
α β γ
λ μ ν
ρ σ τ
⎤
⎦=
⎡
⎣
aα + bλ + cρ
aβ + bμ + cσ
aγ + bν + cτ
pα + qλ + rρ
pβ + qμ + rσ
pγ + qν + rτ
uα + vλ + wρ
uβ + vμ + wσ
uγ + vν + wτ
⎤
⎦
and
BA =
⎡
⎣
α β γ
λ μ ν
ρ σ τ
⎤
⎦
⎡
⎣
a b c
p q r
u v w
⎤
⎦=
⎡
⎣
αa + β p + γ u
αb + βq + γ v αc + βr + γ w
λa + μp + νu
λb + μq + νv λc + μr + νw
ρa + σ p + τu
ρb + σq + τv ρc + σr + τw
⎤
⎦.
For example, given
A =
⎡
⎣
1 2 3
3 4 5
5 6 7
⎤
⎦
and B =
⎡
⎣
2 3 4
4 5 6
6 7 8
⎤
⎦
then
AB =
⎡
⎣
1 2 3
3 4 5
5 6 7
⎤
⎦
⎡
⎣
2 3 4
4 5 6
6 7 8
⎤
⎦=
⎡
⎣
28 34
40
52 64
76
76 92 112
⎤
⎦
and
BA =
⎡
⎣
2 3 4
4 5 6
6 7 8
⎤
⎦
⎡
⎣
1 2 3
3 4 5
5 6 7
⎤
⎦=
⎡
⎣
31 40
49
49 64
89
67 88 109
⎤
⎦.

9.6 Matrix Products
167
9.6.5 Rectangular Matrices
Given two rectangular matrices A and B, where A has a dimension m×n, the product
AB is permitted, if and only if, B has a dimension n × p. The resulting matrix has a
dimension m × p. For example, given
A =
⎡
⎣
a11 a12
a21 a22
a31 a32
⎤
⎦
and B =
b11 b12 b13 b14
b21 b22 b23 b24

then
AB =
⎡
⎣
a11 a12
a21 a22
a31 a32
⎤
⎦
 b11 b12 b13 b14
b21 b22 b23 b24

=
⎡
⎣
(a11b11 + a12b21) (a11b12 + a12b22) (a11b13 + a12b23) (a11b14 + a12b24)
(a21b11 + a22b21) (a21b12 + a22b22) (a21b13 + a22b23) (a21b14 + a22b24)
(a31b11 + a32b21) (a31b12 + a32b22) (a31b13 + a32b23) (a31b14 + a32b24)
⎤
⎦.
9.7 Inverse Matrix
A square matrix Ann that is invertible satisﬁes the condition:
AnnA−1
nn = A−1
nn Ann = In,
where A−1
nn is unique, and is the inverse matrix of Ann. For example, given
A =
4 3
5 4

then
A−1 =

4 −3
−5
4

because
AA−1 =
4 3
5 4
 
4 −3
−5
4

=
 1 0
0 1

.
A square matrix whose determinant is 0, cannot have an inverse, and is known as a
singular matrix.

168
9
Matrices
We now require a way to compute A−1, which is rather easy.
Consider two linear equations:
 x′
y′

=
a b
c d
  x
y

.
(9.10)
Let the inverse of
a b
c d

be
 e f
g h

therefore,
 e f
g h
 a b
c d

=
 1 0
0 1

.
(9.11)
From (9.11) we have
ae + cf = 1
(9.12)
be + d f = 0
(9.13)
ag + ch = 0
(9.14)
bg + dh = 1.
(9.15)
Multiply (9.12) by d and (9.13) by c, and subtract:
ade + cd f = d
bce + cd f = 0
ade −bce = d
therefore,
e =
d
ad −bc.
Multiply (9.12) by b and (9.13) by a, and subtract:
abe + bcf = b
abe + ad f = 0
ad f −bcf = −b

9.7 Inverse Matrix
169
therefore,
f =
−b
ad −bc.
Multiply (9.14) by d and (9.15) by c, and subtract:
adg + cdh = 0
bcg + cdh = c
adg −bcg = −c
therefore,
g =
−c
ad −bc.
Multiply (9.14) by b and (9.15) by a, and subtract:
abg + bch = 0
abg + adh = a
adh −bch = a
therefore,
h =
a
ad −bc.
We now have values for e, f , g and h, which are the elements of the inverse matrix.
Consequently, given
A =
a b
c d

and A−1 =
 e f
g h

,
then
A−1 =
1
det A

d −b
−c
a

.
The inverse matrix permits us to solve a pair or linear equations as follows. Starting
with
 x′
y′

=
a b
c d
  x
y

= A
 x
y


170
9
Matrices
multiply both sides by the inverse matrix:
A−1
 x′
y′

= A−1A
 x
y

A−1
 x′
y′

=
 1 0
0 1
  x
y

=
 x
y

 x
y

= A−1
 x′
y′

 x
y

=
1
det A
 d −b
−c a
  x′
y′

.
Although the elements of A−1 come from A, the relationship is not obvious. However,
if A is transposed, a pattern is revealed. Given
A =
a b
c d

then AT =
a c
b d

and placing A−1 alongside AT, we have
A−1 =
 e f
g h

and AT =
a c
b d

.
The elements of A−1 share a common denominator (det A), which is placed outside
the matrix, therefore, the matrix elements are taken from AT as follows. For any
entry ai j in A−1, mask out the ith row and jth column in AT, and the remaining
entry is copied to the i jth entry in A−1. In the case of e, it is d. For f , it is b, with a
sign reversal. For g, it is c, with a sign reversal, and for h, it is a. The sign change is
computed by the same formula used with determinants:
(−1)i+ j.
which generates this pattern:
+ −
−+

.
You may be wondering what happens when a 3×3 matrix is inverted. Well, the same
technique is used, but when the ith row and jth column in AT is masked out, it leaves
behind a 2 × 2 determinant, whose value is copied to the i jth entry in A−1, with the
appropriate sign change. We investigate this later on.
Let’s illustrate this with an example. Given
42 = 6x + 2y
28 = 2x + 3y

9.7 Inverse Matrix
171
let
A =
 6 2
2 3

then det A = 14, therefore,
 x
y

= 1
14

3 −2
−2
6
 42
28

= 1
14
 70
84

=
5
6

.
which is the solution.
Now let’s investigate how to invert a 3 × 3 matrix. Given three simultaneous
equations in three unknowns:
x′ = ax + by + cz
y′ = dx + ey + f z
z′ = gx + hy + jz
they can be written using matrices as follows:
⎡
⎣
x′
y′
z′
⎤
⎦=
⎡
⎣
a b c
d e f
g h j
⎤
⎦
⎡
⎣
x
y
z
⎤
⎦= A
⎡
⎣
x
y
z
⎤
⎦.
Let
A−1 =
⎡
⎣
l m n
p q r
s t u
⎤
⎦
therefore,
⎡
⎣
l m n
p q r
s t u
⎤
⎦
⎡
⎣
a b c
d e f
g h j
⎤
⎦=
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦.
(9.16)
From (9.16) we can write:
la + md + ng = 1
(9.17)
lb + me + nh = 0
(9.18)
lc + m f + nj = 0.
(9.19)

172
9
Matrices
Multiply (9.17) by e and (9.18) by d, and subtract:
ael + dem + egn = e
bdl + dem + dhn = 0
ael −bdl + egn −dhn = e
l(ae −bd) + n(eg −dh) = e.
(9.20)
Multiply (9.18) by f and (9.19) by e, and subtract:
bf l + ef m + f hn = 0
cel + ef m + ejn = 0
bf l −cel + f hn −ejn = 0
l(bf −ce) + n( f h −ej) = 0.
(9.21)
Multiply (9.20) by ( f h −ej) and (9.21) by (eg −dh), and subtract:
l(ae −bd)( f h −ej) + n(eg −dh)( f h −ej) = e( f h −ej)
l(bf −ce)(eg −dh) + n(eg −dh)( f h −ej) = 0
l(ae −bd)( f h −ej) −l(bf −ce)(eg −dh) = ef h −e2 j
l(aef h −ae2 j −bd f h + bdej −bef g + bd f h + ce2g −cdeh) = ef h −e2 j
l(aef h −ae2 j + bdej −bef g + ce2g −cdeh) = ef h −e2 j
l(af h + bdj + ceg −aej −cdh −bf g) = f h −ej
l(aej + bf g + cdh −af h −bdj −ceg) = ej −f h
but (aej +bf g+cdh−af h−bdj −ceg) is the Sarrus expansion for det A, therefore
l = ej −f h
det A .
An exhaustive algebraic analysis reveals:
l =ej −f h
det A ,
m = −bj −ch
det A ,
n = bf −ce
det A ,
p = −dj −g f
det A ,
q =aj −gc
det A ,
r = −af −dc
det A ,
s =dh −ge
det A ,
t = −ah −gb
det A ,
u = ae −bd
det A ,

9.7 Inverse Matrix
173
where
A−1 =
⎡
⎣
l m n
p q r
s t u
⎤
⎦
A =
⎡
⎣
a b c
d e f
g h j
⎤
⎦.
However, there does not appear to be an obvious way of deriving A−1 from A. But,
as we discovered with the 2 × 2 matrix, the transpose AT resolves the problem:
A−1 =
⎡
⎣
l m n
p q r
s t u
⎤
⎦,
AT =
⎡
⎣
a d g
b e h
c f j
⎤
⎦.
The elements for A−1 share a common denominator (det A), which is placed outside
the matrix, therefore, the matrix elements are taken from AT as follows. For any entry
ai j in A−1, mask out the ith row and jth column in AT, and the remaining elements,
in the form of a 2×2 determinant, is copied to the i jth entry in A−1. In the case of l,
it is (ej −h f ). For m, it is (bj −hc), with a sign reversal, and for n, it is (bf −ec).
The sign change is computed by the same formula used with determinants:
(−1)i+ j,
which generates the pattern:
⎡
⎣
+ −+
−+ −
+ −+
⎤
⎦.
With the above aide-mémoire, it is easy to write down the inverse matrix:
A−1 =
1
det A
⎡
⎣
(ej −f h) −(bj −ch)
(bf −ce)
−(dj −g f )
(aj −gc) −(af −dc)
(dh −ge) −(ah −gb)
(ae −bd)
⎤
⎦.
This technique is known as the Laplacian expansion or the cofactor expansion, after
Pierre-Simon Laplace. The matrix of minor determinants is called the cofactor matrix
of A, which permits the inverse matrix to be written as:
A−1 = (cofactor matrix of A)T
det A
.

174
9
Matrices
Let’s illustrate this solution with an example. Given
18 = 2x + 2y + 2z
20 = x + 2y + 3z
7 = y + z
therefore,
⎡
⎣
18
20
7
⎤
⎦=
⎡
⎣
2 2 2
1 2 3
0 1 1
⎤
⎦
⎡
⎣
x
y
z
⎤
⎦= A
⎡
⎣
x
y
z
⎤
⎦.
and
det A = 4 + 2 −2 −6 = −2
AT =
⎡
⎣
2 1 0
2 2 1
2 3 1
⎤
⎦
therefore,
A−1 = −1
2
⎡
⎣
−1
0
2
−1
2 −4
1 −2
2
⎤
⎦
and
⎡
⎣
x
y
z
⎤
⎦= −1
2
⎡
⎣
−1
0
2
−1
2 −4
1 −2
2
⎤
⎦
⎡
⎣
18
20
7
⎤
⎦=
⎡
⎣
2
3
4
⎤
⎦
which is the solution.
9.7.1 Inverting a Pair of Matrices
Having seen how to invert a single matrix, let’s investigate how to invert of a pair of
matrices.
Given two matrices T and R, the product TR and its inverse (TR)−1 must equal
the identity matrix I:
(TR)(TR)−1 = I

9.7 Inverse Matrix
175
and multiplying throughout by T−1 we have
T−1TR(TR)−1 = T−1
R(TR)−1 = T−1.
Multiplying throughout by R−1 we have
R−1R(TR)−1 = R−1T−1
(TR)−1 = R−1T−1.
Therefore, if T and R are invertible, then
(TR)−1 = R−1T−1.
Generalising this result to a triple product such as STR we can reason that
(STR)−1 = R−1T−1S−1.
9.8 Orthogonal Matrix
A matrix is orthogonal if its transpose is also its inverse, i.e., matrix A is orthogonal if
AT = A−1.
For example,
A =
 1
√
2 −1
√
2
1
√
2
1
√
2

and
AT =

1
√
2
1
√
2
−1
√
2
1
√
2

and
AAT =
 1
√
2 −1
√
2
1
√
2
1
√
2
 
1
√
2
1
√
2
−1
√
2
1
√
2

=
 1 0
0 1

which implies that AT = A−1.

176
9
Matrices
The following matrix is also orthogonal
A =
 cos β −sin β
sin β
cos β

because
AT =
 cos β sin β
−sin β cos β

and
AAT =
cos β −sin β
sin β
cos β
  cos β sin β
−sin β cos β

=
 1 0
0 1

.
Orthogonal matrices play an important role in rotations because they leave the origin
ﬁxed and preserve all angles and distances. Consequently, an object’s geometric
integrity is maintained after a rotation, which is why an orthogonal transform is
known as a rigid motion transform.
9.9 Diagonal Matrix
A diagonal matrix is a square matrix whose elements are zero, apart from its diagonal:
A =
⎡
⎢⎢⎢⎣
a11 0 . . . 0
0 a22 . . . 0
...
...
...
...
0
0 . . . ann
⎤
⎥⎥⎥⎦.
The determinant of a diagonal matrix must be
det A = a11 × a22 × · · · ann.
Here is a diagonal matrix with its determinant
A =
⎡
⎣
2 0 0
0 3 0
0 0 4
⎤
⎦
det A = 2 × 3 × 4 = 24.
The identity matrix I is a diagonal matrix with a determinant of 1.

9.10 Worked Examples
177
9.10 Worked Examples
9.10.1 Matrix Inversion
Invert A and show that AA−1 = I2.
A =
 3 5
2 4

.
Using
A−1 =
1
det A

d −b
−c
a

then det A = 2, and
A−1 = 1
2

4 −5
−2
3

.
Calculating AA−1:
AA−1 = 1
2
 3 5
2 4
 
4 −5
−2
3

= 1
2
 2 0
0 2

=
 1 0
0 1

.
9.10.2 Identity Matrix
Invert A and show that AA−1 = I3.
A =
⎡
⎣
2 3 4
1 2 1
5 6 7
⎤
⎦.
Using Sarrus’s rule for det A:
det A = 28 + 15 + 24 −40 −12 −21 = −6.

178
9
Matrices
Therefore,
AT =
⎡
⎣
2 1 5
3 2 6
4 1 7
⎤
⎦
A−1 = −1
6
⎡
⎣
(14 −6) −(21 −24)
(3 −8)
−(7 −5)
(14 −20) −(2 −4)
(6 −10) −(12 −15)
(4 −3)
⎤
⎦
= −1
6
⎡
⎣
8
3 −5
−2 −6
2
−4
3
1
⎤
⎦
and
AA−1 = −1
6
⎡
⎣
2 3 4
1 2 1
5 6 7
⎤
⎦
⎡
⎣
8
3 −5
−2 −6
2
−4
3
1
⎤
⎦
= −1
6
⎡
⎣
−6
0
0
0 −6
0
0
0 −6
⎤
⎦=
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦.
9.10.3 Solving Two Equations Using Matrices
Solve the following equations using matrices.
20 = 2x + 3y
36 = 7x + 2y.
Let
A =
 2 3
7 2

therefore, det A = −17, and
A−1 = −1
17

2 −3
−7
2

therefore,
 x
y

= −1
17

2 −3
−7
2
 20
36

= −1
17

40 −108
−140 + 72


9.10 Worked Examples
179
= −1
17
−68
−68

=
 4
4

therefore, x = y = 4.
9.10.4 Solving Three Equations Using Matrices
Solve the following equations using matrices.
10 = 2x + y −z
13 = −x −y + z
28 = −x + 2y + z.
Let
A =
⎡
⎣
2
1 −1
−1 −1
1
−1
2
1
⎤
⎦.
Using Sarrus’s rule for det A:
det A = −2 −1 + 2 + 1 −4 + 1 = −3.
Therefore,
AT =
⎡
⎣
2 −1 −1
1 −1
2
−1
1
1
⎤
⎦
A−1 = −1
3
⎡
⎣
(−1 −2) −(1 + 2)
(1 −1)
−(−1 + 1)
(2 −1) −(2 −1)
(−2 −1) −(4 + 1) (−2 + 1)
⎤
⎦
= −1
3
⎡
⎣
−3 −3
0
0
1 −1
−3 −5 −1
⎤
⎦

180
9
Matrices
therefore,
⎡
⎣
x
y
z
⎤
⎦= −1
3
⎡
⎣
−3 −3
0
0
1 −1
−3 −5 −1
⎤
⎦
⎡
⎣
10
13
28
⎤
⎦
= −1
3
⎡
⎣
−30 −39
13 −28
−30 −65 −28
⎤
⎦
= −1
3
⎡
⎣
−69
−15
−123
⎤
⎦
=
⎡
⎣
23
5
41
⎤
⎦
therefore, x = 23,
y = 5,
z = 41.
9.10.5 Solving Two Complex Equations
Solve the following complex equations using matrices.
7 + i8 = 2x + y
−4 −i = x −2y.
Let
A =
2
1
1 −2

therefore, det A = −5, and
AT =
 2
1
1 −2

A−1 = −1
5
 −2 −1
−1
2

therefore,
 x
y

= −1
5
−2 −1
−1
2
  7 + i8
−4 −i

= −1
5
−14 −i16 + 4 + i
−7 −i8 −8 −i2


9.10 Worked Examples
181
= −1
5
−10 −i15
−15 −i10

=
 2 + i3
3 + i2

therefore, x = 2 + i3,
y = 3 + i2.
9.10.6 Solving Three Complex Equations
Solve the following complex equations using matrices.
0 = x + y −z
3 + i3 = 2x −y + z
−5 −i5 = −x + y −2z.
Let
A =
⎡
⎣
1
1 −1
2 −1
1
−1
1 −2
⎤
⎦
therefore, det A = 2 −1 −2 + 1 −1 + 4 = 3, and
AT =
⎡
⎣
1
2 −1
1 −1
1
−1
1 −2
⎤
⎦
A−1 = 1
3
⎡
⎣
(2 −1) −(−2 + 1)
0
−(−4 + 1)
(−2 −1) −(1 + 2)
(2 −1)
−(1 + 1) (−1 −2)
⎤
⎦
therefore,
⎡
⎣
x
y
z
⎤
⎦= 1
3
⎡
⎣
1
1
0
3 −3 −3
1 −2 −3
⎤
⎦
⎡
⎣
0
3 + i3
−5 −i5
⎤
⎦
= 1
3
⎡
⎣
3 + i3
−9 −i9 + 15 + i15
−6 −i6 + 15 + i15
⎤
⎦
=
⎡
⎣
1 + i
2 + i2
3 + i3
⎤
⎦
therefore, x = 1 + i,
y = 2 + i2,
z = 3 + i3.

182
9
Matrices
9.10.7 Solving Two Complex Equations
Solve the following complex equations using matrices.
3 + i5 = ix + 2y
5 + i = 3x −iy.
Let
A =
 i
2
3 −i

therefore, set A = 1 −6 = −5, and
AT =
 i
3
2 −i

A−1 = −1
5
 −i −2
−3
i

therefore,
 x
y

= −1
5
 −i −2
−3
i
  3 + i5
5 + i

= −1
5
−i3 + 5 −10 −i2
−9 −i15 + i5 −1

= −1
5
 −5 −i5
−10 −i10

=
 1 + i
2 + i2

therefore, x = 1 + i,
y = 2 + i2.
9.10.8 Solving Three Complex Equations
Solve the following complex equations using matrices.
6 + i2 = ix + 2y −iz
−2 + i6 = 2x −iy + i2z
2 + i10 = i2x + iy + 2z.

9.10 Worked Examples
183
Let
A =
⎡
⎣
i
2 −i
2 −i i2
i2
i
2
⎤
⎦
therefore, det A = 2 −8 + 2 + i2 + i2 −8 = −12 + i4, and
AT =
⎡
⎣
i
2 i2
2 −i
i
−i i2 2
⎤
⎦
A−1 =
1
−12 + i4
⎡
⎣
(−i2 + 2)
−(4 −1)
(i4 + 1)
−(4 + 4)
(i2 −2)
−(−2 + i2)
(i2 −2) −(−1 −i4)
(1 −4)
⎤
⎦
=
1
−12 + i4
⎡
⎣
2 −i2
−3
1 + i4
−8
−2 + i2 2 −i2
−2 + i2 1 + i4
−3
⎤
⎦
therefore,
⎡
⎣
x
y
z
⎤
⎦=
1
−12 + i4
⎡
⎣
2 −i2
−3
1 + i4
−8
−2 + i2 2 −i2
−2 + i2 1 + i4
−3
⎤
⎦
⎡
⎣
6 + i2
−2 + i6
2 + i10
⎤
⎦
=
1
−12 + i4
⎡
⎣
(2 −i2)(6 + i2) −3(−2 + i6) + (1 + i4)(2 + i10)
−8(6 + i2) + (−2 + i2)(−2 + i6) + (2 −i2)(2 + i10)
(−2 + i2)(6 + i2) + (1 + i4)(−2 + i6) −3(2 + i10)
⎤
⎦
=
1
−12 + i4
⎡
⎣
12 + i4 −i12 + 4 + 6 −i18 + 2 + i10 + i8 −40
−48 −i16 + 4 −i12 −i4 −12 + 4 + i20 −i4 + 20
−12 −i4 + i12 −4 −2 + i6 −i8 −24 −6 −i30
⎤
⎦
=
1
−12 + i4
⎡
⎣
−16 −i8
−32 −i16
−48 −i24
⎤
⎦
multiply by the conjugate of −12 + i4:
⎡
⎣
x
y
z
⎤
⎦= −12 −i4
160
⎡
⎣
−16 −i8
−32 −i16
−48 −i24
⎤
⎦
therefore,
x =
1
160(−12 −i4)(−16 −i8)

184
9
Matrices
=
1
160(192 + i64 + i96 −32)
=
1
160(160 + i160) = 1 + i
y =
1
160(−12 −i4)(−32 −i16)
=
1
160(384 + i128 + i192 −64)
=
1
160(320 + i320) = 2 + i2
z =
1
160(−12 −i4)(−48 −i24)
=
1
160(576 + i192 + i288 −96)
=
1
160(480 + i480) = 3 + i3
therefore, x = 1 + i,
y = 2 + i2,
z = 3 + i3.

Chapter 10
Geometric Matrix Transforms
10.1 Introduction
Geometric matrix transforms are an intuitive way of deﬁning and building geometric
operations such as scale, translate, reﬂect, shear and rotate. In 2D, such operations
are generally associated with images and text, and widely used in internet browsers,
image-processing software, smart phones and watches. In 3D, they are used in com-
puter games, computer animation, ﬁlm special effects, virtual reality and scientiﬁc
visualisation. They have proved so useful that they are incorporated in hardware to
provide the highest possible execution speeds and real-time performance.
In this chapter, we build upon the ideas of matrices described in the previous
chapter, and provide a coherent framework for describing transforms in two and
three dimensions.
10.2 Matrix Transforms
The general 2D transform is
x′ = ax + by
y′ = cx + dy
(10.1)
or in matrix form:
 x′
y′

=
a b
c d
  x
y

where the values of a, b, c and d determine the type of transform. Let’s examine
2D transforms and generalise their application to 3D, and start with the translate
transform, as this reveals a fundamental problem with matrices.
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_10
185

186
10
Geometric Matrix Transforms
10.2.1 2D Translation
The translate transform is described by
x′ = x + tx
y′ = y + ty
where the point (x, y) is translated by (tx, ty). Modifying (10.1), this becomes
x′ = ax + by + tx
y′ = cx + dy + ty
where a = d = 1, and b = c = 0. However, this does not appear to have a single
matrix representation, due to the addition of tx and ty. Fortunately, homogeneous
coordinates come to the rescue, and support any type of transform incorporating
addition or subtraction. The idea is to solve a 2D problem in 3D, where any point
(x, y) becomes (x, y, 1), i.e. the z-coordinate equals 1. Rewriting (10.1) in 3D, we
have
x′ = ax + by + tx
y′ = cx + dy + ty
1 = 0x + 0y + 1
or in matrix form:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
a b tx
c d ty
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
The 2D translation transform is
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 tx
0 1 ty
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
Figure10.1 shows a shape translated by (5, 2) using this matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 5
0 1 2
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
As we are only interested in (x′, y′), the z-coordinate is ignored.

10.2 Matrix Transforms
187
Fig. 10.1 The blue shape is
the translated yellow shape
x
y
10.2.2 2D Scaling
2D scaling is achieved using
x′ = sxx
y′ = sy y
or as a homogeneous matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
sx 0 0
0 sy 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
The homogeneous form is maintained as we often have to combine different matri-
ces. Figure10.2 shows the effect of scaling a shape by ×2 horizontally, and ×1.5
vertically.
The scaling action is relative to the origin, i.e. the point (0, 0) remains unchanged.
Allotherpointsmoveawayfromtheoriginwhenthescalefactorexceeds1,ortowards
the origin when it is less than 1. To scale relative to another point (px, py) we ﬁrst
subtract (px, py) from (x, y). This effectively makes the reference point (px, py) the
new origin. Second, we perform the scaling operation relative to the new origin, and
third, add (px, py) back to the new (x, y) to compensate for the original subtraction.
Algebraically this is
x′ = sx(x −px) + px
y′ = sy(y −py) + py

188
10
Geometric Matrix Transforms
Fig. 10.2 Asymmetric
scaling relative to the origin
x
y
which simpliﬁes to
x′ = sxx + px(1 −sx)
y′ = sy y + py(1 −sy)
or as a homogeneous matrix
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
sx 0 px(1 −sx)
0 sy py(1 −sy)
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
(10.2)
Figure10.3 shows a scale of ×2 horizontally, and ×1.5 vertically relative to the
point (−3, 0) using this matrix:
Fig. 10.3 Asymmetric
scaling relative to (−3, 0)
x
y

10.2 Matrix Transforms
189
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
2 0 3
0 1.5 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
10.2.3 2D Reﬂections
The matrix transform for reﬂecting about the y-axis is
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
−1 0 0
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
(10.3)
or about the x-axis
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1
0 0
0 −1 0
0
0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
(10.4)
where (10.3) reverses the sign of the x-coordinate, and (10.4) reverses the sign of the
y-coordinate. However, to make a reﬂection about an arbitrary vertical or horizontal
axis we need to introduce some more algebraic deception.
To make a reﬂection about a vertical axis x = ax, we ﬁrst subtract ax from the x-
coordinate. This effectively makes the line x = ax coincident with the major y-axis.
Next we perform the reﬂection by reversing the sign of the modiﬁed x-coordinate,
and ﬁnally, we add ax to the reﬂected coordinate to compensate for the original
subtraction. Algebraically, the three steps are
x1 = x −ax
x2 = −(x −ax)
x′ = −(x −ax) + ax
which simpliﬁes to
x′ = −x + 2ax
y′ = y
or as a homogeneous matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
−1 0 2ax
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
(10.5)

190
10
Geometric Matrix Transforms
Fig. 10.4 Reﬂecting a shape
about the line x = 3
x
y
Figure10.4 shows a shape reﬂected about the line x = 3 using (10.6)
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
−1 0 6
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
(10.6)
To reﬂect a point about the line y = ay, the following transform is required:
x′ = x
y′ = −(y −ay) + ay
= −y + 2ay
or as a homogeneous matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0
0
0 −1 2ay
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
10.2.4 2D Shearing
A shape is sheared by leaning it over at an angle β. Figure10.5 illustrates the geom-
etry, where we see that the y-coordinates remain unchanged but the x-coordinates
are a function of y and tan β.
x′ = x + y tan β
y′ = y

10.2 Matrix Transforms
191
Fig. 10.5 Shearing a shape
by β
x
y
ytan
y
or as a homogeneous matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 tan β 0
0
1
0
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
In this example, the angle β is assumed positive when rotating from the y-axis
towards the x-axis.
10.2.5 2D Rotation
Figure10.6 shows a point P(x, y) rotated by an angle β about the origin to P′(x′, y′).
From the ﬁgure:
x′ = R cos(θ + β)
y′ = R sin(θ + β)
Fig. 10.6 Rotating a point
through an angle β
P(x, y)
P (x , y )
x
y
y
y
x
x
R
R

192
10
Geometric Matrix Transforms
and substituting the identities for cos(θ + β) and sin(θ + β) we have
x′ = R(cos θ cos β −sin θ sin β)
y′ = R(sin θ cos β + cos θ sin β)
x′ = R
 x
R cos β −y
R sin β
	
y′ = R
 y
R cos β + x
R sin β
	
x′ = x cos β −y sin β
y′ = x sin β + y cos β
or as a homogeneous matrix
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
cos β −sin β 0
sin β
cos β 0
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
(10.7)
and is the general transform for rotating a point about the origin.
Figure10.7 shows the effect of rotating the yellow arrow by 90◦using this matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
0 −1 0
1
0 0
0
0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
When β = 360◦the matrix becomes the identity matrix, and has a null effect:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
Fig. 10.7 The yellow shape
is rotated 90◦
x
y

10.2 Matrix Transforms
193
To rotate a point (x, y) about an arbitrary point (px, py) we ﬁrst, subtract (px, py)
from (x, y). This enables us to perform the rotation about the origin. Second, we
perform the rotation, and third, we add (px, py) to compensate for the original
subtraction. Here are the steps:
1. Subtract (px, py):
x1 = x −px
y1 = y −py.
2. Rotate β about the origin:
x2 = x1 cos β −y1 sin β
y2 = x1 sin β + y1 cos β.
3. Add (px, py):
x′ = x1 cos β −y1 sin β + px
y′ = x1 sin β + y1 cos β + py.
Simplifying,
x′ = x cos β −y sin β + px(1 −cos β) + py sin β
y′ = x sin β + y cos β + py(1 −cos β) −px sin β
and as a homogeneous matrix:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
cos β −sin β px(1 −cos β) + py sin β
sin β
cos β
py(1 −cos β) −px sin β
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
(10.8)
To rotate a point 90◦about the point (1, 1) (10.8) becomes
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
0 −1 2
1
0 0
0
0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
A simple test is to substitute the point (2, 1) for (x, y), which is transformed correctly
to (1, 2).
The algebraic approach in deriving the above transforms is relatively easy. How-
ever, it is also possible to use matrices to derive compound transforms, such as a
reﬂection relative to an arbitrary line and scaling and rotation relative to an arbi-
trary point. These transforms are called afﬁne, as parallel lines remain parallel after
being transformed. Furthermore, the word “afﬁne” is used to imply that there is a
strong geometric afﬁnity between the original and transformed shape. One can not

194
10
Geometric Matrix Transforms
always guarantee that angles and lengths are preserved, as the scaling transform can
alter these when different x and y scaling factors are used. For completeness, these
transforms are repeated from a matrix perspective.
10.2.6 2D Scaling
The strategy used to scale a point (x, y) relative to some arbitrary point (px, py) is to
ﬁrst, translate (−px, −py); second, perform the scaling; and third translate (px, py).
These three transforms are represented in matrix form as follows:
⎡
⎣
x′
y′
1
⎤
⎦=

translate(px, py)  
 scale(sx, sy)  
translate(−px, −py) 
⎡
⎣
x
y
1
⎤
⎦
which expands to
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 px
0 1 py
0 0 1
⎤
⎦
⎡
⎣
sx 0 0
0 sy 0
0 0 1
⎤
⎦
⎡
⎣
1 0 −px
0 1 −py
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
Note the sequence of the transforms, as this often causes confusion. The ﬁrst trans-
form acting on the point (x, y, 1) is translate (−px, −py), followed by scale (sx, sy),
followed by translate (px, py). If they are placed in any other sequence, you will
discover, like Gauss, that transforms are not commutative!
Now we concatenate these matrices into a single matrix by multiplying them
together. This can be done in any sequence, so long as we preserve the original order.
Let’s start with scale (sx, sy) and translate (−px, −py) matrices. This produces
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 px
0 1 py
0 0 1
⎤
⎦
⎡
⎣
sx 0 −sx px
0 sy −sy py
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
and ﬁnally
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
sx 0 px(1 −sx)
0 sy py(1 −sy)
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
which is the same as the previous transform (10.2).

10.2 Matrix Transforms
195
10.2.7 2D Reﬂection
A reﬂection about the y-axis is given by
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
−1 0 0
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
Therefore, using matrices, we can reason that a reﬂection transform about an arbitrary
line x = ax, parallel with the y-axis, is given by
⎡
⎣
x′
y′
1
⎤
⎦=

translate(ax, 0) 
 reﬂection  
translate(−ax, 0)
⎡
⎣
x
y
1
⎤
⎦
which expands to
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 ax
0 1 0
0 0 1
⎤
⎦
⎡
⎣
−1 0 0
0 1 0
0 0 1
⎤
⎦
⎡
⎣
1 0 −ax
0 1
0
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
Now we concatenate these matrices into a single matrix by multiplying them together.
Let’s begin by multiplying the reﬂection and the translate (−ax, 0) matrices together.
This produces
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 ax
0 1 0
0 0 1
⎤
⎦
⎡
⎣
−1 0 ax
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
and ﬁnally
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
−1 0 2ax
0 1 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
which is the same as the previous transform (10.5).
10.2.8 2D Rotation About an Arbitrary Point
A rotation about the origin is given by
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
cos β −sin β 0
sin β
cos β 0
0
0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.

196
10
Geometric Matrix Transforms
Therefore, using matrices, we can develop a rotation about an arbitrary point (px, py)
as follows:
⎡
⎣
x′
y′
1
⎤
⎦=

translate(px, py)  
rotateβ  
translate(−px, −py) 
⎡
⎣
x
y
1
⎤
⎦
which expands to
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 px
0 1 py
0 0 1
⎤
⎦
⎡
⎣
cos β −sin β 0
sin β
cos β 0
0
0 1
⎤
⎦
⎡
⎣
1 0 −px
0 1 −py
0 0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦.
Now we concatenate these matrices into a single matrix by multiplying them together.
Let’s begin by multiplying the rotate β and the translate (−px, −py) matrices
together. This produces
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 px
0 1 py
0 0 1
⎤
⎦
⎡
⎣
cos β −sin β −px cos β + py sin β
sin β
cos β −px sin β −py cos β
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
cos β −sin β px(1 −cos β) + py sin β
sin β
cos β py(1 −cos β) −px sin β
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
which is the same as the previous transform (10.8).
I hope it is now clear to the reader that one can derive all sorts of transforms either
algebraically, or by using matrices—it is just a question of convenience.
10.3 3D Transforms
Now we come to transforms in three dimensions, where we apply the same reasoning
as in two dimensions. However, translation remains a problem, unless we move the
problem to a four-dimensional homogeneous space, which means turning (x, y, z)
into (x, y, z, 1). Scaling and translation are basically the same, but in 2D, where we
rotated a shape about a point, in 3D, we rotate an object about an axis.
10.3.1 3D Translation
The algebra is so simple for 3D translation, that we can write the homogeneous
matrix directly:

10.3 3D Transforms
197
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1 0 0 tx
0 1 0 ty
0 0 1 tz
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
10.3.2 3D Scaling
The algebra for 3D scaling is
x′ = sxx
y′ = sy y
z′ = szz
and as a homogeneous matrix:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
sx 0 0 0
0 sy 0 0
0 0 sz 0
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
The scaling is relative to the origin, but we can arrange for it to be relative to an
arbitrary point (px, py, pz) using the following algebra:
x′ = sx(x −px) + px
y′ = sy(y −py) + py
z′ = sz(z −pz) + pz
and as a homogeneous matrix:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
sx 0 0 px(1 −sx)
0 sy 0 py(1 −sy)
0 0 sz pz(1 −sz)
0 0 0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
10.3.3 3D Rotation
In two dimensions a shape is rotated about a point, whether it be the origin or some
other position. In three dimensions an object is rotated about an axis, whether it be the
x-, y- or z-axis, or some arbitrary axis. To begin with, let’s look at rotating a vertex
about one of the three orthogonal axes; such rotations are called Euler rotations after
Leonhard Euler.

198
10
Geometric Matrix Transforms
Fig. 10.8 Rotating the point
P about the z-axis
P(x, y, z)
(
)
x
y
z
Recall that a general 2D rotation transform is given by
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
cos β −sin β 0
sin β
cos β 0
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
which in 3D can be visualised as rotating a point P(x, y, z) on a plane parallel with
the xy-plane as shown in Fig.10.8. In algebraic terms this is written as
x′ = x cos β −y sin β
y′ = x sin β + y cos β
z′ = z
and as a homogeneous matrix:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
cos β −sin β 0 0
sin β
cos β 0 0
0
0
1 0
0
0
0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦
which rotates a point about the z-axis.
When rotating about the x-axis, the x-coordinates remain constant whilst the y-
and z-coordinates are changed. Algebraically, this is
x′ = x
y′ = y cos β −z sin β
z′ = y sin β + z cos β

10.3 3D Transforms
199
and as a homogeneous matrix:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
0
0
0
0 cos β −sin β 0
0 sin β
cos β 0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
When rotating about the y-axis, the y-coordinate remains constant whilst the x- and
z-coordinates are changed. Algebraically, this is
x′ = z sin β + x cos β
y′ = y
z′ = z cos β −x sin β
and as a homogeneous matrix:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
cos β 0 sin β 0
0
1
0
0
−sin β 0 cos β 0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
Note that the matrix terms do not appear to share the symmetry seen in the previous
two matrices. Nothing really has gone wrong, it is just the way the axes are paired
together to rotate the coordinates.
The above rotations are also known as yaw, pitch and roll, and great care should
be taken with these angles when referring to other books and technical papers. Some-
times a left-handed system of axes is used rather than a right-handed set, and the
vertical axis may be the y-axis or the z-axis. Consequently, the matrices represent-
ing the rotations can vary greatly. In this book, all Cartesian coordinate systems are
right-handed, and the vertical axis is generally the y-axis.
The roll, pitch and yaw angles are deﬁned as follows:
• roll is the angle of rotation about the z-axis,
•
pitch is the angle of rotation about the x-axis,
• yaw is the angle of rotation about the y-axis.
Figure10.9 illustrates these rotations and the sign convention. The homogeneous
matrices representing these rotations are as follows:
• rotate roll about the z-axis:
⎡
⎢⎢⎣
cosroll −sin roll 0 0
sin roll
cosroll 0 0
0
0
1 0
0
0
0 1
⎤
⎥⎥⎦

200
10
Geometric Matrix Transforms
Fig. 10.9 Rotating the point
P about the z-axis
x
y
z
roll
pitch
yaw
• rotate pitch about the x-axis:
⎡
⎢⎢⎣
1
0
0
0
0 cos pitch −sin pitch 0
0 sin pitch
cos pitch 0
0
0
0
1
⎤
⎥⎥⎦
• rotate yaw about the y-axis:
⎡
⎢⎢⎣
cos yaw 0 sin yaw 0
0
1
0
0
−sin yaw 0 cos yaw 0
0
0
0
1
⎤
⎥⎥⎦.
A common sequence for applying these rotations is roll, pitch, yaw, as seen in
the following transform:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=

 yaw  
 pitch  
roll 
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦
and if a translation is involved,
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=

translate  
 yaw  
 pitch  
roll 
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.

10.3 3D Transforms
201
Fig. 10.10 Rotating the
point P about an arbitrary
axis
P(x, y, z)
(
)
x
y
z
py
px
10.3.4 Rotating About an Axis
The above rotations are relative to the x-, y-, z-axis. Now let’s consider rotations
about an axis parallel to one of these axes. To begin with, we will rotate about an axis
parallel with the z-axis, as shown in Fig.10.10. The scenario is very reminiscent of
the 2D case for rotating a point about an arbitrary point, and the general transform
is given by
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=

translate(px, py, 0)  
rotateβ  
translate(−px, −py, 0) 
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦
and the matrix is
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
cos β −sin β 0 px(1 −cos β) + py sin β
sin β
cos β 0 py(1 −cos β) −px sin β
0
0
1
0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
Hopefully, you can see the similarity between rotating in 3D and 2D: the x- and
y-coordinates are updated while the z-coordinate is held constant. We can now state
the other two matrices for rotating about an axis parallel with the x-axis and parallel
with the y-axis:
• rotating about an axis parallel with the x-axis:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
0
0
0
0 cos β −sin β py(1 −cos β) + pz sin β
0 sin β
cos β
pz(1 −cos β) −py sin β
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦

202
10
Geometric Matrix Transforms
• rotating about an axis parallel with the y-axis:
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
cos β 0 sin β px(1 −cos β) −pz sin β
0
1
0
0
−sin β 0 cos β pz(1 −cos β) + px sin β
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
10.3.5 3D Reﬂections
Reﬂections in 3D occur with respect to a plane, rather than an axis. The homogeneous
matrix giving the reﬂection relative to the yz-plane is
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦
and the reﬂection relative to a plane parallel to, and ax units from the yz-plane is
⎡
⎢⎢⎣
x′
y′
z′
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1 0 0 2ax
0 1 0 0
0 0 1 0
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦.
10.4 Rotating a Point About an Arbitrary Axis
10.4.1 Matrices
Rotating a point about an arbitrary axis is achieved in various ways. We can employ
vectors, analytic geometry, matrices or quaternions. In this example, vectors are used,
and Fig.10.11 shows a view of the geometry associated with the task at hand. For
clariﬁcation, Fig.10.12 shows a cross-section and a plan view of the geometry.
The axis of rotation is given by the unit vector:
ˆn = ai + bj + ck.
P(x p, yp z p) is the point to be rotated by angle α to P′(x′
p, y′
p, z′
p). O is the origin,
whilst p and p′ are position vectors for P and P′ respectively. From Figs.10.11 and
10.12:
p′ = −−→
ON + −→
NQ +
−→
QP′.

10.4 Rotating a Point About an Arbitrary Axis
203
Fig. 10.11 The geometry
associated with rotating a
point about an arbitrary axis
P
p
n
p
O
r
N
n
Q
|r|
Fig. 10.12 A cross-section
and plan view of the
geometry
P
p
n
O
r
N
|r |
Q
P
N
w
n
To ﬁnd −−→
ON:
|n| = |p| cos θ
ˆn · p = |p| cos θ
|n| = ˆn · p
n = ˆn|n|
n = ˆn(ˆn · p)
therefore,
−−→
ON = n = ˆn(ˆn · p).
To ﬁnd −→
NQ:
−→
NQ = NQ
NP r = NQ
NP′ r = cos α r

204
10
Geometric Matrix Transforms
but
p = n + r = ˆn(ˆn · p) + r
therefore,
r = p −ˆn(ˆn · p)
and
−→
NQ = [p −ˆn(ˆn · p)] cos α.
To ﬁnd −→
QP′:
Let
ˆn × p = w
where
|w| = |ˆn||p| sin θ = |p| sin θ
but
|r| = |p| sin θ
therefore,
|w| = |r|.
Now
QP′
NP′ = QP′
|r| = QP′
|w| = sin α
therefore,
−→
QP′ = w sin α = (ˆn × p) sin α
then
p′ = ˆn(ˆn · p) + [p −ˆn(ˆn · p] cos α + (ˆn × p) sin α

10.4 Rotating a Point About an Arbitrary Axis
205
and
p′ = p cos α + ˆn(ˆn · p)(1 −cos α) + (ˆn × p) sin α.
Let
K = 1 −cos α
then
p′ = p cos α + ˆn(ˆn · p)K + (ˆn × p) sin α
and
p′ = (x pi + ypj + z pk) cos α + (ai + bj + ck)(ax p + byp + cz p)K
+ [(bz p −cyp)i + (cx p −az p)j + (ayp −bx p)k] sin α
p′ = [x p cos α + a(ax p + byp + cz p)K + (bz p −cyp) sin α]i
+ [yp cos α + b(ax p + byp + cz p)K + (cx p −az p) sin α]j
+ [z p cos α + c(ax p + byp + cz p)K + (ayp −bx p) sin α]k
p′ = [x p(a2K + cos α) + yp(abK −c sin α) + z p(acK + b sin α)]i
+ [x p(abK + c sin α) + yp(b2K + cos α) + z p(bcK −a sin α)]j
+ [x p(acK −b sin α) + yp(bcK + a sin α) + z p(c2K + cos α)]k
and the transform is:
⎡
⎢⎢⎣
x′
p
y′
p
z′
p
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
a2K + cos α abK −c sin α acK + b sin α 0
abK + c sin α b2K + cos α bcK −a sin α 0
acK −b sin α bcK + a sin α c2K + cos α 0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x p
yp
z p
1
⎤
⎥⎥⎦
where
K = 1 −cos α.
The Worked Examples at the end of this chapter illustrate how this matrix is used.

206
10
Geometric Matrix Transforms
10.5 Determinant of a Transform
The determinant of the transform (10.9) is ad −bc.
 x′
y′

=
a b
c d
  x
y

.
(10.9)
If we subject the vertices of a unit-square to this transform, we create the situation
shown in Fig.10.13. The vertices of the unit-square are transformed as follows:
(0, 0) ⇒(0, 0)
(1, 0) ⇒(a, c)
(1, 1) ⇒(a + b, c + d)
(0, 1) ⇒(b, d).
From Fig.10.13 it can be seen that the area of the transformed unit-square A is
given by
area = (a + b)(c + d) −B −C −D −E −F −G
= ac + ad + bc + bd −bd
2 −bc −ac
2 −bd
2 −bc −ac
2
= ad −bc
which is the determinant of the transform. But as the area of the original unit-square
is 1, the determinant of the transform controls the scaling factor applied to the trans-
formed shape.
Let’s examine the determinants of two transforms: The ﬁrst 2D transform encodes
a scaling of 2, and results in an overall area scaling of 4:
 2 0
0 2

Fig. 10.13 The inner
parallelogram is the
transformed unit square
A
B
C
D
E
F
G
(b, d)
(a, c)
(a+b, c+d)
(0, 0)
a
b
b
a
d
c

10.5 Determinant of a Transform
207
whose determinant is:

2 0
0 2
 = 4.
The second 2D transform encodes a scaling of 3 and a translation of (3, 3), and
results in an overall area scaling of 9:
⎡
⎣
3 0 3
0 3 3
0 0 1
⎤
⎦
whose determinant is:
3

3 3
0 1
 −0

0 3
0 1
 + 0

0 3
3 3
 = 9.
These two examples demonstrate the extra role played by the elements of a matrix.
10.6 Perspective Projection
In any 3D computer graphic application a database stores a collection of virtual
objects in the form of Cartesian coordinates, or other permitted formats. A virtual
camera is then located within this world space with position and direction using
a compound transform Tc. To capture a perspective view, each point (x, y, z) is
transformed to the camera’s coordinate system (xc, yc, zc) using the inverse of the
compound transform T−1
c .
A virtual camera is directed along its z-axis as shown in Fig.10.14. Positioned
d units along the z-axis is a virtual projection screen, which is used to capture the
perspective projection. Figure10.15 shows that any point (xc, yc, zc) is transformed
Fig. 10.14 The axial system
used for the perspective
projection
Xc
Yc
Zc
(xc, yc, zc)
xp
yp
Xp
Yp
d

208
10
Geometric Matrix Transforms
Fig. 10.15 A plan view of
the camera’s axial system
Xc
Zc
(xc, yc, zc)
xp
(xp, yp, d)
screen
zc
d
xc
Fig. 10.16 A side view of
the camera’s axial system
Yc
Zc
(xc, yc, zc)
yp
(xp, yp, d)
screen
zc
d
yc
to (x p, yp, d). It also shows that the screen’s x-axis is pointing in the opposite
direction to the camera’s x-axis, which can be compensated for by reversing the sign
of x p when it is computed.
Figure10.15 shows a plan view of the scenario depicted in Figs.10.14, and 10.16
a side view, which permits us to inspect the geometry and make the following obser-
vations:
xc
zc
= −x p
d
x p = −xc
zc/d
and
yc
zc
= yp
d
yp =
yc
zc/d .

10.6 Perspective Projection
209
This is expressed in matrix form as
⎡
⎢⎢⎣
x p
yp
z p
w
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1 0
0
0
0 1
0
0
0 0
1
0
0 0 1/d 0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
xc
yc
zc
1
⎤
⎥⎥⎦.
At ﬁrst, the transform seems strange, but if we multiply this out we get
[x p
yp
z p
w]T = [−xc
yc
zc
zc/d]T
and if we remember the idea behind homogeneous coordinates, we must divide the
terms x p, yp, z p by w to get the scaled terms, which produces
x p = −xc
zc/d
yp =
yc
zc/d
z p =
zc
zc/d = d
which, after all, is rather elegant. The value of d controls the size of the image, and
acts like a zoom control. Notice that this transform takes into account the sign change
that occurs with the x-coordinate. Some books will leave this sign reversal until the
mapping is made to the hardware display coordinates.
10.7 Worked Examples
10.7.1 2D Scale and Translate
T1 and T2 translate and scale a 2D point (x, y) to (x′, y′). Concatenate them in two
possible ways and show that the point (1, 1) is transformed to two different places.
T1 =
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
2 0 0
0 2 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
T2 =
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 2
0 1 2
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦

210
10
Geometric Matrix Transforms
T1T2 =
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
2 0 0
0 2 0
0 0 1
⎤
⎦
⎡
⎣
1 0 2
0 1 2
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
=
⎡
⎣
2 0 4
0 2 4
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
and the point (1, 1) is transformed to (6, 6).
T2T1 =
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
1 0 2
0 1 2
0 0 1
⎤
⎦
⎡
⎣
2 0 0
0 2 0
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
=
⎡
⎣
2 0 2
0 2 2
0 0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
and the point (1, 1) is transformed to (4, 4).
10.7.2 2D Rotation
Compute the coordinates of the unit square in Table10.1 after a rotation of 90◦.
The points are rotated as follows:
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎣
cos β −sin β 0
sin β
cos β 0
0
0
1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
=
⎡
⎣
0 −1 0
1
0 0
0
0 1
⎤
⎦
⎡
⎣
x
y
1
⎤
⎦
⎡
⎣
0
0
1
⎤
⎦=
⎡
⎣
0 −1 0
1
0 0
0
0 1
⎤
⎦
⎡
⎣
0
0
1
⎤
⎦
Table 10.1 Original and
rotated coordinates of the unit
square
x
y
x′
y′
0
0
0
0
1
0
0
1
1
1
−1
1
0
1
−1
0

10.7 Worked Examples
211
⎡
⎣
0
1
1
⎤
⎦=
⎡
⎣
0 −1 0
1
0 0
0
0 1
⎤
⎦
⎡
⎣
1
0
1
⎤
⎦
⎡
⎣
−1
1
1
⎤
⎦=
⎡
⎣
0 −1 0
1
0 0
0
0 1
⎤
⎦
⎡
⎣
1
1
1
⎤
⎦
⎡
⎣
−1
0
1
⎤
⎦=
⎡
⎣
0 −1 0
1
0 0
0
0 1
⎤
⎦
⎡
⎣
0
1
1
⎤
⎦.
10.7.3 Determinant of the Rotate Transform
Using determinants, show that the rotate transform preserves area.
The determinant of a 2D matrix transform reﬂects the area change produced by
the transform. Therefore, if area is preserved, the determinant must equal 1. Using
Sarrus’s rule:

⎡
⎣
cos β −sin β 0
sin β
cos β 0
0
0
1
⎤
⎦

= cos2 β + sin2 β = 1
which conﬁrms the role of the determinant.
10.7.4 Determinant of the Shear Transform
Using determinants, show that the shear transform preserves area.
The determinant of a 2D matrix transform reﬂects the area change produced by
the transform. Therefore, if area is preserved, the determinant must equal 1. Using
Sarrus’s rule:

⎡
⎣
1 tan β 0
0
1
0
0
0
1
⎤
⎦

= 1
which conﬁrms the role of the determinant.
10.7.5 Yaw, Pitch and Roll Transforms
Using the yaw and pitch transforms in the sequence yaw × pitch, compute how the
point (1, 1, 1) is transformed with yaw = pitch = 90◦.

212
10
Geometric Matrix Transforms
⎡
⎣
x′
y′
1
⎤
⎦=
⎡
⎢⎢⎣
cos yaw 0 sin yaw 0
0
1
0
0
−sin yaw 0 cos yaw 0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
0
0 cos pitch −sin pitch 0
0 sin pitch
cos pitch 0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
0 0 1 0
0 1 0 0
−1 0 0 0
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1 0
0 0
0 0 −1 0
0 1
0 0
0 0
0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x
y
z
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
−1
−1
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
0 1
0 0
0 0 −1 0
−1 0
0 0
0 0
0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦
therefore, (1, 1, 1) is transformed to (1, −1, −1).
10.7.6 Rotation About an Arbitrary Axis
Rotate the point (3, 0, 0), 180◦about the axis deﬁned by the vector n = i + j + k.
⎡
⎢⎢⎣
x′
p
y′
p
z′
p
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
a2K + cos α abK −c sin α acK + b sin α 0
abK + c sin α b2K + cos α bcK −a sin α 0
acK −b sin α bcK + a sin α c2K + cos α 0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x p
yp
z p
1
⎤
⎥⎥⎦
where
K = 1 −cos α.
Given α = 180◦, then K = 1 + 1 = 2, and ˆn =
1
√
3i +
1
√
3j +
1
√
3k. Therefore,
⎡
⎢⎢⎢⎣
x′
p
y′
p
z′
p
1
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
2
3 −1
2
3
2
3
0
2
3
2
3 −1
2
3
0
2
3
2
3
2
3 −1 0
0
0
0
1
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
3
0
0
1
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
−1
2
2
1
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
−1
3
2
3
2
3 0
2
3 −1
3
2
3 0
2
3
2
3 −1
3 0
0
0
0 1
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
3
0
0
1
⎤
⎥⎥⎥⎦
and the point (3, 0, 0) is rotated to (−1, 2, 2).

10.7 Worked Examples
213
10.7.7 3D Rotation Transform Matrix
Show that the matrix for rotating a point about an arbitrary axis corresponds to the
three matrices for rotating about the x-, y- and z-axis.
⎡
⎢⎢⎣
a2K + cos α abK −c sin α acK + b sin α 0
abK + c sin α b2K + cos α bcK −a sin α 0
acK −b sin α bcK + a sin α c2K + cos α 0
0
0
0
1
⎤
⎥⎥⎦
Pitch about the x-axis: ˆn = i, where a = 1 and b = c = 0; K = 1 −cos α.
pitch =
⎡
⎢⎢⎣
1
0
0
0
0 cos α −sin α 0
0 sin α
cos α 0
0
0
0
1
⎤
⎥⎥⎦
Yaw about the y-axis: ˆn = j, where b = 1 and a = c = 0; K = 1 −cos α.
yaw =
⎡
⎢⎢⎣
cos α 0 sin α 0
0
1
0
0
−sin α 0 cos α 0
0
0
0
1
⎤
⎥⎥⎦
Roll about the z-axis: ˆn = k, where c = 1 and a = b = 0; K = 1 −cos α.
roll =
⎡
⎢⎢⎣
cos α −sin α 0 0
sin α
cos α 0 0
0
0
1 0
0
0
0 1
⎤
⎥⎥⎦.
10.7.8 Perspective Projection
Compute the perspective coordinates of a 3D cube stored in Table10.2 with the
projection screen distance d = 20. Sketch the result.
Using the perspective transform:
⎡
⎢⎢⎣
x p
yp
z p
w
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1 0
0
0
0 1
0
0
0 0
1
0
0 0 1/d 0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
xc
yc
zc
1
⎤
⎥⎥⎦.

214
10
Geometric Matrix Transforms
Table 10.2 Coordinates of a 3D cube
Vertex
xc
yc
zc
x p
yp
1
0
0
10
0
0
2
10
0
10
20
0
3
10
10
10
20
20
4
0
10
10
0
20
5
0
0
20
0
0
6
10
0
20
10
0
7
10
10
20
10
10
8
0
10
20
0
10
Fig. 10.17 A perspective
sketch of a 3D cube
Yp
Xp
1, 5
2
3
4
6
7
8
20
20
10
10
the perspective coordinates are stored in Table10.2, and Fig.10.17 shows a sketch
of the result.

Chapter 11
Calculus: Derivatives
11.1 Introduction
Some quantities, such as the area of a circle or an ellipse, cannot be written precisely,
as they incorporate π, which is transcendental. However, an approximate value can be
obtained by devising a deﬁnition that includes a parameter that is made inﬁnitesimally
small. The techniques of limits and inﬁnitesimals have been used in mathematics for
over two-thousand years, and paved the way towards today’s calculus.
Although the principles of integral calculus were being used by Archimedes (287–
212 B.C.) to compute areas, volumes and centres of gravity, it was the English
astronomer, physicist and mathematician Isaac Newton (1643–1727) and Gottfried
Leibniz who are regarded as the true inventors of modern calculus. Leibniz published
his results in 1684, followed by Newton in 1704. However, Newton had been using his
calculus of ﬂuxions as early as 1665. Since then, calculus has evolved conceptually
and in notation.
Up until recently, calculus was described using inﬁnitesimals, which are num-
bers so small, they can be ignored in certain products. However, inﬁnitesimals, no
matter how small they are, do not belong to an axiomatic mathematical system,
and eventually the French mathematician Augustin-Louis Cauchy, and the German
mathematician Karl Weierstrass (1815–1897), showed how they could be replaced
by limits.
11.2 Small Numerical Quantities
The adjective small is a relative term, and requires clariﬁcation in the context of
numbers. For example, if numbers are in the hundreds, and also contain some decimal
component, then it seems reasonable to ignore digits after the 3rd decimal place for
any quick calculation. For instance,
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_11
215

216
11
Calculus: Derivatives
100.000003 × 200.000006 ≈20,000
and ignoring the decimal part has no signiﬁcant impact on the general accuracy of
the answer, which is measured in tens of thousands.
To develop an algebraic basis for this argument let’s divide a number into two
parts: a primary part x, and some very small secondary part δx (pronounced delta
x). In one of the above numbers, x = 100 and δx = 0.000003. Given two such
numbers, x1 and y1, their product is given by
x1 = x + δx
y1 = y + δy
x1y1 = (x + δx)(y + δy)
= xy + x · δy + y · δx + δx · δy.
Using x1 = 100.000003 and y1 = 200.000006 we have
x1y1 = 100 × 200 + 100 × 0.000006 + 200 × 0.000003 + 0.000003 × 0.000006
= 20,000 + 0.0006 + 0.0006 + 0.00000000018
= 20,000 + 0.0012 + 0.00000000018
= 20,000.00120000018
where it is clear that the products x · δy, y · δx and δx · δy contribute very little to
the result. Furthermore, the smaller we make δx and δy, their contribution becomes
even more insigniﬁcant. Just imagine if we reduce δx and δy to the level of quantum
phenomenon, e.g. 10−34, then their products play no part in every-day numbers.
But there is no need to stop there, we can make δx and δy as small as we like, e.g.
10−100,000,000,000. Later on we employ the device of reducing a number towards zero,
such that any products involving them can be dropped from any calculation.
Even though the product of two numbers close to zero is an even smaller number,
care must be taken with their quotients. For example, in the above scenario, where
δy = 0.000006 and δx = 0.000003,
δy
δx = 0.000006
0.000003 = 2
so we must watch out for such quotients.
Differentialcalculusisconcernedwiththerateatwhichafunctionchangesrelative
to one of its independent variables, and employs the ratio δy/δx to compute this
value. The limiting value of this ratio is called the function’s derivative, and we
will explore two ways of computing it, and provide a graphical interpretation of the
process. The ﬁrst method uses simple algebraic equations, and the second uses a
functional representation. Needless to say, they both give the same result.

11.3 Equations and Limits
217
11.3 Equations and Limits
11.3.1 Quadratic Function
Here is a simple algebraic approach using limits to compute the derivative of a
quadratic function. Starting with the function y = x2, let x change by δx, and let δy
be the corresponding change in y. We then have
y = x2
y + δy = (x + δx)2
= x2 + 2x · δx + (δx)2
δy = 2x · δx + (δx)2.
Dividing throughout by δx we have
δy
δx = 2x + δx.
The ratio δy/δx provides a measure of how fast y changes relative to x, in increments
of δx. For example, when x = 10
δy
δx = 20 + δx,
and if δx = 1, then δy/δx = 21. Equally, if δx = 0.001, then δy/δx = 20.001. By
making δx smaller and smaller, δy becomes equally smaller, and their ratio converges
towards a limiting value of 20.
In this case, as δx approaches zero, δy/δx approaches 2x, which is written
lim
δx→0
δy
δx = 2x.
Thus in the limit, when δx = 0, we create a condition where δy is divided by
zero—which is a meaningless operation. However, if we hold onto the idea of a
limit, as δx →0, it is obvious that the quotient δy
δx is converging towards 2x. The
subterfuge employed to avoid dividing by zero is to substitute dy
dx to stand for the
limiting condition:
dy
dx = lim
δx→0
δy
δx = 2x.
dy
dx (pronounced dee y dee x) is the derivative of y = x2, i.e. 2x. For instance, when
x = 0, dy
dx = 0, and when x = 3, dy
dx = 6. The derivative dy
dx , is the instantaneous
rate at which y changes relative to x.

218
11
Calculus: Derivatives
If we had represented this equation as a function:
f (x) = x2
f ′(x) = 2x
where f ′(x) is another way of writing dy
dx .
Now let’s introduce two constants into the original quadratic equation to see what
effect, if any, they have on the derivative. We begin with
y = ax2 + b
and increment x and y:
y + δy = a(x + δx)2 + b
= a

x2 + 2x · δx + (δx)2
+ b
δy = a

2x · δx + (δx)2
.
Dividing throughout by δx:
δy
δx = a(2x + δx)
and the derivative is
dy
dx = lim
δx→0
δy
δx = 2ax.
Thus the added constant b disappears (i.e. because it does not change), whilst the
multiplied constant a is transmitted through to the derivative.
11.3.2 Cubic Equation
Now let’s repeat the above analysis for y = x3:
y = x3
y + δy = (x + δx)3
= x3 + 3x2 · δx + 3x(δx)2 + (δx)3
δy = 3x2 · δx + 3x(δx)2 + (δx)3.

11.3 Equations and Limits
219
Dividing throughout by δx:
δy
δx = 3x2 + 3x · δx + (δx)2.
Using limits, we have
lim
δx→0
δy
δx = 3x2
or
dy
dx = lim
δx→0
δy
δx = 3x2.
We could also show that if y = ax3 + b then
dy
dx = 3ax2.
This incremental technique can be used to compute the derivative of all sorts of
functions.
Ifwecontinuecomputingthederivativesofhigher-orderpolynomials,wediscover
the following pattern:
y = x2,
dy
dx = 2x
y = x3,
dy
dx = 3x2
y = x4,
dy
dx = 4x3
y = x5,
dy
dx = 5x4.
Clearly, the rule is
y = xn,
dy
dx = nxn−1
but we need to prove why this is so. The solution is found in the binomial expansion
for (x + δx)n, which can be divided into three components:
1. Decreasing terms of x.
2. Increasing terms of δx.
3. The terms of Pascal’s triangle.
For example, the individual terms of (x + δx)4 are:

220
11
Calculus: Derivatives
Decreasing terms of x:
x4
x3
x2
x1
x0
Increasing terms of δx:
(δx)0
(δx)1
(δx)2
(δx)3
(δx)4
The terms of Pascal’s triangle:
1
4
6
4
1
which combined, produce
x4 + 4x3(δx) + 6x2(δx)2 + 4x(δx)3 + (δx)4.
Thus when we begin an incremental analysis:
y = x4
y + δy = (x + δx)4
= x4 + 4x3(δx) + 6x2(δx)2 + 4x(δx)3 + (δx)4
δy = 4x3(δx) + 6x2(δx)2 + 4x(δx)3 + (δx)4.
Dividing throughout by δx:
δy
δx = 4x3 + 6x2(δx)1 + 4x(δx)2 + (δx)3.
In the limit, as δx slides to zero, only the second term of the original binomial
expansion remains:
4x3.
The second term of the binomial expansion (1 + δx)n is always of the form
nxn−1
which is the proof we require.
11.3.3 Functions and Limits
In order to generalise the above ﬁndings, let’s approach the above analysis using
a function of the form y = f (x). We begin by noting some arbitrary value of its
independent variable and note the function’s value. In general terms, this is x and
f (x) respectively. We then increase x by a small amount δx, to give x + δx, and
measure the function’s value again: f (x + δx). The function’s change in value is
f (x + δx) −f (x), whilst the change in the independent variable is δx. The quotient
of these two quantities approximates to the function’s rate of change at x:

11.3 Equations and Limits
221
f (x + δx) −f (x)
δx
.
(11.1)
By making δx smaller and smaller towards zero, (11.1) converges towards a limiting
value expressed as
dy
dx = lim
δx→0
f (x + δx) −f (x)
δx
(11.2)
which can be used to compute all sorts of functions. For example, to compute the
derivative of sin x we proceed as follows:
y = sin x
y + δy = sin(x + δx).
Using the identity sin(A + B) = sin A cos B + cos A sin B, we have
y + δy = sin x cos(δx) + cos x sin(δx)
δy = sin x cos(δx) + cos x sin(δx) −sin x
= sin x(cos(δx) −1) + cos x sin(δx).
Dividing throughout by δx we have
δy
δx = sin x
δx
(cos(δx) −1) + sin(δx)
δx
cos x.
In the limit as δx →0, (cos(δx) −1) →0 and sin(δx)/δx = 1, (See Appendix A)
and
dy
dx = d(sin x)
dx
= cos x.
Before moving on, let’s compute the derivative of cos x.
y = cos x
y + δy = cos(x + δx).
Using the identity cos(A + B) = cos A cos B −sin A sin B, we have
y + δy = cos x cos(δx) −sin x sin(δx)
δy = cos x cos(δx) −sin x sin(δx) −cos x
= cos x(cos(δx) −1) −sin x sin(δx).
Dividing throughout by δx we have

222
11
Calculus: Derivatives
Fig. 11.1 Sketch of
f (x) = x2
-1
0
1
2
-1
1
2
3
4
P
R
Q
δy
δx = cos x
δx
(cos(δx) −1) −sin(δx)
δx
sin x.
In the limit as δx →0, (cos(δx) −1) →0 and sin(δx)/δx = 1 (See Appendix A),
and
dy
dx = d(cos x)
dx
= −sin x.
We will continue to employ this strategy to compute the derivatives of other functions
later on.
11.3.4 Graphical Interpretation of the Derivative
To illustrate this limiting process graphically, consider the scenario in Fig.11.1 where
the sample point is P. In this case the function is f (x) = x2 and P’s coordinates
are (x, x2). We identify another point R, displaced δx to the right of P, with coor-
dinates (x + δx, x2). The point Q on the curve, vertically above R, has coordinates

x + δx, (x + δx)2
. When δx is relatively small, the slope of the line P Q approx-
imates to the function’s rate of change at P, which is the graph’s slope. This is
given by
slope = QR
P R = (x + δx)2 −x2
δx
= x2 + 2x(δx) + (δx)2 −x2
δx
= 2x(δx) + (δx)2
δx
= 2x + δx.

11.3 Equations and Limits
223
We can now reason that as δx is made smaller and smaller, Q approaches P, and
slope becomes the graph’s slope at P. This is the limiting condition:
dy
dx = lim
δx→0 (2x + δx) = 2x.
Thus, for any point with coordinates (x, x2), the slope is given by 2x. For example,
when x = 0, the slope is 0, and when x = 4, the slope is 8, etc.
11.3.5 Derivatives and Differentials
Given a function f (x), d f
dx represents the instantaneous change of f for some x, and
is called the ﬁrst derivative of f (x). For linear functions, this is constant, for other
functions, the derivative’s value changes with x and is represented by a function.
The elements d f , dy and dx are called differentials, and historically, the derivative
used to be called the differential coefﬁcient, but has now been dropped in favour of
derivative. One can see how the idea of a differential coefﬁcient arose if we write,
for example:
dy
dx = 3x
as
dy = 3x dx.
In this case, 3x acts like a coefﬁcient of dx. However, today, the derivative is regarded
as an operator, and even though it is written
dy
dx
it really means
d
dx [y]
where the operator d
dx acts on y.

224
11
Calculus: Derivatives
11.3.6 Integration and Antiderivatives
If it is possible to differentiate a function, it seems reasonable to assume the exis-
tence of an inverse operator which turns the derivative back to the original function.
Fortunately, this is the case, but there are some limitations. This inverse process is
called integration and reveals the antiderivative of a function. Many functions can
be paired together in the form of a derivative and an antiderivative, such as 2x with
x2, and cos x with sin x. However, there are many functions where it is impossible
to derive its antiderivative in a precise form. For example, there is no simple, ﬁnite
functional antiderivative for sin x2 or (sin x)/x. To understand integration, let’s begin
with a simple derivative.
If we are given
dy
dx = 18x2 −8x + 8
it is not too difﬁcult to reason that the original function could have been
y = 6x3 −4x2 + 8x.
However, it could have also been
y = 6x3 −4x2 + 8x + 2
or
y = 6x3 −4x2 + 8x + 20
or with any other constant. Consequently, the integration process has to include an
arbitrary constant:
y = 6x3 −4x2 + 8x + C.
The value of C is not always required, but it can be determined if we are given some
extra information, such as y = 10 when x = 0, then C = 10.
Given a function
y = 6x3 −4x2 + 8x + 10
its derivative is written
d
dx [y] = 18x2 −8x + 8.
The antiderivative of 18x2 −8x + 8 reveals the original function, and is written

11.3 Equations and Limits
225
y =

(18x2 −8x + 8) dx
although brackets are not always used:
y =

18x2 −8x + 8 dx.
This equation reads: “y equals the integral of 18x2 −8x +8 dee x.” The dx reminds
us that x is the independent variable. In this case we can write the answer:
y =

18x2 −8x + 8 dx
= 6x3 −4x2 + 8x + C
where C is some constant.
The antiderivatives for the sine and cosine functions are written:

sin x dx = −cos x + C

cos x dx = sin x + C
which you may think obvious, as we have just computed their derivatives. How-
ever, the reason for introducing integration alongside differentiation, is to make you
familiar with the notation, and memorise the two distinct processes, as well as lay
the foundations for the next chapter.
11.4 Function Types
Mathematical functions come in all sorts of shapes and sizes. Sometimes they are
described explicitly where y equals some function of its independent variable(s),
such as
y = x sin x
or implicitly where y, and its independent variable(s) are part of an equation, such as
x2 + y2 = 10.
A function may reference other functions, such as
y = sin(cos2 x)

226
11
Calculus: Derivatives
or
y = xsin x.
There is no limit to the way functions can be combined, which makes it impossible
to cover every eventuality. Nevertheless, we will explore some useful combinations
that prepare us for any future surprises.
First, we examine how to differentiate different types of functions, that include
sums, products and quotients, which are employed later on to differentiate spe-
ciﬁc functions such as trigonometric, logarithmic and hyperbolic. Where relevant, I
include the appropriate antiderivative to complement its derivative.
11.5 Differentiating Groups of Functions
So far, we have only considered simple individual functions, which unfortunately, do
not represent the equations found in mathematics, science, physics or even computer
science. In general, the functions we have to differentiate include sums of functions,
functions of functions, function products and function quotients. Let’s explore these
four scenarios.
11.5.1 Sums of Functions
A function normally computes a numerical value from its independent variable(s),
and if it can be differentiated, its derivative generates another function with the same
independent variable. Consequently, if a function contains two functions of x, such
as u and v, where
y = u(x) + v(x)
which can be abbreviated to
y = u + v
then
dy
dx = du
dx + dv
dx

11.5 Differentiating Groups of Functions
227
Fig. 11.2 Graph of
y = 2x6 + sin x + cos x and
its derivative,
dy
dx = 12x5 + cos x −sin x
(dashed)
-1
0
1
-3
-2
-1
1
2
3
where we just sum their individual derivatives. For example: ﬁnd dy
dx , given
u = 2x6
v = 3x5
y = u + v
y = 2x6 + 3x5.
Differentiating y:
dy
dx = 12x5 + 15x4.
Figure11.2 shows a graph of y = 2x6 + sin x + cos x and its derivative, dy
dx =
12x5 + cos x −sin x. Differentiating such functions is relatively easy, so too, is
integrating. Given
dy
dx = du
dx + dv
dx
then
y =

u dx +

v dx
=

(u + v) dx.
For example, given
dy
dx = 12x5 + cos x −sin x

228
11
Calculus: Derivatives
we ﬁnd y by integrating:
y =

12x5 dx +

cos x dx −

sin x dx
= 2x6 + sin x + cos x + C.
11.5.2 Function of a Function
One of the advantages of modern mathematical notation is that it lends itself to
unlimited elaboration without introducing any new symbols. For example, the poly-
nomial 3x2+2x is easily raised to some power by adding brackets and an appropriate
index: (3x2 + 2x)2. Such an object is a function of a function, because the function
3x2 + 2x is subjected to a further squaring function. The question now is: how are
such functions differentiated? Well, the answer is relatively easy, but does introduce
some new ideas.
Imagine that person A swims twice as fast as person B, who in turn, swims three
times as fast as person C. It should be obvious that person A swims six (2 × 3)
times faster than person C. This product rule, also applies to derivatives, because if
y changes twice as fast as u, i.e. dy
du = 2, and u changes three times as fast as x, i.e.
du
dx = 3, then y changes six times as fast as x:
dy
dx = dy
du · du
dx .
To differentiate
y = (3x2 + 2x)2
we substitute
u = 3x2 + 2x
then
y = u2
and
dy
du = 2u
= 2(3x2 + 2x)
= 6x2 + 4x.

11.5 Differentiating Groups of Functions
229
Next, we require du
dx :
u = 3x2 + 2x
du
dx = 6x + 2
therefore, we can write
dy
dx = dy
du · du
dx
= (6x2 + 4x)(6x + 2)
= 36x3 + 36x2 + 8x.
This result is easily veriﬁed by expanding the original polynomial and differentiating:
y = (3x2 + 2x)2
= (3x2 + 2x)(3x2 + 2x)
= 9x4 + 12x3 + 4x2
dy
dx = 36x3 + 36x2 + 8x.
Figure11.3 shows a graph of y = (3x2 + 2x)2 and its derivative, dy
dx = 36x3 +
36x2 + 8x.
y = sin ax is a function of a function, and is differentiated as follows:
y = sin ax.
Fig. 11.3 Graph of
y = (3x2 + 2x)2 and its
derivative,
dy
dx = 36x3 + 36x2 + 8x
(dashed)
-1
0
1
-2
2

230
11
Calculus: Derivatives
Substitute u for ax:
y = sin u
dy
du = cos u
= cos ax.
Next, we require du
dx :
u = ax
du
dx = a
therefore, we can write
dy
dx = dy
du · du
dx
= cos ax · a
= a cos ax.
Consequently, given
dy
dx = cos ax
then
y =

cos ax dx
= 1
a sin ax + C.
Similarly, given
dy
dx = sin ax
then
y =

sin ax dx
= −1
a cos ax + C.

11.5 Differentiating Groups of Functions
231
The equation y = sin x2 is also a function of a function, and is differentiated as
follows:
y = sin x2.
Substitute u for x2:
y = sin u
dy
du = cos u
= cos x2.
Next, we require du
dx :
u = x2
du
dx = 2x
therefore, we can write
dy
dx = dy
du · du
dx
= cos x2 · 2x
= 2x cos x2.
Figure11.4 shows a graph of y = sin x2 and its derivative, dy
dx = 2x cos x2. In
general, there can be any depth of functions within a function, which permits us to
write the chain rule for derivatives:
dy
dx = dy
du · du
dv · dv
dw · dw
dx
Fig. 11.4 Graph of
y = sin x2 and its derivative,
dy
dx = 2x cos x2 (dashed)
-4
-3
-2
-1
0
1
2
3
4
-8
-4
4
8

232
11
Calculus: Derivatives
11.5.3 Function Products
Function products occur frequently in every-day mathematics, and involve the prod-
uct of two, or more functions. Here are three simple examples:
y = (3x2 + 2x)(2x2 + 3x)
y = sin x cos x
y = x2 sin x.
When it comes to differentiating function products of the form
y = uv
it seems natural to assume that
dy
dx = du
dx · dv
dx
(11.3)
which unfortunately, is incorrect. For example, in the case of
y = (3x2 + 2x)(2x2 + 3x)
differentiating using (11.3) produces
dy
dx = (6x + 2)(4x + 3)
= 24x2 + 26x + 6.
However, if we expand the original product and then differentiate, we obtain
y = (3x2 + 2x)(2x2 + 3x)
= 6x4 + 13x3 + 6x2
dy
dx = 24x3 + 39x2 + 12x
which is correct, but differs from the ﬁrst result. Obviously, (11.3) must be wrong.
So let’s return to ﬁrst principles and discover the correct rule.
So far, we have incremented the independent variable—normally x—by δx to
discover the change in y—normally δy. Next, we see how the same notation can be
used to increment functions.
Given the following functions of x, u and v, where
y = uv

11.5 Differentiating Groups of Functions
233
if x increases by δx, then there will be corresponding changes of δu, δv and δy, in
u, v and y respectively. Therefore,
y + δy = (u + δu)(v + δv)
= uv + uδv + vδu + δuδv
δy = uδv + vδu + δuδv.
Dividing throughout by δx we have
δy
δx = u δv
δx + vδu
δx + δu δv
δx .
In the limiting condition:
dy
dx = lim
δx→0

u δv
δx

+ lim
δx→0

vδu
δx

+ lim
δx→0

δu δv
δx

.
As δx →0, then δu →0 and

δu δv
δx

→0. Therefore,
dy
dx = u dv
dx + vdu
dx .
(11.4)
Using (11.4) for the original function product:
u = 3x2 + 2x
v = 2x2 + 3x
y = uv
du
dx = 6x + 2
dv
dx = 4x + 3
dy
dx = u dv
dx + vdu
dx
= (3x2 + 2x)(4x + 3) + (2x2 + 3x)(6x + 2)
= (12x3 + 17x2 + 6x) + (12x3 + 22x2 + 6x)
= 24x3 + 39x2 + 12x
which agrees with our previous prediction. Figure11.5 shows the graph of y =
(3x2 + 2x)(2x2 + 3x) and its derivative, dy
dx = 24x3 + 39x2 + 12x.

234
11
Calculus: Derivatives
Fig. 11.5 Graph of
y = (3x2 + 2x)(2x2 + 3x)
and its derivative,
dy
dx = 24x3 + 39x2 + 12x
(dashed)
-2
-1
0
1
-1
1
2
3
The equation y = sin x cos x contains the product of two functions and is differ-
entiated using (11.4) as follows:
y = sin x cos x
u = sin x
du
dx = cos x
v = cos x
dv
dx = −sin x
dy
dx = u dv
dx + vdu
dx
= sin x(−sin x) + cos x cos x
= cos2 x −sin2 x
= cos 2x.
Using the identity sin 2x = 2 sin x cos x, we can rewrite the original function as
y = sin x cos x
dy
dx = 1
2 sin 2x
= cos 2x
which conﬁrms the above derivative. Now let’s consider the antiderivative of cos 2x.
Given
dy
dx = cos 2x

11.5 Differentiating Groups of Functions
235
Fig. 11.6 Graph of
y = sin x cos x and its
derivative, dy
dx = cos 2x
(dashed)
-2
-
0
2
-1
1
then
y =

cos 2x dx
= 1
2 sin 2x + C
= sin x cos x + C.
Figure11.6 shows the graph of y = sin x cos and its derivative, dy
dx = cos 2x.
11.5.4 Function Quotients
Next, we investigate how to differentiate the quotient of two functions. We begin
with two functions of x, u and v, where
y = u
v
which makes y also a function of x.
We now increment x by δx and measure the change in u as δu, and the change in
v as δv. Consequently, the change in y is δy:
y + δy = u + δu
v + δv
δy = u + δu
v + δv −u
v
= v(u + δu) −u(v + δv)
v(v + δv)
= vδu −uδv
v(v + δv) .

236
11
Calculus: Derivatives
Dividing throughout by δx we have
δy
δx =
vδu
δx −u δv
δx
v(v + δv) .
As δx →0, δu, δv and δy also tend towards zero, and the limiting conditions are
dy
dx = lim
δx→0
δy
δx
vdu
dx = lim
δx→0 vδu
δx
u dv
dx = lim
δx→0 u δv
δx
v2 = lim
δx→0 v(v + δv)
therefore,
dy
dx =
vdu
dx −u dv
dx
v2
.
As an example, let’s differentiate
y = x3 + 2x2 + 3x + 6
x2 + 3
.
Substitute u = x3 + 2x2 + 3x + 6 and v = x2 + 3, then
du
dx = 3x2 + 4x + 3
dv
dx = 2x
dy
dx = (x2 + 3)(3x2 + 4x + 3) −(x3 + 2x2 + 3x + 6)(2x)
(x2 + 3)2
= (3x4 + 4x3 + 3x2 + 9x2 + 12x + 9) −(2x4 + 4x3 + 6x2 + 12x)
x4 + 6x2 + 9
= x4 + 6x2 + 9
x4 + 6x2 + 9
= 1

11.5 Differentiating Groups of Functions
237
Fig. 11.7 Graph of
y = (x2 +3)(x +2)/(x2 +3)
and its derivative, dy
dx = 1
(dashed)
-4
-3
-2
-1
0
1
2
3
4
-2
-1
1
2
which is not a surprising result when one sees that the original function has the factors
y = (x2 + 3)(x + 2)
x2 + 3
= x + 2
whose derivative is 1. Figure11.7 shows a graph of y = (x2 + 3)(x + 2)/(x2 + 3)
and its derivative, dy
dx = 1.
11.6 Differentiating Implicit Functions
Simple functions conveniently fall into two types: explicit and implicit. An explicit
function, describes a function in terms of its independent variable(s), such as
y = a sin x + b cos x
where the value of y is determined by the values of a, b and x. On the other hand,
an implicit function, such as
x2 + y2 = 25
combines the function’s name with its deﬁnition. In this case, it is easy to untangle
the explicit form:
y =
	
25 −x2.
So far, we have only considered differentiating explicit functions, so now let’s exam-
ine how to differentiate implicit functions. Let’s begin with a simple explicit function
and differentiate it as it is converted into its implicit form.

238
11
Calculus: Derivatives
Let
y = 2x2 + 3x + 4
then
dy
dx = 4x + 3.
Now let’s start the conversion into the implicit form by bringing the constant 4 over
to the left-hand side:
y −4 = 2x2 + 3x
differentiating both sides:
dy
dx = 4x + 3.
Bringing 4 and 3x across to the left-hand side:
y −3x −4 = 2x2
differentiating both sides:
dy
dx −3 = 4x
dy
dx = 4x + 3.
Finally, we have
y −2x2 −3x −4 = 0
differentiating both sides:
dy
dx −4x −3 = 0
dy
dx = 4x + 3
which seems straight forward. The reason for working through this example is to
remind us that when y is differentiated we get dy
dx . Let’s differentiate these two
examples:
y + sin x + 4x = 0 and
y + x2 −cos x = 0.

11.6 Differentiating Implicit Functions
239
Differentiating the individual terms:
y + sin x + 4x = 0
dy
dx + cos x + 4 = 0
dy
dx = −cos x −4.
y + x2 −cos x = 0
dy
dx + 2x + sin x = 0
dy
dx = −2x −sin x.
Buthowdowedifferentiate y2+x2 = r2?Well,theimportantdifferencebetweenthis
implicit function and previous functions, is that it involves a function of a function.
y is not only a function of x, but is squared, which means that we must employ the
chain rule described earlier:
dy
dx = dy
du · du
dx .
Therefore, given
y2 + x2 = r2
2y dy
dx + 2x = 0
dy
dx = −2x
2y
=
−x
√
r2 −x2 .
This is readily conﬁrmed by expressing the original function in its explicit form and
differentiating:
y = (r2 −x2)
1
2
which is a function of a function.
Let u = r2 −x2, then
du
dx = −2x.

240
11
Calculus: Derivatives
As y = u
1
2 , then
dy
du = 1
2u−1
2
=
1
2u
1
2
=
1
2
√
r2 −x2 .
However,
dy
dx = dy
du · du
dx
=
−2x
2
√
r2 −x2
=
−x
√
r2 −x2
which agrees with the implicit differentiated form.
11.7 Differentiating Exponential and Logarithmic Functions
11.7.1 Exponential Functions
Exponential functions have the form y = ax, where the independent variable is the
exponent. Such functions are used to describe various forms of growth or decay, from
the compound interest law, to the rate at which a cup of tea cools down. One special
value of a is 2.718 282.., called e, where
e = lim
n→∞

1 + 1
n
n
.
Raising e to the power x:
ex = lim
n→∞

1 + 1
n
nx
which, using the Binomial Theorem, is
ex = 1 + x + x2
2! + x3
3! + x4
4! + · · ·

11.7 Differentiating Exponential and Logarithmic Functions
241
Fig. 11.8 Graphs of y = ex
and y = e−x
-4
-3
-2
-1
0
1
2
3
4
1
2
3
4
y = ex
y = e-x
If we let
y = ex
dy
dx = 1 + x + x2
2! + x3
3! + x4
4! + · · ·
= ex.
which is itself. Figure11.8 shows graphs of y = ex and y = e−x.
Now let’s differentiate y = ax. We know from the rules of logarithms that
log xn = n log x
therefore, given
y = ax
then
ln y = ln ax = x ln a
therefore
y = ex ln a
which means that
ax = ex ln a.

242
11
Calculus: Derivatives
Consequently,
d
dx [ax] = d
dx [ex ln a]
= ln a ex ln a
= ln a ax.
Similarly, it can be shown that
y = e−x,
dy
dx = −e−x
y = eax,
dy
dx = aeax
y = e−ax, dy
dx = −ae−ax
y = ax,
dy
dx = ln a ax
y = a−x,
dy
dx = −ln a a−x.
The exponential antiderivatives are written:

ex dx = ex + C

e−x dx = −e−x + C

eax dx = 1
a eax + C

e−ax dx = −1
a eax + C

ax dx =
1
ln a ax + C

a−x dx = −1
ln a a−x + C.
11.7.2 Logarithmic Functions
Given a function of the form
y = ln x
then
x = ey.
Therefore,

11.7 Differentiating Exponential and Logarithmic Functions
243
Fig. 11.9 Graph of y = ln x
and its derivative, dy
dx = 1
x
(dashed)
-1
0
1
2
3
4
5
6
-2
-1
1
2
dx
dy = ey
= x
dy
dx = 1
x .
Thus
d
dx [ln x] = 1
x .
Figure11.9 shows the graph of y = ln x and its derivative, dy
dx = 1
x . Conversely,
 1
x dx = ln |x| + C.
When differentiating logarithms to a base a, we employ the conversion formula:
y = loga x
= (ln x)(loga e)
whose derivative is
dy
dx = 1
x loga e.
When a = 10, then log10 e = 0.4343 . . . and
d
dx [log10 x] = 0.4343
x
Figure11.10 shows the graph of y = log10 x and its derivative, dy
dx = 0.4343
x
.

244
11
Calculus: Derivatives
Fig. 11.10 Graph of
y = log10 x and its
derivative, dy
dx = 0.4343
x
(dashed)
-1
0
1
2
3
4
5
6
-2
-1
1
2
11.8 Differentiating Trigonometric Functions
We have only differentiated two trigonometric functions: sin x and cos x, so let’s add
tan x, csc x, sec x and cot x to the list, as well as their inverse forms.
11.8.1 Differentiating Tan
Rather than return to ﬁrst principles and start incrementing x by δx, we can employ
therulesfordifferentiatingdifferentfunctioncombinationsandvarioustrigonometric
identities. In the case of tan ax, this can be written as
tan ax = sin ax
cos ax
and employ the quotient rule:
dy
dx =
vdu
dx −u dv
dx
v2
.
Therefore, let u = sin ax and v = cos ax, and
d
dx [tan ax] = a cos ax cos ax + a sin ax sin ax
cos2 ax
= a(cos2 ax + sin2 ax)
cos2 ax
=
a
cos2 ax
= a sec2 ax
= a(1 + tan2 ax).

11.8 Differentiating Trigonometric Functions
245
-
0
-2
-1
1
2
3
Fig. 11.11 Graph of y = tan x and its derivative, dy
dx = 1 + tan2 x (dashed)
Figure11.11 shows a graph of y = tan x and its derivative, dy
dx = 1 + tan2 x.
It follows that

sec2 ax dx = 1
a tan ax + C.
11.8.2 Differentiating Csc
Using the quotient rule:
y = csc ax
=
1
sin ax
d
dx [csc ax] = 0 −a cos ax
sin2 ax
= −a cos ax
sin2 ax
= −
a
sin ax · cos ax
sin ax
= −a csc ax · cot ax.
Figure11.12 shows a graph of y = csc x and its derivative, dy
dx = −csc x cot x.
It follows that

csc ax · cot ax dx = −1
a csc ax + C.

246
11
Calculus: Derivatives
Fig. 11.12 Graph of
y = csc x and its derivative,
dy
dx = −csc x cot x (dashed)
-2
2
0
-4
-2
2
4
11.8.3 Differentiating Sec
Using the quotient rule:
y = sec ax
=
1
cos ax
d
dx [sec ax] = −(−a sin ax)
cos2 ax
= a sin ax
cos2 ax
=
a
cos ax · sin ax
cos ax
= a sec ax · tan ax.
Figure11.13 shows a graph of y = sec x and its derivative, dy
dx = sec x tan x.
It follows that
Fig. 11.13 Graph of
y = sec x and its derivative,
dy
dx = sec x tan x (dashed)
-2
0
2
-4
-2
2
4

11.8 Differentiating Trigonometric Functions
247

sec ax · tan ax dx = 1
a sec ax + C.
11.8.4 Differentiating Cot
Using the quotient rule:
y = cot ax
=
1
tan ax
d
dx [cot ax] = −a sec2 ax
tan2 ax
= −
a
cos2 ax · cos2 ax
sin2 ax
= −
a
sin2 ax
= −a csc2 ax
= −a(1 + cot2 ax).
Figure11.14 shows a graph of y = cot x and its derivative, dy
dx = −(1 + cot2 x).
It follows that

1 + cot2 ax dx = −1
a cot at + C.
Fig. 11.14 Graph of
y = cot x and its derivative,
dy
dx = −(1+cot2 x) (dashed)
-2
-
0
2
-4
-2
2
4

248
11
Calculus: Derivatives
11.8.5 Differentiating Arcsin, Arccos and Arctan
These inverse functions are solved using a clever strategy.
Let
x = sin y
then
y = arcsin x.
Differentiating the ﬁrst expression, we have
dx
dy = cos y
dy
dx =
1
cos y
and as sin2 y + cos2 y = 1, then
cos y =

1 −sin2 y =
	
1 −x2
and
d
dx [arcsin x] =
1
√
1 −x2 .
Using a similar technique, it can be shown that
d
dx [arccos x] = −
1
√
1 −x2
d
dx [arctan x] =
1
1 + x2 .
It follows that

dx
√
1 −x2 = arcsin x + C

−dx
√
1 −x2 = arccos x + C

dx
1 + x2 = arctan x + C.

11.8 Differentiating Trigonometric Functions
249
11.8.6 Differentiating Arccsc, Arcsec and Arccot
Let
y = arccscx
then
x = csc y
=
1
sin y
dx
dy = −cos y
sin2 y
dy
dx = −sin2 y
cos y
= −1
x2
x
√
x2 −1
d
dx [arccscx] = −
1
x
√
x2 −1
.
Similarly,
d
dx [arcsec x] =
1
x
√
x2 −1
d
dx [arccotx] = −
1
x2 + 1.
It follows:

dx
x
√
x2 −1
= arcsec |x| + C

dx
x2 + 1 = −arccotx + C.
11.9 Differentiating Hyperbolic Functions
Trigonometric functions are useful for parametric, circular motion, whereas, hyper-
bolic functions arise in equations for the absorption of light, mechanics and in integral
calculus. Figure11.15 shows graphs of the unit circle and a hyperbola whose respec-
tive equations are

250
11
Calculus: Derivatives
Fig. 11.15 Graphs of the
unit circle x2 + y2 = 1 and
the hyperbola x2 −y2 = 1
-4
-3
-2
-1
0
1
2
3
4
-2
-1
1
2
P
Q
x2 + y2 = 1
x2 −y2 = 1
where the only difference between them is a sign. The parametric form for the
trigonometric, or circular functions and the hyperbolic functions are respectively:
sin2 θ + cos2 θ = 1
cosh2 x −sinh2 x = 1.
The three hyperbolic functions have the following deﬁnitions:
sinh x = ex −e−x
2
cosh x = ex + e−x
2
tanh x = sinh x
cosh x = e2x −1
e2x + 1
and their reciprocals are:
cosechx =
1
sinh x =
2
ex −e−x
sech x =
1
cosh x =
2
ex + e−x
coth x =
1
tanh x = e2x + 1
e2x −1.
Other useful identities include:

11.9 Differentiating Hyperbolic Functions
251
Table 11.1 Hyperbolic function names
Function
Reciprocal
Inverse function
Inverse reciprocal
sinh
cosech
arsinh
arcsch
cosh
sech
arcosh
arsech
tanh
coth
artanh
arcoth
sech 2x = 1 −tanh2 x
cosech2x = coth2 x −1.
The coordinates of P and Q in Fig.11.15 are given by P(cos θ,
sin θ) and
Q(cosh x, sinh x).
Table11.1 shows the names of the three hyperbolic functions, their reciprocals
and inverse forms. As these functions are based upon ex and e−x, they are relatively
easy to differentiate.
11.9.1 Differentiating Sinh, Cosh and Tanh
Table11.2 gives the rules for differentiating hyperbolic functions, and Table11.3 for
inverse hyperbolic functions.
Table 11.2 Rules for
differentiating hyperbolic
functions
y
dy/dx
sinh x
cosh x
cosh x
sinh x
tanh x
sech2 x
cosechx
−cosechx coth x
sech x
−sech x tanh x
coth x
−cosech2x
Table 11.3 Rules for
differentiating inverse
hyperbolic functions
y
dy/dx
arsinh x
1
√
1 + x2
arcosh x
1
√
x2 −1
artanh x
1
1 −x2
arcsch x
−
1
x
√
1 + x2
arsech x
−
1
x
√
1 −x2
arcoth x
−
1
x2 −1

252
11
Calculus: Derivatives
Table 11.4 Rules for
integrating hyperbolic
functions
f (x)

f (x) dx
sinh x
cosh x + C
cosh x
sinh x + C
sech2 x
tanh x + C
Table 11.5 Rules for
integrating inverse hyperbolic
functions
f (x)

f (x) dx
1
√
1 + x2
arsinh x + C
1
√
x2 −1
arcosh x + C
1
1 −x2
artanh x + C
Table11.4 gives the rules for integrating hyperbolic functions, and Table11.5 for
inverse hyperbolic functions.
11.10 Higher Derivatives
Therearethreepartstothissection:Theﬁrstpartshowswhathappenswhenafunction
is repeatedly differentiated; the second shows how these higher derivatives resolve
local minimum and maximum conditions; and the third part provides a physical
interpretation for these derivatives. Let’s begin by ﬁnding the higher derivatives of
simple polynomials.
11.11 Higher Derivatives of a Polynomial
We have previously seen that polynomials of the form
y = axr + bxs + cxt . . .
are differentiated as follows:
dy
dx = raxr−1 + sbxs−1 + tcxt−1 . . .
For example, given
y = 3x3 + 2x2 −5x

11.11 Higher Derivatives of a Polynomial
253
Fig. 11.16 Graph of
y = 3x3 + 2x2 −5x and its
derivative
dy
dx = 9x2 + 4x −5 (dashed)
-2
-1
0
1
2
-4
-2
2
4
then
dy
dx = 9x2 + 4x −5
which describes how fast y changes relative to x.
Figure11.16 shows the graph of y = 3x3 + 2x2 −5x and its derivative dy
dx =
9x2 + 4x −5, and we can see that when x = −1 there is a local maximum, where
the function reaches a value of 4, then begins a downward journey to 0, where the
slope is −5. Similarly, when x ≃0.55, there is a point where the function reaches
a local minimum with a value of approximately −1.65. The slope is zero at both
points, which is reﬂected in the graph of the derivative.
Having differentiated the function once, there is nothing to prevent us differen-
tiating a second time, but ﬁrst we require a way to annotate the process, which is
performed as follows. At a general level, let y be some function of x, then the ﬁrst
derivative is
d
dx [y].
The second derivative is found by differentiating the ﬁrst derivative:
d
dx
 d
dx [y]

and is written:
d2
dx2 [y] or
d2y
dx2

254
11
Calculus: Derivatives
Similarly, the third derivative is
d3y
dx3
and the nth derivative:
dny
dxn .
When a function is expressed as f (x), its derivative is written f ′(x). The second
derivative is written f ′′(x), and so on for higher derivatives.
Returning to the original function, the ﬁrst and second derivatives are
dy
dx = 9x2 + 4x −5
d2y
dx2 = 18x + 4
and the third and fourth derivatives are
d3y
dx3 = 18
d4y
dx4 = 0.
Figure11.17 shows the original function and the ﬁrst two derivatives. The graph of
the ﬁrst derivative shows the slope of the original function, whereas the graph of
the second derivative shows the slope of the ﬁrst derivative. These graphs help us
identify a local maximum and minimum. By inspection of Fig.11.17, when the ﬁrst
derivative equals zero, there is a local maximum or a local minimum. Algebraically,
this is when
Fig. 11.17 Graph of
y = 3x3 + 2x2 −5x, its ﬁrst
derivative
dy
dx = 9x2 + 4x −5 (short
dashes) and its second
derivative d2y
dx2 = 18x + 4
(long dashes)
-2
-1
0
1
2
-4
-2
2
4

11.11 Higher Derivatives of a Polynomial
255
dy
dx = 0
9x2 + 4x −5 = 0.
Solving this quadratic in x we have
x = −b ±
√
b2 −4ac
2a
where a = 9, b = 4, c = −5:
x = −4 ± √16 + 180
18
x1 = −1,
x2 = 0.555
which conﬁrms our earlier analysis. However, what we don’t know, without referring
to the graphs, whether it is a minimum, or a maximum.
11.12 Identifying a Local Maximum or Minimum
Figure11.18 shows a function containing a local maximum of 5 when x = −1. Note
that as the independent variable x, increases from −2 towards 0, the slope of the
graph changes from positive to negative, passing through zero at x = −1. This is
shown in the function’s ﬁrst derivative, which is the straight line passing through the
points (−2, 6), (−1, 0) and (0, −6). A natural consequence of these conditions
implies that the slope of the ﬁrst derivative must be negative:
d2y
dx2 = −ve.
Fig. 11.18 A function
containing a local maximum,
and its ﬁrst derivative
(dashed)
-2
-1
0
1
-6
-4
-2
2
4
6

256
11
Calculus: Derivatives
Fig. 11.19 A function
containing a local minimum,
and its ﬁrst derivative
(dashed)
-3
-2
-1
0
1
-6
-4
-2
2
4
6
Figure11.19 shows another function containing a local minimum of 5 when x =
−1. Note that as the independent variable x, increases from −2 towards 0, the slope
of the graph changes from negative to positive, passing through zero at x = −1.
This is shown in the function’s ﬁrst derivative, which is the straight line passing
through the points (−2, −6), (−1, 0) and (0, 6). A natural consequence of these
conditions implies that the slope of the ﬁrst derivative must be positive:
d2y
dx2 = +ve.
We can now apply this observation to the original function y = 3x3 + 2x2 −5x
for the two values of x, x1 = −1,
x2 = 0.555:
y = 3x3 + 2x2 −5x
dy
dx = 9x2 + 4x −5
d2y
dx2 = 18x + 4
= 18 × (−1) = −18
= 18 × (0.555) = +10.
Which conﬁrms that when x = −1 there is a local maximum, and when x = 0.555,
there is a local minimum, as shown in Fig.11.16.
11.13 Partial Derivatives
Up to this point, we have used functions with one independent variable, such as
y = f (x). However, we must be able to compute derivatives of functions with more
than one independent variable, such as y = f (u, v, w). The technique employed is to

11.13 Partial Derivatives
257
assume that only one variable changes, whilst the other variables are held constant.
This means that a function can possess several derivatives—one for each independent
variable. Such derivatives are called partial derivatives and employ a new symbol ∂,
which can be read as “partial dee”.
Given a function f (u, v, w), the three partial derivatives are deﬁned as
∂f
∂u = lim
h→0
f (u + h, v, w) −f (u, v, w)
h
∂f
∂v = lim
h→0
f (u, v + h, w) −f (u, v, w)
h
∂f
∂w = lim
h→0
f (u, v, w + h) −f (u, v, w)
h
.
For example, a function for the volume of a cylinder is
V (r, h) = πr2h
where r is the radius, and h is the height. Say we wish to compute the function’s
partial derivative with respect to r. First, the partial derivative is written
∂V
∂r .
Second, we hold h constant, whilst allowing r to change. This means that the function
becomes
V (r, h) = kr2
(11.5)
where k = πh. Thus the partial derivative of (11.5) with respect to r is
∂V
∂r = 2kr
= 2πhr.
Next, by holding r constant, and allowing h to change, we have
∂V
∂h = πr2.
Sometimes, for purposes of clariﬁcation, the partial derivatives identify the constant
variable(s):
∂V
∂r

h
= 2πhr
∂V
∂h

r
= πr2.

258
11
Calculus: Derivatives
Partial differentiation is subject to the same rules for ordinary differentiation—we
just to have to remember which independent variable changes, and those held con-
stant. As with ordinary derivatives, we can compute higher-order partial derivatives.
For example, let’s ﬁnd the second-order partial derivatives of f (u, v), given
f (u, v) = u4 + 2u3v2 −4v3.
The ﬁrst partial derivatives are
∂f
∂u = 4u3 + 6u2v2
∂f
∂v = 4u3v −12v2
and the second-order partial derivatives are
∂2 f
∂u2 = 12u2 + 12uv2
∂2 f
∂v2 = 4u3 −24v.
In general, given f (u, v) = uv, then
∂f
∂u = v
∂f
∂v = u
and the second-order partial derivatives are
∂2 f
∂u2 = 0
∂2 f
∂v2 = 0.
Similarly, given f (u, v) = u/v, then
∂f
∂u = 1
v
∂f
∂v = −u
v2

11.13 Partial Derivatives
259
and the second-order partial derivatives are
∂2 f
∂u2 = 0
∂2 f
∂v2 = 2u
v3 .
Finally, given f (u, v) = uv, then
∂f
∂u = vuv−1
whereas, ∂f/∂v requires some explaining. First, given
f (u, v) = uv
taking natural logs of both sides, we have
ln f (u, v) = v ln u
and
f (u, v) = ev ln u.
Therefore,
∂f
∂v = ev ln u ln u
= uv ln u.
The second-order partial derivatives are
∂2 f
∂u2 = v(v −1)uv−2
∂2 f
∂v2 = uv ln2 u.
11.13.1 Visualising Partial Derivatives
Functions of the form y = f (x) are represented by a 2D graph, and the function’s
derivative f ′(x) represents the graph’s slope at any point x. Functions of the form

260
11
Calculus: Derivatives
Fig. 11.20 Surface of
z = 2.5x2 −2.5y2 using a
right-handed axial system
with a vertical z-axis
z = f (x, y) can be represented by a 3D surface, like the one shown in Fig.11.20,
which is z(x, y) = 2.5x2 −2.5y2. The two partial derivatives are
∂z
∂x = 5x
∂z
∂y = −5y
where ∂z
∂x is the slope of the surface in the x-direction, as shown in Fig.11.21, and
∂z
∂y is the slope of the surface in the y-direction, as shown in Fig.11.22.
The second-order partial derivatives are
∂2z
∂x2 = 5 = +ve
∂2z
∂y2 = −5 = −ve.
As ∂2z
∂x2 is positive, there is a local minimum in the x-direction, and as ∂2z
∂y2 is negative,
there is a local maximum in the y-direction, as conﬁrmed by Figs.11.21 and 11.22.

11.13 Partial Derivatives
261
Fig. 11.21
∂z
∂x describes the
slopes of these contour lines
Fig. 11.22
∂z
∂y describes the
slopes of these contour lines

262
11
Calculus: Derivatives
11.13.2 Mixed Partial Derivatives
We have seen that, given a function of the form f (u, v), the partial derivatives ∂f
∂u and
∂f
∂v provide the relative instantaneous changes in f and u, and f and v, respectively,
whilst the second independent variable remains ﬁxed. However, nothing prevents us
from differentiating ∂f
∂u with respect to v, whilst keeping u constant:
∂
∂v
∂f
∂u

which is also written as
∂2 f
∂v∂u
and is a mixed partial derivative. For example, to ﬁnd the mixed partial derivative of
f , given
f (u, v) = u3v4
we have
∂f
∂u = 3u2v4
and
∂2 f
∂v∂u = 12u2v3.
It should be no surprise that reversing the differentiation gives the same result: Let
f (u, v) = u3v4
then
∂f
∂v = 4u3v3
and
∂2 f
∂u∂v = 12u2v3.

11.13 Partial Derivatives
263
Generally, for continuous functions, we can write
∂2 f
∂u∂v = ∂2 f
∂v∂u .
11.14 Chain Rule
Earlier, we came across the chain rule for computing the derivatives of functions
of functions. For example, to compute the derivative of y = sin2 x we substitute
u = x2, then
y = u
dy
du = cos u
= cos x2.
Next, we compute du
dx :
u = x2
du
dx = 2x
and dy
dx is the product of the two derivatives using the chain rule:
dy
dx = dy
du · du
dx
= (cos x2)2x
= 2x cos x2.
But say we have a function where w is a function of two variables x and y, which in
turn, are a function of u and v. Then we have
w = f (x, y)
x = r(u, v)
y = s(u, v).
With such a scenario, we have the following partial derivatives:

264
11
Calculus: Derivatives
∂w
∂x ,
∂w
∂y
∂w
∂u ,
∂w
∂v
∂x
∂u ,
∂x
∂v
∂y
∂u ,
∂y
∂v .
These are chained together as follows
∂w
∂u = ∂w
∂x · ∂x
∂u + ∂w
∂y · ∂y
∂u
(11.6)
∂w
∂v = ∂w
∂x · ∂x
∂v + ∂w
∂y · ∂y
∂v .
(11.7)
For example, to ﬁnd ∂w
∂u and ∂w
∂v , given
w = f (2x + 3y)
x = r(u2 + v2)
y = s(u2 −v2)
we have
∂w
∂x = 2,
∂w
∂y = 3,
∂x
∂u = 2u,
∂x
∂v = 2v,
∂y
∂u = 2u,
∂y
∂v = −2v,
and plugging these into (11.6) and (11.7) we have
∂w
∂u = ∂w
∂x · ∂x
∂u + ∂w
∂y · ∂y
∂u
= 2 × 2u + 3 × 2u
= 10u
∂w
∂v = ∂w
∂x · ∂x
∂v + ∂w
∂y · ∂y
∂v
= 2 × 2v + 3 × (−2v)
= −2v.

11.14 Chain Rule
265
Thus, when u = 2 and v = 1
∂w
∂u = 20,
and
∂w
∂v = −2.
11.15 Total Derivative
Given a function with three independent variables, such as w = f (x, y, t), where
x = g(t) and y = h(t), there are three primary partial derivatives:
∂w
∂x ,
∂w
∂y ,
∂w
∂t ,
which show the differential change of w with x, y and t respectively. There are also
three derivatives:
dx
dt ,
dy
dt ,
dt
dt
where dt
dt = 1. The partial and ordinary derivatives can be combined to create the
total derivative which is written
dw
dt = ∂w
∂x
dx
dt + ∂w
∂y
dy
dt + ∂w
∂t .
dw
dt measures the instantaneous change of w relative to t, when all three independent
variables change. For example, to ﬁnd dw
dt , given
w = x2 + xy + y3 + t2
x = 2t
y = t −1
we have,
dx
dt = 2
dy
dt = 1
∂w
∂x = 2x + y = 4t + t −1 = 5t −1
∂w
∂y = x + 3y2 = 2t + 3(t −1)2 = 3t2 −4t + 3
∂w
∂t = 2t

266
11
Calculus: Derivatives
dw
dt = ∂w
∂x · dx
dt + ∂w
∂y · dy
dt + ∂w
∂t
= (5t −1)2 + (3t2 −4t + 3) + 2t = 3t2 + 8t + 1
and the total derivative equals
dw
dt = 3t2 + 8t + 1
and when t = 1, dw/dt = 12.
11.16 Power Series
A power series is an inﬁnite string of nomial terms with increasing powers. A general
form is written
∞

n=0
anxn = a0 + a1x + a2x2 + a3x3 + · · ·
where an and xn are generally real quantities. And because each term is individually
simple, they are relatively easy to differentiate and integrate. For example, given
y = a0 + a1x + a2x2 + a3x3 + a4x4 + · · ·
d
dx [y] = a1 + 2a2x + 3a3x2 + 4a4x3 + · · ·
An excellent example is found in the exponential function ex:
ex = 1 + x1
1! + x2
2! + x3
3! + x4
4! + · · ·
d
dx [ex] = 1 + x1
1! + x2
2! + x3
3! + x4
4! + · · ·
In particular:
e1 = e = 1 + 1
1! + 1
2! + 1
3! + 1
4! + · · ·
In 1715, the English mathematician Brook Taylor (1685–1731) published Methods
Incrementorum Directa et Inversa which contained a theorem concerning power
series. Today, this is known as Taylor’s theorem, and the associated series: Taylor’s
series. Lagrange recognised its importance and called it “the main foundation of
differential calculus”.

11.16 Power Series
267
Taylor proposed that any reasonable function, such as sin x and cos x can be
written as a power series:
sin x = a0 + a1x + a2x2 + a3x3 + a4x4 + a5x5 + a6x6 + · · ·
To ﬁnd the values of a0, a1, a2, etc., we proceed as follows. When x = 0, sin 0 = 0,
which implies a0 = 0. Therefore,
sin x = a1x + a2x2 + a3x3 + a4x4 + a5x5 + a6x6 + · · ·
(11.8)
Differentiating (11.8) we get
d
dx [sin x] = cos x = a1 + 2a2x + 3a3x2 + 4a4x3 + 5a5x4 + 6a6x5 + · · ·
When x = 0, cos 0 = 1, which implies a1 = 1. Therefore,
cos x = a1 + 2a2x + 3a3x2 + 4a4x3 + 5a5x4 + 6a6x5 + · · ·
(11.9)
Differentiating (11.9) we get
d
dx [cos x] = −sin x = 2a2 + 6a3x + 12a4x2 + 20a5x3 + 30a6x4 + · · ·
When x = 0, −sin 0 = 0, which implies a2 = 0. Therefore,
−sin x = 6a3x + 12a4x2 + 20a5x3 + 30a6x4 + · · ·
(11.10)
Differentiating (11.10) we get
d
dx [−sin x] = −cos x = 6a3 + 24a4x + 60a5x2 + 120a6x3 + · · ·
When x = 0, −cos 0 = −1, which implies a3 = −1
6. Therefore,
−cos x = 6a3 + 24a4x + 60a5x2 + 120a6x3 + · · ·
(11.11)
Differentiating (11.11) we get
d
dx [−cos x] = sin x = 24a4 + 120a5x + 360a6x2 + · · ·
When x = 0, sin 0 = 0, which implies a4 = 0. Therefore,
sin x = 120a5x + 360a6x3 + · · ·
(11.12)

268
11
Calculus: Derivatives
Differentiating (11.12) we get
d
dx [sin x] = cos x = 120a5 + 1080a6x + · · ·
When x = 0, cos x = 1, which implies a5 =
1
120.
We now have a0 = 0, a1 = 1, a2 = 0, a3 = −1
6, a4 = 0, and a5 =
1
120, which means
that the original sin x function comprises only the odd powers of x, with alternating
signs. This permits us to write
sin x = x −x3
3! + x5
5! −x7
7! + · · ·
Conversely, the cos x function comprises only the even powers of x, with alternating
signs. This permits us to write
cos x = 1 −x2
2! + x4
4! −x6
6! + · · ·
It is clear that sin x and cos x are closely related to ex:
ex = 1 + x1
1! + x2
2! + x3
3! + x4
4! + x5
5! + x6
6! + x7
7! + x8
8! + x9
9! + · · ·
sin x = x −x3
3! + x5
5! −x7
7! + x9
9! + · · ·
cos x = 1 −x2
2! + x4
4! −x6
6! + x8
8! + · · ·
and it was Euler who discovered that by making x imaginary: eix, we have
eix = 1 + ix1
1! −x2
2! −ix3
3! + x4
4! + ix5
5! −x6
6! −ix7
7! + x8
8! + ix9
9! · · ·
= 1 −x2
2! + x4
4! −x6
6! + x8
8! + · · · + ix1
1! −ix3
3! + ix5
5! −ix7
7! + ix9
9! + · · ·
= 1 −x2
2! + x4
4! −x6
6! + x8
8! + · · · + i

x1
1! −x3
3! + x5
5! −x7
7! + x9
9! + · · ·

= cos x + i sin x
which is Euler’s trigonometric formula. If we now reverse the sign of ix, i.e. −ix,
we have:

11.16 Power Series
269
exp(−ix) = 1 −ix1
1! −x2
2! + ix3
3! + x4
4! −ix5
5! −x6
6! + ix7
7! + x8
8! −ix9
9! · · ·
= 1 −x2
2! + x4
4! −x6
6! + x8
8! + · · · −ix1
1! + ix3
3! −ix5
5! + ix7
7! −ix9
9! + · · ·
= 1 −x2
2! + x4
4! −x6
6! + x8
8! + · · · −i

x1
1! −x3
3! + x5
5! −x7
7! + x9
9! + · · ·

= cos x −i sin x
thus we have:
eix = cos x + i sin x
e−ix = cos x −i sin x
from which we obtain
cos x = eix + e−ix
2
sin x = eix −e−ix
2i
.
Given eix = cos x + i sin x, when x = π, we have eiπ = −1, or eiπ + 1 = 0, which
is Euler’s famous equation.
Another strange formula emerges as follows:
cos x + i sin x = eix
cos π/2 + i sin π/2 = eiπ/2
i = eiπ/2
ii =

eiπ/2i
= ei2π/2
= e−π/2
ii ≈0.207 879 576 . . .
which reveals that ii equals a real number, even though i is not a number, as we
know it!
The French mathematician Abraham de Moire (1667–1754), developed Euler’s
trigonometric formula as follows.

270
11
Calculus: Derivatives
cos θ + i sin θ = eiθ
(cos θ + i sin θ)n = (eiθ)n
= einθ
(cos θ + i sin θ)n = cos nθ + i sin nθ
(11.13)
where (11.13) is known as de Moivre’s theorem. Substituting n = 2 in (11.13) we
obtain
cos 2θ + i sin 2θ = (cos θ + i sin θ)2
cos 2θ + i sin 2θ = cos2 θ −sin2 θ + 2i cos θ sin θ.
Equating real and imaginary parts, we have
cos 2θ = cos2 θ −sin2 θ
sin 2θ = 2 cos θ sin θ.
de Moivre’s theorem can be used for similar identities by substituting other values
of n. Let’s try n = 3:
cos 3θ + i sin 3θ = (cos θ + i sin θ)3
cos 3θ + i sin 3θ = (cos θ + i sin θ)(cos2 θ −sin2 θ + 2i cos θ sin θ)
equating the real part:
cos 3θ = cos3 θ −cos θ sin2 θ −2 cos θ sin2 θ
= cos3 θ −3 cos θ sin2 θ
= cos3 θ −3 cos θ(1 −cos2 θ)
= 4 cos3 θ −3 cos θ
equating the imaginary part:
sin 3θ = cos2 θ sin θ −sin3 θ + 2 cos2 θ sin θ
= 3 cos2 θ sin θ −sin3 θ
= 3 sin θ(1 −sin2 θ) −sin3 θ
= 3 sin θ −4 sin3 θ.
Returning to the power series for ex:

11.16 Power Series
271
ex = 1 + x1
1! + x2
2! + x3
3! + x4
4! + x5
5! + x6
6! + x7
7! + x8
8! + · · ·
= 1 + x2
2! + x4
4! + x6
6! + x8
8! + · · · + x1
1! + x3
3! + x5
5! + x7
7! + · · ·
= cosh x + sinh x
which makes
cosh x = ex + e−x
2
sinh x = ex −e−x
2
which are the hyperbolic trigonometric functions and are related to the circular
trigonometric functions using
cosh x = cos ix
sinh x = −i sin ix.
The cosh function is read as “cosh”, whereas the sinh function is read as “shine” or
“sink”. Although there are many more hyperbolic functions, it is worth mentioning
the tanh function, pronounced “than” emphasising the “th”, which is
tanh x = sinh x
cosh x
= ex + e−x
ex −e−x .
11.17 Worked Examples
11.17.1 Antiderivative 1
Given dy
dx = 1, ﬁnd y.
Integrating:
y =

1 dx
= x + C.

272
11
Calculus: Derivatives
11.17.2 Antiderivative 2
Given dy
dx = 6x2 + 10x, ﬁnd y.
Integrating:
y =

6x2 + 10x dx
= 2x3 + 5x2 + C.
11.17.3 Differentiating Sums of Functions
Differentiate y = 2x6 + sin x + cos x.
dy
dx = 12x5 + cos x −sin x.
11.17.4 Differentiating a Function Product
Differentiate y = x2 sin x.
y = x2 sin x
u = x2
du
dx = 2x
v = sin x
dv
dx = cos x
dy
dx = u dv
dx + vdu
dx
= x2 cos x + 2x sin x.
Figure11.23 shows a graph of y = x2 sin x and its derivative, dy
dx = x2 cos x +
2x sin x.

11.17 Worked Examples
273
Fig. 11.23 Graph of
y = x2 sin x and its
derivative,
dy
dx = x2 cos x + 2x sin x
(dashed)
-
0
-4
-2
2
4
11.17.5 Differentiating an Implicit Function
Differentiate x2 −y2 + 4x = 6y.
2x −2y dy
dx + 4 = 6dy
dx .
Rearranging the terms, we have
2x + 4 = 6dy
dx + 2y dy
dx
= dy
dx (6 + 2y)
dy
dx = 2x + 4
6 + 2y .
If we have to ﬁnd the slope of x2 −y2 +4x = 6y at the point (4, 3), then we simply
substitute x = 4 and y = 3 in dy
dx to obtain the answer 1.
11.17.6 Differentiating a General Implicit Function
Differentiate xn + yn = an.
xn + yn = an
nxn−1 + nyn−1 dy
dx = 0
dy
dx = −nxn−1
nyn−1
= −xn−1
yn−1 .

274
11
Calculus: Derivatives
Fig. 11.24 Graph of
y = −3x3 + 9x, its ﬁrst
derivative, dy
dx = −9x2 + 9
(short dashes) and its second
derivative y = −18x (long
dashes)
-2
-1
0
1
2
-18
-12
-6
6
12
18
24
11.17.7 Local Maximum or Minimum
Given y = −3x3 + 9x, ﬁnd the local minimum and maximum for y.
The ﬁrst derivative is
dy
dx = −9x2 + 9
and second derivative
d2y
dx2 = −18x
as shown in Fig.11.24. For a local maximum or minimum, the ﬁrst derivative equals
zero:
−9x2 + 9 = 0
which implies that x = ±1.
The sign of the second derivative determines whether there is a local minimum
or maximum.
d2y
dx2 = −18x
= −18 × (−1) = +ve
= −18 × (+1) = −ve
therefore, when x = −1 there is a local minimum, and when x = +1 there is a local
maximum, as conﬁrmed by Fig.11.24.

11.17 Worked Examples
275
11.17.8 Partial Derivatives
Find the second-order partial derivatives of f , given
f (u, v) = sin(4u) · cos(5v)
the ﬁrst partial derivatives are
∂f
∂u = 4 cos(4u) · cos(5v)
∂f
∂v = −5 sin(4u) · sin(5v)
and the second-order partial derivatives are
∂2 f
∂u2 = −16 sin(4u) · cos(5v)
∂2 f
∂v2 = −25 sin(4u) · cos(5v).
11.17.9 Mixed Partial Derivative 1
Given the formula for the volume of a cylinder is V (r, h) = πr2h, where r and h are
the cylinder’s radius and height respectively, compute the mixed partial derivative.
V (r, h) = πr2h
∂V
∂r = 2πhr
∂2V
∂h∂r = 2πr
or
V (r, h) = πr2h
∂V
∂h = πr2
∂2V
∂r∂h = 2πr.

276
11
Calculus: Derivatives
11.17.10 Mixed Partial Derivative 2
Given f (u, v) = sin(4u) cos(3v), compute the mixed partial derivative.
∂f
∂u = 4 cos(4u) cos(3v)
∂2 f
∂v∂u = −12 cos(4u) sin(3v)
or
∂f
∂v = −3 sin(4u) sin(3v)
∂2 f
∂u∂v = −12 cos(4u) sin(3v).
11.17.11 Total Derivative
Given
w = x2 + xy + y + t
x = 2t
y = t −1
compute the total derivative dw
dt .
dx
dt = 2
dy
dt = 1
∂w
∂x = 2x + y = 4t + t −1 = 5t −1
∂w
∂y = x + 1 = 2t + 1
∂w
∂t = 1
dw
dt = ∂w
∂x · dx
dt + ∂w
∂y · dy
dt + ∂w
∂t
= (5t −1)2 + (2t + 1) + 1
and the total derivative equals
dw
dt = 12t.

Chapter 12
Calculus: Integration
12.1 Introduction
In this chapter we develop the idea that integration is the inverse of differentiation,
and explore the standard algebraic strategies for integrating functions, where the
derivative is unknown; these include simple algebraic manipulation, trigonometric
identities, integration by parts, integration by substitution and integration using par-
tial fractions.
12.2 Indeﬁnite Integral
In the previous chapter we have seen that given a simple function, such as
y = sin x + 23
dy
dx = cos x
and the constant term 23 disappears. Inverting the process, we have
y =

cos x dx
= sin x + C.
An integral of the form

f (x) dx
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5_12
277

278
12
Calculus: Integration
is known as an indeﬁnite integral; and as we don’t know whether the original function
contains a constant term, a constant C has to be included. Its value remains undeter-
mined unless we are told something about the original function. In this example, if
we are told that when x = π/2, y = 24, then
24 = sin π/2 + C
= 1 + C
C = 23.
12.3 Integration Techniques
12.3.1 Continuous Functions
Functions come in all sorts of shapes and sizes, which is why we have to be very
careful before they are differentiated or integrated. If a function contains any form of
discontinuity, then it cannot be differentiated or integrated. For example, the square-
wave function shown in Fig.12.1 cannot be differentiated as it contains discontinu-
ities. Consequently, to be very precise, we identify an interval [a, b], over which a
function is analysed, and stipulate that it must be continuous over this interval. For
example, a and b deﬁne the upper and lower bounds of the interval such that
a ≤x ≤b
then we can say that for f (x) to be continuous
lim
h→0 f (x + h) = f (x).
Fig. 12.1 A discontinuous
square-wave function
-1
0
1
2
3
4
5
-2
-1
1
2

12.3 Integration Techniques
279
Even this needs further clariﬁcation as h must not take x outside of the permitted
interval. So, from now on, we assume that all functions are continuous and can be
integrated without fear of singularities.
12.3.2 Difﬁcult Functions
There are many functions that cannot be differentiated and represented by a ﬁnite
collection of elementary functions. For example, the derivative f ′(x) = sin x/x
does not exist, which precludes the possibility of its integration. Figure12.2 shows
this function, and even though it is continuous, its derivative and integral can only
be approximated. Similarly, the derivative of y = √x sin x does not exist in a pre-
cise form, which precludes the possibility of ﬁnding a precise integral. Figure12.3
shows this continuous function. So now let’s examine how most functions have to
be rearranged to secure their integration.
Fig. 12.2 Graph of
y = (sin x)/x
-2
0
2
-1
1
Fig. 12.3 Graph of
y = √x sin x
0
2
-3
-2
-1
1
2
3

280
12
Calculus: Integration
12.4 Trigonometric Identities
Sometimes it is possible to simplify the integrand by substituting a trigonometric
identity. For example, let’s evaluate

sin2 x dx,

cos2 x dx,

tan2 x dx and

sin 3x cos x dx.
The identity sin2 x = 1
2(1 −cos 2x) converts sin2 x into a double-angle form:

sin2 x dx = 1
2

1 −cos 2x dx
= 1
2

dx −1
2

cos 2x dx
= 1
2 x −1
4 sin 2x + C.
Figure12.4 shows the graphs of y = sin2 x and y = 1
2 x −1
4 sin 2x.
The identity cos2 x = 1
2(cos 2x + 1) converts cos2 x into a double-angle form:

cos2 x dx = 1
2

cos 2x + 1 dx
= 1
2

cos 2x dx + 1
2

dx
= 1
4 sin 2x + 1
2 x + C.
Figure12.5 shows the graphs of y = cos2 x and y = 1
4 sin 2x + 1
2 x.
Fig. 12.4 The graphs of
y = sin2 x (dashed) and
y = 1
2 x −1
4 sin 2x
-4
-3
-2
-1
0
1
2
3
4
5
-3
-2
-1
1
2
3

12.4 Trigonometric Identities
281
Fig. 12.5 The graphs of
y = cos2 x (dashed) and
y = 1
4 sin 2x + 1
2 x
-4
-3
-2
-1
0
1
2
3
4
5
-3
-2
-1
1
2
3
Fig. 12.6 The graphs of
y = tan2 x (dashed) and
y = tan x −x
-8
-4
0
4
8
-4
4
The identity sec2 x = 1 + tan2 x, permits us to write

tan2 x dx =

sec2 x −1 dx
=

sec2 x dx −

dx
= tan x −x + C.
Figure12.6 shows the graphs of y = tan2 x and y = tan x −x.
Finally, to evaluate

sin 3x cos x dx we use the identity
2 sin a cos b = sin(a + b) + sin(a −b)
which converts the integrand’s product into the sum and difference of two angles:
sin 3x cos x = 1
2(sin 4x + sin 2x)

282
12
Calculus: Integration
Fig. 12.7 The graphs of
y = sin 3x cos x (dashed)
and
y = −1
8 cos 4x −1
4 cos 2x
-3
-2
-1
0
1
2
3
-1
1

sin 3x cos x dx = 1
2

sin 4x + sin 2x dx
= 1
2

sin 4x dx + 1
2

sin 2x dx
= −1
8 cos 4x −1
4 cos 2x + C.
Figure12.7 shows the graphs of y = sin 3x cos x and y = −1
8 cos 4x −1
4 cos 2x.
12.4.1 Exponent Notation
Radicals are best replaced by their equivalent exponent notation. For example, to
evaluate

2
4√x dx
we proceed as follows:
The constant 2 is moved outside the integral, and the integrand is converted into an
exponent form:
2

1
4√x dx = 2

x−1
4
= 2

x
3
4
3
4

+ C
= 2
4
3x
3
4

+ C
= 8
3x
3
4 + C.

12.4 Trigonometric Identities
283
Fig. 12.8 The graphs of
y = 2/ 4√x (dashed) and
y = 8x
3
4 /3
-1
0
1
2
3
4
5
6
7
8
-1
1
2
3
4
Figure12.8 shows the graphs of y = 2/ 4√x and y = 8x
3
4 /3.
12.4.2 Completing the Square
Where possible, see if an integrand can be simpliﬁed by completing the square. For
example, to evaluate

1
x2 −4x + 8 dx
we proceed as follows:
We have already seen that

1
1 + x2 dx = arctan x + C
and it’s not too difﬁcult to prove that

1
a2 + x2 dx = 1
a arctan x
a + C.
Therefore, if we can manipulate an integrand into this form, then the integral will
reduce to an arctan result. The following needs no manipulation:

1
4 + x2 dx = 1
2 arctan x
2 + C.
However, the original integrand has x2−4x +8 as the denominator, which is resolved
by completing the square:
x2 −4x + 8 = 4 + (x −2)2.

284
12
Calculus: Integration
Fig. 12.9 The graphs of
y = 1/(x2 −4x + 8)
(dashed) and
y =

arctan x−2
2
	
/2
-3
-2
-1
0
1
2
3
4
5
6
-0.4
-0.2
0.2
0.4
Therefore,

1
x2 −4x + 8 dx =

1
22 + (x −2)2 dx
= 1
2 arctan

x −2
2

+ C.
Figure12.9 shows the graphs of y = 1/(x2 −4x + 8) and y =

arctan x−2
2
	
/2.
To evaluate

1
x2 + 6x + 10 dx.
we factorize the denominator:

1
x2 + 6x + 10 dx =

1
12 + (x + 3)2 dx
= arctan(x + 3) + C.
Figure12.10 shows the graphs of y = 1/(x2 + 6x + 10) and y = arctan(x + 3).
12.4.3 The Integrand Contains a Derivative
An integral of the form

f (x)
f ′(x) dx
is relatively easy to integrate. For example, let’s evaluate

12.4 Trigonometric Identities
285
Fig. 12.10 The graphs of
y = 1/(x2 + 6x + 10)
(dashed) and
y = arctan(x + 3)
-7
-6
-5
-4
-3
-2
-1
0
1
2
-1
1
 arctan x
1 + x2 dx.
Knowing that
d
dx [arctan x] =
1
1 + x2
let u = arctan x, then
du
dx =
1
1 + x2
and
 arctan x
1 + x2 dx =

u du
= u2
2 + C
= 1
2(arctan x)2 + C.
Figure12.11 shows the graphs of y = arctan x/(1 + x2) and y = 1
2(arctan x)2.
An integral of the form

f ′(x)
f (x) dx
is also relatively easy to integrate. For example, let’s evaluate
 cos x
sin x dx.

286
12
Calculus: Integration
Fig. 12.11 The graphs of
y = arctan x/(1 + x2)
(dashed) and
y = 1
2(arctan x)2
-
0
-1
1
Knowing that
d
dx [sin x] = cos x
let u = sin x, then
du
dx = cos x
and
 cos x
sin x dx =
 1
u du
= ln |u| + C
= ln | sin x| + C.
Figure12.12 shows the graphs of y = cos x/ sin x and y = ln | sin x|.
Fig. 12.12 The graphs of
y = cos x/ sin x (dashed)
and y = ln | sin x|
-
0
-2
-1
1
2

12.4 Trigonometric Identities
287
Fig. 12.13 The graphs of
y =
(4x3+x2−8+12x cos x)/4x
(dashed) and y = x3/3 +
x2/8 −2 ln |x| + 3 sin x
-3
-2
-1
0
1
2
3
-4
-2
2
4
6
12.4.4 Converting the Integrand into a Series of Fractions
Integration is often made easier by converting an integrand into a series of fractions.
For example, to integrate
 4x3 + x2 −8 + 12x cos x
4x
dx
we divide the numerator by 4x:
 4x3 + x2 −8 + 12x cos x
4x
dx =

x2 dx +
 x
4 dx −
 2
x dx +

3 cos x dx
= x3
3 + x2
8 −2 ln |x| + 3 sin x + C.
Figure12.13 shows the graphs of y = (4x3 + x2 −8 + 12x cos x)/4x and y =
x3/3 + x2/8 −2 ln |x| + 3 sin x.
12.4.5 Integration by Parts
Integration by parts is based upon the rule for differentiating function products where
d
dx [uv] = u dv
dx + vdu
dx
therefore,
uv =

uv′ dx +

vu′ dx

288
12
Calculus: Integration
which rearranged, gives

uv′ dx = uv −

vu′ dx.
Thus, if an integrand contains a product of two functions, we can attempt to integrate
it by parts. For example, let’s evaluate

x sin x dx.
In this case, we try the following:
u = x
and v′ = sin x
therefore
u′ = 1 and v = C1 −cos x.
Integrating by parts:

uv′ dx = uv −

vu′ dx

x sin x dx = x(C1 −cos x) −

(C1 −cos x)(1) dx
= C1x −x cos x −C1x + sin x + C
= −x cos x + sin x + C.
Figure12.14 shows the graphs of y = x sin x and y = −x cos x + sin x.
Fig. 12.14 The graphs of
y = x sin x (dashed) and
y = −x cos x + sin x
-
0
-2
2

12.4 Trigonometric Identities
289
Note the problems that arise if we make the wrong substitution:
u = sin x
and v′ = x
therefore
u′ = cos x
and v = x2
2 + C1
Integrating by parts:

uv′ dx = uv −

vu′ dx

x sin x dx = sin x

x2
2 + C1

−
 
x2
2 + C1

cos x dx
which requires to be integrated by parts, and is even more difﬁcult, which suggests
the substitution was not useful.
Now let’s evaluate

x2 cos x dx.
In this case, we try the following:
u = x2
and v′ = cos x
therefore
u′ = 2x
and v = sin x + C1.
Integrating by parts:

uv′ dx = uv −

vu′ dx

x2 cos x dx = x2(sin x + C1) −2

(sin x + C1)(x) dx
= x2 sin x + C1x2 −2C1

x dx −2

x sin x dx
= x2 sin x + C1x2 −2C1

x2
2 + C2

−2

x sin x dx
= x2 sin x −C3 −2

x sin x dx.

290
12
Calculus: Integration
Fig. 12.15 The graphs of
y = x2 cos x (dashed) and
y = x2 sin x + 2x cos x −
2 sin x
-
0
-2
2
At this point we come across

x sin x dx, which we have already solved:

x2 cos x dx = x2 sin x −C3 −2(−x cos x + sin x + C4)
= x2 sin x −C3 + 2x cos x −2 sin x −C5
= x2 sin x + 2x cos x −2 sin x + C
Figure12.15 shows the graphs of y = x2 cos x and y = x2 sin x +2x cos x −2 sin x.
Now let’s evaluate

x ln x dx.
In this case, we try the following:
u = ln x
and v′ = x
therefore
u′ = 1
x
and v = 1
2 x2.
Integrating by parts:

uv′ dx = uv −

vu′ dx

x ln x dx = 1
2 x2 ln x −
 
1
2 x2
 1
x dx
= 1
2 x2 ln x −1
2

x dx
= 1
2 x2 ln x −x2
4 + C.

12.4 Trigonometric Identities
291
Fig. 12.16 The graphs of
y = x ln x (dashed) and
y = 1
2 x2 ln x −x2/4
-1
0
1
2
3
4
-2
2
4
6
Figure12.16 shows the graphs of y = x ln x and y = 1
2 x2 ln x −x2/4.
Finally, let’s evaluate
 
1 + x2 dx.
Although this integrand does not look as though it can be integrated by parts, if we
rewrite it as
 
1 + x2(1) dx.
then we can use the formula.
Let
u =

1 + x2
and v′ = 1
therefore
u′ =
x
√
1 + x2
and v = x.
Integrating by parts:

uv′ dx = uv −

vu′ dx
 
1 + x2 dx = x

1 + x2 −

x2
√
1 + x2 dx.
Now we simplify the right-hand integrand:

292
12
Calculus: Integration
Fig. 12.17 The graphs of
y =
√
1 + x2 (dashed) and
y = 1
2 x
√
1 + x2 + 1
2 arsinhx
-4
-3
-2
-1
0
1
2
3
4
-3
-2
-1
1
2
3
 
1 + x2 dx = x

1 + x2 −
 (1 + x2) −1
√
1 + x2
dx
= x

1 + x2 −

1 + x2
√
1 + x2 dx +

1
√
1 + x2 dx
= x

1 + x2 −
 
1 + x2 dx + arsinhx + C1.
Now we have the original integrand on the right-hand side, therefore
2
 
1 + x2 dx = x

1 + x2 + arsinhx + C1
 
1 + x2 dx = 1
2 x

1 + x2 + 1
2arsinhx + C.
Figure12.17 shows the graphs of y =
√
1 + x2 and y = 1
2 x
√
1 + x2 + 1
2arsinhx.
12.4.6 Integration by Substitution
Integration by substitution is based upon the chain rule for differentiating a function
of a function, which states that if y is a function of u, which in turn is a function of
x, then
dy
dx = dy
du
du
dx .
For example, let’s evaluate

x2
x3 dx.

12.4 Trigonometric Identities
293
This is easily solved by rewriting the integrand:

x2
x3 dx =

x
7
2 dx
= 2
9x
9
2 + C.
However, introducing a constant term within the square-root requires integration by
substitution. For example,
evaluate

x2
x3 + 1 dx.
First, we let u = x3 + 1, then
du
dx = 3x2
or dx = du
3x2 .
Substituting u and dx in the integrand gives

x2
x3 + 1 dx =

x2√u du
3x2
= 1
3
 √u du
= 1
3

u
1
2 du
= 1
3 · 2
3u
3
2 + C
= 2
9(x3 + 1)
3
2 + C.
Figure12.18 shows the graphs of y = x2√
x3 + 1 and y = 2
9(x3 + 1)
3
2 .
Now let’s evaluate

2 sin x · cos x dx.
Integrating by substitution we let u = sin x, then
du
dx = cos x
or dx =
du
cos x .

294
12
Calculus: Integration
Fig. 12.18 The graphs of
y = x2√
x3 + 1 (dashed)
and y = 2
9(x3 + 1)
3
2
-1
0
1
2
-1
1
2
Fig. 12.19 The graphs of
y = 2 sin x · cos x (dashed)
and y = sin2 x
-
0
-2
-1
1
2
Substituting u and dx in the integrand gives

2 sin x · cos x dx = 2

u cos x du
cos x
= 2

u du
= u2 + C1
= sin2 x + C.
Figure12.19 shows the graphs of y = 2 sin x · cos x and y = sin2 x.
12.4.7 Partial Fractions
Integration by partial fractions is used when an integrand’s denominator contains
a product that can be split into two fractions. For example, it should be possible to
convert

12.4 Trigonometric Identities
295

3x + 4
(x + 1)(x + 2) dx
into

A
x + 1 dx +

B
x + 2 dx
which individually, are easy to integrate. Let’s compute A and B:
3x + 4
(x + 1)(x + 2) =
A
x + 1 +
B
x + 2
3x + 4 = A(x + 2) + B(x + 1)
= Ax + 2A + Bx + B.
Equating constants and terms in x:
4 = 2A + B
(12.1)
3 = A + B
(12.2)
Subtracting (12.2) from (12.1), gives A = 1 and B = 2. Therefore,

3x + 4
(x + 1)(x + 2) dx =

1
x + 1 dx +

2
x + 2 dx
= ln(x + 1) + 2 ln(x + 2) + C.
Figure12.20 shows the graphs of y = (3x + 4)/((x + 1)(x + 2)) and y = ln(x +
1) + 2 ln(x + 2).
Fig. 12.20 The graphs of
y = (3x+4)/((x+1)(x+2))
(dashed) and
y = ln(x + 1) + 2 ln(x + 2)
-4
-2
0
2
4
-4
-2
2
4

296
12
Calculus: Integration
Now let’s evaluate

5x −7
(x −1)(x −2) dx.
Integrating by partial fractions:
5x −7
(x −1)(x −2) =
A
x −1 +
B
x −2
5x −7 = A(x −2) + B(x −1)
= Ax + Bx −2A −B.
Equating constants and terms in x:
−7 = −2A −B
(12.3)
5 = A + B
(12.4)
Subtracting (12.3) from (12.4), gives A = 2 and B = 3. Therefore,

3x + 4
(x −1)(x −2) dx =

2
x −1 dx +

3
x −2 dx
= 2 ln(x −1) + 3 ln(x −2) + C.
Figure12.21 shows the graphs of y = (5x −7)/((x −1)(x −2)) and y = 2 ln(x −
1) + 3 ln(x −2).
Fig. 12.21 The graphs of
y = (5x−7)/((x−1)(x−2))
(dashed) and
y = 2 ln(x −1) + 3 ln(x −2)
-3
-2
-1
0
1
2
3
4
5
6
-4
-2
2
4

12.5 Area Under a Graph
297
12.5 Area Under a Graph
The ability to calculate the area under a graph is one of the most important discoveries
of integral calculus. Prior to calculus, area was computed by dividing a zone into
very small strips and summing the individual areas. The accuracy of the result is
improved simply by making the strips smaller and smaller, taking the result towards
some limiting value. In this section, we discover how integral calculus provides a
way to compute the area between a function’s graph and the x- and y-axis.
12.6 Calculating Areas
Before considering the relationship between area and integration, let’s see how area
is calculated using functions and simple geometry.
Figure12.22 shows the graph of y = 1, where the area A of the shaded zone is
A = x,
x > 0.
For example, when x = 4, A = 4, and when x = 10, A = 10. An interesting
observation is that the original function is the derivative of A:
d A
dx = 1 = y.
Figure12.23 shows the graph of y = 2x. The area A of the shaded triangle is
A = 1
2base × height
= 1
2 x × 2x
= x2.
Fig. 12.22 Area of the
shaded zone is A = x
y
x
y = 1
1
A = x
x

298
12
Calculus: Integration
Fig. 12.23 Area of the
shaded zone is A = x2
y
x
y = 2x
A = x2
x
Thus, when x = 4, A = 16. Once again, the original function is the derivative of A:
d A
dx = 2x = y
which is no coincidence.
Finally, Fig.12.24 shows a circle where x2 + y2 = r2, and the curve of the ﬁrst
quadrant is described by the function
y =

r2 −x2,
0 ≥x ≥r.
The total area of the shaded zones is the sum of the two parts A1 and A2. To simplify
the calculations the function is deﬁned in terms of the angle θ, such that
x = r sin θ
and
y = r cos θ.
Fig. 12.24 Graph of
y =
√
r2 −x2
y
x
y= r2
x2
A2
x
A1
f(x)
rcos
rsin
r

12.6 Calculating Areas
299
Therefore,
A1 = r2θ
2
A2 = 1
2(r cos θ)(r sin θ) = r2
4 sin 2θ
A = A1 + A2
= r2
2

θ + sin 2θ
2

.
To show that the total area is related to the function’s derivative, let’s differentiate A
with respect to θ:
d A
dθ = r2
2 (1 + cos 2θ) = r2 cos2 θ.
But we want the derivative d A
dx , which requires the chain rule
d A
dx = d A
dθ
dθ
dx
where
dx
dθ = r cos θ
or
dθ
dx =
1
r cos θ
therefore,
d A
dx = r2 cos2 θ
r cos θ
= r cos θ = y
which is the equation for the quadrant.
Hopefully, these three examples provide strong evidence that the derivative of the
function for the area under a graph, equals the graph’s function:
d A
dx = f (x)
which implies that
A =

f (x) dx.

300
12
Calculus: Integration
Fig. 12.25 Relationship
between y = f (x) and A(x)
y
x
y = f(x)
A(x)
x
A
x + x
a
Now let’s prove this observation using Fig.12.25, which shows a continuous
function y = f (x). Next, we deﬁne a function A(x) to represent the area under the
graph over the interval [a, x]. δA is the area increment between x and x + δx, and
δA ≈f (x) · δx.
We can also reason that
δA = A(x + δx) −A(x) ≈f (x) · δx
and the derivative d A
dx is the limiting condition:
d A
dx = lim
δx→0
A(x + δx) −A(x)
δx
= lim
δx→0
f (x) · δx
δx
= f (x)
thus,
d A
dx = f (x),
whose antiderivative is
A(x) =

f (x) dx.
The function A(x) computes the area over the interval [a, b] and is represented by
A(x) =
 b
a
f (x) dx
which is called the integral or deﬁnite integral.

12.6 Calculating Areas
301
Fig. 12.26 A(b) is the area
under the graph y = f (x),
0 ≥x ≥b
y
x
y = f(x)
A(b)
b
Fig. 12.27 A(a) is the area
under the graph y = f (x),
0 ≥x ≥a
y
x
y = f(x)
A(a)
a
Let’s assume that A(b) is the area under the graph of f (x) over the interval [0, b],
as shown in Fig.12.26, and is written
A(b) =
 b
0
f (x) dx.
Similarly, let A(a) be the area under the graph of f (x) over the interval [0, a], as
shown in Fig.12.27, and is written
A(a) =
 a
0
f (x) dx.
Figure12.28 shows that the area of the shaded zone over the interval [a, b] is
calculated by
A = A(b) −A(a)
which is written
A =
 b
0
f (x) dx −
 a
0
f (x) dx

302
12
Calculus: Integration
Fig. 12.28 A(b) −A(a) is
the area under the graph
y = f (x), a ≥x ≥b
y
x
y = f(x)
A(b)-A(a)
a
b
and is contracted to
A =
 b
a
f (x) dx.
(12.5)
The fundamental theorem of calculus states that the deﬁnite integral
 b
a
f (x) dx = F(b) −F(a)
where
F(a) =

f (x) dx,
x = a
F(b) =

f (x) dx,
x = b.
In order to compute the area beneath a graph of f (x) over the interval [a, b], we
ﬁrst integrate the graph’s function
F(x) =

f (x) dx
and then calculate the area, which is the difference
A = F(b) −F(a).
To illustrate how (12.5) is used in the context of the earlier three examples, let’s
calculate the area over the interval [1, 4] for y = 1, as shown in Fig.12.29. We
begin with
A =
 4
1
1 dx.

12.6 Calculating Areas
303
Fig. 12.29 Area under the
graph is
 4
1 1 dx
y
x
y = 1
1
A= 1dx
1
4
4
1
Next, we integrate the function, and transfer the interval bounds employing the sub-
stitution symbol

4
1
, or square brackets
 4
1
. Using

4
1
, we have
A =

4
1
x
= 4 −1
= 3
or using
 4
1
, we have
A = [x]4
1
= 4 −1
= 3.
I will continue with square brackets.
Now let’s calculate the area over the interval [1, 4] for y = 2x, as shown in
Fig.12.30. We begin with
A =
 4
1
2x dx.
Next, we integrate the function and evaluate the area
A =

x24
1
= 16 −1
= 15.

304
12
Calculus: Integration
Fig. 12.30 Area under the
graph is
 4
1 2x dx
y
x
y = 2x
A= 2xdx
1
4
4
1
Fig. 12.31 Area under the
graph is
 r
0
√
r2 −x2 dx
y
y= r2
x2
(rsin , rcos )
r
x
Finally, let’s calculate the area over the interval [0, r] for y =
√
r2 −x2, which
is the equation for a circle, as shown in Fig.12.31. We begin with
A =
 r
0

r2 −x2 dx.
(12.6)
Unfortunately, (12.6) contains a function of a function, which is resolved by substi-
tuting another independent variable. In this case, the geometry of the circle suggests
x = r sin θ
therefore,

r2 −x2 = r cos θ
and
dx
dθ = r cos θ.
(12.7)

12.6 Calculating Areas
305
However, changing the independent variable requires changing the interval for the
integral. In this case, changing 0 ≥x ≥r into θ1 ≥θ ≥θ2:
When x = 0, r sin θ1 = 0, therefore θ1 = 0.
When x = r, r sin θ2 = r, therefore θ2 = π/2.
Thus, the new interval is [0, π/2].
Finally, the dx in (12.6) has to be changed into dθ, which using (12.7) makes
dx = r cos θ dθ.
Now we are in a position to rewrite the original integral using θ as the independent
variable:
A =

π
2
0
(r cos θ)(r cos θ) dθ
= r2

π
2
0
cos2 θ dθ
= r2
2

π
2
0
1 + cos 2θ dθ
= r2
2

θ + 1
2 sin 2θ
 π
2
0
= r2
2
π
2

= πr2
4
which makes the area of a full circle πr2.
12.7 Positive and Negative Areas
Area in the real world is always regarded as a positive quantity—no matter how it is
measured. In mathematics, however, area is often a signed quantity, and is determined
by the clockwise or anticlockwise direction of vertices. As we generally use a left-
handed Cartesian axial system in calculus, areas above the x-axis are positive, whilst
areas below the x-axis are negative. This can be illustrated by computing the area of
the positive and negative parts of a sine wave.
Figure12.32 shows a sketch of a sine wave over one cycle, where the area above
the x-axis is labelled A1, and the area below the x-axis is labelled A2. These areas
are computed as follows.

306
12
Calculus: Integration
Fig. 12.32 The two areas
associated with a sine wave
y
x
A1= sinxdx
0
y = sin x
A2 =
sinxdx
2
2
A1 =
 π
0
sin x dx
= [−cos x]π
0
= [1 + 1]
= 2.
However, A2 gives a negative result:
A2 =
 2π
π
sin x dx
= [−cos x]2π
π
= [−1 −1]
= −2.
This means that the area is zero over the bounds 0 to 2π .
A2 =
 2π
0
sin x dx
= [−cos x]2π
0
= [−1 + 1]
= 0.
Consequently, one must be very careful using this technique for functions that are
negative in the interval under investigation. Figure12.33 shows a sine wave over the
interval [0, π] and its accumulated area.

12.8 Area Between Two Functions
307
Fig. 12.33 The accumulated
area of a sine wave
0
1
2
12.8 Area Between Two Functions
Figure12.34 shows the graphs of y = x2 and y = x3, with two areas labelled A1
and A2. A1 is the area trapped between the two graphs over the interval [−1, 0] and
A2 is the area trapped between the two graphs over the interval [0, 1]. These areas
are calculated very easily: in the case of A1 we sum the individual areas under the
two graphs, remembering to reverse the sign for the area associated with y = x3.
For A2 we subtract the individual areas under the two graphs.
A1 =
 0
−1
x2 dx −
 0
−1
x3 dx
=
x3
3
0
−1
−
x4
4
0
−1
= 1
3 + 1
4
= 7
12.
Fig. 12.34 Two areas
between y = x2 and y = x3
-1
0
1
-1
1
A1
A2
y = x2
y = x2
y = x3
y = x3

308
12
Calculus: Integration
A2 =
 1
0
x2 dx −
 1
0
x3 dx
=
x3
3
1
0
−
x4
4
1
0
= 1
3 −1
4
= 1
12.
Note, that in both cases the calculation is the same, which implies that when we
employ
A =
 b
a
[ f (x) −g(x)] dx
A is always the area trapped between f (x) and g(x) over the interval [a, b].
Let’s take another example, by computing the area A between y = sin x and the
line y = 0.5, as shown in Fig.12.35. The horizontal line intersects the sine curve at
x = 30◦and x = 150◦, marked in radians as 0.5236 and 2.618 respectively.
A =
 150◦
30◦
sin x dx −
 5π/6
π/6
0.5 dx
= [−cos x]150◦
30◦−1
2 [x]5π/6
π/6
=
√
3
2 +
√
3
2

−1
2
5π
6 −π
6

=
√
3 −π
3
≈0.685.
Fig. 12.35 The area
between y = sin x and
y = 0.5
0
1
2
3
1
A
(0.5236, 0.5)
(2.618, 0.5)
y = sin x
y = 0.5

12.9 Areas with the Y-Axis
309
12.9 Areas with the Y-Axis
So far we have only calculated areas between a function and the x-axis. So let’s
compute the area between a function and the y-axis. Figure12.36 shows the function
y = x2 over the interval [0, 4], where A1 is the area between the curve and the
x-axis, and A2 is the area between the curve and y-axis. The sum A1 + A2 must
equal 4 × 16 = 64, which is a useful control. Let’s compute A1.
A1 =
 4
0
x2 dx
=
x3
3
4
0
= 64
3
≈21.333
which means that A2 ≈42.666. To compute A2 we construct an integral relative to
dy with a corresponding interval. If y = x2 then x = y
1
2 , and the interval is [0, 16]:
A2 =
 16
0
y
1
2 dy
=
2
3 y
3
2
16
0
= 2
364
≈42.666.
Fig. 12.36 The areas
between the x-axis and the
y-axis
0
4
16
A1
y = x2
A2

310
12
Calculus: Integration
12.10 Area with Parametric Functions
When working with functions of the form y = f (x), the area under its curve and
the x-axis over the interval [a, b] is
A =
 b
a
f (x) dx.
However, if the curve has a parametric form where
x = fx(t) and
y = fy(t)
then we can derive an equivalent integral as follows.
First: We need to establish equivalent limits [α, β] for t, such that
a = fx(α) and b = fy(β).
Second: Any point on the curve has corresponding Cartesian and parametric coordi-
nates:
x
and
fx(t)
y = f (x) and
fy(t).
Third:
x = fx(t)
dx = f ′
x(t)dt
A =
 b
a
f (x) dx
=
 β
α
fy(t) f ′
x(t) dt
therefore
A =
 β
α
fy(t) f ′
x(t) dt.
(12.8)
Let’s apply (12.8) using the parametric equations for a circle
x = −r cos t
y = r sin t.

12.10 Area with Parametric Functions
311
Fig. 12.37 The parametric
functions for a circle
0
t
-rcost
rsint
x
y
as shown in Fig.12.37. Remember that the Cartesian interval is [a, b] left to right,
and the polar interval [α, β], must also be left to right, which is why x = −r cos t.
Therefore,
f ′
xt = r sin t
fy(t) = r sin t
A =
 β
α
fy(t) f ′
x(t) dt
=
 π
0
r sin t · r sin(t) dt
= r2
 π
0
sin2 t dt
= r2
2
 π
0
1 −cos(2t) dt
= r2
2

t + 1
2 sin(2t)
π
0
= πr2
2
which makes the area of a full circle πr2.
12.11 The Riemann Sum
The German mathematician Bernhard Riemann (1826–1866) (pronounced “Ree-
man”) made major contributions to various areas of mathematics, including integral
calculus, where his name is associated with a formal method for summing areas and
volumes. Through the Riemann Sum, Riemann provides an elegant and consistent

312
12
Calculus: Integration
Fig. 12.38 The graph of
function f (x) over the
interval [a, b]
y
x
y = f(x)
x
h0
h1
h2
h3
h4
h5
h6
h7
h8
a
b
x
x
x
x
x
x
x
x0
x1
x2
x3
x4
x5
x6
x7
x8
notation for describing single, double and triple integrals when calculating area and
volume. Let’s see how the Riemann sum explains why the area under a curve is the
function’s integral.
Figure12.38 shows a function f (x) divided into eight equal sub-intervals where
Δx = b −a
8
and
a = x0 < x1 < x2 < · · · < x7 < x8 = b.
In order to compute the area under the curve over the interval [a, b], the interval
is divided into some large number of sub-intervals. In this case, eight, which is not
very large, but convenient to illustrate. Each sub-interval becomes a rectangle with a
common width Δx and a different height. The area of the ﬁrst rectangular sub-interval
shown shaded, can be calculated in various ways. We can take the left-most height
x0 and form the product x0Δx, or we can take the right-most height x1 and form
the product x1Δx. On the other hand, we could take the mean of the two heights
(x0 + x1)/2 and form the product (x0 + x1)Δx/2. A solution that shows no bias
towards either left, right or centre, is to let x∗
i be anywhere in a speciﬁc sub-interval
Δxi, then the area of the rectangle associated with the sub-interval is f (x∗
i )Δxi, and
the sum of the rectangular areas is given by
A =
8

i=1
f (x∗
i )Δxi.
Dividing the interval into eight equal sub-intervals will not generate a very accurate
result for the area under the graph. But increasing it to eight-thousand or eight-

12.11 The Riemann Sum
313
million, will take us towards some limiting value. Rather than specify some speciﬁc
large number, it is common practice to employ n, and let n tend towards inﬁnity,
which is written
A =
n

i=1
f (x∗
i )Δxi.
(12.9)
The right-hand side of (12.9) is called a Riemann sum, of which there are many. For
the above description, I have assumed that the sub-intervals are equal, which is not
a necessary requirement.
If the number of sub-intervals is n, then
Δx = b −a
n
and the deﬁnite integral is deﬁned as
 b
a
f (x) dx = lim
n→∞
n

i=1
f (x∗
i )Δxi.
12.12 Worked Examples
12.12.1 Integrating a Function Containing Its Own Derivative
Evaluate

sin x
cos x dx.
Knowing that
d
dx [cos x] = −sin x
let u = cos x, then
du
dx = −sin x
du = −sin x dx

314
12
Calculus: Integration
Fig. 12.39 The graphs of
y = sin x/ cos x (dashed)
and y = ln | sec x|
-
0
-2
-1
1
2
and

sin x
cos x dx =
 1
u (−1) du
= −ln |u| + C
= −ln | cos x| + C
= ln | cos x|−1 + C
= ln | sec x| + C.
Figure12.39 shows the graphs of y = sin x/ cos x and y = ln | sec x|.
12.12.2 Dividing an Integral into Several Integrals
Evaluate
 2 sin x + cos x + sec x
cos x
dx.
Divide the numerator by cos x:
 2 sin x + cos x + sec x
cos x
dx = 2

tan x dx +

1 dx +

sec2 x dx
= 2 ln | sec x| + x + tan x + C.
Figure12.40 shows the graphs of y = (2 sin x + cos x + sec x)/ cos x and y =
2 ln | sec x| + x + tan x.

12.12 Worked Examples
315
Fig. 12.40 The graphs of
y = (2 sin x + cos x +
sec x)/ cos x (dashed) and
y = 2 ln | sec x| + x + tan x
-
0
2
12.12.3 Integrating by Parts 1
Evaluate

x cos x dx.
In this case, we try the following:
u = x
and v′ = cos x
where
u′ = 1 and v = sin x + C1.
Integrating by parts:

uv′ dx = uv −

vu′ dx

x cos x dx = x(sin x + C1) −

(sin x + C1)(1) dx
= x sin x + C1x + cos x −C1x + C
= x sin x + cos x + C.
Figure12.41 shows the graphs of y = x cos x and y = x sin x + cos x.

316
12
Calculus: Integration
Fig. 12.41 The graphs of
y = x cos x (dashed) and
y = x sin x + cos x
-
0
-4
-2
2
12.12.4 Integrating by Parts 2
Evaluate

x2 sin x dx.
In this case, we try the following:
u = x2
and v′ = sin x
where
u′ = 2x
and v = −cos x + C1.
Integrating by parts:

uv′ dx = uv −

vu′ dx

x2 sin x dx = x2(−cos x + C1) −2

(−cos x + C1)(x) dx
= −x2 cos x + C1x2 −2C1

x dx + 2

x cos x dx
= −x2 cos x + C1x2 −2C1

x2
2 + C2

+ 2

x cos x dx
= −x2 cos x −C3 + 2

x cos x dx.
At this point we come across

x cos x dx, which we have already solved:

12.12 Worked Examples
317
Fig. 12.42 The graphs of
y = x2 sin x (dashed) and
y = −x2 cos x + 2x sin x +
2 cos x
-
0
-2
2

x2 sin x dx = −x2 cos x −C3 + 2(x sin x + cos x + C4)
= −x2 cos x −C3 + 2x sin x + 2 cos x + C5
= −x2 cos x + 2x sin x + 2 cos x + C
Figure12.42showsthegraphsof y = x2 sin x and y = −x2 cos x+2x sin x+2 cos x.
12.12.5 Integrating by Substitution 1
Evaluate

2ecos 2x sin x · cos x dx.
Integrating by substitution, let u = cos 2x, then
du
dx = −2 sin 2x
or dx = −
du
2 sin 2x .
Substituting a double-angle identity, u and du:

2ecos 2x sin x · cos x dx = −

eu sin 2x
du
2 sin 2x
= −1
2

eu du
= −1
2eu + C
= −1
2ecos 2x + C.

318
12
Calculus: Integration
Fig. 12.43 The graphs of
y = 2ecos 2x sin x cos x
(dashed) and y = −1
2ecos 2x
-
0
-2
-1
1
2
Figure12.43 shows the graphs of y = 2ecos 2x sin x · cos x and y = −1
2ecos 2x.
12.12.6 Integrating by Substitution 2
Evaluate

cos x
(1 + sin x)3 dx.
Integrating by substitution, let u = 1 + sin x, then
du
dx = cos x
or dx =
du
cos x .

cos x
(1 + sin x)3 dx =
 cos x
u3
du
cos x
=

u−3 du
= −1
2u−2 + C
= −1
2(1 + sin x)−2 + C
= −
1
2(1 + sin x)2 + C.
Figure12.44 shows the graphs of y = cos x/(1+sin x)3 and y = −1/2(1+sin x)2.

12.12 Worked Examples
319
Fig. 12.44 The graphs of
y = cos x/(1 + sin x)3
(dashed) and
y = −1/2(1 + sin x)2
-2
-
0
2
-4
4
12.12.7 Integrating by Substitution 3
Evaluate

sin 2x dx.
Integrating by substitution, let u = 2x, then
du
dx = 2 or dx = du
2 .

sin 2x dx = 1
2

sin u du
= −1
2 cos u + C
= −1
2 cos 2x + C
Figure12.45 shows the graphs of y = sin 2x and y = −1
2 cos 2x.
12.12.8 Integrating with Partial Fractions
Evaluate
 6x2 + 5x −2
x3 + x2 −2x dx.

320
12
Calculus: Integration
Fig. 12.45 The graphs of
y = sin 2x (dashed) and
y = −1
2 cos 2x
-
0
-2
-1
1
2
Integrating by partial fractions:
6x2 + 5x −2
x3 + x2 −2x = A
x +
B
x + 2 +
C
x −1
6x2 + 5x −2 = A(x + 2)(x −1) + Bx(x −1) + Cx(x + 2)
= Ax2 + Ax −2A + Bx2 −Bx + Cx2 + 2Cx.
Equating constants, terms in x and x2:
−2 = −2A
(12.10)
5 = A −B + 2C
(12.11)
6 = A + B + C
(12.12)
Fig. 12.46 The graphs of
y =
(6x2+5x −2)/(x3+x2−2x)
(dashed) and y =
ln x +2 ln(x +2)+3 ln(x −1)
-4
-3
-2
-1
0
1
2
3
4
-10
10

12.12 Worked Examples
321
Manipulating (12.10), (12.11) and (12.12): A = 1, B = 2 and C = 3, therefore,
 6x2 + 5x −2
x3 + x2 −2x dx =
 1
x dx +

2
x + 2 dx +

3
x −1 dx
= ln x + 2 ln(x + 2) + 3 ln(x −1) + C.
Figure12.46 shows the graphs of y = (6x2 + 5x −2)/(x3 + x2 −2x) and y =
ln x + 2 ln(x + 2) + 3 ln(x −1).

Appendix A
Limit of (Sin θ)/θ
This appendix proves that
lim
θ→0
sin θ
θ
= 1,
where θ is in radians.
From high-school mathematics we know that sin θ ≈θ, for small values of θ. For
example:
sin 0.1 = 0.099833
sin 0.05 = 0.04998
sin 0.01 = 0.0099998
and
sin 0.1
0.1
= 0.99833
sin 0.05
0.05
= 0.99958
sin 0.01
0.01
= 0.99998.
Therefore, we can reason that in the limit, as θ →0:
lim
θ→0
sin θ
θ
= 1.
FigureA.1 shows a graph of (sin θ)/θ, which conﬁrms this result. However, this is
an observation, rather than a proof. So, let’s pursue a geometric line of reasoning.
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5
323

324
Appendix A: Limit of (Sin θ)/θ
Fig. A.1 Graph of (sin θ)/θ
-3
-2
-
0
2
3
-1
1
Fig. A.2 Unit radius circle
with trigonometric ratios
y
x
A
1
D
O
C
B
sin
cos
tan
FromFig.A.2weseeasthecircle’sradiusisunity,OA = OB = 1,andAC = tan θ.
As part of the strategy, we need to calculate the area of the triangle △OAB, the sector
OAB and the △OAC:
Area of △OAB = △ODB + △DAB
= 1
2 cos θ sin θ + 1
2(1 −cos θ) sin θ
= 1
2 cos θ sin θ + 1
2 sin θ −1
2 cos θ sin θ
= sin θ
2
.
Area of sector OAB = θ
2π π(1)2 = θ
2.
Area of △OAC = 1
2(1) tan θ = tan θ
2
.

Appendix A: Limit of (Sin θ)/θ
325
From the geometry of a circle, we know that
sin θ
2
< θ
2 < tan θ
2
sin θ < θ < sin θ
cos θ
1 <
θ
sin θ <
1
cos θ
1 > sin θ
θ
> cos θ
and as θ →0, cos θ →1 and sin θ
θ
→1. This holds, even for negative values of θ,
because
sin(−θ)
−θ
= −sin θ
−θ
= sin θ
θ
.
Therefore,
lim
θ→0
sin θ
θ
= 1.

Appendix B
Integrating Cosnθ
We start with

cosn x dx =

cos x cosn−1 x dx.
Let u = cosn−1 x and v′ = cos x, then
u′ = −(n −1) cosn−2 x sin x
and
v = sin x.
Integrating by parts:

uv′ dx = uv −

vu′ dx + C

cosn−1 x cos x dx = cosn−1 x sin x +

sin x (n −1) cosn−2 x sin x dx + C
= sin x cosn−1 x + (n −1)

sin2 x cosn−2 x dx + C
= sin x cosn−1 x + (n −1)

(1 −cos2 x) cosn−2 x dx + C
= sin x cosn−1 x + (n −1)

cosn−2 dx −(n −1)
×

cosn x dx + C
n

cosn x dx = sin x cosn−1 x + (n −1)

cosn−2 dx + C

cosn x dx = sin x cosn−1 x
n
+ n −1
n

cosn−2 dx + C
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5
327

328
Appendix B: Integrating Cosnθ
where n is an integer, ̸= 0.
Similarly,

sinn x dx = −cos x sinn−1 x
n
+ n −1
n

sinn−2 dx + C.
For example,

cos3 x dx = sin x cos2 x
3
+ 2
3 sin x + C.

Index
0–9
2D
polygons, 97
scaling transform, 194
vector, 128
3D
complex numbers, 138
coordinates, 99
rotation transform, 197
transforms, 196
vector, 131
A
Absolute complement, 76
Aleph-zero, 33
Algebraic number, 20, 32
AND, 58, 70
Angle
compound, 89
Anticlockwise, 97
Antiderivative, 224
Antisymmetric matrix, 160
Area, 143
between two functions, 307
circle, 298
negative, 305
of a shape, 98
parametric function, 310
positive, 305
under a graph, 297
with the y-axis, 309
Argand, Jean-Robert, 127
Associative law, 10
Associativity, 63
Associativity of ∨, 63
Associativity of ∧, 63
Atan2, 87
Axial systems
left-handed, 99
right-handed, 99
Axioms, 9
B
Bürgi, Joost, 44
Barycentric coordinates, 103
Binary
addition, 17
negative number, 18
number, 12
operation, 9
subtraction, 18
Binomial expansion, 220
Boole, George, 57
Boolean logic, 57
C
Calculus, 215
Cantor, Georg, 33
Cardinality, 33, 72
Cartesian
coordinates, 95
plane, 95
vector, 135
Cauchy, Augustin-Louis, 215
Cayley, Arthur, 28, 151
Cayley numbers, 28
Chain rule, 263
Clockwise, 97
Closed interval, 48
Cofactor expansion, 173
Column vector, 128, 152, 155, 163
© Springer International Publishing Switzerland 2015
J. Vince, Foundation Mathematics for Computer Science,
DOI 10.1007/978-3-319-21437-5
329

330
Index
Commutative law, 9
Commutativity, 62
of ∨, 62
of ∧, 62
Complex
conjugate, 25
number, 24
plane, 22
Composite number, 29
Compound angles, 89
Conjunction, 58, 60
Continuity, 215
Continuous functions, 278
Contradiction, 66
Contrapositive, 68
Coordinates
barycentric, 104
Cartesian, 95
cylindrical, 103
homogeneous, 104
local, 104
polar, 101
spherical polar, 102
Cosecant, 84
Cosine, 84
rule, 89
Cotangent, 84
Cross product, 137
Cubic
equation, 218
function, 96
Cylindrical coordinates, 103
D
Decimal
number, 11
system, 7
Deﬁnite integral, 300
Degree, 81
de Moire, Abraham, 269
de Moivre’s theorem, 269
de Morgan’s Laws, 62, 64
de Morgan, Augustus, 64
Dependent variable, 47
Derivative, 215, 223
graphical interpretation, 222
total, 265
Descartes, René, 21, 38, 95
Determinant, 109, 157, 206
Diagonal matrix, 176
Differential, 223
Differentiating, 226
arccos function, 248
arccot function, 249
arccsc function, 249
arcsec function, 249
arcsin function, 248
arctan function, 248
cosh function, 251
cot function, 247
csc function, 245
exponential functions, 240
function of a function, 228
function products, 232
function quotients, 235
hyperbolic functions, 249
implicit functions, 237
logarithmic functions, 242
sec function, 246
sine function, 229
sinh function, 251
sums of functions, 226
tan function, 244
tanh function, 251
trigonometric functions, 244
Differentiation
partial, 258
Disjunction
exclusive, 58
inclusive, 58
Distance between two points, 98, 99
Distributive law, 10
Distributivity, 64
of ∨over ∧, 64
of ∧over ∨, 64
Domain, 49, 85
Dot product, 136
Double-angle identities, 91
Double negation, 67
E
Element, 5
Empty set, 72
Equation
explicit, 46
implicit, 46
Equivalence, 59, 68
Euclid, 30, 32
Euler
diagram, 72
rotations, 197
Euler, Leonhard, 46
Even function, 50
Excluded middle, 66

Index
331
Exclusive disjunction, 58, 61
Explicit equation, 46
Exportation, 68
F
Fermat, Pierre de, 95
Fraenkel, Abraham, 71
Function, 46, 220
continuous, 278
cubic, 96, 218
domain, 49
even, 50
graph, 96
linear, 96
notation, 47
odd, 50
power, 52
quadratic, 96, 217
range, 49
second derivative, 274
trigonometric, 96
Function of a function
differentiating, 228
Fundamental theorem of arithmetic, 30
Fundamental theorem of calculus, 302
G
Gödel, Kurt, 38
Gauss, Carl, 31, 151
Geometric transforms, 149
Gibbs, Josiah, 28
Goldbach, Christian, 31
Graves, John, 28
H
Half-angle identities, 92
Half-open interval, 48
Hamilton, Sir William Rowan, 27
Hexadecimal number, 13
Higher derivatives, 252
Homogeneous coordinates, 104
I
Idempotence, 62
of ∨, 62
of ∧, 62
Identity matrix, 192
Iff, 59
Imaginary number, 20
Implication, 58, 60, 67
Implicit equation, 46
Inclusive disjunction, 58, 61
Indeﬁnite integral, 277
Independent variable, 47
Indeterminate form, 7
Indices, 43
laws of, 43
Inﬁnitesimals, 215
Inﬁnity, 33
Integer, 19
number, 5
Integral
deﬁnite, 300
Integrating
arccos function, 248
arccot function, 249
arccsc function, 249
arcsec function, 249
arcsin function, 248
arctan function, 248
cot function, 247
csc function, 245
exponential function, 242
logarithmic function, 243
sec function, 246
tan function, 244
Integration, 224
by parts, 287
by substitution, 292
completing the square, 283
difﬁcult functions, 279
integrand contains a derivative, 284
partial fractions, 294
radicals, 282
techniques, 278
trigonometric identities, 280
Intersection, 75
Interval, 48
closed, 48
half-open, 48
open, 48
Inverse
matrix, 167
trigonometric function, 85
Irrational number, 19
K
Kronecker, Leopold, 19
L
Laplace, Pierre-Simon, 173

332
Index
Laplacian expansion, 173
Legendre, Adrien-Marie, 31
Leibniz, Gottfried, 46
Limits, 215, 220
Linear function, 96
Local coordinates, 104
Logarithms, 44
Logic, 57
Logic identities, 62
Logical
connectives, 58
premise, 59
M
Material equivalence, 59
Matrices, 149
Matrix, 328
addition, 162
antisymmetric, 160
determinant, 157
diagonal, 176
dimension, 154
identity, 192
inverse, 167
multiplication, 153
notation, 154
null, 155
order, 154
orthogonal, 175
products, 163
rectangular, 167
scalar multiplication, 163
singular, 167
skew-symmetric, 160
square, 154, 166
subtraction, 162
symmetric, 159
trace, 156
transforms, 185
transpose, 157
unit, 155
Maxima, 255
Member, 5
Mersenne prime, 32
Mersenne, Marin, 32
Minima, 255
Mixed partial derivative, 262
Möbius, August, 104
Modus
ponens, 69
tollens, 68
Multiple-angle identities, 91
N
NAND, 70
Napier, John, 44
Natural number, 18
Negation, 58, 60
Negative number, 7
Non-associative algebra, 29
Non-commutative algebra, 28, 29
NOR, 70
NOT, 58, 70
Notation, 3
Null matrix, 155
Number
algebraic, 20, 32
arithmetic, 8
binary, 12
Cayley, 28
complex, 24
composite, 29
hexadecimal, 13
imaginary, 20
integer, 5, 19
line, 8
Mersenne, 32
natural, 18
negative, 7
octal, 12
perfect, 32
positive, 8
prime, 29
rational, 5, 19
real, 5, 20
transcendental, 20, 32
O
Octal number, 12
Odd function, 50
One-to-one correspondence, 33
Open interval, 48
OR, 58, 70
Oriented axes, 96
Origin, 95
Orthogonal matrix, 175
P
Partial derivative, 256
chain rule, 263
ﬁrst, 258
mixed, 262
second, 275
visualising, 259
Pascal, Blaise, 95

Index
333
Pascal’s triangle, 220
Peirce, Benjamin, 151
Peirce, Charles, 151
Perimeter relationships, 93
Perspective projection, 207
Pitch, 199
Placeholder, 7
Polar coordinates, 101
Polynomial equation, 20
Position vector, 133
Power functions, 52
Power series, 266
Prime number distribution, 31
Prime numbers, 29
Proof by cases, 70
Proposition, 59
necessary, 59
sufﬁcient, 59
Q
Quadratic
equation, 39
function, 96, 217
Quaternion, 27
R
Radian, 81
Range, 49, 85
Rational
coefﬁcients, 20
number, 5, 19
Real number, 5, 20
Rectangular matrix, 167
Reductio ad absurdum, 69
Relative complement, 75
Riemann, Bernhard, 311
Riemann sum, 311
Right-hand rule, 142
Roll, 199
Rotating about an axis, 201, 202
Row vector, 128, 155, 163
Russell, Bertrand, 37, 71
S
Scalar product, 135, 136
Secant, 84
Second derivative, 274
Series
power, 266
Taylor’s, 266
Set, 5, 72
absolute complement, 76
building, 74
cardinality, 72
empty, 72
intersection, 75
member, 72
relative complement, 75
union, 74
universal, 73
Simpliﬁcation, 65
Sine, 84
differentiating, 229
rule, 88
Singular matrix, 167
Skew-symmetric matrix, 160
Skolem, Thoralf, 71
Spherical polar coordinates, 102
Square matrix, 154, 166
Subset, 73
Sufﬁcient proposition, 59
Superset, 73
Symbolic logic, 57
Symmetric matrix, 159
T
Tangent, 84
Tautology, 60
Taylor, Brook, 266
Taylor’s series, 266
Theorem of
Pythagoras, 98, 99
Total derivative, 265
Trace, 156
Transcendental number, 20, 32
Transform
2D reﬂection, 189, 195
2D rotation about a point, 195
2D scaling, 187
2D shearing, 190
2D translation, 186
3D reﬂection, 202
3D scaling, 197
3D translation, 196
afﬁne, 194
geometric, 149
scale, 149
translate, 149
Transpose matrix, 157
Trigonometric
function, 82, 96
identities, 87
ratios, 82

334
Index
Trigonometric function
inverse, 85
Trigonometry, 81
Truth table, 57
Two’s complement, 18
U
Union, 74
Unit
normal vector, 142
vector, 134
Unit matrix, 155
Universal set, 73
V
Vector, 328
2D, 128
3D, 131
addition, 132
Cartesian, 135
column, 128, 152, 155, 163
magnitude, 130
normalising, 134
position, 133
product, 135, 137
row, 128, 155, 163
scaling, 132
subtraction, 132
unit, 134
Venn diagram, 72
Venn, John, 72
Vertices, 97
W
Warren, John, 127
Weierstrass, Karl, 215
Wessel, Caspar, 127
Whitehead, Alfred North, 37
X
XNOR, 70
XOR, 58, 70
xy-plane, 95
Y
Yaw, 199
Z
Zermelo, Ernst, 71
Zermelo-Fraenkel set theory, 71
Zero, 7
ZF, 71

