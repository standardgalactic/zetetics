RNN BASED INCREMENTAL ONLINE SPOKEN LANGUAGE UNDERSTANDING
Prashanth Gurunath Shivakumar1, Naveen Kumar2, Panayiotis Georgiou1, Shrikanth Narayanan1
1University of Southern California, Los Angeles, CA, USA
2Disney Research, Glendale, CA, USA
ABSTRACT
Spoken Language Understanding (SLU) typically comprises of an
automatic speech recognition (ASR) followed by a natural language
understanding (NLU) module. The two modules process signals in
a blocking sequential fashion, i.e., the NLU often has to wait for the
ASR to ﬁnish processing on an utterance basis, potentially leading
to high latencies that render the spoken interaction less natural. In
this paper, we propose recurrent neural network (RNN) based in-
cremental processing towards the SLU task of intent detection. The
proposed methodology offers lower latencies than a typical SLU sys-
tem, without any signiﬁcant reduction in system accuracy. We intro-
duce and analyze different recurrent neural network architectures for
incremental and online processing of the ASR transcripts and com-
pare it to the existing ofﬂine systems. A lexical End-of-Sentence
(EOS) detector is proposed for segmenting the stream of transcript
into sentences for intent classiﬁcation. Intent detection experiments
are conducted on benchmark ATIS, Snips and Facebook’s multilin-
gual task oriented dialog datasets modiﬁed to emulate a continuous
incremental stream of words with no utterance demarcation. We also
analyze the prospects of early intent detection, before EOS, with our
proposed system.
Index Terms— Incremental Processing, Online Processing,
Spoken Language Understanding, Intent Detection, Recurrent Neu-
ral Network
1. INTRODUCTION
With the proliferation of novel interactive technology applications
across domains ranging from entertainment to health, the use of
spoken language for enabling and supporting natural communica-
tion is becoming ever more important. Today, SLU ﬁnds applica-
tions in voice assistants, robot interactions, virtual agents, virtual &
augmented reality applications as well as in mediating human in-
teractions such as meetings. While the rapid development in SLU
techniques has led to a revolution in this ﬁeld, a lot still remains
to be bridged in terms of improving the “naturalness” of these in-
teractions. For example, achieving low latencies remains crucial to
achieve a sense of “naturalness” during conversations. Higher laten-
cies often result in turn-based disruptive conversations which creates
the impression of a transactional interaction [1, 2].
Typical gaps between human-human dyadic turns are of the or-
der of 200 ms [1–4]. In contrast, most ASRs rely on Inter Pausal
Units (IPU) that are upwards of 500 ms to reliably detect the “end
of utterance”. During interactions, this latency is often perceived as
computational delay due to speech recognition, whereas in reality
this delay can be avoided by use of incremental processing architec-
tures.
1email:pgurunat@usc.edu
Previous works [5–7] have investigated the stability of results
when using incremental hypotheses (partials) from a speech recog-
nition system. While these partials can be generated with low la-
tency, their semantic stability for downstream NLU tasks remains
challenging. Earlier attempts to alleviate this issue have used con-
ﬁdence score [8–10] to assess stability of predictions. Some works
have relied instead on auxiliary methods to predict turn-taking be-
havior in an agent [11–14].
Most of the research efforts in SLU in the NLP community as-
sume ofﬂine NLU processing, i.e., (i) ideal, perfect utterance bound-
aries, and (ii) error-less transcriptions void of any speech recognition
errors [15–23]. Leading NLU systems are based on RNN [15–18],
convolutional neural network (CNN) [22] and sequence-to-sequence
architectures [20] with attention modeling [20–22]. Joint modeling
of SLU tasks like intent detection, language modeling (LM), slot-
ﬁlling, and named entity detection are found to be beneﬁcial [15–
21, 23]. Character level features have also been proposed to achieve
state-of-the-art performance in benchmark tasks [18, 22]. Whereas,
research efforts in the speech community assume ofﬂine ASR and
ofﬂine NLU processing, i.e., (i) ideal, perfect utterance boundaries,
and (ii) ASR errors in transcriptions [24–27]. To handle ASR errors,
joint SLU-ASR adaptation [28] and joint learning of SLU and ASR
error correction [26, 27] have been studied. Better feature repre-
sentations involving acoustic information are beneﬁcial in handling
ASR errors for SLU [24, 25]. Although, there have been a few at-
tempts at end-to-end SLU directly from speech signals, the perfor-
mance are not quite up-to the standards achieved by the traditional
approaches involving ASR and NLU [29].
Few research efforts have tried to incorporate incremental pro-
cessing on ASR [5–7]. Even fewer efforts have been made in the
context of ASR incremental processing for SLU. The authors in [19]
proposed a joint online SLU and LM system using RNN. Although,
their proposed model is capable of outputting the intent class poste-
riors for each time-step, the posteriors were not used for intent pre-
diction itself, but were fed back to the hidden state of the RNN. Only
the intent at the last time-step of the input sequence is used. More-
over, the evaluations were made in an ofﬂine fashion assuming each
input sequence equals a single sentence and assuming the sentence
boundaries are known a-priori.
To the best of our knowledge, there has been no prior work deal-
ing with incremental online SLU employing RNN with evaluations
conducted in a truly online sense. In this work, we setup the online
incremental SLU processing along with (i) detection of utterance
boundaries, and (ii) presence of erroneous ASR transcriptions. The
proposed system is capable of recognizing intents on an arbitrarily
long sequence of words with no sentence or utterance demarcations.

How
Are
You
Doing?
How
Are
You
Doing?
a
a
a
a
How
Are
You
Doing?
e
e
e
e
ASR
Output
Incremental 
EOS Output
How
Are
You
Doing?
Offline
Intent 
Detection
i
i
i
i
How
Are
You
Doing?
i
i
i
i
How
Are
You
Doing?
p
p
p
p
Signal
End
Offline Intent 
Detection
w/ Oracle EOS
Proposed
Incremental Online 
SLU Output
A
D
B
C
Speech Signal
Alignment
ASR Output
Offline Intent
Detection on ASR
Incremental EOS 
Detection on ASR 
Incremental EOS + 
Offline Intent
Detection on ASR
Incremental EOS + 
Incremental Intent 
Detection on ASR
Fig. 1: Time-line of the entire ASR-SLU pipeline and latency implication of ofﬂine versus incremental online systems (a, i, e, p are the latencies
of ASR, intent classiﬁer, EOS detector, and the latency of the proposed system respectively. a, i, e, p assumed to be constant for simplicity)
2. NEED FOR INCREMENTAL ONLINE PROCESSING
To motivate the need for incremental online processing, in a real-life,
real-time processing system, we present 2 scenarios:
Scenario 1: Endpoint-based Processing: In a real-time applica-
tion scenario, the ASR receives a stream of continuous speech signal
and outputs the corresponding transcriptions in real-time. Due to the
computational complexity and memory constraints, most ASRs typ-
ically operate by chunking and processing the speech in segments.
This process is often referred to as end-pointing, and is usually deter-
mined based on different heuristics related to duration of IPUs, with
the goal to minimize disruption during speech. Finally, the ASR
outputs the transcript corresponding to each speech segment. In this
scenario, the ASR is tuned for real-time application, by varying the
parameters for end-pointing, often in a heuristic way. As a result,
any application operating on the output of the ASR needs to wait at
least until end-pointing, which gives rise to a fundamental bottleneck
in latency.
Scenario 2: Incremental Processing: Alternatively, during ASR
decoding, intermediate querying of ASR output transcript is possi-
ble. This involves computation of the best path over intermediate, in-
complete decoded lattices (see example in Fig 2). Although, there is
a possibility of the best path deviating between the complete and in-
complete lattice decoding, the deviation is expected to be minimum
in robust ASR systems. Moreover, the prospects of using incremen-
tal outputs of the ASR is attractive. In this scenario, the downstream
application has no constraints of waiting until end-pointing and is
free to process the ASR transcripts in an incremental manner. This
also allows for online processing for downstream application in ad-
dition to the ASR itself for optimal latency.
2.1. Incremental Processing for SLU Tasks
In the context of SLU, under Scenario 1, the NLU module is run in
an ofﬂine fashion, processing an utterance at each end-point of the
ASR. The timeline is illustrated in Fig 1.A & 1.B. It is evident from
the timeline that ofﬂine NLU processing has higher latency implica-
tions. Moreover, the end-pointing algorithm itself has a bearing on
the performance of the NLU, since end-pointing deﬁnes the utter-
ance boundaries fed to the NLU. There have been several research
efforts in predicting optimal end-point for an ASR [30–32]. Thus,
the NLU has to deal with the errors from: (i) ASR, (ii) end-point de-
tection, and (iii) ASR errors due to sub-optimal end-pointing (Note,
suboptimal end-pointing especially false alarms can result in errors
during recognition itself [30]). The aggregated errors often lead to
degradation in the overall performance of SLU.
In this paper, we propose an SLU system under Scenario 2,
where the NLU module can also be run in an online fashion, in par-
allel with the ASR. Additionally, we also propose incremental pro-
cessing independent of ASR end-pointing and a lexical EOS detec-
tion module for utterance boundaries, operating on the ASR output.
This allows for lenient end-pointing schemes (emphasis on lower
false positives) since end-pointing no longer deﬁnes latency. This
comes with the advantage that the NLU has to deal with errors from
only the ASR phase. However, note that the EOS module might still
introduce errors into the system, due to improper segmentation. The
time-line of the incremental processing system is illustrated in the
Fig 1.D. It is apparent that there are signiﬁcant latency advantages
associated with the proposed system.
2.2. Implications on Neural Network Architectures
The online incremental nature of the NLU module imposes certain
design constraints on the architecture of the recurrent neural net-
works. One of the fundamental restrictions due to the online nature
of the problem is the use of only unidirectional LSTM. This is be-
cause, we don’t have access to the future time steps for the backward
step in-case of bidirectional LSTM.
3. PROPOSED TECHNIQUES
3.1. Baseline Ofﬂine RNN
The baseline system consists of a vanilla RNN-LSTM architecture
which consumes a sequence of words and outputs a single deci-
sion similar to most of the works [15–22] with the exception that
the LSTM is unidirectional as per Section 2.2. The network is re-
ferred to as ofﬂine, since the input needs to be segmented such that
each utterance has a single intent label during training. However to
assess its performance for the online task, during evaluation, we de-
rive the output per each time step by sharing the output layer over all
the time-steps. The latency implication of baseline is illustrated in
Fig 1.A.

LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
Time-distributed Linear Output Layer with Softmax
what
what
    is
what 
    is
the
what 
    is
the
  seating
what
    is
the
  seating
  capacity
what
    is
the
  seating
  capacity
               of
what
    is
the
  seating
  capacity
      of
   the
…..
what is the seating capacity of the 757 tell me about the M80 aircraft
(flight_capacity)
      (aircraft)
Embedding Layer
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
Time-distributed Linear Output Layer with Softmax
Pt
intent
Pt+1
intent
Pt+2
intent
Pt+3
intent
Pt+4
intent
Pt+5
intent
Pt+6
intent
Pt
eos
Pt+1
eos
Pt+2
eos
Pt+3
eos
Pt+4
eos
Pt+5
eos
Pt+6
eos
Pt
eos
Pt+1
eos
Pt+2
eos
Pt+3
eos
Pt+4
eos
Pt+5
eos
Pt+6
eos
Fig. 2: Proposed LSTM Architecture (simulation of ASR partials,
bold words are increments. 2 utterances, 2 intents)
3.2. Online RNN Classiﬁcation
The online version of the system is similar architecture wise to the
ofﬂine model with the exception that each input time-step has a cor-
responding output. The network is referred to as online, since it can
process arbitrary length sequences with sequences of multiple intent
labels both during training and testing. The system is trained with in-
put sequences comprising multiple sentences/utterances which pos-
sibly map to a sequence of multiple different intents. During train-
ing, since each utterance has a single label, we mask the loss function
to compute only at utterance boundaries. The loss function is:
Loss = −PT
t=1 IEOS
PC
c=1 yo,c log p(ˆyo,c)
(1)
where t is the time-step, T is the sequence length, IEOS is the in-
dicator function which is 1 for oracle EOS, c is the intent class, C
is the total number of intent classes, yo,c is the indicator function
which is 1 if the observation o belongs to class c, and p(ˆyo,c) is the
softmax probability prediction. The latency of the online system is
pictured in Fig 1.D.
3.3. End-of-Sentence Detection
Both the baseline Ofﬂine and Online RNN system do not have a
sense of utterance boundaries during the evaluation phase. Thus, as
described in Section 2.1, we train an independent EOS classiﬁer. The
architecture of the EOS classiﬁer is similar to the online RNN intent
classiﬁer with 2 exceptions: (i) sigmoid activation at output, and (ii)
binary cross-entropy loss. In conjunction with the EOS system the
latency of baseline ofﬂine system and online system is pictured in
Fig 1.B.
3.4. Online Multi-task Learning
Additionally, we propose to model both the tasks i.e., intent and EOS
detection jointly in a multi-task learning framework. Here, both the
tasks share the embedding layer, but have task speciﬁc LSTM and
time-distributed linear output layer (Fig 2 without feedback). The
loss optimized is:
Loss = −
T
X
t=1
IEOS(
C
X
c=1
yo,c log p(ˆyo,c))
+ ye log pe + (1 −ye) log(1 −pe)
(2)
where ye is the oracle EOS label, pe is the predicted output of the
network, the rest of the parameters comply with Eq.1. The proposed
system has two advantages: (i) the latency is reduced since both tasks
are modeled together, and (ii) joint learning of two tasks can beneﬁt
each other as in [15–21].
3.5. Online Multi-task with EOS Feedback
Further, within the multi-task learning framework, we experiment
with feeding back the EOS output back to the intent detection LSTM.
The embedding layer is shared between the two tasks, with task spe-
ciﬁc LSTM layers and time-distributed linear output layers. The
predicted EOS output is concatenated along with the input features
from the embedding layer and fed to the intent LSTM (see Fig 2).
The loss function is identical to the multi-task learning in equation 2.
With this proposed framework, we believe that explicitly feeding the
EOS markers to the intent detection system could provide perfor-
mance beneﬁts. The latency is identical to the multi-task learning
framework described in section 3.4.
4. DATA & EXPERIMENTAL SETUP
4.1. Data
We employ the ATIS (Airline Travel Information Systems) bench-
mark dataset [33] as the primary dataset for the intent detection ex-
periments. The dataset consists speech recordings of humans speak-
ing to an automated airline travel inquiry system. The speech record-
ings are accompanied by manual transcriptions of the spoken queries
with annotated intent labels. The data consists of 17 unique intent
categories (in-case of multiple intents, more frequent intent is as-
signed). Our data setup is identical to [17, 21, 25].
We also perform experiments on Snips dataset released as a part
of the Snips voice platform [34] and Facebook’s multilingual task
oriented dialog dataset [35]. With Snips we hope to prove the ef-
ﬁcacy of the proposed system over multiple domains and extensive
vocabulary. Snips contains 7 intent categories. The Snips database
split is consistent with previous works [21, 23]. While ATIS is re-
stricted to a single domain of air travel, Snips involves data spanning
multiple domains. Snips is also characterized by a much larger vo-
cabulary (11420 words) compared to ATIS (869 words).
With Facebook’s multilingual task oriented dialog (FMTOD),
we explore the transferability of the proposed system over multiple
languages. FMTOD comprises of data spanning three languages,
English, Spanish and Thai database. Each language contains 12 in-
tent categories spanning 3 domains. The database split is consistent
with [35].
4.2. Experimental Setup
To simulate online continuous stream of transcriptions, random
number of samples from manual transcripts of the ATIS dataset
were stitched together without exceeding a maximum utterances
limit (example in Fig 2). This results in each sample containing
multiple utterances with sequence of multiple intent labels with no
demarcation. Instead of random sampling, we also tried concatenat-
ing utterances in sequence to incorporate any dialog ﬂow. However,
we found no signiﬁcant difference in performance. We believe the
nature of the database with independent utterance structures provide
no signiﬁcant dialog ﬂow. Hence random sampling is adopted. We
create multiple copies of each of the dataset by varying maximum
number of utterances from 1-10 for analysis.
The data contains

Manual
ASR
Fig. 3: Accuracy of ofﬂine, SOTA vs. proposed online models at
oracle EOS on Manual and ASR ATIS transcripts.
exactly the same information and is consistent with previous studies
involving ATIS [17, 21, 25] facilitating direct comparisons.
The RNN-LSTM models are trained on the samples from the
training set and the development set is used for hyper-parameter tun-
ing. Finally, the model with the best performance on the develop-
ment set is chosen and evaluated on the unseen, held out test set. A
single layer LSTM model is adopted with the embedding layer di-
mension set to 556 taking recommendations from [25]. The hidden
dimension of LSTM was tuned over 32, 64, 128, 256 units. The
dropout is varied over 0.1, 0.15, 0.20, 0.25, 0.3. The batch size is
set to 1 with each sequence of utterances viewed as a single sample.
The learning rate of 0.001 is used along with the Adam optimizer
and trained for a total of 20 epochs. The convergence of the model
is ensured by examining the loss and classiﬁcation accuracies.
For performing experiments on the speech recognition output,
we choose the ASpIRE model1 offered by Kaldi open source ASR
toolkit [36]. The ASpIRE model is trained Fisher English Corpus
[37] by augmenting the speech corpora using reverberation and noise
data. The acoustic model is based on the time-delay neural network
(TDNN) architecture which employs sub-sampling of the outputs to
reduce complexity [38]. The language model is a simple tri-gram
model trained on the Fisher English training corpus. An ofﬂine de-
coding of ASR is performed on the test split of the ATIS dataset
which yielded a word error rate (WER) of 15.89%.
5. RESULTS
5.1. Ofﬂine Model vs. Proposed Online Model
We ﬁrst validate the effectiveness of the proposed online model de-
scribed in section 3.2 against the baseline ofﬂine model (Section 3.1)
as well as the state-of-the-art (SOTA) in ofﬂine intent detection [23].
Fig 3 plots the results, i.e., Accuracy of the baseline ofﬂine model
versus the proposed online models over varying utterance lengths.
Three versions of the proposed online models are trained and eval-
uated with combinations of varying length of utterance sequences
(1, 3, 5, 10). The oracle end-of-utterance is assumed during the eval-
uation. From the plot, we observe that the accuracy of the baseline
ofﬂine model as well as the SOTA model is maximum for ofﬂine
decoding (utterance sequence length of 1) and drops with the in-
crease in utterance sequence lengths. Whereas, the performance of
the online models is slightly lower for utterance sequence of length
1http://kaldi-asr.org/models/m1
Oﬄine
Online
SOTA
Fig. 4: Accuracy of ofﬂine, SOTA vs. proposed online models at
oracle EOS on Snips and FMTOD datasets
FMTOD en: English; FMTOD es: Spanish; FMTOD th: Thai
of 1 compared to the baseline and SOTA model, but doesn’t degrade
with increasing length of utterance sequences. In the case of the
SOTA model, we observe that the degradation rate is relatively high.
We believe this is because the SOTA model is more optimally tuned
for ofﬂine evaluations and thus becomes relatively poorer for online
incremental purposes. The proposed model is able to reset the states
and effectively switch the intent outputs dynamically over sequences
of utterances without undergoing much degradation in performance,
thereby validating its use for incremental online SLU. An important
observation is that the model trained on utterance sequence length
of 3 performs just as well, generalizing to higher length sequences.
Thus, we only consider online models trained on utterance sequence
length of 3 from here onwards.
Fig 3 (see dotted lines) also plots the results obtained by evaluat-
ing models (trained using human annotated transcripts) on erroneous
ASR transcripts. As expected the errors in the transcripts cause the
accuracy to fall considerably across all the models. We believe the
proposed models are effected two folds by the presence of errors:
(i) directly on the intent prediction performance, and (ii) through
EOS prediction, which in turn also effects the intent prediction pro-
cess. However, the results portray similar advantages as observed in
section 5.1. The proposed systems are capable of maintaining high
accuracy on ASR transcripts over higher utterance sequence lengths,
while maintaining relatively close performance to baseline ofﬂine
system for utterance sequence length of 1. Moreover, the proposed
system on ASR transcripts outperform both the state-of-the-art and
baseline ofﬂine systems evaluated on clean manual transcripts for
utterance sequence lengths greater than 1.
5.2. Transferability over Multiple Databases
Fig 4 plots the results obtained on the Snips and FMTOD databases.
For Snips database we also compare with the SOTA system pre-
sented in [23].
Comparing the SOTA and ofﬂine systems with
the proposed model, the observations made are identical to those
made with ATIS database in section 5.1 for both Snips and FMTOD
databases. This proves the efﬁcacy of the proposed system over
different databases, multiple domains, multiple languages and dif-
ferent vocabulary sizes. We observe different degrees of degradation
among the three languages in FMTOD databases.
In our exper-
iments, we found the English language (Snips and FMTOD-en)
undergoes high degradation whereas the Thai language (FMTOD-
th) the least. We attribute this to the structure and the linguistic

utt=1
utt=3
utt=5
utt=10
Fig. 5: Accuracy of three proposed online models evaluated on oracle and predicted sentence boundaries
Oracle: Intent accuracy evaluated at oracle EOS boundaries; EOS: Intent accuracy evaluated at predicted EOS boundaries;
Oracle & EOS: Intent accuracy evaluated only at True positives of predicted EOS boundaries; ¬(Oracle) & EOS: Intent accuracy evaluated
at False positives of predicted EOS boundaries.
utt=1
utt=3
utt=5
utt=10
Fig. 6: Accuracy of three proposed online models evaluated on ASR transcripts at oracle and predicted sentence boundaries
Oracle: Intent accuracy evaluated at oracle EOS boundaries; EOS: Intent accuracy evaluated at predicted EOS boundaries;
Oracle & EOS: Intent accuracy evaluated only at True positives of predicted EOS boundaries; ¬(Oracle) & EOS: Intent accuracy evaluated
at False positives of predicted EOS boundaries.
characteristics of the languages.
5.3. EOS Predictions
Our experiments found that the EOS system performs consistently
over varying utterance sequence lengths, achieving an accuracy of
approximately 92% with no signiﬁcant performance differences be-
tween discussed architectures.
On using ASR outputs, the EOS prediction accuracy drops to ap-
proximately 89%. As earlier, no signiﬁcant performance differences
were noted across different architectures.
5.4. EOS Evaluations
Further, evaluations of the online intent module is performed by re-
placing the oracle EOS with predicted outputs of the EOS module.
With this, there are possibilities of false positives / negatives during
EOS detection, hence, we also compute the accuracy of the intent
classiﬁcation when EOS predictions match with Oracle EOS (Or-
acle&EOS). The results of three proposed models are presented in
Fig 5 in terms of accuracies over varying utterance sequence lengths.
Note, for the online system without multi-task architecture, the re-
sults are evaluated by running the independent EOS detection sys-
tem to obtain the estimated EOS boundaries. Comparing with the
ofﬂine version (see Fig 3), the online version outperforms by a large
margin evaluated on predicted EOS boundaries, except for utterance
sequence length of 1 (ofﬂine evaluation). Moreover, whenever the
EOS predictions match with Oracle EOS (see Oracle&EOS in Fig 5),
the performance of the proposed systems are particularly high which
is encouraging. This underlines the practicality of proposed online
SLU in conjunction with EOS predictor for incremental online pro-
cessing.
Evaluations on the ASR transcripts are presented in Fig 6. The
results portray similar observations as earlier, i.e., even when eval-
uating the intent predictions at predicted EOS boundaries, the per-
formance of the proposed systems have drastic advantages over the
baseline and SOTA systems (except for ofﬂine evaluation).
5.5. Multi-task Frameworks
From Fig 5 (Oracle, EOS and Oracle&EOS), it is apparent that the
multi-task frameworks provide better accuracies for intent detection.
However, the EOS detection performance is similar to the indepen-
dent EOS system. Comparing the vanilla multi-task model and the
feedback version, the feedback version achieves better accuracies
for utterance sequence lengths of 1 and 3. We believe this is because

 
 
 
 0.1  
 
 
 0.2  
 
 
 0.3  
 
 
 0.4  
 
 
 0.5  
 
 
 0.6  
 
 
 0.7  
 
 
 0.8  
 
 
 0.9  
 
 
 
1
Normalized Utterance Length
Offline
Online
MT
MT-FB
Earliest Detection Distribution
100
102
Fig. 7: Early Detection Distribution: Ofﬂine: Baseline Ofﬂine RNN; Online: Proposed Online RNN + Independent EOS Detection; MT:
Proposed Online Multi-task RNN; MT+FB: Proposed Online Multi-task RNN with Feedback;
Normalized Utterance Length of 0 refers to the beginning of the sentence and 1.0 refers to EOS; Color map indicate the counts of correct
early predictions; Red markings are averages;
the feed-back version is more sensitive to training data which was
limited to utterance sequence lengths of 3.
However, with the evaluation on ASR outputs presented in Fig 6
the advantages of the multi-task model with EOS feedback is much
more evident. We ﬁnd that the multi-task model with EOS feedback
consistently outperforms the other architectures for all the utterance
sequence lengths. We believe this is because the system relies on
EOS information more in the presence of the noise (errors) in ASR
transcripts. And since the EOS predictor is more robust to errors
in the transcript (see section 5.3: an absolute drop of approximately
3% with EOS predictions versus > 6% absolute drop in performance
with intent detection), it is able to provide the crucial information
required to switch intent states at utterance boundaries.
5.6. Early Prediction of Intents
One of the advantages of the online SLU is the potential to predict
the intent prematurely. To evaluate the prospects of this, we calculate
the accuracy of intent predictions during false positives of the EOS
detection (see ¬(Oracle)&EOS in Fig 5 and Fig 6). We observe that
the accuracy of intent detection at false positives to be high and close
to the accuracy at Oracle EOS. This implies that the intent detection
is accurate even before the EOS, thereby hinting at possibilities of
early prediction.
Additionally, we analyze the earliest time (in terms of number of
words) the network starts predicting the correct intent. We obtain the
early detection distribution normalizing over utterance length and
then over class distribution (see Fig 7). The x-axis corresponds to
normalized utterance length (0.0 is start of utterance; 1.0 is EOS)
and the heat-map correspond to the number of utterances. We ob-
serve an acute peak at 1.0 for ofﬂine system, suggesting the majority
of intents are detected correctly only at EOS. However, for the pro-
posed system, the peak at 1.0 is less pronounced and the distribution
is concentrated relatively more towards smaller values, thus sugges-
tive of earlier detections. We ﬁnd that the early detections with our
proposed models are statistically signiﬁcant (p-value<0.005) com-
pared to ofﬂine system. Moreover, the proposed multi-task model
is capable of earlier detections (signiﬁcant, p-value<0.05) than the
online model.
6. CONCLUSION
In this paper, we motivate the need for incremental and online pro-
cessing for SLU. The low latency proﬁle, prospects of early intent
detection, independence from ASR end-pointing makes the approach
attractive. We deﬁne the incremental online SLU as real-time spo-
ken intent inference on a continuous streaming sequence of utter-
ances with no sentence demarcation. We demonstrate that the typi-
cal ofﬂine approaches to SLU are unsuitable for incremental online
processing. Multiple approaches to online SLU are proposed based
on RNN intent classiﬁcation. For determining the sentence bound-
aries, EOS detection is performed. Multi-task learning is proposed
to better model the EOS and the intent detection jointly. Proposed
models enable switching of intent states implicitly, accompanying
estimation of sentence boundaries.
Final results of the proposed
techniques are indicative of performance approaching the accura-
cies of ofﬂine SLU. Sustained advantages on erroneous ofﬂine ASR
transcripts show promise towards real-world application.
In the future, we would like to make evaluations on top of a real-
time online ASR both in terms of latency proﬁling and accuracy pro-
ﬁling. The impact of ASR errors for ofﬂine versus the proposed sys-
tem is worthy of investigation. Most importantly, we would like to
compare the performance between the ASR end-pointing based SLU
versus the proposed incremental processing. Finally, we intend to
employ more complex neural network architectures, include charac-
ter encodings and extend the framework to other SLU tasks like slot-
ﬁlling, named-entity detection and assess the impact of incremental
online processing. Developing heuristics for early prediction of spo-
ken language intent is also an area of interest. Different weighting
schemes on the loss function can be explored. Particularly, giving
less importance to the intent at the beginning of the utterance and in-
creasing weights along the sentence length could make the training
more efﬁcient. Conversely, providing more weight towards the start
of the sentence could facilitate early detection.
References
[1] Stephen C Levinson, “Turn-taking in human communication–
origins and implications for language processing,” Trends in
cognitive sciences, vol. 20, no. 1, pp. 6–14, 2016.
[2] Tanya Stivers, Nicholas J Enﬁeld, Penelope Brown, Christina
Englert, Makoto Hayashi, Trine Heinemann, Gertie Hoymann,
Federico Rossano, Jan Peter De Ruiter, Kyung-Eun Yoon,
et al., “Universals and cultural variation in turn-taking in con-
versation,” Proceedings of the National Academy of Sciences,
vol. 106, no. 26, pp. 10587–10592, 2009.
[3] Stephen C Levinson and Francisco Torreira, “Timing in turn-
taking and its implications for processing models of language,”
Frontiers in psychology, vol. 6, pp. 731, 2015.
[4] Mattias Heldner and Jens Edlund, “Pauses, gaps and overlaps
in conversations,”
Journal of Phonetics, vol. 38, no. 4, pp.
555–568, 2010.

[5] Gernot A Fink, Christoph Schillo, Franz Kummert, and Ger-
hard Sagerer, “Incremental speech recognition for multimodal
interfaces,”
in IECON’98. Proceedings of the 24th Annual
Conference of the IEEE Industrial Electronics Society (Cat.
No. 98CH36200). IEEE, 1998, vol. 4, pp. 2012–2017.
[6] Ethan O Selfridge, Iker Arizmendi, Peter A Heeman, and Ja-
son D Williams, “Stability and accuracy in incremental speech
recognition,” in Proceedings of the SIGDIAL 2011 Conference.
Association for Computational Linguistics, 2011, pp. 110–119.
[7] Imre Attila Kiss and Hugh Evan Secker-Walker, “Incremen-
tal utterance processing and semantic stability determination,”
Oct. 16 2018, US Patent App. 10/102,851.
[8] David DeVault, Kenji Sagae, and David Traum, “Can i ﬁnish?:
learning when to respond to incremental interpretation results
in interactive dialogue,” in Proceedings of the SIGDIAL 2009
Conference: The 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue. Association for Computa-
tional Linguistics, 2009, pp. 11–20.
[9] David DeVault, Kenji Sagae, and David Traum, “Incremen-
tal interpretation and prediction of utterance meaning for in-
teractive dialogue,” Dialogue & Discourse, vol. 2, no. 1, pp.
143–170, 2011.
[10] David Traum, David DeVault, Jina Lee, Zhiyang Wang, and
Stacy Marsella,
“Incremental dialogue understanding and
feedback for multiparty, multimodal conversation,” in Inter-
national Conference on Intelligent Virtual Agents. Springer,
2012, pp. 275–288.
[11] Hatim Khouzaimi, Romain Laroche, and Fabrice Lefevre,
“Turn-taking phenomena in incremental dialogue systems,” in
Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, 2015, pp. 1890–1895.
[12] Gabriel Skantze,
“Towards a general, continuous model of
turn-taking in spoken dialogue using lstm recurrent neural net-
works,” in Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, 2017, pp. 220–230.
[13] Matthew Roddy, Gabriel Skantze, and Naomi Harte, “Inves-
tigating speech features for continuous turn-taking prediction
using lstms,” arXiv preprint arXiv:1806.11461, 2018.
[14] Ryo Masumura, Taichi Asami, Hirokazu Masataki, Ryo Ishii,
and Ryuichiro Higashinaka,
“Online end-of-turn detection
from speech based on stacked time-asynchronous sequential
networks.,” in Interspeech, 2017, vol. 2017, pp. 1661–1665.
[15] Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey Zweig,
“Joint semantic utterance classiﬁcation and slot ﬁlling with
recursive neural networks,” in 2014 IEEE Spoken Language
Technology Workshop (SLT). IEEE, 2014, pp. 554–559.
[16] Xiaodong Zhang and Houfeng Wang, “A joint model of intent
determination and slot ﬁlling for spoken language understand-
ing.,” in IJCAI, 2016, vol. 16, pp. 2993–2999.
[17] Dilek Hakkani-T¨ur, G¨okhan T¨ur, Asli Celikyilmaz, Yun-Nung
Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang, “Multi-domain
joint semantic frame parsing using bi-directional rnn-lstm.,” in
Interspeech, 2016, pp. 715–719.
[18] Young-Bum Kim, Sungjin Lee, and Karl Stratos,
“Onenet:
Joint domain, intent, slot prediction for spoken language un-
derstanding,” in 2017 IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU). IEEE, 2017, pp. 547–553.
[19] Bing Liu and Ian Lane,
“Joint online spoken language un-
derstanding and language modeling with recurrent neural net-
works,” in Proceedings of the 17th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue, Los Angeles,
Sept. 2016, pp. 22–30, Association for Computational Linguis-
tics.
[20] Bing Liu and Ian Lane, “Attention-based recurrent neural net-
work models for joint intent detection and slot ﬁlling,” in In-
terspeech 2016, 2016, pp. 685–689.
[21] Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo,
Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung Chen,
“Slot-gated modeling for joint slot ﬁlling and intent predic-
tion,”
in Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 2, 2018, pp.
753–757.
[22] Changliang Li, Liang Li, and Ji Qi, “A self-attentive model
with gate mechanism for spoken language understanding,” in
Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing, 2018, pp. 3824–3833.
[23] Haihong E, Peiqing Niu, Zhongfu Chen, and Meina Song, “A
novel bi-directional interrelated model for joint intent detection
and slot ﬁlling,” in Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, Florence, Italy,
July 2019, pp. 5467–5471, Association for Computational Lin-
guistics.
[24] Ryo Masumura, Yusuke Ijima, Taichi Asami, Hirokazu Masa-
taki, and Ryuichiro Higashinaka, “Neural confnet classiﬁca-
tion: Fully neural network based spoken utterance classiﬁca-
tion using word confusion networks,” in 2018 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 2018, pp. 6039–6043.
[25] Prashanth Gurunath Shivakumar, Mu Yang, and Panayiotis
Georgiou, “Spoken Language Intent Detection Using Confu-
sion2Vec,” in Proc. Interspeech 2019, 2019, pp. 819–823.
[26] Raphael Schumann and Pongtep Angkititrakul, “Incorporating
asr errors with attention-based, jointly trained rnn for intent
detection and slot ﬁlling,” in 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2018, pp. 6059–6063.
[27] Su Zhu, Ouyu Lan, and Kai Yu, “Robust spoken language un-
derstanding with unsupervised asr-error adaptation,” in 2018
IEEE International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP). IEEE, 2018, pp. 6179–6183.
[28] Pierre Gotab, Geraldine Damnati, Frederic Bechet, and Lionel
Delphin-Poulat, “Online slu model adaptation with a partial
oracle,” in Eleventh Annual Conference of the International
Speech Communication Association, 2010.

[29] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj
Kumar, Baiyang Liu, and Yoshua Bengio, “Towards end-to-
end spoken language understanding,” in 2018 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 2018, pp. 5754–5758.
[30] Roland Maas, Ariya Rastrow, Chengyuan Ma, Guitang Lan,
Kyle Goehner, Gautam Tiwari, Shaun Joseph, and Bj¨orn
Hoffmeister,
“Combining acoustic embeddings and decod-
ing features for end-of-utterance detection in real-time far-ﬁeld
speech recognition systems,” in 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2018, pp. 5544–5548.
[31] Baiyang Liu, Bjorn Hoffmeister, and Ariya Rastrow, “Accu-
rate endpointing with expected pause duration,” in Sixteenth
Annual Conference of the International Speech Communica-
tion Association, 2015.
[32] Roland Maas, Ariya Rastrow, Kyle Goehner, Gautam Tiwari,
Shaun Joseph, and Bj¨orn Hoffmeister, “Domain-speciﬁc ut-
terance end-point detection for speech recognition.,” in Inter-
speech, 2017, pp. 1943–1947.
[33] Charles T Hemphill, John J Godfrey, and George R Dodding-
ton,
“The atis spoken language systems pilot corpus,”
in
Speech and Natural Language: Proceedings of a Workshop
Held at Hidden Valley, Pennsylvania, June 24-27, 1990, 1990.
[34] Alice Coucke, Alaa Saade, Adrien Ball, Th´eodore Bluche,
Alexandre Caulier, David Leroy, Cl´ement Doumouro, Thibault
Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Ma¨el
Primet, and Joseph Dureau, “Snips voice platform: an em-
bedded spoken language understanding system for private-by-
design voice interfaces,” CoRR, vol. abs/1805.10190, 2018.
[35] Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike
Lewis, “Cross-lingual transfer learning for multilingual task
oriented dialog,”
in Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), Minneapolis, Minnesota, June 2019,
pp. 3795–3805, Association for Computational Linguistics.
[36] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-
get, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr
Motlicek, Yanmin Qian, Petr Schwarz, et al., “The kaldi speech
recognition toolkit,”
in IEEE 2011 workshop on automatic
speech recognition and understanding. IEEE Signal Process-
ing Society, 2011, number CONF.
[37] Christopher Cieri, David Miller, and Kevin Walker, “The ﬁsher
corpus: a resource for the next generations of speech-to-text.,”
in LREC, 2004, vol. 4, pp. 69–71.
[38] Vijayaditya Peddinti, Guoguo Chen, Vimal Manohar, Tom Ko,
Daniel Povey, and Sanjeev Khudanpur, “Jhu aspire system:
Robust lvcsr with tdnns, ivector adaptation and rnn-lms,” in
2015 IEEE Workshop on Automatic Speech Recognition and
Understanding (ASRU). IEEE, 2015, pp. 539–546.

