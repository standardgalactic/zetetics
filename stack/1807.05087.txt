arXiv:1807.05087v1  [cs.SI]  13 Jul 2018
Learning Graph Representations by Dendrograms
Thomas Bonald, Bertrand Charpentier
November 28, 2021
Abstract
Hierarchical graph clustering is a common technique to reveal the multi-scale structure of complex
networks. We propose a novel metric for assessing the quality of a hierarchical clustering. This metric
reﬂects the ability to reconstruct the graph from the dendrogram, which encodes the hierarchy. The
optimal representation of the graph deﬁnes a class of reducible linkages leading to regular dendrograms
by greedy agglomerative clustering.
1
Introduction
Many datasets have a graph structure. Examples include infrastructure networks, communication networks,
social networks, databases and co-occurence networks, to quote a few. These graphs often exhibit a complex,
multi-scale structure where each node belong to many groups of nodes, so-called clusters, of diﬀerent sizes.
Hierarchical graph clustering is a common technique to reveal the multi-scale structure of complex net-
works. Instead of looking for a single partition of the set of nodes, as in usual clustering techniques, the
graph is represented by a hierarchical structure known as a dendrogram, which can then be used to ﬁnd
relevant clusterings at diﬀerent resolutions, by suitable cuts of this dendrogram.
We address the issue of the quality of a dendrogram representing a graph. For usual clustering, the
quality of a partition is commonly assessed through its modularity, which corresponds to the proportion of
edges within clusters, compared to that of a random graph [7]. For hierarchical clustering, a cost function
has recently been proposed by Dasgupta [3] and extended by Cohen-Addad and his co-authors [2]; it can
be viewed as the expected size of the smallest cluster (as induced by the hierarchy) containing two random
nodes. In this paper, we propose a quality metric based on the ability to reconstruct the graph from the
dendrogram. The optimal representation of the graph deﬁnes a class of reducible linkages leading to regular
dendrograms by greedy agglomerative clustering.
In the next section, we introduce the sampling distributions of nodes and clusters induced by the graph;
these play a central node in our approach.
We then formalize the problem of graph representation by
a dendrogram. Our quality metric follows from the characterization of the optimal solution in terms of
graph reconstruction. The corresponding hierarchical graph clustering algorithms are then presented and
interpreted in terms of modularity.
2
Sampling distribution
Consider a weighted, undirected, connected graph G = (V, E) of n nodes, without self-loops. Let w(u, v) be
equal to the weight of edge u, v, if any, and to 0 otherwise. We refer to the weight of node u as:
w(u) =
X
v∈V
w(u, v).
We denote by w the total weight of nodes:
w =
X
u∈V
w(u) =
X
u,v∈V
w(u, v).
1

Similarly, for any sets A, B ⊂V , let
w(A, B) =
X
u∈A,v∈B
w(u, v),
and
w(A) =
X
u∈A
w(u).
Node sampling.
The weights induce a probability distribution on node pairs:
∀u, v ∈V,
p(u, v) = w(u, v)
w
,
with marginal distribution:
∀u ∈V,
p(u) =
X
v∈V
p(u, v) = w(u)
w
.
The joint distribution p(u, v) is the relative frequency of moves from node u to node v by a random walk in
the graph, with transition probability:
∀u, v ∈V,
p(v|u) = p(u, v)
p(u)
= w(u, v)
w(u) .
Cluster sampling.
For any partition P of V into clusters, the weights induce a probability distribution
on cluster pairs:
∀A, B ∈P,
p(A, B) = w(A, B)
w
,
with marginal distribution:
∀A ∈P,
p(A) =
X
B∈P
p(A, B) = w(A)
w
.
The joint distribution p(A, B) is the relative frequency of moves from cluster A to cluster B by the random
walk.
3
Representation by a dendrogram
Assume the graph G is represented by a dendrogram, that is a rooted binary tree T whose leaves are the
nodes of the graph, V . We denote by I the set of internal nodes of the tree T . For each i ∈I, a positive
number d(i) is assigned to node i, corresponding to its height in the dendrogram. We assume that the
dendrogram is regular in the sense that d(i) ≥d(j) if i is an ancestor of j in the tree. For each i ∈I, there
are two subtrees attached to node i, with sets of leaves A and B; these sets uniquely identify the internal
node i so that we can write i = (A, B) and d(i) = d(A, B).
Ultrametric.
The dendrogram deﬁnes a metric on V : for each u, v ∈V , we deﬁne d(u, v) = d(i) where i
is the closest common ancestor of u and v in the dendrogram. This is an ultrametric in the sense that:
∀u, v, x ∈V,
d(u, v) ≤max(d(u, x), d(v, x)),
Conversely, any ultrametric deﬁnes a dendrogram, which can be build from bottom to top by successive
peerings of the closest nodes.
2

Graph representation.
A natural question is whether, given the dendrogram, it is possible to generate
some graph ˆG that is “close” to the original graph G. Let π be some probability distribution on V representing
the prior information known about the relative node weights: the distribution π is uniform in the absence
of such information and is equal to the sampling distribution p for a perfect knowledge of the node weights.
We look for the best representation of G by a dendrogram d in the sense of the reconstruction of G from
d, π, which can be viewed as the autoencoding scheme:
G 7−→d, π 7−→ˆG.
Since d(u, v) can be interpreted as a distance between u and v, its inverse corresponds to a similarity.
Thus we deﬁne the weight ˆw(u, v) between any two nodes u, v ∈V in the graph ˆG as:
ˆw(u, v) = π(u)π(v)
d(u, v) 1{u̸=v}.
Denoting by ˆw the total weight,
ˆw =
X
u,v∈V
ˆw(u, v),
we get the following node pair sampling distribution associated with ˆG:
ˆp(u, v) = ˆw(u, v)
ˆw
.
The distance between graphs G and ˆG can then be assessed through the Kullback-Leibler divergence between
the respective sampling distributions,
D(p||ˆp) =
X
u̸=v
p(u, v) log p(u, v)
ˆp(u, v).
Optimization problem.
Minimizing the Kullback-Leibler divergence D(p||ˆp) in ˆp is equivalent to mini-
mizing the cost function:
J(d) =
X
u̸=v
p(u, v) log d(u, v) + log

X
u̸=v
π(u)π(v)
d(u, v)

,
over all ultrametrics d deﬁned on V . Observe that J(αd) = J(d) for any α > 0, so that the best ultrametric
is deﬁned up to a multiplicative constant. Using the fact that d(u, v) = d(i), where i is the closest common
ancestor of u and v in the dendrogram, we get:
J(d) =
X
A,B:(A,B)∈I
p(A, B) log d(A, B) + log


X
A,B:(A,B)∈I
π(A)π(B)
d(A, B)

,
(1)
with
π(A) =
X
u∈A
π(u).
The problem of the best representation of G by a dendrogram now reduces to the optimization problem:
arg min
d J(d),
(2)
over all ultrametrics d on V .
3

4
Optimal representation
We seek to solve the optimization problem (2).
Optimal distances.
We ﬁrst assume that the underlying tree T of the ultrametric is given and look for
the best corresponding distance d. Let I be the set of internal nodes of the tree. Then for each (A, B) ∈I,
we get by the diﬀerentiation of (1) in d(A, B),
p(A, B)
d(A, B) = λπ(A)π(B)
d(A, B)2 ,
where
λ =


X
A,B:(A,B)∈I
π(A)π(B)
d(A, B)


−1
,
that is
d(A, B) = λπ(A)π(B)
p(A, B) .
(3)
Optimal tree.
Replacing d(A, B) by its optimal value (3) in (1), we deduce that the optimization problem
(2) reduces to:
arg max
T
X
A,B:(A,B)∈I
p(A, B) log
p(A, B)
π(A)π(B),
(4)
where I is the set of internal nodes of the tree T . The dendrogram is then fully determined by (3), for each
internal node (A, B) ∈I. The function to maximize in (4) is the Kullback-Leibler divergence between the
cluster pair distributions when nodes are sampled from the edges and independently from the distribution π,
respectively. It provides a meaningful objective function for hierarchical clustering, that can be interpreted
in terms of graph reconstruction.
A key diﬀerence between our objective function (4) and the cost functions proposed in the literature [3, 2]
lies in the entropy term:
X
A,B:(A,B)∈I
p(A, B) log p(A, B).
Removing this term yields the cost function:
X
A,B:(A,B)∈I
p(A, B)(log π(A) + log π(B)),
to be compared with usual cost functions, of the form:
X
A,B:(A,B)∈I
p(A, B)(π(A) + π(B)).
When π is the uniform distribution (no prior information on the node weights), these cost functions become
respectively:
X
A,B:(A,B)∈I
p(A, B)(log |A| + log |B|)
and
X
A,B:(A,B)∈I
p(A, B)(|A| + |B|).
The latter is Dasgupta’s cost function, equal to the expected size of the smallest cluster containing two
random nodes sampled from p.
The optimization problem (4) is NP-hard, just like minimizing Dasgupta’s cost function is NP-hard [3].
In the next section, we present heuristics based on greedy agglomerative algorithms for ﬁnding approximate
solutions to this optimization problem.
4

5
Hierarchical clustering
The optimal distances (3) suggest a greedy algorithm for solving the optimization problem (4). The algorithm
consists in starting from n clusters (one per node) and in successively merging the two closest clusters in terms
of inter-cluster distance (3). This is a usual agglomerative algorithm with linkage (inter-cluster similarity):
σ(A, B) =
p(A, B)
π(A)π(B).
(5)
The dendrogram is built from bottom to top, with distance σ(A, B)−1 attached to the internal node (A, B)
resulting from the merge of clusters A, B. The agglomeration relies on the following update formula:
Proposition 1 We have:
σ(A ∪B, C) =
π(A)
π(A ∪B)σ(A, C) +
π(B)
π(A ∪B)σ(B, C).
Proof. We have:
π(A)
π(A ∪B)σ(A, C) +
π(B)
π(A ∪B)σ(B, C) =
1
π(A ∪B)π(C)(p(A, C) + p(B, C)),
=
p(A ∪B, C)
π(A ∪B)π(C) = σ(A ∪B, C).
□
The update formula shows that the linkage (5) is reducible, in the sense that:
σ(A ∪B, C) ≤max(σ(A, C), σ(B, C)).
This inequality guarantees that the resulting dendrogram is regular (the sequence of distances attached to
successive internal nodes is non-decreasing) and that the corresponding distance on V is an ultrametric.
Moreover, the search of the clusters to merge can be done through the nearest-neighbor chain to speed up
the algorithm [5].
Linkage.
For π the uniform distribution (no prior information on the node weights), the linkage (5) is
proportional to the usual average linkage:
σ(A, B) ∝w(A, B)
|A||B| ,
corresponding to the density of the cut separating clusters A and B.
For π equal to p (perfect knowledge of the node weights), this is the linkage proposed in [1]:
σ(A, B) =
p(A, B)
p(A)p(B),
which can be interpreted in terms of sampling ratio as:
σ(A, B) = p(A|B)
p(A)
= p(B|A)
p(B) .
We refer to this linkage as modular in view of its relationship with modularity.
5

Modularity.
The modularity of any partition P of the set of nodes V is deﬁned by [7]:
Q =
X
C∈P
X
u,v∈C
(p(u, v) −p(u)p(v)).
This is the diﬀerence between the probabilities that two nodes belong to the same cluster when sampled
from the edges and independently from the nodes, in proportion to their weights. The former sampling
distribution depends on the graph while the latter depends on the graph through the node weights only.
A more general deﬁnition of modularity is:
Q =
X
C∈P
X
u,v∈C
(p(u, v) −π(u)π(v)).
Now the node sampling distribution π is any distribution with support V . For instance, it may be uniform
(no prior information on the node weights) or equal to p (the usual deﬁnition of modularity, where the
information on the node weights is known)1.
Maximizing modularity usually provides a unique clustering. To explore the multi-scale structure of real
graphs, it is common to introduce some positive resolution parameter γ that controls the respective weights
of both terms in the deﬁnition of modularity [8, 4, 6]. The modularity of partition P at resolution γ is
deﬁned by:
Qγ =
X
C∈P
X
u,v∈C
(p(u, v) −γπ(u)π(v)).
When γ →0, the second term becomes negligible and the best partition P is trivial, with a single cluster
equal to the set of nodes V . When γ →+∞, the second term becomes preponderant and the best partition
P has n clusters, one per node. Now the maximum resolution beyond which the best partition P has n
clusters is given by:
max
u̸=v
p(u, v)
π(u)π(v).
The ﬁrst node pair to be merged (at maximum resolution) is that achieving this maximum, which is the
closest pair in terms of linkage (5).
More generally, given some clustering C, the modularity of any coarser partition P at resolution γ is:
Qγ =
X
C∈P
X
A,B∈C:A,B⊂C
(p(A, B) −γπ(A)π(B)).
Again, the best partition is C when γ →+∞and the ﬁrst cluster merge occurs at resolution:
max
A,B∈C,A̸=B
p(A, B)
π(A)π(B).
The cluster pair to be merged (at maximum resolution) is that achieving this maximum, which the closest
pair A, B ∈C in terms of linkage (5).
The agglomerative algorithm based on linkage (5) can thus be
interpreted as the greedy maximization of modularity at maximum resolution.
References
[1] T. Bonald, B. Charpentier, A. Galland, and A. Hollocou. Hierarchical graph clustering based on node
pair sampling. In Proceedings of the 14th International Workshop on Mining and Learning with Graphs
(MLG), 2018.
1Another common interpretation of modularity is the diﬀerence between the proportions of edge weights within clusters
in the original graph and in some null model where nodes u and v are linked with probability π(u)π(v); for π the uniform
distribution (no prior information on the node weights), the null model is an Erd˝os-R´enyie graph while for π = p (perfect
knowledge of the node weights), the null model is the conﬁguration model.
6

[2] V. Cohen-Addad, V. Kanade, F. Mallmann-Trenn, and C. Mathieu. Hierarchical clustering: Objective
functions and algorithms. In Proceedings of ACM-SIAM Symposium on Discrete Algorithms, 2018.
[3] S. Dasgupta. A cost function for similarity-based hierarchical clustering. In Proceedings of ACM sympo-
sium on Theory of Computing, 2016.
[4] R. Lambiotte, J.-C. Delvenne, and M. Barahona. Random walks, Markov processes and the multiscale
modular organization of complex networks. IEEE Transactions on Network Science and Engineering,
2014.
[5] F. Murtagh and P. Contreras. Algorithms for hierarchical clustering: an overview. Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery, 2012.
[6] M. Newman. Community detection in networks: Modularity optimization and maximum likelihood are
equivalent. arXiv preprint, 2016.
[7] M. E. Newman and M. Girvan. Finding and evaluating community structure in networks. Physical review
E, 2004.
[8] J. Reichardt and S. Bornholdt. Statistical mechanics of community detection. Physical Review E, 74(1),
2006.
7

