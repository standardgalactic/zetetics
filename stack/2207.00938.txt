JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
1
Interpretable by Design: Learning Predictors by
Composing Interpretable Queries
Aditya Chattopadhyay, Stewart Slocum, Benjamin D. Haeffele,
RenÂ´e Vidal, Fellow, IEEE and Donald Geman, Life Senior Member, IEEE
Abstractâ€”There is a growing concern about typically opaque decision-making with high-performance machine learning algorithms.
Providing an explanation of the reasoning process in domain-speciï¬c terms can be crucial for adoption in risk-sensitive domains such
as healthcare. We argue that machine learning algorithms should be interpretable by design and that the language in which these
interpretations are expressed should be domain- and task-dependent. Consequently, we base our modelâ€™s prediction on a family of
user-deï¬ned and task-speciï¬c binary functions of the data, each having a clear interpretation to the end-user. We then minimize the
expected number of queries needed for accurate prediction on any given input. As the solution is generally intractable, following prior
work, we choose the queries sequentially based on information gain. However, in contrast to previous work, we need not assume the
queries are conditionally independent. Instead, we leverage a stochastic generative model (VAE) and an MCMC algorithm (Unadjusted
Langevin) to select the most informative query about the input based on previous query-answers. This enables the online
determination of a query chain of whatever depth is required to resolve prediction ambiguities. Finally, experiments on vision and NLP
tasks demonstrate the efï¬cacy of our approach and its superiority over post-hoc explanations.
Index Termsâ€”Explainable AI, Interpretable ML, Computer Vision, Generative Models, Information Theory
!
1
INTRODUCTION
I
N recent years, interpreting large machine learning mod-
els has emerged as a major priority, particularly for
transparency in making decisions or predictions that impact
human lives [1], [2], [3]. In such domains, understanding
how a prediction is made may be as important as achieving
high predictive accuracy. For example, medical regulatory
agencies have recently emphasized the need for computa-
tional algorithms used in diagnosing, predicting a progno-
sis, or suggesting treatment for a disease, to explain why a
particular decision was made [4], [5].
On the other hand, it is widely believed that there exists
a fundamental trade-off in machine learning between inter-
pretability and predictive performance [6], [7], [8], [9], [10].
Simple models like decision trees and linear classiï¬ers are
often regarded as interpretable1 but at the cost of potentially
reduced accuracy compared with larger black box models
such as deep neural networks. As a result, considerable ef-
fort has been given to developing methods that provide post-
hoc explanations of black box model predictions, i.e., given a
prediction from a (ï¬xed) model provide additional annota-
tion or elaboration to explain how the prediction was made.
As a concrete example, for image classiï¬cation problems,
one common family of post-hoc explanation methods pro-
duces attribution maps which seek to estimate the regions
of the image that are most important for prediction. This is
typically approached by attempting to capture the effect
or sensitivity of perturbations to the input (or intermediate
â€¢
The authors are with the Mathematical Institute for Data Science and the
Center for Imaging Science of The Johns Hopkins University, MD, 21218.
E-mail: {achatto1, sslocum3, bhaeffele, rvidal, geman}@jhu.edu
1. Although later in the paper we will discuss situations in which
even these simple models need not be interpretable.
features) on the model output [11], [12], [13], [14], [15], [16],
[17], [18]. However, post-hoc analysis has been critiqued for
a variety of issues [2], [19], [20], [21], [22], [23] (see also Â§2)
and often fails to provide explanations in terms of concepts
that are intuitive or interpretable for humans [24].
This naturally leads to the question of what an ideal
explanation of a model prediction would entail; however,
this is potentially highly task-dependent both in terms of the
task itself as well as what the user seeks to obtain from
an explanation. For instance, a model for image classiï¬ca-
tion is often considered interpretable if its decision can be
explained in terms of patterns occurring in salient parts of
the image [25] (e.g., the image is a car because there are
wheels, a windshield, and doors), whereas in a medical task
explanations in terms of causality and mechanism could be
desired (e.g., the patientâ€™s chest pain and shortness of breath
is likely not a pulmonary embolism because the blood D-
dimer level is low, suggesting thrombosis is unlikely). Note
that some words or patterns may be domain-dependent and
therefore not interpretable to non-experts, and hence what
is interpretable ultimately depends on the end user, namely
the person who is trying to understand or deconstruct the
decision made by the algorithm [26].
In addition to this task-dependent nature of model inter-
pretation, there are several other desirable intuitive aspects
of interpretable decisions that one can observe. The ï¬rst is
that meaningful interpretations are often compositional and
can be constructed and explained from a set of elementary
units [27]. For instance, words, parts of an image, or domain-
speciï¬c concepts [28], [29], [30] could all be a suitable basis
to form an explanation of a modelâ€™s prediction depend-
ing on the task. Moveover, the basic principle that simple
and concise explanations are preferred (i.e., Occamâ€™s razor)
suggests that interpretablity is enhanced when an explana-
arXiv:2207.00938v2  [cs.CV]  25 Nov 2022

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
2
tion can be composed from the smallest number of these
elementary units as possible. Finally, we would like this
explanation to be sufï¬cient for describing model predictions,
meaning that there should be no external variables affecting
the prediction that are not accounted for by the explanation.
Inspired by these desirable properties, we propose a
framework for learning predictors that are interpretable by
design. The proposed framework is based on composing a
subset of user-deï¬ned concepts, i.e., functions of the input
data which we refer to as queries, to arrive at the ï¬nal
prediction. Possible choices for the set of queries Q based
on the style of interpretation that is desired include:
1) Salient image parts: For vision problems, if one is
interested in explanations in terms of salient image
regions then this can be easily accomplished in our
framework by deï¬ning the query set to be a collection
of small patches (or even single pixels) within an image.
This can be thought of as a generalization of the pixel-
wise explanations generated by attribution maps.
2) Concept-based explanations: In domains such as med-
ical diagnosis or species identiï¬cation, the user might
prefer explanations in terms of concepts identiï¬ed by
the community to be relevant for the task. For instance,
a â€œCrowâ€ is determined by the shape of the beak,
color of the feathers, etc. In our framework, by simply
choosing a query for each such concept, the user can
easily obtain concept-based explanations (see Fig. 1(b)).
3) Visual scene interpretation: In visual scene under-
standing, one seeks a rich semantic description of a
scene by accumulating the answers to queries about the
existence of objects and relationships, perhaps generat-
ing a scene graph [31]. One can design a query set Q by
instantiating these queries with trained classiï¬ers. The
answers to chosen queries in this context would serve
as a semantic interpretation of the scene.
4) Deep neuron-based explanations: The above three
examples are query sets based on domain knowledge.
Recent techniques [30], [32], [33] have shown the ability
of different neurons in a trained deep network to act
as concept detectors. These are learnt from data by
solving auxiliary tasks without any explicit supervisory
signal. One could then design a Q in which each
query corresponds to the activation level of a speciï¬c
concept neuron. Such a query set will be useful for
tasks in which it is difï¬cult to specify interpretable
functions/queries beforehand.
Given a user-speciï¬ed set of queries Q, our framework
makes its prediction by selecting a short sequence of queries
such that the sequence of query-answer pairs provides a
complete explanation for the prediction. More speciï¬cally,
the selection of queries is done by ï¬rst learning a generative
model for the joint distribution of queries and output labels
and then using this model to select the â€œmost informativeâ€
queries for a given input. The ï¬nal prediction is made using
the Maximum A Posteriori (MAP) estimate of the output
given these query-answer pairs. Fig. 1(a) gives an illustra-
tion of our proposed framework, where the task is to predict
the bird species in an image and the queries are based on
color, texture and shape attributes of birds. We argue that
the sequence of query-answer pairs provides a meaningful
Input image ğ‘¥!"#
Green Jay with 
99% probability
Predicted bird species
Ask a sequence of interpretable queries about ğ‘¥!"#
ğ‘$. Has shape perching-like?
Yes
ğ‘%.
Has bill shape all-purpose? 
Yes
ğ‘&.
Has belly color yellow? 
Yes
ğ‘'.
Has upperparts color yellow?
No
ğ‘(.
Has throat color yellow?
No
ğ‘).
Has breast color black? 
Yes
ğ‘*.
Has belly color olive? 
Yes
(a)
(b)
What is ğ‘!?
ğ‘& ğ‘¥!"# = No
ğ‘& ğ‘¥!"# = Yes
Fig. 1. (a) An illustration of our proposed learning framework. The
prediction of a bird species is explained through a short sequence of
interpretable queries, (q1, q2, ..., q7), derived from a user-deï¬ned query
set of domain-speciï¬c attribute for birds. (b) Interpretable queries.
Each query in this case corresponds to a well-deï¬ned bird attribute. For
instance, q3 asks â€œDoes the bird have belly color yellow?â€. We visualize
some example images which evaluate to â€œYesâ€ and observe that all of
them correspond to birds with a yellow belly. Similarly, all images which
evaluate to â€œNoâ€ corresponds to birds which do not have a yellow belly.
explanation to the user that captures the subjective nature
of interpretability depending on the task at hand, and that
is, by construction, compositional, concise and sufï¬cient.
At ï¬rst glance, one might think that classical decision
trees [34], [35] based on Q could also produce interpretable
decisions by design. However, the classical approach to
determining decision tree branching rules based on the
empirical distribution of the data is prone to over-ï¬tting due
to data fragmentation. Whereas random forests [36], [37] are
often much more competitive than classical decision trees
in accuracy [38], [39], [40], they sacriï¬ce interpretability,
the very property we want to hardwire into our decision
algorithm. Similarly, the accuracy of a single tree can be
improved by using deep networks to learn queries directly
from data, as in Neural Decision Trees (NDTs) [41]. How-
ever, the opaqueness of the interpretation of these learnt
queries makes the explanation of the ï¬nal output, in terms
of logical operations on the queries at the internal nodes,
unintelligible. Figure 2 illustrates this with an example.
In this paper we make the following contributions;
â€¢ We propose a novel framework for prediction that is
interpretable by design. We allow the end-user to specify
a set Q of queries about input X and formulate learning
as the problem of selecting a minimal set of queries

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
3
Input image ğ‘¥!"#
ğ‘‘$
ğ‘‘%
ğ‘‘&
ğ‘‘'
ğ‘‘(
ğ‘‘)
ğ‘‘*
ğ‘™$
ğ‘™%
ğ‘™&
ğ‘™'
ğ‘™(
ğ‘™)
ğ‘™*
ğ‘™+
Green Jay with 
99% probability
Predicted Green Jay 
because d! x"#$ <
0.5, d% x"#$ â‰¥0.5
and d& x"#$ < 0.5
(a)
What is ğ‘‘$?
ğ‘‘! ğ‘¥"#$ â‰¥0.5
ğ‘‘! ğ‘¥"#$ < 0.5
(b)
Fig. 2. The interpretability of an explanation depends on how inter-
pretable the queries are. (a) An illustration of a Deep Neural decision
tree [41] trained on the CUB-2011 dataset of bird images. The bold
path denotes the trajectory the input image xobs takes through the tree.
Each di corresponds to an internal node of the tree and is a black-box
function/query learnt from data. Each li denotes a leaf and computes
the ï¬nal classiï¬cation for xobs. The prediction can be explained as a
conjunction of internal node functions, but is it really interpretable? (b)
Example images that get routed to the left sub-tree (d1 â‰¥0.5) and right
sub-tree (d1 < 0.5) of the root node. Notice that the interpretation of
d1 is not clear from these examples. Compare this to Fig. 1 where the
semantics of each query is unambiguous to the end-user.
from Q whose answers are sufï¬cient for predicting
output Y . We formulate this query selection problem as
an optimization problem over strategies that minimize
the number of queries needed on average to predict
Y from X. A prediction for Y is then made based
on the selected query-answer pairs, which provide an
explanation for the prediction that is by construction
interpretable. The set of selected query-answer pairs
can be viewed as a code for the input. However, a major
difference between our framework and coding theory
is that, due to the constraint of interpretability, Q is
a vanishingly small collection of the functions of X,
whereas coding theory typically considers Q to be all
possible binary functions of X.
â€¢ Since computing the exact solution to our optimization
problem is computationally challenging, we propose to
greedily select a minimal set of queries by using the
Information Pursuit (IP) algorithm [42]. IP sequentially
selects queries in order of maximum information gain
until enough evidence is gathered from the query-
answer pairs to predict Y . This sequence of query-
answer pairs serves as the explanation for predicting Y
from X. To ameliorate the computational challenge of
computing information gain for high-dimensional in-
put and query spaces, prior work [42] had assumed that
query answers were conditionally independent given
Y , an assumption that is largely inadequate for most
prediction tasks we encounter in practice. In this paper,
we propose a latent variable graphical model for the
joint distribution of queries and outputs, p(Q(X), Y ),
and learn the required distributions using Variational
Autoencoders (VAEs). We then use the Unadjusted
Langevin Algorithm (ULA) to generate samples re-
quired to carry out IP. This gives us a tractable algo-
rithm for any task and query set. To the best of our
knowledge, ours is the ï¬rst implementation of IP that
uses deep generative models and does not assume that
query answers are conditionally independent given Y .
â€¢ Finally, we demonstrate the utility of our framework on
various vision and NLP tasks. In binary image classiï¬-
cation using MNIST, Fashion-MNIST & KMNIST, and
bird species identiï¬cation using CUB-200, we observe
that IP ï¬nds succinct explanations which are highly
predictive of the class label. We also show, across var-
ious datasets, that the explanations generated by our
method are shorter and more predictive of the class la-
bel than state-of-the-art post-hoc explanation methods
like Integrated Gradients and DeepSHAP.
2
RELATED WORK
Methods for interpretable deep learning can be separated
into those that seek to explain existing models (post-hoc
methods) and those that build models that are interpretable
by design. Because they do not negatively impact perfor-
mance and are convenient to use, post-hoc explanations
have been the more popular approach, and include a great
diversity of methods.
Saliency maps estimate the contribution of each feature
through ï¬rst-order derivatives [11], [12], [16], [17], [43].
Linear perturbation-based methods like LIME [44] train a
linear model to locally approximate a deep network around
a particular input, and use the coefï¬cients of this model to
estimate the contribution of each feature to the prediction.
Another popular set of methods use game-theoretic Shapley
values as attribution scores, estimating feature contributions
by generating predictions on randomly sampled subsets
of the input [45]. We provide quantitative comparisons
between IP and these methods in Section 5.1.2. Recently,
there has been interest in concept-based analogues of these
methods that leverage similar approaches to measure the
sensitivity of a prediction to high-level, human-friendly
concepts as opposed to raw features [46], [47], [48].
Despite certain advantages, what all the above post-
hoc methods have in common is that they come with little
guarantee that the explanations they produce actually reï¬‚ect
how the model works [2]. Indeed, several recent studies [18],
[19], [20], [21], [22] call into question the veracity of these
explanations towards the trained model. Adebayo et al. [19]
show that several popular attribution methods act similar
to edge detectors and are insensitive to the parameters of
the model they attempt to explain! Yang et al. [20] ï¬nd that
these methods often produce false-positive explanations,
assigning importance to features that are irrelevant to the
prediction of the model. It is also possible to adversarially
manipulate post-hoc explanations to hide any spurious bi-
ases the trained model might have picked up from data [23].
Interpretability by design. These issues have motivated
recent work on deep learning models which are interpretable
by design, i.e., constrained to produce explanations that
are faithful to the underlying model, albeit with varying
conceptions of â€œfaithfulnessâ€. Several of these models are
constructed so they behave similarly to or can be well-
approximated by a classically interpretable model, such as
a linear classiï¬er [49], [50] or a decision tree [51]. This
allows for an approximately faithful explanation in raw
feature space. In a similar vein, Pillai & Pirsiavash [52] ï¬x

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
4
a post-hoc explanation method (e.g. Grad-CAM [16]), and
regularize a model to generate consistent explanations with
the chosen post-hoc method. However, our method does
not just behave like a fully interpretable model or generate
approximately faithful explanations, but rather it produces
explanations that are guaranteed to be faithful and fully
explain a given prediction.
Another approach to building interpretable models by
design is to generate explanations in terms of high-level,
interpretable concepts rather than in raw feature space,
often by applying a linear classiï¬er to a ï¬nal latent space of
concepts [25], [49], [53]. However these concepts are learned
from data, and may not align with the key concepts iden-
tiï¬ed by the user. For example, Prototypical Part Networks
[25] take standard convolutional architectures and insert a
â€œprototype layerâ€ before the ï¬nal linear layer, learning a
ï¬xed number of visual concepts that are used to represent
the input. This allows the network to explain a prediction in
terms of these â€œprototypeâ€ concepts. Since these prototypes
are learned embeddings, there is no guarantee that their
interpretation will coincide with the userâ€™s requirements.
Furthermore, these explanations may require a very large
number of concepts, while in contrast, we seek minimal-
length explanations to preserve interpretability.
Attention-based models are another popular family of
models that are sometimes considered interpretable by de-
sign [54], [55]. However, attention is only a small part of the
overall computation and can be easily manipulated to hide
model biases [56]. Moreover, the attention coefï¬cients are
not necessarily a sufï¬cient statistic for the model prediction.
Perhaps most similar to our work are Concept Bottleneck
Networks [24], which ï¬rst predict an intermediate set of
human-speciï¬ed concepts c and then use c to predict the
ï¬nal class label. Nevertheless, the learnt mapping from con-
cepts to labels is still a black-box. To remedy this, the authors
suggest using a linear layer for this mapping but this can
be limiting since linearity is often an unrealistic assump-
tion [27]. In contrast, our framework makes no linearity
assumptions about the ï¬nal classiï¬er and the classiï¬cation
is explainable as a sequence of interpretable query-answer
pairs obtained about the input (see Fig. 1(a)).
Neural networks and decision trees. Unlike the above
methods, which can be thought of as deep interpretable
linear classiï¬ers, our method can be described as a deep
decision tree that branches on responses to an interpretable
query set. Spanning decades, there has been a variety of
work building decision trees from trained neural networks
[29], [57], [58], [59] and using neural networks within nodes
of decision trees [41], [60], [61], [62]. Our work differs from
these in three important aspects. First, rather than allowing
arbitrary splits, we branch on responses to an interpretable
query set. Second, instead of using empirical estimates of
information gains based on training data (which inevitably
encounter data-fragmentation [63] and hence overï¬tting),
or using heuristics like agglomerative clustering on deep
representations [29], we calculate information gain from a
generative model, leading to strong generalization. Third,
for a given input, say xobs, we use a generative model to
compute the queries along the branch traversed by xobs in
an online manner. The entire tree is never constructed. This
allows for much very deep terminal nodes when necessary
to resolve ambiguities in prediction. As an example, for
the task of topic classiï¬cation using the HuffPost dataset
(Â§5.0.3), our framework asks about 199 queries (on average)
before identifying the topic. Such large depths are impossi-
ble in standard decision trees due to memory limitations.
Information bottleneck and minimal sufï¬cient statis-
tics. The problem of ï¬nding minimal-length, task-sufï¬cient
codes is not new. For example, the information bottleneck
method [64] seeks a minimum-length encoding for X that
is (approximately) sufï¬cient to solve task Y . Our concept
of description length differs in that we constrain the code
to consist of interpretable query functions rather than all
functions of the input, as in the information bottleneck
and classical information theory. Indeed, arbitrary subsets
of the input space (e.g. images) are overwhelmingly not
interpretable to humans.
Sequential active testing and hard attention. The infor-
mation pursuit (IP) algorithm we use was introduced in
[42] under the name â€active testing,â€ which sequentially
observes parts of an input (rather than the whole input
at once), using mutual information to determine â€where
to look next,â€ which is calculated online using on a scene
model. Sequentially guiding the selection of partial obser-
vations has also been independently explored in Bayesian
experimental design [65]. Subsequent works in these two
areas include many ingredients of our approach (e.g. gen-
erative models [31], [66] and MCMC algorithms [67]). Of
particular interest is the work of Branson et al. [68] which
used the CUB dataset to identify bird species by sequentially
asking pose and attribute queries to a human user. They
employ IP to generate the query sequence based on answers
provided by the user, much like our experiments in Â§5.0.2.
However, for the sake of tractability, all the above works
assume that query answers are independent conditioned on
Y . We do not. Rather, to the best of our knowledge, ours
is the ï¬rst implementation of the IP algorithm that uses
deep generative models and only assumes that queries are
independent given Y and some latent variable Z. This greatly
improves performance, as we show in Â§5.
The strategy of inference through sequential observa-
tions of the input has been recently re-branded in the
deep learning community as Hard Attention [69], [70], [71].
However, high variance in gradient estimates and scalability
issues have prevented widespread adoption. In the future,
we wish to explore how our work could inform more
principled and better-performing reward functions for Hard
Attention models.
Visual question answering. Although it may appear that
our work is also related to the Visual Question Answering
(VQA) literature [72], [73], [74], [75], [76], [77], we note that
our work addresses a very different problem. VQA focuses
on training deep networks for answering a large set of
questions about a visual scene. In contrast, our framework
is concerned with selecting a small number of queries to ask
about a given image to solve a task, say classiï¬cation. As we
move on to more complex tasks, an interesting avenue for
future work would involve using VQA systems to supply
answers to the queries used in our framework. However,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
5
this would require signiï¬cantly more complex generative
models that the ones considered here.
3
LEARNING INTERPRETABLE PREDICTORS
BY
COMPOSING QUERIES VIA INFORMATION PURSUIT
Let X and Y be the input data and the corresponding
output/hypothesis, both random variables assuming values
in X and Y respectively. In supervised learning, we seek to
infer Y from X using a ï¬nite set of samples drawn from
the joint distribution pXY (x, y).2 As motivated in Section 1,
useful explanations for prediction should be task-dependent,
compositional, concise and sufï¬cient. We capture such proper-
ties through a suitably rich set Q of binary functions q(x),
or queries, whose answers {q(x)}qâˆˆQ collectively determine
the task Y . More precisely, a query set Q is sufï¬cient for Y if
p(y | x) = p(y | {xâ€² âˆˆX : q(xâ€²) = q(x) âˆ€q âˆˆQ}).
(1)
In other words, Q is sufï¬cient for Y if whenever two inputs
x and xâ€² have identical answers for all queries in Q, their
corresponding posteriors are equal, i.e., p(y | x) = p(y | xâ€²).
Given a ï¬xed query set Q, how do we compose queries
into meaningful representations that are predictive of Y ?
We answer this by ï¬rst formally deï¬ning an explanation
strategy Ï€ and then formulating the task of composing
queries as an optimization problem.
Explanation strategies based on composing queries. An ex-
planation strategy, or just strategy, is a function, Ï€ : Kâˆ—â†’Q,
where Kâˆ—is the set of all ï¬nite-length sequences generated
using elements from the set K = {(q, q(x)) | q âˆˆQ, x âˆˆX}
of query-answer pairs. We require that Q contains a special
query, qST OP , which signals the strategy to stop asking
queries and output explÏ€
Q(x), the set of query-answer pairs
asked before qST OP . More formally, a strategy Ï€ is recur-
sively deï¬ned as follows; given input sample xobs
1) q1 = Ï€(âˆ…). The ï¬rst query is independent of xobs.
2) qk+1 = Ï€({qi, qi(xobs)}1:k). All subsequent queries de-
pend on the query-answer pairs observed so far for xobs.
3) If qL+1 = qST OP terminate, and return
explÏ€
Q(xobs) := {qi, qi(xobs)}1:L.
(2)
Notice that each qi depends on xobs, but we drop this
dependency in the notation for brevity. We call the number
of pre-STOP queries for a particular xobs as the expla-
nationsâ€™ description length and denote it by tÏ€(xobs) :=
|explÏ€
Q(xobs)|. Computing a strategy on xobs is thus akin
to traversing down the branch of a decision tree dictated
by xobs. Each internal node encountered along this branch
computes the query proposed by the strategy based on the
path (query-answer pairs) observed so far.
Notice also that we restrict out attention to sequential
strategies so that the resulting explanations satisfy the
property of being preï¬x-free.3 This means that explanations
generated for predictions made on an input signal x1 cannot
be a sub-part for explanations generated for predictions
on a different input signal x2; otherwise, the explanation
2. We denote random variables by capital letters and their realizations
with small letters.
3. The term preï¬x-free comes from the literature on instantaneous
codes in information theory.
Strategy (ğœ‹)
q1(xobs)
q1
xobs
;
Strategy (ğœ‹)
{qi, qi(xobs)}1:L
qST OP
explâ‡¡
Q(xobs) :
arg max
y2Y
p(y | explâ‡¡
Q(x))
ypred
iter. 1
iter. L+1
Asking queries for input xobs
Prediction for input xobs
Strategy (ğœ‹)
{q1, q1(xobs)}
q2(xobs)
q2
iter. 2
xobs
qi 2 Q
Queries from a 
user-defined 
query set
explâ‡¡
Q(xobs) := {q1, q1(xobs), . . .
. . . , qL, qL(xobs)}
Explanations are 
conjunctions of 
query-answer 
pairs gleaned 
from ğ‘¥!"#
Fig. 3. Schematic view of the overall framework for quantifying explana-
tions for predicting y from xobs. For details see Sec. 3.
procedure is ambiguous because a terminal node carrying
one label could be an internal node of a continuation leading
to a different label. Sequential strategies generate preï¬x-free
explanations by design. For non-sequential strategies, which
are just functions mapping an input X to a set of queries in
Q, it is not clear how to effectively encode the constraint of
generating preï¬x-free explanations.
Concise and approximately sufï¬cient strategies. In ma-
chine learning, we are often interested in solving a task
approximately rather than exactly. Let Q be sufï¬cient for Y ,
choose a distance-like metric d on probability distributions
and let Ïµ > 0. We propose the following optimization
problem to efï¬ciently compose queries for prediction,
min
Ï€
EX
|explÏ€
Q(X)|
 =: HÏµ
Q(X; Y )
(3)
s.t. EX[d
 p(Y | X), p(Y | explÏ€
Q(X)
] â‰¤Ïµ (Ïµ-Sufï¬ciency),
where the minimum is taken over all strategies Ï€. The
solution Ï€âˆ—to (3) provides a criterion for an optimal strategy
for the task of inferring Y approximately from X. The min-
imal expected description length objective, HÏµ
Q(X; Y ), ensures
the conciseness of the explanations, while the constraint
ensures approximate sufï¬ciency of the explanation. The
â€œmetricâ€ d on distributions could be KL-divergence, total
variation, Wasserstein distance, etc. The hyper-parameter Ïµ
controls how approximate the explanations are. The poste-
rior p(y | explÏ€
Q(xobs)) should be interpreted as the condi-
tional probability of y given the event
[xobs]Ï€,Q := {x âˆˆX | explÏ€
Q(x) = explÏ€
Q(xobs)}.
(4)
explÏ€
Q(X) can also be interpreted as a random variable
which maps input X to its equivalence class [X]Ï€,Q.
The ï¬nal prediction/inference for the input xobs is then
taken to be the usual MAP estimator, namely
ypred = arg max
yâˆˆY
p(y | explÏ€
Q(xobs)).
(5)
The sequence of query-answers streams obtained by Ï€ on
xobs serves as the explanation for ypred. One could also mon-
itor the posterior over the labels Y evolving as successive
queries get asked to gain more insight into the strategyâ€™s
decision-making process. Fig. 3 illustrates the overall frame-
work in detail.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
6
Information Pursuit: a greedy approximation. Unfortu-
nately, solving (3) is known to be NP-Complete and hence
generally intractable [78]. As an approximate solution to (3)
we propose to use a greedy algorithm called Information
Pursuit (IP). IP was introduced by Geman & Jedynak in 1996
[42] as a model-based, online construction of a single but
deep branch. The IP strategy, that is, Ï€ = IP, is recursively
deï¬ned as follows,
q1 = IP(âˆ…) = arg max
qâˆˆQ
I(q(X); Y )
(6)
qk+1 = IP({qi, qi(xobs)}1:k)) = arg max
qâˆˆQ
I(q(X); Y | SIP
k (xobs))
where I denotes mutual information and SIP
k (xobs) cor-
responds to the event {x
âˆˆ
X
|
{qi, qi(xobs)}1:k
=
{qi, qi(x)}1:k}. Ties in choosing qk+1 are broken arbitrarily
if the maximum is not unique.
The algorithm stops when there are no more informative
queries left in Q, that is, it satisï¬es the following condition:
qL+1 = qST OP
if max
qâˆˆQ I(q(X); Y | SIP
m(xobs)) â‰¤Ïµ
âˆ€m âˆˆ{L, L + 1, ..., L + T},
(7)
where hyper-parameter T
>
0 is chosen via cross-
validation. This termination criteria corresponds to taking
the distance-like metric d in (3) as the KL-divergence be-
tween the two distributions. Further details about the rela-
tion between this termination criteria and the Ïµ-Sufï¬ciency
constraint in (3) are provided in Appendix A.3. For tasks in
which Y is a function of X, a common scenario in many
supervised learning problems, we use a simpler alternative,
qL+1 = qST OP
if arg max
yâˆˆY
p(y | SIP
m(xobs)) â‰¥1 âˆ’Ïµ
âˆ€m âˆˆ{L, L + 1, ..., L + T}.
(8)
The key distinction between the information gain criteria
used in standard decision tree induction and IP is that the
former uses the empirical distributions to compute (6) while
the latter is based on generative models (as we will see
in Section 4). The use of generative models guards against
data fragmentation [63] and thus allows for asking longer
sequences of queries without grossly over-ï¬tting.
How does IP compare to the optimal strategy Ï€âˆ—? We begin
by characterizing the constraint in (3) in terms of mutual
information, the quantity that drives IP.
Proposition 1. Let SÏ€
k (X) be a random variable where any
realization SÏ€
k (xobs), xobs âˆˆX , denotes the event
SÏ€
k (xobs) := {xâ€² âˆˆX | {qi, qi(xobs)}1:k = {qi, qi(xâ€²)}1:k},
where qi is the ith query selected by Ï€ for input xobs. Here we use
the convention that SÏ€
0 (X) = â„¦(the entire sample space) and
SÏ€
l (X) = SÏ€
tÏ€(X)(X) âˆ€l > tÏ€(X). If Q is ï¬nite4 and d is taken
to be the KL-divergence, then objective (3) can be rewritten as
HÏµ
Q(X; Y ) := min
Ï€
EX
|explÏ€
Q(X)|

s.t.
Ï„ Ï€
X
k=1
I(Y ; SÏ€
k (X) | SÏ€
kâˆ’1(X)) â‰¥I(X; Y ) âˆ’Ïµ,
(9)
4. The assumption of Q being a ï¬nite set is benign. Many interested
applications can be addressed with a ï¬nite Q as we show in our
experiments.
where Ï„ Ï€ = max{tÏ€(x) : x âˆˆX} and tÏ€(X) is deï¬ned as the
number of queries selected by Ï€ for input X until qST OP .
See Appendix A.1 for a detailed proof. The objective in
(9) can be alternatively stated as,
max
Ï€
Ï„ Ï€
X
k=1
I(Y ; SÏ€
k (X) | SÏ€
kâˆ’1(X))
s.t. EX
|explÏ€
Q(X)|
 â‰¤Î³,
(10)
where Î³ > 0 is a user-deï¬ned hyper-parameter. From (10) it
is clear that the optimal strategy Ï€âˆ—would ask a sequence of
queries about X that would maximize the cumulative sum
of the mutual information each additional query provides
about Y , conditioned on the history of query-answers ob-
served so far, subject to a constraint on the average number
of queries that can be asked. As stated before, solving for Ï€âˆ—
is infeasible but a greedy approximation that makes locally
optimal choices is much more amenable.
Suppose that one has been given the answers to k queries
about a given input, the locally optimal choice would then
be to ask the most informative query about Y conditioned
on the history of these k query-answers observed. This
greedy choice at each stage gives rise to the IP strategy.
Obtaining approximation guarantees for IP is still an open
problem; however in the special case where Q is taken to be
the set of all possible binary functions of X, it is possible to
show that IP asks at most 1 query more than Ï€âˆ—on average.
More formally, we have the following result, whose proof
can be found in Appendix A.2.
Proposition 2. Let Y be discrete. Let ËœHQ(X; Y ) be the expected
description length obtained by the IP strategy. If H(Y |X) = 0
and Q is the set of all possible binary functions of X such that
H(q(X) | Y ) = 0 âˆ€q âˆˆQ, then
H(Y ) â‰¤ËœHQ(X; Y ) â‰¤H(Y ) + 1
(11)
Having posed the problem of ï¬nding explanations as an
optimization problem and proposed a greedy approxima-
tion to solving it, in the next section we propose a tractable
implementation of IP based on deep generative models.
4
INFORMATION
PURSUIT
USING
VARIATIONAL
AUTOENCODERS AND UNADJUSTED LANGEVIN
IP requires probabilistic models relating query-answers and
data to compute the required mutual information terms in
(6). Speciï¬cally, computing qk+1 in (6) (for any iteration
number k) requires computing the mutual information be-
tween q(X) and Y given the history SIP
k (xobs) till time k. As
histories become longer, we quickly run out of samples in
our dataset which belong to the event SIP
k (xobs). As a result,
non-parametric sample-based methods to estimate mutual
information (such as [79]) would be impractical. In this
section, we propose a model-based approach to address this
challenge for a general supervised learning task and query
set Q. In Â§5 we adapt this model to the speciï¬c cases where
Q is taken to be image patches or task-based concepts.
Information Pursuit Generative Model. To make learning
tractable, we introduce latent variables Z to account for all

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
7
the dependencies between different query-answers, and we
posit the following factorization of Q(X), Y, Z
pQ(X)ZY (Q(x), z, y)
(12)
=
Y
qâˆˆQ
pq(X)|ZY (q(x) | z, y)pY (y)pZ(z),
where Q(X) = {q(X) : q âˆˆQ}, and z and q(x) denote
realizations of Z and q(X) respectively. In other words,
we assume that the query-answers are conditionally in-
dependent given the label y and a latent vector z. The
independence assumption in (12) shows up ubiquitously in
many machine learning applications, such as the following.
1) q(X) as object presence indicators evaluated at
non-overlapping windows: Let Q be a set of non-
overlapping windows in the image X with q(X) being
a random variable indicating the presence of an object
at the qth location. The correlation between the qs is
entirely due to latent image generating factors Z, such
as lighting, camera position, scene layout, and texture
along with the scene description signal Y .
2) q(X) as snippets of speech utterances: A common
assumption in speech recognition tasks is that the audio
frame features (q(X)) are conditionally independent
given latent phonemes Z (which is often modeled as
a Hidden Markov Model).
The latent space Z is often a lower-dimensional space
compared to the original high-dimensional X. We learn
Z from data in an unsupervised manner using variational
inference. Speciï¬cally, we parameterize the distributions
{pÏ‰(q(x) | z, y) âˆ€q âˆˆQ} with a Decoder Network with
shared weights Ï‰. These weights are learned using stochas-
tic Variational Bayes [80] by introducing an approximate
posterior distribution pâ€²
Ï†(z | y, Q(x)) parameterized by
another neural network with weights Ï† called the Encoder
Network and priors pY (y) and pZ(z). More speciï¬cally, the
parameters Ï† and Ï‰ are learned by maximizing the Evidence
Lower BOund (ELBO) objective. Appendix A.7 gives more
details on this optimization procedure. The learned Decoder
Network pÏ‰âˆ—(q(x) | z, y) is then used as a plug-in estimate
for the true distribution pq(X)|ZY (q(x) | z, y), which is in
turn used to estimate (12).
Implementing IP using the generative model. Once the
Decoder Network has been learned using variational infer-
ence, the ï¬rst query q1 = IP(âˆ…) is the one that maximizes
the mutual information with Y as per (6). The mutual infor-
mation term for any query q is completely determined by
p(q(x), y), which is obtained by numerically marginalizing
the nuisances Z from (12) using Monte Carlo integration. In
particular, we carry out the following computation âˆ€q âˆˆQ,
pq(X)Y (q(x), y) =
Z
z
pQ(X)ZY (Q(x), z, y)dz
=
Z
z
pq(X)|ZY (q(x) | z, y)pY (y)pZ(z)dz
â‰ˆ1
N
N
X
i=1
pÏ‰âˆ—(q(x) | y, z(i))pY (y)
=: Ëœp(q(x), y)).
(13)
In the last approximation, pÏ‰âˆ—(q(x) | y, z(i)) is the distribu-
tion obtained using the trained decoder network. N is the
number of i.i.d. samples drawn and zi âˆ¼pZ(z). We then
estimate mutual information numerically via the following
formula,
I(Y ; q(X)) =
X
q(x),y
Ëœp(q(x), y) log Ëœp(q(x), y)
Ëœp(q(x))Ëœp(y).
(14)
The computation of subsequent queries qk+1 requires
the mutual information conditioned on observed history
SIP
k (xobs), which can be calculated from the distribution
p(q(x), y | SIP
k (xobs))
(15)
=
Z
p(q(x), z, y | SIP
k (xobs))dz
=
Z
p(q(x) | z, y, SIP
k (xobs))p(z | y, SIP
k (xobs))p(y | SIP
k (xobs))dz
=
Z
p(q(x) | z, y)p(z | y, SIP
k (xobs))p(y | SIP
k (xobs))dz.
The ï¬rst equality is an application of the law of total
probability. The last equality appeals to the assumption that
{q(X), q âˆˆQ} are conditionally independent given Y, Z
(12).
To estimate the right-hand side of (15) via Monte Carlo
integration, one needs to sample zi âˆ¼p
 z | y, SIP
k (xobs)

and compute
p(q(x), y | SIP
k (xobs)) â‰ˆËœp(q(x), y | SIP
k (xobs))
(16)
:= 1
N
N
X
i=1
pÏ‰âˆ—(q(x)|z(i), y)p(y|SIP
k (xobs)),
where the term p(y | SIP
k (xobs)) is estimated recursively via
the Bayesâ€™ theorem. This computation is as follows,
p(y | SIP
k (x)) âˆp(y, SIP
k (x))
= p(qk(x), y, SIP
kâˆ’1(x))
âˆp(qk(x) | y, SIP
kâˆ’1(x))p(y | SIP
kâˆ’1(x))
(17)
SIP
0 (x) = âˆ…(since no evidence via queries has been gathered
from x yet) and so p(y | SIP
0 (x)) = pY (y). The posterior p(y |
SIP
k (x)) is obtained by normalizing the last equation in (17)
such that P
y p(y|SIP
k (x)) = 1. This recursive updating of the
posterior is similar to the posterior updates used in Bayesian
sequential ï¬ltering [81]. The term p(qk(x) | y, SIP
kâˆ’1(x)) is
estimated using (16).
Having estimated p(q(x), y | SIP
k (xobs)), we then nu-
merically compute the mutual information between query-
answer q(X) and Y given history for every q âˆˆQ via the
formula
I(Y ; q(X) | SIP
k (xobs)) =
(18)
X
q(x),y
Ëœp(q(x), y|SIP
k (xobs)) log
Ëœp(q(x), y | SIP
k (xobs))
Ëœp(q(x)|SIP
k (xobs))Ëœp(y|SIP
k (xobs)).
Estimating
p
 z | y, SIP
k (xobs)

with
the
Unadjusted
Langevin Algorithm. Next we describe how to sample
from this posterior p(z | y, SIP
k (xobs)) using the Unadjusted
Langevin Algorithm (ULA). ULA is an iterative algorithm
used to approximately sample from any distribution with a
density known only up to a normalizing factor. It has been
successfully applied to many high-dimensional Bayesian

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
8
inference problems [82], [83], [84]. Given an initialization
z(0), ULA proceeds by
z(i+1) = z(i) + Î·âˆ‡U(z(i)) +
p
2Î·Î¶(i+1).
(19)
Here (Î¶(i))iâ‰¥1 âˆ¼N(0, I) and Î· is the step-size. Asymptoti-
cally, the chain (z(i))iâ‰¥1 converges to a stationary distribu-
tion that is â€œapproximatelyâ€ equal to a measure with density
âˆeU(z) [85].
For IP, we need samples from p(z | y, SIP
k (xobs). This
is achieved by initializing z(0) using the last iterate of the
ULA chain used to simulate p(z | y, SIP
kâˆ’1(xobs).5 We then
run ULA for N iterations by recursively applying (19) with
U(z) := log p(z, SIP
k (xobs)|y) = log p(SIP
k (xobs)|z, y)p(z)p(y).
The number of steps N is chosen to be sufï¬ciently large
to ensure the ULA chain converges â€œapproximatelyâ€ to the
desired z âˆ¼p(z | y, SIP
k (xobs)). We use the trained decoder
network Qk
i=1 pÏ‰(qi(x) | z, y), with qi being the ith query
asked by IP for input x, as a proxy for p
 SIP
k (xobs)
 | z, y).
We then obtain stochastic approximations of (15) by time
averaging the iterates,
p

q(x), y|SIP
k (xobs)

â‰ˆ1
N
N
X
i=1
pÏ‰

q(x)|z(i), y

p

y|SIP
k (xobs)

,
(20)
where (z(i))1:N are the iterates obtained using the ULA
chain whose stationary distribution is â€œapproximatelyâ€
p
 z | y, SIP
k (xobs)

.
Algorithmic complexity for IP. For any given input x, the
per-iteration cost of the IP algorithm is O(N+|Q|m)6, where
|Q| is the total number of queries, N is the number of
ULA iterations, and m is cardinality of the product sample
space q(X) Ã— Y . For simplicity we assume that the output
hypothesis Y and query-answers q(X) are ï¬nite-valued and
also that the number of values query answers can take is the
same. However, our framework can handle more general
cases. See Appendix A.6 for more details.
5
EXPERIMENTS
In this section, we empirically evaluate the effectiveness
of our method. We begin by analyzing the explanations
provided by IP for classifying individual input data, in
terms of words, symbols, or patterns (the queries). We ï¬nd
in each case that IP discovers concise explanations which
are amenable to human interpretation. We then perform
quantitative comparisons which show that (i) IP explana-
tions are more faithful to the underlying model than existing
attribution methods; and (ii) the predictive accuracy of our
method using a given query set is competitive with black-
box models trained on features provided by the same set.
5.0.1
Binary Image Classiï¬cation with Patch Queries
Task and query set. We start with the simple task of binary
image classiï¬cation. We consider three popular datasets â€“
MNIST [86], Fashion-MNIST [87] and KMNIST [88]. We
choose a threshold for binarizing these datasets since they
5. z(0) âˆ¼N(0, I) for the ï¬rst iteration of IP.
6. In this computation we have assumed, for simplicity, a unit cost for
any operation that was computed in a batch concurrently on a GPU.
are originally grayscale. We choose the query set Q as the
set of all w Ã— w overlapping patch locations in the image.
The answer q(X) for any q âˆˆQ is the w2 pixel intensities
observed at the patch indexed by location q. This choice of
Q reï¬‚ects the userâ€™s desire to know which parts of the input
image are most informative for a particular prediction, a
common practice for explainability in vision tasks [25]. We
conduct experiments for multiple values of w and conclude
that w = 3 provides a good trade-off between the required
number of queries and the interpretability of each query.
Note that when w > 1 the factorization in (12) that we
use to model p(Q(x), y, z) and compute mutual informa-
tion no longer holds as the overlapping queries q(X) are
now causally related (and therefore dependent even when
conditioned on Z, making them unable to be modeled by
a VAE). So instead of training a VAE to directly model
the query set p(Q(x) | y, z), we train a VAE to model
the pixel distribution p(x | y, z), and then compute the
probability distribution over the patch query p(q(x) | z, y)
as the product of the probabilities of all pixels in that patch.7
IP in action. Fig. 4(a) illustrates the decision-making process
of IP using 3 Ã— 3 patch queries on an image xobs of a 6
from the MNIST test set. The ï¬rst query is near the center of
the image; recall from (6) that this choice is independent of
the particular input image and represents the patch whose
pixel intensities have maximum mutual information with
Y (the class label). The updated posterior, p
 Y | SIP
1 (xobs)

,
concentrates most of its mass on the digit â€œ1â€, perhaps
because most of the other digits do not commonly have a
vertical piece of stroke at the center patch. However, the next
query (about three pixels below the center patch) reveals
a horizontal stroke and the posterior mass over the labels
immediately shifts to {2, 3, 6, 8}. The next two queries are
well-suited to discerning between these four possibilities
and we see that after asking 4 questions, IP is more than
90% conï¬dent that the image is a 6. Such rich explanations
in terms of querying informative patches based on what is
observed so far and seeing how the belief p
 Y | SIP
k (xobs)

of the model evolves over time is missing from post-hoc at-
tribution methods which output static importance scores for
every pixel towards the black-box modelâ€™s ï¬nal prediction.
Explanation length vs. task complexity. Fig. 6 shows that IP
requires an average of 5.2, 12.9 and 14.5 queries of size 3Ã—3
to predict the label with 99% conï¬dence (Ïµ = 0.01 in (8))
on MNIST, KMNIST and FashionMNIST, respectively. This
reï¬‚ects the intuition that more complex tasks require longer
explanations. For reference, state-of-the-art deep networks
on these datasets obtain test accuracies in order MNIST â‰¥
KMNIST â‰¥FashionMNIST (see last row in Table 1).
Effect of patch size on interpretability. We also run IP
on MNIST with patch sizes of 1 Ã— 1 (single pixels), 2 Ã— 2,
3 Ã— 3, and 4 Ã— 4. We observed that IP terminates at 99%
conï¬dence after 21.1, 9.6, 5.2, and 4.6 queries on average,
respectively. While this suggests that larger patches lead to
shorter explanations, we note that explanations with larger
patches use more pixels (e.g. on MNIST, IP uses 21.1 pixels
7. Since the patches overlap in our query set, when computing the
conditional probability of a patch query given history we only consider
the probability of the pixels in the patch that have not yet been observed
in our history.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
9
p
!
Y | SIP
k (xobs)
"
p
!
Y | SIP
k (xobs)
"
Fig. 4. (a) IP on MNIST. The top row displays the test image with red boxes denoting the current queried patch and blue boxes denoting previous
patches. The second row shows the revealed portion of the image that IP gets to use at each query. The ï¬nal row shows the modelâ€™s estimated
posteriors at each query, beginning at a nearly uniform prior before converging on the true digit â€œ6â€ after 4 queries. (b) IP on CUB Bird Species
Classiï¬cation. On the left we show the input image and on the right we have a heatmap of the estimated class probabilities at each iteration. We
only show the top 10 most probable classes out of the 200. To the right, we display the queries asked at each iteration, with red indicating a â€œnoâ€
response and green a â€œyesâ€ response. (c) IP on HuffPost News. We show the input news item and a heatmap depicting the evolution of topic
probabilities as IP asks queries and gathers answers. Words colored in red are absent from the sentence while words in green are present. For our
visualization, we compute the KL divergence between each successive posterior and plot only the top 20 queries that led to the greatest change in
posterior class probabilities.
on average for 1 Ã— 1 patches and 54.7 pixels on average for
4 Ã— 4 patches). That being said, very small patch queries are
hard to interpret (see Fig. 5) and very large patch queries are
also hard to interpret since each patch contains many image
features. Overall, we found that 3 Ã— 3 patches represented
the right trade-off between interpretability in terms of edge
patterns and minimality of the explanations. Speciï¬cally,
single pixels are not very interpretable to humans but the
explanations generated are more efï¬cient in terms of number
of pixels needed to predict the label. On the other extreme,
using the entire image as a query is not interesting from
an interpretability point of view since it does not help us
understand which parts of the image are salient for predic-
tion. We refer the reader to Appendix B.3.1 for additional
patch size examples and quantitative analysis.
5.0.2
Concept-Based Queries
Task and query set. What if the end-user is interested in a
more semantic explanation of the decision process in terms
of high-level concepts? This can be easily incorporated into
our framework by choosing an appropriate query set Q.
k=0
k=1
k=2
k=3
k=4
k=6
k=21
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
Fig. 5. IP with 1 Ã— 1 patches on MNIST. Through the ï¬rst 6 iterations,
IP asks queries in the same center vertical region as in Fig. 4(a) (which
uses 3Ã—3 queries), outlining the distinctive loop in the bottom of the â€œ6â€.
However, reaching 99% conï¬dence requires a total of 21 1 Ã— 1 queries
as opposed to just 4 3 Ã— 3 ones. For conciseness, we show only the
6 queries that led to the greatest KL divergence between successive
posterior class probabilities.
As an example we consider the challenging task of bird
species classiï¬cation on the Caltech-UCSD Birds-200-2011
(CUB) dataset [89]. The dataset contains images of 200

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
10
different species of birds. Each image is annotated with
312 binary attributes representing high-level concepts, such
as the colour and shape of the beak, wings, and head.
Unfortunately, these attribute annotations are very noisy. We
follow [24] in deciding attribute labels by majority voting.
For example, if more than 50% of images in a class have
black wings, then we set all images in that class to have
black wings. We construct Q by choosing a query for asking
the presence/absence of each of these 312 binary attributes.
Unfortunately, attribute annotations are not available at test
time. To remedy this, we train a CNN (see [24] for details) to
answer each query using the training set annotations, which
is then used to answer queries at test time. Subsequently,
we learn a VAE to model the joint distribution of query-
answers supplied by this CNN (instead of the ground truth
annotations) and Y , so our generative model can account
for any estimation errors incurred by the CNN. Finally, we
carry out IP as explained in Â§4.
IP in action. Consider the image of a Great Crested Flycatcher
in Fig. 4(b). IP proceeds by asking most informative queries
about various bird attributes progressively making the pos-
terior over the species labels more and more peaked. After 5
queries, IP has gathered that the image is of a bird that has
a perching-like shape, all-purpose beak and yellow belly,
but does not have a yellow throat nor yellow upperparts.
This results in a posterior concentrated on just 4 species that
exhibit these characteristics. IP then proceeds to discount
Green Jay and Scott Oriole which have black breasts with
query 6. Likewise, Tropical Kingbirds have grayish back and
is segregated from Great Crested Flycatchers which have buff-
coloured backs with query 7. Finally after 9 queries, IP is
99% conï¬dent about the current class. Such concept-based
explanations are more accessible to non-experts, especially
on ï¬ne-grained classiï¬cation datasets, which typically re-
quire domain expertise. On average IP takes 14.7 queries to
classify a given bird image with Ïµ = 0.007 as the stopping
criteria (See (7)).
5.0.3
Word-based Queries
Task and query set. Our framework can also be successfully
applied to other domains like NLP. As an example we
consider the task of topic identiï¬cation from newspaper ex-
tended headlines (headline + short description ï¬eld) using
the the Hufï¬ngton Post News Category Dataset [90]. We
adopt a simple query set that consists of binary queries
probing the existence of words in the extended headline.
The words are chosen from a pre-deï¬ned vocabulary ob-
tained by stemming all words in the HuffPost dataset and
choosing the top-1,000 according to their tf-idf scores [91].
We process the dataset to merge redundant categories (such
as Style & Beauty and Beauty & Style), remove semantically
ambiguous, HuffPost-speciï¬c categories (e.g. Impact or Fifty)
and remove categories with few samples, arriving at 10 ï¬nal
categories (see Appendix B.1).
IP in action. Fig. 4(c) shows an example run of IP on the
HuffPost dataset. Note that positive responses to queries are
very sparse, since each extended headline only contains 8.6
words on average out of the 1,000 in the vocabulary. As a re-
sult, IP asks 125 queries before termination. As discussed in
Â§2, such long decision paths would be impossible in decision
trees due to data fragmentation and memory limitations.
For clarity of presentation we only show the 20 queries
with the greatest impact on the estimated posterior (as
measured by KL-divergence from previous posterior). Upon
reaching the ï¬rst positive query â€œeatâ€, the probability mass
concentrates on the categories Food & Drink and Wellness
with little mass on Travel. However, as the queries about the
existence of â€œcitiâ€, â€œvisitâ€, â€œyorkâ€, and â€œbarâ€ in the extended
headline come back positive, the model becomes more and
more conï¬dent that â€œTravelâ€ is the correct class. IP requires
about 199.3 queries on average to predict the topic of the
extended headline with Ïµ = 10âˆ’3 as the stopping criteria
(See (7)). Additional details on the HuffPost query set are in
Appendix B.1.
Further examples of IP performing inference on all tasks
can be found in Appendix B.3.
5.1
Quantitative Evaluation
5.1.1
Classiï¬cation Accuracy
We compare the classiï¬cation accuracy of our modelâ€™s pre-
diction based on the query-answers gathered by IP until
termination with several other baseline models. For each of
the models considered, we ï¬rst give a brief description and
then comment on their performance with respect to IP. All
the results are summarized in Table 1.
DECISION TREE refers to standard classiï¬cation trees
learnt using the popular CART algorithm [34]. In the In-
troduction, we mentioned that classical decision trees learnt
using Q to supply the node splitting functions will be
intepretable by construction but are not competitive with
state-of-the-art methods. This is illustrated in our results in
Table 1. Across all datasets, IP obtains superior performance
since it is based on an underlying generative model (VAE)
and only computes the branch of the tree traversed by the
input data in an online manner, thus it is not shackled by
data fragmentation and memory limitations.
MAP USING Q refers to the Maximum A Posteriori
estimate obtained using the posterior distribution over the
labels given the answers to all the queries in Q (for a given
input). Recall, IP asks queries until the stopping criteria is
reached (Equation (7) & Equation (8)). Naturally, there is
a trade-off between the length of the explanations and the
predictive performance observed. If we ask all the queries
then the resulting explanations of length |Q| might be too
long to be desirable. The results for IP reported in Table 1
use different dataset-speciï¬c stopping criteria according to
the elbow in their respective accuracy vs. explanation length
curves (see Fig. 6). On the binary image datasets, (MNIST,
KMNIST, and FashionMNIST) IP obtains an accuracy within
3% of the best achievable upon seeing all the query-answers
with only about 2% of the total queries in Q. Similarly for
the CUB and Huffpost datasets, IP achieves about the same
accuracy as MAP USING Q but asks less than 5% and 20%
of total possible queries respectively.
BLACK-BOX USING Q refers to the best performing deep
network model we get by training on features supplied
by evaluating all q âˆˆQ on input data from the various
training datasets. For the binary image datasets, this is just
a 4-layer CNN with ReLU activations. For CUB we use
the results reported by the sequential model in [24]. For

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
11
0
5
10
15
...
676
Average Explanation Length
0.00
0.25
0.50
0.75
1.00
Test Accuracy
0
5
10
15
...
310
Average Explanation Length
0
50 100 150 200 250 ... 1000
Average Explanation Length
MNIST
KMNIST
FashionMNIST
CUB
HuffPost News
Fig. 6. Trade-off between predictive performance and explanation length Different points along the curves correspond to different values of Ïµ
as the stopping criteria (7) is varied. The colored dotted vertical line in each plot indicates the avg. explanation length v/s test accuracy at the Ïµ
value used as the stopping criteria for reporting results for the IP strategy in this work. For each plot, the x-axis ranges from 0 to the size of the
query set, |Q|, chosen for that task.
HuffPost, we found a single hidden layer with ReLU non-
linearity give the best performance. Further architectural
and training details are in Appendix B.2. In Table 1 we show
that across all datasets, the predictive performance obtained
by MAP USING Q is on par with the best performance we
obtained using black-box expressive non-interpretable net-
works BLACK-BOX USING Q. Thus, our generative models,
which form the backbone for IP, are competitive with state-
of-the-art prediction methods.
BLACK-BOX refers to the best performing black-box
model on these datasets in terms of classiï¬cation accuracy as
reported in literature; to the best of our knowledge. In Table
1, we see a performance gap in each dataset when compared
with MAP USING Q which uses an interpretable query set.
This is expected since explainability can be viewed as an
additional constraint on learning. For example, on Fash-
ionMNIST we see an almost 8.5% relative fall in accuracy
due to binarization. This is because it is harder to decipher
between some classes like shirts and pullovers at the binary
level. On the other hand, binary patches are easily inter-
pretable as edges, foregrounds and backgrounds. Similarly,
there is a relative drop of accuracy of about 17% for the
HuffPost dataset since our queries regarding the existence of
different words ignore their arrangement in sentences. Thus
we lose crucial contextual information used by state-of-the-
art transformer models [92]. Ideally, we would like query
sets to be easily interpretable, lead to short explanations and
be sufï¬cient to solve the task. Finding such query sets is
nontrivial and will be explored in future work.
TABLE 1
Classiï¬cation accuracy of our model (Information Pursuit) relative
to baselines on different test sets. See 5.1.1 for details on each
model.
Model
MNIST
KMNIST
Fashion
CUB
HuffPost
INFORMATION PURSUIT
96.78%
91.02%
85.60%
76.73%
71.21%
DECISION TREE [34]
90.23%
78.00%
80.80%
68.80%
63.00%
MAP USING Q
99.05%
94.25%
87.56%
76.80%
71.72%
BLACK-BOX USING Q
99.15%
95.10%
88.43%
76.30%
71.48%
BLACK-BOX
99.83% [93]
98.83% [88]
96.70% [94]
82.70% [24]
86.45% 8
8. We ï¬ne-tuned a Bert Large Uncased Transformer model [92] with
the last layer replaced with a linear one. See Appendix B.2.3 for details.
5.1.2
Comparison to current attribution methods
At ï¬rst glance, it might seem that using attribution meth-
ods/saliency maps can provide the same insights as to
which parts of the image or more generally which queries
in Q were most inï¬‚uential in a decision made by a black-
box model trained on input features supplied by all the
query-answers. However, the unreliability of these methods
in being faithful to the model they try to explain brings
their utility into question [19], [20], [22]. We conjecture that
this is because current attribution methods are not designed
to generate explanations that are sufï¬cient statistics of the
modelâ€™s prediction. We illustrate this with a simple experi-
ment using our binary image classiï¬cation datasets.
For each input image x, we compute the corresponding
attribution map e(x) for the modelâ€™s predicted class using
two popular attribution methods, Integrated gradients (IG)
[95] and DeepSHAP [45]. We then compute the L most
important 3 Ã— 3 patches, where L is the number of patches
queried by IP for that particular input image. For comput-
ing the attribution/importance of a patch we average the
attributions of all the pixels in that patch (following [20]).
We proceed as follows: (i) Given e(x), compute the patch
with maximum attribution and add these pixels to our ex-
planation, (ii) Zero-out the attributions of all the pixels in the
previously selected patch and repeat step (i) until L patches
are selected. The ï¬nal explanation consists of L possibly
overlapping patches. Now, we evaluate the sufï¬ciency of
the generated explanation for the modelâ€™s prediction by esti-
mating the MAP accuracy of the posterior over labels given
the intensities in the patches included in this explanation.
This is done via a VAE trained to learn the joint distribution
over image pixels and class labels. We experiment with both
the raw attribution scores returned by IG and DeepSHAP
and also the absolute values of the attribution scores for
e(x). The results are reported in Table 2. In almost all cases
(with the exception of DeepSHAP on FashionMNIST), IP
generates explanations that are more predictive of the class
label than popular attribution methods.
6
CONCLUSION
We have presented a step towards building trustwor-
thy interpretable machine learning models that respect
the domain- and user-dependent nature of interpretabil-
ity. We address this by composing user-deï¬ned, inter-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
12
TABLE 2
MAP accuracy of explanations generated by Information Pursuit
(IP) v/s other attribution methods. IP explanations (in almost all
cases) achieve a higher classiï¬cation accuracy than explanations of the
same length generated using baseline attribution methods. The
(absolute) method refers to explanations generated using absolute
values of the attribution map scores. On MNIST and KMNIST, IP
explanations achieve a 10% and 2.38% relative improvement
respectively over the best performing baseline method. On
FashionMNIST, IP explanations are second best with a relative
decrease of about 3.12% from the best performing baseline.
Explanation Method
MNIST
KMNIST
Fashion-MNIST
INFORMATION PURSUIT
96.78%
91.02%
85.60%
IG
78.48%
84.87%
78.49%
IG (ABSOLUTE)
70.39%
84.72%
64.95%
DEEPSHAP
87.98%
88.90%
88.36%
DEEPSHAP (ABSOLUTE)
84.80%
84.56%
84.35%
pretable queries into concise explanations. Furthermore,
unlike many contemporary attempts at explainability, our
method is not post-hoc, but is interpretable by design and
guaranteed to produce faithful explanations. We formulate
a tractable approach to implement this framework through
deep generative models, MCMC algorithms, and the infor-
mation pursuit algorithm. Finally, we demonstrate the effec-
tiveness of our method across various vision and language
tasks at generating concise explanations describing the un-
derlying reasoning process behind the prediction. Future
work will be aimed at extending the proposed framework
to more complex tasks beyond classiï¬cation such as scene
parsing, image captioning, and sentiment analysis.
ACKNOWLEDGMENTS
The authors thank MarÂ´Ä±a PÂ´erez Ortiz and John Shawe-Taylor
for their contributions to the design of the experiments
on document classiï¬cation presented in Section 5.0.3. This
research was supported by the Army Research Ofï¬ce under
the Multidisciplinary University Research Initiative contract
W911NF-17-1-0304 and by the NSF grant 2031985.
REFERENCES
[1]
D. Gunning, M. Steï¬k, J. Choi, T. Miller, S. Stumpf, and G.-Z. Yang,
â€œXaiâ€”explainable artiï¬cial intelligence,â€ Science Robotics, vol. 4,
no. 37, p. eaay7120, 2019.
[2]
C. Rudin, â€œStop explaining black box machine learning models
for high stakes decisions and use interpretable models instead,â€
Nature Machine Intelligence, vol. 1, no. 5, pp. 206â€“215, 2019.
[3]
W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu,
â€œInterpretable machine learning: deï¬nitions, methods, and appli-
cations,â€ arXiv preprint arXiv:1901.04592, 2019.
[4]
European Commission, â€œBuilding trust in human-centric artiï¬cial
intelligence,â€ Communication from the Commission to the European
Parliament, the Council, the European Economic and Social Committee
and the Committee of the Regions, vol. 168, 2019.
[5]
United States Food and Drug Administration, â€œVirtual pub-
lic workshop - transparency of artiï¬cial intelligence/machine
learning-enabled medical devices,â€ Transcript: https://www.fda.
gov/media/154423/download, Oct. 14, 2021.
[6]
U. Johansson, C. SÂ¨onstrÂ¨od, U. Norinder, and H. BostrÂ¨om, â€œTrade-
off between accuracy and interpretability for predictive in silico
modeling,â€ Future medicinal chemistry, vol. 3, no. 6, pp. 647â€“663,
2011.
[7]
J. Wanner, L.-V. Herm, K. Heinrich, and C. Janiesch, â€œStop or-
dering machine learning algorithms by their explainability! an
empirical investigation of the tradeoff between performance and
explainability,â€ in Conference on e-Business, e-Services and e-Society.
Springer, 2021, pp. 245â€“258.
[8]
F. K. DoË‡siloviÂ´c, M. BrË‡ciÂ´c, and N. HlupiÂ´c, â€œExplainable artiï¬cial
intelligence: A survey,â€ in 2018 41st International convention on
information and communication technology, electronics and microelec-
tronics (MIPRO).
IEEE, 2018, pp. 0210â€“0215.
[9]
A. B. Arrieta, N. DÂ´Ä±az-RodrÂ´Ä±guez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, S. GarcÂ´Ä±a, S. Gil-LÂ´opez, D. Molina, R. Benjamins et al.,
â€œExplainable artiï¬cial intelligence (xai): Concepts, taxonomies,
opportunities and challenges toward responsible ai,â€ Information
fusion, vol. 58, pp. 82â€“115, 2020.
[10] D. Gunning and D. Aha, â€œDarpaâ€™s explainable artiï¬cial intelli-
gence (xai) program,â€ AI magazine, vol. 40, no. 2, pp. 44â€“58, 2019.
[11] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen,
and K.-R. MÂ¨uller, â€œHow to explain individual classiï¬cation deci-
sions,â€ The Journal of Machine Learning Research, vol. 11, pp. 1803â€“
1831, 2010.
[12] K. Simonyan, A. Vedaldi, and A. Zisserman, â€œDeep inside con-
volutional networks: Visualising image classiï¬cation models and
saliency maps,â€ arXiv preprint arXiv:1312.6034, 2013.
[13] S. Kolek, D. A. Nguyen, R. Levie, J. Bruna, and G. Kutyniok, â€œA
rate-distortion framework for explaining black-box model deci-
sions,â€ in International Workshop on Extending Explainable AI Beyond
Deep Models and Classiï¬ers.
Springer, 2022, pp. 91â€“115.
[14] A. Shrikumar, P. Greenside, and A. Kundaje, â€œLearning important
features through propagating activation differences,â€ in Interna-
tional conference on machine learning.
PMLR, 2017, pp. 3145â€“3153.
[15] M. D. Zeiler and R. Fergus, â€œVisualizing and understanding
convolutional networks,â€ in European conference on computer vision.
Springer, 2014, pp. 818â€“833.
[16] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, â€œGrad-cam: Visual explanations from deep networks via
gradient-based localization,â€ in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618â€“626.
[17] D. Smilkov, N. Thorat, B. Kim, F. ViÂ´egas, and M. Wattenberg,
â€œSmoothgrad: removing noise by adding noise,â€ arXiv preprint
arXiv:1706.03825, 2017.
[18] A. Subramanya, V. Pillai, and H. Pirsiavash, â€œFooling network in-
terpretation in image classiï¬cation,â€ in Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 2020â€“2029.
[19] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and
B. Kim, â€œSanity checks for saliency maps,â€ Advances in neural
information processing systems, vol. 31, 2018.
[20] M. Yang and B. Kim, â€œBenchmarking attribution methods with
relative feature importance,â€ arXiv preprint arXiv:1907.09701, 2019.
[21] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. SchÂ¨utt,
S. DÂ¨ahne, D. Erhan, and B. Kim, â€œThe (un) reliability of saliency
methods,â€ in Explainable AI: Interpreting, Explaining and Visualizing
Deep Learning.
Springer, 2019, pp. 267â€“280.
[22] H. Shah, P. Jain, and P. Netrapalli, â€œDo input gradients highlight
discriminative features?â€ Advances in Neural Information Processing
Systems, vol. 34, 2021.
[23] D. Slack, S. Hilgard, E. Jia, S. Singh, and H. Lakkaraju, â€œFooling
lime and shap: Adversarial attacks on post hoc explanation meth-
ods,â€ in Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society, 2020, pp. 180â€“186.
[24] P. W. Koh, T. Nguyen, Y. S. Tang, S. Mussmann, E. Pierson,
B. Kim, and P. Liang, â€œConcept bottleneck models,â€ in International
Conference on Machine Learning.
PMLR, 2020, pp. 5338â€“5348.
[25] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, and J. K. Su, â€œThis
looks like that: deep learning for interpretable image recognition,â€
Advances in neural information processing systems, vol. 32, 2019.
[26] C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova, and
C. Zhong, â€œInterpretable machine learning: Fundamental princi-
ples and 10 grand challenges,â€ Statistics Surveys, vol. 16, pp. 1â€“85,
2022.
[27] T. M. Janssen and B. H. Partee, â€œCompositionality,â€ in Handbook of
logic and language.
Elsevier, 1997, pp. 417â€“473.
[28] H. Lakkaraju, S. H. Bach, and J. Leskovec, â€œInterpretable decision
sets: A joint framework for description and prediction,â€ in Proceed-
ings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, 2016, pp. 1675â€“1684.
[29] A. Wan, L. Dunlap, D. Ho, J. Yin, S. Lee, S. Petryk, S. A. Bargal,
and J. E. Gonzalez, â€œ{NBDT}: Neural-backed decision tree,â€ in

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
13
International Conference on Learning Representations, 2021. [Online].
Available: https://openreview.net/forum?id=mCLVeEpplNE
[30] J. Mu and J. Andreas, â€œCompositional explanations of neurons,â€
Advances in Neural Information Processing Systems, vol. 33, pp.
17 153â€“17 163, 2020.
[31] E. Jahangiri, E. Yoruk, R. Vidal, L. Younes, and D. Geman, â€œIn-
formation pursuit: A bayesian framework for sequential scene
parsing,â€ arXiv preprint arXiv:1701.02343, 2017.
[32] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, â€œNetwork
dissection: Quantifying interpretability of deep visual representa-
tions,â€ in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2017, pp. 6541â€“6549.
[33] E. Hernandez, S. Schwettmann, D. Bau, T. Bagashvili, A. Torralba,
and J. Andreas, â€œNatural language descriptions of deep visual
features,â€ arXiv preprint arXiv:2201.11114, 2022.
[34] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classiï¬-
cation and regression trees.
Routledge, 2017.
[35] J. R. Quinlan, â€œInduction of decision trees,â€ Machine learning,
vol. 1, no. 1, pp. 81â€“106, 1986.
[36] Y. Amit and D. Geman, â€œShape quantization and recognition with
randomized trees,â€ Neural computation, vol. 9, no. 7, pp. 1545â€“1588,
1997.
[37] L. Breiman, â€œRandom forests,â€ Machine learning, vol. 45, no. 1, pp.
5â€“32, 2001.
[38] R. Caruana and A. Niculescu-Mizil, â€œAn empirical comparison
of supervised learning algorithms,â€ in Proceedings of the 23rd
international conference on Machine learning, 2006, pp. 161â€“168.
[39] M. FernÂ´andez-Delgado, E. Cernadas, S. Barro, and D. Amorim,
â€œDo we need hundreds of classiï¬ers to solve real world classiï¬-
cation problems?â€ The journal of machine learning research, vol. 15,
no. 1, pp. 3133â€“3181, 2014.
[40] H. Xu, K. A. Kinfu, W. LeVine, S. Panda, J. Dey, M. Ainsworth,
Y.-C. Peng, M. Kusmanov, F. Engert, C. M. White et al., â€œWhen are
deep networks really better than decision forests at small sample
sizes, and how?â€ arXiv preprint arXiv:2108.13637, 2021.
[41] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bulo, â€œDeep
neural decision forests,â€ in Proceedings of the IEEE international
conference on computer vision, 2015, pp. 1467â€“1475.
[42] D. Geman and B. Jedynak, â€œAn active testing model for tracking
roads in satellite images,â€ IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 18, no. 1, pp. 1â€“14, 1996.
[43] A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubra-
manian, â€œGrad-cam++: Generalized gradient-based visual expla-
nations for deep convolutional networks,â€ in 2018 IEEE winter
conference on applications of computer vision (WACV).
IEEE, 2018,
pp. 839â€“847.
[44] M. T. Ribeiro, S. Singh, and C. Guestrin, â€œâ€ why should i trust
you?â€ explaining the predictions of any classiï¬er,â€ in Proceedings
of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, 2016, pp. 1135â€“1144.
[45] S. M. Lundberg and S.-I. Lee, â€œA uniï¬ed approach to interpret-
ing model predictions,â€ Advances in neural information processing
systems, vol. 30, 2017.
[46] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas et al.,
â€œInterpretability beyond feature attribution: Quantitative testing
with concept activation vectors (tcav),â€ in International conference
on machine learning.
PMLR, 2018, pp. 2668â€“2677.
[47] B. Zhou, Y. Sun, D. Bau, and A. Torralba, â€œInterpretable basis de-
composition for visual explanation,â€ in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 119â€“134.
[48] C.-K. Yeh, B. Kim, S. Arik, C.-L. Li, T. Pï¬ster, and P. Ravikumar,
â€œOn completeness-aware concept-based explanations in deep neu-
ral networks,â€ Advances in Neural Information Processing Systems,
vol. 33, pp. 20 554â€“20 565, 2020.
[49] D. Alvarez Melis and T. Jaakkola, â€œTowards robust interpretability
with self-explaining neural networks,â€ Advances in neural informa-
tion processing systems, vol. 31, 2018.
[50] M. Bohle, M. Fritz, and B. Schiele, â€œConvolutional dynamic align-
ment networks for interpretable classiï¬cations,â€ in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 10 029â€“10 038.
[51] M. Wu, S. Parbhoo, M. C. Hughes, V. Roth, and F. Doshi-Velez,
â€œOptimizing for interpretability in deep neural networks with tree
regularization,â€ Journal of Artiï¬cial Intelligence Research, vol. 72, pp.
1â€“37, 2021.
[52] V. Pillai and H. Pirsiavash, â€œExplainable models with consistent
interpretations,â€ UMBC Student Collection, 2021.
[53] Z. Chen, Y. Bei, and C. Rudin, â€œConcept whitening for inter-
pretable image recognition,â€ Nature Machine Intelligence, vol. 2,
no. 12, pp. 772â€“782, 2020.
[54] M. De-Arteaga, A. Romanov, H. Wallach, J. Chayes, C. Borgs,
A. Chouldechova, S. Geyik, K. Kenthapadi, and A. T. Kalai,
â€œBias in bios: A case study of semantic representation bias in a
high-stakes setting,â€ in proceedings of the Conference on Fairness,
Accountability, and Transparency, 2019, pp. 120â€“128.
[55] A. Galassi, M. Lippi, and P. Torroni, â€œAttention in natural language
processing,â€ IEEE Transactions on Neural Networks and Learning
Systems, vol. 32, no. 10, pp. 4291â€“4308, 2020.
[56] D. Pruthi, M. Gupta, B. Dhingra, G. Neubig, and Z. C. Lipton,
â€œLearning to deceive with attention-based explanations,â€ arXiv
preprint arXiv:1909.07913, 2019.
[57] M. Craven and J. Shavlik, â€œExtracting tree-structured representa-
tions of trained networks,â€ Advances in neural information processing
systems, vol. 8, 1995.
[58] D. Dancey, D. A. McLean, and Z. A. Bandar, â€œDecision tree extrac-
tion from trained neural networks,â€ in Proceedings of the Nineteenth
Conference on Artiï¬cial Intelligence.
American Association for
Artiï¬cial Intelligence, 2004.
[59] N. Frosst and G. Hinton, â€œDistilling a neural network into a soft
decision tree,â€ arXiv preprint arXiv:1711.09784, 2017.
[60] A. Roy and S. Todorovic, â€œMonocular depth estimation using
neural regression forest,â€ in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 5506â€“5514.
[61] U. C. BicÂ¸ici, C. Keskin, and L. Akarun, â€œConditional information
gain networks,â€ in 2018 24th International Conference on Pattern
Recognition (ICPR).
IEEE, 2018, pp. 1390â€“1395.
[62] V. N. Murthy, V. Singh, T. Chen, R. Manmatha, and D. Comaniciu,
â€œDeep decision network for multi-class image classiï¬cation,â€ in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2016, pp. 2240â€“2248.
[63] R. Vilalta, G. Blix, and L. Rendell, â€œGlobal data analysis and the
fragmentation problem in decision tree induction,â€ in European
Conference on Machine Learning.
Springer, 1997, pp. 312â€“326.
[64] N. Tishby, F. C. Pereira, and W. Bialek, â€œThe information bottleneck
method,â€ arXiv preprint physics/0004057, 2000.
[65] K. Chaloner and I. Verdinelli, â€œBayesian experimental design: A
review,â€ Statistical Science, pp. 273â€“304, 1995.
[66] R. Sznitman and B. Jedynak, â€œActive testing for face detection
and localization,â€ IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 32, no. 10, pp. 1914â€“1920, 2010.
[67] M. Cuturi, O. Teboul, Q. Berthet, A. Doucet, and J.-P. Vert, â€œNoisy
adaptive group testing using bayesian sequential experimental
design,â€ arXiv preprint arXiv:2004.12508, 2020.
[68] S. Branson, G. Van Horn, C. Wah, P. Perona, and S. Belongie, â€œThe
ignorant led by the blind: A hybrid humanâ€“machine vision system
for ï¬ne-grained categorization,â€ International Journal of Computer
Vision, vol. 108, no. 1, pp. 3â€“29, 2014.
[69] V. Mnih, N. Heess, A. Graves et al., â€œRecurrent models of visual
attention,â€ in Advances in neural information processing systems,
2014, pp. 2204â€“2212.
[70] G. Elsayed, S. Kornblith, and Q. V. Le, â€œSaccader: improving
accuracy of hard attention models for vision,â€ in Advances in
Neural Information Processing Systems, 2019, pp. 702â€“714.
[71] M. Li, S. S. Ge, and T. H. Lee, â€œGlance and glimpse network:
A stochastic attention model driven by class saliency,â€ in Asian
Conference on Computer Vision.
Springer, 2016, pp. 572â€“587.
[72] H. Li, P. Wang, C. Shen, and A. v. d. Hengel, â€œVisual question an-
swering as reading comprehension,â€ in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
6319â€“6328.
[73] M. Malinowski, M. Rohrbach, and M. Fritz, â€œAsk your neurons:
A deep learning approach to visual question answering,â€ Interna-
tional Journal of Computer Vision, vol. 125, no. 1, pp. 110â€“135, 2017.
[74] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu, â€œThe neuro-
symbolic concept learner: Interpreting scenes, words, and sen-
tences from natural supervision,â€ arXiv preprint arXiv:1904.12584,
2019.
[75] K. J. Shih, S. Singh, and D. Hoiem, â€œWhere to look: Focus regions
for visual question answering,â€ in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 4613â€“4621.
[76] J. Lu, J. Yang, D. Batra, and D. Parikh, â€œHierarchical question-
image co-attention for visual question answering,â€ Advances in
neural information processing systems, vol. 29, 2016.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
14
[77] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, â€œNeural module
networks,â€ in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 39â€“48.
[78] H. Laurent and R. L. Rivest, â€œConstructing optimal binary decision
trees is np-complete,â€ Information processing letters, vol. 5, no. 1, pp.
15â€“17, 1976.
[79] M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio,
A. Courville, and R. D. Hjelm, â€œMine: mutual information neural
estimation,â€ arXiv preprint arXiv:1801.04062, 2018.
[80] D. P. Kingma and M. Welling, â€œAuto-encoding variational bayes,â€
arXiv preprint arXiv:1312.6114, 2013.
[81] A. Doucet, A. M. Johansen et al., â€œA tutorial on particle ï¬ltering
and smoothing: Fifteen years later,â€ Handbook of nonlinear ï¬ltering,
vol. 12, no. 656-704, p. 3, 2009.
[82] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, â€œInstance-optimal
compressed sensing via posterior sampling,â€ Proceedings of Ma-
chine Learning Research, vol. 139, 2021.
[83] E. Nijkamp, M. Hill, T. Han, S.-C. Zhu, and Y. N. Wu, â€œOn the
anatomy of mcmc-based maximum likelihood learning of energy-
based models,â€ in Proceedings of the AAAI Conference on Artiï¬cial
Intelligence, vol. 34, no. 04, 2020, pp. 5272â€“5280.
[84] A. Durmus and E. Moulines, â€œHigh-dimensional bayesian infer-
ence via the unadjusted langevin algorithm,â€ Bernoulli, vol. 25,
no. 4A, pp. 2854â€“2882, 2019.
[85] M. Welling and Y. W. Teh, â€œBayesian learning via stochastic
gradient langevin dynamics,â€ in Proceedings of the 28th international
conference on machine learning (ICML-11).
Citeseer, 2011, pp. 681â€“
688.
[86] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, â€œGradient-based
learning applied to document recognition,â€ Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278â€“2324, 1998.
[87] H. Xiao, K. Rasul, and R. Vollgraf, â€œFashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,â€ arXiv
preprint arXiv:1708.07747, 2017.
[88] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto,
and D. Ha, â€œDeep learning for classical japanese literature,â€ arXiv
preprint arXiv:1812.01718, 2018.
[89] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, â€œThe
caltech-ucsd birds-200-2011 dataset,â€ 2011.
[90] R. Misra, â€œNews category dataset,â€ 06 2018.
[91] M. Lavin, â€œAnalyzing documents with tf-idf,â€ 2019.
[92] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBert: Pre-
training of deep bidirectional transformers for language under-
standing,â€ arXiv preprint arXiv:1810.04805, 2018.
[93] J. Hu, L. Shen, and G. Sun, â€œSqueeze-and-excitation networks,â€
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 7132â€“7141.
[94] H. Xiao, K. Rasul, and R. Vollgraf, â€œFashion-mnist: A mnist-like
fashion product database,â€ in GitHub, 2017.
[95] M. Sundararajan, A. Taly, and Q. Yan, â€œAxiomatic attribution for
deep networks,â€ in International conference on machine learning.
PMLR, 2017, pp. 3319â€“3328.
Aditya Chattopadhyay is a PhD student in the
Computer Science Department, Johns Hopkins
University. He received the Bachelor of Technol-
ogy degree in Computer Science and Master of
Science by Research degree in Computational
Natural Sciences from the International Institute
of Information Technology, Hyderabad in 2016
and 2018 respectively. His research interests
include explainable AI, probabilistic graphical
models and Bayesian inference.
Stewart Slocum received his BS in computer
science and applied mathematics from Johns
Hopkins University in 2021. His research inter-
ests center on principled deep learning methods
with performance and robustness guarantees.
Benjamin Haeffele is an Associate Research
Scientist in the Mathematical Institute for Data
Science at Johns Hopkins University. His re-
search interests involve developing theory and
algorithms for processing high-dimensional data
at the intersection of machine learning, optimiza-
tion, and computer vision. In addition to basic
research in data science he also works on a vari-
ety of applications in medicine, microscopy, and
computational imaging. He received his Ph.D. in
Biomedical Engineering at Johns Hopkins Uni-
versity in 2015 and his B.S. in Electrical Engineering from the Georgia
Institute of Technology in 2006.
RenÂ´e Vidal received his B.S. degree in Electrical
Engineering (valedictorian) from the Pontiï¬cia
Universidad CatÂ´olica de Chile in 1997 and his
M.S. and Ph.D. degrees in Electrical Engineering
and Computer Science from the University of
California at Berkeley in 2000 and 2003, respec-
tively. He is currently the Director of the Mathe-
matical Institute for Data Science (MINDS) and
the Hershel L. Seder Professor of Department of
Biomedical Engineering at The Johns Hopkins
University, where he has been since 2004. He
is co-author of the book â€œGeneralized Principal Component Analysisâ€
(Springer 2016), co-editor of the book â€œDynamical Visionâ€ (Springer
2006) and co-author of over 300 articles in machine learning, computer
vision, signal and image processing, biomedical image analysis, hybrid
systems, robotics and control. He is or has been Associate Editor
in Chief of the IEEE Transactions on Pattern Analysis and Machine
Intelligence and Computer Vision and Image Understanding, Associate
Editor or Guest Editor of Medical Image Analysis, the IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, the SIAM Journal
on Imaging Sciences, Computer Vision and Image Understanding, the
Journal of Mathematical Imaging and Vision, the International Journal
on Computer Vision and Signal Processing Magazine. He has received
numerous awards for his work, including the 2021 Edward J. McCluskey
Technical Achievement Award, the 2016 Dâ€™Alembert Faculty Fellowship,
the 2012 IAPR J.K. Aggarwal Prize, the 2009 ONR Young Investigator
Award, the 2009 Sloan Research Fellowship and the 2005 NSF CA-
REER Award. He is a Fellow of the IEEE, Fellow of IAPR, Fellow of
AIMBE, and a member of the ACM and SIAM.
Donald Geman (Life Senior Member, IEEE) re-
ceived the B.A. degree in literature from the
University of Illinois and the Ph.D. degree in
mathematics from Northwestern University. He
was a Distinguished Professor with the Uni-
versity of Massachusetts until 2001, when he
joined the Department of Applied Mathematics
and Statistics, Johns Hopkins University, where
he is currently a member of the Center for Imag-
ing Science and the Institute for Computational
Medicine. His current research interests include
statistical learning, computer vision, and computational biology. He is a
member of the National Academy of Sciences and a fellow of the IMS
and SIAM.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
15
APPENDIX A
In proofs of propositions and lemmas we rewrite the statement (un-numbered) for convenience.
A.1
Characterizing the optimal strategy Ï€âˆ—
In this subsection we characterize the optimal strategy for any such arbitrary query set chosen by the user.
Proposition. Assuming Q is ï¬nite and when d is taken to be the KL-divergence then objective (3) can be rewritten as,
HÏµ
Q(X; Y ) := min
Ï€
EX
|explÏ€
Q(X)|

(21)
s.t.
Ï„ Ï€
X
k=1
I(Y ; SÏ€
k (X) | SÏ€
kâˆ’1(X)) â‰¥I(X; Y ) âˆ’Ïµ
where, Ï„ Ï€ = max{tÏ€(x) : x âˆˆX} and tÏ€(X) is deï¬ned as the number of queries selected by Ï€ for input X until qST OP . We deï¬ne
SÏ€
k (X) as a random variable where any realization SÏ€
k (xobs), xobs âˆˆX , denotes the event
SÏ€
k (xobs) := {xâ€² âˆˆX | {qi, qi(xobs)}1:k = {qi, qi(xâ€²)}1:k},
where qi is the ith query selected by Ï€ for input xobs. Here we use the convention that SÏ€
0 (X) = â„¦(the entire sample space) and
SÏ€
l (X) = SÏ€
tÏ€(X)(X) âˆ€l > tÏ€(X).
Proof. We begin by reformulating our sufï¬ciency constraint in (3) in terms of entropy by taking the â€œdistanceâ€ between
probability distributions as the KL divergence.9 The Ïµ-Sufï¬ciency constraint can then be rewritten as,
Ïµ â‰¥EX[KL
 p(Y | X), p(Y | explÏ€
Q(X)
]
= EX
"
EY
"
log
p(Y | X)
p(Y | explÏ€
Q(X)) | X
##
= EX

EY

log p(Y | X)
P(Y )
| X

+ EX
"
EY
"
log
p(Y )
P(Y | explÏ€
Q(X)) | X
##
= I(X; Y ) âˆ’I(explÏ€
Q(X); Y )
= H(Y | explÏ€
Q(X)) âˆ’H(Y | X)
In the third equality we multiplied the term inside the log by the identity P (Y )
P (Y ) = 1. The ï¬fth inequality is by deï¬nition of
mutual information. Thus, we can rewrite (3) as,
HÏµ
Q(X; Y ) := min
Ï€
EX
|explÏ€
Q(X)|

(22)
s.t. H(Y | explÏ€
Q(X)) âˆ’H(Y | X) â‰¤Ïµ
(Ïµ-Sufï¬ciency)
Letâ€™s deï¬ne tÏ€(X) as the number of queries selected by Ï€ for input X until qST OP . Deï¬ne Ï„ Ï€ = max{tÏ€(X) : X âˆˆX}.
For the purpose of analysis we can vacuously modify Ï€ such that for any given xobs âˆˆX , Ï€ asks a ï¬xed Ï„ Ï€ number of
queries by ï¬lling the remaining Ï„ Ï€ âˆ’tÏ€(xobs) queries with qST OP . An immediate consequence of this modiï¬cation is that
SÏ€
l (X) = SÏ€
tÏ€(X)(X) âˆ€l > tÏ€(X).
We will now show that the sufï¬ciency criteria H(Y | explÏ€
Q(X)) âˆ’H(Y | X) can be rewritten as a sum of successive
mutual information terms.
H(Y | explÏ€
Q(X)) âˆ’H(Y | X)
= (H(Y ) âˆ’H(Y | X)) âˆ’(H(Y ) âˆ’H(Y | explÏ€
Q(X)))
= I(X; Y ) âˆ’(H(Y ) âˆ’H(Y | SÏ€
Ï„ (X))
(23)
The third equality uses the fact that H(Y | explÏ€
Q(X)) = H(Y | SÏ€
Ï„ (X)) since the âˆ€x âˆˆX, SÏ€
Ï„ (x) = SÏ€
tÏ€(x)(x).
We can now write H(Y ) âˆ’H(Y | SÏ€
Ï„ (X)) as a telescoping series,
H(Y ) âˆ’H(Y | SÏ€
Ï„ (X) = H(Y ) âˆ’H(Y | SÏ€
1 (X)) + H(Y | SÏ€
1 (X)) âˆ’H(Y | SÏ€
1 (X), SÏ€
2 (X))
+ H(Y | SÏ€
1 (X), SÏ€
2 (X)) . . . + H(Y | SÏ€
1 (X), SÏ€
2 (X), . . . , SÏ€
Ï„âˆ’1(X)) âˆ’H(Y | SÏ€
Ï„ (X))
= I(Y ; SÏ€
1 (X)) + I(Y ; SÏ€
2 (X) | SÏ€
1 (X))) + . . . + I(Y ; SÏ€
Ï„ (X) | SÏ€
Ï„âˆ’1(X))
(24)
The last equality is obtained by noticing that
9. In favour of a clearer exposition, we abuse notation here and use p(Y | explÏ€
Q(X)) to denote P(Y | SÏ€
tÏ€(X)(X)). In reality, explÏ€
Q(x) refers
to the sequence of query-answer pairs chosen for x as deï¬ned in (2) whereas SÏ€
tÏ€(X=x)(X = x) refers to the event which is the set of all possible
data-points that agree on the ï¬rst tÏ€(x) query-answers observed for x.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
16
1) H(Y | SÏ€
1 (X), . . . , SÏ€
kâˆ’1(X)) âˆ’H(Y | SÏ€
1 (X), . . . , SÏ€
k (X)) = I(Y ; SÏ€
k (X) | SÏ€
kâˆ’1(X)),
2) H(Y | SÏ€
Ï„ (X)) = H(Y | SÏ€
1 (X), SÏ€
2 (X), . . . , SÏ€
Ï„ (X)) since the events {SÏ€
k (x)}1:Ï„ are nested âˆ€x âˆˆX .
Putting it all together we can rewrite (22) as,
HÏµ
Q(X; Y ) := min
Ï€
EX
|explÏ€
Q(X)|

(25)
s.t. I(X; Y ) âˆ’
Ï„ Ï€
X
k=1
I(Y ; SÏ€
k (X) | SÏ€
kâˆ’1(X)) â‰¤Ïµ
A.2
Approximation guarantees for IP
In Proposition 2 we prove that the IP strategy comes within 1 bit of H(Y ) (the entropy of Y ) under the assumption that
one has access to all possible binary functions of X, that are also binary functions of Y , as queries. Given such a query set,
it is well-known that the optimal strategy Ï€âˆ—is given by the Huffman Code for Y which is also within 1 bit of H(Y ). Thus,
the result is immediate that IP asks at most 1 query more than Ï€âˆ—on average. We restate Proposition 2 below for ease.
Proposition. Let Y be discrete. Let ËœHQ(X; Y ) be the expected description length obtained by the IP strategy. If H(Y |X) = 0 and
Q is the set of all possible binary functions of X such that H(q(X) | Y ) = 0 âˆ€q âˆˆQ, then H(Y ) â‰¤ËœHQ(X; Y ) â‰¤H(Y ) + 1.
We make two remarks before turning to the proof.
Remark 1: We have observed data X, a categorical discrete r.v. Y , and binary queries {q(X), q âˆˆQ} from which Y can be
estimated. In fact, letâ€™s suppose that Y is determined by X, say Y = f(X); equivalently, H(Y |X) = 0. We assume that Y
represents some high-level, possibly semantic, interpretation of X which can only be seen through the eyes of the queries
q âˆˆQ. Ideally, we would like to be able to query the membership of Y in any subset of Y (the set of possible values of
Y ); this is the information we would have in coding Y . We will say that the query IY âˆˆD is realizable for some D âŠ‚Y if
the query IXâˆˆf âˆ’1(D) is in Q, i.e., we can test for Y âˆˆD by one of our observable data queries. In general, not all subsets
of Y can be associated with attributes or features of X, e.g., â€œNapoleanâ€ or â€œDeadâ€ in â€œ20 Questionsâ€, or â€œblack beakâ€ in
bird species classiï¬cation. If there was a query q(X) for every subset D of values of Y , then our theorem says that mean
number of queries needed to determine the state of Y with IP is bounded below by H(Y ) and above by H(Y ) + 1.
Remark 2: The sequence of queries q1, q2, ... generated by the IP algorithm for a particular data point can be seen
as one branch, root to leaf, of a decision tree constructed by the standard machine learning strategy based on
successive reduction of uncertainty about Y as measured by mutual information: q1 = arg maxqâˆˆQ I(q(X); Y ), qk+1 =
arg maxqâˆˆQ I(q(X); Y |SIP
k (x0)) where the SIP
k (x0) is the event that for the ï¬rst k questions the answers agree with those
for x0. We stop as soon as Y is determined. Whereas a decision tree accommodates all x simultaneously, the questions
along the branch depends on having a particular, ï¬xed data point. But the learning problem in the branch version (â€œactive
testingâ€) is exponentially simpler.
Proof. The lower bound H(Y ) â‰¤ËœHQ(X; Y ) comes from Shannonâ€™s source coding theorem for stochastic source Y .
Now for the upper bound, since I(q(X); Y | SIP
k (x0)) = H(q(X)|SIP
k (x0)) âˆ’H(q(X)|Y, SIP
k (x0)) and since Y determines
q(Y ) and hence also q(X), the second entropy term is zero (since given H(q(X) | Y ) = 0). So our problem is maximize
the conditional entropy of the binary random variable q(X) given SIP
k (x0). So the IP algorithm is clearly just â€œdivide and
conquerâ€:
q1 = arg max
qâˆˆQ
H(q(X)),
qk+1 = arg max
qâˆˆQ
H(q(X)|SIP
k (x0)).
Equivalently, since entropy of a binary random variable Ï is maximized when P(Ï) = 1
2,
qk+1 = arg min
qâˆˆQ |P(q(X) = 1|SIP
k (x0)) âˆ’1
2|.
Let Yk be the set of â€œactive hypothesesâ€ after k queries (denoted as Ak), namely those y with positive posterior
probability: P(Y = y|SIP
k (x0)) > 0. Indeed,
P(Y = y|SIP
k (x0)) =
P(SIP
k (x0)|Y = y)p(y)
P
y P(SIP
k (x0)|Y = y)p(y)
=
1Ykp(k)
P
yâˆˆAk p(y)
since
P(SIP
k (x0)|Y = y) =
 1,
if y âˆˆAk
0,
y /âˆˆAk

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
17
In particular, the classes in the active set have the same relative weights as at the outset. In summary:
p(y|SIP
k (x0)) =
 p(y)/ P
Ak p(l), y âˆˆAk
0,
otherwise
The key observation to prove the theorem is that if a hypothesis y generates the same answers to the ï¬rst m or more
questions as y0, and hence is active at step m, then its prior likelihood p(y) is at most 2âˆ’(mâˆ’1), m = 1, 2, . . .. This is
intuitively clear: if y has the same answer as y0 on the ï¬rst question, and p(y0) > 1
2, then only one question is needed and
the active set is empty at step two; if q1(y) = q1(y0) and q2(y) = q2(y0) and p(y0) > 1
4, then only two question are needed
and the active set is empty at step three, etc.
Finally, since C, the code length, takes values in the non-negative integers {0, 1, . . . , }:
ËœHQ(X; Y )
:=
E[C]
=
âˆ
X
m=1
P(C â‰¥m)
â‰¤
âˆ
X
m=1
P(p(Y ) < 2âˆ’(mâˆ’1))
=
âˆ
X
m=1
X
y:p(y)<2âˆ’(mâˆ’1)
p(y)
=
X
yâˆˆY
âˆ
X
m=1
1{p(y)<2âˆ’(mâˆ’1)}p(k)
=
X
yâˆˆY
p(k)(1 âˆ’log p(k))
=
H(Y ) + 1
A.3
Termination Criteria for IP
We would ï¬rst analyze the termination criteria for the exact case, that is, Ïµ = 0 in (3), and then move on to the more general
case.
Termination Criteria when Ïµ = 0 Ideally for a given input xobs, we would like to terminate (IP outputs qST OP ) after L
steps if
p(y | xobs) = p(y | xâ€²) âˆ€xâ€² âˆˆSIP
L (xobs), y âˆˆY
(26)
Recall, SIP
L (xobs) = {xâ€² âˆˆX | {qi, qi(xâ€²)}1:L = {qi, qi(xobs)}1:L}. In other words, its the event consisting of all xâ€² âˆˆX
which share the ï¬rst L query-answer pairs with xobs.
If (26) holds for all xobs âˆˆX , the it is easy to see that this is equivalent to the sufï¬ciency constraint in the case Ïµ = 0,
p(y | x) = p(y | explIP
Q(x)) âˆ€(x, y) Ã— (X Ã— Y)
where p(y | explIP
Q(x)) := p(y | SIP
tIP(xobs)(xobs)) âˆ€(x, y) âˆˆ(X Ã— Y) and tIP(x) is the number of iterations IP takes on input x
before termination.
Unfortunately, detecting (26) is difï¬cult in practice. Instead we have the following lemma which justiï¬es our stopping
criteria for IP.
Lemma 1. For a given input xobs if event SIP
L (xobs) (after asking L queries) satisï¬es the condition speciï¬ed by (26) then for all
subsequent queries qm, m â‰¥L, maxqâˆˆQ I(q(X); Y |SIP
m(xobs)) = 0.
Refer to Appendix A.4 for a proof.
Inspired from Lemma 1 we formulate an optimistic stopping criteria as,
L = inf{k âˆˆ{1, 2, ..., |Q|} : max
qâˆˆQ I(q(X); Y |SIP
m(xobs)) = 0 âˆ€m â‰¥k, m â‰¤|Q|}
(27)
Evaluating (27) would be computationally costly since it would involve processing all the queries for every input x. We
employ a more practically amenable criteria
qL+1 = qST OP
if
max
qâˆˆQ I(q(X); Y |SIP
m(xobs)) = 0 âˆ€m âˆˆ{L, L + 1, ..., L + T}
(28)
T > 0 is a hyper-parameter chosen via cross-validation. Note, it is possible that there does not exist any informative
query in one iteration, but upon choosing a question there suddenly appears informative queries in the next iteration.
For example, consider the XOR problem. X âˆˆR2 and Y âˆˆ{0, 1}. Let Q be the set to two axis-aligned half-spaces. Both

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
18
half-spaces have zero mutual information with Y . However, upon choosing any one as q1, the other half-space is suddenly
informative about Y . Equation (28) ensures that we do not stop prematurely.
Termination Criteria for general Ïµ when d is taken as the KL-divergence For a general Ïµ > 0 we would like IP to terminate
such that on average,
EX[KL
 p(Y | X), p(Y | explIP
Q(X)
] â‰¤Ïµ
(29)
Detecting this is difï¬cult in practice since IP is an online algorithm and only computes query-answers for a given input
x. So it is not possible to know apriori when to terminate such that in expectation the KL divergence would be less than Ïµ.
Instead we opt for the stronger requirement that,
KL
 p(Y | x), p(Y | explIP
Q(x)
 â‰¤Ïµ
âˆ€x âˆˆX.
(30)
It is easy to see that (30) implies (29).
As before, p(y | explIP
Q(x)) := p(y | SIP
tIP(xobs)(xobs)) âˆ€(x, y) âˆˆ(X Ã— Y) and tIP(x) is the number of iterations IP takes on
input x before termination. We have the following lemma (analogous to the Ïµ = 0 case).
Lemma 2. We make the following assumptions:
1) Y is a countable set (recall Y âˆˆY).
2) For any xobs âˆˆX and x1, x2 âˆˆSIP
tIP(xobs)(xobs), we have p(Y | x1) and p(Y | x2) have the same support.
Then, for given input xobs if event SIP
tIP(xobs)(xobs) (after asking tIP(xobs) queries) satisï¬es the condition speciï¬ed by (30) then for all
subsequent queries qm, m â‰¥tIP(xobs), maxqâˆˆQ I(q(X); Y |SIP
m(xobs)) â‰¤Ïµâ€², where Ïµâ€² = CÏµ for some constant C > 0.
Refer to Appendix A.5 for a proof. The assumption of Y being countable is typical for supervised learning (the scenario
considered in this paper) where the set of labels is often ï¬nite. The second assumption intuitively means that P(y | x2) =
0 =â‡’P(y | x1) = 0 for any y âˆˆY. This is a reasonable assumption since we envision practical scenarios in which Ïµ is
close to 0 and thus different inputs which share the same query-answers until termination by IP are expected to have very
â€œsimilarâ€ posteriors.10
Inspired from Lemma 2 we formulate an optimistic stopping criteria âˆ€xobs âˆˆX as,
tIP(xobs) = inf{k âˆˆ{1, 2, ..., |Q|} : max
qâˆˆQ I(q(X); Y |SIP
m(xobs)) â‰¤Ïµâ€² âˆ€m â‰¥k, m â‰¤|Q|}
(31)
Evaluating (31) would be computationally costly since it would involve processing all the queries for every input xobs. We
employ a more practically amenable criteria
qtIP(xobs)+1 = qST OP
if
max
qâˆˆQ I(q(X); Y |SIP
m(xobs)) â‰¤Ïµâ€² âˆ€m âˆˆ{L, L + 1, ..., L + T}
(32)
T > 0 is a hyper-parameter chosen via cross-validation.
A.4
Proof of Lemma 1
Proof. Recall each query q partitions the set X and SIP
L (xobs) is the event {xâ€² âˆˆX | {qi, qi(xobs)}1:L = {qi, qi(xâ€²)}1:L}. It is
easy to see that if SIP
L (x) satisï¬es the condition speciï¬ed by (26) then
P(y | SIP
m(xobs)) = P(y | xâ€²) âˆ€xâ€² âˆˆSIP
m(xobs) âˆ€m â‰¥L, âˆ€q âˆˆQ
(33)
This is because subsequent query-answers partition a set in which all the data points have the same posterior distributions.
Now, âˆ€q âˆˆQ, âˆ€a âˆˆRange(q), y âˆˆY
p(q(X) = a, y|SIP
m(xobs)) = p(q(X) = a | SIP
m(xobs))p(y | q(X) = a, SIP
m(xobs))
(34)
(34) is just an application of the chain rule of probability. The randomness in q(X) is entirely due to the randomness in X.
For any a âˆˆRange(q), y âˆˆY
p(y | q(X) = a, SIP
m(xobs)) =
X
xâ€²
p(y, X = xâ€² | a, SIP
m(xobs))
=
X
xâ€²
p(y | X = xâ€², a, SIP
m(xobs))p(X = xâ€² | a, SIP
m(xobs))
=
X
xâ€²
p(y | X = xâ€²)p(X = xâ€² | a, SIP
m(xobs))
= p(y | SIP
m(xobs))
X
xâ€²
p(X = xâ€² | a, SIP
m(xobs))
= p(y | SIP
m(xobs))
(35)
10. We refer to the distribution p(y | x) for any x âˆˆX as the posterior distribution of x.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
19
The ï¬rst equality is an application of the law of total probability, third due to conditional independence of the history
and the hypothesis given X = xâ€² (assumption) and the fourth by invoking ((33)).
Substituting (35) in (34) we obtain Y âŠ¥âŠ¥q(X) | SIP
m(xobs)) âˆ€m â‰¥L, q âˆˆQ. This implies that for all subsequent queries
qm, m > L, maxqâˆˆQ I(q(X); Y |SIP
m(xobs))) = 0. Hence, Proved.
A.5
Proof of Lemma 2
Proof. Condition (30) implies bounded KL divergence between inputs on which IP has identical query-answer
trajectories Recall SIP
tIP(xobs))(xobs)) is the event {xâ€² âˆˆX | {qi, qi(xobs))}1:tIP(xobs)) = {qi, qi(xâ€²)}1:tIP(xobs))}. If SIP
tIP(xobs))(xobs))
satisï¬es (30) then using Pinskerâ€™s inequality we conclude,
Î´(p(Y | xobs)), p(Y | SIP
tIP(xobs))(xobs))) â‰¤
r Ïµ
2
(36)
Here Î´ is the total variational distance between the two distributions. Since Î´ is a metric we conclude for any x1, x2 âˆˆ
SIP
tIP(xobs))(xobs)),
Î´(p(Y | x1), p(Y | x2)) â‰¤Î´(p(Y | x1), p(Y | SIP
tIP(xobs)(xobs)) + Î´(p(Y | x2), p(Y | SIP
tIP(xobs)(xobs))
â‰¤
âˆš
2Ïµ
(37)
Since Y is countable, deï¬ne Î· = min{p(y | Ë†x) : y âˆˆY, p(y | Ë†x)) > 0, Ë†x âˆˆX}. Then, by the reverse Pinskerâ€™s inequality
we conclude,
KL(p(Y | x1), p(Y | x2))) â‰¤Ïµ
Î· =: Ïµâ€²
âˆ€x1, x2 âˆˆSIP
tIP(xobs)(xobs)
(38)
Note, the above upper bound holds since by assumption 2, p(Y | x1) and p(Y | x2)) have the same support.
Bounded KL divergence between inputs implies subsequent queries have mutual information bounded by Ïµ For any
subsequent query q âˆˆQ that IP asks about input xobs we have âˆ€x âˆˆSIP
tIP(xobs)(xobs),
KL

p(Y | x), p(Y | SIP
tIP(xobs)+1(xobs))

=
X
Y
p(Y | x) log p(Y | x) âˆ’
X
Y
p(Y | x) log p(Y | SIP
tIP(xobs)+1(xobs))
(39)
where, SIP
tIP(xobs)+1(xobs) := SIP
tIP(xobs)(xobs) âˆ©{xâ€² âˆˆX : q(xâ€²) = q(x)}. For brevity, we denote SIP
tIP(xobs)+1(xobs) as B,
X
Y
p(Y | x) log p(Y | B) =
X
Y
p(Y | x) log
" X
xâ€²âˆˆB
p(Y | xâ€², B)P(xâ€² | B)
#
=
X
Y
p(Y | x) log
" X
xâ€²âˆˆB
p(Y | xâ€²)P(xâ€² | B)
#
â‰¥
X
Y
p(Y | x)
X
xâ€²âˆˆB
p(xâ€² | B) log p(Y | xâ€²)
=
X
xâ€²âˆˆB
p(xâ€² | B)
X
Y
p(Y | x) log p(Y | xâ€²)
(40)
In the third inequality Jensenâ€™s inequality was used. Substituting (40) in (39),
KL (p(Y | x)||p(Y | B)) â‰¤
X
xâ€²âˆˆB
p(xâ€² | B)
X
Y
p(Y | x) log p(Y | x)
p(Y | xâ€²)
â‰¤Ïµâ€² X
xâ€²
p(xâ€² | B)
= Ïµâ€²
(41)
In the second inequality we substituted from (38) since x, xâ€² âˆˆB âŠ†SIP
tIP(xobs)(xobs). In the third equality we used the identity
P
xâ€²âˆˆB p(xâ€² | B) = 1,
It is easy to see that (41) holds for all xâ€² âˆˆSIP
tIP(xobs)+1(xobs) and thus I(X; Y | SIP
tIP(xobs)+1(xobs)) â‰¤Ïµ
Since Y â†’X â†’q(X) we can apply the data-processing inequality to obtain,
I(q(X); Y | SIP
tIP(xobs)+1(xobs)) â‰¤I(X; Y | SIP
tIP(xobs)+1(xobs)) â‰¤Ïµâ€²
âˆ€q âˆˆQ.
This implies that for all subsequent queries qm, m > tIP(xobs), maxqâˆˆQ I(q(X); Y |SIP
m(xobs)) â‰¤Ïµ. Hence, Proved.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
20
A.6
Complexity of the Information Pursuit Algorithm
For any given input x, the per-iteration cost of the IP algorithm is O(N + |Q|m), where |Q| is the total number of queries,
N is the number of ULA iterations, and m is cardinality of the product sample space q(X) Ã— Y . For simplicity we assume
that the output hypothesis Y and query-answers q(X) are ï¬nite-valued and also that the number of values query answers
can take is the same but our framework can handle more general cases.
More speciï¬cally, to compute qk+1 = arg maxqâˆˆQ I(q(X); Y | SIP
k (xobs)). We ï¬rst run ULA for N iterations to get
samples from p(z | y, SIP
k (x)) which are then used to estimate the distribution p(q(x), y | SIP
k (x)) (using (15)) for every query
q âˆˆQ and every possible query-answer hypothesis, (q(x), y), pair. This incurs a cost of O(N +|Q|m). We then numerically
compute the mutual information between query-answer q(X) and Y given history for every q âˆˆQ as described in (18).
This has a computational complexity O(|Q|m). Finally, we search over all q âˆˆQ to ï¬nd the query with maximum mutual
information (refer (6)).
It is possible to reduce N by using advanced MCMC sampling methods which converge faster to the required
distribution p(z | y, SIP
k (x))). The linear cost of searching of all queries can also be reduced by making further assumptions
about the structure of the query set. For example, we conjecture that this cost can be reduced to log |Q| using hierarchical
query sets where answers to queries would depend upon to answers to queries higher up in the hierarchy. Note that
if the query answers q(X) and Y are continuous random variables then we would need to resort to sampling to
construct stochastic estimates of the mutual information between q(X) and Y instead of carrying our explicit numerical
computations. We would explore these directions in future work.
A.7
Network Architectures and Training Procedure
Here we describe the architectures and training procedures for the Î²-VAEs used to calculate mutual information as
described in Section 4.
A.7.1
Architectures
Conv 32
Conv 64
MaxPool
Conv 128
Conv 256
MaxPool
Image X
28x28x3
FC 256
Label Y
10x1
Global Avg Pool
FC 2048
FC 100
FC 2048
FC 100
Concat
Î¼
Î£
FC 256
Label Y
10x1
Concat
FC 256
ConvTranspose 256
kernel 4
ConvTranspose 256
kernel 2, stride 2
ConvTranspose 128
ConvTranspose 64
ConvTranspose 64
kernel 2, stride 2
ConvTranspose 64
kernel 2, stride 2
ConvTranspose 32
ConvTranspose 3
Generated Image
28x28x3
Encoder
Decoder
Latent Z
100x1
Latent Z ~ N(Î¼, Î£)
100x1
Fig. 7. Binary Image VAE
Binary Image Classiï¬cation In the encoder, all convolutional layers use kernel size 3 and stride 1, and max pool layers use a
pooling window of size 2. In the decoder all transposed convolutions use kernel size 3 and stride 1 unless otherwise noted.
In both the encoder and decoder, all non-pooling layers are followed by a BatchNorm layer and LeakyReLU activation
(with slope -0.3) except for the ï¬nal encoder layer (no nonlinearities) and the ï¬nal decoder layer (sigmoid activation).

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
21
FC 512
FC 128
Concepts C
312x1
Label Y
200x1
FC 100
FC 100
Concat
Î¼
Î£
Label Y
200x1
Concat
FC 128
FC 512
Generated Concepts
312x1
Z ~ N(Î¼, Î£)
100x1
Latent Z
100x1
Encoder
Decoder
FC 512
FC 128
Bag-of-Words X
1000x1
Label Y
10x1
FC 100
FC 100
Concat
Î¼
Î£
Label Y
10x1
Concat
FC 128
FC 512
Generated Words X
1000x1
Z ~ N(Î¼, Î£)
100x1
Latent Z
100x1
Encoder
Decoder
(a) CUB Concepts VAE
(b) HuffPost News Headlines VAE
Fig. 8. CUB Concepts and HuffPost News Headlines VAEs
CUB Bird Species Classiï¬cation and HuffPost News Headline Classiï¬cation For CUB and HuffPost News, we use
essentially the same VAE architecture, only designed to handle different-sized inputs X and one-hot labels Y . All layers
are followed by a BatchNorm layer and ReLU activation except for the ï¬nal encoder layer (no nonlinearities) and the ï¬nal
decoder layer (sigmoid activation).
A.7.2
Training
The Î²-VAE was trained by optimizing the Evidence Lower BOund (ELBO) objective
max
Ï‰,Ï† ELBO(Ï‰, Ï†) =
n
X
i=1
h
EpÏ†(z|y(i),x(i))[log pÏ‰(x(i) | z, y(i))] âˆ’Î²DKL(pÏ†(z | y(i), x(i))âˆ¥p(z))
i
(42)
where pÏ†(z | y(i), x(i)) denotes the encoder and pÏ‰(x(i) | z, y(i)) the decoder. The prior over latents p(z) is taken to be
standard Gaussian.
For all binary image datasets, we trained our VAE for 200 epochs using Adam with learning rate 0.001 and the Î²-VAE
parameter Î² = 5.0. We also trained the CUB VAE for 200 epochs using Adam with learning rate 0.001 but with Î² = 1.0.
Finally, we trained the HuffPost News VAE for 100 epochs using Adam with learning rate 0.0002 and Î² = 1.0.
APPENDIX B
B.1
HuffPost News Task and Query Set
The Hufï¬ngton Post News Category dataset consists of 200,853 articles published by the Hufï¬ngton Post between 2012
and 2018. Each datapoint contains the article â€œheadlineâ€, a â€œshort descriptionâ€ (a one to two-sentence-long continuation
of the headline), and a label for the category/section it was published under in the newspaper. We concatenate the article
headline and short description to form one extended headline. Additionally, many of the 41 category labels are redundant
(due to changes in how newspaper sections were named over the years), are semantically ambiguous and HuffPost-speciï¬c
(e.g. â€œImpactâ€, â€œFiftyâ€, â€œWorldpostâ€), or have very few articles. Therefore, we combine category labels for sections with
equivalent names (e.g. â€œArts & Cultureâ€ with â€œCulture & Artsâ€), remove ambiguous HuffPost-speciï¬c categories, and then
keep only the 10 most frequent categories to ensure that each category has an adequate number of samples. This leaves us
with a ï¬nal dataset of size 132,508 and category labels â€œEntertainmentâ€, â€œPoliticsâ€, â€œQueer Voicesâ€, â€œBusinessâ€, â€œTravelâ€,
â€œParentingâ€, â€œStyle & Beautyâ€, â€œFood & Drinkâ€, â€œHome & Livingâ€, and â€œWellnessâ€.
B.2
Comparison Models
B.2.1
MAP using Q
For IP, we use ULA to sample from p(z | y, SIP
k (x)), but now that we have access to the all the query answers, Q(x), we
can improve performance by making use of the VAEâ€™s encoder instead. Following equation 16, we draw many samples
from the encoder p(z | Q(x), y) and then decode these samples to estimate the VAEâ€™s posterior distribution p(y|Q(x)).
For each problem, we set the number of samples to be the same as what we draw during each iteration of IP (12,000 for
binary image tasks, 12,000 for CUB, 10,000 for HuffPost News). We expect the accuracy of this model to serve as an upper
bound for what we can achieve with IP given that it uses all queries. Our task-speciï¬c VAE architectures can be found in
Appendix A.7.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
22
B.2.2
Black-Box Using Q
We also compare to non-interpretable supervised models which receive all queries Q(x) as input and try to predict the
associated label y. This allows a comparison between the accuracy of the posterior of our generative model and traditional
supervised approaches on the chosen interpretable query set.
For the binary image datasets, we use a simple CNN where all convolutional layers use kernel size 3 Ã— 3 and a stride
of 1, all max pooling uses a kernel size of 2. For CUB and HuffPost News we use simple MLPs.
TABLE 3
Binary Image CNN Architecture
Layer
Input Size/Channels
Output Size/Channels
Nonlinearity
Convolution
3
32
BatchNorm + ReLU
Convolution
32
64
BatchNorm + ReLU + MaxPool
Convolution
64
128
BatchNorm + ReLU
Convolution
128
256
BatchNorm + ReLU + MaxPool + Global Avg Pool
Fully-connected
256
2048
BatchNorm + ReLU
Fully-connected
2048
10
Sigmoid
TABLE 4
CUB Attributes MLP Architecture
Layer
Input Size/Channels
Output Size/Channels
Nonlinearity
Fully-connected
312
100
ReLU
Fully-connected
100
25
ReLU
Fully-connected
25
200
Sigmoid
TABLE 5
HuffPost Bag-of-Words MLP Architecture
Layer
Input Size/Channels
Output Size/Channels
Nonlinearity
Fully-connected
1000
100
ReLU
Fully-connected
100
25
ReLU
Fully-connected
25
10
Sigmoid
B.2.3
Black-Box
Since we ourselves pre-processed the cleaned 10-class version of HuffPost News, there are no reported accuracies in
the literature to compare IP with. Therefore, as a strong black-box baseline, we ï¬ne-tune a pre-trained Bert Large
Uncased Transformer model [92] with an additional dropout layer (with dropout probability 0.3) and randomly initialized
fully-connected layer. Our implementation is publicly available at https://www.kaggle.com/code/stewyslocum/news-
classiï¬cation-using-bert.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
23
B.3
Additional Example Runs
B.3.1
IP with Various Patch Scales
For binary image classiï¬cation, we also experimented with patch queries of sizes other than 3 Ã— 3, from single pixel 1 Ã— 1
queries up to 4 Ã— 4 patches.
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
k=0
k=1
k=2
k=3
k=4
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
k=0
k=1
k=2
k=3
k=4
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
(a) IP with 1x1 patch queries
(b) IP with 2x2 patch queries
(c) IP with 3x3 patch queries
(d) IP with 4x4 patch queries
k=0
k=1
k=2
k=4
k=5
k=6
k=7
k=11
k=20
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
Fig. 9. IP on MNIST with different sized patch queries. In each subï¬gure, the top row displays the test image with red boxes denoting the current
queried patch and blue boxes denoting previous patches. The second row shows the revealed portion of the image that IP gets to use at each
query. The ï¬nal row shows the modelâ€™s estimated posteriors at each query. For conciseness in (a), we only display the 8 iterations of IP with highest
KL divergence between successive posteriors (i.e. the most inï¬‚uential iterations). Observe that at all patch sizes, the queries chosen by IP cover
roughly the same parts of the image, illustrating the importance of this region during classiï¬cation. Reaching the stopping criteria of 99% posterior
conï¬dence takes 20 queries with 1 Ã— 1 patches, 7 queries with 2 Ã— 2 patches, 4 queries with 3 Ã— 3 patches, and 4 queries with 4 Ã— 4 patches.
TABLE 6
Number of queries and pixels gleaned by IP (until termination) using query sets of different patch scales on MNIST.
Patch Size
1 Ã— 1
2 Ã— 2
3 Ã— 3
4 Ã— 4
# Queries
21.1
9.6
5.2
4.6
# Pixels
21.1
32.8
44.7
54.7
% Pixels
2.7
4.2
5.7
6.9
While 1 Ã— 1 patch queries use the smallest total number of pixels and most similarly resemble existing post-hoc
attribution maps, they lead to a large number of scattered queries that are hard to interpret individually. On the other
extreme, as the patch size grows larger, the number of total queries decreases, but queries becomes harder to interpret
since each patch would contain many image features. On the MNIST dataset, we found 3 Ã— 3 patches to be a sweet spot
where explanations tended to be very short, but were also at a level of granularity where each patch could be individually
interpreted as a single edge or stroke. Remarkably, at all chosen patch scales, only a very small fraction (2-7%) of the image
needs to be revealed to classify the images with high conï¬dence.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
24
B.3.2
IP for Binary Image Classiï¬cation
Now we provide additional example runs of IP for each of the three binary image classiï¬cation datasets.
As in Figure 4 in the main paper, in each plot, the top row displays the test image with red boxes denoting the current
queried patch and blue boxes denoting previous patches. The second row shows the revealed portion of the image that IP
gets to use at each query. The ï¬nal row shows the modelâ€™s estimated posteriors at each query.
k=0
k=1
k=2
k=3
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
(a) Each query reveals the patch with maximum mutual information with Y , conditioned on query history. This is initially independent
of the particular image and asks for the pixel intensities in the center patch (see k = 1 in row 1). After the ï¬rst query reveals that the
center patch is all black, the posterior concentrates on â€œ0â€ and â€œ7â€. After observing a white corner in the bottom left (which would
be black for a â€œ7â€), the model becomes conï¬dent that the image is a â€œ0â€, and even more so after one ï¬nal query when it reaches
termination.
k=0
k=1
k=2
k=3
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
(b) The ï¬rst query reveals a vertical white stroke in the center of the image, leading to a concentration of the posterior on â€œ1â€. In the
next two queries, IP determines that there is a single long vertical stroke center stroke taking up the entire height of the image, and so
it reaches a 99% conï¬dence of the image being a â€œ1â€.
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
(c) As in the top left example, the ï¬rst query is all black, and the posterior mass shifts onto â€œ0â€. Because the answer to this ï¬rst query
is identical as in the top left example, the second query chosen, in the bottom left of the image, is also the same. The response to this
query is also a white corner as in the top left example, and so the posterior continues to concentrate on â€œ0â€, and the third query is also
in the same left area of the image. However, this third query reveals a black patch, indicating that the image might be a â€œ5â€ instead
of a â€œ0â€. In the remaining four queries, IP discovers other portions of the â€œ5â€ digit and ï¬nally arrives at the right answer with high
conï¬dence.
Fig. 10. Additional Examples of IP on MNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
25
Recall that in order to improve performance on the KMNIST and FashionMNIST datasets (at the expense of asking
a few more queries) we modiï¬ed IPâ€™s termination criteria to include a stability condition: terminate when the original
criterion (maxY p(Y |SIP
k (x)) â‰¥0.99) is true for 5 queries in a row.
ã™
ãŠã
ã¤ãªã¯ã¾ã‚„ã‚Œã‚’
ki
su
tsu
na
ha
ma
ya
re
wo
o
(a) KMNIST is a 10-class dataset of handwritten Japanese Hiragana characters. To assist the reader in understanding the examples
below, we display each typed character along with its romanized name.
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=11
okisutsunahamayarewo
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
(b) For each image, IP selects the ï¬rst query to be in the middle right of the image, where several characters are likely to have a stroke.
Upon ï¬nding none, IP rules out â€œoâ€, â€œtsuâ€, and â€œyaâ€ but otherwise distributes probability mass rather equally. On the second query,
IP discovers a closed loop in the bottom of the character, a clear sign of â€œnaâ€ and â€œmaâ€, which increase the most. The discovery of the
double crosses in the remaining queries concentrate the posterior on â€œmaâ€ until termination. For conciseness, we only display the 8
iterations of IP with highest KL divergence between successive posteriors (i.e. the most inï¬‚uential iterations).
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=8
okisutsunahamayarewo
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
(c) In the ï¬rst query, IP discovers a left edge, hinting at the presence of a large loop on the right side of the image, features of â€œoâ€ and
â€œtsuâ€. The second query is likely intended to disambiguate these two characters as â€œoâ€ contains white strokes in this region. Upon
ï¬nding a black patch here, IP is already conï¬dent, and reaches 99% conï¬dence in just four queries.
Fig. 11. Additional Examples of IP on KMNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
26
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=10
okisutsunahamayarewo
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
(d) The ï¬rst query reveals a right edge, suggesting a slightly smaller loop on the right side of the image, a feature shared by several
characters. However, most of these queries have a busy center region except for â€œtsuâ€, whose probability increases after the second query
reveals a black patch. The next two queries outline the shape of the loop which is very large, a distinctive characteristic of â€œtsuâ€.
Fig. 11. Additional Examples of IP on KMNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
27
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=8
k=13
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
(a) On this dataset, IP always selects the ï¬rst query to be the patch in the top center, which being all black in this case rules out the
possibility of the object being a type of pant or upper body garment, which would take up the entire height of the image. Over the next
several queries, IP focuses on queries that would allow it to distinguish types of shoes from each other, in particular ï¬nding the shoe to
have a high top and a small heel, eventually causing the posterior to concentrate on the correct â€œAnkle Bootâ€ category. For conciseness,
we only display the 8 iterations of IP with highest KL divergence between successive posteriors (i.e. the most inï¬‚uential iterations).
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=8
k=16
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
(b) The ï¬rst query detects a white corner in the top center of the image, which hints at the presence of a collar, causing the posterior
mass to move to the â€œCoatâ€ and â€œShirtâ€ categories. Determining between these two categories is relatively difï¬cult however, especially
with binary images. But in general, coats tend to be bulkier than shirts. Therefore, after ï¬nding a white patch in iteration k = 2, the
probability of â€œCoatâ€ slightly increases, but as more partially black queries along the outside of the shirt are revealed and the slimmer
outline of the shirt comes into view, the posterior converges on the correct category of â€œShirtâ€.
k=0
k=1
k=2
k=3
k=4
k=5
k=7
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
(c) Detecting an all white patch in the top center, IP rules out the possibility of the image being some type of shoe. The second query in
the lower left returns a black patch, suggesting that the image is also not an upper body garment, which would take up the width of the
image. In query k = 3, IP queries the center bottom of the image, discovering the space in between the two legs of the trouser, causing
the posterior probability of â€œTrouserâ€ to jump to nearly 100%, which remains stable over the last few queries.
Fig. 12. Additional Examples of IP on FashionMNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
28
B.3.3
IP for Bird Species Classiï¬cation
As in the main text, for each plot, on the left, we show the input image and on the right we have a heatmap of the estimated
class probabilities per iteration. For readability, we only show the top 10 most probable classes out of the 200. To the right,
we display the queries asked at each iteration, with red indicating a â€œnoâ€ response and green a â€œyesâ€ response.
Black-footed Albatross
Mallard
Black-footed Albatross
Sooty Albatross
Mangrove Cuckoo
Geococcyx
Gadwall
White-breasted Kingfisher
Northern Flicker
Brown Creeper
Nighthawk
1. shape::perching-like?
2. primary_color::black?
3. nape_color::white?
4. bill_length::about_the_same_as_head?
5. size::medium_(9_-_16_in)?
6. breast_color::brown?
7. under_tail_color::grey?
8. underparts_color::grey?
9. back_color::grey?
10. bill_shape::hooked_seabird?
p(Y
SIP
k ((xobs)))
0.0
0.2
0.4
0.6
0.8
1.0
(a) The ï¬rst few queries narrow down the potential species from 200 down to a small number. Since we only show the 10 most probable
classes, the ï¬rst few queries increase the probability of all shown classes. The ï¬rst queries that distinguish among these classes concern
bill length (which rules out the short-billed Nighthawk) and size (which rule out the smaller Mangrove Cuckoo, Geococcyx, Kingï¬sher,
Northern Flicker, and Brown Creeper). The remaining birds are quite similar, all medium-sized brown water birds. The next two color
queries suggest that the bird in question is a Black-footed Albatross, which is conï¬rmed by the answers to the next few queries, which
all match up with the characteristics of that bird.
Groove-billed Ani
Common Raven
Brandt Cormorant
Bronzed Cowbird
Groove-billed Ani
Shiny Cowbird
Brewer Blackbird
Fish Crow
American Crow
Pelagic Cormorant
Pigeon Guillemot
1. shape::perching-like?
2. primary_color::black?
3. throat_color::black?
4. head_pattern::plain?
5. bill_length::about_the_same_as_head?
6. bill_shape::specialized?
7. bill_shape::all-purpose?
8. eye_color::red?
9. size::small_(5_-_9_in)?
p(Y
SIP
k ((xobs)))
0.0
0.2
0.4
0.6
0.8
1.0
(b) Again, the ï¬rst few queries narrow down the likely species into the top 10 displayed classes. In just two queries (bill length and
bill shape), IP distinguishes among these similar-looking, black, plain-headed birds that are hard for non-expert humans to differentiate
between. Again, the last few queries serve to conï¬rm the posterior prediction that the bird is a Groove-billed Ani.
Ringed Kingfisher
Brown Pelican
Pomarine Jaeger
Ringed Kingfisher
Forsters Tern
Least Tern
Artic Tern
Elegant Tern
Caspian Tern
Common Tern
Heermann Gull
1. shape::perching-like?
2. primary_color::black?
3. nape_color::white?
4. crown_color::white?
5. upper_tail_color::white?
6. shape::long-legged-like?
7. bill_shape::hooked_seabird?
8. primary_color::grey?
9. breast_color::white?
10. bill_shape::dagger?
11. crown_color::black?
p(Y
SIP
k ((xobs)))
0.0
0.2
0.4
0.6
0.8
1.0
(c) After establishing the top few most probable classes, IP converges on the class Ringed Kingï¬sher after just 7 queries. The last four
queries simply serve to increase its conï¬dence in its prediction.
Fig. 13. Additional Examples of IP on CUB Bird Species Identiï¬cation

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
29
B.3.4
IP for HuffPost News Headline Category Classiï¬cation
Entertainment
Politics
Queer Voices
Business
Travel
Parenting
Style & Beauty
Food & Drink
Home & Living
Wellness
1. trump
2. photo
3. gay
4. travel
5. parent
6. kid
7. recip
8. fashion
9. republican
10. mom
11. food
13. health
17. babi
19. style
20. life
23. children
60. trailer
70. new
72. market
0.0
0.2
0.4
0.6
0.8
1.0
Category: Entertainment
Short Description: Margot Robbie goes for 
the gold as the infamous ice skater.
Headline: 'I, Tonya' Looks Like A Winner In 
Slick New Trailer
Category: Style & Beauty
Short Description:PHOTO: See the rest of 
Kim's maternity gear: Want more? Be sure to 
check out HuffPost Style on Twitter, Facebook, 
Tumblr",
Headline: Kim Kardashian's Flip Flops, 
Dorothy Perkins Dress Is A Cute Change 
(PHOTO)
Entertainment
Politics
Queer Voices
Business
Travel
Parenting
Style & Beauty
Food & Drink
Home & Living
Wellness
1. trump
2. photo
3. home
4. recip
5. style
7. check
8. twitter
9. want
0.0
0.2
0.4
0.6
0.8
1.0
Category: Queer Voices
Short Description: "I donâ€™t know if it 
changed me, but it changed my life."
Headline: Last Words: Ginger Minj Reflects 
On â€˜RuPaulâ€™s All Stars Drag Raceâ€™
Entertainment
Politics
Queer Voices
Business
Travel
Parenting
Style & Beauty
Food & Drink
Home & Living
Wellness
1. trump
2. photo
3. gay
4. travel
5. parent
6. kid
7. recip
8. fashion
10. food
12. babi
20. life
21. children
26. live
32. star
62. race
131. chang
208. word
229. reflect
234. clear
0.0
0.2
0.4
0.6
0.8
1.0
(a)
(b)
(c)
Fig. 14. Additional Examples of IP on HuffPost News Headline Classiï¬cation. As before, when more than 20 queries were asked, we only
display the 20 queries that led to greatest KL divergence between successive posteriors. (a) Because of the sparse structure of natural language, it
typically takes a signiï¬cant number of queries before the ï¬rst word that is present in the sentence is found. Until this point, no query is particularly
informative, and the posterior distribution remains mostly unchanged from the prior. However, at query 60, IP asks the word â€œtrailerâ€, which is present
in the extended headline. Naturally, the posterior shifts heavily towards â€œEntertainmentâ€, and a few queries later IP reaches its termination criteria.
Analyzing this run, we can say that IP reached its decision primarily because of the presence of the word â€œtrailerâ€, leading us to say that this is
a reasonable and trustworthy prediction. (b) This is an example of a relatively short explanation as IP happens to discover words present in the
sentence after just two queries. Initially, the presence of â€œphotoâ€ causes the categories â€œTravelâ€, â€œHome & Livingâ€, and â€œStyle & Beautyâ€ to become
more probable. Several words later however, the word â€œstyleâ€ is found, which is very strongly associated with the â€œStyle & Beautyâ€ category. (c)
The posterior remains mostly unchanged until IP discovers the word â€œlifeâ€, which reasonably, shifts probability mass onto the â€œWellnessâ€ category.
However, several queries later, â€œstarâ€ is found to be present, which shifts the posterior away from â€œWellnessâ€ onto â€œEntertainmentâ€ and â€œQueer
Voicesâ€. After discovering several more words that are present, â€œraceâ€, â€œchangeâ€, â€œwordâ€, â€œreï¬‚ectâ€, the posterior progressively converges on â€œQueer
Voicesâ€, but still with relatively high uncertainty, likely because it never came across identifying words such as â€œdragâ€, which was not present in the
vocabulary.

