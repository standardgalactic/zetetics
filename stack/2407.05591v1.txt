On the Power of Convolution Augmented Transformer
Mingchen Li
Xuechen Zhang
University of Michigan
{milii,zxuechen}@umich.edu
Yixiao Huang
UC Berkeley
yixiaoh@berkeley.edu
Samet Oymak
University of Michigan
oymak@umich.edu
Abstract
The transformer architecture has catalyzed revolutionary advances in language
modeling. However, recent architectural recipes, such as state-space models, have
bridged the performance gap. Motivated by this, we examine the benefits of
Convolution-Augmented Transformer (CAT) for recall, copying, and length gener-
alization tasks. CAT incorporates convolutional filters in the K/Q/V embeddings
of an attention layer. Through CAT, we show that the locality of the convolution
synergizes with the global view of the attention. Unlike comparable architectures,
such as Mamba or transformer, CAT can provably solve the associative recall
(AR) and copying tasks using a single layer while also enjoying guaranteed length
generalization. We also establish computational tradeoffs between convolution and
attention by characterizing how convolution can mitigate the need for full attention
by summarizing the context window and creating salient summary tokens to attend.
Evaluations on real datasets corroborate our findings and demonstrate that CAT
and its variations indeed enhance the language modeling performance.
32
64
128 256 512 1024
0.00
0.25
0.50
0.75
1.00
 
CAT, 1 layer
Attention, 2 layers
Attention, no PE
Mamba, 2 layers
Train Length
Test Length
Accuracy
512
1K
2K
4K
8K
16K
0
100
200
300
400
500
600
CAT, RoPE
MH-CAT, RoPE
Pythia, RoPE
CAT, no PE
MH-CAT, no PE
Train Length
Test Length
Perplexity
Figure 1: Evaluations on synthetic and real data. The models are trained on 128 and 2,048 context length
(vertical dashed lines) and tested on varying context lengths respectively. Left figure: We conduct synthetic
experiments on the Associative Recall task and contrast 1-layer CAT with 2-layers of alternative architectures.
The embedding dimension is 128. We find that CAT is the only model that solves AR with length generalization
in line with our theory (also see Fig. 6). Right figure: Evaluations on language modeling where we train CAT
models by equipping Pythia with short convolutions (window size 21). Convolution allows the model to pretrain
without positional encoding and further improves perplexity when combined with RoPE. Importantly, it also
generalizes to longer context lengths more robustly with or without RoPE. For length generalization, we used
YaRN [36] which incorporates position interpolation [8] (for RoPE only) and temperature scaling (see Sec. 6.2).
1
Introduction
The attention mechanism is the central component of the transformer architecture [47] which empow-
ers modern large language models. Through the self-attention layer, all pairs of tokens get to interact
with each other which equips the model with a global view of the context window. On the other hand,
without positional-encoding (PE), self-attention lacks locality. For instance, without PE or causal
masking, self-attention layer is permutation-equivariant and does not distinguish between nearby
vs distant tokens. In contrast, convolution operator is a well-established primitive that facilitates
arXiv:2407.05591v1  [cs.LG]  8 Jul 2024

𝑋= {𝑥!, 𝑥", 𝑥#}
𝑊$
𝑊%
𝑊&
𝑄= (𝑋∗𝐹$)𝑊$
𝐾= (𝑋∗𝐹%)𝑊%
𝑉= (𝑋∗𝐹&)𝑊&
𝐹$
𝐹%
𝐹&
Attention 𝑄, 𝐾, 𝑉
=  Softmax 𝑄𝐾'
𝑑%
 V
Convolutions
64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Associative Recall Comparison
Figure 2: Left figure: Illustration of the Convolution-Augmentated Attention (CAT) block, where separate
filters are applied to the K/Q/V embeddings, before self-attention (see Sec. 3.1 for details). Right figure:
Performance of 1-layer CAT models trained on multi-query AR (MQAR, see Sec. 3.2.1 for details) tasks with
model embedding dimension 64 and varying sequence length. The LinCAT replaces the standard attention in
CAT with linear attention. We observe that the CAT model outperforms the baseline models across all sequence
lengths with only 1 layer compared to 2 layers baselines.
local feature aggregation based on relative positions and provides a natural alternative to PE. While
convolution has enjoyed major success in vision during the last three decades, its explicit use in
language modeling is relatively recent [11]. On the other hand, there is a growing recent interest in
using convolution-based blocks in language models: For instance, state-space models (SSM) [17]
and linear RNNs [33] are efficient parameterizations of long convolutional filters. These models
have enjoyed significant success in long-range sequence modeling as they provide fast inference and
parallelizable training. On the other hand, purely convolutional architectures are known to suffer from
recall capability as they lack the global view of the context window [2]. These insights motivated a
recent push toward hybrid architectures [12, 2, 35, 1] that combine the strengths of both attention and
convolution-like approaches, including short convolutional filters, SSMs, linear RNNs, or Mamba.
In this work, we explore the synergy between attention and convolution which reveals new theoretical
principles that inform hybrid architecture design. Specifically, we introduce an intuitive hybrid
architecture called Convolution-Augmented Transformer (CAT)1. CAT incorporates convolutional
filters to the K/Q/V embeddings of the attention layer as depicted on the left hand side of Figure
2. We explore the capabilities of the CAT layer through mechanistic tasks including associative
recall (AR), selective copying [15, 22], and length generalization. For instance, AR is a fundamental
task motivated from the associative memory in cognitive science [4]. This task underpins critical
applications such as bigram retrieval, where a specific sequence, such as ‘Rings’ following ‘The Lord
of the’, must be correctly retrieved. It is also a generalization of the induction head task [32] and
known to be crucial for LLM functionality and mechanistic understanding [32, 14, 2, 31, 38].
We theoretically and empirically show that, within the CAT layer, attention and convolution exhibit
strong synergy and complementarity to solve these mechanistic tasks while enjoying length gener-
alization benefits. As a concrete example, the left side of Figure 1 displays the AR performance
for various test-time sequence lengths. As the sequence length grows, we observe two distinct
failure modes: Mamba’s accuracy degrades due to its finite state dimension whereas attention-only
models degrade due to the length extension bottlenecks of PE. In contrast, CAT maintains perfect
accuracy and length generalization because attention and convolution patch these failure modes in a
complementary fashion. Overall, we make the following contributions:
• We propose the convolution-augmented attention layer and prove that it can solve the N-gram
AR (NAR) and Selective Copying tasks using a single layer (Theorems 1 and 4). Comparison to
alternatives (Mamba, Based, attention, linear attention) reveals that CAT can uniquely solve NAR
with length generalization.
• To explain this, we establish a length generalization result on the loss landscape (Theorem 2):
Under mild assumptions, all CAT models that solve AR for a particular context length provably
generalize to all other context lengths.
• We evaluate CAT on real data and demonstrate that even 1-dimensional short convolutions notice-
ably aids language modeling: In line with theory, convolution enables the model to train stably
1The transformer architecture consists of attention and MLP layers. For theoretical analysis and synthetic
experiments, we will entirely focus on the Convolution Augmented Attention layer described in Fig. 2. For this
reason, we will use the CAT acronym to refer to both Convolution-Augmented Transformer and Attention.
2

without PE and improves length generalization. We also develop a multihead version of CAT
which yields further accuracy improvements (see Table 2).
• We show that long convolutions, such as SSMs, bring the benefit of context summarization and
mitigates the need for dense attention: We describe the Landmark CAT model (following Landmark
Attention [30]) which first creates landmark/summary tokens through convolution and then attends
on these landmarks to efficiently locate the most relevant subsets of the input sequence (Sec. 5).
Focusing on the AR problem, we characterize fundamental tradeoffs between the embedding
dimension, amount of summarization, and the sparsity of attention. Through these, we show that
the use of long convolutions can provably enable the success of sparse/cheaper attention.
2
Related Works
Convolution-like sequence models. Gated-convolutions [11] and state-space models, such as
S4 [17], utilize long convolutions to reduce the computational demands associated with attention
mechanisms. Performance enhancements have also been achieved through novel filter parametrization
techniques [18, 16]. Despite these innovations, challenges in Multi-query Associative Recall (MQAR)
prompted the development of input-dependent convolution techniques. Notable developments in
this area include, Liquid S4 [19], Mamba [15, 10] and [48, 24] where convolution filters are directly
parametrized by inputs and include correlation terms between input tokens to enhance state mixing.
[26] empirically explores the reason underlying the success of convolutional models.
Expressivity, recall, length generalization. Recent works [2, 21, 1, 14] explore the limitations
of purely convolutional models, including Mamba, and demonstrate that, these models inherently
lack the capability to solve recall problems unless they have large state dimensions (i.e. memory).
[21] also provides a construction for 2-layer self-attention to solve AR with length generalization.
Interestingly, this construction uses Hard Alibi, which is a variation of Alibi PE [39] that utilize
explicit linear biases in attention. Their Hard Alibi restricts the attention layer to focus on and
aggregate only the recent N tokens. In this regard, this construction is related to our short convolution.
On the other hand, while this work is constructive, we also prove that CAT has good loss landscape
and all CAT solutions to AR provably length generalize. It has also been observed that PE can hurt
length generalization and reasoning. In fact, [23] has found NoPE to be viable. On the other hand, in
our real data evaluations, we have found pure NoPE to be highly brittle as it either fails to converge
or optimization is unreasonably slow. Our AR experiments also corroborate that NoPE by itself is
indeed not a viable strategy.
Hybrid architectures. There is a growing interest in integrating different language modeling prim-
itives to obtain best-of-all-world designs. To this end, mechanistic tasks such as AR, copying,
induction head, and in-context learning have been important to demystify the functionalities of lan-
guage models [32, 35] and have been utilized to guide architecture design [1, 38]. Gating mechanisms
have been integrated within convolutional frameworks to enhance the model’s selectivity. Models
employing gating functions, have shown substantial improvements in AR tasks [14, 37]. Additionally,
recent innovations on hybrid architecture, such as BaseConv [1, 2], GLA [49], MambaFormer [35],
and [27, 28, 40] have provided more effective solutions to AR tasks. This comprehensive foundation
of hybrid architectures informs our exploration into the convolution-attention synergy.
3
Problem Setup
3.1
Convolutional-Augmented Attention
Let us first introduce helpful notation. Id is the identity matrix of size d. Di denotes the causal delay
filter that shifts a signal x i-timesteps forward i.e. (x ∗Di) j = x j−i. For an integer n ≥1, we denote
the set {0, . . . , n −1} by [n]. We use lower-case and upper-case bold letters (e.g., m, M) to represent
vectors and matrices, respectively. mi denotes the i-th entry of a vector m.
Below, we introduce the Convolution-Augmented Attention layer, which incorporates learnable filters
into the K/Q/V embeddings. Let X = [x0 . . . xL−1]⊤∈RL×d denote the input to the layer containing
L tokens with embedding dimension d. Let F ∈RW denote the convolutional filter with temporal
length W. We examine two convolution types which handle multi-head attention in different ways:
• 1D per-head convolution: For each attention head, we have a distinct 1D filter F ∈RW. F is
applied temporally to each of the d embedding dimensions. This results in F ∗X where (F ∗X)i =
P
j∈[W] F jxi−j, with F j being the j-th entry of F.
3

• Multi-head convolution: Suppose we have H sequences ¯X = [X1, . . . , XH] ∈RL×d×H each
corresponding to one of the H attention heads. We use a filter ¯F = [F1, . . . , FH] ∈RW×H×H. Each Fi
is convolved with ¯X to obtain the i-th head’s output of size L × d.
Observe that both convolution types are identical when there is a single attention head. However,
multi-head convolution is more expressive because it mixes the attention heads. In Section 6, we
will also examine a variation of multi-head convolution where we mix the attention maps rather than
embeddings. The architecture of CAT is illustrated in Fig. 2 and is formally defined as follows:
Definition 1 (Convolution-Augmented Attention (CAT)). A CAT layer incorporates learnable
convolutional filters to the key/query/value embeddings. For a single-head CAT, the key embeddings
are given by K = (X ∗Fk)Wk with weights Fk,Wk (same for query and value embeddings).
3.2
Mechanistic Tasks for Language Modeling
To proceed, we describe the Associative Recall and Selective Copying tasks that will help us
mechanistically study CAT. Table 1 provides an illustration of these tasks which are adapted from the
sequence modeling literature [15, 1, 38, 32].
Definition 2 (Associative Recall Problem). Consider a discrete input sequence X = [x0, x1, . . . , xL−1],
with tokens drawn from a vocabulary V of size |V|. The AR problem is defined as follows: Suppose
that there is a unique index i (0 ≤i < L −1) such that xi = xL−1. A model f successfully solves the
AR problem if f(X) = xi+1 for all inputs X. In this problem, xi becomes the key, xi+1 is the associated
value, and the last token xL−1 is the query.
Building on the AR problem, we introduce its N-gram variation: The model needs to identify the
copy of the last N tokens in the context window and return the associated value.
Definition 3 (N-gram AR Problem). Consider a discrete input sequence X = [x0, x1, . . . , xL−1], with
tokens drawn from a vocabulary V of size V|. Let X{i, j} = [xi, xi+1, . . . , xj] denote the subsequence
of X from index i to j. The N-gram associative recall (NAR) problem is formulated as follows: for
X{L−N,L−1} (which are the last N tokens), there exists a unique index i (0 ≤i < L −N) such that
X{i,i+N−1} = X{L−N,L−1}. A model f solves NAR if f(X) = xi+N for all inputs X.
Selective copying (SC) task is originally introduced by [22] and it is utilized by the recent Mamba
[15] and Griffin [12] papers to assess their model’s approximation capabilities. In SC, given an input
sequence X containing noisy tokens, the model should denoise X and return the signal tokens within.
Definition 4 (Selective Copying). Consider a vocabulary V composed of a set of signal tokens S, a
set of noise tokens N, and special token ⊥i.e. V = S ∪N ∪{⊥}. Let X be a sequence whose tokens
are drawn from S ∪N and let XS be the sub-sequence of X that includes all signal tokens in order. f
solves selective copying over S if it autoregressively outputs XS following the prompt [X ⊥] for all
inputs X. f solves unique selective copying if it outputs all unique tokens of XS in order for all X.
3.2.1
Multi-Query Associative Recall
In this section, we introduce the multi-query versions of the AR and NAR tasks, abbreviated as
MQAR and MQNAR, respectively. In the multi-query (MQ) setting, a model receives multiple
queries in a single input and must generate corresponding outputs in a single forward pass, at varying
Table 1: Illustrative examples of synthetic tasks. In all AR-based tasks, keys and queries are highlighted in red
and the values in green. For NAR tasks, parentheses denote N-gram queries; note that the parentheses are not
part of the input. In SC tasks, signal tokens are in green and noise tokens in gray, and the model begins output
when ⊥appears in the sequence.
Input
Query
Output
Single Query
AR
a 2 c 1
a
2
NAR
(a b) 2 (b a) q (a a) 4
b a
q
SC
a [n] [n] c [n] k
⊥
a c k
Multi Query
AR
a 2 c 1
c a
1 2
NAR
(a b) 2 (b a) q (a a) 4
(b a) (a a)
q 4
4

positions in the sequence. This approach was first introduced in [1], which demonstrated that while
the Mamba model successfully addresses single-query AR tasks, it struggles with MQAR when
operating with a limited model dimension. This highlights the increased complexity of MQAR tasks
where models need to memorize more sequence information and recall queries at different positions.
Definition 5 (Multi-Query Associative Recall (MQAR)). Consider a discrete input sequence
X = [x0, x1, . . . , xL−1] with tokens drawn from a vocabulary V. Let X{i, j} = [xi, . . . , xj] denote
a subsequence of X from index i to j. The multi-query N-gram associative recall (MQNAR) problem
is defined as follows: for every N-gram query Qk = Xk−N+1...k, N ≤k < L, determine if there exists a
N ≤j < k such that X{j−N+1,j} = Qk. If so, output the value xj+1 as the result, else output a special
token to indicate no match is found. A model f solves MQNAR if it outputs the correct values for all
N-gram queries and all inputs X. The standard MQAR problem [1] is a special instance of MQNAR
by setting N = 1.
Table 1 provides examples of the synthetic tasks we consider in this work. Specifically, we conduct AR
and NAR experiment on their multi-queiry variants to evaluate the model’s ability to recall multiple
queries. For the selective copying task, we generate the output auto-regressively by predicting the
signal tokens in the input sequence after the special token ⊥.
4
Provable Benefits of Convolution-Augmented Attention
Before diving into the theoretical results, we make a few clarifying remarks. We assume that all token
embeddings have unit ℓ2 norm. Secondly, a CAT layer maps each query to a vector-valued output
f(X) ∈Rd. To sample the discrete output token, we will simply return the nearest neighbor in the
vocabulary of token embeddings. For associative recall problems, we will use a single head attention
layer with weights Wq,Wk are chosen as suitably scaled identity matrices. With this choice, attention
essentially implements a nearest neighbor retrieval. It suffices for the theory thanks to the simple
nature of the AR problem where we wish to identify the replica of a query within the context window.
In general, we can easily contrive natural generalizations of AR and Selective Copy problems that
necessitate a more sophisticated attention mechanism (see [38]). One such generalization is, given
query q, we wish to retrieve a general key k (possibly k , q) and return the value associated with k.
N-gram AR. Our first result shows that a single CAT layer can solve the NAR problem under fairly
general conditions.
Theorem 1 (Solving NAR). Let F ∈RN be a causal 1-D convolutional filter of length N and
norm(X) normalize the rows of a matrix to unit ℓ2 norm. Consider a single CAT layer f(X) =
(XvWv)⊤S(XkWkW⊤
q q) where q is the final token of Xq and Xq = norm(X ∗Fq) ∈RL×d (same for
Xk,). Set Fq = F and Wk = Wq = √cId. Use either
• Value delay: Fk = Fq, Fv = D−1 and Wv = 2Id or,
• Key delay: Fk = D1 ∗Fq, Fv = D0 and Wv = Id
Let ε > 0 be the minimum ℓ2 distance between two distinct tokens embeddings. For almost all choices
of F, there is a scalar c0 > 0 depending on F such that, setting c = c0 log(4L/ε), CAT layer solves
the NAR problem of Def. 3 for all input sequences up to length L.
As a corollary, using a simple 1-D convolutional filter on the key embeddings solves the AR problem.
Corollary 1 (1-D CAT solves AR). Consider a CAT layer employing 1-D convolution on key
embeddings with the delay filter Fk = D1 = [0 1 0 . . . 0] and Fq = Fv = D0. This model solves AR.
Length generalization. Our next result shows that the global minima of CAT provably exhibit length
generalization, thereby shedding light on the empirical benefits of CAT in Figure 1. Concretely, even
if we train CAT to solve AR for a fixed context length, the AR capability will generalize to all other
context lengths. This result is distinct from Theorem 1 because it establishes length generalization
for all CAT models that approximately solve the AR problem for a context length, rather than
constructing one such solution. The proof is provided in Section C.2.
Theorem 2 (Length generalization). Let Fv ∈R2W+1
+
be a convolutional filter from time t = −W
to t = W where W ≤L −1. Consider a CAT layer of the form f(X) = X⊤
v S(XWxL−1) where
5

X ∈RL×d, Xv = X ∗Fv ∈RL×d and xL−1 is the last token of X and W = WkW⊤
q . Suppose that token
embeddings have unit norm. Consider any model f = (W, Fv) that can solve the AR problem defined
in Def. 2 up to ε-accuracy on all sequences of length L ≥3. That is, for all (X, y) where query xL−1
repeats twice and y being the associated value token, we have ∥y −f(X)∥ℓ2 ≤ε. Define the minimum
embedding distance within vocabulary V as ∆= (1 −maxa,b∈V(a⊤b)2)1/2 and assume that ∆> 0.
There are absolute constants R0, R > 0 such that, if ε0 := ε/∆≤R0/L, we have that
• The filter obeys ∥F −D−1∥ℓ1 ≤Lε0, which is in line with Theorem 1.
• Let X be an input sequence of length L′ following Def. 2. Let s⋆(X) ∈RL′ be the “golden
attention map” with entries equal to 1/2 at the positions of the query xL′−1 and 0 otherwise. For
all such X, the attention map of f obeys ∥S(XWxL′−1) −s⋆(X)∥ℓ1 ≤L′ε0.
• For all X of length L′ following Def. 2, we have that ∥y −f(X)∥ℓ2 ≤RL′ε0.
Here it worths noting that all CAT models that approximately solve the AR problem ends up learning
convolution and attention weights that are consistent with the constructive result of Theorem 1. This
simple “universal solution” is in contrast to attention-only models where length generalization not
only requires standard positional encoding but also additional adjustments to extend the context
window of PE [36, 8].
Additionally, in Appendix C.3, we generalize the length generalization result to the N-gram AR
problem under slightly stronger assumptions, which is specified in Assumption 1. The reader is
referred to Proposition 3. Besides showcasing the value of convolution-attention hybrids, these results
also motivate future research into the optimization landscape: Under what conditions gradient methods
provably converge to generalizable CAT models, namely those described in Theorem 2? Answers to
such questions could build on the recent optimization theoretic results on the transformer/attention
models [45, 44, 13, 34, 25, 31, 20, 3, 29, 9] and extend them to hybrid designs.
Selective Copy. Our next result shows that, 1-layer CAT model can solve the unique selective copy
problem. That is, it can provably generate all signal tokens in the correct order as long as the input
contains each distinct signal token at most once. Corroborating this, our experiments demonstrate
that 1-layer CAT performs on par with or better than alternative architectural choices. The proof is
deferred to Section C.4.
Theorem 3 (Selective Copy). Consider the setting of Def. 4. There is a 1-layer CAT using exponential-
decay query-convolution (i.e. Fq,i = ρi) and d = |S| + 3 dimensional token embeddings such that, it
outputs all signal tokens in order for all inputs where signal tokens appear uniquely.
Selective Copy problem is distinct from AR in the sense that, it requires a global view of the
token positions as the model has to distinguish the order of the distinct signal tokens within the
context window. In Theorem 4, we actually describe two ways to achieve this (see appendix for
the details): The first option is using an infinitely long convolution Fq,i = ρi which admits a simple
parameterization as a state-space model [17]. We show that this convolution choice can aggregate
all signal tokens in the query embedding while distinguishing their order. This also partly explains
how Mamba/SSMs are equally effective in solving Selective Copying. An alternative construction
is using a short convolution together with a simple positional encoding. Here, convolution equips
the query with local context (specifically the summary of the signal tokens generated so far) and PE
provides the global context on the locations of remaining signal tokens. This synergy of PE and short
convolution is in line with our real language modeling experiments where CAT with PE outperforms
CAT without PE in terms of perplexity as well as length generalization.
5
Benefits of Long Convolution for Enabling Sparse-Attention
So far we have discussed the benefits of short convolutions to equip transformer with local context to
solve AR and its variations. During this discussion, we have used dense attention which has exact
recall capabilities thanks to its ability to scan the full context window. In this section, we ask the
following: Can convolution also help mitigate the need for dense attention? Intuitively, we should be
able to tradeoff the accuracy of attention computation with computation. Here, we describe how long
convolutions can enable this by effectively summarizing the context window so that we can identify
where to attend in (extremely) long-context settings.
6

𝑋= {𝑥!, 𝑥", 𝑥#, 𝑥$, 𝑥%, … , 𝑥&}
𝑊'
𝐾= (𝑋∗𝐹')𝑊'
𝐹'
Convolution
Augmentation
Landmark
Attention
𝐾= {𝑘!, 𝑘", 𝑘#, 𝑘$, 𝑘%, … , 𝑘&}
Sample Rate B=2
Hard Attention
𝐾!! = {𝑘", 𝑘#, 𝑘$, 𝑘%}
𝑞&
0.1
0.5
1.2
0.2
𝑏= arg max 𝐾!!𝑞&
Sample Rate B=2
Retrieved Block b=2
Local Attention
𝑦& = Softmax 𝑲'()𝑞& 𝑽
Retrieved 
block
𝐾'() = {𝑘*, 𝑘$, 𝑘+}
Long Convolution
Figure 3: Illustration of the Landmark CAT. We first apply
long convolution on the input sequence and subsample it to
obtain landmark tokens representing individual blocks. Hard
Attention computes the similarity between the query and land-
marks to retrieve the most relevant block. Local Attention
concatenates the retrieved block with the final block contain-
ing the query and computes the output token.
Specifically, we will prove that, long con-
volutions (such as SSMs) allow us to uti-
lize sparse attention while retaining (high-
probability) recall guarantees. These find-
ings complement the recent research that
establish the recall limitations of purely re-
current models [2, 1]. Our theory will also
shed light on the mechanics of landmark
attention [30]. While [30] does not rely on
convolution, we will describe how convolu-
tion can generate landmark tokens by sum-
marizing/hashing the chunks of the context
window, and attention can efficiently solve
recall by attending only to these summary
tokens.
Landmark
Convolutional
Attention
(LCAT): Figure 3 describes the LCAT
block that apply on input sequence X. Let
Fk ∈RL be the convolutional filter on keys,
B be the sampling rate, and ¯L = ⌈L/B⌉.
Setting K = (X ∗Fk)Wk ∈RL×d, we obtain
Kss ∈R ¯L×d by sampling K at every B
tokens. Additionally, define Xi to be the ith block of X of size B spanning tokens (i −1)B + 1 to iB.
Let V = (Fv ∗X)Wv denote the value embeddings. For a query qi for i ∈[L], the LCAT layer outputs:
(1) Hard Attention:
b = arg max
j,⌈i/B⌉Kssqi
(LCAT)
(2) Local Attention:
y = S(Klocqi)Vl
where
Kloc = concat(K⌈i/B⌉, Kb).
Above, hard attention phase aims to retrieve the correct block associated to the query. This block is
merged with the local block ⌈i/B⌉that contains the query itself similar to sliding window attention.
We then apply dense local attention on the concatenated blocks Kloc.
Computational complexity of LCAT: For a fixed query, (LCAT) requires O(d(L/B + B)) computa-
tions. This is in contrast to O(dL) computations of vanilla attention. Choosing a suitable block size
(e.g. B = O(
√
L)), this model should save up to ×
√
L in computational savings. Importantly, our
theory will highlight the interplay between the embedding dimension d and the allowable acceleration
by characterizing the exact performance of (LCAT) under a random context model.
Definition 6 (Random Context Model). The query token xL occurs twice in the sequence and has
unit ℓ2 norm. All other tokens of X are IID and drawn with IID N(0, σ2/d) entries.
The following proposition shows that, (LCAT) will solve AR if and only if
d
2B log ¯L ≥1 + o(1).
Proposition 1. Recall ¯L = ⌈L/B⌉is the number of blocks. Let Wv = 2Id, Fv = D−1, and Wk = Wq =
√c · Id with c →∞. Set key convolution as Fk,i = 1 for 0 ≤i < B and zero otherwise.
(A) If d ≥2σ2B(
p
log ¯L +t)2, then (LCAT) solves AR for fixed xL with probability at least 1−3e−t2/4.
(B) Conversely, for any ε > 0 there is Cε > 0 as follows: If ¯L ≥Cε and d ≤2σ2B(
p
(1 −ε) log ¯L−t)2,
then (LCAT) fails to solve AR with the same probability.
(C) Finally, suppose we wish to solve AR uniformly for all queries xL over a subspace S . This
succeeds with the same probability whenever d ≥2σ2B(
p
log ¯L + √dim(S ) + t)2.
Figure 4 corroborates the predictive accuracy of Proposition 1: As the block size increases, the
embedding dimension to maintain success of AR grows approximately linearly. One can expand on
this proposition in two directions. Firstly, a fundamental bottleneck in (LCAT) is the requirement
d ≳B log ¯L. This arises from a memory-recall tradeoff [2, 21] as we are summarizing the information
of block Xi of length B through its landmark token. However, once this requirement is satisfied, the
model can identify the correct block in O( ¯L) cost. To avoid paying the additional O(B) cost of local
attention, we could apply the LCAT approach hierarchically within the selected block to reduce the
compute cost to d( ¯L + log B) per token. The dominant term d ¯L captures the recall capacity of the
7

LCAT model: Consistent with our theorem and lower bounds of [2], for AR to succeed, we need
recall_capacity = d ¯L ≥L = required_memory
101
102
103
Block size B
103
104
105
Embedding d
Theory (single query)
10%-50% error range
Theory (uniform, dim=5)
Theory (uniform, dim=20)
Figure 4: Behavior of the embedding dimension as a
function of block size for context length L = 220 ≈1
million (noise level σ2 = 1). Shaded region highlights
te range of d that exhibits 10%-50% empirical success.
Proposition 1 accurately captures the empirical behavior.
For the success of uniform AR, we need larger d as the
dimension of the query space S grows.
Secondly, Proposition (1) chooses a particular
long convolution where landmarks become the
mean of the input tokens within the block. In
practice, we can use a state-space model [16] to
parameterize convolution efficiently. A particu-
lar SSM choice of state dimension 1 is simply
using exponential smoothing. This yields the
following SSM variant of Proposition 1.
Proposition 2. Consider the setting of Propo-
sition 1 with the exponential smoothing filter
Fi = ρi for i ≥0. Set ρ = e−1/B so that ρB = e−1.
Suppose d ≥50B(
p
log ¯L + t)2. Then, (LCAT)
solves AR with probability at least 1 −3e−t2/4.
Above, we fixed the decay rate ρ for exposition
purposes. More generally, any ρ choice with an
effective context size of O(B) would result in
similar guarantee.
6
Experiments
6.1
Model Evaluation on N-gram AR and Length Generalization Capability
For the synthetic experiments on associative recall problems, we employ the CAT architecture as
detailed in Section 3.1. We utilize convolution kernels with a width of W = 3 and explore model
embedding sizes of d = 32, 64, and 128 across MQAR and MQNAR problems to assess the impact
of model dimension on performance. In addition to the standard attention mechanism, we introduce a
perturbation strategy by implementing linear attention on the convoluted Q, K, and V embeddings,
referred to as LinCAT. We adhere strictly to the parameters set by [1]. Our experimental setup and
code are available on GitHub2. More detailed information on the training setup can be found in
Section A including the data generation and hyperparameters. For reporting results, we conduct each
experiment three times and present the maximum accuracy achieved across these runs, aligning with
the methodologies of [1] and [2].
As illustrated in Fig. 5, the CAT model consistently outperforms all baseline models across a range of
sequence lengths and model dimensions. Notably, both Mamba and Based models exhibit improved
performance as the model dimension increases, particularly with shorter sequence lengths. This
improvement is due to the memory-recall tradeoff [2] where models store and recall sequence
information more as their dimensionalities expand. In contrast, thanks to the short convolution, the
single-layer CAT model maintains 100% accuracy across all experimental settings, aligned with our
theorem 1. Interestingly, aside from CAT, Mamba is the only model demonstrating the potential to
effectively address the MQAR task within a single-layer network architecture. We will discuss this
observation in further detail in Section B.
Evaluation of Length Generalization. In Fig. 6, we train models with 128 sequence length (the
vertical red dashed line) and evaluate their performance on varying sequence lengths from 32 to
1,024. Fig. 6 shows the results of length generalization, which is aligned with our Theorem 2: CAT
models maintain 100% accuracy while all other models exhibit a sharp decline in performance as
the sequence length increases. This decrease is due to the increased demand of recall which requires
the model to store and retrieve more information as the sequence length grows. The CAT model,
however, is able to maintain its performance by leveraging the convolutional filters to shift the context
and retrieve the necessary information. Remarkably, in Fig. 5, we observe non-monotonic accuracy
behavior for Mamba and Attention-only models as a function of sequence length. This is due to the
2https://github.com/umich-sota/CAT
8

64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Figure 5: Evaluation of models on MQAR and MQNAR tasks with varying model dimensions and sequence
lengths. Model dimensions are 32, 64, 128 for each column of the figures, from left to right. Top: Models
trained on the MQAR setup. Bottom: Models trained on the MQNAR setup. Note that CAT models employ a
single-layer architecture, whereas all other models utilize two layers. Refer to Section 6.1 for detailed setup
descriptions.
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Figure 6: Evaluation of models on length generalization. Model dimensions are 32, 64, 128 for each column of
the figures, from left to right. The models are trained with sequence length 128 (vertical red dashed lines) and
tested on varying test length. Top: Models trained on the MQAR. Bottom: Models trained on the MQNAR.
Note that CAT models establish length generalization aligned with Theorem 2 .
fact that these models are more sensitive and harder to optimize in AR problems. In Fig 6, we used a
denser hyperparameter grid and more trials to ensure smoother curves with better reproducibility.
6.2
Evaluations on Language Modeling
Based on the outcomes from the synthetic experiments, we further explore the efficacy of the CAT
model in real-world NLP tasks by integrating a 1D CAT structure into the Pythia [5] framework. We
pretrain the modified 370M-parameter model on the SlimPajama [42] dataset, involving 15 billion
tokens. We then assess the model on a variety of downstream zero-shot tasks, including Wikitext,
Lambada, Piqa, Hella, Winogrande, Arc-E, and Arc-C, a methodology commonly used in the field to
evaluate generalization capabilities across diverse tasks [5, 15, 1, 2]. The findings are compiled in
Table 2.
9

Table 2: Experiment results for model pretraining. ∗are results from [49], which uses a same dataset and
training procedure as ours. We use the same hyperparameters as [49] for fair comparison. For perplexity, lower
is better, and for accuracy, higher is better. The average accuracy in last column is calculated by averaging the
accuracy across all tasks but excluding the perplexity tasks. The best and second best results are highlighted in
boldface and underline, respectively.
Model
Wikitext
ppl↓
Lambada_std
ppl↓
Lambada_openai
ppl↓
Lambada_std
acc↑
Lambada_openai
acc↑
Piqa
acc↑
Hella
acc_norm↑
Winogrande
acc↑
Arc-E
acc↑
Arc-C
acc_norm↑
Avg
Acc↑
Pythia
27.410
74.663
34.023
0.281
0.343
0.651
0.355
0.529
0.443
0.235
0.405
CAT, no PE
29.216
86.318
42.260
0.266
0.321
0.640
0.339
0.515
0.436
0.237
0.393
CAT, RoPE
26.776
65.423
38.557
0.288
0.341
0.654
0.362
0.507
0.461
0.239
0.407
MH-CAT, no PE
27.417
58.959
32.822
0.296
0.355
0.644
0.352
0.531
0.460
0.240
0.411
MH-CAT, RoPE
25.858
47.593
28.273
0.330
0.377
0.662
0.376
0.512
0.466
0.231
0.422
TF++ [46]∗
28.390
NA
42.690
NA
0.310
0.633
0.340
0.504
0.445
0.242
NA
Mamba [15]∗
28.390
NA
39.660
NA
0.306
0.650
0.354
0.501
0.463
0.236
NA
GLA [49]∗
28.650
NA
43.350
NA
0.303
0.648
0.345
0.514
0.451
0.227
NA
In this series of experiments, the CAT model is trained in two variants: one incorporating rotary
positional embedding [43] (PE) and another without positional embedding (noPE). We observe
that the CAT model with PE not only consistently outperforms the Pythia model but also achieves
performance better than state-of-the-art models, including Mamba [15], TF++ [46], and GLA [49].
Notably, the CAT model secures a superior perplexity gain compared to the standard model while
maintaining a similar level of parameters.
Regarding the noPE variant, training a Pythia model without positional encoding leads directly
to divergence and extremely large losses during training, affirming the critical role of positional
encoding in enabling standard transformer models to learn and converge. Intriguingly, despite the
absence of positional encoding, the CAT model still performs competitively with the leading models.
This suggests that the convolutional structure in the CAT model effectively captures positional
information within the data. We conjecture that the short convolutions provide positional information
for neighboring tokens, while the deep multi-layer network structure hierarchically aggregates this
information to establish long-range positional information.
This observation aligns with our synthetic experiment results, where the CAT model demonstrated
the capability to handle the AR task without positional encoding. These insights indicate that the
convolutional structure could potentially replace positional encoding, which might benefit length
extrapolation and generalization in the model. This offers a promising direction for further model
design and optimization in the field of NLP.
•Length Generalization Figure 1 presents the results from a length generalization experiment
with the CAT model, in which we trained the model on sequences of length 2,048 and assessed its
zero-shot performance on the Wikitext dataset across varying test sequence lengths. As a baseline
in our analysis, we implemented position interpolation (PI) [8] and YaRN [36] tempreture scaling
on RoPE models, including CAT/MH-CAT RoPE, to facilitate length generalization. The results
indicate that among the three RoPE models examined, the CAT model consistently demonstrates
excellent performance across all test sequence lengths. In contrast, the Pythia model exhibits a sharp
decline in performance as the sequence length increases. We suggest that is due to the additional
positional embeddings introduced by PI that was absent during the training phase. Despite this,
CAT models proficiently manage the relative positioning of tokens (especially overcome the new
positional embeddings by leveraging convolution information), which significantly boosts its ability
for length generalization. Additionally, the CAT model without PE is superior to the Pythia model
with RoPE, suggesting the effectiveness of the convolutional structure within the CAT model in
capturing essential positional data in length extrapolation.
6.3
Model Evaluation on Selective Copying
Fig. 7 displays the selective copying results for 1-layer and 2-layer models. We train these models
across a variety of model dimensions and sequence lengths. The models are required to copy 16
signal tokens from the input sequence and output them in the correct order. We observe that all
2-layer models perform well and show overlapping results, except for LinCAT. Among the 1-layer
models, CAT and Mamba achieve nearly 100% accuracy, while the performance of other models is
lower. These results are consistent with Theorem 4 and demonstrate that the 1-layer CAT model can
solve the selective copying problem without repetitions.
10

64
128
256
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
Sequence Length
64
128
256
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
64
128
256
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
Sequence Length
64
128
256
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 2 layers
Based, 2 layers
Mamba, 2 layers
Figure 7: Evaluation of models on selective copying tasks with varying model dimensions and sequence lengths.
Model dimensions are 32, 64, 128 for each column of the figures, from left to right. Top: Models trained on
1-layer architectures. Bottom: Models trained on 2-layer architectures. Note on 1-layer experiment, CAT and
Mamba achieve nearly 100% and their curves are overlapped.
7
Discussion
In this work, we have examined the synergy between the attention and convolution mechanisms
by introducing Convolution-Augmented Attention where K/Q/V embeddings are equipped with
convolution. We have shown that CAT layer enjoys strong theoretical guarantees when it comes to
AR and copying tasks with length generalization and also described insightful tradeoffs between the
need for attention and convolution. Importantly, real experiments confirm the benefit of CAT model
both in accuracy and in length generalization. Ultimately, we believe this work as well as the related
recent literature [15, 2, 38, 10] contributes to stronger design principles for the next generation of
(hybrid) architectures.
Limitations and future work. This work has a few shortcomings. We have only focused on
pretraining. However, Fig. 1 shows the potential of CAT in finetuning as a future direction. While
K/Q convolution helps in theoretical constructions for N-gram AR, in real experiments, they don’t
provide noticeable performance benefits. We suspect that K/Q convolution might be diluting the
attention scores and incorporating normalization or better parameterization can address this issue. An
important parameterization to explore is replacing the short convolutions within CAT with SSMs.
Finally, Section 5 introduced Landmark CAT as a sparse attention strategy. It would be interesting to
evaluate this proposal on real language modeling tasks.
Acknowledgements
This work was supported in part by the National Science Foundation grants CCF-2046816, CCF-
2403075, the Office of Naval Research award N000142412289, and gifts by Open Philanthropy and
Google Research.
References
[1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri
Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efficient language
models. arXiv:2312.04927, 2023.
[2] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley,
James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance
the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024.
11

[3] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin
token selection in attention mechanism. Advances in Neural Information Processing Systems,
36:48314–48362, 2023.
[4] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
weights to attend to the recent past. Advances in neural information processing systems, 29,
2016.
[5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In
International Conference on Machine Learning, pages 2397–2430. PMLR, 2023.
[6] Emmanuel Candes and Terence Tao. Near optimal signal recovery from random projections:
Universal encoding strategies?, 2006.
[7] Emmanuel J Candes. The restricted isometry property and its implications for compressed
sensing. Comptes rendus. Mathematique, 346(9-10):589–592, 2008.
[8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context
window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595,
2023.
[9] Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai.
In-context learning with transformers: Softmax attention adapts to function lipschitzness. arXiv
preprint arXiv:2402.11639, 2024.
[10] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms
through structured state space duality. arXiv preprint arXiv:2405.21060, 2024.
[11] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with
gated convolutional networks. In International conference on machine learning, pages 933–941.
PMLR, 2017.
[12] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,
Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin:
Mixing gated linear recurrences with local attention for efficient language models. arXiv
preprint arXiv:2402.19427, 2024.
[13] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis.
On the
optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680,
2023.
[14] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint
arXiv:2212.14052, 2022.
[15] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752, 2023.
[16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and
initialization of diagonal state space models. Advances in Neural Information Processing
Systems, 35:35971–35983, 2022.
[17] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured
state spaces. arXiv preprint arXiv:2111.00396, 2021.
[18] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured
state spaces. Advances in Neural Information Processing Systems, 35:22982–22994, 2022.
[19] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and
Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022.
[20] M Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From
self-attention to markov models: Unveiling the dynamics of generative transformers. arXiv
preprint arXiv:2402.13512, 2024.
[21] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me:
Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032,
2024.
12

[22] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and
Yoshua Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation,
31(4):765–783, 2019.
[23] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva
Reddy. The impact of positional encoding on length generalization in transformers. Advances
in Neural Information Processing Systems, 36, 2024.
[24] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. Time-parameterized convo-
lutional neural networks for irregularly sampled time series. arXiv preprint arXiv:2308.03210,
2023.
[25] Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak.
Mechanics of next token prediction with self-attention. In International Conference on Artificial
Intelligence and Statistics, pages 685–693. PMLR, 2024.
[26] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolu-
tional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022.
[27] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May,
Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and
inference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024.
[28] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint
arXiv:2209.10655, 2022.
[29] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji
Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of
transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024.
[30] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. NeurIPS, 2023.
[31] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with
gradient descent. arXiv preprint arXiv:2402.14735, 2024.
[32] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and
induction heads. arXiv preprint arXiv:2209.11895, 2022.
[33] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In
International Conference on Machine Learning, pages 26670–26698. PMLR, 2023.
[34] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the
role of attention in prompt-tuning. In International Conference on Machine Learning, pages
26724–26768. PMLR, 2023.
[35] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,
Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative
study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.
[36] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context
window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.
[37] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional
language models. In International Conference on Machine Learning, pages 28043–28078.
PMLR, 2023.
[38] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian
Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design
and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024.
[39] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
[40] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba:
Simple hybrid state space models for efficient unlimited context language modeling. arXiv
preprint arXiv:2406.07522, 2024.
13

[41] David Slepian. The one-sided barrier problem for gaussian noise. Bell System Technical Journal,
41(2):463–501, 1962.
[42] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan
Dey. Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023.
[43] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
[44] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans-
formers as support vector machines. arXiv preprint arXiv:2308.16898, 2023.
[45] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
Joma: De-
mystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint
arXiv:2310.00535, 2023.
[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[48] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally
parameterized convolutions for efficient inference. Advances in neural information processing
systems, 32, 2019.
[49] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear
attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.
14

64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 8: Performance of 1-layer models on MQAR tasks with varying model dimension and
sequence length. Noted that all models are trained using 1-layer architecture.
A
Detailed Experiment Setup
A.1
Associative Recall Experiments
We first introduce the training setup for the synthetic experiments. In our MQAR and MQNAR
experiments, we create a dataset with a vocabulary size of 8,092 to ensure that the vocabulary
replicates the scope of real language data. The dataset is constructed as described in Sec. 3.2.1, with
varying sequence lengths L of 64, 128, and 256, and 512. Specifically, we formulate the dataset in
the form of key-value pairs accompanied by multiple queries where the keys are unique within each
sequence. For each example, we initially select k keys from the vocabulary without replacement and
subsequently draw the values from the remaining vocabulary. We then randomly shuffle the keys
and associated values to form the input sequence. The number of queries is set to match k, ensuring
each key in the sequence is queried. It should be noted that while the keys are unique within a single
example, they may be repeated across different examples. For sequence lengths of L = 64, 128,
256, and 512, we set k = 16, 32, 64, and 128 respectively, indicating that the number of keys and
queries scales with the sequence length, thus increasing the task complexity. We generate 100,000
training examples and 3,000 testing examples for each of the sequence lengths. For NAR experiment,
we primarily focus on N = 2 to evaluate the performance. We construct the dataset similarly to
the MQAR task with sequence lengths of 64, 128, and 256. Consequently, the number of keys and
queries is reduced to k = 10, 20, 40 respectively, to accommodate the larger N. We generate 200,000
training examples and 3,000 testing examples for each sequence length.
For the training, we adhere strictly to the parameters set by [1], and their experimental setup and code,
using learning rate sweep among 0.001, 0.01, 0.1 and train the model for 64 epoches. The maximum
accuracy achieved across these learning rate is reported.
We remark that for the length generalization experiments, we sweep the learning rate among
0.001, 0.003, 0.01, 0.03, 0.1 and report the maximum accuracy over 5 runs to ensure the robust-
ness and reproducibility of the results.
A.2
Language Modeling Experiments
For the language modeling experiments, we exactly follow the setup from [49]. For the length
generalization experiment, we train the model on sequences of length 2,048 and assess its zero-shot
performance on the Wikitext dataset across varying test sequence lengths.
B
Additional Experiments
We conduct additional Experiments, Fig. 8 and 9 shows the result of 1-layer models on 1-gram and
2-gram MQNAR tasks with varying hidden sizes and sequence lengths. The model dimension is set
to 32, 64, and 128 for each column of the figures, from left to right. All other models perform much
worse compare to their 2-layer counterparts. Fig. 10 and 11 show the length generalization results
of 1-layer models on 1-gram and 2-gram MQNAR tasks. The results are consistent with the 2-layer
models.
15

64
128
256
512
Sequence Length
0.00
0.25
0.50
0.75
1.00
Accuracy
64
128
256
512
Sequence Length
64
128
256
512
Sequence Length
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 9: Performance of 1-layer CAT models on 2-gram MQNAR tasks with varying hidden sizes
and sequence length. All models are trained using 1-layer architecture.
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 10: 1-gram Length generalization
C
Proofs on Associative Recall and Selective Copying
C.1
Proof of Theorem 1
Proof. Given an N-gram Z ∈RN×d, let us define s(Z) = norm(P
i∈[N] FN−izi) to be its signature.
We will first show that for almost all F, each N-gram admits a unique signature. To see this, let
A, Z ∈RN×d be two distinct N-grams. Let us write the difference between their signatures as a
correlation coefficient. Set s′(Z) = P
i∈[N] FN−izi. Note that if s(A) = s(Z), we would have the
following function of F that arises from correlation coefficient as zero:
gA,Z(F) = (s′(Z)⊤s′(A))2 −∥s′(Z)∥2
ℓ2∥s′(A)∥2
ℓ2.
Now, observe that g is a fourth-order polynomial of the entries of F ∈RN and we can expand g(F)
further as follows
g(F) = (
X
i∈[N]
X
j∈[N]
FN−iFN−ja⊤
i z j)2 −∥
X
i∈[N]
FN−iai∥2
ℓ2∥
X
i∈[N]
FN−izi∥2
ℓ2.
(1)
Above, let ci be the coefficient of the fourth moment term F4
N−i. Note that
ci = (a⊤
i zi)2 −∥ai∥2
ℓ2∥zi∥2
ℓ2.
Since A , Z, there exists i ∈[N] such that ai , zi. This implies that ci , 0 and g(F) is a nonzero
polynomial. As a result, g(F) , 0 almost everywhere implying the same for s(Z) , s(A). Since there
32
64
128
256
512 1024
Test Length
0.00
0.25
0.50
0.75
1.00
Accuracy
 
32
64
128
256
512 1024
Test Length
 
32
64
128
256
512 1024
Test Length
 
CAT, 1 layer
LinCAT, 1 layer
Attention, 1 layer
Based, 1 layer
Mamba, 1 layer
Figure 11: 2-gram Length generalization
16

are finitely many N-grams, repeating the same argument for all N-gram pairs, we find that all N-gram
signatures are unique for almost all F.
Next, suppose we have an F resulting in unique signatures. We will prove the ability of CAT layer to
solve the N-AR problem. Consider an arbitrary sequence X and denote the last N tokens by Z. Let
X∗= norm(X ∗F) be the convolved sequence and let q be the final token of X∗. By assumption, q
repeats exactly twice in the sequence. Let α be the position of the q in the sequence. By definition, the
target token v = xα+1. Let Ii ∈RL be the indicator function that has 1 at position i and 0 everywhere
else. Since all N-grams are unique and their signatures have unit norm, we have that
lim
c→∞S(cX∗q) = s∗(X) := IL + Iα
2
.
(2)
Above we use the standard fact that softmax will saturate at the top entry as the inverse-temperature
goes to infinity. For the purposes of length generalization, we provide the precise temperature
requirement. Let a, b be two vectors in the normalized N-gram token set SN (the set of tokens
obtained after convolving with F). Over all such a, b, define the minimum cosine distance to be
∆= 1 −max
a,b∈SN a⊤b.
Given sequence X∗, using the worst case likelihood ratios of e−∆between the two q-tokens vs the
remaining L −2 non-q N-grams tokens, for any X, we have that
∥map(X, c) −s∗(X)∥ℓ1 = ∥S(cX∗q) −s∗(X)∥ℓ1 ≤
2(L −2)e−c∆
2 + (L −2)e−c∆.
(3)
To make the right hand side ≤ε/2 for all (admissible) sequences X of length at most L, we need
2(L −2)e−c∆≤ε which implies c ≥∆−1 log( 2(L−2)
ε
).
Value delay. For value delay, we will use (3) as key and query embeddings use the same filter. Let
Xv = 2 · X ∗D−1. Using the fact that rows of Xv are unit norm, for c ≥∆−1 log( 2(L−2)
ε
)
∥X⊤
v map(X, c) −X⊤
v s∗(X)∥ℓ2 ≤2∥S(cX∗q) −s∗(X)∥ℓ1 ≤ε.
Next, note that
X⊤
v s∗(X) = X⊤
v (IL + Iα
2
) = vα + vL
2
.
Now observe that, thanks to −1 delay, vα
=
2xα+1 and vL
=
2xL+1
=
0 resulting in
limc→∞X⊤
v map(X, c) = xα+1. Combining the above results, we find that ∥V⊤map(X, c) −v∥ℓ2 ≤ε for
all X.
Key delay. In this scenario, we are delaying X∗forward by one. Because of this, we have Xk = X∗∗D1
and, within Xk, q appears in positions α + 1 and L + 1. Since the latter is out of bounds, repeating the
argument (3) and defining map(X, c) := S(cXkq), for any sequence X, we find that
∥S(cXkq) −Iα+1∥ℓ1 ≤
2(L −1)e−c∆
1 + (L −1)e−c∆.
Similarly, the right hand side is upper bounded by ε, whenever c ≥∆−1 log( 2(L−1)
ε
).
To conclude, using the fact that tokens are unit norm and the target value vector is v = xα+1, for any
X, we obtain
∥X⊤map(X, c) −v∥ℓ2 ≤∥S(cX∗q) −Iα+1∥ℓ1 ≤ε,
completing the proof that ∥X⊤map(X, c) −v∥ℓ2 for all X of length at most L.
Concluding the proof of the theorem statement. So far, we have concluded that, for all input
sequences X, CAT layer output guarantees ∥f(X) −v∥ℓ2 < ε0 where v is the target value token and
ε0 is under our control by choosing c = ∆−1 log(2L/ε0). Since we assume the minimum distance
between distinct token embeddings are ε, to accurately and uniquely decode the target v, we choose
ε0 = ε/2 and apply nearest neighbor on f(X) to conclude.
□
17

C.2
Proof of Theorem 2
In this section, we will use the shorthand F to denote the value filter Fv for notational simplicity.
Recall that R0 > 0 is an absolute constant throughout the proof. Finally, the constant R used in
Theorem 2’s statement will be subsumed within the O(·) notation below.
Lemma 1. Consider the same setting in Theorem 2. For any f = (W, F) that can solve the AR
problem defined in Def. 2 up to ε-accuracy on all sequences of length L ≥3, if ε0 := ε/∆≤1/8, we
have that
∥F −D−1∥ℓ1 ≤O(Wε0(1 + Lε0) + Lε0) ≤O(Lε0(1 + Wε0))
(4)
∥F≥0∥ℓ1 =
W
X
i=0
Fi ≤O(ε0(1 + Lε0))
(5)
where we use O(·) notation to denote an upper bound up to a constant i.e. for some absolute r > 0,
O(x) ≤r · x. Moreover, let q be a token within vocabulary V and v be the top query not equal to q
that maximizes the similarity v⊤Wq i.e. v = arg maxx∈V,x,q x⊤Wq, we have
oq = sv/sq ≤Γ =
ε0
1 −4ε0
= O(ε0)
(6)
where sq and sv are the softmax values for q and v.
Proof. Throughout, we assume ε ≤∆
8 and ∆> 0 where ∆is the minimum embedding distance, i.e.,
∆= (1 −maxa,b∈V(a⊤b)2)1/2. Before proceeding, we first note that, without losing generality, we
can assume L ≥W + 1. The reason is that, if L ≤W, left or right end of the convolutional filter will
never interact with features. Thus, we simply set them to zero, truncating the filter. Define sequence
Xi ∈RL×d where xi
L−1 = q, xi
i = q and xi
j = v for all j , i. Let Zi = F ∗Xi. Let si = S(XiWq) and
sq = si
L−1 and sv = (1 −2sq)/(L −2). Here sq and sv are the softmax values for q and v respectively.
Additionally, observe that
sv/sq = exp((v −q)⊤Wq).
Finally, let I = [L] −{L −1, i} and recalling value sequence Zi, note that
f(Xi) = sv
X
j∈I
zi
j + sq(zi
i + zi
L).
By assumption, we also have that
∥v −f(Xi)∥ℓ2 ≤ε
for
i < L −2,
∥q −f(XL−2)∥ℓ2 ≤ε.
(7)
We will leverage these inequalities to prove the statement of the theorem. Let ρ = ρ(q, v) = q⊤v be
the correlation between q, v. Define v⊥=
q−ρv
∥q−ρv∥ℓ2 . Observe that convolution output has the form
f(Xi) = αv + βq for some α = αi, β = βi > 0. For i < L −2, we have that
ε ≥∥v −f(Xi)∥ℓ2 ≥|(v⊥)⊤(v −f(Xi))| ≥β|(v⊥)⊤q| ≥β
q
1 −ρ2.
Recalling that the minimum embedding distance is defined as ∆=
p
1 −maxq,v ρ2(q, v) ≤1 and
setting ε0 = ε/∆, this implies that
βi ≤ε0 := ε/∆
for
i < L −2,
αL−2 ≤ε0 := ε/∆.
(8)
Additionally, writing ε ≥|v⊤(v −f(Xi))| = |1 −αi −βiv⊤q| for i < L −2 and using |v⊤q| ≤1, we can
deduce
αi ≥1 −(1 + 1/∆)ε ≥1 −2ε0
for
i < L −2,
βL−2 ≥1 −(1 + 1/∆)ε ≥1 −2ε0
(9)
αi ≤1 + (1 + 1/∆)ε ≤1 + 2ε0
for
i < L −2,
βL−2 ≤1 + (1 + 1/∆)ε ≤1 + 2ε0.
(10)
We note that when L = W + 1, the problem only has a subtle difference, which we discuss at the end.
Case 1: L ≥W + 2. For i = 0 and i = L −2, the coefficients αi, βi can be written in terms of
convolution as
β0 = 2sqF0 + sv
X
i,0
Fi
(11)
βL−2 = sq(2F0 + F−1 + F1) + sv[2
X
i<0
Fi −(F−1 + F−W)].
(12)
18

Let ¯F1 = F−1 + F1. Observing 2 P
i<0 Fi −(F−1 + F−W) ≤2(P
i,0 Fi), we can write
1 −(1 + 1/∆)ε ≤βL−2 ≤sq ¯F1 + 2β0 ≤sq ¯F1 + 2ε/∆.
(13)
Combining these implies sq ¯F1 ≥1 −4ε0. Also, we know the trivial bound sq ¯F1 ≤βL−2 ≤1 + 2ε0.
Thus, we obtain
1 + 2ε0 ≥sq ¯F1 ≥1 −4ε0.
To proceed, we wish to prove that sv is small. From (11), we have that sv ¯F1 ≤ε0. Consequently, we
have that
sv
sq
≤Γ =
ε0
1 −4ε0
.
Using 2sq + (L −2)sv = 1, we get
1 = 2sq + (L −2)sv ≤(2 + (L −2)Γ)sq =⇒sq ≥
1
2 + (L −2)Γ =
1 −4ε0
2 + (L −10)ε0
.
Since sq ≤1/2 (due to query repeating twice), this also implies that
2(1 + Lε0)(1 + 2ε0)
1 −4ε0
≥2(1 + 2ε0)(1 + (L/2 −5)ε0)
1 −4ε0
≥¯F1 ≥2(1 −4ε0).
Using above, in essence, so far we have established that | ¯F1 −2| ≤O(Lε0) and sv/sq ≤O(ε0). Both
statements hold whenever ε0 ≤1/8 (e.g. so that 1/(1 −4ε0) ≤1 + O(ε0)). The primary remaining
item in the proof is establishing |Fi| ≤O(ε0) for all i , −1.
To prove this, we utilize the following observations: First, by keeping track of the contributions of
the last two q vectors on αL−2, we observe that
sq
W
X
i=1
Fi ≤αL−2 ≤ε0.
This implies PW
i=1 Fi ≤ε0/sq ≤ε0
2+(L−10)ε0
1−4ε0
= O(ε0(1+ Lε0)). We similarly find F0 ≤ε0/2sq through
(11). Finally, since F1 ≤O(ε0(1 + Lε0)) ≤O(Lε0), we also find the critical bound
|F−1 −2| ≤O(Lε0).
Finally, we wish to bound P
i≤−2 Fi. To do so, we can bound the contribution of the first q vector on
βi as follows. For any W ≥j ≥2, letting i = L −1 −j, we have that
ε0 ≥βi ≥sqF−j =⇒F−j ≤ε0
2 + (L −10)ε0
1 −4ε0
= O(ε0(1 + Lε0)).
Aggregating these, we have found the advertised bounds:
∥F −D−1∥ℓ1 ≤O(Wε0(1 + Lε0) + Lε0) ≤O(Lε0(1 + Wε0))
(14)
∥F≥0∥ℓ1 =
W
X
i=0
Fi ≤O(ε0(1 + Lε0))
(15)
oq = sv/sq ≤Γ =
ε0
1 −4ε0
= O(ε0)
(16)
where v is chosen to be the most similar token in terms of attention probabilities. Note that, the bound
on left entries of F that retrieves the past values is tighter than the right entries.
Case 2: L = W + 1. In this scenario, the main difference is we have the following estimates rather
than (11)
β0 = sq(2F0 + FW + F−W) + sv
X
i,0,|i|<W
Fi
(17)
βL−2 = sq(2F0 + ¯F1) + sv[2
X
i<0
Fi −(F−1 + F−W)].
(18)
19

So we can’t immediately use the estimate provided right below (13) because of the missing F−W sv
term. On the other hand, considering X1 and contribution of the first v token on β1, we have that
svF−W ≤β1 ≤ε0. As a result, we can instead use the fact that β0 + β1 ≤O(ε0) and the fact that
2sqF0 + sv(2
X
i<0
Fi −(F−1 + F−W)) ≤2(β0 + β1)
so that we have again established |1 −sq ¯F1| ≤O(ε0) and can proceed similarly.
□
Now that we have established the fine-grained control of the filter and attention map with Lemma 1,
we can conclude with length generalization.
Proof of Theorem 1. Given a query q and a sequence of length L′, let us define sq similarly (i.e. at-
tention probability that falls on the q token) and study the attention output. Let q appear at i for the
first time, v be the token following q, and I = [L′] −{i, L′ −1}. Let a = S(XWq) ∈RL′ be softmax
scores with ai = aL′−1 = sq. We write
f(X) =
X
j∈I
ajz j + sq(zi + zL′−1).
where z j = PW
i=−W Fixj−i To proceed, let R be a universal constant and Ξ = RLε0(1 + Wε0) so that
∥F∥ℓ1 ≤2 + Ξ from (14) in Lemma 1. Then we get ∥zj∥ℓ2 ≤∥F∥ℓ1 ≤2 + Ξ for all j ∈[L′]. Secondly,
due to right-clipped convolution we have ∥zL′−1∥ℓ2 ≤∥PW
i=0 Fi∥ℓ1 ≤Ξ and thanks to value retrieval at
i’th position, we get
∥zi −2v∥ℓ2 ≤|F−1 −2|∥v∥ℓ2 + |
X
j,−1
F j| ≤Ξ
(19)
Next, observe that aj/sq ≤sv/sq ≤Γ =
ε0
1−4ε0 for all j ∈I and that 2sq + P
j∈I aj = 1, consequently,
for some constant R0 > 0,
1
2 ≥sq ≥
1
2 + (L′ −2)Γ =
1 −4ε0
2 + (L′ −10)ε0
=⇒|2sq −1| ≤R0L′ε0.
and
X
j∈I
aj = 1 −2sq ≤R0L′ε0.
Aggregating these, we find that
∥f(X) −v∥ℓ2
(a)
≤∥
X
j∈I
ajz j∥ℓ2 + ∥sq(zi + zL′−1) −2sqv∥ℓ2 + |2sq −1|∥v∥ℓ2
(20)
(b)
≤|
X
j∈I
a j|(2 + Ξ) + ∥sq(zi + zL′−1) −2sqv∥ℓ2 + |2sq −1|
(21)
≤R0(2 + Ξ)L′ε0 + Ξ + R0L′ε0
(22)
≤3R0L′ε0 + Ξ + R0ΞL′ε0
(23)
≤3ε0
 R0L′ + RL(1 + Wε0)(1 + R0L′ε0)
(24)
where (a) follows triangle inequality and (b) follows Cauchy-Schwarz inequality. Let c0, c1 be
absolute constants to be determined. Assuming Wε0 ≤O(1) (i.e. bounded by constant), we have that
∥f(X) −v∥ℓ2 ≤c0ε0(L′ + L + LL′ε0)
where c0 ≥3 max{R0, R(1 + Wε0), R0R(1 + Wε0)}. Assuming the stronger bound Lε0 ≤O(1) and
c1 ≥c0(1 + L/L′ + Lε0), we have that
∥f(X) −v∥ℓ2 ≤c1ε0L′
This concludes the advertised proof.
□
20

C.3
Proving Length Generalization for N-gram AR (Proposition 3)
In this section we use Fv = F for the filter applied on value token and Fq = Fk = ¯F for filters on
query and keys.
Assumption 1. Recall that V is the vocabulary from which the token embeddings are drawn. We
have the following two assumptions to make the output f(X) more tractable:
a) The filter weights are bounded and obey ∥F∥ℓ1 ≤1. Besides, assuming that ∆= 1 −
maxa,b∈V,a,b b⊤a > 0
b) Any subset of 2N tokens within the vocabulary V is linearly independent.
Note that Assumption 1.b is essentially a restricted isometry property condition on the embedding
matrix induced by the vocabulary V. Specifically, if embeddings are randomly chosen, as soon as
the embedding dimension obeys d ≳O(N log |V|
N ), this assumption will hold with high probability
[7, 6]. In the following analysis, we will leverage either one of the assumptions to establish the length
generalization result.
Lemma 2. Suppose Assumption 1.b holds. Let B be any subset of 2N tokens within V and U :=
{uj| j ∈[|U|]} be the orthonormal tokens obtained after applying the Gram-Schmidt process on B
where bj = P j
l=0 βj,lul. Then we have 0 < δ = min j∈[|B|] |βj, j| ≤1.
Proof. First note that βj,l = b⊤
j ul ≤1 for any j, l ∈[|B|] and β0,0 = 1. Then δ = minj∈[|B|] |βj,j| ≤1.
Moreover, we can prove δ > 0 by contradiction. Assuming there exists j ≥1 such that βj, j = 0. This
indicates that bj can represented as a linear combination of the previously orthogonalized vectors
{u0, . . . , u j−1}. In other words, bj lies entirely in the span of these previous vectors. This contradicts
the fact that tokens in B are linearly independent. As a result we have δ > 0.
□
Proposition 3. Let ¯F ∈RN be a 1-D causal convolutional filter and F ∈R2W+1
+
be a 1-D convolutional
filter from time t = −W to t = W where W ≤L −N. Suppose that token embeddings have unit
norm. Consider the same CAT Layer f(X) = (XvWv)⊤S(XkWkW⊤
q q) defined in Theorem 1 where
q is the final token of Xq and Xq = norm(X ∗Fq) ∈RL×d (same for Xk, Xv). We set Fq = Fk = ¯F,
W = WkW⊤
q , and Wv = 2Id Consider any f = (W, F) that can solve the N-AR problem up to
ε-accuracy on all sequences of length L ≥O(N). That is, for all (X, y) where N-gram Z occurs
within X exactly twice and y being the associated value token that follows the first occurrence of
Z, we have ∥y −f(X)∥ℓ2 ≤ε. Let B is any subset of 2N tokens within vocabulary V and βj, j be the
corresponding projection coefficients defined in Lemma 2. Assume either Assumption 1.a or 1.b holds
and define
ε0 =

ε/∆,
∆= 1 −maxa,b∈V,a,b b⊤a
under Assumption 1.a
ε e2N/δ
δ ,
δ = min j∈[|B|] |βj, j|
under Assumption 1.b
For almost all choices of ¯F, there are absolute constants R0, R > 0 such that, if ε0 ≤R0/L, we have
that
• ∥F −D−1∥ℓ1 ≤Lε0
• Let s⋆∈RL′ be a vector with entries equal to 1/2 at the positions of query q in Xq and 0
otherwise. For all inputs X of arbitrary length L′, attention map obeys ∥S(XkWq) −s⋆∥ℓ1 ≤
L′ε0.
• For all N-AR sequences X of arbitrary length L′, we have that ∥y −f(X)∥ℓ2 ≤RL′ε0.
Lemma 3. Consider the same setting in Prop. 3, for any f = (W, F) that can solve the N-AR problem
defined in Def. 3 up to ε-accuracy on all sequences of length L ≥O(N). There are absolute constants
R0 > 0 such that, if ε0 ≤R0/N, we have that
∥F −D−1∥ℓ1 ≤O(Nε0(1 + Lε0) + Nε0) ≤O(Lε0(1 + Nε0))
(25)
∥F≥0∥ℓ1 =
W
X
i=0
Fi ≤O(Nε0(1 + Lε0))
(26)
21

where we use O(·) notation to denote an upper bound up to a constant i.e. for some absolute r > 0,
O(x) ≤r · x. Moreover, we consider N-gram Zq ∈RN×d that ends with a token q′, which can be any
token from the vocabulary BN. Let q be the final token of norm(Zq ∗¯F) and v be the top query not
equal to q that maximizes the similarity v⊤Wq. i.e. v = arg maxx∈BN,x,q x⊤Wq, we have
oq = sv
sq
≤Γ =
O(ε0)
1 −O(Nε0) ≤O(Nε0)
(27)
where sq and sv are the softmax values for q and v.
Proof. Following the proof of Thoerem 1, suppose that we have an ¯F that results in unique signatures.
We argue that the length generalization fails when W > L −N, which is explained at the end.
Throughout, we assume that W = L −N. When W < L −N, it is equivalent to the setting where
W = L −N and F j = 0 for W + 1 ≤|j| ≤L −N. Denote the corresponding N-gram that results in q
and v after convolving with ¯F be Zq = [q0, q1, . . . , qN−1] ∈RN×d and Zv = [v0, v1, . . . , vN−1] ∈RN×d
respectively, i.e., q = norm(Zq∗¯F) and v = norm(Zv∗¯F). Zq and Zv are unique due to the assumption
on ¯F. For brevity, let q′ = qN−1, v′ = vN−1 and Zk
v′ = [v′, v′, . . . , v′] ∈Rk×d, Z′
q = [q1, . . . , qN−1] ∈
R(N−1)×d, where q0 is removed from Zq.
Xi,k =
h
ZN−1+i
v′
Zv
Zq
ZN−1+k
v′
Z′
q
q0
ZN−1
v′
Zni,k
v′
Zv
Zq
i
∈RL×d
(28)
¯Xi,k =
h
ZN−1+i
v′
Zv
Zq
q0
ZN−1+k
v′
Z′
q
ZN−1
v′
Zni,k
v′
Zv
Zq
i
∈RL×d
(29)
where ni,k = L −8N + 3 −i −k ≥0, and this naturally introduces a lower bound for L, i.e., L ≥O(N)
and upper bounds for both i and k. Note that Xi,k and ¯Xi,k have different labels. By assumption, we
have that
∥v′ −f(Xi,k)∥ℓ2 ≤ε,
∥q0 −f( ¯Xi,k)∥ℓ2 ≤ε.
(30)
Let si,k = S(Xi,kWq), ¯si,k = S( ¯Xi,kWq). Define the probability of selecting the j-th entry of Xi,k and
¯Xi,k as si,k
j and ¯si,k
j and selecting the token q and v as sq, sv. Here we omit i, k for sq and sv since it’s
invariant to the values of i, k. Additionally, observe that
sv/sq = exp((v −q)⊤Wq) and (L −2)sv + 2sq ≥1
where the inequality comes from the fact that v = arg maxx∈BN,x,q x⊤Wq.
We will lever-
age these inequalities to prove the statement of the theorem. Define the vocabulary set B =
{v0, v1, . . . , vN−1, q0, q1, . . . , qN−1} which includes all tokens in Xi,k and ¯Xi,k. Note that the vocab-
ulary B is a subset of tokens chosen from V, i.e., B ⊆V and the vocabulary size |B| is at most
2N, i.e., |B| := K ≤2N. Observe that convolution output has the form f(Xi,k) = P
j∈[|B|] mi,k
j bj and
f( ¯Xi,k) = P
j∈[|B|] ¯mi,k
j b j where {mi,k
j , ¯mi,k
j }j∈[|B|] are non-negative coefficients due to the assumption that
entries in ¯F and softmax probabilities si,k and ¯si,k are non-negative. In particular, we are interested
in mi,k
q , ¯mi,k
q and mi,k
v , ¯mi,k
v , which correspond to the coefficients of token q0 and v′. To proceed, we
leverage Assumption 1 to bound the coefficients:
When Assumption 1.a holds. By expanding the coefficients, we get
X
j∈[|B|]
mi,k
j =
X
j∈[|B|]
X
t∈[L]
Fvt −jsi,k
t
≤∥F∥ℓ1
X
t∈[L]
si,k
t
≤1
(31)
Combining this with the fact that
ε ≥∥v′ −f(Xi,k)∥ℓ2 ≥|v′⊤(v′ −f(Xi,k))| ≥|
X
j∈[|B|],bj,v′
v′⊤bjmi,k
j + mi,k
v −1|
(32)
, we have
ε ≥
X
j∈[|B|],bj,v′
(1 −v′⊤bj)mi,k
j ≥(1 −v′⊤bj)mi,k
j
for any
j ∈{j | j ∈[|B|], bj , v′}
(33)
→mi,k
j ≤ε/∆:= ε0
for any
j ∈{j | j ∈[|B|], bj , v′}
(34)
22

where ∆= 1 −maxa,b∈B,a,b b⊤a > 0. In terms of mi,k
v , we apply Triangle Inequality on (32) and (34):
|1 −mi,k
v | ≤|
X
j∈[|B|],bj,v′
v′⊤bjmi,k
j + mi,k
v −1| + |
X
j∈[|B|],b j,v′
v′⊤b jmi,k
j |
(35)
≤ε + 2Nε0 ≤(2N + 1)ε0
(36)
Similarly for ¯Xi,k we have
¯mi,k
j ≤ε0
for any
j ∈{j | j ∈[|B|], b j , q0},
|1 −¯mi,k
q | ≤O(Nε0)
(37)
When Assumption 1.b holds. Based on the linear independence property, we can apply the
Gram–Schmidt process to transform the tokens in B to orthonormal tokens U = {uj| j ∈[|U|]}
where bj = P j
l=0 βj,lul where βj,l = b⊤
j ul. Since the order of tokens in U does not matter, we can
set u0 = v′. Then for any j ≥1, uj is orthogonal to v′ and bi for all i < j. Consider the case of Xi,k
whose label is v′, utilizing the orthogonality we get
ε ≥∥v′ −f(Xi,k)∥ℓ2 ≥|u⊤
j (v′ −f(Xi,k))| ≥|
|B|−1
X
l= j
mi,k
l u⊤
j bl|
(38)
Using backward induction, we can then bound mi,k
j for 1 ≤j ≤K−1. First consider j = |B|−1 = K−1.
Then we have:
ε ≥|mK−1u⊤
K−1bK−1| = |mK−1βK−1,K−1| ≥|mK−1δ|
(39)
where δ = min j∈[|B|] |βj, j| = minj∈[|B|] |b⊤
j u j|. Following Lemma 2 we have 0 < δ ≤1. As a result we
get mK−1 ≤ε/δ. Next we prove that if j ≥1 and mi,k
l
≤ε (1+1/δ)K−l−1
δ
for j < l ≤K−1, mj ≤ε (1+1/δ)K−j−1
δ
.
When 1 ≤j ≤K −2, from equation (38) we can derive
ε ≥|
K−1
X
l=j
mi,k
l u⊤
j bl| = |
K−1
X
l=j+1
mi,k
l u⊤
j bl + mju⊤
j bj|
(40)
For the first term we have
|
K−1
X
i=j+1
miu⊤
j bi| ≤
K−1
X
i=j+1
mi ≤
K−j−2
X
i=0
ε(1 + 1/δ)i
δ
= ε (1 + 1/δ)K−j−1 −1
Using Triangle Inequality we get
|mju⊤
j bj| ≤ε(1 + 1/δ)K−j−1 →mj ≤ε(1 + 1/δ)K−j−1
δ
≤εe
K−j−1
δ
δ
≤εe2N/δ
δ
(41)
We can hereby bound mi,k
j for any j ∈{j | bj ∈B, bj , v′}. Let ε1 := ε e2N/δ
δ , we have
mi,k
j ≤ε1
for any
j ∈[|B|], bj , v′
(42)
Additionally, writing ε ≥|v′⊤(v′ −f(Xi,k))| = |1 −mi,k
v −v′⊤P
j∈[|B|],bj,v′ mi,k
j bj| and using |v′⊤bj| ≤1
for any bj ∈B, we can deduce
|1 −mi,k
v | ≤ε +
K−1
X
i=1
mi
(43)
≤ε +
K−2
X
i=0
ε(1 + 1/δ)i
δ
(44)
≤ε(1 + 1/δ)K−1 ≤ε1
(45)
Similarly for ¯Xi,k, we have
¯mi,k
j ≤ε1
for any
j ∈[|B|], b j , q0,
|1 −¯mi,k
q | ≤ε1
(46)
23

To summarize, using Assumption 1, we can have an upper bound on {mi,k
j , ¯mi,k
j } j∈[|B|]:
mi,k
j ≤ε0
for any
j ∈{j | j ∈[|B|], b j , v′},
|1 −mi,k
v | ≤O(Nε0)
(47)
¯mi,k
j ≤ε0
for any
j ∈{j | j ∈[|B|], b j , q0},
|1 −¯mi,k
q | ≤O(Nε0)
(48)
where
ε0 :=

ε/∆,
∆= 1 −maxa,b∈B,a,b b⊤a > 0
under Assumption 1.a
ε e2N/δ
δ ,
δ = min j∈[|B|] |βj, j|
under Assumption 1.b
(49)
We proceed by comparing mi,k
q and ¯mi,k
q :
mi,k
q = 2sq
 F−L+4N+i−2 + F−2N+1−k + 2FN−1 + FL−5N−i−k + FL−2N−i

(50)
+ 2
X
j∈[L]−{3N−2+i,L−1}
si,k
j (F−L+N+ j + F−5N−i−k+j+3 + F−2N−i+ j+1)
(51)
¯mi,k
q = 2sq
 F−L+4N+i−2 + F−1 + 2FN−1 + FL−3N−i + FL−2N−i

(52)
+ 2
X
j∈[L]−{3N−2+i,L−1}
¯si,k
j (F−L+N+j + F−3N−i+ j+1 + F−2N−i+j+1)
(53)
Observing that
• si,k
j = ¯si,k
j for j ∈[3N −1 + i] and 6N −3 + i + k ≤j ≤L −1
• si,k
j+1 = ¯si,k
j for 4N −1 + i + k ≤j ≤5N −3 + i + k
• si,k
j+(2N−1) = ¯si,k
j for 5N −2 + i + k1 ≤j ≤6N −4 + i + k
• si1,k1
j+2N−2+k1+i1−i2 = ¯si2,k2
j
for 3N −1 + i2 ≤j ≤4N −2 + i2
Utilizing these observations, we have
X
j∈[3N−1+i]
2¯si,k
j (F−L+N+j + F−3N−i+ j+1 + F−2N−i+j+1) ≤mi,k
q + mi+N,k
q
X
j∈[6N−3+i+k,L−1]
2¯si,k
j (F−L+N+j + F−3N−i+ j+1 + F−2N−i+j+1) ≤mi,k
q + mi,k−N
q
X
4N−1+i+k≤j≤5N−3+i+k
2¯si,k
j (F−L+N+j + F−3N−i+ j+1 + F−2N−i+j+1) ≤mi+1,k
q
+ mi−N+1,k
q
+ mi,k+1
q
X
5N−2+i+k≤j≤6N−4+i+k
2¯si,k
j (F−L+N+j + F−3N−i+ j+1 + F−2N−i+j+1) ≤mi+(N−1),k
q
+ mi+(2N−1),k
q
+ mi,k+(2N−1)
q
X
3N−1+i≤j≤4N−2+i
2¯si,k
j (F−L+N+j + F−3N−i+ j+1 + F−2N−i+j+1) ≤mi−(2N−2),k
q
+
X
3N−1+i≤j≤4N−2+i
¯si,k
j (F−3N−i+j+1 + F−2N−i+j+1)
(a)
≤mi−(2N−2),k
q
+ sv
X
l∈[2N]
Fl
(b)
≤mi−(2N−2),k
q
+
X
j∈{j | bj=¯xi,k
l ,l∈[2N]}
¯m1,k
j
where (a) comes from v = arg maxx∈BN,x,q x⊤Wq and (b) comes from the attention from the first v to
itself and its previous 2N −1 terms. Let i = 2N −2, k = N, combining the inequalities above, we get
1 −O(Nε0) ≤¯m2N−2,N
q
≤2sq(F−1 + FL−5N+2) + m2N−2,N
q
+ m3N−2,N
q
+ ... + m0,N
q
+
X
j∈{j | bj=¯xi,k
l ,l∈[2N]}
¯m1,N
j
(54)
≤2sq(F−1 + FL−5N+2) + O(Nε0)
(55)
24

Combining these we get sq(F−1 + FL−5N+2) ≥1/2 −O(Nε0). Moreover, from (52), we have sq(F−1 +
FL−5N+2) ≤¯mi,k
q /2 ≤1/2 + O(Nε0), which results in
|sq(F−1 + FL−5N+2) −1/2| ≤O(Nε0)
(56)
Based on this, we wish to show that sv is small. Substituting l ∈{4N −4, L −N} into (51), we have
sv(F−1 + FL−5N+2) ≤m2N−2,N
q
≤ε0, which implies that
sv
sq
≤Γ =
O(ε0)
1/2 −O(Nε0)
(57)
Using 2sq + (L −2)sv ≥1, we have
1 ≤2sq + (L −2)sv ≤(2 + (L −2)Γ)sq =⇒sq ≥
1
2 + (L −2)Γ ≥
1 −O(Nε0)
2 + (L −2N −2)O(ε0)
(58)
Combining this with 2sq ≤1, we get
(1 + LO(ε0))(1 + O(Nε0))
1 −O(Nε0)
≥(1 + O(Nε0))(1 + (L/2 −N −1)O(ε0))
1 −O(Nε0)
(59)
≥F−1 + FL−5N+2 ≥1 −O(Nε0)
(60)
At this stage, we have already proved that when Nε0 ≤O(1), |F−1 + FL−5N+2 −1| ≤O(Lε0) and
sv/sq ≤O(ε0). The primary remaining proof is to prove |Fl| < O(ε0) for all l , −1. Since ¯mi,k
j ≤ε0
for b j , q0, by tracking the contribution of the last q on ¯mi,k
j , we have
sq
X
0≤l≤L−N,l<{N−1,L−2N,L−3N}
Fl ≤
X
j∈{j | bj∈B,bj,q0}
¯m0,0
j
≤O(Nε0)
(61)
sq
X
l∈{L−2N,L−3N}
Fl ≤
X
j∈{ j | bj∈{q′,v′}}
¯m1,0
j
≤ε0
(62)
sqFN−1 ≤m0,0
q
≤ε0
(63)
Combining these three inequalities and W
=
L −N, we get PW
l=0 Fl
≤
O(Nε0)/sq
≤
O(Nε0) 2+(L−2N−2)O(ε0)
1−O(Nε0)
≤O(Nε0(1+Lε0)). Additionally, since FL−5N+2 ≤O(Nε0(1+Lε0)) ≤O(NLε0),
we get
|F−1 −1| < O(NLε0)
(64)
Finally, we want to bound P
−L+N≤j≤−2 F j. By tracking the contribution of the first q to ¯m0,0
j
for any
j ∈{j | bj ∈B, bj , q0}, we have sqFl ≤¯m0,0
j
where b j = xi,k
−l+3N−2 for any −L + 3N −1 ≤l ≤−2.
Summing over l we get,
sq
X
−L+3N−1≤l≤−2,l,W−4N+3
Fl ≤
X
j∈{ j | bj∈B,bj,q0}
¯m0,0
j
≤O(Nε0)
(65)
Note that F−L+4N−2 touches q0 for ¯X0,0 so we need to handle it separately. We can consider ¯m1,0
j
instead and get
sqF−L+4N−2 ≤¯m1,0
L−N+1 ≤ε0
(66)
For P
−L+N≤j≤−L+3N−2 F j, we consider the following example:
ˆXi,k =
h
Zq
v0
ZN−1+i
v′
Zv
ZN−1+k
v′
Z′
q
ZN−1
v′
Zni,k
v′
Zq
i
∈RL×d
(67)
Similarly we define ˆsi,k = S( ˆXi,kWq) and the probability that selects q and v as ˆsq and ˆsv. Note that
ˆsv/ˆsq = sv/sq ≤O(ε0) and (L−2)ˆsv+ ˆsq ≥1. We can then obtain the exact lower bound as sq for ˆsq, i.e.,
ˆsq ≥
1−O(Nε0)
2+(L−2N−2)O(ε0). Note that the output of f( ˆXi,k) can also be written as f( ˆXi,k) = P
bj∈B ˆmi,k
j bj where
{ ˆmi,k
j }b j∈B is the corresponding coefficients. Moving forward, by tracking the attendance of the first q
to the last 2N −1 terms, we have ˆsqFl ≤ˆm0,0
j
where bj = ˆxi,k
−l+i+N−1 for any −L+ N ≤j ≤−L+3N −2.
25

Repeating the same argument based on Assumption 1, we get ˆm0,0
j
≤ε0 for any j ∈{j|bj ∈B, b j , v0},
which leads to
ˆsq
X
−L+N≤l≤−L+3N−2
Fl ≤ˆsq
X
j∈{ j | bj∈B,bj,v0}
ˆm0,0
j
≤O(Nε0)
(68)
Combining (65), (66) and (68), we have
X
−L+N≤l≤−2
Fl ≤O(Nε0)(1/ˆsq + 1/sq) ≤O(Nε0)2 + (L −2N −2)O(ε0)
1 −O(Nε0)
≤O(Nε0(1 + Lε0))
(69)
In summary, we get
∥F −D−1∥ℓ1 ≤O(Nε0(1 + Lε0) + Nε0) ≤O(Lε0(1 + Nε0))
(70)
∥F≥0∥ℓ1 =
W
X
i=0
Fi ≤O(Nε0(1 + Lε0))
(71)
oq = sv
sq
≤Γ =
O(ε0)
1 −O(Nε0) ≤O(Nε0)
(72)
where v is chosen to be the most similar tokens to q in terms of attention probabilities. Now
we discuss the scenario where W > L −N. First note that the output f(X) can be written as
f(X) = 2 P
l∈[L] sl
PW
j=−W F jxl−j where sl is the softmax value at l-th position. We are interested in
sq in particular, which is proven to converge to 1/2 when ε diminishes. Also, note that the smallest
possible index of q is N −1 since it’s the last token of an N-gram. Then, when W > L −N, the left
end of the convolutional filter never interacts with sq since the index of xi−j is out of bound, i.e.,
i −j = N −1 + W > L −1
□
Using the results from Lemma 3, we can establish the length generalization on N-AR task.
Proof of Proposition 3. Given a sequence X of length L′, let q be the last token of Xq = Xk =
norm(X ∗¯F) and we define sq as the attention probability that selects q. Assume the first occurrence
of q in Xk is i and q′ = xi. By definition, the target vector v is the token following q′ in X, i.e.,
v = xi+1. Let I = [L′] −{i, L′ −1}. Let a = S(XkWq) ∈RL′ be the softmax probabilities where
ai = aL′−1 = sq
f(X) =
X
j∈I
ajzj + sq(zi + zL′−1)
(73)
where zj = 2 PW
i=−W Fix j−i. We define R as a universal constant and Ξ = RLε0(1 + Nε0) such that
∥F∥ℓ1 ≤1 + Ξ from Lemma 3. Then we get ∥z j∥ℓ2 ≤2∥F∥ℓ1 ≤2(1 + Ξ) for all j ∈[L′]. Note that
aj/sq ≤sv/sq = Γ =
O(ε0)
1−O(Nε0) for all j ∈I and that 2sq + P
j∈I a j = 1. As a result, there exists some
constant R0 > 0 such that
1
2 ≥sq ≥
1
2 + (L′ −2)Γ =
1 −O(Nε0)
2 + (L′ −2N −2)O(ε0) =⇒|2sq −1| ≤R0L′ε0
(74)
and
X
j∈I
aj = 1 −2sq ≤R0L′ε0
(75)
Moreover, due to right-clipped convolution, we have ∥zL′−1∥ℓ2 = 2∥PW
i=0 Fi∥ℓ1 ≤2Ξ. Next, according
to the value retrieval at i-th position, we have
∥zi −2v∥ℓ2 ≤|2F−1 −2|∥v∥ℓ2 + 2|
X
j,−1
F j| ≤2Ξ
(76)
26

Utilizing these findings above, we get
∥f(X) −v∥ℓ2 ≤∥
X
j∈I
a jzj∥ℓ2 + ∥sq(zi + zL′−1) −2sqv∥ℓ2 + |2sq −1|∥v∥ℓ2
(77)
≤|
X
j∈I
aj| max
j
∥zj∥ℓ2 + sq(∥zi −2v∥ℓ2 + ∥zL′−1∥ℓ2) + |2sq −1|
(78)
≤2R0L′ε0(1 + Ξ) + 2Ξ + R0L′ε0
(79)
≤3R0L′ε0 + 2Ξ + 2R0ΞL′ε0
(80)
≤3ε0(R0L′ + 2RL(1 + Nε0)(1 + R0L′ε0))
(81)
Let c0, c1 be absolute constants to be determined. Assuming Nε0 ≤O(1) (i.e. bounded by constant),
we have that
∥f(X) −v∥ℓ2 ≤c0ε0(L′ + L + LL′ε0)
where c0 ≥3 max{R0, 2R(1 + Nε0), 2R0R(1 + Nε0)}. Assuming the stronger bound Lε0 ≤O(1) and
c1 ≥c0(1 + L/L′ + Lε0), we have that
∥f(X) −v∥ℓ2 ≤c1ε0L′
This concludes the advertised results.
□
C.4
Proof of Theorem 4
Below we state a generalization of Theorem 4 which distinguishes two scenarios: Short convolution
with PE and Long convolutions with no PE.
Theorem 4 (Selective Copy). Consider the setting of Def. 4. There is a 1-layer CAT f using
exponential-decay query-convolution Fq as follows:
• Suppose Fq is infinitely long (namely parameterized as an SSM with state matrix A = ρ for
some decay parameter ρ < 1). Then, using d = |S| + 3 dimensional token embeddings, f
solves unique selective copying.
• Suppose Fq ∈RN and input sequences contain at most N signal tokens. Using d = |S| + 4
and 1-D positional encodings, f solves unique selective copying.
Proof. Let T be the maximum context length the model encounters. Specifically, T = L + N + 1
where L is the maximum length of the input sequence X that precedes the special token ⊥and N
is the maximum number of signal tokens in X. Recall that the cardinality of the signal vocabulary
S is allowed to be larger than N and we resume generation until outputting all signal tokens. Let
Z = [X ⊥zL+2 . . . zt] denote the current input sequence where [X ⊥] is the initial input that kickstarts
decoding. Denote boldface Z, X to be the embedded sequences of Z, X. We use query convolution
thus the CAT model is given by f(Z) = nearest_token_embedding(Z⊤S(ZWz∗
t )) where Z∗= Fq ∗Z
is the convolved sequence and z∗
t is the last token of the convolved query sequence for L + 1 ≤t ≤T.
We set convolution to be Fq,i = ρi for 0 ≤i < W for a suitable ρ ≤1 to be determined where W is
the window size of the convolution. This choice aggregates the current token and the W −1 most
recent tokens and allows for all all-ones filter as a special case. For the first statement of the theorem
W = ∞whereas for the second statement W = N.
The choice of token embeddings. We construct the token embeddings as follows:
• Token embedding of the ith token has the form xi = [x′
i, si, pi]. Here
– Base embedding. x′
i is the base embedding vector associated to the discrete token
value xi. We choose these x′
i embeddings to have unit Euclidean norm.
– Signal indicator. si ∈R is an indicator of whether the token is a signal token or not.
We set si = 1 for signal tokens and the ⊥token and si = 0 for noise tokens.
– Position encoding. pi ∈R is the positional encoding of the i’th token. We simply set
pi = i/T where T = L + N + 1. pi is only required for short convolution i.e. when
W = N.
• The base embeddings of noise tokens N are orthogonal to that of signal tokens and ⊥token.
27

• The base embeddings of signal tokens S and ⊥are also orthogonal to each other.
Let Dnoisy be the dimension of the subspace spanned by the base embeddings of noise tokens. We
can choose Dnoisy = 1 by setting all base embeddings for the noise tokens to be identical. The signal
tokens and ⊥token occupies an orthogonal |S| + 1 dimensional subspace. Together, this recovers the
embedding dimensions advertised in the theorem statement, namely
• With positional encoding and W = N: We need an embedding dimension of d = |S| +
Dnoisy + 3 ≥|S| + 4 where two additional dimension is due to si and pi.
• Without positional encoding and W = ∞: We need an embedding dimension of d =
|S| + Dnoisy + 2 ≥|S| + 3 where the additional dimension is due to si.
• Construction of the CAT model. We construct a one layer CAT model with the following criteria
in the order of priority:
1. The model should always select signal tokens.
2. The model should select a signal token not previously selected.
3. The model should select the farthest signal token from the current/last token (i.e. generates
signal tokens that are closer to the start of the sequence).
To satisfy the three criteria above, we pick the attention weights W as follows when W = N:
W =

−αIN+1+Dnoisy
0
0
0
β
0
0
0
−θ
.
(82)
The choice for W = ∞is same except that we do not have the positional encoding coefficient θ.
Recall that we also choose the convolutional filter as Fq,i = ρi for 0 ≤i < W for ρ < 1. Specifically,
we choose ρ = 2−1/T so that ρT = 1/2. This choice guarantees that ρi −ρi+1 ≥c/T for all 0 ≤i < T
for some absolute constant c > 0.
We will accomplish the proof inductively. Suppose that X contains N′ unique signal tokens and that
until time t for some L + 1 ≤t ≤L + N′ + 1, the model outputs the correct sequence of t′ = t −L −1
unique signal tokens. We will prove that it will accurately output the next signal token in line with
suitable choice of α, β, θ. To this end, we state the following lemma regarding the output of the
query-convolution z∗
t .
Note that z∗
t = Pt
i=1 ρizt−i. Recall that zL+1 to zt are unique correctly-ordered signal tokens where we
set z0 = ⊥. Denote the rest of the N′ −t′ signal tokens with correct order by q1 to qN′−t′. Here q1 is
the left most such token in X and the token we wish to output next. We can write z∗
t in terms of signal
tokens and noise tokens as follows:
z∗
t =
t′+1
X
i=1
bizt−i + n +
N′−t′
X
j=1
ajq j,
(83)
where we set bi := ρi. Here the first term Pt′+1
i=1 bizt−i is due to last t′ + 1 tokens (including ⊥) that
are already generated. The n term denotes the aggregated contribution of the noise tokens to the
convolution. PN′−t′
j=1 a jqj is the contributions of the signal tokens that are yet to be generated. Crucially
note that,
• If W = ∞, ai is strictly increasing because convolution coefficients Fq,j are strictly decreasing
(with a gap lower bounded by c/T).
• Whether W = N or W = ∞, bi = ρi is strictly decreasing and bt′+1 ≥aN′−t′ + c/T. That is,
the contribution of any token already generated is larger than any token that is yet to be
generated.
28

Let us write z∗
t = [z′∗
t s p]. Note that s ≥1/2 because ⊥token is involved in the convolution and
ρT = 1/2. Similarly, if we employ PE, we have that p ≥(L + 1)/2T ≥1/4 for the same reason. Given
a token xi = [x′
i si pi], through (82), we have that
scorei = x⊤
i Wz∗
t = −α 
z′∗
t , x′
i
 + βssi −θppi.
(84)
We now proceed with the proof which relies on choosing α, β, θ > 0 in a suitable fashion. Specifically,
we will choose their relative ratios β/α, α/θ suitably to ensure the desired token q1 receives the
highest score. After ensuring this, we can suitably scale up α, β, θ in a proportional fashion, which
will also scale up the scores of each token. Thanks to softmax attention, this will ensure that the
model precisely retrieves the token with the highest score.
Scenario 1: W = ∞. In this scenario, we don’t use PE, thus, effectively θ = 0. We need to satisfy
aforementioned criteria: First, we want the highest score to be a signal token. We will guarantee this
by observing si = 0 for noise tokens, s > 0 and by setting β/α ≫1. Second, we want the highest
score to be q1, the left most signal token that has not been output yet. Now, since W = ∞, q1 receives
the lowest coefficient of a1 in (83). Using orthogonality and unit Euclidean norm, this implies that
D
z′∗
t , q′
1
E
= a1. In contrast, any other signal token has a larger inner product by at least c/T. Choosing
α = 1 (and then suitably scaling it up together with β), this implies that, q1 is indeed the token with
the highest score that will be generated next.
Scenario 2: W = N and we employ PE. We again follow the score decomposition (84). Observe that
D
z′∗
t , x′
i
E
, ssi, ppi are all bounded by 1 in absolute value. Thus, by controlling the relative ratios of
the scalar knobs β > α > θ = 1, we can enforce the three criteria listed above. Recall that q1 denotes
the next signal token we wish to output next. We will prove that q1 achieves the strictly highest score
amoung the tokens of Z. To proceed, set β/α ≫1 and α/θ ≫1.
• Since β dominates α and θ, following the same argument in Scenario 1, noise tokens will
have strictly lower scores than signal tokens, thus cannot be generated next.
• Following (84), the signal tokens have a score contribution of −α · bi or −α · aj from the
inner product term
D
z′∗
t , x′
i
E
. Here bi denoted the coefficient of a generated signal token
whereas aj denoted the coefficient of a missing signal token. Next recall from (83) that
bi ≥c/T > 0 and bi ≥aj + c/T thanks to the Fq choice. Since α dominates θ, this implies
that the generated signal tokens have strictly less score than the missing signal tokens.
• Finally, we wish to show that q1 has the highest score among missing signal tokens. First,
recall from (83) that a1 is the smallest coefficient among the missing signal tokens. As a
result, it achieves the largest inner product score −α·
D
z′∗
t , x′
i
E
. To complete the proof, we use
positional encoding to break any score ties. Since q1 is the left most missing signal token,
any other missing signal token will achieve a strictly worse position encoding score −θppi
as p ≥1/4, θ = 1, and pi = i/T is strictly increasing. This guarantees that q1 achieves the
strictly highest score as desired.
To summarize, by choosing suitable β ≫α ≫θ = 1 and proportionally scaling up α, β, θ sufficiently,
we conclude with the proof.
□
D
Proofs for Section 5 – Convolution-Attention Tradeoff
D.1
Proof of Proposition 1
The key to the proof is establishing the detection threshold of the correct block ID β in (LCAT)
i.e. we wish to guarantee b = β. Once correct block is retrieved, the rest of the argument is
identical to AR over dense attention as we retrieve the correct blocks. Observe that, we have ¯L −1
blocks in total (not counting the local/final block). Note that zi ∼N(0, σ2BId/d) for i , β and
zβ ∼N(xL, σ2(B −1)Id/d).
Set gi = z⊤
i xL · √d/B for i < ¯L and gβ. Observe that gi’s and gβ are independent random variables.
Additionally, gi,β ∼N(0, σ2), gβ ∼N( √d/B, (1 −1/B)σ2). Let gmax = maxi,β gi. We have the
29

following gaussian concentration inequalities
P(gmax ≥σ(
p
2 log L′ + t)) ≤e−t2/2
(85)
P(gβ ≤
p
d/B −σt) ≤e−t2/2.
(86)
Combining these three, we find that, with probability 1−2e−t2/2, whenever √d/B ≥σ(
p
2 log L′+2t),
we have that
gβ > gmax = max
i,b gi.
This condition is implied by d ≥σ2B(
p
2 log ¯L+2t)2. Applying change of variable on t, we conclude
with the result.
Retrieving the value token. Once the correct block is identified, (query, value) pair is retrieved by
applying full softmax attention with W = cI with c →∞within the selected two blocks. Recall that
local attention retrieves the query and the choice of convolutional filter will return the value ahead of
the query. To guarantee this, we only need to prove that xL also has the largest correlation to itself
within the two selected blocks we apply local attention. To this aim, we similarly use the fact that
correlations between xL and the other tokens in the selected blocks are IID N(0, σ2/d) variables.
There are at most 2B −2 such other tokens. Consequently, the maximum local correlation gloc
max obeys
P(gloc
max ≥σ(
p
2 log(2B) + t)/
√
d) ≤e−t2/2. We wish to guarantee that gloc
max < 1. This holds with
1 −e−t2/2 probability whenever d ≥σ2(
p
2 log(2B) + t)2. This latter condition is implied by the
original condition because
√
B(
p
2 log ¯L + 2t) ≥
p
2B log 2 + 2t ≥
p
2 log(2B) + t. Union bounding,
we end up with a success probability of at least 1 −3e−t2/4.
Next, we wish to show the converse result. We recall that as the expectation of supremum of K IID
N(0, 1) become (1 ± o(1))
p
2 log K as K grows to infinity. Thus, for sufficiently large ¯L ≥Cε, we
have that E[gmax] ≥
p
(2 −ε) log ¯L. Consequently, we can write the reversed inequalities
P(gmax ≤σ(
q
(2 −ε) log ¯L −t)) ≤e−t2/2
(87)
P(gβ ≥
p
d/B + σt) ≤e−t2/2.
(88)
Combining these, we conclude with the advertised reverse inequality. As a result, with the same
probability, we fail to identify the block containing the target query/value pair.
The next subsection proves the uniform AR guarantees via an application of Slepian’s lemma.
D.2
Proof of Uniform Associative Recall via Slepian’s Lemma (Proposition 1 continued)
Slepian’s Lemma [41] is an important gaussian comparison inequality. A specific variation is the
following result that holds for a random gaussian matrix. We first introduce the Gaussian width
definition that is important for assessing the complexity of a geometric set in a high-dimensional
space.
Definition 7 (Gaussian width). Let S ∈Rd and g ∼N(0, Id). Gaussian width of S is defined as
ω(S ) = E[supx∈S x⊤g]
Proposition 4 (Slepian’s Lemma). Let X ∈Rn×d be a matrix with IID N(0, 1) entries. Let g ∼
N(0, In) and h ∼N(0, Id). Given sets A ∈Rn, B ∈Rd, we have that
E[supa∈A,b∈Ba⊤X⊤b] ≤E[supa∈A,b∈Ba⊤g∥b∥ℓ2 + b⊤h∥a∥ℓ2].
We have the following application of Slepian’s lemma.
Lemma 4. Let X ∈RL×d be a matrix with IID N(0, 1) entries. Let g ∼N(0, Id) be an independent
vector. Fix a subset of unit sphere S ∈Rd. With probability 1 −e−t2/2, we have that
sup
β∈S
∥Xβ∥ℓ∞≤
√
2(
p
log L + ω(S ) + t).
Proof. Define the augmented matrix X′ =
"
X
−g
#
. Define the set A ∈RL+1 such that a ∈A has the
following form. a has two nonzero entries both of which are equal to 1. Additionally, last entry is
30

nonzero i.e. aL+1 = 1. Using ∥a∥ℓ2 =
√
2 and ∥β∥ℓ2 = 1, we now apply Slepian’s lemma as follows
E[supa∈A,β∈S a⊤X⊤β] ≤E[supa∈A,b∈Ba⊤g∥β∥ℓ2 + β⊤h∥a∥ℓ2]
(89)
≤E[∥g∥ℓ∞+
√
2 sup
β∈S
β⊤h]
(90)
≤
√
2(
p
log L + ω(S )).
(91)
To proceed, observe that a⊤X⊤β is a
√
2-Lipschitz function of X. This implies the statement of the
lemma.
□
Proposition 5. Consider the setting of Proposition 1. Suppose we wish to solve AR for the worst
query drawn from a set S which is subset of the unit sphere. If d ≥2σ2B(
p
log ¯L+ω(S )+t)2, (LCAT)
solves AR with probability at least 1 −2e−t2/2 for all xL ∈S .
Proof. The proof follows the steps of Section D.1 with the following distinction. Note that, to
determine the correct block, we now need to do a worst case analysis. Namely, let z′
β = xL −zβ,
z′
i = zi for i , β, and set Z′ =

z′
1...
z ¯L−1
. Also let Z = [z1 . . . zβ−1 zβ+1 . . . z ¯L−1]. The accurate
detection of the block β coincides with the following event
inf
xL∈S ∥ZxL∥ℓ∞−z⊤
β xL > 0.
Using z⊤xL = 1 −z′⊤xL and defining the set A to be the set of all vectors with exactly two 1s with
one of the 1 appearing at position β, the above event can alternatively be written as
sup
a∈A,xL∈S
a⊤Z′xL < 1.
Now applying Lemma 4 on the left hand side, we find that, with probability 1 −e−t2/4,
sup
a∈A,xL∈S
a⊤Z′xL ≤
r
2B
d σ(
p
log L + ω(S ) + t)
Consequently, whenever d > 2σ2B(log L+ω(S )+t)2, we conclude with the result. Note that, when S
is an r-dimensional subspace, we plug in the well-known bound ω(S ) ≤√r. Finally, we need to union
bound this event with the event that the query token can be identified through local attention by letting
Wk = Wq = √cI and c →∞. To do so, we apply Lemma 4 over the 2B−2 non-query tokens. Denoting
these tokens by Xloc ∈Rd×(2B−2), we have that P(Xlocq ≥
p
2σ2/d · (
p
log(2B) + ω(S ) + t)) ≤e−t2/2.
Consequently, Xlocq < q⊤q = 1 as soon as the same condition d ≥2σ2B(
p
log ¯L + ω(S ) + t)2 holds.
This introduces an additional e−t2/4 probability of failure.
□
D.3
Proof of Proposition 2
We essentially follow the proof of Proposition (1). The only differences are that, the variance
calculations, comparison of block correlations, and signal-to-noise ratio bounds will all slightly
change due to exponential smoothing impacting the full context window. To proceed, let us observe
the following scenarios for a block ID 1 ≤i < ¯L:
• Scenario 1: i < β. In this scenario, zi is exponentially-smoothed sum of IID vectors with
N(0, 1) entries. Recalling ρ = e−1/B, the variance σ2
z of entries of zi is upper bounded by
σ2
z =
∞
X
i=0
ρ2i =
σ2
1 −ρ2 ≤1.2B.
(92)
Here, we used the fact that for B = 1, the bound holds and for B ≥2, we have that
ρ2 = e−2/B ≤1 −1
B. The latter implies 1 −ρ2 ≥1/B and 1/(1 −ρ2) ≤B.
The above bound on σ2
z implies that, setting gi = z⊤
i xL · √d/B, we have that gi ∼N(0, σ2
i )
with σ2
i ≤1.2σ2.
31

• Scenario 2: i = β. In this scenario, the variance upper bound σ2
i above is still applicable. The
key is to estimate and lower bound the mean component similar to the proof of Proposition
(1). Let the query token appear in the kth position of block β for k ∈[B]. Define p = (k−1)/B.
Observe that
E[gβ] = E[z⊤
β xL ·
p
d/B] = e−p p
d/B.
• Scenario 3: i > β. This is essentially same as Scenario 2, because, thanks to the exponential
smoothing, the signal token from block β will propagate to future zi’s. The coefficient of the
propagation satisfies
E[gi] = E[z⊤
i xL ·
p
d/B] = e−(p+i−β) p
d/B.
Now that we have gathered these three scenarios, we can define gmax = maxi,β gi −E[gi]) similar
to above. gmax is a supremum of independent Gaussians of bounded variance controlled by (92).
Through this, we have that
P(gmax ≥1.6σ(
q
log ¯L + t)) ≤e−t2
(93)
P(gβ −E[gβ] ≤1.6σt) ≤e−t2.
(94)
Secondly, for i , β, using p ≤1, we have that
E[gβ] −E[gi] ≥(e−p −e−(p+i−β))
p
d/B ≥(e−1 −e−2)
p
d/B ≥0.23
p
d/B.
Consequently, we require 0.23 √d/B > 1.6σ(
p
log ¯L + 2t). Using τ = t/2, this is guaranteed by
d ≥50Bσ2(
p
log ¯L + τ)2 with probability at least 1 −2e−τ2/4. Once the correct block is identified,
(query, value) pair is retrieved by applying dense softmax attention with W = cI with c →∞over the
selected blocks thanks to the choice of convolutional filter. This argument is identical to “Retrieving
the value token” proof in Section D.1 and introduces an additional e−τ2/2 probability of failure in the
union bound.
32

