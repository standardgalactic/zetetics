This is the first in a series of blog posts on either new or important ideas in multi-agent
learning. There really isn't much out there trying to put some of these ideas into easily
understood prose and consequently a lot of key innovations are locked up in fairly
inaccessible papers. My goal here is to make it easier to catch up on the field and to
highlight some key advances that have been happening recently.
TLDR:
If we're coordinating with an unknown agent, it's useful to treat it as though it
doesn't know any conventions and instead is just interpreting only grounded
information it observes.
Off-belief learning gives us a way to generate agents that are grounded / convention-
free and assume that the agents they are playing with are convention-free as well.
More generally, off-belief learning lets us build agents that assume their
observations are coming from playing with an arbitrary agent of our choosing and
that their actions in turn will be interpreted by an arbitrary other agent.
We're starting with what I think is one of the biggest innovations to come out in the last
year or two: Off-Belief Learning (OBL). OBL is kind of like the blockchain in that a lot of
questions I ask are often answered with "pretty sure off-belief learning solves this" but
it's different than the blockchain because it's obviously useful. It's a clever solution to a
very ubiquitous problem in multi-agent learning.
First, the problem OBL is looking to tackle is the problem of improving zero-shot
coordination by removing arbitrary convention formation from multi-agent self-play (I'll
define what each of these pieces mean, don't worry). First, arbitrary convention
formation. There's a nice example of this in the paper:
Introduction

Here Alice is trying to communicate whether she has a cat or a dog to Bob who will try to
guess which she has. They both get +10 points if he gets it right and -10 to both if he gets
it wrong. She can also pay 5 points to remove the wall, clearly exposing which pet she
has or flip a light bulb to its on or off state. Finally, Alice can choose to exit from the game
for a reward of 0.5 and Bob can exit the game for a reward of 1 instead of guessing. If we
run self-play (i.e. Alice and Bob play this game together hundreds of times), Alice and
Bob will learn a convention like "light-bulb on means dog, light-bulb off means cat" and
will use the light switch instead of removing the wall.
That's all fine and good for Alice and Bob but what if we suddenly swap out Bob with
another player, Candace? Candace has no idea about the convention; when Alice flicks
the lighbulb Candace will think "I have no idea what that means. I'm going to exit the
game for a reward of 1." It's a fine strategy for playing with Bob, but it's not a good
strategy for playing with an arbitrary, new player. This is the challenge of zero-shot
coordination: how can we construct a strategy that should perform well no matter who
our partner is?
Note, this isn't a super well-defined problem, in principle no such strategy exists.
For example, even if Alice were to remove the wall, it's possible that Bob could look at the
revealed dog and exclaim "Cat!" Under the pessimal assumption that Bob is unable to
distinguish cats from dogs, the optimal thing for Alice to do is to simply exit the game.
We need to make some assumptions for coordination to be possible at all. One nice
assumption that you could make on a partner is that Bob is an optimal grounded agent
(OGA): an agent that acts optimally based on everything it observes but does not assume

that observed actions are indicative of any conventions (this is a loose definition, we'll
give a more technical one later). If Bob is an OGA agent and Alice flips the switch, Bob
will simply exit the game because they don't know what the switch means. On the other
hand, if Alice opens the wall, Bob will immediately guess the right pet.
In other words, an OGA is an agent that acts only based on its observations of the world,
but ignores the actions of its partner except to the extent that they reveal information
about the world i.e. optimally grounded Bob ignores the light switch but pays attention
to the pets when they're revealed.
Okay, so how do we construct a self-play process that returns grounded agents instead of
agents with arbitrary conventions? One of the key insights of the OBL paper is defining
such an agent to be one that acts as though "all prior actions came from a random agent
and that all my actions will be interpreted by an agent that believes all actions came
from a random agent." Lets unpack that a bit because it's deeply unintuitive. If we have
such an agent, then looking at the history of actions we have seen so far, we can't try to
read conventions out of them since we believe they came from a random agent. However,
for our action we want to signal as much information as possible to our partner since we
believe that our partner is a copy of ourselves and will extract as much information as
possible from the actions without assuming any conventions.
Lets filter this through the lens of Alice and Bob. For any action Alice takes, she knows
that Bob will believe her action was taken randomly. If she flips the light switch, he
won't know what to do with that information and will exit the game. If she opens the
door, it doesn't matter to Bob that the door opening occured randomly, he knows what
the pet is and will guess it right.
Lets move up one layer of abstraction and try to define this in technical terms. We're
going to be slightly more general for a second and then restrict things to refer back to our
OGA.
First, some notation. First, we're going to assume for simplicity that we're playing a turn-
based game that has some max horizon 
. We're also going to assume that all actions are
observable by all agents. None of these assumptions are strictly necessary, but they make
the exposition a lot easier.
We're working with partially observed systems here so instead of the MDP state  the
agents are going to operate on action observation histories (AOH) 
where 
 is the agents partial observation of the true state 
 at time-step . As an
More intuition on optimal grounded agents
Building an optimal grounded agent
H
s
τ i = (o1, a1, … , at−1, ot)
oj
st
j

example, in poker the true world state  would be the concatenation of all the players
cards and the agent observation for agent  , 
 would be all the cards visible to agent .
We'll also have the fully-observed historical trajectory 
. Finally,
we'll use 
 to represent the policy of agent  and  to denote the joint policy of all the
agents. Since we'll want to distinguish between different joint policies, we will also use
subscripts like 
 or 
 to distinguish between different joint policies.
First, lets define the notion of counterfactual value. This game is partially observed so
based on the trajectory agent  has seen so far, 
 it's going to have some belief over the
true trajectory . We denote this as 
) i.e. the probability of being in
trajectory  given that we've seen trajectory  and we're playing joint policy . Note the
conditioning on joint policy , the belief assumes we know who we're playing with.
The counterfactual value is defined as
Similarly, we can define a counterfactual Q-value, the value of seeing an observation and
taking a particular action as
We can then define the OBL operator which takes in an initial policy 
 and converts it
into a new policy 
 via:
The OBL operator just returns the softmax of the counterfactual Q values with a
temperature T.
Key takeaway / theorem: As 
, the OBL operator returns the optimal grounded
policy as long as 
 is a policy that does not condition on the actions.
The counterfactual value is the expected value of trajectory 
 if we've been playing with 
 up till now and are going to be playing with 
 afterwards. An agent that optimizes
this quantity is going to interpret the trajectory it sees as coming from 
 but then play
afterwards as though it is playing as part of joint policy 
.
Now, lets connect this back to the OGA. Suppose that 
 is a totally random agent (or any
other convention-free agent i.e. an agent that conditions only on observations but not
s
i oi
i
τ = (s1, a1, … , at−1, st)
πi
i
π
π0
π1
i
τ i
τ
Bπ(τ|τi) = P(τ|τi, π
τ
τi
π
π
V π0→π1(τ i) = Eτ∼Bπ0(τ|τi) [V π1(τ)]
Qπ0→π1(a|τ i) = ∑
τi,τi+1
Bπ0(τt, τ i
t ) [r(st, a) + T (τi+1, τi)V π1(τi+1)]
π0
π1
π1(a|τ i) =
exp (Qπ0→π1(a|τ i)/T)
∑a′ exp (Qπ0→π1(a′|τ i)/T)
T →0
π0
High level intuition on the OBL operator
τ i
π0
π1
π0
π1
π0

the actions of its partner) and we've currently observed trajectory 
. Then, there's no
value in trying to read a convention out of 
, just extract all the information about the
true state of the world you can without assuming any convention. Now, we know as
much as we can but what action should we take? Well, if we assume that in the future
our actions are going to be interpreted as though they're coming from a random agent,
we might as well signal as much information about the true state of the world as we can.
We have our OBL operator, we now need an RL procedure that returns the optimum.
We can perform standard Bellman iteration. If we expand the counterfactual Q-values
slightly, we can write them in a form that's amenable to sampling:
A quick note, the term 
 refers to the fact that our current player might not play
again till  steps later. This is just an expanded Bellman equation for the counterfactual
Q function and we can try to converge to a fixed point by drawing samples from this
distribution and minimizing the difference between the LHS and RHS as we do in Q-
learning. They call this approach Q-OBL.
There's one serious challenge though: drawing samples. Samples from this distribution
correspond to playing 
 up till time , and our current policy 
 thereafter and these two
policies might reach very different state distributions. For example, if 
 is a uniform
random policy, it might reach any interesting states with very low probability.
Here they introduce another trick to let them sample interesting states with higher
probability, a method they call LB-OBL: Learned Belief OBL. We're going to play forwards
under 
 and then at each state we reach, we're going to take the partially observed
trajectory we've seen under 
, 
, and sample a trajectory  that is consistent with 
 but
would have come from a random policy. In practice, this means we're training a belief
model 
 and drawing samples from it (for more info on this, check out the section
on learned belief models). We'll then take this new fictitious sample  and use it to roll
out a fictitious transition along it and compute the counterfactual Bellman update along
that fictitious transition.
τ i
τ i
Actually solving for the OBL policy
Option 1:
Qπ0→π1(a|τ i) = Eτt,τt+k [
t+k−1
∑
t′=t
r(s′
t, a′
t) + ∑
at+k
π1(at+k|τ i
t+k)Qπ0→π1(at+k|τ i
t+k)]
Eτt,τt+k
k
Option 2
π0
t
π1
π0
π1
π1 τ i
τ
τ i
Bπ0(τ|τ i)
τ

The paper provides a fairly helpful diagram of these two approaches with Q-OBL on the
left and LB-OBL on the right.
Really well! You can read the paper for that since the goal here was to get the ideas across.
The idea of using a learned belief model is fairly neat so here's a little section diving into
it more, with a full exposition in the paper Learned Belief Search:
Efficiently Improving Policies in Partially Observable
Sethttps://arxiv.org/pdf/2106.09086.pdftings. For partially observable games, it is
useful to have a persistent estimate of the likely true state of the world. Sometimes, we
can explicitly enumerate this true state of the world but in general it can be fairly high
dimensional. If we train a model that predicts from the current partially observed AOH a
distribution over the true state of the world, we can simply sample from it for all sorts of
tasks; in the linked paper they use it for search.
However, the true state of the world, which combines all private agent observations +
agent actions + public knowledge can be very high dimensional. Here we can apply a
useful decomposition of a game into the set of common knowledge (what everyone
knows) and private knowledge (what only you know). For many games, the set of private
knowledge can be fairly small! For example, in 2-player poker games like Texas Hold-em,
the private knowledge is simply the two cards that the opponent holds. Our belief model
can simply draw samples of the private knowledge and combine this with the public
knowledge to form the complete state of the game.
In short, we are going to draw samples from our game and train a supervised model for
our agents by taking the public observations and using them to predict the private
observations of the agents. Since we're training this model on rollouts, we have the
private observations and can use them as labels.
How well does it work?
Quick aside: learned belief models


