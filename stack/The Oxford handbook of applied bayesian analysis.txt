


























































































































































































































































































































































Analysis of Economic Data
313
multiscale decomposition coupled with the state-space time evolution allow
an extremely efï¬cient divide-and-conquer estimation algorithm. This estimation
algorithm has good granularity and can, therefore, be implemented in a parallel
computing environment. As a consequence, our multiscale spatio-temporal
approach can be made extremely computationally efï¬cient and thus is well
designed for dealing with massive data sets. For example, it is conceptually
feasible to analyse economic data sets comprising county level information for
large countries through several years.
Appendix
A. Broader context and background
A.1 Multiscale models
Multiscale modelling broadly refers to models for data and processes that may
be structured by scale. A broad coverage of existing statistical multiscale models
is given by Ferreira and Lee (2007). Multiscale modelling includes not only
the well-known wavelet multiscale decompositions (Daubechies, 1992; Mallat,
1999; Vidakovic, 1999), but many other models such as for example Gaussian
models on trees (Willsky, 2002), hidden Markov models on trees (Kato et al.,
1996), multiscale random ï¬elds (Chapter 10, Ferreira and Lee, 2007), multi-
scale time series (Ferreira et al., 2006), change of support models (Banerjee
et al., 2003), and implicit computationally linked multiscale models (Higdon
et al., 2002; Holloman et al., 2006). Multiscale models have been successfully
applied to several different scientiï¬c areas such as image segmentation (Nowak,
1999), permeability estimation (Ferreira et al., 2003), agronomy (Banerjee and
Johnson, 2006), single photon emission computed tomography (Holloman
et al., 2006), and disease mapping (Louie and Kolaczyk, 2006).
A.2 Dynamic linear models
Dynamic models, also known as state-space models, have been successfully
used in many areas of science to model processes that evolve through time, as
for example in economics (Azevedo et al., 2006), ï¬nance (Jacquier et al., 2007),
ecology (Wikle, 2003), epidemiology (Knorr-Held and Richardson, 2003), and
climatology (Wikle et al., 2001). The books by West and Harrison (1997) and
Harvey (1989) give thorough coverage of dynamic models from the Bayesian
and frequentist perspectives, respectively. Migon et al. (2005) provide a review
along with recent developments on Bayesian dynamic models.
In this chapter we consider dynamic linear models, which can be written as
yt = Fâ€²
tÃ‹t + Îµt,
Îµt âˆ¼N(0, Vt),
Ã‹t = GtÃ‹tâˆ’1 + Ë˜t,
Ë˜t âˆ¼N(0, Wt),

314
The Oxford Handbook of Applied Bayesian Analysis
where the ï¬rst equation is known as the observation equation and the second
equation is the system equation. At time t, yt is the vector of observations,
Ã‹t is the latent process, Ft relates the observations to the latent process, Gt
describes the evolution of the latent process through time, Vt is the obser-
vational covariance matrix, and Wt is the covariance matrix of the system
equation innovation. Typically, Ft, Gt, Vt and Wt are known up to a few
hyperparameters.
If the matrices Ft, Gt, Vt, and Wt are completely known, then the Kalman
ï¬lter can be used to estimate the latent process. If Ft, Gt, Vt, and/or Wt are
functions of unknown hyperparameters, then estimation can be performed
using Markov chain Monte Carlo (MCMC). See Robert and Casella (2004) and
Gamerman and Lopes (2006) for detailed information on MCMC methods.
In this context, each iteration of the MCMC algorithm is then divided in two
blocks: simulation of the unknown hyperparameters, and simulation of the
latent process. The details of the simulation of the hyperparameters is model-
speciï¬c. If the matrices Vt and Wt are positive deï¬nite, the latent process can
be efï¬ciently simulated using the forward ï¬lter backward sampler (FrÃ¼hwirth-
Schnatter, 1994; Carter and Kohn, 1994).
B. Multiscale decomposition and computations
B.1 Multiscale decomposition
Since the joint distribution of the observations at the ï¬nest level L con-
ditional on the mean process ÃL,1:nL is multivariate Gaussian, it follows
that yl,1:nl|Ãl,1:nl, l âˆ¼N(Ãl,1:nl, l). Moreover, the joint distribution of yl j
and yDl j is

yl j
yDl j
 ÃL,1:nL, Ã›2
L,1:nL âˆ¼N
âŽ¡
âŽ£

Ãl j
ÃDl j

,
âŽ›
âŽÃ›2
l j

Ã›2
Dl j
â€²
Ã›2
Dl j
Dl j
âŽž
âŽ 
âŽ¤
âŽ¦.
Therefore, using standard results from the theory of multivariate normal distri-
butions (Mardia et al., 1979), the conditional distribution of yDl j given yl j is
yDl j
 yl j, ÃL,1:nL, Ã›2
L,1:nL âˆ¼N(ÃŒl j yl j + Ã‹l j, 	l j),
with
ÃŒl j = Ã›2
Dl j /Ã›2
l j,
Ã‹l j = ÃDl j âˆ’ÃŒl jÃl j,
and
	l j = Dl j âˆ’Ã›âˆ’2
l j Ã›2
Dl j

Ã›2
Dl j
â€²
,

Analysis of Economic Data
315
l = 1, . . . , L âˆ’1, j = 1, . . . , nl. Note that the scale speciï¬c parameter Ã‹l j is a
vector corresponding to the difference between the mean level process at the
descendants of subregion Bl j and the scaled mean level process at Bl j.
Next, consider
Ã‹e
l j = yDl j âˆ’ÃŒl j yl j,
which is an empirical estimate of Ã‹l j. Then,
Ã‹e
l j|yl j, ÃL, Ã›2
L âˆ¼N(Ã‹l j, 	l j),
(12.6)
which is a singular Gaussian distribution (Anderson, 1984, p. 33). Straight-
forward linear algebra shows that Ã‹e
l j is subject to the constraint 1â€²
ml j Ã‹e
l j = 0.
Moreover, this constraint is implicitly embedded in the singular Gaussian dis-
tribution in (12.6). In order to see this is true, note that
E

1â€²
ml j Ã‹e
l j

= 1â€²
ml j Ã‹l j = 1â€²
ml j (ÃDl j âˆ’ÃŒl jÃl j) = 0
and
V

1â€²
ml j Ã‹e
l j

= 1â€²
ml j 	l j1ml j = 1â€²
ml j Dl j 1ml j âˆ’Ã›âˆ’2
l j 1â€²
ml j Ã›2
Dl j

Ã›2
Dl j
â€²
1ml j = 0.
B.2 Derivation of full conditional distributions
Full conditional of Ã“k. The full conditional density of Ã“k, k = 1, . . . , n1, is
p

Ã“k|Ã0:T,1k, Ã›2
1k, DT

âˆp(Ã“k|Ã™k, Ãk)
T

t=1
p

Ãt1k|Ãtâˆ’1,1k, Ã›2
1k, Ã“k

âˆÃ“âˆ’0.5(T+Ã™k)âˆ’1
k
exp
6
âˆ’1
2Ã“k

Ãk + Ã›âˆ’2
1k
T

t=1
(Ãt1kâˆ’Ãtâˆ’1,1k)2
7
.
Therefore, Ã“k|Ã0:T,1k, Ã›2
1k, DT âˆ¼IG

0.5Ã™âˆ—
k, 0.5Ãâˆ—
k

, where Ã™âˆ—
k = Ã™k + T and Ãâˆ—
k =
Ãk + Ã›âˆ’2
1k
T
t=1(Ãt1k âˆ’Ãtâˆ’1,1k)2.
Full conditional of Â¯lj. The full conditional density of Â¯l j,l = 1, . . . , L, j = 1, . . . , nl,
is
p(Â¯l j|Ã‹0:T,l j, Ã›2, DT) âˆp(Â¯l j|Ã™, Ã)
T

t=1
p(Ã‹tl j|Ã‹tâˆ’1,l j, Â¯l j, Ã›2, Dtâˆ’1)
âˆÂ¯
âˆ’1
2(T(ml j âˆ’1)+Ã™)âˆ’1
l j
Ã— exp
6
âˆ’1
2Â¯l j

Ãl j +
T

t=1

Ã‹tl j âˆ’Ã‹tâˆ’1,l j
â€²	âˆ’
l j

Ã‹tl j âˆ’Ã‹tâˆ’1,l j

7
.
Therefore Â¯l j|Ã‹0:T,l j, Ã›2, DT âˆ¼IG

0.5Ã™âˆ—
l j, 0.5Ãâˆ—
l j

, where Ã™âˆ—
l j = T(ml j âˆ’1) + Ã™
and Ãâˆ—
l j = Ã + T
t=1(Ã‹tl j âˆ’Ã‹tâˆ’1,l j)â€²	âˆ’
l j(Ã‹tl j âˆ’Ã‹tâˆ’1,l j).

316
The Oxford Handbook of Applied Bayesian Analysis
B.3 Singular forward ï¬lter backward sampler
From the multiscale decomposition of the observation equation, we have that
Ã‹e
tl j = Ã‹tl j + vtl j,
vtl j âˆ¼N(0, 	l j).
Moreover, from the system equations,
Ã‹tl j = Ã‹tâˆ’1,l j + Ë˜tl j,
Ë˜tl j âˆ¼N(0, Â¯l j	l j).
Then the singular forward ï¬lter backward sampler proceeds as follows.
1. Use the Kalman ï¬lter to obtain the mean and covariance matrix of
p(Ã‹1l j|Ã›2, Â¯l j, D1), . . . , p(Ã‹Tl j|Ã›2, Â¯l j, DT):
r posterior at t âˆ’1: Ã‹tâˆ’1,l j|Dtâˆ’1 âˆ¼N

mtâˆ’1,l j, Ctâˆ’1,l j	l j

;
r prior at t: Ã‹tl j|Dtâˆ’1 âˆ¼N

atl j, Rtl j	l j

, where atl j = mtâˆ’1,l j and Rtl j =
Ctâˆ’1,l j + Â¯l j;
r posterior at t: Ã‹tl j|Dt âˆ¼N

mtl j, Ctl j	l j

, where Ctl j =

1 + Râˆ’1
tl j
âˆ’1
and
mtl j = Ctl j

Ã‹e
tl j + Râˆ’1
tl j atl j

.
2. Simulate Ã‹Tl j from Ã‹Tl j|Ã›2, Â¯l j, DT âˆ¼N(mTl j, CTl j	l j).
3. Recursively simulate Ã‹tl j, t = T âˆ’1, . . . , 0, from
Ã‹tl j|Ã‹(t+1):T,l j, DT â‰¡Ã‹tl j|Ã‹t+1,l j, Dt âˆ¼N(htl j, Htl j	l j),
where Htl j =

Câˆ’1
tl j + Â¯âˆ’1
l j
âˆ’1
and htl j = Htl j

Câˆ’1
tl j mtl j + Â¯âˆ’1
l j Ã‹t+1,l j

.
Acknowledgments
Marco A. R. Ferreira was partially supported by CNPq grant 479093/2004-0. Part
of this work was performed while Adelmo I. Bertolde was a graduate student
in the Statistics Program at the Federal University of Rio de Janeiro with a
scholarship from CAPES, Brazil.
References
Anderson, T. W. (1984). An Introduction to Multivariate Statistical Analysis (2nd edn). John Wiley,
New York.
Azevedo, J. V., Koopman, S. J. and Rua, A. (2006). Tracking the business cycle of the euro area:
A multivariate model-based bandpass ï¬lter. Journal of Business and Economic Statistics, 24,
278â€“290.
Banerjee, S., Carlin, B. P. and Gelfand, A. E. (2003). Hierarchical Modeling and Analysis for
Spatial Data. Chapman & Hall, Boca Raton, Florida.

Analysis of Economic Data
317
Banerjee, S. and Johnson, G. A. (2006). Coregionalized single- and multi-resolution
spatially-varying growth curve modelling with application to weed growth. Biometrics, 61,
617â€“625.
Basseville, M., Benveniste, A. and Willsky, A. S. (1992a). Multiscale autoregressive processes,
part I: Schur-Levinson parametrizations. IEEE Transactions on Signal Processing, 40,
1915â€“1934.
Basseville, M., Benveniste, A. and Willsky, A. S. (1992b). Multiscale autoregressive processes,
part II: Lattice structures for whitening and modeling. IEEE Transactions on Signal Process-
ing, 40, 1935â€“1953.
Berliner, L. M., Wikle, C. K. and Milliff, R. F. (1999). Multiresolution wavelet analyses in
hierarchical Bayesian turbulence models. In Bayesian Inference in Wavelet Based Models, (ed.
P. MÃ¼ller and B. Vidakovic), pp. 341â€“359. Springer-Verlag, New York.
Carlin, B. P. and Louis, T. A. (2000). Bayes and Empirical Bayes Methods for Data Analysis, (2nd
edn.) Chapman Hall / CRC. Boca Raton, Florida.
Carter, C. K. and Kohn, R. (1994). On Gibbs sampling for state space models. Biometrika, 81(3),
541â€“553.
Daubechies, I. (1992). Ten Lectures on Wavelets. SIAM: Philadelphia, PA.
Donoho, D. L. and Johnstone, I. M. (1994). Ideal spatial adaptation via wavelet shrinkage.
Biometrika, 81, 425â€“455.
Ferreira, M. A. R., West, M., Bi, Z., Lee, H. and Higdon, D. (2003). Multi-scale modelling of
1-D permeability ï¬elds. In Bayesian Statistics 7, (ed. J. Bernardo, M. J. Bayarri, J. O. Berger,
A. P. Dawid, D. Heckerman, A. F. M. Smith, and M. West), pp. 519â€“527. Oxford University
Press, Oxford.
Ferreira, M. A. R. and Lee, H. K. H. (2007). Multiscale Modeling: A Bayesian Perspective. Springer
Series in Statistics. Springer, New York.
Ferreira, M. A. R., West, M. Lee, H. K. H. and Higdon, D. (2006). Multi-scale and hidden
resolution time series models. Bayesian Analysis, 1, 947â€“968.
FrÃ¼hwirth-Schnatter, S. (1994). Data augmentation and dynamic linear models. Journal of Time
Series Analysis, 15, 183â€“202.
Gamerman, D. and Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Simulation for
Bayesian Inference, (2nd edn.) Chapman Hall/CRC, Boca Raton, Florida.
Gelfand, A. E. and Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal
densities. Journal of the American Statistical Association, 85, 398â€“409.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6,
721â€“741.
Harvey, A. C. (1989). Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge
University Press, Cambridge.
Higdon, D., Lee, H. and Bi, Z. (2002). A Bayesian approach to characterizing uncertainty
in inverse problems using coarse and ï¬ne scale information. IEEE Transactions on Signal
Processing, 50, 389â€“399.
Holloman, C. H., Lee, H. K. H. and Higdon, D. M. (2006). Multi-resolution genetic algo-
rithms and Markov chain Monte Carlo. Journal of Computational and Graphical Statistics, 15,
861â€“879.
Jacquier, E., Johannes, M. and Polson, N. (2007). MCMC maximum likelihood for latent space
models. Journal of Econometrics, 137, 615â€“640.
Johannesson, G., Cressie, N. and Huang, H. (2007). Dynamic multi-resolution spatial models.
Environmental and Ecological Statistics, 14, 5â€“25.

318
The Oxford Handbook of Applied Bayesian Analysis
Kato, Z., Berthod, M. and Zerubia, J. (1996). A hierarchical markov random ï¬eld model and
multi-temperature annealing for parallel image classiï¬cation. Graphical Models and Image
Processing, 58, 18â€“37.
Knorr-Held, L. and Richardson, S. (2003). A hierarchical model for space-time surveillance data
on meningococcal disease incidence. Applied Statistics, 52, 169â€“183.
Kolaczyk, E. D. and Huang, H. (2001). Multiscale statistical models for hierarchical spatial
aggregation. Geographical Analysis, 33, 95â€“118.
Louie, M. M. and Kolaczyk, E. D. (2006). A multiscale method for disease mapping in spatial
epidemiology. Statistics in Medicine, 25, 1287â€“1308.
Mallat, S. G. (1999). A Wavelet Tour of Signal Processing, (2nd edn). Academic Press: San Diego.
Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979). Multivarite Analysis. Academic Press: San
Diego.
Migon, H. S., Gamerman, D., Lopes, H. F. and Ferreira, M. A. R. (2005). Bayesian dynamic
models. In Handbook of Statistics, Volume 25, (ed. D. Dey and C. R. Rao), pp. 553â€“588.
Elsevier, Amsterdam.
Nowak, R. D. (1999). Multiscale hidden Markov models for Bayesian image analysis. In
Bayesian Inference in Wavelet Based Models, (ed. P. MÃ¼ller and B. Vidakovic), pp. 243â€“265.
Springer-Verlag, New York.
Robert, C. P. and Casella, G. (2004). Monte Carlo Statistical Methods, (2nd edn.) Springer-Verlag,
New York.
Sweldens, W. (1996). The lifting scheme: a custom-design construction of biorthogonal
wavelets. Applied and Computational Harmonic Analysis, 3, 186â€“200.
Vidakovic, B. (1999). Statistical Modeling by Wavelets. John Wiley, New York.
West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models (2nd ed.). Springer-
Verlag, New York.
Wikle, C. K. (2003). A kernel-based spectral approach for spatiotemporal dynamic models.
Technical report, Department of Statistics, University of Missouri.
Wikle, C. K., Milliff, R. F., Nychka, D. and Berliner, L. M. (2001). Spatio-temporal hierarchical
Bayesian modeling: Tropical ocean surface winds. Journal of the American Statistical Associa-
tion, 96, 382â€“397.
Willsky, A. S. (2002). Multiresolution Markov models for signal and image processing. Proceed-
ings of the IEEE, 90, 1396â€“1458.

Â·13Â·
Extracting S&P500 and NASDAQ volatility:
The credit crisis of 2007â€“2008
Hedibert F. Lopes and Nicholas G. Polson
13.1 Introduction
Volatility and volatility dynamics are important for understanding the behaviour
of ï¬nancial markets and market pricing. In this chapter we estimate volatility
and volatility dynamics for daily data for the year 2007 for three market indicies:
the Standard and Poorâ€™s S&P500, the NASDAQ NDX100 and the ï¬nancial
equity index called XLF. We study how the three series reï¬‚ect the beginning
of the credit crisis; the XLF index reï¬‚ects the effect on ï¬nancial companies
that were among the earliest to be affected while the other two market indices
reï¬‚ect broader, economy wide effects. Three models of ï¬nancial time series are
estimated: a model with stochastic volatility, a model with stochastic volatility
that also incorporates jumps in volatility and a Garch model. We compare our
volatility estimates from these three models with subjective implied market
volatility calculated from option prices, the VIX and VXN, for the S&P500,
the NASDAQ NDX100, respectively. By sequentially computing marginal like-
lihoods or Bayes factors we can also evaluate the ability of the different models
to capture the time series behavior over this turbulent period.
Volatility and volatility dynamics are central to many issues in ï¬nancial
markets including derivative prices, leverage ratios, credit spreads, and portfolio
decisions. In times of low market volatility it is relatively straightforward to
measure volatility and understand volatility dynamics. At other times, ï¬nan-
cial markets are affected by severe disruptions which may be largely isolated
events like the market crash of 1987 or may be a series of events such as the
Russian default and Long Term Capital Management Fund Crisis of 1998, or
the current credit crisis. During such periods, apparent spikes in volatility and
large movements in asset prices complicate estimation of volatility and volatility
dynamics. This chapter describes how Bayesian particle ï¬ltering methods can
track volatility in an online fashion under these circumstances. Our particle
ï¬ltering methodology presented here provides sequential inference for ï¬nancial
time series in general and can also be applied to other problems such as
longitudinal time series studies.

320
The Oxford Handbook of Applied Bayesian Analysis
Sequential Bayesian methods based on particle ï¬ltering provide a nat-
ural solution to estimating volatility and volatility dynamics. Sequential infer-
ence is important as it provides estimates of current (spot) volatility and
parameters for the evolution of the volatility dynamics given the currently
available information. This includes the historical path of prices or returns
available up until the current time together with market beliefs about volatil-
ity dynamics implicit in option prices. These subjective market beliefs are
measured by the so-called implied option volatilities and can be compared
to model-based estimated volatility after accounting for the market price of
volatility risk. We describe this further at the end of this section and in the
appendicies.
Our primary ï¬ltering results are from two ï¬nancial asset price models, one
with stochastic volatility (SV) and one with stochastic volatility and jumps in
volatility (SVJ). Jumps are transient by their nature and the resulting price and
volatility dynamics can be very different depending upon whether jumps are
included or not. In our estimation, we will identify the posterior distribution
of the latent state variables at each point in time, denoted by Lt = (Vt, Jt, Zt)
corresponding to the stochastic variance, jump time and the jump size. Our
approach will also examine how quickly volatility estimation procedures react
to changes in underlying asset returns.
We ï¬nd a number of important empirical ï¬nance results. First, we compare
our ï¬ltering results from the SV model and SVJ model with a Garch(1,1)
model frequently applied to ï¬nancial time series. We ï¬nd that in periods of
market turbulence that the Garch(1,1) model does not track option implied
volatility as as well the two stochastic volatility models particularly for the
more volatile NDX100 index. These facts also lead to differences in market
prices of volatility risk between stochastic volatility models and Garch(1,1)
throughout the period under investigation. Second, we examine the reaction
of volatility estimates in periods of market stress and extreme movements
can be surprisingly different even though SVJ is designed to capture jump
effects. An attractive feature of the particle ï¬ltering approach is that we have
a model diagnostic in the form of a sequential Bayes factor. We ï¬nd that
while the SVJ model outperforms the SV model for the entire period under
investigation, the relative superiority of the SVJ model increases with market
turbulence.
The methods used here build upon a number of recent papers that develop
particle ï¬ltering algorithms for SV and SVJ models. Carvalho and Lopes (2007)
develop sequential particle methods for Markov switching SV models and
Carvalho et al. (2008) provide a general particle learning algorithm for SV
models. Johannes et al. (2008) consider a general class of continuous-time asset
price diffusion models and continuous-time stochastic volatility jump mod-
els. Particle ï¬ltering has an advantage over commonly used MCMC methods

Extracting S&P500 and NASDAQ volatility
321
(Jacquier et al., 1994, 2004, Johannes and Polson, 2009) which are computation-
ally burdensome and inherently non-sequential.
The rest of the paper proceeds as follows. We begin by describing the
applied problem and goal of the Bayesian analysis. Section 13.2 presents the
estimation models. Section 13.3 discusses the simulation-based methodology
for inference, including MCMC and particle ï¬ltering methods for ï¬ltering
and parameter learning. Detailed analysis of our empirical ï¬ndings appear in
Section 13.4. Finally, Section 13.5 concludes.
13.1.1 Problem context and goals
Subjective beliefs about volatility are available from option prices in the form
of implied volatilities. Model-based volatility estimates can be determined in
an online fashion using historical return data and a speciï¬cation for volatil-
ity dynmaics. Our goal then is to extract volatility estimates sequentially
and compare them with market-based volatility measures. Our three under-
lying equity time series are daily data for the S&P500, the NDX100 and
the XLF. We use two common market volatility indicies, the VIX and VXN
indicies corresponding to the option implied volatilities of the benchmark
indicies.
We begin by studying the volatility of the ï¬nancial stock index, XLF.
Figure 13.1 plots the XLF together with realized volatility estimates based on
a simple rolling window of past returns. At the end of February, there are is a
volatility spike, when the ï¬rst credit crisis indicators in the form of sub-prime
mortgage issues and collateral debt obligations (CDOs) came to light. The XLF
stock index fell from 36 to 31 in the week at the end of February. The volatility
spike on February 27th, 2007 occurred when the Dow Jones index dropped
320 points. This followed a fall of over 10% in the Chinese stock market. Our
estimate of volatility, âˆšVt, of the XLF index moved from around 10% to nearly
30% very quickly in a matter of days.
In the next few months prices stabilized an volatility mean-reverted decaying
back to the 20% range. The most dramatic change in volatility dynamics occurs
just before the beginning of August and persists throughout the rest of the
year. For example, the volatility spikes higher again in October back to the 40%
range with the XLF dropping further from 36 at the beginning of October to
30 in December. We now describe theoretically and empirically the relationship
between option prices and volatility.
13.1.2 Option prices and volatility
To study the relationship between option prices and volatility more closely we
observe that a ï¬nancial model describes asset price dynamics for the physical
or objective measure P whereas derivatives are priced under Q the risk-neutral

322
The Oxford Handbook of Applied Bayesian Analysis
XLF
28
30
32
34
36
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
Vt
10
20
30
40
50
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
Fig. 13.1 Year 2007: XLF underlying price (top) and rolling window annualized standard deviations
(bottom). Volatility increases just before August 2007.
dynamics. These two different probability distributions underpin ï¬nancial
market pricing. In the next section we describe clearly the stochastic volatility
dynamics describing these two distributions. Historical returns can be used to
estimate the dynamics of the physical dynamics P and option returns can be
used to assess the risk neutral dynamics Q. Both returns and option prices will
have information concerning the latent stochastic variance Vt. This is because,
derivative prices depend crucially on the current volatility state, Vt, through a
pricing formula
C (St, Vt) = eâˆ’r(Tâˆ’t)E Q
t [max (ST âˆ’K, 0) | St, Vt],
where St is the current value of the equity index and Vt is the current (spot)
volatility. Appendix B provides explicit formulas for option pricing with stochas-
tic volatility. This is related to the implied volatilities, VIX and VXN indices.
The VIX index is computed from option prices and it also serves as a basis for
volatility swaps which are a tradable asset.

Extracting S&P500 and NASDAQ volatility
323
We term these indices option market implied volatility denoted by IVt. For-
mally, they are related to the expected future cumulative volatility via
IVt = E Q
t
8 t+Ã™
t
Vsds
9
.
If we include jumps, then
IVJ
t = E Q
t
8 t+Ã™
t
Vsds
9
+ varQ
t
8Ns
t+Ã™
n=Ns
t +1Zs
n
9
,
where Vs denotes the path of volatility and Nt and Zt denote the number
of jumps and the jump sizes, respectively. As expected, the option implied
volatility is providing a market-based prediction of future cumulative volatility.
Most approaches for estimating volatility and jumps rely exclusively on
returns, ignoring the information embedded in option prices. In principle,
options are a rich source of information regarding volatility, which explains
the common use of Blackâ€“Scholes implied volatility as a volatility proxy for
practitioners. In contrast to this is the common academic approach of only
using returns to estimate volatility.
Options are highly informative about volatility, but the information content
is dependent on the model speciï¬cation and P and Q-measure parameter
estimates. Misspeciï¬cation or poorly estimated risk premia could results in
directionally biased estimates of Vt using option prices. This informativeness
of index options is mitigated by the fact that these options contain relatively
large bid-ask spreads, as noted by Bakshi et al. (1997) or George and Longstaff
(1993). This implies that while option prices are informative, they may be quite
noisy as a measure of volatility. There is also empirical evidence for jumps in
returns (Bates, 2000; Bates, 2006; Bakshi et al., 1997; and Eraker, 2004) as well
as volatility (see Eraker et al., 2003).
Figure 13.2 compares the ï¬ltered XLF volatilities with the option implied
volatilities of the broader indices. Not surprisingly, the volatilities are better
tracked by the VIX than for the VXN. This is primarily due to the fact that
ï¬nancial companies comprised 25% of the SP500 index at the time. Even
though the ï¬nancial weighting in the NASDAQ is lower, it is surprising that
the VXN volatility also tracks the movements in the XLF albeit not as well. This
shows the inï¬‚uence of contagion effects of the credit crisis even to a sector
of the market that has very little ï¬nancial leverage. Implied volatility versus
a 10-day moving average tracks very well, hence conï¬rming the notion that
implied volatility has information about future average volatility.
Over our data period the move in XLF prices was mirrored by the changes in
the market option volatility indices (VIX and VXN). The sharp moves in market

324
The Oxford Handbook of Applied Bayesian Analysis
10
20
30
40
17â€“Jan
27â€“Mar
5â€“May
14â€“Aug
22â€“Oct
31â€“Dec
XFL vol
VIX
10
20
30
40
17â€“Jan
27â€“Mar
5â€“May
14â€“Aug
22â€“Oct
31â€“Dec
XFL vol
VXN
Fig. 13.2 Year 2007: XLF volatility versus VIX and VXN.
option implied volatility are hard to fully capture with a pure diffusive SV model
even though ï¬ltered volatility estimates move quicker than smooth estimates
which incorporate future and past returns. Hence we will also study a stochastic
volatility model that allows for jumps. However, we also show that these moves
are even harder to capture with a deterministic time-varying volatility model
like Garch (Rosenberg, 1972, Engle, 1982). We will see later than these types of
return movements provide large statistical evidence in favor of the stochastic
volatility jump model. For the next three months volatility mean reverts back
to around the 10% level before the next shock hits at the end of July. In the
next section we describe more clearly our methodology and comparison with
the SP500 and NDX100 stock indices.
In principle, option prices also allow us to extract information about the
current volatility state although this is not the focus of the study here. It is
important to realise that in going from risk-neutral to physical measures we
can assess the market price of volatility risk, see Polson and Stroud (2003),
Eraker (2004) and Todorov (2010) and Appendix B for further discussion. This
is also related to a well-known volatility puzzle where option implied volatility
is usually higher than estimated historical volatility. This is due to the fact
that there exists a positive market price for taking volatility risk. Our particle
methods will be able to measure this quantity.

Extracting S&P500 and NASDAQ volatility
325
13.2 Models
Our empirical analysis will focus on sequential volatility ï¬ltering in the
2007. Parameter estimates will be performed on a longer historical period of
returns from 2002â€“2006. Our particle ï¬ltering approach will then be imple-
mented for the year of 2007. We use daily return data for the Standard
and Poorâ€™s SP500 stock index and the NASDAQ NDX 100 index, denoted
here by SP500 and NASDAQ, respectively. We also study the behaviour of
the XLF which is an equity index for the prices for US ï¬nancial ï¬rms.
The corresponding option volatility indices are the VIX and the VXN; again
these are available on a continuous basis. We now describe the three mod-
els that we use for price dynamics: pure diffusive SV model, SVJ model
and Garch(1,1) model. Then, we describe the different volatility indices and
how they are related to option prices and the market price of volatility
risk.
13.2.1 Stochastic volatility (SV) model
A common model of asset price dynamics results in the following two
equation describing the movements of an equity index St and its stochastic
volatility Vt,
dSt
St
= Ã +
1
VtdBP
t
d log Vt = Ãv(Ã‹v âˆ’log Vt) + Ã›vdBV
t
where Ã is an expected rate of return and the parameters governing the volatility
evolution are  = (Ãv, Ã‹v, Ã›v). The Brownian motions

BP
t , BV
t

are possibly
correlated giving rise to a leverage effect.
This is a pure stochastic volatility (SV) model. The probabilistic evolution
P describes what is known as the physical dynamics as opposed to the risk-
neutral dynamics Q which is used for pricing. To analyse this model in a
sequential fashion it is common to using an Euler discretization of the above
model for continuously compounded returns (see details in Appendix A). In the
subsequent analysis we will use daily time scale.
Let Yt+1 = log (St+1/St) be log-returns and transform volatility to logarithms
as well, Xt+1 = ln Vt+1, then the Euler discretization is
Yt+1 = e
Xt+1
2 Îµt+1
Xt+1 = Â·v + â€šv Xt + Ã›vÃt+1.

326
The Oxford Handbook of Applied Bayesian Analysis
Table 13.1 Stochastic volatility (SV) model. Mean and
StDev are posterior mean and standard deviation, respec-
tively. 2.5% and 97.5% are posterior percentiles. Time
span: 1/02/2002â€“12/29/2006.
Mean
StDev
2.5%
97.5%
SP500
Â·v
âˆ’0.0031
0.0029
âˆ’0.0092
0.0022
â€šv
0.9949
0.0036
0.9868
1.0011
Ã›2
v
0.0076
0.0026
0.0041
0.0144
NASDAQ
Â·v
âˆ’0.0003
0.0023
âˆ’0.0046
0.0045
â€šv
0.9968
0.0024
0.9914
1.0011
Ã›2
v
0.0046
0.0015
0.0024
0.0084
XLF
Â·v
âˆ’0.0020
0.0032
âˆ’0.0082
0.0040
â€šv
0.9924
0.0042
0.9830
0.9996
Ã›2
v
0.0115
0.0036
0.0064
0.0203
Here Îµt+1 and Ãt+1 are normally distributed, serially and contemporaneously
independent shocks. The parameters that govern the evolution of volatility
dynamics are transformed to Â· = ÃvÃ‹v and â€šv = 1 âˆ’Ãv.
The model speciï¬cation is completed with independent prior distributions
for the components of  =

X0, Â·v, â€šv, Ã›2
v

, i.e. N(X0, VX), N(Â·0, VÂ·), N(â€š0, Vâ€š)
and IG

ÃŒ/2, ÃŒÂ¯Â¯Ã›2
v/2

, respectively, where N and IG denote the normal dis-
tribution and the inverse Gamma distribution. Table 13.1 provides posterior
summaries for parameter estimates the stochastic volatility (SV) model for
all three series, SP500, NASDAQ and XLF, for based on data from January
2002 to December 2006. We assumed relatively uninformative priors and set
the hyperparameters in common choices from the literature, i.e. VX = 10,
VÂ· = Vâ€š = 1 and ÃŒ = 3. The hyperparameters Â·0, â€š0 and Â¯Â¯Ã›2
v are simple ordinary
least square estimates based on a 10-day moving window procedure. Setting
these hyperparameters at (0, 0, 1), for instance, produced virtually the same
posterior summaries from Table 13.1. The estimates (Â·0, â€š0) and Â¯Â¯Ã›2
v are used
as initial values for the MCMC algorithm. Other sound initial values also
produced the same posterior results. Finally, the results are similar to the ones
found in Eraker et al. (2003), with all three series exhibiting high persistent
evolution.
13.2.2 Stochastic volatility jump (SVJ) model
The stochastic volatility jump (SVJ) model includes the possibility of jumps
to asset prices. Now an equity index St and its stochastic variance Vt

Extracting S&P500 and NASDAQ volatility
327
jointly solve
dSt
St
= Ã +
1
VtdBP
t + d
âŽ›
âŽ
Nt+1

s=Nt
Zs
âŽž
âŽ 
d log Vt = Ãv(Ã‹v âˆ’log Vt) + Ã›vdBV
t
where the additional term in the equity price evolution describes the jump
process. Since data are observed in discrete time it is again common to
use an Euler discretization of this continuous time process (Appendix A).
Speciï¬cally,
Yt+1 = e
Xt+1
2 Îµt+1 + Jt+1Zt+1
Xt+1 = Â·v + â€šv Xt + Ã›vÃt+1
Jt+1 âˆ¼Ber(ÃŽ)
Zt+1 âˆ¼N

Ãz, Ã›2
z

with Yt+1 = log (St+1/St). The log-returns with Îµt+1 and Ãt+1 normally distributed,
serially and contemporaneously independent shocks. The parameter vector is
 = (ÃŽ, Ãz, Ã›z, Â·v, â€šv, Ã›v).
Prior distributions are required for the initial volatility state, X0, and for all
parameters governing the dynamic of the volatilities

Â·v, â€šv, Ã›2
v

. See Section
13.2.1 for explicit details. For the jump speciï¬cation, we use a conditionally
conjugate prior structure for parameters

ÃŽ, Ãz, Ã›2
z

, i.e. ÃŽ âˆ¼Beta(a, b), Ãz âˆ¼
N(c, d) and Ã›2
z âˆ¼IG

ÃŒ/2, ÃŒÂ¯Â¯Ã›2
z/2

, respectively. We set c = âˆ’3 and d = 0.01 and
a = 2 and b = 100 such that the prior mean and standard deviation of ÃŽ are
around 0.02 and 0.014. ÃŒ and Â¯Â¯Ã›2
z are set at 20 and 0.05, respectively, such
that the prior mean and standard deviation of Ã›2
z are roughly 0.05 and 0.02.
These prior speciï¬cations predict around ï¬ve large negative jumps per year
(roughly 250 business days) whose magnitude are around an additional three
percent.
This structure naturally leads to conditional posterior distributions that
can be easily simulated to form a Gibbs sampler (see Eraker et al., 2003).
Table 13.2 provides posterior summaries for parameter estimates the stochastic
volatility jump (SVJ) model for all three series, SP500, NASDAQ and XLF.
Jump probability estimates are all similar at about 0.04 or 10 jumps per
year. The largest estimates jump sizes are âˆ’3.72% for the XLF, âˆ’2.14% for
the SP500 and âˆ’1.98% for the NASDAQ. The table also provides posterior
means, standard deviations and 5% and 95% quantiles for

Â·v, â€šv, Ã›2
v

and

ÃŽ, Ãz, Ã›2
z

.

328
The Oxford Handbook of Applied Bayesian Analysis
Table 13.2 Stochastic volatility with jump (SVJ) model. Mean
and StDev are posterior mean and standard deviation,
respectively. 2.5% and 97.5% are posterior percentiles. Time
span: 1/02/2002â€“12/29/2006.
Mean
StDev
2.5%
97.5%
SP500
Â·v
âˆ’0.0117
0.0070
âˆ’0.0262
0.0014
â€šv
0.9730
0.0084
0.9551
0.9886
Ã›2
v
0.0432
0.0082
0.0302
0.0613
ÃŽ
0.0025
0.0017
0.0003
0.0066
Ãz
âˆ’2.7254
0.1025
âˆ’2.9273
âˆ’2.5230
Ã›2
z
0.3809
0.2211
0.1445
0.9381
NASDAQ
Â·v
0.0079
0.0065
âˆ’0.0042
0.0215
â€šv
0.9785
0.0073
0.9631
0.9916
Ã›2
v
0.0390
0.0071
0.0275
0.0553
ÃŽ
0.0031
0.0021
0.0004
0.0082
Ãz
âˆ’3.9033
0.1001
âˆ’4.0906
âˆ’3.7054
Ã›2
z
0.6420
0.3856
0.2445
1.6314
XLF
Â·v
âˆ’0.0044
0.0064
âˆ’0.0176
0.0083
â€šv
0.9728
0.0085
0.9554
0.9880
Ã›2
v
0.0472
0.0091
0.0324
0.0677
ÃŽ
0.0026
0.0018
0.0003
0.0071
Ãz
âˆ’3.2676
0.0997
âˆ’3.4593
âˆ’3.0700
Ã›2
z
0.8983
0.5490
0.3467
2.2171
13.2.3 Garch model
The Garch(1,1) model is a time-varying volatility model that uses The evolution
of returns and volatility is then given by
Yt+1 =
1
Vt+1Îµt+1
Vt+1 = Â·v + â€švVt + â€žvÎµ2
t .
This leads to a time-varying volatility sequence given the residuals from the
observation equation. The parameters have the usual constraints: Â·v > 0, â€šv >
0 and â€žv > 0 to ensure positive variance, and Â·v + â€šv + â€žv < 1 for stationar-
ity. This model is fundamentally different from stochastic volatility, since last
periodâ€™s return shock Îµ2
t is used as a regressor as opposed to a stochastic volatil-
ity term Ã›vÃt+1. Other differences include assessments of tail-probabilities and
predictives. MCMC approaches for Bayesian inference on Garch models and
several of its variants appear in MÃ¼ller and Pole (1994), Bauwens and Lubrano
(1998), Vontros and Politis (2000), Wago (2004) and AusÃ­n and Galeano (2007)
amongst many others.
Table 13.3 shows the parameters Â·v, â€šv and â€žv based on the 2002â€“2006
period. We then sequentially apply the model through the time period of 2007.

Extracting S&P500 and NASDAQ volatility
329
Table 13.3 Garch model. Parameter estimates.
Time span: 1/02/2002â€“12/29/2006.
Index
Â·v
â€šv
â€žv
Â·v + â€šv + â€žv
SP500
0.0042
0.9440
0.0501
0.9983
NADSAQ
0.0035
0.9652
0.0319
1.0006
XLF
0.0072
0.9354
0.0573
1.0000
Of particular interest is the comparison in August 2007 at the beginning of
the credit crisis. We will also compare with the implied volatility series to see
what the implications are for market prices of volatility risk. We ï¬nd that the
Garch volatility effect â€žv on the squared residual Îµ2
t is largest for the XLF at
0.057 and smallest for the NASDAQ at 0.03. This is not surprising as the credit
crisis affected leveraged ï¬nance companies as opposed to technology stocks that
traditionally has low debt levels.
13.3 Sequential learning via particle ï¬ltering
Given a time series of observations, Y1:T = (Y1, . . . , YT), the usual estima-
tion problem is to estimate the parameters, , and the unobserved states,
L 1:T, from the observed data. In our case, the latent variables include
(1) the volatility states, (2) the jump times, and (3) the jump sizes. In a
Bayesian setting, this information is summarized by the joint posterior dis-
tribution p (, L1:T|Y1:T). In turn this joint posterior can be used to ï¬nd
estimates of the current state variables and parameters estimates using Ë†Lt =
E(L t|Y1:t) and
Ë†t = E(|Y1:t). Samples from this distribution are usually
obtained via MCMC methods by iteratively sampling from the complete con-
ditional distributions, p (L1:T|, Y1:T) and p (|L1:T, Y1:T). From these sam-
ples, it is straightforward to obtain smoothed estimates of the parameters and
states.
Alternatively, particle ï¬ltering provides sequential estimates from the set of
joint distributions p(L 1:t|Yt:t). Particle ï¬ltering (PF, Gordon et al., 1993) meth-
ods are a simulation-based approach to sequential Bayesian ï¬ltering. Doucet
et al. (2001) provide a reference text for a detailed discussion of the theoretical
properties and applications. We also use a variant of the PF known as the
auxiliary particle ï¬lter (APF, Pitt and Shephard, 1999).
The sequential estimation procedure is implemented in what follows, where
 is assumed known to simplify the presentation. In our application,  is
estimated based on daily data on SP500, NASDAQ and XLF from January
2002 to December 2006. Carvalho et al. (2008) provides more details about the
following resample-propagate scheme including parameter learning.

330
The Oxford Handbook of Applied Bayesian Analysis
13.3.1 Extracting state variables Lt = (Vt, Jt, Zt)
The optimal ï¬ltering problem is solved by the sequential computation of the
set of posteriors p(Vt, Jt, Zt|Y1:t). By Bayes rule these posteriors satisfy the
recursion
p(Vt+1, Jt+1, Zt+1|Y1:t+1) âˆp(Yt+1|Vt+1, Jt+1, Zt+1)p(Vt+1, Jt+1, Zt+1|Y1:t)
where the normal likelihood p(Yt+1|Vt+1, Jt+1, Zt+1) has mean Jt+1Zt+1 and vari-
ance Vt+1, while the prior p(Vt+1, Jt+1, Zt+1|Y1:t) is given by
p(Vt+1, Jt+1, Zt+1|Y1:t) âˆp(Vt+1|Vt)p(Jt+1, Zt+1)p(Vt, Jt, Zt|Y1:t).
The state evolution gives rise to a normal density p(Vt+1|Vt) with mean Â·v + â€švVt
and variance Ã›2
v. Finally, as the jumps as transient and conditionally i.i.d. with
Pr(Jt+1 = 1) = ÃŽ and normal density p(Zt+1|Jt+1 = 1) with mean Ãz and variance
Ã›2
z. It would also be straightforward to allow for the jump probability to depend
on the volatility state through a conditional p(Jt+1 = 1|Vt) (see Johannes et al.,
1998). These equations will form the basis of our particle ï¬ltering algorithm that
we develop in what follows. Following Carvalho et al. (2008) we now describe
our resample-propagate ï¬ltering scheme.
13.3.2 Resample-propagation ï¬lter
Let the current ï¬ltering posterior be denoted by p(Lt|Y1:t). The next likelihood
is p(Yt+1|Lt+1) and the state evolution is p(Lt+1|Lt). Bayes rule links these to the
next ï¬ltering distribution through Kalman updating. This takes the form of a
smoothing step and a prediction step
p(L t|Y1:t+1) âˆp(Yt+1|Lt)p(Lt|Y1:t)
p(L t+1|Y1:t+1) =

p(Lt+1|L t)p(Lt|Y1:t+1)dLt
where Y1:t are the continuously compounded log-returns. Speciï¬cally, for
extracting volatility and jumps as latent states we let Lt = (Vt, Jt, Zt). Uncer-
tainty about these quantities is summarized via the ï¬ltered posterior distribu-
tion p(L t|Y1:t). Particle methods will represent this distribution as
pN(Lt|Y1:t) = 1
N
N

i=1
â€°L(i)
t .
for particles L (1)
t , . . . , L(N)
t
. The previous ï¬ltered distribution is represented by
its particle approximation and the key is how to propagate particle forward.
From the updating formulas we can approximate
pN(Lt+1|Y1:t+1) =
N

i=1
w(i)
t p

Lt+1|L (i)
t , Yt+1


Extracting S&P500 and NASDAQ volatility
331
where weights are given by
w(i)
t
= p

Yt+1|L(i)
t
: N
â„“=1 p

Yt+1|L(â„“)
t

i = 1, . . . , N.
Hence after we have resampled the initial particles with weights proportional to
w(i)
t
we then propagate new particles using p

Lt+1|L(i)
t , Yt+1

. This then leads
us to the following simulation algorithm.
Resample-propagate ï¬lter
1. Resample: For i = 1, ..., N, compute
w(i)
t
= p

Yt+1|L (i)
t
: N
â„“=1 p

yt+1|L(â„“)
t

,
draw
z(i) âˆ¼Mult

N; w(1)
t , . . . , w(N)
t

,
and set L (i)
t
= L z(i)
t
for i = 1, ...N.
2. Propagate: For i = 1, ..., N, draw
L(i)
t+1 âˆ¼p

Lt+1|L(i)
t , Yt+1

.
For the SV and SVJ models, it is common to include Vt+1 into the deï¬ni-
tion of the latent state variable L t. This is due to the nonlinearities that the
volatility induces into the asset price dynamics. In the SVJ model the state
variables describing the jump process (Jt, Zt) can be marginalized out. This
is due to the fact that they are independent of Vt and we have the integral
decomposition
p(Yt+1|Vt+1) =

p(Yt+1|Vt+1, Jt+1, Zt+1)p(Jt+1, Zt+1)dJt+1dZt+1.
Therefore, in practice, we propagate forward the volatility Vt+1 âˆ¼p(Vt+1|Vt) and
then attach it to the current particle before using the resample-propagate ï¬lter
described above.
13.3.3 Sequential model choice
An important by-product of sequential Monte Carlo methods is the ability
to easily compute approximate marginal predictive densities and then Bayes
factors. Let M denote a given model, then the sequential marginal predictive

332
The Oxford Handbook of Applied Bayesian Analysis
for any t can be approximated via
p(Yt+1|Y1:t, M) = 1
N
N

i=1
p

Yt+1|L (i)
t , M

where L (i)
t is the particle for the latent volatility state of model M. This approx-
imation allows one to sequentially compute the Bayes factors (West, 1986),
namely BF1:t, for competing models M0 and M1
BF1:t = p(Y1:t|M1)
p(Y1:t|M0).
The Bayes factor is related to the Bayesian posterior probabilities of the models
being true via the following identity
p(M1|Y1:t)
p(M0|Y1:t) = BF1:t Ã— p(M1)
p(M0)
where p(M1)/p(M0) is typically set equal to one to denote a priori equal weight
on either model. One advantage of this approach is that we can interpret the
relative posterior model probabilities whether or not the â€˜trueâ€™ data generating
process is either on of the models under consideration.
A sequential decomposition of the joint distribution is also available and we
write
p(Y1:t|M) = p(Y1|M)
tâˆ’1

j=1
p(Yj+1|Y1: j, M)
Now we can use the particle approximation in an on-line fashion to compute
the quantities p(Yj+1|Y1: j, M). Hence we can sequentially compute the Bayes
factor for the full data sequence by compounding
BF1:T =
Tâˆ’1

j=1
BF j: j+1.
When competing models (and priors) are nested, which is the case in the SV
versus SVJ case, for example, a further simpliï¬cation is the implementation
of the Savageâ€“Dickey density ratio (Verdinelli and Wasserman, 1995). More
speciï¬cally, let  =

ÃŽ, Ãz, Ã›âˆ’2
z

, i.e. the parameters of the SVJ model (the full
model or M1) that are not in the SV model (the reduced model or M0), then
BF1:t = p( = 0|Y1:t, M1)
p( = 0|M1)
â‰ˆ1
N
N

i=1
p

 = 0|Y1:t, L(i)
1:t, M1

p( = 0|M1)

Extracting S&P500 and NASDAQ volatility
333
where 0 = (0, 0, 0). The computation of p( = 0|Y1:t, L(i)
1:t, M1) is the main
challenge since
p

 = 0|Y1:t, L(i)
1:t, M1

âˆp

Y1:t|L(i)
1:t,  = 0, M1

p

 = 0|L(i)
1:t, M1

.
Hence this provides a computational tractable approach for calculating Bayes
factors from particle ï¬ltering output.
Figure 13.6 below provides the sequential log-Bayes factor BF(SV, SV J ) for
comparing the pure SV model with an SVJ model. We provide the Bayes factor
diagnostic for all three series, SP500, NASDAQ and XLF. Not surprisingly, the
large negative shock in February 2007 leads to substantial evidence in favour of
SVJ over SV. However, for the rest of the year as there are no extreme shocks to
returns the evidence decays back and at the end of the period slightly favours
the pure SV model. Providing these estimates within a pure MCMC frame-
work would be computationally expensive, see, for example, the discussion in
Chapter 7 of Gamerman and Lopes (2006).
13.4 Empirical results
We implement our particle ï¬ltering methodology to ï¬nd volatility estimates for
all the models on a daily basis in 2007. Special focus is on the period at the
onset of the credit crisis, namely August 2007. We compare volatility estimates
and implied prices of volatility risk for each model. Table 13.4 provides this
comparison for the month of August. The volatilities are ï¬ltered and a direct
comparison with the VIX and VXN for each model gives a measurement of the
price of volatility risk for each model. The SVJ model tracks the movements
more closely than the pure SV model. The SVJ model implies a relatively
constant ÃŽv until the end of the month when all model imply a very small ÃŽv. It
is also interesting to see how the sequential Bayes factor discriminates between
these models. see Figure 13.6 and the subsequent discussion.
Periods of high volatility risk premia occur at the end of July for both SV
and SVJ models while lower premia occur for the Garch(1,1) model. A high
premia is empirically justiï¬ed by subsequent increases and volatility spikes in
the August period. Remember that one can interpret the risk premia as market
expectations of future changes in average volatility.
This difference can be explained as follows. In the last week of July the Dow
Jones index dropped from 14, 000 to 13, 000 with a sequence of negative shocks.
The Garch(1,1) model therefore estimates volatility at 23% on August 1st as
negative shocks feed directly into the Garch(1,1) model via â€žÎµ2
t . Both SV and
SVJ models attribute some of their negative shocks to the stochastic volatility
error term of the jump component rather than directly to âˆšVt. To initialize our
estimate of volatility we use 17% for the SV model.

334
The Oxford Handbook of Applied Bayesian Analysis
Table 13.4 SVJ, SV and Garch comparison. Columns 2â€“10 are annualized standard
deviations.
Day
SP&500
NASDAQ
XLF
SVJ
SV
Garch
SVJ
SV
Garch
SVJ
SV
Garch
VIX
VXN
1
18.1
16.5
17.4
18.6
14.9
16.5
25.9
22.6
21.3
23.8
23.6
2
17.0
16.1
17.0
18.3
14.9
16.4
24.2
22.0
20.9
25.1
25.4
3
21.1
18.3
16.5
20.2
15.8
16.2
31.0
26.3
20.3
23.4
24.5
6
23.2
19.5
18.6
20.1
15.8
16.9
37.9
29.9
23.5
22.8
24.5
7
21.9
19.1
19.5
19.3
15.7
16.8
35.4
29.3
26.4
24.6
27.1
8
21.7
19.2
18.9
19.1
15.7
16.6
35.0
29.6
25.6
24.0
26.6
9
25.3
21.1
18.7
22.0
17.0
16.5
37.6
31.4
25.4
26.2
29.2
10
23.6
20.7
20.4
21.3
16.9
17.5
34.7
30.6
26.1
27.4
29.9
13
21.8
20.2
19.7
20.4
16.7
17.3
32.2
29.8
25.3
25.3
27.5
14
22.8
20.7
19.0
21.4
17.2
17.1
31.3
29.5
24.5
25.0
28.0
15
22.7
20.7
19.2
22.7
17.8
17.4
29.1
28.8
24.1
24.8
27.4
16
21.1
20.2
19.0
22.2
17.8
17.8
31.9
30.0
23.4
24.9
27.0
17
23.7
21.3
18.4
23.2
18.3
17.7
36.5
32.6
24.1
26.5
28.7
20
22.0
20.8
19.4
21.8
18.1
18.0
34.4
31.9
25.5
20.4
23.3
21
20.4
20.3
18.7
21.3
18.0
17.6
31.7
30.8
24.9
20.0
22.5
22
19.8
20.3
18.1
21.3
18.1
17.5
29.0
29.7
24.2
20.4
22.0
23
18.4
19.8
17.9
20.1
17.9
17.4
26.5
28.5
23.4
19.0
20.7
24
18.3
19.6
17.3
20.4
18.0
17.1
24.0
27.4
22.6
19.4
20.8
27
17.7
19.3
17.1
19.5
17.8
17.2
23.3
26.8
21.9
18.6
20.8
28
21.1
20.6
16.8
22.8
18.9
16.9
28.1
28.8
21.5
17.6
20.6
29
22.7
21.5
18.3
25.9
20.4
17.9
26.8
28.2
22.4
17.0
20.6
30
21.2
21.0
18.9
24.4
20.0
18.9
24.6
27.0
22.0
18.0
21.0
31
20.7
20.7
18.3
23.8
19.9
18.5
24.0
26.5
21.4
17.8
21.1
Our comparisons are summarized in more detail in Figures 13.3â€“13.5. They
provide sequential comparisons of volatility estimates relative to the VIX and
VXN indices. Speciï¬cally, Figure 13.4 compares the underlying indices S&P500
and NASDAQ with their respective implied volatility series. Figure 13.4 com-
pares Garch(1,1) with SV and SVJ against the option implied series VIX and
VXN.
Figure 13.5 provides a direct comparison between the models at the begin-
ning of the credit crisis in August 2007. For reasons described before, the
Garch(1,1) model does not react as quickly as a stochastic volatility model. It
also starts the month of August at higher estimates of Vt or equivalently lower
estimates of the volatility risk premia. The ï¬ltered Garch(1,1) estimates are also
considerably smoother than the SV and SVJ estimates, this is because the SV
model has the extra ï¬‚exibility in the random variance term to adapt to large
shocks.
Figure 13.5 also compares the three models with their option implied volatil-
ities. This allows the researcher to gauge the movement in the market price
of volatility risk, ÃŽv. It is also useful to look at the sequential Bayes factors

10
15
20
25
30
17â€“Jan
27â€“Mar
5â€“May
14â€“Aug
22â€“Oct
31â€“Dec
SP500 vol
VIX
15
20
25
30
35
17â€“Jan
27â€“Mar
5â€“May
14â€“Aug
22â€“Oct
31â€“Dec
NASDAQ vol
VXN
Fig. 13.3 Year 2007: SP500 and NASDAQ volatilities and VIX and VXN indices.
SVJ
Jan
Apr
Jul
Oct
SP500
VIX
MODEL
SVJ
Jan
Apr
Jul
Oct
NASDAQ
VXN
MODEL
SV
Jan
Apr
Jul
Oct
SV
Jan
Apr
Jul
Oct
GARCH
Jan
Apr
Jul
Oct
GARCH
5
10
15
20
25
30
35
5
10
15
20
25
30
35
5
10
15
20
25
30
35
5
10
15
20
25
30
35
5
10
15
20
25
30
35
5
10
15
20
25
30
35
Jan
Apr
Jul
Oct
Fig. 13.4 Year 2007: Comparing Garch, SV and SVJ models.

336
The Oxford Handbook of Applied Bayesian Analysis
SVJ
15
20
25
30
1
3
7
9
13
15
17
21
23
27
29
31
SP500
VIX
MODEL
SVJ
15
20
25
30
1
3
7
9
13
15
17
21
23
27
29
31
NASDAQ
VXN
MODEL
SV
15
20
25
30
1
3
7
9
13
15
17
21
23
27
29
31
SV
15
20
25
30
1
3
7
9
13
15
17
21
23
27
29
31
GARCH
15
20
25
30
1
3
7
9
13
15
17
21
23
27
29
31
GARCH
15
20
25
30
1
3
7
9
13
15
17
21
23
27
29
31
Fig. 13.5 August 2007: Comparing Garch, SV and SVJ models.
as a measurement of the market price of volatility risk is conditional on the
model. Within the SV model, the average level of risk neutral volatility is
Ã‹â‹†= ÃÃ‹/(Ã + ÃŽv). See Dufï¬e et al. (2000) and Pan (2002) for a discuss with the
SVJ model. By the end of August the initial shock has dissipated and all three
models imply that there is little price of volatility risk.
Figure 13.6 shows that just before August 2007 the Bayes factor favors the
stochastic volatility jump model. Hence we would expect the volatility estimates
from the SVJ model to more closely track the option implied volatility VXN.
This is indeed the case. The market volatility risk premia is effectively constant
for this model over this data period, except at the very end of the period where
the implied option volatility decay quickly and the estimated volatility does not.
This is also coincident with the Bayes factor decaying back in favor of the pure
SV model for the NASDAQ index. By the end of 2007, the odds favor the pure
SV model over the SVJ model for the NASDAQ index.
For the XLF, most of the evidence for jumps is again contained in the
February move. The sequential Bayes factor tends to lie in between the strong
evidence for the SP500 and weaker evidence for the NASDAQ index. The story

Extracting S&P500 and NASDAQ volatility
337
LOG-BAYES FACTOR
â€“2
0
2
4
6
8
10
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
SP500
NASDAQ
XLF
Fig. 13.6 Year 2007: Sequential (log) Bayes factor, BF(M1, M0). M1 â‰¡SVJ model M0 â‰¡SV model.
for the SP500 is different. Figure 13.6 shows that after the February shock, the
SVJ model is preferred to the SV model for the whole period. When comparing
with VIX the jump model seems to track the option implied volatility with an
appropriate market price of volatility risk.
13.5 Conclusions
In this chapter we proposed and implemented sequential particle ï¬ltering
methods to study the US credit crisis which began in 2007. The purpose of
our study was to show how model-based volatility estimates can be compared
to market-based ones in a dynamic setting. Online daily volatility for S&P500,
NDX100 and XLF are estimated for SV, SVJ and Garch(1,1) models. Market-
based volatilities are based on the implied volatility indicies for the S&P500
and NDX100 indices, namely the VIX and VXN. We show that particle ï¬ltering
methods naturally allow for online volatility, jump and sequential model com-
parison.

338
The Oxford Handbook of Applied Bayesian Analysis
We ï¬nd a number of empirical results. First, tracking volatility in turbulent
periods is much harder than low volatility periods. The inclusion of the possi-
bility of jumps can change current estimates of volatility dramatically. The pure
stochastic volatility (SV) and Garch(1,1) models perform signiï¬cantly worse in
periods of market stress, both in terms of tracking subjective market-based
volatility and relative marginal likelihoods versus the SVJ model. Second, by
calculating sequential marginal likelihoods for our data period, we see that
the stochastic volatility jump model is clearly preferred. Not surprisingly, this
evidence accumulates mainly on a few days where the stock returns are most
extreme.
Extensions include multivariate modelling of the return series possibly with
the use of a factor stochastic volatility models as proposed by Aguilar and West
(2000), Lopes (2000) and Lopes and Migon (2002). Lopes and Carvalho (2007)
show how to perform sequential inference for this class of models. More ï¬‚exible
volatility dynamics can also be modeled. Recently, Carvalho et al. (2009) intro-
duced a class of afï¬ne shot-noise continuous-time stochastic volatility model that
accounts for both fast moving, rapidly mean-reverting shot-noise volatility and
slow-moving square-root diffusive volatility.
Appendix
A. Broader context and background
A major advance in ï¬nancial modelling was the development of continuous-
time models for the evolution of stock prices and their volatility. The simplest
model is a geometric Brownian motion Black â€“ Scholes model where stock
prices St solves
dSt = St(Ã dt + Ã› dBt)
for an instantaneous expected return Ã and volatility Ã›. Here Bt is a standard
Brownian motion with distribution Bt âˆ¼N(0, t). A natural extension is the
inclusion of stochastic volatility (SV). Transforming to a log-scale and replacing
the volatility by âˆšVt gives a model of the form
d ln St = Ãdt +
1
VtdBt
where typically Vt solves its own stochastic evolution such as a square-root
Ornsteinâ€“Uhlenbeck process.
In practice, we observe data on a discrete time scale and it is common to use
a time discretisation of these models. The most common discretisation is an
Euler scheme. Speciï¬cally, on a time interval (t, t + ) we have the evolution
ln St+ âˆ’ln St = Ã +
1
Vt
âˆš
Ã‚t

Extracting S&P500 and NASDAQ volatility
339
where Ã‚t âˆ¼N(0, 1). With loss of generality we take  = 1. Moreover, writing the
log-returns Rt+1 = ln(St+1/St) we we now have a dynamic linear models (DLM)
with a hidden state that can be ï¬ltered from the partially observed data.
B. Informational content of returns and option prices
Option prices provide information about state variables and parameters via the
pricing equation. An option price is given by
C (St, Vt, Ã‹) = eâˆ’r(Tâˆ’t)EQ
t [max (St âˆ’K, 0) |St, Xt, ]
where the expectation is taken under Q the risk-neutral probability measure.
here we assume that the risk-free rate r is constant. This simpliï¬es by letting
A = {ST â‰¥K } denote the event that the stock ends in the money. Then the
option price is given by
eâˆ’rtEQ
t (max (ST âˆ’K, 0)) = eâˆ’rtEQ
t (STIA) âˆ’eâˆ’rt KE (IA)
= eâˆ’rtEQ
t (STIA) âˆ’eâˆ’rt KP(A).
A common approach is to use a leverage stochastic volatility model (Heston,
1993). Here the underlying equity price St evolves according to a stochas-
tic volatility model with square-root dynamics for variance Vt, and correlated
errors. The logarithmic asset price and volatility follow an afï¬ne process with
Yt = (log St, Vt) âˆˆâ„œÃ— â„œ+ satisfying
dYt =

r âˆ’1
2Vt

dt +
1
VtdB1,t
dVt = Ã(Ã‹ âˆ’Vt)dt + Ã›v
1
VtdZt
where Zt = Ã’B1,t +
1
1 âˆ’Ã’2B2,t. The correlation, Ã’, or so-called leverage effect
(Black, 1976) is important to explain the empirical fact that volatility increases
faster as equity prices drop. The parameters (Ã, Ã‹) govern the speed of mean
reversion and the long-run mean of volatility and Ã›v measures the volatility of
volatility. Under the risk-neutral measure they become (Ã/Ã + ÃŽv)Ã‹ and Ã + ÃŽv
where ÃŽv is the market price of volatility risk.
From afï¬ne process theory, the discounted transform
Â¯(u) = eâˆ’rtE

euYT|Yt

= eÂ·(t,u)+uYt+â€š(t,u)Vt
where Â·, â€š satisfy Riccati equations. We can then use transform analysis (Dufï¬e
et al., 2000) and compute prices by inverting a fast fourier transform. Speciï¬-
cally, there exists a pair of probabilities P j(Vt, , ), j = 1, 2, such that the call
price C

Y S
t , Vt, , 

is given by:
C

Y S
t , Vt, , 

= StP1(Vt, , ) âˆ’K eâˆ’rÃ™P2(Vt, , ).

340
The Oxford Handbook of Applied Bayesian Analysis
Speciï¬cally, P j(Vt, , ) = Pr j (ln(ST/K )|Vt, , ), where these probabilities
can be determined by inverting a characteristic function
P j(Vt, , ) = 1
2 + 1
ï£¿
 âˆž
0
Re
8eâˆ’iË† ln(K ) f j(Vt, , )
iË†
9
dË†
where
f j(Vt, , ) = eC(Ã™;Ë†)+D(Ã™;Ë†)Vt+iË† ln(St),
and the coefï¬cients C(Ã™; Ë†) and D(Ã™; Ë†) are deï¬ned in Heston (1993).
C(Ã™; Ë†) = rË†iÃ™ + a
Ã›2

(b j âˆ’Ã’Ã›Ë†i + d)Ã™ âˆ’2 log
81 âˆ’gedÃ™
1 âˆ’g
9
,
D(Ã™; Ë†) = b j âˆ’Ã’Ã›Ë†i + d
Ã›2
8 1 âˆ’edÃ™
1 âˆ’gedÃ™
9
,
with parameters
g = b j âˆ’Ã’Ã›Ë†i + d
b j âˆ’Ã’Ã›Ë†i âˆ’d ,
d =
;
(Ã’Ã›Ë†i âˆ’b j)2 âˆ’Ã›2(2u jË†i âˆ’Ë†2).
The correlation and volatility of volatility parameters affect the skewness and
kurtosis of the underlying return distribution and also how the correlation Ã’
affects the probabilities P j(Vt, , ). For example, as is typically the case, when
the correlation is negative then the underlying risk neutral distribution has
a skewed left tail which in turn decreases the price of out-of-the-money call
options while increasing the out-of-the-money put options.
Acknowledgements
We are grateful to the Editors.
References
Aguilar, O. and West, M. (2000). Bayesian dynamic factor models and variance matrix discount-
ing for portfolio allocation. Journal of Business and Economic Statistics, 18, 338â€“357.
AusÃ­n, M. C. and Galeano, P. (2007). Bayesian estimation of the Gaussian mixture Garch
model. Computational Statistics & Data Analysis, 51, 2636â€“2652.
Bakshi, G., Cao, C. and Chen, Z. (1997). Empirical performance of alternative option pricing
models. Journal of Finance, 52, 2003â€“2049.
Bates, D. S. (2000). Post-â€™87 crash fears in S&P500 futures options. Journal of Econometrics, 94,
181â€“238.
Bates, D. S. (2006). Maximum likelihood estimation of latent afï¬ne processes. Review of
Financial Studies, 19, 909â€“965.

Extracting S&P500 and NASDAQ volatility
341
Bauwens, L. and Lubrano, M. (1998). Bayesian inference on Garch models using the Gibbs
sampler. Econometrics Journal, 1, C23â€“C46.
Black, F. (1976). Studies of stock price volatility changes. In 1976 Meetings of the Busi-
ness and Economics Statistics Section, pp. 177â€“81. Proceedings of the American Statistical
Association.
Carvalho, C., Johannes, M., Lopes, H. F. and Polson, N. (2008). Particle learning and smooth-
ing. Working paper, University of Chicago Booth School of Business.
Carvalho, C., Johannes, M., Lopes, H. F. and Polson, N. (2009). Stochastic volatility shot noise.
Working paper, University of Chicago Booth School of Business.
Carvalho, C. and Lopes, H. F. (2007). Simulation-based sequential analysis of markov
switching stochastic volatility models. Computational Statistics & Data Analysis, 51,
4526â€“4542.
Doucet, A., de Freitas, N. and Gordon, N. (2001). Sequential Monte Carlo Methods in Practice,
Springer: New York.
Dufï¬e, D., Pan, J. and Singleton, K. (2000). Transform analysis and asset pricing for afï¬ne
jump-diffusions. Econometrica, 68, 1343â€“1376.
Engle, R. (1982). Autoregressive conditional heteroskedasticity with estimates of the variance
of U.K. inï¬‚ation. Econometrica, 50, 987â€“1008.
Eraker, B. (2004). Do equity prices and volatility jump? reconciling evidence from spot and
option prices. Journal of Finance, 59, 1367â€“1403.
Eraker, B., Johannes, M. and Polson, P. (2003). The impact of jumps in volatility and returns.
Journal of Finance, 59, 227â€“160.
Gamerman, D. and Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Simulation for
Bayesian Inference. Chapman & Hall/CRC, Baton Rouge.
George, T. and Longstaff, F. (1993) Bid-ask spreads and trading activity in index options market.
Journal of Financial and Quantitative Analysis, 28, 381â€“397.
Gordon, N., Salmond, D. and Smith, A. F. M. (1993). Novel approach to nonlinear/non-
Gaussian Bayesian state estimation. IEE Proceedings, F,140, 107â€“113.
Heston, S. (1993). A closed-form solution for options with stochastic volatility with applications
to bond and currency options. Review of Financial Studies, 6, 327â€“343.
Jacquier, E., Polson, N. G. and Rossi, P. E. (1994). Bayesian analysis of stochastic volatility
models. Journal of Business and Economic Statistics, 20, 69â€“87.
Jacquier, E., Polson, N. G. and Rossi, P. E. (2004). Bayesian analysis of stochastic volatility
models with fat-tails and correlated errors. Journal of Econometrics, 122, 185â€“212.
Johannes, M., Kumar, R. and Polson, N. G. (1998). State dependent jump models: How do US
equity indices jump? Working paper, University of Chicago Booth School of Business.
Johannes, M. and Polson, N. (2009). MCMC methods for continuous-time ï¬nancial economet-
rics. In Handbook of Financial Econometrics, Vol. 2 (ed. Y. Ait-Sahalia, L. Hansen), pp. 1â€“72.
Elsevier, Oxford.
Lopes, H. F. (2000). Bayesian analysis in latent factor and longitudinal models. Ph.D. thesis,
Institute of Statistics and Decision Sciences, Duke University.
Lopes, H. F. and Carvalho, C. M. (2007). Factor stochastic volatility with time varying load-
ings and Markov switching regimes. Journal of Statistical Planning and Inference, 137,
3082â€“3091.
Lopes, H. F. and Migon, H. S. (2002). Comovements and contagion in emergent markets: stock
indexes volatilities. In Case Studies in Bayesian Statistics, Vol. 6 (ed. A. Carriquiry, C. Gatsonis
and A. Gelman), pp. 285â€“300. Springer, New York.
MÃ¼ller, P. and Pole, A. (1994). Monte Carlo posterior integration in Garch models. Sankhya,
Series B, 60, 127â€“144.

342
The Oxford Handbook of Applied Bayesian Analysis
Pan, J. (2002). The jump-risk premia implicit in options: evidence from an integrated time-
series study. Journal of Financial Economics, 63, 3â€“50.
Pitt, M. and Shephard, N. (1999). Filtering via simulation: auxiliary particle ï¬lter. Journal of the
American Statistical Association, 94, 590â€“599.
Polson, N. G. and Stroud, J. (2003). Bayesian Statistics 7, Chapter on Bayesian inference for
derivative prices, pp. 641â€“650. Clarendon Press, Oxford.
Rosenberg, B. (1972). The behaviour of random variables with nonstationary variance and
the distribution of security prices. Working Paper 11, Research Programme in Finance,
Graduate School of Business Administration, Institute of Business and Economic Research,
University of California, Berkeley.
Todorov, V. (2010). Variance risk premium dynamics: The role of jumps. Review of Financial
Studies, 23, 345â€“383.
Verdinelli, I. and Wasserman, L. (1995). Computing Bayes factor using a generalization of the
Savage-Dickey density ratio. Journal of the American Statistical Association, 90, 614â€“618.
Vontros, I., D. P. and Politis, D. I. (2000). Full Bayesian inference for Garch and EGarch models.
Journal of Business & Economic Statistics, 18, 187â€“198.
Wago, H. (2004). Bayesian estimation of smooth transition Garch model using Gibbs sampling.
Mathematics and Computers in Simulation, 44, 63â€“78.
West, M. (1986). Bayesian model monitoring. Journal of the Royal Statistical Society (Series
B), 48, 70â€“78.

Â·14Â·
Futures markets, Bayesian forecasting
and risk modelling
JosÃ© M. Quintana, Carlos M. Carvalho,
James Scott and Thomas Costigliola
14.1 Introduction
On the one hand, the media often refers (presumably in a pejorative sense)
to certain trading activities, particularly those involving ï¬nancial derivative
instruments, as gambling. On the other hand, the origin of probability theory
is traced back to a betting puzzle, on how to redistribute fairly the stakes of
an interrupted game of chance, posed by the gambler Antoine Gombaud (also
known as Chevalier de MÃ©rÃ©) to Blaise Pascal. Thus, it should not be surprising
that a very strong bond exists between the ï¬elds of subjective probability and
derivative ï¬nance. In this chapter, we explore overlapping concepts in these
ï¬elds while focusing on the application of dynamic risk modeling Ã  propos the
hedging activity from the speculative perspective of a hedge fund manager.
Subjective expectations are motivated as fair prices of futures contracts in
Section 14.2. The futures markets are presented, in Section 14.3, as a Bayesian
market maker engine that dynamically reveals rational (coherent and proï¬cient)
expectations of random quantities as prices of futures contracts. A portfo-
lio mean-variance efï¬ciency generalization is motivated, in Section 14.4, as
a sensible quantitative trading strategy for a hedge fund manager adopting
the role of a Bayesian speculator (as opposed to the role of a Bayesian mar-
ket maker) to highlight the critical role of hedging to ensue attractive risk-
adjusted performance. Finally, general Bayesian dynamic models and speciï¬c
Bayesian dynamic linear models are presented, in Section 14.5, to entertain
a method, in Section 14.6, for assessing risk models in terms of their hedg-
ing effectiveness in the context of the risk-adjusted performance of trading
strategies.
14.2 Subjective expectations
Most children know how to make a sibling cut two pieces of a pie fairly:
she would let her brother do it as long as he agrees that she can choose

344
The Oxford Handbook of Applied Bayesian Analysis
her piece ï¬rst. To elicit from someone a current fair price of an item with a
future settlement (payment and delivery), one can proceed in a similar way:
the person is free to name any price as long as it has been agreed that one
can decide afterwards how many items to buy from, or sell to, the individual.
The transaction will happen in the future but at a speciï¬c place, date and price
agreed at the present time (e.g. a gallon of regular gasoline in Hoboken, New
Jersey on the last business day of the next month at a speciï¬ed price). The
individual should avoid naming a price that is too low (or too high) just in
case he or she is forced to sell (or to buy) the item. Furthermore, a smart
(money-seeking, risk-averse) person should name the expected future spot price
of the item on the settlement date as the fair price because this value mini-
mizes the maximum, and potentially huge, expected loss (assuming that the
individual will buy, or sell, the items at the future spot price to accomplish the
settlement).
The above betting scheme corresponds to de Finettiâ€™s 1931; 1974 operational
foundation of Bayesian subjective probability and expectations in which the
individual is forced to play the role of a liquid market maker. Interestingly,
recent informal experiments performed by the ï¬rst author conï¬rm that per-
sonal fair prices (with future settlement dates), acquired by simultaneous elici-
tation from individuals familiar with the concept of coherence (the avoidance of
becoming a sure loser in this setup), diverge. This is just a conï¬rmation of the
obvious: subjective personal fair prices (i.e. expectations) are subjective and per-
sonal; yet, this implies that a group of individually coherent people, operating
independently, typically would act incoherently as an entity. It is even unfair to
force a single person to act as a liquid market maker and one should never really
quote fair prices because one would be potentially vulnerable unnecessarily.
Only an immeasurably rich irrational individual or the ultimate genius would
dare to do so for an extended period of time because, in the words of Barnard
(1980), â€˜fanatical insistence on freedom from â€œincoherenceâ€ can lead to such
complicatedly interrelated analyses of data as to go well beyond the capacity of
our understanding.â€™ Yet, to a great extent, fair prices (with future settlement
dates) of many standardized commodities began to be quoted almost three
centuries ago, and they continue to be quoted in the present futures markets.
This suggests the following questions: Are the futures markets coherent? Are
their expectations worthwhile? Are they beatable? Furthermore, if they are, to
what degree are they beatable?
14.3 Futures markets
A futures contract represents the obligation to deliver a standardized commod-
ity at a speciï¬ed future maturity date and at a prearranged location and price.

Futures Markets, Bayesian Forecasting and Risk Modelling
345
Price (points)
Bid/Ask
Bid/Ask
125
Price
Total value
100
75
50
25
0
125
100
75
50
25
0
Total value (millions of $Â¢s)
Source: Bloomberg
Fig. 14.1 A typical liquid market snapshot.
From an operational standpoint perfect liquid futures markets, as entities, are
quoting fair prices (i.e. they are prepared to buy or sell at the quoted prices
unlimited quantities of standardized commodities at future settlement dates).
Perfect liquid markets, of course, do not exist; but, futures markets trade daily a
variety of commodities (e.g. gallons of gasoline, S&P 500 Index units; Japanese
Government Bonds, pounds of rice, shares of Google, barrels of WTI crude
oil, euros vs. dollars, troy oz. of gold, etc.) with a total worth of trillions of
dollars per day. Figure 14.1 shows, in random order, a snapshot of bid and
ask prices of the US Treasury Bill Futures Contract and their corresponding
stakes. It is apparent that the bid and ask prices are virtually indistinguishable.
In addition, a $25,000,000 transaction would not have affected the bid and
ask price quotes. Indeed, the spread between bid and ask prices of liquid
futures markets are typically measured by a few basis points (a basis point is
one percent of one percent) and the associated stakes are worth millions of
dollars.
Are these liquid futures markets coherent over time? The straightforward
answer is yes, and a simple ï¬nancial economics explanation resembles the
anthropic principle arguments: the fact that the question can be posed implies
its answer (if the futures markets were incoherent they would have been an
ideal fountain of wealth but only for a short while; afterwards they would have
dried out and disappeared). More plausibly, as soon as traders would identify
and exploit incoherencies in the markets, the trading activity would eventually
restore coherent prices, since perfect liquid markets do not exist (prices are
affected by supply and demand). Remarkably, although the futures markets
participants might not be individually coherent, as an entity, they are acting
in concert coherently by sharing between all of them two crucial pieces of
information: the bid and ask prices. The ending value of a futures contract
(i.e. the future spot price) can be tied to a general commodity (e.g. gallons of
gasoline) but it also can be associated to an artiï¬cial item (e.g. S&P 500 index)

346
The Oxford Handbook of Applied Bayesian Analysis
and in that case is settled in cash. Further, in principle, the future spot price
could be linked to any random variable yet to be revealed. For example, a well
deï¬ned global average temperature measurement for the year 2025 could deï¬ne
a futures market where the global warmers and global coolers could settle their
differences (actually, contracts of this kind but with near settlements are traded
in the form of binary bets, which are described below).
A futures contract where the future spot price can only take two values and
one (and only one) potential value is zero (also known as a binary bet) is of
particular interest. Thus, a binary bet on an arbitrary event, the condition that
deï¬nes a non zero value (e.g. the S&P500 Index will be up at the end of the
day, Barack Obama will be the 2008 US presidential election winner, etc.), is
a futures contract that pays a speciï¬ed amount (e.g. $100) if the event occurs
and zero otherwise. The concept of a binary bet, from a practical perspective,
was introduced in the UK early in this millennium; however, from a theo-
retical perspective it can be traced back to de Finetti (1931), Ramsey (1926)
and arguably to Bayes (1763). Indeed, his deï¬nition â€˜The probability of any
event is the ratio between the value at which an expectation depending on the
happening of the event ought to be computed, and the chance of the thing
expected upon its happeningâ€™ written in centuries old English might be hard
to comprehend at ï¬rst sight but it is easily illustrated by example. Bayesâ€™ insight
behind his deï¬nition of probability of an event becomes apparent if we try to
compute the expected payoff of a binary contract divided by the conditional
reward ($100) according to a hypothetical market probability of its associated
event,
E ($Payoff)
$100
= (1 âˆ’P (Event)) $0 + P (Event) $100
$100
= P (Event) .
Figure 14.2 depicts a partial time series of fair price transactions from the Irish
Intrade exchange of several binary contracts for the US presidential election
winner (paying $100 if, and only if, the particular candidate wins). Thus, using
the connection between fair prices and expectations, the time series of market
fair prices represent the probabilities in percent (according to the market par-
ticipants) of their associated events. These are time series of coherent market
probabilities (they do not add up to 100% because other contracts of potential
contenders, such as Al Gore, with small non-zero probabilities are not shown
in the graph) assimilating relevant information as it is uncovered (e.g. the
January primaries results). In fairness, the markets are as coherent as they are
liquid, and this particular market is not as liquid (in terms of tight bid and
ask spreads and deep associated stakes) as other markets. Yet, the implication
is that since liquid markets ought to be coherent then liquid markets trading
binary contracts must obey the rules of probability theory associated to coher-
ence. That is, denoting generic events by A and B and a sure event by 	, the

Futures Markets, Bayesian Forecasting and Risk Modelling
347
Jan 07
Last update: January 31
Source: Intrade
Clinton 36.7%
McCain 35.0%
Obama 20.2%
Romney 4.4%
Giullani 0.7%
Huckabee 0.5%
Intrade Prediction Markets
Presidential Election Winner
Jul 07
Jan 08
0
10
20
30
Chance of victory
40
50
Fig. 14.2 Market implied probabilities.
market implied probabilities should comply with: P(A) takes a unique value;
P(A) â‰¥0; P(	) = 1; P(A + B) = P(A) + P(B), where A + B denotes the union
of two inconsistent events, and P(AB) = P(A)P(B|A) where P(B|A) denotes a
conditional binary bet on the event B that is called off if and only if A fails to
happen. For completeness, a concise direct derivation of these coherent rules,
following Quintana (2005), is presented in the Appendix A. Furthermore, as
pointed out by de Finetti (1974), these probability rules can be alternatively
derived indirectly from three basic rules regarding generic fair prices of payoffs
connected to arbitrary random quantities (denoting the fair price of a contract
tied to a random quantity X as E(X) to emphasize that it is, or should be, its
expectation): E(X) takes a unique value; E(X + Y) = E(X) + E(Y); and X â‰¤a
(meaning, a ï¬xed quantity, a, is greater than or equal to any possible outcome
of X) implies E(X) â‰¤a. Therefore, liquid futures markets ought to obey these
rules and all their implications to remain coherent (e.g. for several random
quantities, the associated fair pricesâ€™ points must lie in the closed convex hull
of the possible outcomes).
Are these implied market expectations worthwhile? Are the futures markets
unbeatable? The supporters of the Efï¬cient Markets Hypothesis (EMH) answer
these questions afï¬rmatively. The conjecture and its underlying argument can
be traced to Bachelierâ€™s (1900) conclusion: â€˜the mathematical expectation of
the speculator is zeroâ€™. The argument stretches convincingly the reasoning
supporting the liquid marketsâ€™ coherence as follows: if the futures markets
were inefï¬cient then the trading triggered by the speculators taking advantage
of expected proï¬table opportunities would re-establish price levels that fully
assimilate the relevant information and no more expected gains would remain,
thus, effectively restoring the market efï¬ciency. Empirical analyses (e.g. Fama

348
The Oxford Handbook of Applied Bayesian Analysis
1970) support the EMH in a variety of setups, including binary bets (also
known as prediction) markets (e.g. Carvalho and Rickershauser 2010) and the
early period of the Dojima futures market (Hamori et al. 2001). Interestingly,
the Dojima rice futures market, which can be regarded as the ï¬rst Bayesian
practitioner, started to quote fair prices in 1730; that is, 33 years before the work
of the ï¬rst Bayesian theorist was published posthumously.
The liquid futures marketsâ€™ coherence and efï¬ciency appears to be accom-
plishing an amazing feat. Yet, it can be explained because: ï¬rst, key mar-
ketsâ€™ speculators are well aware of the arbitrage-free concept (essentially the
ï¬nancial economics jargon for coherence) and eager to exploit any potential
opportunities; second, the market consensus expectations (fair prices) integrate
over a spectrum of presumably relevant pieces of information pondered by
the conviction measured by the amount of money willing to be committed by
each one of the participants; and third, a ï¬nancial evolution process ensures
that only the best ï¬tted participants survive in the long run. Furthermore,
the market coherence and efï¬ciency ought to be dynamic; that is, coherent
conditional market expectations (fair prices) are produced in real time assimilat-
ing available relevant information under all circumstances even while extreme
events are happening. These liquid futures markets come across, as a whole,
as an invincible juggernaut assimilating not only relevant information but also
any applicable knowledge (e.g. ï¬nancial economics, computing and statistical
technologies among others) in its path.
14.4 Bayesian speculation
Last section ending notwithstanding, one might sense a vulnerability on any
market maker due to the willingness to buy or sell at the, virtually, same
quoted price. Let us adopt henceforth the offensive role of a Bayesian spec-
ulator instead of the defensive attitude of a Bayesian market maker (just as
one would rather choose a piece, as opposed to cutting two pieces fairly, of a
pie). An encouraging indication, from the speculatorâ€™s viewpoint of a hedge
fund manager, is that eighty years after the aforementioned Bachelierâ€™s (1900)
pronouncement, a disarming counterargument was proposed by Grossman and
Stiglitz (1980): If the markets were efï¬cient there would not be any incentives to
get, gather and process information at a certain non-zero cost, and incorporate
it, by trading, into the markets; therefore, the market would not assimilate
the relevant information and it would become inefï¬cient. This leads to the
EMH paradox: marketsâ€™ efï¬ciency implies marketsâ€™ inefï¬ciency, and marketsâ€™
inefï¬ciency implies marketsâ€™ efï¬ciency! As a smart (proï¬t-seeking, risk-averse)
speculator one could perceive markets incoherence and efï¬ciency as black
(when a sure proï¬t is available) and white (when a positive expected gain is

Futures Markets, Bayesian Forecasting and Risk Modelling
349
unattainable) with shades of gray in between. A general speculating strategy
consists of two steps: ï¬rst, form a (probabilistic) view of the markets; second,
trade an advantageous book of bets based on that view.
One way a Bayesian speculator can quantitatively implement the latter step
is by maximizing the expected net payoff (also known as excess return) of a
book of bets

wâ€²
tyt

at a certain time (t) for a certain holding period (e.g. a
week) per unit of risk measured by its standard deviation (the so-called ex-ante
information ratio); that is,
max
wt
âŽ›
âŽ
E
yt|x1,y1,...,ytâˆ’1,xtwâ€²
tyt
;
V
yt|x1,y1,...,ytâˆ’1,xtwâ€²
tyt
âŽž
âŽ 
(14.1)
where yt, xt, wt denote respectively the vector of net payoffs (the differences
between buying prices and selling prices of the futures contracts), their associ-
ated vector of relevant explanatory variables (that presumably would contribute
to its predictability) and the corresponding vector of trading (holding) amounts.
Notice that the weights do not have to comply with the traditional investments
constraints (i.e. they do not have to be non-negative, neither do they have to add
up to one) since wt represents a book of side bets as opposed to an allocation of
investment capital (capital would still be required to support the book of bets to
cover potential losses but it could be earning interest or be invested in any other
liquid assets). The optimal information ratio is bounded by zero on the left hand
side and by inï¬nity on the right hand side; the former bound corresponds, from
the speculatorâ€™s point of view, to market efï¬ciency whereas the latter implies an
incoherent market condition (although market incoherence, in principle, could
also occur in a situation with a ï¬nite information ratio). Thus, the information
ratio provides quantitative means to evaluate to what degree the market expec-
tations are beatable by measuring the speculatorâ€™s expected gain per unit of risk
taken in an optimal scenario.
This optimization step is equivalent to the following two-way Pareto opti-
mization,
max
wt

E
yt|x1,y1,...,ytâˆ’1,xtyt

âˆ’1
2ÃŽt

V
yt|x1,y1,...,ytâˆ’1,xtyt

(14.2)
and optimal trades are given by,
wâˆ—
t = 1
ÃŽt

V
yt|x1,y1,...,ytâˆ’1,xtyt
âˆ’1
E
yt|x1,y1,...,ytâˆ’1,xtyt
(14.3)
where ÃŽt is an arbitrary inverse scaling factor representing the speculatorâ€™s
greed-fear tradeoff. The optimization (14.1) corresponds to the Modern Port-
folio Theory (MPT) objective function but the feasible set for the weights is

350
The Oxford Handbook of Applied Bayesian Analysis
different: in this setup there are no constraints whereas in the MPT setup
the weights are constrained to lie on the simplex and the solution formula
(14.3) would not apply; instead, a quadratic programming technique would be
required to obtain the solution. The online optimal book (portfolio) given by the
formula (14.2) turns out to be also optimal in a multiperiod utility maximization
setup; this is discussed brieï¬‚y in Appendix A.
It is interesting to note at this point that once a liquid futures market has
emerged the subjectivity (of probabilities and expectations elicited according
to de Finettiâ€™s recipe) should disappear because a smart market maker should
simply quote the liquid market price and hedge accordingly since this would
not only make his expected loss null but the potential loss itself would be
null (quoting a different price would not necessarily imply incoherence but
a quasi-incoherence in the sense that he would be willing to buy at a higher
price or to sell at a lower price than the market price giving a perfect arbi-
trage opportunity to another speculator to make sure money). However, if the
expectations of the Bayesian speculator coincides with the futures marketâ€™s
expectations (i.e. prices), then the expected excess return of any book would
necessarily be null. To avoid this pitfall the loophole should be closed and the
elicitation process should, in principle, assume that the Bayesian speculator
has no access to liquid futures markets during his hypothetical role as a market
maker.
14.5 Bayesian forecasting
The ï¬rst step of the aforementioned trading strategy can be implemented
quantitatively in the following manner: form the predictive density function
of the payoffs for each holding period by a Bayesian updating process of an
underlying Bayesian dynamic model. Coherency, as mentioned in the appen-
dix, requires that conditional fair prices must correspond to the customary
conditional expectations before the conditioning information is observed, but
does not require that this correspondence must remain after the conditioning
information is revealed. Although perceived by some as a weakness in the
foundations of the Bayesian subjective approach, this feature is actually a bless-
ing in disguise from the Bayesian speculatorâ€™s perspective: on the one hand,
the speculator could maintain consistency and rely routinely on the default
Bayesian updating based on conditional probability and expectations since they
ensure coherence for the predictive probabilistic view at each holding period,
and this is an essential requirement for the strategy (otherwise, for example,
one could dangerously conclude that there is a perfect arbitrage opportunity
when in fact there is none); on the other hand, the Bayesian speculator is always
free to consider and switch to an alternative Bayesian model at any time if a

Futures Markets, Bayesian Forecasting and Risk Modelling
351
uQtâ€“1
uQt
uQt+1
Qtâ€“1
Qt
Qt+1
ytâ€“1
yt
yt+1
vtâ€“1
vt
vt+1
Fig. 14.3 Functional network of a generic Bayesian dynamic model.
decisive new information, knowledge or technology becomes available (only a
mad speculator would blindly commit perpetually to a ï¬xed model).
A Bayesian dynamic (also known as stochastic) model presumably would
allow the trading strategy to adapt to regime changes and assimilate ï¬nancial
market shocks. Figure 14.3 represents the stochastic structure of a generic
Bayesian dynamic model in a manner introduced by Quintana et al. (2003). This
representation relies on the principle that a stochastic model is well deï¬ned if
and only if its simulation is fully speciï¬ed. Arrows denote inputs to a deter-
ministic function at each node; double arrows denote generated random entity
(vector, matrix, etc.) inputs whose distributions are fully speciï¬ed (involving
only known parameters) and they constitute the primary sources of random-
ness; and all the primary random entities are jointly independent. The Bayesian
dynamic model consists of a Markovian evolution of the parameter time series
t coupled with contemporaneous random observations yt. The basic premise
is that the model structure, described by the values of the system parameters
t, is changing according to a stochastic process (as opposed to traditional rigid
static formulations) but there is a degree of persistence making the sampling
and ï¬ltering process worthwhile.
A Dynamic Linear Model (DLM) variant of the formulations entertained by
West and Harrison (1997) is of particular interest when the speculator suspects
that the markets are not assimilating efï¬ciently all the available public (relevant)
information; that is when the so-called semi-strong form of the EMH (Fama
1970) might be vulnerable. This model is a particular Bayesian dynamic model
where the parameters are decomposed as follows: the observation equation,
at each time period, takes the form of a multivariate multiple regression
that relates the excess returns (or logarithmic excess returns that are more
convenient from a modelling perspective) yt and associated relevant available
public information xt via the regression coefï¬cients â€št with a residual variance-
covariance t representing the variance-covariance of the difference of the
random observations and their corresponding expectations; and the random
evolution of the parameters follows a form of random walk.

352
The Oxford Handbook of Applied Bayesian Analysis
ubt â€“1
bt â€“1
bt
bt+1
ubt
ubt+1
uSt â€“1
uSt
uSt+1
yt â€“1
yt
yt+1
vt â€“1
vt
vt+1
St â€“1
St
St+1
Fig. 14.4 Functional network of a speciï¬c Bayesian DLM model.
This modelâ€™s functional network can be decomposed as a marginal network
(for t) coupled with a conditional network (given t), or vice versa in terms
of â€št, in a hierarchical fashion as is depicted in Figure 14.4. A more detailed
speciï¬cation of this model, including speciï¬c functional forms and random
generation procedure descriptions, is presented in the Appendix B.
To what degree are the market expectations beatable? A comprehensive dis-
cussion on this subject is beyond the scope of this chapter; however, the answer
to the question depends, of course, on the respondent. According to simulated
and, more importantly, actual trading of strategies based on the above described
speciï¬c Bayesian DLM and a variant the information ratio optimization have
generated attractive risk-adjusted returns (see Quintana, 2005 and Quintana
et al., 2003 for details). Nevertheless, the customary disclaimer applies: PAST
PERFORMANCE IS NOT A GUARANTEE OF FUTURE RESULTS and the
only way to beat the markets going forward is by taking some risk of losing
money (unless market incoherencies are identiï¬ed and exploited).
14.6 Risk modelling
Risk reduction is more important than expected return enhancement in the
sense that according to the ex-ante information ratio formula 14.1 a risk reduc-
tion of 50% is equivalent to an expected return enhancement of 100%. Someone
might still try to argue pointing out that doubling the expected excess return at
the same risk in some circumstances could be more appealing than halving the
risk while maintaining the same expected return. However, the former book

Futures Markets, Bayesian Forecasting and Risk Modelling
353
can be derived from the latter book using leverage (by doubling the bets; that is,
the allocation weights). Thus, risk modeling is a crucial mission for a speculator
seeking the best risk-adjusted performance (i.e. the best information ratio). In
this section the focus is on the predictive ability, for hedging purposes, of the
variance covariance of a book of bets

wâ€²
tyt

; which is the variance appearing in
the denominator of expression (14.1),
V
yt|x1,y1,...,ytâˆ’1,xtwâ€²
tyt = wâ€²
tVtwt
(14.4)
where Vt is shorthand for the predictive variance of the book of bets
V
yt|x1,y1,...,ytâˆ’1,xtyt.
Although risk and return are inherently connected, it is useful from a
modelling perspective to analyze them as separately as possible. Because of
the availability of leverage, a speculator trying to evaluate the expected return
capability of a model cannot just set ÃŽt equal to zero in the optimization (14.2);
one possible way to alleviate this difï¬culty is, for evaluation purposes, to force
the weights to add up to one. However, because the book expectation, given a
direction, is directly proportional to the square of its norm, forcing the weights
to add up to one would unfairly punish the minimum length book satisfying
the constraint (i.e. the long equal weighted book) and favour other books
as their length increase. This drawback can be avoided simply by constraining
the potential weights to have a unitary norm instead. Similarly, an ideal testing
hedging task would be to ï¬nd the minimum variance book lying on the hyper-
circumference,
min
wt

wâ€²
tVtwt

, s.t. wâ€²
twt = 1
(14.5)
or equivalently, to ï¬nd the minimum Vtâ€™s Rayleigh quotient

wâ€²
tVtwt

/

wâ€²
twt

.
The solution to both problems can be derived directly but it is a known result in
extremizing quadratic forms (e.g. Noble and Daniel 1987): the optimal weights
correspond to the normalized eigenvector associated to the smallest eigenvalue
(which itself is the value of the minimum variance book) of the spectral decom-
position of Vt; this is the counterpart of the other better known maximum vari-
ance result where the book (portfolio) corresponding to the largest eigenvalue
exhibits the most variation (as discussed by Quintana and West 1987).
Although modelling assumptions about the payoff expectations would obvi-
ously inï¬‚uence risk predictions given the liquid marketsâ€™ efï¬ciency those expec-
tations should not be far away from the marketsâ€™ expectations. Thus, risk mod-
els evaluation results based on marketsâ€™ payoff expectations should still apply
and for this reason these marketsâ€™ expectations are entertained henceforth. That
is, it is assumed in this section that the expectation
E
yt|x1,y1,...,ytâˆ’1,xtyt corresponds
to the marketâ€™s expectation (i.e. it is null).

354
The Oxford Handbook of Applied Bayesian Analysis
The merits of alternative risk models, since the expected excess returns
are assumed to be null, can be assessed by comparing the cumulative sum
of squares of the excess returns of these extreme books over time associated
to each model, or equivalently, rolling (e.g. one year) ex-post (sample) stan-
dard deviations can be used to discriminate between models. Two alterna-
tive risk models embedded in the functional network structure (Figure 14.4),
for â€št = 0, are considered in this study. The ï¬rst, and the simplest, assumes
that the t stochastic evolution follows a form of random walk that allows
for a conjugate analysis with Wishart probability distributions; this model
corresponds to the stochastic multiple-factor model discussed by Quintana
et al. 2003 with the constraint â€št = 0 which was introduced by Quintana and
West 1987. The second model, entertains a graphical formulation for t that
encompasses a conditional independence structure via a pattern of zeros in
the corresponding precision matrix. As introduced by Carvalho and West
(2007), the idea of conditioning the sequence of variance-covariance matri-
ces on set of graphical constraints provides a parsimonious model for 
potentially improving the empirical accuracy of book (portfolio) predictions
and reducing the variation of realized returns. In this application, the set of
constraints (i.e. the graph) was selected as the top model from a stochastic
search procedure (Scott and Carvalho 2008) using observations prior to the
period analysed here. A comprehensive description of both models appears in
Appendix B.
The dataset used to evaluate the trading performance of the alternative
dynamic variance-covariance models consists of weekly excess returns, from
5/25/1990 through 6/19/2008, derived from adjusted indexed futures prices
provided by Bloomberg using the indexed price series. This futures dataset
entails currency, government bonds and stock indices instruments typically
traded by a Global Macro Hedge Fund and it is described at the end of this
section. Figures 14.5â€“14.7 show the hedging performance of the full model ver-
sus the graphical model. The ï¬guresâ€™ left-hand sides depict the corresponding
annualized one-year rolling ex-post standard deviations for each model. The
right-hand sides show the ratio (full/graph) of these rolling standard devia-
tions. The different pictures result from varying the discount factor as follows:
â€° = 0.97, 0.98, 0.99. It is apparent from these ï¬gures that for a high discount
factor the performances of the models are similar but as the values decrease the
dominance of the graphical model over the full model becomes apparent. This
result is consistent with the intuition that the regularization from the graphical
structure is more relevant when information is scarcer (i.e. when the discount
factor is smaller) and suggests that the hedging performance of the graphical
model is more robust in relation to the choices of the discount factors making
it very attractive.

Futures Markets, Bayesian Forecasting and Risk Modelling
355
0.04
52 Week Rolling Annualized Risk
Ratio
FULL
GRAPHICAL
FULL
GRAPHICAL
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
1.6
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
12/26/93
12/26/94
12/26/95
12/26/96
12/26/97
12/26/98
12/26/99
12/26/00
12/26/01
12/26/02
12/26/03
12/26/04
12/26/05
12/26/06
12/26/07
12/26/08
12/26/93
12/26/94
12/26/95
12/26/96
12/26/97
12/26/98
12/26/99
12/26/00
12/26/01
12/26/02
12/26/03
12/26/04
12/26/05
12/26/06
12/26/07
12/26/08
Fig. 14.5 â€° = 0.97.
0.04
52 Week Rolling Annualized Risk
Ratio
FULL
GRAPHICAL
FULL
GRAPHICAL
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
1.6
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
12/26/93
12/26/94
12/26/95
12/26/96
12/26/97
12/26/98
12/26/99
12/26/00
12/26/01
12/26/02
12/26/03
12/26/04
12/26/05
12/26/06
12/26/07
12/26/08
12/26/93
12/26/94
12/26/95
12/26/96
12/26/97
12/26/98
12/26/99
12/26/00
12/26/01
12/26/02
12/26/03
12/26/04
12/26/05
12/26/06
12/26/07
12/26/08
Fig. 14.6 â€° = 0.98.
0.04
52 Week Rolling Annualized Risk
FULL
GRAPHICAL
Ratio
FULL
GRAPHICAL
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
1.6
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
12/26/93
12/26/94
12/26/95
12/26/96
12/26/97
12/26/98
12/26/99
12/26/00
12/26/01
12/26/02
12/26/03
12/26/04
12/26/05
12/26/06
12/26/07
12/26/08
12/26/93
12/26/94
12/26/95
12/26/96
12/26/97
12/26/98
12/26/99
12/26/00
12/26/01
12/26/02
12/26/03
12/26/04
12/26/05
12/26/06
12/26/07
12/26/08
Fig. 14.7 â€° = 0.99.
The discussion at the beginning of this section hints that the hedging perfor-
mance dominance of the graphical models over the full models should be car-
ried into risk-adjusted performance dominance. This is conï¬rmed, in the case
at hand, by the simulated risk-adjusted performance exhibited in Figures 14.8â€“
14.10. The ï¬gures depict the one-year rolling ex-post information ratio attained
when the risk models are coupled with the expected returns formulation of the
Bayesian DLM model mentioned in Section 14.5 using proprietary explanatory

356
The Oxford Handbook of Applied Bayesian Analysis
4
1 Year Rolling IR
Full
Graphical
3.5
3
2.5
2
1.5
1
0.5
0
â€“0.5
12/12/96
12/12/97
12/12/98
12/12/99
12/12/00
12/12/01
12/12/02
12/12/03
12/12/04
12/12/05
12/12/06
12/12/07
12/12/08
â€“1
Fig. 14.8 â€° = 0.97.
4
Full
1 Year Rolling IR
Graphical
3.5
3
2.5
2
1.5
1
0.5
0
â€“0.5
â€“1
12/12/1996
12/12/1997
12/12/1998
12/12/1999
12/12/2000
12/12/2001
12/12/2002
12/12/2003
12/12/2004
12/12/2005
12/12/2006
12/12/2007
12/12/2008
Fig. 14.9 â€° = 0.98.

Futures Markets, Bayesian Forecasting and Risk Modelling
357
12/12/1996
12/12/1997
12/12/1998
12/12/1999
12/12/2000
12/12/2001
12/12/2002
12/12/2003
12/12/2004
12/12/2005
12/12/2006
12/12/2007
12/12/2008
Full
1 Year Rolling IR
5
4
3
2
1
0
â€“1
Graphical
Fig. 14.10 â€° = 0.99.
variables that include measures of short, medium and long interest rates,
liquidity and credit conditions, proxies for inï¬‚ation, etc. These simulations
begin after a buffer of three years used as a learning period for estimating
the parameters associated to the payoffs expectations and take into account
estimated transaction costs and employ a realistic production four-way (mean-
variance-size-turnover) Pareto optimization variant of the simpler theoretical
two-way (mean-variance) optimization discussed in Section 14.4 to promote the
bookâ€™s liquidity and control its transaction costs.
A more detailed description of the dataset follows to complete this section.
The currencies weekly excess returns in the dataset were calculated as yt =
Ct/Ctâˆ’1 âˆ’1 where Ctâˆ’1 and Ct are the speciï¬c indexed closing currency futures
prices on consecutive Fridays (i.e. they are excess returns from a US dollar
speculatorâ€™s perspective); the stock indices and the government bond weekly
excess returns were calculated as yt = (Pt/Ptâˆ’1 âˆ’1)Ct/Ctâˆ’1 where Ptâˆ’1 and Pt
are the indexed closing futures prices and Ctâˆ’1 and Ct are the corresponding
indexed closing currency futures prices (i.e. they are currency hedged excess
returns from a U.S. dollar speculatorâ€™s perspective).
The Bloomberg tickers of the liquid futures contracts considered are as
follows,
Currencies: AD1 Crncy, CD1 Crncy, SF1 Crncy, BP1 Crncy, EC1 Crncy, JY1
Crncy.

358
The Oxford Handbook of Applied Bayesian Analysis
Government Bonds: XM1 Comdty, CN1 Comdty, FB1 Comdty, RX1 Comdty, G
1 Comdty, JB1 Comdty, ZT1 Comdty, TY1 Comdty.
Stock Indices: XP1 Index, PT1 Index, SM1 Index, GX1 Index, IB1 Index, CF1
Index, Z 1 Index, ST1 Index, TP1 Index, EO1 Index, SP1 Index, ZI1 Index.
Indexing based on price ratio was used to smooth out jumps that occur when
a contract rolls into a new month (i.e. the speculator is assumed to maintain
the same exposure while rolling contracts). To adjust by ratio when a contract
changes, the previous indexed prices were multiplied by the ratio of the price
of the new contract on the roll date to the price of the previous contract just
before the roll date. The rolling schedule used for the currency assets was ï¬ve
days prior to the contract expiration date. The equity indices roll schedule was
four days prior to the contract expiration date with the exception of the French
CAC40 Equity Index (CF1 Index) â€“ which was ï¬ve days prior to expiration.
The bond assets were rolled ï¬ve days prior to the contract expiration with the
following exceptions: CN1 Comdty â€“ 28 days prior to expiration, G 1 Comdty
â€“ 35 days prior to expiration, TY1 Comdty â€“ 25 days prior to expiration, XM1
Comdty and ZT1 Comdty â€“ on the expiration date. Adjustments to imply excess
returns were performed for the latter due to the Australian ad hoc quoting
methods (described in its Bloomberg description page). The Deutsche mark
was considered instead of the Euro prior to 1999. Finally, where futures prices
were not available for the entire history considered, implied fair future prices
were calculated based on suitable proxies.
14.7 Conclusions
According to subjective formulations of probability theory, Bayesians are, in
principle, playing the role of altruist bookmakers in a betting scheme. Corre-
spondingly, liquid ï¬nancial futures markets act virtually as Bayesian market
makers quoting fair prices that, consequently, represent objective coherent
consensus expectations of the underlying commodities (including currencies,
government bonds, stock indices, etc.). This amazing feat is accomplished by
market participants acting in concert, albeit driven by selï¬sh motivation, and
putting into practice the Japanese proverb â€˜none of us is as smart as all of
usâ€™. Furthermore, the futures markets, as an entity, assimilates not only all the
relevant information but also the applicable methodologies and technologies
resulting in a formidable apparatus providing key economic ongoing expecta-
tions that, according to the efï¬cient markets hypothesis advocates, cannot be
improved.
Although the futures markets appear to be coherent, a Bayesian speculator
driven by dynamic models predictions, which are hypothetically elicited without

Futures Markets, Bayesian Forecasting and Risk Modelling
359
access to the markets to allow back subjectivity, could be able to generate
attractive returns, given the risk taken, by trading in different markets as
a counterpart. Regarding risk-adjusted performance, risk modeling and risk
predictions are at least as important as the expected payoffsâ€™ formulations.
Moreover, the hedging ability of different risk models can be assessed by
comparing their success in producing normalized books (portfolios) with min-
imum risk; by this criterion, graphical models seem to be more robust having
either better or similar hedging effectiveness in relation to the corresponding
models without a conditional independence structure. Furthermore, their asso-
ciated risk-adjusted performance exhibits, as expected, a similar dominance
relationship.
Ultimately, resistance is futile, since the market Borg would dynamically
assimilate any Bayesian speculating trading strategy that is implemented and
would reward (or punish) the Bayesian speculator to the exact extent to which
the futures marketsâ€™ expectations improve (or deteriorate) as a result of this
particular trading activity.
Appendix
A. Broader context and background
A.1 Foundations: Probabilty axioms derivation
An implication of the probability axioms (showing the steps to get sure money
from an otherwise incoherent market maker) is concisely presented in Quin-
tana (2005) as follows:
1. P(A) is unique (otherwise, buy it low and sell it high).
2. P(A) â‰¥0 (otherwise, buy it if P(A) < 0).
3. P(	) = 1 (otherwise, buy it if P(	) < 1, or sell it if P(	) > 1; recall, 	
denotes a sure event).
4. P(A + B) = P(A) + P(B) (a synthetic binary bet for A + B is a binary bet
for A plus a binary bet for B; A + B denotes the union of two inconsistent
events).
5. P(AB) = P(A)P(B | A) (a synthetic binary bet for AB is P(B | A) units
of a binary bet on A plus a contingent binary bet on B | A if A happens,
using the contingent payoff to buy it; recall, a binary bet on B | A is called
off if A does not happen).
However, there are two caveats worth mentioning: First, Axiom 4 only
implies ï¬nite additivity, and its implied generality is somewhat controversial.
Second, the price P(B | A) is quoted beforehand; but the market maker could
quote a different one when and if A happens, even if this is the only extra
information revealed (then again, as pointed out by Goldstein (2005), typically

360
The Oxford Handbook of Applied Bayesian Analysis
extra relevant information would also become simultaneously available). There
is an alleviating provision, the market maker cannot tell in advance, without
becoming incoherent, that her future quote will always be higher (or lower)
afterwards.
A.2 Foundations: Bayesian portfolio optimization
The optimization (14.2) corresponds to the Markowitz (1959) mean-variance
approach to ï¬nding optimal allocation weights in a traditional investment setup
but without any of the traditional investment constraints for the weights to lie
on the simplex. Although this approach, where only the ï¬rst two predictive
moments of the excess returns are in play, is not a panacea it seems to work
well in practice. This direct predictive formulation is equivalent to solving ï¬rst
the optimization problem conditional on the modelâ€™s unknown parameters and
taking the utility expectation afterwards, as prescribed by the early work of
Zelner and Chetty (1965), and naturally deals with the uncertainty inherent
in the unknown modeling parameters. In contrast, there is an industry (e.g.
DiBartolomeo 2003) to compensate for naive non-Bayesian alternatives that rely
on plug-in estimates.
Moreover, it can be justiï¬ed by the Bayesian paradigm of maximizing
expected utility and it is the solution to a multiple-period decision problem even
though it can be implemented myopically. This can be induced by deï¬ning the
multiple-period utility as a weighted additive (where the components are the
single-period utilities). In this framework, the associated stochastic dynamic
programming problem is broken down into several isolated single-period opti-
mizations and each optimal portfolio can be determined online requiring only
single-step conditional predictive distributions for the payoffs. In addition, the
single-period optimization (14.2) can be derived from the principle of utility
maximization when the single-period utility is set as the difference of (positive)
target payoff minus a square error penalty for deviating from that target. This
approach and its motivations are discussed by Quintana (1992) and Quintana
et al. (2003) and references therein. It is worth noting that, despite common
Wall Street wisdom to the contrary, the speculator does not necessarily have
to assume normality, nor miss potentially unlimited opportunities (due to the
presence of market incoherence) as it would be the case when using a naive
quadratic loss formulation.
B. Models and computations
B.1 Models
Generic Bayesian dynamic model The generic statistical model depicted by the
functional network in Figure 14.3 and described by the corresponding
speciï¬c functional forms for the observations and parameters random

Futures Markets, Bayesian Forecasting and Risk Modelling
361
evolution,
yt = f (t, vt)
t = g

tâˆ’1, ut

(A.1)
coupled with independent random generation procedures for vt and ut has
a conditional probabilistic structure; this generic model speciï¬cation can be
deï¬ned in a traditional way by the set of stochastic Markovian parametric
probability density functions p(t | tâˆ’1) and the set of observational proba-
bility density (i.e. likelihood) functions p(yt | t). For notational convenience
we are omitting all the given available information (e.g. x1, y1, . . . , ytâˆ’1, xt). The
initial parameter, without loss of generality, can be set 0 = 0. The updating
recurrences for the evolution, prediction and ï¬ltering are respectively:
p (t) =

tâˆ’1
p (t | tâˆ’1) p (tâˆ’1) dtâˆ’1
p (yt) =

t
p (y | t) p ( t) dt
p (t | yt) = p (yt | t) p (t)
p (yt)
that is, the steps involve the computation of two marginal densities and one
conditional density.
Speciï¬c Bayesian DLM The observational functional equations and the associ-
ated random generation procedures (given t) of the generic Bayesian DLM,
depicted in Figure 14.4, correspond to a multivariate multiple regression model
with a global scaling factor for the residual variance covariance and are deï¬ned
as follows, yt = Xtâ€št + C (t) vt , where vt âˆ¼N (0, I) and C (t) denotes the
Cholesky decomposition of t (i.e. (yt | â€št) âˆ¼N (Xtâ€št, t).)
The aim of the evolution functional equations and the (independent) random
generation procedures corresponding to the network depicted in Figure 14.4
(given t) is to provide a bridge from a posterior conjugate distribution at time
t âˆ’1 to a prior conjugate distribution at time t. That is, to evolve from â€štâˆ’1 âˆ¼
N (mtâˆ’1, Ctâˆ’1) (i.e. â€štâˆ’1 âˆ¼N (mtâˆ’1, Ctâˆ’1) ) to â€št âˆ¼N

mâˆ—
t , Câˆ—
t

where
mâˆ—
t = Gtmtâˆ’1
(A.2a)
Câˆ—
t = Ctâˆ’1 + GtUtGâ€²
t
(A.2b)
This is accomplished by the following functional equations and random gen-
eration procedures, â€št = Gtâ€štâˆ’1 + C(Ut)uâ€št where uâ€št âˆ¼N (0, I); Ut denotes a
variance-covariance matrix associated to the evolution of â€št. Conversely, the
evolutional functional equations and the (independent) random generation
procedures for the evolution of t is chosen aiming to produce a dynamic

362
The Oxford Handbook of Applied Bayesian Analysis
conjugate updating procedure given â€št. The details of this evolution are found
in Quintana et al. (2003) and can be summarized as follows: First, pre-multiply
and post-multiply t by the inverse of the Cholesky decomposition of the scale
parameter of its inverse Wishart distribution (this new random matrix has
also a Wishart distribution but with an identity matrix as its scale parameter).
Second, ï¬nd the Cholesky decomposition of this new random matrix. Third,
multiply the diagonal elements of the latter by suitable transformations of Beta
independent random variables to discount their degrees of freedom. Finally,
reverse the transformations of the ï¬rst and second steps. The entire process
is similar to taking a computer apart, tweaking one component, and putting it
back together. The end result of choosing this form of evolution is that (given
â€št) if the posterior distribution of tâˆ’1 âˆ¼IW (dtâˆ’1, Stâˆ’1) is an inverse Wishart
then the prior distribution of t âˆ¼IW

dâˆ—
t , Sâˆ—
t

where
Sâˆ—
t = â€°tStâˆ’1
(A.3a)
dâˆ—
t = â€°tdtâˆ’1
(A.3b)
and 0 < â€°t â‰¤1 is interpreted as an information discount factor.
This provides a framework where the sequence of estimates of t keep
adapting to new information while further discounting past observations.
Variance â€“ covariance graphical model A graphical model is a probability model that
characterizes the conditional independence structure of a set of random vari-
able by a graph (Lauritzen 1996; Jones et al. 2005). Graphs provide a way to
decompose the sample space into subsets of variables generating efï¬cient ways
to model conditional and marginal distributions locally. In high-dimensional
problems, graphical model structuring is a key approach to parameter dimen-
sion reduction and, hence, to scientiï¬c parsimony and statistical efï¬ciency
when appropriate graphical structures can be identiï¬ed.
In the context of a multivariate normal distribution, conditional indepen-
dence restrictions are simply expressed through zeros in the off-diagonal ele-
ments of the precision matrix (inverse of the covariance matrix), establishing a
parsimonious way to model covariance structures. Let a p-vector y âˆ¼N(0, )
and 	 = âˆ’1 with elements Ë˜i j. Write G = (V, E) for the undirected graph
whose vertex set V corresponds to the set of p random variables in y, and E
contains the elements (i, j) for which Ë˜i j are not equal to 0. The canonical
parameter 	 belongs to M(G), the set of all positive-deï¬nite symmetric matrices
with elements equal to zero for all (i, j) /âˆˆE.
In working with decomposable Gaussian graphical models, Dawid and Lau-
ritzen (1993) deï¬ned a family of conjugate Markov probability distributions
called hyper-inverse Wishart. If 	 âˆˆM(G), the hyper-inverse Wishart
 âˆ¼HIWG(d, S)

Futures Markets, Bayesian Forecasting and Risk Modelling
363
has a degree-of-freedom parameter b and location matrix S âˆˆM(G) implying
that each clique C âˆˆC, C âˆ¼IW(d, SC) where SC is the diagonal block of S
corresponding to the vertices in C.
As shown by Carvalho and West (2007), graphical structuring can be incorpo-
rated in matrix normal DLMs to provide parsimonious models for the innova-
tion variance-covariance matrix . For a given decomposable graph G, take the
hyper-inverse Wishart as a conjugate prior for ; it turns out that the closed-
form, sequential updating theory of DLMs can be generalized to this richer
model class.
This is also true for the case described in the previous section where  is
time-varying. In detail, if the posterior at time t âˆ’1 is
tâˆ’1 âˆ¼HIWG(dtâˆ’1, Stâˆ’1)
the prior at time t takes the form
t âˆ¼HIWG(â€°tdtâˆ’1, â€°tStâˆ’1).
This development is based on the same stochastic model of independent Beta
shocks applied to the diagonal elements of the Cholesky decomposition of tâˆ’1
presented above.
B.2 Computations
The speciï¬c DLM described above does not allow for tractable exact recursive
updating formulas; however, as a result of the choice for the speciï¬c Markovian
parametric evolution, given the time series t, the conventional updating recur-
sions (West and Harrison 1997) apply for updating the hyperparameters of the
normal distribution corresponding to the parameter â€št; and conversely, given
the time series â€št, the conventional updating recursions (West and Harrison
1997) apply for updating the hyperparameters of the inverse Wishart distri-
bution corresponding to the parameters t. This suggests a Gibbs sampling
scheme for the implementation of the model.
References
Bachelier, L. (1900). ThÃ©orie de la SpÃ©culation. Annales de lâ€™Ã‰cole normale supÃ©rieure. Paris.
Translated from French in The Random Character of Stock Market Prices. (ed. P.H. Cootner).
The MIT Press, Cambridge, MA. 1964, pp. 17â€“78.
Barnard, G. A. (1980). Discussion of H. Akaike â€˜Likelihood and the Bayes procedureâ€™ and
A. P. Dawid â€˜A Bayesian look at nuisance parametersâ€™. In Bayesian Statistics, (ed. J. M.
Bernardo, M. H. DeGroot, D. V. Lindley, and A. F. M. Smith), pp. 185â€“189. Valencia Uni-
veristy Press, Valencia.
Bayes, T. (1763). An essay towards solving a problem in the doctorine of chances. Philosophical
Transactions of the Royal Society, London , 53, 370â€“418. Published posthumously. Electronic
edition: http://www.stat.ucla.edu/history/essay.pdf.

364
The Oxford Handbook of Applied Bayesian Analysis
Carvalho, C. and West, M. (2007). Dynamic matrix-variate graphical models. Bayesian Analy-
sis, 2, 69â€“96.
Carvalho, C. M. and Rickershauser, J. (2010). Volatility in prediction markets: a measure of
information ï¬‚ow in political campaigns. In The Handbook of Applied Bayesian Analysis. (ed.
A. Oâ€™Hagan and M. West), Oxford University Press, Oxford.
Dawid, A. P. and Lauritzen, S. L. (1993). Hyper-Markov laws in the statistical analysis of
decomposable graphical models. The Annals of Statistics, 3, 1272â€“1317.
de Finetti, B. (1931). Sul Signiï¬cato Soggettivo della ProbabilitÃ . Fundamenta Mathemati-
cae, 17, 298â€“329. Translated into English as â€œOn the subjective meaning of probability,â€
in ProbabilitÃ e Induzione, (ed. Paola Monari and Daniela Cocchi, 1993) Clueb, Bologna,
pp. 291â€“321.
de Finetti, B. (1974). Theory of Probability, a Critical Introductory Treatment. John Wiley, New
York.
DiBartolomeo, D. (2003). Portfolio optimization: The robust solution. In Prudential Securi-
ties Quantitative Conference. Electronic edition: http://www.northinfo.com/documents/
45.pdf.
Fama, E. (1970). Efï¬cient capital markets: A review of theory and empirical work. Journal of
Finance, 25, 383â€“417.
Goldstein, M. (2005). Subjective Bayesian analysis: Principles and practice. In Case Studies
in Bayesian Statistics, Workshop 8â€“2005. Electronic edition http://lib.stat.cmu.edu/
bayesworkshop/2005/goldstein.pdf.
Grossman, S. and Stiglitz, J. (1980). On the impossibility of informationally efï¬cient markets.
American Economic Review, 70, 393â€“408.
Hamori, S., Hamori, N. and Anderson, D. A. (2001). An empirical analysis of the efï¬ciency
of the Osaka rice market during Japanâ€™s Tokugawa era. Journal of Futures Markets, 21,
861â€“874.
Jones, B., Carvalho, C., Dobra, A., Hans, C., Carter, C. and West, M. (2005). Experiments
in stochastic computation for high-dimensional graphical models. Statistical Science, 20,
388â€“400.
Lauritzen, S. L. (1996). Graphical Models. Clarendon Press, Oxford.
Markowitz, H. M. (1959). Portfolio Selection: Efï¬cient Diversiï¬cation of Investments (2nd edn).
Blackwell, Oxford.
Noble, B. and Daniel, J. (1987). Applied Linear Algebra (2nd edn). Prentice Hall, Englewood
Cliffs.
Quintana, J. M. (1992). Optimal portfolios of forward currency contracts. In Bayesian Statistics
4, (ed. J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith), pp. 753â€“762. Oxford
University Press, Oxford.
Quintana, J. M. (2005). Bayesian efï¬cient trading. In Proceedings of the Second Brazilian Confer-
ence on Statistical Modelling in Insurance and Finance. (ed. N. Kolev and P. Morettin), Institute
of Mathematics and Statistics at the University of Sao Paulo.
Quintana, J. M., Lourdes, V., Aguilar, O. and Liu, J. (2003). Global gambling. In Bayesian
Statistics 7, (ed. J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heck-
erman, A. F. M. Smith, and M. West), pp. 349â€“367. Oxford University Press. (with
discussion).
Quintana, J. M. and West, M. (1987). Multivariate time series analysis: New techniques applied
to international exchange rate data. The Statistician, 36, 275â€“281.
Ramsey, F. P. (1926). Truth and probability, In: Ramsey, 1931, The Foundations of Mathematics
and other Logical Essays, R.B. Braithwaite, Ch. VII, p. 156â€“198, London: Kegan, Paul, Trench,
Trubner & Co., New York: Harcourt, Brace and Company.

Futures Markets, Bayesian Forecasting and Risk Modelling
365
Scott, J. G. and Carvalho, C. M. (2008). Feature-inclusion stochastic search for Gaussian
graphical models. Journal of Computational and Graphical Statistics, 17, 790â€“808.
West, M. and Harrison, P. J. (1997). Bayesian Forecasting and Dynamic Models (2nd edn).
Springer-Verlag, New York.
Zelner, A. and Chetty, V. K. (1965). Prediction and decision problems in regression models
from the Bayesian point of view. Journal of the American Statistical Association, 60, 608â€“616.

Â·15Â·
The new macroeconometrics:
A Bayesian approach
JesÃºs FernÃ¡ndez-Villaverde, Pablo GuerrÃ³n-Quintana and
Juan F. Rubio-RamÃ­rez
15.1 Introduction
This chapter studies the dynamics of the US economy over the last 50 years
via the Bayesian analysis of dynamic stochastic general equilibrium (DSGE)
models. Our data are aggregate, quarterly economic variables and our approach
combines macroeconomics (the study of aggregate economic variables like
output or inï¬‚ation) with econometrics (the application of formal statistical
tools to economics). This is an example application in what is often called the
new macroeconometrics.
Our application is of particular interest because modern macroeconomics is
centred around the construction of DSGE models. Economists rely on DSGE
models to organize and clarify their thinking, to measure the importance of
economic phenomena, and to provide policy prescriptions. DSGE models start
by specifying a number of economic agents, typically households, ï¬rms, a
government, and often a foreign sector, and embodying them with behavioural
assumptions, commonly the maximization of an objective function like utility
or proï¬ts.
The inelegant name of DSGE comes from what happens next. First, econo-
mists postulate some sources of shocks to the model: shocks to productivity, to
preferences, to taxes, to monetary policy, etc. Those shocks are the â€˜stochasticâ€™
part that will drive the model. Second, economists study how agents make their
decisions over time as a response to these shocks (hence â€˜dynamicâ€™). Finally,
economists focus on the investigation of aggregate outcomes, thus â€˜general
equilibriumâ€™.
Before we discuss our application, it is important to emphasize that econo-
mists employ the word â€˜equilibriumâ€™ in a particular and precise technical sense,
which is different from daily use or from its meaning in other sciences. We
call equilibrium a situation where (a) the agents in the model follow a concrete
behavioural assumption (usually, but not necessarily, maximization of either
their utility or of proï¬ts) and (b) the decisions of the agents are consistent with

The New Macroeconometrics
367
each other (for example, the number of units of the good sold must be equal to
the number of units of the good bought). Nothing in the deï¬nition of equilib-
rium implies that prices or allocations of goods are constant over time (they may
be increasing, decreasing, or ï¬‚uctuating in rather arbitrary ways), or that the
economy is at a rest point, the sense in which equilibrium is commonly used
in other ï¬elds, particularly in the natural sciences. Many misunderstandings
about the accomplishments and potentialities of the concept of equilibrium in
economics come about because of a failure to appreciate the subtle difference
in language across ï¬elds.
General equilibrium has a long tradition in economics, being already implicit
in the work of the French Physiocrats of the 18th century, such as Cantil-
lon, Turgot, or Quesnay, or in Adam Smithâ€™s magnum opus, The Wealth of
Nations. However, general equilibrium did not take a front seat in economics
until 1874 when LÃ©on Walras published a pioneering book, Elements of Pure
Economics, in which he laid down the foundations of modern equilibrium
theory.
For many decades after Walras, general equilibrium theory focused on sta-
tic models. This was not out of a lack of appreciation by economists of the
importance of dynamics, which were always at the core of practical concerns,
but because of the absence of proper methodological instruments to investigate
economic change.
Fortunately, the development of recursive techniques in applied mathematics
(dynamic programming, Kalman ï¬ltering, and optimal control theory, among
others) during the 1950s and 1960s provided the tools that economists required
for the formal analysis of dynamics. These new methods were ï¬rst applied
to the search for microfoundations of macroeconomics during the 1960s,
which attempted to move from ad hoc descriptions of aggregate behaviour
motivated by empirical regularities to descriptions based on ï¬rst principles of
individual decision-making. Later, in the 1970s, recursive techniques opened
the door to the rational expectations revolution, started by John Muth (1961)
and Robert Lucas (1972), who highlighted the importance of specifying the
expectations of the agents in the model in a way that was consistent with their
behaviour.
This research culminated in 1982 with an immensely inï¬‚uential paper by
Finn Kydland and Edward Prescott, Time to Build and Aggregate Fluctuations.1
This article presented the ï¬rst modern DSGE model, on this propitious occa-
sion, one tailored to explaining the ï¬‚uctuations of the US economy. Even if
most of the material in the paper was already present in other articles written in
the previous years by leading economists like Robert Lucas, Thomas Sargent,
1 The impact of this work was recognized in 2004 when Kydland and Prescott received the Nobel Prize
in economics for this paper and some previous work on time inconsistency in 1977.

368
The Oxford Handbook of Applied Bayesian Analysis
Christopher Sims, or Neil Wallace, the genius of Kydland and Prescott was to
mix the existing ingredients in such a path-breaking recipe that they redirected
the attention of the profession to DSGE modelling.
Kydland and Prescott also opened several methodological discussions about
how to best construct and evaluate dynamic economic models. The conver-
sation that concerns us in this chapter was about how to take the models
to the data, how to assess their behaviour, and, potentially, how to compare
competing theories. Kydland and Prescott were skeptical about the abilities of
formal statistics to provide a useful framework for these three tasks. Instead,
they proposed to â€˜calibrateâ€™ DSGE models, i.e. to select parameter values by
matching some moments of the data and by borrowing from microeconomic
evidence, and to judge models by their ability to reproduce properties of the
data that had not been employed for calibration (for example, calibration often
exploits the ï¬rst moments of the data to determine parameter values, while the
evaluation of the model is done by looking at second moments).
In our previous paragraph, we introduced the idea of â€˜parameters of the
modelâ€™. Before we proceed, it is worthwhile to discuss further what these para-
meters are. In any DSGE economy, there are functions, like the utility function,
the production function, etc. that describe the preferences, technologies, and
information sets of the agents. These functions are indexed by a set of para-
meters. One of the attractive features of DSGE models in the eyes of many
economists is that these parameters carry a clear behavioral interpretation: they
are directly linked to a feature of the environment about which we can tell an
economic history. For instance, in the utility function, we have a discount factor,
which tells us how patient the households are, and a risk aversion, which tells us
how much households dislike uncertainty. These behavioural parameters (also
called â€˜deep parametersâ€™ or, more ambiguously, â€˜structural parametersâ€™) are of
interest because they are invariant to interventions, including shocks by nature
or, more important, changes in economic policy.2
Kydland and Prescottâ€™s calibration became popular because, back in the early
1980s, researchers could not estimate the behavioural parameters of the models
in an efï¬cient way. The bottleneck was how to evaluate the likelihood of the
model. The equilibrium dynamics of DSGE economies cannot be computed
analytically. Instead, we need to resort to numerical approximations. One key
issue is, then, how to go from this numerical solution to the likelihood of the
model. Nowadays, most researchers apply ï¬ltering theory to accomplish this
2 Structural parameters stand in opposition to reduced-form parameters, which are those that index
empirical models estimated with a weaker link with economic theory. Many economists distrust the
evaluation of economic policy undertaken with reduced parameters because they suspect that the historical
relations between variables will not be invariant to the changes in economic policy that we are investigating.
This insight is known as the Lucasâ€™ critique, after Robert Lucas, who forcefully emphasized this point in a
renown 1976 article (Lucas, 1976).

The New Macroeconometrics
369
goal. Three decades ago, economists were less familiar with ï¬lters and faced the
speed constraints of existing computers. Only limited estimation exercises were
possible and even those were performed at a considerable cost. Furthermore, as
recalled by Sargent (2005), the early experiments on estimation suggested that
the prototype DSGE models from the early 1980s were so far away from ï¬tting
the data that applying statistical methods to them was not worth the time and
effort. Consequently, for over a decade, little work was done on the estimation
of DSGE models.3
Again, advances in mathematical tools and in economic theory rapidly
changed the landscape. From the perspective of macroeconomics, the stream-
lined DSGE models of the 1980s begot much richer models in the 1990s.
One remarkably successful extension was the introduction of nominal and
real rigidities, i.e. the conception that agents cannot immediately adjust to
changes in the economic environment. In particular, many of the new DSGE
models focused on studying the consequences of limitations in how frequently
or how easily agents can changes prices and wages (prices and wages are
â€˜stickyâ€™). Since the spirit of these models seemed to capture the tradition of
Keynesian economics, they quickly became known as New Keynesian DSGE
models (Woodford, 2003). By capturing these nominal and real rigidities, New
Keynesian DSGE models began to have a ï¬ghting chance at explaining the
dynamics of the data, hence suggesting that formally estimating them could
be of interest. But this desire for formal estimation would have led to naught if
better statistical/numerical methods had not become widely available.
First, economists found out how to approximate the equilibrium dynamics
of DSGE models much faster and more accurately. Since estimating DSGE
models typically involves solving for the equilibrium dynamics thousands of
times, each for a different combination of parameter values, this advance was
crucial. Second, economists learned how to run the Kalman ï¬lter to evaluate
the likelihood of the model implied by the linearized equilibrium dynamics.
More recently, economists have also learned how to apply sequential Monte
Carlo (SMC) methods to evaluate the likelihood of DSGE models where equi-
librium dynamics is nonlinear and/or the shocks in the model are not Gaussian.
Third, the popularization of Markov chain Monte Carlo (McMc) algorithms
has facilitated the task of exploring and characterizing the likelihood of DSGE
models.
3 There was one partial exception. Lars Peter Hansen proposed the use of the Generalized Method of
Moments, or GMM (Hansen, 1982), to estimate the parameters of the model by minimizing a quadratic
form built from the difference between moments implied by the model and moments in the data. The
GMM spread quickly, especially in ï¬nance, because of the simplicity of its implementation and because
it was the generalization of well-known techniques in econometrics such as instrumental variables OLS.
However, the GMM does not deliver all the powerful results implied by likelihood methods and it often
has a disappointing performance in small samples.

370
The Oxford Handbook of Applied Bayesian Analysis
Those advances have resulted in an explosion of the estimation of DSGE
models, both at the academic level (An and Schorfheide, 2006) and, more
remarkable still, at the institutional level, where a growing number of policy-
making institutions are estimating DSGE models for policy analysis and
forecasting (see Appendix A for a partial list). Furthermore, there is growing
evidence of the good forecasting record of DSGE models, even if we compare
them with the judgmental predictions that staff economists prepare within
these policy-making institutions relying on real-time data (Edge, Kiley and
Laforte, 2008).
One of the features of the new macroeconometrics that the readers of this
volume will ï¬nd exciting is that the (overwhelming?) majority of it is done from
an explicitly Bayesian perspective. There are several reasons for this. First, pri-
ors are a natural device for economists. To begin with, they use them extensively
in game theory or in learning models. More to the point, many economists feel
that they have reasonably concrete beliefs about plausible values for most of
the behavioural parameters, beliefs built perhaps from years of experience with
models and their properties, perhaps from introspection (how risk adverse am
I?), perhaps from well-educated economic intuition. Priors offer researchers a
ï¬‚exible way to introduce these beliefs as pre-sample information.
Second, DSGE models are indexed by a relatively large number of parameters
(from around 10 to around 60 or 70 depending on the size of the model) while
data are sparse (in the best case scenario, the US economy, we have 216 quarters
of data from 1954 to 2007).4 Under this low ratio data/parameters, the desirable
small sample properties of Bayesian methods and the inclusion of pre-sample
information are peculiarly attractive.
Third, and possibly as a consequence of the argument above, the likelihoods
of DSGE models tend to be wild, with dozens of maxima and minima that defeat
the best optimization algorithms. McMc are a robust and simple procedure
to get around, or at least alleviate, these problems (so much so, that some
econometricians who still prefer a classical approach to inference recommend
the application of McMc algorithms by adopting a â€˜pseudo-Bayesianâ€™ stand; see
Chernozhukov and Hong, 2003). Our own experience has been that models
that we could estimate in a few days using McMC without an unusual effort
turned out to be extremely difï¬cult to estimate using maximum likelihood.
Even the most pragmatic of economists, with the lowest possible interest
in axiomatic foundations of inference, ï¬nds this feasibility argument rather
compelling.
4 Data before 1954 are less reliable and the structure of the economy back then was sufï¬ciently different
as to render the estimation of the same model problematic. Elsewhere (FernÃ¡ndez-Villaverde and Rubio-
RamÃ­rez, 2008), we have argued that a similar argument holds for data even before 1981. For most countries
outside the US, the situation is much worse, since we do not have reliable quarterly data until the 1970s or
even the 1980s.

The New Macroeconometrics
371
Fourth, the Bayesian approach deals in a transparent way with misspeciï¬ca-
tion and identiï¬cation problems, which are pervasive in the estimation of DSGE
models (Canova and Sala, 2006). After all, a DSGE model is a very stylized
and simpliï¬ed view of the economy that focuses only on the most important
mechanisms at play. Hence, the model is false by construction and we need to
keep this notion constantly in view, something that the Bayesian way of thinking
has an easier time with than frequentist approaches.
Finally, Bayesian analysis has a direct link with decision theory. The connec-
tion is particularly relevant for DSGE models since they are used for policy
analysis. Many of the relevant policy decisions require an explicit consideration
of uncertainty and asymmetric risk assessments. Most economists, for example,
read the empirical evidence as suggesting that the costs of a 1% deï¬‚ation are
considerably bigger than the costs of 1% inï¬‚ation. Hence, a central bank with
a price stability goal (such as the Federal Reserve System in the US or the
European Central Bank) faces a radically asymmetric loss function with respect
to deviations from the inï¬‚ation target.
To illustrate this lengthy discussion, we will present a benchmark New Key-
nesian DSGE model, we will estimate it with US data, and we will discuss
our results. We will close by outlining three active areas of research in the
estimation of DSGE models that highlight some of the challenges that we see
ahead of us for the next few years.
15.2 A benchmark new Keynesian model
Due to space constraints, we cannot present a benchmark New Keynesian
DSGE model in all its details. Instead, we will just outline its main features
to provide a ï¬‚avour of what the model is about and what it can and cannot
deliver. In particular, we will omit many discussions of why are we doing
things the way we do. We will ask the reader to trust that many of our
choices are not arbitrary but are the outcome of many years of discussion in
the literature. The interested reader can access the whole description of the
model at a complementary technical appendix posted at www.econ.upenn.
edu/âˆ¼jesusfv/benchmark_DSGE.pdf. The model we select embodies what
is considered the current standard of New Keynesian macroeconomics (see
Christiano, Eichenbaum and Evans, 2005) and it is extremely close to the
models being employed by several central banks as inputs for policy decisions.
The agents in the model will include (a) households that consume, save,
and supply labour to a labour â€˜packerâ€™, (b) a labour â€˜packerâ€™ that puts together
the labour supplied by different households into an homogeneous labour unit,
(c) intermediate good producers, who produce goods using capital and aggre-
gated labour, (d) a ï¬nal good producer that mixes all the intermediate goods

372
The Oxford Handbook of Applied Bayesian Analysis
into a ï¬nal good that households consume or invest in, and (e) a government
that sets monetary policy through open market operations. We present in turn
each type of agents.
15.2.1 Households
There is a continuum of inï¬nitely lived households in the economy indexed
by j. The households maximize their utility function, which is separable in
consumption, c jt, real money balances (nominal money, mo jt, divided by the
aggregate price level, pt), and hours worked, l jt:
E0
âˆž

t=0
â€štedt

log

c jt âˆ’hc jtâˆ’1

+ Ä± log
mo jt
pt

âˆ’eÏ•tÂ¯
l 1+Ë‡
jt
1 + Ë‡

where â€š is the discount factor, h controls habit persistence, Ë‡ is the inverse of the
Frisch labour supply elasticity (how much labour supply changes when the wage
changes while keeping consumption constant), dt is a shock to intertemporal
preference that follows the AR(1) process:
dt = Ã’ddtâˆ’1 + eÃ›dÎµd,t where Îµd,t âˆ¼N(0, 1),
and Ï•t is a labour supply shock that also follows an AR(1) process:
Ï•t = Ã’Ï•Ï•tâˆ’1 + eÃ›Ï•ÎµÏ•,t where ÎµÏ•,t âˆ¼N(0, 1).
These two shocks, Îµd,t and ÎµÏ•,t, are equal across all agents.
Households trade on assets contingent on idiosyncratic and aggregate events.
By a jt+1 we indicate the amount of those securities that pay one unit of con-
sumption at time t + 1 in event Ë˜ j,t+1,t purchased by household j at time t at
real price q jt+1,t. These one-period securities are sufï¬cient to span the whole
set of ï¬nancial contracts regardless of their duration. Households also hold an
amount b jt of government bonds that pay a nominal gross interest rate of Rt
and invest a quantity xt of the ï¬nal good. Thus, the jth householdâ€™s budget
constraint is given by:
c jt + xjt + mo jt
pt
+ b jt+1
pt
+

q jt+1,ta jt+1dË˜ j,t+1,t
= w jtl jt +

rtu jt âˆ’Ãâˆ’1
t 

u jt

k jtâˆ’1 + mo jtâˆ’1
pt
+ Rtâˆ’1
b jt
pt
+ a jt + Tt + Ït (15.1)
where w jt is the real wage, rt the real rental price of capital, u jt > 0 the intensity
of use of capital, Ãâˆ’1
t [u jt] is the physical cost of u jt in resource terms (where
[u] = 1(u âˆ’1) + 2/2(u âˆ’1)2 and 1, 2 â‰¥0), Ãt is an investment-speciï¬c
technological shock that we will describe momentarily, Tt is a lump-sum trans-
fer from the government, and Ït is the household share of the proï¬ts of the
ï¬rms in the economy.

The New Macroeconometrics
373
Given a depreciation rate â€° and a quadratic adjustment cost function
V
8 xt
xtâˆ’1
9
= Ã
2
 xt
xtâˆ’1
âˆ’x
2
,
where Ã â‰¥0 and x is the long-run growth of investment, capital evolves as:
k jt = (1 âˆ’â€°) k jtâˆ’1 + Ãt

1 âˆ’V
8 xjt
xjtâˆ’1
9
xjt.
Note that in this equation there appears an investment-speciï¬c technological
shock Ãt that also follows an autoregressive process:
Ãt = Ãtâˆ’1 exp

Ã + zÃ,t

where zÃ,t = Ã›ÃÎµÃ,t and ÎµÃ,t âˆ¼N(0, 1).
Thus, the problem of the household is a Lagrangian function formed by the
utility function, the law of motion for capital, and the budget constraint. The
ï¬rst order conditions of this problem with respect to c jt, b jt, u jt, k jt, and xjt are
standard (although messy), and we refer the reader to the online appendix for a
detailed exposition.
The ï¬rst order condition with respect to labor and wages is more involved
because, as we explained in the introduction, households will face rigidities
in changing their wages.5 Each household supplies a slightly different type
of labour service (for example, some households write chapters on Bayesian
estimation of DSGE models and some write chapters on hierarchical models)
that is aggregated by a labour â€˜packerâ€™ (in our example, an editor) into an
homogeneous labor unit (â€˜Bayesian researchâ€™) according to the function:
ld
t =
 1
0
l
Ãâˆ’1
Ã
jt d j

Ã
Ãâˆ’1
(15.2)
where Ã controls the elasticity of substitution among different types of labour
and l d
t is the aggregate labour demand. The technical role that the labour
â€˜packerâ€™ plays in the model is to allow for the existence of many different types
of labour, and hence for the possibility of different wages, while keeping the
heterogeneity of agents at a tractable level.
The labour â€˜packerâ€™ takes all wages wjt of the differentiated labor and the
wage of the homogeneous labor unit wt as given and maximizes proï¬ts subject
to the production function (15.2). Thus, the ï¬rst order conditions of the labour
5 In this type of model, households set up their wages and ï¬rms decide how much labour to hire. We
could also have speciï¬ed a model where wages are posted by ï¬rms and households decide how much to
work. For several technical reasons, our modelling choice is easier to handle. If the reader has problems
seeing a household setting its wage, she can think of the case of an individual contractor posting a wage
for hour worked, or a union negotiating a contract in favour of its members.

374
The Oxford Handbook of Applied Bayesian Analysis
â€˜packerâ€™ are:
l jt =
w jt
wt
âˆ’Ã
ld
t
âˆ€j.
(15.3)
We assume that there is free entry into the labour packing business. After a few
substitutions, we get an expression for the wage wt as a function of the different
wages w jt:
wt =
 1
0
w1âˆ’Ã
jt d j

1
1âˆ’Ã
.
The way in which households set their wages is through a device called Calvo
pricing (for the economist Guillermo Calvo, who proposed this formulation in
1983). In each period, the household has a probability 1 âˆ’Ã‹w of being able to
change its wages. Otherwise, it can only partially index its wages by a fraction
Ëœw âˆˆ[0, 1] of past inï¬‚ation. Therefore, if a household has not been able to
change its wage for Ã™ periods, its real wage is
Ã™
s=1
Ëœw
t+sâˆ’1
t+s
w jt,
the original wage times the indexation divided by the accumulated inï¬‚ation.
This probability 1 âˆ’Ã‹w represents a streamlined version of a more subtle expla-
nation of wage rigidities (based on issues like contracts costs, Caplin and Leahy,
1991, or information limitations, Sims, 2002), which we do not ï¬‚esh out entirely
in the model to keep tractability.6
The average duration of a wage decision in this economy will be 1/ (1 âˆ’Ã‹w) ,
although all wages change period by period because of the presence of index-
ation (in that sense the economy would look surprisingly ï¬‚exible to a naÃ¯ve
observer!). Since a suitable law of large numbers holds for this economy, the
probability of changing wages, 1 âˆ’Ã‹w, will also be equal to the fraction of
households reoptimizing their wages in each period.
Calvo pricing implies three results. First, the form of the utility function that
we selected and the presence of complete asset markets deliver the remarkable
result that all households that choose their wage in this period will choose
exactly the same wage, wâˆ—
t (also, the consumption, investment, and Lagrangian
multiplier ÃŽt of the budget constraint of all households will be the same and we
drop the subindex j when no confusion arises).
Second, in every period, a fraction 1 âˆ’Ã‹w of households set wâˆ—
t as their wage,
while the remaining fraction Ã‹w partially index their price by past inï¬‚ation.
6 There is, however, a discussion in the literature regarding the potential shortcomings of Calvo pricing.
See, for example, Dotsey, King and Wolman (1999), for a paper that prefers to be explicit about the problem
faced by agents when changing prices.

The New Macroeconometrics
375
Consequently, the real wage index evolves as:
w1âˆ’Ã
t
= Ã‹w

Ëœw
tâˆ’1
t
1âˆ’Ã
w1âˆ’Ã
tâˆ’1 + (1 âˆ’Ã‹w) wâˆ—1âˆ’Ã
t
.
Third, after a fair amount of algebra, we can show that the evolution of wâˆ—
t and
an auxiliary variable ft are governed by two recursive equations:
ft = Ã âˆ’1
Ã

wâˆ—
t
1âˆ’Ã ÃŽtwÃ
t ld
t + â€šÃ‹wEt

Ëœw
t
t+1
1âˆ’Ã wâˆ—
t+1
wâˆ—
t
Ãâˆ’1
ft+1
and:
ft = Â¯dtÏ•t
 wt
wâˆ—
t
Ã(1+Ë‡) 
ld
t
1+Ë‡ + â€šÃ‹wEt

Ëœw
t
t+1
âˆ’Ã(1+Ë‡) wâˆ—
t+1
wâˆ—
t
Ã(1+Ë‡)
ft+1.
15.2.2 The ï¬nal good producer
The ï¬nal good producer aggregates all intermediate goods into a ï¬nal good with
the following production function:
yd
t =
 1
0
y
Îµâˆ’1
Îµ
it di

Îµ
Îµâˆ’1
.
(15.4)
where Îµ controls the elasticity of substitution.
The role of the ï¬nal good producer is to allow for the existence of many ï¬rms
with differentiated products while keeping a simple structure in the hetero-
geneity of products and prices. Also, the ï¬nal good producer takes as given
all intermediate goods prices pti and the ï¬nal good price pt, which implies a
demand function for each good
yit =
 pit
pt
âˆ’Îµ
yd
t
âˆ€i,
where yd
t is the aggregate demand and by a zero proï¬t condition:
pt =
 1
0
p1âˆ’Îµ
it
di

1
1âˆ’Îµ
.
15.2.3 Intermediate good producers
There is a continuum of intermediate good producers. Each intermediate good
producer i has access to a technology described by a production function of the
form yit = AtkÂ·
itâˆ’1

ld
it
1âˆ’Â· âˆ’Ë†zt where kitâˆ’1 is the capital rented by the ï¬rm,
l d
it is the amount of the homogeneous labour input rented by the ï¬rm, the
parameter Ë† corresponds to the ï¬xed cost of production, and where At follows

376
The Oxford Handbook of Applied Bayesian Analysis
a unit root process in logs, At = Atâˆ’1 exp (A + zA,t) where zA,t = Ã›AÎµA,t and
ÎµA,t âˆ¼N(0, 1).
The ï¬xed cost Ë† is scaled by the variable zt = A
1
1âˆ’Â·
t
Ã
Â·
1âˆ’Â·
t
to guarantee that
economic proï¬ts are roughly equal to zero. The variable zt evolves over time as
zt = ztâˆ’1 exp (z + zz,t) where zz,t = zA,t+Â·zÃ,t
1âˆ’Â·
and z = A+Â·Ã
1âˆ’Â· . We can also prove
that z is the mean growth rate of the economy.
Intermediate good producers solve a two-stage problem. First, given wt and
rt, they rent ld
it and kitâˆ’1 to minimize real costs of production, which implies a
marginal cost of:
mct =

1
1 âˆ’Â·
1âˆ’Â· 1
Â·
Â· w1âˆ’Â·
t
r Â·
t
At
.
This marginal cost does not depend on i: all ï¬rms receive the same shocks and
rent inputs at the same price.
Second, intermediate good producers choose the price that maximizes dis-
counted real proï¬ts under the same Calvo pricing scheme as households. The
only difference is that now the probability of changing prices is given by 1 âˆ’Ã‹p
and the partial indexation by the parameter Ëœ âˆˆ[0, 1]. We will call pâˆ—
t the price
that intermediate good producers select when they are allowed to optimize their
prices at time t.
Again, after a fair amount of manipulation, we ï¬nd that the price index
evolves as:
p1âˆ’Îµ
t
= Ã‹p

Ëœ
tâˆ’1
1âˆ’Îµ p1âˆ’Îµ
tâˆ’1 +

1 âˆ’Ã‹p

pâˆ—1âˆ’Îµ
t
and that pâˆ—
t is determined by two recursive equations in the auxiliary variable
g 1
t and g 2
t :
g 1
t = ÃŽtmct yd
t + â€šÃ‹pEt
 Ëœ
t
t+1
âˆ’Îµ
g 1
t+1
g 2
t = ÃŽtâˆ—
t yd
t + â€šÃ‹pEt
 Ëœ
t
t+1
1âˆ’Îµ  âˆ—
t
âˆ—
t+1

g 2
t+1
where Îµg 1
t = (Îµ âˆ’1)g 2
t and âˆ—
t = pâˆ—
t /pt.
15.2.4 The government
The government plays an extremely limited role in this economy. It sets the
nominal interest rates according to the following policy rule:
Rt
R =
 Rtâˆ’1
R
â€žR
âŽ›
âŽ
t

â€ž
âŽ›
âŽ
yd
t
yd
tâˆ’1
ez
âŽž
âŽ 
â€žyâŽž
âŽ 
1âˆ’â€žR
emt.
(15.5)

The New Macroeconometrics
377
The policy is implemented through open market operations that are ï¬nanced
with lump-sum transfers Tt to ensure that the government budget is bal-
anced period by period. The variable  is the target level of inï¬‚ation, R
the steady-state gross return of capital, and z, as mentioned above, is the
average gross growth rate of yd
t (these last two variables are determined by
the model, not by the government). The term mt is a random shock to
monetary policy that follows mt = Ã›mÎµmt where Îµmt is distributed according to
N(0, 1).
The intuition behind the policy rules, motivated on a large body of empirical
literature (Orphanides, 2002), is that a good way to describe the behavior of
the monetary authority is to think about central banks as setting up the (short-
term) interest rate as a function of the past interest rate, Rtâˆ’1, the deviation of
inï¬‚ation from a target, and the deviation of output growth from the long-run
average growth rate.
15.2.5 Aggregation
With some additional work, we can sum up the behaviour of all agents in the
economy to ï¬nd expressions for the remaining aggregate variables, including
aggregate demand:
yd
t = ct + xt + Ãâˆ’1
t  [ut] ktâˆ’1
aggregate supply:
ys
t = At (utktâˆ’1)Â· 
ld
t
1âˆ’Â· âˆ’Ë†zt
v p
t
where:
v p
t =
 1
0
 pit
pt
âˆ’Îµ
di
and aggregate labour supply l d
t = lt/vw
t where
vw
t =
 1
0
w jt
wt
âˆ’Ã
d j.
The terms v p
t and vw
t represent the loss of efï¬ciency induced by price and wage
dispersion. Since all households and ï¬rms are symmetrical, a social planner
would like to set every price and wage at the same level. However, the nominal
rigidities prevent this socially optimal solution. By the properties of the index

378
The Oxford Handbook of Applied Bayesian Analysis
under Calvoâ€™s pricing, v p
t and vw
t evolve as:
v p
t = Ã‹p

Ëœ
tâˆ’1
t
âˆ’Îµ
v p
tâˆ’1 +

1 âˆ’Ã‹p

âˆ—âˆ’Îµ
t
vw
t = Ã‹w

wtâˆ’1
wt
Ëœw
tâˆ’1
t
âˆ’Ã
vw
tâˆ’1 + (1 âˆ’Ã‹w) 
wâˆ—
t
âˆ’Ã .
15.2.6 Equilibrium
The equilibrium of this economy is characterized by the ï¬rst order conditions
of the household, the ï¬rst order conditions of the ï¬rms, the policy rule of the
government, and the aggregate variables that we just presented, with a total
of 21 equations (the web appendix has the list of all of these 21 equations).
Formally, we can think about them as a set of nonlinear functional equations
where the unknowns are the functions that determine the evolution over time
of the variables of the model. The variables that enter as arguments of these
functions are the 19 state variables that determine the situation of the system at
any point in time.
A simple inspection of the system reveals that it does not have an analytical
solution. Instead, we need to resort to some type of numerical approxima-
tion to solve it. The literature on how to do so is enormous, and we do not
review it here (see, instead, the comprehensive textbook by Judd, 1998). In
previous work (Aruoba, FernÃ¡ndez-Villaverde, and Rubio-RamÃ­rez, 2006), we
convinced ourselves that a nice compromise between speed and accuracy in
the solution of systems of this sort can be achieved by the use of perturbation
techniques.
The foundation of perturbation methods is to substitute the original sys-
tem by a suitable variation of it that allows for an analytically (or at least
numerically) exact solution. In our particular case, this variation of the prob-
lem is to set the standard deviations of all of the modelâ€™s shocks to zero.
Under this condition, the economy converges to a balanced growth path,
where all the variables grow at a constant (but potentially different) rate. Then,
we use this solution and a functional version of the implicit function theo-
rem to compute the leading coefï¬cients of the Taylor expansion of the exact
solution.
Traditionally, the literature has focused on the ï¬rst order approximation to
the solution because it generates a linear representation of the model whose
likelihood is easily evaluated using the Kalman ï¬lter. Since this chapter is a
brief introduction to the new macroeconometrics, we will follow that conven-
tion. However, we could also easily compute a second order approximation.
Higher order approximations have the advantage of being more accurate and
of capturing precautionary behaviour among agents induced by variances.

The New Macroeconometrics
379
We have found in our own research that in many problems (but not in all!),
those second order terms are important.
Further elaborating this point, note that we end up working with an approx-
imation of the solution of the model and not with the solution itself raises
an interesting technical issue. Even if we were able to evaluate the likelihood
implied by that solution, we would not be evaluating the likelihood of the exact
solution of the model but the likelihood implied by the approximated solution
of the model. Both objects may be quite different and some care is required
when we proceed to perform inference (FernÃ¡ndez-Villaverde, Rubio-RamÃ­rez
and Santos, 2006).
However, before digging into the details of the solution method, and since
we have technological progress, we need to rescale nearly all variables to make
them stationary. To do so, deï¬ne <ct = ct
zt , <ÃŽt = ÃŽtzt, <rt = rtÃt, <qt = qtÃt, <xt = xt
zt ,
<wt = wt
zt , <wâˆ—
t = wâˆ—
t
zt , <kt =
kt
ztÃt , and <yd
t = yd
t
zt . Also note that c = x = w = wâˆ—=
yd = z. Furthermore, we normalize u = 1 in the steady state by setting 1 =
<r, eliminating it as a free parameter.
For each variable vart, we deï¬ne =
var t = vart âˆ’var, as the deviation with
respect to the steady state. Then, the states of the model St are given by:
St =
 tâˆ’1, <wtâˆ’1,g 1
tâˆ’1,g 2
tâˆ’1,<ktâˆ’1, Rtâˆ’1,<y
d
tâˆ’1,<ctâˆ’1,v p
tâˆ’1,vw
tâˆ’1,
<q tâˆ’1,<f tâˆ’1,<xtâˆ’1,<ÃŽtâˆ’1,<ztâˆ’1, zÃ,tâˆ’1,dtâˆ’1,Ï•tâˆ’1, zA,tâˆ’1
â€²
,
and the exogenous shocks are Îµt =

ÎµÃ,t, Îµd,t, ÎµÏ•,t, ÎµA,t, Îµm,t
â€² .
From the output of the perturbation, we build the law of motion for the states:
St+1 = 
s1

S
â€²
t, Îµâ€²
t
â€²
(15.6)
where 
s1 is a 19Ã—24 matrix, and the law of motion for the observables:
Yt = 
o1

Sâ€²
t, Îµâ€²
t
â€²
(15.7)
where St =

S
â€²
t, S
â€²
tâˆ’1, Îµâ€²
tâˆ’1

and 
o1 is a 5Ã—48 matrix. We include lagged states
in our observation equation, because some of our data will appear in ï¬rst
differences. Equivalently, we could have added lagged observations to the states
to accomplish the same objective. Also, note that the function (15.7) includes
a constant that captures the mean of the observables as implied by the equilib-
rium of the model.
Our observables are the ï¬rst differences of the relative price of investment,
output, real wages, inï¬‚ation, and the federal funds rate, or in our notation:
Yt =

â–³log Ãâˆ’1
t , â–³log yt, â–³log wt, log t, log Rt
â€² .
While the law of motion for states is unique (or at least belonging to an equiv-
alence class of representations), the observation equation depends on what the

380
The Oxford Handbook of Applied Bayesian Analysis
econometrician actually observes or chooses to observe. Since little is known
about how those choices affect our estimation, we have selected the time series
that we ï¬nd particularly informative for our purposes (see GuerrÃ³n-Quintana,
2008, for a detailed discussion).
For observables, we can only select a number of series less than or equal to
the number of shocks in the model. Otherwise, the model will be stochastically
singular, i.e. we could write the extra observables as deterministic functions
of the other observables and the likelihood would be âˆ’âˆž. In the jargon of
macroeconomics, these functions are part of the equilibrium cross-equation
restrictions.
A popular way around the problem of stochastic singularity has been to
assume that the observables come with measurement error. This assumption
delivers, by itself, one shock per observable. There are three justiï¬cations for
measurement error. First, statistical agencies make mistakes. Measuring US
output or wages is a daunting task, undertaken with extremely limited resources
by different government agencies. Despite their remarkable efforts, statistical
agencies can only provide us with an estimate of the series we need. Second,
there are differences between the deï¬nitions of variables in the theory and
in the data, some caused by divergent methodological choices (see, for instance,
the discussion in Appendix B about how to move from nominal output into real
output), some caused by the limitations of the data the statistical agency can
collect. Finally, measurement errors may account for parts of the dynamic of
the data that are not captured by the model.
On the negative side, including measurement error complicates identiï¬ca-
tion and faces the risk that the data ends up being explained by this mea-
surement error and not by the model itself. In our own research we have
estimated DSGE models with and without measurement error, according to
the circumstances of the problem. In this chapter, since we have a rich DSGE
economy with many sources of uncertainty, we assume that our data come
without measurement error. This makes the exercise more transparent and
easier to follow.
15.2.7 The likelihood function
Equations (15.6) and (15.7) plus the deï¬nition of St are the state space repre-
sentation of a dynamic model. It is well understood that we can use this state
representation to ï¬nd the likelihood L (Y1:T; 
) of our DSGE model, where we
have stacked all parameter values in:

 =

â€š, h, Ä±, Ë‡, â€°, Ã, Îµ, Â·, Ë†, Ã‹w, Ëœw, Ã‹p, Ëœ p, 2, â€žR, â€žy, â€ž,
, Ã, A, Ã’d, Ã’Ï•, Ã›Ã, Ã›d, Ã›A, Ã›m, Ã›Ï•

.
and where Y1:T is the sequence of data from period 1 to T.

The New Macroeconometrics
381
To perform this evaluation of the likelihood function given some parameter
values, we start by factorizing the likelihood function as:
L (Y1:T; 
) =
T

t=1
L (Yt|Y1:Tâˆ’1; 
) .
Then:
L (Y1:T; 
) =

L (Y1|S0; 
) dS0
T

t=2

L (Yt|St; 
) p (St|Y1:Tâˆ’1; 
) dSt.
(15.8)
If we know St, computing L (Yt|St; 
) is conceptually simple since, conditional
on St, the measurement equation (15.7) is a change of variables from Îµt to Yt.
Similarly, if we know S0, it is easy to compute L (Y1|S0; 
) with the help of
(15.6) and (15.7). Thus, all that remains to evaluate the likelihood is to ï¬nd the
sequence {p (St|Y1:tâˆ’1; 
)}T
t=1 and the initial distribution p (S0; 
).
This second task is comparatively simple. There are two procedures for doing
so. First, in the case where we can characterize the ergodic distribution, for
example, in linearized models, we can draw directly from it (this procedure
takes advantage of the stationarity of the model achieved by the rescaling of
variables). In our model, we just need to set the states to zero (remember, they
are expressed in differences with respect to the steady state) and specify their
initial variance-covariance matrix. Second, if we cannot characterize the ergodic
distribution, we can simulate the model for a large number of periods (250 for
a model like ours may sufï¬ce). Under certain regularity conditions, the values
of the states at period 250 are a draw from the ergodic distribution of states.
Consequently, the only remaining barrier is to characterize the sequence of
conditional distributions {p (St|Y1:Tâˆ’1; 
)}T
t=1 and to compute the integrals in
(15.8).
Since (a) we have performed a ï¬rst order perturbation of the equilibrium
equations of the problem and generated a linear state space representation,
and (b) our shocks are normally distributed, the sequence {p (St|Y1:Tâˆ’1; 
)}T
t=1
is composed of conditionally Gaussian distributions. Hence, we can use
the Kalman ï¬lter, ï¬nd {p (St|Y1:Tâˆ’1; 
)}T
t=1 , and evaluate the likelihood in
a quick and efï¬cient way. A version of the Kalman ï¬lter applicable to our
setup is described in Appendix B. The Kalman ï¬lter is extremely efï¬cient:
it takes less than one second to run for each combination of parameter
values.
However, in many cases of interest, we may want to perform a higher order
perturbation or the shocks to the model may not be normally distributed. Then,
the components of {p (St|Y1:Tâˆ’1; 
)}T
t=1 would not be conditionally Gaussian.
In fact, in general, it will not belong to any known parametric family of

382
The Oxford Handbook of Applied Bayesian Analysis
density functions. In all of those cases, we can track the desired sequence
{p (St|Y1:Tâˆ’1; 
)}T
t=1 using SMC methods (see Appendix A).
Once we have the likelihood of the model L (Y1:T; 
) , we combine it with a
prior density for the parameters p (
) to form a posterior
p (
| Y1:T) âˆL (Y1:T; 
) p (
) .
Since the posterior is also difï¬cult to characterize (we do not even have an
expression for the likelihood, only an evaluation), we generate draws from it
using a random walk Metropolis â€“ Hastings algorithm. The scaling matrix of
the proposal density will be selected to generate the appropriate acceptance ratio
of proposals (Roberts, Gelman and Gilks, 1997).
We can use the resulting empirical distribution to obtain point estimates,
variances, etc., and to build the marginal likelihood that is a fundamental com-
ponent in the Bayesian comparison of models (see FernÃ¡ndez-Villaverde and
Rubio-RamÃ­rez, 2004, for an application of the comparison of dynamic models
in economics). An important advantage of having draws from the posterior
is that we can also compute other objects of interest, like the posterior of the
welfare cost of business cycle ï¬‚uctuations or the posterior of the responses of
the economy to an unanticipated shock. These objects are often of more interest
to economists than the parameter themselves.
15.3 Empirical analysis
Now we are ready to take our model to the data and obtain point estimates and
posterior distributions. Again, because of space limitations, our analysis should
be read more as an example of the type of exercises that can be implemented
than as an exhaustive investigation of the empirical properties of the model.
Furthermore, once we have parameter posteriors, we can undertake many
additional exercises of interest, like forecasting (where is the economy going?),
counterfactual analysis (what would have happen if?), policy design (what is
the best way to respond to shocks?), and welfare analysis (what is the cost of
business cycle ï¬‚uctuations?).
We estimate our New Keynesian model using ï¬ve time series for the US
economy: (1) the relative price of investment with respect to the price of con-
sumption, (2) real output per capita growth, (3) real wages per capita growth,
(4) the consumer price index growth, and (5) the federal funds rate (the inter-
est rate at which banks lend balances at the Federal Reserve System to each
other, usually overnight). Our sample goes from 1959Q1â€“2007Q1. Appendix B
explains how we construct these series.
Before proceeding further, we specify priors for the parameters. We will have
two sets of priors that will provide us with two sets of posteriors. First, we adopt

The New Macroeconometrics
383
ï¬‚at priors for all parameters except a few that we ï¬x at some predetermined
values. These ï¬‚at priors are modiï¬ed only by imposing boundary constraints to
make the priors proper and to rule out parameter values that are incompatible
with the model (i.e. a negative value for a variance or Calvo parameters outside
the unit interval). Second, we use a set of priors nearly identical to the one
proposed by Smets and Wouters (2007) in a highly inï¬‚uential study in which
they estimate a similar DSGE model to ours.
Our ï¬rst exercise with ï¬‚at priors is motivated by the observation that, with
this prior, the posterior is proportional to the likelihood function.7 Conse-
quently, our Bayesian results can be interpreted as a classical exercise where
the mode of the likelihood function (the point estimate under an absolute value
loss function for estimation) is the maximum likelihood estimate. Moreover, a
researcher who prefers alternative priors can always reweight the draws from
the posterior using importance sampling to encompass his favorite priors.
We do not argue that our ï¬‚at priors are uninformative. After a reparame-
terization of the model, a ï¬‚at prior may become highly curved. Instead, we
have found in our research that an estimation with ï¬‚at priors is a good ï¬rst
step to learn about the amount of information carried by the likelihood and
a natural benchmark against which to compare results obtained with nonï¬‚at
priors. The main disadvantage of this ï¬rst exercise is that, in the absence of
further information, it is extremely difï¬cult to get the McMc to converge and
characterize the posterior accurately. The number of local modes is so high that
the chain wanders away for the longest time without settling into the ergodic
distribution.
In comparison, our second exercise, with Smets and Woutersâ€™ (2007) pri-
ors, is closer to the exercises performed at central banks and other policy
institutions. The priors bring a considerable amount of additional information
that allows us to achieve much more reasonable estimates. However, the use
of more aggressive priors requires a careful preparation by the researcher,
and in the case of institutional models, a candid conversation with the
principal regarding the assumptions about parameters she feels comfortable
with.
15.3.1 Flat priors
Now we present results from our exercise with ï¬‚at priors. Before reporting
results, we ï¬x some parameter values at the values reported in Table 15.1. In
a Bayesian context, we can think about ï¬xing parameter values as having Dirac
priors over them.
7 There is a small qualiï¬er: the bounded support of some of the priors. We can eliminate this small
difference by thinking about those bounds as frontiers of admissible parameter values in a classical
perspective.

384
The Oxford Handbook of Applied Bayesian Analysis
Table 15.1 Fixed parameters.
â€š
h
Â¯
Ë‡
â€°
Â·
Îµ
Ã
Ã
Ë†
2
0.99
0.9
9
1.35
0.015
0.30
8
8
30
0
0.001
Macroeconomists ï¬x some parameters for different reasons. One reason
is related to the amount of data: being too ambitious in estimating all the
parameters in the model with limited data sometimes delivers posteriors that
are too spread for useful interpretation and policy analysis. A related reason is
that some parameters are poorly identiï¬ed with macro data (for example, the
elasticity of output with respect to capital), while, at the same time, we have good
sources of information from micro data not used in the estimation. It seems,
then, reasonable to set these parameters at the conventional values determined
by microeconometricians. In our ï¬rst exercise with ï¬‚at priors, the number of
parameters that we need to ï¬x is relatively high, 11. Otherwise, the likelihood
has too many local peaks, and it turns out to be extremely difï¬cult to design a
successful McMC.
We brieï¬‚y describe some of these parameter values in Table 15.1. The dis-
count factor, â€š, is ï¬xed at 0.99, a number close to one, to match the low risk
free rate observed in the US economy (the gross risk free rate depends on the
inverse of â€š and the growth rate of the economy). Habit persistence, h = 0.9,
arises because we want to match the evidence of a sluggish adjustment of
consumption decisions to shocks. The parameter Ë‡ pins down a Frisch elasticity
of 0.74, a value well within the range of the ï¬ndings of microeconometrics
(Browning, Hansen and Heckman, 1999). Our choice of â€°, the depreciation
rate, matches the capital-output ratio in the data. The elasticity of capital to
output, Â· = 0.3, is informed by the share of national income that goes to capital.
The elasticity of substitution across intermediate goods, Îµ, accounts for the
microeconomic evidence on the average mark-up of US ï¬rms, which is esti-
mated to be around 10â€“15 percent (the mark-up in our model is approximately
Îµ/(Îµ âˆ’1)). By symmetry, we pick the same value for Ã. We take a high value
for the adjustment cost Ã, 30, to dampen the ï¬‚uctuations of investment in the
model and get them closer to the behaviour of investment in the data. We set Ë†
to zero, since we do not have information on pure proï¬ts by ï¬rms. Luckily, since
we do not have entry and exit of ï¬rms in the model, the parameter is nearly
irrelevant for equilibrium dynamics. Finally, the parameter controlling money
demand Ä± does not affect the dynamics of the model because the monetary
authority will supply as much money as required to implement the nominal
interest rate determined by the policy rule.
We summarize the posterior distribution of the parameters in Table 15.2,
where we report the median of each parameter and its 5 and 95 percentile
values and in Figure 15.1, where we plot the histograms of the draws of each

The New Macroeconometrics
385
Table 15.2 Median estimated parameters (5 and 95 percentile in parentheses).
Ã‹p
Ëœ
Ã‹w
Ëœw
â€žR
â€žy
â€žï£¿

0.72
[0.68,
0.77]
0.94
[0.81,
0.99]
0.62
[0.56,
0.70]
0.92
[0.73,
0.99]
0.87
[0.83,
0.90]
0.99
[0.59,
1.73]
2.65
[2.10,
3.67]
1.012
[1.011,
1.013]
Ã’d
Ã’Ï•
Ã›A
Ã›d
Ã›Ï•
Ã›Ã
Ã›e
Ã
A
0.14
[0.02,
0.27]
0.91
[0.86,
0.93]
âˆ’4.45
[âˆ’4.61,
âˆ’4.29]
âˆ’2.53
[âˆ’2.64,
âˆ’2.43]
âˆ’2.29
[âˆ’2.59,
âˆ’1.74]
âˆ’5.49
[âˆ’5.57,
âˆ’5.40]
âˆ’5.97
[âˆ’6.09,
âˆ’5.84]
3eâˆ’3
[0.002,
0.004]
1eâˆ’4
[0.00,
0.0003]
parameter. The results are based on a chain of 75,000 draws, initialized after a
considerable amount of search for good initial starting values, and a 30 percent
acceptance rate. We performed standard analysis of convergence of the chain to
ensure that the results are accurate.
We highlight a few results. The Calvo parameter for prices, Ã‹p, is closely
estimated to be around 0.72. This indicates that ï¬rms revise their pricing deci-
sions around 3.5 quarters on average, a number that seems close to a natural
benchmark of a yearly pricing cycle in many ï¬rms complemented with some
ï¬rms that revise their prices more often. The indexation level, Ëœ, of 0.94, has
a higher degree of uncertainty but also suggests that ï¬rms respond quickly to
inï¬‚ation. High indexation generates a high level of inï¬‚ation inertia, and with
it, a bigger role for monetary policy. The Calvo parameter for wages, Ã‹w, implies
wage decisions every 2.6 quarters, also with high indexation.
The parameters of the policy rule show that the monetary authority smooths
to a large degree the evolution of the federal funds rate, â€žR is equal to 0.87,
cares about the growth level of the economy, â€žy is equal to 0.99, and ï¬nally,
is rather aggressive against inï¬‚ation, â€žï£¿is equal to 2.65. This last parameter
value is important, because it implies that the monetary authority respects
the â€˜Taylor principleâ€™ that requires nominal interest rates to rise more than
inï¬‚ation rises. Otherwise, the real interest rate falls, inï¬‚ation rises, and we
generate bad policy outcomes due to the indeterminacy of equilibria (Lubick
and Schorfheide, 2004). The inï¬‚ation target of 1 percent per quarter is higher
than the stated goals of the Federal Reserve but consistent with the behavior of
inï¬‚ation in the sample.
The standard deviations of the ï¬ve shocks in the economy show the impor-
tance of the preference shocks and the higher importance of the neutral techno-
logical shock in comparison with the investment-speciï¬c technological shock
in accounting for ï¬‚uctuations in the economy. In comparison, the posterior
clearly indicated that the average growth of the economy is driven by investment-
speciï¬c technological change, whose mean, Ã, is an order of magnitude bigger
than neutral technological change, A. These ï¬ndings are similar to the ones

386
The Oxford Handbook of Applied Bayesian Analysis
0.6
0.7
0.8
0
5000
10000
15000
qp
qw
0.7
0.8
0.9
0
5000
10000
15000
c
0.6
0.7
0.8
0
5000
10000
15000
0.4 0.6 0.8
0
0.5
1
1.5
2
2.5
âˆ‘ 104
c
w
0.8 0.85
0.9
0
5000
10000
gr
gp
Ã•
rd
sm
Lm
se
sa
sd
gy
0.5
1 1.5
2 2.5
0
5000
10000
2
3
4
0
5000
10000
1.01
1.012 1.014
0
5000
10000
0.2
0.4
0
2000
4000
6000
8000
0.75 0.8 0.85 0.9 0.95
0
0.5
1
1.5
2
rf
âˆ‘ 104
â€“4.6 â€“4.4 â€“4.2
0
5000
10000
â€“2.6
â€“2.4
0
5000
10000
â€“2
â€“1
0
0
0.5
1
1.5
2
2.5
âˆ‘ 104
â€“5.6
â€“5.4
0
5000
10000
â€“6.2
â€“6
â€“5.8
0
5000
10000
2.5 3 3.5 4
âˆ‘ 10-3
0
5000
10000
2
4
6
âˆ‘ 10â€“4
0
0.5
1
1.5
2
La
âˆ‘ 104
sf
Fig. 15.1 Posterior distribution, ï¬‚at priors.
obtained using a calibration approach by Greenwood, Herkowitz and Krusell
(1997).
15.3.2 Smets and Woutersâ€™ (2007) priors
In our second estimation exercise, we incorporate priors for the parameters of
the model based on the work of Smets and Wouters (2007). First, as we did
before, we ï¬x some parameter values. In this case, since the priors help to

The New Macroeconometrics
387
Table 15.3 Fixed parameters.
â€°
Îµ
Ã
Ë†
2
0.025
10
10
0
0.001
achieve identiï¬cation, we can free six out of the 11 parameters ï¬xed in the ï¬rst
exercise. The remaining ï¬ve ï¬xed parameters are those still difï¬cult to identify
with aggregate data and are summarized in Table 15.3. Note that, to maintain
comparability with Smets and Woutersâ€™ paper, we increase the elasticities of
substitution, Îµ and Ã, to 10, an increase that has a minuscule effect on the
dynamics of the data.
We modify several of the priors of Smets and Wouters to induce better
identiï¬cation and to accommodate some of the differences between their model
and ours. Instead of a detailed discussion of the reasons behind each prior, it is
better to highlight that the priors are common in the literature and are centered
around values that are in the middle of the usual range of estimates, using both
macro and micro data, and diverse econometric methodologies. We summarize
our priors in Table 15.4.
We generate 75,000 draws from the posterior using our Metropolis â€“ Hast-
ings, also after an exhaustive search for good initial parameters of the chain.
In this case, the search is notably easier than in the ï¬rst exercise, showing
the usefulness of the prior in achieving identiï¬cation and a good behaviour
of the simulation. Table 15.5 reports the posterior medians, and the 5 and 95
percentile values of the 23 estimated parameters of the model, while Figure 15.2
plots the histograms of each parameter.
The value of the discount factor, â€š, goes very close to 1. This result is typical
in DSGE models. The growth rate of the economy and the log utility function
for consumption induce a high risk free real interest rate even without any
discounting. This result is difï¬cult to reconcile with an observed low average
real interest rate in the US Hence, the likelihood wants to push â€š as close
as possible to 1 to avoid an even worse ï¬t of the data. We ï¬nd a quite high
Table 15.4 Priors.
100

â€šâˆ’1 âˆ’1

h
Â¯
Ã‹p
Ëœ
Ã‹w
Ga(0.25, 0.1)
Be(0.7, 0.1)
N (9, 3)
Be (0.5, 0.1)
Be (0.5, 0.15)
Be (0.5, 0.1)
Ëœw
â€žR
â€žy
â€žï£¿
100( âˆ’1)
Ë‡
Be (0.5, 0.1)
Be (0.75, 0.1)
N (0.12, 0.05)
N (1.5, 0.125)
Ga (0.95, 0.1)
N(1, 0.25)
Ã
Â·
Ã’d
Ã’Ï•
exp(Ã›A)
exp(Ã›d)
N (4, 1.5)
N (0.3, 0.025)
Be (0.5, 0.2)
Be (0.5, 0.2)
IG (0.1, 2)
IG (0.1, 2)
exp(Ã›Ï•)
exp(Ã›Ã)
exp(Ã›e)
100Ã
100A
IG (0.1, 2)
IG (0.1, 2)
IG (0.1, 2)
N (0.34, 0.1)
N (0.178, 0.075)

388
The Oxford Handbook of Applied Bayesian Analysis
Table 15.5 Median estimated parameters (5 and 95 percentile in parentheses).
â€š
h
Â¯
Ë‡
Ã
Â·
0.998
0.997,0.999]
0.97
[0.95,0.98]
8.92
[4.09,13.84]
1.17
[0.74,1.61]
9.51
[7.47,11.39]
0.21
[0.17,0.26]
Ã‹p
Ëœ
Ã‹w
Ëœw
â€žR
â€žy
0.82
0.78,0.87]
0.63
[0.46,0.79]
0.68
[0.62,0.73]
0.62
[0.44,0.79]
0.77
[0.74,0.81]
0.19
[0.13,0.27]
â€žï£¿

Ã’d
Ã’Ï•
Ã›A
Ã›d
1.29
1.02,1.47]
1.010
[1.008,1.011]
0.12
[0.04,0.22]
0.93
[0.89,0.96]
âˆ’3.97
[âˆ’4.17,âˆ’3.78]
[âˆ’1.82,âˆ’1.11]
âˆ’1.51
Ã›Ï•
Ã›Ã
Ã›e
Ã
A
âˆ’2.36
âˆ’2.76,âˆ’1.74]
âˆ’5.43
[âˆ’5.52,âˆ’5.35]
âˆ’5.85
[âˆ’5.94,âˆ’5.74]
3.4eâˆ’3
[0.003,0.004]
2.8eâˆ’3
[0.002,0.004]
level of habit, 0.97, which diverges from other ï¬ndings in the literature that
hover around 0.7. The Frisch elasticity of labor supply of 0.85 (1/1.17) is similar
to the one we ï¬xed in the ï¬rst estimation exercise, a reassuring result since
DSGE models have often been criticized for relying on implausible high Frisch
elasticities.
The adjustment cost of investment, Ã, is high, 9.51, although lower than the
value we ï¬xed in the ï¬rst exercise. Since our speciï¬cation of adjustment costs
was parsimonious, this result hints at the importance of further research on
the nature of investment plans by ï¬rms. The parameter Â· is centred around
0.2. This result, which coincides with the ï¬ndings of Smets and Wouters,
is lower than in other estimates, but it is hard to interpret because the
presence of monopolistic competition complicates the mapping between this
parameter and observed income shares in the national income and product
accounts.
The Calvo parameter for price adjustment, Ã‹p, is 0.82 (an average ï¬ve-quarter
pricing cycle) and the indexation level Ëœ is 0.63, although the posterior for this
parameter is quite spread. The Calvo parameter for wage adjustment, Ã‹w, is
0.68 (wage decisions are revised every three quarters), while the indexation,
Ëœw, is 0.62. We see how the data push us considerably away from the prior.
There is information to be learned from the observations. However, at the
same time, the median of the posterior of the nominal rigidities parameters is
relatively different from the median in the case with ï¬‚at priors. This shows how
more informative priors do have an impact on our inference, in particular in
parameter values of key importance for policy analysis as Ã‹p and Ã‹w. We do not
necessarily judge this impact in a negative way. If the prior is bringing useful
information into the table (for example, from micro data on individual ï¬rmsâ€™
price changes, as in Bils and Klenow, 2004), this is precisely what we want.
Nevertheless, the researcher and the ï¬nal user of the model must be aware of
this fact.

The New Macroeconometrics
389
c
qp
rd
rf
sf
sa
sm
Lm
se
La
sd
gp
P
qw
cw
gr
gy
0.996
0.998
0
5000
10000
b
0.94 0.96 0.98
0
5000
10000
h
5
10 15 20
0
5000
10000
y
1
2
0
5000
10000
J
a
k
0.15 0.2 0.25
0
5000
10000
5
10
0
5000
10000
15000
0.75
0.8
0.85
0
5000
10000
0.4 0.6 0.8
0
5000
10000
0.6 0.65 0.7 0.75
0
5000
10000
0.4 0.6 0.8
0
5000
10000
0.7
0.8
0
5000
10000
0.1 0.2 0.3
0
5000
10000
15000
1.2
1.4
1.6
0
5000
10000
1.008 1.011.012
0
5000
10000
0.2
0.4
0
5000
10000
15000
0.85 0.9 0.95
0
5000
10000
15000
-4.4-4.2 -4 -3.8-3.6
0
5000
10000
-2
-1.5
-1
0
5000
10000
15000
-3
-2
-1
0
5000
10000
15000
-5.6
-5.4
0
5000
10000
15000
-6
-5.8
-5.6
0
5000
10000
15000
2.5 3 3.5 4 4.5
x 10-3
0
5000
10000
2
4
x 10-3
0
5000
10000
15000
Fig. 15.2 Posterior distribution, Smets â€“ Wouters priors.

390
The Oxford Handbook of Applied Bayesian Analysis
The parameters of the policy rule

â€žR, â€ž, â€žy, 

are standard, with the only
signiï¬cant difference that now the Fed is less responsive to inï¬‚ation, with the
coefï¬cient â€žï£¿= 1.29. However, this value still respects the Taylor principle (even
the value at the 5 percentile does). The Fed smooths the interest rate over time
(â€žR is estimated to be 0.79) and responds actively to inï¬‚ation (â€žR is 1.25) and
weakly to the output growth gap (â€žy is 0.19). We estimate that the Fed has a
target for quarterly inï¬‚ation of 1 percent.
The growth rates of the investment-speciï¬c technological change, Ã, and
of the neutral technology, A, are roughly equal. The estimated average growth
rate of the economy in per capita terms,

A + Â·Ã

/ (1 âˆ’Â·) is 0.43 percent per
quarter, or 1.7 percent annually, roughly the historical mean in the US in the
period 1865â€“2007.
15.4 Lines of further research
While the previous pages offered a snapshot of where the frontier of the
research is, we want to conclude by highlighting three avenues for further
research. Our enumeration is only a small sampler from the large menu of
items to be explored in the new macroeconometrics. We could easily write
a whole chapter just signaling areas of investigation where entrepreneurial
economists can spend decades and yet leave most of the territory uncharted.
First, we are particularly interested in the exploration of â€˜exotic preferencesâ€™
that go beyond the standard expected utility function used in this chapter
(Backus, Routledge and Zin, 2005). There is much evidence that expected utility
functions have problems accounting for many aspects of the behaviour of eco-
nomic agents, in particular those aspects determining asset prices. For example,
expected utility cannot explain the time inconsistencies that we repeatedly see
at the individual level (how long did you stick with your last diet?). Recently,
Bansal and Yaron (2004) and Hansen and Sargent (2007) have shown that many
of the asset pricing puzzles can be accounted for if we depart from expected
utility. However, these authors have used little formal econometrics and we do
not know which of the different alternatives to expected utility will explain the
data better. Therefore, the estimation of DSGE models with â€˜exotic preferencesâ€™
seems an area where the interaction between empirical work and theory is
particularly promising.
Second, we would like to relax some of the tight parametric restrictions of the
model, preferably within a Bayesian framework. DSGE models require many
auxiliary parametric assumptions that are not central to the theory. Unfortu-
nately, many of these parametric assumptions do not have a sound empirical
foundation. But picking the wrong parametric form may have horrible conse-
quences for the estimation of dynamic models. These concerns motivate the

The New Macroeconometrics
391
study of how to estimate DSGE models combining parametric and nonpara-
metric components and, hence, conducting inference that is more robust to
auxiliary assumptions.
Finally, a most exciting frontier is the integration of microeconomic het-
erogeneity within estimated DSGE models. James Heckman and many other
econometricians have shown beyond reasonable doubt that individual hetero-
geneity is the deï¬ning feature of micro data (see Browning, Hansen and Heck-
man, 1999). Our macro models need to move away from the basic represen-
tative agent paradigm and include richer conï¬gurations. Of course, this raises
the difï¬cult challenge of how to effectively estimate these economies, since the
computation of the equilibrium dynamics of the model is a challenge by itself.
However, we are optimistic: advances in computational power and methods
have allowed us to do things that were unthinkable a decade ago. We do not see
any strong reasons why estimating DSGE models with heterogeneous agents
will not be any different in a few years.
Appendix
A. Broader context and background
A.1 Policy-making institutions and DSGE models
We mentioned in the main text that numerous policy institutions are using
estimated DSGE models as an important input for their decisions. Without
being exhaustive, we mention the Federal Reserve Board (Erceg, Guerrieri and
Gust, 2006), the European Central Bank (Christoffel, Coenen and Warne, 2007),
the Bank of Canada (Murchison and Rennison, 2006), the Bank of England
(Harrison et al., 2005), the Bank of Sweden (Adolfson et al., 2005), the Bank of
Finland (Kilponen and Ripatti, 2006 and Kortelainen, 2002), and the Bank of
Spain (AndrÃ©s, Burriel and Estrada, 2006), among several others.
A.2 Sequential Monte Carlo methods and DSGE models
The solution of DSGE models can be written in terms of a state space represen-
tation. Often, that state space representation is nonlinear and/or non-Gaussian.
There are many possible reasons for this. For example, we may want to capture
issues, such as asymmetries, threshold effects, big shocks, or policy regime
switching that are approximated very poorly (or not at all) by a linear solution.
Also, one of the key issues in macroeconomics is the time-varying volatility in
time series. McConnell and PÃ©rez-QuirÃ³s (2000) and Kim and Nelson (1998)
presented extremely deï¬nitive evidence that the US economy had been more
stable in the 1980s and 1990s than before. FernÃ¡ndez-Villaverde and Rubio-
RamÃ­rez (2007) and Justiniano and Primiceri (2008) demonstrated that DSGE

392
The Oxford Handbook of Applied Bayesian Analysis
models, properly augmented with non-Gaussian shocks, could account for this
evidence.
Nonlinear and/or non-Gaussian state space representations complicate the
ï¬ltering problem. All the relevant conditional distributions of states became
non-normal and, except for a few cases, we cannot resort to analytic methods
to track them. Similarly, the curse of dimensionality of numerical integration
precludes the use of quadrature methods to compute the relevant integrals of
ï¬ltering (those that appear in the Chapman-Kolmogorov formula and in Bayesâ€™
theorem).
Fortunately, during the 1990s, a new set of tools was developed to handle
this problem. This set of tools has become to be known as sequential Monte
Carlo (SMC) methods because they use simulations period by period in the
sample. The interested reader can see a general introduction to SMC meth-
ods in Arulampalam et al. (2002), a collection of background material and
applications in Doucet, de Freitas and Gordon (2001), and the ï¬rst applica-
tions in macroeconomics in FernÃ¡ndez-Villaverde and Rubio-RamÃ­rez (2005
and 2007). The appendix in that last paper also offers references to alternative
approaches.
The simplest SMC method is the particle ï¬lter, which can easily be applied
to estimate our DSGE model. This ï¬lter replaces the conditional distribu-
tion of states {p (St|Y1:Tâˆ’1; 
)}T
t=1 by an empirical distribution of N draws

s i
t|tâˆ’1
N
i=1
T
t=1 from the sequence {p (St|Y1:Tâˆ’1; 
)}T
t=1. These draws are gen-
erated by simulation. Then, by a trivial application of the law of large numbers:
L (Y1:T; 
) â‰ƒ1
N
N

i=1
L
Y1|s i
0|0; 

T

t=2
1
N
N

i=1
L
Yt|s i
t|tâˆ’1; 

where the subindex tracks the conditioning set (i.e., t|t âˆ’1 means a draw at
moment t conditional on information until t âˆ’1) and lower cases are realiza-
tions of a random variable.
The
problem
is
then
to
draw
from
the
conditional
distributions
{p (St|Y1:Tâˆ’1; 
)}T
t=1. Rubin (1988) ï¬rst noticed that such drawing can be done
efï¬ciently by an application of sequential sampling:
Proposition 15.1 Let

s i
t|tâˆ’1
N
i=1 be a draw from p (St|Y1:Tâˆ’1; 
). Let the
sequence

<s i
t
N
i=1 be a draw with replacement from

s i
t|tâˆ’1
N
i=1 where the resampling
probability is given by
q i
t =
L
Yt|s i
t|tâˆ’1; 

N
i=1 L
Yt|s i
t|tâˆ’1; 
.
Then

<s i
t
N
i=1 is a draw from p (St|Y1:T; 
).

The New Macroeconometrics
393
Proposition 15.1 recursively uses a draw

s i
t|tâˆ’1
N
i=1 from p (St|Y1:Tâˆ’1; 
) to
draw

s i
t|t
N
i=1 from p (St|Y1:T; 
). But this is nothing more than the update of
our estimate of St to add the information on yt that Bayesâ€™ theorem is asking
for. Resampling ensures that this update is done in an efï¬cient way.
Once we have

s i
t|t
N
i=1, we draw N vectors of the ï¬ve exogenous innovations
in the DSGE model (the two shocks to productivity, the two shocks to tech-
nology, and the monetary policy shock) from the corresponding distributions
and apply the law of motion for states to generate

s i
t+1|t
N
i=1. This step, known
as forecast, puts us back at the beginning of Proposition 15.1, but with the
difference that we have moved forward one period in our conditioning, imple-
menting in that way the Chapman â€“ Kolmogorov equation. By going through
the sample repeating these steps, we complete the evaluation of the likelihood
function. KÃ¼nsch (2005) provides general conditions for the consistency of
this estimator of the likelihood function and for a central limit theorem to
apply.
The following pseudo-code summarizes the description of the algorithm:
Step
0,
Initialization:
Set t â‡1. Sample N values

s i
0|0
N
i=1
from
p (S0; 
).
Step 1, Prediction: Sample N values

s i
t|tâˆ’1
N
i=1 using

s i
tâˆ’1|tâˆ’1
N
i=1, the
law of motion for states and the distribution of shocks Îµt.
Step
2,
Filtering:
Assign to each draw

s i
t|tâˆ’1
 the weight Ë˜i
t in
Proposition 15.1.
Step 3, Sampling: Sample N times with replacement from

s i
t|tâˆ’1
N
i=1
using the probabilities 
q i
t
N
i=1. Call each draw 
s i
t|t
. If t < T
set t â‡t + 1 and go to step 1. Otherwise stop.
B. Model and computation
B.1 Computation of the model
A feature of the new macroeconometrics that some readers from outside eco-
nomics may ï¬nd less familiar is that the researcher does not specify some
functional forms to be directly estimated. Instead, the economist postulates an
environment with different agents, technology, preferences, information, and
shocks. Then, she concentrates on investigating the equilibrium dynamics of
this environment and how to map the dynamics into observables. In other
words, economists are not satisï¬ed with describing behaviour, they want to
explain it from ï¬rst principles. Therefore, a necessary ï¬rst step is to solve for the
equilibrium of the model given arbitrary parameter values. Once we have that

394
The Oxford Handbook of Applied Bayesian Analysis
equilibrium, we can build the associated estimating function (the likelihood,
some moments, etc.) and apply data to perform inference. Thus, not only is
ï¬nding the equilibrium of the model of key importance, but doing it rapidly
and accurately, since we may need to do it for many different combinations of
parameter values (for example, in an McMC simulation, we need to solve the
model for each draw of parameter values).
As we described in the main text, our solution algorithm for the model relies
on the perturbation of the equations that characterize the equilibrium of the
model given some ï¬xed parameter values. The ï¬rst step of the perturbation
is to take partial derivatives of these equations with respect to the states and
control variables. This step, even if conceptually simple, is rather cumbersome
and requires an inordinate amount of algebra.
Our favourite approach for solving this step is to write a Mathematica
program that generates the required analytic derivatives (which are in the
range of several thousands) and that writes automatic Fortran 95 code with
the corresponding analytic expressions. This step is crucial because, once we
have paid the ï¬xed cost of taking the analytic derivatives (which takes several
hours on a good desktop computer), solving the equilibrium dynamics for a
new combination of parameter values takes less than a second, since now we
only need to solve a numerical system in Fortran. The solution algorithm will
be nested inside a Metropolis â€“ Hastings. For each proposed parameter value
in the chain, we will solve the model, ï¬nd the equilibrium dynamics, use those
dynamics to ï¬nd the likelihood using the Kalman ï¬lter, and then accept or reject
the proposal.
B.2 Kalman ï¬lter
The implementation of the Kalman ï¬lter to evaluate the likelihood function in a
DSGE model like ours follows closely that of Stengel (1994). We start by writing
the ï¬rst order linear approximation to the solution of the model in a standard
state space representation:
st = Astâˆ’1 + BÎµt
(15.9)
yt = Cst + DÎµt
(15.10)
where st are the states of the model, yt are the observables, and Îµt âˆ¼N (0, I) is
the vector of innovations to the model.
We introduce some deï¬nitions. Let st|tâˆ’1 = E(st|Ytâˆ’1) and st|t = E(xt|Yt)
where
Yt = {y1, y2, ..., yt}.
Also,
we
have
Ptâˆ’1|tâˆ’1 = E(stâˆ’1 âˆ’stâˆ’1|tâˆ’1)
(stâˆ’1 âˆ’stâˆ’1|tâˆ’1)â€² and Pt|tâˆ’1 = E(stâˆ’1 âˆ’st|tâˆ’1)(stâˆ’1 âˆ’st|tâˆ’1)â€². With this notation,
the one-step-ahead forecast error is Ãt = yt âˆ’Cst|tâˆ’1.
We forecast the evolution of states:
st|tâˆ’1 = Astâˆ’1|tâˆ’1.
(15.11)

The New Macroeconometrics
395
Since the possible presence of correlation in the innovations does not change
the nature of the ï¬lter, it is still the case that
st|t = st|tâˆ’1 + K Ãt,
(15.12)
where K is the Kalman gain at time t. Deï¬ne variance of forecast as Vy =
C Pt|tâˆ’1Câ€² + DDâ€².
Then, the conditional likelihood is just:
log L = âˆ’n
2 log 2ï£¿âˆ’1
2 log det

Vy

âˆ’1
2ÃtVâˆ’1
y Ãt.
The last step is to update our estimates of the states. Deï¬ne residuals Ã“t|tâˆ’1 =
st âˆ’st|tâˆ’1 and Ã“t|t = st âˆ’st|t. Subtracting equation (15.11) from equation (15.9)
st âˆ’st|sâˆ’1 = A

stâˆ’1 âˆ’stâˆ’1|tâˆ’1

+ Bwt,
Ã“t|tâˆ’1 = AÃ“tâˆ’1|tâˆ’1 + Bwt.
(15.13)
Now subtract equation (15.12) from equation (15.9)
st âˆ’st|t = st âˆ’st|tâˆ’1 âˆ’K

Cxt + Dwt âˆ’Cxt|tâˆ’1

Ã“t|t = Ã“t|tâˆ’1 âˆ’K

CÃ“t|tâˆ’1 + Dwt

.
(15.14)
Note Pt|tâˆ’1 can be written as
Pt|tâˆ’1 = EÃ“t|tâˆ’1Ã“â€²
t|tâˆ’1,
= E

AÃ“tâˆ’1|tâˆ’1 + Bwt
 
AÃ“tâˆ’1|tâˆ’1 + Bwt
â€²
= APtâˆ’1|tâˆ’1Aâ€² + BBâ€².
(15.15)
For Pt|t we have:
Pt|t = EÃ“t|tÃ“â€²
t|t
= (I âˆ’KC) Pt|tâˆ’1

I âˆ’Câ€²K â€²
+ K DDâ€²K â€² âˆ’K DBâ€²
âˆ’BDâ€²K â€² + KC BDâ€²K â€² + K DBâ€²Câ€²K â€².
(15.16)
The optimal gain minimizes Pt|t:
Kopt =

Pt|tâˆ’1Câ€² + BDâ€² 
Vy + C BDâ€² + DBâ€²Câ€²âˆ’1
and, consequently, the updating equations are:
Pt|t = Pt|tâˆ’1 âˆ’Kopt

DBâ€² + C Pt|tâˆ’1

,
st|t = st|tâˆ’1 + KoptÃt
and we close the iterations.

396
The Oxford Handbook of Applied Bayesian Analysis
B.3 Construction of data
When we estimate the model, we need to make the series provided by the
national and income product accounts (NIPA) consistent with the deï¬nition
of variables in the theory. The main adjustment that we undertake is to express
both real output and real gross investment in consumption units. Our DSGE
model implies that there is a numeraire in terms of which all the other prices
need to be quoted. We pick consumption as the numeraire. The NIPA, in com-
parison, uses an index of all prices to transform nominal GDP and investment
into real values. In the presence of changing relative prices, such as the ones
we have seen in the US over the last several decades with the fall in the relative
price of capital, NIPAâ€™s procedure biases the valuation of different series in real
terms.
We map theory into data by computing our own series of real output and real
investment. To do so, we use the relative price of investment, deï¬ned as the ratio
of an investment deï¬‚ator and a deï¬‚ator for consumption. The denominator is
easily derived from the deï¬‚ators of nondurable goods and services reported in
the NIPA. It is more complicated to obtain the numerator because, historically,
NIPA investment deï¬‚ators were poorly constructed. Instead, we rely on the
investment deï¬‚ator computed by Fisher (2006), a series that unfortunately ends
early in 2000Q4. Following Fisherâ€™s methodology, we have extended the series
to 2007Q1.
For the real output per capita series, we ï¬rst deï¬ne nominal output as
nominal consumption plus nominal gross investment. We deï¬ne nominal
consumption as the sum of personal consumption expenditures on nondurable
goods and services. We deï¬ne nominal gross investment as the sum of personal
consumption expenditures on durable goods, private residential investment,
and non-residential ï¬xed investment. Per capita nominal output is equal to the
ratio between our nominal output series and the civilian non-institutional pop-
ulation between 16 and 65. To obtain per capita values, we divide the previous
series by the civilian non-institutional population between 16 and 65. Finally,
real wages are deï¬ned as compensation per hour in the non-farm business
sector divided by the CPI deï¬‚ator.
Acknowledgement
We thank Alex Groves for his help with preparing the data used in the esti-
mation. Beyond the usual disclaimer, we must note that any views expressed
herein are those of the authors and not necessarily those of the Federal Reserve
Bank of Atlanta or the Federal Reserve System. Finally, we also thank the NSF
for ï¬nancial support.

The New Macroeconometrics
397
References
Adolfson, M., LasÃ©en, S., LindÃ©, J. and Villani, M. (2005). Bayesian estimation of an open
economy DSGE model with incomplete pass-through. Sveriges Riksbank. Working Paper
Series 179.
An, S. and Schorfheide, F. (2006). Bayesian analysis of DSGE models. Econometric Reviews, 26,
113â€“172.
AndrÃ©s, J., Burriel, P. and Estrada, A. (2006). BEMOD: A DSGE model for the Spanish economy
and the rest of the euro area. Documento de Trabajo del Banco de EspaÃ±a 0631.
Aruoba, S.B., FernÃ¡ndez-Villaverde, J. and Rubio-RamÃ­rez, J. (2006). Comparing solution
methods for dynamic equilibrium economies. Journal of Economic Dynamics and Control, 30,
2477â€“2508.
Arulampalam, A.S., Maskell, S., Gordon, N. and Clapp, T. (2002). A tutorial on particle ï¬lters for
online nonlinear/non-Gaussian Bayesian tracking. IEEE Transactions on Signal Processing,
50, 174â€“188.
Backus, D.K., Routledge, B.R. and Zin, S.E. (2005). Exotic preferences for macroeconomists.
NBER Macroeconomics Annual, 2004, 319â€“390.
Bansal, R. and Yaron, A. (2004). Risks for the long run: A potential resolution of asset pricing
puzzles. Journal of Finance, 59, 1481â€“1509.
Bils, M. and Klenow, P. (2004). Some evidence on the importance of sticky prices. Journal of
Political Economy, 112, 947â€“985.
Browning, M., Hansen, L.P. and Heckman, J.J. (1999). Micro data and general equilibrium
models. In Handbook of Macroeconomics, (ed. J.B. Taylor and M. Woodford), vol. 1, pp. 543â€“
633. Elsevier, Amsterdam.
Calvo, G. A. (1983). Staggered prices in a utility-maximizing framework. Journal of Monetary
Economics, 12, 383â€“398.
Canova, F. and Sala, L. (2006). Back to square one: Identiï¬cation issues in DSGE models.
Mimeo. Pompeu Fabra University.
Caplin, A. and Leahy, J. (1991). State-dependent pricing and the dynamics of money and output.
Quarterly Journal of Economics, 106, 683â€“708.
Chernozhukov, V. and Hong, H. (2003). A MCMC approach to classical estimation. Journal of
Econometrics, 115, 293â€“346.
Christiano, L., Eichenbaum, M. and Evans, C.L. (2005). Nominal rigidities and the dynamic
effects of a shock to monetary policy. Journal of Political Economy, 113, 1â€“45.
Christoffel, K., Coenen, G. and Warne, A. (2007). The new area-wide model of the euro area:
Speciï¬cation and ï¬rst estimation results. Mimeo. European Central Bank.
Dotsey, M., King, R.G. and Wolman, A. (1999). State dependent pricing and the gen-
eral equilibrium dynamics of money and output. Quarterly Journal of Economics, 144,
655â€“690.
Doucet, A., de Freitas, N. and Gordon, N. (2001). Sequential Monte Carlo Methods in Practice.
Springer Verlag, Berlin.
Edge, R., Kiley, M. and Laforte, J.P. (2008). A comparison of forecast performance between
Federal Reserve staff forecasts, simple reduced-form models, and a DSGE model. Mimeo.
Federal Reserve Board.
Erceg, C.J., Guerrieri, L. and Gust, C. (2006). SIGMA: A new open economy model for policy
analysis. International Journal of Central Banking, 2, 1â€“50.
FernÃ¡ndez-Villaverde, J. and Rubio-RamÃ­rez, J. (2004). Comparing dynamic equilibrium mod-
els to data: A Bayesian approach. Journal of Econometrics, 123, 153â€“187.

398
The Oxford Handbook of Applied Bayesian Analysis
FernÃ¡ndez-Villaverde, J. and Rubio-RamÃ­rez, J. (2005). Estimating dynamic equilibrium
economies: Linear versus nonlinear likelihood. Journal of Applied Econometrics, 20,
891â€“910.
FernÃ¡ndez-Villaverde, J. and Rubio-RamÃ­rez, J. (2007). Estimating macroeconomic models: A
likelihood approach. Review of Economic Studies, 74, 1059â€“1087.
Fernandez-Villaverde, J. and Rubio-Ramirez, J. (2008). How structural are structural parame-
ters? NBER Macroeconomics Annual, 2007, 83â€“137.
FernÃ¡ndez-Villaverde, J., Rubio-RamÃ­rez, J. and Santos, M.S. (2006). Convergence properties
of the likelihood of computed dynamic models. Econometrica, 74, 93â€“119.
Fisher, J. (2006). The dynamic effects of neutral and investment-speciï¬c technology shocks.
Journal of Political Economy, 114, 413â€“52.
Greenwood, J., Herkowitz, Z. and Krusell, P. (1997). Long-run implications of investment-
speciï¬c technological change. American Economic Review, 87, 342â€“362.
GuerrÃ³n-Quintana, P. (2008). What you match does matter: The effects of data on DSGE
estimation. Working Paper. North Carolina State University.
Judd, K.L. (1998). Numerical Methods in Economics. MIT Press, Cambridge, MA.
Justiniano, A. and Primiceri, G.E. (2008). The time varying volatility of macroeconomic ï¬‚uctu-
ations. American Economic Review, 98, 604â€“641.
Hansen, L.P. (1982). Large sample properties of generalized method of moments estimators.
Econometrica, 50, 1029â€“54.
Hansen, L.P. and Sargent, T. (2007). Robustness. Princeton University Press, Princeton.
Harrison, R., Nikolov, K., Quinn, M., Ramsey, G., Scott, A. and Thomas, R. (2005). The Bank of
England Quarterly Model. The Bank of England.
Kim, C. and Nelson, C.R. (1998). Has the U.S. economy become more stable? A Bayesian
approach based on a Markov-switching model of the business cycle. Review of Economics and
Statistics, 81, 608â€“616.
Kilponen, J. and Ripatti, A. (2006). Introduction to AINO. Mimeo, Bank of Finland.
Kortelainen, M. (2002). EDGE: A model of the euro area with applications to monetary policy.
Bank of Finland Studies E:23.
KÃ¼nsch, H.R. (2005). Recursive Monte Carlo ï¬lters: Algorithms and theoretical analysis. Annals
of Statistics, 33, 1983â€“2021.
Kydland, F. and Prescott, E.C. (1982). Time to build and aggregate ï¬‚uctuations. Econometrica,
50, 1345â€“1370.
Lubick, T. and Schorfheide, F. (2004). Testing for indeterminacy: An application to U.S. mone-
tary policy. American Economic Review, 94, 190â€“217.
Lucas, R. (1972). Expectations and the neutrality of money. Journal of Economic Theory, 4,
103â€“124.
Lucas, R. (1976). Econometric policy evaluation: A critique. Carnegie-Rochester Conference Series
on Public Policy, 1, 19â€“46.
McConnell, M.M. and PÃ©rez-QuirÃ³s, G. (2000). Output ï¬‚uctuations in the United States: What
has changed since the early 1980â€™s? American Economic Review, 90, 1464â€“1476.
Murchison, S. and Rennison, A. (2006). ToTEM: The Bank of Canadaâ€™s new quarterly projection
model. Bank of Canada Technical Report, 97.
Muth, J. F. (1961). Rational expectations and the theory of price movements. Econometrica, 29,
315â€“335.
Orphanides, A. (2002). Monetary policy rules and the great inï¬‚ation. American Economic Review,
92, 115â€“120.
Roberts, G., Gelman, A. and Gilks, W. (1997). Weak convergence and optimal scaling of random
walk Metropolis algorthims. Annals of Applied Probability, 7, 110â€“120.

The New Macroeconometrics
399
Rubin, D.B. (1988). Using the SIR algorithm to simulate posterior distributions. In Bayesian
Statistics 3, (ed. J.M. Bernardo, M.H. DeGroot, D.V. Lindley, and A.F.M. Smith), pp. 395â€“402.
Oxford University Press, Oxford.
Sargent, T.J. (2005). An interview with Thomas J. Sargent by George W. Evans and Seppo
Honkapohja. Macroeconomic Dynamics, 9, 561â€“583.
Sims, C. A. (2002). Implications of rational inattention. Mimeo. Princeton University.
Smets, F. and Wouters, R. (2007). Shocks and frictions in US business cycles: A Bayesian DSGE
approach. American Economic Review, 97, 586â€“606.
Smith, A. (1776). An Inquiry into the Nature and Cause of the Wealth of Nations. Published by
Clarendon Press (1979), Oxford, Glasgow Edition of the Works and Correspondence of Adam
Smith, vol. 2.
Stengel, R.F. (1994). Optimal Control and Estimation. Dover Publications, New York.
Walras, L. (1874). Elements of Pure Economics. Republished by Harvard University Press (1954),
Cambridge.
Woodford, M. (2003). Interest and Prices: Foundations of a Theory of Monetary Policy. Princeton
University Press, Princeton.


PART III
Environment and Ecology


Â·16Â·
Assessing the probability of
rare climate events
Peter Challenor, Doug McNeall, and James Gattiker
16.1 Introduction
16.1.1 The problem of rare climate events
Until recently, climate scientists have concentrated on demonstrating that
global warming is a problem that needs to be addressed. This has involved
predicting expected climate change conditional on some future greenhouse gas
emissions scenario (e.g. IPCC 2007). The purpose of these studies has been
to show the most probable consequences of anthropogenic (human induced)
climate change. In addition to the widely predicted warming, there are other
possible, albeit less likely, climate futures where dramatic changes occur. These
are possible futures with â€˜low probability, high impactâ€™ events, which are often
associated with tipping elements (Lenton et al., 2008) in the Earth climate
system. A tipping element is a part of the Earth system that shows some
form of non-linear behaviour. A small external forcing can â€˜tipâ€™ the system
into a different type of behaviour. Such a change often happens faster than
the forcing (termed â€˜abruptâ€™), and may be irreversible, or at least show some
hysteresis. Such events have been seen often in the past, and possible future
events include; the collapse of the Greenland or West Antarctic Ice Sheets
(Huybrechts and De Wolde, 1999), the loss of sea ice in the Arctic, the die-back
of the tropical Amazon rainforest (Cox et al., 2000) or the collapse of the ther-
mohaline circulation in the North Atlantic (Rahmstorf and Ganopolski, 1999).
All of these events, though unlikely, would have severe consequences for both
the Earth system, and any society which had to cope with them. Some events
would result in strong positive feedbacks to a changing climate; for example
a die-back of the Amazon rainforest would hugely increase the concentration
of carbon dioxide in the atmosphere, further increasing global warming. The
most severe of these events could effect entire societies on a continental scale.
If we informally deï¬ne the risk of an unwelcome future climate event as the
probability of its occurrence, multiplied by the damage that it would cause, it is
arguable that we should devote signiï¬cant resources to identify and study â€˜rareâ€™
events.

404
The Oxford Handbook of Applied Bayesian Analysis
100Â°W
40Â°S
0Â°
40Â°N
10
5
0
0
0
0
5
0
0
0
0
0
0
5
5
10
10
5
20
0
0
5
15
5
10
0
0
5
5
15
â€“5
0
0
80Â°N
0Â°
100Â°E
Fig. 16.1 The difference in mean winter surface air temperature from the zonal average (K).
In this paper we will look at statistical methods for estimating the probability
of such rare events happening, using complex numerical models of the climate
system. To illustrate the methods we will concentrate on a single such rare
event â€“ the potential collapse of the meridional overturning circulation in the
Atlantic Ocean.
16.1.2 The meridional overturning circulation
A look at winter air temperature around the globe in Figure 16.1 conï¬rms that
the eastern North Atlantic region, including North West Europe, is signiï¬cantly
warmer than similar regions on the same latitude. This is not just the effect
of the ocean; British Columbia, on the east side of the Paciï¬c, is colder than
western Europe at the same latitude. The reason is the meridional (North-
South) overturning circulation in the Atlantic; the MOC. A crucial part of
the MOC is driven by density differences due to heat and salt, and is known
as the thermohaline circulation (THC). At high latitudes in the Atlantic the
atmosphere is much colder than the ocean. This means that (i) sea ice is
created by freezing and (ii) there is a strong ï¬‚ux of water from the ocean to
the atmosphere. These processes make the water both cold â€“ near freezing â€“
and more saline. This cold, saline water is denser than the water beneath it,
and so it sinks. To replace the surface water, less dense, warmer, fresher water
is moves northward through the Atlantic. This ï¬‚ow of warm, fresh water at the
surface of the Atlantic acts as a â€˜heat pumpâ€™, and keeps Europe relatively warm
in winter.

Assessing the Probability of Rare Climate Events
405
The dense water that has sunk to the bottom of the ocean makes the oppo-
site, southward trip. Moving south though the Atlantic deep in the ocean it
slowly warms and freshens, rising to the surface in the Southern Ocean and
Paciï¬c. This water is now the warm, fresh water that can be drawn into the
Atlantic. The shallow northwards circulation, and deep southwards circulation
can be thought of as a great ocean conveyor belt of heat and salt Broecker (1997).
The full MOC is more complex than this simple conveyer belt; there is also a
wind driven component that creates western boundary currents such as the
Gulf Stream in the Atlantic, and the Kuroshio in the Paciï¬c.
It is possible to model the thermohaline circulation simply, as two boxes
representing the low and high latitude oceans, connected by â€˜pipesâ€™, carrying an
overturning ï¬‚ow (Stommel, 1961). In this simple case, the so-called Stommel
model, it is possible to show that there are equilibrium solutions to the sys-
tem both with, and without, a thermohaline circulation. This raises questions
about the possibility of the stable (equilibrium) solutions for the thermohaline
circulation in the real world. Some studies have suggested that global warming
would result in changes in both the heat driven (through surface heat ï¬‚ux), and
salt driven (through increased fresh meltwater from Greenland, or enhanced
rainfall) parts of the THC.
Is it feasible that the MOC is at risk of a collapse, and is global warming likely
to make it happen in the near future? We can seek to answer these questions in
two ways. The ï¬rst is to reconstruct past climate, using palaeodata; data locked
away for millennia in ice cores, deep ocean sediments and caves, for example.
The second is to use our knowledge of the dynamics of the Earth system
to construct models, and then simulate the future in computer experiments.
Ultimately, these two activities overlap, as past climate can provide valuable
out-of-sample data to test the models, and models can be the best method of
reconstructing past climate.
The use of palaeoclimate data is a useful way to test our theories about
how the climate system works and the mechanisms of climate change. Unfor-
tunately there are no exact analogues for the current anthropogenic global
warming so we cannot use past data to directly make predictions about what
will happen in the future. However, there is good evidence that abrupt changes
in the MOC system occurred in the past (Alley et al., 2003). If we look at the time
since the last ice age, the Holocene, we have good records of the temperature
in Greenland from ice cores (Grootes et al., 1993). These records show that
about eight and a half thousand years ago there was a sudden cooling of the
temperatures in Greenland. Other palaeodata around the North Atlantic show
a similar cooling but the phenomenon was not global (Rohling and Palike,
2005). It is hypothesized that as the ice sheet retreated across North America,
the ice dam holding back the huge Lake Agassiz broke, and vast amounts of
fresh water ï¬‚owed through Hudson Bay into the North Atlantic (Barber et al.,

406
The Oxford Handbook of Applied Bayesian Analysis
1999). This low density, fresh water â€˜cappedâ€™ the North Atlantic, stopping the
deep convection and effectively shutting down the thermohaline circulation,
causing rapid cooling. In this case the circulation recovered after a few hundred
years.
To ascertain whether there is a signiï¬cant risk that the thermohaline cir-
culation could collapse in the near future (100â€“200 years) we need to use
climate models. Studies using more complex models than the simple box model
(Vellinga and Wood, 2002; Gregory et al., 2005; Rahmstorf et al., 2005) have
shown mixed results in reproducing the nonlinear behaviour in the simple
Stommel model, with the most complex models the least likely to see a collapse
of the MOC.
16.2 Climate models
A climate model is primarily a tool for learning about the climate. It is an
encoding of our knowledge about the way that the climate system works. The
climate is a system where it is not possible to perform multiple, controlled, real-
world experiments on a scale that encompasses the whole system,1 and so we
construct a climate model as a form of thought experiment.
These models can consist of several parts. A conceptual model of the dynamics
(feedbacks, physics, chemistry, biology, and possibly, economics) of the Earth
system is encoded in a mathematical model â€“ usually a series of partial dif-
ferential equations and equations of state. The mathematical model is then
discretized and rendered as code, so that it can be solved on a computer. We can
then perform experiments on this climate simulator within the conï¬nes of the
computer, varying properties of the model at will. Some processes within the
Earth system will be better understood than others; for example, geophysical
ï¬‚uid dynamics (e.g. Pedlosky 1992) gives us the tools to solve for the circulation
of both the atmosphere and the ocean with a relatively low uncertainty. Other
processes â€“ parts of the carbon cycle, for example â€“ are relatively unknown, and
contain many simpliï¬cations or â€˜paramatrizationsâ€™.
16.2.1 How the climate system works
The Earthâ€™s climate is ultimately driven by energy from the Sun. In steady state,
the amount of energy radiated and reï¬‚ected back into space must equal the
amount coming in from the Sun, otherwise the Earth would warm, or cool.
Without an atmosphere, the Earth would behave as a â€˜greyâ€™ body, reï¬‚ecting
and emitting radiation according to itâ€™s albedo, and maintaining a (roughly)
1 It is possible to regard the increase in greenhouse gas emissions as one, uncontrolled, high-risk
experiment, of course.

Assessing the Probability of Rare Climate Events
407
steady temperature of âˆ’18 â—¦C. The Earthâ€™s actual temperature is modiï¬ed by the
atmosphere. Incoming short-wave radiation is absorbed by the planet surface,
and re-emitted as long-wave radiation. This long-wave energy is then â€˜trappedâ€™
by gas molecules in the atmosphere, and the resulting increase in average
surface temperature to 14 â—¦C is known as the â€˜greenhouse effectâ€™. Every planet
in the Solar System with an atmosphere is subject to a greenhouse effect,
with the exact value of the steady state temperature increase determined in
part by the chemical composition of the atmosphere. The gases responsible
for the greenhouse effect on Earth include water vapour, carbon dioxide and
methane. Changing the relative composition of the atmosphere, for example
by increasing the concentration of these greenhouse gases, forces a change in
the radiative balance of the Earth. Stopping the forcing, by stopping the release
of greenhouse gases, for example, would lead to a recovery to a steady state
albeit at a higher temperature. The radiative balance would not be achieved
instantly however, because unlike the atmosphere, the ocean can store energy
on long time-scales, and adjusts its temperature slowly. Thus, even if we ceased
to emit any more greenhouse gases tomorrow it would take many decades for
the system to reach equilibrium.
Solar radiation heats the tropics more than the poles, so that the atmosphere
and ocean can be thought of as heat engines transferring heat from tropical
to polar regions. As we have seen above there is also transfer of heat from
the surface to the ocean depths. The simplest climate models have a single
box representing the whole Earth, and balance the radiation coming in and
out. These are known as energy balance models and, although they are sur-
prisingly effective, they give us no information about the spatial distribution
of climate variables caused by the transfer of energy over the surface of the
planet. The most complex models are modiï¬ed from atmospheric models used
for weather forecasting. The resolution is often reduced, and components to
model the oceans and sea ice, and in some cases the terrestrial and marine
biosphere are added. These General Circulation Models (GCMs) take huge
amounts of computer time on the largest computers currently available, often
taking many weeks to simulate a hundred years of climate. Such models are
expensive. In between these two extremes are the Earth system models of
intermediate complexity (EMICs). These models are more complex than the
simple radiation balance models but not as complex as a full GCM. They aim
to represent as many of the important components of the Earth system as
possible, without incurring the expense of the full GCM based models. They
are solved on a geographical grid and usually resolve a number of layers in
the ocean. Although mainly used for simulating climates on long time scales
(thousands or tens of thousands of years) such models are fast enough to allow
us to run large sets of experiments without the need of specialised computer
hardware.

408
The Oxford Handbook of Applied Bayesian Analysis
16.2.2 Climate scenarios
To run climate simulations into the future we need to be able to predict the
important drivers, or forcings of the climate. Forcings such as those due to
the variation in Earthâ€™s orbit are known with a very small uncertainty, far into
the future. Much less easy to predict are those due to societal factors, such as
greenhouse gas emissions, and land use changes. Ideally such forcings would
be predicted by an integrated model, that not only modelled the Earth system
but also the social and economic systems that interact with it. Such models
are being developed but are still some years away from being usable, and the
uncertainties associated with societal forcing are very large. The approach of
the IPCC is to specify a range of indicative scenarios to give a possible time
series of emissions through the 21st century (Nakicenovic and Swart, 2000). It
is important to note that the scenario developers refuse to attach probabilities to
these scenarios, and it is therefore impossible to integrate over them to obtain
the â€˜expectedâ€™ future.
16.3 Inference from climate models
16.3.1 Uncertainty in climate simulators
There are two main sources of uncertainty in our modelling. The ï¬rst is epis-
temic uncertainty, deriving from our lack of knowledge about the system. The
second is aleatoric uncertainty; this is the randomness in the natural world. The
uncertainty in weather forecast for the next few days is believed to be epistemic;
if we knew perfectly the state of the atmosphere and ocean at the start of our
forecast, there would be little or no uncertainty in the forecast. Beyond a few
days, the weather is chaotic; that is, tiny perturbations in the initial conditions
lead to very different and fundamentally unpredictable, states of the system
(Lorentz, 1963). The uncertainty in the weather beyond this horizon of a few
days has become aleatory. The climate can be thought of as the distribution of
weather, and so on small scales of time and space, the uncertainty is aleatory
in nature. Over larger scales however, the climate is driven by the long term
energy balance and dynamics, or the boundary conditions. The uncertainty has
once again become epistemic.
The epistemic uncertainty can itself be split into two parts. There is structural
uncertainty which is the uncertainty that arises because we do not know the
form of the model. We can start to estimate this structural error by looking at
data from the system we are trying to model. As an example, imagine predict-
ing the temperature in the Northern Hemisphere in winter; an intermediate
complexity model may be too simple to include seasonal cycle, so we could
include a model discrepancy term; an offset, with associated uncertainty, that took
us from the annual mean temperature to a seasonal one. We could estimate

Assessing the Probability of Rare Climate Events
409
this discrepancy by comparing the model output with observations. However,
it is more difï¬cult to get at the possible model discrepancy in modelling the
future, as we have no access to such calibration or validation data. If the
future relationship between the annual mean temperature and the seasonal
temperature changes in an unpredictable fashion, our estimates â€“ including our
uncertainty â€“ will be wrong. We can also examine the structural uncertainty by
examining multimodel ensembles (Furrer et al., 2007). Here, the same climate
is predicted by different versions of models, or even those built by different
institutions and people. There are several problems with this. The ï¬rst is that
many models share a common underlying conceptual model, which may be
wrong. Models are often built in collaboration between centres, developed from
older, simpler models, or perhaps even share some of the code that performs the
climate calculations. Because of this, the models are not independent samples
from the possible â€˜model spaceâ€™, and are unlikely even to span this space.
There have been attempts to deï¬ne a model independence metric, but drawing
inference from groups of models will continue to be an active area of research
for some time.
The second form of epistemic uncertainty is parameter uncertainty. Climate
models contain a number of parametrizations, that represent simpliï¬cations
of processes within the real system. The paramatrizations are often expressed
as numerical inputs2 to the model, and are often treated as variable to some
degree. The relationship between the parameters and the model output can be
represented by the mapping Y = g(X), where Y is the climate model output,
g is a function that represents the climate model, and X âˆˆX is a vector of
parameters. The parameters, or inputs X can be â€˜tunedâ€™ to ï¬nd Xâˆ—, the â€˜bestâ€™
input conï¬guration, so that the model reproduces some observed data well.
In most situations, however, it is better to allow uncertainty in X, ï¬nding a
distribution Pr(Xâˆ—) rather than committing to a single best input conï¬guration.
Individual components of X may be relatively well known, for example the
acceleration due to gravity. Others may be hard to measure in the real world, or
very uncertain in the future. Uncertainty about these inputs propagates through
the model g to give us uncertainty on the outputs Y.
16.3.2 The role of data in climate prediction
A major problem with making predictions of climate is the absence of data
that are directly relevant to the predictions we are trying to make. Due to
the timescales of climate, and unlike weather prediction, we have no chance
of directly testing our predictions with validation data. Palaeodata are often
very uncertain, and no climate of the past is a direct analogue to that of the
present day or the immediate future. If we can build a climate model from ï¬rst
2 Generally, parameters are time independent inputs.

410
The Oxford Handbook of Applied Bayesian Analysis
principles that also simulates known climates, we can have some conï¬dence
that it will do a good job predicting the future. There is no guarantee that this is
correct, however if we have a model which cannot reproduce the known climate,
we have little conï¬dence in it and would wish to down weight or discard its
predictions. The role of data is to help us learn which model structures and
parameter values best simulate the climate we know. Learning about the model
structure often occurs on an ad hoc basis, in the process of model development.
Learning which choice of parameter values lead to a given model best repro-
ducing climate is the process of calibration. We link our uncertainty in inputs
to the climate data through the output of the climate model, and modify our
prior distributions for the model parameters using Bayesâ€™ theorem. To ï¬nd
the probability of the future state being in some state (a collapsed MOC at
2100, for example), we integrate over the posterior model input space where
our model output shows a corresponding collapse. To perform the integral, we
must specify the model discrepancy, and include uncertainty due to the error in
observations.
16.3.3 Monte Carlo methods
If the climate model were very cheap to run, an effective way of performing
the integral over the model input space might be via Monte Carlo methods.
Sampling randomly from the uncertainty distributions of the inputs, we run an
ensemble of model simulations at the chosen points in input space. The output of
the model can be estimated over all of the input space by taking some average
of the output at the chosen points. The mean of the output E[g(X)] can be
evaluated by drawing samples xi, i = 1, . . . , N from our input distributions, and
then approximating
E[g(X)] â‰ˆ1
N
N

i=1
g(xi).
(16.1)
As is well known the variance of this estimate reduces as
Ã›2
N
(16.2)
where Ã›2 is the variance of the function, g(.). The usefulness of such naive
Monte Carlo methods is limited because a climate model is often too expensive
to run a large number of times.
16.3.4 Emulators
As we cannot run the expensive climate model as many times as we need
to to make inferences about the climate system, we use statistical methods
developed for such complex computer codes (Kennedy and Oâ€™Hagan, 2001).

Assessing the Probability of Rare Climate Events
411
0
1
2
3
4
5
0
1
2
3
4
5
6
B = 1
X
Y
Design
True function
Prior
Mean
Â± 2 s.d.
0
1
2
3
4
5
0
1
2
3
4
5
6
B = 10
X
Y
Design
True function
Prior
Mean
Â± 2 s.d.
0
1
2
3
4
5
0
1
2
3
4
5
6
B = 100
X
Y
Design
True function
Prior
Mean
Â± 2 s.d.
Fig. 16.2 Gaussian process emulator behaviour is dependent on hyperparameters; in this case,
smoothing parameter B. The function Y = 1 + X + cos(2X) is approximated with a GP emulator
trained at ï¬ve design points.
These are based on the idea of an emulator; a â€˜cheapâ€™ statistical approximation
to the model. We run the full climate model in a designed experiment and use
the results to build our emulator. Any inference, by Monte Carlo sampling for
example, then uses the emulator in place of the model itself. The emulator
describes what (Kennedy and Oâ€™Hagan, 2001) call code uncertainty, that is, our
uncertainty about the model output because we have a limited number of
samples. The emulator describes the full posterior distribution for the output
of the model at any input, given the training sample of inputs D and outputs Y.
Because the model is deterministic, we know the output at the training sample,
and so the uncertainty at those points is zero. The uncertainty between the
training points expands, according to the distance from the nearest training
point, and the smoothness of the response of the model. We use a Gaussian
process emulator, seen in use in an example in Figure 16.2, and described in
the appendix.
16.4 Application to the collapse of the meridional overturning
circulation
We run an experiment to combine information from experts, with that of
observed climate data, to simulate the evolution of the MOC through the 21st
century. We take into account uncertainty due to the small number of expensive
simulations that we can afford to run.
16.4.1 GENIE-1
We use an ensemble of simulations of the â€˜Grid ENabled Integrated
Earth system modelâ€™, GENIE-1 (see Edwards and Marsh (2005), and
http//www.genie.ac.uk for full details). GENIE-1 is a highly computationally
efï¬cient EMIC. It is capable of completing a several thousand year integration

412
The Oxford Handbook of Applied Bayesian Analysis
X
S2000
H
S2100
SI
F
Fig. 16.3 Schematic of the experiment to simulate the MOC over the 21st century. X is a 16-
dimensional vector of inputs to the model, and H is the historic CO2 forcing for the years 1800â€“2000.
F is a two-dimensional vector that controls forcing over the 21st century. S is the state of the system,
with subscript indicating the time. We hope that spinning up the model for 4000 years removes the
dependence on initial conditions SI, so S2000 depends solely on the inputs X and forcing H.
in a few hours on a UNIX workstation. The model features a reduced physics
(frictional geostrophic) 3D ocean circulation component coupled to a 2D
energy-moisture balance (EMBM) atmosphere and a dynamic-thermodynamic
sea ice model. The model grid differs from that of Edwards and Marsh (2005),
in that there is increased resolution in longitude, and slightly decreased in
latitude. This version of GENIE has 64 longitudes and 32 latitudes, uniform
in the longitude and sin(latitude) coordinates, giving boxes of equal area in
physical space. There are eight depth levels in the ocean on a uniformly logarith-
mically stretched grid, so that the box depth increases from 175 m to 1420 m.
GENIE-1 is deterministic; when run with the same input parameters, it always
produces the same output. A consequence of this is that stochastic processes
(e.g. weather) are not represented in the model. The version we use does not
include any seasonal variation.
16.4.2 Structure of the experiment
Our experiment is in two parts; the ï¬rst is an initialisation, or â€˜spin-upâ€™ to bring
the state of the model to a state representing the present day. The second part
is a projection of this model state into the future. A schematic of the structure
can be seen in ï¬gure 16.3.
After identifying 16 uncertain model inputs, we vary them together according
to a space-ï¬lling Latin hypercube design (McKay et al., 1979). The inputs are
perturbed randomly and uniformly across an input space identiï¬ed by one of
GENIE-1â€™s developers (Marsh) as beyond the maximum plausible region where
the model is of use in understanding the climate. It is important to note that this
design is not meant to represent the true uncertainty associated with the input
parameters, except in a very broad sense; the design is primarily to learn as
much about the behaviour of the model as possible, in a plausible input region.
We use a maximin variant of the Latin hypercube design, by choosing the exam-
ple with the largest minimum distance between pairs of points, from 10,000

Assessing the Probability of Rare Climate Events
413
â€“2000
â€“1000
0
1000
2000
0
10
20
30
40
50
60
70
Year
Î¨max (Sv)
CO2 forcing starts 1800
Fig. 16.4 Trajectories of the maximum meridional overturning circulation (
max) in the spin-up
phase of an ensemble of GENIE-1.
randomly generated candidates. This type of design is a compromise, selected
on a number of criteria including (i) space ï¬lling properties â€“ important both
in sampling input space X as efï¬ciently as possible, and reducing numerical
problems with the inversion of large matrices with nearly identical rows in the
later emulation process; (ii) â€˜wideâ€™ boundaries, to ensure that the emulator is
interpolating and not extrapolating, where it is expected to perform poorly; and
(iii) ease and speed of calculation.
The spin-up of the model is important, not just as a prerequisite to our
further experiment, but in its own right as an opportunity to examine the
climate in the model in equilibrium or near-equilibrium. The ensemble of
trajectories of the maximum value of the MOC in the North Atlantic, 
max,
throughout the spin-up phase of the experiment can be seen in Figure 16.4.
Most of the trajectories follow a typical pattern; a start from the â€˜bland earthâ€™
value of between 10 Sv and 19 Sv is followed by a rapidly increasing overturning.
After a peak, the overturning generally reduces to nearer an equilibrium value
after 200 years. The MOC appears unstable at this point, as some ensemble
members show a collapse in overturning circulation, continuing at a very low
overturning throughout the remainder of the spin-up phase. The majority of
more stable ensemble members have reached a near-equilibrium overturning
after approximately 1000 years. Of the 108 original members of the ensemble
design, eight collapse early in the integration, and a further 12 fail, usually due
to numerical error.
We designate the model year 3800 as the calendar 1800AD. From this date the
model is forced with historical atmospheric CO2 concentrations, based initially
on ice core data (Etheridge et al., 1998) and then on direct atmospheric mea-
surements from Mauna Loa (Keeling and Whorf, 2005). The forcing perturbs

414
The Oxford Handbook of Applied Bayesian Analysis
the MOC trajectories from equilibrium. Many of the ensemble members with
less vigorous overturning exhibiting a signiï¬cant and accelerating reduction.
Ensemble members with a higher equilibrium overturning tend to increase
in strength when forced with higher CO2 concentrations. There are no stable
ensemble members with an overturning strength between zero (or close to it)
and around 10 Sv. As the ensemble is carefully designed to sample parameter
space as evenly as possible, it is likely that this is an emergent property of the
model, and not a sampling error.
Surviving members of the ensemble at the end of the spin-up are then run on
to simulate the 21st century. Emissions of CO2 into the atmosphere during the
21st century are varied according to the Intergovernmental Panel on Climate
Change (IPCC) Special Report on Emissions Scenarios (SRES) (Nakicenovic and
Swart, 2000). These are six indicative and self-consistent â€˜emissions pathwaysâ€™:
scenarios of greenhouse gas emissions that take into account possible energy
use, economic development and various other factors. These scenarios only
give the anthropogenic emissions. To convert these into atmospheric concen-
trations we need to model the natural removal of CO2, both by the terrestrial
and oceanic ecosystems. We do this though a simple exponential decay in the
concentration. The possible response of the Greenland Ice Sheet to warming
in the Arctic region is accounted for via a parameter which increases fresh-
water input to the North Atlantic Ocean as the Greenland Ice Sheet melts
with increased warming. The SRES authors are also careful not to make state-
ments regarding the probability of any of the scenarios being more correct
than any other. We therefore run our model ensemble through the 21st cen-
tury using three indicative scenarios, and choose scenario A1B for illustrative
diagrams.
This is our ï¬rst formal experiment with this version of the model, and it is
important to note that the design of experiment, and associated ensemble is
exploratory in nature. Previous work looking at this problem (Challenor et al.,
2006) used a version of the model with a 36 Ã— 36 Ã— 8 grid. Our work builds
on this analysis, but the version of GENIE we are using behaves signiï¬cantly
differently from this earlier version.
16.4.3 Eliciting bounds on input parameters
The output of the model ensemble does not represent our uncertainty about the
way that the true system will behave. We have further sources of information
about the model outputâ€™s ability to represent reality, in the form of (i) expert
prior information about good values of the input parameters, and (ii) data from
the real system at the end of spin-up, that can be used to calibrate the model.
The expert information would ideally take the form of a full, multivariate prior
input distribution, and should be elicited from the model developer before any

Assessing the Probability of Rare Climate Events
415
output from the climate model is seen, or taken as a calibration exercise from a
previous version of the model.
In reality, only the ï¬rst few moments of the marginal input distributions,
perhaps with some idea about correlation between a few of the more important
inputs, can be elicited from a model expert. Due to the nature of model devel-
opment, it is very likely that output from the model will have been seen by the
developer, and therefore a certain amount of â€˜double countingâ€™ will take place in
choosing good inputs. The effect can be to produce overconï¬dence in the set of
input distributions, leading to a corresponding overconï¬dence in predictions of
the model. Climate modellers often use a set of uniform prior distributions over
fairly arbitrary input ranges as a more â€˜objectiveâ€™ alternative to a fully Bayesian
treatment. This can be a poor choice, as such distributions often do not describe
their prior beliefs or knowledge about what constitutes a â€˜goodâ€™ model input.
We ï¬nd that climate model developers often have a good idea of the â€˜bestâ€™
value of an input parameter, as well as a ï¬rm â€˜range of applicabilityâ€™ for many
inputs, beyond which conï¬dence that the model can accurately describe climate
diminishes. We can choose input distributions that are centre weighted, for
example triangular or trapezoidal, or even Gaussian. The latter are unbounded,
which can be problematic if there is a natural threshold within the input para-
meter space (e.g. a zero). This problem can be solved by transforming the input
space, for example eliciting a distribution for the log of the input, or truncating
a Gaussian distribution. We choose to model the expert prior knowledge as
a set of independent Beta distributions with Â· = 2, â€š = 2, as these are centre
weighted, and probability density can be set to zero at a deï¬ned threshold.
A priori there is no reason to believe that the â€˜bestâ€™ value should be in the centre
of the range and an asymmetric Beta might be a better choice â€“ our expert was
happier with the symmetric distributions.
16.4.4 Simulations of the MOC in the 21st century
Many model runs show a signiï¬cant weakening of the MOC by the end of the
21st Century (Figure 16.5). If we are interested in the â€˜collapseâ€™ of the MOC,
we need to deï¬ne what we mean by collapse. The simplest answer would be
to deï¬ne collapse as the reduction of the strength of the overturning to zero;
however, a severe reduction in the strength of the overturning would also have
a signiï¬cant impact on the climate. Challenor et al. (2006) deï¬ne collapse as the
strength of the overturning being less than 5 Sv at 2100. We can imagine future
scenarios where this information does not fully describe the state of the climate,
and its impacts. A trajectory which included a sudden collapse at 2030 could
have much more profound consequences for society than one where there was
a steady decline in overturning through the 21st century. Alternatively, compare
the case where the overturning just dips below 5 Sv at 2100 and then recovers,

416
The Oxford Handbook of Applied Bayesian Analysis
2000
2040
2080
0
20
40
60
Data
Year
Î¨max (Sv)
2000
2040
2080
0
20
40
60
Reconstructed
Year
Î¨max (Sv)
2000
2040
2080
â€“40
â€“20
0
20
40
Error
Year
Î¨max (Sv)
Fig. 16.5 Maximum meridional overturning circulation over the 21st Century, SRES scenario A1B
(left). A leave-one-out cross validation reconstruction of the ensemble (centre), and the error, calcu-
lated as the difference between the two (right).
with a climate where the strength of the overturning at 2100 is 5.01 Sv but
continues to reduce into the future; the latter is a more long term problem.
In this paper we therefore reject the idea of deï¬ning collapse as simply a
measure of the overturning at a speciï¬ed point in time and instead investigate
the concept of climate trajectories. In this study, the training data (ensemble)
would appear to rule out a possible abrupt collapse of the system. However,
there is an absence of ensemble members with a stable MOC below 10 Sv in the
equilibrium spin-up part of the experiment. This suggests that, given continued
forcing, an ensemble member that goes below this level is likely to continue to
reduce. We suggest then, that 10 Sv may be a â€˜tipping pointâ€™, or threshold in
the system, and as such we are interested in trajectories that go below this level
whichever year that this occurs. Our training data now encompasses the state of
the MOC in every ensemble member, at every year over the entire 21st century â€“ a
dataset with a possible dimensionality of 100. For this reason, we need to

Assessing the Probability of Rare Climate Events
417
construct a multivariate emulator, able to predict climate model output of very
high dimension.
16.5 Multivariate and high dimensional emulators
16.5.1 Constructing a high dimensional emulator
We wish to predict the output Y of the climate model g at a previously untried
input conï¬guration x. The model output is usually a vector of very high dimen-
sion, containing ï¬elds and time series of many climate variables. In our case,
each output vector contains a length 100 time series of the strength of the MOC
through the 21st century. Previous studies using a Gaussian process emulators
have been limited to scalar output; choosing a single representative output
(Challenor et al., 2006), or summary value of model output, such as a mean
(Hankin, 2005). One approach to emulation of high dimensional output would
be would be to model each point in the timeseries independently, using time (or
location) as a further input to the emulator. Rougier (2007) points out that this
would be to ignore the (typically positive) covariances between outputs close to
one another and may lead to systematic errors in the emulator, and perhaps
larger uncertainty than necessary.
We believe the climate model output (and by implication the climate) lies
on a low dimensional manifold embedded within the high dimensional output
space. We know that the MOC in any given ensemble member at time t + 1 will
be very similar to that at time t. This correlation lets us reduce the dimension-
ality of the output data to q, a manageable number of outputs. We then build
q separate Gaussian process emulators and combine them to predict model
output of high dimension. Decomposition of high dimensional training data to
emulate high dimensional functional output is also used by Higdon et al. (2008)
and Bayarri et al. (2007).
The training data is Y, a centred output matrix of n ensemble members yi, i =
1, . . . , n in the rows, each a vector of length p. An ensemble member can be
represented
yi = Ã +
n

j=1
ÃŽi jv j
(16.3)
where Ã is the ensemble mean vector, v j are the columns of V, a set of n basis
vectors and ÃŽ the reduced dimension components of Y. We choose ÃŽ and V so
that a few components retain a large portion of the variability of Y of interest.
Truncating V and ÃŽ to the q most important basis vectors and components,
the ensemble member can be reconstructed with some error, Ã‚. The vector ÃŽ j
describes the behaviour of the simulator projected into the space deï¬ned by the
basis vector v j. Each vector of components ÃŽ1, . . . , ÃŽq in turn substitutes Y as

418
The Oxford Handbook of Applied Bayesian Analysis
training data, so that q separate Gaussian process emulators (see appendix) are
built. The emulators are used to estimate Ë†ÃŽj for an untested proposal input x.
High dimensional output at a proposed input can be constructed
Ë†y = Ã +
q

j=1
Ë†ÃŽ j(x)v j + Ã‚
(16.4)
where Ã‚ is an error term and q < n â‰ªp. These components are constrained to
be orthogonal, or independent. The error component Ã‚ can be decomposed into
a truncation error, Ã‚trunc, due to the use of q < n components, and component
error, Ã‚comp, due to imperfect estimation of those components that are used for
reconstruction.
Ã‚ = Ã‚comp + Ã‚trunc
(16.5)
where
Ã‚comp =
q

j=1
(Ë†ÃŽ j(x) âˆ’ÃŽ j(x))v j,
(16.6)
and
Ã‚trunc =
n

j=q+1
ÃŽ j(x)v j.
(16.7)
It is important to choose enough components (sufï¬cient q) to ensure that
the dimension reduction step does not lose too much information about the
climate model behaviour, and thus minimize Ã‚trunc. It is also important to have
enough ensemble members to properly sample the possible model behaviour,
build a good emulator and thus minimize Ã‚comp. There are several choices
for the dimension reduction step; Independent Components Analysis, Hyvarinen
and Oja (2000), would ensure that the components are both independent and
uncorrelated. Bayarri et al. (2007) use a wavelet decomposition of functional
output. We ï¬nd that Principal Components Analysis (PCA, e.g. Jolliffe, 2002).
via the singular value decomposition is quick, and has the advantage that the
ï¬rst few components explain the majority of the variance across the ensemble.
A visual inspection of the ï¬rst few PCs in a scatter plot matrix ensures that
the components are approximately independent. However this independence is
only guaranteed across the training set, not across all of output space. Because
our original design has attempted to span output space we assume that the
property of independence can be carried across to the full output space. This
assumption is very difï¬cult to test. Performing the calculations with subsets of
the training data show that our components are robust within the training set,
giving us some conï¬dence that their properties will carry across the whole of
output space.

Assessing the Probability of Rare Climate Events
419
16.6 Uncertainty analysis
We perform a calibrated uncertainty analysis of the MOC through the 21st
century in three steps.
1. We calibrate the model (ï¬nd Pr(Xâˆ—)) using observational data from the
20th century, and a univariate Gaussian process emulator trained on
model spin-up data representing the same period.
2. We use the trajectories of maximum MOC, 
max, through the 21st century
as training data for a high dimensional Gaussian process emulator.
3. We then use the high dimensional emulator to project our calibrated
input distribution into model output space corresponding with the MOC
through the 21st century.
16.6.1 Calibration of GENIE-1 using MOC data
We use a simple rejection sampling approach, similar to the method of Smith
and Gelfand (1992) to generate input samples from the â€˜best inputâ€™ posterior
distribution Pr(Xâˆ—). This approach can be summarized as â€˜sample from the
prior and weight by the likelihoodâ€™. We use the prior distributions for the inputs
elicited from our climate modeller, and construct a likelihood based on the
observational data of the MOC.
The overturning circulation at a latitude of around 25 â—¦N has been estimated
by hydrographic section at various times during the last half-century, most
recently in 2004 by Bryden et al. (2005), who compared their results with
previous cruises. Cunningham et al. (2007) gives an estimate for the strength of
the overturning in 2006 but their methods are different those used by Bryden
et al. (2005) and are not directly comparable. Bryden et al. (2005) found that the
overturning had weakened by around 30% during the previous ï¬ve decades.
The data are a sparse timeseries of ï¬ve measurements of the MOC from 1957
to 2004. The â€˜best estimateâ€™ of each data point is pooled to calculate a mean
mÂ¯ = 18.4 Sv, and standard deviation Ã›Â¯ = 3.1 Sv. (1 Svedrup (Sv) = 106 m3sâˆ’1).
This ignores the stated uncertainty of 6 Sv in the observed data, and any model
discrepancy (for example that caused by the difference between the MOC at
26 â—¦N, and the maximum MOC, as represented by the model). However, it is
reasonable when considering the variability and estimate of the MOC in 2006,
made by Cunningham et al. (2007), of mean 18.7 Sv, with a standard deviation
of 2.8 Sv.
16.6.2 Rejection sampling
We train a Gaussian process emulator, on output from the 92 members of the
ensemble that complete their run to the end of the 21st century. The output is

max, a scalar representing the maximum value in Sverdrups of the overturning

420
The Oxford Handbook of Applied Bayesian Analysis
circulation in the North Atlantic, extracted at the year 1998. This is near the end
of the spin-up period, and so the ensemble varies 16 input parameters over the
design space XD.
The posterior input distributions are approximated by Monte Carlo integra-
tion, using the following procedure:
1. Take a large sample x1, . . . , xi from the prior input distribution.
2. For each xi, take a single sample yi that represents a model output
corresponding to historical data from the emulator posterior distribution
Pr(yi|xi).
3. Calculate the likelihood, based on distance of sample yi from the historical
data, and the model discrepancy, and accept xi with a probability that is
proportional to the likelihood.
For the likelihood function, we use a normal density, based on mean and
standard deviation of the observational data. After generating a sample xi from
the prior distribution the output sample yi is drawn from emulator posterior t
distribution, approximated by a normal distribution. We generate a likelihood
density for yi based on the probability density function of the normal distribu-
tion, with mean mÂ¯ and standard deviation Ã›Â¯:
Ë†(yi) =
1
Ã›Â¯ + Ã›Ã‚
âˆš
2ï£¿
exp

âˆ’[yi âˆ’(mÂ¯ + mÃ‚)]2
> 
2

Ã›2
Â¯ + Ã›2
Ã‚

.
(16.8)
Because of missing physical processes and the sheer complexity of the climate
system there will be a discrepancy between what the model produces and the
real world. If we had data on future discrepancies, we could model them as
another Gaussian process, as in Kennedy and Oâ€™Hagan (2001). However we
have no data for climate predictions such as these â€“ our estimates of model
discrepancy have to come from our experts. They are unlikely to be able to
specify the discrepancy to this level of detail so we may have to model it as a
simple uncertainty, increasing the variance of our predictions (although they
may be able to say if they believe there will be a bias). We could therefore
include a simple model inadequacy term, with mean mÃ‚ and standard deviation
Ã›Ã‚. We have no evidence to suspect that the model has a systematic bias, and the
aforementioned sparsity of data means we have little with which to estimate a
model inadequacy term for the historical behaviour. We have not taken this step
here so our inferences are strictly speaking on a calibrated model rather than
the real world.
We generate 100,000 samples from the prior distribution for the inputs, and
reject around 80% of them to form a sample of approximately 20,000 input
conï¬gurations, to approximate the posterior distribution of Xâˆ—. The 80% of
rejected runs are the proportion of climates that have been simulated from the
input distributions elicited from the expert are not close enough to the data to

Assessing the Probability of Rare Climate Events
421
be regarded as good representations of the present day MOC. Thus the 20% is
a measure of how well the elicited distributions correspond to the data. Once
we have a calibrated input distribution, we can project the corresponding model
output into the future via an emulator trained on the model future.
Given that we can, fairly accurately, simulate (and emulate) the present day
strength of the MOC, does this imply that we can accurately predict the future
strength of the MOC and in particular the probability of MOC collapse? The
answer is undoubtedly; not necessarily. It would be easy to build a model that
reproduced the present day data but failed miserably to predict the future. Our
complex simulators are designed to include the physics of those parts of the
climate system that our experts believe are important in predicting the future of
the MOC. The model we are using is an intermediate complexity model so does
not have the fullest and most accurate representation of Earth system processes
available. However, our experts believe it should give reasonable answers. In
some ways, the intermediate complexity model it is better than many IPCC
models (IPCC, 2007) in that it includes a crude model of the melting of the
Greenland ice sheet, and has been somewhat validated in experiments to simu-
late long periods of past climate.
16.6.3 Training and verifying the high dimensional emulator
Transforming the model output through the 21st century, Y f via principal com-
ponents analysis, we reduce the dimensionality of the data from 100 time-steps
to two curves plus a mean term (Figure 16.6). The ï¬rst principal component
accounts for 99% of the variance of the output across the ensemble, while the
second accounts for just under 1%. After building a high-dimensional Gaussian
process emulator, we can measure the its performance using leave-one-out
cross-validation. Each member in turn is removed from the the ensemble,
and then estimated by an emulator trained on all of the other members. The
estimated ensemble, and its difference from the original data can be seen in
Figure 16.5. Some estimated ensemble members have a large error, but we ï¬nd
that these are usually from the extremes of the input design space, simulate
past climate poorly, and are therefore ruled out by calibration.
16.6.4 Estimating the probability of MOC slowdown
We calculate a calibrated prediction for Yf , the overturning circulation through
the 21st century, by running the emulator at the calibrated input samples. We
introduce high and low scenario distributions for the two extra parameters
controlling the Greenland melt rate and atmospheric CO2 removal rate. The
uncertainty analysis uses the method of Oakley and Oâ€™Hagan (2002), gener-
ating 20 simulated design points from the emulator posterior distribution. We
repeating this sampling 30 times, rebuilding the posterior emulator distribution

422
The Oxford Handbook of Applied Bayesian Analysis
2000 2020 2040 2060 2080 2100
24
25
26
27
28
Mean
Year
Î¨max (Sv)
2000 2020 2040 2060 2080 2100
â€“0.105
â€“0.100
â€“0.095
â€“0.090
PC1
Year
Î¨max (arbitrary)
2000 2020 2040 2060 2080 2100
â€“0.2
â€“0.1
0.0
0.1
PC2
Year
Î¨max (arbitrary)
Fig. 16.6 Mean (left), and the ï¬rst two principal component patterns (centre, right) of the ensemble.
for yi each time, for every accepted input conï¬guration, to fully account for
the code uncertainty in the model output. There are too many trajectories to
plot, so we generate a 2D density plot of each, using a kernel density estimator
(ï¬gure 16.7).
The posterior probability that the MOC lies beneath the 10 Sv threshold
at sometime during the 21st century (Table 16.1) is simply the proportion of
years where any emulated MOC trajectory lies below that threshold in the
scenario.
Pr (
max < 10) = 1
N
N

i=1
I(yi)
(16.9)

Assessing the Probability of Rare Climate Events
423
0e+00
2eâ€“04
4eâ€“04
6eâ€“04
8eâ€“04
density
2000 2020 2040 2060 2080 2100
0
10
20
30
40
B priors, A1B Low GRM/CREFT
Year
Î¨max (Sv)
0e+00
2eâ€“04
4eâ€“04
6eâ€“04
8eâ€“04
density
2000 2020 2040 2060 2080 2100
0
10
20
30
40
B priors, A1B High GRM/CREFT
Year
Î¨max (Sv)
Fig. 16.7 Probability density of the strength of the MOC through the 21st century. The density is
conditional on the rate of Greenland melting, CO2 removal time, expert prior distributions for the
model inputs, and a calibration using observed climate data.
where N is the number of simulated data points, or p Ã— a, when we have a
accepted samples of Pr(Xâˆ—), and I is the indicator function
I =
1 if yi â‰¤10
0 if yi > 10.
(16.10)
The probability of signiï¬cant reduction (i.e. below 10 Sv) of the MOC is condi-
tional on a low or high rate of Greenland melting, and carbon removal time-
scale. A low scenario of these parameters generates a probability of crossing the
threshold of around 0.1, whereas a high scenario has a probability of above
0.3. The future MOC strength in GENIE-1 is very sensitive to uncertainty
in these parameters, implying sensitivity to these poorly understood physical
processes.
Table 16.1 Probability of the MOC dropping
below a threshold of 10 Sv during the 21st
century, for three indicative scenarios.
Scenario
Low melt rate
High melt rate
A1B
0.09
0.35
A2
0.10
0.33
B1
0.10
0.30

424
The Oxford Handbook of Applied Bayesian Analysis
As the model does not simulate an â€˜abruptâ€™ change in the MOC, the estimated
probabilities of such a shift are near zero. However, the model does suggest a
high probability that the system will cross the emergent threshold of 10 Sv,
which may lead to an inevitable shutdown of the MOC over the 21st century.
If the threshold in the model is indicative of a real property of the system,
this slowdown would have a severe impact on the climate of Northern Europe.
A repeat of this experiment with similar forcings, using a spectrum of Earth
system models, would help to discover if such a threshold is indeed a property
of the system, or is in fact an artefact of this particular model.
16.7 Summary and further discussions
r High dimensional output of the climate model (trajectories of the MOC)
can be approximated well using a small number of principal components,
and a Gaussian process emulator.
r Expert knowledge in the form of prior distributions for model inputs con-
strains the model more than the available historical MOC data.
r The model does not simulate an abrupt climate change in our ensemble,
but most runs show an increasingly rapidly decreasing MOC. The most
feasible runs, as measured by their ability to reproduce past climate data,
show a steady decline in MOC strength during the 21st century.
r The model does not show a stable MOC below around 10 Sv, an apparent
emergent property of the model. It is unclear if runs that cross this thresh-
old through transient forcing are committed to inevitable shutdown.
These results are conditional on the prior knowledge of the model, as well
as the validity of the model itself, the data used in calibration, the scenario of
future greenhouse gas emissions, and the statistical framework that was used
to combine all of these elements. At every stage, there are subjective decisions
being made about what is important to include in the inference. However, this
analysis does represent an attempt to comprehensively include uncertainty in a
prediction of the future state of the Earth system.
16.7.1 Where next?
Stakeholders such as governments and businesses have an increasing need
for accurate climate predictions in order to make informed decisions. This
information is always needed at the limit of scientiï¬c understanding, and ability
to provide; for example future climate impacts are desired at an ever greater
spatial resolution. We see no reason to expect that computational power will
outstrip the complexity of state-of-the-art climate models, and lead to cheap
climate predictions in the foreseeable future. In this case, the development of

Assessing the Probability of Rare Climate Events
425
cheap and accurate climate model emulators should continue to be an area
of active research. There is still much work to be done on building efï¬cient
high dimensional emulators, especially where the model output consists of
many time-steps of ï¬eld data that may have some correlation and nonlinear
interaction; for example, temperature and precipitation. The units used to
measure these data, and corresponding variances are very different, and so
donâ€™t lend themselves well to simultaneous dimension reduction, although
some progress in simultaneous empirical orthogonal functions has been made
in observational climate research. A related problem is in ï¬nding sensible
ways of dealing with very high dimensional initial conditions â€“ particularly in
ï¬nding sensible strategies to the sampling the input space for ensemble design.
Other difï¬culties in building emulators include modelling categorical inputs
(e.g. discrete switches), or model versions with alternative sections of code.
This study uses a single model, which limits our ability to make probability
statements about the MOC in the future. A more comprehensive study would
use a suite of climate models to predict the future behaviour of the system.
The synthesis of information from such a â€˜grand ensembleâ€™ of models is a very
active area in climate research, a particular problem being the fact that a col-
lection of climate models does not represent an independent sample from the
possible â€˜model spaceâ€™. Ultimately, emulators will be used to link a hierarchy,
or ensemble of models together, taking into account different approaches to
the representation of physical processes, efï¬ciency, and ability to represent past
climate.
Appendix
A. Broader context and background
Throughout this paper we use a Gaussian Process (GP) emulator to learn about
the model, and perform climate experiments. This type of emulator has been
used to model computer codes in, for example, Haylock and Oâ€™Hagan (1996);
Oâ€™Hagan et al. (1999); Oakley and Oâ€™Hagan (2002) and Oakley (2004). A full
demonstration of the GP emulator can be found in Kennedy and Oâ€™Hagan
(2001), and Oâ€™Hagan (2006) provides a good non-technical summary.
The climate model is represented as an unknown function g(Â·) that relates
model inputs X to output Y
Y = g(X).
(16.11)
The method of learning about g(Â·) is to draw a sample of inputs, or design
D = (x1, . . . , xn)T from the input space X of the model, and run the model to
produce output vector y. D is a matrix of n ensemble members (rows) of d input
parameters (columns). The model output is assumed to be smooth, so that we

426
The Oxford Handbook of Applied Bayesian Analysis
have some knowledge of g(xâ€²) for xâ€² close to x. We might a priori believe that
g(x) is approximately linear in x, and so in general, the mean of g(x) is given by
E{g(x)|â€š} = h(x)Tâ€š,
(16.12)
conditional on a vector of coefï¬cients â€š, and where the vector h(Â·) consists of q
regression functions. The covariance between g(xâ€²) and g(x) is given by
cov{g(x), g(xâ€²)|Ã›2} = Ã›2c(x, xâ€²),
(16.13)
where c(x, xâ€²) is a function that decreases as the distance |x âˆ’xâ€²| increases. We
choose
c(x, xâ€²) = exp{âˆ’(x âˆ’xâ€²)TB(x âˆ’xâ€²)}
(16.14)
where B is a diagonal matrix of roughness parameters. We use a Gaussian
process model to ï¬nd the distribution of g(x) conditional on Ã›2. A Gaussian
process can be thought of as an inï¬nite collection of random variables with
the property that any subset of these variables has a multivariate normal dis-
tribution. For any set of inputs (x1, . . . , xn) then, we can model the outputs
(g(x1), . . . , g(xn)) as having a multivariate normal distribution.
Using a non-informative prior in the form of p(â€š, Ã›2) âˆÃ›âˆ’2, it can be shown
that
g(x) âˆ’mâˆ—(x)
Ë†Ã›
1
câˆ—(x, xâ€²)
 y, B âˆ¼tr+n
(16.15)
where
mâˆ—(x) = h(x)T Ë†â€š + t(x)TAâˆ’1(y âˆ’HË†â€š)
(16.16)
câˆ—(x, xâ€²) = c(x, xâ€²) âˆ’t(x)TAâˆ’1t(xâ€²) + {h(x)T âˆ’t(x)TAâˆ’1H}Vâˆ—{h(xâ€²)TAâˆ’1H}T
(16.17)
t(x) = (c(x, x1), . . . , c(x, xn))T
(16.18)
H = (h(x1), . . . , h(x))T
(16.19)
A =
âŽ›
âŽœâŽœâŽœâŽœâŽ
1
c(x1, x2) . . . c(x1, xn)
c(x1, xn)
1
...
...
...
c(xn, x1)
. . .
1
âŽž
âŽŸâŽŸâŽŸâŽŸâŽ 
(16.20)

Assessing the Probability of Rare Climate Events
427
Ë†â€š = Vâˆ—(Vâˆ’1 + HTAâˆ’1y)
(16.21)
Ë†Ã›2 = a + zTVâˆ’1z + yTAâˆ’1y âˆ’Ë†â€šT(Vâˆ—)âˆ’1Ë†â€š
n + r âˆ’2
(16.22)
Vâˆ—= (Vâˆ’1 + HTAâˆ’1H)âˆ’1
(16.23)
y = (g(x1), . . . , g(xn))T.
(16.24)
The estimator mâˆ—(x) is the posterior mean of the emulator at x, and from a
frequentist perspective can be thought of as the best linear unbiased predictor
of g(x). From equation (16.16), we can see it is made up of two parts: the ï¬rst
component h(x)T Ë†â€š is our linear prior expectation, with â€š updated in the light
of data y. The second component t(x)TAâˆ’1(y âˆ’HË†â€š) adjusts the posterior mean
so that it passes through all of the observed outputs; there is no uncertainty at
locations where we have run the model. The smoothness of the deviation from
h(x)T Ë†â€š towards the observed output yi for x close to xi depends on B. Each
element (i, i) of B describes the â€˜roughnessâ€™ of g(.) in the ith input dimension. A
smoother g(.) has a higher correlation between g(x) and g(xâ€²). As the correlation
depends on the distance, the matrix B determines how close two inputs x and
xâ€² need to be for the correlation between g(x) and g(xâ€²) to take on a particular
value (illustrated in Figure 16.2).
The matrix B describes the roughness of the function g(.) in each input
dimension. Since g(.) is unknown, there is also uncertainty about the elements
of B; however, there is no analytical way of dealing with this uncertainty. A sim-
ple option is to keep B ï¬xed, however, since the estimation of g(x) is conditional
on B (e.g. see ï¬gure 16.2), this can lead to poor emulator performance. A full
Bayesian treatment of B is possible however, this can be computationally expen-
sive. Oakley (1999) suggests estimating B from the data, using either cross val-
idation, or via the posterior mode. The density of B conditional on y is given by
f (B|y) âˆ(Ë†Ã›2)âˆ’nâˆ’q
2 |A|âˆ’1
2 |HTAâˆ’1H|âˆ’1
2 .
(16.25)
We use optimization methods to ï¬nd a â€˜goodâ€™ set of values for the diagonal
matrix B. In practice, both the cross-validation and posterior mode techniques
can have problems. With even a moderate number of dimensions, say 10, the
estimation of B via cross validation is computationally expensive, and very
slow. Estimation via the posterior mode can also run into problems when the
posterior distribution of B|y is very ï¬‚at, or very rough. We ï¬nd that estimates
of B used to initialize optimization routines, have a large impact on the speed
of convergence, and outcome â€“ local minima are common. For the example
in this study, we optimise B for each principal component individually. The
ï¬rst (and most important) principal component required 27,000 iterations of
a Nelder â€“ Mead optimization routine to converge, with other components
requiring fewer iterations.

428
The Oxford Handbook of Applied Bayesian Analysis
Acknowledgements
This work was in part funded by NERCâ€™s Rapid Climate Change programme
and by the RCUK â€˜Managing Uncertainty in Complex Modelsâ€™ project. We are
grateful to J. Hirschi for providing Figure 16.1.
References
Alley, R., Marotzke, J., Nordhaus, W., Overpeck, J., Peteet, D., Peilke Jr, R., Pierrehum-
bert, R., Rhines, P., Stocker, T., Talley, L. and Wallace, J., (2003). Abrupt climate change.
Science, 299, 2005â€“2010.
Barber, D. C., Dyke, A., Hillaire-Marcel, C., Jennings, A. E., Andrews, J. T., Kerwin, M. W.,
Bilodeau, G., McNeely, R., Southon, J., Morehead, M. D. and Gagnon, J.-M. (1999). Forcing
of the cold event of 8,200 years ago by catastrophic drainage of laurentide lakes. Nature, 400,
344â€“348.
Bayarri, M., Berger, J., Cafeo, J., Garcia-Donato, G., Liu, F., Paolmo, J., Parthasarathy, R., Paulo,
R., Sack, J. and Walsh, D. (2007). Computer model vaildation with functional output. The
Annals of Statistics, 35, 1874â€“1906.
Broecker, W. (1997). Thermohaline circulation, the Achilles heel of our climate system: Will
man-made CO2 upset the current balance? Science, 278, 1582â€“1588.
Bryden, H. L., Longworth, H. R. and Cunningham, S. A. (2005). Slowing of the Atlantic
meridional overturning circulation at 25 degrees N. Nature, 438, 655â€“657.
Challenor, P., Hankin, R. and Marsh, R. (2006). Towards the probability of rapid climate change.
In Avoiding Dangerous Climate Change, (ed. H.J. Schellnhuber, W. Cramer, N. Nakicenoric, T.
Wigley and G. Yohe), Chapter 7, pp. 55â€“63. Cambridge University Press, Cambridge.
Cox, P., Betts, R., Jones, C., Spall, S. A. and Totterdell, I. (2000). Acceleration of global
warming due to carbon-cycle feedbacks in a coupled climate model. Nature, 408,
184â€“187.
Cunningham, S., Kanzow, T., Rayner, D., Baringer, M., Johns, W., Marotzke, J., Longworth, H.,
Grant, E., Hirschi, J., Beal, L., Meinen, C., and Bryde, H. (2007). Temporal variability of the
Atlantic meridional overturning circulation at 26.5 â—¦N. Science, 317, 935â€“938.
Edwards, N. and Marsh, R. (2005). Uncertainties due to transport-parameter sensitivity in an
efï¬cient 3-D ocean-climate model. Climate Dynamics, 24, 415â€“433.
Etheridge, D., Steele, L., Langenfelds, R., Francey, R., Barnda, J.-M., and Morgan, V. (1998).
Historical CO2 records from the Law Dome DE08, DE08-2, and DSS ice cores. In Trends:
A Compendium of Data on Global Change, online at Carbon Dioxide Information Analysis
Center, Oak Ridge National Laboratory, U.S. Department of Energy, Oak Ridge, Tenn., U.S.A.
[http://cdiac.ornl.gov/trends/co2/lawdome.html].
Furrer, R., Sain, S. R., Nychka, D. and Meehl, G. R. (2007). Multivariate Bayesian analysis of
atmosphere â€“ ocean general circulation models. Environmental and Ecological Statistics, 14,
249â€“266.
Gregory, J., Dixon, K., Stouffer, R., Weaver, A., Driesschaert, E., Eby, M., Fichefet, T., Hasumi,
H., Hu, A., Jungclaus, J., Kamenkovich, I., Levermann, A., Montoya, M., Murakami, S.,
Nawrath, S., Oka, A., Sokolov, A. and Thorpe, R. (2005). A model intercomparison of
changes in the Atlantic thermohaline circulation in response to increasing atmospheric CO2
concentration. Geophysical Research Letters, 32, L12703.

Assessing the Probability of Rare Climate Events
429
Grootes, P., Stuiver, M., White, J., Johnsen, S. and Jouzel, J. (1993). Comparison of oxygen
isotope records from the GISP2 and GRIP Greenland ice cores. Nature, 366, 552â€“554.
Hankin, R. (2005). Introducing bacco, an R bundle for Bayesian analysis of computer code
output. Journal of Statistical Software, 14, 1â€“21.
Haylock, R. and Oâ€™Hagan, A. (1996). On inference for outputs of computationally expensive
algorithms with uncertainty in the inputs. In Bayesian Statistics 5, (ed. J. Bernardo, J. Berger,
A. Dawid, and A. Smith), pp. 629â€“637. Oxford University Press, Oxford.
Higdon, D., Gattiker, J., Williams, B. and Rightley, M. (2008). Computer model calibra-
tion using high dimensional output. Journal of the American Statistical Association., 103,
570â€“583.
Huybrechts, P. and De Wolde, J. (1999). The dynamic response of the Greenland and
Antarctic ice sheets to multiple-century climatic warming. Journal of Climate, 12,
2169â€“2188.
Hyvarinen, A. and Oja, E. (2000). Independent component analysis: algorithms and applica-
tions. Neural Networks, 13, 411â€“430.
IPCC (2007). Climate Change 2007 â€“ The Physical Basis. Cambridge University Press, Cam-
bridge.
Jolliffe, I. (2002). Principal Component Analysis, (2nd edn.). Springer, New York.
Keeling, C. and Whorf, T. (2005). Atmospheric CO2 records from sites in the SIO air sampling
network. In Trends: A Compendium of Data on Global Change, online at Carbon Dioxide
Information Analysis Center, Oak Ridge National Laboratory, U.S. Department of Energy,
Oak Ridge, Tenn., U.S.A. [http://cdiac.ornl.gov/trends/co2/sio-mlo.htm].
Kennedy, M. and Oâ€™Hagan, A. (2001). Bayesian calibration of computer models. Journal of the
Royal Statistical Society: B, 63, 425â€“464.
Lenton, T. M., Held, H., Kriegler, E., Hall, J. W., Lucht, W., Rahmstorf, S. and Schellnhuber, H.
J. (2008). Tipping elements in the Earthâ€™s climate system. Proceedings of the National Academy
of Sciences of the United States of America, 105, 1786â€“1793.
Lorentz, E. (1963). Deterministic nonperiodic ï¬‚ow. Journal of the Atmospheric Sciences, 20,
130â€“141.
McKay, M., Beckman, R. and Conover, W. (1979). A comparison of three methods for selecting
values of input variables in the analysis of output from a computer code. Technometrics, 21,
239â€“245.
Nakicenovic, N. and Swart, R. (2000). Special Report on Emissions Scenarios: A Special Report of
Working Group III of the Intergovernmental Panel on Climate Change. Cambridge University
Press.
Oakley, J. (1999). Bayesian uncertainty analysis for complex computer codes. Ph.D. thesis,
University of Shefï¬eld, Shefï¬eld.
Oakley, J. E. (2004). Estimating percentiles of uncertain computer code outputs. Applied Statis-
tics, 53(Part 1), 83â€“93.
Oakley, J. E. and Oâ€™Hagan, A. (2002). Bayesian inference for the uncertainty distribution of
computer model outputs. Biometrika, 89, 769â€“784.
Oâ€™Hagan (2006). Bayesian analysis of computer code output: A tutorial. Reliability Engineering
and System Safety, 91, 1290â€“1300.
Oâ€™Hagan, A., Kennedy, M. C. and Oakley, J. E. (1999). Uncertainty Analysis and other inference
tools for complex computer codes. In Bayesian Statistics 6, (ed. J. Bernardo, J. Berger, A.
Dawid, and A. Smith), pp. 525â€“582. Oxford University Press, Oxford.
Pedlosky, J. (1992). Geophysical Fluid Dynamics, (2nd edn.) Springer-Verlag, Berlin.
Rahmstorf, S., Cruciï¬x, M., Ganopolski, A., Goosse, H., Kamenkovich, I., Knutti, R.,
Lohmann, G., Marsh, R., Mysak, L., Wang, Z. and Weaver, A. (2005). Thermohaline

430
The Oxford Handbook of Applied Bayesian Analysis
circulation hysteresis: A model comparison. Geophysical Research Letters, 32. L23605,
doi:10.1029/2005GL023655.
Rahmstorf, S. and Ganopolski, A. (1999). Long-term global warming scenarios computed with
an efï¬cient coupled climate model. Climatic Change, 43, 353â€“367.
Rohling, E. and Palike, H. (2005). Centennial scale climate cooling with a sudden cold event
around 8,200 years ago. Nature, 434, 975â€“979.
Rougier, J. (2007). Probabilistic inference for future climate using an ensemble of climate
model evaluations. Climatic Change, 81, 247â€“264.
Smith, A. and Gelfand, A. (1992). Bayesian statistics without the tears: A sampling-resampling
perspective. The American Statistician, 46, 84â€“88.
Stommel, H. (1961). Thermohaline convection with two stable regimes of ï¬‚ow. Tellus, 13,
224â€“230.
Vellinga, M. and Wood, R. (2002). Global climatic impacts of a collapse of the Atlantic thermo-
haline circulation. Climatic change, 54, 251â€“267.

Â·17Â·
Models for demography of plant populations
James S. Clark, Dave Bell, Michael Dietze, Michelle Hersh, Ines Ibanez,
Shannon L. LaDeau, Sean McMahon, Jessica Metcalf, Emily Moran,
Luke Pangle and Mike Wolosin
17.1 Introduction
Ecologists seek to understand how demographic rates contribute to species
diversity. Birth, growth, and survival together determine population growth.
Demographic rates are related to one another, and they depend in complex
ways on environmental variables. At a given age, an organism allocates energy
in ways that affects current and future growth, fecundity, and survival risk.
These relationships change through time as resource availability changes, and
organisms develop and age. For the population ecologist challenges include
inference not only on speciï¬c demographic rates, but also on how they combine
to determine population growth. In this chapter we discuss how hierarchical
Bayes analysis can help synthesize information from a range of sources to
understand how demographic rates relate to one another and might contribute
to biodiversity.
The key challenges to demographic inference in ecology include the avail-
ability of many sources of incomplete information, often measured at different
scales, and the large number of interactions among demographic components.
Population growth depends on all demographic rates, birth, growth, death,
and migration. Each of these rates can respond to a ï¬‚uctuating environment,
including other organisms. Many factors are observable or only weakly related
to factors that can be measured. For example, rarely can the cause of death
be determined for organisms in the wild. Without knowledge of cause it is
difï¬cult to isolate and model individual risk factors, let alone how they interact.
Even where cause might be identiï¬ed, challenges remain. If death could be
attributed to, say, â€˜droughtâ€™ (Condit et al. 1995, Suarez et al. 2004, Nepsted et al.
2007, van Mantgem and Stephenson 2007), a series of questions arise: is it
the daily, seasonal, or annual average soil moisture that is most important?
Is it duration or intensity? Which interactions determine why only a fraction
of the population died? We rarely have the information to address such ques-
tions, and we often lack understanding of the important scales (e.g. weekly
or monthly drought?). These challenges have necessitated a superï¬cial view

432
The Oxford Handbook of Applied Bayesian Analysis
of demography that is focused on annual rates with limited connection to
covariates.
To motivate some of the complexity that follows, consider limitations of
current demographic models. Traditionally, most demographic analyses include
no predictors beyond age or size (i.e. the standard age-structured model (Leslie
matrix) or stage-structured model (see the reviews of Caswell 2001, Gurevitch
et al. 2002)). Models involving covariates tend to include a demographic rate
as a response variable (offspring born per female per year, annual growth rate,
fraction of the population making a transition to a different stage of life, survival
probability) and, perhaps, a small number of predictors. The conclusions that
can be drawn from such models are limited, because it is recognized that they
may not accommodate important factors affecting the data, even for simple
experiments. For example, seed production of trees is rarely directly observed
in natural populations, because it cannot be quantiï¬ed in forest canopies. The
covariates are often represented by crude indices. The standard assumption that
error should enter as a stochastic envelope around a deterministic function of
predictor variables, such as
yi = xâ€²
iâ€š + Îµi
implies that xi is known much better than yi. If xi is a GIS layer, an instrument
that records with error, an incubation culture of biological activity, a classiï¬-
cation scheme based on unreliable detection, or the interpretation of a fuzzy
image, we might better represent the problem by including a model for x. If so,
we need to consider how to coherently connect a model for x and a model for y
and how to allow for uncertainty and still estimate everything. Growth rates are
usually measured with less error than are the environmental covariates used
as predictors, including such difï¬cult-to-quantify resources as solar radiation
reaching the partially shaded crown, soil moisture, and nutrient supply, having
substantial spatiotemporal variation that is never well quantiï¬ed (Beckage and
Clark 2003, Kobe et al. 2006, Mohan et al. 2007). If mortality is modelled as
a function of growth, and observations are not available until after death, the
growth rates are rarely available, requiring either annual measurements on
trees that both survive and die up until the time of death or increment cores
of trees (which are too laborious to obtain on large numbers of individuals).
Either way, a model is needed to account for the way in which the covariate
data were collected (Kobe et al. 1995, Wyckoff and Clark 2000). Moreover,
we are now considering multiple demographic rates (growth and survival),
connected by virtue of the fact that individuals near death may allocate less to
growth.
As ecologists increasingly want comprehensive inference that derives from
multiple underlying processes connecting inputs and responses, both of which
are partially known, the challenge comes in devising ways to synthesize the

Models for Demography of Plant Populations
433
sources of information, allow for observation error and uncertainty in the
underlying model itself. How can we capture interactions between individual
models for growth, fecundity, and mortality risk? Doing so requires that they be
ï¬tted simultaneously, as part of an integrated model of demographic change.
The model must reï¬‚ect the uncertainties that enter through both process and
data at multiple stages. Useful inference requires that we exploit information
coming not only from data, but also from theory and previous observations and
experiments.
Multistage models provide a natural framework for organizing how ecolo-
gists think about inference. Bayesian techniques provide a natural approach for
analysis of such models. Implementation requires that they can be structured to
provide transparency regarding assumptions, model behaviour, and parameter
estimates. For example, does the process model capture the relationships in
realistic ways, and would we recognize failure to do so? Does the model make
realistic predictions at all levels, including for state variables and observations
at different scales? The application presented here highlights ways to integrate
information for demographic inference, connecting models for important com-
ponents of the problem, each allowing for uncertainty. We model fecundity,
dispersal, growth, and survival with covariates that include some of these demo-
graphic rates and light availability, a key resource that limits plant growth. We
show how the large number of estimates that come from the analysis can be
summarized synthetically to provide deeper insight about relationships among
demographic rates and how they respond to covariates.
17.2 Demographic data
We use data collected from tree plots j = 1, . . . , J and covariates to infer
demographic rates. Data include observational studies and whole-stand manip-
ulations, which allow us to break up correlations in some of the important
covariates. Study areas include J = 9 plots of mapped forest stands in the Pied-
mont (Duke Forest) and southern Appalachian Mountains (Coweeta) of North
Carolina, USA. The plots were selected to span a range of topographic, soil
moisture, soil types, and elevation characteristics (Table 17.1), supplemented
with experimental manipulations. In this study we report observational data
come from trees i = 1, . . . , IJ on plots j in years t âŠ‚{tij, tij + 1, . . . , Tij}. Plots
were established in years t j âŠ‚{1991, . . . , 2000}, when trees were ï¬rst mapped,
identiï¬ed to species, and measured for diameter. The ï¬rst observation year
for individual ij is the year when plot j was established, tj, or when the
tree grew to sufï¬cient size (2 m in height) to be measured, whichever came
ï¬rst. The last observation year Tij â‰¤2008 is the last year the individual was
observed, at which point it might still be alive or not. Species codes are listed

434
The Oxford Handbook of Applied Bayesian Analysis
Table 17.1 Plot characteristics and and number of trees by plot. Species codes are
listed in Table 17.3.
Plot
C118
C218
C318
C427
C527
CLG
CUG
DBW
DHW
Total
Elev (m)
780
820
870
1110
410
030
140
70
70
Lat/Long
35â—¦03â€²N, 83â—¦27â€²W
35â—¦58â€²36â€²â€²N
79â—¦5â€²48â€²â€²W
Soil type
Typic & humic hapludults, typic dystrochrepts,
Typic & oxyaquic
typic haplumbrepts
vertic hapludalfs
First year
1992
1992
1992
1992
1992
2000
2000
2000
1999
Area (ha)
0.64
0.64
0.64
0.64
0.64
2.75
1.45
4.11
2.40
13.91
acru
496
136
211
257
14
982
608
2761
666
6131
acsa
0
2
1
0
82
35
0
13
0
133
acpe
5
223
15
15
230
451
21
0
0
960
acba
0
0
0
0
0
0
0
4
114
118
acun
0
0
0
0
0
13
0
2
0
15
beal
0
1
2
0
157
0
0
0
0
160
bele
5
38
12
4
66
8
1
0
0
134
beun
0
0
0
0
0
29
38
0
0
67
caca
0
0
0
0
0
0
0
146
439
585
cagl
56
33
41
15
5
36
10
153
31
380
caov
0
0
2
0
0
0
0
0
66
68
cato
1
0
0
0
0
48
3
395
51
498
caun
1
0
1
2
0
162
5
77
73
321
ceca
0
0
0
0
0
0
0
282
39
321
coï¬‚
45
78
27
10
3
118
14
1405
501
2201
fram
0
0
1
4
63
46
0
658
802
1574
list
0
0
0
0
0
0
0
1523
648
2171
litu
11
70
12
9
0
654
6
371
161
1294
nysy
103
30
113
117
0
282
335
361
457
1798
piri
36
0
0
0
0
0
0
0
0
36
pist
2
16
0
0
0
0
0
0
0
18
pita
0
0
0
0
0
0
0
391
175
566
piec
0
0
0
0
0
0
0
76
1
77
pivi
0
0
0
0
0
0
0
25
1
26
qual
12
0
0
0
1
0
0
231
65
309
quco
25
7
1
14
0
51
44
0
0
142
qufa
0
0
0
0
0
0
0
21
5
26
quma
10
0
0
0
0
0
0
20
0
30
quph
0
0
0
0
0
0
0
18
73
91
qupr
91
34
102
101
0
180
173
0
0
681
quru
38
5
21
57
24
139
49
65
0
398
qust
0
0
0
0
0
0
0
41
41
82
quve
45
6
7
3
0
12
14
30
0
117
quun
0
0
0
0
0
18
18
4
0
40
rops
24
6
4
12
0
95
30
1
0
172
tiam
0
5
0
0
87
0
0
0
0
92
tsca
8
27
8
74
3
243
38
0
0
401
ulal
0
0
0
0
0
0
0
719
558
1277
ulam
0
0
0
0
0
0
0
69
147
216
ulru
0
0
0
0
0
0
0
0
64
64
ulun
0
0
0
0
0
0
0
139
86
225
Total trees
1014
717
581
694
735
3602
1407
10001
5264
24015
Seed traps
20
20
20
20
20
73
43
128
66
410
Total seeds 6413
46964
22685
13298
334374
20814
10801
75481
23487
554317

Models for Demography of Plant Populations
435
max ave seed = 110
max ave seed per trap = 71
max ave ln(f) per tree = 11.4
max ave se ln (f) = 4.4
100 m
max ave ln(f) = 11.1
max ave se ln(f) = 2.8
C118
DBW
100 m
N
Acer rubrum
Fig. 17.1 Examples of two mapped stands used for demographic inference, showing stems of a
single genus, Acer, represented by circles. Boxes are shown at seed trap locations with box size
proportional to average annual seed collection for the entire study period. Plot C118 is small and
has not been manipulated. Plot DBW is larger (note scale bars), with seed traps clustered in and
around the locations of eight canopy gaps created in 2002. From left the series of three maps for
both locations have circle size scaled to (1) stem diameter, (2) mean estimate of tree fecundity, and
(3) standard error of tree fecundity estimates (not to be confused with the standard deviation in
fecundity over time).
in Table 17.2. Example maps of two study plots are shown in Figure 17.1. Envi-
ronmental data include static variables (elevation, slope, aspect) and variables
that ï¬‚uctuate over time, with additional variation among plots (temperature)
and within plots (soil moisture, light penetration). This analysis focuses on how
light availability and tree size affects demographic rates, including relationships
among them.
A subset of the mapped plots was used for canopy gap experiments. Follow-
ing collection of several years of pretreatment data, trees were removed from
the canopy in 20 m or 40 m wide patches to simulate the small and large treefall
gaps that occur when a single tree falls or when groups of trees fall, as during
storms. Canopy trees were pulled down using a skidder or bulldozer located
outside the plot and left in position, some having snapped, others uprooted.

436
The Oxford Handbook of Applied Bayesian Analysis
Table 17.2 Species codes used in other tables and ï¬gures.
Code
Species
Code
Species
acru
Acer rubrum
piri
Pinus rigida
acsa
Acer saccharum
pist
Pinus strobus
acpe
Acer pensylvanicum
pita
Pinus taeda
acba
Acer barbatum
piec
Pinus echinata
acun
Acer unknown
pivi
Pinus virginiana
beal
Betula alleghaniensis
qual
Quercus alba
bele
Betula lenta
quco
Quercus coccinea
beun
Betula unknown
qufa
Quercus falcata
caca
Carpinus caroliniana
quma
Quercus marilandica
cagl
Carya glabra
quph
Quercus phellos
caov
Carya ovata
qupr
Quercus montana
cato
Carya tomentosa
quru
Quercus rubra
caun
Carya unknown
qust
Quercus stellata
ceca
Cercis canadensis
quve
Quercus velutina
coï¬‚
Cornus ï¬‚orida
quun
Quercus unknown
fram
Fraxinus americana
rops
Robinia pseudoacacia
ilde
Ilex decidua
tiam
Tilia americana
ilop
Ilex opaca
tsca
Tsuga canadensis
list
Liquidambar styraciï¬‚ua
ulal
Ulmus alata
litu
Liriodendron tulipifera
ulam
Ulmus americana
nysy
Nyssa sylvatica
ulru
Ulmus rubra
oxar
Oxydendron arboreum
ulun
Ulmus unknown
Damage to canopy trees and understory trees (snapped or bent by pulled trees)
was recorded as the basis for analysis of damage effects on growth (Dietze and
Clark 2008). For the analysis presented here, manipulation had the greatest
effect on light availability, represented by â€˜exposed canopy areaâ€™.
To allow for inference on demographic interactions, we use a structure that
combines important relationships but is constrained by what can be observed or
inferred. An individual is characterized by several state variables, some of which
are constant, some change over time, some are continuous, and others discrete.
State variables differ in terms of how directly each can be observed. Trees are
classiï¬ed according to genus (e.g. Quercus for oak) and species (Q. rubrum for
red oak). The genus to which a tree belongs is known. The species is also taken
to be known, in the sense that it will not be inferred, but seeds are conï¬dently
identiï¬ed only to genus. For this reason, trees of the same genus are modeled
together. The genera and species are summarized in Table 17.3. There is an
unknown species class for trees from several genera, where large individuals
could not be conï¬dently ascribed to a particular species (Table 17.3). Because
many seeds can only be classiï¬ed to the level of genus, we infer seed produc-
tion as the combined contributions from all trees in that genus. For example,
the analysis for Quercus includes 10 species, because not all acorns could be
identiï¬ed to species level (Clark et al. 1998, 2004), whereas the analysis of

Models for Demography of Plant Populations
437
Table 17.3 Seed-trap years (same for all taxa) and tree years, grouped by plot and by
genus, as analysed here. Species codes are listed in Table 17.3.
Plot
C118
C218
C318
C427
C527
CLG
CUG
DBW
DHW
Total
Trap year
320
320
320
320
320
490
301
1016
342
3749
Acer
acru
8432
2312
3587
4369
238
8838
5472
24849
6660
64757
acsa
0
34
17
0
1394
315
0
117
0
1877
acpe
85
3791
255
255
3910
4059
189
0
0
12544
acba
0
0
0
0
0
0
0
36
1140
1176
acun
0
0
0
0
0
117
0
18
0
135
Betula
beal
0
17
34
0
2669
0
0
0
0
2720
bele
85
646
204
68
1122
72
9
0
0
2206
beun
0
0
0
0
0
261
342
0
0
603
Carpinus
caca
0
0
0
0
0
0
0
1314
4390
5704
Carya
cagl
952
561
697
255
85
324
90
1377
310
4651
caov
0
0
34
0
0
0
0
0
660
694
cato
17
0
0
0
0
432
27
3555
510
4541
caun
17
0
17
34
0
1458
45
693
730
2994
Cercis canadensis
ceca
0
0
0
0
0
0
0
2538
390
2928
Cornus ï¬‚orida
coï¬‚
765
1326
459
170
51
1062
126
12645
5010
21614
Fraxinus americana
fram
0
0
17
68
1071
414
0
5922
8020
15512
Liquidambar styraciï¬‚ua
list
0
0
0
0
0
0
0
13707
6480
20187
Liriodendron tulipifera
litu
187
1190
204
153
0
5886
54
3339
1610
12623
Nyssa sylvatica
nysy
1751
510
1921
1989
0
2538
3015
3249
4570
19543
Pinus
piri
612
0
0
0
0
0
0
0
0
612
pist
34
272
0
0
0
0
0
0
0
306
pita
0
0
0
0
0
0
0
3519
1750
5269
piec
0
0
0
0
0
0
0
684
10
694
pivi
0
0
0
0
0
0
0
225
10
235
Quercus
qual
204
0
0
0
17
0
0
2079
650
2950
quco
425
119
17
238
0
459
396
0
0
1654
qufa
0
0
0
0
0
0
0
189
50
239
quma
170
0
0
0
0
0
0
180
0
350
quph
0
0
0
0
0
0
0
162
730
892
qupr
1547
578
1734
1717
0
1620
1557
0
0
8753
quru
646
85
357
969
408
1251
441
585
0
4742
qust
0
0
0
0
0
0
0
369
410
779
(cont.)

438
The Oxford Handbook of Applied Bayesian Analysis
Table 17.3 (Continued).
Plot
C118
C218
C318
C427
C527
CLG
CUG
DBW
DHW
Total
quve
765
102
119
51
0
108
126
270
0
1541
quun
0
0
0
0
0
162
162
36
0
360
Robinia pseudoacacia
rops
408
102
68
204
0
855
270
9
0
1916
Tilia americana
tiam
0
85
0
0
1479
0
0
0
0
1564
Tsuga canadensis
tsca
136
459
136
1258
51
2187
342
0
0
4569
Ulmus
ulal
0
0
0
0
0
0
0
6471
5580
12051
ulam
0
0
0
0
0
0
0
621
1470
2091
ulru
0
0
0
0
0
0
0
0
640
640
ulun
0
0
0
0
0
0
0
1251
860
2111
Carpinus includes a single species (Table 17.3). Trees retain the species identity,
having some parameters that are species-speciï¬c, whereas seeds are modelled
as being potentially produced by trees of the entire genus on a probabilistic
basis. This approach allows combination of observations at the scale of individ-
ual trees and at seed-trap scale (a seed traps accumulate seeds from all trees
simultaneously).
In addition to diameter and species, individual level observations include sur-
vival, canopy status, and reproductive maturation status. Canopy status involved
ordinal classes derived from standard classiï¬cations used in forestry. At one to
four year intervals, individuals were assigned to:
Class 1: suppressed in the understory, with access limited to sunï¬‚ecks (e.g.
intermittent patches of direct sunlight);
Class 2: intermediate, with not more than 20% of the canopy exposed to
some direct sunlight;
Class 3: codominant, with > 20% of the canopy exposed to direct sunlight
during part of each day.
A suppressed individual in the understory would be assigned Class 1 status, but
could change to Class 3 status if it occupied a canopy gap following loss of the
overstory.
Additional information on canopy status comes from low-altitude aerial
photo coverage of plots, used to segment and measure canopy areas of trees
visible from above. The modelling of canopy exposure based on status and
remote sensing observations, combined with allometric models of canopy area
is described in Valle et al. (2009). Posterior means and variances from that
analysis are used as prior means and variances for the analysis presented here.

Models for Demography of Plant Populations
439
Maturity status observations were made at irregular years during the ï¬‚ow-
ering season (taxa such as Ulmus and Acer rubrum have conspicuous ï¬‚owers
before leaf-out in spring), the fruiting season (fruits often visible in the lower
canopy include Cornus ï¬‚orida, Cercis canadensis), and winter (lack of leaves
permits identiï¬cation reproductive structures of Liriodendron tulipifera, Liq-
uidambar styraciï¬‚ua). Classes of observations are:
uncertain: the largest class, because fruits and/or ï¬‚owers are difï¬cult to
observe;
not mature: if the entire canopy could be clearly observed to have no fruiting
structures during the known ï¬‚owering and/or fruiting season;
ï¬‚owering: establishes that the individual is mature;
seeds/fruits present: establishes maturity and, for dioecious species, female
status;
female or male ï¬‚owers present: in rare cases individuals of a dioecious species
could be assigned gender on the basis of ï¬‚ower structure.
The observations are summarized by maturation status and gender status in
Table 17.4, where Ã‹ij,t = p(Q ij,t = 1) is the probability that individual i on plot
j is mature in year t, v = p(qij,t = 1|Q ij,t = 1) is the probability that a mature
individual will be identiï¬ed as such, Ë† = p(Hij = 1) is the fraction of individuals
that are female for the entire population. The probabilities of Q|q and H|h are
provided in Table 17.4 primarily for explanatory purposes, because data models
discussed in Section 17.3 involve multiple observations. Note that statuses
must be modeled only for Table 17.4 entries not containing a zero or a one.
Table 17.4 Indicators and probabilities of maturity and gender conditional
on observations.
Observation
Maturity
indicator
qij,t
Maturity
probability1
Pr(Q ijt = 1|qij,t)
Gender
indicator
hij,t
Gender
probability1,2
Pr(Hij = 1|hij,t)
no observation
â€”
Ã‹ij,t
â€”
Ë†
uncertain
0
(1 âˆ’v)Ã‹ij,t
1 âˆ’vÃ‹ij,t
0
Ë†
not mature
âˆ’1
0
0
Ë†
ï¬‚owering
1
1
0
Ë†
seeds/fruits
1
1
1
1
male or female ï¬‚owers
1
1
0 or 1
0 or 1
1 The probabilities are shown for the special case that there is a single observation
per individual. In fact, there are multiple status observations, modelling of which is
discussed in Section 17.3.1. Symbols are Ã‹ â€“ probability of being in the mature state,
v â€“ probability of detecting mature status, Ë† â€“ female fraction of the population.
2 Gender status is assumed static (H has subscript ij), whereas there are multiple
observations of status for each individual (h has subscript ij,t).

440
The Oxford Handbook of Applied Bayesian Analysis
Unfortunately, most observations are â€˜uncertainâ€™, requiring that most statuses
must be modelled.
Seed trap data are the basis for fecundity estimates, and they further con-
tribute to estimates of maturation and gender statuses. Seeds cannot be counted
in the dense canopies characteristic of closed forests, but spatiotemporal seed
data can be used to model fecundity (Clark et al. 1998, 1999, 2004). Fecundity
is estimated from seed traps k = 1, . . . , K J using models of dispersal (Clark
et al. 1998, 1999, 2004), where K J ranged from 20 to 128 seed traps. Seed
traps were emptied from twoâ€“four times annually, seeds identiï¬ed to genus
or species (some seeds can only be identiï¬ed to the genus level), and counted.
For modelling purposes, seed data were accumulated to total per trap per year,
skj,t (Table 17.3).
Diameter measurements and increment cores provide information on indi-
vidual tree growth. Because diameter ï¬‚uctuates with stem moisture content,
and diameter measurements have error, we measured diameters Dij,t at 1â€“4
year intervals. In addition to diameter measurements, increment cores were
extracted from a subset of trees, providing a record of past growth dij,t =
Dij,t+1 âˆ’Dij,t for the individual up to the year in which the core was extracted.
Modeling of diameter-census and increment-core data are described by Clark
et al. (2007). Posterior means and variances for each tree year obtained from
that analysis are used as priors for the analysis presented here.
17.3 Models to synthesize data and previous knowledge
Consider a forest containing trees of different species, gender, age, and size,
each experiencing the local environment in ways that depend on some factors
that can be measured and others that cannot; here we focus on light availability.
The responses of interest include growth rate, gender ratio, maturation status,
fecundity, seed dispersal, and survival risk. Combinations of these demographic
rates within individuals determine the growth rates of populations and, thus,
community biodiversity.
An individualâ€™s response to the environment produces variation in demo-
graphic rates. Resource availability (here we consider light) contributes to
overall health. Resources vary at many scales, depending on supply and on
competition with neighbours. For example, light levels vary throughout the day
and seasonally, and they are reduced by nearby trees that shade one another.
Fine scale heterogeneity is most obvious where canopy gaps form, allowing
from 10 to 100% of full sunlight to penetrate to the forest ï¬‚oor, depending
on gap size. By contrast, the uninterrupted canopy intercepts 95 to 99% of
incoming radiation. Due to spatial heterogeneity, individual trees are exposed
to conditions that differ from their neighbors, and these differences can change

Models for Demography of Plant Populations
441
over time with changes in canopy structure and as interannual climate vari-
ation moderates the impact of resource supply. Some of these factors can be
measured in the environment, but it is important to also allow for variation
that cannot be ascribed to measurable factors. Here we describe the model for
demographic rates.
17.3.1 Gender and maturation
The gender of a tree remains constant, whereas maturation status changes from
immature to mature over time. Maturation status and gender can be conï¬rmed
for some trees (Table 17.4). Presence of seeds indicates maturity and (for dioe-
cious species) female status. The presence of ï¬‚owers indicates maturity, but it
does not mean that an individual belonging to a dioecious species is female,
unless it can be identiï¬ed as a female ï¬‚ower. When the entire crown can be
observed, lack of ï¬‚owers or fruits during the ï¬‚owering/fruiting season is taken
to indicate immaturity. In crowded stands, absence of reproductive effort can
rarely be conï¬rmed by such observations, so detection is uncertain.
The transition from immature to mature is treated as a hidden Markov
process. Modelling of gender and maturation is complicated by the fact
that probabilities depend on the entire history of observations on individual
ij. Consider an individual that is observed once. If maturation status is
uncertain (qij,t = 0 in Table 17.4), then the individual is mature with probability
(1 âˆ’v)Ã‹ij,t/(1 âˆ’vÃ‹ij,t). But additional observations complicate the model. For
example, mature status is more likely for an individual last known to be
immature 10 year ago than it is for an individual last known to be immature
one year ago. Likewise, an individual is more likely to be mature in year t if it
is ï¬rst known to be mature in year t + 1 than if it is ï¬rst known to be mature in
year t + 10. Furthermore, an individual observed to be of unknown status once
is more likely to be mature than is an individual observed to be of unknown
status 10 times. In other words, modelling of status must accommodate
not only the probabilities contained in Table 17.4, but also how they must
be combined to accommodate the differing observation histories of each
individual. Here we discuss these probabilities. We ï¬rst discuss conditioning
on observations listed in Table 17.4, followed by seed data.
The unconditional probability that individual i on plot j is female is termed
the female fraction Pr(Hij = 1) = Ë†, (Table 17.2) and the probability of being
male Pr(Hij = 0) = 1 âˆ’Ë†. Observations of gender status were obtained at irreg-
ular intervals hij,t. If the individual is observed to be female hij,t = 1 or male
hij,t = 0 then
Pr

Hij = 1
hij,t = 1

= 1
Pr

Hij = 0
hij,t = 0

= 1.

442
The Oxford Handbook of Applied Bayesian Analysis
In other words, gender is only assigned if it is certain. If there are no observa-
tions for gender, information enters solely though seed rain data. If seed density
near an individual is high, then the probability that it is female is large, and vice
versa. Thus, fecundity and maturation must be modeled together.
The unconditional probability that the individual is in the mature state
Pr(Q ij,t = 1) = Ã‹ij,t, increases with tree size and canopy exposure (access to
sunlight). The probability is parameterized as a logit link to diameter D and
canopy exposure ÃŽ
Ã‹ij,t =
exp

â€šÃ‹
0 + â€šÃ‹
1Dij,t + â€šÃ‹
2ÃŽij,t

1 + exp

â€šÃ‹
0 + â€šÃ‹
1Dij,t + â€šÃ‹
2ÃŽij,t
.
(17.1)
For computation we require conditional probabilities that derive from this
relationship. Two simple examples we detail in the appendix include the prob-
ability of making the transition in year t, given previous immaturity and future
maturity
â€°ij,t = Pr

Q ij,t = 1
Q ij,tâˆ’1 = 0, Q ij,t+1 = 1

=
dÃ‹ij,t
dÃ‹ij,t + dÃ‹ij,t+1
where
dÃ‹ij,t = â€šÃ‹
1dij,tÃ‹ij,t

1 âˆ’Ã‹ij,t

dt
and the probability that the transition year Ã™ij occurred in year t
â€°ij = Pr

Ã™ij = t
Ã™0
ij < Ã™ij < Ã™1
ij

=
dÃ‹ij,t

Ã‹ij,Ã™1
ij âˆ’Ã‹ij,Ã™0
ij

where the lower limit represents that last year the individual was known to
be immature and the upper limit represents the ï¬rst year the individual was
known to be mature. These two fundamental relationships are the basis for
models that incorporate observations for dioecious and monoecious species,
respectively (Appendix: Computation). The ï¬rst is a Bernoulli probability for
each year t; there is a probability for every tree year. The second is a probabil-
ity associated with an individual and depends on when the maturation event
occurred for that individual.
For monoecious species the joint distribution of maturation and fecundity is
represented by a discrete mixture that is conditional on the full history of status
observations on the individual, a vector qij = {qij,t, t = (tij, . . . , Tij)},
p

Q ij,t, fij,t
qij, xij,tâˆ’1, Q ij,tâˆ’1, Q ij,t+1

=

1 âˆ’â€°ij,t
1âˆ’Q ij,t 
â€°ij,t N

ln fij,t
Ã f |d
ij,t , V f |d
ij,t
Q ij,t
(17.2)
where the probability â€°ij,t = p(Q ij,t = 1|qij, Q ij,tâˆ’1 = 0, Q ij,t+1 = 1) is based on
the history of observations on ij, and the parameters for the log normal

Models for Demography of Plant Populations
443
distribution are conditional, coming from a bivariate state-space model for
fecundity (Appendix: Computation).
For dioecious species, we require gender and the probability for the entire
history of maturation Q ij and the probability associated with it, â€°ij. This distrib-
ution is given by
p

Hij, Q ij, fij
qij, xij,tâˆ’1

= (1 âˆ’Ë†)1âˆ’Hij â€°ij

Ë†

t

N

ln fij,t
Ã f |d
ij,t , V f |d
ij,t
Q ij,t
Hij
.
(17.3a)
If an observation establishes an individual as male we have the probability for a
maturation history during which no reproduction occurred
p

Q ij, fij = 0
qij, xij,tâˆ’1, Hij = 0

= â€°ij
(17.3b)
and for a female during which reproduction may or may not have occurred
p

Q ij, fij
qij, xij,tâˆ’1, Hij = 1

= â€°ij

t

N

ln fij,t
Ã f |d
ij,t , V f|d
ij,t
Q ij,t.
(17.3c)
Thus far, we have conditional relationships involving fecundity and maturation.
The conditional dependence on seed rain data is discussed in the next section.
17.3.2 Seed data and fecundity
Fecundity (seed production per individual per year) is not directly observed,
because seeds cannot be counted in crowded canopies. Like gender and matu-
ration status, indirect information comes from seed trap data, linked by way of
a transport model. Individuals that are mature and female can produce seeds.
Fecundity is thus zero for immature individuals and all male trees. For mature
females fecundity is taken as a continuous, positive variable.
Seeds accumulating in traps located throughout each stand j provide a basis
for inverse modeling of fecundity. The likelihood for seeds collected in trap k in
stand j in year t is taken to be Poisson
Po

sjk,t
Ajkgjk

f j,t; u

(17.4)
where Ajk is the area of the seed trap (0.16 m2 or 0.125 m2), and gjk,t is the
expected density of seed (mâˆ’2), including a parameter u. The expected seed
density depends on fecundities of all trees in stand k and a dispersal kernel K ,
added to a crude estimate of small background density of seed that might enter
the plot from outside the mapped boundaries, proportional to basal area of the
species in stand j, or BA j,t.
gjk

f j,t

= c Â· BAj,t +

i
fij,t K (rik; u).
(17.5)

444
The Oxford Handbook of Applied Bayesian Analysis
The dispersal kernel is taken to be a two-dimensional Studentâ€™s t, previously
found to ï¬t seed dispersal data well,
K (rik; u) =
1
ï£¿u

1 + r 2
ik
A
u
2
(17.6)
containing the scale parameter u (Clark et al. 1999, 2004). The term in equa-
tion (17.5) that includes basal area BA allows for the fact that some small
fraction of seed can derive from outside the plot, roughly proportional to the
basal area of the species.
In addition to seed data, fecundity depends on covariates. In the appendix we
discuss conditional relationships involving the likelihood for seed data (equa-
tion 17.3) and a multivariate regression for growth and fecundity, inï¬‚uenced
by covariates. We include as covariates tree diameter and light availability,
summarized by exposed canopy area. The relationship between covariates and
fecundity is described by a bivariate state-space model that additionally includes
growth (diameter increment) as a response variable. This bivariate model is
described with diameter growth in the next section.
17.3.3 Diameter growth and fecundity
Diameter growth (cm per year) is informed by two sparse data sets and by the
state-space model that includes fecundity. Censuses conducted at two to four
year intervals, which include measurements of diameter on all trees, provide
observations of diameter change over the measurement interval. Increment
cores are obtained for some individuals and provide annual rates of growth
up until the year in which the core was extracted. Because they are laborious to
obtain and they can damage trees, increment cores are not available for many
trees. Thus, both types of data are sparse, but in different ways; census data
exist for all individuals, but only in a few years, and increment cores were
taken from a subset of individuals, but cover all years up until the year the core
was collected. In consideration of the multiple data types and sparsity of both,
diameter growth was modeled in a two-step process, the ï¬rst step being a model
that assimilates the different types of data and generates posterior estimates
of growth for each tree year. This analysis is described in Clark et al. (2007)
and Metcalf et al. (2009). Products of this analysis include estimated means
and standard deviations for diameter and diameter increment in each tree-
year (Figure 17.2). These are used as priors for the second step, the analysis
described here.
The state-space model for growth and fecundity was developed to allow
for measurable covariates known to affect demography, random effects at the
individual level, year effects, and error. Random effects are included, because
many factors could affect individual health that cannot be assigned to observed

Models for Demography of Plant Populations
445
Census data
(a) First census 1992,
(b) First census 1992,
(c) First census 1992, died 2000,
(d) First census 1999,
increment core in 2006
increment core in 1998
increment core in 2008
1995
2000
2005
1995
2000
2005
Year
no increment core
12  20
Diameter (cm)
Diameter increment (cm)
12  20
14  22
22  30
0.0     1.5
0.0     1.5
0.0     1.5
0.0     1.5
Increment core data
1,  44,  44
1,  56,  56
2,  41,  262
9,  79,  1811
Fig. 17.2 Posteriors for diameter at left and diameter increment at right, represented by median
(solid) and 95% CI (dashed) compared with observations (dots). Series vary in length, depending
on when observations began and tree survival. Posteriors are generally narrow where increment
core data are available, and vice versa. At left are shown corresponding intervals for tree diameter
with diameter census data (dots). Numbers at right indicate (individual i, plot j, and a unique order
number in the data base). This example is for Quercus.
variables. These factors are related to genotype and spatial variation in resources
and factors that limit growth. Year effects are included because year-to-year
variation in climate is difï¬cult to quantify in ways that might be important for
trees. Year effects allow for variation in time that is shared across the entire
population. The growth-fecundity submodel is
yij,t = xij,tâˆ’1A + bt + bij + Îµij,t
bij âˆ¼N2(0, Vb)
Îµij,t âˆ¼N2(0, )
(17.7)
with a response vector that includes diameter growth and fecundity
yij,t =

ln

dij,t

, ln

fij,t

;
(17.8)

446
The Oxford Handbook of Applied Bayesian Analysis
covariates
xij,tâˆ’1 =
B
1, ln

Dij,tâˆ’1

, ln2
Dij,tâˆ’1

, ln

ÃŽij,tâˆ’1

, ln

dij,tâˆ’1
C
;
(17.9)
parameters for ï¬xed covariate effects A, ï¬xed year effects bt, random individual
effects bij, and error Îµij,t. If the genus includes multiple species, the vector
xij,t includes a ï¬xed effect for each species of the genus. Priors for regression
parameters are speciï¬ed such that ln(D) term takes up allometric effects of
size on fecundity when trees are small, and ln2(D) takes up senescence effects
on growth and fecundity when trees are large. Growth and fecundity rates are
both affected by individual size and by abiotic covariates in any given year. By
including allometric relationships in the ln(D) term, the exposed canopy area ÃŽ
term estimates the effect of light availability (as opposed to size) on growth and
fecundity (Section 17.4).
In addition to the regression for growth and fecundity, both variables condi-
tionally depend on data. For fecundity, conditional dependence for fij,t includes
all seed traps on plot j in year t. Because seed trap count sjk,t depends, in turn,
on all trees on plot j, the conditional dependence for all trees on plot j in year
t is f j,t > 0 involves equations (17.2) and (17.3),
p

f j,t
s j,t, . . .

âˆ

k
Po

sjk,t
Ajkgjk

f j,t; u
 
i
p

fij,t
Q ij,t, qij, xij,tâˆ’1

.
(17.10)
The second factor on the right hand side comes from equation (17.2). The
conditional means and variances for equation (17.2) are
Ã f |d
ij,t = Ã f + 12

ln dij,t âˆ’Ãd

11
V f |d
ij,t
= 22 âˆ’2
12/11
(17.11)
ij is the ijth element of , and unconditional means are
Ãd = xij,tâˆ’1aâ€¢d + bd,t + bd,i j
Ã f = xij,tâˆ’1aâ€¢ f + b f,t + b f,i j.
The terms on the right hand side include the columns of A, i.e. A = [aâ€¢d aâ€¢ f ],
and the year and individual effects associated with growth and fecundity. For
growth, we have the conditional dependence
p

dij,t

âˆN

ln

dij,t
Ãd| f , Vd| f

N

ln

dij,t
ln

d(0)
ij,t

, vij,t

I

d(0)
ij,t âˆ’âˆšvij,t

< ln

dij,t

<

d(0)
ij,t + âˆšvij,t

Bernoulli

zij,t
ÃŠij,t

(17.12)

Models for Demography of Plant Populations
447
where zij,t is the event that the individual survived interval (tâˆ’1, t), d(0)
ij,t and vij,t
are the prior mean and variance for log growth rate from the analysis of Clark
et al. (2007)(Figure 17.2), and conditional means and variances are
Ãd| f
j,t = Ãd + 12

fij,t âˆ’Ã f

22
V d| f
j,t
= 11 âˆ’2
12
A
22.
Note that the prior for the growth data is truncated to a width of two standard
deviations. The standard deviations are large for tree years in which there are
census data but no increment data (Figure 17.2). For years lacking increment
core data, estimates are more heavily inï¬‚uenced by random effects for
individuals and ï¬xed effects for years, thus borrowing information from within
the individual over time and from year-to-year variation that is shared by the
entire population. The last factor in equation (17.12) comes from survival
(Section 17.3.5). The survival probability enters the conditional probability,
because it depends on growth rate, through the binned diameter classes
(Section 17.3.5).
In addition to equation (17.7) that applies to mature individuals, we ï¬tted
an additional model for growth using all trees, regardless of maturation sta-
tus. This model is univariate, but includes the same covariates as used for
mature trees,
ln dij,t = xij,tâˆ’1a + bt + bij + Îµij,t
(17.13)
bij âˆ¼N

0, Vb(1,1)

Îµij,t âˆ¼N(0, w)
where a is the parameter vector (the ï¬rst column of A in equation 17.7), bt is the
ï¬rst element of bt in equation (17.7), and Vb(1,1) is the random effects variance
for diameter growth (element 1,1 of covariance matrix Vb) from equation (17.7).
17.3.4 Exposed canopy area
Light availability is included as a predictor of growth and fecundity. It is summa-
rized by an index, the area of the crown potentially exposed to sunlight, ÃŽij,t. This
index has non-zero values, but can be small, such as for the case of a suppressed
individual in the understory. It is estimated on the basis of three sources of
information in a separate analysis (Wolosin et al., in review). These data sources
are (i) low-altitude imagery on which crown area can be measured, (ii) ordinal
status classes assigned on the basis of ground observations, and allometric
measurements combined with models of solar geometry that yield calculations
of light availability throughout the day and over the growing season. A model
combines these sources of information. Posterior means and variances enter
this analysis as priors, just as described for diameter growth (Section 17.3.3).

448
The Oxford Handbook of Applied Bayesian Analysis
17.3.5 Survival
Survival probability is typically modeled as a function of growth rate (see the
appendix), which integrates many aspects of tree health (Kobe et al. 1995, Clark
and Clark 1996, Wyckoff and Clark 2000, Bigler and Bugmann 2004) and of size
(Clark and Clark 1996, King et al. 2006, Coomes and Allen 2007). A number of
functional forms have been used to relate survival to growth rate. The problem
with any functional form comes from the facts that (i) this relationship can be
strongly nonlinear, changing abruptly at growth rate values close to the lowest
range of values typically observed, and (ii) the distribution of data has a large
impact on estimates, and individuals close to death may be poorly represented
in data sets, because such individuals died disproportionately before the study
began. We have developed or modiï¬ed nonparametric approaches (Wyckoff
et al. 2000, 2000, Clark et al. 2007, Metcalf et al., in review) to describe this
relationship and apply one that combines not only growth rate, but also tree size
effects on mortality risk (Clark et al., 2007). We include tree size as a predictor of
survival, because mortality risk may increase as trees become large and senesce
or become susceptible to high winds (Batista et al. 1998, Uriarte et al. 2004,
Rich et al. 2007).
Let zij,t be the event that an individual ij is alive in year t, in which case it
survived from year t to t + 1 with probability
ÃŠij,t = 1 âˆ’

Ãdij,tâˆ’1 + ÃDij,tâˆ’1 âˆ’Ãdij,tâˆ’1ÃDij,tâˆ’1

.
(17.14)
There are discrete bins for both growth rate and diameter. In year t âˆ’1 indi-
vidual ijâ€™s growth rate bin is indicated by Ãdij,tâˆ’1 and its diameter bin is indi-
cated by ÃDij,tâˆ’1. There are monotonicity priors for both sequences, decreasing
for growth rate and increasing for diameter (Section 17.4). The likelihood is
Bernoulli(zij,t|ÃŠij,t). In the next section we specify priors.
17.4 Prior distributions
The model includes both informative and non-informative prior distributions.
Due to the size and complexity of the model, where possible, we used informa-
tive priors that are ï¬‚at but truncated in some fashion, to maximize transparency,
i.e. for identiï¬cation of the contributions of prior versus likelihood. Here we
summarize priors and how they were selected to balance information.
The ï¬xed effects in the state-space model (17.7) have ï¬‚at priors bounded by
values either having theoretical justiï¬cation or sufï¬ciently wide to not impact
estimates,
vec(A) âˆ¼I(a1 < vec(A) < a2)
(17.15)

Models for Demography of Plant Populations
449
where a1 and a2 are vectors of minimum and maximum values, respectively. We
describe prior values for speciï¬c elements of A, using indexing for elements
of A that assume a single intercept. Recall that there are separate intercepts
for each species included in a given genus (Table 17.3). The actual number
of rows of A is p = number of species + 4 (there are four covariates). The
ï¬rst subscript indicates the covariate (equation 17.9) and the second subscript
indicates the response (equation 17.8). Truncation points that affect estimates
include:
A21 â€“ diameter effect on diameter growth increment constrained to be near zero:
Diameter is included as a predictor, because we expect it to directly affect
fecundity â€“ large trees are capable of high seed production. We expect it to
also be correlated with the other response variable, diameter growth incre-
ment. Because that correlation should be taken up by canopy exposure,
rather than by tree diameter directly, we constrain this parameter value to
be near zero. From open-grown trees there is no clear evidence for a direct
size effect on diameter increment. The correlation between tree size and
diameter growth increment is expected to result from the fact that large
trees are more likely to have higher light exposure. Because there is no
theoretical justiï¬cation for non-zero values, limits on this parameter are
(âˆ’0.02, 0.02).
A22 â€“ diameter effect on fecundity constrained between 1.5 and 2.5: Allometric
arguments and empirical evidence suggest that potential fruiting yield
should scale with canopy width, which, in turn is roughly proportional to
diameter. In fact, this potential should not be realized for trees crowded by
neighbours. We allow for this effect of size on potential yield effect due to
size with the constraints (1.5, 2.5), as modiï¬ed by competition, which is
reï¬‚ected in exposed canopy area, i.e. the term including ÃŽ.
(A31, A32) â€“ large diameter effect is negative: The squared diameter term in
equation (17.9) is included to allow for potential senescence, a decline in
physiological function with age. Tree data sets rarely have sufï¬cient large
(potentially old) individuals to estimate these effects, but we can allow
that senescence does eventually occur by specifying that this effect only
has impact for especially large individuals. This term is constrained to be
negative.
(A41, A42) â€“ lag-1 effect of growth rate on growth rate and fecundity: This
effect was constrained to be effectively zero for growth rate (A41) but
unconstrained for fecundity (A42). We wanted to parametrize the effect on
fecundity, so it could be used for predictive modeling of potential tradeoffs
in time between growth and fecundity.
The estimates of growth and fecundity represent a balance between contri-
butions from the regression (i.e. the size and light covariates in equation 17.7)

450
The Oxford Handbook of Applied Bayesian Analysis
and data models for growth rates and seed data. It is not necessarily â€˜objectiveâ€™ to
use a non-informative prior for the state-space error covariance matrix, because
there is no objective criterion for balancing information that enters from mul-
tiple data types. Data are known to be noisy, particularly seed rain. We used
an informative prior on the error covariance matrix  to represent a level of
variation expected after that taken up by covariates, random effects, and year
effects and to assure that covariates were not overwhelmed by noise. The values
used for variances on the log growth (cm) and log fecundity (seeds per tree)
were 0.05 and 0.2, respectively. These allow for realistic levels of variation on
the non-log scale. We used the prior
Wishart
âŽ›
âŽâˆ’1

6
0.05 0
0
0.2
7âˆ’1
,
nIJT
âŽž
âŽ 
(17.16)
where nIJT = 
i, j (Tij âˆ’tij) is the number of tree-years in the study. Through
extensive sensitivity analysis, this prior was found to provide an acceptable
balance of data and regression model, contributing to the conditional posterior
approximately twice the weight coming from the regression.
By contrast, priors on random effects and year effects were weak â€“ we wanted
data to dominate these estimates. The prior for random effects is
Wishart
âŽ›
âŽVâˆ’1
b

6
0.2 0
0 2
7âˆ’1
,
max

3, nIJ/100

âŽž
âŽ 
(17.17)
where nIJ = 
j Ij is the number of trees. The second parameter in equa-
tion (17.17) is rounded to an integer value and ranged from 3 to 60 for different
species. The contribution to the conditional posterior ranged from about 1/10
to 1/50 of the weight coming from the regression.
The prior for ï¬xed year effects is
N2

bt


0 0
â€² , diag(100, 100)

(17.18)
and includes a sum-to-zero constraint (intercepts are included in A), imple-
mented directly in the Gibbs sampler.
Because many individuals are not mature, a separate univariate regression
is ï¬tted to all tree years, regardless of maturation status (equation 17.13). The
covariates are the same as those listed for the multivariate regression given
above, and sampling makes use of the univariate distributions corresponding to
each of the foregoing multivariate ones. These are Gaussian for ï¬xed (including
years) and random effects, and inverse Gamma for variances.

Models for Demography of Plant Populations
451
Diameter growth increments have a prior for each tree year taken from the
posterior from the analysis of Clark et al. (2007) and shown in equation (17.12).
Because there are thousands of such densities, the truncation of this posterior
at a width of two standard deviations was based on a prior belief that true
increments should be within this range. Diagnostics showed that posterior
estimates from this analysis did not tend to acccumulate at these truncation
values.
As with diameter increment, the Gaussian prior for canopy values is trun-
cated to two standard deviations in width,
ln ÃŽij,t | âˆ¼N

c(o)
ij,t,Cij,t

I

c(0)
ij,t âˆ’
;
Cij,t

< ln

ÃŽij,t

<

c(0)
ij,t +
;
Cij,t

(17.19)
where c(0)
ij,t and Cij,t are the prior mean and variance (log scale), taken from the
posterior for the analysis of canopy area (Wolosin et al., in review).
Priors for fecundity, maturation, gender, and missing seed data were either
non-informative or derived from previous observation. A ï¬‚at prior was used for
fecundity, truncated at the smallest number of seeds observed for a tree and at
values much larger than implied by observation of seed densities,
fij,t âˆ¼unif ( fmin, fmax) .
(17.20)
For instance, when deï¬ning fmin, we did not expect that a mature individual
would produce less seeds than typically contained in a single fruiting structure
(e.g. Pinus, Liriodendron, Liquidambar). For maximum values, we used parame-
ter estimates similar to those obtained in a simpler model (Clark et al. 1999) to
â€˜invertâ€™ seed density observations and thus approximate what might constitute
unrealistically high seed production for an individual of a given species and
size. The model for seed dispersal provides an expected seed density given the
spatial locations of trees of different sizes. For example, Acer rubrum seeds have
been observed at average densities of 102 seeds mâˆ’2 beneath mature trees but
not at average densities of 103 seeds mâˆ’2. This inversion was used not only
to set limits on fecundities for individual trees, but also to deï¬ne the (prior)
Poisson means for missing seed data.
The maturation diameter for an individual was assigned a prior that was trun-
cated at values below which we believed that no individuals could be mature and
above which we believed all individuals would be mature. These beliefs came
from independent observations of trees in similar settings. These limits on mat-
uration diameters translate to limits on maturation year (see appendix). There
are prior minimum and maximum diameters, which differ among species. The
female fraction was given the prior Be(Ë†|h1, h2) with h1 = h2 = 4, having a mean
of 0.5 and being dominated by the data. The probability of recognizing a mature
individual was assigned the prior Be(v|v1, v2) with v1 = v2 = 0.002nIJ, which has
a mean of 0.5 and is dominated by the data.

452
The Oxford Handbook of Applied Bayesian Analysis
Parameters for the logit function of maturation equation (17.1) were assigned
the prior
N

â€šÃ‹ bÃ‹, VÃ‹ 
I

â€šÃ‹
2,3 > 0

(17.21)
a truncated normal prior with mean vector bÃ‹ = [âˆ’3, 0.1, 0.1] and covariance
VÃ‹ = diag[10, 10, 10]. The positivity constraint on the second and third ele-
ments of the vector comes from the prior belief that the relationship between
maturation and diameter and light availability is non-negative.
Priors for the seed data model in equation (17.3), including the dispersal
parameter and the seed fraction originating outside the map, were
p(u, c) = N(u |u0, Vu ) N(c |c0, Vc ) I(u, c > 0)
(17.22)
where parameter values were chosen to be informative. For u they differ among
species; we used c0 = 0.02 and Vc = 0.01. There is a positivity constraint on u
and c.
The monotonicity priors on the parameter sequences Ãd and ÃD in equa-
tion (17.14) were designed to allow for uneven distribution of data and strong
nonlinearities. Because slow growth is associated with death, the observations
of growth rate below a certain threshold are rarely observed. However, this lack
of slow growth observations results from the fact that mortality risk increases
sharply at slow growth rates. For this reason, our sequence of Ãd values has
an intercept at 1. Although zero growth rates do occur in particular years, we
used this assumption as a way of approximating the sharp increase in risk that
can occur at low growth. This assumption is obviously ï¬‚exible. In addition to
monotonicity, there was an informative prior for values within the sequence ÃD,
which was Be(ak, bk), where ak is 0.001 for k = 1, 2, 3 and ak = 10 for k = 4, 5, 6,
bk = ak
 1
Ã0
k
âˆ’1

Ã0
k = [0.00001, 0.00002, 0.00003, 0.00004, 0.00005] .
(17.23)
This prior assures essentially zero values for juvenile trees (bins 1, 2, 3), but
is non-informative (but monotonically increasing) for large trees. Thus, the
diameter effect only affects large trees. Although small trees grow slowly and
thus are at higher mortality risk, this is a growth effect, not a diameter effect.
This informative prior allows us to separate the effect of slow growth from that
of large size, which could indicate senescence.
17.5 Computation
The posterior was simulated with Gibbs sampling, based on conditional poste-
riors that are discussed in the appendix, some of the embedded steps being

Models for Demography of Plant Populations
453
Metropolis. The simulation was initialized at prior mean parameter values
(diameter increments and crown areas), random draws from priors, or MLEs
based on simpler models (fecundities for trees were initially estimated without
year or individual effects using the approach of Clark et al. 1999).
Due to the size of the model, efforts were made to optimize code. Despite
the large number of years across many individuals within multiple plots, the
main Gibbs loop contains only three loops over years (including one to update
maturation/fecundity, one for missing seed data, and another for dispersal and
Poisson parameters), and no loops over individuals or plots. Data structures
that include pointer arrays were used to rebuild (reorder and restack) matrices
of state variables based on the changing gender and maturation statuses of trees
and tree-years, respectively.
Convergence was achieved with 10,000 iterations for species with moderate
numbers of individuals, but required up to 200,000 iterations for trees with
many individuals. There are a large number of parameters, not all of which
could be sampled efï¬ciently. The lowest updating rates and highest auto-
correlations were obtained for fecundities of dioecious species (Acer rubrum,
A. pennsylvatica, Fraxinus americana, and Nyssa sylvatica), due to the discrete
nature of Q ij,t and Hij, and the blocking over all tree-years within a plot. Thus,
for fecundity/maturation/gender of dioecious species, we selected for updating
at random 30% of the trees for a given iteration and embedded ï¬ve such
iterations within each Gibbs step.
17.6 Diagnostics
From 50,000 to 1,000,000 Gibbs steps were discarded, followed by 50,000 to
100,000 iterations that were retained for analysis. We inspected Gibbs chains for
all parameters as well as for samples of individual effects. Experiments involved
many parameter initializations; however, results presented here come from
single long runs for each taxon group. Acceptance rates for Metropolis steps
were generally above 0.2. The exception was for dioecious species, where low
acceptance rates were addressed by embedding multiple iterations per Gibbs
step (Section 17.5). To help evaluate results we compared priors and posteriors,
we considered predictive capacity, in terms of data used to the ï¬t model, and
we compared predictive intervals from the model with estimates of latent states
that could not be directly observed. Here we discuss some comparisons.
17.6.1 Some prior/posterior comparisons
Some of the estimates for parameters from an example taxon, Quercus, are
shown in Table 17.5. Estimates for marginal posteriors are accompanied by
ï¬tted truncated normal distributions, which would be used in the event that it

454
The Oxford Handbook of Applied Bayesian Analysis
Table 17.5 Posterior estimates for some of the main parameters in the model, shown for the taxon Quercus.
Symbol
(equation)
Parameter
p1
Marginal posterior
Fitted model2
N(p|m, s 2)I(p1 < p < p2)
mean
std dev
0.025
0.975
m
s
p1
p2
A (17.7)
quun.d
âˆ’1.65
0.0271
âˆ’1.7
âˆ’1.59
âˆ’1.65
0.0271
âˆ’4
1
quun.f
4.15
0.283
3.7
4.65
4.15
0.283
âˆ’1
10
diam.d
0.0196
0.000394
0.0186
0.02
0.393
0.0122
âˆ’0.02
0.02
diam2.d
âˆ’0.000155
5.55e âˆ’05
âˆ’0.000301
âˆ’0.000101
0.00662
0.000619
âˆ’0.2
âˆ’1eâˆ’04
cnpy.d
0.0591
0.00478
0.0496
0.0683
0.0591
0.00479
0.01
1
dlast.d
0.00992
8.34e âˆ’05
0.00969
0.01
0.0376
0.00163
âˆ’0.01
0.01
qual.d
âˆ’1.3
0.0151
âˆ’1.33
âˆ’1.27
âˆ’1.3
0.0151
âˆ’4
1
quco.d
âˆ’1.66
0.0168
âˆ’1.69
âˆ’1.62
âˆ’1.66
0.0169
âˆ’4
1
qufa.d
âˆ’1.26
0.0324
âˆ’1.32
âˆ’1.2
âˆ’1.26
0.0324
âˆ’4
1
quma.d
âˆ’1.61
0.0359
âˆ’1.68
âˆ’1.54
âˆ’1.61
0.0359
âˆ’4
1
quph.d
âˆ’1.37
0.0182
âˆ’1.41
âˆ’1.34
âˆ’1.37
0.0182
âˆ’4
1
qupr.d
âˆ’1.7
0.0115
âˆ’1.72
âˆ’1.68
âˆ’1.7
0.0115
âˆ’4
1
quru.d
âˆ’1.62
0.0123
âˆ’1.64
âˆ’1.6
âˆ’1.62
0.0123
âˆ’4
1
qust.d
âˆ’1.5
0.0214
âˆ’1.54
âˆ’1.46
âˆ’1.5
0.0214
âˆ’4
1
quve.d
âˆ’1.76
0.0219
âˆ’1.8
âˆ’1.72
âˆ’1.76
0.022
âˆ’4
1
diam.f
1.51
0.0117
1.5
1.54
0.473
0.0724
1.5
2.5
diam2.f
âˆ’0.183
0.00782
âˆ’0.193
âˆ’0.159
âˆ’0.183
0.00782
âˆ’0.25
âˆ’1eâˆ’04
cnpy.f
0.131
0.0277
0.0777
0.183
.131
0.0278
0.01
3
dlast.f
âˆ’0.0891
0.0949
âˆ’0.236
0.107
âˆ’0.0891
0.095
âˆ’2
2
qual.f
4.09
0.16
3.84
4.51
4.09
0.161
âˆ’1
10
quco.f
3.9
0.186
3.6
4.32
3.9
0.186
âˆ’1
10
qufa.f
4.03
0.202
3.72
4.54
4.03
0.202
âˆ’1
10
quma.f
4.06
0.181
3.74
4.45
4.06
0.181
âˆ’1
10
quph.f
4.29
0.155
4.06
4.68
4.29
0.155
âˆ’1
10
qupr.f
3.9
0.18
3.63
4.34
3.9
0.18
âˆ’1
10
quru.f
4.28
0.159
4.03
4.69
4.28
0.16
âˆ’1
10
qust.f
4.19
0.151
3.95
4.58
4.19
0.151
âˆ’1
10
quve.f
4.02
0.187
3.75
4.52
4.02
0.187
âˆ’1
10

Models for Demography of Plant Populations
455
a (17.13)
diam
0.0185
0.00137
0.015
0.02
0.184
0.016
âˆ’0.02
0.02
diam2
âˆ’0.0538
0.00564
âˆ’0.0669
âˆ’0.0434
âˆ’0.0538
0.00564
âˆ’1
âˆ’0.001
cnpy
0.0228
0.00366
0.0155
0.03
0.0228
0.00366
0.005
1
dlast
0.00987
0.000129
0.00952
0.01
0.0394
0.00211
âˆ’0.01
0.01
qual
âˆ’0.76
0.18
âˆ’1.1
âˆ’0.398
âˆ’0.761
0.18
âˆ’4
0
quco
âˆ’1.08
0.289
âˆ’1.6
âˆ’0.477
âˆ’1.08
0.289
âˆ’4
0
qufa
âˆ’0.554
0.354
âˆ’1.36
âˆ’0.0301
âˆ’0.397
0.465
âˆ’4
0
quma
âˆ’0.95
0.464
âˆ’1.97
âˆ’0.107
âˆ’0.907
0.509
âˆ’4
0
quph
âˆ’0.67
0.275
âˆ’1.21
âˆ’0.165
âˆ’0.666
0.282
âˆ’4
0
qupr
âˆ’1.14
0.0674
âˆ’1.27
âˆ’1.01
âˆ’1.14
0.0675
âˆ’4
0
quru
âˆ’1.08
0.114
âˆ’1.31
âˆ’0.884
âˆ’1.08
0.114
âˆ’4
0
qust
âˆ’0.736
0.345
âˆ’1.44
âˆ’0.149
âˆ’0.728
0.357
âˆ’4
0
quve
âˆ’1.24
0.272
âˆ’1.67
âˆ’0.57
âˆ’1.24
0.272
âˆ’4
0
quun
âˆ’1.12
0.471
âˆ’2.2
âˆ’0.305
âˆ’1.12
0.473
âˆ’4
0
 (17.7)
(1,1)
0.0511
0.000422
0.0503
0.0519
â€“
â€“
â€“
â€“
(2,2)
0.218
0.00174
0.214
0.221
â€“
â€“
â€“
â€“
(1,2)
6.35e âˆ’05
0.000593
âˆ’0.00111
0.00123
â€“
â€“
â€“
â€“
â€šâ€ž (17.4)
intercept
0.757
0.102
0.539
0.935
â€“
â€“
â€“
â€“
slope
0.1
0.00714
0.0887
0.118
â€“
â€“
â€“
â€“
Vb (17.7)
Vb (1,1)
0.105
0.00433
0.0967
0.114
â€“
â€“
â€“
â€“
Vb (2,2)
0.34
0.0177
0.307
0.375
â€“
â€“
â€“
â€“
Vb (1,2)
0.0205
0.00977
0.000808
0.0387
â€“
â€“
â€“
â€“
â€šÃ‹ (17.2)
intercept
âˆ’1.42
0.0421
âˆ’1.5
âˆ’1.34
â€“
â€“
â€“
â€“
diameter
0.0891
0.00296
0.0826
0.0942
â€“
â€“
â€“
â€“
light
0.0197
0.00355
0.0126
0.0264
â€“
â€“
â€“
â€“
w (17.13)
error variance
0.0415
0.00054
0.0404
0.0425
â€“
â€“
â€“
â€“
u (17.6)
dispersal
29.9
0.0624
29.8
30
â€“
â€“
â€“
â€“
v
detection
0.0732
0.00371
0.0662
0.0807
â€“
â€“
â€“
â€“
1 For A, parameters are referenced as â€˜covariate.dâ€™ or â€˜covariate.fâ€™. Thus, â€˜cnpy.dâ€™ is the effect of canopy, or light response, on diameter
growth d, and â€˜quru.fâ€™ is the fecundity intercept for Quercus rubra. For a, there is a single response variable.
2 p1 and p2 are the lower and upper limits for the truncated normal. For A, they are referenced in equation (17.15).

456
The Oxford Handbook of Applied Bayesian Analysis
predictions at diameters = 10, 40 cm
predictions at mean canopy = 5 m2
Fraction
(b) Posteriors: intercept, diameter, and canopy area
Parameter value
Posterior 95% Cl for nf
Predictive mean for q
Prior mean for q
Posterior for individuals Q ij,t
Observations qij,t
(c) Posterior mean maturation and gender
Diameter (cm)
Maturation Pr, E[Qij,t]
0
0
20
40
60
80
100
120
0
20
40
60
80
100
120
0.0
1.0
0.0
5
100
60
40
20
0
0.0
Pr(female) = E[Hij]
1.0
0.0
1.0
0
0
â€“5.0
â€“4.6
0.135 0.155
0.05
0
0
1
20
40
60
80
100
120
0.09
1.0
50
100
150
0
50
100
150
(a) Data and predictions
Liquidambar styraciflua (monoecious)
Acer (2 spp monoecious, 2 spp dioecious)
Exposed canopy area (m2)
mean canopy = 3.6 m2
Diameter (cm)
Fig. 17.3 (a) Data and ï¬tted models for maturation and gender. Liquidambar (left) is monoecious,
Acer (right) includes both monoecious and dioecious species. Lower histograms show the faction
of observations in diameter bins for which qij,t = 1. Upper histograms show the fraction for which
the estimates Q ij,t = 1. The function Ã‹ is shown for prior (dashed) and posterior (solid) values of â€šÃ‹.
Horizontal dashed lines are 95% CIs for vË†. (b) Posteriors for â€šÃ‹ for Liquidambar compared with
priors (ï¬‚at lines). (c) Posterior means for gender plotted against maturation status.
was desirable to draw samples from it, for purposes of prediction. This would be
necessary if one did not have access to the full Gibbs chains. There are two such
distributions, one for the parameters of A and second for those of a. The full
covariances are not included for space considerations; we have included only
standard deviations in Table 17.5. We consider aspects of data, priors, and pos-
teriors, beginning with gender and maturation, followed by growth/fecundity,
then survival.
Figure 17.3 provides perspective on how data, priors, and posteriors relate for
a monoecious species (Liquidambar styraciï¬‚ua on the left side of Figure 17.3)
and the genus Acer, which includes both monoecious (A. barbatum, A. sac-
charum) and dioecious (A. rubrum, A. pensylvanicum) species (right side of

Models for Demography of Plant Populations
457
Figure 17.3). In fact, A. rubrum is polygamo-dioecious, having some individuals
that are male, some female, and some supporting both male and female ï¬‚owers.
Our female fraction for this species includes both female and monoecious indi-
viduals. For the monoecious Liquidambar, all mature individuals have female
function, so Ë† = 1. With increasing diameter and canopy area, larger numbers
of individuals are observed to be mature (grey histogram in Figure 17.3a) and
still more are estimated to be mature (black histogram), because detection prob-
ability v < 1 (horizontal dashed grey lines). Note that the posterior 95% credible
interval for the estimate of v roughly averages the red histogram (observations)
at sizes where maturation is reached, whereas the black histogram (estimates)
approaches 1. This is the expected relationship between observations, detection
probability, and the true states. The values approach zero for small diameters,
because small trees cannot reproduce. However, values do not approach zero
for small exposed canopy areas, because it is possible for trees that are highly
shaded to produce at least some fruit.
The estimates for the population-level relationship are given by predictions
of Ã‹, shown in Figure 17.3 as predictive means only. These are plotted against
exposed canopy area ÃŽ (for two values of diameter) and against diameter D (for
the mean canopy area). We show prior and posterior means for Ã‹. The estimates
of â€šÃ‹, which are the basis for predictions of Ã‹, are well resolved (Figure 17.3b).
They predict maturation at smaller sizes and at lower canopy exposure for
Acer than they do for Liquidambar. The population level predictions (smooth
lines) do not appear to run precisely through the histograms of individual level
predictions, because the individual level predictions effectively marginalize over
diameter and canopy distributions for the entire population, whereas the pre-
dictive mean curves are conditional on speciï¬c diameter and canopy values.
Capacity to predict gender increases with tree size, because large trees are
more likely to be reproductive, and reproduction is the only evidence for gender.
The probability of being female tends to zero or one with increasing diameter
(Figure 17.3c). As probability of being mature increases, so too does predictabil-
ity of gender. If data were static, at small diameters, the probability that any
individual is female would tend to the posterior estimate of Ë†. This does not
occur in Figure 17.3c, because small diameter individuals may later become
mature, thus providing evidence of their gender even at small size, i.e. before
they were mature. With increasing conï¬dence in maturation status, we see a
greater tendency to be female than male. This tendency results from the fact
that two of the species in Acer are modeled as monoecious and thus will always
be counted in the female class.
The inï¬‚uence of truncated priors (equation 17.15) is evident in posteriors for
parameters from the growth/fecundity state-space model (Figure 17.4). Due to
the size and complexity of the model the ï¬‚exibility to assign hard boundaries
to one or both limits for these parameters and the transparency of prior effects

458
The Oxford Handbook of Applied Bayesian Analysis
Two responses
Betula
B. alleghaniensis
B. lenta
Cornus
Current growth
Diameter
Large diameter
Canopy exposure
Growth rate
Four predictors
Intercepts
30
4
20
4
2
0
10
0
2
0
40
0
300
5
60
40
20
0
â€“0.25
â€“0.10
40
20
0
01
0
10
15
30
30
20
10
0
0.0 1.0 2.0 3.0
10
0
0.0
0.4
0.8
0
0
400
2.0
400
1.0
0.0
0.8
15
0.8
8
4
0
8
0.8
0.0
04
â€“4
â€“2
0
0
4
8
Parameter value
1
0.0
0
4
8
10
5
0
â€“6 â€“4 â€“2
0
0.4
0.0
â€“2
0
0
4
8
2
200
0
â€“0.01
0
0.01
0.0
0
0.0
â€“0.01
â€“4
â€“2
0 1
â€“2
0
2
0
0.01
0.4
0.8
0.0 1.0 2.0 3.0
â€“0.02
â€“0.2
â€“0.25
â€“0.10
â€“0.20 â€“0.10 0.00
â€“0.1
0.0
0
1.6 2.0
2.4
â€“0.02
0
0.02
1.6
2.0
2.4
0.02
In (Dij,t-1)
[In (Dij,t-1)]2
In (Î»ij,t-1)
In (dij,t-1)
In (dij,t)
In (fij,t)
In (dij,t)
In (fij,t)
Current fecundity
Current fecundity
posterior
One intercept
prior
Current growth
Fig. 17.4 Comparison of priors (ï¬‚at grey lines) and posteriors (black) for the ï¬xed effects in the
matrix A for the state space model (Equation 17.7) for two genera, Betula and Cornus. Betula has two
intercepts, one for each of two species. The horizontal axis bounds the prior.
on posteriors was deemed an advantage. We include in this example two shade-
tolerant taxa (the canopy exposure effects ÃŽ are near zero for both diameter
growth and fecundity), one having high fecundity (Betula) and another low
fecundity (Cornus). The ï¬t for Betula includes two sets of intercepts, one for
the higher fecundity and faster growing B. alleghaniensis and one for the lower
fecundity and slower growing B. lenta (lower panels of Figure 17.4). For this
particular ï¬t, we held the diameter effect D on growth rate d to be near zero
(there is no prior knowledge to suggest growth rate should respond directly to
size until trees become large), but assumed that the effect of D on fecundity
should fall between 1.5 and 2.5. Together these assumptions allow for a direct
size effect on fecundity that accords with allometric theory, thus allowing that
effects of canopy exposure, which can be correlated with size, are more realistic.

Models for Demography of Plant Populations
459
Nyssa sylvatica
Fraximus americana
(a) Diameter increment
(b) Fecundity
Year
Year
In cm
In seeds
0.6
0.2
â€“0.2
â€“0.6
4
2
0
1995
2000
2005
1995
2000
2005
â€“2
â€“4
Fig. 17.5 Posterior medians and 95% CIs for year effects bt for two species. Separate year effects
were used for southern Appalachian plots and for Piedmont plots. Solid thin lines in lower panels
are proportional to log seed rain averaged over both sites.
To allow for declining physiological function with size, we included the
(ln D)2 term in the model and constrained it to be negative. This term will have
increasing inï¬‚uence with size. We did not have large enough trees in these
data sets to show clear effects on growth (posteriors clumped at the upper zero
boundary), but there was evidence for this negative effect on fecundity for a
number of species. Canopy exposure ÃŽ has a positive effect on both growth and
fecundity. For these shade tolerant species, these estimates were close to zero
for both growth and fecundity.
The lagged growth rate effect was constrained to be near zero for growth,
because we wanted long-term trends in growth to be taken up by year effects.
The tendency for positive correlation was constrained by the upper boundary at
0.01. However, we wanted to explicitly parameterize the lag-1 effect of growth
on fecundity, because this could be important for demographic prediction. We
obtained a range of values from strongly positive to strongly negative for the
lag-1 effect of growth on fecundity.
The different intercepts for genera having more than one species (e.g. Betula)
allowed us to model fecundities for groups of species having indistinct seeds.
Although B. alleghaniensis and B. lenta have similar life histories, we found
substantially higher fecundity for B. alleghaniensis (recall that intercepts are on
a log scale).
Two sets of ï¬xed year effects bt were used, one for each of the two regions
and shown in Figure 17.5 as longer curves for Coweeta (back to 1992) and
shorter curves for Duke Forest (beginning in 1999), both having a sum to zero

460
The Oxford Handbook of Applied Bayesian Analysis
(a) Increment data
Acer
10000
6000
2000
0
0.2
0.1
0.0
0.0
0.2
0.4
0.6
0.8
1.0
0.08
0.04
0.00
0
40
80
120
alive = 75942
Frequency
Probability
dead = 835
(b) Death frequency by increment and posterior for md
(c) Death frequency by diameter and posterior for mD
Posterior
Frequency of observations
death due to
slow growth
Diameter (cm)
Diameter increment (cm)
Fig. 17.6 (a) Survival data, (b) relative frequency of deaths (histogram) and posterior median and
95% CI for Ãd, (c) relative frequency of deaths (histogram) and posterior median and 95% CI for ÃD.
constraint. Those for growth show increasing rates in recent years, which could
result from several mechanisms. Those for fecundity show a tendency for
two-year cycles in Nyssa. Note that year effects need not strictly track seed rain
trends (solid thin line in Figure 17.5), because other covariates vary from year
to year.
The posterior estimates for effects of diameter increment Ãd and diameter ÃD
on survival show the effects of the monotonicity assumptions (Figure 17.6). The
relationship between growth increment and mortality risk is highly nonlinear
close to the limit of increment core data (Figure 17.6a, b). Apparently, trees
reach a threshold of low growth, below which mortality risk rises substantially.
The histogram of observations in Figure 17.6(c) shows modes not only at the
largest sizes, but also the smallest. The latter mode results from the slow growth
that results from low light levels in the forest understory. The priors help to
discriminate the growth from size effects, by recognizing that mortality risk
declines with growth rate, but increases with size. Beyond this relationship
already known from previous studies, the prior is weak as to the shapes of these
relationships. The prior from equation 17.23 allows the assumption that death

Models for Demography of Plant Populations
461
Prediction year ahead and increment core year
0
0
0
0
80
80
60
Diameter (cm)
100
200
Fraxinus
Quercus
Carya
Fig. 17.7 Comparison of increment data from tree ring data not used in ï¬tting the model (thin lines)
and predictive distributions of tree diameter from the model (Section 17.6.2).
of these small individuals is not due directly to size, but rather indirectly, due to
low growth rates.
17.6.2 Data prediction
To provide further insight into model behaviour we predicted data and latent
states. Here we describe some of these predictions and how they compare to
data or estimates of latent states.
Predictions of diameter growth were evaluated against an independent data
set of growth, obtained from measurements of increment cores spanning
decades. The example in Figure 17.7 is typical â€“ we obtain good coverage of size
distributions for decade-ahead prediction. It is worth mention that the model
contains no explicit age information. And there is no attractor in the model that
would necessarily make it converge to a particular diameter value. Moreover,
these are not one-step ahead predictions, as is often used to evaluate ï¬ts of
time series models, but rather 200 year ahead predictions. Here we simply
initialized the model and incremented year-by-year predictive distributions,
approximating
p

dt+1, ft+1
Xâˆ—
t , X

=

p

yt+1
Xâˆ—
t , ï£¿

p(ï£¿|X )dï£¿
where X is taken to be all data and priors entering the model, ï£¿is a vector
of parameters, Xâˆ—
t is taken to be the current covariates, which include the
previously predicted diameter and increment, and a random draw from the
distribution of canopy exposure values ÃŽij,t contained in the data. The integrand
includes the state-space structure of the model (equation 17.7) and the posterior.

462
The Oxford Handbook of Applied Bayesian Analysis
The integral is approximated by drawing at random a row from the iteration-
by-parameter matrix of Gibbs sampler output. The tree is initially immature
(1 cm diameter) and subject to growth rate in equation (17.13), risk of matura-
tion from equation (17.2), and risk of death from equation (17.14). A draw from
Vb determines its random individual effect. If it does not survive an iteration,
it is pronounced dead and removed from the simulation. The next growth
rate is drawn from a univariate or bivariate normal, depending on maturation
status.
A similar approach to prediction was used to evaluate other aspects of the
model. We do not observe fecundity, so we cannot compare direct observations
of fecundity against model predictions. However, we can check predictions of
seed rain. We did this in two ways. Consider that seed rain can be predicted
from different levels in the model. The model generates estimates of latent
states fij,t. Based on these latent states for all trees on plot j in year t, there
is a likelihood for seed rain data at location k in year t. Thus we can consider
how well the expected seed rain for all trees at j in year t predict seed rain data
at k in year t, or
p

sk,t
E

f j,t, Q j,t

, X

=

p

sk,t
E

f j,t, Q j,t

, ï£¿1

p(ï£¿1 |X ) dï£¿1
where X represents all data and priors, and the vector ï£¿1 = (u, â€šâ€ž). Alternatively,
we could predict from a lower level to include the uncertainty in f j,t itself
p

sk,t
E

X j,tâˆ’1

, X

=

p

sk,t
 f j,t, Q j,t, ï£¿1

p

f j,t, Q j,t
E

X j,tâˆ’1

, ï£¿2

Ã— p

f j,t, Q j,t, ï£¿1, ï£¿2 |X

d

f j,t, Q j,t, ï£¿1, ï£¿2

where the vector ï£¿2 = (â€šÃ‹, A, a, bt, {bij}, ). Figure 17.8 compares predictions
from these two levels with data (black) and priors for missing data (grey). As
expected, the predictions conditional on mean estimates of fecundity (right)
have narrow predictive intervals â€“ they include only a subset of the uncertainty,
i.e. that contributed by the seed data model assuming known fecundity. Predic-
tions that incorporate the uncertainty in the state-space model itself (left) have
broader predictive intervals and provide a more realistic prediction.
We constructed predictive intervals for a population and for individuals
within the population, the latter conditioning on known covariates or estimated
latent states associated with that individual. In principle, the predictive intervals
obtained by methods discussed above should agree with the distributions not
only of data (e.g. Figure 17.8), but also of latent states being estimated in
the model. To illustrate that this is the case, Figure 17.9 provides an example
that includes predictive intervals for growth, fecundity, and survival of Quercus
where the latent states are represented as grey dots and predictive intervals
are in black (a dark understory with ÃŽ = 0.1), grey (an intermediate exposure

Models for Demography of Plant Populations
463
From regression
Liquidambar styraciflua
500
50
5
1
Observed
1
1
5
50
5
50
Predicted
From imputed fecundity
Fig. 17.8 Predictions for seed data conditioned on posterior mean estimates of fecundity (right)
and on mean estimates of covariates (left). Predictive intervals are broader on the left, because they
integrate not only uncertainty associated with dispersal and sampling, but also in the state-space
model of fecundity.
level of ÃŽ = 40), and, for fecundity, dark grey (intermediate exposure, conditional
on being mature). For black the sources of uncertainty, from the predictive
mean outward are in order: parameter uncertainty (dashed â€“ hardly visible),
random individual effects (dotted â€“ in this case small), year-to-year variation
(dashed â€“ in this case large), and process error (in this case small). In general
we ï¬nd agreement between estimates of latent states and the predicted variation
from the model. The latent states for fecund individuals are covered by the
predictive distributions conditional on being mature. The centre plot includes
a large number of dots along the bottom of the plot, indicating immature
individuals. The black, unconditional fecundity prediction marginalizes over
the probability of being mature.
17.7 Summarizing the complexity
The large number of estimates generated by this analysis satisï¬es the need for
a detailed representation of how demographic rates relate to one another, but it
produces a new challenge: How do we summarize these results in meaningful
ways? Despite the effort needed to simulate the distribution, obtaining the
posterior is only the beginning. These results are now the basis for forward
simulation experiments to determine how changing environments affect pre-
dictions of biodiversity. Here we simply point out the rich set of products that

464
The Oxford Handbook of Applied Bayesian Analysis
(a) Diameter growth
Quercus rubra
E [dij,t]
E [fij,t]
E [zij,t]
p (d\D,l=40)
p (d\D,l=0.1)
p (f\D,l=40)
p (f\D,l=40, Q =1)
p (f\D,l=0.1)
p (z\D,l=0.1)
p (z\D,l=40)
cm
Seeds per tree per year
Probability
(b) Fecundity
(c) Survival
Diameter D (cm)
5.00
1.00
0.20
0.05
0.01
1e+06
1e+04
1e+02
1e+00
0.500
0.100
0.020
0.005
0
20
40
60
80
100
120
Fig. 17.9 Posterior mean estimates of latent states (dots) and predictive intervals for low (ÃŽ = 0.1;
lower set of solid mean and dashed 95%) and high (ÃŽ = 40; upper set of solid mean and dashed 95%)
canopy exposure. Included in (b) are predictive intervals for fecundity conditioned on mature status
and high canopy exposure (Q = 1); zero values are jittered and plotted as oneâ€™s to make them visible
on this log scale.
can be derived from such results. For example, the predictive distributions
provide a basis for simulation of interacting populations, a requirement for
understanding how competition contributes to species diversity. The simu-
lations in Figure 17.10 are conditioned on a particular assumption of light
availability (in this case, a random draw from the data). Competition models
generate the light availability based on shading from neighbours (e.g. Pacala
et al. 1996, Govindarajan et al. 2004).
Despite the complexity of this analysis and the large numbers of estimates,
predictions can be simple and valuable. For example, the decline in pre-
dictive intervals with increasing elevation for fecundity and growth rates in
Figure 17.11 can help to explain mechanisms behind species range limits. Cor-
relations among series of rates predicted from the model can be used to identify

Models for Demography of Plant Populations
465
Cercis canadensis
Diameter
(cm)
30
80
0
105
0
0
100
Age (year)
200
0
100
200
200
0
106
0
1
0
1500
0
0.1
0
0
100
0
Maturation
Pr
Mortality risk
(1/year)
Fecundity
(seeds/year)
Fraxinus americana
Liriodendron tulipifera
Fig. 17.10 Predictive simulations of demographic rates based on naÃ¯ve scenarios for light availability,
showing inherent differences among species. In each case are shown 95% predictive intervals, which
include all sources of uncertainty in the model. For fecundity 30 individual simulations show the
range of variability. Note that fecundity has a different scale for each species. For diameter, data are
also shown (thin lines).
how lags in growth may affect fecundity (Figure 17.12, left side) and how and
when rates of growth and fecundity deteriorate prior to death (Figure 17.12,
centre and left). Predictive intervals for speciï¬c combinations of demographic
rates (Figure 17.13) can be used to test hypotheses about the types of tradeoffs
that might be needed for species to coexist. In each of these cases a seemingly
incomprehensive number of estimates has been synthesized in ways that allow
clear consideration of basic ecological relationships.

466
The Oxford Handbook of Applied Bayesian Analysis
Basal area
Acer rubrum
8
6
4
2
0
200
800
1400
200
800
1400
200
800
1400
200
800
1400
â€“4
â€“2
0
15
0
â€“2
â€“4
â€“6
5
0
Growth rate
Elevation (m)
In(cm)
In(seeds)
In(probability)
m2/ha
Fecundity
Mortality Pr
Fig. 17.11 Predictive intervals for demographic rates at different elevations. The predictive densities
for different elevations (oriented horizontally) integrate to 1, with solid and dashed lines connecting
posterior median and 95% CIs, respectively. The densities include parameter error and individual
effects, but they do not include observation error.
Growth: fecundity
Quercus montana
Correlation
0.5
0.5
0.5
â€“0.5
â€“0.5
â€“0.5
â€“5
0
5
0
2
4
6
8
0
2
4
6
8
Growth: mortality
Year before death
Growth year leading fecundity year
Fecundity: mortality
Fig. 17.12 Cross-correlations for demographic rates. The sequences of demographic rates for each
individual were detrended and cross-correlated. Shown are bounds for 95% of the individuals
(dashed) and medians (solid). In other words, dashed lines bound 95% of the individual cross-
correlations.
Growth (cm)
Survival Pr
0.1
0.94
0.96
0.98
1.00
High light growth vs low light survival
0.2
0.5
1.0
2.0
Fig. 17.13 Relationship among species in terms of capacity to grow fast at high light versus survival
probability at low light. Predictive intervals are 95% and include only parameter error and variation
among individuals.

Models for Demography of Plant Populations
467
17.8 Potential
Hierarchical modelling provides a mechanism for synthesis of complex infor-
mation and interactions. The Bayesian framework is important for incorpo-
ration of prior knowledge, the strength of which differs for all parts of the
model, including data and theoretical understanding. The advantage it provides
for admitting the complexity unavoidable in the real world brings with it the
challenge of understanding complex models. Once a large posterior is in hand,
predictive distributions of key relationships can help to elucidate patterns of
special interest.
Appendix
A. Broader context and background
A.1 Demographic models
Demographic modelling of natural populations has progressed substantially in
recent years. Until a decade ago, models for inference on species in the wild
involved a single stage, something like response = f (known inputs, stochastic
error). The response could be growth rate or fecundity, the inputs being aspects
of the individualâ€™s state, resources, or other factors that affect health. In fact,
the most widely applied methods for inference involved a single predictor, the
individualâ€™s stage in life, and it assumed that the response was linear. The
approach owed its popularity to limited availability of covariate data and to
readily available software. For a single response variable, such as fecundity fi
of tree i, linear models dominated. One might apply an allometric function of
tree diameter Di,
ln fi = Â·0 + Â·0 ln Di + Îµi
Îµi âˆ¼N

0, Ã›2
.
Likewise, tree growth di might be modelled as a function of covariates xi, such
as light availability,
di = f (xi) + Îµi
Îµi âˆ¼N

0, Ã›2
.
As maximum likelihood has increased in popularity, nonlinear models have
been more widely applied. For example, a growth rate might saturate with
increasing resource availability. Ecologists have increasing developed code to
perform optimizations (e.g. to obtain MLEs) and to simulate distributions (e.g.
using a Metropolis algorithm). Mortality risk might be modelled as a function
of past annual diameter increment di, because growth rate provides some

468
The Oxford Handbook of Applied Bayesian Analysis
indication of overall tree health,
zi âˆ¼Bernoulli(ÃŠi)
ÃŠi = f (di)
using a standard GLM or some alternative form (Kobe et al. 1995, Yao et al.
2001). Such applications have been tremendously valuable and predispose
ecologists to more advanced techniques, such as hierarchical modelling (Clark
2005, Latimer et al. 2006, Carlin et al. 2007).
The hierarchical application described in this chapter provides examples of
many distributional forms and opportunity to mention some background for
the hierarchical context. Hierarchical models are now being applied in ways
that synthesize complex data to better understand species distributions (Latimer
et al. 2006), migration (Wikle 2003, Hooten et al. 2003), mortality (Metcalf et al.,
in review), disease spread (LaDeau et al. 2007) and environmental variation
(Ibanez et al. 2007, Ogle et al. 2006, Dietze et al. 2008), growth (Clark et al. 2003,
Mohan et al. 2007, Ibanez et al. 2007), and fecundity and dispersal (Clark et al.
2004, HilleRisLambers et al. 2006). Our chapter involves a speciï¬c application,
emphasizing techniques that could be adapted for multiple applications, in part
because it models demographic rates together.
A.2 GLMs in a hierarchical setting
Components of our model involve generalized linear models (GLMs) for mat-
uration (Section 17.3.1) and seed data. GLMs involve an underlying linear
predictor that is linked to a data distribution, typically binomial or Poisson, by
a function that translates the predictor from the real line (âˆ’âˆž, âˆž) to (0,1) for
a binomial or (0, âˆž) for Poisson by way of a link function. As an example, the
Bernoulli example in equation (17.1) shows Ã‹ij,t as an inverse logit function of
the linear predictor â€šÃ‹
0 + â€šÃ‹
1Dij,t + â€šÃ‹
2ÃŽij,t. A large literature on GLMs dates at least
to Nelder and Wedderburn (1972) with applications described for classical and
Bayesian settings in many recent texts, including Gelman and Hill (2007). In
the Bayesian context, parameters for ï¬xed effects might have Gaussian priors.
Our truncated normal priors (equation 17.21) do not importantly complicate
the computation.
Hierarchically structured GLMs (or Generalized Linear Mixed Models) typ-
ically involve a stochastic speciï¬cation of one or more terms in the linear
predictor. Ecological applications often include random effects associated with
location. For example, Latimer et al. (2006) include ï¬xed effects as predictors
of species occurrence at a geographic location, with a spatial random effect
that introduces dependence based on proximity. The prior on this random
effect depends on the value for neighbouring cells. This dependence introduces
spatial smoothing. Ibanez et al. (2008) use random effects for location to absorb

Models for Demography of Plant Populations
469
some of the differences among stands not accounted for by covariates, in an
application where stands are distant from one another. In both of these cases,
the variance for random effects requires a prior, introducing an additional stage
to the model.
Our application to the binary maturation process in equation (17.1) is applied
to a time series for each individual tree, conditionally dependent on previous
and future states. However, the underlying predictor is linear, with a logit link.
Our approach differs from previous ones in placing the stochasticity not on the
coefï¬cients, but rather on the covariates. This structure (parameters as ï¬xed
effects assigned to random predictors) was motivated by the fact that tree size is
estimated, and canopy exposure estimates are especially crude. The problem
is constrained by informative priors on the predictor variables, particularly
knowledge that they are bounded within a known range. This range enters as
a truncated normal distribution (equations 17.12 and 17.19). Thus, the added
stage involves sampling the covariates, rather than the coefï¬cients.
The Poisson distribution for seed density (equation 17.4) differs from a
standard GLM in that the expectation is a transport model for seed dispersal.
More typically, applications would involve a linear predictor with a log link. Like
the binomial, it may have a hierarchical speciï¬cation. The inclusion of trap area
as a coefï¬cient in the Poisson piece of equation (17.3) is standard, accounting
for the fact that collecting areas of traps differ.
Related to both the binomial and Poisson is the Zero-Inï¬‚ated Poisson (ZIPo)
model, a binomial-Poisson mixture, where the binomial piece can be inter-
preted as the probability that the Poisson-generating process exists (Lambert
1992, Hall 2000). For example, our fecundity model can be simpliï¬ed to a
simple probability of being mature given tree size (binary) and the expected
fecundity, given that the individual is mature (LaDeau and Clark 2001),
p( f |Ã‹, y ) = [1 âˆ’Ã‹ + Ã‹Po(0 |y )]1( f =0) [Ã‹Po( f |y )]1( f >0)
where f is seed production by a tree, 1() is the indicator function, Ã‹ is the
probability of being mature, and y is the fecundity given that an individual
is mature. Marginally we expect E( f ) = Ã‹y. Note that the binary part, assigned
probability Ã‹, and the conditional Poisson fecundity with mean f are interpreted
as different processes. We might include different predictors for Ã‹ and f or not
(e.g. both depend on tree size and resources). This is a standard interpretation
of a ZIPo model (e.g. Hall 2000, LaDeau and Clark 2001).
B. Models and computations
Gibbs sampling (Gelfand and Smith 1990) is widely used to simulate posteriors
and is described in a number of recent texts (e.g. Carlin and Louis 2000, Gelman
et al. 1995). In brief, one factors a high-dimensional posterior into a collection of

470
The Oxford Handbook of Applied Bayesian Analysis
lower-dimensional densities that can be sampled successively, each conditional
on others already updated. Conditionals may be sampled directly or indirectly
using, for example, a Metropolis step. Here we describe conditional posteriors
used for our Gibbs sampling algorithm. The general idea was to embed within
the Gibbs sampler Metropolis steps, where direct sampling was not an option,
with attention to blocking for efï¬ciency.
B.1 State-space model
For the state space model (equation 17.7), all sampling was direct. The condi-
tional posterior for ï¬xed effect parameters is
vec(A)| X,Y, , . . . âˆ¼N2p(Vv,( + Vb) âŠ—V) I(a1 < vec(A) < a2)
where Vâˆ’1 = X T X, v = X T Z, X is the stacked matrix of (Tij âˆ’tij) by p Xij
matrices, one for each tree, Z = [Z11 Z21. . .] is the stacked matrix Zij = Yij âˆ’
1ijbt, where 1ij is the length (Tij âˆ’tij) vector of 1â€™s, and bt are taken for the
appropriate years in X and Z. The truncated multivariate normal is sampled
from the conditional univariate truncated normals.
The error covariance matrix was sampled from an inverse Wishart condi-
tional posterior. Let V be the parameter matrix for this prior. Then the condi-
tional posterior for âˆ’1 is
âˆ’1 âˆ¼W
âŽ›
âŽ
âŽ¡
âŽ£
i, j,t
Q ij,t

yij,t+1 âˆ’xij,t A âˆ’bt âˆ’bij
â€²
yij,t+1 âˆ’xij,t A âˆ’bt âˆ’bij

+ nIJTV
âŽ¤
âŽ¦
âˆ’1
,

i, j,t
Q ij,t + nIJT
âŽž
âŽ 
where nIJT is the total number of tree years (equation 17.16 of the text). The
conditional posterior for the random effects variance is
Vâˆ’1
b
âˆ¼W
âŽ›
âŽ
âŽ¡
âŽ£
i, j
maxt

Q ij,t

bâ€²
ijbij + rb Rb
âŽ¤
âŽ¦
âˆ’1
,

i, j
maxt

Q ij,t

+ rb
âŽž
âŽ .
Note that individual ij contributes to the conditional posterior only if it
is imputed to be mature at some point during the study, in which case
maxt(Q ij,t) = 1. The random effects are sampled from
bij âˆ¼N2(Vv, V) ,
where
V âˆ’1 =

Tij âˆ’tij

âˆ’1 + Vâˆ’1
b , v = âˆ’1
Tij

t=Ã™ij

yâ€²
ij,t+1 âˆ’Aâ€²xâ€²
ij,t âˆ’bt

,

Models for Demography of Plant Populations
471
Ã™ij and Tij are the ï¬rst and last years during the study in which individual ij
is imputed to be mature, and Tij âˆ’Ã™ij is the number of years mature that ij is
mature during the study.
The ï¬xed year effects are sampled from a conditional normal. Let Vt be the
covariance matrix for the prior. The conditional posterior is bt âˆ¼N2(Vv, V),
where V âˆ’1 = âˆ’1 
i, j Q ij,t + Vâˆ’1
t
, and v = âˆ’1 
i, j Q ij,t(yij,t+1 âˆ’xij,t A âˆ’bij).
This draw was followed by subtraction of the mean for year effect for both
lnd and ln f . Of course bt and A could be sampled with a single draw from
a multivariate normal. A separate step was used due to the large number of bt
and the fact that each tree could have a different subset of total years.
B.2 Diameter growth
Diameter growth increments were updated from the conditional posterior given
in equation (17.12) using a Gaussian approximation for the third factor, i.e.
that corresponding to survival probability. That probability is approximated
as ÃŠij,t â‰ˆ1 âˆ’Ãdij,t, where the Ãdij,t sequence contains probabilities for discrete
diameter increment bins. The contribution of diameter is omitted, because its
contribution to survival probability is small relative to that of growth rate. Then
the conditional distribution for the k bins is
p

ln dk
 z

=
p

z
ln dk

p(ln dk)

k p

z
ln dk

p(ln dk)
where p(z = 1| ln dk) = 1 âˆ’Ãk and p(z = 0| ln dk) = Ãk, z being survival (1) or
death (0) in the subsequent year, and p(ln dk) the distribution of log growth
rates. The conditional expectations and variances are
Ãd|zij,t+1 â‰¡E

ln d
 zij,t+1

=

k ln dk p

ln dk |zij,t+1

and
Vd|zij,t+1 â‰¡Var

ln d
 zij,t+1

=

k(ln dk)2 p

ln dk
zij,t+1

âˆ’

E

ln d
 zij,t+1
2 .
There is a conditional mean and variance for z = 0 and z = 1. The log growth
rates are sampled from ln dij,t âˆ¼N(Vv, V), where
V âˆ’1 = Vâˆ’1
d| f + vâˆ’1
ij,t + Vâˆ’1
d|zij,t+1, v = Ãd| f V âˆ’1
d| f + ln

d(0)
ij,t

vâˆ’1
ij,t + Ãd|zij,t+1V âˆ’1
d|zij,t+1,
with conditional means and variances contributed by survival are for z = 0 or
z = 1, depending on whether or not the individual survived until the next year.
B.3 Canopy exposure
Canopy values are sampled from a conditional posterior that depends on the
prior means and variances coming from the analysis that assimilates data and

472
The Oxford Handbook of Applied Bayesian Analysis
from the state-space model. If the individual is mature in year t, then canopy
area is sampled from
ln ÃŽij,t | âˆ¼N(Vv,V) I

c(0)
ij,t âˆ’
;
Cij,t

< ln

ÃŽij,t

<

c(0)
ij,t +
;
Cij,t

where V âˆ’1 = Aâ€²
ÃŽâ€¢âˆ’1AÃŽâ€¢ + Câˆ’1
ij,t , and v = AT
ÃŽâ€¢âˆ’1(yij,t âˆ’x(âˆ’ÃŽ)
ij,t A(âˆ’ÃŽâ€¢) âˆ’bt âˆ’bij) +
c(0)
ij,t/Cij,t. Notation for the ï¬rst factor follows that from the previous section. If
immature in year t, ln ÃŽij,t is sampled from a normal distribution having
V âˆ’1 = a2
ÃŽ/w + Câˆ’1
ij,t
v =

ln

dij,t

âˆ’x(âˆ’ÃŽ)
ij,t a(âˆ’ÃŽ) âˆ’bt âˆ’bij

aÃŽ
w
+
c(0)
ij,t
Cij,t
where a(âˆ’ÃŽ) is the vector a from equation (17.13), but lacking the coefï¬cient for
ln ÃŽ, aÃŽ is the coefï¬cient for ln ÃŽ, w is the currently imputed error variance for
growth rate regression, having an IG prior.
B.4 Fecundity, maturation, gender
Due to their conditional dependence structure, fecundity, maturation, and (for
dioecious species) gender are sampled together in a Metropolis step. Here
we describe sampling. The basic factoring used for maturation, gender, and
fecundity is
p

fij,t, Q ij,t, Hij, sj,t
qij, hij, dij,tâˆ’1, Dij,t, ÃŽij,t

= p

s j,t
 fij,t, Q ij,t, Hij

Ã— p

fij,t, Q ij,t, Hij
qij, hij, dij,tâˆ’1Dij,t, ÃŽij,t

where qij and hij represent the history of observations on individual ij, both past
(before t) and future (after t). The ï¬rst distribution on the right-hand side is the
likelihood for seed trap data, indicating that all seed traps on plot j in year t
conditionally depend on every tree i on plot j. The second factor on the right-
hand side is the probability of being mature (Q = 1), female (H = 1), and having
fecundity f .
For monoecious species, we use a Metropolis step where maturation status
and fecundity are jointly proposed and rejected for all trees in a given stand j in
a given year t. For dioecious species we must further sample gender. Because
gender applies to an individual across all years, dioecious species are sampled
in a different way and are discussed after monoecious species. The blocking
differs between these two data types, which we describe here.
Efï¬cient Gibbs sampling requires blocking of variables to facilitate mixing,
which is challenging given the ways in which latent variables are linked with
the unknown year in which an individual becomes mature Ã™ij. These linkages
include:

Models for Demography of Plant Populations
473
(i) the Q ij,t and fij,t are inherently linked, by virtue of the fact that non-zero
fecundity is deï¬ned only for mature individuals,
fij,t

Q ij,t = 0

= 0
fij,t

Q ij,t = 1

> 0;
(ii) maturation statuses for an individual over time are mutually dependent
according to the one-way transition to maturity in year Ã™ij;
(iii) gender is considered to be ï¬xed; and
(iv) seed trap data conditionally depend on all trees in the plot in a given year.
In light of the conditional relationships involving status and seed production,
the choices for blocking are to (1) sample individually every year for every tree
(conditioned on other trees for that year and other years for that individual), (2)
sample as a block all individuals within a plot for a given year, and (3) sample as
a block all trees and years within a plot. The ï¬rst option has the advantage that
high acceptance rates can be achieved, but is computationally slow, entailing
loops over plots, individuals, and years, e.g. a Metropolis step for every tree-
year in the data set. The third option necessarily results in a high rejection
rate, each proposal consisting of n j
i=1 (Tij âˆ’tij) values. The binary nature of Q
and H proposals can make acceptance rates low. Nonetheless, because gender
Hij applies to an individual across all years, we use a modiï¬cation of option
3 for dioecious species. We begin with a description for monoecious species,
followed by the description for dioecious species.
Monoecious species â€“ We use the second option for monoecious species, blocking
on time and modelling each year successively. The factoring is
p

f j,t, Q j,t
q j,t, xj,tâˆ’1, xj,t, Q j,tâˆ’1, Q j,t+1, s j,t

âˆp

s j,t
 f j,t, Q j,t

p

Q j,t, f j,t
q j,t, xj,tâˆ’1, Q j,tâˆ’1, Q j,t+1, xj,t

.
We propose all values of {Q, f } j,t together and accept or reject them as a block.
The Markov transition probabilities from t to t + 1 are conditioned on obser-
vations of status, and they must be combined with probabilities for fecundities
and seed trap data. The transition from immature to mature is a hidden Markov
process, but only for tree-years in which status is unknown, which is the case
after the last year in which immaturity is certain, and before reproduction has
been observed. If the status is known through past observations (if previously
observed to be mature, then still mature), a current observation (mature or
immature), or future observations (if later known to be immature, then imma-
ture now), then status Q ij,t is known. This is also the case for imputed statuses.
If unknown, status must be modeled as the conditional probability of being in
state Q ij,t = 0 or 1 given Q ij,tâˆ’1 and Q ij,t+1. These probabilities involve the age-
speciï¬c rates of making the transition from immature to mature states and can

474
The Oxford Handbook of Applied Bayesian Analysis
be derived from the cumulative logit probability of being mature given diameter
Dij,t and canopy status ÃŽij,t (equation 17.2a). Because blocking is year-by-year,
we condition the transition probability on both the foregoing and the following
years. Then the trivial probabilities are
p

Q ij,t = 1
Q ij,tâˆ’1

= 1
p

Q ij,t = 1
Q ij,t+ = 0

= 0.
For failure to recognize the reproductive state, we need the additional factor
p

qij,t = 0
Q ij,t = 1

= 1 âˆ’v.
For Gibbs sampling, we need the year-by-year transition probabilities from
immature to mature between t âˆ’1 and t given that the transition was made
between t âˆ’1 and t + 1. Let â€°ij,t be the probability of being in the mature
state conditional on states in years t âˆ’1, t + 1, and on observations. Ignoring
observations for the moment, we have
â€°ij,t = p

Q ij,t = 1
Q ij,tâˆ’1 = 0, Q ij,t+1 = 1

=
p

Q ij,t = 1
Q ij,tâˆ’1 = 0

p

Q ij,t+1 = 1
Q ij,t = 1


k=0,1
p

Q ij,t = k
Q ij,tâˆ’1 = 0

p

Q ij,t+1 = 1
Q ij,t = k

=
dÃ‹ij,t
A
1 âˆ’Ã‹ij,tâˆ’1

dÃ‹ij,t
A
1 âˆ’Ã‹ij,tâˆ’1

+

1 âˆ’dÃ‹ij,t
A
1 âˆ’Ã‹ij,tâˆ’1

Ã— dÃ‹ij,t+1
A
1 âˆ’Ã‹ij,t

=
dÃ‹ij,t
dÃ‹ij,t +
 1âˆ’Ã‹ij,tâˆ’1âˆ’dÃ‹ij,t
1âˆ’Ã‹ij,t

dÃ‹ij,t+1
=
dÃ‹ij,t
dÃ‹ij,t + dÃ‹ij,t+1
where
dÃ‹ij,t =
 dÃ‹ij,t
d Dij,t
Ã— d Dij,t
dt

dt =
 dÃ‹ij,t
dDij,t
Ã— dij,t

dt
= â€šÃ‹
1dij,tÃ‹ij,t

1 âˆ’Ã‹ij,t

dt.
Because ÃŽij,t changes much slower than Dij,t, we do not include it in the chain
rule calculation for the derivative. Because dt is always equal to 1 year, we
hereafter omit it.
Observations change the transition probabilities. The previous equation for
dij,t describes the probability of transition in the absence of an observation. If
there is an observation in year t and it is â€˜uncertainâ€™ (qij,t = 0: see Table 17.4),
then the observer did not identify the tree as mature, and the probability

Models for Demography of Plant Populations
475
becomes
â€°ij,t = Pr

Q ij,t = 1
Q ij,tâˆ’1 = 0, Q ij,t+1 = 1, qij,t = 0

=
dÃ‹ij,t(1 âˆ’v)
dÃ‹ij,t(1 âˆ’v) + dÃ‹ij,t+1
.
For the ï¬rst study year, in the absence of an observation (maturation statuses
were not obtained on all individuals the ï¬rst year of the study), we have
â€°ij,t = p

Q ij,t = 1
Q ij,t+1 = 1

=
Ã‹ij,t
Ã‹ij,t + dÃ‹ij,t+1
.
If there was an observation and that observation was qij,t = 0 (Table 17.4), this
becomes
Ã‹ij,t(1 âˆ’v)
Ã‹ij,t(1 âˆ’v) + dÃ‹ij,t+1
.
For the last observation year, absent observation,
â€°ij,T = p

Q ij,T = 1
Q ij,Tâˆ’1 = 0

=
dÃ‹ij,T

1 âˆ’Ã‹ij,T
.
If there was an observation, we have
â€°ij,T = dÃ‹ij,T(1 âˆ’v)

1 âˆ’Ã‹ij,T
 .
The Metropolis steps entail a loop over time (17 year), at each time step propos-
ing values for Q t and ft, with the constraints on Q discussed above and fij,t = 0
for all imputed Q ij,t = 0. For those imputed to be immature at t âˆ’1, candidate
values come from Qâˆ—
ij,t âˆ¼Bernoulli(0.5). All others remain mature. For indi-
viduals previously imputed to be mature, we propose ln f âˆ—
ij,t âˆ¼N(ln f (g)
ij,t , 0.1),
where (g) denotes the current Gibbs step, prior to updating. For individ-
uals previously imputed to be immature, but now mature, we propose
from f âˆ—
ij,t|(Q ij,tâˆ’1 = 0) âˆ¼N(ln f â€²
ij,t, 0.1), where f â€²
ij,t is an auxiliary variable hav-
ing the value retained from the most recent iteration of the Gibbs sam-
pler in which Q ij,t = 1. The acceptance criterion involves the products from
equation (17.2a),
Q j,t, f j,t
Q j,tâˆ’1, Q j,t+1, q j,t, s j,t, Ã f |d
j,t , V f |d
j,t
âˆ

i

1 âˆ’â€°ij,t
1âˆ’Q ij,t 
â€°ij,t N

ln

fij,t
 Ã f |d
ij,t , V f |d
ij,t
Q ij,t
Ã—

k=1
Po

sjk,t
Ajkgjk

f j,t; u

.

476
The Oxford Handbook of Applied Bayesian Analysis
Note that all individuals imputed to be mature have a conditional density asso-
ciated with the state-space model. The â€°ij,t are different for each individual and
year, as discussed above. All trees contribute to the likelihood for the seed data
for plot j in year t, in that by producing seed or not, they affect the parameters
of the Poisson sampling distribution for seed. Of course, the set of proposals is
accepted or rejected as a block. The sampler is more efï¬cient than it appears,
because we can propose statuses and fecundities for all plots simultaneously,
and accept/reject them on a plot by plot basis, without actually looping over
plots. Once states are updated for time t, we move to t + 1.
Dioecious species â€“ For dioecious species gender is unchanging over time, so
we evaluate the full history of observations for each tree, but still avoiding
loops over individual trees. It is efï¬cient to factor the conditional somewhat
differently, taking together all trees on plot j over all years,
p

f j, Q j, Hj
q j, h j, X j, s j

âˆp

s j
 f j, Q j, Hj

p

f j, Q j, Hj
q j, h j, X j

.
Because maturation is no longer modeled year-by-year, we require the proba-
bility for a history of maturation status, conditioned on observations obtained
sporadically over individuals and years.
Let Ã™ij
be the year in which an individual becomes mature, Ã™0
ij =
maxt(qij,t = âˆ’1) be the last year an individual is known to have been immature,
and Ã™1
ij = mint(qij,t = 1) be the ï¬rst year an individual is known to have been
mature. Thus, we have the constraint Ã™0
ij â‰¤Ã™ij â‰¤Ã™1
ij. The probability assigned to
an individual that became mature in year t is,
â€°ij = p

Ã™ij = t
Ã™0
ij < Ã™ij < Ã™1
ij

= dÃ‹ij,t
:
Ã‹ij,Ã™1
ij âˆ’Ã‹ij,Ã™0
ij

= â€šÃ‹
1dij,t
Ã‹ij,t

1 âˆ’Ã‹ij,t


Ã‹ij,Ã™1
ij âˆ’Ã‹ij,Ã™0
ij
.
For individuals imputed to be still immature at the end of the observation period
at Tij, the probability is
â€°ij = p

Ã™ij > Tij
Ã™ij > Ã™0
ij

= 1 âˆ’p

Ã™ij â‰¤Tij
Ã™ij > Ã™0
ij

= 1 âˆ’Ã‹Tij
1 âˆ’Ã‹ij,Ã™0
ij
.
For individuals imputed to be already mature before observations began at tij,
the probability is
â€°ij = p

Ã™ij < tij
Ã™ij < Ã™1
ij

= Ã‹tij
Ã‹ij,Ã™1
ij
.
We now have a probability for the history of an individual that became mature
at time Ã™ij or remained immature throughout. Our prior speciï¬cation allows for

Models for Demography of Plant Populations
477
a minimum and maximum maturation diameter, in which case Ã‹ij,Ã™0
ij and Ã‹ij,Ã™1
ij
are the values of Ã‹ taken at these prior maturation diameter values.
The probability for maturation is combined with observations of status
between those years that established it as immature and mature and for gender.
Thus far, we have considered observations that deï¬nitively establish maturity or
immaturity (qij,t = âˆ’1 or 1 in Table 17.4). For qij,t = 0, status is uncertain. Status
detection is deï¬ned as p(qij,t = 1|Q ij,t = 0) = 0 and p(qij,t = 1|Q ij,t = 1) = v. The
individual has unknown gender if the gender is not observed, the observation is
uncertain, or if ï¬‚owers are observed but not identiï¬ed to sex, and no observation
is available from the fruiting season (Table 17.4). Considering both observations
and gender, the probability for individual ij becomes
p

Ã™ij, Hij
Ã™0
ij < Ã™ij < Ã™1
ij, qij

= â€°ij(1 âˆ’v)nv
ij(1 âˆ’Ë†)1âˆ’Hij Ë†Hij
where nv
ij is deï¬ned to be the number of times mature status was â€˜undetectedâ€™
during the interval (Ã™ij, Ã™1
ij), i.e. the number of times that an individual imputed
to be mature in year Ã™ij was not identiï¬ed as such. If gender is known, the
third factor disappears. The full reproductive history on all individuals has
conditional probability
p

Q j, Hj, f j
q j, h j, s j, Ã f |d
j
, V f |d
j

=

i
â€°ij(1 âˆ’v)nv
ij(1 âˆ’Ë†)1âˆ’Hij
Ã—
6
Ë†

t
N

ln

fij,t
 Ã f |d
ij,t , V f |d
ij,t
Q ij,t
7Hij 
t

k
Po

sjk,t
Ajkgjk

f j,t; u

.
For each individual a maturation diameter is proposed from a uniform distrib-
ution
Ã™ij âˆ¼unif

t0
ij, t1
ij

t0
ij = max

tmin
ij
, Ã™0
ij

t1
ij = min

tmax
ij
, Ã™1
ij

.
The bounds for minimum and maximum maturation diameters are not sooner
than the ï¬rst year in which ij reached the minimum prior diameter for matu-
ration tmin
ij
= max(t|(Dij,t > Dmin)) or it was last known to be immature Ã™0
ij and
not later than the last year in which ij had not yet reached the prior diameter for
certain maturation tmax
ij
= min(t|(Dij,t < Dmax)) or it was known to be mature
Ã™1
ij. There are prior minimum and maximum diameters, which differ among
species. The fecundity for an individual proposed to be immature is zero. For
individuals currently imputed to be mature, the proposed fecundity is a trun-
cated normal on ( fmin, fmax) centered on the current estimate. The conditional

478
The Oxford Handbook of Applied Bayesian Analysis
densities are then the product of Poisson seed data, Gaussian fecundity, and the
probability associated with maturation in year t. Because the probability of seed
data conditionally depends on all trees in the stand in all years, the ensemble of
( f j, Q j, Hj) is accepted or rejected as a block.
Recognition error is sampled from
Bin

v(1) v(0) + v(1), v

Be(v |v1, v2 ) = Be

v
v1 + v(1), v2 + v(0) 
where the two arguments are sums of prior values and numbers of currently
imputed mature individuals for which maturation was recognized as such or
not, i.e.
v(0) =

ij,t
I

qij,t = 0, Q ij,t = 1

v(1) =

ij,t
I

qij,t = 1, Q ij,t = 1

.
For the female fraction we sample from the conditional posterior
Bin

h(1) h(0) + h(1), Ë†

Be(Ë† |h1, h2 ) = Be

Ë†
h1 + h(1), h2 + h(0) 
where the two arguments are sums of prior and currently imputed numbers of
the females and males, respectively,
h(0) =

ij

1 âˆ’Hij

h(1) =

ij
Hij.
Prior values are h1 = h2 = 4, which has a mean of 0.5 and is dominated by
the data.
Parameters for the logit function of maturation equation (17.1) are sampled
with Metropolis step. Conditionally we have

i, j
â€°

Ã™ij

N

â€šÃ‹ bÃ‹, VÃ‹ 
I

â€šÃ‹
2,3 > 0

where the ï¬rst product is the probability associated with maturation years,
which depend on â€šÃ‹, and the truncated normal prior. A proposal is generated
from a multivariate normal truncated at zero for these two parameters.
Parameters for the seed data model are sampled in a single Metropolis step.
Conditionally,
Tj

t=tj
K j

k=1
Po

sjk,t
Ajkgjk

f j,t; u, c

N(u |u0, Vu ) N(c |c0, Vc ) I(u, c > 0) .

Models for Demography of Plant Populations
479
Values are proposed from a normal distribution. In the case of missing data,
seed counts were replaced with the currently imputed seed value.
Imputation of missing data involved a Metropolis step with proposals of plus
or minus 1 from the current value with probability 0.5. The conditional poste-
rior includes a Poisson prior with a mean density as discussed in Section 17.4
multiplied by Poisson density for sample jk,t. Proposals were accepted as a block
for s j,t.
A Metropolis step is used to simultaneously update all of the diameter growth
and diameter bins for the nonparametric survival relationship. The growth
rates and diameters of all individuals are binned in the sequences Ãd and ÃD
are for the all years. For diameter increment there are 31 bins equally spaced
with width 0.1 on the log10 scale. For diameter there are six bins, also equally
spaced on the log10 scale, with the maximum value chosen to exceed that largest
diameter in the data set. Survival from year t to t + 1 is the event zij,t = 1 and
death in the subsequent year is zij,t = 0. At each Gibbs step new sequences
of Ãd and ÃD are proposed each being Gaussian and centred on the currently
imputed values, but truncated midway between the current values. For diameter
increment the proposal distribution is
N

Ãâˆ—
d
Ãd, V

I
(Ãdâˆ’1 âˆ’Ãd) /2 < Ãâˆ—
d <(Ãd âˆ’Ãd+1) /2

where V is a small value (0.1 in this case). In other words, if the currently
imputed value for Ãd,k was 0.5 and those for bins k âˆ’1 and k + 1 were 0.6 and
0.48, then the proposal would come from the normal centred at 0.5, truncated
at 0.55 and 0.49. This procedure allows for any shape subject to monotonic
decline. The proposals for the diameter values are done in the same way,
with the constraint being monotonic increase and with an informative prior
Be(ak, bk). All values (both growth rate and diameter) are proposed together and
accepted as a block.
References
Batista, W. B., Platt, W. J. and Macchiavelli, R. E. (1998). Demography of a shade-tolerant tree
(Fagus grandifolia) in a hurricane-disturbed forest. Ecology, 79, 38â€“53.
Beckage, B. and Clark, J. S. (2003). Seedling survival and growth in southern Appalachian
forests: Does spatial heterogeneity maintain species diversity? Ecology, 84, 1849â€“1861.
Bigler, C. and Bugmann, H. (2004). Predicting the time of tree death using dendrochronological
data. Ecological Applications, 14, 902â€“914.
Carlin, B., Clark, J. S. and Gelfand, A. (2006). Elements of Bayesian Inference. In Hierarchical
Models of the Environment, (ed. J.S. Clark and A. Gelfand), pp. 3â€“24. Oxford University Press,
Oxford.
Carlin, B. P. and Louis, T. A. (2000). Bayes and Empirical Bayes Methods for Data Analysis.
Chapman and Hall, Boca Raton, FL.
Caswell, H. (2001). Matrix Population Models. Sinauer, Sunderland, MA.

480
The Oxford Handbook of Applied Bayesian Analysis
Clark, J. S., LaDeau, S. and Ibanez, I. (2004). Fecundity of trees and the colonization-
competition hypothesis, Ecological Monographs, 74, 415â€“442.
Clark, J. S. (2005). Why environmental scientists are becoming Bayesians. Ecology Letters, 8,
2â€“14.
Clark, D. B. and Clark, D. A. (1996). Abundance, growth and mortality of very large trees in
neotropical lowland rain forest. Forest Ecology and Management, 80, 235â€“244.
Clark, J. S., Macklin, E. and Wood, L. (1998). Stages and spatial scales of recruitment limitation
in southern Appalachian forests. Ecological Monographs, 68, 213â€“235.
Clark, J. S., Silman, M., Kern, R., Macklin, E. and Hille Ris Lambers, J. (1999). Seed disper-
sal near and far: Generalized patterns across temperate and tropical forests. Ecology, 80,
1475â€“1494.
Clark, J. S., Wolosin, M., Dietze, M., Ibanez, I., LaDeau, S., Welsh, M. and Kloeppel, B. (2007).
Tree growth inferences and prediction from diameter censuses and ring widths. Ecological
Applications, 17, 1942â€“1953.
Condit, R., Hubbell, S. P. and Foster, R. B. (1995). Mortality rates of 205 neotropical tree and
shrub species and the impact of severe drought. Ecological Monographs, 65, 419â€“439.
Coomes, D. A. and Allen, R. B. (2007). Mortality and tree-size distributions in natural mixed-
age forests. Journal of Ecology, 95, 27â€“40.
Dietze, M., Wolosin, M. and Clark, J. (2008). Capturing diversity and individual variability in
allometries: A hierarchical approach. Forest Ecology and Management, 256, 1939â€“1948.
Govindarajan, S., Dietze, M., Agarwal, P. and Clark, J. S. (2004). A scalable model of forest
dynamics. Proceedings of the 20th Symposium on Computational Geometry SCG, pp. 106â€“
115.
Gurevitch, J., Scheiner, S. M. and Fox, G. A. (2002). The Ecology of Plants. Sinauer, Sunder-
land, MA.
Gelfand, A. E. and Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal
densities. Journal of the American Statistical Association, 85, 398â€“409.
Gelman, A. and Hill, J. (2007). Data Analysis using Regression and Multilevel/Hierarchical Models.
Cambridge University Press, Cambridge.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (1995). Bayesian Data Analysis. Chapman
and Hall, London.
Gurevitch, J., Scheiner, S. M. and Fox, G. A. (2002). The Ecology of Plants. Sinauer, Sunder-
land, MA.
Hall, D. B. (2000). Zero-inï¬‚ated Poisson and binomial regression with random effects: A case
study. Biometrics, 56, 1030â€“1039.
Hooten, M. B., Larsen, D. R. and Wikle, C. K. (2003). Predicting the spatial distribution of
ground ï¬‚ora on large domains using a hierarchical Bayesian model. Landscape Ecology, 18,
487â€“502.
IbÃ¡Ã±ez, I., Clark, J. S. and Dietze, M. (2007). Evaluating the sources of potential migrant species:
Implications under climate change. Ecological Applications, 18, 1664â€“1678.
IbÃ¡Ã±ez, I., Clark, J. S., LaDeau, S. and Hille Ris Lambers, J. (2007). Exploiting temporal
variability to understand tree recruitment response to climate change. Ecology Monographs,
77, 163â€“177.
King, D. A., Davies, S. J. and Noor, N. S. M. (2006). Growth and mortality are related to adult
tree size in a Malaysian mixed dipterocarp forest. Forenerty and Ecological Management, 223,
152â€“158.
Kobe, R. K., Pacala, S. W., Jr. Silander, J. A. and Canham, C. D. (1995). Juvenile tree survivorship
as a component of shade tolerance. Ecological Applications, 5, 517â€“532.

Models for Demography of Plant Populations
481
Kobe, R. K. (2006). Sapling growth as a function of light and landscape-level variation in soil
water and foliar N in northern Michigan. Oecologia, 147, 119â€“133.
LaDeau, S. L. and Clark, J. S. (2006). Elevated CO2 and tree fecundity: The role of tree size,
interannual variability, and population heterogeneity. Global Change Biology, 12, 822â€“833.
LaDeau, S., Kilpatrick, M. and Marra, P. P. (2007). West Nile virus emergence and large-scale
declines of North American bird populations. Nature, 447, 710â€“714.
Lambert, D. (1992). Zero-inï¬‚ated Poisson regression, with an application to defects in manu-
facturing. Technometrics, 34, 1â€“14.
Latimer, A. M., S. Wu, A. E. Gelfand, and J. A. Silander. (2006). Building statistical models to
analyze species distributions, Ecological Applications, 16, 33â€“50.
Metcalf, C. J. E., Clark, J. S. and McMahon, S. M. (2009). Overcoming data sparseness and
parametric constraints in modeling of tree mortality: A new non-parametric Bayesian model.
Canadian Journal of Forest Research, 39, 1677â€“1687.
Mohan, J. E., Clark, J. S. and Schlesinger, W. H. (2007). Long-term CO2 enrichment of an intact
forest ecosystem: Implications for temperate forest regeneration and succession. Ecological
Applications, 17, 1198â€“1212.
Nepstad, D. C., Tohver, I. M., Ray, D., Moutinho, P. and Cardinot, G. (2007). Mortality of large
trees and lianas following experimental drought in an Amazon forest. Ecology, 88, 2259â€“2269.
Nelder, J. A. and Wedderburn, R. W. M. (1972). Generalized linear models. Journal of the Royal
Statistical Society (Series A), 135, 370â€“384.
Ogle, K., Uriarte, M., Thompson, J., Johnstone, J., Jones, A., Lin, Y., McIntire, E., Zimmerman,
J. (2006). Implications of vulnerability to hurricane damage for long-term survival of tropical
tree species: A Bayesian hierarchical analysis. In Hierarchical Modeling for the Environmen-
tal Sciences: Statistical Methods and Applications, (ed. J.S. Clark and A.E. Gelfand), Oxford
University Press, Oxford.
Rich, R. L., Frelich, L. E. and Reich, P. B. (2007). Wind throw mortality in the southern boreal
forest: Effects of species diameter and stand age. Journal of Ecology, 95, 1261â€“1273.
Suarez, M. L., Ghermandi, L. and Kitzberger, T. (2004). Factors predisposing episodic drought-
induced tree mortality in Nothofagus site, climatic sensitivity and growth trends. Journal of
Ecology, 92, 954â€“966.
Thomas, S. C. (1996). Relative size at the onset of maturity in rain forest trees: A comparative
analysis of 37 Malaysian species. Oikos, 76, 145â€“154.
Uriarte, M., Canham, C. D., Thompson, J. and Zimmerman, J. K. (2004). A neighborhood
analysis of tree growth and survival in a hurricane-driven tropical forest. Ecological Mono-
graphs, 74, 591â€“614.
Valle, D., Clark, J. S., Dietze, M., Agarwal, P. K., Millette, T. and Schultz, H. (2009). The effect
of light competition on growth rate of large trees. In preparation.
van Mantgem, P.J. and Stephenson, N. L. (2007). Apparent climatically induced increase of tree
mortality rates in a temperate forest. Ecology Letters, 10, 909â€“916.
Wyckoff, P. H., and Clark, J. S. (2000). Predicting tree mortality from diameter growth: A
comparison of maximum likelihood and Bayesian approaches Canadian Journal of Forest
Research, 30, 156â€“167.
Wikle, C. K. (2003). Hierarchical Bayesian models of predicting the spread of ecological
processes. Ecology, 84, 1382â€“1394.
Yao X., Titus, S. J. and MacDonald, S. E. (2001). A generalized logistic model of individual
tree mortality for aspen, white spruce, and lodgepole pine in Alberta mixedwood forests.
Canadian Journal of Forestry Research, 31, 283â€“291.

Â·18Â·
Combining monitoring data and
computer model output in assessing
environmental exposure
Alan E. Gelfand and Sujit K. Sahu
18.1 Introduction
The demand for spatial models to assess regional progress in air quality has
grown rapidly over the past decade. For improved environmental decision-
making, it is imperative that such models enable spatial prediction to reveal
important gradients in air pollution, offer guidance for determining areas in
non-attainment with air standards, and provide air quality input to models
for determining individual exposure to air pollution. Spatial prediction has the
potential to suggest new perspectives in the development of emission control
strategies and to provide a credible basis for resource allocation decisions,
particularly with regard to network design.
Space-time modelling of pollutants has some history including, e.g. Guttorp
et al. (1994), Haas (1995) and Carroll et al. (1997). In recent years, hierar-
chical Bayesian approaches for spatial prediction of air pollution have been
developed (Brown et al., 1994; Le et al., 1997; Sun et al., 2000). Cressie et al.
(1999) compared kriging and Markov-random ï¬eld models in the prediction
of PM10 (particles with diameters less than 10 Ãm) concentrations around the
city of Pittsburgh. Zidek et al. (2002) developed predictive distributions on
non-monitored PM10 concentrations in Vancouver, Canada. They noted the
under-prediction of extreme values in the pollution ï¬eld, but their method-
ology provided useful estimates of uncertainties for large values. Sun et al.
(2000) developed a spatial predictive distribution for the space-time response
of daily ambient PM10 in Vancouver. They exploit the temporal correlation
structure present in the observed data from several sites to develop a model
with two components, a common deterministic trend across all sites plus a
stochastic residual. They illustrate the methods by imputing daily PM10 ï¬elds
in Vancouver. Kibria et al. (2000) developed a multivariate spatial prediction
methodology in a Bayesian context for the prediction of PM2.5 in the city of
Philadelphia. This approach used both PM2.5 and PM10 data at monitoring sites

Assessing Environmental Exposure
483
with different start-up times. Shaddick and Wakeï¬eld (2002) proposeed short
term space-time modeling for PM10.
Smith et al. (2003) proposed a spatio-temporal model for predicting weekly
averages of PM2.5 and other derived quantities such as annual averages within
three southeastern states in the United States. The PM2.5 ï¬eld is represented
as the sum of semiparametric spatial and temporal trends, with a random
component that is spatially correlated, but not temporally. These authors apply
a variant of the expectation-maximization (EM) algorithm to account for high
percentages of missing data. Sahu and Mardia (2005) present a short-term fore-
casting analysis of PM2.5 data in New York City during 2002. Within a Bayesian
hierarchical structure, they use principal kriging functions to model the spatial
structure and a vector random-walk process to model temporal dependence.
Sahu et al. (2006) consider modelling of PM2.5 through the use of rural and
urban process models while Sahu et al. (2007) deal with misalignment between
ozone data and meteorological information. Sahu et al. (2009) develop a hier-
archical space-time model for daily eight-hour maximum ozone concentration
data covering much of the eastern United States. The model combines observed
data and forecast output from a computer model known as the Community
Multi-scale Air Quality (CMAQ) Eta forecast model (see below for references) so
that the next day forecasts can be computed in real time. They validate the model
with a large amount of set-aside data and obtain much improved forecasts of
daily O3 patterns. Berrocal et al. (2010) propose a downscaling approach by
regressing the observed point level ozone concentration data on grid cell level
computer model output with spatially varying regression co-efï¬cients speciï¬ed
through a Gaussian process. Rappold et al. (2008) study wet mercury deposition
over space and time. Finally, Wikle (2003) provides a nice overview of the role
of hierarchical modeling in environmental science. With so much interest in
space-time exposure prediction, attention to data fusion models to improve
such prediction is not surprising.
18.1.1 Environmental computer models
Computer models are playing an increasing role in our quest to under-
stand complex systems. In this regard, the discussion paper of Kennedy and
Oâ€™Hagan (2001) reviews prediction and uncertainty analysis for systems which
are approximated by complex mathematical models. These models are often
implemented as computer codes and typically depend on a number of input
parameters which determine the nature of the output. The input parameters
are often unknown and are customarily estimated by ad hoc methods such
as very crude ï¬tting of the computer model to the observed data. Kennedy
and Oâ€™Hagan present a Bayesian calibration technique which improves on this
usual approach in two respects. First, Bayesian prediction methods allow one

484
The Oxford Handbook of Applied Bayesian Analysis
to account for all sources of uncertainty including the ones from the estimation
of the parameters. Second, any inadequacy in the model speciï¬cation, even
under the best-ï¬tting parameter values, is revealed by discrepancies between
the observed data and the model predictions. Illustration is provided using
data from a nuclear radiation release at Tomsk and also from a more complex
simulated nuclear accident exercise.
Cox et al. (2001) describe a statistical procedure for estimation of unknown
parameters in a complex computer model from an observational or experimen-
tal data base. They develop methods for accuracy assessments of the estimates
and illustrate their results in the setting of computer code which models nuclear
fusion reactors. Fuentes et al. (2003) develop a formal method for evaluation
of the performance of numerical models. They apply the method to an air
quality model (essentially the CMAQ model) and discuss related issues in the
estimation of nonstationary spatial covariance structures.
Turning to environmental computer models, high spatial resolution numer-
ical model output is now widely available for various air pollutants. Our focus
here is on the CMAQ forecast model. CMAQ is a modelling system which
has been designed to approach air quality as a whole by including capabili-
ties for modelling multiple air quality issues, including tropospheric ozone,
ï¬ne particles, toxics, acid deposition, and visibility degradation. CMAQ was
also designed to have multiscale capabilities so that separate models are not
needed for urban and regional scale air quality modelling. The target grid
resolutions and domain sizes for CMAQ range spatially and temporally over
several orders of magnitude. With the temporal ï¬‚exibility of the model, sim-
ulations can be performed to evaluate longer term (annual to multi-year) pol-
lutant climatologies as well as short term (weeks to months) transport from
localized sources. The ability to handle a large range of spatial scales enables
CMAQ to be used for urban and regional scale model simulations. See, e.g.
http://www.epa.gov/asmdnerl/CMAQ/.
It is worth distinguishing the goal of CMAQ which is to provide ambient
exposure at high spatial and temporal resolution from computer models that
provide individual level exposure. In particular, the former, assimilated with
station data, provide the ambient exposures which drive the latter. Again, the
contribution of this chapter is to discuss fully model-based implementations of
this fusion.
With regard to the latter, Zidek and his co-authors have written a series of
papers considering prediction of human exposure to air pollution. In particular,
Zidek et al. (2007) present a general framework for constructing a predictive
distribution for the exposure to an environmental hazard sustained by a ran-
domly selected member of a designated population. The individualâ€™s exposure is
assumed to arise from random movement through the environment, resulting

Assessing Environmental Exposure
485
in a distribution of exposure that can be used for environmental risk analysis.
Zidek et al. (2005) develop a computing platform, referred to as pCNEM, to
produce such distributions. This software is intended for simulating exposures
to airborne pollutants. In the paper they illustrate with a model for predicting
human exposure to PM10.
Further work along these lines has been an objective of US Environmental
Protection Agency (EPA) initiatives. The EPAâ€™s National Exposure Research
Laboratory (NERL) has developed a population exposure and dose model, par-
ticularly for particulate matter (PM), called the Stochastic Human Exposure
and Dose Simulation (SHEDS) model (Burke et al., 2003). SHEDS-PM uses
a probabilistic approach that incorporates both variability and uncertainty to
predict distributions of PM exposure, inhaled dose, and deposited dose for
a speciï¬ed population. SHEDS-PM estimates the contribution of PM from
both outdoor and indoor sources (e.g. cigarette smoking, cooking) to total per-
sonal PM exposure and dose. In particular, SHEDS-PM generates a simulation
population using US Census demographic data for the user-speciï¬ed popula-
tion with randomly assigned activity diaries of individuals. Output from the
SHEDS-PM model includes distributions of exposure and dose for the speciï¬ed
population, as well as exposure and dose proï¬les for each simulated individual.
It is Bayesian in its conception in the sense that the input parameters (e.g.
air exchange rates, penetration rates, cooking and smoking emission rates) are
drawn at random from suitable priors.
A similar EPA product, the Air Pollutants Exposure Model (APEX) was
developed by the Ofï¬ce of Air Quality and Planning (Richmond et al. 2002).
It is derived from the probabilistic National Ambient Air Quality Standards
(NAAQS) Exposure Model for carbon monoxide (pNEM/CO). APEX serves
as the human inhalation exposure model within the Total Risk Integrated
Methodology (TRIM) model framework. APEX is intended to be applied at
the local, urban, or consolidated metropolitan area scale and currently only
addresses inhalation exposures. The model simulates the movement of indi-
viduals through time and space and their exposure to the given pollutant
in various micro-environments (e.g. outdoors, indoors residence, in-vehicle).
Results of the APEX simulations are provided as hourly and summary expo-
sure and/or dose estimates, depending on the application, for each individual
included in the simulation as well as summary statistics for the population
modelled.
The format of the remainder of this chapter is as follows. In Section 18.2
we review some algorithmic and pseudo-statistical approaches in weather
prediction. Section 18.3 provides a review of current state of the art fusion
methods for environmental data. We develop a non-dynamic downscaling
approach based on our recent work (Sahu, Gelfand, and Holland, 2010) in

486
The Oxford Handbook of Applied Bayesian Analysis
Section 18.4. A few summary remarks are provided in Section 18.5. Appendix A
contains an introduction to Gaussian processes (GP) and Appendix B outlines
the full conditional distributions for the downscaler approach proposed in
Section 18.4.
18.2 Algorithmic and pseudo-statistical approaches in
weather prediction
A convenient framework within to review algorithmic and pseudo-statistical
approaches to data assimilation is in the context of numerical weather
prediction. Kalnay (2003) provides a recent development of this material. Such
assimilation has a long history dating at least to Charney (1951) who recognized
that hand interpolation of available weather observations to a regular grid was
too time consuming and that numerical interpolation methods were needed.
Earliest work created local polynomial interpolations using quadratic trend
surfaces in locations in order to interpolate observed values to grid values. Of
course, in the past half century, such polynomial interpolation has come a long
way to become a standard device in the statisticianâ€™s toolkit; we do not detail this
literature here.
Instead, we note that what emerged in the meteorology community was
the recognition that a ï¬rst guess (or background ï¬eld or prior information)
was needed (Bergthorsson and DÃ¶Ã¶s, 1955), supplying the initial conditions.
As short-range forecasts became better and better, their use as a ï¬rst guess
became universal. The climatological intuition here is worth articulating. Over
â€˜data-richâ€™ areas the observational data dominates while in â€˜data-poorâ€™ regions
the forecast facilitates transport of information from the data-rich areas. Of
course, in the setting of fully-speciï¬ed models and fully model-based inference
we can quantify this adaptation and the associated uncertainty. Indeed, this is
the contribution of the following sections of this chapter.
We illustrate several numerical approaches using, illustratively, temperature
as the variable of interest. At time t, we let Tobs(t) be an observed measurement,
Tb(t) a background level, Ta(t) an assimilated value, and Ttrue(t) the true value.
An early scheme is known as the successive corrections method (SCM) which
obtains Ti,a(t) iteratively through
T (r+1)
i,a
(t) = T (r)
i,a (t) +
6
k
wik

Tk,obs(t) âˆ’T (r)
k,a(t)
7 D 
k
wik + Ã‚2

.
Here, i indexes the grid cells for the interpolation while k indexes the observed
data locations. T (r)
k,a(t) is the value of the assimilator at the rth iteration at
the observation point k (obtained from interpolating the surrounding grid
points). The weights, wik, can be deï¬ned in various ways but usually as a

Assessing Environmental Exposure
487
decreasing function of the distance between the grid point and the observation
point. In fact, they can vary with iteration, perhaps becoming increasingly
local. See, e.g. Cressman (1959) and Bratseth (1986). The analysis reï¬‚ects
the observations more faithfully when Ã‚2 is taken to be zero, see Cressman
(1959). Non-zero values of Ã‚2 are assumed when the observations have errors,
and the resulting analyses provide some positive weight to the background
ï¬eld.
Another empirical approach is called nudging or Newtonian relaxation. Sup-
pose, suppressing location, we think about a differential equation driving tem-
perature, e.g. dT(t)/dt = a(T(t), t, Ã‹(t)). If we write a(Â·) as an additive form say
a(T(t), t) + Ã‹(t) and let Ã‹(t) = (Tobs(t) âˆ’T(t))/Ã™ then Ã™ controls the relaxation.
Small Ã™ implies that the Ã‹(t) term dominates while large Ã™ implies that the
nudging effect will be negligible.
We next turn to a least squares approach. Again, suppressing location, sup-
pose we assume that T(1)
obs(t) = Ttrue(t) + Ã‚1(t) and T (2)
obs(t) = Ttrue(t) + Ã‚2(t) where
we envision two sources of observational data on the true temperature at t.
The Ã‚l have mean 0 and variance Ã›2
l ,l = 1, 2. Then, with the variances known,
it is a familiar exercise to obtain the best unbiased estimator of Ttrue(t) based
upon these two pieces of information. That is, Ta(t) = a1T (1)
obs(t) + a2T(2)
obs(t) where
a1 = Ã›2
2/(Ã›2
1 + Ã›2
2) and a2 = Ã›2
1/(Ã›2
1 + Ã›2
2). Of course, we obtain the same solution
as the maximum likelihood estimates (MLE) if we use independent normal
likelihoods for the T (l)
obs(t)s.
A last idea here is simple sequential assimilation and its connection to the
Kalman ï¬lter. In the univariate case suppose we write Ta(t) = Tb(t) + â€ž(Tobs(t) âˆ’
Tb(t)). Here, Tobs(t) âˆ’Tb(t) is referred to as the observational innovation or
observational increment relative to the background. The optimal weight â€ž =
Ã›2
obs/(Ã›2
obs + Ã›2
b), analogous to the previous paragraph. Hence, we only need a
prior estimate of the ratio of the observational variance to the background
variance in order to obtain Ta(t). To make this scheme dynamic, suppose the
background is updated through the assimilation, i.e. Tb(t + 1) = h(Ta(t)) where
h(Â·) denotes some choice of forecast model. Then we will also need to create a
revised background variance; this is usually taken to be a scalar (>1) multiple
of the variance of Ta(t).
Finally, the multivariate assimilation idea is now clear. Now we collect the grid
cell variables to vector variables and write Ta(t) = Tb(t) + W(Yobs(t) âˆ’g(Tb(t))).
Here, the vector Yobs denotes variables that are different from the ones we seek
to interpolate. For temperature, these might be Doppler shifts, radar reï¬‚ec-
tivities, or satellite radiances. Then, g is the nonlinear operator that converts
background temperatures into guesses for these new variables. The dimension
of Y is not necessarily the same as that of T. W is the gain matrix that usually
appears in the Kalman ï¬lter. Finally, we introduce errors in the transitional
stage, Tb(t) = Ttrue(t) + Ã‚b(t) and Ta(t) = Ttrue(t) + Ã‚a(t), as well as errors in the

488
The Oxford Handbook of Applied Bayesian Analysis
observational stage, i.e. for the Yobs(t). Assuming all errors are Gaussian, the
dynamic model is speciï¬ed and the Kalman ï¬lter can be implemented to ï¬t the
model.
18.3 Review of data fusion methods for environmental exposure
Recall that our objective is to combine model output and station data to improve
assessment of environmental exposure. Such synthesis is referred to as assimi-
lation or fusion. Here we move from the more algorithmic strategies of the pre-
vious section to fully model-based approaches. In the next two subsections we
review the work of Fuentes and Raftery (2005) which has received considerable
attention and the very recent work of McMillan et al. (2008). A full development
of the approach of Sahu et al. (2010) with an example is deferred to the following
sections.
18.3.1 Fusion modelling using stochastic integration
The fusion approach proposed by Fuentes and Raftery (2005) builds upon
earlier Bayesian melding work in Poole and Raftery (2000). It conceptualizes
a true exposure surface and views the monitoring station data as well as the
model output data as varying in a suitable way around the true surface. In
particular, the average exposure in a grid cell A, denoted by Z(A), differs from
the exposure at any particular location s, Z(s). The so-called change of support
problem in this context addresses converting the point level Z(s) to the grid
level Z(A) through the stochastic integral,
Z(A) =
1
|Aj|

Aj
Z(s) ds,
(18.1)
where |A| denotes the area of the grid cell A. Fusion modelling, working with
block averaging as in (18.1) has been considered by, e.g. Fuentes and Raftery
(2005).
Let Y(s) denote the true exposure corresponding to Z(s) at a station s. The
ï¬rst model assumption is:
Z(s) = Y(s) + Ã‚(s)
(18.2)
where Ã‚(s) âˆ¼N(0, Ã›2
Ã‚) represents the measurement error at location s. The true
exposure process is assumed to be:
Y(s) = Ã(s) + Ã(s)
(18.3)
where Ã(s) provides the spatial trend often characterized by known functions
of the site characteristics such as the components of s, elevation etc. The error
term Ã(s) is a spatially coloured process assumed to be the zero mean GP with

Assessing Environmental Exposure
489
a speciï¬ed covariance function. (Appendix A provides an introduction to GPs.)
The output of the computer model denoted by Q(s) is often known to be biased
and hence this is modelled as:
Q(s) = a(s) + b(s)Y(s) + â€°(s)
(18.4)
where a(s) denotes the additive bias and b(s) denotes the multiplicative bias.
The error term, â€°(s), is assumed to be a white noise process given by N(0, Ã›2
â€°).
However, the computer model output is provided in a grid, A1, . . . , AJ so
the point level process is converted to a grid level one by the stochastic inte-
gral (18.1) for the model (18.4), i.e.
Q(Aj) =
1
|Aj|
6
Aj
a(s) ds +

Aj
b(s)Y(s) ds +

Aj
â€°(s) ds
7
.
It is acknowledged that unstable model ï¬tting accrues to the case where we
have spatially varying b(s) so b(s) = b is adopted. Spatial prediction at a new
location s â€² is done through the posterior predictive distribution p(Y(s â€²)|Z, Q)
where Z denote all the station data and Q denote all the grid-level computer
output Q(A1), . . . , Q(AJ ).
This fusion strategy becomes computationally infeasible in the setting of
fusing say, CMAQ data at 12 km2 grid cells for the eastern United States with
station data for this region. We have a very large number of grid cells with a rel-
atively sparse number of monitoring sites. An enormous amount of stochastic
integration is required. In this regard, a dynamic implementation over many
time periods becomes even more infeasible. Recently Berrocal et al. (2010)
have shown that the fusion strategy can be outperformed by their proposed
downscaling approach both in terms of computing speed and out-of-sample
validation.
18.3.2 Fusion modelling by upscaling
While the Fuentes and Raftery (2005) approach models at the point level, the
strategy in McMillan et al. (2008) scales up to, models at, the grid cell level.
In this fashion, computation is simpliï¬ed and fusion with space-time data is
manageable.
In particular, suppose that we have, say, n monitoring stations. As before,
let Q(Aj) denote the CMAQ output value for cell Aj while ZAj (si) denotes the
station data for site si within cell Aj, i = 1, . . . , k j. Of course, for most of the
jâ€™s, k j will be 0 since n â‰ªJ . Let Y(Aj) denote the true value for cell Aj.
Then, paralleling (18.2) and (18.4), for each j = 1, . . . , J,
ZAj (si) = Y(Aj) + Ã‚Aj (si),
i = 1, . . . , k j
(18.5)

490
The Oxford Handbook of Applied Bayesian Analysis
and
Q(Aj) = Y(Aj) + b(Aj) + â€ž(Aj).
(18.6)
In (18.6), the CMAQ output is modelled as varying around the true value
with a bias term, denoted by b(Aj), speciï¬ed using a B-spline model. Also,
the Ã‚â€™s are assumed to be independently and identically distributed and so
are the â€žâ€™s, each with a respective variance component. So, the station data
and the CMAQ data are conditionally independent given the true surface.
Finally, the true surface is modeled analogously to (18.3). But now, the Ãâ€™s are
given a CAR speciï¬cation (see, e.g. Banerjee et al., 2004). For space-time data,
McMillan et al. (2008) offer a dynamic version of this approach, formulated by
assuming a dynamic CAR speciï¬cation for the Ãâ€™s. They illustrate with a fusion
for the year 2001.
18.4 A downscaling approach
Very recently, Sahu et al. (2010) proposed a modelling approach that avoids
the computationally demanding stochastic integrations required in Fuentes
and Raftery (2005) but models at the point rather than the grid cell level as
in McMillan et al. (2008). In particular, they formalize a latent atmospheric
process which is modeled at two different scales, at the point level to align with
the station data and at the grid cell level to align with the resolution for the
computer model output. The models at these two scales are connected through
a measurement error model (MEM). The latent processes are introduced to
capture point masses at 0 with regard to chemical deposition while the MEM
circumvents the stochastic integration in (18.1). In particular, the point level
observed data represent â€˜ground truthâ€™ while gridded CMAQ output are antic-
ipated to be biased. As a result, the MEM enables calibration of the CMAQ
model. The opposite problem of disaggregation, i.e. converting the grid level
computer output Q(Aj) to point level ones, Q(si) is not required. The only
assumption is that Q(Aj) is a reasonable surrogate for Z(si) if the site si is
within the grid cell Aj. In this sense, the approach is a downscaler, scaling the
grid cell level CMAQ data to the point-level station data.
Sahu et al. (2010) model the above fusion approach in a dynamic setting mod-
elling weekly chemical deposition data over a year. They utilize precipitation
information to model wet deposition since there can be no deposition without
precipitation. They also handle occurrences of zero values in both precipitation
and deposition. They introduce a latent space-time atmospheric process which
drives both precipitation and deposition as assumed in the mercury deposition
modelling of Rappold et al. (2008). However, Rappold et al. do not address
the fusion problem with modeled output. Rather, they used a point level joint

Assessing Environmental Exposure
491
process model, speciï¬ed conditionally for the atmospheric, precipitation and
deposition processes. Sahu et al. illustrate their methods separately for both wet
sulfate and wet nitrate deposition in the eastern United States.
18.4.1 The modelling detail
Here we present detail for the static version of the dynamic spatial model devel-
oped in Sahu et al. (2010). Let R(si) and Z(si) denote the observed precipitation
and deposition respectively at a site si, i = 1, . . . , n. We suppose that R(si) and
Z(si) are driven by a point level latent atmospheric process, denoted by V(si),
and both take the value zero if V(si) < 0 to reï¬‚ect that there is no deposition
without precipitation. That is,
R(si) =

exp

U(si)

if V(si) > 0
0
otherwise,
(18.7)
and
Z(si) =

exp

Y(si)

if V(si) > 0
0
otherwise.
(18.8)
The random variables U(si) and Y(si) are thus taken as log observed precipita-
tion and deposition respectively when V(si) > 0. The models described below
will specify their values when V(si) â‰¤0 and/or the corresponding R(si) or Z(si)
are missing.
Similar to (18.8) we suppose that the CMAQ model output at grid cell Aj,
Q(Aj), is positive if an areal level latent atmospheric process, denoted by ËœV(Aj),
is positive,
Q(Aj) =

exp

X(Aj)

if ËœV(Aj) > 0
0
otherwise.
(18.9)
The values of X(Aj) when ËœV(Aj) â‰¤0 will be given by the model described
below. As computer model output, there are no missing values in the Q(Aj).
Let R, Z, and Q denote all the precipitation values, wet deposition values
and the CMAQ model output, respectively. Similarly deï¬ne the vectors U, V,
and Y collecting all the elements of the corresponding random variable for i =
1, . . . , n. Let X and ËœV denote the vectors collecting the elements X(Aj) and
ËœV(Aj), j = 1, . . . , J, respectively.
The ï¬rst stage likelihood implied by the deï¬nitions (18.7), (18.8) and (18.9)
is given by:
p(R, Z, Q|U, Y, X, V, ËœV) = p(R|U, V) Ã— p(Z|Y, V) Ã— p(Q|X, ËœV)
(18.10)

492
The Oxford Handbook of Applied Bayesian Analysis
which takes the form
n

i=1

1exp(u(si))1exp(y(si))I(v(si) > 0)

J
j=1

1exp(x(Aj ))I(Ëœv(Aj) > 0)

where 1x denotes a degenerate distribution with point mass at x and I(Â·) is the
indicator function.
18.4.2 Second stage speciï¬cation
In the second stage of modelling we begin by specifying a spatially coloured
regression model for log-precipitation based on the latent process V(si). In
particular, we assume the model:
U(si) = Â·0 + Â·1V(si) + â€°(si),
i = 1, . . . , n
(18.11)
where â€° = (â€°(s1), . . . , â€°(sn))â€² is an independent GP following the N(0, â€°) distri-
bution; â€° has elements Ã›â€°(i, j) = Ã›2
â€° exp(âˆ’Ë†â€°di j), the usual exponential covari-
ance function, where di j is the geodetic distance between sites si and s j. Using
vector notation, the above speciï¬cation is equivalently written as:
U âˆ¼N (Â·0 + Â·1V, â€°) .
To model Y(si), we assume that:
Y(si) = â€š0 + â€š1U(si) + â€š2V(si) +

b0 + b(si)

X(Aki) + Ã(si) + Ã‚(si),
(18.12)
for i = 1, . . . , n where, unless otherwise mentioned, Aki is the grid cell which
contains the site si.
The error terms Ã‚(si) are assumed to follow N(0, Ã›2
Ã‚) independently, provid-
ing the so-called nugget effect. The reasoning for the rest of the speciï¬cation
in (18.12) is as follows. The term â€š1U(si) is included because of the strong lin-
ear relationships between log-deposition and log-precipitation, see Figure 18.3
below. The term â€š2V(si) captures any direct inï¬‚uence of the atmospheric
process V(si) on Y(si) in the presence of precipitation.
It is anticipated that the relationship between the station data and the CMAQ
model output will be roughly linear but that this relationship may vary locally.
To specify a rich class of locally linear models we may think of a spatially
varying slope for the regression of Y(si) on log-CMAQ values X(Aj), speciï¬ed as

b0 + b(si)

X(Aki) in (18.12). Writing b = (b(s1), . . . , b(sn))â€² we propose a mean
0 GP for b, i.e.
b âˆ¼N(0, b)
where b has elements Ã›b(i, j) = Ã›2
b exp(âˆ’Ë†bdi j).

Assessing Environmental Exposure
493
The term Ã(si) provides a spatially varying intercept which can also be inter-
preted as a spatial adjustment to the overall intercept parameter â€š0. We assume
that
Ã âˆ¼N(0, Ã),
where Ã = (Ã(s1), . . . , Ã(sn))â€² and Ã has elements Ã›Ã(i, j) = Ã›2
Ã exp(âˆ’Ë†Ãdi j). The
regression model (18.12) is now equivalently written as:
Y âˆ¼N

Ë‡, Ã›2
Ã‚ In

where Y = (Y(s1), . . . , Y(sn))â€² and Ë‡ = â€š0 + â€š1u + â€š2v + b0x + Xmb + Ã where x is
the n-dimensional vector with the ith element given by x(Aki) and Xm is a
diagonal matrix whose ith diagonal entry is x(Aki), i = 1, . . . , n and In is the
identity matrix of order n.
The CMAQ output X(Aj) is modelled using the latent process ËœV(Aj) as
follows:
X(Aj) = â€ž0 + â€ž1 ËœV(Aj) + Â¯(Aj),
j = 1, . . . , J.
(18.13)
where Â¯(Aj) âˆ¼N(0, Ã›2
Â¯) independently for all j = 1, . . . , J , and Ã›2
Â¯ is unknown.
In vector notation, this is given by:
X âˆ¼N

â€ž0 + â€ž1 ËœV, Ã›2
Â¯IJ

where as before, X = (X(A1), . . . X(AJ ))â€² and ËœV = ( ËœV(A1), . . . ËœV(AJ ))â€², see the
partitioning of ËœV below equation (18.14) regarding the order of the grid cell
indices 1, . . . , J .
We now turn to speciï¬cation of the latent processes V(si) and ËœV(Aj). Note
that it is possible to have Z(si) > 0 and Q(Aki) = 0 and vice versa since Q(Aki) is
the output of a computer model which has not used the actual observation Z(si).
This implies that V(si) and ËœV(Aki) can be of different signs. To accommodate
this ï¬‚exibility and to distinguish between the point and areal processes we
assume the simple measurement error model:
V(si) âˆ¼N
 ËœV(Aki), Ã›2
v

,
i = 1, . . . , n
(18.14)
where Ã›2
v is unknown. Without loss of generality we write ËœV = ( ËœV(1), ËœV(2)) where
the n-dimensional vector ËœV (1) contains the values for the grid cells where the
n observation sites are located and ËœV(2) contains the values for the remaining
J âˆ’n grid cells. The speciï¬cation (18.14) can now be written equivalently as
V âˆ¼N
 ËœV (1), Ã›2
vIn

.

494
The Oxford Handbook of Applied Bayesian Analysis
The latent process ËœV(Aj) is assumed to follow a conditionally autoregressive
(CAR) process in space (see e.g. Banerjee et al., 2004). That is,
ËœV(Aj) âˆ¼N
 J

i=1
h ji ËœV(Ai),
Ã›2
ÃŠ
mj

(18.15)
where
h ji =
 1
mj
if i âˆˆâˆ‚j
0
otherwise
and âˆ‚j deï¬nes the mj neighbouring grid cells of the cell Aj. The above improper
CAR speciï¬cation can be written as:
p
 ËœV|Ã›2
ÃŠ

âˆexp

âˆ’1
2
ËœV
â€²Dâˆ’1(I âˆ’H) ËœV

(18.16)
where D is diagonal with the jth diagonal entry given by Ã›2
ÃŠ/mj. In summary,
the second stage speciï¬cation is given by:
p (Y|U, V, X, Ã, b, Ã‹) Ã— p (Ã|Ã‹) Ã— p (U|V, Ã‹) Ã— p
 ËœV|Ã‹

Ã—p (V|Ã‹) Ã— p

X| ËœV, Ã‹

Ã— p (b|Ã‹)
where Ã‹ denote the parameters Â·0, Â·1, â€š0, â€š1, â€š2, b0, â€ž0, â€ž1, Ã’, Ã›2
â€°, Ã›2
b, Ã›2
Ã, Ã›2
Ã‚, Ã›2
Â¯,
Ã›2
v and Ã›2
ÃŠ. See Appendix B for the prior distributions, the form of the joint
posterior distribution and the full conditional distributions needed for Gibbs
sampling.
18.4.3 Spatial interpolation at a new location
We can interpolate the deposition surface using the above models as follows.
Consider the problem of predicting Z(s â€²) at any new location s â€² falling on the
grid cell Aâ€². The prediction is performed by constructing the posterior predictive
distribution of Z(s â€²) which in turn depends on the distribution of Y(s â€²) as
speciï¬ed by equation (18.12) along with the associated V(s â€²). We estimate the
posterior predictive distribution by drawing samples from it.
Several cases arise depending on the nature of information available at the
new site s â€². If precipitation information is available and there is no positive
precipitation, i.e. r(s â€²) = 0, then we have Z(s â€²) = 0 and no further sampling
is needed, since there can be no deposition without precipitation. Now sup-
pose that there is positive precipitation, i.e. r(s â€²) > 0, then set u(s â€²) = log(r(s â€²)).
We need to generate a sample Y(s â€²). We ï¬rst generate V(s â€²) âˆ¼N( ËœV(Aâ€²), Ã›2
v)
following the measurement error model (18.14). Note that ËœV(Aâ€²) is already
available for any grid cell Aâ€² (within the study region) from model ï¬tting,
see equation (18.15). Similarly, X(Aâ€²) is also available either as the log of the

Assessing Environmental Exposure
495
CMAQ output, log(Q(Aâ€²)), if Q(Aâ€²) > 0 or from the MCMC imputation when
Q(Aâ€²) = 0, see Appendix B. To sample Ã(s â€²) we note that:
Ã(s â€²)
Ã

âˆ¼N
80
0

, Ã›2
Ã

1
SÃ,12
SÃ,21
SÃ
9
,
where SÃ,12 is 1 Ã— n with the ith entry given by exp{âˆ’Ë†Ãd(si, s â€²)} and SÃ,21 =
Sâ€²
Ã,12. Therefore,
Ã(s â€²)|Ã, Ã‹ âˆ¼N

SÃ,12Sâˆ’1
Ã Ã, Ã›2
Ã

1 âˆ’SÃ,12Sâˆ’1
Ã SÃ,21

.
(18.17)
If the term b(s) is included in the model we need to simulate b(s â€²) conditional
on b and model parameters. To do this we note that:
b(s â€²)
b

âˆ¼N
80
0

, Ã›2
b

1
Sb,12
Sb,21
Sb
9
,
where Sb,12 is 1 Ã— n with the ith entry given by exp

âˆ’Ë†Ãd(si, s â€²)

and Sb,21 =
Sâ€²
b,12. Therefore,
b(s â€²)|b, Ã‹ âˆ¼N

Sb,12Sâˆ’1
b b, Ã›2
b

1 âˆ’Sb,12Sâˆ’1
b Sb,21

.
(18.18)
If it is desired to predict Z(s â€²) where R(s â€²) is not available, we proceed as
follows. We generate V(s â€²) âˆ¼N( ËœV(Aâ€²), Ã›2
v) following the measurement error
model (18.14). If this V(s â€²) < 0, then we set both R(s â€²) and Z(s â€²) to zero. If,
however, V(s â€²) > 0 we need to additionally draw U(s â€²) using the precipitation
model (18.11). For this we note that,
U(s â€²)
U

âˆ¼N
8Â·0 + Â·1V(s â€²)
Â·0 + Â·1V

, Ã›2
â€°

1
Sâ€°,12
Sâ€°,21
Sâ€°
9
,
where Sâ€°,12 is 1 Ã— n with the ith entry given by exp

âˆ’Ë†â€°d(si, s â€²)

and Sâ€°,21 =
Sâ€²
â€°,12. Therefore,
U(s â€²)|U, Ã‹ âˆ¼N

Ã(s â€²), Ã›2
â€°

1 âˆ’Sâ€°,12Sâˆ’1
â€° Sâ€°,21

,
(18.19)
where
Ã(s â€²) = Â·0 + Â·1V(s â€²) + Sâ€°,12Sâˆ’1
â€° (U âˆ’Â·0 âˆ’Â·1v).
If Z(s â€²) is not inferred to be zero then we set it to be exp

Y(s â€²)

. If we want
the predictions of the smooth deposition surface without the nugget term we
simply ignore the nugget term Ã‚(s â€²) in generating Y(s â€²).
18.4.4 An illustration
We illustrate with weekly wet deposition data for 2001 from 120 sites mon-
itored by the National Atmospheric Deposition Program (NADP, nadp.
sws.uiuc.edu) in the eastern United States, see Figure 18.1. We analyze data

496
The Oxford Handbook of Applied Bayesian Analysis
Fig. 18.1 A map of the eastern US with the 120 NADP sites plotted as points.
from the year 2001 since this is the year for which the most recent outputs from
the CMAQ model for wet chemical deposition are currently available. These
outputs are available for J = 33, 390 grid cells covering the study region. Our
approach is applied separately to the wet sulfate and wet nitrate data. Since
there is no need to make any simultaneous inference, a joint model is not
required. There is high correlation between the two types of deposition but
this is expected since both are driven by precipitation. To facilitate spatial
interpolation, we also use weekly precipitation data obtained from a network
of 2827 sites located inside the study region.
We model the data separately for the week of January 16â€“22 and May 22â€“
28 in 2001 to make a comparison between a week in the winter and another
one in the summer. Deposition data for these two weeks show signiï¬cant
differences according to classical t-tests, see Figure 18.2. This conï¬rms the
fact that the deposition levels are generally higher during the wet summer
months and lower during the drier winter months, see e.g. Brook et al. (1995).
However, in both the weeks there is strong linear relationship between depo-
sition and precipitation (on the log-scale), see Figure 18.3. There is also some,
although not very strong, linear relationship between observed NADP data and
the CMAQ output for the corresponding grid cell containing the NADP site,
see Figure 18.4. Deposition and precipitation values that are 0 are ignored in
obtaining the above Figures 18.3 and 18.4.
The spatial interpolation maps are provided in Figure 18.5 for sulfate and
Figure 18.7 for nitrate. As seen in Figure 18.2 the model has reconstructed
higher levels of both deposition types for May 22â€“28 than that for January
16â€“22. Observe that the grey scales are different for the two weeks in each

Assessing Environmental Exposure
497
0.0
0.5
1.0
1.5
Jan 16â€“22
May 22â€“28
wetso4
(a)
0.0
0.5
1.0
1.5
2.0
2.5
Jan 16â€“22
May 22â€“28
wetno3
(b)
Fig. 18.2 Boxplot of depositions for two weeks in 2001 from 120 NADP sites: (a) wet sulfate and
(b) wet nitrate.
log precipitation
log sulfate
â€“2
â€“1
0
1
2
â€“4
â€“3
â€“2
â€“1
0
(a)
log precipitation
log sulfate
â€“3
â€“2
â€“1
0
1
2
â€“5
â€“4
â€“3
â€“2
â€“1
0
(b)
log precipitation
log nitrate
â€“2
â€“1
0
1
2
â€“4
â€“3
â€“2
â€“1
0
1
(c)
log precipitation
log nitrate
â€“3
â€“2
â€“1
0
1
2
â€“5
â€“4
â€“3
â€“2
â€“1
0
(d)
Fig. 18.3 Plot of log deposition against log precipitation in 2001: (a) wet sulfate for January 16â€“22,
(b) wet sulfate for May 22â€“28, (c) wet nitrate for January 16â€“22, (d) wet nitrate for May 22â€“28.

498
The Oxford Handbook of Applied Bayesian Analysis
log cmaq values
log sulfate
â€“4
â€“3
â€“2
â€“1
0
â€“4
â€“3
â€“2
â€“1
0
(a)
â€¢
log cmaq values
log sulfate
â€“6
â€“4
â€“2
0
â€“5
â€“4
â€“3
â€“2
â€“1
0
(b)
log cmaq values
log nitrate
â€“4
â€“3
â€“2
â€“1
0
â€“4
â€“3
â€“2
â€“1
0
1
(c)
log cmaq values
log nitrate
â€“6
â€“4
â€“2
0
â€“5
â€“4
â€“3
â€“2
â€“1
0
(d)
Fig. 18.4 Plot of log deposition against log CMAQ value for the cell containing the corresponding
NADP site in 2001: (a) wet sulfate for January 16â€“22, (b) wet sulfate for May 22â€“28, (c) wet nitrate for
January 16â€“22, (d) wet nitrate for May 22â€“28.
of the Figures 18.5 and 18.7. In the corresponding week similar spatial pat-
terns are seen for the sulfate and nitrate deposition values, as expected. Fig-
ures 18.6 and 18.8 provide the standard deviation maps for the predictions
in Figures 18.5 and 18.7. From these ï¬gures we conclude that higher levels
of deposition values are predicted with higher levels of uncertainty which is
common in this sort of data analysis.
Parameter estimates, model choice analysis, and full spatio-temporal analysis
of all the 52 weekâ€™s dataset with a dynamic version of the foregoing model is
presented in the paper of Sahu et al. (2010), and hence are not repeated here.
They also discuss methods for choosing the decay parameter values Ë†â€°, Ë†b and
Ë†Ã. In addition, they validate the models with set aside data and, by suitable
aggregation, obtain total annual deposition maps along with their uncertainties.
18.5 Further discussion
A related question of interest is to estimate dry deposition which is deï¬ned
as the exchange of gases, aerosols, and particles between the atmosphere and
earthâ€™s surface. Such analysis will enable prediction of total (wet plus dry) sulfur
and nitrogen deposition. Using the total predictive surface it will be possible
to estimate deposition â€˜loadingsâ€™ as the integrated volume of total deposition
over ecological regions of interest. If successful, this effort will lead to the ï¬rst

Assessing Environmental Exposure
499
0.0
0.2
0.4
0.6
0.8
1.0
0.17
0.19
0.21
0.12
0.28
 0
0.14
 0
0.14
 0
 0
0.07
0.21
0.24
0.2
0.65
0.5
0.39
0.27
0.3
0.32
0.31
0.64
0.22
0.04
0.21
0.3
 0
0.27
0.11
 0
 0
0.01
0.3
 0
0.17
0.27
0.26
0.24
0.1
 0
0.05
0.02
0.02
0.02
0.03
 0
0.61
0.29
0.26 0.33
0.21
 0
0.12
0.45
0.12
0.02
0.18
0.1
0.32
0.09
0.3
0.05
0.06
0.67
0.02
0.11
0.41
0.27
0.14
0.02
0.19
0.16
0.32
0.37
0.21
0.07
0.1
0.05
0.15
0.46 0.31
0.0
0.5
1.0
1.5
2.0
2.5
0.49
0.93
0.89
 0
0.04
0.84
0.09
0.01
0.08
0.16
0.41
0.03
0.43
0.09
0.26
0.25
0.34
0.13
0.36
0.4
0.33
0.31
0.03
0.25
0.32
0.4
0.54
0.37
1.8
0.51
0.1
0.13
0.09
0.36
 0
0.27
0.3
0.6
0.69
0.25
0.07
0.2
0.34
0.06
0.12
0.06
0.19
0.33
0.11
 0 1.07
0.85
0.37
0.71
0.74
1.14
0.55
0.58
0.07
1.16
0.38
1.16
0.32
0.53
0.29
0.72
0.55
1.05
0.87
0.94
0.41
0.44
0.13
0.35
0.63
1.13
0.81
0.44
0.37
0.32
0.87 0.83
Fig. 18.5 Model interpolated maps for sulfate deposition for two weeks in 2001: top panel for January
16â€“22 and bottom panel for May 22â€“28. Observed deposition values from some selected sites are
superimposed; the data from the remaining sites are not shown to enhance readability.

500
The Oxford Handbook of Applied Bayesian Analysis
0.0
0.1
0.2
0.3
0.4
0.5
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Fig. 18.6 The standard deviation maps for the predictions in Figure 18.5.

Assessing Environmental Exposure
501
0.0
0.2
0.4
0.6
0.8
0.06
0.1
0.12
0.08
0.16
 0
0.04
 0
0.04
 0
 0
0.04
0.06
0.21
0.15
0.29
0.28
0.26
0.16
0.19
0.2
0.13
0.51
0.12
0.05
0.14
0.1
 0
0.27
0.09
 0
 0
0.06
0.39
 0
0.19
0.2
0.27
0.22
0.22
 0
0.1
0.11
0.03
0.06
0.13
 0
0.43
0.14
0.09 0.11
0.08
 0
0.05
0.28
0.12
0.02
0.12
0.13
0.15
0.09
0.32
0.11
0.15
2.45
0.04
0.13
0.47
0.12
0.09
0.01
0.12
0.09
0.17
0.31
0.21
0.12
0.11
0.13
0.18
0.59 0.57
0.0
0.5
1.0
1.5
0.42
0.76
0.91
 0
0.03
0.55
0.06
0.01
0.06
0.15
0.24
0.05
0.34
0.17
0.32
0.32
0.33
0.12
0.41
0.46
0.28
0.34
0.05
0.27
0.24
0.32
0.44
0.3
1.17
0.51
0.12
0.11
0.06
0.44
 0
0.28
0.27
0.61
0.91
0.16
0.08
0.14
0.16
0.07
0.11
0.05
0.25
0.29
0.11
 0 0.93
0.88
0.32
0.57
0.53
0.64
0.4
0.33
0.06
1.09
0.29
0.62
0.43
0.39
0.32
0.55
0.49
0.54
0.67
0.74
0.27
0.26
0.2
0.32
0.5
0.88
0.86
0.7
0.4
0.32
0.85 0.52
Fig. 18.7 Model interpolated maps for nitrate deposition for two weeks in 2001: top panel for January
16â€“22 and bottom panel for May 22â€“28. Observed deposition values from some selected sites are
superimposed; the data from the remaining sites are not shown to enhance readability.

502
The Oxford Handbook of Applied Bayesian Analysis
0.0
0.05
0.10
0.15
0.20
0.25
0.30
0.0
0.2
0.4
0.6
0.8
Fig. 18.8 The standard deviation maps for the predictions in Figure 18.7.

Assessing Environmental Exposure
503
ever estimation of total deposition loadings, perhaps the most critical quantity
for making ecological assessments. Future work will also address trends in
deposition to assess whether regulation has been successful.
Appendix
A. Broader context and background
Gaussian processes play a key role in modeling for spatial and spatio-temporal
data. By now, there is an extensive literature on formalizing and characterizing
stochastic processes along with analysis their behaviour. However, in a practical
setting, ensuring that a stochastic process has been properly deï¬ned when the
index of the process is over a continuum, say a spatial region, requires care.
The primary issue is to guarantee that the joint distribution associated with
the entire collection of random variables is consistently deï¬ned. The usual
strategy is to deï¬ne the process through its ï¬nite dimensional distributions
and verify that these distributions satisfy a consistency condition. In this regard,
the Gaussian process becomes very attractive since its ï¬nite dimensional
distributions are all multivariate normals. Speciï¬cation over the set D
only requires a mean function, Ã(s), s âˆˆD and a valid covariance function,
C(s, s â€²) = Cov(Y(s), Y(s â€²)), the latter supplying the covariance matrix associated
with any ï¬nite set of locations.
The convenient conditional distribution theory associated with the multi-
variate normal distribution is at the heart of kriging (spatial prediction); the
convenient marginal distributions facilitate local model speciï¬cation. Moreover,
the only ï¬nite dimensional distributions within the class of jointly elliptical
distributions that are able to support a stochastic process over a continuum
are normals (or scale mixtures of normals).
Additionally, spatial dependence is typically introduced into the modeling in
the form of spatial random effects. In general, random effects are modeled as
normal variables so a multivariate normal speciï¬cation for such effects in a spa-
tial setting seems appropriate. In this regard, hierarchical modelling naturally
emerges. The spatial random effects are introduced at the second stage of mod-
elling. They appear in the mean (perhaps on a transformed scale if the ï¬rst stage
speciï¬cation is non-Gaussian as in a spatial generalized linear model such as a
binary process where the observation at any location is a 0 or a 1). In this regard,
there is practical interest in these random effects. Given their prior speciï¬ca-
tion, the associated revised posterior distributions are of interest with regard to
â€˜seeingâ€™ a spatial pattern, again emphasizing the role of Bayesian inference in
spatial analysis. While the genesis of spatial modelling for data over a contin-
uum was based primarily on simple least squares theory, modern, fully model-
based spatial analysis is almost entirely done within a Bayesian framework.

504
The Oxford Handbook of Applied Bayesian Analysis
Speciï¬cation of a valid covariance function is a separate issue. Such a func-
tion must be positive deï¬nite, i.e. for any number of and set of spatial locations,
the resultant covariance matrix must be positive deï¬nite. By now there is a rich
literature regarding the choice of such functions in space, enabling isotropy,
stationarity, and non-stationarity, and in space time, enabling space-time depen-
dence in association. See, e.g. the book of Banerjee et al. (2004) and the recent
paper of Stein (2005) and references therein.
Finally, Bayesian computation for space and space-time data analysis is
much more demanding than usual analysis. Of course, this is true in general,
for ï¬tting hierarchical models but the rewards of full inference will usually
justify the effort. Bayesian software to ï¬t spatial data models includes Win-
bugs (http://www.mrc-bsu.cam.ac.uk/bugs/), and two R-packages Geo-R
(Ribeiro and Diggle, 1999) and and SpBayes (Finley et al., 2008).
B. Distributions for Gibbs sampling
B.1 Prior and posterior distributions
We complete the Bayesian model speciï¬cation by assuming prior distributions
for all the unknown parameters. We assume that, a priori, each of Â·0, Â·1,
â€š0, â€š1, â€š2, b0, â€ž0, â€ž1 is normally distributed with mean 0 and variance 103,
essentially a ï¬‚at prior speciï¬cation. The inverse of the variance components
1
Ã›2
â€°
,
1
Ã›2
b
,
1
Ã›2
Ã
,
1
Ã›2
Ã‚
,
1
Ã›2
Â¯
,
1
Ã›2
v
, and
1
Ã›2
ÃŠ
, are all assumed to follow the Gamma dis-
tribution G(ÃŒ, ÃŽ) having mean ÃŒ/ÃŽ. In our implementation we take ÃŒ = 2 and
ÃŽ = 1 implying that these variance components have prior mean 1 and inï¬nite
variance.
The log of the likelihood times prior in the second stage speciï¬cation up to
an additive constant is given by:
âˆ’n
2 log

Ã›2
Ã‚

âˆ’1
2Ã›2
Ã‚
(y âˆ’Ë‡)â€²(y âˆ’Ë‡) âˆ’n
2 log

Ã›2
Ã

âˆ’1
2Ã›2
Ã
Ãâ€²Sâˆ’1
Ã Ã
âˆ’n
2 log

Ã›2
â€°

âˆ’1
2Ã›2
â€°
(u âˆ’Â·0 âˆ’Â·1v)â€²Sâˆ’1
â€° (u âˆ’Â·0 âˆ’Â·1v)
âˆ’n
2 log

Ã›2
v

âˆ’1
2Ã›2
v
(v âˆ’Ëœv(1))â€²(v âˆ’Ëœv(1))â€²
âˆ’J
2 log

Ã›2
Â¯

âˆ’
1
2Ã›2
Â¯
(x âˆ’â€ž0 âˆ’â€ž1 Ëœv)â€² (x âˆ’â€ž0 âˆ’â€ž1 Ëœv)
âˆ’J
2 log

Ã›2
ÃŠ

âˆ’1
2 Ëœvâ€²Dâˆ’1(I âˆ’H)Ëœv
âˆ’n
2 log

Ã›2
b

âˆ’1
2Ã›2
b
bâ€²Sâˆ’1
b b + log(p(Ã‹))
where p(Ã‹) is the prior distribution of Ã‹ and â€° = Ã›2
â€°Sâ€°, b = Ã›2
bSb, Ã = Ã›2
ÃSÃ.

Assessing Environmental Exposure
505
B.2 Handling of the missing values
Note that the transformation equation (18.8) does not deï¬ne a unique value
of Y(si) and in addition, there will be missing values corresponding to the
missing values in Z(si). Any missing value of Y(si) is sampled from the
model (18.12).
The sampling of the missing U(si) for the precipitation process is a bit
more involved. The sampling of the missing values must be done using the
model (18.11) conditional on all the parameters. Since this model is a spatial
model we must use the conditional distribution of U(si) given all the U(s j)
values for j = 1, . . . , n and j =/ i. This conditional distribution is obtained using
the covariance matrix â€° of â€° and is omitted for brevity.
Similarly, equation (18.9) does not deï¬ne unique values of X(Aj) when
Q(Aj) = 0. Those values, denoted by Xâˆ—(Aj), are sampled using the model
equation (18.13), Xâˆ—(Aj) is sampled from N(â€ž0 + â€ž1 Ëœv(Aj), Ã›2
Â¯).
B.3 Conditional posterior distribution of Ã‹
Straightforward calculation yields the following full conditional distributions:
1
Ã›2
Ã‚
âˆ¼G
n
2 + ÃŒ, ÃŽ + 1
2(y âˆ’Ë‡)â€²(y âˆ’Ë‡)

,
1
Ã›2
b
âˆ¼G
n
2 + ÃŒ, ÃŽ + 1
2bâ€²Sâˆ’1
b b

,
1
Ã›2
Ã
âˆ¼G
n
2 + ÃŒ, ÃŽ + 1
2Ãâ€²Sâˆ’1
Ã Ã

,
1
Ã›2
â€°
âˆ¼G
n
2 + ÃŒ, ÃŽ + 1
2(u âˆ’Â·0 âˆ’Â·1v)â€²Sâˆ’1
â€° (u âˆ’Â·0 âˆ’Â·1v)

,
1
Ã›2
Â¯
âˆ¼G
 J
2 + ÃŒ, ÃŽ + 1
2 (x âˆ’â€ž0 âˆ’â€ž1 Ëœv)â€² (x âˆ’â€ž0 âˆ’â€ž1 Ëœv)

,
1
Ã›2
v
âˆ¼G
n
2 + ÃŒ, ÃŽ + 1
2(v âˆ’Ëœv(1))â€²(v âˆ’Ëœv(1))

,
1
Ã›2
ÃŠ
âˆ¼G
âŽ›
âŽJ
2 + ÃŒ, ÃŽ + 1
2
J

j=1

m j( ËœV(Aj) âˆ’Ã j)2
âŽž
âŽ 
where Ã j = J
i=1 h ji ËœV(Ai).
Let â€š = (â€š0, â€š1, â€š2) and G = (1, u, v) so that G is an n Ã— 3 matrix. The full
conditional distribution of â€š is N(Ëœ, ) where
âˆ’1 = 1
Ã›2
Ã‚
Gâ€²G + 10âˆ’3I3,
Ëœ = 1
Ã›2
Ã‚
Gâ€²(y âˆ’b0x âˆ’Xmb âˆ’Ã).

506
The Oxford Handbook of Applied Bayesian Analysis
The full conditional distribution of b0 is N(Ëœ, ) where
âˆ’1 = 1
Ã›2
Ã‚
xâ€²x + 10âˆ’3,
Ëœ = 1
Ã›2
Ã‚
xâ€²(y âˆ’â€š0 âˆ’â€š1u âˆ’â€š2v âˆ’Xmb âˆ’Ã).
The full conditional distribution of b is N(Ëœ, ) where
âˆ’1 = 1
Ã›2
Ã‚
Xâ€²X + âˆ’1
b ,
Ëœ = 1
Ã›2
Ã‚
Xâ€²(y âˆ’â€š0 âˆ’â€š1u âˆ’â€š2v âˆ’b0x âˆ’Ã).
The full conditional distribution of Ã is N(Ëœ, ) where
âˆ’1 = In
Ã›2
Ã‚
+ âˆ’1
Ã ,
Ëœ = 1
Ã›2
Ã‚
(y âˆ’â€š0 âˆ’â€š1u âˆ’â€š2v âˆ’b0x âˆ’Xmb).
Let G = (1, v) so that now G is a n Ã— 2 matrix. The full conditional distribution
of Â· = (Â·0, Â·1) is N(Ëœ, ) where
âˆ’1 = Gâ€²âˆ’1
â€° G + 10âˆ’3I2, Ëœ = Gâ€²âˆ’1
â€° u.
Let G = (1, Ëœv) so that now G is a J Ã— 2 matrix. The full conditional distribution
of â€ž = (â€ž0, â€ž1) is N(Ëœ, ) where
âˆ’1 = 1
Ã›2
Â¯
Gâ€²G + 10âˆ’3I2,
Ëœ = Gâ€²x.
B.4 Conditional posterior distribution of V
Note that due to the missing and zero precipitation values the full conditional
distribution of V will be in a restricted space. First, the unrestricted full condi-
tional distribution of V is N(Ëœ, ) where
âˆ’1 = â€š2
2
In
Ã›2
Ã‚
+ Â·2
1âˆ’1
â€°
+ In
Ã›2
v
,
and
Ëœ = â€š2
Ã›2
Ã‚
a + Â·1âˆ’1
â€° (u âˆ’Â·0) + 1
Ã›2
v
Ëœv(1),
where a = y âˆ’â€š0 âˆ’â€š1u âˆ’b0x âˆ’Xmb âˆ’Ã. From this n-dimensional joint dis-
tribution we obtain the conditional distribution V(si) âˆ¼N(Ãi, i), say. If the
precipitation value, r(si), is missing then there will be no constraint on V(si) and
we sample V(si) unrestricted from N(Ãi, i). If on the other hand the observed
precipitation value is zero, r(si) = 0, we must sample V(si) to be negative, i.e. we
sample from N(Ãi, i)I(V(si) < 0). Corresponding to non-zero precipitation
value r(si) > 0 we sample V(si) from N(Ãi, i)I(V(si) > 0).

Assessing Environmental Exposure
507
B.5 Conditional posterior distribution of ËœV
The full conditional distribution of ËœV = ( ËœV(1), ËœV(2)) is N(Ëœ, ) where
âˆ’1 =
 In
Ã›2
v
0
0
0

+ â€ž2
1
IJ
Ã›2
Â¯
+ Dâˆ’1(I âˆ’H),
Ëœ =
 1
Ã›2
v
v
0

+ â€ž1
Ã›2
Â¯
(x âˆ’â€ž0).
Note that this full conditional distribution is a J -variate normal distribution
where J is possibly very high (33,390 in our example) and simultaneous update
is computationally prohibitive. In addition, we need to incorporate the con-
straints implied by the ï¬rst stage likelihood speciï¬cation (18.10).
The partitioning of ËœV, however, suggests an immediate univariate sampling
scheme detailed below. First, note that the conditional prior distribution for
ËœV(Aj) from the vectorized speciï¬cation (18.16), as calculated above is given by
N(Ã“ j, Ë˜2
j) where:
Ë˜2
j = Ã›2
ÃŠ
1
mj
and
Ã“ j =
J

i=1
h ji Ëœv(Ai).
Now for each component ËœV(Aj) of ËœV(1) we extract the full conditional distri-
bution to be viewed as the likelihood contribution from the joint distribution
N((1)Ëœ(1), (1)) where
âˆ’1
(1) = In
Ã›2
v
+ â€ž2
1
In
Ã›2
Â¯
and
Ëœ(1) = 1
Ã›2
v
v + â€ž1
Ã›2
Â¯
(x(1) âˆ’â€ž0),
where x = (x(1), x(2)), partitioned analogusly to ËœV. This conditional likelihood
contribution is given by N(Ã j, 2) where
Ã j = 2 
Ëœv(Aj)/Ã›2
v + â€ž1(x(Aj) âˆ’â€ž0

/Ã›2
Â¯,
2 = 1/

1/Ã›2
v + 1/Ã›2
Â¯

.
The conditional likelihood contribution for each component of ËœV(2) is the nor-
mal distribution N(Ã j, 2) where
Ã j = x(Aj) âˆ’â€ž0
â€ž1
and
2 =
Ã›2
Â¯
â€ž2
1
.
Now the unconstrained full conditional distribution of ËœV(Aj), according to
the second stage likelihood and prior speciï¬cation, is obtained by combining
the likelihood contribution N(Ã j, 2) and the prior conditional distribution
N(Ã“ j, Ë˜2
j) and is given by N( jËœ j,  j) where
âˆ’1
j
= âˆ’2 + Ë˜âˆ’2
j , Ëœ j = âˆ’2Ã j + Ë˜âˆ’2
j Ã“ j.

508
The Oxford Handbook of Applied Bayesian Analysis
In order to respect the constraints implied by the ï¬rst stage speciï¬cation we
simulate the ËœV(Aj) to be positive if x(Aj) > 0 and negative otherwise.
Acknowledgment
The authors thank David Holland, Gary Lear and Norm Possiel of the US EPA
for many helpful comments and also for providing the monitoring data and
CMAQ model output used in this chapter.
References
Banerjee, S., Carlin, B. P. and Gelfand, A. E. (2004). Hierarchical Modeling and Analysis for
Spatial Data. Chapman & Hall/CRC, Bora Raton, Florida.
Bergthorsson, P. and DÃ¶Ã¶s, B. (1955). Numerical weather map analysis, Tellus, 7, 329â€“340.
Berrocal, V. J., Gelfand, A. E. and Holland, D. M. (2010). A spatio-temporal downscaler for
output from numerical models. Journal of Agricultural, Biological and Environmental Statistics,
in press.
Bratseth, A. M. (1986). Statistical interpolation by means of successive corrections. Tellus, 38A,
439â€“447.
Brook, J. R., Samson, P. J. and Sillman, S. (1995). Aggregation of selected three-day periods to
estimate annual and seasonal wet deposition totals for sulfate, nitrate, and acidity. Part I: A
synoptic and chemical climatology for eastern North America. Journal of Applied Meteorology,
34, 297â€“325.
Brown, P. J., Le, N. D. and Zidek, J. V. (1994). Multivariate spatial interpolation and exposure
to air pollutants. The Canadian Journal of Statistics, 22, 489â€“510.
Burke, J. M., Vedantham, R., McCurdy, T. R., Xue, J. and Ozkaynak. A. H. (2003). SHEDS-PM:
A population exposure model for predicting distributions of PM exposure and dose from
both outdoor and indoor sources. Presented at International Society of Exposure Analysis,
Stresa, Italy, September 21â€“25, 2003.
Carroll, R. J., Chen, R., George, E. I., Li, T. H., Newton, H. J., Schmiediche, H. and Wang, N.
(1997). Ozone exposure and population density in Harris County. Texas, Journal of the
American Statistical Association, 92, 392â€“404.
Charney, J. G. (1951). Dynamic forecasting by numerical process, Compendium of Meteorology,
American Meteorological Society, Boston, MA.
Cox, D. D., Park, J. S. and Singer C. E. (2001). A statistical method for tuning a computer code
to a data base. Computational Statistics and Data Analysis, 37, 77â€“92.
Cressie, N., Kaiser, M. S., Daniels, M. J., Aldworth, J., Lee, J., Lahiri, S. N. and Cox, L. (1999).
Spatial analysis of particulate matter in an urban environment. In GeoEnv II: Geostatistics
for Environmental Applications, (ed. J. Gmez-Hernndez, A. Soares, R. Froidevaux). pp. 41â€“52.
Kluwer, Dordrecht.
Cressman, G. P. (1959). An operational objective analysis system, Monthly Weather Review, 87,
367â€“374.
Finley, A. O., Banerjee, S. and Carlin, B. P. (2008). Univariate and multivariate spa-
tial modeling. Technical Report, Department of Bio-statistics, University of Minnesota,
http://blue.for.msu.edu/software.

Assessing Environmental Exposure
509
Fuentes, M., Guttorp, P. and Challenor, P. (2003). Statistical assessment of numerical models.
International Statistical Review, 71. 201â€“221.
Fuentes, M. and Raftery, A. (2005). Model evaluation and spatial interpolation by Bayesian
combination of observations with outputs from numerical models. Biometrics, 61,
36â€“45.
Guttorp, P., Meiring, W. and Sampson, P. D. (1994). A space-time analysis of ground-level
ozone data. Environmetrics, 5, 241â€“254.
Haas, T. C. (1995). Local prediction of a spatio-temporal process with an application to wet
sulfate deposition. Journal of the American Statistical Association, 90, 1189â€“1199.
Kalnay, E. (2003). Atmospheric Modeling, Data Assimilation and Predictability. Cambridge
University Press, Cambridge.
Kennedy, M. C. and Oâ€™Hagan, A. (2001). Bayesian calibration of computer models (with discus-
sion). Journal of the Royal Statistical Society, Series B, 63, 425â€“464.
Kibria, B. M. G., Sun, L., Zidek, J. V. and Le, N. D. (2002). Bayesian spatial prediction of random
space-time ï¬elds with application to mapping PM2.5 exposure. Journal of the American
Statistical Association, 97, 112â€“124.
Le, N. D., Sun, W. and Zidek, J. V. (1997). Bayesian multivariate spatial interpolation with data
missing by design. Journal of the Royal Statistical Society, Series B, 59, 501â€“510.
McMillan, N., Holland, D., Morara, M. and Feng, J. (2008). Combining numerical
model output and particulate data using Bayesian space-time modeling. Environmetrics,
DOI:10.1002/env.984.
Poole, D. and Raftery, A. E. (2000). Inference for deterministic simulation models: The
Bayesian melding approach. Journal of the American Statistical Association, 95, 1244â€“1255.
Rappold, A. G., Gelfand, A. E. and Holland, D. M. (2008). Modeling mercury deposition
through latent space-time processes. Journal of the Royal Statistical Society, Series C, 57,
187â€“205.
Ribeoro Jr, P. J. and Diggle, P. J. (1999). geoS: A geostatistical library for S-PLUS.
Technical report ST-99â€“09. Dept of Mathematics and Statistics, Lancaster University,
http://www.leg.ufpr.br/geoR/.
Richmond, H. M., Palma, T., Langstaff, J., McCurdy, T., Glenn, G. and Smith L. (2002).
Further reï¬nements and testing of APEX (3.0): EPAâ€™s population exposure model for criteria
and air toxic inhalation exposures. Joint Meeting of the Society of Exposure Analysis and
International Society of Environmental Epidemiology, Vancouver, Canada.
Sahu, S. K. and Mardia, K. V. (2005). A Bayesian kriged-Kalman model for short-term forecast-
ing of air pollution levels. Journal of the Royal Statistical Society, Series C, 54, 223â€“244.
Sahu, S. K., Gelfand, A. E. and Holland, D. M. (2006). Spatio-temporal modeling of
ï¬ne particulate matter. Journal of Agricultural, Biological and Environmental Statistics, 11,
61â€“86.
Sahu, S. K., Gelfand, A. E. and Holland, D. M. (2007). High resolution space-time
ozone modeling for assessing trends. Journal of the American Statistical Association, 102,
1221â€“1234.
Sahu, S. K., Gelfand, A. E. and Holland, D. M. (2010). Fusing point and areal level space-time
data with application to wet deposition. Journal of the Royal Statistical Society, Series C, 59,
77â€“103.
Sahu,
S.
K.,
Yip,
S.
and
Holland,
D.
M.
(2009).
Improved
space-time
forecast-
ing of next day ozone concentrations in the eastern US. Atmospheric Environment,
doi:10.1016/j.atmosenv.2008.10.028.
Shaddick, G. and Wakeï¬eld, J. (2002). Modelling daily multivariate pollutant data at multiple
sites. Journal of the Royal Statistical Society, Series C, 51, 351â€“372.

510
The Oxford Handbook of Applied Bayesian Analysis
Smith, R. L., Kolenikov, S. and Cox, L. H. (2003). Spatio-temporal modelling of PM2.5
data with missing values. Journal of Geophysical Research-Atmospheres, 108, D249004,
doi:10.1029/2002JD002914.
Stein, M. L. (2005). Space-time covariance functions. Journal of the American Statistical Associa-
tion 100, 310â€“321.
Sun, L., Zidek, J. V., Le, N. D. and Ozkaynak, H. (2000). Interpolating Vancouverâ€™s daily ambient
PM10 ï¬eld. Environmetrics, 11, 651â€“663.
Wikle, C. K. (2003). Hierarchical models in environmental science. International Statistical
Review. 71, 181â€“199.
Zidek, J. V., Shaddick, G., White, R., Meloche, J. and Chatï¬eld, C. (2005). Using a probabilistic
model (pCNEM) to estimate personal exposure to air pollution context sensitive links.
Environmetrics, 16, 481â€“493.
Zidek, J. V., Shaddick, G., Meloche, J., Chatï¬eld, C. and White, R. (2007). A framework
for predicting personal exposures to environmental hazards. Environmental and Ecological
Statistics, 14, 411â€“431.
Zidek, J. V., Sun, L., Le, N. and Ozkaynak, H. (2002). Contending with space-time interaction
in the spatial prediction of pollution: Vancouverâ€™s hourly ambient PM10 ï¬eld. Environmetrics,
13, 595â€“613.

Â·19Â·
Indirect elicitation from ecological experts:
From methods and software to habitat
modelling and rock-wallabies
Samantha Low Choy, Justine Murray, Allan James and Kerrie Mengersen
19.1 Introduction
This work was prompted by a need to model habitat requirements for a
threatened Australian species, the brush-tailed rock-wallaby, Petrogale penicil-
lata. Modelling wildlife habitat requirements is important for mapping their
distribution and therefore informing conservation and management (Guisan
and Zimmermann, 2000). For rare and threatened species, it is often difï¬cult
to obtain sufï¬cient data coverage over the large spatial and temporal scales
required, especially during the early phases of investigation. Moreover there
has been considerable debate on appropriate modelling approaches (Austin,
2002; Miller and Franklin, 2002; Guisan and Thuiller, 2005; Elith et al., 2006).
This partly explains why expert-deï¬ned habitat models are often promoted as
best practice in comparison to their statistical data-driven counterparts (e.g.
Langhammer et al., 2007).
The Bayesian statistical modelling framework provides a useful â€˜bridgeâ€™,
from purely expert-deï¬ned models, to statistical models allowing survey data
and expert knowledge to be â€˜viewed as complementary, rather than alternative
or competing, information sourcesâ€™ (Ferrier et al., 2002). Eliciting an expert-
deï¬ned prior also clariï¬es existing knowledge at the outset of modelling
(Spiegelhalter et al., 2004).
As summarized in Table 19.1 and discussed in Section A.1.1, quantifying
expert opinions for input into a Bayesian statistical framework can be robustly
achieved by following statistical design principles: formulate the statistical
model, appropriately target and encode expert knowledge, and design an accu-
rate and repeatable elicitation protocol. In particular the desire to achieve
elicitation in a transparent, repeatable and robust manner suggests judicious
use of technology. Despite the acknowledged beneï¬ts of technology during
elicitation, there is currently a dearth of elicitation tools available for general
application (Leal et al., 2007), with many tools developed for speciï¬c problems
(Kadane et al., 1980; Kadane and Wolfson, 1998; Denham and Mengersen, 2007;

512
The Oxford Handbook of Applied Bayesian Analysis
Table 19.1 A six-component framework for designing elicitation, within a broader
framework for implementing a full informative Bayesian analysis (Low Choy et al.,
2009).
E1.
Determine motivation and purpose for using prior information.
E2.
Specify available prior knowledge from experts or other sources, to deï¬ne an
appropriate and achievable goal of elicitation.
E3.
Formulate a statistical model representing the ecological conceptual model. Deï¬ne
the likelihood p(y|Ã) characterizing the data model and the prior p(Ã) reï¬‚ecting
available prior knowledge.
E4.
Design numerical encoding (measurement technique) for effective elicitation of prior
information and representation as a statistical distribution.
E5.
Manage uncertainty for accurate and robust elicitation.
E6.
Design an elicitation protocol to manage logistics of implementing elicitation.
Du Mouchel, 1988; Chaloner et al., 1993; Oâ€™Hagan, 1997). Nevertheless some
exceptions are available (Kynn, 2006; Al-Awadhi and Garthwaite, 2006; Leal
et al., 2007).
This chapter has two main aims: to showcase a rigorously designed and
implemented expert elicitation for multiple experts; and to describe the use
of software to streamline, automate and facilitate an indirect approach to
elicitation. For exposition, the elicitation components E1â€“E6 in Table 19.1
will be used throughout to indicate and consolidate the proposed elicitation
framework. Section 19.2 introduces the ecological problem that motivates this
work: modelling habitat preferences of a relatively rare rock-wallaby (Murray
et al., 2008b). Logistic regression is a popular approach for habitat modelling
(Guisan and Zimmermann, 2000), and provides a basis for use of the Bayesian
framework for elicitation and inference, as outlined in Section 19.3. We propose
an extension (Section 19.3.2.2) to the conditional means approach of Bedrick
et al. (1996), that addresses the practical situation where the number of elici-
tations exceeds the number of covariates. This new approach has substantial
beneï¬ts when providing feedback to experts (Sections 19.4,19.5.1). Section 19.4
describes a new software tool Elicitator that can be harnessed to assist elicitation
for regression within this broader framework of designed elicitation. Results
from eliciting expert opinion about preferred habitat of the rock wallaby (Mur-
ray et al., 2008a) are presented in Section 19.5. We conclude with a discussion in
Section 19.6.
19.2 Ecological application: Modelling and mapping habitat of a
rock-wallaby
Ecological models of habitat requirements or environmental responses of indi-
vidual species typically rely on just a few expert opinions (e.g. Kynn, 2005;

Indirect Elicitation from Ecological Experts
513
Martin et al., 2005; Denham and Mengersen, 2007; Oâ€™Leary et al., 2008b, 2009).
For this study, however, we had access to a substantial number of experts.
This could be attributed to several factors: the iconic nature of the species;
its threatened status both in New South Wales (NSW) and nationwide; the
subsequent interest from conservation managers, community groups and sci-
entists; the proï¬le and experience of the research team as well as funding
attracted.
Experts for the study on Petrogale penicillata were determined by experience
in ï¬eld observation targeting this species (E6). Field data had recently been
collated across two regions but had not yet been reported. Preliminary analysis
revealed that rock wallabies used the terrain differently in each region (E6). The
ï¬rst region Q predominantly comprised volcanic rocky areas, where wallabies
had often been sighted taking cover amongst rocky outcrops. To avoid predators,
wallabies sought the less accessible areas with steeper slopes. The second region
N comprised gorge country, therefore of lower elevation, but also containing
steep slopes. In these areas wallabies were often observed to take cover amongst
vine thicket vegetation as well as rocky outcrops. Hence a comparable number
of experts from each region were sought, with ï¬ve from region N and four from
region Q, agreeing to participate (Murray et al., 2008a). Expert opinions were
considered relatively independent, due to diversity in training, work history as
well as rock wallaby expertise (E5).
The elicitor on the project was acknowledged as having the most relevant,
comprehensive and up-to-date knowledge of the speciesâ€™ habitat requirements.
However, this experience was derived from recent ï¬eld studies (Murray et al.,
2008b) which comprised the observed data intended for combining with expert
opinion within the Bayesian statistical model. This disqualiï¬ed the elicitor from
actually providing elicited responses (E6). Nevertheless, there were numerous
other advantages. Being the acknowledged â€˜bestâ€™ expert on the species lent sub-
stantial credibility to elicitation and subsequent modelling, attracting involve-
ment from most other experts (E6). Moreover this expert could be interpreted
as the â€˜supra-Bayesianâ€™ (Genest and Zidek, 1986) and therefore able to assess
and mediate contributions from others (E5).
Most experts worked within the conservation management sector, and were
therefore located remotely, relatively close to known rock wallaby colonies. This
geographic dispersion, together with funding constraints, meant that elicitation
could not employ popular expert consultation approaches that rely on all experts
being in the same place at the same time (for instance as promoted by Oâ€™Hagan
et al., 2006). This led to an imperative for an easily standardised and portable
approach to elicitation.
Available ï¬eld data recorded observed presence/absence at sites across the
two regions (E1). As detailed in Murray et al. (2008b), the sampling design
comprised a combination of existing sites supplemented by stratiï¬ed random

514
The Oxford Handbook of Applied Bayesian Analysis
sampling to target gaps in existing surveys, with strata based on landscape-
scale factors for remnant vegetation, landcover and geology. Gradsect sampling
(Austin and Heyligers, 1989) was used to ensure coverage of ï¬ner scale vari-
ation with respect to elevation, slope and aspect. Landscape scale predictors
were sourced from Geographic Information Systems (GIS) map layers (E3).
The spatial datasets for geology and remnant vegetation were based on expert
delineation of boundaries. Landcover was derived from remote sensing imagery
with semi-automated interpretation and ground-truthing based on expert inter-
pretation at ï¬eld sites. Slope, elevation and aspect were calculated via standard
spatial analysis from digital elevation models for topography. Transforming
aspect to its cosine provided a measure of â€˜northerlinessâ€™ that accounted for
the discontinuity between 0â—¦and 360â—¦, of substantial importance for a species
believed a priori to prefer northerly aspects (Murray et al., 2008b).
19.3 Elicitation for regression
The way in which expert knowledge in incorporated as prior information in a
Bayesian framework depends on the choice of a speciï¬c model. From a practical
point of view, logistic regression is a popular choice for habitat modelling for
both explanatory and predictive purposes (Guisan and Zimmermann, 2000;
Miller and Franklin, 2002). More theoretically, logistic regression is a special
case of a generalized linear model (GLM); since two of the encoding methods
described below provide the same prior distribution (conditional means and
data augmentation), this model is useful for exposition.
We consider presence/absence ï¬eld data Yi recorded at independent sites
i = 1 : n, following a Bern(Ãi) sampling model. The linear predictor Ã weights
covariates xi by regression coefï¬cients â€š via Ã = J
j=1 â€š j xi j. Denote by Ã the
mean response, here probability of presence, given covariates Ãi = E[yi|xi]. For
simplicity we focus on logit link function g(Ã) = Ã such that g(Ãi) = log(Ãi/
(1 âˆ’Ãi)). The inverse link function is the logistic distribution function,
g âˆ’1(Ã) = F(Ã) = (exp Ã)/(1 + exp Ã) with corresponding probability density f (Ã)
= F(Ã) (1 âˆ’F(Ã)). The likelihood for â€š can therefore be written (Bedrick et al.,
1996) as
p(y|â€š) =
n

i=1
F

xT
i â€š
yi 
1 âˆ’F

xT
i â€š
1âˆ’yi .
(19.1)
Within a Bayesian framework, prior distributions are placed on the unknown
parameters â€š. Typically a multivariate normal prior is applied â€š|Ë† âˆ¼NJ (b, ),
with  a symmetric J Ã— J matrix with diagonal elements being the variances
Ã›2
j and off-diagonal elements for the covariances Ã› j j â€². A weakly informative prior

Indirect Elicitation from Ecological Experts
515
for â€š will assign vague distributions favouring neither positive nor negative
effects (see for example Spiegelhalter et al., 2003; Robert, 2001; Box and Tiao,
1982). However for an informative Bayesian analysis, prior information can be
included but requires careful speciï¬cation of b and .
19.3.1 Indirect elicitation approaches for regression
An important decision in designing an elicitation method is the choice of a
direct or indirect approach (E4). Direct elicitation of regression coefï¬cients
from ecological experts is possible for simple regressions with one or two
covariates (Appendix, Table 19.5). In this case, however, direct elicitation would
pose a substantial cognitive challenge for experts since there are multiple habi-
tat covariates, as discussed in Appendix A.2. Instead we consider an indirect
method, particularly useful in ecology (Low Choy et al., 2009), since experts
are asked about concrete observable quantities, such as probability of presence,
rather than abstract concepts, such as regression coefï¬cients. See Appendix A.2
for further discussion of direct and indirect elicitation approaches.
A useful decomposition of indirect elicitation methods separates elicita-
tion into two components. The ï¬rst fundamental component elicits expert
knowledge together with uncertainty. Standard methods for eliciting distri-
butions are available (Appendix, Table 19.4). The second component com-
prises elicitation tasks, more unique to the particular indirect method, that
take these distributions and encode them into an informative prior for the
model. In this section we brieï¬‚y evaluate options, as outlined in Appendix,
Table 19.6.
The posterior predictive approach (Kadane et al., 1980; Kadane and Wolfson,
1998; Denham and Mengersen, 2007) is attractive since it asks experts to predict
response at a future hypothetical site, given data observed at previous sites. This
approach assumes that experts have already been exposed to observational data,
and the posterior predictive distribution is tractable. In this case experts were
not exposed to the ï¬eld data. In addition the effort required to encode the scale
parameter  can be prohibitive, both for the expert and the modeller (Kadane
et al., 1980).
Another option is data augmentation, whereby experts describe a range of
possible responses and assign these a prior â€˜weightâ€™ or effective sample size.
These approaches have been promoted for log-linear regression in epidemio-
logical contexts (e.g. Greenland, 2006), where priors arise naturally from speci-
fying example cross-tabulations. For logistic regression with multiple covariates
this approach was considered too demanding, particularly in this study where
several ecological experts had limited statistical experience and where variable
selection was still uncertain.

516
The Oxford Handbook of Applied Bayesian Analysis
A related approach presented by Chen et al. (1999) requires construction of a
full initial dataset {Zk}, from preliminary data, for speciï¬c values of covariates
Xk=1:K . Application of Bayes theorem, with non-informative prior p(â€š), provides
an initial posterior:
p(â€š|X1:K , Z) âˆp(Z|â€š, X1:K )p(â€š).
(19.2)
This initial posterior provides a prior estimate for the next cycle of Bayesian
learning, when combined with observed data Y elicited at potentially different
covariate values Xi=1:N with
p(â€š|Xk=1:K,i=1:N, Y, Z) âˆp(Y|â€š, X1:N)p(â€š|Z, X1:K).
(19.3)
A key assumption is that the same regression coefï¬cients â€š relate prior â€˜dataâ€™
Z to covariates X in the initial cycle, and observed data Y to covariates X.
We may apply this approach in our context, using elicited â€˜dataâ€™ as preliminary
data Zk=1:K , and using a Beta sampling distribution for Z, to enable capture of
expert uncertainty, but requiring careful interpretation as the expertâ€™s estimate
for the probability of presence. Under the Bayesian paradigm, we assume the
expert â€˜dataâ€™ or opinions are ï¬xed, corresponding to a range of potential expert
models represented by â€š. The prior p(â€š|Z) may instead be deï¬ned using a
point estimate and standard errors obtained from a regression of Z on X.
This assumes that the expert has an underlying conceptual model based on
some unknown true value of â€š, with sampling variation affecting their stated
opinions Z.
Alternatively experts may be asked to describe the overall or aggregate
relationship between the response and covariates (e.g. Chaloner et al., 1993).
Graphically this may involve drawing a response curve of y against covariate xj
(e.g. Willems et al., 2005). This can be difï¬cult in high-dimensional covariate
spaces. To simplify the process it is possible to take a conditional approach,
based on response curves, by eliciting the relationship between the response
and each covariate separately, conditional on all other covariates held ï¬xed,
at say their optimum values (Kynn, 2006) or some other reference values (Al-
Awadhi and Garthwaite, 2006). Software is already available for the latter curve
conditional mean approach, but may appeal to scientists with a more theoretical
understanding of the topic, e.g. physiological conceptual understanding that
relates biological requirements to species response (Denham and Mengersen,
2007; Oâ€™Leary et al., 2008b).
The similar case conditional mean prior (CMP) approach of Bedrick et al.
(1996) asks experts to assess probability of presence Ãk at a number of
sites conditioning on the full list of covariate values Xk. For example, in
habitat modelling, ï¬eld-based ecologists often ï¬nd it easier to conceptual-
ize the probability of presence for known environmental attributes (Denham
and Mengersen, 2007). In our study most experts had a predominately

Indirect Elicitation from Ecological Experts
517
ï¬eld-based background, so we also consider the case CMP approach of
Bedrick et al. (1996).
19.3.2 Case conditional mean elicitation for regression
19.3.2.1 Case CMP without elicitation error
When the case CMP elicitation approach was ï¬rst formulated in a general
context (Bedrick et al., 1996), an exact computational approach to encoding
was proposed, based on eliciting minimal information to solve the regression
equations exactly. This exact solution was constrained by an assumption that
the number of covariates equals the number of elicitations J = K . Following
Bedrick et al. (1996), the case-speciï¬c conditional mean prior relies on eliciting a
Beta prior for Ãk = E[yk|xk] for cases k = 1 : K , to induce a prior on â€š. Estimation
of the induced prior on â€š assumed K = J , to ensure existence of Xâˆ’1 and
that dimensionality was maintained on changing variable (from Ãk to â€š). In
the â€˜under-speciï¬edâ€™ case, this constraint was achieved by conditioning on zero
values for excess covariates; in the â€˜over-speciï¬edâ€™ case, additional latent vari-
ables were introduced to match the number of elicitations. This deterministic
approach assumes that each expert directly assesses the probability of presence
at each site Ãk, and that no adjustment is required to ensure coherence of the
underlying trends across sites.
In the following section we take issue with Bedrick et al.â€™s (1996) assertion
that the case K > J is â€˜unlikelyâ€™, and argue that in many practical situations,
more elicitations than covariates are required to adequately cover covariate
space and manage variation. For example in a medical context Landrum and
Normand (1999) elicit ratings for 890 covariate sets, whereas covariates num-
bered less than 100. For this study (Murray et al., 2008a) 20 elicitations are
obtained on a problem with eight variables. Thus we take a stochastic rather
than deterministic approach, positing that elicitations are likely to involve some
elicitation or measurement error, which can be â€˜averagedâ€™ out across sites. This
is more practical for elicitation compared to a more parameter-rich approach
that explicitly adjusts for bias and precision in estimates at each site (in the
spirit of Lindley, 1983).
A logistic regression ought to provide a good approximation to the expertâ€™s
implicit underlying trends in species response: ecological experts are quite
familiar with species response curves and their interpretation. However this
model, like any other, is only an approximation to the expertâ€™s knowledge. The
beneï¬t of this case-conditional mean prior approach is that the elicited informa-
tion does not presume the underlying model is logistic regression. For instance
other models could be used on the same elicited information, such as binomial
or beta regression with a different link function (probit or complementary log-
log), or a regression tree (Oâ€™Leary et al., 2008a). Indeed just as much effort could

518
The Oxford Handbook of Applied Bayesian Analysis
be expended to â€˜revealâ€™ the model that best ï¬ts the elicited expert opinions,
rather than assuming a particular model form. At the moment, the software
Elicitator only ï¬ts one possible model to the expertâ€™s opinion, although more
ï¬‚exibility is intended for future extensions.
19.3.2.2 Case CMP with elicitation error
Denote by Zk the expertâ€™s opinion on the conditional mean, Ãk. This forms
the basis of a Beta regression (Branscum et al., 2007) where shape and scale
parameters ak and bk can be reparameterized in terms of effective prior sample
size â€žk and mean Ãk:
Zk âˆ¼Be(ak, bk),
Ãk â‰¡ak/â€žk,
â€žk = ak + bk,
logit(Ãk) = XT
k â€š.
(19.4)
This formulation is equivalent to the expert â€˜dataâ€™ approach inspired by Chen
et al. (1999). It permits the expert to have a different level of uncertainty â€žk
at each elicited site, rather than enforcing uncertainty â€ž to be constant across
all cases (as in Branscum et al., 2007). It is also possible to elicit a rating of
certainty, say on a scale from 0 to 100, about each elicited Zk. Thus an expert
should expect that 90% of their assessments at the 90% level of certainty are
correct. These conï¬dence ratings can be normalized, then used to weight the
regression of the expert data on the covariates, to reï¬‚ect unequal precisions in
elicitation across sites.
The prior distribution (19.4) obtained in this way establishes an interim
result, which is an advantage when collating multiple opinions, or when pro-
viding feedback to the expert. Standard regression diagnostics can be used
to provide feedback on variation across elicitations (elicitation â€˜errorâ€™). This
variation may be dominated by either the expertâ€™s epistemic uncertainty in
their knowledge or aleatory uncertainty reï¬‚ected by their difï¬culty in accurately
expressing this knowledge (Oâ€™Hagan et al., 2006). Alternatively equation (19.4)
can be used within a full Bayesian analysis including the observed data via
equation (19.3). Computations are simpliï¬ed since this prior can be expressed
in the same form as the likelihood (Bedrick et al., 1996). For computational
convenience we may approximate Zk âˆ¼Be(ak, bk) by Zâˆ—
k âˆ¼Bin

Ãk, â€žâˆ—
k

with
â€žâˆ—
k = Ceiling[â€žk] and Zâˆ—
k = â€žâˆ—
k Zk. Any Bayesian or classical statistical software can
be used to ï¬t the full Bayesian model by modifying the observed data to reï¬‚ect
the prior information, through addition of pseudo-observations (Greenland,
2006).
19.3.2.3 Encoding the Beta distribution
Minimally, estimation of the two parameters ak and bk requires elicitation of two
summary statistics about the distribution (Low Choy et al., 2009). The most obvi-
ous starting point is to ask the expert for their â€˜best estimateâ€™ of the probability

Indirect Elicitation from Ecological Experts
519
of presence at a site. Chaloner and Duncan (1983) justify elicitation of the
mode since it identiï¬es the point of largest departure from a uniform prior,
taking advantage of an expertâ€™s tendency to anchor (Tversky and Kahneman,
1981). They argue that the mean is too difï¬cult cognitively, whereas the median
is no more difï¬cult than the mode but offers no simpliï¬cation of arithmetic.
Several summary statistic pairs could be used to encode the Beta distribution
(see Appendix, Table 19.4). Various methods which average over more than
two summary statistics have also been proposed (Chaloner and Duncan, 1983;
Hughes and Madden, 2002; Gavasakar, 1988). As shown by Low Choy et al.
(2008), encoding of skewed distributions, such as the lognormal and the Beta
beneï¬ts strongly from the use of additional quantities.
For these reasons, in this study and in the software described in Section 19.4,
the mode is elicited, together with the upper and lower quartiles, and two
more extreme quantiles (E4). We assume that the usual preparations have been
made: the protocol has been designed carefully (E6); key terms and quantities
to be elicited have been precisely deï¬ned (E2); experts have been conditioned to
be aware of, or trained to reduce, the main types of biases (E5); and experts
have been selected, motivated and quality assessed (E6). The way in which
questions are worded strongly impacts on the response and must be tailored
to the expertise of the respondents. For example, questions in the habitat
modelling context may be worded (E4): â€˜Let us consider one hundred sites with
these habitat characteristics â€“ rainforest, basalt, slope 35â—¦, aspect predominantly
north-northeast, and elevation 450 m above sea level. How many of these sites
do you believe are occupied by the species? Provide the answer you think is
the most likely.â€™ Wording in terms of frequencies often leads to more accurate
elicitation than probabilities (Kynn, 2008).
It is also advisable to precede this question by a more extreme one: â€˜What
are the absolute minimum and maximum possible values for the number of
sites. ...â€™ This provides natural progression to asking the expert to specify the
upper and lower bounds (corresponding to a 95% credible interval) on the
plausible values for the probability of presence. Asking the expert for these
assessments at the outset helps to broaden their thinking initially, and thus
helps to avoid over-conservatism arising from anchoring biases (Kynn, 2008;
Low Choy et al., 2009). Experts are also asked to estimate another interval, such
as the 50% credible interval (CrI), such that the probability of presence has a
25% chance of falling below or above these bounds. In practice we have found
that asking for the 95% and 50% CrIs before seeking the mode is also crucial
for avoiding representation bias, whereby experts may confuse the 50% CrI for
Zk with the precision of their estimate of its mean or mode.
Exact solutions for estimating a beta distribution given a mode and several
quantiles are not available, in the same way that they are for the lognormal
distribution (Low Choy et al., 2008) (E4). A simple numerical approach to

520
The Oxford Handbook of Applied Bayesian Analysis
ï¬nding a close-ï¬tting beta distribution would ï¬rst ï¬x the mode at the elicited
value <Z, assuming that this is the most accurately elicited quantity. Then we ï¬nd
the pair (at, bt) that provides the best ï¬t between the elicited values<q(Â·) and the
encoded values Q(Â·|at, bt) over the probability levels Â·. If only two quantiles are
used for ï¬tting, then the remaining two can be used for feedback (Kynn, 2005).
A balance is required regarding the number of quantiles that can be accurately
and efï¬ciently elicited.
19.4 Software tool for elicitation
Recently prototype software was developed for a geographically assisted
approach to elicitation, modifying the posterior predictive approach of Kadane
and Wolfson (1998). The software was â€˜hardwiredâ€™ for use on two case-studies
(Denham and Mengersen, 2007). This section summarizes development of a
new software tool, Elicitator, inspired by the interfaces of these prototypes,
but implemented on a more modern computing platform and based on the
new elicitation method (Section 19.3.2.2). The substantial software design and
implementation issues are detailed by James et al. (2009).
Continually updating visual aids and computations to reï¬‚ect expert opinions
and revisions, including encoding these opinions as prior distributions, can be
achieved by hand. However these processes are faster, more accurate and more
interactive when supported through software. Other key beneï¬ts of a software
tool are: bias and error minimization via standardisation of key aspects (E5),
inclusion of an elicitation protocol (E6) and underlying statistical design (E5);
facilitation of exploration of a wider range of elicitation options real-time during
elicitation (E4); and provision of a research tool to enable controlled testing of
various elicitation options.
In Elicitator, we relaxed the requirement of Denham and Mengersen (2007)
that covariates have a geographical interpretation, so that the tool would no
longer be embedded within a GIS, but instead be loosely coupled to a GIS, being
able to communicate via shared ï¬letypes. This also bypassed the difï¬culty of
maintaining version control with commercial GIS packages. The addition of an
underlying relational database helped to structure and streamline the collation
of information from several elicitations, across multiple phases, for different
experts, on various projects (having different elicitation sites). The relational
database also facilitates communication with GIS or other databases.
Elicitatorâ€™s core contains three modules: an Elicitation window for manag-
ing interaction with the expert to obtain their assessments (Figure 19.1); an
Encoding module to calculate prior distributions from elicited information; and
a Feedback window to provide the expert with an opportunity to evaluate their
assessments. Input and output to the core is achieved via three additional

Indirect Elicitation from Ecological Experts
521
Fig. 19.1 Elicitation window in Elicitator. Experts are asked to specify the range of plausible proba-
bilities of presence, and may manipulate the boxplot, beta density plot, or numbers.
modules: a Setup window for selecting options deï¬ning the elicitation protocol
and method; an Import window for importing covariate information for each
case to be elicited or revisiting previous elicitations; and an Export window
for exporting and evaluating the prior model. Experts participate in an iter-
ative cycle based on this Elicitation-Encoding-Feedback core. Separating the
modules in this way facilitates later extensions, such as: formulating priors
for a different GLM; elicitation of a different distribution; different methods
of encoding; and feedback tailored to the encoding method and distributional
assumptions.
The main windows for elicitation and feedback are separated to streamline
functionality. The elicitation window (Figure 19.1; right-hand window) permits
experts to specify their estimated probability of success at each elicitation
site/case using: numbers for mode and quantiles; a simple graph interactive
boxplot to visualize the mode and quantiles; and a more sophisticated inter-
active beta density plot again depicting mode and quantiles. In addition the
encoded parameters of the corresponding beta distribution are shown, for
reference. All three representations of the mode and quantiles are updated
dynamically when any are edited. Furthermore, the expert may supply a con-
ï¬dence rating for each elicitation case. This window also lists the covariates

522
The Oxford Handbook of Applied Bayesian Analysis
for the particular elicitation case. The feedback window provides feedback to
experts after collating and encoding their elicitations for sufï¬cient cases/sites,
and shows univariate response curves and regression diagnostics. Univariate
response curves are marginal estimates from predictions formed across covari-
ate proï¬les taken from the elicitation sites (E4, E5). The regression diagnostics
comprise standard GLM diagnostics (Venables and Ripley, 2002), highlighting
unusual elicitations (residual versus ï¬tted value plots, standardized residual
quantile-quantile plots) or those highly inï¬‚uential in formulating the prior
model (Cookâ€™s D inï¬‚uence statistic) (E5). The expert may use the mouse to
click directly on the feedback graphs, together with numeric identiï¬cation of
cases listed in a table, to select any cases and revisit or review their elicitation.
The inclusion of a project menu enforces a structured ï¬ling of elicitation
information, ï¬rst by project deï¬ned by a list of experts and a list of elicitation
cases/sites each accompanied by covariate information, then by phase within
project. This initializes corresponding tables within the database, so that all
potential elicitations for each case and expert are appropriately indexed for later
retrieval. In addition, the elicitor may use this menu to set database connection
details, export elicitation data, set current expert and project phases, and for
loading and saving elicitation projects. Encoding can be initiated either from
the menu, elicitation windows or feedback windows. This initiates the commu-
nication with the R server (Urbanek, 2006), and initialises the data required for
the statistical computing environment (covariate information). The database is
updated each time elicitations are reviewed and changed. Each time encoding
is initiated these details are also uploaded to the statistical computing environ-
ment. Results from encoding prior models are also stored in the database, and
can be exported as a text ï¬le via the menu. Finally the output tab window can
also be accessed within the main application window this uses the syntax of the
WinBUGS modelling environment (Spiegelhalter et al., 2003) to express the
priors for each coefï¬cient for each expert.
To achieve this functionality Elicitator links to several open-source libraries
in order to manage and dynamically update the graphic user interface windows
that link to the underlying data objects, link to a graphing library for graph-
ical representation of elicitations and feedback, link to a relational database
package for data persistence and management, import ï¬les containing data
on elicitation cases/sites and covariates, and link to a statistical computing
package for encoding. A modern object-oriented computing platform in Java
was selected, for its ï¬‚exibility, stability, extensibility, portability and popularity,
as well as the availability of a wide range of well-developed and thoroughly tested
libraries. The dynamic graphing features were achieved using the JFreeChart
Java libraries. MySQL was selected as a well-respected and freely available
relational database management system. Elicitation information was imported
into MySQL from a simple ï¬le format that can be output from most commercial

Indirect Elicitation from Ecological Experts
523
or freely available GIS packages. The tool also employs the use of the statistical
computing package R (Venables and Ripley, 2002), which is popular (ensuring
longevity) and freely available.
19.5 Results
19.5.1 Design of elicitation
Twenty elicitation sites were located within an area, chosen by the elicitor so that
it was not familiar to any of the experts consulted, but had good representation
of each of the habitat variables (E4), particularly geology. Elicitation sites were
selected using stratiï¬ed random sampling, according to the same strata as data
in Murray et al. (2008b) (E5). Covariates at those sites were determined using
GIS, then entered into Elicitator (E4). Experts were thus able to view an inte-
grated spatial database, comprising maps showing each habitat characteristic,
elicitation site locations, and other useful geographic features. Care was taken
to ensure that whilst the expert was able to view maps at scales helpful to
interpretation, they were unable to view them at scales allowing identiï¬cation
of the area (E5). Encoding followed the new indirect method (Section 19.3.2.2)
(E4).
Use
of
the elicitation tool
followed
general principles outlined
in Section 19.4 (E6). Further details on the design are detailed in Murray et al.
(2008a). A more complete summary of the Bayesian analysis, comparing non-
informative priors to expert-informed priors pooled across regions using the
moment-averaging approach, is presented in Murray et al. (2009).
19.5.2 Encoding
Recall that 4â€“5 experts from each of two adjacent regions Q and N were
consulted. The method of elicitation (described in Sections 19.3 and 19.4)
proved sensitive to the diverse range of estimated probabilities of presence
provided by individual experts across twenty elicitation sites. For example,
Figure 19.2 shows the range in elicited opinion across elicitation sites for one
expert from region Q. The raw elicited information is represented by a non-
standard boxplot, highlighting the elicited mode, 50% and 98% CrI limits. Some
sites are clearly believed to correspond to rock wallaby absence, with narrow
encoded beta distributions having mode very close to zero (i.e. sites 6, 12â€“14,
26, 30). One site had low probability of presence that was deï¬nitely non-zero
(site 17). Where rock wallaby presence was likely, this expert provided a range
of plausible values for the probability of presence. The expertâ€™s opinion was
fairly vague for one site (site of the esteem of their peers rather than by force of
personality).
Experts varied in the consistency of their opinions, across sites, regarding
the main effects impacting on the probability of presence for the rock-wallaby.

524
The Oxford Handbook of Applied Bayesian Analysis
0
1
2
3
0
4
8
0.0
1.5
0
5
15
0
2
4
0.0
1.5
0.0
1.5
3.0
0.0
1.5
3.0
0
10
20
0
5
15
0
10
20
0.0
1.0
2.0
0
2
4
0
2
4
6
0.0
1.0
2.0
0.0
1.5
0
10
25
0.0
1.0
0.0
1.0
2.0
0
10
20
S1:Î± = 12.87 , Î² = 5.08
S2:Î± = 2.78 , Î² = 34.35
S3:Î± = 6.62 , Î² = 3.57
S6:Î± = 5.55 , Î² = 106.18
S7:Î± = 17.66 , Î² = 7.89
S8:Î± = 7.1 , Î² = 4.3
S9:Î± = 8.52 , Î² = 10.57
S11:Î± = 10.04 , Î² = 10.21
S12:Î± = 6.85 , Î² = 153.86
S13:Î± = 5.46 , Î² = 98.44
S14:Î± = 6.16 , Î² = 155.83
S16:Î± = 5.46 , Î² = 3.4
S17:Î± = 9.4 , Î² = 20.86
S23:Î± = 3.12 , Î² = 19.33
S24:Î± = 5.25 , Î² = 3.3
S25:Î± = 7.05 , Î² = 5.05
S26:Î± = 7.33 , Î² = 190.93
S27:Î± = 3.15 , Î² = 2.47
S29:Î± = 4.6 , Î² = 2.51
S30:Î± = 8.1 , Î² = 140.65
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 19.2 Elicited distribution of the probability of presence for each of twenty sites (each plot), for
a single expert. Encoded using mode and upper quartile (dashed line) or using mode, quartiles and
98% credible interval (solid line). Boxplot indicates elicited mode (line), quartiles (box), and 98%
credible interval (whiskers).
This consistency is reï¬‚ected in the regression diagnostics resulting from ï¬tting
a main effects model (using Elicitator) to the information elicited from a sin-
gle expert. Regression diagnostics provided by Elicitator are shown here are
standard outputs from R, and two examples are shown (Figures 19.4â€“19.5).
Note these ï¬gures show a compressed version of plots produced by Elicitator,
redrawn for display here. The elicited information obtained from Expert 3
from region N shows an excellent ï¬t to the logistic regression model, clearly
indicating that rock-wallabies prefer higher slopes, forested areas and basalt
geology. The accumulation of site-by-site elicitations provided by Expert 3 from
region Q show an adequate ï¬t to a logistic regression model, with a moderately
strong message that rock-wallabies prefer open forest to other vegetation and
forest to other land uses. This expert is less clear on the impact of geology,
although conï¬rms a preference for basalt. The effect of aspect estimated by
either expert is less clear, with the region N expert suggesting a preference
for north-westerly to northerly aspects, and the region Q expert suggesting
a preference for northerly to north-easterly aspects. The effect of elevation is

Indirect Elicitation from Ecological Experts
525
Site 16
0
.2
.4
.6
.8
1
Site 2
0
.2
.4
.6
.8
1
Site 27
0
.2
.4
.6
.8
1
Site 29
Plausibility
Site 23
Site 14
Fig. 19.3 Comparison of elicited probability of presence across experts, for six sites.
indirect and related to rock type and region: in NSW rock-wallabies tend to be
found in gorges (leading to inverse relationship between probability of presence
and elevation by region N expert); and in QLD rock-wallabies tend to be found
in hilly basalt regions (leading to increasing probability of presence with ele-
vation estimated by region Q expert). Furthermore, the site-by-site elicitations
performed by region Q expert did not reveal a strong impact by slope. For both
experts, there is evidence to suggest that their opinions are subject to more
variation as probability of presence increases, conï¬rming the evidence gained
across experts (e.g. Figure 19.6).
0.2
0.4
0.6
0.8
âˆ’4
âˆ’2
0
2
Slope
Partial for Slope
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
âˆ’2
âˆ’1
0
1
2
GEOLOGY
Partial for GEOLOGY
IntIg SedMet
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
âˆ’1.5
âˆ’0.5
0.5
1.5
REMVEG
Partial for REMVEG
Cleared
OpenFrst
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
0.0
0.2
0.4
0.6
0.8
âˆ’1.5
âˆ’0.5
0.5
Cos.aspect
Partial for Cos.aspect
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
âˆ’3 âˆ’2 âˆ’1
0
1
2
LANDCOVER
Partial for LANDCOVER
Crop ClearedP
l
l
l
l
ll
l
l
l
l
l
l
ll
l
l
l
l
l
l
0.0
0.2
0.4
0.6
0.8
âˆ’2
0
2
4
Elevation
Partial for Elevation
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
âˆ’3
âˆ’2
âˆ’1
0
âˆ’1.5
âˆ’0.5
0.5
1.5
Predicted values
Residuals
16
8
9
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
âˆ’2
âˆ’1
0
1
2
âˆ’1.5
âˆ’0.5
0.5
1.5
Theoretical quantiles
Std. deviance resid.
8
7 25
Fig. 19.4 Expert #3 from region N: regression diagnostics from ï¬tting a model to elicited infor-
mation across sites. The ï¬rst three plots in the top row show partial residuals and response (with
pointwise SE in red) to the effects of: slope, remnant vegetation type (cleared, closed forest, open
forest), and landcover (crop, cleared/pasture, forest). The ï¬rst three plots in the bottom row similarly
show partial residuals and response to the effects of: geology (igneous, sedimentary/metamorphic,
basalt), cosine of aspect, and elevation. The last plot in the top row shows the pattern of residuals
across ï¬tted values, and the last plot in the bottom row shows the quantile-quantile plot of the
standardized residuals.

526
The Oxford Handbook of Applied Bayesian Analysis
0.2
0.4
0.6
0.8
âˆ’0.8
âˆ’0.4
0.0
0.4
Slope
Partial for Slope
âˆ’0.5
0.0
0.5
GEOLOGY
Partial for GEOLOGY
IntIg SedMet
âˆ’1.0
âˆ’0.5
0.0
0.5
REMVEG
Partial for REMVEG
Cleared
OpenFrst
0.0
0.2
0.4
0.6
0.8
âˆ’1.0
0.0
0.5
1.0
Cos.aspect
Partial for Cos.aspect
âˆ’2.0
âˆ’1.0
0.0
LANDCOVER
Partial for LANDCOVER
Crop ClearedP
0.0
0.2
0.4
0.6
0.8
âˆ’3
âˆ’2
âˆ’1
0
1
2
Elevation
Partial for Elevation
âˆ’2
âˆ’1
0
1
âˆ’1.5
âˆ’0.5
0.5
Predicted values
Residuals
17
16
2
âˆ’2
âˆ’1
0
1
2
âˆ’1.5
âˆ’0.5
0.5
Theoretical quantiles
Std. deviance resid.
23
2
24
Fig. 19.5 Expert #3 from region Q. Regression diagnostics from ï¬tting a model to elicited informa-
tion across sites, as detailed in caption to Figure 19.4.
Table 19.2 shows that, after encoding the opinion of expert Q3, we obtained
four prior estimates of regression coefï¬cients with high probability of being
non-zero. After combining these priors with the observed landscape scale
dataset, the posterior probability of being non-zero decreased for two coef-
ï¬cients, increased for one, and became indistinguishable from zero for the
fourth (geology). The data provided strong evidence of probability of presence
increasing with slope (99% chance that this is a positive effect), overriding the
expertâ€™s opinion. The posterior estimate of the impact of forest cover increased
in magnitude (1.5 instead of 1.3) and had decreased chance of being non-
zero (<0.1% instead of 0.6%) with respect to the prior. Posterior correlations
between regression coefï¬cients were negligible, mostly 0.09 or below, with the
highest being 0.23.
Expert N3 provided case-by-case estimates that corresponded to very low
overall prevalence (2%) for baseline habitat. This expert estimated substan-
tial negative impact for increasing elevation, and positive impact of sev-
eral habitat factors, on probability of presence. On combining with data,
Intercept
âˆ’6
âˆ’5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
Density
Fig. 19.6 Intercept in habitat model encoded from each expertâ€™s 20 site elicitations (coloured lines),
pooled across experts using moment averaging (thick grey line) or linear pooling (thick black line).

Indirect Elicitation from Ecological Experts
527
Table 19.2 The most experienced experts from each region: prior and posterior
estimates of regression coefï¬cients â€š j on logit scale. Prior was encoded from ï¬ve
elicited quantities; posterior estimated using informative prior. Baseline habitat cor-
responds to minimum values of habitat covariates: ï¬‚at ground, eastern or western
aspect, elevation at sealevel, intrusive/igneous rock types, forest cleared (of remnant
vegetation), and cropping the major landuse.
Covariate j
Prior
Posterior
mean
s.e.
p(â€š j < 0|x, z)
mean
s.d.
p(â€š j < 0|x, y)
Expert Q3
Intercept
âˆ’1.955
0.671
0.998
âˆ’1.928
0.629
0.999
Slope
0.068
0.161
0.335
0.326
0.148
0.014
Sedimentary/Metamorphic
0.461
0.283
0.052
0.072
0.252
0.389
Volcanic
0.582
0.401
0.073
âˆ’0.282
0.290
0.835
Closed Forest
0.129
0.323
0.344
0.173
0.304
0.285
Open Forest
âˆ’0.039
0.293
0.553
0.233
0.251
0.177
Cleared/Pasture use
âˆ’0.029
0.499
0.523
âˆ’0.063
0.449
0.559
Forest cover
1.309
0.527
0.006
1.449
0.448
0.000
Expert N3
Intercept
âˆ’4.024
1.027
1.000
âˆ’3.899
0.972
1.000
Slope
1.613
0.302
0.000
1.340
0.231
0.000
Sedimentary/Metamorphic
0.237
0.462
0.304
0.656
0.375
0.040
Volcanic
1.979
0.663
0.001
1.598
0.438
0.000
Closed Forest
1.163
0.561
0.019
0.444
0.474
0.174
Open Forest
0.438
0.548
0.212
0.462
0.410
0.133
Cleared/Pasture use
1.569
0.724
0.015
1.412
0.694
0.025
Forest cover
1.060
0.756
0.080
1.458
0.531
0.002
Elevation
âˆ’1.035
0.443
0.990
âˆ’0.171
0.101
0.9579
sedimentary/metamorphic geology became a clear positive impact, the effect of
closed forest became close to zero, and the effect of elevation diminished. The
introduction of data appears to have had little impact on the estimated intercept
and effect of cleared/pasture landcovers.
In summary this expert from region Q appeared to refer mostly to geology
and forest landcover in assessing probability of presence at each site, whereas
the expert from region N had a conceptual model for rock wallaby habitat that
was clearly driven by strong gradients in at least some categories of all habitat
factors considered. This diversity in opinion reï¬‚ects regional differences as well
as the different types of experience held by the two experts. Expert Q3 had
broader experience of both regions, from both ï¬eld-based and desktop GIS-
informed studies.
19.5.3 Collating opinions
For exposition we focus on region N, which exhibited less polarity in expressed
opinions. For a few experts, sites were eliminated when encoding a particular

528
The Oxford Handbook of Applied Bayesian Analysis
Table 19.3 Encoded intercepts.
Expert
Encoded intercept
Sites
omitted
Estimate
SE
p-value
N1
âˆ’2.208
0.893
0.013
24
N2
âˆ’3.037
0.961
0.002
12
N3
âˆ’4.024
1.027
0.000
N4
âˆ’3.127
0.801
0.000
N5
âˆ’2.018
0.827
0.015
Q1
âˆ’3.504
0.895
0.000
30
Q2
âˆ’3.268
0.659
0.000
26, 27
Q3
âˆ’1.955
0.671
0.004
Q4
âˆ’6.064
0.799
0.000
12
expertâ€™s opinion, only if they were identiï¬ed as a statistical and conceptual out-
lier; the site had to be conï¬rmed by the expert to be an unusual site substantially
distorting their overall opinion of species response to habitat (see last column
in Table 19.3). We apply moment-averaging and linear pooling to combine
opinions for the ï¬ve experts from region N (Figure 19.6). Moment-averaging
simply averages, across experts within a region, the means and variances of the
Normal priors for each covariate to provide a pooled prior that is also Normally
distributed (Martin et al., 2005; Kuhnert et al., 2005). Linear pooling constructs
a ï¬nite mixture of expert opinions, providing a more complex distribution than
a simple Normal, which has more ï¬‚exibility to reï¬‚ect diversity and polarity of
multiple opinions (Genest and Zidek, 1986; Clemen and Winkler, 1999; Cooke
and Goossens, 2004; Oâ€™Hagan et al., 2006).
Combining multiple priors on regression coefï¬cients should account for the
implicit conditional interpretation of these coefï¬cients. Thus each coefï¬cient
indicates the effect of the corresponding variable on the response, conditional
on all other variables remaining constant (and in the model).
In region N, priors for the intercept derived from each expert (Table 19.3) are
generally overlapping (Figure 19.6). This allows us to avoid the extra complexity
of ï¬rst pooling information on the intercept, then setting this as an offset in
each expertâ€™s prior model (19.2), before considering pooling information on the
habitat covariates. We also ascribe equal weight to each expert, in the absence of
any additional information to the contrary, and since experts in this case study
derive their knowledge from quite different personal experience.
The moment-averaged estimate (grey) in Figure 19.6 was calculated as
N(âˆ’2.88, .90562), with 95% and 50% credible intervals of [âˆ’4.66, âˆ’1.11] and
[âˆ’3.49, âˆ’2.27] respectively, and mode equal to the mean âˆ’2.88. The linear
pooling estimate (black) has 95% and 50% credible intervals of [âˆ’5.18, âˆ’0.89]
and [âˆ’3.64, âˆ’2.09] respectively, with a mode of âˆ’2.75. Although both pooled
priors are centred at similar values, the linear pooling estimate reï¬‚ects two

Indirect Elicitation from Ecological Experts
529
Intercept
âˆ’6
âˆ’5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
Slope
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
Sedimentary/Metamorphic
âˆ’2
âˆ’1
0
1
2
3
4
Volcanic
0
1
2
3
4
Closed Forest
âˆ’1.5
âˆ’0.5
0.5
1.0
1.5
2.0
Open Forest
0.0
0.5
1.0
1.5
2.0
Cleared/Pasture use
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
Forest use
â€“1
0
1
2
3
4
Fig. 19.7 Region N: comparison of posterior (solid line) and prior (dashed line) distributions for
Bayesian analysis, with pooled (with voting) expert-informed priors (black) or non-informative priors
(grey).
features of the individual opinions not evident in the simple moment-averaged
estimate. Firstly, the range of acceptable values has broader range, extending
the 95% credible interval by 0.6 and 0.2 units at the lower and upper bounds,
respectively. Secondly, the prior is slightly skewed towards values closer to zero.
Posterior estimates are shown for pooling via moment-averaging with voting
(Figure 19.7) and by linear pooling (Figure 19.8). In the ï¬rst case, the posterior
is obviously driven by the prior estimated effects of geology, but not inï¬‚uenced
by the prior on the intercept. The effects of slope, closed forest and open
forest are slightly reduced when accounting for expert knowledge pooled in
this way. Analysis of forest use is non-conclusive with non-informative priors,
yet clearly indicates a positive impact after accounting for this form of expert
knowledge.
Priors derived from linear pooling are generally much more diffuse than
corresponding posterior estimates, indicating that the data are re-iterating the

530
The Oxford Handbook of Applied Bayesian Analysis
Intercept
âˆ’6
âˆ’5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
Density
Slope
âˆ’1
0
1
2
3
Sedimentary/Metamorphic
âˆ’2
âˆ’1
0
1
2
Volcanic
âˆ’2
âˆ’1
0
1
2
3
4
Density
Density
Closed Forest
âˆ’1
0
1
2
3
Density
Open Forest
âˆ’1
0
1
2
Cleared/Pasture use
âˆ’2
âˆ’1
0
1
2
Forest cover
âˆ’2
âˆ’1
0
1
2
3
Fig. 19.8 Region N: comparison of posterior (black) and prior (grey) distributions for Bayesian
analysis, with linear pooling of expert-informed priors.
opinions of at least some experts. Incorporating information from observed
data increases the low level of prevalence of rock-wallabies estimated by experts
from a mode under 5% to a mode over 11%, suggesting that expert opinions
reï¬‚ect conservative estimates of prevalence. The positive effects of slope and

Indirect Elicitation from Ecological Experts
531
volcanic rocktype on likely occurrence of rock-wallabies is emphasized through
informed posterior analysis, with posteriors more tightly constrained around
the mode of more diffuse pooled opinions of experts. The data concur with the
more positive estimates of the impact of sedimentary/metamorphic geologies,
and the more negative estimates of the impact of closed forest. The posterior
estimate of the impact of forest cover is much higher than expected by any of
the experts, and is balanced by a much more negative posterior estimate of the
impact of cleared land or pasture use. In the posterior estimate of the effect
of open forest, the data have dampened the prior estimate of a large positive
impact.
19.6 Discussion
The practical outcome of this analysis, is the clear demonstration that, even
with a carefully designed empirical dataset obtained from rigorous ï¬eldwork,
expert knowledge can contribute to habitat modelling. This is the ï¬rst case
(that we are aware of) where a habitat model where equally rigorous effort
has been applied to obtaining empirical ï¬eld-based data and quantiï¬cation
of expert opinion. Previous use of expert-informed habitat models involving
several habitat covariates have been based on elicitation from one or two experts
(Kynn, 2005; Al-Awadhi and Garthwaite, 2006; Denham and Mengersen, 2007;
Oâ€™Leary et al., 2008b). Where several experts were used, only a single covari-
ate was considered (Kuhnert et al., 2005; Martin et al., 2005). Elicitation of
expert knowledge and empirical data have been used to populate Bayesian
networks for deï¬ning a habitat model, although in these cases the expert and
empirical information are not used to inform estimation of the same model
parameters (e.g. Smith et al., 2007). One exception deï¬nes a Bayesian net as a
prior model, where the prior for each model parameter is deï¬ned by model
averaging over an expert-deï¬ned and data-determined point estimate (Pollino
et al., 2007).
This chapter has reviewed the perceived beneï¬ts of Bayesian statistical mod-
els with informative priors constructed from expert opinion for input into
regression models. One advantage of eliciting expert knowledge for input into
Bayesian models is the need to quantify precision as well as point estimates
in order to encode prior distributions. The issue of assessing the informative-
ness of priors has been explored in great depth in the context of theoretical
construction of reference priors, which seek to maximize the informativeness
of the data relative to the prior (e.g. Box and Tiao, 1982). For informative priors,
the validity of calibration has been questioned (Kadane and Wolfson, 1998):
â€˜The philosophical argument, and in our opinion, the more compelling [than
the mathematical argument] is that what is being elicited is expert, not perfect,

532
The Oxford Handbook of Applied Bayesian Analysis
opinions, and thus they should not be adjusted.â€™ This view is reinforced by
Spiegelhalter et al. (2004) where the importance of establishing prior opinions
before undertaking any analysis is an important undertaking in its own right.
A philosophical investigation of the implications of a â€˜well-calibratedâ€™ Bayesian
are examined by Dawid (1982) and discussants. He concludes that it is simply
too demanding to require that a subjective forecaster is simultaneously coher-
ent and calibrated. Currently we are examining the issue of calibrating and
combining expert opinion in more detail.
One of the advantages of the indirect case-by-case approach to elicitation
embodied in recent software tools (Denham and Mengersen, 2007; James et al.,
2009) is that the information elicited may be used to ï¬t different linear predic-
tors (Bedrick et al., 1996). This arises since elicitation of probability of presence
is conditional only on values of covariates, and does not proscribe their transfor-
mation or interaction, in contrast with other approaches. This issue has been
examined using a standard variable selection approach to indirect elicitation
for logistic regression (Chen et al., 1999), or using Bayesian classiï¬cation trees
(Oâ€™Leary et al., 2008a). Future extensions of Elicitator could provide the modeller
with more ï¬‚exibility to specify prior â€˜modelsâ€™ to be assessed during the encoding
phase.
There is little research comparing the use of quite different elicitation tech-
niques, apart from consideration of more subtle differences, such as the num-
ber and probabilities of quantiles selected (e.g. Kadane and Wolfson, 1998;
Garthwaite and Oâ€™Hagan, 2000; Garthwaite et al., 2005; Leal et al., 2007). One
recent paper that does make these broader comparisons investigates results
from applying three approaches to elicitation with two experts, for habitat
modelling using regression (Oâ€™Leary et al., 2008b). This study revealed some
large differences in encoded priors and corresponding posteriors, depending
on the elicitation technique, and to a lesser extent on the expert. More work is
required on comparing and integrating alternative elicitation methods.
The tool Elicitator already provides functionality to allow different encoding
methods to be used. It is conceivable that different elicitation methods may
also be incorporated in later versions. In addition the modes of communica-
tion selected by the elicitor â€“ numeric, simple graphical summaries, specialist
statistical diagnostic graphs, spatial maps â€“ have led to innovative use of tech-
nology. Elicitator demonstrates that various communication modes are feasibly
incorporated into a single tool, e.g. numeric/graphical representations of the
elicited distribution of the response variable available in the elicitation window
or multiple diagnostics available in the feedback window.
We have endeavoured to illustrate recent technological advances in elicitation
by describing one software tool developed as an outcome from our recent
experience. Its design has laid the foundations for several extensions, which
take advantage of the modern computing environment. The ability to tailor

Indirect Elicitation from Ecological Experts
533
minor aspects of GUIs for speciï¬c applications could greatly increase user
appeal. Flexibility in terms of model choice provides the modeller with many
more choices, by allowing a greater range of GLMs, and potentially even other
models. The case-by-case indirect elicitation of expert â€˜dataâ€™ may be extended
to GLMs by appealing to the very general CMP/DAP framework presented
in Bedrick et al. (1996). Accuracy of elicitation can depend heavily on the
few summary statistics elicited, such as quantiles or which choice of central
measure (mean/median/mode), as demonstrated by Low Choy et al. (2008).
Providing elicitors with ï¬‚exible choices in terms of summary statistics, and
therefore encoding method, enables elicitation to target those summary sta-
tistics considered most accurately represented by experts in that ï¬eld Chaloner
et al. (1993).
For the brush-tailed rock-wallaby, we found that expert knowledge was infor-
mative for modelling habitat in region N (northern NSW). Experts concurred
that preferred habitat increases with slope, volcanic rock, open forest and
areas with cleared/pasture landuse. Supplementing expert information with
empirical data within a Bayesian analysis had least impact on a low estimate of
prevalence (5â€“10%) and conï¬rmed though substantially tightened the expertsâ€™
estimated positive effect of slope. Inclusion of empirical data slightly changed
the expertsâ€™ estimated little effect of sedimentary/metamorphic rock to be
positive, and of negative effect of forest cover to also be positive. In contrast
empirical data maintained but decreased the expertsâ€™ opinions on a positive
effect of volcanic rock and of open forest, yet provided sufï¬cient evidence to
override the expertsâ€™ opinions of generally positive impacts of closed forest or of
cleared/pasture use to conclude these factors in fact reduced habitat preference.
Thus empirical and expert information have varying inï¬‚uence on each habitat
factor, so are both essential to the analysis. Full discussion of the ecological
interpretation of one way of aggregating expert opinions is provided in Murray
et al. (2009).
Overall expert and empirical evidence indicates that, in this region, the brush-
tailed rock-wallaby clearly prefers sites occurring in landscapes with greater
slope, volcanic but less clearly sedimentary/metamorphic rather than intrusive/
igneous rocktypes, remnant vegetation that is open forest rather than closed or
cleared, and areas that are forested (remnant or regrowth) rather than cleared
or used as pasture or cropping. This information will be useful for future work
in constructing species distribution maps and assessing spatial connectivity of
sites and therefore population dispersal. This work will inform future recovery
planning and conservation management of the species, which can refer to rigor-
ous collection and use of both expert knowledge and empirical data. In addition
it provides a useful demonstration of an approach and accompanying software
that can be applied to any rare and threatened species, particularly where expert
knowledge (typically based on experience in the ï¬eld) is considered valuable.

534
The Oxford Handbook of Applied Bayesian Analysis
Applications, including habitat modelling, have provided an important
motivation for development and reï¬nement of elicitation techniques for
constructing informative priors. The development of software tools to stream-
line and support elicitation could lead to further improvements in uptake,
dissemination as well as improved methodology. We note that tools should
be used in conjunction with: training on statistical concepts (Kynn, 2008);
conditioning to alert experts to potential biases (Low Choy et al., 2009); an
interview proforma, with wording of questions carefully chosen to minimize
biases (Chaloner and Duncan, 1983; Oâ€™Hagan et al., 2006; Kynn, 2008); within
a clearly deï¬ned protocol (Low Choy et al., 2009). Where software tools package
well-designed elicitation methods together with practical guidelines, they may
also promote wider application of good elicitation practice and, consequently,
strongly enhance capacity to quantitatively address important practical prob-
lems in a wide range of ï¬elds.
Appendix
A. Broader context and background
A.1 Designing elicitation
Elicitation, like any other data collection exercise, is more effective, transparent
and repeatable when carried out following a designed approach. Five stages dur-
ing an elicitation (interview) process were proposed by Spetzler and StaÃ«l von
Holstein (1975) and are still considered relevant (e.g. Renooij, 2001): motivate
experts to contribute and clarify their motivations, structure the problem to
focus on quantities of interest, condition the expert to follow good practices
of probabilistic thinking and to avoid common biases, elicit and encode their
opinions and uncertainty as statistical distributions, and verify that the expert
is satisï¬ed with their responses. These stages of elicitation reï¬‚ect the expertâ€™s
experiences, but rely on signiï¬cant preparation undertaken by the statistical mod-
eller before elicitation to design the process. This is acknowledged by Oâ€™Hagan
et al. (2006) who propose a broader view of the whole elicitation process, con-
sidered more from the statisticianâ€™s perspective. They identify two preliminary
phases to (1) establish background and undertake preparation and to (2) identify
and recruit experts. These authors also expand the initial motivation stage,
combined to some extent with conditioning, in a stage of (3) motivating and
training experts.
Here we focus on expanding the ï¬rst preparatory phase identiï¬ed by
Oâ€™Hagan et al. (2006) to detail design of elicitation (Section A.1.1); this phase
is of primary concern when designing software to accompany elicitation. Six
main components in designing elicitation, distilled from experiences in quan-
tifying knowledge of ecologists, are presented in Table 19.1. These are further

Indirect Elicitation from Ecological Experts
535
expanded to include up to ï¬ve subcomponents per component, with illustrating
examples, in Low Choy et al. (2009). Here we provide an indication of what each
component involves. When applied in practice, varying levels of emphasis on
the six components will be required to tailor the framework to the particular
ecological context and ecologists involved.
This framework (Section A.1.1) includes a mathematical step of encoding
the expert knowledge into distributions, addressed at its most basic level when
encoding a single distribution (Section A.1.2). For more complex situations,
indirect approaches to encoding may be more effective (Section A.2).
A.1.1 A framework for designed elicitation At the outset of elicitation it is important to
determine how the expert opinion is to be utilized [E19.1]. The end purpose
can place constraints on the content and form of information elicited. In some
cases expert knowledge is important as an end in itself (Alho et al., 1996) or for
designing data collection (Golicher et al., 2006). Expert opinion can be crucial
for representing the current state of knowledge before extensive monitoring
data is available, either when constructing complex biophysical models (e.g.
Boyce et al., 2005; Uusitalo, 2007) or developing environmental guidelines (e.g.
Low Choy et al., 2005). Expert knowledge can provide a starting point to be
updated with new empirical data within the overall Bayesian learning cycle (e.g.
McCarthy and Masters, 2005). In complex process models, expert knowledge
may prove valuable for quantifying important input parameters (e.g. Boyce
et al., 2005).
A key factor in successful elicitation is determining what experts know,
and how they are most comfortable expressing it [E2]. Consider elicitation of
preferred habitat for input into logistic regression (Oâ€™Leary et al., 2008b) or
classiï¬cation trees (Oâ€™Leary et al., 2008a). Can experts only provide an indication
of whether factors increase, decrease or have no effect on the response (Oâ€™Leary
et al., 2008b)? Or are they able to sketch the ecological response to a single factor,
with all others held constant (Kynn, 2006)? Alternatively, perhaps they are more
comfortable estimating ecological response at a particular site where all the
factors are known (Section 19.2; Denham and Mengersen, 2007; Murray et al.,
2008a). These different forms of elicited information required quite different
mathematical encoding techniques.
Model formulation requires understanding the information available, both
expert and empirical [E3]. In many cases decomposition of the initial problem
may reveal underlying ecological processes, and thus enable the elicitor to
better target the expertâ€™s knowledge. For example, expert panels are proï¬cient
in deï¬ning boundaries to delineate bioregions (Neldner, 2006). Alternatively
a common data-driven approach clusters sites in a training dataset compris-
ing measurements on various environmental factors, then uses discriminant
analysis to allocate new sites to each cluster. The question is how can this

536
The Oxford Handbook of Applied Bayesian Analysis
expert knowledge deï¬ne a prior distribution to supplement this type of data
within a Bayesian framework? The problem is to deï¬ne a single model that
both identiï¬es and describes the clusters, as well as allocating sites to clusters.
One solution is a ï¬nite mixture model which focuses on describing the range of
values for each environmental factor within each region/cluster, and uses latent
variables to allocate sites (training or future) to each region/cluster (Rochester
et al., 2004; Accad et al., 2005).
The way in which expert knowledge is translated mathematically into statisti-
cal distributions can have substantial impact on results [E4], and is a major con-
cern of this research. An overall decision is whether to use a direct or indirect
method of encoding (Section A.2). Other encoding decisions are highly context-
speciï¬c and may depend on the ecologists involved and resources available, for
example: the summary statistics elicited; the ordering of questions; and whether
visual feedback is used. Section 19.3 illustrates these concepts in terms of the
rock-wallaby application and Section A.1.2 addresses this issue more generally.
Like any measurement technique, the whole process of elicitation is subject
to many sources of uncertainty, and it is important to manage these [E5].
For this application, we determine major sources of uncertainty and design a
strategy that manages these uncertainties arising from elicitation with a single
expert (Section 19.3), as well as variability arising across several experts (Sec-
tion 19.5.3). See Kynn (2008) for a modern review of the cognitive biases that
may inform design and Oâ€™Hagan et al. (2006, Appendix C) for several practical
tips.
Finally an elicitation protocol must address logistical issues and stipulate
a repeatable and transparent protocol [E6]. One practical decision concerns
delivery. Expert responses can be compiled via questionnaire (Martin et al.,
2005), or via interview. Experts can be consulted either individually, perhaps
with the aid of a computer (Section 19.4, Kynn, 2005; Denham and Mengersen,
2007), or in groups (Vose, 1996; Garthwaite and Oâ€™Hagan, 2000; Low Choy et al.,
2005). In some cases experts can be trained in the skill of quantifying their
opinions using feedback in the form of scoring rules (Gneiting and Raftery,
2007), or seed variables elicited to help calibrate their responses (Cooke, 1991;
Burgman, 2005). An efï¬cient approach with small numbers of experts relies
on the elicitor to identify knowledge gaps and address these in situ just prior
to, or during, elicitation. With a large group of experts, this training can occur
during a workshop held prior to elicitation. Sampling issues are paramount
in elicitation: the selection of a balanced group of experts, each with sufï¬cient
expertise on the topic, greatly affects both the information elicited (particularly
in group elicitations) as well as the credibility of the results. Furthermore it is
important that each expertâ€™s motivating interests (e.g. recreation, conservation
or development) are clearly stated, which can be related to methods used by the
elicitor to motivate their participation. The issues of selecting and motivating

Indirect Elicitation from Ecological Experts
537
experts have been carefully addressed in the environmental risk assessment
context (Burgman, 2005, Sections 19.4.1â€“19.4.2).
A.1.2 Encoding a distribution A plethora of techniques have been developed for
encoding a univariate distribution [E4], as outlined in Table 19.4. Moment-
matching approaches typically rely on elicitation of a small number of summary
statistics. Most often two statistics are elicited to characterize a two-parameter
distribution. The most common summary statistic elicited as a â€˜best estimateâ€™
is often taken to represent a measure of location for the distribution (mean,
median or mode). Others include quantiles, cumulative probabilities and pre-
dictions. Minimally the same number of summary statistics as distributional
parameters is required for an exact solution of the distributional parameters.
Where these relationships are less algebraically tractable, numerical encoding
may be necessary (e.g. Low Choy et al., 2008). Where more statistics are elicited
compared to the number of parameters a regression approach can be used
(Section 19.3). In some cases a carefully selected series of summary statistics
are elicited to fully parametrize a complex variance structure (Kadane et al.,
1980; Garthwaite and Oâ€™Hagan, 2000).
Table 19.4 Common methods for encoding a distribution (Hughes and Madden,
2002; Oâ€™Hagan et al., 2006; Low Choy et al., 2008).
Elicitation
Description
method
Moment-matching approaches
Mean with
A tail probability (Gross, 1971; Weiler, 1965); a quantile (Duran and Booker,
1988); the mode (LÃ©on et al., 2003); or the mean absolute deviation about
the mean (Pham-Gia et al., 1992).
Median with
The mean absolute deviation about the median (Pham-Gia et al., 1992), or a
quantile (Kynn, 2005; Denham and Mengersen, 2007).
Mode with
The probability of an interval centered on the mode (Fox, 1966); an upper
quantile (Gilless and Fried, 2000); several quantiles (Low Choy et al.,
2008); â€˜drop-offsâ€™, the odds ratios of the mode Â±1 compared to the mode
(Chaloner and Duncan, 1983; Gavasakar, 1988).
Other approaches
Fractile method Minimum two quantiles (Duran and Booker, 1988), or more, esp. for
skewed distributions (Oâ€™Hagan, 1998; Low Choy et al., 2008). Quantiles
considered include: quartiles, tertiles and sextiles (Garthwaite and
Oâ€™Hagan, 2000).
The bisection
algorithm
A sequence of quantiles are elicited (Oâ€™Hagan et al., 2006)
(Leal et al., 2007).
Histogram
method
Minimum two intervals (Weiler, 1965), or more (Low Choy et al., 2008).
Hypothetical
future
samples
Assess future response in light of data to impute prior (Winkler, 1967;
Oâ€™Hagan et al., 2006); mode with a series of updated estimated responses
given hypothetical datasets (Gavasakar, 1988).

538
The Oxford Handbook of Applied Bayesian Analysis
A.2 Direct versus indirect elicitation methods
Encoding methods [E4] may be categorized as direct (structural) or indirect
approaches. Direct approaches simplify statistical encoding, by asking experts
to directly describe prior distributions for parameters in the model. For a
normal prior on a parameter (such as a regression coefï¬cient), this amounts
to asking experts for the mean and standard deviation of the parameter, across
all values of other parameters (marginal prior) or for speciï¬c values of other
parameters (conditional prior). Direct elicitation relies on experts being very
familiar with the explanatory side of a model, in particular the interpretation
of parameters which are abstract concepts. In contrast, indirect elicitation
exploits familiarity of experts with the predictive side of a model, in assessing
observables such as expected or predicted responses which are more concrete
concepts. Indirect approaches simplify communication with the expert, but
incur overheads in statistical encoding to reformulate the model to better target
expert knowledge.
In this context, direct elicitation (Winkler, 1967) of regression coefï¬cients â€š
and their variability  assumes experts have comprehensive understanding
of the role of both these parameters in the regression model. Direct meth-
ods for encoding regressions are tabulated in Table 19.5. For uncomplicated
regressions with a single covariate this may be achievable (e.g. Fleishman
et al., 2001; Du Mouchel, 1988). In the more general context with multiple
covariates experts are often asked both to select the relevant covariates, as
well as specify their â€˜bestâ€™ estimate of the coefï¬cients. Indeed this mirrors the
approach taken by the popular, non-statistical approach, Multicriteria Decision
Analysis (MCDA), often applied to eliciting expert opinion for constructing
habitat models in the absence of data (Roloff and Kernohan, 1999).
However, for multiple covariates, direct speciï¬cation of a covariance matrix
can be complicated even with as few as three covariates (Kadane et al., 1980;
Table 19.5 Methods of eliciting information for encoding priors for regression coef-
ï¬cients. Part 1: Direct approaches.
I.
Simple. What do you think the effect of the covariate is on the response?
I-a. Moment. Provide the standard error of your estimated effect (Fleishman et al.,
2001).
I-b. Fractile. Estimate quantiles of the estimated effect (Du Mouchel, 1988;
Low Choy et al. 2005a).
I-c. Histogram. Estimate the likelihood of a range of values for the effect (West, 1988).
I-d. Relative precision. How much information is there in your prior knowledge
compared to the observed data (Zellner, 1996).
II.
Sign. Do you think the response increases, decreases or is unchanged as the covariate
increases (Martin et al., 2005; Kuhnert et al., 2005; Oâ€™Leary et al., 2008b)?
III.
Multiple comparisons. Describe quantiles of a group of effects simultaneously
(Du Mouchel, 1988).

Indirect Elicitation from Ecological Experts
539
Table 19.6 Methods of eliciting information for encoding priors for regression coef-
ï¬cients. Part 2: Main questions deï¬ning indirect approaches.
IV.
Predicted response. Having seen the observed data, what do you predict the response
to be for speciï¬c cases with known values of all covariates (Kadane et al., 1980;
Kadane and Wolfson, 1998)?
IV-a. Spatial prediction. As above, replacing â€˜casesâ€™ with â€˜sitesâ€™ (Denham and
Mengersen, 2007)?
V.
Case Conditional Mean. What do you expect the response to be, given values of
covariates? The number of questions equals the number of theoretical covariates
(Oman, 1985; Bedrick et al., 1996).
VI.
Curve Conditional Mean. What do you expect the response to be, given values of one
covariate? All other covariates are ï¬xed at reference values (Willems et al., 2005;
Kynn, 2005, 2006; Al-Awadhi and Garthwaite, 2006).
VII.
â€˜Priorâ€™ data. Generate potential responses for speciï¬c cases with known values of all
covariates (Chen et al., 1999; James et al., 2009).
VIII.
Data Augmentation. Give examples of expected responses, weighted according to
their plausibility, for speciï¬c cases with known values of all covariates (Bedrick
et al., 1996; Chen et al., 1999; James et al., 2009)?
IX.
Conditional Mean via Measurement Error. What do you expect the response to be,
given values of covariates? Questions may exceed the number of covariates. This
new approach (James et al., 2009) extends that of Bedrick et al. (1996) inspired by
Lindley (1983).
Lindley, 1983; Oâ€™Hagan, 1998). One way to simplify its assessment is to
reparameterize as  = cÃ›2X T Xâˆ’1 with p(Ã›2) âˆ1/Ã›2 (Zellner, 1996). Then c is
elicited as the amount of information provided by the prior in comparison to the
data. Alternatively assessment of the coefï¬cients can be simpliï¬ed by eliciting
the sign of a sole covariate (Martin et al., 2005; Kuhnert et al., 2005), or several
(Oâ€™Leary et al., 2008b). Experts can be asked whether they believe the response
increases, decreases or is insensitive to each covariate. This information can be
encoded as an indicator â€š j âˆˆ{âˆ’1, 0, +1} and then encoded as a single normal
distribution representing opinions of several experts (Martin et al., 2005) or as
a mixture of normals representing plausibility of each value of the indicator for
a single expert, or collated over several experts (Oâ€™Leary et al., 2008b).
These simpliï¬cations will not always be appropriate. In such situations
experts may ï¬nd direct elicitation difï¬cult (Kadane et al., 1980; Denham and
Mengersen, 2007; Oâ€™Leary et al., 2008b). Inaccurate representation of expert
knowledge would introduce cognitive and motivational biases (Kynn, 2008).
These biases can be reduced by providing experts with a more natural frame-
work for expressing their knowledge, in terms of observables on the same scale
as the data, rather than unobservable and abstract model parameters (Kadane
and Wolfson, 1998). To this end, elicitation can be restructured (Spetzler and
StaÃ«l von Holstein, 1975), resulting in indirect elicitation of model parame-
ters (Winkler, 1967), often achieved via decomposition of the statistical model
(Low Choy et al., 2009). Table 19.6 lists several indirect approaches to elicitation

540
The Oxford Handbook of Applied Bayesian Analysis
for regression, with the main question determining how â€š is encoded. Please
see cited references to see how additional questions are required to encode .
Several indirect approaches to elicitation have been developed for regression;
these are reviewed in Section 19.3.1.
Acknowledgments
We gratefully acknowledge funding support for the ï¬rst, third and fourth
authors under Australian Research Council Discovery Grant DP0667168, and
an ARC Linkage Project with Healthy Waterways, Queensland Environmental
Protection Agency and Queensland Department of Natural Resources. Initial
work by the ï¬rst author on elicitation was supported by a Queensland Uni-
versity of Technology Postdoctoral research award. Work on collating multiple
expert opinions was also seeded by a travel grant from Lâ€™Institut Nationale
Recherche Agricole (INRA), Paris, France. Work by the third author was sup-
ported by the Collaborative Centre for Complex Dynamic Systems & Control.
We thank Robert Denham for useful discussion and access to some of the
code from his prototype software, an important inspiration on many levels for
Elicitator. We thank the organizers of the Environmetrics satellite workshop and
the main conference of the International Society for Bayesian Analysis, for an
opportunity for initial presentation of aspects of these ideas. The ensuing dis-
cussion and comments provided by participants such as Ross McVinish, David
Draper and Tony Oâ€™Hagan stimulated more careful thought. We also thank Kris-
ten Williams, Mike Austin and Rebecca Oâ€™Leary for fruitful discussions about
ecological applications of elicitation. The diligence and enthusiasm of ecologists
involved in elicitation was pivotal to the project, and greatly appreciated.
References
Accad, A., Low Choy, S., Pullar, D., and Rochester, W. (2005). Bioregional classiï¬cation via
model-based clustering. In MODSIM 2005 International Congress on Modelling and Simu-
lation. Modelling and Simulation Society of Australia and New Zealand.
Al-Awadhi, S. A. and Garthwaite, P. H. (2006). Quantifying expert opinion for modelling fauna
habitat distributions. Computational Statistics, 21, 121â€“140.
Alho, J. M., Kangas, J., and Kolehmainen, O. (1996). Uncertainty in expert predictions of the
ecological consequences of forest plans. Applied Statistics, 45, 1â€“14.
Austin, M. P. (2002). Spatial prediction of species distribution: an interface between ecological
theory and statistical modelling. Ecological Modelling, 157, 101â€“118.
Austin, M. P. and Heyligers, P. C. (1989). Vegetation survey design for conservation â€“ gradsect
sampling of forests in northeastern New South Wales. Biological Conservation, 50, 13â€“32.
Bedrick, E. J., Christensen, R., and Johnson, W. (1996). A new perspective on priors for
generalized linear models. Journal of the American Statistical Association, 91, 1450â€“1460.
Box, G. E. P. and Tiao, G. C. (1982). Bayesian Inference in Statistical Analysis. John Wiley,
New York.

Indirect Elicitation from Ecological Experts
541
Boyce, M. S., Irwin, L. L., and Barker, R. (2005). Demographic meta-analysis: synthesizing vital
rates for spotted owls. Journal of Applied Ecology, 42, 38â€“49.
Branscum, A. J., Johnson, W. O., and Thurmond, M. C. (2007). Bayesian Beta regres-
sion: applications to household expenditure data and genetic distance between foot-
and-mouth disease viruses. Australian and New Zealand Journal of Statistics, 49,
287â€“301.
Burgman, M. (2005). Risks and Decisions for Conservation and Environmental Management.
Ecology, Biodiversity and Conservation. Cambridge University Press, Cambridge.
Chaloner, K., Church, T., Louis, T. A., and Matts, J. P. (1993). Graphical elicitation of a prior
distribution for a clinical trial. The Statistician, 42, 341â€“353.
Chaloner, K. M. and Duncan, G. T. (1983). Assessment of a Beta prior distribution: PM
elicitation. The Statistician, 32, 174â€“180.
Chen, M. H., Ibrahim, J. G., and Yiannoutsos, C. (1999). Prior elicitation, variable selection and
Bayesian computation for logistic regression models. Journal of the Royal Statistical Society,
Series B (Methodological), 61, 223â€“242.
Clemen, R. T. and Winkler, R. L. (1999). Combining probability distributions from experts in
risk analysis. Risk Analysis, 19, 187â€“203.
Cooke, R. (1991). Experts in Uncertainty: Opinion and Subjective Probability in Science. Oxford
University Press, New York.
Cooke, R. M. and Goossens, L. H. J. (2004). Expert judgement elicitation for risk assessments
of critical infrastructures. Journal of Risk Research, 7, 643â€“656.
Dawid, A. P. (1982). The well-calibrated Bayesian (with discussion). Journal of the American
Statistical Association, 77, 605â€“610.
Denham, R. and Mengersen, K. (2007). Geographically assisted elicitation of expert opinion for
regression models. Bayesian Analysis, 2, 99â€“136.
Du Mouchel, W. (1988). A Bayesian model and a graphical elicitation procedure for multiple
comparisons. In Bayesian Statistics 3, (ed. J. M., Bernardo, M. H., de Groot, D. V., Lindley
and A. F. M., Smith), pages 127â€“145. Oxford University Press, Oxford.
Duran, B. and Booker, J. (1988). A Bayes sensitivity analysis when using Beta distribution as a
prior. IEEE Transactions on Reliability, 37, 239â€“247.
Elith, J., Graham, C. H., Anderson, R. P., DudÃ­k, M., Ferrier, S., Guisan, A., Hijmans, R. J.,
Huettmann, F., Leathwick, J. R., Lehmann, A., Li, J., Lohmann, L. G., Loiselle, B. A.,
Manion, G., Moritz, C., Nakamura, M., Nakazawa, Y., Overton, J. M., Peterson, A. T., Phillips,
S. J., Richardson, K., Scachetti-Pereira, R., Schapire, R. E., SoberÃ³n, J., Williams, S., Wisz,
M. S., and Zimmermann, N. E. (2006). Novel methods improve prediction of speciesâ€™
distributions from occurrence data. Ecography, 29, 129â€“151.
Ferrier, S., Watson, G., Pearce, J., and Drielsma, M. (2002). Extended statistical approaches
to modelling spatial pattern in biodiversity in northeast New South Wales. I. Species-level
modelling. Biodiversity and Conservation, 11, 2275â€“2307.
Fleishman, E., Nally, R. M., Fay, J. P., and Murphy, D. D. (2001). Modeling and predicting
species occurrence using broad-scale environmental variables: An example with butterï¬‚ies
of the great basin. Conservation Biology, 15, 1674â€“1685.
Fox, B. (1966). A Bayesian approach to reliability assessment. Technical Report Memorandum
RM-5084-NASA, The Rand Corporation, Santa Monica, CA.
Garthwaite, P. H., Kadane, J. B., and Oâ€™Hagan, A. (2005). Elicitation. Technical report, Univer-
sity of Shefï¬eld. http://www.shef.ac.uk/beep.
Garthwaite, P. H. and Oâ€™Hagan, A. (2000). Quantifying expert opinion in the UK water
industry: an experimental study. The Statistician, 49, 455â€“477.
Gavasakar, U. (1988). A comparison of two elicitation methods for a prior distribution for a
binomial parameter. Management Science, 34, 784â€“790.

542
The Oxford Handbook of Applied Bayesian Analysis
Genest, C. and Zidek, J. V. (1986). Combining probability distributions: A critique and an
annotated bibliography. Statistical Science, 1, 114â€“135.
Gilless, J. K. and Fried, J. S. (2000). Generating Beta random rate variables from probabilistic
estimates of ï¬reline production times. Annals of Operation Research, 95, 205â€“215.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102, 359â€“378.
Golicher, D. J., OHara, R. B., Ruiz-Montoya, L., and Cayuela, L. (2006). Lifting a veil on diversity:
A Bayesian approach to ï¬tting relative-abundance models. Ecological Applications, 16, 202â€“
212.
Greenland, S. (2006). Bayesian perspectives for epidemiological research: I. Foundations and
basic methods. International Journal of Epidemiology, 35, 765â€“775.
Gross, A. (1971). The application of exponential smoothing to reliability assessment. Techno-
metrics, 13, 877â€“883.
Guisan, A. and Thuiller, W. (2005). Predicting species distribution: offering more than simple
habitat models. Ecology Letters, 8, 993â€“1009.
Guisan, A. and Zimmermann, N. E. (2000). Predictive habitat distribution models in ecology.
Ecological Modelling, 135, 147â€“186.
Hughes, G. and Madden, L. V. (2002). Some methods for eliciting expert knowledge of plant
disease epidemics and their application in cluster sampling for disease incidence. Crop
Protection, 21, 203â€“215.
James, A., Low Choy, S., and Mengersen, K. (2009). Elicitator: A software-based expert elicita-
tion tool for regression in ecology. Environmental Modelling and Software, 25, 129â€“145.
Kadane, J. B., Dickey, J. M., Winkler, R. L., Smith, W. S., and Peters, S. C. (1980). Interactive
elicitation of opinion for a normal linear model. Journal of the American Statistical Association,
75, 845â€“854.
Kadane, J. B. and Wolfson, L. J. (1998). Experiences in elicitation. The Statistician, 47, 3â€“19.
Kuhnert, P. M., Martin, T. G., Mengersen, K., and Possingham, H. P. (2005). Assessing the
impacts of grazing levels on bird density in woodland habitat: A Bayesian approach using
expert opinion. Environmetrics, 16, 717â€“747.
Kynn, M. (2005). Eliciting expert opinion for a Bayesian logistic regression model in natural
resources. Ph.D. thesis, School of Mathematical Sciences, Faculty of Science, Queensland
University
of
Technology.
http://adt.library.qut.edu.au/adt-qut/public/adt-
QUT20050830.084943.
Kynn, M. (2006). Designing elicitor: Software to graphically elicit expert priors for logis-
tic regression models in ecology. http://www.winbugs-development.org.uk/elicitor/
files/designing.elicitor.pdf.
Kynn, M. (2008). The â€œheuristics and biasesâ€ bias in expert elicitation. Journal of the Royal
Statistical Society, Series A, 171, 239â€“264.
Landrum, M. B. and Normand, S.-L. T. (1999). Applying Bayesian ideas to the development of
medical guidelines. Statistics in Medicine, 18, 117â€“137.
Langhammer, P. F., Bakarr, M. I., Bennun, L. A., Brooks, T. M., Clay, R. P., Darwall,
W., Silva, N. D., Edgar, G. J., Eken, G., Fishpool, L. D. C., da Fonseca, G. A. B., Fos-
ter, M. N., Knox, D. H., Matiku, P., Radford, E. A., Rodrigues, A. S. L., Salaman, P.,
Sechrest, W., and Tordoff, A. W. (2007). Identiï¬cation and gap analysis of key biodiver-
sity areas: Targets for comprehensive protected area systems. Best Practice Protected Area
Guidelines Series No. 15, IUCN (The World Conservation Union): Gland, Switzerland.
http://www.iucn.org/dbtw-wpd/edocs/PAG-015.pdf.
Leal, J., Wordsworth, S., Legood, R., and Blair, E. (2007). Eliciting expert opinion for economic
models: An applied example. Value in Health, 10, 195â€“203.

Indirect Elicitation from Ecological Experts
543
LÃ©on, C. L., VÃ¡zquez-Polo, F. J., and GonzÃ¡lez, R. L. (2003). Elicitation of expert opinion
in beneï¬t transfer of environmental goods. Environmental and Resource Economics, 26,
199â€“210.
Lindley, D. (1983). Reconciliation of probability distributions. Operations Research, 31,
866â€“880.
Low Choy, S., Mengersen, K., and Rousseau, J. (2008). Encoding expert opinion on skewed
non-negative distributions. Journal of Applied Probability and Statistics, 3, 1â€“21.
Low Choy, S., Oâ€™Leary, R., and Mengersen, K. (2009). Elicitation by design for ecology: using
expert opinion to inform priors for Bayesian statistical models. Ecology, 90, 265â€“277.
Low Choy, S., Stewart-Koster, B., Eyre, T., Kelly, A., and Mengersen, K. (2005). Benchmarking
indicators of vegetation condition: A Bayesian modelling and decision theoretic approach.
In MODSIM 2005 International Congress on Modelling and Simulation. Modelling and
Simulation Society of Australia and New Zealand.
Martin, T. G., Kuhnert, P. M., Mengersen, K., and Possingham, H. P. (2005). The power of
expert opinion in ecological models: A Bayesian approach examining the impact of livestock
grazing on birds. Ecological Applications, 15, 266â€“280.
McCarthy, M. A. and Masters, P. (2005). Proï¬ting from prior information in Bayesian analyses
of ecological data. Journal of Applied Ecology, 42, 1012â€“1019.
Miller, J. and Franklin, J. (2002). Modeling the distribution of four vegetation alliances
using generalized linear models and classiï¬cation trees with spatial dependence. Ecological
Modelling, 157, 227â€“247.
Murray, J., Goldizen, A., Oâ€™Leary, R., McAlpine, C., Possingham, H., and Low Choy, S. (2008a).
How useful is expert opinion for predicting the distribution of a species within and beyond
the region of expertise? A case study using brush-tailed rock-wallabies (Petrogale penicillata).
Journal of Applied Ecology, 46, 842â€“851.
Murray, J., Low Choy, S., McAlpine, C., Possingham, H., and Goldizen, A. (2008b). The impor-
tance of ecological scale for wildlife conservation in naturally fragmented environments: A
case study of the brush-tailed rock-wallaby (Petrogale penicillata). Biological Conservation, 141,
7â€“22.
Murray, J.V., Goldizen, A.W., Oâ€™Leary, R.A., McAlpine, C.A., Possingham, H.P. and Low Choy,
S. (2009). How useful is expert opinion for predicting the distribution of a species within
and beyond the region of expertise? A case-study using brush-tailed rock wallabies Petrogale
penicillata. Journal of Applied Ecology, 46, 842â€“851.
Neldner, J. (2006). Why is vegetation condition important to government? A case study from
Queensland. Ecological Management & Restoration, 7, S5â€“S7.
Oâ€™Hagan, A. (1997). The ABLE story: Bayesian asset management in the water industry. In
The Practice of Bayesian Analysis, (ed. French, S. and Smith, J. Q), pages 173â€“198. Arnold,
London.
Oâ€™Hagan, A. (1998). Eliciting expert beliefs in substantial practical applications. The Statistician,
47, 21â€“35.
Oâ€™Hagan, A., Buck, C. E., Daneshkhah, A., Eiser, R., Garthwaite, P., Jenkinson, D., Oakley, J.,
and Rakow, T. (2006). Uncertain Judgements: Eliciting Expertsâ€™ Probabilities. John Wiley, New
York.
Oâ€™Leary, R., Murray, J., Low Choy, S., and Mengersen, K. (2008a). Expert elicitation for Bayesian
classiï¬cation trees. Journal of Applied Probability and Statistics, 3, 95â€“106.
Oâ€™Leary, R. A., Low Choy, S. J., Fensham, R. J., and Mengersen, K. L. (2009). Improving the
performance and interpretation of habitat models: using a two-scale modelling approach to
model the envelope and identify excess zeros, with a case study on stemmacantha australis.
Submitted.

544
The Oxford Handbook of Applied Bayesian Analysis
Oâ€™Leary, R. A., Low Choy, S. J., Murray, J. V., Kynn, M., Denham, R., Martin, T. G.,
and Mengersen, K. (2008b). Comparison of three expert elicitation methods for logistic
regression on predicting the presence of the threatened brush-tailed rock-wallaby (petrogale
penicillata). Environmetrics, 19, 1â€“20.
Oman, S. D. (1985). Specifying a prior distribution in structured regression problems. Journal
of the American Statistical Association, 80, 190â€“195.
Pham-Gia, T., Turkkan, N., and Duong, Q. (1992). Using the mean deviation in the elicitation
of the prior distribution. Statistics and Probability Letters, 13, 373â€“381.
Pollino, C. A., Woodberry, O., Nicholson, A., Korb, K., and Hart, B. T. (2007). Parameterisation
and evaluation of a Bayesian network for use in an ecological risk assessment. Environmental
Modelling and Software, 22, 1140â€“1152.
Renooij, S. (2001). Probability elicitation for belief networks: issues to consider. Knowledge
Engineering Reviews, 16, 255â€“269.
Robert, C. P. (2001). The Bayesian Choice: A Decision-Theoretic Framework. Springer-Verlag,
New York.
Rochester, W., Accad, A., Low Choy, S., Neldner, V., Pullar, D., and Williams, K. (2004). Final
report UQ-EPA subregion classiï¬cation project. Technical report, The University of Queens-
land, Brisbane, Australia.
Roloff, G. J. and Kernohan, B. J. (1999). Evaluating reliability of habitat suitability index models.
Wildlife Society Bulletin, 27, 973â€“985.
Smith, C. S., Howes, A. L., Price, B., and McAlpine, C. A. (2007). Using a Bayesian belief
network to predict suitable habitat of an endangered mammal â€“ the Julia Creek dunnart
(sminthopsis douglasi). Biological Conservation, 139, 333â€“347.
Spetzler, C. S. and StaÃ«l von Holstein, C.-A. S. (1975). Probability encoding in decision analysis.
Management Science, 22, 340â€“358.
Spiegelhalter, D. J., Adams, K. R., and Myles, J. P. (2004). Bayesian Approaches to Clinical Trials
and Health-Care Evaluation. John Wiley, Chichester.
Spiegelhalter, D. J., Thomas, A., Best, N. G., and Lunn, D. (2003). WinBUGS version 1.4 user
manual. Technical report, MRC Biostatistics Unit, Cambridge.
Tversky, A. and Kahneman, D. (1981). The framing of decisions and the psychology of choice.
Science, 211, 453â€“458.
Urbanek, S. (2006). RServe. http://www.rforge.net/Rserve/.
Uusitalo, L. (2007). Advantages and challenges of bayesian networks in environmental mod-
elling. Ecological Modelling, 203, 312â€“318.
Venables, W. N. and Ripley, B. R. (2002). Modern Applied Statistics using S-Plus (4th edn).
Springer-Verlag, New York.
Vose, D. (1996). Quantitative Risk Analysis: a Guide to Monte Carlo Simulation Modelling. John
Wiley, Chichester.
Weiler, H. (1965). The use of incomplete Beta functions for prior distributions in binomial
sampling. Technometrics, 7, 335â€“347.
West, M. (1988). Combining expert opinion. In Bayesian Statistics 3, (ed. J. M. Bernardo, M. H.
de Groot, D. V. Lindley and A. F. M. Smith), pp. 493â€“508. Oxford University Press, Oxford.
Willems, A., Janssen, M., Verstegen, C., and Bedford, T. (2005). Expert quantiï¬cation of uncer-
tainties in a risk analysis for an infrastructure project. Journal of Risk Research, 8, 3â€“17.
Winkler, R. L. (1967). The assessment of prior distributions in Bayesian analysis. Journal of the
American Statistical Association, 62, 776â€“800.
Zellner, A. (1996). Models, prior information, and Bayesian analysis. Journal of Econometrics,
75, 51â€“68.

Â·20Â·
Characterizing the uncertainty of climate
change projections using
hierarchical models
Claudia Tebaldi and Richard L. Smith
20.1 Climate change and human inï¬‚uences, the current state
and future scenarios
There is substantial consensus on many aspects of climate change. It is already
with us; we have a large responsibility in many facets of it; and future changes
in the absence of signiï¬cant curbing of greenhouse gas emissions are going to
be much more dramatic than what is already experienced, with consequences
that will be predominantly detrimental to social and natural systems. Adap-
tation and mitigation decisions need however detailed information about the
reality and the future of climate change, and more often than not at regional,
rather than global mean scales. To produce this information, deterministic
numerical models are run under different hypothetical scenarios correspond-
ing to alternative greenhouse gas emission pathways into the future. These
extremely complex computer models discretize the surface of the Earth, the
depths of the oceans and the layers of the atmosphere into regularly spaced grid
boxes. Computations through differential equations provide the evolution of a
high dimensional state vector representing climate variables, by applying our
understanding of climate dynamics and climate interacting processes over land,
water, air and sea ice (Washington and Parkinson, 2005). With the evolution of
science and technology, more and more processes at increasingly ï¬ner scales
can be represented explicitly in these simulations, but there still remains the
need for approximations, for those processes that act at scales not explicitly
represented. It is in these approximations that the source of large uncertainties
resides.
20.1.1 Many climate models, all of them right, all of them wrong
Because of the complexities and high dimensional nature of global climate sim-
ulations, many alternative solutions to the representation and parametrization

546
The Oxford Handbook of Applied Bayesian Analysis
of processes are consistent with the state of our scientiï¬c understanding. Thus,
different models approach the simulation of climate over the Earth with differ-
ent strategies. It may be as basic as the choice of the resolution used to discretize
the globe in its three dimensions (which also affects the time step of the ï¬nite
difference equations). This choice has important consequences on the range of
processes that can be directly simulated and those that have to be represented
by parametrization of the subgrid scales.1 Of course there are less basic choices,
like the processes that are included, independently of the computing limitation:
is an interactive carbon cycle present? Is vegetation changing with climate? Are
urban areas and their effect on climate represented? Even for a given resolu-
tion and a given set of processes, the actual computation of the time-evolving
series of quantities that make up the climate system may differ because of
different values in exogenous parameters of the equations (usually estimated by
studying the individual processes in the ï¬eld) or of different numerical solvers
adopted.
Within this large population of model conï¬gurations we are hard pressed to
ï¬nd ï¬rst and second class citizens. There are more than a dozen global climate
models (many of which run at different resolutions) which are comparable in
terms of overall performance, when validated against the observed record and
paleo-climate proxy records. Of course some models are better than others for
a given set of metrics (average temperature over North America, precipitation
in the monsoon region, the frequency and intensity of the El NiÃ±o Southern
Oscillation phenomenon) but for any set of â€˜better than average modelsâ€™ a
different set of metrics can be found for which those models will underperform
compared to a different set (Gleckler et al., 2007).
Thanks to international efforts like the Intergovernmental Panel on Cli-
mate Change periodic assessment reports, the last of which was published in
2007 (IPCC, 2007), modelling centres participate in concerted sets of experi-
ments, running their models under standardized scenarios of external forc-
ings. These are standard greenhouse gas concentration pathways, derived
after scientists devised a set of alternative, but equally plausible, future story-
lines about the social, political, technological and economic future of our
world. From these story lines consequences in terms of greenhouse gases and
other pollutantsâ€™ emissions were derived using economic integrated modeling.
1 Parametrization is performed by estimating the relation between the large scale, resolved processes
and the small scale, unrepresented processes that happen within a model grid box. Observational or
experimental studies provide the basis for the estimation of the parameters that govern the interaction
between them. Uncertainties in observations and in the relations themselves are at the source of the
uncertainties in parametrizations, causing a range of values for these parameters to be consistent with
their function, but at the same time causing sugniï¬cant differences in the evolution of quantities in the
model integrations.

The Uncertainty of Climate Change Projections
547
For different families of scenarios (varying from a future of fast technologi-
cal progress and international collaboration to one of slower integration and
fossil-fuel-intensive economies) very different trajectories of greenhouse gas
rates of emissions over this century and beyond are speciï¬ed. These sce-
narios of greenhouse gas emissions, known as SRES scenarios (Nakicenovic,
2000), have been used by climate modellers as external inputs to run sim-
ulations of alternative climate scenarios in the future. By the second half of
this century, different emission scenarios cause signiï¬cantly different climate
outcomes, starting from global average temperature change but riverberating
in all aspects of Earthâ€™s climate. By the end of this century, the uncertainty
across scenarios of greenhouse gas increases is larger than the inter-model
differences under a speciï¬c scenario. However, the climate community feels
it inappropriate to attach probabilities to different emission pathways, and we
are left with performing our uncertainty analyses conditionally on a given SRES
scenario.
After running model experiments under alternative SRES scenarios, the
modelling centres are contributing the resulting simulationsâ€™ output into open
access archives. These collections of model simulations have been labelled
ensembles of opportunity, i.e. multiple modelsâ€™ collections that are not the
result of a statistically designed experiment or random sampling from a pop-
ulation of models, but a post facto collection of what is available, thanks
to the voluntary and self-selecting participation of the worldâ€™s largest and
most advanced research centres. The most recent and largest archive of
such data sets is maintained by the Program for Climate Model Diagnosis
and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory
(LLNL), and can be found at http://www-pcmdi.llnl.gov/ipcc/about_
ipcc.php.
20.1.2 Goals and challenges of analysing ensembles of opportunity
The most direct way to obtain regionally detailed future projections is to process
the output of global climate models and determine statistics of the regional
climate variables of interest. Determining which model to trust above all others
is a daunting task, and one defensible strategy is to utilize all that are available,
synthesizing the projections and their uncertainty through a rigorous statistical
analysis. This will provide optimal estimates of the changes in store, and will
quantify their uncertainty, conditionally on the available information. This kind
of representation is of great value to decision makers and stakeholders, notwith-
standing the need of communicating the underlying limitations of our current
understanding and working hypotheses as embodied by these models, and the
assumptions that are at the basis of any statistical representation of the data.

548
The Oxford Handbook of Applied Bayesian Analysis
Bayesian statistics has a natural advantage in this particular setting, not only
because we are dealing with uncertain events that are not easily framed in a fre-
quentist perspective, but more fundamentally because of its natural framework
for incorporating expert judgement, and updating current assessment with
the in-ï¬‚ow of additional pieces of information. There are great challenges
underlying any statistical analysis of such multimodel ensembles (Tebaldi and
Knutti, 2007). They stem from the non-systematic and conversely non-random
nature of the sampling of models, which hampers a full representation of the
uncertainties at stake; from the dependence among the models in the ensemble,
some sharing components, some sharing even their full name, when the same
modelling centre contributes output from runs at different resolutions; from
the lack of a theory linking model performance for current climate, that we can
measure and validate, to model reliability for future projections. A particular
difï¬culty is the impossibility of verifying modelsâ€™ future projections, which sets
this problem well apart from weather forecasting, where feedback about the
performance of the model can be as immediate as six hours after the forecast is
issued.
All these challenges stand in the way of a robust representation of climate
change projections, especially at regional scales and especially for variables
other than temperature, which is relatively easier to model because of its
smoothness. We are using â€˜robustâ€™ here in a more generic sense than is usually
understood by the phrase Bayesian robustness, though the latter may also be
worth exploring in our setting. Climate scientists, hydrologists and agricultural
modelers â€“ among others â€“ are often interested in studying the impacts of cli-
mate change and of adaptation measures. The traditional scientistsâ€™ approach is
through scenarios, where alternative futures used to span a range of outcomes
are fed through impact models. In contrast with this approach, we argue for a
rigorous uncertainty analysis via Bayesian statistics. In fact much of the impact
research efforts are veering towards full probabilistic analysis, and probabilistic
projection of climate change constitute their stepping stone (Tebaldi and Lobell,
2008). We think it is fair to say that the area is still in a phase of methodological
development. We hope that this chapter will offer enough motivation, and open
up enough interesting research directions to invite fresh perspectives to this
important application of statistical analysis.
20.2 A world of data. Actually, make that â€˜many worldsâ€™
The most up to date archive of a multimodel ensemble, hosted by PCMDI at
LLNL, contains over 35 terabytes of data. It collects output from 23 models,
run under a range of SRES emission scenarios. Scores of variables constitute
the output of a climate model simulation, as familiar as temperature and

The Uncertainty of Climate Change Projections
549
precipitation or as esoteric as the mass of water that evaporates over an ice
sheet, or metrics of ocean overturning. Many of the variables are archived as
daily or monthly means, some are six-hourly, some are yearly averages. The
potential for statistical analysis of model output is practically inï¬nite, when we
consider that these quantities are produced at each point of grids that discretize
the entire Earth surface, and at many layers in the atmosphere and oceans.
The median resolution of the climate models in the PCMDI archive is about
2.75 degrees in latitude/longitude, making the standard grid output for each
variable and each time step, when vectorized, 8,192 components in length. A
typical climate change experiment consists of a simulation that starts from
conditions describing the state of the system at a pre-industrial time, chosen
often by convention as 1870, and run with only external forcing imposed,
otherwise in a self-consistent and isolated manner, until year 2100. External
forcings are meant to represent changing greenhouse gas concentrations over
time, aerosols, volcano eruptions and solar cycles. The system evolves (at a few
minutesâ€™ time step) according to the laws of physics known to govern climate
dynamics. To the extent permitted by their resolution, climate models represent
coastlines and topography, and they impose vegetation types, often evolving
along the simulation timeline, in order to represent urbanization, deforestation,
switches from natural vegetation to crop growing areas and vice versa. Even for
a given experiment (i.e. a given SRES scenario of external forcings) a model is
run for a handful of slightly different initial conditions. The trajectories of the
members of these single model ensembles give a handle on the characteriza-
tion of natural internal variability of the system, generated by the intrinsically
chaotic nature of weather and, in the long run, climate processes. For an
analysis of climate change, its signal has to be isolated and extracted from the
background noise of natural variability by averaging the members of the initial-
conditions ensemble, and by considering differences between two at least 20-
year averages, usually. The idea is that there is initial condition uncertainty to
take care of, and there are low frequency natural modes of variability (decadal
oscillations and other phenomena like the alternating phases of El NiÃ±o/La
NiÃ±a conditions in the Paciï¬c) that need to be averaged out before starting
to talk about anthropogenic (i.e. externally forced by human actions) climate
change.
Averaging in time is one way of facilitating the extraction of the signal of
change. Averaging over space is its natural counterpart. Local climate is noisy,
and climate models, because of their coarse resolution, are not expected to
reproduce faithfully the statistics of local climate. Even in the face of a constant
push to provide regionally detailed information from these models, it is fair to
say that our conï¬dence in their ability to simulate climate is highest when large
regions of sub-continental size are concerned, and their climate considered in
terms of large area-averages.

550
The Oxford Handbook of Applied Bayesian Analysis
20.3 Our simpliï¬ed datasets
In the following sections of the chapter we present a suite of statistical analyses
combining output from ensemble of climate models, from now on referred
to as GCMs (General Circulation Models). We will signiï¬cantly simplify
the problem by using summary statistics of their output, regional means of
temperature and precipitation, seasonally averaged and aggregated as 10 or 20
year means. We always condition our analysis to a given experiment, deï¬ned
in terms of the greenhouse gas emission scenario (SRES) used as part of the
external forcings by the simulation. For a given scenario a number of GCMs
have run simulations covering the period 1870 through 2100 and have archived
temperature and precipitation output. We also have observed records (in some
region of the world extending as far back) that can be used to gauge the GCM
ability to reproduce historic conditions. The model runs take account of changes
in forcings, including observed changes in emissions or solar output, but they
are not directly calibrated to observational climate data. One consequence of
this is that the model outputs include dynamical features such as El NiÃ±os,
but the El NiÃ±os of the model do not correspond in time to the El NiÃ±os of
the observational climate record. One reason for considering 20-year averages
is that over a 20-year time span, such short term ï¬‚uctuations are likely to
average out.
Suppose there are M GCMs, X j is a simulated value of some current climate
variable generated by GCM j, and Yj a simulated value of some future climate
variable generated by GCM j. We also have an observation X0 of the true
current climate, with its associated standard error ÃŽâˆ’1/2
0
that we can estimate
from the observationsâ€™ series and ï¬x in our model. In our typical application, X j
is the mean temperature or precipitation in a particular region for the period
1981â€“2000, X0 is the corresponding value calculated from the observational
climate record, and Yj is the corresponding temperature average calculated
from the 2081â€“2100 segment of the GCM simulation.
A modiï¬cation of this simple setup will involve R regional averages at a
time. Accordingly, we add a subscript i = 1, . . . , R to the variables, and consider
Xi j, Yi j, Xi0, ÃŽ0i.
Finally we will model the joint distribution of two variables, say temperature
and precipitation, for a given region and season, and over the entire length
of the simulation, as a bivariate time series of decadal averages. Accordingly
we will consider X jt, t = 1, . . . , 15 a bivariate vector of temperature and pre-
cipitation averages derived from the jth GCM output. Here the time index
corresponds to the decades centred at 1955, 1965, . . . , 2005, 2015, . . . , 2095,
so that both historical and future periods will be modelled jointly. Similarly,
Ot, t = 1, . . . 6 will indicate a two-component vector of observed temperature
and precipitation averages. In this case the time index, t, corresponds to the

The Uncertainty of Climate Change Projections
551
decades centered at 1955, 1965, . . . , 2005 (the last one at present is approxi-
mated by an estimate based on eight years of data only).
20.4 A hierarchy of statistical models
Our strategy in presenting our approach to multimodel synthesis and uncer-
tainty characterization is to start from a basic representation, highlight its
shortcomings and increase the complexity of the statistical treatment gradually
to account for as many additional details as possible. This way, we hope to
highlight issues, limitations and solutions step by step. Hopefully this will help
the reader achieve familiarity with the data and the ultimate goals of the analysis
that will engender his or her constructive criticisms and original thinking in the
face of this challenging application.
20.4.1 One region at a time
Let us then start from the simplest series of assumptions. We treat each region
separately, and we assume the following likelihood model: For j = 1, . . . , M,
X j âˆ¼N

Ã, ÃŽâˆ’1
j

Yj âˆ¼N

ÃŒ, (Ã‹ÃŽ j)âˆ’1
and for the observed mean climate variable (from now on we refer to it as
temperature, for simplicity),
X0 âˆ¼N

Ã, ÃŽâˆ’1
0

.
(20.1)
This likelihood model simply states that each GCM approximates the true
mean temperature of the region (Ã for current, ÃŒ for future climate) with a
Gaussian error, whose variance is model speciï¬c. The parameter Ã‹ allows for
the possibility that future precision will be different than current precision, by a
factor common to all GCMs (i.e. the likely degradation of the accuracy of future
projections affects all GCMs equally). Notice that this is an assumption dictated
by our data, which would not permit the identiï¬cation of a parameter modeling
GCM-speciï¬c change in precision. In fact the precision parameter ÃŽ j will be
estimated on the basis of the minimum necessary number of datapoints, two.
Also, the likelihood model assumes no correlation between the error of GCM j
in simulating current climate and its error in simulating future climate.
The use of uninformative improper priors for Ã and ÃŒ, U(âˆ’âˆž, +âˆž), and
proper but very diffuse Gamma priors for the precision parameters ÃŽj and Ã‹,
Ga(a, b), with a = b = 0.01 completes this basic model. A simple Gibbs sampler
can be used to explore the joint posterior distribution of the parameters (see
Appendix B1). This approach was ï¬rst presented in Tebaldi et al. (2004, 2005).

552
The Oxford Handbook of Applied Bayesian Analysis
Here we highlight a few features of the approximate posterior estimates for
the parameters Ã, ÃŒ and ÃŽ j. The form of the means of Ã and ÃŒ, given ÃŽ0, ÃŽ j, j =
1, . . . , M as well as given the Xs and Ys is respectively,
ËœÃ = ÃŽ0X0 +  ÃŽ j X j
ÃŽ0 +  ÃŽ j
and
ËœÃŒ =
 ÃŽ jYj
 ÃŽ j
.
The posterior distribution of ÃŽ j can be approximated by
ÃŽ j|rest âˆ¼Ga
8
a + 1, b + 1
2(X j âˆ’Ã)2 + Ã‹
2{Yj âˆ’ÃŒ}2
9
.
An easy addition to this model may accommodate the obvious critique that we
expect correlation between errors within a single GCMâ€™s simulations of current
and future climate. We can substitute to the likelihood of Yj the following:
Yj|X j âˆ¼N

ÃŒ + â€š(X j âˆ’Ã), (Ã‹ÃŽ j)âˆ’1
.
By so doing we estimate a possible correlation between X j and Yj. Here as
before with Ã‹ we are forced to assume a common correlation across all GCMs.
Perhaps this is too simplistic an assumption, but it is needed for identiï¬ability
of the parameter â€š. The posterior distribution of â€š will tell us if such assumption
is indeed substantiated by the data. The interpretation of the parameter is
particularly interesting if we note that â€š = 0 corresponds to X j and Yj being
independent, while â€š = 1 corresponds to X j and Yj âˆ’X j being independent,
conditionally on the other model parameters. We choose an uninformative prior
for the correlation parameter by assuming â€š âˆ¼U(âˆ’âˆž, +âˆž), and the Gibbs
sampler with an added step still converges to a stable set of posterior estimates
for all random variables.
We are being extremely liberal, here, letting each of the parameters have
their own diffuse prior. We will see in the application section that the result is
indeed a Markov Chain converging to a stationary distribution, but the posterior
distribution of temperature change appears extremely sensitive to the actual
location of the GCMsâ€™ projections in a way that is not easily supported by the
scientiï¬c understanding behind this application. The difï¬culty is illustrated
by Figure 20.1 where we show some posterior distributions of ÃŒ âˆ’Ã based
on actual climate models data, in comparison to two alternative approaches
introduced later. The current approach (dashed line in Figure 20.1) often shows
multimodal posterior distributions, with modes sometimes corresponding to
single climate modelsâ€™ projections (see also the original analysis in Tebaldi
et al. (2004, 2005), which is based on an older set of experiments, prepared

The Uncertainty of Climate Change Projections
553
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0
1
2
3
4
NAU, DJF
Density
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0.0
1.0
2.0
3.0
SAU, DJF
Density
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0.0
1.0
2.0
AMZ, DJF
Density
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0
1
2
3
4
CAM, DJF
Density
1
2
3
4
5
6
0.0
1.0
2.0
CAM, JJA
Density
1
2
3
4
5
6
0
1
2
3
4
ENA, JJA
Density
1
2
3
4
5
6
0.0
1.0
2.0
MED, JJA
Density
1
2
3
4
5
6
0.0 0.5 1.0 1.5 2.0
SAS, JJA
Density
Fig. 20.1 Posterior PDFs of temperature change for a set of four regions and two seasons (December
through January, DJF and June through August, JJA). Dashed line: simplest univariate model; dotted
line: univariate model with common prior for ÃŽ js; solid line: multivariate model.
for the third assessment report of the IPCC, published in 2001). However, we
would expect change in temperature over a given time period to have a smooth
distribution. This is the result of dealing with a limited set of climate models,
that happen to populate the interval between the extremes of their range very
sparsely, and of a statistical model that attributes an uneven set of weights to
the participating members of the ensemble, as the ÃŽ j parameters may actually
be interpreted according to the form of ËœÃ and ËœÃŒ.
Let us then formalize the fact that we do not expect such an uneven dis-
tribution of weights among the different modelsâ€™ precision parameters in this
ensemble. The natural way to do that is to use a hierarchical approach, and
hypothesize that all ÃŽ js are samples from the same prior distribution with

554
The Oxford Handbook of Applied Bayesian Analysis
parameters that we estimate in turn, as in
ÃŽ j âˆ¼Ga(aÃŽ, bÃŽ)
with aÃŽ, bÃŽ sampled from a Gamma prior, Ga(aâˆ—, bâˆ—) with aâˆ—= bâˆ—= 0.01.
This simple extra layer has the effect of smoothing out the estimates of ÃŽ js
and, accordingly, the shape of the posterior for ÃŒ âˆ’Ã. Thus, sensitivity of the
estimates to the precise location of X j and Yj values is signiï¬cantly diminished
as well, as Figure 20.1 demonstrates by comparing posterior estimates by the
ï¬rst model (dashed lines) to its hierarchical variation (dotted lines), for the same
values of the GCMsâ€™ Yj âˆ’X j. The only operational consequence is that the joint
posterior for this model is no longer estimated by a simple Gibbs sampler, but
a Metropolis â€“ Hastings step needs to handle the iterative simulation of aÃŽ, bÃŽ
(see Appendix B1). This modiï¬cation of the univariate approach was introduced
in Smith et al. (2009), but there too the application uses the older data set from
IPCC 2001.
20.4.2 Borrowing strength by combining projections over multiple regions
It is sensible to assume that the characteristics of each GCM, which in our
development so far were represented by the parameters ÃŽj, Ã‹ and â€š, could be
estimated by gathering information on the modelâ€™s simulations over a set of
regions, rather than just a single one. Consider then Xi j and Yi j, which in
addition to representing different models j = 1, . . . , M, also represent different
regions i = 1, . . . , R. We also have a set of Xi0, the current observed mean tem-
perature in region i which is an estimate of the true current mean temperature
with standard deviation ÃŽâˆ’1/2
0i
.
The likelihood model is a natural extension of the univariate case, where
Xi0 âˆ¼N

Ã0 + ÃŠi, ÃŽâˆ’1
0i

,
(ÃŽ0i known),
(20.2)
Xi j âˆ¼N

Ã0 + ÃŠi + Â· j, (Ãi jË†iÃŽ j)âˆ’1
,
(20.3)
Yi j|Xi j âˆ¼N

ÃŒ0 + ÃŠâ€²
i + Â·â€²
j + â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j), (Ãi jÃ‹iÃŽ j)âˆ’1
.
(20.4)
We choose joint prior densities as in:
Ã0, ÃŒ0, â€ši, â€š0, ÃŠi, ÃŠâ€²
i âˆ¼U(âˆ’âˆž, âˆž),
(20.5)
Ã‹i, Ë†i, Â¯0, Ã‹0, c, aÃŽ, bÃŽ âˆ¼Ga(a, b),
(20.6)
ÃŽ jâˆ¥aÃŽ, bÃŽ âˆ¼Ga(aÃŽ, bÃŽ)
(20.7)
Ãi jâˆ¥c âˆ¼Ga(c, c),
(20.8)
Â· j|Â¯0 âˆ¼N

0, Â¯âˆ’1
0

,
(20.9)
Â·â€²
j|Â· j, â€š0, Ã‹0, Â¯0 âˆ¼N

â€š0Â· j, (Ã‹0Â¯0)âˆ’1
,
(20.10)
all mutually independent unless explicitly indicated otherwise.

The Uncertainty of Climate Change Projections
555
This approach is presented in Smith et al. (2009). As can be noticed, there are
a few substantial changes from the model of Section 20.4.1: new parameters Â·j
and Â·â€²
j, ÃŠi and ÃŠâ€²
i are introduced in the mean components of the likelihood, and
the variances in (20.3) and (20.4) have a more complex structure.
The parameters Â· j and Â·â€²
j represent model biases. They are model-speciï¬c
quantities, but they are constant across regions, thus introducing correlation
between projections from the same model in different regions. By (20.10) we
introduce the possibility of a correlation between Â· j and Â·â€²
j, i.e. between the
bias in the current period of the simulation and the future period, using the
regression parameter â€š0. Similarly, ÃŠi and ÃŠâ€²
i represent region-speciï¬c mean
components. The idea is that different regions will tend to warm differently, and
this region effect will be common to all GCMs. We treat these two sets of mean
components in a fundamentally different manner by imposing on the Â· j, Â·â€²
j
Gaussian priors with mean zero, while letting the priors for ÃŠi, ÃŠâ€²
i be improper
priors, U(âˆ’âˆž, +âˆž). It is the difference between allowing for signiï¬cantly dif-
ferent region effects, unconstrained by one another, and imposing a shrinkage
effect across model biases according to the expectation that they should cancel
each other out.
The variances in (20.3) and (20.4) are modelled as three multiplicative fac-
tors, one model-speciï¬c, one region-speciï¬c, one allowing for an interaction.
In the case where Ãi j â‰¡1 (a limiting case of (20.8) in which c â†’âˆž) the
variance factorizes, with ÃŽ j representing a model reliability and either Ë†i or
Ã‹i a region reliability. (We note here that two different parameters for current
and future simulations of a given region have the same effect of what the
model in Section 20.4.1 accomplished by using the parameter Ã‹.) Compared
with ï¬tting a separate univariate model to each region, there are many fewer
parameters to estimate, so we should get much improved precision. However
there is a disadvantage to this approach: if model A has higher reliability than
model B for one region, then it will for all regions (and with the same ratio of
reliabilities). This is contrary to our experience with climate models, where it is
often found that a modelâ€™s good performance in one region is no guarantee of
good performance in another. The parameter Ãi j, then, may be thought of as an
interaction parameter that allows for the relative reliabilities of different models
to be different in different regions. As with the other reliability parameters,
we assume a prior Gamma distribution, and there is no loss of generality in
forcing that Gamma distribution to have mean 1, so we set the shape and scale
parameters both equal to some number c that has to be speciï¬ed. However, if
c is too close to 0, then the model is in effect no different from the univariate
model, allowing the variances for different models in different regions to be
completely unconstrained. Our recommendation is that c should be chosen not
too close to either 0 or âˆž, to represent some reasonable judgment about the
strength of this interaction term. A different solution will be to let c be a random

556
The Oxford Handbook of Applied Bayesian Analysis
variable, and let the data give us an estimate of it. We can give c a diffuse prior
distribution, as with c âˆ¼Ga(0.01, 0.01).
The Gibbs sampler, with a Metropolis â€“ Hastings steps to sample values for
c, is described in detail in Appendix B.2. Together with the rest of the discussion
of the multivariate approach, it was ï¬rst detailed in Smith et al. (2009).
20.4.3 A bivariate, region-speciï¬c model
The statistical treatment described in the preceding section can be applied
separately to temperature and precipitation means. The latter does not need
to be logarithmically transformed. In our experience, since the data points are
seasonal, regional and multidecadal means, the Gaussian likelihood model ï¬ts
the data without need for transformations. Many studies of impacts of climate
change are predicated on the availability of joint projections of temperature
and precipitation change. A warmer, wetter future is very different from a
warmer, drier one if you are dealing with adaptation of agricultural practices,
or water resources management (Groves et al., 2008). Here we propose a joint
model for the two climate variables at the individual regional level that we
ï¬rst introduced in Tebaldi and SansÃ³ (2009). Rather than simply extending
the set up of Section 20.4.1, we consider decadal means covering the entire
observed record and the entire simulation length. These additional data points
will help estimate the correlation coefï¬cients. We will model trends underlying
these decadal mean time series, and estimate a correlation parameter between
temperature and precipitation, once the trend is accounted for.
Here are the new assumptions:
r The vector of observed values Ot is a noisy version of the underlying
temperature and precipitation process, with correlated Gaussian noise (we
estimate the correlation from the data, through the estimation of the
parameter â€šxo).
r The true process is piecewise linear, for both temperature and precipitation.
We ï¬x the â€˜elbowâ€™ at year 2000, which may allow for future trends steeper
than the observed ones. Of course a slightly more general model could use
a random change point approach, but given the coarse resolution of our
time dimension and the limited amount of data at our disposal we choose
to ï¬x the change point.
r The model output X jt is a biased and noisy version of the truth. We assume
an additive bias and a bivariate Gaussian noise.
r We expect the model biases to be related across the population of models,
i.e. we impose a common prior, and we estimate its mean parameter, so that
we may determine an overall bias for the ensemble of model simulations,
different from zero.

The Uncertainty of Climate Change Projections
557
In the notation, superscripts T and P refer to the temperature and precipitation
components of the vectors. Thus, the likelihood of the data is:
OT
t âˆ¼N

ÃT
t ; ÃT
for t = 1, . . . , Ã™0
O P
t |OT
t âˆ¼N

ÃP
t + â€šxo

OT
t âˆ’ÃT
t

; ÃP
for t = 1, . . . , Ã™0
(20.11)
where
â€šxo âˆ¼N (â€š0, ÃŽo) ,
X T
jt âˆ¼N

ÃT
t + d T
j ; Ã“T
j

for t = 1, . . . , Ã™âˆ—and j = 1, . . . , M
X P
jt|X T
jt âˆ¼N

ÃP
t + â€šxj

XT
jt âˆ’ÃT
t âˆ’dT
j

+ d P
j ; Ã“P
j

for t = 1, . . . , Ã™âˆ—and j = 1, . . . , M.
In equations (20.11) we specify bivariate normal distributions for Ot and X jt
using conditionality. After accounting for the underlying trends and biases
terms, â€šx1, . . . , â€šxM are used to model the correlation between temperature
and precipitation in the climate model simulations, while â€šxo is ï¬xed at the
value estimated through the observed record. Also in the likelihood of the
observations, ÃT and ÃP are ï¬xed to their empirical estimates.
The time evolution of the true climate process Ãâ€²
t = (ÃT
t , ÃP
t ), consists of a piece-
wise linear trend in both components:

ÃT
t
ÃP
t

â‰¡

Â·T + â€šTt + â€žT(t âˆ’Ã™0)I{tâ‰¥Ã™0}
Â·P + â€šPt + â€žP(t âˆ’Ã™0)I{tâ‰¥Ã™0}

.
(20.12)
The priors for the parameters in model (20.11) are speciï¬ed hierarchically by
assuming that
â€šxj âˆ¼N (â€š0, ÃŽB) ,
d T
j âˆ¼N

aT; ÃŽT
D

,
d P
j âˆ¼N

a P; ÃŽP
D

for j = 1, . . . , M
Ã“T
j âˆ¼Ga

aÃ“T, bÃ“T
and
Ã“P
j âˆ¼Ga

aÃ“P, bÃ“P
.
ÃŽo is ï¬xed to a value estimated on the basis of the observed record.
All the other quantities are assigned uninformative priors:
â€š0, aT, a P âˆ¼U(âˆ’âˆž, +âˆž)

558
The Oxford Handbook of Applied Bayesian Analysis
and
ÃŽB, ÃŽT
D, ÃŽP
D, aÃ“T, bÃ“T, aÃ“P, bÃ“P âˆ¼Ga(g, h),
where g = h = 0.01. Similarly, for the parameters in (20.12), we assume
Â·T, â€šT, â€žT, Â·P, â€šP, â€žP âˆ¼U(âˆ’âˆž, +âˆž).
We are assuming that each climate model has its own precision in simulating
the true temperature and precipitation time series, but we impose common
priors to Ã“T
j and Ã“P
j âˆ€j, whose parameters are in turn estimated by the data.
As we discussed in Section 20.4.1, this choice produces more robust estimates
of the relative precisions of the different GCMs, not overly sensitive to small
perturbations in the GCM trajectories.
The model-speciï¬c bias terms dT
j , d P
j are assumed constant over the length
of the simulation. They model systematic errors in each GCM simulated vari-
able. All the GCM biases for temperature, like all GCM biases for precipitation,
are realization from a common Gaussian distribution, whose mean (aT or a P),
as mentioned, may be different from zero, when the set of model trajectories is
distributed around the truth non-symmetrically. We do not expect a systematic
behaviour across models when it comes to precipitation versus temperature
biases, that is, we do not expect that models having relatively larger temperature
biases would show relatively larger precipitation biases, so we do not model
a correlation structure between dT
j , d P
j . In fact, this correlation structure, if
there at all, would not to be identiï¬able/separable from the correlation modeled
through â€šxo, â€šx1, . . . , â€šxM, given the conï¬guration of the present dataset. Notice
that the correlation coefï¬cients, â€šxo and â€šxj also have a common mean, â€š0 pos-
sibly different from zero and that will be heavily inï¬‚uenced by the value of the
observed correlation coefï¬cient, â€šxo. All the remaining parameters of the model
have non-informative, conjugate distributions. Notice that we use improper
priors for the location parameters of the Gaussian distributions and linear
regression parameters in the correlation structure and in the trend structure,
and proper but diffuse priors for the precision parameters and as hyper-priors
of the Ã“. parameters. The likelihood and priors form a conjugate model, and as
before a Gibbs sampler can be programmed to explore the posterior distribu-
tions for this model, with a Metropolis â€“ Hastings step used to generate sample
values for aÃ“T, bÃ“T, aÃ“P, bÃ“P. See Appendix B.3 details of the Markov chain Monte
Carlo implementation, or the original article Tebaldi and SansÃ³ (2009).
20.5 Validating the statistical models
Climate predictions are a fairly safe business to be in: validation of the forecast
comes about no sooner than every 10 or 20 years! We have already commented

The Uncertainty of Climate Change Projections
559
on how this situation sets climate forecasting well apart from weather fore-
casting, where statistical calibration of numerical models can be ï¬ne tuned
continuously, using diagnostics of their performance coming at the steady and
frequent rate of every six hours or so. The work by Adrian Raftery, Tilmann
Gneiting and colleagues (Raftery et al. (2005) and references therein) has set
the standard for Bayesian statistical methods brought to bear on ensemble
weather forecast calibration and validation. In our application, we are also
forecasting conditionally on a speciï¬c scenario of greenhouse gases emissions,
which is always an idealized scenario. This discussion shows why we are not
going to rely on observations to validate our statistical approaches. Rather,
we validate our statistical assumptions by performing cross-validation. In all
cases but for the simplest univariate model introduced ï¬rst, we can estimate a
posterior predictive distribution for a new modelâ€™s trajectory or change. We can
therefore compare the left-out model projections with their posterior-predictive
distribution. In both cases we can do this exercise over all possible models,
regions and â€“ separately or jointly â€“ both climate variables, temperature and
precipitation averages. Statistical theory provides us with null hypotheses to
test about the expected distribution of the values of P(xâˆ—) where xâˆ—is the left-
out value and P() is the posterior predictive cumulative distribution function of
that quantity. We expect the values of P(xâˆ—) obtained by performing the cross-
validation exercise across models and across regions to be a sample from a
uniform distribution. In all cases, after conducting the tests, we do not reject the
hypothesis more frequently that what would be expected as a result of multiple
testing. In the remainder of this section we detail the procedure of the cross-
validation exercise, for each of the statistical models proposed. More details and
actual results from the cross validation exercises can be found in Smith et al.
(2009) and Tebaldi and SansÃ³ (2009).
20.5.1 Univariate model with hyperprior on precision parameters
The predictive distribution can be calculated under the assumption that
the climate models are exchangeable. Conditionally on the hyperparameters
Ã, ÃŒ, â€š, Ã‹, aÃŽ and bÃŽ, the distribution of YM+1 âˆ’XM+1 can be derived derived
from conditioning on ÃŽM+1 âˆ¼Ga(aÃŽ, bÃŽ), since, then, YM+1 âˆ’X M+1|ÃŽM+1 âˆ¼
N(ÃŒ âˆ’Ã, {(â€š âˆ’1)2 + Ã‹âˆ’1}ÃŽâˆ’1
M+1).
The conditional distribution should then be convolved with the joint posterior
distribution of (Ã, ÃŒ, â€š, Ã‹, aÃŽ, bÃŽ), to obtain the full posterior predictive distribu-
tion. In practice we can carry out this integration within the Gibbs â€“ Metropolis
algorithm by
1. Sampling at each step n the hyperparameter values a(n)
ÃŽ , b(n)
ÃŽ , ÃŒ(n), Ã(n),
â€š(n), Ã‹(n), corresponding to one draw from their joint posterior distribution.

560
The Oxford Handbook of Applied Bayesian Analysis
A draw of a random ÃŽ j,n âˆ¼Ga(a(n)
ÃŽ , b(n)
ÃŽ ) can be generated and the statistic
U(n)
j
= 
âŽ§
âŽªâŽªâŽªâŽªâŽ¨
âŽªâŽªâŽªâŽªâŽ©
Yj âˆ’X j âˆ’ÃŒ(n) + Ã(n)
E
â€š(n)
x âˆ’1
2
+ Ã‹(n)âˆ’1

(ÃŽ j,n)âˆ’1
âŽ«
âŽªâŽªâŽªâŽªâŽ¬
âŽªâŽªâŽªâŽªâŽ­
calculated.
2. Over all n iterations we can compute Uj, the mean value of U(n)
j , repre-
senting an estimate of the predictive distribution function evaluated at the
true Yj âˆ’X j. If the statistical model is consistent with the data, Uj should
have a uniform distribution on (0, 1).
3. By computing Uj for each region, and each GCM we have a set of
test statistics that we can evaluate for discrepancies, applying tests of ï¬t
to evaluate the hypothesis that the values are samples from a uniform
distribution.
20.5.2 Multivariate model, treating multiple regions at once
The procedure for cross-validation in this model is very similar to what was just
described. In this setting we do cross-validation of the variable Yi j âˆ’Xi j, model
jâ€™s projected change in region i. The full conditional, predictive distribution of
this quantity is
Yi j âˆ’Xi j|rest âˆ¼N

ÃŒ0 âˆ’Ã0 + ÃŠâ€²
i âˆ’ÃŠi + Â·â€²
j âˆ’Â· j,
1
Ãi jÃŽ j
(â€ši âˆ’1)2
Ë†i
+ 1
Ã‹i

.
The implementation of the cross-validation exercise within the Gibbs â€“
Metropolis algorithm is as follows:
1. We leave model j out and we run the Gibbs â€“ Metropolis simulation.
2. After burn-in, at every step n, on the basis of the set of parameters cur-
rently sampled we generate corresponding values of ÃŽ(n)
j , Â·(n)
j , Â·â€²(n)
j
and Ã(n)
i j
as
ÃŽ(n)
j
âˆ¼Ga

a(n)
ÃŽ , b(n)
ÃŽ

Â·(n)
j
âˆ¼N

0, 1
Â¯(n)
0

Â·â€²(n)
j
âˆ¼N

â€š(n)
0 Â·(n)
j ,
1
Â¯(n)
0 Ã‹(n)
0

,
Ã(n)
i j âˆ¼Ga

c(n), c(n)
.

The Uncertainty of Climate Change Projections
561
3. From these values we compute the statistic
Ui j = 1
N
N

n=1

âŽ¡
âŽ¢âŽ¢âŽ¢âŽ¢âŽ£
Yi j âˆ’Xi j âˆ’

ÃŒ(n)
0 âˆ’Ã(n)
0

âˆ’

ÃŠâ€²(n)
i
âˆ’ÃŠ(n)
i

âˆ’

Â·â€²(n)
j
âˆ’Â·(n)
j

E
ÃŽ(n)
j Ã(n)
i j
âˆ’1 
Ë†(n)
i
âˆ’1 
â€š(n)
i
âˆ’1
2
+

Ã‹(n)
i
âˆ’1
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¥âŽ¦
.
(20.13)
by generating a sample value at each iteration and averaging them at the
completion of the simulation.
4. As with the univariate analysis, we then perform various goodness of ï¬t
tests on the statistics Ui j. If the model is a good ï¬t, they should be consis-
tent with independent draws from the uniform distribution on [0, 1].
20.5.3 Bivariate model of joint temperature and precipitation projections
After leaving each individual GCMâ€™s trajectories out in turn, we simplify the
validation of the bivariate time series model by computing three marginal
bivariate predictive distributions, one for current climate (deï¬ned as the bivari-
ate distribution of average values of temperature and precipitation for the
period 1981â€“2000), one for future climate (deï¬ned as the corresponding dis-
tribution for average values over the period 2081â€“2100) and one for climate
change (deï¬ned as the joint distribution of the temperature and precipitation
differences between the two same periods). We can then compute the two sets
of pairs (U1 = PT(XT
âˆ—= xT
âˆ—), U2 = PP|T(X P
âˆ—= xP
âˆ—)) for both current and future
time windows, and the pair (U1 = PT(XT
âˆ—= xT
âˆ—), U2 = PP|T(X P
âˆ—= xP
âˆ—))
where the ï¬rst univariate distribution function is simply the marginal predictive
distribution of temperature change, while the second distribution function is
the predictive distribution of precipitation change, conditional on the corre-
sponding simulated temperature change. Finally we test the null hypothesis
that the pairs of (z1 j, z2 j) for j = 1, . . . , M are independent and identically
distributed random variates, sampled from uniform distributions on the (0, 1)
interval. The form of the posterior predictive and the way we simulate the U
statistics is similar to what we described in the previous subsections, with all
the parameters being sampled from the estimated value of the hyperparameters
within the Gibbs sampler, and the conditional marginal predictive distributions
having a manageable Gaussian form.
20.5.4 U-statistics and goodness-of-ï¬t tests
Following Smith et al. (2009), in all cases described in Sections 20.5.1
through 20.5.3, the sets of Ui j (or Zj) can be tested for goodness of ï¬t with
respect to a uniform distribution and independence by traditional tests like

562
The Oxford Handbook of Applied Bayesian Analysis
Kolmogorov â€“ Smirnov, CramÃ©r â€“ von Mises and Anderson â€“ Darling. Smith
et al. (2009) also proposes the use of a correlation test where the ordered
values of Ui j, j = 1, . . . , M are correlated to the sequence 1/(M + 1), 2/(M +
1), . . . , M/(M + 1) and large values of the correlation coefï¬cient (or small val-
ues of 1 âˆ’cor) indicate a close ï¬t. In most cases the goodness-of-ï¬t tests result
in acceptance of the null hypothesis that the Ui j or Zj are independent within
each region i.
20.6 Application: The latest model projections, and their synthesis
through our Bayesian statistical models
The website of PCMDI provides instructions for the download of model output
from all the GCMs that have contributed their simulations to the IPCC-AR4
effort. We choose a set of experiments run under a scenario of greenhouse
gas emissions that can be thought of as a â€˜business-as-usualâ€™ scenario, where
concentrations of greenhouse gases increase over this century at a rate similar
to what is being emitted currently, worldwide. We extract average temperature
and precipitation projections from seventeen models. We area-average their
output over standard subcontinental regions that have been used by IPCC and
many studies in the literature (Giorgi and Francisco, 2000), see Figure 20.2. We
consider two seasonal averages, December through February (DJF) and June
90N
ALA
WNA
CNA
CAM
AMZ
SSA
WAF
SAH
MED
NEU
CAS
TIB
SAS
NAS
EAS
NAU
SAU
SEA
EAF
SAF
GRL
ENA
60N
30N
EQ
30S
60S
90S
120W
60W
0
60E
120E
180
Fig. 20.2 The 22 Giorgi regions. Output of temperature and precipitation for each GCM and obser-
vations is area-averaged over each of these large regions, for a given season and (multi-) decadal
periods.

The Uncertainty of Climate Change Projections
563
through August (JJA). For the models treating temperature or precipitation
separately we also average 20-year periods, 1980â€“1999 as the current climate
averages and 2080â€“2099 as the future. For the model that estimates the joint
distribution of temperature and precipitation we consider time series of 15
decadal averages covering the period 1950â€“2100. We compute the joint posterior
distribution of all random parameters according to the univariate (one region at
a time, with or without hyperprior over the precision parameters), multivariate
(all regions at the same time) and bivariate (temperature and precipitation
jointly) models. We then proceed to display and compare results for some
quantities of interest. Most importantly, we consider changes in temperature
(and precipitation) for all the regions, across the different statistical treatment.
We also show posterior distributions of other parameters of interest, like model
biases and interaction effects.
20.6.1 Changes in temperature, univariate and multivariate model
The eight panels of Figure 20.1 compare the posterior distribution of temper-
ature change ÃŒ âˆ’Ã for the two univariate models (dashed and dotted curves)
and ÃŒ0 + ÃŠâ€²
i âˆ’Ã0 âˆ’ÃŠi for the multivariate model (solid line) for a group of four
regions in DJF and a different group of four regions in JJA. We have already
mentioned in Section 20.4.1 that the ï¬rst model, where the GCM-speciï¬c preci-
sion parameters are each a sample from a diffuse Gamma prior, suffers from an
uneven distribution of â€˜weightâ€™ among GCMsâ€™ projections. The multimodality
of some of the PDFs in Figure 20.1 is an indication of this behaviour. And even
more generally, the fact that these PDFs are often shifted signiï¬cantly from the
location of the ensemble mean (indicated by a cross along the x-axis, whereas
the individual GCMs are marked by dots along the basis of the curves) is the
effect of some of the GCMs â€˜stealing the showâ€™, i.e. being attributed a much
larger precision than others, where we would not expect such difference in
relative importance among this family of state-of-the-art climate models. The
univariate model with a common prior over the ÃŽjs produces smoother, better
centered PDFs that are not signiï¬cantly different from those produced by the
multivariate model (comparing the dashed and solid lines). The two series of
boxplots (one for DJF, to the left and one for JJA, to the right) in Figure 20.3 offer
a comparison of the three distributions for each region/season combination.
For each region, the ï¬rst boxplot from the bottom shows interquartile range,
median and 5thâ€“95th quantiles of the posterior distribution estimated by the
unconstrained version of the univariate model, the second and third boxplot
show the extent of the posterior for the univariate model with common prior
for the ÃŽ js and the multivariate model. These two sets of boxplots conï¬rm
the results of Figure 20.1, with the position of the ï¬rst boxplot in each triplet
often shifted away from the other two, which are more similar to one another.

564
The Oxford Handbook of Applied Bayesian Analysis
Temperature change, DJF
NAU
SAU
AMZ
SSA
CAM
WNA
CNA
ENA
ALA
GRL
MED
NEU
WAF
EAF
SAF
SAH
SEA
EAS
SAS
CAS
TIB
NAS
Temperature change, JJA
2
3
4
5
6
7
8
9
2
3
4
5
NAU
SAU
AMZ
SSA
CAM
WNA
CNA
ENA
ALA
GRL
MED
NEU
WAF
EAF
SAF
SAH
SEA
EAS
SAS
CAS
TIB
NAS
Fig. 20.3 Posterior distributions of temperature change for the 22 regions of Figure 20.2 in DJF (left
panel) and JJA (right panel). For each region we show three boxplots corresponding to the posterior
distributions derived from the three statistical models represented also in Figure 20.1. The labelling
along the vertical axis and the horizontal lines in the plot region identify each region by the acronyms
of Figure 20.2. For each region, the lower boxplot, lightest in colour, corresponds to the simplest
univariate model. The middle boxplot corresponds to the univariate model with hyperprior on the
reliability parameters. The third boxplot, darkest in colour corresponds to the multivariate model.
This display of all the 22 region is also indicative of the large differences in
the amount of warming across regions. This justiï¬es the choice of modeling
region-speciï¬c parameters in the mean component of the likelihood of the
multivariate model, ÃŠi, ÃŠâ€²
i, with a Uniform prior over the real line that does not
have a shrinkage effect towards zero.
20.6.2 Changes in temperature, multivariate and bivariate model
We now take the same eight region/season combinations of Figure 20.1 and
compare, in Figure 20.4, the posterior distribution of temperature change for
the multivariate model, ÃŒ0 + ÃŠâ€²
i âˆ’Ã0 âˆ’ÃŠi, (solid line) to the marginal posterior
distribution of the temperature change signal from the bivariate model (dotted
line). We also show the posterior predictive distribution of a new unbiased
GCMâ€™s projection of temperature change (dashed line) in the same panels.
We note immediately that the posterior distribution from the bivariate model
is much narrower than the posterior from the multivariate model. This is no
surprise, given that we are using a much richer dataset (six observed decades,
15 simulated decades for each of seventeen GCMs) and we are hypothesizing

The Uncertainty of Climate Change Projections
565
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0
1
2
3
4
5
NAU, DJF
Density
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0
2
4
6
8 10
SAU, DJF
Density
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0
2
4
6
8
AMZ, DJF
Density
1.5
2.0
2.5
3.0
3.5
4.0
4.5
0
2
4
6
8
CAM, DJF
Density
1
2
3
4
5
6
0
1
2
3
4
CAM, JJA
Density
1
2
3
4
5
6
0
1
2
3
4
ENA, JJA
Density
1
2
3
4
5
6
0.0
1.0
2.0
MED, JJA
Density
1
2
3
4
5
6
0 1 2 3 4 5
SAS, JJA
Density
Fig. 20.4 Similar to Figure 20.1, but now the three PDFs are the posterior from the multivariate
model (solid line), the posterior from the bivariate model (dotted line) and the posterior predic-
tive from the bivariate model (dashed line). The curves drawn as solid lines are the same as in
Figure 20.1.
a piecewise linear function of time as the trajectory of mean temperature,
thus we are in fact estimating only three parameters Â·T, â€šT and â€žT. Note
that the joint posterior distribution from the bivariate model is a probabilistic
representation of the entire trajectory of temperature (ad precipitation) over the
decades (ÃT
t ) but the availability of the Gibbs sample makes it straightforward
to compute any deterministic function of it (in this case the difference between
two bidecadal means). The other obvious feature in the majority of these panels
is that the posterior distributions from the bivariate model are signiï¬cantly
shifted from the centre of the multivariate modelâ€™s PDFs. Remember that the
bivariate model is not simply â€˜smoothingâ€™ sets of GCM â€˜snapshotsâ€™ in terms
of multidecadal averages, but rather ï¬tting a trend to their whole trajectories,

566
The Oxford Handbook of Applied Bayesian Analysis
anchoring its estimation to the observed series. We are thus comparing two
different deï¬nitions of â€˜temperature changeâ€™, at least in terms of how the
statistical model is estimating it. Likely, some of the models produce steeper
trends than the underlying trend estimated from the entire ensemble, thus
ï¬lling the distribution of the â€˜snapshotsâ€™ estimate to the right of the trend
estimates, and shifting the mean accordingly. This feature may be scenario
dependent, with lighter emission scenarios (forcing less of an acceleration in
the trends) facilitating a better agreement between the two methodsâ€™ results.
The dashed curves in each ï¬gure are posterior predictive distribution of a
new GCMâ€™s projection. The signal of temperature (change) underlying truth
and model simulation is an abstract concept. Even the observations are a
noisy representation of this signal. We could be justiï¬ed then if we thought
of a model trajectory as a possible future path for our climate, and accord-
ingly we represented the uncertainty in this future projection by the posterior
predictive distribution of a new GCM, whose width is of the same order of
magnitude as the range of model projections, rather than being an inverse
function of the square root of the number of data points, as the posterior
distribution width is. In Figure 20.5 we complete the representation of prob-
abilistic projections of climate change by showing contours of the posterior
(tighter set of contours in each panel) and posterior predictive (wider set of
contours in each panel) for the same set of region/season pairs of Figures 20.1
and 20.4.
20.6.3 Other uncertain quantities of interest, their full probabilistic characterization
Even the simple univariate treatment, after imposing the regularizing common
prior over the ÃŽ js, offers additional information besides the posterior PDFs of
temperature change that may be used in applications of impact analysis. Pos-
terior means of ÃŽ js can be regarded as optimal estimates of model reliabilities,
and utilized as â€˜weightsâ€™ for some further analysis of climate change impacts.
For example, Fowler et al. (2007) shows how the normalized set of reliabilities
can be used for a Monte Carlo simulation of weather scenarios at regional
scales by stochastic weather generator models â€˜drivenâ€™ by different GCMs. The
larger the GCM reliability, the larger the number of weather scenarios one
wants to generate according to that GCM, and the total number of Monte Carlo
simulations from the weather generator may be divided up proportionally to the
normalized weights. In Figure 20.6 we show boxplots of the 16 GCM-speciï¬c
ÃŽ jâ€™s for the eight region/season combinations of Figures 20.1 and 20.4, accord-
ing to the univariate treatment, displaying them in each panel in increasing
order from left to right according to their posterior means. From the ï¬gure
we note how the distribution of â€˜weightâ€™ among GCMs is very balanced, with
no disproportionate attribution to a small subset of models. It is true however

The Uncertainty of Climate Change Projections
567
NAU, DJF
Temp. change, degrees C
% Precip change
 0.005 
 0.01 
 0.015 
 0.02 
 0.025 
 0.03 
 0.04 
1.0
1.5
2.0
2.5
3.0
3.5
4.0
â€“20
0 10
30
 0.1 
 0.2 
SAU, DJF
Temp. change, degrees C
% Precip change
 0.005 
 0.01 
 0.015 
 0.02 
1.0
1.5
2.0
2.5
3.0
3.5
4.0
â€“60
â€“20 0
20
AMZ, DJF
Temp. change, degrees C
% Precip change
 0.02 
 0.04 
 0.06 
 0.08 
 0.1 
 0.16 
1.0
1.5
2.0
2.5
3.0
3.5
4.0
â€“5
0
5
10 15
 0.5 
CAM, DJF
Temp. change, degrees C
% Precip change
 0.01 
 0.02 
 0.03 
 0.04 
 0.05 
1.0
1.5
2.0
2.5
3.0
3.5
4.0
â€“40
â€“20
0
 0.2 
CAM, JJA
Temp. change, degrees C
% Precip change
 0.02 
 0.04 
 0.06 
 0.08 
 0.1 
1
2
3
4
5
â€“20
â€“10 â€“5
0
 0.2 
 1 
ENA, JJA
Temp. change, degrees C
% Precip change
 0.01 
 0.02 
 0.03 
 0.04 
 0.05 
 0.06 
 0.07 
 0.09 
1
2
3
4
5
â€“5
0
5
10
 0.5 
MED, JJA
Temp. change, degrees C
% Precip change
 0.005 
 0.01 
 0.015 
 0.02 
 0.025 
 0.03 
1
2
3
4
5
â€“40
â€“20
0
 0.05 
 0.1 
SAS, JJA
Temp. change, degrees C
% Precip change
 0.01 
 0.02 
 0.03 
 0.04 
 0.05 
 0.06 
 0.07 
 0.09 
1
2
3
4
5
0
5 10 15
 0.2 
Fig. 20.5 Contours of joint changes in temperature and precipitation (the latter as percentage of
current average precipitation) for the same set of regions/seasons as in Figures 20.1 and 20.4.
The tight contours correspond to the posterior PDF from the bivariate model. The wider contors
represent the posterior predictive PDFs for the projections of a new GCM, unbiased.
that different models gain different ranking depending on the region and
season combination. This is consistent with the shared opinion among climate
scientists and modelers that no model outperforms all others in every respect,
different models showing different strengths in the simulation of regional
climates, seasonal processes and so on.
We may be interested in evaluating the posterior estimates of the model-
speciï¬c mean factors in the multivariate treatment, Â·j and Â·â€²
j, as they may be
interpreted as global average biases that a speciï¬c GCM imposes on its simula-
tion of temperature. Figure 20.7 shows boxplots of the two sets of parameters

568
The Oxford Handbook of Applied Bayesian Analysis
giss.a
miroc.h
ipsl
echam
gfdl1
pcm
cnrm
mri
iap
ccsm
gfdl0
miroc.m
giss.eh
inmcm3
hadcm3
giss.er
0.0
0.5
1.0
1.5
2.0
2.5
3.0
NAU, DJF 
GCM reliabilities
ipsl
miroc.h
iap
inmcm3
giss.a
echam
miroc.m
giss.eh
gfdl1
hadcm3
cnrm
mri
ccsm
pcm
gfdl0
giss.er
0.0
0.5
1.0
1.5
SAU, DJF 
GCM reliabilities
ipsl
hadcm3
miroc.h
miroc.m
pcm
cnrm
mri
giss.eh
echam
giss.a
giss.er
inmcm3
iap
gfdl1
ccsm
gfdl0
0
1
2
3
4
5
AMZ, DJF 
GCM reliabilities
miroc.h
pcm
giss.eh
echam
gfdl1
gfdl0
miroc.m
giss.a
ipsl
iap
mri
hadcm3
giss.er
cnrm
ccsm
inmcm3
0.0
0.5
1.0
1.5
2.0
CAM, DJF 
GCM reliabilities
giss.eh
miroc.h
cnrm
miroc.m
pcm
mri
hadcm3
echam
giss.a
gfdl0
ipsl
iap
ccsm
giss.er
inmcm3
gfdl1
0.0
0.5
1.0
1.5
2.0
2.5
3.0
CAM, JJA 
GCM reliabilities
gfdl0
miroc.m
miroc.h
hadcm3
giss.er
gfdl1
giss.a
giss.eh
pcm
ipsl
echam
mri
ccsm
iap
inmcm3
cnrm
0.0
0.5
1.0
1.5
2.0
2.5
ENA, JJA 
GCM reliabilities
giss.eh
pcm
miroc.h
giss.a
hadcm3
gfdl0
mri
miroc.m
iap
giss.er
gfdl1
inmcm3
ccsm
ipsl
echam
cnrm
0.0
0.5
1.0
1.5
MED, JJA 
GCM reliabilities
giss.a
pcm
ipsl
miroc.h
cnrm
iap
gfdl1
echam
hadcm3
ccsm
mri
giss.er
gfdl0
inmcm3
giss.eh
miroc.m
0.0
0.5
1.0
1.5
2.0
SAS, JJA 
GCM reliabilities
Fig. 20.6 Posterior PDFs for the model reliability parameters ÃŽ js derived from the univariate model
with common prior on them. The region/season combinations are the same as in Figures 20.1
and 20.4. Each boxplot in a given panel corresponds to a GCM. Relatively larger values of ÃŽ j are
interpretable as better model reliability in the given region/season.

The Uncertainty of Climate Change Projections
569
pcm
gfdl0
cnrm
gfdl1
inmcm3
miroc.m
ccsm
giss.er
hadcm3
miroc.h
mri
ipsl
iap
echam
giss.a
giss.eh
Model effect 
(current climate simulations)
pcm
gfdl0
cnrm
gfdl1
inmcm3
miroc.m
ccsm
giss.er
hadcm3
miroc.h
mri
ipsl
iap
echam
giss.a
giss.eh
â€“3
â€“2
â€“1
0
1
2
3
â€“3
â€“2
â€“1
0
1
2
3
Model effect 
(future climate simulations)
Fig. 20.7 Posterior distributions of model-speciï¬c mean effects in the multivariate model. The left
panel shows boxplots for the Â· js, interpretable as global average biases in each modelâ€™s current
climate simulation. The corresponding distributions of the Â·â€²
j s, interpretable as future simulation
biases, are shown in the panel on the right.
for the simulation of temperature in DJF, as an example. The models have
been ordered with respect to the value of the posterior mean of the respective
parameter in the current climate simulation (Â· j). As can be assessed by the two
series of boxplots, there are models with biases that are signiï¬cantly different
from zero, mostly with negative (cold) biases in the left plot. Many more GCMs
show signiï¬cant biases in the future part of the simulation, on both sides of the
zero line.
Another interesting question that can be answered through the computation
of the marginal posterior distribution of a set of parameters in the multivariate

570
The Oxford Handbook of Applied Bayesian Analysis
cnrm
gfdl0
gfdl1
giss.a
giss.eh
giss.er
iap
inmcm3
ipsl
miroc.h
miroc.m
echam
mri
ccsm
pcm
hadcm3
Interaction terms,
averaged over regions
NAU
SAU
AMZ
SSA
CAM
WNA
CNA
ENA
ALA
GRL
MED
NEU
WAF
EAF
SAF
SAH
SEA
EAS
SAS
CAS
TIB
NAS
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
Interaction terms,
averaged over models
Fig. 20.8 Interaction effect parameters (Ãi j) of the precisions in the likelihood of the multivariate
model. Left panel shows their posterior distributions integrated over regions (Ã. j). Right panel shows
the distributions integrated over models (Ãi.).
model regards the form of the precision (or variance) components in the like-
lihood of the GCM simulated temperatures. The parameters Ãi js are let free to
assume values signiï¬cantly different from one, if an interaction between model-
speciï¬c and region-speciï¬c behaviour is deemed necessary to ï¬t the dataset
at hand. As Figure 20.8 demonstrates, however, by showing the distributions
of Ãi. and Ã. j, i.e. by combining all sample values in the Gibbs simulation
across models and across regions, the distribution of these parameters is always
centered around the value one, lending support to a model that factorizes the
variance component into two terms without interaction (boxplots of all 22âˆ—16

The Uncertainty of Climate Change Projections
571
NAU.DJF
SAU.DJF
AMZ.DJF
CAM.DJF
CAM.JJA
ENA.JJA
MED.JJA
SAS.JJA
0.10
0.15
0.20
0.25
0.30
Change in trend at 2000
Temperature
NAU.DJF
SAU.DJF
AMZ.DJF
CAM.DJF
CAM.JJA
ENA.JJA
MED.JJA
SAS.JJA
â€“0.04â€“0.02 0.00 0.02 0.04 0.06
Change in trend at 2000
Precipitation
Fig. 20.9 Incremental trend parameters â€žT (temperature time series) and â€žP (precipitation time
series) in the bivariate model. When signiï¬cantly different from zero they estimate a change in the
slope of the linear trend after 2000.
region-and-model-speciï¬c parameters do not reveal any behavior inconsistent
with this averages).
Last, we show posterior PDFs of the â€žT, â€žP trend parameters in the bivariate
model. Recall that these parameters are introduced in order to estimate a
possible change in the trend underlying the observed and modelled time series,
after the year 2000. As can be seen from Figure 20.9 for the usual region/season
combinations (listed along the vertical axis), all the parameters of the temper-
ature trends are signiï¬cantly different from zero (the zero line does not even
appear in the plot). The trend parameters estimated for precipitation instead
show a whole range of behaviors, with some of the distributions straddling the

572
The Oxford Handbook of Applied Bayesian Analysis
zero line, some indicating a signiï¬cant decrease in the rate of change (with the
distributions lying for the most part to the left of the zero line), some, at the
contrary, estimating an increase in the trend.
20.7 Further discussion
We have presented a series of statistical models aiming at combining multiple
climate model simulations. The goal is to arrive at a rigorous characterization
of the uncertainty in model projections, quantifying it into probability dis-
tributions of climate change. More precisely, we determine PDFs of average
temperature and precipitation change at seasonal, multidecadal and regional
scales, that can be used for further development as input to impact models in
vulnerability and adaptation studies.
The challenge of synthesizing multiple model projections stems from the
idiosyncratic nature of the data sample. GCMs are arguably not independent,
they do not span the entire range of uncertainty, being fairly conservative
guesses of the future trajectory of our climate system, and they may have
systematic errors (biases) in their simulation of climate, that do not necessarily
cancel out in an overall mean.
Given access to the recently organized, rich archives of GCM data, we are
however able to start making sense of these ensembles of simulations. In this
chapter we have shown how increasingly complex statistical models can account
for features of the data like correlations between current and future simulation
errors, biases, differential skill in different periods of the simulation or different
regions of the globe by a given model. We have also shown how the difï¬culty of
validating climate predictions may be side-stepped thanks to the calculation of
posterior predictive distributions that open the way to cross-validation exercises.
We have dedicated some time to discuss two ways of characterizing the uncer-
tainty of these climate change experiments. One approach cares especially about
the common central tendency of these models, and the uncertainty around
this estimated central tendency. Thanks to the number of models available
and the observed record that â€˜anchorsâ€™ our estimates of the true signal of
climate, the central tendency is very precisely estimated, within a narrow band
of uncertainty. The reality of climate change, however, may look more like one
of these model trajectories than like this abstract climate signal. If we subscribe
to this view, then a more consistent representation of our uncertain future is
naturally found in the posterior predictive distribution, after integrating out all
the uncertain parameters of the Bayesian model. The posterior predictive width
is of the same order of magnitude as the range of model projections, much
wider than the posterior estimate of the climate (change) signalâ€™s uncertainty
and may look to expert eyes as a more realistic representation of the uncertainty
at hands.

The Uncertainty of Climate Change Projections
573
There are many ways by which the kind of analyses presented here may
be carried forward. Two in particular seem germane to our approach. Furrer
et al. (2007) chose to model the entire high resolution ï¬elds of variables that
constitute the original output of these models, after registering them to a
common grid. This approach has the advantage of providing a representation
of projections that is very familiar to the climate science community, in the
form of detailed maps of change, with the added dimension of a probabilistic
characterization of their likely ranges. Rougier (2007) uses a set of experiments
aimed at perturbing the parametrization of a speciï¬c climate model, the Hadley
Center GCM. The work therefore represents a natural complement to ours,
where intra-model uncertainties are fully characterized. Ideally, it could be
incorporated in our framework, in order to treat within model and between
model variability in concert.
We think of two main avenues for further development of multimodel analy-
sis. The ï¬rst is the representation of modelsâ€™ dependencies, which would protect
us from the overoptimistic narrowing of the uncertainty with the increasing
number of members in these ensembles. It seems clear that not all models add
an independent piece of information to the ensemble, while the treatment of
each one as independent of the other makes the estimate of the central tendency
increasingly more precise with the increasing number of GCMs.
A second direction should lead to the exploration of modelsâ€™ performance
within current climate simulations and how it correlates to modelsâ€™ reliability
in the future part of their simulation. In our treatment, a coarse metric of
performance was implicitly found in the difference between modelsâ€™ average
projections in the part of the simulation that overlaps with observed records
and the corresponding observed climate average. There are potentially many
alternative metrics of performance that could be relevant for a modelâ€™s ability to
simulate future climate reliably, but the question is open, as is that of how to
incorporate this information into our statistical models.
Appendix
A. Broader context and background
All of Bayesian statistics derives ultimately from the formula
ï£¿(Ã‹|x) =
ï£¿(Ã‹) f (x|Ã‹)

 ï£¿(Ã‹â€²) f (x|Ã‹â€²)dÃ‹â€² .
(20.14)
Here:
r x is an observation vector, lying in some sample space X;
r Ã‹ is a vector of unknown parameters, lying in a parameter space ;

574
The Oxford Handbook of Applied Bayesian Analysis
r ï£¿(Ã‹) represents the prior distribution. If  is discrete, this is a probability
mass function; if  is continuous, this is a probability density function;
r f (x|Ã‹) is the likelihood function; that is, the conditional distribution of x
given Ã‹. This is a probability mass function if X is discrete and a probability
density function if X is continuous.
In the following discussion, we ï¬rst review simple applications of (20.14) using
conjugate priors; then we describe extensions to hierarchical models; ï¬nally we
describe computational methods via the Gibbs sampler and the Hastings â€“
Metropolis sampler, which the two best known examples of Markov chain
Monte Carlo (MCMC) algorithms. We illustrate these concepts with refer-
ence to the models for climate change projections that form the bulk of this
chapter.
A.1 Conjugate priors
The simplest examples of (20.14) have the following structure: both ï£¿(Ã‹) and
f (x|Ã‹) have a parametric structure, which is such that ï£¿(Ã‹|x) has the same
parametric structure as that of ï£¿(Ã‹). In that case, the prior is known as a
conjugate prior.
A simple example is the Beta prior for a binomial distribution. In this case,
X = {0, 1, 2, . . . , n} for known n,  = [0, 1], and f (x|Ã‹) =
n!
x!(nâˆ’x)!Ã‹x(1 âˆ’Ã‹)nâˆ’x
(the binomial distribution with n trials and probability of success Ã‹). Consider
the prior density
ï£¿(Ã‹) = (a + b)
(a)(b)Ã‹aâˆ’1(1 âˆ’Ã‹)bâˆ’1,
Ã‹ âˆˆ[0, 1],
(20.15)
the Beta distribution with constants a > 0, b > 0. Then, after cancelling com-
mon terms in the numerator and denominator, (20.14) becomes
ï£¿(Ã‹|x) =
Ã‹a+xâˆ’1(1 âˆ’Ã‹)b+nâˆ’xâˆ’1

 1
0 (Ã‹â€²)a+xâˆ’1(1 âˆ’Ã‹â€²)b+nâˆ’xâˆ’1dÃ‹â€² .
(20.16)
But the fact that (20.15) is a proper probability density for all a > 0, b > 0
implies
 1
0
(Ã‹â€²)a+xâˆ’1(1 âˆ’Ã‹â€²)b+nâˆ’xâˆ’1dÃ‹â€² = (a + x)(b + n âˆ’x)
(a + b + n)
.
Hence (20.16) reduces to
ï£¿(Ã‹|x) =
(a + b + n)
(a + x)(b + n âˆ’x)Ã‹a+xâˆ’1(1 âˆ’Ã‹)b+nâˆ’xâˆ’1.

The Uncertainty of Climate Change Projections
575
Thus, the posterior distribution is of the same parametric form as the prior
(20.15), but with a and b replaced by a + x and b + n âˆ’x. This is the key idea of
a conjugate prior.
A second example of a conjugate prior is the Gamma-normal prior. Consider
the sequence of conditional distributions
ÃŽ âˆ¼Ga(a, b),
(20.17)
Ã|ÃŽ âˆ¼N[m, (kÃŽ)âˆ’1],
(20.18)
x1, . . . , xn|Ã, ÃŽ âˆ¼N[Ã, ÃŽâˆ’1] (independent).
(20.19)
In this case the data equation (20.19) consists of normal observations with
unknown mean and variance, but for notational convenience, we have written
the variance of the normal distribution as ÃŽâˆ’1 rather than the more conventional
Ã›2. The parameter ÃŽ is often called the precision.
In this case, equations (20.17) and (20.18) together deï¬ne the joint prior of
(ÃŽ, Ã), with joint density
baÃŽaâˆ’1eâˆ’bÃŽ
(a)
Â·
0
kÃŽ
2ï£¿exp

âˆ’kÃŽ
2 (Ã âˆ’m)2

âˆÃŽaâˆ’1/2 exp
8
âˆ’ÃŽ

b + k
2(Ã âˆ’m)2
9
.
(20.20)
Now consider the prior Ã— likelihood
baÃŽaâˆ’1eâˆ’bÃŽ
(a)
Â·
0
kÃŽ
2ï£¿exp

âˆ’kÃŽ
2 (Ã âˆ’m)2

Â·
 ÃŽ
2ï£¿
n/2
exp

âˆ’ÃŽ
2

(xi âˆ’Ã)2

.
(20.21)
Noting the side calculation
k(Ã âˆ’m)2 +

(xi âˆ’Ã)2 =
kn
k + n(m âˆ’Â¯Â¯x)2
+

(xi âˆ’Â¯Â¯x)2 + (k + n)

Ã âˆ’km + nÂ¯Â¯x
k + n
2
and deï¬ning Â¯Â¯x = 1
n
 xi, Ë†m = km+nÂ¯Â¯x
k+n , (20.21) reduces to
âˆš
kbaÃŽa+(nâˆ’1)/2
(2ï£¿)(n+1)/2(a) exp
8
âˆ’ÃŽ

b +
kn
2(k + n)(m âˆ’Â¯Â¯x)2
+1
2

(xi âˆ’Â¯Â¯x)2 + k + n
2
(Ã âˆ’Ë†m)2
9
.
(20.22)

576
The Oxford Handbook of Applied Bayesian Analysis
Comparing (20.22) with (20.20), we see that the posterior distribution is of the
same form but with the constants a, b, m, k replaced by Ë†a, Ë†b, Ë†m, Ë†k, where
Ë†a = a + n
2,
Ë†b = b +
kn
2(k + n)(m âˆ’Â¯Â¯x)2 + 1
2

(xi âˆ’Â¯Â¯x)2,
Ë†m = km + nÂ¯Â¯x
k + n ,
Ë†k = k + n.
In practice, we generally try to choose the prior to be as uninformative as
possible. In (20.20), this is often achieved by setting k = 0 (in which case the
improper conditional prior for density Ã, derived from (20.18), is just a ï¬‚at prior
over (âˆ’âˆž, âˆž)) and setting a and b to very small constants, such as a = b = 0.01.
A.2 Hierarchical models
Although all Bayesian statistical models may ultimately be written in the form
(20.14), this may not be the most convenient form either for conceptual model
building or for mathematical solution. Often it is better to impose some struc-
ture on the unknown parameters that makes their interdependence clear. The
most common way to do this is through a hierarchical model.
The simplest structure of hierarchical model consists of three layers:
r a top layer of parameters, that are common to all the individual units of the
model,
r a middle layer of parameters that are speciï¬c to individual units,
r a bottom layer consisting of observations.
An example is the main univariate model considered in Smith et al. (2009),
which is deï¬ned by the equations
Ã, ÃŒ, â€š âˆ¼U(âˆ’âˆž, âˆž),
(20.23)
Ã‹ âˆ¼Ga(a, b),
(20.24)
aÃŽ, bÃŽ âˆ¼Ga(aâˆ—, bâˆ—),
(20.25)
ÃŽ1, . . . , ÃŽM|aÃŽ, bÃŽ âˆ¼Ga(aÃŽ, bÃŽ),
(20.26)
X0|Ã âˆ¼N

Ã, ÃŽâˆ’1
0

,
(ÃŽ0 known)
(20.27)
X j|Ã, ÃŽ j âˆ¼N

Ã, ÃŽâˆ’1
j

,
(20.28)
Yj|X j, Ã, ÃŒ, Ã‹, ÃŽ j, â€š âˆ¼N

ÃŒ + â€š(X j âˆ’Ã), (Ã‹ÃŽ j)âˆ’1
.
(20.29)

The Uncertainty of Climate Change Projections
577
Here the top layer of parameters consists of (Ã, ÃŒ, â€š, Ã‹, aÃŽ, bÃŽ), with prior distri-
butions speciï¬ed by (20.23)â€“(20.25). The middle layer is (20.26), which deï¬nes
the precisions ÃŽ1, . . . , ÃŽM of the M climate models. The bottom layer is (20.27)â€“
(20.29), which deï¬nes the distributions of the data X0 (present-day observed
climate mean) and X j, Yj (present-day and future projection under the jth
climate model, 1 â‰¤j â‰¤M). In this case, ÃŽ0 is not considered a parameter
because it is assumed known. The constants a, b, aâˆ—, bâˆ—are not unknown
parameters of the model but are ï¬xed at the start of the analysis â€“ in practice
we set them all equal to 0.01. Sometimes they are called hyperparameters to
distinguish them from parameters that are actually estimated during the course
of the model ï¬tting.
The more general â€˜multivariateâ€™ model of Smith et al. (2009) may also be
represented in this three-layer hierarchical form, where the model precisions
ÃŽ j, having the same prior distribution (20.26), and the interaction parameters
Ãi j, with independent prior distributions Ga(c, c), are in the middle layer of
the hierarchy to indicate their dependence on additional unknown parameters
aÃŽ, bÃŽ and c. For this model, there are many more unknown parameters, but the
basic structure of the model is not more complicated than the univariate version
of the model. In general, hierarchical models may have more than three layers,
or may be subject to more complex dependencies (e.g. the components of the
model may be represented as the vertices of a graph, where the edges of the
graph represent the dependence structure), but the simple three-layer structure
is sufï¬cient for the present application.
A.3 MCMC
Any hierarchical model may be represented conceptually in the form (20.14)
by representing all the unknown parameters together as a single long vec-
tor Ã‹. However, in virtually all such cases, the model cannot be solved by
simple analytic integration to compute the posterior density, while standard
numerical methods of integration, such as the trapezoidal rule, are too slow
or insufï¬ciently accurate to yield practical results. Markov chain Monte Carlo
(MCMC) methods are a class of Monte Carlo simulation techniques whose
purpose is to simulate a random sample from the posterior distribution of
Ã‹. Although these methods are also used in non-hierarchical problems, in
cases where a conjugate prior is not available or is inappropriate for a par-
ticular application, they really come into their own in solving hierarchical
Bayesian problems. The name derives from the fact that they generate a ran-
dom sample from a Markov chain, which can be proved to converge to the
true posterior distribution as the size of the Monte Carlo sample tends to
inï¬nity.

578
The Oxford Handbook of Applied Bayesian Analysis
The Gibbs sampler is based on ï¬rst partitioning Ã‹ into components Ã‹1, . . . , Ã‹p.
Here, p may simply be the total number of unknown parameters in the model,
though it is also possible to group parameters together when it is convenient to
do so (for example, in the Gamma-normal prior, we are effectively grouping ÃŽ
and Ã together as a single parameter Ã‹: in a hierarchical model, the precision
and mean for a single unit could together be one of Ã‹1, . . . , Ã‹p). The key element
is that it must be possible to generate a random variable from any one of
Ã‹1, . . . , Ã‹p conditional on the other p âˆ’1 held ï¬xed: this is often achieved
in practice through the use of conjugate priors. The algorithm is then as
follows:
1. Choose arbitrary starting values Ã‹1, . . . , Ã‹p, say Ã‹(1)
1 , . . . , Ã‹(1)
p . Set the
counter b = 1.
2. Given current values Ã‹(b)
1 , . . . , Ã‹(b)
p , generate a Monte Carlo random variate
from the distribution of Ã‹1, conditional on Ã‹ j = Ã‹(b)
j , j = 2, . . . , p. Call
the result Ã‹(b+1)
1
. Next, generate a Monte Carlo random variate from the
distribution of Ã‹2, conditional on Ã‹1 = Ã‹(b+1)
1
, Ã‹ j = Ã‹(b)
j , j = 3, . . . , p. Call
the result Ã‹(b+1)
2
. Continue with Ã‹3, Ã‹4, . . . , up to generating a Monte
Carlo random variate Ã‹(b+1)
p
from the distribution of Ã‹p, conditional on
Ã‹ j = Ã‹(b+1)
j
, j = 1, . . . , p âˆ’1.
3. Set counter b â†’b + 1 and return to step 2.
4. Continue until b = B, the desired number of Monte Carlo iterations. The
value of B depends on the complexity of the problem and available com-
puter time, but a value in the range 5000â€“100,000 is typical.
5. The ï¬rst B0 < B samples are discarded as burn-in samples, reï¬‚ecting that
the Markov chain has not yet converged to its stationary distribution.
Among the last B âˆ’B0 iterations, it is usual to retain only every tth
iteration, for some suitable t > 1. This reï¬‚ects the fact that successive
iterations from the Gibbs sampler are highly correlated, but by thinning
in this way, the remaining iterations may be treated as approximately
uncorrelated.
6. The resulting (B âˆ’B0)/t values of Ã‹ are treated as a random sample from
the posterior distribution of Ã‹. Posterior means and standard deviations,
predictive distributions, etc., are calculated by averaging over these ran-
dom samples.
As an example, let us consider the simpler form of (20.23)â€“(20.29) in which
equation (20.25) is omitted and aÃŽ, bÃŽ are treated as known constants. As a
notational point, Ã‹ in (20.24) is a single (scalar) parameter in a multiparameter
model, not to be confused with our notation of Ã‹ as the vector of all unknown
parameters in (20.14). The context should make clear which of the two uses of
Ã‹ is intended in any particular instance.

The Uncertainty of Climate Change Projections
579
In this case, the joint density of all the unknown parameters and observations
is proportional to
Ã‹a+M/2âˆ’1eâˆ’bÃ‹eâˆ’1
2 ÃŽ0(X0âˆ’Ã)2
M

j=1

ÃŽaÃŽ
j eâˆ’bÃŽÃŽ j Â· eâˆ’1
2 ÃŽ j (X j âˆ’Ã)2âˆ’1
2 Ã‹ÃŽ j {Yj âˆ’ÃŒâˆ’â€š(X j âˆ’Ã)}2
. (20.30)
It is not possible to integrate (20.30) analytically with respect to all the unknown
parameters Ã, ÃŒ, â€š, Ã‹, ÃŽ1, . . . , ÃŽM. However, for any single one of these parame-
ters, we can integrate using standard forms based on the normal or Gamma
densities. For example, as a function of Ã‹ alone, conditional on all the others,
(20.30) is proportional to
Ã‹a+M/2âˆ’1eâˆ’Ã‹[b+ 1
2

j ÃŽ j {Yj âˆ’ÃŒâˆ’â€š(X j âˆ’Ã)}2],
which in turn is proportional to a Gamma density with parameters a + M
2
and b + 1
2

j ÃŽ j{Yj âˆ’ÃŒ âˆ’â€š(X j âˆ’Ã)}2. Therefore the updating step for Ã‹ is to
generate a random variate with this Gamma distribution, replacing the previous
value of Ã‹.
Similarly, the conditional density of ÃŽ j (for ï¬xed j âˆˆ{1, 2, . . . , M}) is
Gamma with parameters aÃŽ + 1 and bÃŽ + 1
2[(X j âˆ’Ã)2 + Ã‹{Yj âˆ’ÃŒ âˆ’â€š(X j âˆ’Ã)}2].
The remaining parameters Ã, ÃŒ, â€š have conditional normal distributions that
can be calculating by completing the square in the exponent. As an example,
we give the explicit calculation for Ã. Expressed as a function of Ã, (20.30) is of
the form eâˆ’Q(Ã)/2, where
Q(Ã) = ÃŽ0(X0 âˆ’Ã)2 +

j
ÃŽ j(X j âˆ’Ã)2 + Ã‹

j
ÃŽ j{Yj âˆ’ÃŒ âˆ’â€š(X j âˆ’Ã)}2.
Completing the square,
Q(Ã) =
âŽ›
âŽÃŽ0 +

j
ÃŽ j + Ã‹â€š2 
j
ÃŽ j
âŽž
âŽ 
Ã—

Ã âˆ’
ÃŽ0X0 + 
j ÃŽ j X j âˆ’Ã‹â€š 
j ÃŽ j(Yj âˆ’ÃŒ âˆ’â€šX j)
ÃŽ0 + 
j ÃŽ j + Ã‹â€š2 
j ÃŽ j
2
+ const
where â€˜constâ€™ contains terms that do not depend on Ã. But based on this
representation, we recognize that eâˆ’Q(Ã)/2 has the form of a normal density with
mean and variance
ÃŽ0X0 + 
j ÃŽ j X j âˆ’Ã‹â€š 
j ÃŽ j(Yj âˆ’ÃŒ âˆ’â€šX j)
ÃŽ0 + 
j ÃŽ j + Ã‹â€š2 
j ÃŽ j
,
1
ÃŽ0 + 
j ÃŽ j + Ã‹â€š2 
j ÃŽ j
,
so the Gibbs sampling step for Ã is to generate a random variate from this
normal distribution. The calculations for ÃŒ and â€š are similar so we omit the
details.

580
The Oxford Handbook of Applied Bayesian Analysis
In a typical application of this algorithm, the Gibbs sampler was run for an
initial 12,500 iterations as a burn-in, followed by 50,000 iterations, with every
50th iteration being saved. Thus, we ended up with a Monte Carlo sample
of size 1000, which are approximately independent from the joint posterior
distribution of the unknown parameters.
A.4 The Metropolis â€“ Hastings algorithm
The Gibbs sampler still relies on the assumption that, after partitioning Ã‹ (the
full vector of unknown parameters) into Ã‹1, . . . , Ã‹p, the conditional distribution
of each Ã‹ j, given all the Ã‹k, k =/ j, may be represented in a sufï¬ciently explicit
form that it is possible to generate a Monte Carlo variate directly. In cases where
this is not possible, there are by now a wide variety of alternative MCMC algo-
rithms, but the oldest and best known is the Metropolis â€“ Hastings algorithm,
which we now describe.
Its general form is as follows. We assume again that Ã‹ is an unknown para-
meter vector lying in some parameter space . Suppose we want to generate
Monte Carlo samples from Ã‹, with the probability mass function or probability
density function g(Ã‹). We also assume we have some stochastic mechanism
for generating an â€˜updateâ€™ Ã‹â€², given the current value of Ã‹, that is represented
by a Markov transition kernel q(Ã‹, Ã‹â€²). In other words, for each Ã‹ âˆˆ, we
assume there exists a probability mass function or probability density function
q(Ã‹, Â·), that represents the conditional density of Ã‹â€² given Ã‹. In principle, q(Ã‹, Ã‹â€²)
is arbitrary subject to only mild restrictions (for example, the Markov chain
generated by q(Ã‹, Ã‹â€²) must be irreducible, in the sense that samples generated
by this Markov chain will eventually cover the whole of ). In practice, certain
simple forms such as random walks are usually adopted.
The main steps of the algorithm are:
1. Start with an initial trial value of Ã‹, call it Ã‹(1). Set counter b = 1.
2. Given current value Ã‹(b), generate a new trial Ã‹â€² from the conditional
density q(Ã‹(b), Ã‹â€²).
3. Calculate
Â· = min
 g(Ã‹â€²)q(Ã‹â€², Ã‹(b))
g(Ã‹(b))q(Ã‹(b), Ã‹â€²), 1

.
4. With an independent draw from the random number generator, deter-
mine whether we â€˜acceptâ€™ Ã‹â€², where the probability of acceptance is Â·.
5. If the result of step 4 is to accept Ã‹â€², set Ã‹(b+1) = Ã‹â€². Otherwise, Ã‹(b+1) = Ã‹(b).
6. Set counter b â†’b + 1 and return to step 2.
7. Continue until b = B, the desired number of Monte Carlo iterations.
As with the Gibbs sampler, it is usual to discard a large number of ini-
tial iterations as â€˜burn-inâ€™, and then to retain only a thinned subset of the

The Uncertainty of Climate Change Projections
581
remaining iterations, to ensure approximate independence between Monte
Carlo variates.
A key feature of the algorithm is that the acceptance probability Â· depends
only on ratios of the density g â€“ in other words, it is not necessary to specify g
exactly, so long as it is known up to a normalizing constant. This is precisely
the situation we face with (20.14), where the numerator ï£¿(Ã‹) f (x|Ã‹) is known
explicitly but the denominator requires an integral which is typically intractable.
However, with the Metropolis â€“ Hastings algorithm, it is not necessary to
evaluate the denominator.
A simpler form of the algorithm arises if the kernel q is symmetric, i.e.
q(Ã‹â€², Ã‹) = q(Ã‹, Ã‹â€²) for all Ã‹ =/ Ã‹â€². In that case, the formula for the acceptance
probability reduces to
Â· = min
 g(Ã‹â€²)
g(Ã‹(b)), 1

.
In this form, the algorithm is equivalent to the Monte Carlo sampling algorithm
of Metropolis et al. (1953), which was used for statistical physics calculations for
decades before the algorithmâ€™s rediscovery by Bayesian statisticians. The general
form of the algorithm, and its justiï¬cation in terms of the convergence theory
of Markov chains, was due to Hastings (1970).
Although it is possible to apply the Metropolis â€“ Hastings sampler entirely
independently of the Gibbs sampler, in practice, the two ideas are often com-
bined, where the Gibbs sampler is used to update all those components of Ã‹
for which explicit conditional densities are available in a form that can easily
be simulated, and a Metropolis â€“ Hastings step is used for the remaining
components. This is exactly the way the algorithm was applied in Smith et al.
(2009). To be explicit, let us return to the model deï¬ned by equations (20.23)â€“
(20.29), where now we treat aÃŽ, bÃŽ as unknown parameters with prior density
(20.25). Isolating those parts of the joint density that depend just on aÃŽ, bÃŽ, we
have to generate Monte Carlo samples from a density proportional to
g(aÃŽ, bÃŽ) = (aÃŽbÃŽ)aâˆ—âˆ’1eâˆ’bâˆ—(aÃŽ+bÃŽ)
M

j=1
baÃŽ
ÃŽ ÃŽaÃŽ
j eâˆ’bÃŽÃŽ j
(aÃŽ)
.
(20.31)
This is not of conjugate prior form so we use the Metropolis â€“ Hastings algo-
rithm to update aÃŽ and bÃŽ. The form of the updating rule is that given the current
aÃŽ and bÃŽ, we deï¬ne aâ€²
ÃŽ = aÃŽeâ€°(U1âˆ’1
2 ), bâ€²
ÃŽ = bÃŽeâ€°(U2âˆ’1
2 ), for random U1 and U2 that
are uniform on [0, 1] (independent of each other and all other random variates).
Both the form of this updating rule and the choice of â€° are arbitrary, but we have
found it works well in practice, and have used â€° = 1 in most of our calculations.

582
The Oxford Handbook of Applied Bayesian Analysis
With this rule,
q

(aÃŽ, bÃŽ),

aâ€²
ÃŽ, bâ€²
ÃŽ

=
1
â€°2aâ€²
ÃŽbâ€²
ÃŽ
,
aâ€²
ÃŽ âˆˆ

aÃŽeâˆ’â€°/2, aÃŽeâ€°/2
,
bâ€²
ÃŽ âˆˆ

bÃŽeâˆ’â€°/2, bÃŽeâ€°/2
.
Therefore q{(aâ€²
ÃŽ, bâ€²
ÃŽ), (aÃŽ, bÃŽ)}/q{(aÃŽ, bÃŽ), (aâ€²
ÃŽ, bâ€²
ÃŽ)} = aâ€²
ÃŽbâ€²
ÃŽ/(aÃŽbÃŽ) and the accep-
tance probability Â· reduces to
min

g

aâ€²
ÃŽ, bâ€²
ÃŽ

aâ€²
ÃŽbâ€²
ÃŽ
g(aÃŽ, bÃŽ)aÃŽbÃŽ
, 1

.
A.5 Diagnostics
In this section, we brieï¬‚y review some of the available diagnostics.
With the Metropolis â€“ Hastings algorithm, the kernel q(Ã‹, Ã‹â€²) often contains
a tuning parameter (â€° in the above example), which controls the size of the
jumps. This raises the question of how to choose the tuning parameter. For
a speciï¬c class of Metropolis sampling rules based on a multivariate normal
target, Gelman et al. (1996) showed that there is an optimal acceptance probabil-
ity Â·, which ranges from 0.44 in one dimension to 0.23 as the dimension tends
to âˆž. Using this result as a guideline, it is often recommended that for general
Metropolis sampling rules, the acceptance probability should be tuned so that it
lies between about 0.15 and 0.5 on average. There are many speciï¬c proposals
for adaptively optimizing this choice; the paper by Pasarica and Gelman (2010)
contains one recent proposal and reviews earlier literature.
The question of how many iterations of a MCMC algorithm are required to
achieve adequate convergence to the stationary distribution has been the subject
of much research. One well-regarded procedure was proposed by Gelman and
Rubin (1992) and extended by Brooks and Gelman (1998). The Gelman â€“ Rubin
procedure requires running several samplers in parallel, using widely dispersed
initial values for the parameters. Then, it calculates â€˜potential scale reduction
factorâ€™ R. This can be interpreted as an estimate of the possible reduction in
variance of the posterior mean of a particular parameter, if the Markov chain
were run to convergence, compared with the current iterations. Since R is itself
an unknown parameter estimated from the sample output, it is common to
quote both the median and some upper quantile (say, the 0.975 quantile) of the
sampling distribution. The ideal value of R is 1; values much above 1 are taken
to indicate non-convergence of the MCMC procedure.
Another issue is inference after sampling. Suppose we are trying to estimate
the posterior mean of a particular parameter; in our climate application, ÃŒ, the
estimated mean of future values of a climatic variable, is of particular interest.
If we had a Monte Carlo sample of independent estimates of ÃŒ, the usual
standard error calculation would give an estimate of the sampling variability of
the posterior mean of ÃŒ. In an MCMC sample, it is desirable to take account of

The Uncertainty of Climate Change Projections
583
the fact that successive draws from the sample may be autocorrelated. There are
many possible procedures; one method, due to Heidelberger and Welch (1981),
constructs a nonparametric estimate of the spectral density at low frequencies,
and uses this to correct the standard error.
The Gelman â€“ Rubin and Heidelberger â€“ Welch procedures are included,
among several others, in the CODA diagnostics package (Plummer et al., 2006),
which is available as a downloadable package within R (R Development Core
Team, 2007).
A.6 Further reading
There are by now many books on the principles of Bayesian data analysis and
MCMC algorithms; a small selection includes Gamerman and Lopes (2006);
Gelman et al. (2003); Robert and Casella (2004). The book by Robert (2005) is
a somewhat more theoretical treatment that makes the link between modern
practices in Bayesian statistics and classical decision theory and inference.
B. Computational details
B.1 Computation for the univariate model
We detail here the Gibbs/Metropolis algorithm to sample the posterior dis-
tribution of the model in Section 20.4.1, ï¬tting one region at a time, with a
hyperprior on the parameters of the prior for the precisions, ÃŽ js. This algorithm
is originally described in Smith et al. (2009).
Under the region-speciï¬c model, the joint density of Ã‹, Ã, ÃŒ, â€š, aÃŽ, bÃŽ, X0 and
ÃŽ j, X j, Yj( j = 1, . . . , M) is proportional to
Ã‹a+M/2âˆ’1eâˆ’bÃ‹eâˆ’1
2 ÃŽ0(X0âˆ’Ã)2aaâˆ—âˆ’1
ÃŽ
eâˆ’bâˆ—aÃŽbaâˆ—âˆ’1
ÃŽ
eâˆ’bâˆ—bÃŽ Â·
Â·
M

j=1

ÃŽaÃŽ
j eâˆ’bÃŽÃŽ j Â· eâˆ’1
2 ÃŽ j (X j âˆ’Ã)2âˆ’1
2 Ã‹ÃŽ j {Yj âˆ’ÃŒâˆ’â€š(X j âˆ’Ã)}2
.
(20.32)
Deï¬ne
ËœÃ = ÃŽ0X0 +  ÃŽ j X j âˆ’Ã‹â€š  ÃŽ j(Yj âˆ’ÃŒ âˆ’â€šX j)
ÃŽ0 +  ÃŽ j + Ã‹â€š2  ÃŽ j
,
(20.33)
ËœÃŒ =
 ÃŽ j{Yj âˆ’â€š(X j âˆ’Ã)}
 ÃŽ j
,
(20.34)
Ëœâ€š =
 ÃŽ j(Yj âˆ’ÃŒ)(X j âˆ’Ã)
 ÃŽ j(X j âˆ’Ã)2
.
(20.35)
In a Monte Carlo sampling scheme, all the parameters in (20.32), with the
exception of aÃŽ and bÃŽ, may be updated through Gibbs sampling steps,

584
The Oxford Handbook of Applied Bayesian Analysis
as follows:
Ã|rest âˆ¼N
8
ËœÃ,
1
ÃŽ0 +  ÃŽ j + Ã‹â€š2  ÃŽ j
9
,
(20.36)
ÃŒ|rest âˆ¼N
8
ËœÃŒ,
1
Ã‹  ÃŽ j
9
,
(20.37)
â€š|rest âˆ¼N
8
Ëœâ€š,
1
Ã‹  ÃŽ j(X j âˆ’Ã)2
9
,
(20.38)
ÃŽ j|rest âˆ¼G
8
a + 1, b + 1
2(X j âˆ’Ã)2 + Ã‹
2{Yj âˆ’ÃŒ âˆ’â€š(X j âˆ’Ã)}2
9
, (20.39)
Ã‹|rest âˆ¼G
8
a + M
2 , b + 1
2

ÃŽ j{Yj âˆ’ÃŒ âˆ’â€š(X j âˆ’Ã)}2
9
.
(20.40)
For the parameters aÃŽ, bÃŽ, the following Metropolis updating step is proposed
instead:
1. Generate U1, U2, U3, independent uniform on (0, 1).
2. Deï¬ne new trial values aâ€²
ÃŽ = aÃŽeâ€°(U1âˆ’1/2), bâ€²
ÃŽ = bÃŽeâ€°(U2âˆ’1/2). The value of â€°
(step length) is arbitrary but â€° = 1 seems to work well in practice, and is
therefore used here.
3. Compute
â„“1 = MaÃŽ log bÃŽ âˆ’M log (aÃŽ) + (aÃŽ âˆ’1)

log ÃŽ j âˆ’bÃŽ
Ã—

ÃŽ j + aâˆ—log(aÃŽbÃŽ) âˆ’bâˆ—(aÃŽ + bÃŽ),
â„“2 = Maâ€²
ÃŽ log bâ€²
ÃŽ âˆ’M log (aâ€²
ÃŽ) + (aâ€²
ÃŽ âˆ’1)

log ÃŽ j âˆ’bâ€²
ÃŽ
Ã—

ÃŽ j + aâˆ—log

aâ€²
ÃŽbâ€²
ÃŽ

âˆ’bâˆ—
aâ€²
ÃŽ + bâ€²
ÃŽ

.
This computes the log likelihood for both (aÃŽ, bÃŽ) and (aâ€²
ÃŽ, bâ€²
ÃŽ), allowing for
the prior density and including a Jacobian term to allow for the fact that
the updating is on a logarithmic scale.
4. If
logU3 < â„“2 âˆ’â„“1
then we accept the new (aÃŽ, bÃŽ), otherwise keep the present values for the
current iteration, as in a standard Metropolis accept-reject step.
This process is iterated many times to generate a random sample from the joint
posterior distribution. In the case where aÃŽ, bÃŽ are treated as ï¬xed, the Metropo-
lis steps for these two parameters are omitted and in this case the method is
a pure Gibbs sampler. An R program (REA.GM.r) to perform the sampling is
available for download from http://www.image.ucar.edu/Ëœtebaldi/REA.

The Uncertainty of Climate Change Projections
585
B.2 Computation for the multivariate model
The following computations were originally described in Smith et al. (2009).
Omitting unnecessary constants, the joint density of all the random variables
in the model that treats all regions at the same time is
(caÃŽbÃŽ)aâˆ’1eâˆ’b(c+aÃŽ+bÃŽ) Â·
6 R

i=0
Ã‹aâˆ’1
i
eâˆ’bÃ‹i
7
Â·
6 R

i=1
Ë†aâˆ’1
i
eâˆ’bË†i
7
Â·
âŽ¡
âŽ£
M

j=1
ÃŽaÃŽâˆ’1
j
eâˆ’bÃŽÃŽ j
bÃŽ
(aÃŽ)
âŽ¤
âŽ¦
Ã—

Â¯aâˆ’1
0
eâˆ’bÂ¯0
Ã—
âŽ¡
âŽ£
R

i=1
M

j=1
Ãcâˆ’1
i j eâˆ’cÃi j cc
(c)
âŽ¤
âŽ¦Â·
âŽ¡
âŽ£
M

j=1
1
Â¯0eâˆ’1
2 Â¯0Â·2
j
âŽ¤
âŽ¦Â·
âŽ¡
âŽ£
M

j=1
1
Ã‹0Â¯0eâˆ’1
2 Ã‹0Â¯0(Â·â€²
j âˆ’â€š0Â· j )2
âŽ¤
âŽ¦Â·
Ã—
6 R

i=1
eâˆ’1
2 ÃŽ0i(Xi0âˆ’Ã0âˆ’ÃŠi)2
7
Â·
âŽ¡
âŽ£
R

i=1
M

j=1
1
Ãi jË†iÃŽ jeâˆ’1
2 Ãi j Ë†iÃŽ j (Xi j âˆ’Ã0âˆ’ÃŠiâˆ’Â· j )2
âŽ¤
âŽ¦Â·
Ã—
âŽ¡
âŽ£
R

i=1
M

j=1
1
Ãi jÃ‹iÃŽ je
âˆ’1
2 Ãi j Ã‹iÃŽ j

Yi j âˆ’ÃŒ0âˆ’ÃŠâ€²
iâˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0âˆ’ÃŠiâˆ’Â· j )
2
âŽ¤
âŽ¦.
(20.41)
Deï¬ne
ËœÃ0 =

i ÃŽ0i(Xi0âˆ’ÃŠi)+
i Ë†i

j Ãi j ÃŽ j(Xi j âˆ’ÃŠiâˆ’Â· j )âˆ’
i â€šiÃ‹i

j Ãi j ÃŽ j

Yi j âˆ’ÃŒ0âˆ’ÃŠiâˆ’Â·â€²
j âˆ’â€ši(Xi jâˆ’ÃŠiâˆ’Â· j )


i{ÃŽ0i+(Ë†i+â€š2
i Ã‹i) 
j Ãi jÃŽ j}
,
(20.42)
ËœÃŒ0 =

i Ã‹i

j Ãi jÃŽ j

Yi j âˆ’ÃŠâ€²
i âˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)


i Ã‹i{
j Ãi jÃŽ j}
,
(20.43)
ËœÃŠi =
ÃŽ0i(Xi0âˆ’Ã0)+Ë†i

j Ãi j ÃŽ j (Xi j âˆ’Ã0âˆ’Â· j )âˆ’â€šiÃ‹i

j Ãi j ÃŽ j

Yi j âˆ’ÃŒ0âˆ’ÃŠâ€²
iâˆ’Â·â€²
jâˆ’â€ši(Xi j âˆ’Ã0âˆ’Â· j )

ÃŽ0i+(Ë†i+â€š2
i Ã‹i) 
j Ãi j ÃŽ j
,
(20.44)
ËœÃŠâ€²
i =

j Ãi jÃŽ j

Yi j âˆ’ÃŒ0 âˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)


j Ãi jÃŽ j
,
(20.45)
Ëœâ€š0 =

j Â·â€²
jÂ· j

j Â·2
j
,
(20.46)

586
The Oxford Handbook of Applied Bayesian Analysis
Ëœâ€ši =

j Ãi jÃŽ j

Yi j âˆ’ÃŒ0 âˆ’ÃŠâ€²
i âˆ’Â·â€²
j

(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)

j Ãi jÃŽ j(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)2
, (i =/ 0), (20.47)
ËœÂ· j =
â€š0Ã‹0Â¯0Â·â€²
j + ÃŽ j

i Ãi j Ë†i(Xi j âˆ’Ã0 âˆ’ÃŠi) âˆ’ÃŽ j

i Ãi j Ã‹iâ€ši

Yi j âˆ’ÃŒ0 âˆ’ÃŠâ€²
i âˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi)

Â¯0 + â€š2
0Ã‹0Â¯0 + ÃŽ j

i Ãi j Ë†i + ÃŽ j

i Ãi j Ã‹iâ€š2
i
,
(20.48)
ËœÂ·â€²
j = â€š0Ã‹0Â¯0Â· j + ÃŽ j

i Ãi jÃ‹i

Yi j âˆ’ÃŒ0 âˆ’ÃŠâ€²
i âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)

Ã‹0Â¯0 + ÃŽ j

i Ãi jÃ‹i
.
(20.49)
The conditional distributions required for the Gibbs sampler are as follows:
Ã0|rest âˆ¼N
6
ËœÃ0,
1

i{ÃŽ0i +

Ë†i + â€š2
i Ã‹i
 
j Ãi jÃŽ j}
7
,
(20.50)
ÃŒ0|rest âˆ¼N
6
ËœÃŒ0,
1

i{Ã‹i

j Ãi jÃŽ j}
7
,
(20.51)
ÃŠi|rest âˆ¼N
6
ËœÃŠi,
1
ÃŽi +

Ë†i + â€š2
i Ã‹i
 
j Ãi jÃŽ j
7
,
(20.52)
ÃŠâ€²
i|rest âˆ¼N
6
ËœÃŠâ€²
i,
1
Ã‹i

j Ãi jÃŽ j
7
,
(20.53)
â€š0|rest âˆ¼N
6
Ëœâ€š0,
1
Ã‹0Â¯0

j Â·2
j
7
,
(20.54)
â€ši|rest âˆ¼N
6
Ëœâ€ši,
1
Ã‹i

j Ãi jÃŽ j(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)2
7
, (i =/ 0)
(20.55)
Â· j|rest âˆ¼N
8
ËœÂ· j,
1
Â¯0 + â€š2
0Ã‹0Â¯0 + ÃŽ j

i Ãi jË†i + ÃŽ j

i Ãi jÃ‹iâ€š2
i
9
,
(20.56)
Â·â€²
j|rest âˆ¼N
8
ËœÂ·â€²
j,
1
Ã‹0Â¯0 + ÃŽ j

i Ãi jÃ‹i
,
9
,
(20.57)
Ã‹0|rest âˆ¼Gam
âŽ¡
âŽ£a + M
2 , b + 1
2Â¯0

j

Â·â€²
j âˆ’â€š0Â· j
2
âŽ¤
âŽ¦,
(20.58)
Ã‹i|rest âˆ¼Gam
âŽ¡
âŽ£a + M
2 , b + 1
2

j
Ãi jÃŽ j
Ã—

Yi j âˆ’ÃŒ0 âˆ’ÃŠâ€²
i âˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)
2
âŽ¤
âŽ¦, (i =/ 0) (20.59)

The Uncertainty of Climate Change Projections
587
Ë†i|rest âˆ¼Gam
âŽ¡
âŽ£a + M
2 , b + 1
2

j
Ãi jÃŽ j(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)2
âŽ¤
âŽ¦,
(20.60)
ÃŽ j|rest âˆ¼Gam
6
aÃŽ + R, bÃŽ + 1
2

i
Ãi jË†i(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)2
(20.61)
+1
2

i
Ãi jÃ‹i

Yi j âˆ’ÃŒ0 âˆ’ÃŠâ€²
i âˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)
2
7
,
Â¯0|rest âˆ¼Gam
âŽ¡
âŽ£a + M, b + 1
2

j
Â·2
j + 1
2Ã‹0

j

Â·â€²
j âˆ’â€š0Â· j
2
âŽ¤
âŽ¦,
(20.62)
Ãi j|rest âˆ¼Gam
8
c + 1, c + 1
2Ë†iÃŽ j(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)2
+1
2Ã‹iÃŽ j

Yi j âˆ’ÃŒ0 âˆ’ÃŠâ€²
i âˆ’Â·â€²
j âˆ’â€ši(Xi j âˆ’Ã0 âˆ’ÃŠi âˆ’Â· j)
2
9
.
(20.63)
Were aÃŽ, bÃŽ and c ï¬xed, as in the univariate analysis, the iteration (20.50)â€“(20.63)
could be repeated many times to generate a random sample from the joint
posterior distribution. Having added a layer by making the three parameters
random variates, two Metropolis steps are added to the iteration (20.50)â€“(20.63),
as follows.
For the sampling of aÃŽ and bÃŽ jointly, deï¬ne U1, U2 two independent random
variables distributed uniformly over the interval (0, 1), and the two candidate
values aâ€²
ÃŽ = aÃŽe(â€°(u1âˆ’1
2 )) and bâ€²
ÃŽ = bÃŽe(â€°(u2âˆ’1
2 )), where â€° is an arbitrary increment,
chosen as â€° = 1 in the implementation to follow. We then compute
l1 = MRaÃŽ log(bÃŽ) âˆ’MR log((aÃŽ) + R(aÃŽ âˆ’1)

j
log(ÃŽ j)
âˆ’RbÃŽ

j
ÃŽ j + a log(aÃŽbÃŽ) âˆ’b(aÃŽ + bÃŽ),
(20.64)
l2 = MRaâ€²
ÃŽ log

bâ€²
ÃŽ

âˆ’MR log(

aâ€²
ÃŽ

+ R

aâ€²
ÃŽ âˆ’1
 
j
log(ÃŽ j)
âˆ’Rbâ€²
ÃŽ

j
ÃŽ j + a log

aâ€²
ÃŽbâ€²
ÃŽ

âˆ’b

aâ€²
ÃŽ + bâ€²
ÃŽ

.
(20.65)
In (20.64) and (20.65) we are computing the log likelihoods of (aÃŽ, bÃŽ) and
(aâ€²
ÃŽ, bâ€²
ÃŽ), allowing for the prior densities and including a Jacobian term, allowing
for the fact that the updating is taking place on a logarithmic scale. Then, within
each iteration of the Gibbs â€“ Metropolis simulation, the proposed values (aâ€²
ÃŽ, bâ€²
ÃŽ)
are accepted with probability el2âˆ’l1.

588
The Oxford Handbook of Applied Bayesian Analysis
Similarly, the updating of c takes place by proposing câ€² = ce(â€°(u3âˆ’1
2 )), where u3
is a draw from a uniform distribution on (0, 1), and computing
k1 = MRc log(c) âˆ’MR log((c))
+ (c âˆ’1)

i

j
log(Ãi j) âˆ’c

i

j
Ãi j + a log(c) âˆ’bc
(20.66)
k2 = MRcâ€² log(câ€²) âˆ’MR log((câ€²))
+ (câ€² âˆ’1)

i

j
log(Ãi j) âˆ’câ€² 
i

j
Ãi j + a log(câ€²) âˆ’bcâ€².
(20.67)
Then, within each iteration of the Gibbs â€“ Metropolis simulation, the proposed
value câ€² is accepted with probability ek2âˆ’k1.
The method is available as an R program (REAMV.GM.r) from http://www.
image.ucar.edu/Ëœtebaldi/REA/.
B.3 Computation for the bivariate model
The MCMC algorithm for the bivariate model is taken from Tebaldi and SansÃ³
(2009). Note that in the following, the â€˜primeâ€™ symbol denotes the operation
of centring a variable (Ot or X jt) by the respective climate signal Ãt = Â· + â€št +
â€žtI{tâ‰¥Ã™0}.
Coefï¬cients of the piecewise linear model:
Deï¬ne
A = Ã™0ÃT + Ã™0ÃPâ€š2
xo + Ã™âˆ—
j
Ã“T
j + Ã™âˆ—
j
Ã“P
j â€š2
xj
and
B = ÃT 
tâ‰¤Ã™0

OT
t âˆ’â€šTt

+ ÃP 
tâ‰¤Ã™0
â€š2
xo

OT
t âˆ’â€šTt âˆ’â€šxo O Pâ€²
t

+

j
Ã“T
j

tâ‰¤Ã™0

X T
jt âˆ’â€šTt âˆ’d T
j

+

j
Ã“T
j

t>Ã™0

XT
jt âˆ’â€šTt âˆ’â€žT(t âˆ’Ã™0) âˆ’dT
j

+

j
Ã“P
j â€š2
xj

tâ‰¤Ã™0

X T
jt âˆ’â€šTt âˆ’dT
j âˆ’â€šxj

X Pâ€²
jt âˆ’d P
j

+

j
Ã“P
j â€š2
xj

t>Ã™0

X T
jt âˆ’â€šTt âˆ’â€žT(t âˆ’Ã™0) âˆ’dT
j âˆ’â€šxj

X Pâ€²
jt âˆ’d P
j

.
Then
Â·T âˆ¼N
 B
A, (A)âˆ’1

.

The Uncertainty of Climate Change Projections
589
Deï¬ne
A = Ã™0ÃP + Ã™âˆ—
j
Ã“P
j
and
B = ÃP 
tâ‰¤Ã™0

O P
t âˆ’â€šPt âˆ’â€šxo OT â€²
t

+

j
Ã“P
j

tâ‰¤Ã™0

X P
jt âˆ’â€šPt âˆ’d P
j âˆ’â€šxj

XT â€²
jt âˆ’d T
j

+

j
Ã“P
j

t>Ã™0

X P
jt âˆ’â€šPt âˆ’â€žP(t âˆ’Ã™0) âˆ’d P
j âˆ’â€šxj

XT â€²
jt âˆ’dT
j

.
Then
Â·P âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne
A = ÃT 
tâ‰¤Ã™0
t2 + ÃPâ€š2
xo

tâ‰¤Ã™0
t2 +

j
Ã“T
j

tâ‰¤Ã™âˆ—
t2 +

j
Ã“P
j â€š2
xj

tâ‰¤Ã™âˆ—
t2
and
B = ÃT 
tâ‰¤Ã™0
t

OT
t âˆ’Â·T
+ ÃP 
tâ‰¤Ã™0
â€š2
xot

OT
t âˆ’Â·T
âˆ’â€šxotO Pâ€²
t )
+

j
Ã“T
j

tâ‰¤Ã™0
t

XT
jt âˆ’Â·T âˆ’d T
j

+

j
Ã“T
j

t>Ã™0
t

X T
jt âˆ’Â·T âˆ’â€žT(t âˆ’Ã™0) âˆ’dT
j

+

j
Ã“P
j

tâ‰¤Ã™0
t

â€š2
xj

XT
jt âˆ’Â·T âˆ’d T
j

âˆ’â€šxj

X Pâ€²
jt âˆ’d P
j

+

j
Ã“P
j

t>Ã™0
t

â€š2
xj

XT
jt âˆ’Â·T âˆ’â€žT(t âˆ’Ã™0) âˆ’dT
j

âˆ’â€šxj

X Pâ€²
jt âˆ’d P
j

.
Then
â€šT âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne
A = ÃP 
tâ‰¤Ã™0
t2 +

j
Ã“P
j

tâ‰¤Ã™âˆ—
t2

590
The Oxford Handbook of Applied Bayesian Analysis
and
B = ÃP 
tâ‰¤Ã™0
t

O P
t âˆ’Â·P âˆ’â€šxo OT â€²
t

+

j
Ã“P
j

tâ‰¤Ã™0
t

X P
jt âˆ’Â·P âˆ’d P
j âˆ’â€šxj

XT â€²
jt âˆ’d T
j

+

j
Ã“P
j

t>Ã™0
t

X P
jt âˆ’Â·P âˆ’â€žP(t âˆ’Ã™0) âˆ’d P
j âˆ’â€šxj

XT â€²
jt âˆ’d T
j

.
Then
â€šP âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne
A =

j
Ã“T
j

t>Ã™0
(t âˆ’Ã™0)2 +

j
Ã“P
j â€š2
xj

t>Ã™0
(t âˆ’Ã™0)2
and
B =

j
Ã“T
j

t>Ã™0
(t âˆ’Ã™0)

XT
jt âˆ’Â·T âˆ’â€šTt âˆ’dT
j

+

j
Ã“P
j

t>Ã™0
(t âˆ’Ã™0)

â€š2
xj

XT
jt âˆ’Â·T âˆ’â€šTt âˆ’dT
j

âˆ’â€šxj

X Pâ€²
jt âˆ’d P
j

.
Then
â€žT âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne
A =

j
Ã“P
j

t>Ã™0
(t âˆ’Ã™0)2
and
B =

j
Ã“P
j

t>Ã™0
(t âˆ’Ã™0)

X P
jt âˆ’Â·P âˆ’â€šPt âˆ’d P
j âˆ’â€šxj

XT â€²
jt âˆ’dT
j

.
Then
â€žP âˆ¼N
 B
A, (A)âˆ’1

.
Bias terms and their priorsâ€™ parameters:
Deï¬ne
A = Ã™âˆ—Ã“T
j + Ã™âˆ—Ã“P
j â€š2
xj + ÃŽT
D

The Uncertainty of Climate Change Projections
591
and
B = Ã“T
j

tâ‰¤Ã™âˆ—
X T â€²
jt + Ã“P
j

tâ‰¤Ã™âˆ—

â€š2
xj X T â€²
jt âˆ’â€šxj

X Pâ€²
jt âˆ’d P
j

+ ÃŽT
DaT.
Then
d T
j âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne
A = Ã™âˆ—Ã“P
j + ÃŽP
D
and
B = Ã“P
j

tâ‰¤Ã™âˆ—

X Pâ€²
jt âˆ’â€šxj

XT â€²
jt âˆ’dT
j

+ ÃŽP
Da P.
Then
d P
j âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne A = MÃŽT
D and B = ÃŽT
D

j d T
j , then
aT âˆ¼N
 B
A, (A)âˆ’1

.
Deï¬ne A = MÃŽP
D and B = ÃŽP
D

j d P
j , then
a P âˆ¼N
 B
A, (A)âˆ’1

.
ÃŽT
D âˆ¼G
âŽ›
âŽœâŽ1 + M
2 ; 1 +

j

dT
j âˆ’aT2
2
âŽž
âŽŸâŽ .
ÃŽP
D âˆ¼G

1 + M
2 ; 1 +

j(d P
j âˆ’a P)2
2

.
The correlation coefï¬cients between temperature and precipitation in the
models, and their prior parameters:
Deï¬ne A = Ã“P
j

t(XT â€²
jt âˆ’dT
j )2 + ÃŽB and B = Ã“P
j

t(XT â€²
jt âˆ’dT
j )(X Pâ€²
jt âˆ’d P
j ) + ÃŽBâ€š0,
then
â€šxj âˆ¼N
 B
A, (A)âˆ’1

.

592
The Oxford Handbook of Applied Bayesian Analysis
Deï¬ne A = MÃŽB + ÃŽo and B = ÃŽB

j>0 â€šxj + ÃŽoâ€šxo, then
â€š0 âˆ¼N
 B
A, (A)âˆ’1

.
ÃŽB âˆ¼G

0.01 + M
2 ; 0.01 +

j(â€šxj âˆ’â€š0)2
2

.
Precision terms for the models:
Ã“T
j âˆ¼G

aÃ“T + Ã™âˆ—
2 ; bÃ“T +

t(XT â€²
jt âˆ’dT
j )2
2

.
Ã“P
j âˆ¼G

aÃ“P + Ã™âˆ—
2 ; bÃ“P +

t(X Pâ€²
jt âˆ’d P
j âˆ’â€šxj(XT â€²
jt âˆ’dT
j ))2
2

.
Only the full conditionals of the hyperparameters aÃ“T, bÃ“T, aÃ“P, bÃ“P cannot be
sampled directly, and a Metropolis step is needed. We follow the solution
described in Smith et al. (2009). The algorithm works identically for the two
pairs, and we describe it for aÃ“T and bÃ“T (the sampling is done jointly for the
pair). We deï¬ne U1, U2 as independent random variables, uniformly distributed
over the interval (0, 1), and we compute two proposal values aâ€²
Ã“T = aÃ“Te(â€°(u1âˆ’1
2 ))
and bâ€²
Ã“T = bÃ“Te(â€°(u2âˆ’1
2 )), where â€° is an arbitrary increment, that we choose as â€° = 1.
We then compute
â„“1 = MaÃ“T log bÃ“T âˆ’M log (aÃ“T) + (aÃ“T âˆ’1)

j
log Ã“T
j âˆ’bÃ“T

j
Ã“T
j
+ 0.01 log(aÃ“TbÃ“T) âˆ’0.01(aÃ“T + bÃ“T),
(20.68)
â„“2 = Maâ€²
Ã“T log bâ€²
Ã“T âˆ’M log 

aâ€²
Ã“T

+

aâ€²
Ã“T âˆ’1
 
j
log Ã“T
j âˆ’bâ€²
Ã“T

j
Ã“T
j
+ a log

aâ€²
Ã“Tbâ€²
Ã“T

âˆ’b

aâ€²
Ã“T + bâ€²
Ã“T

.
(20.69)
In (20.68) and (20.69) we are computing the log likelihoods of (aÃ“T, bÃ“T) and
(aâ€²
Ã“T, bâ€²
Ã“T). Then, within each iteration of the Gibbs â€“ Metropolis algorithm, the
proposed values (aâ€²
Ã“T, bâ€²
Ã“T) are accepted with probability eâ„“2âˆ’â„“1 if â„“2 < â„“1, or 1 if
â„“2 â‰¥â„“1.
An R program that implements this simulation is available as REA.BV.r from
http://www.image.ucar.edu/Ëœtebaldi/REA/.
Acknowledgements
We thank Linda O. Mearns, Doug Nychka and Bruno Sansoâ€™ who have been
working with us on various aspects of these analyses. Richard Smith is

The Uncertainty of Climate Change Projections
593
supported by NOAA grant NA05OAR4310020. Claudia Tebaldi is grateful to the
Department of Global Ecology, Carnegie Institution of Washington, Stanford,
and the National Center for Atmospheric Research, Boulder for hosting her at
the time when this chapter was written.
References
Brooks, S. and Gelman, A. (1998). General methods for monitoring convergence of iterative
simulations. Journal of Computational and Graphical Statistics, 7, 434â€“455.
Furrer, R., Sain, S., Nychka, D. and Meehl, G. (2007). Multivariate Bayesian analysis of
atmosphere-ocean general circulation models. Environmental and Ecological Statistics, 14,
249â€“266.
Gamerman, D. and Lopes, H. (2006). Markov Chain Monte Carlo: Stochastic Simulation for
Bayesian Inference (2nd edn). Chapman & Hall, Boca Raton, Florida.
Gelman, A., Carlin, J., Stern, H. and Rubin, D. (2003). Bayesian Data Analysis. (2nd edn.)
Chapman & Hall, CRC Press, Boca Raton, Florida.
Gelman, A., Roberts, G. and Gilks, W. (1996). Efï¬cient Metropolis jumping rules. In Bayesian
Statistics 5, (ed. J. M. Bernardo, J. Berger, A. P. Dawid and A.F.M. Smith), pp. 599â€“607.
Oxford University Press, Oxford.
Gelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple
sequences. Statistical Science, 7, 457â€“511.
Giorgi, F. and Francisco, R. (2000). Evaluating uncertainties in the prediction of regional
climate change. Geophysical Research Letters, 27, 1295â€“1298.
Gleckler, P. J., Taylor, K. and Doutriaux, C. (2007). Performance metrics for climate models.
Journal of Geophysical Research, 113(D06104). doi:10.1029/2007JD008972.
Groves, D. G., Yates, D. and Tebaldi, C. (2008). Uncertain global climate change projec-
tions for regional water management planning. Water Resources Research, 44(W12413).
doi:10.1029/2008WR006964.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains and their applica-
tions. Biometrika, 57, 97â€“109.
Heidelberger, P. and Welch, P. (1981). A spectral method for conï¬dence interval generation
and run length control in simulations. Communications of the ACM, 24, 233â€“245.
IPCC (2007). Climate Change 2007 â€“ The Physical Science Basis. Contribution of Working Group
I to the Fourth Assessment Report of the IPCC. Solomon, S. et al. (eds.), Cambridge University
Press, Cambridge.
Metropolis, N., Rosenbluth, A., Rosenbluth, Teller, A. and Teller, E. (1953). Equation of state
calculations by fast computing machines. Journal of Chemical Physics, 21, 1087â€“1092.
Nakicenovic, N. (2000). Special Report on Emissions Scenarios: A Special Report of Working
Group III of the Intergovernmental Panel on Climate Change. Cambridge University Press,
Cambridge.
Pasarica, C. and Gelman, A. (2010). Adaptively scaling the Metropolis algorithm using expected
squared jumped distance. Statistica Sinica, 20, 343â€“364.
Plummer, M., Best, N., Cowles, K. and Vines, K. (2006). CODA: Convergence Diagnosis and
Output Analysis for MCMC. R NEWS, 6, 7â€“11.
R Development Core Team (2007). R: A Language and Environment for Statistical Computing.
R Foundation for Statistical Computing. Available from http://www.R-project.org.

594
The Oxford Handbook of Applied Bayesian Analysis
Raftery, A. E., Gneiting, T., Balabdoui, F. and Polakowski, M. (2005). Using Bayesian model
averaging to calibrate forecast ensembles. Monthly Weather Review, 133, 1155â€“1174.
Robert, C. (2005). The Bayesian Choice. (2nd edn). Springer, New York.
Robert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. (2nd edn). Springer,
New York.
Rougier, J. (2007). Probabilistic inference for future climate using an ensemble of climate
model evaluations. Climatic Change, 81, 247â€“264. doi: 10.1007/s10584-006-9156-9.
Smith, R., Tebaldi, C., Nychka, D. and Mearns, L. (2009). Bayesian modeling of uncertainty in
ensembles of climate models. Journal of the American Statistical Association, 104, 97â€“116.
Tebaldi, C. and Knutti, R. (2007). The use of the multi-model ensemble in probabilistic climate
projections. Philosophical Transactions of the Royal Society, Series A, 1857, 2053â€“2075.
Tebaldi, C. and Lobell, D. (2008). Towards probabilistic projections of climate change impacts
on global crop yields. Geophysical Research Letters, 35 (L08705). doi:10.1029/2008GL033423.
Tebaldi, C., Mearns, L., Nychka, D. and Smith, R. (2004). Regional probabilities of precipitation
change: A Bayesian analysis of multimodel simulations. Geophysical Research Letters, 31
(L24213). doi:10.1029/2004GL021276.
Tebaldi, C. and SansÃ³, B. (2009). Joint projections of temperature and precipitation change
from multiple climate models: A hierarchical Bayesian approach. Journal of the Royal Statis-
tical Society. Series A, 172, 83â€“106.
Tebaldi, C., Smith, R., Nychka, D. and Mearns, L. (2005). Quantifying uncertainty in projections
of regional climate change: A Bayesian approach to the analysis of multi-model ensembles.
Journal of Climate, 18, 1524â€“1540.
Washington, W. and Parkinson, C. L. (2005). Introduction to Three-dimensional Climate Modeling.
University Science Books, London.

PART IV
Policy, Political and Social Sciences


Â·21Â·
Volatility in prediction markets:
A measure of information ï¬‚ow
in political campaigns
Carlos M. Carvalho and Jill Rickershauser
21.1 Introduction
When asked to describe the pivotal moments of the 2004 campaign, Mark
Mellman, senior strategist for John Kerryâ€™s campaign, ï¬rst noted that analysts
should be careful to:
Avoid what psychologists call fundamental attribution error. Fundamental attribution error consists,
as some of you know, of overweighting the signiï¬cance, the importance of individuals, of personalities,
of events, and underweighting the signiï¬cance, the salience of the structure of the situation of the
underlying circumstances.
He went on to outline the factors he believed to have been the most important
in determining the vote in the 2004 election: the decline of the Democratic
plurality among voters, culture as the primary division in American politics as
opposed to class, and Bushâ€™s incumbency advantage. While he did list a number
of events that the Kerry campaign viewed as important, his central message was
that the campaign did the best it could and very well given the political situation
in 2004. Matthew Dowd, a Bush strategist, also viewed the campaign as a series
of relatively minor ups and downs with a largely expected outcome. Yet when
most people think back to the 2004 campaign, some events seem pivotal, such
as the Swift Boat Veterans for Truth (SBVT) ads in August and Kerryâ€™s comment
that he voted for spending $87 billion before he voted against spending $87
billion for emergency funds for troops and reconstruction in Iraq. Did those
two events matter much in the ï¬nal vote tally, or did they not really change
the campaign as much as we might think? Analysts look back at campaigns
and identify deï¬ning moments but, as Popkin (1991) notes, those anecdotes
can be simply the story that â€˜ï¬tsâ€™ our overall impression of the campaign as
opposed to the events that actually affected the election outcome. How can
we tell the difference between anecdotal explanations and the events that were
actually important? By identifying which campaign events did, in fact, provide

598
The Oxford Handbook of Applied Bayesian Analysis
new information in a given election year, we can better understand the â€˜normalâ€™
dynamics of campaigns.
In order to investigate what kind of information and events have which kinds
of effects on expectations of a candidateâ€™s success, we need a measure that
reacts quickly to new information. Opinion polls give us a somewhat nuanced
view of public opinion during campaigns, but there is a practical problem
with surveys: they are taken over several days. A poll taken the day before,
the day of, and the day after a presidential debate does measure the effect of
a candidateâ€™s performance in the debate on vote choice but it also registers the
effects of events (e.g. candidate visits or speeches) that occur before and after
the debate. If something shocking happened in a debate (would that debate â€“
watchers could be so lucky!), the results from that standard three-day poll
would partially reï¬‚ect it, but we could only surmise what change in candidate
preference occurred because of that gaffe and what change was caused by other
events.
Prediction markets, like the Iowa Electronic Markets (IEM) and Intrade (pre-
viously called Tradesports), offer an attractive alternative to polls or academic
panel studies to answer this kind of question because they allow us to clearly
analyse peopleâ€™s expectations of political outcomes and the ï¬‚uctuation in those
expectations given new information. The pricing of futures contracts in these
markets provides a continuous measure of the probability of a candidate win-
ning the election. The immediacy of online futures markets lets us see exactly
how information is incorporated, how quickly, and how expectations about
political events stabilize.
This paper uses data from futures on â€˜Bush wins the popular vote in
2004â€™, or the traded probability, of Bush winning the election, to build a mea-
sure of information ï¬‚ow. Our approach builds on theoretical developments
in the economics literature, especially the market microstructure literature,
that view the variation of asset prices as being driven by the arrival of new
information and the process that incorporates that information into market
prices. We extend a theoretically justiï¬ed model that combines returns and
volume in creating a measure of information ï¬‚ow to include two regimes of
variation. This provides a novel approach to differentiate between the â€˜nor-
malâ€™ campaign occurrences and the noteworthy, important events of particular
campaigns.
While the Swift Boat ads have become central to popular accounts of the 2004
presidential race, it appears that Kerryâ€™s nomination speech and the debates,
particularly the third one, imparted much more information. Other events that
were particularly important in the campaign were the CBS story about Bushâ€™s
National Guard service and the subsequent retraction, the report that explosives
went missing in Iraq in late October, and the release of the bin Laden tape a few
days before the election.

Volatility in Prediction Markets
599
21.2 Political prediction markets
Prediction markets, which are also called â€˜information marketsâ€™, â€˜idea futuresâ€™,
or â€˜event futuresâ€™, pay out returns based on whether or not a speciï¬ed event
occurs. Winner-take-all futures pay $1 if event X occurs and $0 if it does not.
At one point during the 2004 campaign, a person could buy a â€˜Bush wins the
popular voteâ€™ contract for $0.55. If that person held onto that contract until
November 2, then she would receive $1, making a proï¬t of $0.45 cents per
contract. Investors can resell or buy contracts at any point. For example, she
could buy a Bush contract at $0.55 and then sell it at $0.58, for a proï¬t of $0.03
per contract. This paper examines winner-take-all futures markets, though there
are several other types of contracts in prediction markets.
Before proceeding to a discussion of information ï¬‚ow in political prediction
markets and the evidence about campaign effects in the 2004 election, a few
theoretical points about these kinds of markets must be reviewed. In particular,
we discuss why prediction markets are able to aggregate information such that
the prices of future contracts are reï¬‚ective of the eventâ€™s actual probability of
occurring.
How do markets aggregate information? When a group of people are
asked how many marbles are in a given large jar, very few of them will
choose the correct answer. However, the average of the guesses is always
a close estimate of the actual number. Similarly, few people can estimate
the number of yards between point A and point B, but together the group
â€˜ï¬ndsâ€™ the answer. This occurs even without discussion; opinions are polled
privately and then aggregated to determine the distance. This is an intu-
itive explanation of how a prediction market works. The theoretical expla-
nation rests on the efï¬cient market hypothesis and no arbitrage arguments
(Wolfers and Zitzewitz, 2004).
Prediction markets are likely to produce quality information because they
provide: (i) incentives to seek information; (ii) incentives for truthful informa-
tion revelation and (iii) an algorithm for aggregating diverse opinions (Wolfers
and Zitzewitz, 2004). These markets force people to â€˜put their money where
their mouths areâ€™, so their expressed beliefs about a candidateâ€™s chances of
success are not just cheap talk. It is important to note that the average belief
of traders exhibited in the futures price is actually a weighted average, where
people with more conï¬dence in their assessment of a candidateâ€™s success trade
more so that their opinions count more in the price equilibrium. This can
be viewed as an advantage over traditional polls: equilibrium prices take into
account the strength of peopleâ€™s beliefs about the candidateâ€™s chance of electoral
success.
Without discussing a candidateâ€™s chances, the information held by people
all over the country will be combined so that the market estimate of the

600
The Oxford Handbook of Applied Bayesian Analysis
candidateâ€™s probability of winning is accurate. That is, a person who lives in
Ohio might trade based on his impression of voters in his town and a person in
North Carolina might trade on her impression of the local newspaper coverage.
When added together with traders from around the country (and around the
world), all the information is combined to produce the accurate value of the
contract.
21.2.1 Bounded rationality
The economic theory behind prediction markets suggests that the prices of
assets are accurate because people rationally and correctly understand and
incorporate information into the market prices. However, behavioral research
shows that people are limited information processors and are subject to biases
due to their preferences. This is not a major problem because â€˜all agents
need not be informed for a market to be efï¬cient . . . survey evidence documenting
biased expectations does not necessarily imply that market prices will be similar
biasedâ€™ (Sauer, 1998). A market is efï¬cient in the sense that it fully reï¬‚ects
all public information because of marginal traders or market makers. In a
market of sufï¬cient size and with large enough payoffs to counter transac-
tion costs, arbitrage in prices will occur until all traders, specialists as well as
informed and uninformed investors, agree on the appropriate price (Andersen,
1996). At this point in time Tradesports represents the largest political pre-
diction market and appear to have enough participation to alleviate the bias
problem.
It is hard to deny that most traders are biased much of the time. Berg and
Rietz (2006) notes that â€˜traders are biased and mistake proneâ€™ and Forsythe et al.
(1999) concur that â€˜traders frequently leave money on the table through violations
of arbitrage restrictionsâ€™. These conclusions are documented by surveys adminis-
tered by the Iowa Electronic Market to its traders. The surveys have consistently
shown that traders are not in any way representative of the voting population.
In particular, traders on the IEM are younger, more educated, more wealthy,
and â€˜more maleâ€™ than the US population and the voting population. Due to
the incentives to seek information when trading, they are also probably more
informed than the voting population.
Traders in the IEM tend to hold portfolios that are biased towards their
preferred candidates (Forsythe et al., 1992). Forsythe et al. (1998) show that
investors in the 1993 Canadian elections futures markets held unbalanced
portfolios in ways consistent with their preferences. That is, supporters of a
party were much more likely to hold more futures of that party than was the
market as a whole. Forsythe et al. (1999) demonstrate that this bias also exists in
the US presidential prediction markets. The trading population of Tradesports
is likely somewhat different than the trading population of the IEM due to its

Volatility in Prediction Markets
601
larger size and commercial aspect. However, there is still reason to suspect that
individual bettors in Tradesports markets are also biased.
These types of biases have been described as â€˜wish fulï¬llmentâ€™ in both the
political science literature and the psychological literature. Granberg and Brent
(1983) and Uhlaner and Grofman (1986) document the persistence and ubiquity
of this phenomenon. Supporters of Carter in 1980 were much more likely to
predict that Carter would win than were Reagan supporters (though a few of
each candidateâ€™s supporters were less biased in their predictions). Whether due
to the â€˜false consensus effectâ€™, where supporters of a particular candidate see
themselves as more representative of other voters than they actually are, or the
â€˜assimilation-contrast effectâ€™, where supporters of a candidate interpret news
about that candidate much more positively (Uhlaner and Grofman, 1986), most
traders in the IEM exhibit this wishful thinking as evidenced in their portfolio
holdings.
21.2.2 Prediction market efï¬ciency
Demonstrations of these biases does not necessarily mean that the prices
of candidate futures are incorrect. While there is some evidence that most
traders in the IEM are biased in their trading decisions (or at least likely
to make mistakes) and it is likely that many, if not most, of the traders in
Tradesports markets are similarly behaved, markets are nonetheless accurate
because of marginal traders. Though this paper speciï¬cally focuses on the
effects of information on futures prices as opposed to the marketsâ€™ predictive
power, this section will brieï¬‚y review the evidence of the accuracy of prices in
prediction markets in order to show that analysis of prediction marketsâ€™ prices is
worthwhile.
Many studies of the IEM show that prediction markets are as accurate as or
more accurate than opinion polls, both in the week or days before the election
and at longer time horizons. Berg et al. (2003) note that since 1988, the IEM
political prediction markets (conducted in US elections as well as in other
countries such as Canada, Austria, and Turkey) have consistently outperformed
polls and â€˜in a few cases (the 1988 and 1992 U.S. Presidential Elections) the market
dramatically outperformed pollsâ€™. Not surprisingly, markets with a higher volume
of contracts traded and markets with fewer candidates or parties are more
successful. US presidential election markets are the most accurate, though
the overall market accuracy is also impressive: the average market error in
vote-share contracts1 was 1.49% or 1.58% (depending on the exact measure
used) whereas the average poll error was 1.91%. It is more difï¬cult to judge
the true probability of an event occurring, but in markets where a winner-
take-all contract and a vote-share contract are both traded, evidence suggests
1 A vote-share contract pays out $1* the percent of the vote won by the candidate or party.

602
The Oxford Handbook of Applied Bayesian Analysis
that the prices of the two contracts move together such that the probability
expressed in the winner-take-all contract is similarly accurate. The ability to
predict the winner is intellectually interesting,2 though the accuracy of the
election markets has been studied so extensively in part because other appli-
cations of idea futures markets rely on the marketsâ€™ ability to accurately forecast
events.
If surveys of traders and ï¬ndings from political science and psychology all
show that people invested in the market are biased, how are market predictions
themselves accurate? The apparent paradox is resolved by the â€˜marginal traderâ€™
(e.g. Forsythe et al., 1992, 1999). As Forsythe et al. (1998) explain, â€˜as long as there
are some traders relatively free of such wish fulï¬llment biases and with deep enough
pockets, they will take advantage of the biases of other traders and in the process bring
prices to levels consistent with unbiased expectationsâ€™. These less-biased investors
serve as â€˜market makersâ€™, which is to say that they will continue to trade until
the price of each futures contract is correct.
Oliven and Rietz (2004) show that the average trader leaves money on the
table, either due to bias or mistakes, but that the marginal trader takes advan-
tage of those arbitrage opportunities and adjusts the prices. Other work has
shown that market makers, the marginal traders, whose portfolios and trades
exhibit no bias, invest much more money than the average trader and are more
active in trading. As a result, marginal traders are able to drive prices to efï¬cient
levels while proï¬ting from the mistakes of more error-prone traders (Forsythe
et al., 1999).
Marginal tradersâ€™ behavior explains the accuracy of prediction markets, so
our starting premise is correct: these markets are worth studying especially in
regards to campaign and information effects. While it is true that the equilib-
rium price in a market with positive information and transaction costs does not
summarize some small amount of the information, that fraction is negligible
(Grossman and Stiglitz, 1980). These ï¬ndings suggest that prices of futures
are, in fact, reasonable assessment of the â€˜trueâ€™ probabilities of a candidateâ€™s
success given that all information is rationally incorporated into prices, even if
individual traders are not always (or even usually) rational.
21.3 Volatility, trading volume and information ï¬‚ow
Studying the changes in the probability of a candidate winning an election
is as important as studying the probability itself. In the context of prediction
markets, this means that price movements and trading volume must also be
2 It is also potentially ï¬nancially rewarding to predict the outcome of elections (see Hartog and Monroe,
2008).

Volatility in Prediction Markets
603
considered in order to understand the dynamics of the campaign. In their study
of presidential approval, Gronke and Brehm (2002) write:
No one would argue that a statistical distribution can be described solely by its central tendency; nor
should our exploration of the patterns and causes of presidential approval look only at the mean.
As important in many circumstances is the frequency and sharpness of shifts in public sentiment-
volatility.
Similarly, if we are interested in the question of what information during
campaigns is seen as important or consequential (or at least potentially conse-
quential), the movement of futures prices matters. There may be a signiï¬cant
amount of action in a futures price when new information is acquired, though
ultimately the price may settle back to the original equilibrium. An analysis
of the mean price over one or several days would hide that effect of political
information. Therefore, such an analysis would underestimate the perceived
news content of speciï¬c campaign events.
Prices remain relatively stable when investors learn no new information
about the underlying asset. However, when new information such as earn-
ings reports does become public, stock and futures prices can move more
dramatically as traders incorporate the arrival of information. This results in
prices revealing the full information content through a sequence of trades. This
phenomenon is well studied, with a vast literature in economists and ï¬nance
focusing on the effects of earnings reports and news releases on volatility
and volume (see Jennings and Starks, 1985; Brown and Hartzell, 2001; Patell
and Wolfson, 1984; Chan, 2003; Dubinsky and Johannes, 2006) as well as the
relationship between prices, volatility and volume (e.g. Gallant et al. 1992).
In analysing the effects of politics on ï¬nancial markets, Bernhard and
Leblang (2006) pay careful attention to the volatility of asset prices as a measure
of uncertainty about a political outcome. Their particular contribution is to high-
light the fact that information affects markets conditional on prior expectations,
so simple dummy variables in a time period in which some event occurs can
miss the actual impact of the event. When they control for expectations about
the 2000 presidential election (as measured by poll numbers), they ï¬nd that
exit poll results from battleground states did affect the Standard and Poors and
NASDAQ futures and the US dollar-Japanese yen exchange rate. Similarly, this
paper tries to identify unexpected events that affected the electoral chances of
the candidates in 2004, above and beyond the long-term, underlying electoral
landscape.
Volatility can be conceptualized as a measure of the information ï¬‚ow into
a market. In the market microstructure literature in economics, the mixture
of distributions hypothesis â€˜posits a joint dependence of returns and volume on
an underlying latent event or information ï¬‚ow variableâ€™ (Andersen, 1996). While

604
The Oxford Handbook of Applied Bayesian Analysis
the theory describes the â€˜informativeness of market prices, the presence of liquidity
traders in the market and the manner in which news is disseminatedâ€™ as factors
inï¬‚uencing both volatility and volume, it â€˜points to the rate of information arrival
to the market as the primary variable of interestâ€™ (Fleming et al., 2006). The
volatility, or the variance of the prices, is â€˜primarily [caused] by the arrival of new
information and the process that incorporates this information into market pricesâ€™
(Andersen, 1996).
Information ï¬‚ows into the market and affects both the prices of the assets
and the volume of the assets that are traded. When this trading occurs, the
price of the asset reaches a new equilibrium state. Andersen (1996) explains:
An important ï¬nding is that over the course of a (short) period, the sequence of trades reveals the
pricing implications of the private signals and subsequently â€“ until new private information arrives â€“
all market participants agree on the value of the traded asset. Thus, private information arrivals induce
a dynamic learning process that results in prices fully revealing the content of the private information
through the sequence of trades and transaction prices.
It is new information that starts this process. In the context of political pre-
diction markets, when new information about the campaign â€“ whether polling
numbers, a larger ad buy in a battleground state, or an endorsement â€“ becomes
known, the price of the candidateâ€™s futures contract adjusts accordingly through
the increased participation of market makers. Volatility is low despite some new
information when rational anticipation of that event was already incorporated
into the price. In contrast, high volatility occurs when unexpected information
enters the market.
This price adjustment process, as evidenced in the estimated volatility, can
be used to characterize information and events. Jennings and Starks (1985)
study the effects of high-content earnings announcements as compared with
low-content announcements and ï¬nd that high-content ones induce more
volatility for longer periods of time. Studies like that one take the infor-
mation content as ï¬xed and study the effects on the market, whereas this
paper looks at the effects on the market and attempts to make statements
about the information content contained in different campaign events. This
is done by ï¬rst estimating a measure of information ï¬‚ow (or volatility) using
returns and volume. This measure is then used to evaluate the informative-
ness of a campaign event. While this approach presents some hurdles to
making assertions about causality, it provides a novel and theoretically sound
way to identify consequential events, therefore enhancing our understanding
of the role of information, campaign events, and other factors on election
outcomes.
In this paper we work with a extended version of Andersen (1996), where
the underlying information ï¬‚ow process is determined by a Markov-switching

Volatility in Prediction Markets
605
stochastic volatility model (Carvalho and Lopes, 2007; So et al., 1998) that
allows for occasional shifts in the parameter determining its level. The incor-
poration of structural changes in the model solves the problem of overes-
timation of the persistency of volatility in high-frequency data (Lamoureux
and Lastrapes, 1990) while providing a straightforward way to identify clus-
ters of high and low information ï¬‚ow in the market. In Andersenâ€™s model,
returns and trading volume follow a mutually and serially independent bivari-
ate distribution, conditional on the number of information arrivals, where
information arrivals are modeled as a stochastic process. This is an empir-
ical representation of a well deï¬ned market microstructure model in which
both informed and liquidity traders (traders that do not react to news) partic-
ipate in determining prices in the market and where the volatility, or infor-
mation ï¬‚ow, process is inferred simultaneously from returns and trading
volume.
In detail, let Rt represent the logarithm of returns, Vt a measure of trading
volume and It the ï¬‚ow of information at time t. The model takes the following
form:
Rt âˆ¼N (0, It)
(21.1)
Vt âˆ¼Po (m0 + m1It)
(21.2)
log (It) = Â·St + â€š log (Itâˆ’1) + Ã‚t
(21.3)
St âˆ¼MS(P)
(21.4)
where Ã‚t âˆ¼N(0, Ã™2) and MS(P) denotes a two-stage ï¬rst order Markov-
switching process with transition probabilities deï¬ned by Pr(St = i|Stâˆ’1 = j) =
pi j, for (i, j) = 1, 2.
In the model, returns are conditionally normal with variances that reï¬‚ect
the intensity of information arrivals that also characterize how strongly volume
ï¬‚uctuates in response to news. In this context, m0 represents the participation
of liquidity traders in the market whereas the proportion of trades by â€˜informedâ€™
traders (or market makers, see Section 21.2.2) is deï¬ned by m1. Moreover,
the information ï¬‚ow It (also refereed to as volatility) follows a conditionally
stationary (|â€š| < 1) log AR(1) process where the level Â·St is determined through
a two-stage Markovian process. We will refer to St as the the level of information
(or information state), meaning that when St equals 1 there is a reduced level
of news arrival. This implies that Â·1 < Â·2 as these parameters deï¬ne the uncon-
ditional mean of the information ï¬‚ow process. It is important to point out that
the choice of two stages for St has, on the one hand, an applied motivation as we
are looking for a way to segregate between periods of low and high information.
This device is used by Carvalho and Lopes (2007) in ï¬‚agging currency crises in
emerging markets. On the other hand, evidence in the literature suggests that

606
The Oxford Handbook of Applied Bayesian Analysis
at least two stages are necessary to properly characterize the volatility of assets
(So et al., 1998; Lopes and Carvalho, 2007; Eraker et al., 2003).3
Equations (21.1)â€“(21.4) represent a complex, bivariate dynamic nonlinear
system with two latent (non-observable) states, the information ï¬‚ow It and the
information state St. Using standard priors, posterior estimation of this model
is carried by a customized Markov chain Monte Carlo algorithm described in
the appendix.
We implement the described model and construct a measure of information
ï¬‚ow for the 2004 presidential race. The data were acquired from Tradesports.
They include all transactions made on the Tradesports â€˜Bush wins the popular
voteâ€™ contract from July 1 to Election Day. The transactions were grouped into
3 hour time periods. This choice reï¬‚ects our hope to work with the ï¬nest
level of resolution while guaranteing that enough transactions occurred in
the period. Observations of the price of the contract at the close of the three
hour period and the volume traded in that period were recorded at 3am,
6am, 9am, noon, 3pm, 6pm, 9pm, and 11:59pm. Following the standard in
the literature (Andersen, 1996), volume was transformed to the log scale and
rounded to the closest integer due to the Poisson model speciï¬cation. Futures
prices, returns and volume are presented in Figure 21.1 and 21.2. Finally,
we catalog events from the Drudge Report (www.drudgereport.com) in the
same 3 hour window. We use the Drudge Report for several reasons. First, to
investigate whether the market reacts to campaign events, we need a â€˜time-
stampâ€™ on a news story. Traditionally, media studies have â€˜timedâ€™ events by
using the day that the story was on the front page of a major newspaper
such as the New York Times. Theoretically, we expect the market will react
much more quickly, so we need a more ï¬ne-grained measure. The Drudge
Report provides that; it is constantly updated and is archived every 15 minutes.
Second, the Drudge Report is a widely-read political gossip source. In a
recent book (Halperin and Harris, 2006), two inï¬‚uential political reporters
write:
It is a guarantee that most of the reporters, editors, producers, and talk show bookers who serve
up the daily national buffet of news recently have checked out his eponymous website, and that
www.drudgereport.com is bookmarked on their computers. That is one reason Drudge is the single
most inï¬‚uential purveyor of information about American politics.
People do use this as a source of information, and that information is on
the â€˜earlyâ€™ side â€“ Drudge routinely reports stories that the major media news
outlets have not yet reported or vetted. The most famous example is the Monica
Lewinsky story; the Drudge Report was the ï¬rst place to break most of the news,
3 We did assess the performance of a model with three regimes and due to the similarity between the
levels (Â·) of of two states we are conï¬dent that the model with two regimes is sufï¬cient and appropriate for
what we are trying to accomplish here.

Volatility in Prediction Markets
607
64
62
60
58
56
54
52
7/06
7/29 8/05
8/20
Time
9/08
Bush wins popular vote
9/19
9/30 10/08
10/26
Fig. 21.1 Closing price for â€˜Bush wins the popular voteâ€™ at every three-hour period. Source:
Tradesports.com.
0.02
0.00
â€“0.02
10
8
6
4
2
0
7/06
7/29 8/05
8/20
Returns
Volume
Time
9/08
9/19
9/30 10/08
10/26
7/06
7/29 8/05
8/20
Time
9/08
9/19
9/30 10/08
10/26
Fig. 21.2 Returns and trading volume.
and covered the story for weeks before the major news outlets. Because it is an
agenda-setter with early information and has precise time stamps, we feel that
this is the best available source.4
4 To collect the data, we recorded the headlines on the front page of the site at each three hour interval.
Major stories remained in the headlines for many periods. In theory, smaller stories could appear and
disappear in-between our data collection points. However, if that is the case, we ï¬nd it unlikely that that
could be driving the markets.

608
The Oxford Handbook of Applied Bayesian Analysis
2
1
0.012
0.008
0.004
0.000
7/06
7/29 8/05
8/20
Information State
Information Flow
Time
9/08
9/19
9/30 10/08
10/26
7/06
7/29 8/05
8/20
Time
9/08
9/19
9/30 10/08
10/26
Fig. 21.3 Posterior estimates of S1:T (top) and âˆšI1:T (bottom). We deï¬ne Ë†St = 2 if the posterior
Pr(St = 2) > 0.5. The red lines indicate the key events described in Table 21.2.
21.4 The 2004 presidential election
After ï¬tting our model, we can initially conclude that the ï¬‚ow of information
during the campaign was not static. Estimates of the states It and St are
presented in Figure 21.3 and a posterior summary of all parameters appears
in Table 21.1. Volatility rose signiï¬cantly at many points during the campaign,
though it remained low for much (but not all) of July, August, and September.
The duration of increased volatility also varied. Spikes in volatility were often
accompanied by longer stretches of a state of high volatility, indicating greater
uncertainty about the effect of new information. However, on many occasions,
the volatility increased to a high state and then quickly fell back to the normal
level, indicating that the effects of some new information were easily under-
stood and incorporated into the probability of Bushâ€™s electoral success. This is
also reï¬‚ected in the estimates of p11 and p22 given that p11 > p22.
We are able to assess whether the microstructure assumptions underlying
the proposed model are justiï¬ed by looking at the estimates of m0 and m1. If
m1 is close to zero that would indicate that the information arrival does not
affect â€˜informed tradersâ€™ and the model would be reduced to a simple stochastic
volatility model (Jacquier et al., 1994). From the estimates in Table 21.1 we
can conclude that the participants in this market do react to new information
as, on average, 50% of trades are related to the arrival of news when St = 2

Volatility in Prediction Markets
609
Table 21.1 Posterior summary.
Parameter
Posterior mean
95% Interval
Â·1
âˆ’0.369
(âˆ’0.395; âˆ’0.342)
Â·2
âˆ’0.306
(âˆ’0.327; âˆ’0.284)
â€š
0.967
(0.927;0.983)
Ã™2
0.019
(0.018;0.020)
m0
3.936
(3.81;4.03)
m1
28370
(25478;32005)
p11
0.983
(0.932;0.991)
p22
0.853
(0.801;0.912)
(high information state). When St = 1 (low information state) this proportion
goes down to 15% with m0 accounting for the remainder in both cases. These
numbers are very similar to the ï¬ndings in the ï¬nance literature as reported by
Andersen (1996) and Watanabe (2000).
We would like to underscore an important point: the fact that most cam-
paign events, particularly in mid-August to mid-September, had little or no new
information is not saying that the campaign did not have any impact. Rather,
it is saying that given the cast of characters and the political and economic
context, the campaign itself played out predictably. It is true that during the
month of August, there was an elevated state of information on just seven days,
for a total of under 70 hours (or 22 periods with our measure), and July had
just nine days with elevated information, for a total of just over 70 hours (or
24 periods with our measure). Despite a number of high-proï¬le news stories
during August, particularly the Swift Boat Veterans for Truth ads, the investors
in the market felt that the underlying context of the election did not change
very much. However, information that is not unexpected could, nonetheless,
have been crucial to the electoral outcome.
Electoral outcomes can be thought to be composed of three pieces: context
(economy, partisanship), standard campaign events, and election year-speciï¬c
events. First, we will address the events speciï¬c to the 2004 presidential election.
We will then present evidence regarding the effects of standard campaign
events like the nominating conventions. A list of the most inï¬‚uential events
appears in Table 21.2 and are marked by the red vertical lines in Figure 21.3.
21.4.1 National security and the 2004 election
Many people have described 2004 as a â€˜national securityâ€™ election. Opinion
polls showed that people were highly concerned about the issue â€“ in an ABC
News5 poll taken in early October, nearly half the respondents reported that
either terrorism or the war in Iraq were the most important issues for the
5 ABC News Poll. October 7, 2004. Data provided by The Roper Center for Public Opinion Research,
University of Connecticut. [USABC.100804.R5].

610
The Oxford Handbook of Applied Bayesian Analysis
Table 21.2 Key dates in 2004 presidential election.
July 6
Kerry picks Edwards as VP
July 29
Kerryâ€™s nomination speech
August 5
First Swift Boat Veterans for Truth advertisement airs
August 20
Second SBVT ad airs
September 8
Bushâ€™s National Guard service questioned in CBS story
September 19
CBS retracts the National Guard story and Dan Rather releases statement
September 30
First debate â€“ Bush performs poorly
October 8
Second debate
October 13
Third debate â€“ Kerry makes comment about Cheneyâ€™s daughter
October 26
Story about missing explosives in Iraq in 2003
next president to deal with, and more importantly, made their vote choice
while thinking about that issue (Newsweek, 10/29; Institute of Politics, 2006).6
Our analysis of the information ï¬‚ow during the campaign corroborates this:
a number of national security-related issues signiï¬cantly inï¬‚uenced the ï¬‚ow
of information in the market. We will discuss several of these: the Swift Boat
Veterans for Truth advertising campaign, the CBS story about Bushâ€™s National
Guard service, the news about missing weapons in Iraq, and Osama bin Ladenâ€™s
taped message released just before the election.
Swift Boat Veterans for Truth (SBVT) ads After the 2004 election, a new verb appeared
in political discussions: to â€˜swift boatâ€™ someone meant to damage his can-
didacy with unfair and â€˜dirtyâ€™ campaigning. The term itself may be useful,
but is it accurate to apportion so much impact to the Swift Boat Veterans
for Truth in the 2004 election? Our analysis suggests that while there was
an increase in information ï¬‚ow at two points during the height of the story,
the inï¬‚uence of the group was actually much less than is conventionally
believed.
Though a book had been released months before detailing them, allega-
tions made by the Swift Boat Veterans for Truthâ€™s that John Kerry had not
earned his medals in the Vietnam War became a major news story on August
4, when SBVT ï¬rst aired an advertisement. Because SBVT did not have
much money to buy air time for their ï¬rst ad, their strategy was to gener-
ate free media through the outlandish claims made in the ad. Needless to
say, the strategy worked. Their ï¬rst ad buy was for $500K, though after they
gained notoriety, they signiï¬cantly increased fundraising and by the end of the
campaign, they had spent about $19M on television advertising (Institute of
Politics, 2006).
News of the ï¬rst advertisement regarding Kerryâ€™s medals elevated the
information state for two days. Several days later, on August 9, the Drudge
6 Princeton Survey Research International/Newsweek Poll. October 29, 2004. Data provided by The
Roper Center for Public Opinion Research, University of Connecticut. [USPSRNEW.103004.R15].

Volatility in Prediction Markets
611
Report and others began questioning whether John Kerry was in Cambodia
on Christmas 1969, as he previously claimed. It was later determined (and
Kerry conï¬rmed) that he was in Cambodia, but he was there in January 1970,
not on Christmas. In retrospect, a rather unimportant point, but the markets
reacted: the volatility increased on August 9 and 10. SBVT remained a major
news story for most of August and has since made it into the vernacular, but
the â€˜informationâ€™ from the SBVT allegations was apparently fully incorporated
in approximately 36 hours (12 of our periods) over four days. There is another
spike in the information ï¬‚ow on August 12 and 13, though there does not
appear to be speciï¬c information about the SBVT allegations during that time.
Even if that increase in volatility is attributed to SBVT, the information was
incorporated into the market in just about 50 hours. It should be noted that
the debates increased the information ï¬‚ow for a longer period of time than did
the SBVT story (see below) and that the story regarding Bushâ€™s National Guard
service (see below) increased the information ï¬‚ow for about the same amount
of time. Additionally, though it is not the point of this paper, it is interesting to
note that the likelihood that Bush would win the election did not change much
from the beginning of August to the beginning of the Republican convention on
August 30; the likelihood went from about 54% to about 56%. At the very least,
because the change in the probability of Bush winning reelection was slight and
our analysis suggests that the information was incorporated relatively quickly,
it seems incorrect to describe Kerryâ€™s loss as a result of the â€˜swift-boatingâ€™ that
took place in August 2004.
CBS story on Bushâ€™s national guard story John Kerryâ€™s military service was discussed
frequently (by his campaign, his supporters, and also his detractors) throughout
the primary and general election campaign. However, George W. Bushâ€™s military
service had largely been ignored until September 8, when CBS News aired a
story on â€˜60 Minutesâ€™ that Bush had not properly reported for duty while he was
in the National Guard. Dan Ratherâ€™s reporting highlighted documents allegedly
written by Bushâ€™s commander in the National Guard. The story developed over
12 days, with conservative bloggers immediately questioning the story and the
documents and investigating the print on 1970s typewriters. It was several
days into the story that the information state increased, on September 13,
which coincided with the ï¬rst reports by major news organizations that the
papers were forged. The information ï¬‚ow remained high for most of the time
between the 13th and the 20th and peaked on Sunday, September 19, when
the â€˜60 Minutesâ€™ producer wrote an editorial in the Washington Post that stated
the story was incorrect. It remained high until Dan Rather released a statement
on the evening of September 20 admitting that the story was not properly
reported and that the evidence was false. As this story developed, the volatility
in the market rose and fell several times, indicating that new information was

612
The Oxford Handbook of Applied Bayesian Analysis
impacting investors and that it took some time for the potential consequences
of the story to become clear.
Missing explosives in Iraq Another national security issue burst into the campaign
one week before Election Day. On Monday, October 25, the New York Times
broke the story that 380 tons of powerful explosives vanished from an Iraq
storage depot when the cache was supposed to be secured by American troops.
Early that morning, when the Drudge Report initially reported it (presumably
after it had appeared on the New York Times website), the information state
increased immediately. Because this story went along with Senator Kerryâ€™s
accusations that President Bush had incompetently handled the Iraq war
after the initial invasion and the news was completely unexpected, it was
viewed as informative. This heightened state of information lasted for three
days.
Osama bin Laden videotape The last days of the campaign showed an elevated level
of volatility. On the Friday before the election (October 29), Al-Jazeera television
showed a taped message from Osama bin Laden. After being authenticated
by the FBI, American television stations began showing it. Bin Laden did not
directly address the election, but he did say that, â€˜Your security is not in the
hands of Kerry or Bush or al Qaeda. Your security is in your own hands. Any
nation that does not attack us will not be attackedâ€™.7 When the ad was ï¬rst played,
the information ï¬‚ow increased dramatically; evidently people thought that bin
Ladenâ€™s discussion of 9-11 could inï¬‚uence the election by raising the salience of
the issue. The information state remained high throughout the last three days
of the campaign, in part because of the tape but presumably also because of
new polls and reports of frenzied campaigning.
21.4.2 Effects of standard campaign events
Scholars have found that nominating conventions (Campbell et al., 1992
Holbrook, 1996) and presidential debates (Lanoue, 1992; Benoit et al., 2003;
Hillygus and Jackman, 2003) regularly impact voters. This is in part due to the
fact that these events are highly publicized. Presidential debates are watched by
millions of people; in 2004, 62.5 million viewers tuned into the ï¬rst debate, 46.7
million watched the second, and 51 million people watched the third.8 Viewer-
ship of nominating conventions has declined, but the candidateâ€™s acceptance
speeches are still viewed by millions of people. It is debated whether or not
the choice of a vice presidential candidate has an inï¬‚uence on the outcome,
7 â€˜Bin Laden: â€œYour security is in your own handsâ€ â€™. CNN, Friday, October 29, 2004 Posted: 10:05 PM EDT.
www.cnn.com/2004/WORLD/meast/10/29/bin.laden.transcript/, viewed February 17, 2008.
8 This is in comparison the 15.2 million who watched Foxâ€™s coverage of baseball playoffs. Though we
ï¬nd this heartening for democracy, it is nonetheless shocking. We have no idea why anyone would choose
a debate over a Red Sox â€“ Yankees game.

Volatility in Prediction Markets
613
but at the least we can agree that the selection is a high-volume news event.
Our measure of information ï¬‚ow reveals that the John Edwardsâ€™ selection as
John Kerryâ€™s running mate was viewed as important by investors and that the
debates contained a signiï¬cant amount of information. For the most part and
contrary to prior ï¬ndings, we show that the nominating conventions did not
alter expectations about the electionâ€™s outcome.
Vice presidential nomination Political observers begin discussing possible vice pres-
idential candidates early in the nominating process. Though it is unclear
whether or not the vice presidential candidate adds much to the partyâ€™s ticket,
the conventional wisdom suggests that it can potentially impact election out-
comes by winning additional states or bringing the party together after a nom-
ination battle (Mayer, 2000). Our estimates of information ï¬‚ow suggest John
Kerryâ€™s announcement that he would pick John Edwards, former senator from
North Carolina, was considered to have a potential impact on the electionâ€™s
outcome.
In 2004, after a strong second-place showing in the Iowa caucus, John
Edwards was viewed as affable and a charismatic speaker. More importantly,
he could perhaps help Kerry win the support of southern voters. On July 6,
the day that the Kerry campaign announced that Edwards would be the vice
presidential candidate, the information state was heightened. Investors thought
that this could affect the outcome of the election. Interestingly, Edwardsâ€™ selec-
tion was incorporated into expectations relatively quickly; the information state
remained elevated for only 18 hours.
Nominating conventions Presidential nominating conventions have become highly
choreographed media spectacles, though many people do watch parts of the
four day conventions. Prior research suggests that party conventions do move
voters (e.g. Holbrook, 1996). These ï¬ndings are based on polls taken just before
and just after conventions. Because our information ï¬‚ow variable is continuous
and responds immediately to new information, we are able to better investigate
the dynamics associated with conventions.
In 2004, the Democratic convention was held in Boston, Mass. on July
26â€“29 and the Republican convention was held in New York City on August
30â€“September 2. Our ï¬ndings suggest that only one small part of one conven-
tion affected expectations about the ultimate election outcome: Kerryâ€™s accep-
tance speech on July 29 started an information incorporation period. The infor-
mation state remained elevated for a full day.9 At ï¬rst, the ï¬nding that conven-
tions had little effect seems somewhat surprising, though after further consid-
eration, it makes sense: expected information has previously been incorporated
9 We assume that this elevated information ï¬‚ow was due to Kerryâ€™s speech, and not to the failure of the
balloons to drop immediately after his speech.

614
The Oxford Handbook of Applied Bayesian Analysis
into the marketâ€™s prices. After four years, there was little that voters could learn
about Bush or his policies and nearly all had opinions of him. The fact that
volatility did not increase in demonstrates that the Republican convention did
not contain any unexpected information.
In contrast, John Kerry was relatively unknown before his presidential bid;
before the Democratic convention many people did not know much about Kerry,
his background, or his policies. His speech was meant as an introduction to the
candidate, and evidently investors thought that that introduction was, in fact,
informative. Before the convention, Kerryâ€™s chances to win the popular vote
were about even, and they remained around 50% after his speech. An analysis
of simply the mean price, or probability, of a futures contract would suggest
that the convention was completely irrelevant. This volatility measure suggests
that despite the fact that there was not a signiï¬cant change in price, people
did learn about Kerry from the convention. The information acquired in this
period likely conditioned the effect of later information and was thus indirectly
important.
Both campaigns note that a key moment during the campaign occurred
during the Republican convention (Institute of Politics, 2006). On September 1,
Chechen terrorists invaded an elementary school in the Russian town of Beslan;
after the Russian government stormed the school several days later, 200 people,
mostly children, were killed. The campaigns argued that this reinforced Bushâ€™s
message that the world is dangerous. Our analysis ï¬nds no such impact.
Clearly, this incident could not possibly have been forecast by investors in
the market, so what explains this? We can only speculate that because people
already placed the issue of terrorism as a high priority for the next president, a
certain ceiling effect existed; if the issue was ranked as the most important, it
would be hard for the salience to increase very much.
Debates There is a signiï¬cant amount of evidence suggesting that televised
presidential debates impact elections (Geer, 1988; Benoit et al., 2003), though
â€˜their impact has ï¬‚uctuated, from inconsequential to decisiveâ€™ (Graber, 1993). Even
if debates do not change the winner of the election, citizens can and do learn
from them (Lanoue, 1992; Holbrook, 1994; Miller and MacKuen, 1979). Our
research conï¬rms that debates provide a signiï¬cant amount of information. In
2004, there were three presidential debates: on Thursday, September 30; Friday,
October 8; and Wednesday, October 13. Bush was widely viewed as performing
poorly in the ï¬rst debate (e.g. Los Angeles Times, 10/2), though the reviews of the
second and third debates were mixed. Each debate increased the volatility of the
market. In particular, the ï¬rst and third debates led to an elevated information
state for several days after the event. This extended length of volatility ï¬ts
with ï¬ndings that post-debate news analysis conditions peopleâ€™s responses to
debates (Hwang et al., 2007).

Volatility in Prediction Markets
615
The volatility surrounding the days of the debates highlights the difï¬culty iso-
lating the cause of that increased information. The information ï¬‚ow increased
immediately following the ï¬rst debate, so it seems reasonable to conclude that
that increase was caused by the debate. However, while the debate was the
most notable event that occurred from September 30 to October 4 (when the
volatility died down), a number of other plausible stories can be told for why
the information ï¬‚ow was high three days later, on October 3rd.
Similarly, the level of volatility increased more than a day before the third
debate and continued for more than three days after. What information was the
market incorporating? In that debate, when asked whether he believed homo-
sexuality was a choice, Kerry replied, â€˜Dick Cheneyâ€™s daughter, who is a lesbian,
she would tell you that sheâ€™s being who she wasâ€™. Both liberals and conservatives
found the comment awkward and odd and thought he should not have said
it. Was it that comment, or the fact that the polls moved only slightly over the
course of the three debates, that caused the increased volatility? Or was it some
other event? Days after that debate, the popular summary source of political
information, ABCâ€™s The Note, commented:
Because the polls moved slightly in Bushâ€™s favor after the debates, there must be a reason, and the
only two reasons [the media] have been able to come up with are (i) the Mary Cheney remark; and/or
(ii) the nation, having considered the totality of the debate round robin, decided it wanted a steady,
likeable leader â€“ rather than a voluble debating champion â€“ to be the honcho of the free world. We
have no idea if those are the reasons, but thatâ€™s the best anyone has offered that has smacked against
our ears.
The problem of pinpointing causality in this kind of case seems, to us,
intractable. Our method points to the fact that the debates added information to
campaign, but we cannot apportion how much is due to comments during the
debate, media analysis immediately following the debate, or even to separate
events that happen the next day. In July and August, when less is going on in
the campaign, it is easier to isolate events. By mid-October, so many polls are
being released, so much money is being spent, and so many people are paying
attention that it is much more difï¬cult to make any kind of causal statement.
21.5 Concluding remarks
The approach we develop here provides a novel and useful way to investigate
the effects of campaigns. The information ï¬‚ow measure allows us to recognize
key moments and separate the anticipated, normal dynamics of campaigns
from the unexpected occurrences. By using data from prediction markets and a
model that attempts to characterize the reactions of market participants to new
information, we are able to address an as-yet unresolved question in political
science. We build on a vast literature in ï¬nance and economics and propose a

616
The Oxford Handbook of Applied Bayesian Analysis
model with two volatility regimes that simpliï¬es the task of associating events
with periods of high information. The development and estimation of these
models is greatly facilitated by the use of the Bayesian framework.
We identify a number of events that increased the information ï¬‚ow in the
2004 campaign. Among the standard campaign events, we ï¬nd that the selec-
tion of the vice presidential candidate and the televised debates increased volatil-
ity in the prediction market. John Kerryâ€™s acceptance speech at the Democratic
nominating convention also increased volatility though neither the rest of the
convention or any of the Republican convention introduced new information.
A number of events speciï¬c to the 2004 election were also viewed as informative
by investors. In particular, stories related to national security-related issues
increased volatility. These include the report that explosives vanished in Iraq
under the USâ€™s watch, the CBS story about Bushâ€™s National Guard service and
its subsequent retraction, and the release of the bin Laden tape a few days before
the election. We ï¬nd evidence that the Swift Boat Veterans for Truth ads that
were critical of John Kerryâ€™s military service record were somewhat informative,
but not nearly as central to the outcome of the 2004 campaign as is popularly
believed.
We would like to reiterate two important points about this project. First, our
goal was to develop a quantitative way to objectively describe the information
level over the course of a presidential campaign. The price of the â€˜Bush wins
the popular voteâ€™ futures contract chronicles the likelihood of Bush winning
in 2004, and clearly whether or not certain events or certain classes of events
impact that likelihood is an important question. However, the question of
whether an event is informative is related to, but distinct from, the question of
what effect a given event has on a particular election. That is, certain events can
contain signiï¬cant information even if they do not alter the outcome in a partic-
ular year. For example, if there is an overriding factor in the election like a bad
economy then a candidateâ€™s acceptance speech at the party convention may not
change the likelihood that the incumbent party will lose. Nonetheless, speeches
at conventions do contain information, and in a year where the economy was
not so salient, that speech could alter the election outcome. Put differently,
some information that comes out during the campaign may be trumped by
other information or considerations and thus not impact the vote percentage
of a candidate. But that event does, in fact, have information in it. In a year
with a different underlying political context, that same event may move vote
totals signiï¬cantly. Because our model focuses on the information level itself,
we hope our model can be used to develop a more a general theory of campaign
event effects over time.
Second, when we ï¬nd relatively low levels of information in some given
period of time, we are not saying that the campaign itself during those periods
is irrelevant. We certainly believe that campaigns matter. The expectations about

Volatility in Prediction Markets
617
what will happen on election day are dependent on our prior understanding of
how campaigns generally progress. When we ï¬nd low levels of information, we
are only arguing that those are periods where people are not learning anything
new about the election.
We believe our model is a promising technique for research on campaigns
and elections. We will be able to test it and our theory more explicitly in the
future because prediction markets grow in popularity in each election cycle.
Intrade has had contracts on the party nominees and the winner of the 2008
general election trading with high volume for months. Going forward with
this research, we would like to ï¬nd ways to make stronger causal claims. The
question of causality looms large in the social sciences, and this work is no
exception. We want to extend our modelling approach in an attempt to more
clearly isolate the effects of different events. Addressing that becomes both
more difï¬cult and more important in the last weeks of the campaign. The 2008
Democratic primary season provides a particularly useful testing ground for us;
because partisanship cannot be the deciding factor between two candidates in
a primary, we are likely to see more dramatic movement in the markets during
the campaign as the situation progresses.
There certainly is no shortage of political pundits, campaign strategists, and
journalists willing to declare â€˜what the election was aboutâ€™ and highlight the
â€˜turning pointâ€™ in the campaign, the event or story that pushed the campaign
towards its ultimate result. Undoubtedly, pundits and politicos are sometimes
correct; but often they are not. When we explain election outcomes with a
compact anecdote, we can both overlook events that were actually important and
overstate our conï¬dence in our conclusions. A Bayesian modeling approach
to this problem, combined with theoretical insight from other ï¬elds, gives us
greater leverage on a question that is of both academic interest and practical
signiï¬cance â€“ it helps us to understand the dynamics and responsiveness of
our democratic institutions.
Appendix
A. Broader context and background
A.1 Posterior computation in state-space models
Consider the general class of state-space models (West and Harrison, 1997):
yt âˆ¼py(xt, Ã‹)
(21.5)
xt âˆ¼px(xtâˆ’1, Ã‹),
(21.6)
where xt represents the states and yt the vector of observations, both at time t.
Observations yt are treated as independent thought time conditionally on the

618
The Oxford Handbook of Applied Bayesian Analysis
hidden states xt and ï¬xed parameters Ã‹. Moreover, the functional form of the
densities px(Â·) and py(Â·) is assumed known. These models are often referred
to as hidden Markov models (Scott, 2002) and are extensively used in many
scientiï¬c ï¬elds, including economics, biology, engineering, etc.
After observing a sequence y1:T, the central element for inference is the joint
posterior distribution p(x1:T, Ã‹|y1:T), which, in most cases, cannot be assessed
analytically and its exploration requires the development of posterior simulation
procedures. Following basic Markov chain Monte Carlo ideas the most direct
approach to posterior simulation is to iterate thought the full conditional pos-
terior of individual states p(xt|x(âˆ’t), Ã‹, y1:T) and ï¬xed parameters p(Ã‹|x1:T, y1:T).
This strategy is generally referred to as single-move MCMC (Carlin et al., 1992).
Given the markovian structure of the model the full posterior for each state can
be expressed as
p(xt|x(âˆ’t), Ã‹, y1:T) âˆp(xt|xtâˆ’1, Ã‹)p(xt+1|xt, Ã‹)p(yt|xt, Ã‹).
(21.7)
If direct simulation from (21.7) is possible the single-move MCMC becomes
a Gibbs sampler, however in many situations draws from (21.7) require a
Metropolis â€“ Hastings step.
Although very general, the single-move MCMC can mix very slowly due to the
high temporal dependence between states. In some models, the Markov struc-
ture can be exploited in the construction of a block-move MCMC that iterates
through p(x1:T|Ã‹, y1:T) and p(Ã‹|x1:T, y1:T). Drawing the states jointly reduces the
the autocorrelation of the Markov chain providing signiï¬cant efï¬ciency gains in
the MCMC sampler. This process begins by writing the joint
p(x1:T|Ã‹, y1:T) = p(xT|Ã‹, y1:T)
Tâˆ’1

t=1
p(xt|xt+1, Ã‹, y1:t),
(21.8)
and noticing that samples from (21.8) can be obtained by ï¬rst sampling xT âˆ¼
p(xT|Ã‹, y1:T) followed by, for all t, samples from xt âˆ¼p(xt|xt+1, Ã‹, y1:t). This
procedure requires the availability of both p(xT|Ã‹, y1:T) and p(xt|xt+1, Ã‹, y1:t).
This is always possible for situations where the states are discrete (Scott,
2002) or in Dynamic Linear Models (DLM) (Carter and Kohn, 1994; Fruhwirth-
Schnatter, 1994) where both px(Â·) and py(Â·) are normal distributions linear in
the states. This strategy is also known as forward ï¬ltering backward sampling
algorithms where a forward ï¬lter is necessary to assess p(xT|Ã‹, y1:T) that are
then propagated backwards through p(xt|xt+1, Ã‹, y1:t). Details of the implemen-
tation in DLM are presented in Chapters 4 and 15 of West and Harrison
(1997) while elements of the discrete case are described below in the drawing
of S1:T.
Connecting back to the model described in (21.1), we are working with a
bivariate, non-normal, state space model with one continuous state ÃŽt and one

Volatility in Prediction Markets
619
discrete state St. Posterior simulation for S1:T will be possible through a block-
move MCMC whereas a single-move MCMC is necessary for ÃŽ1:T. We complete
the model with the following priors: Â·i âˆ¼N(mÂ·i, CÂ·i), â€š âˆ¼N(mâ€š, Câ€š)I{âˆ’1<â€š<1},
Ã™2 âˆ¼IG(aÃ™, bÃ™), pii âˆ¼Be(ai, bi), m0 âˆ¼Ga(am0, bm0) and m1 âˆ¼Ga(am1, bm1). Let
Ã‹ =

m0, m1, Â·1, Â·2, â€š, Ã™2, P

. Based on the following decomposition of the joint
posterior distribution
p (I1:T, S1:T, Ã‹|R1:T, V1:T) âˆp (R1:T|I1:T) p (V1:T|I1:T, Ã‹)
p (I1:T|S1:T, Ã‹) p (S1:T|Ã‹) p (Ã‹)
(21.9)
we can generate posterior samples thought a MCMC schemes deï¬ned by the
full conditionals described below.
A.2 Drawing the states I1:T and S1:T
The full conditional for S1:T can be written as:
p (S1:t|Â·) = p (ST|ÃŽ1:T, Ã‹)
1

t=Tâˆ’1
p (St|St+1, ÃŽ1:T, Ã‹)
âˆp (ST|ÃŽ1:T, Ã‹)
1

t=Tâˆ’1
p (St+1|St) p (St|ÃŽt, Ã‹) .
(21.10)
This allows for a block sampling of S1:T through a forward ï¬ltering backward
sampling algorithm as described above (?)see also][]Kim99. In detail, assume
that at time t we have available the set of posterior probabilities p (St = i|ÃŽt, Ã‹),
for i = 1, 2 (clearly, this can be generalized to k discrete states). Prior probabili-
ties for time t + 1 can be obtained via
p (St+1 = j|ÃŽt, Ã‹) =
2

i=1
p (St+1 = j|St = i, Ã‹) p (St = i|ÃŽt, Ã‹) .
(21.11)
This probabilities are then updated via
p (St+1 = j|ÃŽt+1) âˆp (ÃŽt+1|St+1 = j, Ã‹) p (St+1 = j|ÃŽt, Ã‹) .
(21.12)
The above steps provide all the necessary distributions for the generation of a
joint draw of S1:T.
Sampling of It is the only â€˜hardâ€™ step in the MCMC. The full conditional
p(It|Iâˆ’t, Â·) takes the following form
p (It|Iâˆ’t, Â·) âˆexp

âˆ’1
2It
R2
t

exp (m0 + m1It) (m0 + m1It)Vt
(21.13)
Ã— exp

âˆ’1
2Ã™2

ÃŽt âˆ’Â·St âˆ’â€šÃŽtâˆ’1
2

exp

âˆ’1
2Ã™2

ÃŽt+1 âˆ’Â·St+1 âˆ’â€šÃŽt
2

.
(21.14)

620
The Oxford Handbook of Applied Bayesian Analysis
Joint samples from the full conditional of I1:T cannot be generated without
approximations such as de Jong and Shephard (1995). In this work, we imple-
ment a single-move MCMC and generate draws from p(It|Iâˆ’t, Â·). Random-
walk metropolis was used. It is our experience that this simple approach
performed well enough and our MCMC presented satisfactory convergence
diagnostics.
A.3 Drawing Ã‹
Full conditionals for Ã‹ can be easily obtained as follows:
(Â·i|Â·) âˆ¼N
Ti
Ã™2 + 1
CÂ·i
 
t:St=i ÃŽt âˆ’â€šÃŽtâˆ’1
Ã™2
+ mÂ·i
CÂ·i

,
Ti
Ã™2 + 1
CÂ·i
âˆ’1
;
(21.15)
(â€š|Â·) âˆ¼N
T
t=1 ÃŽ2
tâˆ’1
Ã™2
+ 1
Câ€š
 T
t=1

ÃŽt âˆ’Â·St

ÃŽtâˆ’1
Ã™2
+ mâ€š
Câ€š

,
T
t=1 ÃŽ2
tâˆ’1
Ã™2
+ 1
Câ€š
âˆ’1âŽ«
âŽ¬
âŽ­;
(21.16)
(Ã™2|Â·) âˆ¼IG

aÃ™ + T
2 ,
bÃ™ + 1
2
T

t=1
ÃŽt âˆ’Â·St âˆ’â€šÃŽtâˆ’1

;
(21.17)

p11| Â·

âˆ¼Be

a1 + n11, b1 + n12

;
(21.18)

p22| Â·

âˆ¼Be

a2 + n22, b2 + n21

;
(21.19)
p(m0|Â·) âˆexp
6
âˆ’(bm0 + T) m0 +
T

t=1
log

m
am0âˆ’1
0
(m0 + m1It)Vt7
;
(21.20)
p(m1|Â·) âˆexp
6
âˆ’

bm1 +
T

t=1
It

m1 +
T

t=1
log

m
am1âˆ’1
1
(m0 + m1It)Vt7
, (21.21)
where ÃŽt = log(It), Ti represents the number of observations allocated to the
ith state and ni j are counts of transitions from state i to j. Sampling from
the above distributions is straightforward. The only minor complication is the
Metropolis step necessary for sampling m0 and m1. We use a random-walk
Metropolis with normal proposals. Initial values where deï¬ned based on the
MLE of both parameters in a Poisson regression where I1:T was ï¬xed at the
posterior mean from a standard two stage Markov-switching stochastic volatility
model where an efï¬cient Gibbs sampler can be implemented. These estimates
of the information ï¬‚ow are also used as initial values for I1:T.

Volatility in Prediction Markets
621
References
Andersen, T. (1996). Return volatility and trading volume: An information ï¬‚ow interpretation
of stochastic volatility. Journal of Finance, 51, 169â€“204.
Benoit, W. L., Hansen, G. and Verser, R. (2003). A meta-analysis of the effects of viewing us
presidential debates. Communication Monographs, 70, 335â€“350.
Berg, J., Nelson, F. and Rietz, T. (2003). Accuracy and forecast standard error of prediction
markets. Discussion paper, Tippie College of Business, University of Iowa.
Berg, J. and Rietz, T. (2006). The Iowa electronic market: Lessons learned and answers yearned.
In Information Markets: A New Way of Making Decisions, (ed. R. Hahn and P. Tetlock), AEI
Press, Washington, DC.
Bernhard, W. and Leblang, D. (2006). Democratic Processes and Financial Markets: Pricing Politics.
Cambridge University Press, New York.
Brown, G. and Hartzell, J. (2001). Market reaction to public information: The atypical case of
the Boston celtics. Journal of Financial Economics, 60, 333â€“370.
Campbell, J., Cherry, L. and Wink, K. (1992). The convention bump. American Politics
Research, 20, 287â€“307.
Carlin, B., Polson, N. and Stoffer, D. (1992). A Monte Carlo approach to nonnormal
and nonlinear state-space modeling. Journal of the American Statistical Association, 87,
493â€“500.
Carter, C. and Kohn, R. (1994). On Gibbs sampling for state space models. Biometrika, 81,
541â€“553.
Carvalho, C. and Lopes, H. (2007). Simulation-based sequential analysis of Markov switching
stochastic volatility models. Computational Statistics and Data Analysis, 51, 4526â€“4542.
Chan, W. (2003). Stock price reaction to news and no-news: drift and reversal after headlines.
Journal of Financial Economics, 70, 223â€“260.
de Jong, P. and Shephard, N. (1995). The simulation smoother for time series models. Bio-
metrika, 82, 339â€“350.
Dubinsky, A. and Johannes, M. (2006). Earnings annoucement and equity options. Work-
ing paper. University of Columbia Business School. Available at SSRN: http://ssrn.com/
abstract=600593.
Eraker, B., Johannes, M. and Polson, N. (2003). The impact of jumps in volatility and returns.
The Journal of Finance, 58, 1269â€“1300.
Fleming, J., Kirby, C. and Ostdiek, B. (2006). Stochastic volatility, trading volume, and the daily
ï¬‚ow of information. The Journal of Business, 79, 1551â€“1590.
Forsythe, R., Frank, M., Krishnamurthy, V. and Ross, T. (1998). Markets as predictors of election
outcomes: Campaign events and judgement bias in the 1993 UBC election stock market.
Canadian Public Policy, 24, 329â€“351.
Forsythe, R., Nelson, F., Neumann, G. and Wright, J. (1992). Anatomy of an experimental
political stock market. American Economic Review, 82, 1142â€“1161.
Forsythe, R., Rietz, T. and Ross, T. (1999). Wishes, expectations and actions: a survey on price
formation in election stock markets. Journal of Economic Behavior and Organization, 39,
83â€“110.
Fruhwirth-Schnatter, S. (1994). Data augmentation and dynamic linear models. Journal of Time
Series Analysis, 15, 183â€“202.
Gallant, A., Rossi, P. and Tauchen, G. (1992). Stock prices and volume. Review of Financial
Studies, 5, 199â€“242.
Geer, J. G. (1988). The effects of presidential debates on the electorateâ€™s preferences for candi-
dates. American Politics Quarterly, 16, 486â€“501.

622
The Oxford Handbook of Applied Bayesian Analysis
Graber, D. (1993). Mass Media and American Politics, (4th edn). Congressional Quarterly Press,
Washington.
Granberg, D. and Brent, E. (1983). When prophecy bends: The preferenceâ€“expectation link
in US presidential elections, 1952â€“1980. Journal of Personality and Social Psychology, 45,
477â€“491.
Gronke, P. and Brehm, J. (2002). History, heterogeneity, and presidential approval: a modiï¬ed
arch approach. Electoral Studies, 21, 425â€“452.
Grossman, S. and Stiglitz, J. (1980). On the impossibility of informationally efï¬cient markets.
American Economic Review, 70, 393â€“408.
Halperin, M. and Harris, J. F. (2006). The Way to Win: Taking the White House in 2008. Random
House, New York.
Hartog, D. and Monroe, N. (2008). The value of majority status: The effect of Jeffordsâ€™s switch
on asset prices of republican and democratic ï¬rms, Legislative Studies Quarterly, 33, 63â€“84.
Hillygus, D. and Jackman, S. (2003). Voter decision making in election 2000: Campaign
effects, partisan activation, and the Clinton legacy. American Journal of Political Science, 47,
583â€“596.
Holbrook, T. (1994). The behavioral consequences of vice-presidential debates: Does the under-
card have any punch? American Politics Research, 22, 469â€“482.
Holbrook, T. (1996). Do Campaigns Matter? Sage Publications, London.
Hwang, H., Gotlieb, M., Nah, S. and McLeod, D. (2007). Applying a cognitive-processing model
to presidential debate effects: Postdebate news analysis and primed reï¬‚ection. Journal of
Communication, 57, 40â€“59.
Jacquier, E., Polson, N. and Rossi, P. (1994). Bayesian analysis of stochastic volatility models.
Journal of Business and Economic Statistics, 12, 371â€“389.
Jennings, R. and Starks, L. (1985). Information content and the speed of stock price adjustment.
Journal of Accounting Research, 23, 336â€“350.
Kim, C. and Nelson, C. (1999). State-Space Models with Regime Switching: Classical and Gibbs-
Sampling Approaches with Applications. The MIT Press, Cambridge, Massachusetts.
Lamoureux, C. and Lastrapes, W. (1990). Persistence in variance, structural change and the
garch model. Journal of Business and Economic Statistics, 8, 225â€“234.
Lanoue, D. (1992). One that made a difference: Cognitive consistency, political knowledge, and
the 1980 presidential debate. Public Opinion Quarterly, 56, 168â€“184.
Lopes, H. and Carvalho, C. (2007). Factor stochastic volatility with time varying loadings and
markov switching regimes. Journal of Statistical Planning and Inference, 137, 3082â€“3091.
Mayer, W. G. (2000). In Pursuit of the White House 2000: How we Choose our Presidential
Nominees. Chatham House Publishers, New York.
Miller, A. and MacKuen, M. (1979). Learning about the candidates: The 1976 presidential
debates. Public Opinion Quarterly, 43, 326â€“346.
Oliven, K. and Rietz, T. (2004). Suckers are born but markets are made: Individual rationality,
arbitrage, and market efï¬ciency on an electronic futures market. Management Science, 50,
336â€“351.
Patell, J. and Wolfson, M. (1984). The intraday speed of adjustment of stock prices to earnings
and dividend announcements. Journal of Financial Economics, 13, 223â€“252.
Popkin, S. (1991). The Reasoning Voter: Communication and Persuasion in Presidential Cam-
paigns. University of Chicago Press, Chicago.
Sauer, R. (1998). The economics of wagering markets. Journal of Economic Literature, 36,
2021â€“2064.
Scott, S. (2002). Bayesian methods for hidden Markov models: Recursive computing in the 21st
century. Journal of the American Statistical Association, 97, 337â€“351.

Volatility in Prediction Markets
623
So, M., Lam, K. and Li, W. (1998). A stochastic volatility model with Markov switching. Journal
of Business and Economic Statistics, 16, 244â€“253.
Uhlaner, C. and Grofman, B. (1986). The race may be close but my horse is going to win: Wish
fulï¬llment in the 1980 presidential election. Political Behavior, 8, 101â€“129.
Watanabe, T. (2000). Bayesian analysis of dynamic bivariate mixture models: Can they explain
the behavior of returns and trading volume? Journal of Business and Economic Statistics, 18,
199â€“210.
West, M. and Harrison, P. (1997). Bayesian Forecasting and Dynamic Models. Springer-Verlag,
New York.
Wolfers, J. and Zitzewitz, E. (2004). Interpreting prediction market prices as probabilities.
NBER Working Paper No. 12200, National Bureau of Economic Research, USA. Available
at www.nber.org/papers/w12200.

Â·22Â·
Bayesian analysis in item response
theory applied to a large-scale
educational assessment
Dani Gamerman, Tuï¬M. Soares and FlÃ¡vio B. GonÃ§alves
22.1 Introduction
The OECDâ€™s1 Programme for International Student Assessment (PISA) collects
information, every three years, about 15-year-old students in several countries
around the world. It examines how well students are prepared to meet the chal-
lenges of the future, rather than how well they master particular curricula. The
data collected in PISA surveys contains valuable information for researchers,
policy makers, educators, parents and students. It allows countries to assess
how well their 15-year-old students are prepared for life in a large context and
to compare themselves to other countries. As it is mentioned in PISA 2003
Technical Report2, it is now recognized that the future economic and social
well-being of countries is closely linked to the knowledge and skills of their
population.
The ï¬rst PISA survey was in 2000 and 43 countries participated in the assess-
ment. The others were in 2003 with 41 countries, in 2006 with 57 countries
and in 2009 with 67 countries. At least one country from each continent is
present in the surveys, the majority being from Europe. A list with the countries
participating in 2003 is presented in Appendix B.
Since PISA is applied to a large number of countries, many differences are
found among the students participating, from cultural to curricular ones. These
differences may inï¬‚uence the characteristics of some items in each country.
For example, if an item is strongly related to some cultural aspect of an speciï¬c
country, it is expected that this item would be easier for students from this
country than for students from another one where such cultural aspect is not
present. If this difference is not taken into account, such item is not good to
assess the abilities of students from these two countries. Such phenomenon is
called differential item functioning (DIF) and has been heavily studied in the
1 OECD â€“ Organisation for Economic Co-Operation and Development.
2 PISA 2003 Technical Report. http://www.pisa.oecd.org/dataoecd/49/60/35188570.pdf.

Item Response Theory Applied to Educational Assessment
625
psychometric literature. In general, one group is ï¬xed as the reference group and
the other ones as the focal groups, and the item functioning in the latter ones is
compared to the item functioning in the reference group.
DIF analysis can be separated in two different stages. The ï¬rst one is the
detection of items with DIF and the second one is the quantiï¬cation and
explanation of the DIF detected in the ï¬rst stage. Normally, the latter one is
based on regression structures to identify features of the items that present
differential functioning. Usually, the ï¬rst stage is divided in two substages:
the estimation of the students abilities or a matching of the students using
groups, where students in the same group are assumed to have the same
ability; and the DIF detection using these estimated abilities. Both substages
are linked because, in general, it is impossible to generate abilities which are
totally free of DIF. Therefore, the most natural approach would be to treat them
together.
However, along with the integrated modelling of the DIF, comes the problem
of model identiï¬ability, which reï¬‚ects the conceptual difï¬culty of the problem.
For this reason, additional hypotheses on the DIF parameters of the model are
assumed in order to guarantee identiï¬ability. Nevertheless, such hypotheses
may be too restrictive.
Situations like this, where, besides the information from the data, initial
hypotheses on the parameters are needed, are very suitable for Bayesian
approaches. The use of appropriate prior distributions, elicited in a Bayesian
framework and based on previous knowledge on the itemsâ€™ DIF, can be less
restrictive than usual hypotheses, but still enough to guarantee identiï¬ability.
Preparation of a large test like PISA somehow screens the DIF to avoid such
items to compose the test. It is then not expected that items with large DIF, that
have a great inï¬‚uence on the proï¬ciencies, are found. On the other hand, it is
impossible to eliminate all DIF in the tests, specially in such large-scale tests.
This chapter presents a DIF analysis of the Mathematics test in PISA 2003,
for the English speaking countries, using an integrated Bayesian model. Such
model allows items to present DIF without affecting the quality of the estimated
proï¬ciencies. Moreover, the DIF analysis may be useful to detect educational
differences among the countries.
22.2 Programme for International Student Assessment (PISA)3
22.2.1 What does PISA assess?
PISA is based on a dynamic learning model that takes into account the fact
that knowledge and ability must be continuously acquired to have a successful
3 The information presented in this section is based on the PISA 2003 Technical Report. http://www.
pisa.oecd.org/dataoecd/49/60/35188570.pdf.

626
The Oxford Handbook of Applied Bayesian Analysis
adaptation to a constant changing world. Therefore, PISA aims to assess how
much knowledge and ability, essential for an effective participation in society,
is acquired by students near the end of compulsory education. Differently
from previous international assessments (IEA, TIMMS, OREALC, etc.),4 PISA
does not focus only in curricular contents. It also emphasizes the knowledge
required in modern life. Besides that, data about the studentâ€™s study habits and
their motivation and preferences for different types of leaning situation is also
collected. The word Literacy is used in the sequel to show the amplitude of the
knowledge, abilities and competencies being assessed, and it covers:
r contents or structures of the knowledge the students have to acquire in each
domain;
r processes to be executed;
r the context in which these knowledge and abilities are applied.
In Mathematics, for example, the competence is assessed in items that cover
from basic operations to high order abilities involving reasoning and mathe-
matical discoveries. The Literacy in Maths is assessed in three dimensions:
r the content of Mathematics â€“ ï¬rstly deï¬ned in terms of wide mathematical
concepts and then related to branches of the discipline;
r the process of Mathematics â€“ general mathematical competence in use of
mathematical language, selection of models and procedures, and abilities
to solve problems;
r situations where Mathematics is used â€“ varying from speciï¬c contexts to
the ones related to wider scientiï¬c and public issues.
In PISA 2003, four subject domains were tested, with mathematics as the
major domain, and reading, science and problem solving as minor domains. It
was a paper-and-pencil test. The tests were composed by three item formats:
multiple-choice response, closed-constructed response and open-constructed
response. Multiple choice items were either standard multiple choice, where
the students had to choose the best answer out of a limited number of options
(usually four), or complex multiple choice where the students had to choose
one of several possible responses (true/false, correct/incorrect, etc.) for each
of the statements presented. In closed-constructed response items, a wider
range of responses was possible. Open-constructed response items required
more extensive writing, or showing a calculation, and frequently included some
explanation or justiï¬cation. Pencils, erasers, rulers, and in some cases calcula-
tors, were provided. The decision on providing or not calculators was made
4 IEA â€“ International Association for Evaluation of Educational Achievement, TIMSS â€“ Trends in
International Mathematics and Science Study, OREALC/UNESCO â€“ Oï¬cina Regional de EducaciÃ³n para
AmÃ©rica Latina e Caribe.

Item Response Theory Applied to Educational Assessment
627
by the national centres and was based on standard national practice. No items
in the pool required a calculator but, in some items, its use could facilitate
computation. Since the model used here to analyse the data is for standard
multiple-choice items, the items which do not have this format were converted
into dichotomous items. Most of them were already dichotomized by PISA. The
remaining items were dichotomized by putting 1 for full credit and 0 for partial
or no credit.
The complex probabilistic sample, involving stratiï¬cation and clustering, was
obtained by randomly selecting the schools. Inside the selected schools, the
students who were from 15 years and 3 months to 16 years and 2 months
old were also randomly chosen. The students had to be in the 7th or 8th
year of Basic School or in High School. The sample was stratiï¬ed by the
schoolsâ€™ location (urban or rural). Besides, information on physical infra-
structure of the school, geographic region, type of school (private or public)
and number of enroled students were also used as variables in the implicit
stratiï¬cation.
22.2.2 The structure of PISA 2003 Maths test
More than a quarter of a million students, representing almost 30 million
15-year-old students enrolled in the schools of the 41 participating countries,
were assessed in 2003.
The Maths test was composed by 85 items, from which 20 were retained from
PISA 2000 for linking purposes. These items were selected from a previous
set of 217 items by expert groups based on several criteria. Some important
characteristics of the 85 selected items are presented in Tables 22.1, 22.2 and
22.3. The characteristics presented in these tables will be considered in the DIF
analysis performed here.
22.3 Differential item functioning (DIF)
Differential item functioning (DIF) is the phenomenon where the characteris-
tics of an item differ among groups of individuals. Such characteristics may
Table 22.1 Mathematics main study items (item format by competence cluster).
Item format
Competence cluster
Reproduction
Connections
Reï¬‚ection
Total
Multiple-choice response
7
14
7
28
Closed-constructed response
7
4
2
13
Open-constructed response
12
22
10
44
Total
26
40
19
85

628
The Oxford Handbook of Applied Bayesian Analysis
Table 22.2 Mathematics main study items (content category by competence cluster).
Content category
Competence cluster
Reproduction
Connections
Reï¬‚ection
Total
Space and shape
5
12
3
20
Quantity
9
11
3
23
Change and relationships
7
8
7
22
Uncertainty
5
9
6
20
Total
26(31%)
40(47%)
19(22%)
85
include discrimination and difï¬culty. For example, students with same level of
knowledge and ability but different cultural background may lead to different
probabilities of correctly answering an item with DIF.
The concern about DIF arose with the desire of creating test items that were
not affected by cultural and ethnic characteristics of the individuals taking the
tests (cf. Cole, 1993). If one does not consider DIF when it exists, one may
be led to wrong conclusions regarding, especially, the students knowledge and
abilities.
Studies conducted by the Educational Testing Service (ETS) (see Stricker and
Emmerich, 1999), in the USA, indicate that DIF may exist due to three factors in
a large-scale assessment context: the familiarity to the itemâ€™s content, which can
be associated to the exposure to the theme or to a cultural factor; the personal
interest in the content; and a negative emotional reaction caused by the itemâ€™s
content.
It seems reasonable to think that items with DIF should be avoided since
these would favor some group(s) of students, besides having some technical
and statistical implications and other ethical issues. Nevertheless, DIF items
may be very useful to study social and cultural differences that are not easily
noticed. In particular, in educational assessment, DIF items can help detecting
contents that are treated differently among the groups and may point out in
which groups the instruction of such contents should change.
Table 22.3 Mathematics main study items (content category by item format).
Content category
Item format
Multiple-choice
Closed-constructed
Open-constructed
response
response
response
Total
Space and shape
8
6
6
20
Quantity
6
2
15
23
Change and relationships
3
4
15
22
Uncertainty
11
1
8
20
Total
28
13
44
85

Item Response Theory Applied to Educational Assessment
629
Explanation of DIF is a hard task. Besides that, the pedagogical and technical
structure of a large-scale assessment like PISA aims to create high quality items
that do not in general present differential functioning. However, it is known
that the characteristics of a country and its level of economic development have
great inï¬‚uence in the social and cultural life of its population, which reï¬‚ects
in education. For this reason, different countries are expected to organize the
curricula in different ways, some countries may give more importance to some
themes and explore more some contents. The challenge in explaining the DIF
in PISA is to notice patterns in the items that present DIF. In order to do so, it
is important to have a large number of items that are quite different.
22.4 Bayesian model for DIF analysis
The most common approach to undertake a statistical analysis of educational
assessment data is by using item response theory (IRT). It is a psychometric
theory extensively used in education assessment and cognitive psychology to
analyse data arising from answers given to items belonging to a test, question-
naire, etc., and it is very useful to produce scales.
The IRT arose, formally, in the work of Lord (1952) and Rasch (1960). The
basic idea of the theory is to apply models, generally parametric, where the para-
meters represent important features of the items and of the subjects answering
the items. Some common item parameters are discrimination, difï¬culty and
guessing. The subject parameters are individual characteristics, for example, a
type of ability or some other latent trait possibly associated to his/her psycho-
logical condition.
An item can be dichotomous, if the answer is either correct or not; polyto-
mous, if the answer can be classiï¬ed in more than two categories; and also
continuous, if the answer is classiï¬ed in a continuous scale. There are IRT
models for all those situations, but only models for dichotomic items will be
considered here.
The increasing use of IRT in educational assessment and the concerns about
differential item functioning are leading researchers to propose new models
that take DIF into account. In this context, Soares et al. (2009) propose a new
IRT Bayesian model that is a generalization of the three-parameter logistic
model. The proposed model incorporates the detection, quantiï¬cation and
explanation of DIF in an integrated approach.
Typically, in educational assessment, a test is formed by I items, but a student
j only answers a subset I( j) of these items. Let Yi j, j = 1, . . . , J , be the score
attributed to the answer given by the student j to the item i âˆˆI( j) âŠ‚{1, . . . , I}.
In the case where i is a dichotomous item, Yi j = 1 if the answer is correct and
Yi j = 0 if the answer is wrong.

630
The Oxford Handbook of Applied Bayesian Analysis
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
â€“4
â€“3
â€“2
â€“1
0
q
1
2
3
4
5
pr (y = 1)
Fig. 22.1 Itemâ€™s characteristic curve for the three-parameter logistic model with a = 0.5, b = 0
and c = 0.2.
Deï¬ne Pr(Yij = 1|Ã‹ j, ai, bi) = ï£¿ij, and consider logit(ï£¿ij) = ln(ï£¿ij/1 âˆ’ï£¿i j). If
logit(ï£¿ij) = ij, then ï£¿ij = logitâˆ’1(i j) = 1/1 + eâˆ’ij. One of the most used models
in IRT is the three parameters logistic model proposed by Birnbaum (1968),
which models the probabilities ï£¿ij as
ï£¿ij = ci + (1 âˆ’ci)logitâˆ’1(ij)
(22.1)
ij = Dai(Ã‹ j âˆ’bi)
where ai, bi and ci are the discrimination, difï¬culty and guessing of item i,
respectively, âˆ€i. Ã‹ j is the ability, proï¬ciency or knowledge of student j, âˆ€j, and
D is a scale factor designed to approximate the logistic link to the normal one
(and set here to 1.7).
A good way to interpret the three-parameter logistic model is by analysing the
itemâ€™s characteristic curve generated by the model, presented in Figure 22.1.
Note from the model that the greater ai is, the greater will be the slope of
the curve. This means that, the more discriminant an item is, the larger is
the difference in the probability of correct answer for students with different
proï¬ciencies, that is, the larger is the capability of the item to discriminate the
students. Moreover, the maximum slope is attained at Ã‹ = b, as can be seen in
the ï¬gure.
The difï¬culty parameter bi interferes in the location of the curve. If the value
of bi is increased, the curve moves right and the probability of correct answer
is reduced (the item becomes harder). Alternatively, if bi is decreased, the curve
moves left and the probability of correct answer is increased (the item becomes
easier).
Furthermore, note that
lim
Ã‹ j â†’âˆ’âˆžPr(Yij = 1|Ã‹ j, ai, bi, ci) = ci. Hence, ci is the
minimum probability that a student has to correctly answer item i. ci is then

Item Response Theory Applied to Educational Assessment
631
called guessing parameter because, since it is a multiple choice item, there
is always a probability that the student answers it correctly by guessing. Lord
(1952) noticed that, in general, the percentage of correct answers for an item,
in very low levels of proï¬ciency, was smaller than the inverse of the number of
options in the item. Experts in the area report that they have observed varied
behaviours for those percentages.
In general, there can be different types of DIF (see Hanson (1998) for a
wider characterization). For the three parameters model, the types of DIF can
be characterized according to the difï¬culty, discrimination and guessing. The
model proposed here does not consider the possibility of DIF in the guessing
parameter. Although it is possible, the applicability of this case is substantially
limited by the known difï¬culties in the estimation of this parameter and by
practical restrictions.
Suppose that the students are grouped in G groups. The integrated Bayesian
DIF model used in this chapter associates the studentâ€™s answer to his/her ability
via (22.1) with
i j = Daig(Ã‹ j âˆ’big),
where aig is the discrimination parameter for item i and group g, big is the
difï¬culty parameter for item i and group g, ci is the guessing parameter for
item i, for i = 1, . . . , I , j = 1, . . . , J and g = 1, . . . , G.
Since the item difï¬culty is a location parameter, it is natural to think about
its DIF in the additive form and thus it is set as big = bi âˆ’db
ig. Analogously,
since the discrimination is a scale parameter, it is natural to think about its
DIF in the multiplicative form and thus it is set as aig = eda
igai (>0). Thus,
db
ig (with db
i1 = 0) represents the DIF related to the difï¬culty of the item in each
group and da
ig (with da
i1 = 0) represents the DIF related to the discrimination
of the item in each group. Use of the exponential term in the discrimination
places the DIF parameter over the line and combines naturally with a normal
regression equation setting. Alternatively, the DIF parameter can be speciï¬ed
directly without the exponential form. This leads to a log-normal regression
model. The two forms are equivalent but the former was preferred here.
It is assumed, a priori, that Ã‹ j|ÃŽg( j) âˆ¼N(Ãg( j), Ã›2
g( j)), where g( j) means the
group which the student j belongs to and ÃŽg = (Ãg( j), Ã›2
g( j)), j = 1, . . . , J . It is
admitted that ÃŽ1 = (Ã1 , Ã›2
1) = (0, 1) to guarantee the identiï¬cation of the model.
On the other hand, ÃŽg = (Ãg, Ã›2
g), g = 2, . . . , G, is unknown and must be esti-
mated along with the other parameters.
The model is completed with speciï¬cations of the prior distributions for the
parameters. Let N be the Normal distribution, L N the Log-Normal distribution,
Be the Beta distribution and IG the Inverse-Gamma distribution, so, the prior

632
The Oxford Handbook of Applied Bayesian Analysis
distributions assumed for the structural parameters are:
ai âˆ¼L N

Ãai, Ã›2
ai

, bi âˆ¼N

Ãbi, Ã›2
bi

and ci âˆ¼Be

Â·ci, â€šci

, for i = 1, . . . , I.
The prior distributions for the parameters of the abilitiesâ€™ distributions are:
Ãg âˆ¼N

Ã0g, Ã›2
0g

and
Ã›2
g âˆ¼IG

Â·g, â€šg

,
âˆ€g = 2, . . . , G.
The set of anchor items (items for which da
ig = db
ig = 0, âˆ€g = 1, . . . , G) is rep-
resented by IA âŠ‚{1, . . . , I}. The set of items for which the parameters vary
between the groups is represented by Idi f = {1, . . . , I} âˆ’IA. Moreover, Ia
dif âŠ‚
Idif is the set of items with DIF in the discrimination and Ib
dif âŠ‚Idif is the set
corresponding to the DIF in the difï¬culty. Naturally Idif = Ia
dif âˆªIb
dif . Notice that
if an item belongs to Idif , it does not necessarily mean that this item has DIF in
the usual meaning of the term. It means that it is not an anchor item and it can
potentially have DIF. Besides that, this can be used as admissible information
for the explanatory structure imposed to the DIF, which cannot be performed
with the anchor items.
Let Zh
ig, be the DIF indicator variable of item i in group g, for parameter h,
for h = a, b. Therefore, Zh
ig = 1 if parameter h of item i has DIF in group g,
and Zh
ig = 0, otherwise. Two possibilities may be considered: one where Zh
ig is
known, which means that the anchor items are known a priori and the DIF
analysis considers all the other items; and another one where Zh
ig is unknown
and must be identiï¬ed. In other words, it is not known a priori if the item has
or does not have DIF. The latter one will be used in the analysis of PISA.
Finally, a regression structure is considered for dh
ig in the DIF explanation as
follows
dh
ig = â€žh
0g +
K h

k=1
â€žh
kgWh
ik + Ãh
ig , if Zh
ig = 1.
(22.2)
â€žh
kg are the ï¬xed parameters of the DIF model, Wh
ik are the explanatory variables
associated to the items and Ãh
ig is the item speciï¬c random factor for each
group. It is also assumed for modelling simpliï¬cation that random factors are
independent with Ãh
ig âˆ¼N(0, (Ã™h
g)2), for g = 2, . . . , G.
The regression structure is imposed for all items but the anchor ones.
Set Wh
i = (1, Wh
i1, . . . , Wh
iKh) and â€žh
g = (â€žh
0g, . . . , â€žh
K hg)â€². When Zh
ig = 1, the condi-
tional distribution of dh
ig is given by (dh
ig|â€žh
g, Wh
i , (Ã™h
g)2) âˆ¼N(Wh
i â€žh
g, (Ã™h
g)2). When
Zh
ig = 0, dh
ig will be assumed to have a reduced variance normal distribution
(dh
ig|(Ã™h
g)2, Zh
ig = 0) âˆ¼N(0, s 2(Ã™h
g)2), where s 2 is chosen to be small enough to
ensure that dh
ig is tightly concentrated around (but not degenerated at) 0. This
strategy was proposed for variable selection in a regression model by George
and McCulloch (1993).

Item Response Theory Applied to Educational Assessment
633
The distribution of dh
ig|â€žh
g, Wh
i , Zh
ig, (Ã™h
g)2 can then be written as follows

dh
ig|â€žh
g, Wh
i , Zh
ig,

Ã™h
g
2
âˆ¼N

Wh
i â€žh
g

Zh
ig , [s 2]1âˆ’Zh
ig 
Ã™h
g
2
.
Suitable prior distributions are â€žh
g âˆ¼N(â€žh
0, Sh
0), (Ã™h
g)2 âˆ¼IG(Â·h
g, â€šh
g) and Zh
ig âˆ¼
Ber(ï£¿h
ig), where Ber is the Bernoulli distribution.
The model proposed here is very general. Apart from the usual parameters,
it also has a DIF indicator variable Zh
ig which can either be estimated along with
all the other parameters of the model or be ï¬xed a priori. The items for which
Zh
ig is not ï¬xed at 0 include additional variables to the model. These variables
are used for the DIF explanation and constitute a regression model which may
or not have covariates.
Prior distributions play a very important role in a Bayesian model. In this
particular model, they are very important in the selection of the anchor items,
as it will be seen. If one wishes to set an item as an anchor one, one simply
sets ï£¿h
ig = 0, âˆ€g = 2, . . . , G, âˆ€h = a, b. Naturally, ï£¿h
ig may be set as zero for
some but not all groups. In the same way, if one wishes to include an item
in the DIF analysis, independent of the DIFâ€™s magnitude, one simply sets
ï£¿h
ig = 1, âˆ€g = 2, . . . , G, for some h. However, the most interesting use of this
prior distributions is to consider previous information and beliefs about the
itemsâ€™ functioning to identify parameters more precisely and effectively. The
estimation of the parameters is performed by using MCMC methods to obtain
a sample from the joint posterior distribution of all the parameters. Details on
these methods are presented in Appendix B.
The level of generality introduced by this model aims to represent the com-
plexity of the problem. However, along with the integrated modelling of the
DIF, comes the problem of model identiï¬ability. Such problems are described
and studied in Soares et al. (2009). The authors show that identiï¬ability is
achieved by either imposing informative prior distributions for some Zh
ig para-
meters or ï¬xing some items not to have DIF. Nevertheless, in many cases, the
model is identiï¬able without any of these two actions because of the informative
priors attributed to the other parameters.
22.5 DIF analysis of PISA 2003
The Bayesian model presented in Section 22.4 is now used to perform a DIF
analysis of the PISA 2003 Maths test in English speaking countries. The data-
base obtained had 84 of the 85 items of the test. The estimates of such missing
itemâ€™s parameters are not presented in the PISA Technical Report 2003 either.
Great Britain is deï¬ned as the reference group (group 1). Table 22.4 shows the
other groups.

634
The Oxford Handbook of Applied Bayesian Analysis
Table 22.4 English speaking countries
used in the DIF analysis.
Country
Group
Great Britain
1
Canada
2
Australia
3
Ireland
4
USA
5
New Zealand
6
22.5.1 Prior distribution
The following prior distributions are used:
ai âˆ¼L N(0, 2), bi âˆ¼N(0, 1), ci âˆ¼Be(5, 17), Ãg âˆ¼N(0, 1), Ã›2
g âˆ¼IG(0.1, 0.1),
â€žh
g âˆ¼N(0, I) ,

Ã™h
g
2 âˆ¼IG(0.5, 0.5)
and
Zh
ig âˆ¼Ber (0.5),
for
i = 1, . . . , 84, g = 2, . . . , 6, h = a, b.
The value chosen for parameter s 2 is 1/200,000.
The prior distributions of the item parameters are chosen according to what
is expected, in general, when the proï¬ciencies follow a standard Normal distrib-
ution, which is the case of the reference group. The discrimination parameters
are expected to vary mostly between 0 and 3, and the L N(0, 2) has probability
0.70 of being smaller than 3. Most of the difï¬culty parameters are expected to lie
between âˆ’2 and 2, and a few of them to have absolute value greater than 2. The
standard Normal distribution is then a suitable prior to describe this behavior.
The guessing parameter is expected to be low (less than 0.3), since many items
were not multiple choice ones, and then should have a low probability of correct
guessing. The Be(5, 17) distribution gives 0.8 probability to values smaller than
0.3.
The mean of the proï¬ciencies in the focal groups are not expected to be very
far (more than 1 unit) from the mean of the reference group. For this reason, a
standard Normal distribution is used for these parameters. For the variance of
the proï¬ciencies, a prior distribution with large variance was preferred.
The DIF parameters are expected to have absolute value smaller than 0.5
and, for a few items, between 0.5 and 1. For this reason, the effect of a binary
covariate is also expected to be around these values. Therefore, a standard
Normal distribution is used for the coefï¬cients in the regression analysis. For
the variance of the regression error, a prior distribution with large variance is
adopted.

Item Response Theory Applied to Educational Assessment
635
2.5
1.5
0.5
Difficulty
â€“0.5
â€“1.5
â€“2.5
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
Item
43
45
47
49
51
53
55
57
GBR
CAN
AUS
IRE
USA
NZL
59
61
63
65
67
69
71
73
75
77
79
81
83
Fig. 22.2 Estimates of difï¬culty parameters in the reference group and in the groups where they had
a posterior probability of DIF greater than 0.5.
Since there is no prior information about how likely each item is to present
DIF, a symmetric prior distribution Ber(0.5) is used for the DIF indicator
variables Zh
ig.
22.5.2 DIF detection
Figures 22.2, 22.3 and 22.4 show some results for the item parameters. For the
discrimination and difï¬culty parameters, the estimates in the reference group
(GBR) and in the groups where the item is likely to have DIF a posteriori are
presented.
A nice feature of the model is that it incorporates the uncertainty about DIF
in the estimates. This means that the model does not have to â€˜decideâ€™ if an item
has DIF. It outputs a posterior distribution on the parameters that describes
2.0
Discrimination
1.5
1.0
0.5
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
Item
43
45
47
49
51
53
55
57
GBR
CAN
AUS
IRE
USA
NZL
59
61
63
65
67
69
71
73
75
77
79
81
83
Fig. 22.3 Estimates of discrimination parameters in the reference group and in the groups where
they had a posterior probability of DIF greater than 0.5.

636
The Oxford Handbook of Applied Bayesian Analysis
0.30
0.25
0.20
0.15
0.10
0.05
0.00
Guessing
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
Item
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
Fig. 22.4 Estimates of guessing parameter.
the uncertainty about that, particularly, for the posterior distribution of the
DIF parameters. It is up to the researcher to analyse this uncertainty and draw
conclusions from it.
If an item i is likely to have DIF a posteriori in group g and parameter
h, say P(Zh
ig = 1|Y) > 0.5, the posterior distribution of dh
ig will probably be
bimodal with one mode around 0. In order words, this posterior distribution
is a mixture of a distribution with mean around 0 and a very small variance and
another distribution. The former one is the distribution of (dh
ig|Zh
ig = 0, Y) and
the latter is the distribution of (dh
ig|Zh
ig = 1, Y). The more likely the item is to
have DIF a posteriori, the greater is the weight of the second component of the
mixture.
The decision on an item presenting or not DIF is based on the posterior
distribution of the respective Zh
ig. If one item is assumed to have DIF, the esti-
mation of this DIF can be made by the distribution of (dh
ig|Zh
ig = 1, Y). For this
reason, the estimates of the item parameters in the focal groups, when the item
has a posterior probability of DIF greater than 0.5, presented in Figures 22.2
and 22.3, are the posterior means of (dh
ig|Zh
ig = 1, Y).
Concerning difï¬culty, 20 items have posterior probability of DIF smaller than
0.5 in all groups, that is, they are more likely not to present any noticeable
DIF. No item has this DIF probability greater than 0.5 for more than three
countries. Forty-ï¬ve DIF parameters have absolute value greater than 0.3, which
is a considerable magnitude for DIF, and nine have this value greater than 0.5,
which is a large value for DIF.
Considering discrimination, seven items have posterior probability of DIF
smaller than 0.5 in all groups. Nine items have this DIF probability greater
than 0.5 for four countries and no item has this probability greater than 0.5 for
all the countries. On the other hand, only eight DIF parameters are greater than

Item Response Theory Applied to Educational Assessment
637
0.3 (which increases the discrimination by 35% if it is positive and decreases by
26% if it is negative). Only one DIF parameter is greater than 0.5; this value
increases the discrimination by 65% if it is positive and decreases by 40% if it
is negative.
Finally, note that most of the guessing parameters estimates are less than
0.15. This was expected since most of the items are not multiple choice ones. It
would be reasonable to ï¬x c = 0 for these items. However, this was not imposed,
in order to enable the comparison of the results obtained with the integrated
Bayesian model with the ones from Bilog-mg software (see Thissen, 2001)
without DIF. Bilog-mg uses the same scale for the proï¬ciencies and ï¬ts a three
parameters logistic model.
22.5.3 DIF explanation
Four covariates were chosen to explain the DIF in the English speaking coun-
tries. The ï¬rst one is the Content category shown in Table 22.2. It is a cate-
gorical variable with four categories: Space and shape; Quantity; Change and
relationships; and Uncertainty. Three dummy variables are used to introduce
this covariate to the regression model. Quantity is the base category, the ï¬rst
dummy variable refers to Space and shape, the second one to Change and
relationships, and the last one to Uncertainty.
The second covariate used in the DIF explanation is Competence clus-
ter shown in Table 22.1. It is also a categorical variable and has three
categories: Reproduction; Connections; and Reï¬‚ections. Two dummy vari-
ables represent the covariate where Connections is the base category, the
ï¬rst dummy variable refers to Reproduction and the second one represents
Reï¬‚ection.
The third covariate is a binary variable and indicates if the item has support
of graphical resource. Finally, the fourth covariate represents the size of the
question, measured by the number x of words and standardized using the rule
(x âˆ’75)/100.
Although the complete analysis can be performed in an integrated way,
incorporating all model uncertainty, the DIF explanation is performed here in
a separate step. The DIF magnitude among the six countries is not large, since
few items with large DIF were detected in the previous section. This way, it
would be difï¬cult to highlight possible effects of covariates in the DIF.
The analysis is performed by ï¬xing the items that had a small probability of
presenting DIF in all groups in the ï¬rst analysis as anchor items (13, 21, 43,
61, 67 and 74). All the other items are ï¬xed to have DIF, that is ï£¿h
ig = 1. This
way, the analysis presented in this section is designed only to identify possible
effects of covariates in the DIF.

638
The Oxford Handbook of Applied Bayesian Analysis
Table 22.5 Results of the DIF explanation regression model for DIF in difï¬culty.
âˆ—â€“ signiï¬cant at 90% and âˆ—âˆ—â€“ signiï¬cant at 95%. Signiï¬cant at Â·% means that
the Â·% posterior credibility interval does not include 0.
Covariates
Coefï¬cients
Canada
Australia
Ireland
USA
New Zealand
Model 1
Intercept
0.03
âˆ’0.09
0.001
0.01
âˆ’0.02
Space and shape
âˆ’0.11
0.02
âˆ’0.27âˆ—âˆ—
âˆ’0.13
0.11âˆ—
Change
âˆ’0.13âˆ—
âˆ’0.01
âˆ’0.06
âˆ’0.15
0.01
Uncertainty
âˆ’0.14âˆ—âˆ—
âˆ’0.15âˆ—âˆ—
âˆ’0.15
âˆ’0.33âˆ—
âˆ’0.07
Model 2
Intercept
âˆ’0.03
âˆ’0.05
0.08
âˆ’0.03
0.03
Space and shape
âˆ’0.04
0.06
âˆ’0.24âˆ—âˆ—
âˆ’0.07
0.10âˆ—
Uncertainty
âˆ’0.08
âˆ’0.15âˆ—âˆ—
âˆ’0.11âˆ—
âˆ’0.29âˆ—âˆ—
âˆ’0.10âˆ—
Reproduction
0.04
0.003
âˆ’0.09
âˆ’0.003
0.009
Reï¬‚ection
âˆ’0.02
âˆ’0.03
âˆ’0.10
0.08
0.002
Model 3
Intercept
âˆ’0.03
âˆ’0.19âˆ—
0.03
0.02
âˆ’0.04
Space and shape
âˆ’0.05
0.02
âˆ’0.25âˆ—âˆ—
âˆ’0.03
0.10âˆ—
Uncertainty
âˆ’0.08
âˆ’0.12âˆ—
âˆ’0.13âˆ—
âˆ’0.27âˆ—âˆ—
âˆ’0.09âˆ—
Graphical support
0.003
0.11
âˆ’0.02
âˆ’0.09
0.01
Model 4
Intercept
0.02
âˆ’0.03
0.03
âˆ’0.01
0.02
Space and shape
âˆ’0.04
0.05
âˆ’0.24âˆ—âˆ—
âˆ’0.02
0.11âˆ—
Uncertainty
âˆ’0.08
âˆ’0.15âˆ—
âˆ’0.13âˆ—
âˆ’0.23âˆ—âˆ—
âˆ’0.08
Question size
0.03
âˆ’0.03
âˆ’0.01
0.07
0.002
Several models were ï¬tted for the DIF explanation. The ï¬rst model for
DIF in difï¬culty has only the covariates to indicate the Content category. In
each of the following steps, a new covariate was included and the covariates
that were not signiï¬cant at 95% in all group in the previous model were
removed. For the discrimination, the same models used for difï¬culty were
ï¬tted.
The results, presented in Tables 22.5 and 22.6, show that Space and shape
items are more difï¬cult for students from Ireland compared to the other coun-
tries. These items are also somewhat easier for students from New Zealand.
Moreover, items related to Uncertainty are harder for students from the USA
compared to the other ï¬ve countries. They are also slightly harder for stu-
dents from Australia and Ireland than for the ones from the other three
countries.
Regarding the discrimination of the items, the results show that, in general,
items with DIF in discrimination are more discriminant for students from
Great Britain, followed by New Zealand and Ireland, Canada and Australia,
and they are less discriminant for students from the USA. On the other hand,

Item Response Theory Applied to Educational Assessment
639
Table 22.6 Results of the DIF explanation regression model for DIF in dis-
crimination. âˆ—â€“ signiï¬cant at 90% and âˆ—âˆ—â€“ signiï¬cant at 95%. Signiï¬cant at
Â·% means that the Â·% posterior credibility interval does not include 0.
Covariates
Coefï¬cients
Canada
Australia
Ireland
USA
New Zealand
Model 1
Intercept
âˆ’0.14âˆ—âˆ—
âˆ’0.14âˆ—âˆ—
âˆ’0.12âˆ—
âˆ’0.27âˆ—âˆ—
âˆ’0.05
Space and shape
0.06
âˆ’0.006
0.12
0.09
0.008
Change
âˆ’0.01
âˆ’0.02
0.02
âˆ’0.01
âˆ’0.02
Uncertainty
âˆ’0.006
0.05
0.07
0.15
0.03
Model 2
Intercept
âˆ’0.16âˆ—âˆ—
âˆ’0.17âˆ—âˆ—
âˆ’0.06
âˆ’0.26âˆ—âˆ—
âˆ’0.11âˆ—
Space and shape
0.08
0.008
0.10
0.09
0.02
Uncertainty
âˆ’0.003
0.04
0.05
0.15âˆ—
0.03
Reproduction
âˆ’0.01
0.01
âˆ’0.08
âˆ’0.02
âˆ’0.02
Reï¬‚ection
0.02
0.10
âˆ’0.03
0.07
0.05
Model 3
Intercept
âˆ’0.19âˆ—âˆ—
âˆ’0.16âˆ—âˆ—
âˆ’0.16âˆ—âˆ—
âˆ’0.27âˆ—âˆ—
âˆ’0.09âˆ—
Space and shape
0.06
0.02
0.09
0.09
0.05
Uncertainty
0.01
0.07
0.07
0.17âˆ—
0.03
Graphical support
0.04
0.02
0.05
0.04
âˆ’0.05
Model 4
Intercept
âˆ’0.17âˆ—âˆ—
âˆ’0.16âˆ—âˆ—
âˆ’0.12âˆ—âˆ—
âˆ’0.22âˆ—âˆ—
âˆ’0.09âˆ—
Space and shape
0.08
0.01
0.08
0.11
0.02
Uncertainty
âˆ’0.02
0.05
0.01
0.16âˆ—
0.01
Question size
âˆ’0.06
âˆ’0.09
âˆ’0.20âˆ—âˆ—
âˆ’0.08
âˆ’0.13âˆ—
if only items related to Uncertainty are considered, they are, on average, as
discriminant for students from the USA as they are for students from Great
Britain and they are more discriminant in these two countries than in the other
four countries. Furthermore, the question size has an inï¬‚uence on the DIF in
discrimination in Ireland and New Zealand. Larger questions make the items
less discriminant in these countries, specially in Ireland.
22.5.4 Analysis of the proï¬ciencies
The results obtained for the distribution of the proï¬ciencies in the DIF
analysis without covariates is presented. They are compared with the results
from the original analysis of PISA and from the analysis with the Bilog-mg
software.
Bilog-mg also allows the existence of DIF, but only in the difï¬culty. The
scale in Bilog-mg is deï¬ned by assuming a standard normal distribution for
the proï¬ciencies from the reference group. The same is used for the Bayesian
model proposed in this chapter.

640
The Oxford Handbook of Applied Bayesian Analysis
Table 22.7 Estimates of the mean and standard deviation of the proï¬ciencies in
each country. âˆ—refers to the modiï¬ed scales.
Country
Parameter
PISA
Bilog-mgâˆ—
IBMâˆ—
Bilog-mg
IBM
Australia
Mean
524.11
522.95
522.15
0.1602
0.1515
N = 235,486
Std. Dev.
95.60
94.40
93.78
1.0209
1.1042
Canada
Mean
532.70
529.43
528.77
0.2303
0.2231
N = 330,098
Std. Dev.
87.33
90.00
85.26
0.9734
0.9221
Great Britain
Mean
508.14
508.14
508.14
0
0
N = 696,215
Std. Dev.
92.47
92.47
92.47
1
1
Ireland
Mean
503.52
502.87
508.10
âˆ’0.0570
âˆ’0.0005
N = 54,838
Std. Dev.
85.32
85.62
82.37
0.9259
0.8908
New Zealand
Mean
524.17
521.56
523.68
0.1452
0.1681
N = 48,606
Std. Dev.
98.17
96.72
92.71
1.0459
1.0026
U.S.A.
Mean
483.64
484.85
474.99
âˆ’0.2518
âˆ’0.3585
N = 3,140,301
Std. Dev.
95.37
91.40
97.52
0.9884
1.0546
Total
Mean
493.81
494.33
487.45
âˆ’0.1494
âˆ’0.2238
N = 4,505,544
Std. Dev.
95.72
92.88
97.46
1.0045
1.0540
Table 22.7 shows the mean and variance of the distributions of the proï¬cien-
cies in each country considering:
r the original PISA proï¬ciency (pv1math), which does not consider DIF;
r the results obtained with Bilog-mg without DIF;
r the results obtained with Bilog-mg without DIF in a modiï¬ed scale;
r the results obtained with the integrated Bayesian model (IBM) proposed in
this chapter;
r the results obtained with the integrated Bayesian model (IBM) in a modi-
ï¬ed scale.
The modiï¬ed scale referred to above consists in transforming the estimates
in order to make the mean and variance of the reference group (GBR) the same
as in the PISA scale.
PISA uses the Rasch model and a partial credit model and ï¬xes the mean
and standard deviation of all the proï¬ciencies to be 500 and 100, respectively.
For this reason, the proï¬ciencies obtained with the integrated Bayesian model
can not be directly compared to the ones from the original PISA results. They
should be compared to the results from Bilog-mg, with GBR as the reference
group. The transformed scales of the IBM and Bilog-mg are presented just to
give an idea about the differences among the countries, compared with the
original results from PISA.
Table 22.7 shows that the results are very similar with and without DIF in
four of the six countries. Differences are only found in Ireland, where the mean
increases when DIF is considered, and in the USA, where the mean decreases

Item Response Theory Applied to Educational Assessment
641
if DIF is accounted. It is important to mention that the data was weighted by
the sampling weights.
22.6 Conclusions
The analysis presented here shows the importance of appropriately accounting
for all sources of heterogeneity present in educational testing. Incorporation of
differentiation in the education pattern of countries allows the possibility for
explanation of possible causes for it. This can lead the way for improvement in
the schooling systems. The use of the Bayesian paradigm provides a number
of advantages ranging from inclusion of relevant background information to
allowance for model identiï¬cation.
In the context of the speciï¬c application considered, a host of indicators
differentiating the educational systems of the English-speaking countries were
identiï¬ed. These may help to understand the nature and possible origins of
the difference between them and lead the way for incorporation of beneï¬cial
practices in the currently available systems.
Appendix
A. Broader context and background
A.1 Markov chain Monte Carlo methods
The use of Bayesian statistics to handle statistical inference problems has grown
considerably since the 1990s. This is due, mainly, to the advances in the ï¬eld of
stochastic simulation, particulary Markov chain Monte Carlo (MCMC) methods.
Bayesian inference is mainly based on the posterior distribution of the quan-
tities of interest (generally parameters). But in most practical situations this
distribution is not fully available. In most of the cases it is known apart from
a constant. Bayes theorem states that the posterior distribution of a quantity Ã‹,
which is the distribution of Ã‹ given some observed data X, is given by:
p (Ã‹|X) =
p (X|Ã‹) p (Ã‹)

 p (X|Ã‹) p (Ã‹) dÃ‹.
(22.3)
The posterior distribution of Ã‹ is proportional to the product of the likelihood
function and the prior distribution of Ã‹ as it is shown in the numerator of (22.3).
The denominator of (22.3) is a constant with respect to Ã‹ and is generally not
analytically available due to the complexity of the integral.
Since such integral cannot be solved analytically, it is not possible to compute
expectations with respect to the posterior distribution of Ã‹. Examples include the
expectation and variance of Ã‹ or any probability like P(Ã‹ âˆˆA|X) for a given sub-
set A of the state space of Ã‹. Therefore, specially when Ã‹ is multidimensional,

642
The Oxford Handbook of Applied Bayesian Analysis
knowing only the numerator in (22.3) is not enough to have useful posterior
information on Ã‹.
However, if it is possible to have a sample from this posterior distribution,
one could obtain the Monte Carlo (MC) estimator of the desired expectations.
The Monte Carlo estimator of E[g(Ã‹)|X] is given by
Ë†E[g (Ã‹) |X] = 1
n
n

i=1
g

Ã‹(i)
(22.4)
where Ã‹(1), . . . , Ã‹(n) is a sample from the posterior distribution of Ã‹.
Monte Carlo estimators have some nice properties. First of all, they are
unbiased estimators. Secondly, if g 2(Ã‹) has ï¬nite expectation under p(Ã‹|X), the
variance of the MC estimator is O(1/n). Moreover, they are strongly consistent
estimators, since Ë†E[g(Ã‹)|X] converges almost surely to E[g(Ã‹)|X] by the strong
law of large numbers.
Markov chain Monte Carlo methods consist of obtaining a sample from the
posterior distribution of Ã‹ through iterative simulation and using this sample
to calculate estimates of expectations. The idea is to construct a Markov chain
for Ã‹ which has p(Ã‹|X) as invariant distribution. This means that, p(Ã‹(i))
iâ†’âˆž
âˆ’âˆ’âˆ’â†’
p(Ã‹|X), for any arbitrary initial value Ã‹(0) of the chain. Thus, for an arbitrarily
large iteration i, Ã‹(i) âˆ¼p(Ã‹|X) approximately, and consequently Ã‹(i+1) âˆ¼p(Ã‹|X).
This way, starting from an arbitrary seed Ã‹(0), values simulated from such
Markov chain after a large number of iterations should come from a distribution
very close to the posterior distribution of Ã‹.
In practice, a number N of iterations is chosen in a way that the chain is
believed to be in equilibrium after this iteration and a sample Ã‹(N+1), . . . , Ã‹(N+M)
is used to obtain estimates of expectations. These ï¬rst N iterations of the chain
are called burn-in period and the estimators used are
Ë†E(g(Ã‹)|X) = 1
M
N+M

i=N+1
g(Ã‹(i)).
(22.5)
Note that the sample obtained is not independent since it consists of a reali-
sation of a Markov chain. However, it is still reasonable to use the estimator
in (22.5) since the ergodic theorem states that, if the chain is ergodic (see
Gamerman and Lopes, 2006) and E[g(Ã‹)|X] < âˆž, then
1
n
n

i=1
g(Ã‹(i))
a.s.
âˆ’â†’E[g(Ã‹)|X], as n â†’âˆž.
(22.6)
This is a Markov chain equivalent of the law of large numbers and states that
averages of values from the Markov chain are strongly consistent estimators of

Item Response Theory Applied to Educational Assessment
643
expectations w.r.t. to the invariant distribution, despite the dependence struc-
ture imposed by the chain.
The most used MCMC methods in Bayesian inference are the Gibbs sam-
pling and the Metropolis â€“ Hastings algorithms. The former consists in deï¬n-
ing the transition distribution of the chain as the full conditional distributions.
Suppose that Ã‹ = (Ã‹1, . . . , Ã‹d), where each component Ã‹i can be a scalar, a
vector or a matrix, and that the full conditional distributions p(Ã‹i|Ã‹âˆ’i, X) are
known and it is possible to simulate from them, where Ã‹âˆ’i is the vector Ã‹
without the i-th component.
It can be shown that a Markov chain with transition distributions given by
the full conditional distributions has p(Ã‹|X) as invariant distribution. This way,
the Gibbs sampling algorithm is given by:
1. Initialize the iteration counter of the chain j = 1 and set initial values Ã‹(0) =
(Ã‹(0)
1 , . . . , Ã‹(0)
d ).
2. Obtain a new value Ã‹( j) = (Ã‹( j)
1 , . . . , Ã‹( j)
d ) from Ã‹( jâˆ’1) through successive
generation of values
Ã‹( j)
1 âˆ¼p

Ã‹1|Ã‹( jâˆ’1)
2
, . . . , Ã‹( jâˆ’1)
d

Ã‹( j)
2 âˆ¼p

Ã‹2|Ã‹( j)
1 , Ã‹( jâˆ’1)
3
, . . . , Ã‹( jâˆ’1)
d

...
Ã‹( j)
d âˆ¼p

Ã‹1|Ã‹( j)
1 , . . . , Ã‹( j)
dâˆ’1

.
3. Change counter j to j + 1 and return to step 2 until convergence is
reached.
In the case where it is not feasible to simulate directly from the full condi-
tional distributions, the Metropolis â€“ Hastings algorithm can be used. The idea
is to choose a transition distribution p(Â¯, Ë†) in a way that it constitutes a Markov
chain that has p(Ã‹|X) as invariant distribution. A sufï¬cient condition for that is
to have a reversible chain, that is
ï£¿(Â¯) p (Â¯, Ë†) = ï£¿(Ë†) p (Ë†, Â¯) ,
âˆ€(Â¯, Ë†),
(22.7)
where ï£¿represents the posterior distribution.
The distribution p(Â¯, Ë†) consists of two elements: an arbitrary transition
distribution q(Â¯, Ë†) and a probability Â·(Â¯, Ë†) such that
p(Â¯, Ë†) = q(Â¯, Ë†)Â·(Â¯, Ë†), i f Â¯ =/ Ë†.
(22.8)
This transition kernel deï¬nes a density p(Â¯, Ë†) for every possible value of Ã‹
different from Â¯. Therefore, there is a positive probability left for the chain to

644
The Oxford Handbook of Applied Bayesian Analysis
remain in Â¯ given by
p(Â¯, Â¯) = 1 âˆ’

q(Â¯, Ë†)Â·(Â¯, Ë†)dË†.
(22.9)
Hastings (1970) proposed to deï¬ne the acceptance probability in a way that it
deï¬nes a reversible chain when combined with the arbitrary transition distrib-
ution. Such probability is given by
Â·(Â¯, Ë†) = min

1, ï£¿(Ë†)q(Ë†, Â¯)
ï£¿(Â¯)q(Â¯, Ë†)

.
(22.10)
Note that it is not necessary to know the constant in the denominator of
(22.3), since it cancels in the expression of the acceptance probability. The
Metropolis â€“ Hastings algorithm may be detailed as follows:
1. Initialize the iteration counter of the chain j = 1 and set initial values Ã‹(0) =

Ã‹(0)
1 , . . . , Ã‹(0)
d

.
2. Move the chain to a new value Ë† generated from the density q(Ã‹( jâˆ’1), Â·).
3. Evaluate the acceptance probability of the move Â·(Ã‹( jâˆ’1), Ë†) given by
(22.10).
4. If the move is accepted, Ã‹( j) = Ë†. If it is not accepted, Ã‹( j) = Ã‹( jâˆ’1) and the
chain does not move.
5. Change counter j to j + 1 and return to step 2 until convergence is
reached.
Step 4 is performed by generating a value u from a unit uniform distribution.
If u â‰¤Â·, the move is accepted, if u > Â· the chain does not move.
In the case where some (but not all) the full conditional distributions are
known, the algorithm sometimes called Metropolis-within-Gibbs can be used.
This is the algorithm used in this chapter to draw samples from the joint
posterior distribution of all the parameters.
The idea of the Metropolis-within-Gibbs algorithm is to construct a Gibbs
sampler and, for the components of Ã‹ that cannot be directly sampled from the
full conditional distribution, a Metropolis â€“ Hastings step is used. That is, such
components are drawn from an arbitrary transition distribution and have the
moves accepted with an acceptance probability that constitutes a chain that has
the correspondent full conditional as invariant distribution. In other words, on
the ( j + 1)-th iteration, a component Ã‹i which cannot be directly drawn from its
full conditional distribution is drawn from an arbitrary transition distribution

Item Response Theory Applied to Educational Assessment
645
q

Ã‹( j)
i , Ë†

and has the move accepted with probability
Â·(Ã‹( j)
i , Ë†) = min
âŽ§
âŽ¨
âŽ©1,
ï£¿i(Ë†)q

Ë†, Ã‹( j)
i

ï£¿i

Ã‹( j)
i

q

Ã‹( j)
i , Ë†

âŽ«
âŽ¬
âŽ­
where ï£¿i is the full conditional distribution of the i-th component of Ã‹.
Such transition distributions constitute a Markov chain with invariant distri-
bution given by the posterior distribution of Ã‹. For more details on the MCMC
methods described here and other issues on stochastic simulation in Bayesian
inference see Gamerman and Lopes (2006).
A.2 MCMC details for our DIF model
The estimation of the parameters of the model presented in this chapter is
performed by using MCMC methods to obtain a sample from the joint posterior
distribution of these parameters. The method used is Gibbs sampling with
Metropolis â€“ Hastings steps.
Basically, all the parameters that appear explicitly in the likelihood are drawn
using a Metropolis â€“ Hastings step with a suitable random walk as the proposal
distribution since it is not possible to directly draw from their full conditional
distributions. These parameters are the proï¬ciencies, the item parameters and
the DIF parameters. For all the other parameters, it is possible to directly draw
from their full conditional distributions.
The parameters are simulated as follows:
r Abilities.
Samples from p(Ã‹|â€š, d, ÃŽ, â€ž, T, Y, W, Z), are drawn from
p(Ã‹ j|Ã‹=/ j, â€š, d, ÃŽ, â€ž, T, Y, W, Z) = p

Ã‹ j|â€šI( j), dI( j)g( j), ÃŽg( j), Yj

âˆp

Yj|Ã‹ j, â€šI( j), dI( j)g( j), ÃŽg( j)

p

Ã‹ j|â€šI( j), dI( j)g( j), ÃŽg( j)

= p

Yj|Ã‹ j, â€šI( j), dI( j)g( j)

p

Ã‹ j|ÃŽg( j)

=

iâˆˆI( j)
p

Yi j|Ã‹ j, â€ši, dig( j)

p

Ã‹ j|ÃŽg( j)

,
âˆ€j = 1, . . . , J.
The calculations above are obtained by assuming independence between
the studentsâ€™ abilities, and between the answers Yi j when conditioned to the
abilities and to the itemsâ€™ parameters. It is not easy to directly draw from the
distribution above because of its complexity. So, the Metropolis â€“ Hastings
algorithm is used. A normal transition kernel is used and the proposal for

646
The Oxford Handbook of Applied Bayesian Analysis
the new state is
Ã‹l
j âˆ¼q

Ã‹ j|Ã‹lâˆ’1
j

= N

Ã‹lâˆ’1
j
, c2
Ã‹

.
The tuning parameter is set as cÃ‹ = 0.1, chosen after a pilot study to assure
an appropriate acceptance rate of the chain.
r Parameters of the distributions of the groupsâ€™ abilities.
It is possible to directly draw from the full conditional distributions of
the means and variances of the abilitiesâ€™ distribution since conjugate prior
distributions were chosen.
â€“ Mean of the distributions of the groupsâ€™ abilities.
If a prior distribution Ãg âˆ¼N(Ã0g, Ã›2
0g) is chosen, the following full con-
ditional distribution is obtained:
p(Ãg|Â·) = p

Ãg|Ã‹J (g), Ã›2
g

âˆp

Ã‹J (g)|Ãg, Ã›2
g

p

Ãg|Ã›2
g

=

jâˆˆJ (g)
p

Ã‹ j|Ãg, Ã›2
g

p(Ãg)
so:
(Ãg|Â·) âˆ¼N

mg, s 2
g

,
where:
mg =

jâˆˆJ (g) Ã‹ jÃ›2
0g + Ã0gÃ›2
g
ngÃ›2
0g + Ã›2g
and sg =
Ã›gÃ›0g
;
ngÃ›2
0g + Ã›2g
.
J (g) represents the set of the students in group g and ng is the number
of students in group g, for g = 2, . . . , G.
â€“ Variance of the distributions of the groupsâ€™ abilities.
p

Ã›2
g|Â·

= p

Ã›2
g|Ã‹J (g), Ãg

âˆ

jâˆˆJ (g)
p

Ã‹ j|Ãg, Ã›2
g

p

Ã›2
g

.
If a prior distribution Ã›2
g âˆ¼IG(Â·g, â€šg) is chosen, the following full con-
ditional distribution is obtained:

Ã›2
g|Â·

âˆ¼IG

Â·g + ng
2 ,

jâˆˆJ (g)(Ã‹ j âˆ’Ãg)2) + 2â€šg
2

, g = 2, . . . , G.

Item Response Theory Applied to Educational Assessment
647
r Structural parameters â€š.
Under the hypotheses of local independence of the items, samples of p(â€š|Â·)
are drawn from:
p

â€ši|Ã‹J (i), di, YJ (i)

âˆp

YJ (i)|Ã‹J (i), â€ši, di

p

â€ši|Ã‹J (i), di

=

jâˆˆJ (i)
p

Yi j|Ã‹ j, â€ši, dig( j)

p(ai)p(bi)p(ci),
âˆ€i = 1, . . . , I.
The last equality comes from the prior independence between item para-
meters. The chosen prior distributions of the parameters are:
ai âˆ¼L N

Ãai, Ã›2
ai

;
bi âˆ¼N

Ãbi, Ã›2
bi

and
ci âˆ¼Beta

Â·ci, â€šci

.
In general, Ãai = 0, Ã›2
ai = 2, Ãbi = 0, Ã›2
bi = 1, Â·ci = 5, â€šci = 17. These val-
ues are used, for example, as default values in the software Bilog-mg. In
some cases, these values have to be modiï¬ed to assure a better ï¬t of current
data or to add past information about the parameters.
Once again, it is not possible to directly draw from the full conditional
distribution and the Metropolis â€“ Hastings algorithm is used assuming the
following transition kernels:
al
i âˆ¼L N

ln

alâˆ’1
i

, ca

, bl
i âˆ¼N

blâˆ’1
i
, c2
b

e cl
i âˆ¼U

clâˆ’1
i
âˆ’â€°, clâˆ’1
i
+ â€°

.
In general, the values ca = 0.02, cb = 0.1, â€° = 0.05 are used.
r Structural DIF parameters.
To draw samples from p(dh|Â·), h = a, b, samples are independently drawn
from:
p

dh
ig|Â·

= p

dh
ig|dh
=/i,g, d=/h
g , Zh, Ã‹J (i,g), â€ši, â€žg, T, YJ (i,g), W

âˆp

YJ (i)|Ã‹J (i,g), â€ši, dh
ig

p

dh
ig|dh
=/i,g, d=/h
g , W, â€žg, T, Zh
ig

=

jâˆˆJ (i,g)
p

Yi j|Ã‹ j, â€ši, dh
ig

p

dh
ig|Wh
i , â€žh
g, Ã™h
g, Zh
ig

,
âˆ€i âˆˆIh
di f , g = 2, . . . , G.
In the last equality, it is assumed that T h =

Ã™h
g
2 I, where I is the identity
matrix nidh Ã— nidh, and nidh is the number of items for which DIF in
parameter h is assumed, h = a, b. The conditional prior distribution of
the DIF parameters is (dh
ig|Wh
i , â€žh
g, Ã™h
g, Zh
ig) âˆ¼N(Wh
i â€žh
g, (Ã™h
g)2) if Zh
ig = 1, and
the transition kernel used in the Metropolis â€“ Hastings algorithm is the

648
The Oxford Handbook of Applied Bayesian Analysis
following:

dh
ig
l+1
âˆ¼N

dh
ig
l
, 0.1

, âˆ€i.
On the other hand, (dh
ig|Wh
i , â€žh
g, Ã™h
g, Zh
ig) âˆ¼N(0, s 2
i (Ã™h
g)2) if Zh
ig = 0. In
practical situations, since si is very small, dh
ig âˆ¼= 0 if Zh
ig = 0.
r Parameters of the DIF regression structure.
For the parameters â€žg, g = 2, . . . , G, samples are drawn from:
p

â€žh
g|Â·

= p

â€žh
g|dh
g , T h
g , Wh, Zh
âˆp

dh
g |â€žh
g, Wh, T h
g , Zh
p

â€žh
g

.
If a prior distribution â€žh
g âˆ¼N

â€žh
0, Sh
0

is assumed, the following full condi-
tional distribution is obtained:

â€žh
g|dh
g , T h
g , Wh, Zh
âˆ¼N(H, L),
where:
L =
6
Wh
I

Zh
ig=1

T 
T h
g
âˆ’1 Wh
I

Zh
ig=1
 +

Sh
0
âˆ’1
7âˆ’1
and
H = L
6
Wh
I

Zh
ig=1

T 
T h
g
âˆ’1 dh
I

Zh
ig=1

,g +

Sh
0
âˆ’1 â€žh
0
7
.
Samples of

Ã™h
g
2 are drawn from:
p

Ã™h
g
2 |Â·

= p

Ã™h
g
2 |dh
g , â€žh
g, Wh
I

Zh
ig=1
, Zh

âˆp

dh
g |

Ã™h
g
2 , â€žh
g, Wh
I

Zh
ig=1
, Zh

p

Ã™h
g
2
.
If a prior distribution (Ã™h
g)2 âˆ¼IG(Â·Ã™hg, â€šÃ™hg) is assumed, the following full
conditional distribution is obtained:

Ã™h
g
2 |Â·

âˆ¼IG

Â·Ã™hg +
 Zh
ig
2
,
6
1
2

dh
g âˆ’Wh
I

Zh
ig=1
â€žg
T

dh
g âˆ’Wh
I

Zh
ig=1
â€žg

+ â€šÃ™hg
9
,
for g = 2,...,G.
r DIF indicator variable.
p

Zh
ig|Â·

= p

Zh
ig|dh
ig, T h, Wh
i , â€žh
= p

dh
ig|Zh
ig, T h, Wh
i , â€žh
p

Zh
ig


Item Response Theory Applied to Educational Assessment
649
If a prior distribution Zh
ig âˆ¼Ber(ï£¿h
ig) is assumed, the full conditional distri-
bution (Zh
ig|Â·) âˆ¼Ber (Ë˜h
ig) is obtained, where:
Ë˜h
ig = 1
cË˜
ï£¿h
ig exp

âˆ’
1
2

Ã™2g
 
dig âˆ’Wh
i â€žh
g
2

, and:
cË˜ = ï£¿h
ig exp

âˆ’1
2

Ã™2g
 
dig âˆ’Wh
i â€žh
g
2

+

1 âˆ’ï£¿h
ig

exp

âˆ’1
2

s 2
i Ã™2g


dh
ig
2

,
for g = 2, . . . , G.
A.3 Bayesian methods in item response theory
Since the late 1960s, knowledge about populations of students has been used in
order to provide improved estimates for individual abilities. In the early works
of Birnbaum (1969) and Owen (1975), Bayes estimates of ability parameters
were obtained in item response models under the assumption that the item
parameters are known. Novick et al. (1972) and Rubin (1980) proposed Bayesian
and empirical Bayesian solutions, respectively, to predict the performance of
students from a Law school using information from other Law schools. Also, the
most used traditional Bayesian methods for ability estimation were proposed in
the early 1980s: the Expected A-Posteriori (EAP; Bock and Mislevy 1982) and
Modal A-Posteriori (MAP; Samejima 1980).
Bayesian methods have been used in IRT for the estimation of structural
parameters since the early 1980s. Applying the hierarchical Bayesian approach
suggested in Lindley and Smith (1972), Swaminathan and Gifford (1982),
Swaminathan and Gifford (1985) and Swaminathan and Gifford (1986) pro-
posed Bayesian procedures for simultaneous estimation of the item and ability
parameters in the Rasch, two-parameter and three parameter models, respec-
tively. The authors showed that the non-convergence of the estimates of the
discrimination and guessing parameters, which is common in joint maxi-
mum likelihood methods, can be controlled by appropriate speciï¬cation of
prior distributions for all parameters. Mislev (1986) and Tsutakawa and Lin
(1986) proposed to use the EM algorithm (Dempster et al. 1977) to obtain a
modal marginal posterior distribution estimator for the item parameters, as
it is proposed by Bock and Aitkin (1981) for maximum marginal likelihood
estimation.
The importance of Bayesian approaches in item response theory have been
steadily growing since the 1980s, with incorporation of new models. Such
models try to include the effect of covariates or grouping in the estimation of
latent abilities. Multidimensional structures and local dependence of the items
have also been proposed. This way, as the complexity of the models increase,
also does the difï¬culty on the estimation of the parameters of the models.

650
The Oxford Handbook of Applied Bayesian Analysis
Classical methods, like maximization of marginal likelihood, and even some
Bayesian methods, like maximum posterior distribution, use the EM algorithm
or some of its variations and, basically, treat the latent variables as â€˜missing
dataâ€™.
However, the use of the EM algorithm becomes harder as the complexity
increases. For this reason, the use of methods based on stochastic simulation
has grown since the 1990s, specially MCMC methods. In a pioneering study,
Albert (1992) applied the Gibbs sampler to the two-parameter model using
the normal distribution function as the link function. BÃ©guin and Glas (2001)
proposed a Gibbs sampler for the three-parameter and the multidimensional
models. Fox and Glass (2001, 2003) studied an IRT model with a hierarchical
regression structure on the ability parameters, with both latent and observed
covariates. The latent covariates are jointly estimated with the parameters of the
model. All of those works use normal distributions for the latent variables and
estimate the abilities using augmented data techniches (cf. Tanner and Wong,
1987).
Patz and Junker (1999b) applied the Gibbs sampler to estimate parameters in
a two-parameter model. Differently from the works cited above, that use aug-
mented data, the authors use the Metropolis-within-Gibbs algorithm. Patz and
Junker (1999a) extended this work for the three-parameter model, polytomic
models and models with missing data.
DIF analysis is an appropriate environment for a genuine Bayesian formu-
lation due to the complex structure of the models and the subjective decision
features involved, which can be formulated through Bayesian arguments. For
example, Zwick et al. (1999, 2000) and Zwick and Thayer (2002) considered a
formulation where the MH D-DIF statistic is represented by a normal model
where the mean is equal to the real DIF parameter. These authors use Empirical
Bayes (EB) for the posterior estimation of the parameters. Sinharay et al. (2006)
considered the same formulation and proposed informative prior distributions
based on past information. They showed that the full Bayes (FB) method leads
to improvements if compared to the two other approaches, specially in small
samples. Wang et al. (2008) proposed a Bayesian approach to study DIF based
on testlet response theory (cf. Wainer et al., 2007).
B. Countries that participated in PISA 2003
OECD countries:
Australia, Austria, Belgium, Canada, Czech Republic, Denmark, Finland,
France, Germany, Greece, Hungary, Iceland, Ireland, Italy, Japan, Korea,
Luxembourg, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal,
Slovak Republic, Spain, Sweden, Switzerland, Turkey, United Kingdom,
Scotland, United States.

Item Response Theory Applied to Educational Assessment
651
Partner countries:
Brazil, Hong Kong-China, Indonesia, Latvia, Liechtenstein, Macao-China,
Russian Federation, Serbia and Montenegro, Thailand, Tunisia, Uruguay.
Acknowledgments
The ï¬rst author acknowledges grants from CNPq-Brazil and FAPERJ-Brazil.
References
Albert, J. H. (1992). Bayesian estimation of normal ogive item response models using Gibbs
sampling. Journal of Educational Statistics, 17, 251â€“269.
BÃ©guin, A. A. and Glass, C. A. W. (2001). MCMC estimation and some model-ï¬t analysis of
multidimensional IRT models. Psychometrika, 66, 541â€“562.
Birnbaum, A. (1968). Some latent traits models and their use in inferring examineeâ€™s ability.
In reissue of Statistical Theories of Mental Test Scores (ed. F. M. Lord and M. R. Novick).
Addison-Wesley, Reading, MA.
Birnbaum, A. (1969). Statistical theory for logistic mental test models with a prior distribution
of ability. Journal of Mathematical Psychology, 6, 258â€“276.
Bock, R. D. and Aitkin, M. (1981). Marginal maximum likelihood estimator of item parameters:
an application of an EM algorithm. Psychometrika, 46, 443â€“459.
Bock, R. D. and Mislevy, R. J. (1982). Adaptive EAP estimation of ability in a microcomputer
environment. Applied Psychological Measurement, 6, 431â€“444.
Cole, N. S. (1993). Differential Item Functioning, Chapter on History and development of DIF.
Lawrence Erlbaum, Hillsdale, NJ.
Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm (with discussion). Journal of the Royal Statistical Society B, 39, 1â€“38.
Fox, J. P. and Glass, C. A. W. (2001). Bayesian estimation of a mutilevel model using Gibbs
sampling. Psychometrika, 66, 271â€“288.
Fox, J. P. and Glass, C. A. W. (2003). Bayesian modeling of measurement error in predictor
variables using IRT. Psychometrika, 68, 169â€“191.
Gamerman, D. and Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Simulation for
Bayesian Inference, (2nd edn). Chapman & Hall/CRC, New York.
George, E. I. and McCulloch, R. E. (1993). Variable selection via Gibbs sampling. Journal of
American Statistical Association, 85, 398â€“409.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their appli-
cations. Biometrika, 57, 97â€“109.
Lindley, D. V. and Smith, A. F. (1972). Bayesian estimates for the linear model (with discussion).
Journal of the Royal Statistical Society B, 34, 1â€“41.
Lord, F. M. (1952). A theory of test scores. Psychometric Monograph, 7. Psychometric society.
Mislev, R. J. (1986). Bayes modal estimation in item response models. Psychometrika, 51,
177â€“195.
Novick, M. R., Jackson, P. H., Thayer, D. T. and Cole, N. S. (1972). Estimating multiple regres-
sion in M-groups: a cross-validation study. British Journal of Mathematical and Statistical
Psychology, 5, 33â€“50.

652
The Oxford Handbook of Applied Bayesian Analysis
Owen, R. J. (1975). A Bayesian sequential procedure for quantal response in the context of
adaptive mental testing. Journal of the American Statistical Association, 70, 351â€“360.
Patz, R. J. and Junker, B. W. (1999a). Applications and extensions of MCMC in IRT. Journal of
Educational and Behavioral Statistics, 24, 342â€“366.
Patz, R. J. and Junker, B. W. (1999b). A straightforward approach to Markov chain Monte
Carlo methods for item response models. Journal of Educational and Behavioral Statistics, 24,
146â€“178.
Rasch (1960). Probabilistic Models for Some Intelligence and Attainment Tests. Copenhagen:
Institute for Educational Research.
Rubin, D. B. (1980). Using empirical Bayes techniques in the law school validity studies. Journal
of the American Statistical Association, 73, 801â€“827.
Samejima, F. (1980). Is Bayesian estimation proper for estimating the individualâ€™s ability?
Research Report 80â€“3, University of Tennessee, Department of Psychology, Knoxville, TN.
Sinharay, S., Dorans, N. J., Grant, M. C., Blew, E. O. and Knorr, C. M. (2006). Using past
data to enhance small-sample DIF estimation: a Bayesian approach. Technical Report ETS
RR-06-09, Educational Testing Service, Princeton, NJ.
Soares, T. M., GonÃ§alves, F. B. and Gamerman, D. (2009). An integrated Bayesian model for
DIF analysis. Journal of Educational and Behavioral Statistics, 34, 348â€“377.
Stricker, L. J. and Emmerich, W. (1999). Possible determinants of differential item functioning:
familiarity, interest and emotional reaction. Journal of Educational Measurement, 36, 347â€“366.
Swaminathan, H. and Gifford, J. A. (1982). Bayesian estimation in the rasch model. Journal of
Educational Statistics, 7, 175â€“191.
Swaminathan, H. and Gifford, J. A. (1985). Bayesian estimation in the two parameter logistic
model. Psychometrika, 50, 349â€“364.
Swaminathan, H. and Gifford, J. A. (1986). Bayesian estimation in the three parameter logistic
model. Psychometrika, 51, 589â€“601.
Tanner, M. A. and Wong, W. H. (1987). The calculation of posterior distribution by data
augmentation. Journal of the American Statistical Association, 82, 528â€“550.
Thissen, D. (2001). IRTLRDIF v.2.0.b: Software for the Computation of the Statistics
Involved in Item Response Theory Likelihood-Ratio Tests for Differential Item Functioning.
http://www.unc.edu/ dthissen/dl.html: Scientiï¬c Software International.
Tsutakawa, R. K. and Lin, H. Y. (1986). Bayesian estimation of item response curves. Psychome-
trika, 51, 251â€“267.
Wainer, H., Bradlow, E. T. and Wang, X. (2007). Testlet Response Theory. Cambridge University
Press, New York.
Wang, X., Bradlow, E. T., Wainer, H. and Muller, E. S. (2008). A Bayesian method for studying
DIF: a cautionary tale ï¬lled with surprises and delights. Journal of Educational and Behavioral
Statistics, 33, 363â€“384.
Zwick, R. and Thayer, D. T. (2002). Application of an empirical Bayes enhancement of mantelâ€“
haenszel DIF analysis to a computerized adaptive test. Applied Psychological Measurement, 26,
57â€“76.
Zwick, R., Thayer, D. T. and Lewis, C. (1999). An empirical Bayes approach to Mantelâ€“Haenszel
DIF analysis. Journal of Educational Measurement, 36, 1â€“28.
Zwick, R., Thayer, D. T. and Lewis, C. (2000). Using loss functions for DIF detection: an
empirical Bayes approach. Journal of Educational and Behavioral Statistics, 25, 225â€“247.

Â·23Â·
Sequential multilocation auditing and
the New York food stamps program
Karl W. Heiner, Marc C. Kennedy and Anthony Oâ€™Hagan
23.1 Introduction
23.1.1 The food stamps program
In the United States, the Department of Agricultureâ€™s Food and Nutrition Ser-
vices (FNS) Division oversees a programme designed to assist needy families
with the purchase of groceries, the food stamps program. This programme is
administered by the states who share in the cost of the programme, although
in some larger states, e.g. New York State (NYS), administration is actually
handled by the counties within the state. The programme allows low-income
families to buy nutritious food with coupons and Electronic Beneï¬ts Trans-
fer cards. To be eligible for this programme, a household must have a gross
monthly income below a level that depends on household size. For example, in
2004 a one-person household must have had a gross monthly income no more
than 960 US dollars while an eight-person household could earn no more than
3296 dollars. Furthermore, there are limits on resources, e.g. bank accounts.
This limit is 2000 dollars unless one member of the household is at least
60 years old, in which case the resource limit is 3000 dollars. A householdâ€™s
monthly food stamps beneï¬t is a function of household size and income.
Because resources, income and even household size are likely to ï¬‚uctuate in
time, beneï¬ts determination is prone to error.
As part of its oversight, FNS directs each state to sample and audit approxi-
mately 1000 transactions each year. State auditors review each transaction and
categorize them as correct, ineligible, overpaid, or underpaid. When there is an
error, the amount in error is also determined. Federal auditors review a sample
of the stateâ€™s ï¬ndings. Based on the results of this quality control audit, the
Department of Agriculture imposes rather large sanctions on states whose ratio
of error dollars to total payments is excessive. This excessive ratio is based on
the average of the error rates for the states and a target error rate of 6%. Should
a stateâ€™s error rate exceed both 6% and the average error rate of all states, a large
penalty is imposed on the state by FNS. From 1998 to 2002, the proportion of
audited transactions found to be in error in NYSs food stamps quality control

654
The Oxford Handbook of Applied Bayesian Analysis
audits were 0.124, 0.101, 0.121, 0.084, and 0.074, which suggests an improving
trend. However, in view of the 6% error rate limit, the uncertainty over whether
this trend would be maintained and the uncertainty associated with the average
error rate among all states, NYS ofï¬cials were rightly concerned about the
possibility that their food stamps program would soon face large federally-
imposed ï¬scal sanctions.
In NYS, the counties administer the food stamps program and the stateâ€™s
portion of the food stamps programâ€™s cost is shared equally between the state
and each county. Should there be a sanction, state ofï¬cials wish to share
the sanction with the counties, in particular those contributing most to the
error rate, but with slightly more than one thousand randomly audited cases
statewide, sample sizes in many counties are very small. It would be hard to
justify penalising a county with a high error rate when that rate was observed in
only a small sample number of transactions. A statistical method was required
to estimate error rates at the county level that would be perceived as fair.
23.1.2 Auditing concepts and terminology
Statistical methods are widely used in auditing all but very small organizations,
because of the impracticality of checking every single transaction. Sampling
may be by means of a random sample of all transactions, but more often a
form of â€˜sampling proportional to sizeâ€™ known as monetary unit sampling is
used.
The key terminology in auditing distinguishes between the book value, which
is the monetary value of a transaction recorded in the accounts being audited,
and the audit value, which is the value determined by the auditor as the correct
value for that transaction. Book values are known for all transactions, or can
be found readily from the organizationâ€™s account data. Audit values are only
known for the sample of transactions that have been audited. When the audit
value differs from the book value, there is an error. If we denote the book value
of transaction k by bk and its audit value by ak, then the error is ek = bk âˆ’ak.
A positive error denotes an overpayment, while a negative error is an
underpayment.
Although it is natural to be interested in the total error 
k ek, auditors may
focus on the total absolute error 
k |ek| since this better represents the quality
of the accounts. This is the case in audits conducted by the US Department of
Agriculture for statesâ€™ food stamps determination and distribution.
It is usual to express errors proportionately to the book value; the taint in
transaction k is ek/bk. Book values and audit values are non-negative,1 so the
1 Accounts for payments and receipts are audited separately. A negative value would indicate a transac-
tion that should be in the other account. Whilst this can happen, it would trigger a speciï¬c investigation
rather than being handled within the statistical analysis.

Sequential Multilocation Auditing
655
taint is less than or equal to 1, but is unbounded below. When we are concerned
with absolute error, taint is bounded below by zero, but not bounded above.
A taint of 1 is sometimes referred to as a bogus transaction, since it means that
no payment should have been made. In the food stamps program audit, another
term for such a transaction is â€˜ineligible.â€™
The food stamps program is characterized also by multilocation audits. Large
organizations very often operate at a number of locations. Accounts for the
transactions at a given location may be located there, making simple random
sampling of transactions impractical since it is necessary to visit the location
of each sampled transaction. Sampling may then be stratiï¬ed (if every location
is audited) or two-stage (if only a sample of locations is audited). There is then
interest in estimating the total (absolute) error at each location as well as for the
organization as a whole. This is obviously challenging in the case of two-stage
sampling, but is not straightforward even if all locations have been sampled. To
identify a location as having a particularly high error rate on the basis of a small
sample may be contentious, and it is natural to think in this case of estimation
methods that involve shrinkage.
It was in this context that the National Audit Ofï¬ce (UK) commissioned
the development of Bayesian methods for the analysis of multilocation audit
data, leading to the models and techniques described by Laws and Oâ€™Hagan
(2000, 2002). This work represented the ï¬rst systematic approach for multi-
location sampling that allowed the incorporation of the auditorâ€™s non-sample
based knowledge of the organization, as well as allowing the total error in
each location to be estimated. A key feature of this analysis was a hier-
archical model that linked each locationâ€™s error rate and taint proï¬le to
underlying common error rates and taint distributions, and so facilitated
the shrinkage of small sample estimates and inference about unsampled
locations.
Whereas this method was clearly well suited to the context of the NYS food
stamps program audit, where the sample sizes in individual counties (locations)
were often small, it was necessary also to address another complication. The
food stamps program audit demanded extensions to allow for repeated annual
audits and for the possibility of trends over time in the error rates. In general,
an organization is nearly always audited periodically, usually at least annually.
There are therefore data available from previous years which may provide useful
information about current error rates. There is some previous work suggesting
methods to incorporate such information sequentially (Heiner 1999, Heiner
and Laws 1999). but nothing that combines both multilocation and sequential
elements. The method of Laws and Oâ€™Hagan (2002) allows knowledge from
previous periods to be incorporated informally through the auditorâ€™s prior
distribution. We present here a model that allows more formal and open incor-
poration of such data.

656
The Oxford Handbook of Applied Bayesian Analysis
23.1.3 Review of previous work
Several features of the audit problem make it a challenging one for statisti-
cal analysis. The ï¬rst of these is that errors are comparatively rare in most
audits and yet the audit samples are usually rather small. The great majority of
transactions yield zero error but the few transactions that are in error can have
large positive or negative errors. The distributions of errors are often referred
to in the ï¬eld as non-standard. With samples typically comprising only a few
hundred transactions and containing only a handful of errors, inference about
such distributions is not straightforward. There has been extensive study of
statistical methods to address the problem of estimating the total error in the
population.
Neter and Loebbecke (1977) empirically study the precision and reliability of
various classical approaches (e.g. mean per unit, ratio, etc.) to estimating total
error amounts. They concluded that these classical methods did not provide
good coverage, and although they did not note that in positively skewed audit
distributions which are typical, both ends of mean per unit conï¬dence intervals
are too low, their tabulated results clearly demonstrate this point. Cochran
(1977) had observed this fact which is particularly useful in adversarial situ-
ations since judges and hearing ofï¬cers frequently rely on lower conï¬dence
limits when determining damages (Heiner, Wagner and Fried 1984; Heiner
and Whitby 1980). Smith (1979) discusses statistical sampling to estimate
total errors in an audit, while Frost and Tamura (1982) suggest the use of a
jackknife ratio estimator for obtaining reliable interval estimates of the true
value of an audit population total. Because ï¬nancial audit sample results are
typically a mixture of mainly zero ï¬ndings and various non-zero ï¬ndings,
Neter and Godfrey (1985) discuss using Bayesian mixture models in monetary
unit sampling to estimate total overstatement. Their mixture distribution com-
bines the likelihood of an overstated error and the density of the magnitude
of the overstated errors. Kvanli, Shen and Deng (1998) propose constructing
interval estimates by inverting likelihood ratio tests when data are assumed to
come from a mixture of a Bernoulli distribution and a normal distribution or
an exponential distribution. Their simulation demonstrated that when error
distributions were Gaussian or Exponential, the coverage probabilities were
better than the traditional methods. Lower conï¬dence limits for their likelihood
proï¬le method were almost always greater than the lower control limit of
the traditional method. In a 1989 review of statistical models and analysis in
auditing, the Panel on Nonstandard Mixtures of Distributions (1989) discusses
various approaches used in statistical auditing and provides an annotated bib-
liography on statistical practices in auditing. Tsui, Matsumura and Tsui (1985)
provide a method of constructing bounds on the proportion of amounts that are
overstated using a Bayesian multinomial Dirichlet model applied to monetary

Sequential Multilocation Auditing
657
unit sampling and Moors and Janssens (1989) consider using Bayesian models
to ï¬nd an upper bound on total overstatement error. van Batenburg, Oâ€™Hagan
and Veenstra (1994) describe a Bayesian hierarchical model that may be used
by auditors to establish compliance with a prescribed minimum standard of
error prevalence in ï¬nancial statements that uses the auditorâ€™s experience with
a ï¬rm. Cohen and Sackrowitz (1996) develop lower conï¬dence bounds for total
overpayments in insurance claims using pilot samples to decide whether or
not a more extensive audit is necessary and then adjusts the conï¬dence limits
obtained by the more extensive sample.
We describe a Bayesian model suited for audits when within the audit sam-
pling frame there are multiple locations and when these audits are conducted
periodically. Using this model, we analyse several years of data from audits of
the food stamps program in NYS.
23.1.4 Notation and outline of the paper
Laws and Oâ€™Hagan (2002) (L&Oâ€™H) model the probability distribution of the
error in any given transaction in location i (for i = 1, 2, . . . , L , where L is the
number of locations in the organization) using four steps.
r The transaction is in error with probability Ã‹i, and so is correct (zero error)
with probability 1 âˆ’Ã‹i. We refer to Ã‹i as the error rate in location i.
r If the transaction is in error, the error falls into error class j with probability
Â¯i j, for j = 1, 2, . . . , p, where p is the number of error classes. Error classes
can be ï¬‚exibly deï¬ned with reference to the range of possible taints. L&Oâ€™H
primarily used p = 3 classes â€“ bogus transactions (taint 1), overpayments
(taint greater than 0 but less than 1) and underpayments (taint less than
0). The vector of class error probabilities for location i is denoted by Â¯i =
(Â¯i1, Â¯i2, . . . , Â¯ip).
r If the transaction falls into error class j, and if that class covers more than
a single taint value, then it has a taint sampled from a taint distribution
appropriate to that class.
r Finally, having determined the taint, the error arises from multiplying the
taint by the book value. Book values are usually known exactly, although in
some situations might be uncertain.
The model presented here extends L&Oâ€™H by modelling evolution over time
of the Ã‹is and the Â¯is. The modelling of the taint distribution within error
classes is as in L&Oâ€™H, and these distributions are assumed to be constant over
time. Apart from a small change detailed in Section 23.5.1 the treatment of book
values is also as in L&Oâ€™H, and the reader is referred to that source for details
of both these aspects of the modelling and analysis.

658
The Oxford Handbook of Applied Bayesian Analysis
We denote the values of Ã‹i and Â¯i at time period t by Ã‹t
i and Â¯t
i =
(Â¯t
i1, Â¯t
i2, . . . , Â¯t
ip), respectively. L&Oâ€™H introduce hyperparameters Ã‹0 and Â¯0 by
modelling the location-speciï¬c parameters hierarchically, and these will also
have values Ã‹t
0 and Â¯t
0 = (Â¯t
01, Â¯t
02, . . . , Â¯t
0p) at time t. Furthermore, to model the
evolution over time we introduce a trend parameter that takes value Ã™t
0 at time
t. The set of parameters at time t is therefore
â€št =

Ã‹t
0, Ã‹t
1, . . . , Ã‹t
L, Ã™t
0, Â¯t
0, Â¯t
1, . . . , Â¯t
L

.
Finally, we denote the information up to time t by Dt. The sequential analysis
can then be considered as a cycle in two stages.
1. Update. Given the prior distribution for â€št, based on Dtâˆ’1, and given the
audit data obtained in period t, derive the posterior distribution for â€št.
2. Project. Given the posterior distribution of â€št, based on Dt, construct
a distribution for â€št+1, also based on Dt. This then becomes the prior
distribution for the next update step.
In Section 23.2 we present the form of the hierarchical prior distribution for
â€št based on Dtâˆ’1, which is the same as in L&Oâ€™H except for the addition of
the trend parameter. The update step is described in Section 23.3. Section 23.4
presents the project step, which embodies the modelling of the evolution of the
system over time. In Section 23.5 we return to the food stamps problem which
originally motivated the development of this model, and present the resulting
analysis of the New York food stamps data. We conclude with a brief discussion
in Section 23.6.
23.2 Modelling of error rates and error classes
23.2.1 Modelling relationships between parameters
We introduce here a technique that we will use repeatedly to relate one parame-
ter, or vector of parameters, to another. Suppose ï¬rst that p and pâ€² are random
variables lying in [0.1] such that we view pâ€² as a perturbation of p. We represent
this via the Beta conditional distribution
pâ€² | p âˆ¼Be(Ë˜g(p,r), Ë˜{1 âˆ’g(p,r)}) ,
(23.1)
where
g(p,r) =
p exp(r)
1 + p{exp(r) âˆ’1} .
(23.2)

Sequential Multilocation Auditing
659
The interpretation of this modelling is as follows. First note from (23.1) that
E(pâ€² | p) = g(p,r), and from (23.2) that
r = log
8
g(p,r)
1 âˆ’g(p,r)
>
p
1 âˆ’p
9
.
Therefore r is the log-odds ratio of the expected value of pâ€² versus p. So r
represents an expected shift of pâ€² away from p. If r = 0, g(r, p) = p and there
is no systematic shift of pâ€² from p. If r is positive/negative, then there is a
positive/negative shift such that pâ€² is expected to be larger/smaller than p.
The second parameter in this model is Ë˜, which is a precision parameter that
controls the variability of the perturbation:
V(pâ€² | p) = g(p,r){1 âˆ’g(p,r)}/(Ë˜ + 1) .
(23.3)
Now consider a generalization of this idea to vectors p = (p1, p2, . . . , pd) and
pâ€² =

pâ€²
1, pâ€²
2, . . . , pâ€²
d

taking values in the (d âˆ’1)-dimensional simplex. That is,
each pi and pâ€²
i lies in [0, 1] and d
i=1 pi = d
i=1 pâ€²
i = 1. We similarly represent pâ€²
as a perturbation of p via the Dirichlet conditional distribution
pâ€² | p âˆ¼Di(Ë˜h(p,r)) ,
(23.4)
where the vector function h has elements
hi(p,r) =
pi exp(ri)
d
k=1 pk exp(rk)
,
i = 1, 2, . . . , d.
(23.5)
The Beta model is the special case d = 2 of the Dirichlet model where
p = (p, 1 âˆ’p), and with the g function corresponding to the h function by set-
ting r = (r, 0). In the Dirichlet case we can without loss of generality constrain
rd to equal zero.
23.2.2 Error rates
Following L&Oâ€™H, we model the location error rates Ã‹t
i at any time t hierar-
chically, introducing a â€˜typicalâ€™ error rate Ã‹t
0. Each Ã‹t
i is linked to Ã‹t
0 through a
log-odds ratio parameter Ã’t
i and a precision parameter Ë˜t
i:
Ã‹t
i | Ã‹t
0, Dtâˆ’1 âˆ¼Be

Ë˜t
ig

Ã‹t
0, Ã’t
i

, Ë˜t
i

1 âˆ’g

Ã‹t
0, Ã’t
i

.
Note that L&Oâ€™H modelled this relationship with an odds ratio parameter â€° that
corresponds to the exponential of our Ã’.
At the ï¬rst time point, the information D0 is the prior information before
any audit data become available. The initial values Ã’1
i and Ë˜1
i are then set as
in L&Oâ€™H to reï¬‚ect the auditorâ€™s prior knowledge. The Ã’1
i can represent the
auditorâ€™s belief that location i is more or less error-prone than the typical. In the
absence of any special beliefs about differing error rates at location i, we could
set Ã’1
i = 0, but positive or negative values can reï¬‚ect the auditorâ€™s expectation of

660
The Oxford Handbook of Applied Bayesian Analysis
a higher or lower error rate than the typical. The meaning of a â€˜typicalâ€™ location is
determined by these initial choices of the auditor, since a typical location is one
having Ã’1
i = 0. The location speciï¬c precision parameter Ë˜t
i allows the auditor to
express different levels of prior information concerning each location.
The values of Ã’t
i and Ë˜t
i at subsequent time points are then determined by
the updateâ€“project cycle as described in subsequent sections. Although in later
times there may be no location with Ã’t
i = 0, the meaning of Ã‹0 as an underly-
ing error rate to which the error rates in other locations are referred is pre-
served, and the hierarchical structure induces posterior shrinkage at each time
point.
At the next stage of the hierarchy we follow L&Oâ€™H by assuming a Beta prior
distribution for the typical error rate Ã‹t
0, but this is now conditional on the trend
Ã™t
0. The trend parameter works as another log-odds ratio parameter in
Ã‹t
0 | Ã™t
0, Dtâˆ’1 âˆ¼Be

Ë˜t
0g

mt
0, Ã™t
0

, Ë˜t
0

1 âˆ’g

mt
0, Ã™t
0

,
(23.6)
where Ë˜t
0 determines precision and mt
0 expresses the expectation of Ã‹t
0 if there
is no trend, i.e. if Ã™t
0 = 0. However, if Ã™t
0 > 0 there is an increasing trend and the
expectation of Ã‹t
0 will be higher, whereas if Ã™t
0 < 0 it will be lower. The initial
values Ë˜1
0 and m1
0 will be based on the auditorâ€™s initial prior information, as in
L&Oâ€™H. Then subsequent values are determined by the updateâ€“project cycle.
The prior model for error rates is completed by a normal distribution for Ã™t
0,
Ã™t
0 | Dtâˆ’1 âˆ¼N

dt
0, vt
0

.
(23.7)
Again, the initial values d1
0 and v1
0 will represent the auditorâ€™s prior information
and subsequent values will be determined by the updateâ€“project cycle.
23.2.3 Error classes
We again follow L&Oâ€™H and use a Dirichlet hierarchical model to link Â¯t
i to a
baseline vector of typical category probabilities Â¯t
0 = (Â¯t
01, Â¯t
02 . . . , Â¯t
0p).
Â¯t
i | Â¯t
0, Dtâˆ’1 âˆ¼Dir

â€žt
ih

Â¯t
0, Ã’t
i

,
where the distribution is determined by the precision parameter â€žt
i (an effective
sample size) and the Ã’t
i vector. Without loss of generality, we let Ã’t
ip = 0. The
initial values â€ž1
i and Ã’1
i deï¬ne the auditorâ€™s prior knowledge about how Â¯t
i relates
to the typical Â¯t
0, and thereafter they are determined by the updateâ€“project
cycle.
The prior distribution for Â¯t
0 is also Dirichlet, but we do not introduce a trend
component:
Â¯t
0 | Dtâˆ’1 âˆ¼Dir

â€žt
0ht
0

,
(23.8)

Sequential Multilocation Auditing
661
where â€žt
0 is a scalar precision parameter and ht
0 is the mean vector and satisï¬es
p
j=1 ht
0 j = 1. Initially, â€ž1
0 and h1
0 deï¬ne the auditorâ€™s prior information, and the
updateâ€“project cycle determines subsequent values.
23.3 Updating
At time t, the auditor visits some or all of the locations, takes samples of
transactions at the visited locations, and observes which transactions are in
error, which error class those transactions fall into, and the size of the taint.
All of this information is added to Dtâˆ’1 to form Dt.
Consider ï¬rst the posterior distribution for the error rates. Suppose that the
auditor discovers r t
i errors in a sample of size nt
i from location i, i = 1, 2, . . . , L.
For any location which is not visited, r t
i = nt
i = 0. Following the development in
L&Oâ€™H, the posterior distributions of the Ã‹t
is conditional on Ã‹t
0 are independent
Beta distributions, i.e. the Beta posterior distribution whose parameters are
the parameters of the Beta prior updated by adding the number of errors and
number of correct cases, respectively:
Ã‹t
i | Ã‹t
0, Dt âˆ¼Be

Ë˜t
ig

Ã‹t
0, Ã’t
i

+ r t
i , Ë˜t
i

1 âˆ’g

Ã‹t
0, Ã’t
i

+ nt
i âˆ’r t
i

.
(23.9)
After integrating out these parameters, the joint posterior distribution of Ã‹t
0 and
Ã™t
0 is found to be
p

Ã‹t
0, Ã™t
0 | Dt
âˆ

Ã‹t
0
a1(Ã™t
0)âˆ’1 
1 âˆ’Ã‹t
0
a2(Ã™t
0)âˆ’1
B(a1

Ã™t
0

, a2

Ã™t
0

)
exp

âˆ’1
2vt
0

Ã™t
0 âˆ’dt
0
2

Ã—
L

i=1
B

Ë˜t
ig

Ã‹t
0, Ã’t
i

+ r t
i , Ë˜t
i

1 âˆ’g

Ã‹t
0, Ã’t
i

+ nt
i âˆ’r t
i

B

Ë˜t
ig

Ã‹t
0, Ã’t
i

, Ë˜t
i

1 âˆ’g

Ã‹t
0, Ã’t
i

, (23.10)
where a1(Ã™t
0) = Ë˜t
0g(mt
0, Ã™t
0) and a2(Ã™t
0) = Ë˜t
0{1 âˆ’g(mt
0, Ã™t
0)}.
Now consider the posterior distribution for the error class probabilities. The
auditor observes the vector ct
i = (ct
i1, ct
i2, . . . , ct
ip) of error class counts at location
i, i = 1, 2, . . . , L, where p
j=1 ct
i j = r t
i . The posterior distributions of the Â¯t
is
conditional on Â¯t
0 are independent Dirichlet distributions:
Â¯t
i | Â¯t
0, Dt âˆ¼Dir

â€žt
ih

Â¯t
0, Ã’t
i

+ ct
i

.
After integrating out these parameters, the posterior distribution of Â¯t
0 is given
by
p

Â¯t
0 | Dt
âˆ
âŽ›
âŽ
p

j=1

Â¯t
0 j
â€žt
0ht
0 j âˆ’1
âŽž
âŽ 
L

i=1
D

â€žt
ih

Â¯t
0, Ã’t
i

+ ct
i

D

â€žt
ih

Â¯t
0, Ã’t
i

,
(23.11)
where if x has elements x1, x2, . . . , xp then D(x) = 	p
i=1 (xi)/(p
i=1 xi).

662
The Oxford Handbook of Applied Bayesian Analysis
Finally, the auditor will also learn the taint values for the transactions in
error, and may also learn some book value information, but these are handled
in exactly the same way as in L&Oâ€™H. In particular, since we assume that
taint distributions within error classes are constant over time, the posterior
distribution for these at time t is based on the simple accumulation of all
taint values observed up to time t combined with the auditorâ€™s original prior
distribution.
The posterior distributions (23.10) and (23.11) are generally intractable, but
low-dimensional. Hence we can derive posterior inference by numerical inte-
gration of these distributions. This is discussed in L&Oâ€™H, particularly with
reference to using importance sampling for Â¯0. Through their conditional dis-
tributions, we can make inference also about Ã‹t
i and Â¯t
i. L&Oâ€™H discuss doing
this by Monte Carlo sampling but we can also evaluate moments directly by
numerical integration with respect to (23.10) and (23.11); for example E

Ã‹t
i | Dt
can be evaluated as the expectation of
Ë˜t
ig

Ã‹t
0, Ã’t
i

+ r t
i
Ë˜t
i + nt
i
with respect to (23.10). Note that the structure of our model means that we can
use this simple method based on Monte Carlo sampling, and so do not need to
employ the much more intricate and computationally intensive Markov chain
Monte Carlo (MCMC) approach that is widely used for deriving inferences in
many other Bayesian applications.
23.4 Projection
23.4.1 Models for projection
In the projection step we formalize the evolution over time of the parameters
from â€št to â€št+1. Based on the information Dt available at time t, we model
beliefs about the parameters at time t + 1. In order to keep the sequential
analysis of audit data manageable, we need to express the projection so that
the distribution of â€št+1 based on data Dt has the same form as that of â€št based
on data Dtâˆ’1, given in Section 23.2. For instance, if we consider the typical error
rate parameter Ã‹0 we have a Beta distribution Be(Ë˜t
0g(mt
0, Ã™t
0), Ë˜t
0{1 âˆ’g(mt
0, Ã™t
0)})
for Ã‹t
0 given Ã™t
0 and data Dtâˆ’1. We therefore need to express the distribution of
Ã‹t+1
0
given Ã™t+1
0
and data Dt as Be(Ë˜t+1
0 g(mt+1
0 , Ã™t+1
0 ), Ë˜t+1
0 {1 âˆ’g(mt+1
0 , Ã™t+1
0 )}). The
projection step needs to determine how the parameters Ë˜t+1
0
and Ã™t+1
0
are to be
constructed based on the current information about Ã‹t
0 given by its posterior
distribution (23.10).
To motivate the projection in the audit model, consider the following simple
process model for a vector pt of error rates at time t. We suppose that rates

Sequential Multilocation Auditing
663
evolve by the model
pt+1 | pt âˆ¼Di(k pt),
(23.12)
so that the expectation of pt+1 given pt is pt, and the evolution precision
parameter k determines how far it might be from pt. Hence k represents the
time volatility of error rates or the level of information decay, with smaller k
implying more variability from t to t + 1.
Now suppose that the distribution of pt given information at time t is
Di(Ë†mt). The marginal distribution of pt+1 resulting from this formulation
is not Dirichlet, but we can ï¬nd its mean and variance and approximate as a
Di(Ë˜mt) where the precision parameter is
Ë˜ =
kË†
k + Ë† + 1.
(23.13)
Our formulation of the projection step will use (23.13) as a model for the way
that uncertainty is increased by the evolution of error rates from time t to time
t + 1. However, a variety of considerations are needed to specify the evolution
mean.
23.4.2 Error rates
First consider the distribution of Ã‹t+1
i
conditional on Ã‹t+1
0
and information Dt.
As discussed in Section 23.4.1, this needs to match the prior model in Section
23.2. So it should be of the form Be(Ë˜t+1
i
g(Ã‹t+1
0 , Ã’t+1
i
), Ë˜t+1
0
{1 âˆ’g(Ã‹t+1
0 , Ã’t+1
0 )}).
The projection step needs to determine Ë˜t+1
i
and Ã’t+1
i
.
The posterior conditional distribution (23.9) is Beta with precision parameter
Ë˜t
i + nt
i. Following the above model and equation (23.13), we let
Ë˜t+1
i
=
kt
i

Ë˜t
i + nt
i

kt
i + Ë˜t
i + nt
i + 1 ,
(23.14)
for some speciï¬ed evolution decay parameter kt
i. We do not suppose any trend
in the location-speciï¬c error rates conditional on the typical error rate (the trend
being expressed in the typical error rate Ã‹t
0), so we should let the conditional
expectation of Ã‹t+1
i
be the same as the posterior conditional mean of Ã‹t
i, but this
is not of the required form g(Ã‹t+1
0 , Ã’t+1
i
) for any Ã’t+1
i
. We therefore refer back to
the idea of Ã’ as a log-odds ratio and deï¬ne
Ã’t+1
i
= log
6
E

Ã‹t
i | Dt
1 âˆ’E

Ã‹t
i | Dt
>
E

Ã‹t
0 | Dt
1 âˆ’E

Ã‹t
0 | Dt
7
,
(23.15)
where both expectations in this equation need to be evaluated numerically from
the posterior distribution (23.10). In particular, the expectation of Ã‹t
i is evaluated
as described at the end of Section 23.3.

664
The Oxford Handbook of Applied Bayesian Analysis
Next consider the distribution of Ã‹t+1
0
conditional on the trend Ã™t+1
0 . As
discussed above, this needs to be of the form Be(Ë˜t+1
0 g(mt+1
0 , Ã™t+1
0 ), Ë˜t+1
0 {1 âˆ’
g(mt+1
0 , Ã™t+1
0 )}). The projection is deï¬ned by identifying appropriate formulae for
Ë˜t+1
0
and mt+1
0 . From the way the trend is modelled, we set
mt+1
0
= E

Ã‹t
0 | Dt
,
(23.16)
where this expectation is evaluated numerically from the posterior distribution
(23.10). To set the precision parameter Ë˜t+1
0
we use again the model of Section
23.4.1. The posterior distribution of Ã‹t
0 from (23.10) is not Beta, but we infer a
precision value
Ë†t
0 = E

Ã‹t
0 | Dt 
1 âˆ’E

Ã‹t
0 | Dt
V

Ã‹t
0 | Dt
âˆ’1 ,
(23.17)
where V

Ã‹t
0 | Dt
is also evaluated numerically from (23.10). Then we have
Ë˜t+1
0
=
kt
0Ë†t
0
kt
0 + Ë†t
0 + 1 ,
(23.18)
for some speciï¬ed evolution decay parameter kt
0.
We now need a projection for the trend parameter. The distribution of Ã™t+1
0
given data Dt needs to be of the same form as that of Ã™t
0 given data Dtâˆ’1 in
Section 23.2, i.e. N(dt+1
0 , vt+1
0 ). We need to deï¬ne dt+1
0
and vt+1
0 , which we do as
follows. First
dt+1
0
= r t E

Ã™t
0 | Dt
,
(23.19)
where the trend shrinkage parameter r t represents a belief that any trend can
peter out over time. If r t = 1 the prior expectation of Ã™t+1
0
is the same as the
posterior expectation of Ã™t
0, while if r t = 0 the prior expectation of Ã™t+1
0
is 0. So
at one extreme r t = 1 represents an expectation that a trend at time t will be
sustained at time t + 1, whereas at the other extreme r t = 0 means that we donâ€™t
believe in trends. For the variance evolution we let
vt+1
0
= (r t)2V

Ã™t
0 | Dt
+ wt,
(23.20)
where the role of wt is as an evolution variance, which represents the loss of
information over time. Both E

Ã™t
0 | Dt
and V

Ã™t
0 | Dt
are evaluated numerically
from (23.10).
23.4.3 Error classes
The projection for Â¯t+1
i
given Â¯t+1
0
proceeds as for Ã‹t+1
i
given Ã‹t+1
0 . Thus we ï¬rst
have the new precision parameter
â€žt+1
i
=
l t
i

â€žt
i + r t
i

lt
i + â€žt
i + r t
i + 1 ,

Sequential Multilocation Auditing
665
where lt
i is a speciï¬ed evolution decay parameter. The log-odds parameters are
then deï¬ned by solving the equations
E

Â¯t
i | Dt
= hi

E

Â¯t
0 | Dt
, Ã’t+1
i

,
i = 1, 2, . . . , p,
where the expectations are evaluated with respect to the posterior distribution
(23.11).
Finally, we need a projection for Â¯t+1
0 , which should have a Dirichlet distrib-
ution Di(â€žt+1
0 ht+1
0 ). We do not model any trend in these parameters over time,
and hence we set
ht+1
0
= E

Â¯t
0 | Dt
.
The choice of â€žt+1
0
entails a compromise because we could deï¬ne it by reference
to the posterior variance of any one element of Â¯t
0. We do so conservatively by
ï¬rst letting
Ëœt
0 = min
i
E

Â¯t
0i | Dt 
1 âˆ’E

Â¯t
0i | Dt
V

Â¯t
0i | Dt
âˆ’1 ,
and then
â€žt+1
0
=
lt
0Ëœt
0
lt
0 + Ëœt
0 + 1 ,
for some speciï¬ed evolution decay parameter l t
0.
23.4.4 Choosing the evolution parameters
The remaining question is how to specify the evolution parameters for the
projection step. We have parameters {kt
0, kt
1, . . . , kt
p} and {lt
0,lt
1, . . . ,l t
p} which all
describe the volatility of the error and class rates over time. Small/large values
of these parameters imply high/low variability from one time point to the next.
These evolution decay parameters should be set to reï¬‚ect how much we think
error rates might change over time. We would generally expect less volatility in
the overall baseline error rates than in individual locations, in which case kt
i and
l t
i should be less than kt
0 and l t
0, for instance.
There may be reasons for supposing that some locations might change more
from time t to time t + 1 than others, and this would lead us to use different kt
i
values for different locations. A location that has been warned to put its house
in order, for example, could change more and we could give it a lower kt
i.
We also need to specify the trend persistence parameter r t and the trend evo-
lution variance parameter wt, which should reï¬‚ect beliefs about the consistency
and longevity of trends in the underlying mean error rate.
We discuss these parameters further in the next section when we return to
the food stamps program, and also in Section 23.6.

666
The Oxford Handbook of Applied Bayesian Analysis
23.5 Application to New York food stamps audit
23.5.1 Modelling
To apply the general model the locations are the L = 57 upstate counties (i.e.
excluding New York city) of New York state. We report here an analysis for the
ï¬scal years 1998 to 2003, so t = 1 corresponds to 1998 and t = 6 to 2003. Because
the statewide sample is random and county populations are extremely varied,
data in some counties for some years are sparse or even nonexistent.
We ï¬rst deï¬ne error classes. Whereas previous applications of the L&Oâ€™H
model used three classes, we chose to subdivide the understatements class.
Understatements for which the absolute magnitude of the taint was greater
than 5 were treated separately from those with less extreme taint values because
the transactions were seen to have quite different book values from the other
classes. Our model assumes that book values and taints are independent within
each error class. The data clearly showed a small set of negative taint values
associated with the minimum beneï¬t payouts. Since the values of these taints
were larger, in absolute value, than the other transactions, it was simple to
create a new error category for large understatements. We therefore had p = 4
error classes: bogus transactions (taint 1), overstatements (taint greater than 0
and less than 1), understatements (taint less than 0 but greater than âˆ’5), large
understatements (taint less than or equal to âˆ’5).
Evolution parameters were set as follows. We set kt
0 = l t
0 = 99 for all time
periods t, and kt
i = l t
i = 24 for all counties i and time periods t. The value 99
says that if the baseline rate for some error category is 0.1, then the standard
deviation of its change in one time period is 0.03, which we consider to be a
suitably small value. Similarly, if the baseline rate for some category is 0.04,
the standard deviation of its one period change is 0.02. The value of 24 for
individual county evolutions implies a standard deviation of the one period
change being 0.06 if the current rate is 0.1, or 0.04 if the current rate is 0.04.
Evolution parameters for the trend were taken as r t = 0.7 and wt =
2.0, implying a belief in any trend being reasonably persistent but highly
variable.
Finally, we used deliberately vague priors in order to see how the trend and
rates would adapt to the data. Speciï¬cally, we assumed m1
0 = 0.5, Ë˜1
0 = 2 for the
prior distribution of the typical error rate; d1
0 = 0, v1
0 = 100 for the trend; Ã’1
i =
Ã’1
i j = 0, for all locations and classes (i.e. we regard all counties as â€˜typicalâ€™, having
no prior information to expect any given county to be more error prone than any
other); equal county speciï¬c precisions â€ž1
i = 2, Ë˜1
i = 2 for all i; and â€ž1
0 = 4 and
h1
01 = . . . = h1
04 = 0.25 for the typical error class rates. Table 23.1 illustrates how
trend and rates adapt to data.
In Section 23.5.3 some of these values are modiï¬ed as part of a sensitivity
study.

Sequential Multilocation Auditing
667
Table 23.1 Means (variances) of typical error
rate Ã‹0 and trend Ã™0.
Year
Ã‹0
Ã™0
1998
0.217
(9.07 Ã—10âˆ’4)
0.143
(0.359)
1999
0.177
(7.19 Ã—10âˆ’4)
âˆ’0.276
(0.132)
2000
0.131
(5.25 Ã—10âˆ’4)
âˆ’0.399
(0.170)
2001
0.130
(5.70 Ã—10âˆ’4)
âˆ’0.108
(0.192)
2002
0.126
(5.11 Ã—10âˆ’4)
âˆ’0.082
(0.193)
2003
0.128
(5.32 Ã—10âˆ’4)
âˆ’0.036
(0.196)
23.5.2 Results
Figure 23.1 plots the posterior mean for the typical overall error rate Ã‹t
0 with its
uncertainty, together with posterior mean location speciï¬c error rates for two
example counties, Genessee and Monroe. Figure 23.2 plots in the same way the
posterior mean and 95% credible intervals for the typical proportion of errors Â¯t
i j
in each of the error classes and, for comparison, the posterior mean proportions
for Genessee and Monroe counties. In 2002 and 2003 the proportions of under-
statements and overstatements in Genessee have deviated signiï¬cantly from
the typical proportions, but it is clear from Figure 23.1 that error information
generally is poor for these years within Genessee. This can be explained by
the fact that sampled transactions consist of only one understatement and one
correct transaction for 2002, and three correct transactions for 2003. Sampled
transactions from earlier years contain no errors, so the rates are almost equal
to the typical rates. The error class proportions for Monroe deviate much more
from the typical, as we have more sampled error transactions of each type from
this county.
Figure 23.3 plots posterior mean error rates for all counties in NYS. Each
panel in this trellis display shows a countyâ€™s estimated error rate for each year.
Panels are ordered from highest error rate (lower left) to lowest error rate (upper
right) in the most recent year. This allows one to observe the trend for each
county and each countyâ€™s pattern compared to the other counties.
Since simulation draws are made from the posterior distributions, it is easy
to estimate the probability that a county error rate will exceed any threshold and
to develop the distribution of ranks. Figure 23.4 shows the posterior densities
of error rates for nine example counties. The vertical line at 0.06 is of particular
interest because of the Federally deï¬ned threshold having to do with sanctions.
Figure 23.5 graphs cumulative posterior distribution functions (CDF) for four
counties while Figure 23.6 displays the posterior probability that the error rate
is less than 6% for each county. The posterior means and credible intervals for
the ranks are shown in Figure 23.7. State and county ofï¬cials have found these
ï¬gures useful in managing the quality of the program.

668
The Oxford Handbook of Applied Bayesian Analysis
1998
1999
2000
2001
2002
2003
Year
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Error rate (theta)
typical error rate
genessee error rate
monroe error rate
Fig. 23.1 A selection of error rates, with 95% posterior probability intervals.
Year
Overall proportion of understatements
Overall proportion of large understatements
Overall proportion of overstatements
1998
1999
2000
2001
2002
2003
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Year
1998
1999
2000
2001
2002
2003
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Year
1998
1999
2000
2001
2002
2003
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Year
1998
1999
2000
2001
2002
2003
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Overall proportion of bogus transactions
Fig. 23.2 Typical error class proportions, showing the means (circles) and 95% posterior probability
intervals. Corresponding proportions for Genessee county (ï¬lled squares) and Monroe county (ï¬lled
circles) are also shown.

Sequential Multilocation Auditing
669
1998
1999
2000
2001
2002
2003
1998
1999
2000
2001
2002
2003
1998
1999
2000
2001
2002
2003
1998
1999
2000
2001
2002
2003
1998
1999
2000
2001
2002
2003
1998
1999
2000
2001
2002
2003
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
Error rate
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
Tioga
Yates
Cortland
Chemung
Cayuga
Rensselaer
Onondaga
Schenectady
Herkimer
Oneida
Warren
Broome
Jefferson
Nassau
Tompkins
Westchester
Rockland
Madison
Orange
Genesee
Delaware
Oswego
Suffolk
Albany
Monroe
Ontario
Steuben
St. Lawrence
Saratoga
Franklin
Seneca
Chenango
Wayne
Chautauqua
Montgomery
Lewis
Niagara
Fulton
Schuyler
Allegany
Putnam
Otsego
Schoharie
Greene
Orleans
Dutchess
Wyoming
Erie
Essex
Hamilton
Columbia
Sullivan
Livingston
Cattaraugus
Ulster
Washington
Clinton
Federal Fiscal Year
Food Stamp QC County Rates(1998â€“2003)
Fig. 23.3 Trellis display of food stamp quality control error rates by county and year (1998â€“2003).

670
The Oxford Handbook of Applied Bayesian Analysis
Density plot for county error rate 
depicting probability > 6%
Albany
Error rate
0.0
0.05
0.15
0.25
0
2
4
6
8
10
Density plot for county error rate
depicting probability > 6%
Allegany
Error rate
0.0
0.1
0.2
0.3
0
5
15
10
Density plot for county error rate
depicting probability > 6%
Broome
Error rate
0.0
0.1
0.2
0.3
0
2
4
6
8
10
Density plot for county error rate
depicting probability > 6%
Cattaraugus
Error rate
0.0
0.05
0.10
0.15
0
10
20
30
Density plot for county error rate
depicting probability > 6%
Cayuga
Error rate
0.0
0.1
0.2
0.3
0.4
0
1
2
3
4
5
6
Density plot for county error rate
depicting probability > 6%
Chautauqua
Error rate
0.0
0.1
0.2
0.3
0
2
4
6
8
10
Density plot for county error rate
depicting probability > 6%
Chemung
Error rate
0.0
0.1
0.2
0.3
0.4
0
2
4
6
8
Density plot for county error rate
depicting probability > 6%
Chenango
Error rate
0.0
0.05
0.15
0.25
20
15
10
5
0
Density plot for county error rate
depicting probability > 6%
Clinton
Error rate
0.0
0.05
0.10
0.15
50
30
10
0
Fig. 23.4 Densities of error rates for some example counties.

Sequential Multilocation Auditing
671
Error rate
Probability less than x
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.05
0.10
0.15
0.20
0.25
n:1000 m:0
Distribution functions for county error rates Albany, Allegany, Broome, Cattaraugus
Cattaraugus
Broome
Albany
Allegany
Fig. 23.5 CDFs of error rate for four example counties.
23.5.3 Sensitivity study
We tried four variations on the model and considered the effect on the evolu-
tion of Ã‹0 and Ã™0. The ï¬rst three modify the evolution parameters. The ï¬nal
one turns off trend learning altogether. The tables should be compared with
Table 23.1.
1. Set ve = 10 (increases loss of information about trend) (Table 23.2).
2. Set r e = 0.3 (trend more likely to peter out) (Table 23.3).
3. Set ke
0 = 30 and ke
i = 10 (reduced precision of error rate evolutions) (Table
23.4).
4. Set ve = 1 Ã— 10âˆ’8 and Ã™0 âˆ¼N(0, 1 Ã— 10âˆ’8) (effectively turns trend mod-
elling off) (Table 23.5).
Table 23.2 Means (variances) of typical error
rate Ã‹0 and trend Ã™0 Variation 1.
Year
Ã‹0
Ã™0
1998
0.217
(9.07 Ã—10âˆ’4)
0.143
(0.359)
1999
0.176
(6.50 Ã—10âˆ’4)
âˆ’0.324
(0.143)
2000
0.131
(5.18 Ã—10âˆ’4)
âˆ’0.411
(0.195)
2001
0.129
(5.49 Ã—10âˆ’4)
âˆ’0.104
(0.205)
2002
0.123
(4.96 Ã—10âˆ’4)
âˆ’0.104
(0.210)
2003
0.125
(5.44 Ã—10âˆ’4)
âˆ’0.064
(0.203)

672
The Oxford Handbook of Applied Bayesian Analysis
0.0
0.2
0.4
0.6
0.8
1.0
Probability that 2003 error rate is less than 0.06
Erie
Ulster
Clinton
Cattaraugus
Washington
Columbia
Sullivan
Niagara
Dutchess
Essex
Steuben
Chenango
Allegany
Wyoming
Orleans
Monroe
Wayne
Otsego
Franklin
Schuyler
Jefferson
Greene
Putnam
Genesee
Hamilton
Saratoga
Montgomery
Fulton
Rockland
Schoharie
St. Lawrence
Orange
Livingston
Lewis
Nassau
Ontario
Chautauqua
Albany
Tompkins
Oneida
Delaware
Broome
Warren
Herkimer
Chemung
Seneca
Suffolk
Schenectady
Rensselaer
Yates
Westchester
Cayuga
Oswego
Madison
Tioga
Cortland
Onondaga
Fig. 23.6 Probability that the error rate is less than 6% for each county.
The ï¬rst two variations are seen to have little effect on the posterior distri-
butions for the typical error rate and underlying trend. In the third variation,
the increased volatility assumed for all the error rates seems to obscure the
learning about trend, with the result that the underlying trend in the data is
not picked up so well by the model. Similarly, the fourth variation removes the

Sequential Multilocation Auditing
673
0
10
20
30
40
50
Food stamp rank with credible intervals, 2003
Erie
Clinton
Ulster
Washington
Columbia
Cattaraugus
Sullivan
Essex
Wyoming
Dutchess
Schuyler
Chenango
Orleans
Niagara
Allegany
Putnam
Otsego
Greene
Hamilton
Steuben
Wayne
Franklin
Genesee
Jefferson
Montgomery
Schoharie
Fulton
Saratoga
Monroe
Rockland
St. Lawrence
Orange
Livingston
Lewis
Nassau
Ontario
Chautauqua
Oneida
Albany
Broome
Tompkins
Delaware
Suffolk
Herkimer
Warren
Seneca
Chemung
Schenectady
Rensselaer
Westchester
Yates
Oswego
Cayuga
Madison
Onondaga
Cortland
Tioga
Fig. 23.7 County 2003 food stamp error rate ranks with credible intervals.
trend component from the model, and also results in the posterior estimates
not showing the full trend towards lower overall error rates that was seen in
the data. To the extent that we can learn from this simple sensitivity analysis,
it seems that the trend component is important in the model, and its posterior
distribution may be quite robust to the trend evolution parameters. Also, if there

674
The Oxford Handbook of Applied Bayesian Analysis
Table 23.3 Means (variances) of typical error
rate Ã‹0 and trend Ã™0 Variation 2.
Year
Ã‹0
Ã™0
1998
0.217
(9.07 Ã—10âˆ’4)
0.143
(0.359)
1999
0.175
(6.52 Ã—10âˆ’4)
âˆ’0.302
(0.132)
2000
0.132
(5.07 Ã—10âˆ’4)
âˆ’0.351
(0.145)
2001
0.130
(6.06 Ã—10âˆ’4)
âˆ’0.063
(0.179)
2002
0.126
(5.11 Ã—10âˆ’4)
âˆ’0.086
(0.178)
2003
0.126
(5.35 Ã—10âˆ’4)
âˆ’0.059
(0.180)
Table 23.4 Means (variances) of typical error
rate Ã‹0 and trend Ã™0 Variation 3.
Year
Ã‹0
Ã™0
1998
0.217
(9.07 Ã—10âˆ’4)
0.143
(0.359)
1999
0.180
(7.78 Ã—10âˆ’4)
âˆ’0.258
(0.285)
2000
0.147
(7.18 Ã—10âˆ’4)
âˆ’0.304
(0.342)
2001
0.150
(9.40 Ã—10âˆ’4)
âˆ’0.093
(0.374)
2002
0.153
(7.98 Ã—10âˆ’4)
âˆ’0.078
(0.371)
2003
0.165
(9.86 Ã—10âˆ’4)
âˆ’0.005
(0.342)
is (assumed to be) much annual volatility in error rates (as expressed in the Ã’
parameters), then learning about underlying trend is reduced.
23.6 Discussion
23.6.1 Success of the method in New York state
In 2003, reporting of county error rates was initiated in New York state. Coun-
ties with high error rates objected, but it was pointed out that the hierarchical
nature of the model shrunk county speciï¬c error rates toward the statewide
rate, resulting in modeled rates being less than empirical rates in counties with
high error rates. Some counties reported that they reviewed and revised their
Table 23.5 Means (variances) of
typical error rate Ã‹0 Variation 4.
Year
Ã‹0
1998
0.212
(8.48 Ã—10âˆ’4)
1999
0.180
(5.28 Ã—10âˆ’4)
2000
0.144
(4.15 Ã—10âˆ’4)
2001
0.138
(4.41 Ã—10âˆ’4)
2002
0.132
(4.46 Ã—10âˆ’4)
2003
0.134
(4.40 Ã—10âˆ’4)

Sequential Multilocation Auditing
675
administrative approach despite believing that their systems were not prone to
error. Subsequently, a number of counties improved and the statewide error
rates for 2003 through 2006 were 0.0588, 0.0574, 0.0723, and 0.0456, respec-
tively, causing the federal government to award the State several million dollars
as part of an incentive programme to promote cost savings. The incentive was
then distributed among the counties based in part on the rates resulting from
the sequential multicentre model.
During this period, at a meeting of county ofï¬cials, a state ofï¬cial was asked
by a county ofï¬cial to explain how his county could have a published error
rate when hardly any of the statewide sample came from his county. The state
ofï¬cial could not explain and announced that the practice would be abolished.
After a short moratorium, at the request of some state-level programme
managers and some county ofï¬cials, modelling county level error rates was
reinstated, although the rates from 2007 have been used internally but have
not been published.
23.6.2 Wider applicability
In our applied experience we have encountered several audit situations
where the organization being audited is distributed over several locations.
Furthermore, a number of these multilocation audits occur periodically. The
example that we cite, food stamp quality control sampling across counties
within a state, is not atypical. Many governmental programmes are audited in
a similar fashion.
In the United States, health care is frequently provided by large Health
Maintenance Organizations (HMOs) that consist of a health care insurance
mechanism and several health care providers, e.g. hospitals, physicians, etc.
In fact, individual hospitals are composed of several units, each responsible for
their own record keeping. At universities and in research organizations, each
principal investigator manages a budget, but there is collective responsibility
and therefore audit requirements that dictate sampling over several locations.
Of course, large corporations almost invariably have multiple locations and
accounts.
When these multilocation organizations are sampled, the sample size in
many locations is small and sometimes zero. The hierarchical nature of the
Bayesian approach provides useful shrinkage allowing information to ï¬‚ow from
the whole organization to the subunits or locations and from period to period.
This allows statements to be made about each location and precision to be
reported.
The question of the importance of the information from previous periods
and from the whole organization to its parts requires careful consideration
and sensitivity analysis is highly recommended. Research into this question
is needed.

676
The Oxford Handbook of Applied Bayesian Analysis
The sequential multilocation auditing model allows for prior information
from expert opinion. Elicitation in this setting needs study.
We have found the Bayesian sequential multilocation model appropriate in
a number of audits. Results have been useful and accepted by managers who
have successfully used the results in managing their programmes.
Appendix
A. Broader context and background
A.1 Prior information
In any Bayesian analysis it is interesting to see the extent to which the prior dis-
tribution is based on genuine prior information. This chapter extends an earlier
approach to multilocation auditing, Laws and Oâ€™Hagan (2002), in which there
was substantial emphasis on real prior information. In that approach, auditors
were required to formulate quite complex prior information. We argued that
they generally have such information, based in part on the fact that auditors
typically audit the same organizations on an annual basis. The present work
explores the basis for using previous audit results by explicitly modelling the
evolution of error rates over time, rather than requiring the auditor to accom-
modate earlier results implicitly in his/her prior speciï¬cation.
The auditor is still required to make subjective judgements, but now in a
much simpler way. He/she is required only to specify a few evolution parame-
ters to describe, for instance, how rapidly the error rate in a given location may
change, and how consistent any trend will be. It is possible, however, to reintro-
duce some of the subjective judgement of the earlier model, because in practice
the auditorâ€™s information prior to each audit comprises more than the results of
previous audits. This includes some examination of the ï¬nancial systems and
controls that are in place, knowledge of any changes in key personnel, and so
on. It will be possible for the auditor to use the prior distribution coming from
our new dynamic model, and to modify it to reï¬‚ect any other factors that he/she
feels represent relevant prior information.
Similarly, in the context of Bayesian forecasting, West and Harrison (1997)
have stressed the possibility of intervening to modify the prior distribution
at any time point. In any Bayesian analysis, there should be scope for using
genuine prior information.
A.2 Dynamic models
The modelling and analysis here also has similarities to Bayesian forecasting
using dynamic linear models. Such models have an observation equation to
relate observations to the underlying system state, plus a system equation that
expresses how that state evolves over time. We have a set of parameters Ã‹t deï¬n-
ing the system state at time t, and the current distribution of these parameters

Sequential Multilocation Auditing
677
given observations up to time t âˆ’1 is p(Ã‹t | Dtâˆ’1). Using the observation
equation, we update this distribution by Bayesâ€™ theorem to p(Ã‹t | Dt), where the
information Dt comprises the prior information Dtâˆ’1 plus the new observation
at time t. We then project using the system equation to p(Ã‹t+1 | Dt), which then
becomes the prior distribution for the next time step. In the usual Gaussian
form of this model, all of these distributions are (multivariate) normal. The
updating and projection steps then comprise the well-known Kalman ï¬lter (see
Chapter 1 of West and Harrison 1997).
If we make the underlying distributions non-Gaussian, or the equations
nonlinear in the state parameters, then it is no longer possible to retain
the simplicity of the Kalman ï¬lter without making approximations. Exact
analysis would result in increasingly complex distributions p(Ã‹t|Dt) and
p(Ã‹t+1|Dt) after each updating and projection step, and the computations
rapidly become unmanageable. One approach to this problem is to use
particle ï¬ltering (see Doucet and Gordon 2001, and http://www-sigproc.
eng.cam.ac.uk/smc/papers.html), while another is to employ approxima-
tions at each step (Chapters 13 and 14 of West and Harrison 1997).
In our dynamic auditing model, we employ the approximation approach. Our
equivalent of the state vector Ã‹t is made up of Ã‹t
0, Ã™t
0, Ã‹t
i for each location i, Â¯t
0 and
Â¯t
i for each location. After Bayesian updating with the results of the audit at time
t, the posterior distribution of these parameters is complex and no longer of
the same form as the prior. Hence we apply a simpliï¬ed approximation during
the projection step to reinstate a consistent form of prior distribution for the
parameters at the next time period. Thus Ã‹t+1
0
and Ã‹t+1
i
have beta distributions,
Â¯t+1
0
and Â¯t+1
i
have Dirichlet distributions, and Ã™t
0 has a normal distribution,
given data Dt.
Acknowledgements
The work presented here was begun with the collaboration of David Laws, and
owes much to his contributions. Sadly, David was already suffering from Motor
Neurone Disease, a dreadful progressive condition for which there is no cure.
David died during the preparation of this chapter, in 2008.
The authors are also grateful to Arilee Bagley and Shannon Smosarski for
their help with the preparation of this chapter.
References
Cochran, W. G. (1977). Sampling Techniques. John Wiley, New York.
Cohen, A. and Sackrowitz, H. B. (1996). Lower conï¬dence bounds using pilot samples with an
application to auditing. Journal of the American Statistical Association, 91, 338â€“342.
Doucet, A. de Freitas, N. and Gordon, N. (2001). Sequential Monte Carlo Methods in Practice.
Springer Verlag, New York.

678
The Oxford Handbook of Applied Bayesian Analysis
Frost, P. A. and Tamura, H. (1982). Jackknifed ratio estimation in statistical auditing. Journal of
Accounting Research, 20, 103â€“120.
Heiner, K. and Laws, D. (1999). Simulation based inference for auditing. In Computing Science
and Statistics. Models, Predictions, and Computing. (ed. K. Berk and M. Pourahmadi), pp. 64â€“
68. Proceedings of the 31st Symposium on the Interface, Interface Foundation of North
America.
Heiner, K. W. (1999). Issues in statistical auditing and some Bayesian solutions. In ASA Pro-
ceedings of the Business and Economic Statistics Section, American Statistical Association,
pp. 18â€“24.
Heiner, K. W., Wagner, N. L. and Fried, A. C. (1984). Successfully using statistics to determine
damages in ï¬scal audits. Jurimetrics, 24, 273â€“281.
Heiner, K. W. and Whitby, O. (1980). Maximizing restitution for erroneous medical payments
when auditing samples. Interfaces (INFORMS), 10, 46â€“53.
Kvanli, A. H., Shen, Y. K. and Deng, L. Y. (1998). Construction of conï¬dence intervals for
the mean of a population containing many zero values. Journal of Business and Economic
Statistics, 16, 362â€“368.
Laws, D. J. and Oâ€™Hagan, A. (2000). Bayesian inference for rare errors in populations with
unequal unit sizes. Journal of the Royal Statistical Society, Series C: Applied Statistics, 49,
577â€“590.
Laws, D. J. and Oâ€™Hagan, A. (2002). A hierarchical Bayes model for multilocation auditing.
Journal of the Royal Statistical Society, Series D: The Statistician, 51, 431â€“450.
Moors, J. J. A. and Janssens, M. J. B. T. (1989). Exact distributions of Bayesian Coxâ€“Snell bounds
in auditing. Journal of Accounting Research, 27, 135â€“144.
Neter, J. and Godfrey, J. (1985). Robust Bayesian bounds for monetary-unit sampling in audit-
ing. Applied Statistics, 34, 157â€“168.
Neter, J. and Loebbecke, J. K. (1977). On the behavior of statistical estimators when sampling
accounting populations. Journal of the American Statistical Association, 72, 501â€“507.
Smith, T. M. F. (1979). Statistical sampling in auditing: A statisticianâ€™s viewpoint. The Statisti-
cian, 28, 267â€“280.
The Panel on Nonstandard Mixtures of Distributions (1989). Statistical models and analysis in
auditing. Statistical Science, 4, 2â€“33.
Tsui, K.-W., Matsumura, E. M. and Tsui, K.-L. (1985). Multinomial-Dirichlet bounds for dollar-
unit sampling in auditing. Accounting Review, 60, 76â€“96.
van Batenburg, P. C., Oâ€™Hagan, A. and Veenstra, R. H. (1994). Bayesian discovery sampling
in ï¬nancial auditing: A hierarchical prior model for substantive test sample sizes. The
Statistician, 43, 99â€“110.
West, M. and Harrison, P. (1997). Bayesian Forecasting and Dynamic Models, (2nd edn),
Springer-Verlag, New York.

Â·24Â·
Bayesian causal inference: Approaches to
estimating the effect of treating hospital type
on cancer survival in Sweden using
principal stratiï¬cation
Donald B. Rubin, Xiaoqin Wang, Li Yin, and Elizabeth R. Zell1
24.1 Introduction
This chapter concerns Bayesian causal inference, in particular, using the pos-
terior predictive approach of the â€˜Rubin causal modelâ€™, originally outlined in
Rubin (1975) and developed in numerous articles by him and coauthors, as
referenced in this chapter. Here we apply this approach to a difï¬cult and
important problem in medicine, deciding which type of hospital, large volume
versus small volume, is superior for treating certain serious conditions, where
volume refers to the number of patients with that condition treated there. The
data we use came from central and northern Sweden, which arose in a study at
the Karolinska Institute in Stockholm. More precisely, the situation we have is
the following.
If one has a serious medical condition, which is diagnosed in a small volume
hospital, is it be better to be transferred to a major medical centre for treatment,
where the medical staff are relatively more experienced with the condition and
possibly better equipped to handle it, or is it better to be treated in the local
hospital where personal attention is more likely, including from family and
friends? A version of this question also confronts governmental agencies when
making budgetary decisions concerning maintaining smaller volume treatment
facilities in local hospitals versus just maintaining treatment facilities in the
larger volume hospitals, and transferring all patients who are diagnosed at small
volume hospitals to larger volume hospitals for treatment. If data suggest that
patient outcomes are better when treated at large-volume hospitals, then there
is no reason to maintain treatment facilities at small volume hospitals, thereby
saving resources.
1 The authorsâ€™ names are in alphabetic order.
The ï¬ndings and conclusions in this chapter are those of the authors and do not necessarily represent
the views of their respective institutions.

680
The Oxford Handbook of Applied Bayesian Analysis
Ideally, one would have data from randomized experiments, where patients
with speciï¬c conditions are randomized to be treated in large versus small vol-
ume hospitals, but such data are not available, and so to address this question,
observational data must be relied upon. In some cases, such as ours, medical
judgment may be that the assignment of the diagnosing (home) hospital type
(large versus small) may reasonably be considered ignorable, at least with a rich
enough set of covariates available. Thus, the estimation of the causal effect of
large versus small volume hospital type is standard. However, the estimation of
the causal effects of treating hospital type (large versus small) is complicated by
the existence of transfers between hospital types for treatment, typically from
small home hospital types to large treating hospital types, which implies that
the assignment of treating hospital type is nonignorable because the reasons for
the transfers undoubtedly involved unmeasured variables, such as the patientsâ€™
willingness to undergo invasive medical operations.
We address this problem using principal stratiï¬cation (Frangakis and Rubin,
2002), where we view the transfers between hospital types as a form of non-
compliance with assigned type of hospital. We apply our formulation to the data
from Sweden on cardia and stomach cancer. This formulation leads to a Baye-
sain version of the classical â€˜instrumental variablesâ€™ method from economics
(Angrist, Imbens and Rubin, 1996), as initially described in Imbens and Rubin
(1997). Critical assumptions are discussed at length: the monotonicity assump-
tion and exclusion restriction. The Bayesian analyses suggest an interesting and
intuitively reasonable conclusion: the positive effect of large treating hospital
type on cardia cancer, which is a rare and difï¬cult to treat condition, but no such
effect for the more common stomach cancer. Our analytic approach should have
many applications for addressing similar questions in other contexts.
The remainder of the chapter has two main parts. The ï¬rst part summarizes
the general Bayesian approach to causal inference, primarily with ignorable
treatment assignment. The second part describes our application in more detail,
motivates our approach using simple method-of-moments summary statistics,
explicates our Bayesian model, and analyzes our data.
24.2 Bayesian causal inference â€“ General framework
The framework for Bayesian causal inference that we use is now commonly
referred to as â€˜Rubinâ€™s causal modelâ€™ (RCM, Holland, 1986), for a series of
articles written in the 1970s (Rubin, 1974â€“1980). The full RCM framework
has three parts. The ï¬rst part deï¬nes causal effects of treatments on units
through potential outcomes and is developed in Section 24.2.1. The second
part of this framework concerns the assignment mechanism, as described in
Section 24.2.2. The third part of the RCM is the use of Bayesian posterior

Bayesian Causal Inference
681
predictive inference for causal effects as initially developed in Rubin (1975,
1978), and this is presented in Section 24.2.3. These descriptions in Section 24.2
borrow in some places from a related chapter on general causal inference in
Rubin (2008).
24.2.1 Units, treatments, potential outcomes, and SUTVA
For causal inference, there are several primitives â€“ concepts, that are basic and
on which we must build. A â€˜unitâ€™ is a physical object, e.g. a person, at a particular
point in time. A â€˜treatmentâ€™ is an action that can be applied or withheld from
that unit. We focus on the case of two treatments, although the extension
to more than two treatments is simple in principle although not necessarily
so with real data. Associated with each unit are two â€˜potential outcomesâ€™: the
value of an outcome variable Y at a point in time after the active treatment
is applied and the value of that outcome variable at the same point in time
when the active treatment is withheld. The objective is to learn about the causal
effect of the application of the active treatment relative to the control treatment
(the control treatment implies that the active treatment is withheld) on the
variable Y.
For example, the unit could be â€˜you nowâ€™ with your headache, the active
treatment could be taking aspirin for your headache, and the control treatment
could be not taking aspirin (as in Rubin, 1974). The outcome Y could be the
intensity of your headache pain in two hours, with the potential outcomes being
the headache intensity if you take aspirin now and if you do not take aspirin
now. â€˜You nowâ€™ is a different unit from â€˜you laterâ€™.
Notationally, let W indicate which treatment the unit, you, received: W = 1
for the active treatment, W = 0 for the control treatment. Also let Y(1) be the
value of the potential outcome if the unit received the active version, and Y(0)
the value if the unit received the control version. The causal effect of the active
treatment relative to its control version is the comparison of Y(1) and Y(0) â€“
typically the difference, Y(1) minus Y(0), or perhaps the difference in logs,
log[Y(1)] minus log[Y(0)], or some other comparison, possibly the ratio.
We can observe the value of Y(W) as indicated by W. The fundamental
problem facing causal inference (Rubin, 1978: p. 38, Holland, 1986) is that, for
any individual unit, we observe only the value of the potential outcome under
one of the possible treatments, namely the treatment actually assigned, and
the potential outcome under the other treatment is missing. Thus, inference
for causal effects is a missing-data problem â€“ the â€˜otherâ€™ value is missing.
For example, your reduction in blood pressure one week after taking a drug
is a change in time, in particular, a change from before taking the drug to
after taking the drug on you, and so is not a causal effect without additional
assumptions. The comparison of your blood pressure after taking the drug with

682
The Oxford Handbook of Applied Bayesian Analysis
what it would have been at the same point in time without taking the drug is a
causal effect.
We learn about causal effects using replication, more units. The way we
personally learn from our own experience is replication involving the same
physical object (e.g. you) at more points in time, thereby generating more
experimental units (e.g. you at the various points in time). That is, if I want
to learn about the effect of taking aspirin on headaches for me, I learn from
replications in time when I do and do not take aspirin to relieve my headache,
thereby having some observations of Y(1) and some of Y(0). When we want to
generalize to units other than ourselves, we typically use more physical objects;
that is what is done in epidemiology and medical experiments.
Suppose instead of only one unit we have two, and use subscripts 1 and 2
on Y to denote their potential outcomes. Now in general we have at least four
potential outcomes for each unit: the outcome for unit 1 if both unit 1 and
unit 2 received control, Y1(0, 0); the outcome for unit 1 if both units received
the active treatment, Y1(1, 1); the outcome for unit 1 if unit 1 received control
and unit 2 active, Y1(0, 1), and the outcome for unit 1 if unit 1 received active
and unit 2 received control, Y1(1, 0); and analogously for unit 2 with potential
outcomes Y2(1, 1), Y2(0, 0), Y2(1, 0), and Y2(0, 1). In fact, generally there are
even more potential outcomes because there have to be at least two â€˜dosesâ€™ of
the active treatment available to contemplate all assignments, and it could make
a difference which doses was taken. For example, for the aspirin case, one tablet
could be very effective and the other quite ineffective.
Clearly, replication does not help unless we can restrict the explosion of
potential outcomes. As in all theoretical work with applied value, simplifying
assumptions are crucial. The most straightforward assumption to make is the
â€˜stable unit treatment value assumptionâ€™ (SUTVA, Rubin, 1980; 1990a, 1990b)
under which the potential outcomes for the ith unit just depend on the treat-
ment the ith unit received. That is, there is â€˜no interference between unitsâ€™
(Cox, 1958) and there are â€˜no unrepresented treatments for any unitâ€™. Then, all
potential outcomes for N units with two possible treatments can be represented
by an array with N rows and two columns, the ith unit having a row with two
potential outcomes, Yi(0) and Yi(1).
Obviously, SUTVA is a major assumption. But there is no assumption-
free causal inference, and nothing is wrong with this. It is the quality of the
assumptions that matters, not their existence or even their absolute correctness.
Good researchers attempt to make such assumptions plausible by the design of
their studies. For example, SUTVA becomes more plausible when units are
isolated from each other. For example, when studying an intervention such as a
smoking prevention programme in schools (e.g. see Peterson et al., 2000), the
units were deï¬ned to be intact schools that were geographically separated rather
than individual students or classes in the schools.

Bayesian Causal Inference
683
The stability assumption (SUTVA) is very commonly made, even though it is
not always appropriate. For example, consider a study of the effect of vaccination
on a contagious disease. The greater the proportion of the population that gets
vaccinated, the less any unitâ€™s chance of contracting the disease, even if the
unit is not vaccinated â€“ an example of interference. Throughout this chapter,
we assume SUTVA, although there are other assumptions that could be made
to restrict the exploding number of potential outcomes with replication and no
assumptions.
In general, some of the N units may receive neither the active treatment
Wi = 1 nor the control treatment Wi = 0. For example, some of the units may
be in the future, as when we want to generalize to a future population. Then
formally Wi must take on a third value, Wi = âˆ—representing neither 1 nor 0; we
often avoid this extra notation in this chapter.
In addition to (a) the vector indicator of the treatment for each unit in the
study, W, (b) the array of potential outcomes when exposed to the treatment,
Y(1) = {Yi(1)}, and (c) the array of potential outcomes when exposed to the
control, Y(0) = {Yi(0)}, we have (d) an array of covariates X = {Xi}, which are,
by deï¬nition, unaffected by treatment exposure, such as age or pretreatment
baseline blood pressure. All causal estimands involve comparisons of Yi(0) and
Yi(1), on either all N units, or a common subset of units; for example, the
average causal effect across all units that are female as indicated by their Xi,
or the median causal effect for units with Yi(0) values indicating unacceptably
high blood pressure after exposure to the control treatment.
Thus, under SUTVA, all causal estimands can be calculated from the matrix
of â€˜scientiï¬c valuesâ€™ with ith row: (Xi, Yi(0), Yi(1)). By deï¬nition, all relevant
information is encoded in Xi, Yi(0), Yi(1) and so the labelling of the N rows is
a random permutation of 1, . . . , N. In other words, the N-row array
(X, Y(0), Y(1)) =
âŽ¡
âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢âŽ£
X1 Y1(0) Y1(1)
...
Xi Yi(0) Yi(1)
...
X N YN(0) YN(1)
âŽ¤
âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥âŽ¦
is row exchangeable. We call this array â€˜the scienceâ€™ because its values are
beyond our control; by changing treatments, we get to change which values are
actually observed, but not the values themselves. That is, the observed values of
Y are Yobs = {Yobs,i}, where Yobs,i = (1 âˆ’Wi)Yi(0) + WiYi(1). In Section 24.2, we
consider X fully observed. However, the model we use in Section 24.3 has some
unobserved covariates that deï¬ne â€˜principal strataâ€™, as deï¬ned in Frangakis and
Rubin (2002).

684
The Oxford Handbook of Applied Bayesian Analysis
Covariates (such as age, race and sex) play a particularly important role in
observational studies for causal effects. In some studies, the units exposed
to the active treatment differ on their distribution of covariates in important
ways from the units not exposed. To see how this issue inï¬‚uences our for-
mal framework, we must deï¬ne the â€˜assignment mechanismâ€™, the probabilistic
mechanism that determines which units get the active version of the treatment
and which units get the control version. The assignment mechanism is the topic
of Section 24.2.2.
24.2.2 Treatment assignment mechanism
A model for the assignment mechanism is needed for all forms of statistical
inference for causal effects. The assignment mechanism gives the conditional
probability of each vector of assignments for all the units given the covariates
and potential outcomes:
Pr(W |X, Y(0), Y(1)).
(24.1)
Here W is an N Ã— 1 vector and X, Y(1) and Y(0) are all matrices with N rows.
A speciï¬c example of an assignment mechanism is a completely randomized
experiment with N units, where n < N are assigned to the active treatment, and
N âˆ’n are assigned to the control treatment:
Pr(W |X, Y(0), Y(1)) =

1/C N
n
if  Wi = n
0
otherwise.
(24.2)
The assignment mechanism is fundamental to causal inference because it tells
us how we got to see what we saw. As stated earlier, causal inference is basically
a missing data problem with at least half of the potential outcomes missing.
When we have no understanding of the process that creates missing data, we
have no hope of inferring anything about the missing values. That is, without a
stochastic model for how treatments are assigned to individuals, formal causal
inference, at least using probabilistic statements, is impossible. This statement
does not mean that we need to know the assignment mechanism, but rather
that without positing one, we cannot make any statistical claims about causal
effects, such as the coverage of Bayesian posterior intervals, deï¬ned later.
A special class of assignment mechanisms is particularly important to
Bayesian inference: ignorable assignment mechanisms (Rubin, 1978). Ignor-
able assignment mechanisms are deï¬ned by their freedom from dependence
on any missing potential outcomes:
Pr(W |X, Y(0), Y(1)) = Pr(W |X, Yobs).
(24.3)
In our application in Section 24.3, the assignment mechanism is â€˜latentlyâ€™
ignorable (Frangakis and Rubin, 1999) in the sense that we posit an assignment

Bayesian Causal Inference
685
mechanism such that one of the â€˜covariatesâ€™ needed to make the assignment
mechanism ignorable is partially missing, meaning if it were fully observed,
the assignment mechanism would be ignorable.
24.2.3 Posterior predictive causal inference
Bayesian inference for causal effects requires a model for the underlying sci-
ence, Pr(X, Y(0), Y(1)). A virtue of the RCM framework is that it separates
science â€“ a model for the underlying science â€“ from what we do to learn
about the science â€“ the assignment mechanism, Pr(W |X, Y(0), Y(1)). Notice
that together, these two models specify a joint distribution for all observables,
the approach commonly called Bayesian.
Bayesian inference for causal effects directly and explicitly confronts
the missing potential outcomes, Ymis = {Ymis,i}, where Ymis,i = WiYi(0) + (1 âˆ’
Wi)Yi(1). The perspective takes the speciï¬cation for the assignment mechanism
and the speciï¬cation for the underlying science and derives the â€˜posterior
predictiveâ€™ distribution of Ymis, that is, the distribution of Ymis given all observed
values:
Pr(Ymis|X, Yobs, W ).
(24.4)
This distribution is â€˜posteriorâ€™ because it is conditional on all observed values
(X, Yobs, W) and â€˜predictiveâ€™ because it predicts (stochastically) the missing
potential outcomes. From (a) this distribution, (b) the observed values of the
potential outcomes, Yobs, (c) the observed assignments, W, and (d) the observed
covariates, X, the posterior distribution of any causal effect can, in principle, be
calculated.
This conclusion is immediate if we view the posterior predictive distribution
in equation (24.4) as specifying how to take a random draw of Ymis, because
once a value of Ymis is drawn using equation (24.4), any causal effect can
be directly calculated from the drawn value of Ymis and the observed values
of X and Yobs. For example, the median causal effect for males: med{Yi(1) âˆ’
Yi(0)|Xiindicate males}. Repeatedly drawing values of Ymis and calculating the
causal estimand for each draw generates the posterior distribution of the
causal estimand. Thus, we can view causal inference entirely as a missing
data problem (Little and Rubin, 1987, 2002), where we multiply-impute (Rubin,
1987, 2004) the missing potential outcomes to generate a posterior distribu-
tion for the causal effects, and thereby point and interval estimates for these
effects.
We now describe how to generate these imputations, that is the posterior
predictive distribution of Ymis under ignorable treatment assignment. Under
non-ignorable assignment mechanisms, the situation is more complicated, and

686
The Oxford Handbook of Applied Bayesian Analysis
although the general case is discussed in Rubin (1978), real progress depends
on the particular situation, for example, the case of Section 24.3.
In general:
Pr(Ymis|X, Yobs, W) =
Pr(X, Y(0), Y(1)) Pr(W |X, Y(0), Y(1))

Pr(X, Y(0), Y(1)) Pr(W |X, Y(0), Y(1))dYmis
.
(24.5)
With ignorable treatment assignment, equation (24.5) becomes:
Pr(Ymis|X, Yobs, W) =
Pr(X, Y(0), Y(1))

Pr(X, Y(0), Y(1))dYmis
.
(24.6)
Equation (24.6) reveals that, under ignorability, all that we need to model is
the science, Pr(X, Y(0), Y(1)). With latent ignorability, as in our application,
we assume that the partially observed X is actually known, and then multiply-
impute the missing values in this partially observed variable, and then carry
out the remaining analysis assuming ignorable treatment assignment for each
such imputation. Details are given in the context of our application, where a
model also needs to be speciï¬ed for the partially missing X given the fully
observed Xâ€™s.
Because all information is in the underlying data, the unit labels are
effectively just random numbers, and hence the array (X, Y(0), Y(1)) is row
exchangeable. With essentially no loss of generality as in Rubin (1978),
therefore, by de Finettiâ€™s (1963) theorem, we have that the distribution of
(X, Y(0), Y(1)) may be taken to be iid (independent and identically distributed)
given some parameter Ã‹, with prior distribution p(Ã‹):
Pr(X, Y(0), Y(1)) =
 6 N

i=1
f (Xi, Yi(0), Yi(1)|Ã‹))
7
p(Ã‹)dÃ‹.
(24.7)
Equation (24.7) provides the bridge between fundamental theory and the com-
mon practice of using iid models. Of course, there remains the critical point
that the distributions f (Â·|Ã‹) and p(Ã‹) are rarely, if ever known, and this limitation
haunts Bayesian inference despite its great ï¬‚exibility.
Without loss of generality, we can factor f (Xi, Yi(0), Yi(1)|Ã‹) into:
f (Yi(0), Yi(1)|Xi, Ã‹yÂ·x) f (Xi|Ã‹x),
where Ã‹yÂ·x = Ã‹yÂ·x(Ã‹) is the parameter governing the conditional distribution of
Yi(0), Yi(1) given Xi, and analogously, Ã‹x = Ã‹x(Ã‹) is the parameter governing
the marginal distribution of X, where both parameters are functions of Ã‹.
The reason for doing this factorization is that we are assuming X is fully
observed, and so we wish to predict the missing potential outcomes Ymis
from X and the observed potential outcomes, Yobs, and therefore must use
f (Yi(0), Yi(1)|Xi, Ã‹yÂ·x) as the basic distribution.

Bayesian Causal Inference
687
To do this, we factor f (Yi(0), Yi(1)|Xi, Ã‹yÂ·x) into either
f (Yi(0)|Xi, Yi(1), Ã‹0Â·x1) f (Yi(1)|Xi, Ã‹1Â·x)
when Yi(1) is observed, indexed by the set S1 = {i|Wi = 1}, or
f (Yi(1)|Xi, Yi(0), Ã‹1Â·x0) f (Yi(0)|Xi, Ã‹0Â·x)
when Yi(0) is observed, indexed by the set S0 = {i|Wi = 0}, where the various
subscripted Ã‹â€™s are all functions of Ã‹ governing the appropriate distributions in
an obvious notation.
These factorizations allow us to write (24.7) as
 
iâˆˆS1
f (Yi(0)|Xi, Yi(1), Ã‹0Â·x1)

iâˆˆS1
f (Yi(1)|Xi, Ã‹1Â·x)
(24.8a)
Ã—

iâˆˆS0
f (Yi(1)|Xi, Yi(0), Ã‹1Â·x0)

iâˆˆS0
f (Yi(0)|Xi, Ã‹0Â·x)
(24.8b)
Ã—

iâˆˆSâˆ—
f (Yi(0), Yi(1)|Xi, Ã‹yÂ·x)
(24.8c)
Ã—

iâˆˆS
f (Xi|Ã‹x)p(Ã‹)dÃ‹,
(24.8d)
where Sâˆ—= {i|Wi =âˆ—}.
Notice that the ï¬rst factor in (24.8a), times the ï¬rst factor in (24.8b), times
(24.8c) is proportional to the posterior predictive distribution of Ymis given Ã‹ (i.e.
given X, Yobs and Ã‹), Pr(Ymis|X, Yobs, Ã‹). Also notice that the remaining factors in
(24.8), that is the second factor in (24.8a) times the second factor in (24.8b) times
(24.8d) is proportional to the posterior distribution of Ã‹, Pr(Ã‹|X, Yobs), which is
equal to the likelihood of Ã‹, L(Ã‹|X, Yobs), times the prior distribution of Ã‹, p(Ã‹).
24.2.4 Assumptions on the science
Let us now assume that Ã‹yÂ·x and Ã‹x are a priori independent:
p(Ã‹) = p(Ã‹yÂ·x)p(Ã‹x).
(24.9)
This assumption is not innocuous, but it is useful and is standard in many
prediction environments. For an example of a situation where it might not be
reasonable, suppose X includes many base-line measurements of cholesterol
going back many years; the relationships among previous X values may provide
useful information for predicting Y(0), (i.e. Y without intervention), from X,
using for example, a time-series model (e.g. Box and Jenkins, 1970).
For simplicity, here we make the assumption implied by equation (24.9),
although, as with all such assumptions, it should be carefully considered. Then
the integral over Ã‹x in (24.8) passes through all the products in (24.8a), (24.8b)

688
The Oxford Handbook of Applied Bayesian Analysis
and (24.8c), and we are left with the integral over (24.8d); (24.8d) after this
integration is proportional to p(Ã‹yÂ·x)dÃ‹yÂ·x.
As a consequence, (24.8) becomes

Pr(Ymis|X, Yobs, Ã‹yÂ·x) Pr(Ã‹yÂ·x|X, Yobs)dÃ‹yÂ·x,
(24.10)
where the second factor in (24.10) is proportional to the product of the second
factors in (24.8a) and (24.8b), and the ï¬rst factor in (24.10) is, as before, propor-
tional to the product of the ï¬rst factors of (24.8a) and (24.8b) times (24.8c).
We now assume that entirely separate activities are to be used to impute
the missing Yi(0) and the missing Yi(1). This is accomplished with two formal
assumptions:
f (Yi(0), Yi(1)|Xi, Ã‹yÂ·x) = f (Yi(0)|Xi, Ã‹0Â·x) f (Yi(1)|Xi, Ã‹1Â·x)
(24.11)
and
p(Ã‹yÂ·x) = p(Ã‹0Â·x)p(Ã‹1Â·x).
(24.12)
Thus, in (24.11) we assume Yi(0) and Yi(1) are conditionally independent given
Xi and Ã‹yÂ·x, and in (24.12), that the parameters governing these conditional
distributions are a priori independent. We make this assumption in our appli-
cation in Section 24.3. Consequently, f (Yi(0)|Xi, Yi(1), Ã‹0Â·x1) = f (Yi(0)|Xi, Ã‹0Â·x),
and f (Yi(1)|Xi, Yi(0), Ã‹1Â·x0) = f (Yi(1)|Xi, Ã‹1Â·x).
Thus, equations (24.8) or (24.10) can be described in four distinct parts with
associated activities as follows.
1. Using the control units, obtain the posterior distribution of Ã‹0Â·x, the para-
meters governing the distribution of the control potential outcomes given
the covariates:
Pr(Ã‹0Â·x|X, Yobs) âˆL(Ã‹0Â·x|X, Yobs)p(Ã‹0Â·x) âˆ

iâˆˆS0
f (Yi(0)|Xi, Ã‹0Â·x)p(Ã‹0Â·x).
2. Using Ã‹0Â·x, obtain the conditional posterior predictive distribution of the
missing control potential outcomes, Yi(0):

iâˆˆS1âˆªSâˆ—
f (Yi(0)|Xi, Ã‹0Â·x).
3. Using the treated units, obtain the posterior distribution of Ã‹1Â·x, the para-
meters governing the distribution of the treatment potential outcomes
given the covariates.
Pr(Ã‹1Â·x|X, Yobs) âˆL(Ã‹1Â·x|X, Yobs)p(Ã‹1Â·x) âˆ

iâˆˆS1
f (Yi(1)|Xi, Ã‹1Â·x)p(Ã‹1Â·x).

Bayesian Causal Inference
689
4. Using Ã‹1Â·x, obtain the conditional posterior predictive distribution of the
missing treatment potential outcomes, Yi(1):

iâˆˆS0âˆªSâˆ—
f (Yi(1)|Xi, Ã‹1Â·x).
For simulation, perform steps 1â€“4 repeatedly with random draws, thereby
multiply imputing Ymis. As stated earlier, with latently ignorable treatment
assignment, a model relating the partially observed X to the fully observed Xâ€™s
must be speciï¬ed, and then the partially observed covariates missing values can
be multiply imputed. This process is described in Section 24.3 that follows.
24.3 Bayesian inference for the causal effect of large versus
small treating hospitals
The task of many researchers in the medical and social sciences is to esti-
mate the causal effects of treatments (e.g. medical interventions, social poli-
cies) on outcomes (e.g. cancer survival, unemployment). This task is often
attempted from observational studies where the treatment assignment can be
non-ignorable because of unmeasured background covariates, which makes the
estimation of causal effects using simple methods of data analysis inappropri-
ate. Here we illustrate how principal stratiï¬cation (Frangakis and Rubin, 2002)
and Bayesian causal inference can be used to address such a problem in a
study of the effect of hospital type, deï¬ned by cancer treatment volume â€“ large
versus small â€“ on the survival of cardia cancer and stomach cancer patients in
central and northern Sweden, an example where the assumptions underlying
conventional methods are not medically plausible, but where the assumptions
underlying our approach are quite plausible. The most critical issue concerns
whether the treatment centres at small volume hospitals can be closed without
having a deleterious effect on patient survival. If so, resources could be saved
because patients diagnosed at small volume hospitals could be transferred to
large treating hospitals.
24.3.1 Assignment of home and treating hospital type
According to investigators at the Karolinska Institute, the assignment of â€˜home
hospital typeâ€™, where patients were ï¬rst diagnosed with their medical condi-
tion, can reasonably be considered ignorable, at least after conditioning on
known demographic covariates describing patients, such as age, male/female
and urban/rural. Assuming this ignorability allows us to estimate the causal
effect of the home hospital type using conventional methods, but this approxi-
mates the effect of treating hospital type only when transfers between hospital
types are rare. However, in this data set, transfers between hospital types for

690
The Oxford Handbook of Applied Bayesian Analysis
treatment are frequent, and although the transfers are well documented, the
reasons for the transfers often appear to involve factors that are unmeasured,
such as patientsâ€™ willingness to tolerate surgical interventions and to be treated
in locations distant from friends and family. Consequently, it would be inappro-
priate to estimate the causal effect of treating hospital type assuming ignora-
bility of the assignment of treating hospital type given home hospital type and
observed covariates.
Fortunately, however, our situation with transfers between hospital types has
strong analogies to randomized trials with non-compliance. The assignment to
home hospital type is considered ignorable, and the transfers to a different treat-
ing hospital type can be thought of as non-compliance with the assigned home
hospital type. For such situations, Angrist, Imbens and Rubin (1996) showed
that the econometric â€˜instrumental variablesâ€™ method can be interpreted, under
certain assumptions, as estimating the causal effect for the â€˜trueâ€™ compliers,
who are deï¬ned to be those who would comply with their assigned treatment
no matter what their assignment, and Imbens and Rubin (1997) have addressed
such non-compliance using fully Bayesian methods from the RCM perspective,
which provides better estimation than the conventional instrumental variables
approach. Little and Yau (1998) and Hirano et al. (2000) have extended the
approach to allow for the presence of covariates. More recently, Frangakis and
Rubin (2002) extended the idea behind instrumental variables to a general
framework for a class of causal inference problems where treatment compar-
isons are validly adjusted for post-treatment variables.
In Section 24.3.2 we introduce our data sets and their medical importance,
and present some descriptive statistics. Section 24.3.3 discusses home (= diag-
nosing) hospital type and treating hospital type, and the conditions for valid
causal inference for each type, as well as deï¬nes â€˜principal stratiï¬cationâ€™ and
its application to our problem. Section 24.3.4 introduces the â€˜monotonicityâ€™
assumption and how to obtain rough estimates of the sizes of principal strata
in our problem under it. Section 24.3.5 discusses general problems with two
commonly used estimators, the â€˜as-treatedâ€™ and â€˜per-protocolâ€™ estimators. Sec-
tion 24.3.6 introduces the â€˜exclusion restrictionâ€™ and shows how it leads to the
â€˜instrumental variablesâ€™ estimate of the â€˜complier average causal effectâ€™ of treat-
ing hospital on survival. The speciï¬c Bayesian models that we use, including
their prior distributions, is the topic of Section 24.3.7, and Section 24.3.8 dis-
cusses the MCMC computation. Section 24.3.9 presents the resulting posterior
distributions and discusses practical implications.
24.3.2 Data sets, notation and basic results
Our two data sets involve, ï¬rst, 150 cardia cancer patients, and second, 853
stomach cancer patients between ages 35 and 85, who were diagnosed between
1988 and 1995 at hospitals located in central and northern Sweden. For each

Bayesian Causal Inference
691
patient, we have measurements of the following variables: home hospital,
treating hospital, survival time from date of diagnosis with cardia or stomach
cancer, and demographic background variables: age (old/young), male/female,
and area type of residence (urban/rural). The home and treating hospitals
are categorized into two types: large volume, deï¬ned for the ï¬rst data set
as treating more than 10 patients for cardia cancer during the period 1988â€“
1995, and for the second data set deï¬ned as treating more than 60 patients
for stomach cancer during the period 1988â€“1995; the remaining hospitals for
each data set are considered small volume. If our analyses of these data sets
are considered medically revealing, extensions of them may be applied to more
recent data sets to inform decisions on the status of small volume treatment
centers.
Throughout
the
analysis,
we
make
the
stable-unit-treatment-value-
assumption â€“ SUTVA discussed in Section 24.2.2, so that there is no
interference between units, and there are no hidden versions of treatments
available to any patient. SUTVA appears entirely plausible under the Swedish
medical system where medical care is free with reasonably sufï¬cient resources,
so there is no competition for treating hospital type among patients, and there
is only one plausible home hospital for each patent.
Denote the home hospital type by h, which takes the value â„“when assigned
large hospital type and s when assigned small home hospital type. Similarly,
let T denote treating hospital type, which takes the value L when the treating
hospital is large, and takes the value S when treating hospital is small. We
observe two types of transfers between home and treating hospital types: large
volume to small volume, â„“â†’S and small volume to large volume, s â†’L.
Columns (1)â€“(3) of Table 24.1 show the observed transfers for cardia cancers.
Columns (1)â€“(3) of Table 24.2 show the corresponding results for stomach
cancers. Both tables show, not surprisingly, that patients mostly transferred to
large-volume hospitals. Comparing Tables 24.1 and 24.2, we see that cardia
cancer patients transferred proportionally more often than stomach cancer
patients.
We assume that h is ignorably assigned (conditionally given age category,
male/female, and urban versus rural residence of the patient), which is con-
sidered reasonable by Swedish medical experts. There are two types of out-
comes: (1) the treating hospital type, denoted by T = L or S, which will be
considered a partially missing covariate that deï¬nes the principal strata; and
(2) the survival time since diagnosis, Y, which, for medical reasons, takes
the values 5+ for survival for more than ï¬ve years, 1â€“5 for survival between
one and ï¬ve years, and 0 for death within one year. Under the RCM, each
patient has potential outcomes T(â„“), T(s) and Y(â„“), Y(s) when assigned home
hospital type large and small, respectively. The observed assignment for patient
i is denoted by hi, and the observed potential outcomes for patient i are
denoted by Tobs,i and Yobs,i, where the fully observed background variables

692
The Oxford Handbook of Applied Bayesian Analysis
Table 24.1 Cardia cancer: Observed counts in observed groups and approximate counts in principal strata.
Conventional comparisons
Generally valid for ITT for LS
Observed groups
N for
observed
groups
Valid for
home
hospital type
Generally invalid
for any causal
effect
â€˜Assignedâ€™/
randomized
home
hospital type
Treating
hospital
type T
N
Intent to
treat
As
treated
Per
protocol
Under-
lying
principal
strataâˆ—
Approximate
proportion in
population
in principal
strata
Approximate
N in LS
principal
stratum
Ignores
T
Ignores
h
Ignores
observed
non-
compliers
H
N
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(1)
(2)
â„“
75
L
73
Trt = 73
Trt = 73
Trt = 73
L
L
L
S
33/75
40/75âˆ—âˆ—
Trt = 40âˆ—âˆ—âˆ—
S
2
Trt = 2
Ctl = 2
â€”
S
S
2/75
(3)
(4)
s
75
L
33
Ctl = 33
Trt = 33
â€”
L
L
33/75
S
42
Ctl = 42
Ctl = 42
Ctl = 42
S
L
S
S
2/75
40/75âˆ—âˆ—
Ctl = 40âˆ—âˆ—âˆ—
âˆ—Assuming monotonicity, i.e. no deï¬ers, no SL principal stratum; ï¬rst letter equals treating hospital with h = â„“; second letter equals
treating hospital when h = s.
âˆ—âˆ—1 âˆ’(%SS + %LL) = 1 âˆ’( 2
75 + 33
75) = 40
75 â‰ˆ53.3%; %LL â‰ˆ44.0%; %SS â‰ˆ2.7%.
âˆ—âˆ—âˆ—Number assigned to home hospital type Ã— proportion that are true compliers (i.e. in the LS principal stratum).

Bayesian Causal Inference
693
Table 24.2 Stomach cancer: Observed counts in observed groups and approximate counts in principal strata.
Conventional comparisons
Generally valid for ITT for LS
Observed groups
N for
observed
groups
Valid for
home
hospital type
Generally invalid
for any causal
effect
â€˜Assignedâ€™/
randomized
home
hospital type
Treating
hospital
type T
N
Intent to
treat
As
treated
Per
protocol
Under-
lying
principal
strataâˆ—
Approximate
proportion in
population
in principal
strata
Approximate
N in LS
principal
stratum
Ignores
T
Ignores
h
Ignores
observed
non-
compliers
H
N
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(1)
(2)
â„“
506
L
502
Trt = 502
Trt = 502
Trt = 502
L
L
L
S
44/347
âˆ¼86.5%**
Trt = 438***
S
4
Trt = 4
Ctl = 4
â€”
S
S
4/506
(3)
(4)
s
347
L
44
Ctl = 44
Trt = 44
â€”
L
L
44/347
S
303
Ctl = 303
Ctl = 303
Ctl = 303
S
L
S
S
4/506
âˆ¼86.5%**
Ctl = 300***
âˆ—Assuming monotonicity, i.e. no deï¬ers, no SL principal stratum; ï¬rst letter equals treating hospital with h = â„“; second letter equals
treating hospital when h = s.
âˆ—âˆ—1 âˆ’(%SS + %LL) = 1 âˆ’
 4
506 + 44
347

â‰ˆ86.5%; %LL â‰ˆ12.7%; %SS â‰ˆ0.8%.
âˆ—âˆ—âˆ—Number assigned to home hospital type Ã— proportion that are true compliers (i.e. in the LS principal stratum).

694
The Oxford Handbook of Applied Bayesian Analysis
for patients are denoted by a three component vector, xi, where each com-
ponent of xi is dichotomous. The partially missing covariate for person i is
the bivariate Gi â‰¡(Ti(â„“), Ti(s)), whose values will be denoted by LL, LS, SL,
or SS (where for simplicity LL means the same as (L, L), etc.). These deï¬ne
four possible â€˜principal strataâ€™ (Frangakis and Rubin, 2002), as developed in
Section 24.3.3.
The average causal effect of home hospital type on survival is the comparison
of the potential survival outcomes of all N patients under h = â„“and under h = s,
ITT = 1
N
N

i=1
[Yi(â„“) âˆ’Yi(s)],
(24.13)
where ITT is the Intention-To-Treat (ITT) effect in randomized trials. Under
ignorable treatment assignment, we are able to estimate ITT by, for example,
using xi to deï¬ne eight strata of patients, then taking the average observed
difference in Y for treated (i.e. large volume hospitals) and control (i.e. small
volume hospitals) patients within each stratum, and ï¬nally averaging across
these stratum-speciï¬c estimates using the stratum sizes as weights. The fourth
columns of Tables 24.1 and 24.2 show which groups of patients are to be
considered treated (Trt) and control (Ctl) for estimating ITT for cardia cancer
and stomach cancer, respectively.
In contrast to the home hospital type, the treating hospital type cannot be
considered to be ignorably assigned given the observed background covariates
and home hospital type. Invalid methods of estimating the causal effect of
treating hospital type involve directly comparing patients from the treating
hospital types, a so-called â€˜as-treatedâ€™ analysis â€“ indicated in column (5) in
Tables 24.1 and 24.2. Also invalid is removing all observed transfer patients
from the analysis, a so-called â€˜per-protocolâ€™ analysis displayed in column (6) in
Tables 24.1 and 24.2. The general invalidity of the as-treated and per-protocol
comparisons is well known (e.g. Lee et al., 1991). Principal stratiï¬cation leads
to valid inference for causal effects by estimating the ITT effect within each
principal stratum.
24.3.3 Causal effects of hospital type within principal strata
The idea of principal stratiï¬cation (Frangakis and Rubin, 2002) is to stratify
units according to the potential outcomes of an â€˜intermediateâ€™ post-treatment
variable, which here is the treating hospital type indicated by Gi. Such stratiï¬-
cation is legitimate because the values of post-treatment potential outcomes are
not affected by assignment of home hospital type â€“ which values are observed is
affected by treatment assignment, but not the values themselves, and therefore
Gi is, formally, a partially observed covariate.

Bayesian Causal Inference
695
As deï¬ned in Section 24.3.2, there are four possible principal strata, which
are labeled LL, LS, SL, SS. LS can be thought of as the stratum of compliers, i.e.
non-transfer patients; the LL and SS strata can be thought of as non-compliers
who will always be treated at the same home hospital type no matter where
assigned (diagnosed), and SL can be thought of as deï¬ers, who will transfer no
matter where assigned (diagnosed). Because the assignment of home hospital
type is ignorable given xi, it is ignorable given both xi and Gi.
When Gi = LS, the home hospital type equals the treating hospital type,
i.e. hi = Ti. The causal effect of home hospital type when Gi = LS is deï¬ned
to be
ITTL S =
1
NL S

iâˆˆL S
(Yi(L) âˆ’Yi(S)),
(24.14)
where NLS is the number of LS patients; ITTLS is easily estimated once we
identify the individuals in the LS stratum, but explicit assumptions are needed
to make this estimation straightforward. Notice that ITTLS can be interpreted as
either the intention-to-treat effect of home hospital type for complying patients
or the intention-to-treat effect of treating hospital type for complying patients,
because for the LS principal stratum, hi = Ti. The LS and SL principal strata are
the only strata of patients where we can learn about the causal effects of treating
hospital types because the patients in the other principal strata, LL and SS, will
always be exposed to the same treating hospital type. Imbens and Rubin (1997)
call ITTLS â€˜CACEâ€™, for â€˜Complier Average Causal Effectâ€™.
24.3.4 The monotonicity assumption and the approximate size of the LS group
First, we consider what is called the â€˜monotonicityâ€™ assumption or the â€˜no-deï¬erâ€™
assumption â€“ that is, the SL principal stratum is empty. Speciï¬cally this
assumption can be viewed as having two parts. First, if patient i is the type
of patient who would be treated in a large volume hospital when assigned to a
small volume hospital, patient i would be treated in a large volume hospital
when assigned a large volume hospital. Second, if patient i is the type of
patient who would be treated in a small volume hospital when assigned to a
large volume hospital, patient i would be treated in a small volume hospital
if assigned to a small volume hospital. In our setting, this assumption is very
plausible, and because it excludes the SL principal stratum, we have only three
possible principal strata: LL, LS, and SS.
The possible principal strata for each observed combination of home hospital
type and treating hospital type (i.e. each of the four rows of Tables 24.1 and 24.2)
are shown in the seventh columns of Table 24.1 and 24.2. The observed â„“â†’S
group (the second row in Tables 24.1 and 24.2) must be composed of SS patients
because they can be neither LL nor SL patients, respectively â€“ because they were

696
The Oxford Handbook of Applied Bayesian Analysis
assigned â„“but treated in S and therefore not LL patients, and there are no SL
patients by the monotonicity assumption. Similarly, the observed s â†’L group
(the third row of Tables 24.1 and 24.2) must be LL patients because they were
assigned s but were treated in L.
In contrast, the observed â„“â†’L subgroup (the ï¬rst row of Tables 24.1
and 24.2) could be compliers, and so be in LS, or non-compliers who are
members of the LL principal stratum (who were assigned to home hospital type
L, and to which they would have transferred for their treating hospital type if
they were assigned to a small home hospital type). Hence, we split row 1 into
two subrows in columns (7) of Tables 24.1 and 24.2. Similarly, the observed
s â†’S subgroups (the fourth row of Tables 24.1 and 24.2) could be compliers,
and so be in LS, or non-compliers who are members of the SS principal stratum,
and so is also split into two subrows.
We begin with a simple method-of-moments analysis to convey the essential
idea of the estimation of the size of the LS group before considering our fully
Bayesian analysis. Ignoring the covariates and thinking of the assignment of
small home hospital type as assigned completely at random, we can approx-
imate the size of each principal stratum, as shown in the eighth columns
of Tables 24.1 and 24.2. More explicitly, from the second row of Table 24.1,
columns (1) and (3), we see that 2/75 are observed to be â„“â†’S. Because of
the assumed random assignment into â„“and s, we have that approximately
2/75 of the patients belong to the principal stratum SS for cardia cancer, as
shown in column (8) in Table 24.1. Analogously, we see from Table 24.2 that
approximately 4/506 of the patients belong to principal stratum SS for stomach
cancer, as shown in column (8). Similarly, from the third row of Table 24.1,
columns (1) and (3), we infer that approximately 33/75 of patients belong to
principal stratum LL for cardia cancer, and approximately 44/347 of patients
belong to principal stratum LL for stomach cancer, as shown in columns (8) of
Tables 24.1 and 24.2, respectively.
Thus we can approximate the fraction of patients in the SS principal stra-
tum, 2/75 for cardia cancer and 4/506 (â‰ˆ0.8%) for stomach cancer, and we
can approximate the fraction of patients in the LL principal stratum, 33/75
(â‰ˆ44.0%) for cardia cancer and 44/347 (â‰ˆ12.7%) for stomach cancer.
Hence, we can approximate the fraction of compliers, the LS principal stra-
tum, by simple subtraction: 1 âˆ’( 2
75 + 33
75) = 44
75 â‰ˆ53.3% for cardia cancer; and
1 âˆ’( 4
506 + 44
347) â‰ˆ86.5% for stomach cancer. These results are displayed in the
ï¬rst and fourth rows of columns (7) and (8) of Tables 24.1 and 24.2, and sum-
marized in the ï¬rst three rows of Table 24.3. The ninth columns in Tables 24.1
and 24.2 indicate the approximate number of LS patients in each of the four
rows of observed patients, and the approximate number to be considered
assigned to an L large volume hospital (labelled Trt for treated) and a S small
volume hospital (labelled Ctl for controls).

Bayesian Causal Inference
697
Table 24.3 Method-of-moments estimates ignoring covariates.
% in principal strata
Cardia cancer
Stomach cancer
ï£¿LL
44.0
12.7
ï£¿SS
2.7
0.8
ï£¿LS
53.3
86.5
ï£¿LL/(ï£¿LL + ï£¿LS)
94.2
94.1
% survival - Ã›
1 year
5 year
5Â·1 year
1 year
5 year
5Â·1 year
Ã›LL
60.61
18.21
30.0
50.02
25.02
50.0
Ã›SS
50.03
50.03
100.0
75.04
25.04
33.3
Ã›â„“
34.75
13.35
38.5
43.06
17.46
40.4
Ã›s
37.37
9.37
25.0
45.58
17.68
38.6
Ã›LSâ€¢â„“
12.59
7.49
43.09
41.79
16.29
39.19
Ã›LSâ€¢s
17.410
0.010
17.610
44.610
16.410
37.010
CACE = ITTLS
âˆ’4.911
7.511
âˆ’2.311
âˆ’0.211
Estimated y survival among:
1 33 s â†’L in Table 24.1 (20,6); 2 44 s â†’L in Table 24.2 (22,11); 3 2 â„“â†’s in Table 24.1
(1,1); 4 4 â„“â†’s in Table 24.2 (3,1); 5 75 assigned â„“in Table 24.1 (26,10); 6 506 assigned â„“
in Table 24.2 (218,88); 7 75 assigned s in Table 24.1 (28,7); 8 347 assigned s in Table 24.2
(158,61); 9 [Ã›â„“âˆ’ï£¿LLÃ›LL âˆ’ï£¿SSÃ›SS]/ï£¿LS; 10 [Ã›s âˆ’ï£¿LLÃ›LL âˆ’ï£¿SSÃ›SS]/ï£¿LS; 11 [Ã›â„“âˆ’Ã›s]/ï£¿LS.
24.3.5 Problems with the as-treated and per-protocol analyses
We can also see why the as-treated [column (5)] and per-protocol [column (6)]
estimators generally do not estimate actual causal effects. For example, con-
sider the as-treated analysis for cardia cancer. The â€˜Treatmentâ€™ group in these
analyses, in rows one and three of Table 24.1, has 73 + 33 = 106 patients. But
from column (9), this group of 106 patients has approximately 40 LS patients,
and therefore the remaining 66 must be LL patients. In contrast, the â€˜Controlâ€™
group in these analyses, in rows two and four, has 2 + 42 = 44 patients, but from
column (9) has approximately 40 LS patients, and therefore the remaining four
patients must be SS. Even assuming complete randomization to home hospital
type, the as-treated analysis compares the observed values of potential outcomes
for overlapping but not identical types of patients, in contrast to doing what is
valid: comparing observed values of potential outcomes on a randomly divided
common set of units, here, those in the LS principal stratum, who comprise less
than half (only about 40 of 106) of those treated in the large volume hospitals,
but a great majority (about 40 of the 44) of those treated in the small volume
hospitals.
We will use the stomach cancer data to illustrate a similar problem with the
per-protocol estimator [column (6)]. Here the â€˜Treatmentâ€™ group in row one of
Table 24.2 has 502 patients, but from column (9) has approximately 438 LS
patients, and therefore the remaining 64 must be LL patients. In contrast,
the â€˜Controlâ€™ group in row four has 303 patients, but from column (9) has

698
The Oxford Handbook of Applied Bayesian Analysis
approximately 300 LS patients, and the remaining three must be SS patients.
Even assuming complete randomization to home hospital type, we are com-
paring the potential outcomes for subsets of non-identical types of patients, in
contrast to doing what is valid: comparing potential outcomes on a randomly
divided common set of units, here, those in the LS principal stratum, who
comprise most (about 438 of 502) of those randomized to and treated in the
large volume hospitals, but nearly all (about 300 of 303) of those randomized to
and treated in the small volume hospitals. For the stomach cancer data, this is
not a terribly serious problem because the SS principal stratum is very small,
and the LL principal stratum is relatively small.
24.3.6 The exclusion restriction and the instrumental variables estimate of CACE
Notice that we have not yet identiï¬ed any particular member of the â„“â†’L or
s â†’S rows (rows one and four) in Tables 24.1 and 24.2 as being in the LS
principal stratum, and so we cannot yet compare average outcomes in this
stratum. Nevertheless, we can ï¬nd a unique method-of-moments estimate of
the causal effect of assigned (=home) hospital type within the LS principal
stratum under, what are considered, medically very justiï¬able assumptions,
which in general are called â€˜exclusion restrictionsâ€™. This estimator of CACE
is known as the â€˜instrumental variable estimateâ€™ (Angrist, Imbens and Rubin,
1996). We follow this simple analysis with a Bayesian model for estimating this
causal effect.
The ï¬rst exclusion restriction is for patients in the LL principal stratum. It
states that, for all i âˆˆLL, Yi(â„“) = Yi(s); that is, there is no effect on potential
outcomes Y of being assigned to a large (â„“) or small (s) home hospital type
for patient i âˆˆLL. The medical justiï¬cation for this restriction is that patient i
would be treated in a large hospital type (L) under either assignment, and oneâ€™s
medical outcome is considered a result of where one is treated not where one is
diagnosed. The exclusion restriction for patients in the SS principal stratum is
analogous; for all i âˆˆSS, Yi(â„“) = Yi(s); that is, for those patients who would be
treated in a small hospital type (S) whether assigned to l or s, there is no effect
of assignment on the Y potential outcomes.
Table 24.3 presents simple method-of-moments estimates to convey essential
ideas. The ï¬rst three rows have already been discussed, and they present the
relative sizes of the principal strata for both the cardia and the stomach cancer
data sets, as estimated in Tables 24.1 and 24.2. Row 4 presents the approximate
relative sizes of the LL group among noncompliers (LL or SS) for both cancers.
Row 5 gives one-year and ï¬ve-year survival rates, for both kinds of cancers, for
the LL group, as estimated by the known LL groups (i.e. group of 33 patients
with cardia cancer and the group of 44 patients with stomach cancer who were
assigned to small hospital type but treated in a large hospital type). For these,

Bayesian Causal Inference
699
by the exclusion restriction, their survival rate would be the same if assigned
to a large hospital type. Row 6 gives the analogous results for the SS group,
now based on those observed to be â„“â†’S. Row 7 gives the survival rate for all
patients assigned â„“, and row 8 gives the survival rate for all patients assigned s.
Because of the assumed randomization to home hospital type (â„“versus s), the
survival rates among those assigned â„“, Ã›â„“, is the weighted combination of the
survival rates across the principal strata when assigned to â„“,
Ã›â„“= ï£¿L LÃ›L L + ï£¿SSÃ›SS + ï£¿L SÃ›L Sâ€¢â„“,
where by exclusion, Ã›LL and Ã›SS are unaffected by assignment to â„“or s. Thus,
the survival rate within the LS principal stratum when assigned to â„“is,
Ã›L Sâ€¢â„“= (Ã›â„“âˆ’ï£¿L LÃ›L L âˆ’ï£¿SSÃ›SS)
A
ï£¿L S,
which is displayed in row 9 for both cancers and both survival outcomes.
Analogously, the survival rate for the LS principal stratum when assigned to
s is:
Ã›L Sâ€¢s = (Ã›s âˆ’ï£¿L LÃ›L L âˆ’ï£¿SSÃ›SS)
A
ï£¿L S,
which is displayed in row 10. Now ITTLS â‰¡CACE is the difference,
CACE = Ã›L Sâ€¢â„“âˆ’Ã›L Sâ€¢s = (Ã›â„“âˆ’Ã›s)
A
ï£¿L S,
which is displayed in row 11 for both cancers and both survival outcomes.
Another, more direct derivation of CACE, which we believe is less insightful,
is as follows. ITT for all patients can be written as
ITT = ï£¿L SITTL S + ï£¿SSITTSS + ï£¿L LITTL L
(24.15)
where ITTLS, ITTSS, and ITTLL are the intention-to-treat effects in the LS, SS,
and LL strata, respectively. Because the exclusion restrictions force ITTSS and
ITTLL to be identically zero, equation (24.15) becomes
ITT = ï£¿L SITTL S.
(24.16)
The instrumental variables estimate of the ITT effect of treating hospital
on one-year survival is âˆ’2.6%/53.3% = âˆ’4.9%, and on ï¬ve-year survival is
4%/53.3% = 7.5%. The corresponding estimates for stomach cancer are both
approximately 0% because there appears to be no evidence of an ITT effect of
home hospital type for stomach cancer survival rates. However, these estimates
are not good ones for two reasons. First, they ignore important differences
between patients in diagnosing hospital types in the distributions of observed
covariates, and second, because they are inefï¬cient method of moment esti-
mates rather than the preferable Bayesian estimates.

700
The Oxford Handbook of Applied Bayesian Analysis
24.3.7 Speciï¬c Bayesian models
We now proceed to describe the speciï¬c model that follows the general frame-
work of Section 24.2. First, we assume latently ignorable treatment assignment;
that is, we assume equation (24.3) holds when Xi is deï¬ned to include the three
fully observed background variables, xi, and the three level indicator Gi of the
principal stratum of patient i, Gi âˆˆ{LS, LL, SS}. Thus, no further modelling is
needed for the assignment mechanism. What is needed, however, is a model
for the principal strata given xi, which is described later in this section.
Also in parallel with equation (24.9), we assume that the parameters govern-
ing the marginal distribution of the fully observed covariates, xi, are a priori
independent of all other parameters. All additional model speciï¬cations use
distributions that are independent for each distinct value of xi. Thus, because
xi deï¬nes a 2 Ã— 2 Ã— 2 table, there are eight entirely independent models. This
allows the following descriptions to drop the repeated conditioning on xi. We
ï¬rst describe the distribution of Gi governed by parameter Ã‹g, and then the
distribution of (Yi(â„“), Yi(s)) given Gi.
The distribution of Gi is speciï¬ed by two Bernoulli probabilities, two because
Gi has three levels. The ï¬rst parameter, â€ž, gives the probability that Gi âˆˆLS (i.e.
that patient i is a complier). The second parameter, Ã, gives the probability, if
patient i is a noncomplier, that Gi = LL versus Gi = SS. The reason for using
this parametrization with Ã‹g = (â€ž, Ã) is that interest focuses on the compliers
and CACE, the causal effect of the treating hospital where it can be estimated.
These two Bernoulli parameters, â€ž and Ã, are a priori independent, with prior
distributions for each created by appending one fake observation to the data.
This one observation for â€ž is split according to the crude estimates of the mar-
ginal proportion of compliers among all patients given in row 3 of Table 24.3.
More speciï¬cally, for cardia cancer the proportions of compliers is estimated
to be about 53.3%, and for stomach cancer it is about 86.5%, so, the fake
observation for â€ž is split according to these fractions. The one fake observation
for Ã is split between LL and SS according to the marginal proportion of LL
versus SS among all patients for cardia cancer; this split is estimated to be 94.2%
versus 5.8%, and for stomach cancer about 94.1% versus 5.9%. The idea to use
such prior distribution was ï¬rst proposed in Rubin (1984; reprinted in Rubin
2004, Appendix II). It was explored further in Rubin and Schenker (1987) and
Clogg et al. (1991).
We complete the model speciï¬cation by providing the conditional distribu-
tion of the potential outcomes given Xi = (Gi, xi) and Ã‹yâ€¢g, described ignoring
xi because the models are speciï¬ed independently for each of the eight val-
ues of xi. First, we make the two assumptions implied by equations (24.11)
and (24.12): the potential outcomes Yi(s) and Yi(â„“) are conditionally inde-
pendent given Gi and Ã‹yâ€¢g, and their associated parameters are also a priori

Bayesian Causal Inference
701
independent. Second, for medically relevant reasons, we summarize Yi by two
indicator variables. The ï¬rst, Y+1
i , for lived at least a year

Y+1
i
= 1

versus
died within a year after the cancer diagnosis

Y+1
i
= 0

, and the second for,
given Y+1
i
= 1, lived at least ï¬ve years

Y+5
i
= 1

versus died within ï¬ve years

Y+5
i
= 0

. In the LS principal stratum there is one distribution of Yi(â„“) to be
estimated and one distribution of Yi(s) to be independently estimated. By the
exclusion restrictions, within the LL principal stratum there is one distribution
for Yi(â„“) â‰¡Yi(s), and in the SS principal stratum there is one distribution for
Yi(â„“) â‰¡Yi(s). Each of these four distributions of Yi is modelled in the same way
as the Gi; that is, each of the four will be modelled as two independent Bernoulli
distributions, and thus will have associated Beta posterior distributions.
In LS, there is one Bernoulli distribution for Y1+
i (â„“) with parameter Ë†â„“
LS and
one Bernoulli distribution for Y5+
i (â„“) given Y1+
i (â„“) = 1 with parameter Â¯â„“
LS; and
analogously for Y1+
i (s) and Y5+
i (s) with Bernoulli distribution parameters Ë†s
LS
and Â¯s
LS. The parameters for the distribution of Yi(â„“) = Yi(s) in the LL principal
stratum and the distribution of Yi(â„“) = Yi(s) in the SS principal stratum are
analogously Ë†LL, Â¯LL, Ë†SS, and Â¯SS. Thus, the total number of Bernoulli parame-
ters being estimated for the conditional distribution of the potential outcomes
is eight, and over all eight values of xi, the number of scalar parameters is
8 Ã— 8 = 64. So Ã‹ has 64 + 16 = 80 a priori independent Bernoulli parameters,
because there is a â€ž and an Ã at each value of xi. The prior distribution on these
is created by appending one fake observation to each of the 80 subdata sets,
split according to the crude estimates of the marginal (across all eight values
of xi) probabilities of â€˜successâ€™ and â€˜failureâ€™, implied by the point estimates in
Table 24.3 of one-year survival for Ë† and of ï¬ve-year survival given one-year
survival for Â¯. Other prior distributions could, of course, be used: better ones
would be based on carefully considered medical knowledge.
24.3.8 Computation
Our plan is to use DA (data augmentation, Tanner and Wong 1987), which
is a stochastic version of the EM algorithm (Demster, Laird, Rubin 1977) to
implement the computation. Many references now exist for this Markov Chain
Monte Carlo (MCMC method) e.g. Gelman, et al. (1995) and Tierney (1994).
We begin with a description of how we create a starting â€˜completedâ€™ data set,
with all missing Gi and missing Yi randomly imputed. Speciï¬cally, use the
method of moments estimates of the probabilities in Table 24.3 to draw, ï¬rst
the missing values of Gi, initially, LS versus LL or SS, and then if the latter is
drawn, LL versus SS; now knowing all the values of Gi, we draw the survival
outcomes, ï¬rst one-year survival versus one-year death, and then if one-year
survival is drawn, we draw ï¬ve-year survival using the drawn data set. In order
to use this completed dataset to create an overdispersed starting distribution

702
The Oxford Handbook of Applied Bayesian Analysis
for drawing parameters, we reduced the sample size by a factor of 60%. All
subsequent iterations are done independently across all values of x.
Each iteration of the MCMC has two main steps. The ï¬rst step draws
the parameters from their complete data posterior distribution, assuming all
imputed values are real. The ï¬rst step consists of ï¬ve independent substeps: (1)
one for the parameters governing the principal strata; (2) one for the parameters
governing the parameters of the Y(1) potential outcomes in LS; (3) one for the
parameters governing the parameters of the Y(0) potential outcomes in LS; (4)
one for the parameters governing the Y(1) potential outcomes in LL; and (5)
one for the parameters governing the Y(0) potential outcomes in SS. Each of
these substeps comprises two sub-substeps: (i) one based on all the data at that
value of x (for drawing being a complier or not for substep 1), and the other
(ii) conditional on the being a noncomplier (for the ï¬rst substep) or surviving
at least one year (for the other four substeps). Each of these 10 sub-substeps
draws the parameter from a Beta posterior distribution governed by: the prior
distribution (fake observation) plus the data that has been observed plus the
data that has been imputed.
The second step of the MCMC considers the values of the parameters drawn
in the ï¬rst step to be the correct values and draws the missing principal strata
and missing potential outcomes in ï¬ve substeps: (1) independently draw the
missing principal strata for all the patients using the parameters drawn in step
1, substep 1; (2) independently draw the missing Y(1) potential outcomes for
the patients in LS who were assigned 0 (control = Small) using the parameters
drawn in step 1, substep 2; (3) independently draw the missing Y(0) for the
patients in LS who were assigned to 1 (treated=Large), using the parameters
drawn in step 1, substep 3; and analogously for substeps (4) and (5). Each of
these substeps has two parts, corresponding to the sub-substeps of step 1. Each
of the 10 sub-substeps draws missing data from a Bernoulli distribution.
So in all, within each stratum deï¬ned by x, there are 10 draws from Beta
posterior distributions. and 10 sets of draws from Bernoulli sampling distri-
butions. Three such independent chains were drawn at each value of x, and
allowed to â€˜burn inâ€™ until approximate convergence. Convergence of the logits of
all 80 Bernoulli parameters was monitored using the GR statistic (Gelman and
Rubin, 1992a,b). All GR statistics were very close to 1.0 after only 100 iterations.
Thereafter 100,000 iterations were run for each chain with every tenth iteration
saved as a draw from the posterior distribution.
24.3.9 Results and discussion
The results of our MCMC simulation are partially summarized in Table 24.4,
which gives the posterior medians and central 95% posterior intervals for sev-
eral important estimands. We ï¬rst discuss the results in Table 24.4 for cardia
cancer and then for stomach cancer.

Bayesian Causal Inference
703
Table 24.4 Posterior medians (and 95% intervals) for one- and ï¬ve-year survival
adjusted for background variables.
Principal
strata
% in strata
When assigned
% Survival rates
One year survival
Five year survival
Cardia cancer
LS
52.0 (42.7, 60.0)
Large home hospital
Small home hospital
33.3 (19.4, 48.8)
21.1 (12.0, 31.4)
13.0 (3.7, 25.3)
4.7 (0.0, 11.5)
LL
44.0 (36.7, 51.3)
No effectâˆ—
52.1 (43.9, 60.6)
16.9 (12.2, 21.7)
SS
4.0 (1.3, 8.7)
No effectâˆ—
33.3 (12.5, 71.4)
22.2 (9.1, 50.0)
All
100
Large home hospital
Small home hospital
42.0(36.0, 48.0)
35.3(30.0, 41.3)
14.7(11.3, 20.7)
10.7(8.0, 14.7)
Stomach cancer
LS
85.9 (83.0, 88.4)
Large home hospital
Small home hospital
44.1 (40.6, 47.7)
44.7 (40.5, 49.2)
16.0 (13.2, 18.8)
16.3 (13.4, 19.8)
LL
12.8 (10.4, 15.5)
No effectâˆ—
50.9 (41.4, 60.6)
25.0 (17.5, 33.3)
SS
1.3 (0.7, 2.3)
No effectâˆ—
66.7 (40.0, 90.0)
26.7 (8.3, 50.0)
All
100
Large home hospital
Small home hospital
42.3(42.7, 47.8)
45.8 (42.0, 49.9)
17.2 (15.6, 19.5)
17.6 (14.9, 20.6)
âˆ—By the exclusion restriction
Notice ï¬rst we estimate that only about 50% of the cardia cancer patients are
compliers in the sense that they would be treated in their diagnosing (home)
hospital type. We estimate that roughly 45% would be treated in a large volume
hospital no matter where diagnosed, and only roughly 5% would be treated
in a small volume hospital no mater where diagnosed. Notice that the non-
compliers appear to be healthier than the compliers, no matter where the
compliers are treated with better one-year or ï¬ve-year survival. This may make
medical sense: LL consists of patients who may be willing to undergo more
aggressive treatment provided by large volume centres because their prognosis
is relatively good, and SS may consist of patients whose cancers were caught
early and thus also have a reasonably good long-term prognosis and thus choose
to be closer to family or friends for treatment. These results are not causal but
describe the type of cardia cancer patient in the three principal strata; all these
descriptive statements are subject to substantial posterior uncertainty due to the
small sample sizes involved.
With respect to the results in Table 24.4 for the much more common stomach
cancer, we see that the vast majority, roughly 86%, are estimated to be com-
pliers, in the sense of being treated in the same hospital type as where they
were diagnosed, and nearly all the rest, roughly 13%, are estimated to be in
LL; only 1% are estimated to be in SS. But as with the cardia cancer patients,

704
The Oxford Handbook of Applied Bayesian Analysis
Table 24.5 â€˜Causalâ€™ answers from four Bayesian analyses of cardia
and stomach cancers: Posterior medians (and 50% and 95% intervals)
for effects of large versus small hospital types on one- and ï¬ve-year
survival rates, adjusted for background covariates.
One year survival
Five year survival
% (Large â€“ Small)
% (Large â€“ Small)
Cardia cancer
CACE = ITTLS
12.2 (âˆ’4.8, 6.3, 18.3, 29.6)
8.1 (2.8, 3.9, 12.6, 21.5)
ITT
4.0 (âˆ’7.3, 0.7, 8.0, 15.3)
7.3 (âˆ’0.7, 4.7, 10.7, 16.7)
Stomach cancer
CACE = ITTLS
âˆ’0.7 (âˆ’5.4, âˆ’2.4, 0.9, 4.2)
âˆ’0.4 (âˆ’4.7, âˆ’1.8, 1.1, 3.7)
ITT
âˆ’2.9 (âˆ’7.7, âˆ’4.6, âˆ’1.3, 1.9)
âˆ’0.1 (âˆ’3.9, âˆ’1.4, 1.1, 3.4)
the non-complying stomach cancer patients appear to have somewhat better
survival rates than the complying stomach cancer patients, no matter where
treated, presumably for similar reasons as for the cardia cancer patients. In
fact, it appears that the survival rates for the complying patients are very similar
no matter where treated.
Table 24.5 directly addresses the primary causal questions by providing
posterior medians and central 50% and central 95% posterior intervals for
CACE = ITTLS and for ITT. For stomach cancer, there appears to be no evidence
for any advantage to being treated in one type of hospital or the other: even the
50% intervals for CACE comfortably cover zero. The story for cardia cancer
appears different, however. The posterior medians for the causal CACE effects
are in favor of being treated in a large-volume hospital, approximately a 12%
advantage for one-year survival â€“ 33% versus 21%, a factor of about 1.5, and
approximately an 8% advantage for ï¬ve-year survival â€“ 13% versus 5%, a factor
of nearly 3. Both 50% posterior intervals for CACE easily exclude zero, and even
the 95% interval for CACE for ï¬ve-year survival is entirely positive. Therefore,
it is more likely than not that being treated for cardia cancer in a large-volume
hospital versus a small volume hospital causes better survival outcomes, both
short term and long term, and very likely that it causes better long-term survival.
Using ITT instead of CACE just makes conclusions weaker and far more
ambiguous.
The implications for patient choices and for policy seem clear. For patients
with stomach cancer, there is essentially no difference on survival whether
treated in a large-volume or small-volume hospital. In contrast, for patients
with the much rarer cardia cancer, there appears to be a relatively clear sur-
vival advantage to being treated in a large-volume rather than a small-volume
hospital, and therefore it would appear prudent for patients diagnosed in small-
volume hospitals to be transferred to large-volume hospitals.

Bayesian Causal Inference
705
Sharper, that is, more precise, estimation could possibly be obtained by
including medical knowledge to modify prior distributions, either by restricting
parameters or by assuming some parameters are a priori dependent across
some values of x and principal strata. However, there is something very â€˜cleanâ€™
and agnostic about the Bayesian models that we have used. And notice that this
sort of ï¬tting could not have been accomplished using standard frequency tools
at each value of x, such as the method-of-moments estimates used to provide
the motivation for Tables 24.1â€“24.3.
Appendix
A. Broader context and background
Here we provide additional background about the RCM in the context of
observational studies for causal effects. A critical feature of the RCM is that
assumptions must be made explicit using full probability models (Rubin, 1978,
2008).
First consider the simpler case in which Bayesian causal inference can be
based on one regression model relating the observed value of the outcome
variable to the treatment indicator and background covariates. Under the RCM,
the model for the treatment assignment mechanism is distinct from the model
for the data, which include the potential outcomes and covariates. Given a rich
enough set of covariates X, the treatment assignment W is assumed to be
ignorable
Pr(W |X, Y(0), Y(1)) = Pr(W |X, Yobs),
where Y(0) and Y(1) are the potential outcomes under two treatments indicated
by W = 0 and W = 1, and Yobs is the observed outcome. Under this assumption
and the stability assumption (SUTVA), Bayesian causal inference can be based
on a single regression model using as predictors the treatment indicator and
the covariates X. In observational studies, where there does not exist a known
mechanism for treatment assignment, one posits an assignment mechanism
and then assesses the propriety of the assumption of ignorable treatment
assignment.
In this chapter, the assumption of the ignorable assignment of home hospital
type h given the observed background covariates X was plausible because of the
Swedish health care system and Swedenâ€™s social economic structure. Although
we could not assess all aspects of this assumption, we did assess some aspects,
such as its implications for weight loss, an important prognostic correlate,
and the results suggested that ignorability was reasonable. The ignorability
assumption for the assignment of treating hospital type, however, was consid-
ered unreasonable. If we had observed covariates, such as the patientsâ€™ true

706
The Oxford Handbook of Applied Bayesian Analysis
cancer stage and their willingness to undergo invasive medical operations, then
the assumption of ignorable assignment of the treating hospital type could have
been plausible. If so, then Bayesian causal inference for the effect of treating
hospital type could have been based on a single regression model predicting
the observed outcome from an indicator for treating hospital type, an indicator
for home hospital type, and the covariates.
Because we had to allow for a non-ignorable treatment assignment mecha-
nism, we had to consider the case where Bayesian causal inference could not
be based on just one single regression model. Our solution is to consider those
patients who are treated in the same hospital type as their home hospital type to
be compliers with their original assignment, and the other patients to be non-
compliers, and seek causal inference for the compliers â€“ where home hospital
is the same as treating hospital, and the former is ignorably assigned. Even
though the true compliers cannot be exactly identiï¬ed from the data, under
medically very plausible assumptions in our setting, called â€˜monotonicityâ€™ and
â€˜exclusion restrictions,â€™ estimation of the complier average causal effect (CACE)
can be accomplished. The critical idea is to conduct Bayesian inference in what
economists call instrumental variables models (Imbens and Rubin, 1997). The
resulting analyses are related to those in Hirano et al. (2000), and are special
cases of more general analyses based on the concept of â€˜principal stratiï¬cationâ€™
(Frangakis and Rubin, 2002).
Acknowledgements
We are grateful to Olof Nyren for stimulating discussions and advice on medical
aspects of the problem, without which the work would be impossible. Donald
B. Rubinâ€™s work was partially supported by grants from the US National Science
Foundation (Grant SES 0550887) and from the US National Institutes of Health
(Grant R01 DA023879-01).
References
Angrist, J.D., Imbens, G.W. and Rubin, D.B. (1996). Identiï¬cation of causal effects using
instrumental variables (with discussion). Journal of the American Statistical Association, 91,
444â€“455.
Box, G.E.P. and Jenkins, G.M. (1970). Time Series Analysis: Forecasting and Controls. Holden-
Day, San Francisco.
Carlin, B.P. and Louis, T.A. (2000). Bayes and Empirical Bayes Methods for Data Analysis, (2nd
edn). Chapman & Hall, London.
Clogg, C.C., Rubin, D.B., Schenker, N., Schultz, B. and Weidman, L. (1991). Mul-
tiple imputation of industry and occupation codes in census public-use samples
using Bayesian logistic regression. Journal of the American Statistical Association, 86,
68â€“78.

Bayesian Causal Inference
707
Cox, D.R. (1958). The Planning of Experiments. John Wiley, New York.
de Finetti, B. (1963). Foresight: Its logical laws, its subjective sources. In Studies in Subjective
Probability, (ed. H.E. Kyburg and H.E. Smokler). John Wiley, New York.
Dempster, A.P., Laird, N. and Rubin, D.B. (1977). Maximum likelihood estimation from incom-
plete data using the EM algorithm (with discussion). Journal of the Royal Statistical Society,
Series B, 39, 1â€“38.
Frangakis, C.E. and Rubin, D.B. (1999). Addressing complications of intention-to-treat analysis
in the combined presence of all-or-none treatment-noncompliance and subsequent missing
outcomes. Biometrika, 86, 365â€“379.
Frangakis, C.E. and Rubin, D.B. (2002). Principal stratiï¬cation in causal inference. Biometrics,
58, 21â€“29.
Gelman, A. and Rubin, D.B. (1992a). A single sequence from the Gibbs sampler gives a false
sense of security. In Bayesian Data Analysis 4, (ed. J.M. Bernardo, J.O. Berger, A.P. Dawid,
and A.F.M. Smith), pp. 623â€“631. Oxford University Press, New York.
Gelman, A. and Rubin, D.B. (1992b). Inference from iterative simulation using sequence (with
discussion). Statistical Science, 7, 457â€“511.
Gelman, A., Carlin, J.B., Stern, H.S. and Rubin, D.B. (1995). Bayesian Data Analysis, Chapter 11.
Chapman & Hall/CRC, Boca Raton.
Gelman, A., Carlin, J. Rubin, D.B. and Stern, H. (2003). Bayesian Data Analysis (2nd edn).
CRC Press, New York.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6
721â€“741.
Imbens, G.W. and Rubin, D.B. (1997). Bayesian inference for causal effects in randomized
experiment with non-compliance. Annals of Statistics, 25, 305â€“327.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains and their applica-
tion. Biometrika, 57, 97â€“109.
Hirano, K., Imbens, G.W., Rubin, D.B. and Zhou, X. (2000). Assessing the effect of an inï¬‚uenza
vaccine in an encouragement design. Biostatistics, 1, 69â€“88.
Holland, P. (1986). Statistics and causal inference. Journal of the American Statistical Association,
81, 945â€“970.
Lee, Y.J., Ellenberg, J.H., Hirtz, D.G. and Nelson, K.B. (1991). Analysis of clinical trials by
treatment actually received: Is it really an option? Statistics in Medicine, 10, 1595â€“1605.
Little, R.J.A. and Rubin, D.B. (1987). Statistical Analysis with Missing Data. John Wiley,
New York.
Little, R.J.A. and Rubin, D.B. (2002). Statistical Analysis with Missing Data (2nd edn.).
John Wiley, New York.
Little, R. and Yau, L. (1998). Statistical technique for analyzing data from prevention trials,
treatment of no-shows using Rubinâ€™s causal model. Psychological Methods, 3, 147â€“159.
Metropolis, N. and Ulam, S. (1949). The Monte Carlo method. Journal of the American Statistical
Association, 44, 335â€“341.
Peterson, A.V. Jr., Kealey, K.A., Mann, S.L., Marek, P.M. and Sarason, I.G. (2000). Hutchin-
son smoking prevention project: long-term randomized trial in school-based tobacco use
prevention-results on smoking. Journal of the National Cancer Institute, 92, 1979â€“1991.
Robert, C. and Casella, G. (2008). A History of Markov Chain Monte Carlo â€“ Subjective
Recollections from Incomplete Data. Technical Report, Department of Statistics, University
of Florida; http://www.stat.ufl.edu/âˆ¼casella/Papers/MCMCHistory.pdf
Rubin, D.B. (1974). Estimating causal effects of treatments in randomized and non-
randomized studies. Journal of Educational Psychology, 66, 688â€“701.

708
The Oxford Handbook of Applied Bayesian Analysis
Rubin, D.B. (1975). Bayesian inference for causality: The importance of randomization. In The
Proceedings of the Social Statistics Section of the American Statistical Association, pp. 233â€“239.
American Statistical Association, Alexandria, VA.
Rubin, D.B. (1976). Inference and missing data. Biometrika, 63, 581â€“592.
Rubin, D.B. (1977). Assignment to treatment group on the basis of a covariate. Journal of
Educational Statistics, 2, 1â€“26. Printerâ€™s correction note 3, p. 384.
Rubin, D.B. (1978). Bayesian inference for causal effects: The role of randomization. The Annals
of Statistics, 6, 34â€“58.
Rubin, D.B. (1979). Discussion of â€˜Conditional Independence in Statistical Theoryâ€™, by A.P.
Dawid. The Journal of the Royal Statistical Society, Series B, 41, 27â€“28.
Rubin, D.B. (1980). Discussion of â€˜Randomization analysis of experimental data in the Fisher
randomization testâ€™, by Basu. Journal of the American Statistical Association, 75, 591â€“593.
Rubin, D.B. (1984). Progress Report on Project for Multiple Imputation of 1980 Codes. Report
for the U.S. Bureau of the Census.
Rubin, D.B. (1987). Multiple Imputation for Nonresponse in Surveys. John Wiley, New York.
Rubin, D.B. (1990a). Formal modes of statistical inference for causal effects. Journal of Statisti-
cal Planning and Inference, 25, 279â€“292.
Rubin, D.B. (1990b). Comment: Neyman (1923) and causal inference in experiments and
observational studies. Statistical Science, 5, 472â€“480.
Rubin, D.B. (2004). Multiple Imputation for Nonresponse in Surveys. Reprinted with appendices
as a â€œWiley Classics.â€ John Wiley, New York.
Rubin (2008). Statistical inference for causal effects, with emphasis on applications in epidemi-
ology and medical statistics. In: Handbook of Statistics: Epidemiology and Medical Statistics.
(ed. C.R. Rao, J.P. Mill and D.C. Rao). Elsevier, Amsterdam.
Rubin, D.B. and Schenker, N. (1987). Logit-based interval estimation for binomial data using
the Jeffreys prior. Sociological Methodology, 17, 131â€“144.
Tanner, M. and Wong, W. (1987). The calculation of posterior distribution by data augmentation
(with discussion). Journal of the American Statistical Association, 82, 528â€“550.
Tierney, L. (1994). Markov chains for exploring posterior distributions (with discussion). The
Annals of Statistics, 22, 1701â€“1762.

PART V
Natural and Engineering Sciences


Â·25Â·
Bayesian statistical methods for audio and
music processing
A. Taylan Cemgil, Simon J. Godsill, Paul Peeling and Nick Whiteley
25.1 Introduction
Computer-based music composition and sound synthesis date back to the ï¬rst
days of digital computation. However, despite recent technological advances in
synthesis, compression, processing and distribution of digital audio, it has not
yet been possible to construct machines that can simulate the effectiveness of
human listening â€“ for example, an expert human listener can accurately write
down a fairly complex musical score based solely on listening to the audio.
Statistical methodolgies are now migrating into human â€“ computer interaction,
computer games and electronic entertainment computing. Here, one ambi-
tious research goal focuses on computational techniques to equip computers
with musical listening and interaction capabilities. This is essential for the
construction of intelligent music systems and virtual musical instruments that
can listen, imitate and autonomously interact with humans. For ï¬‚exible inter-
action it is essential that music systems are aware of the semantic content of
the music, are able to extract structure and can organize information directly
from acoustic input. For generating convincing performances, they need to be
able to analyse and mimic master musicians. These outstanding technological
challenges motivate this research, in which fundamental modelling principles
are applied to gain as much information as possible from ambiguous audio
data.
Musical audio processing is a rather broad ï¬eld and the research is driven
by both scientiï¬c and technological motivations â€“ two related but distinct
goals. For technological needs, the primary motivation is to develop practical
engineering solutions to enhance classiï¬cation, denoising, source separation
or score transcription. The ultimate goal here is to construct computer systems
that display aspects of human, or super-human, performance levels in an auto-
mated fashion. In the second, the goal is to aid the scientiï¬c understanding of
cognitive processes behind the human auditory system (Moore 1997) and the
physical sound generation process of musical instruments or voices (Fletcher
and Rossing 1998).

712
The Oxford Handbook of Applied Bayesian Analysis
The starting point in this chapter is that in both contexts, scientiï¬c and tech-
nological, Bayesian statistical methods provide a sound formalism for making
progress. This is achieved via models which quantify prior knowledge about
the physical properties and semantics of sound, combined with powerful com-
putational methodology. The key equation, then, is Bayesâ€™ theorem and in the
context of audio processing it can be stated as
p(Structure|Audio Data) âˆp(Audio Data|Structure)p(Structure).
Thus inference is made from the posterior distribution for the hidden structure
given observed audio data. One of the strengths of this simple and intuitive
view of audio processing is that it uniï¬es a variety of tasks such as source
tracking, enhancement, transcription, separation, identiï¬cation or resynthesis
into a single Bayesian inference framework. The approach also inherits the
beneï¬t common to all applications of Bayesian statistical methods that the
problem formulation and computational solution strategy are well separated.
This is in contrast with many of the more heuristic and ad hoc approaches to
audio processing. Popular aproaches here involve the design of custom-built
algorithms for solving speciï¬c tasks, and in which the problem formulation
and computational solution are blended together, taking account of practical
and pragmatic considerations only. These techniques potentially miss out on
the generality and accuracy afforded by a well-deï¬ned Bayesian model and
associated estimation algorithms.
We ï¬rstly consider mainstream applications of audio signal processing, give
a very brief introduction to the properties of musical audio, and then proceed to
pose the principal challenges as Bayesian inference tasks.
25.1.1 Applications
A fundamental task that will be a focus of this paper is music-to-score tran-
scription (Cemgil 2004; Klapuri and Davy 2006). This involves the analysis
of raw audio signals to produce a musical score representation. This is one
of the most challenging and comprehensive tasks facing us in computational
music analysis, and one that is certainly ill-deï¬ned, since there are many
possible written scores corresponding to one performance. An expert human
listener could transcribe a relatively complex piece of musical audio but the
score produced would be dissimilar in many respects to that of the composer.
However, it would be reasonable to hope that the transcriber could generate
a score having similar pitches and durations to those of the composer. The
subtask of generating a pitch-and-duration map of the music is the main aim
of many so-called â€˜transcriptionâ€™ systems. Others have considered the task of
score generation from this point on and software is available commercially for
this highly subjective part of the process â€“ we will not consider it further here.

Bayesian Statistical Methods for Audio and Music Processing
713
Applications that require the transcription task include analysis of ethnomu-
sicological recordings, transcription of jazz and other improvised forms for
analysis or publication of performance versions, and transcriptions of rare or
historical pieces which are no longer available in the form of a printed score.
Apart from applications which directly require the full transcription there are
many applications, for example those below, which are fully or partially solved
as a result of a solution to the transcription problem.
Signal separation is a second fundamental challenge (HyvÃ¤rinen, Karhunen,
and Oja 2001; Virtanen 2006b) â€“ here we attempt to separate out individ-
ual instruments or notes from a polyphonic (many-note) mixture. This ï¬nds
application in many areas from sound remastering in the recording studio
through to karaoke (extraction of a principal vocal line from a source, leaving
just the accompaniment). Source separation ï¬nds much wider application of
course in non-musical audio, especially in hearing aids, see below. Instrument
classiï¬cation is a further important component of musical analysis systems,
i.e. the task of recognizing which instruments are playing at any given time
in a piece. A related concept is timbre determination â€“ extraction of the tonal
character of a pitched musical note (in coarse terms, is it harsh, sweet, bright,
etc.; Herrera-Boyer, Klapuri, and Davy 2006).
Finally, at the signal level, audio restoration and enhancement (Godsill and
Rayner 1998) form another key area. In this application the quality of an audio
source is enhanced, for example by reduction of background noise. This task
comes as a by-product of many model-based analysis tasks, such as source
separation above, since a noise-reduced version of the input signal will often
be available as one of the possible inferences from the Bayesian posterior
distribution.
The fundamental tasks above will ï¬nd use in many varied acoustical applica-
tions. For example, with vast amounts of audio data available digitally in on-line
repositories, it is not unreasonable to predict that almost all audio material will
be available digitally in the near future. This has rendered automated processing
of audio for sorting and choice of musical content an important and central
information processing task, affecting literally millions of end users. For ï¬‚exible
interaction it is essential that systems are able to extract structure and organize
information from the audio signal directly. Our view is that the associated fun-
damental computational problems require both a fresh look at existing signal
processing techniques and development of novel statistical methodologies.
25.1.2 Introduction to musical audio
The following discussion gives a basic introduction to some of the properties of
musical audio signals, following closely that of Godsill (2004). Musical audio
is highly structured, both in the time domain and in the frequency domain.

714
The Oxford Handbook of Applied Bayesian Analysis
Piano
Time
Amplitude
Viola
Piccolo
French horn
Cymbals
Congas
Frame index
Frequency index
Fig. 25.1 Some acoustical instruments, examples of typical time series and corresponding spectro-
grams (time varying magnitude spectra â€“ modulus of short time Fourier transform) computed with
FFT. (Audio data and images from RWCP Instrument samples database.)
In the time domain, tempo and beat specify the range of likely times where
note transitions occur. In the frequency domain, two levels of structure can be
considered. First, each note is composed of a fundamental frequency (related
to the â€˜pitchâ€™ of the note) and partials whose relative amplitudes determine
the timbre of the note. This frequency-domain description can be regarded as
an empirical approximation to the true process, which is in reality a complex
nonlinear time-domain system (McIntyre, Schumacher, and Woodhouse 1983;
Fletcher and Rossing 1998). The frequencies of the partials are approximately
integer multiples of the fundamental frequency, although this clearly doesnâ€™t
apply for instruments such as bells and tuned percussion. See Figure 25.1
for examples of pitched and percussive musical instruments and typical time
series. Second, several notes played at the same time form chords, or polyphony
(Figure 25.2). The fundamental frequencies of each note comprising a chord
are typically related by simple multiplicative rules. For example, a C major
chord may be composed of the frequencies 523 Hz, 659 Hz â‰ˆ5/4Ã—523 Hz
and 785 Hz â‰ˆ3/2Ã—523 Hz. Figure 25.3 shows the waveform for a simple
monophonic (single note) ï¬‚ute recording and Figure 25.4 shows the corre-
sponding time â€“ frequency spectrogram analysis (this may be auditioned at
www-sigproc.eng.cam.ac.uk/âˆ¼sjg/haba, where other extracts used in this
paper may also be found). In this both the temporal segmentation and the
frequency-domain structure are clearly visible on the plot. Focusing on a single
localized time frame, at around 2 s in the same extract, we can clearly see

Bayesian Statistical Methods for Audio and Music Processing
715
Piano + piccolo + cymbals
Fig. 25.2 Superposition. The time series and the magnitude spectrogram of the resulting signal
when some of the instruments play concurrently.
2
4
6
8
10
12
14
16
â€“1
â€“0.5
0
0.5
1
âˆ‘104
t/sec
Amplitude
Fig. 25.3 Time-domain waveform for a solo ï¬‚ute extract.
t/sec
f/Hz
0
200
400
600
800
1000
1200
0
0.5
1
1.5
2
âˆ‘104
0
5
10
15
20
25
Fig. 25.4 Time â€“ frequency spectrogram representation for the ï¬‚ute recording.

716
The Oxford Handbook of Applied Bayesian Analysis
0
500
1000
1500
2000
2500
3000
3500
4000
4500
102
103
104
105
106
Frequency
Amplitude
â€˜Partialsâ€™ or â€˜Harmonicsâ€™
Ï‰0
2Ï‰0
3Ï‰0
Fig. 25.5 Short-time Fourier analysis of a single frame of data from the ï¬‚ute extract.
the fundamental frequency component, labelled Ë˜0, and the partial stucture,
at frequencies 2Ë˜0, 3Ë˜0, ...of a single musical note in Figure 25.5. It is clear
from spectra such as Figure 25.5 that it will be possible to estimate the pitch
from single-note data that is well segmented in time (so that there is not signif-
icant overlap between more than one separate musical note within any single
segment). We will refer to pitch interchangeably with fundamental frequency
Ë˜0, although it should be noted that perceived pitch is a more complex function
of the fundamental and amplitudes and number of its harmonics. There are
many ways to achieve pitch detection, based on sample autocorrelation func-
tions, spectral peak locations, etc. Of course, real musical extracts donâ€™t usu-
ally arrive in conveniently segmented single-note form or extracts, and much
more complex structures need to be considered, as detailed in the sections
below.
25.1.3 Superposition and the Bayesian approach
In applications that involve acoustical and computational modelling of sound,
a fundamental obstacle is superposition, i.e. concurrent sound events (music,
speech or environmental sounds) are mixed and modiï¬ed due to reverberation
and noise present in the acoustic environment. This situation is of primary
importance in polyphonic music, in which several instruments sound simulta-
neously and one of the many possible processing goals is to separate or identify
the individual voices. In domains such as these, information about individual
sources cannot be directly extracted, owing to the superposition effect, and
signiï¬cant focus is given in the literature to source separation (HyvÃ¤rinen,
Karhunen, and Oja 2001), deconvolution and perceptual organization of sound
(Wang and Brown 2006).

Bayesian Statistical Methods for Audio and Music Processing
717
25.1.4 Fundamental audio processing tasks
From the above discussion of the challenges facing audio processing, some
fundamental tasks can be identiï¬ed for treatment by Bayesian techniques.
Firstly, we can hope to address the superposition task in a model-based fashion
by posing models that capture the behaviour of superimposed signals. These
are similar in ï¬‚avour to the latent factors analysed in some statistical modelling
problems. A generic model for observed data Y, under a linear superposition
assumption, will then be:
Y =
I

i=1
si
(25.1)
where the si represent each of the I individual audio sources present. We pose
this very basic model here as a single-channel observation model, although
it is straightforward to extend the model to the multichannel case, in which
case it will be usual to include also channel-speciï¬c mixing coefï¬cients. The
sources and data will typically be audio time series but can also represent
expansion coefï¬cients of the audio in some other domain such as the Fourier
or wavelet domain, as will be made clear in context later. We may render the
model a little more sophisticated by making the data a stochastic function of
the sources, and in this case we will specify some non-degenerate likelihood
function p(Y| I
i=1 si) that models an additive noise component in addition to
the desired signals.
We typically assume that the individual sources si are independent a priori.
They are parametrized by Ã‹i, which represent information about the sound
generation process for that particular source, including perhaps its pitch and
other characteristics (number of partials, etc.), encoded through a conditional
distribution and prior distribution for each source:
p(si, Ã‹i) = p(si|Ã‹i)p(Ã‹i).
Dependence between the Ã‹i, for example to model the harmonic relationships
of notes within a chord, can of course be included as desired when considering
the joint distribution of sources and parameters. To this model we can add
unknown hyperparameters  with prior p() in the usual way, and incorporate
model uncertainty through an additional prior distribution on the number of
components I. The speciï¬cation of suitable source models p(si|Ã‹i) and p(Ã‹i),
as well as the form of likelihood function p(Y| I
i=1 si), will form a substantial
part of the remainder of the paper.
Several fundamental inference tasks can then be identiï¬ed from this generic
model, including the source separation and polyphonic music transcription
tasks previously identiï¬ed.

718
The Oxford Handbook of Applied Bayesian Analysis
25.1.4.1 Source separation
In source separation the task is to infer the source signals si themselves, given
the observed signal Y. Collecting the sources together as S = {si}I
i=1 and the para-
meters as  = {Ã‹i}I
i=1, the Bayesian formulation of the problem can be stated,
under a ï¬xed number of sources I, as (see for example Mohammad-Djafari
1997; Knuth 1998; Rowe 2003; FÃ©votte and Godsill 2006; Cemgil, Fevotte, and
Godsill 2007)
p(S|Y) =
1
P(Y)

p(Y|S, )p(S|, )p()p()dd
(25.2)
where, under our deterministic model above in equation (25.1), the likelihood
function p(Y|S, ) will be degenerate. The marginal likelihood P(Y) plays a key
role when model order uncertainty is to be incorporated into the problem, for
example when the number of sources N is unknown and needs to be estimated
(Miskin and Mackay 2001). Additional considerations which may additionally
be included in the above framework include convolutive (ï¬ltered) and non-
stationary mixing of the sources â€“ both scenarios are of practical interest and
still pose signiï¬cant computational challenges. Once the posterior distribution
is computed by evaluating the integral, point estimates of the sources can be
obtained using suitable estimation criteria, such as marginal MAP or posterior
mean estimation, although in both cases one has to be especially careful with
the interpretation of expectations in models where likelihoods and priors are
invariant to source permutations.
25.1.4.2 Polyphonic music transcription
Music transcription refers to extraction of a human readable and interpretable
description from a recording of a music performance, see Figure 25.6. In
cases where more than a single musical note plays at a given time instant,
we term this task polyphonic music transcription and we are once again in the
superposition regime. The general task of interest is to infer automatically a
musical notation, such as the traditional western music notation, listing the
pitch values of notes, corresponding timestamps and other expressive infor-
mation in a given performance. These quantities will be encoded in the above
model through the parameters Ã‹i of each note present at a given time. Simple
models will encode only the pitch of the note in Ã‹i while more complex models
can include expressive information, instrument-speciï¬c characteristics and
timbre, etc.
Apart from being an interesting modelling and computational problem in
its own right, automated extraction of a score-like description is potentially
very useful in a broad spectrum of applications such as interactive music
performance systems, music information retrieval and musicological analysis

Bayesian Statistical Methods for Audio and Music Processing
719
t/sec
f/Hz
0
1
2
3
4
5
6
7
8
0
1000
2000
3000
4000
5000
0
10
20
Prelude no.7
dolce
F.Chopin
Fig. 25.6 Polyphonic music transcription. The task is to generate a human readable score as shown
below, given the acoustic input. The computational problem here is to infer pitch, number of notes,
rhythm, tempo, meter, time signature. The inference can be achieved online (ï¬ltering) or ofï¬‚ine
(smoothing), depending upon requirements.
of musical performances, not to mention as an aid to the source separation task
identiï¬ed above. However, in its most unconstrained form, i.e. when operating
on an arbitrary acoustical input, music transcription remains a very challenging
problem, owing to the wide variation in acoustical conditions and characteris-
tics of musical instruments. In spite of these difï¬culties, a practical engineering
solution is possible by careful incorporation of prior knowledge from cognitive
science, musicology, musical acoustics, and by use of computational techniques
from statistics and digital signal processing.
Music transcription is an inference problem in which we wish to ï¬nd a
musical score that is consistent with the encoded music. In this context, a score
can be contemplated as a collection of â€˜musical objectsâ€™ (e.g. note events) that
are rendered by a performer to generate the observed signal. The term â€˜musical
objectâ€™ comes directly from an analogy to visual scene analysis where a scene is
â€˜explainedâ€™ by a list of objects along with a description of their intrinsic proper-
ties such as shape, colour or relative position. We view music transcription from
the same perspective, where we wish to â€˜explainâ€™ individual samples of a music
signal in terms of a collection of musical objects and where each object has
a set of intrinsic properties such as pitch, tempo, loudness, duration or score
position. It is in this respect that a score is a high level description of music.
Musical signals have a very rich temporal structure, and it is natural to think
of them as being organized in a hierarchical way. At the highest level of this
organization, which we may call as the cognitive (symbolic) level, we have a
score of the piece, as, for instance, intended by a composer.1 The performers
add their interpretation to music and render the score into a collection of
1 In reality the music may be improvised and there may be actually not a written score. In this case
we replace the generative model with the intentions of the performer, which can still be expressed in our
framework as a â€˜virtualâ€™ musical score.

720
The Oxford Handbook of Applied Bayesian Analysis
Score
Expression
acc
=200
ppp
Piano-roll
Signal
Fig. 25.7 A hierarchical generative model for music transcription. In this model, an unknown
score is rendered by a performer into a â€˜piano rollâ€™. The performer introduces expressive timing
deviations and tempo ï¬‚uctuations. The piano roll is rendered into audio by a synthesis model. The
piano roll can be viewed as a symbolic representation, analogous to a sequence of MIDI events.
Given the observations, transcription can be viewed as Bayesian inference of the score. Somewhat
simpliï¬ed, the techniques described in this chapter can be viewed as inference techniques as applied
to subgraphs of this graphical model.
â€˜control signalsâ€™. Further down at the physical level, the control signals trigger
various musical instruments that synthesize the observed sound signal. We
illustrate these generative processes using a hierarchical graphical model (see
Figure 25.7), where the arcs represent generative links.
In describing music, we are usually interested in a symbolic representation
and not so much in the â€˜detailsâ€™ of the actual waveform. To abstract away from
the signal details we deï¬ne an intermediate layer that represents the control sig-
nals. This layer, that we call a â€˜piano rollâ€™, forms the interface between a symbolic
process and the actual signal process. Roughly, the symbolic process describes
how a piece is composed and performed. Conditioned on the piano roll, the
signal process describes how the actual waveform is synthesized. Conceptually,
the transcription task is then to â€˜invertâ€™ this generative model and recover back
the original score. As an intermediate and but still very challenging task, we
may try and invert back only as far as the piano roll.
25.1.5 Organization of the chapter
In Section 25.2, signal models for audio are developed in the time domain,
including some examples of their inference for a musical acoustics problem.

Bayesian Statistical Methods for Audio and Music Processing
721
Section 25.3 describes models in the frequency transform domain that lead
to greater computational tractability. In particular, we describe new depen-
dence structures across time and frequency that allow for very accurate prior
modelling for the audio. A ï¬nal conclusion section is followed by appendices
covering some basic methods and technical detail.
25.2 Time-domain models for audio
We begin by describing some basic note and chord models for musical audio,
based in the time domain. As already discussed, a basic property of most non-
percussive musical sounds is a set of oscillations at frequencies related to
the fundamental frequency Ë˜0. Consider for the moment a short-time frame
of musical audio data, denoted y(Ã™), in which note transitions do not occur.
This would correspond, for example, to the analysis of a single musical chord.
Throughout, we assume that the continuous time audio waveform y(Ã™) has been
discretised with a sampling frequency Ë˜s rad sâˆ’1, so that discrete time obser-
vations are obtained as yt = y(2ï£¿t/Ë˜s), t = 0, 1, 2, . . . , N âˆ’1. We assume that
y(Ã™) is bandlimited to Ë˜s/2 rad sâˆ’1, or equivalently that it has been preï¬ltered
with an ideal low-pass ï¬lter having cut-off frequency Ë˜s/2 rad sâˆ’1. We will not
consider for the moment the time evolution of one chord to the next, or of note
changes in a melody. This critical issue is treated in later sections.
The following model for, say, the ith note out of a chord comprising I notes
in total can be written as
si,t =
Mi

m=1
Â·m,i cos (mË˜0,it) + â€šm,i sin (mË˜0,it)
(25.3)
for t âˆˆ{0, . . . , N âˆ’1}. Here, Mi > 0 is the number of partials present in note
i,
;
Â·2
m,i + â€š2
m,i gives the amplitude of a partial and tanâˆ’1(â€šm,i/Â·m,i) gives the
phase of that partial. Note that Ë˜0,i âˆˆ(0, ï£¿) is here scaled for convenience â€“ its
actual frequency is Ë˜0,i
2ï£¿Ë˜s. The unknown parameters for each note are thus Ë˜0,i,
the fundamental frequency, Mi, the number of partials and Â·m,i, â€šm,i, which
determine the amplitude and phase of each partial.
The extension to the multiple note case is then straightforwardly obtained by
linear superposition of a number of notes:
yt =
I

i=1
si,t + vt
where vt is a random background noise component (compare this with the
deterministic mixture in equation 25.1). In this model vt will also have to model
any residual transient noise from the musical instruments themselves. We now

722
The Oxford Handbook of Applied Bayesian Analysis
have in addition an unknown parameter I, the number of notes present, plus
any unknown statistics of the background noise process.
Such a model is a reasonable approximation for many steady musical sounds
and has considerable analytical tractability, especially if a Gaussian form is
assumed for vt and for the priors on amplitudes Â· and â€š. Nevertheless, the pos-
terior distribution is highly non-Gaussian and multimodal, and sophisticated
computational tools are required to infer accurately from this model. This was
precisely the topic of the work in Walmsley, Godsill, and Rayner (1998, 1999),
where a reversible jump sampler was developed for such a model under the
above-mentioned Gaussian prior assumptions.
The basic form above is, however, over-idealized in a number of ways: prin-
cipally from the assumption of constant amplitudes Â· and â€š over time, and
in the ï¬xed integer relationships between partials, i.e. partial m in note i lies
exactly at frequency mË˜0,i. The modiï¬cation of the basic model to remove
these assumptions was the topic of our later work (Davy and Godsill 2002;
Godsill and Davy 2002; Davy, Godsill, and Idier 2006; Godsill and Davy 2005),
still within a reversible jump Monte Carlo framework. In particular, it is fairly
straightforward to modify the model so that the partial amplitudes Â· and â€š may
vary with time,
si,t =
Mi

m=1
Â·m,i,t cos (mË˜0,it) + â€šm,i,t sin (mË˜0,it)
(25.4)
and we typically expand Â·m,i,t and â€šm,i,t on a ï¬nite set of smooth basis functions
Â¯i,t with expansion coefï¬cients ai and bi:
Â·m,i,t =
J

j=1
aiÂ¯i,t, â€šm,i,t =
J

j=1
biÂ¯i,t.
In our work we have adopted 50%-overlapped Hamming windows for the basis
functions, see Figure 25.8, with support either chosen a priori by the user or
treated as a Bayesian random variable (Godsill and Davy 2005).
Alternative more general representations allow a fully stochastic variation
of Â·m,i,t in the state-space formulation. Further idealisations in these models
include the assumption of constant fundamental frequencies with time and the
Gaussian prior and noise assumptions, but in principle all can be addressed in
a principled Bayesian fashion.
25.2.1 A prior distribution for musical notes
Under the above basic time-domain model we need to assign prior distri-
butions over the unknown parameters for a single note in the mix, cur-
rently {Ë˜0,i, Mi, Â·i, â€ši}, where Â·i, â€ši are the vectors of parameters Â·m,i, â€šm,i,

Bayesian Statistical Methods for Audio and Music Processing
723
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time index, t
Ïˆ1,t
Ïˆ2,t
Ïˆ3,t
Ïˆ1,t
Ïˆ9,t
Fig. 25.8 Basis functions Â¯i,t, I = 9, 50% overlapped Hamming windows.
m = 1, 2, . . . , Mi. Under an assumed note system such as an equally tempered
Western note system, we can augment this with a note number index ni. A
suitable scheme is the MIDI note numbering system2 which labels middle C
(or â€˜C4â€™) as note number 60, and all other notes as integers relative to this â€“ the
A below this would be 57, for example, and the A above middle C (usually at
440 Hz in modern Western tuning systems) would be note number 69. Other
non-Western systems could also be encoded within variants of such a scheme.
The fundamental frequency would then be expected to lie â€˜closeâ€™ to the expected
frequency for a particular note number, allowing for performance and tuning
deviations from the ideal. Thus a prior for the observed fundamental frequency
Ë˜0,i can be constructed fairly straightforwardly. We adopt here a truncated log-
normal distribution for the noteâ€™s fundamental frequency:
p(log(Ë˜0,i)|ni)
âˆ

N

Ã(ni), Ã›2
Ë˜

,
log(Ë˜0,i) âˆˆ[(Ã(ni âˆ’1) + Ã(ni))/2, (Ã(ni) + Ã(ni + 1))/2)]
0,
otherwise
where Ã(n) computes the expected log-frequency of note number n, i.e. when
we are dealing with music in the equally tempered Western system,
Ã(n) = (n âˆ’69)/12 log(2) + log(440/Ë˜s)
(25.5)
where once again Ë˜s rad sâˆ’1 is the sampling frequency of the data. Assuming
p(n) is uniform for now, the resulting prior p(Ë˜0,i) is plotted in Figure 25.9,
capturing the expected clustering of note frequencies at semitone spacings
relative to A440.
2 See for example www.harmony-central.com/MIDI/doc/table2.

724
The Oxford Handbook of Applied Bayesian Analysis
â€“8
â€“6
â€“4
â€“2
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1
1.2
1.4
log (Ï‰0,i), in semitones relative to A440 Hz
Prior probability density
Fig. 25.9 Prior for fundamental frequency p(Ë˜0,i).
The prior model for a note is completed with two components. Firstly, a
prior for the number of partials, p(Mi|Ë˜0,i), is speciï¬ed as uniform over the
range {Mmin, . . . , Mmax}, with limits truncated to prevent partials at frequencies
greater than Ë˜s/2, the Nyquist rate. Secondly, a prior for the amplitude parame-
ters Â·i, â€ši must be speciï¬ed. This turns out to be quite crucial to the modelling
performance and here we initially proposed a Gaussian form. It is expected
however that partials at high frequencies will have lower energy than those at
lower frequencies, generally following a low-pass ï¬lter shape in the frequency
domain. Coefï¬cents Â·m,i and â€šm,i are then assigned independent Gaussian prior
distributions such that their amplitudes are assumed to decay with increasing
frequency of the partial number m. The general form of this is
p(Â·m,i, â€šm,i) = N

â€šm,i|0, g 2
i km

N

Â·m,i|0, g 2
i km

.
Here gi is a scaling factor common to all partials in a note and km is a
frequency-dependent scaling factor to allow for the expected decay with increas-
ing frequency for partial amplitudes. Following Godsill and Davy (2005) the
amplitudes are assumed to decay as follows:
km = 1/(1 + (Tm)ÃŒ)
where ÃŒ is a decay constant and T determines the cut-off frequency. Such a
model is based on empirical observations of the partial amplitudes in many
real instrument recordings, and essentially just encodes a low pass ï¬lter with
unknown cut-off frequency and decay rate. See for example the family of curves

Bayesian Statistical Methods for Audio and Music Processing
725
100
101
10â€“10
10â€“8
10â€“6
10â€“4
10â€“2
100
Partial number, m
Increasing Î½
Fig. 25.10 Family of km curves (log-log plot), T = 5, ÃŒ = 1, . . . , 10.
with T = 5, ÃŒ = 1, 2, . . . , 10, Figure 25.10. It is worth pointing out that this
model does not impose very stringent constraints on the precise amplitude
of the partials: the Gaussian distribution will allow for signiï¬cant departures
from the km = 1/(1 + (Tm)ÃŒ) rule, as dictated by the data, but it does impose a
generally low-pass shape to the harmonics across frequency. It is possible to
keep these parameters as unknowns in the MCMC scheme (see Godsill and
Davy 2005), although in the examples presented here we ï¬x these to appropri-
ately chosen values for the sake of computational simplicity. gi, which can be
regarded as the overall â€˜volumeâ€™ parameter for a note, is treated as an additional
random variable, assigned an inverted Gamma distribution for its prior. The
Gaussian prior structure outlined here for the Â· and â€š parameters is readily
extended to the time-varying amplitude case of equation (25.4), in which case
similar Gaussian priors are applied directly to the expansion coefï¬cients a and
b, see Davy, Godsill, and Idier (2006).
In the simplest case, a polyphonic model is then built by taking an indepen-
dent prior over the individual notes and the number of notes present:
p() = p(I)
I
i=1
p(Ã‹i)
where
Ã‹i = {ni, Ë˜0,i, Mi, Â·i, â€ši, gi}.
This model can be explored using MCMC methods, in particular the reversible
jump MCMC method (Green 1995), and results from this and related models

726
The Oxford Handbook of Applied Bayesian Analysis
0
2000
4000
6000
8000
10000
12000
14000
16000
â€“0.5
0
0.5
Waveformâ€“slow release
0
2000
4000
6000
8000
10000
12000
14000
16000
â€“0.5
0
0.5
Waveformâ€“fast release
Fig. 25.11 Waveforms for release transient on pipe organ. Top: slow release; bottom: fast release.
can be found in Godsill and Davy (2005) and Davy, Godsill, and Idier (2006).
In later sections, however, we discuss simple modiï¬cations to the generative
model in the frequency domain which render the computations much more
feasible for large polyphonic mixtures of sounds.
The models of this section provide a quite accurate time-domain description
of many musical sounds. The inclusion of additional effects such as inhar-
monicity and time-varying partial amplitudes (Godsill and Davy 2005; Davy,
Godsill, and Idier 2006) makes for additional realism.
25.2.2 Example: Musical transient analysis with the harmonic model
A useful case in point is the analysis of musical transients, i.e. the start or end
of a musical note, when we can expect rapid variation in partial amplitudes
with time. Here we take as an example a pipe organ transient, analysed under
different playing conditions: one involving a rapid release at the end of the
note, and the other involving a slow release, see Figure 25.11. There is some
visible (and audible) difference between the two waveforms, and we seek to
analyse what is being changed in the structure of the note by the release mode.
Such questions are of interest to acousticians and instrument builders, for
example.
We analyse these datasets using the prior distribution of the previous section
and the model of equation (25.4). A ï¬xed length Hamming window of duration
0.093 s was used for the basis functions. The resulting MCMC output can be
used in many ways. For example, examination of the expansion coefï¬cients
Â·i and â€ši allows an analysis of how the partials vary with time under each
playing condition. In both cases the reversible jump MCMC identiï¬es nine
signiï¬cant partials in the data. In Figures 25.12 and 25.13 we plot the ï¬rst ï¬ve
(m = 1, . . . , 5) partial energies a2
m,i + b2
m,i as a function of time.

Bayesian Statistical Methods for Audio and Music Processing
727
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m=1
Pipe organâ€“slow release
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m=2
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m =3
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m =4
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
Expansion coefficient i
m =5
Fig. 25.12 Magnitudes of partials with time: slow release.
Examining the behaviour from the MCMC output we can see that the third
partial is substantially elevated during the slow release mode, between coef-
ï¬cients i = 30 to 40. Also, in the slow release mode, the fundamental fre-
quency (m = 1) decays at a much later stage relative to, say, the ï¬fth partial,
which itself decays more slowly in that mode. One can also use the model
output to perform signal modiï¬cation; for example time stretching or pitch
shifting of the transient are readily achieved by reconstructing the signal using
the MCMC-estimated parameters but modifying the Hamming window basis
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m=1
Pipe organâ€“fast release
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m= 2
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m =3
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
m=4
0
10
20
30
40
50
60
70
80
90
0
0.02
0.04
Expansion coefficient i
m=5
Fig. 25.13 Magnitudes of partials with time: fast release.

728
The Oxford Handbook of Applied Bayesian Analysis
function length (for time-stretching) or reconstructing with modiï¬ed fun-
damental frequency Ë˜0, see www-sigproc.eng.cam.ac.uk/Ëœsjg/haba. The
details of our reversible jump MCMC scheme are quite complex, involving a
combination of specially designed independence Metropolis â€“ Hastings pro-
posals and random-walk-style proposals for the note frequency variables. In the
frequency-domain models described in Section 25.3 we use essentially the same
MCMC scheme, with simpler likelihood functions â€“ some more details of the
proposals used are given there.
25.2.3 State-space models
A more general and potentially more realistic modelling of audio in the time
domain is given by the state-space formulation â€“ essentially extending the
sinusoidal models so far considered to allow for dynamic evolution with time.
Speciï¬cally these models are readily amenable to inclusion of note change-
points, stochastic amplitude/frequency variations and polyphonic music. For
space reasons we do not include any detailed discussion here but the inter-
ested reader is referred to Cemgil, Kappen, and Barber (2006) and Cemgil
(2007). Such state-space models are quite accurate for many examples of audio,
although they show some non-robust properties in the case of signals which
are far from steady-state oscillation and for instruments which do not closely
obey the laws described above. Perhaps more critically, for large polyphonic
mixes of many notes, each having potentially many partials, the computa-
tions â€“ in particular the calculation of marginal likelihood terms in the pres-
ence of many Gaussian components Â·i and â€ši â€“ can become very expensive.
Computing the marginal likelihood is costly as this requires computation of
Kalman ï¬ltering equations for a large state space (that scales with the number
of tracked harmonics) and for very long time series (as typical audio signals
are sampled at 44.1 kHz). Hence, either efï¬cient approximations need to be
developed or simpliï¬ed models need to be constructed. The latter approach
is taken by frequency domain models which we will review in the following
section.
25.3 Frequency-domain models
The preceding sections described various time-domain models for musical
audio based on sinusoidal modelling. In this section we at least partially
bypass the computational issues of the time-domain models by working with
approximate models in the frequency domain. These allow for direct likelihood
calculations without resorting to expensive matrix inversions and determinant
calculations. Later in the chapter these models will be elaborated further to
give sophisitcated Bayesian non-negative matrix factorization algorithms which

Bayesian Statistical Methods for Audio and Music Processing
729
are capable of learning the structure of the audio events in a semi-blind
fashion. Here initially, though, we work with simple model-based structures
in the frequency-domain that are analogous to the time-domain priors of the
Section 25.2. There are several routes to a frequency-domain representation,
including multiresolution transforms, wavelets, etc., though here we use a
simple windowed discrete Fourier transform as examplar. We now propose two
versions of a frequency-domain likelihood model, both of which bypass the
main computational burden of the high-dimensional time-domain Gaussian
models.
25.3.1 Gaussian frequency-domain model
The ï¬rst model proposed is once again a Gaussian model. In the frequency
domain we will have typically complex-valued expansion coefï¬cients of the data
on a one-dimensional lattice of frequency values ÃŒ âˆˆN, i.e. a set of spectrum
values yÃŒ. The assumption is that the contribution of each musical source term
to the expansion coefï¬cients is as independent zero-mean (complex) Gaussians,
with variance determined by the parameters of the musical note:
si,ÃŒ âˆ¼NC(0, ÃŽÃŒ(Ã‹i))
where Ã‹i = {ni, Ë˜0,i, Mi, gi} has the same interpretation as for the earlier time-
domain model, but now we can neglect the Â· and â€š coefï¬cients since the
random behaviour is now directly modelled by si,ÃŒ. This is a very natural for-
mulation for generation of polyphonic models since we can add a number of
sources together to make a single complex Gaussian data model:
yÃŒ âˆ¼NC(0, Sv,ÃŒ +
I

i=1
ÃŽÃŒ(Ã‹i)).
Here, Sv,ÃŒ > 0 models a Gaussian background noise component in a manner
analogous to the time-domain formulationâ€™s vt and it then remains to design
the positive-valued â€˜templateâ€™ functions ÃŽ. Once again, Figure 25.5 gives some
guidance as to the general characteristics required. We then model the template
using a sum of positive valued pulse waveforms Ë†ÃŒ, shifted to be centred at the
expected partial position, and whose amplitude decays with increasing partial
number:
ÃŽÃŒ(Ã‹i) =
Mi

m=1
g 2
i kmË†ÃŒâˆ’mË˜0,i
(25.6)
where km, gi and Mi have exactly the same interpretation as in the time-domain
model. An example template construction is shown in Figure 25.14, in which a
Gaussian pulse shape has been utilized.

730
The Oxford Handbook of Applied Bayesian Analysis
0.2
0.4
0.6
0.8
1
1.2
1.4
0
10
20
30
40
50
Normalised frequency
Fig. 25.14 Template function ÃŽÃŒ(Ã‹i) with Mi = 8, Ë˜0,i = 0.71, Gaussian pulse shape.
25.3.2 Point process frequency-domain model
The Gaussian frequency-domain model requires a knowledge of the conditional
distribution for the whole range of spectrum values. However, the salient
features in terms of pitch estimation appear to be the peaks of the spectrum
(see Figure 25.5). Hence a more parsimonious likelihood model might work
only with the peaks detected from the Fourier magnitude spectrum. Thus we
propose, as an alternative to the Gaussian spectral model, a point process model
for the peaks in the spectrum. Speciï¬cally, if the peaks in the spectrum of an
individual note are assumed to be drawn from a one-dimensional inhomoge-
neous Poisson point process having intensity function ÃŽÃŒ(Ã‹i) (considered as
a function of continuous frequency ÃŒ), then the combined set of peaks from
many notes may be combined, under an independence assumption, to give a
Poisson point process whose intensity function is the sum of the individual
intensities (Grimmett and Stirzaker 2001). Suppose we detect a set of peaks in
the magnitude spectrum {p j}J
j=1, ÃŒmin < p j < ÃŒmax. Then the likelihood may be
readily computed using:
p

{p j}J
j=1, J |

= Po(J |Z())
J
j=1

Sv,p j + I
i=1 ÃŽp j (Ã‹i)

Z()
where Z() =

 ÃŒmax
ÃŒmin (Sv,ÃŒ + I
i=1 ÃŽÃŒ(Ã‹i))dÃŒ is the normalizing constant for the
overall intensity function. Here once again we include a background intensity
function Sv,ÃŒ which models â€˜false detectionsâ€™, i.e. detected peaks that belong
to no existing musical note. The form of the template functions ÃŽ can be very
similar to that in the Gaussian frequency model, equation (25.6). A modiï¬ed
form of this likelihood function was successfully applied for chord detection
problems by Peeling, Li, and Godsill (2007).

Bayesian Statistical Methods for Audio and Music Processing
731
25.3.3 Example: Inference in the frequency-domain models
The frequency-domain models provide a substantially faster likelihood calcula-
tion than the earlier time-domain models, allowing for rapid inference in the
presence of signiï¬cantly larger chords and tone complexes. Here we present
example results for a tone complex containing many different notes, played on
a pipe organ. Analysis is performed on a very short segment of 4096 data points,
sampled at a rate of Ë˜s = 2ï£¿Ã— 44, 100 rad sâˆ’1 â€“ hence just under 0.1 s of data,
see Figure 25.15. From the score of the music we know that there are four notes
simultaneously playing: C5, Fâ™¯5, B5, and D6, or MIDI note numbers 72, 78,
83 and 86. However, the mix is complicated by the addition of pipes one octave
below and one or more octaves above the principal pitch, and hence we have at
least 12 notes present in the complex, MIDI notes 60, 66, 71, 72, 74, 78, 83, 84,
86, 90, 95, and 98. Since the upper octaves share all of their partials with notes
from one or more octaves below, it is not clear whether the models will be able
to distinguish all of the sounds as separate notes. We run the frequency-domain
models using the prior framework of Section 25.2.1 and a reversible jump
MCMC scheme of the same form as that used in the previous transient analysis
example. Firstly, using the Gaussian frequency-domain model of Section 25.3.1,
the MCMC burn-in for the note number vector n = [n1, n2, . . . , nI] is shown in
Figure 25.16. This is a variable-dimension vector under the reversible jump
MCMC and we can see notes entering or leaving the vector as iterations pro-
ceed. We can also see large moves of an octave (Â±12 notes) or a ï¬fth (+7 or âˆ’5
notes), corresponding to specialized Metropolis â€“ Hastings moves which centre
their proposals on the octave or ï¬fth as well as the locality of the current note. As
is typical of these models, the MCMC becomes slow-moving once converged to
a good mode of the distribution and further large moves only occur occasionally.
There is a good case here for using adaptive or population MCMC schemes
to improve the properties of the MCMC. Nevertheless, convergence is much
0
500
1000
1500
2000
2500
3000
3500
4000
4500
â€“0.2
â€“0.15
â€“0.1
â€“0.05
0
0.05
0.1
0.15
0.2
Time (samples)
yt
Fig. 25.15 Audio waveform â€“ single chord data.

732
The Oxford Handbook of Applied Bayesian Analysis
0
500
1000
1500
2000
2500
3000
50
60
70
80
90
100
110
MCMC Iteration No. 
MIDI Note number (Middle C = 60)
Fig. 25.16 Evolution of the note number vector with iteration number â€“ single chord data. Gaussian
frequency-domain model.
faster than for the earlier proposed time-domain models, particularly in terms
of the model order sampling, which was here initialized at I = 1, i.e. one single
note present at the start of the chain. Specialized independence proposals have
also been devised, based on simple pitch estimation methods applied to the
raw data. These are largely responsible for the initiation of new notes in the
MCMC chain. In this instance the MCMC has identiï¬ed correctly seven out
of the (at least) 12 possible pitches present in the music: 60, 66, 71, 72, 74,
78, 86. The remaining ï¬ve unidentiï¬ed pitches share all of their partials with
lower pitches estimated by the algorithm, and hence it is reasonable that they
remain unestimated. Examination of the discrete Fourier magnitude spectrum
(Figure 25.17) shows that the higher pitches (with the possible exception of
n7 = 83, whose harmonics are modelled by n3 = 71) are generally buried at very
low amplitude in the spectrum and can easily be absorbed into the model for
pitches one or more octaves lower in pitch.
We can compare these results with those obtained using the Poisson model of
Section 25.3.2. The MCMC was run under identical conditions to the Gaussian
model and we plot the equivalent note index output in Figure 25.18. Here we
see that fewer notes are estimated, since the basic point process model takes no
account of the amplitudes of the peaks in the spectrum, and hence is happy
to assign all harmonics to the lowest possible fundamental pitch. The four
predominant pitches estimated are the four lowest fundamentals: 60, 66, 71
and 74. The sampler is, however, generally more mobile and we see a better
and more rapid exploration of the posterior.

Bayesian Statistical Methods for Audio and Music Processing
733
10â€“1
100
0
5
10
15
20
25
30
Frequency (log scale)
Amplitude
n1= 60
n2=66
n3=71
n4=72
n12=98
n11=95
n5=74
n10=90
n9=86
n8=84
n6=78
n7=83
Fig. 25.17 Discrete Fourier magnitude spectrum for 12-note chord. True note positions marked with
a pentagram.
0
1000
2000
3000
4000
5000
6000
7000
8000
40
50
60
70
80
90
100
110
MCMC iteration number
MIDI note number (Middle C= 60)
Fig. 25.18 Evolution of the note number vector with iteration number â€“ single chord data. Poisson
frequency-domain model.
25.3.4 Further prior structures for transform domain representations
In audio processing, the energy content of a signal across frequencies is time-
varying and hence it is natural to model audio as an evolving process with a
time-varying power spectral density in the time-frequency plane and several
prior structures are proposed in the literature for modelling the expansion
coefï¬cients (Reyes-Gomez, Jojic, and Ellis 2005; Wolfe, Godsill, and Ng 2004;
FÃ©votte, Daudet, Godsill, and TorrÃ©sani 2006). The central idea is to choose a
latent variance model varying over time and frequency bins
sÃŒ,k|qÃŒ,k âˆ¼N(sÃŒ,k; 0, qÃŒ,k)
where the normal is interpreted either as complex Gaussian or real Gaussian
depending on the transform used â€“ the Fourier representation is complex, the
discrete sine/cosine representation is real. In Wolfe, Godsill, and Ng (2004), the

734
The Oxford Handbook of Applied Bayesian Analysis
following structure is proposed under the name Gabor regression. The variance
parameters qÃŒ,k are treated as independent conditional upon a lattice of activ-
ity variables rÃŒ,k which are modelled as dependent using Markov chains and
Markov random ï¬elds:
qÃŒ,k|rÃŒ,k âˆ¼[rÃŒ,k = on] IGa(qÃŒ,k; a, b/a) + [rÃŒ,k = off] â€°(qÃŒ,k).
Moreover, the joint distribution over the latent indicators r = r0:Wâˆ’1,0:K âˆ’1 is
taken as a pairwise Markov random ï¬eld (MRF) where u denotes a double index
u = (ÃŒ, k)
p(r) âˆ

(u,uâ€²)âˆˆE
Ë†(ru,ruâ€²).
Several MRF constructions are considered, including Markov chains across
time or frequency and Ising-type models.
25.3.5 Gamma chains and ï¬elds
An alternative model is introduced by Cemgil and Dikmen (2007) and Cemgil
et al. (2007), where a Markov random ï¬eld is directly placed on the variance
terms as
p(q) =

dÃŽp(q, ÃŽ)
using a so-called Gamma ï¬eld.
To understand the construction of a Gamma ï¬eld, it is instructive to look
ï¬rst at a chain, where we have an alternating sequence of Gamma and inverse
Gamma random variables
qu|ÃŽu âˆ¼IGa(qu; aq, aqÃŽ)
ÃŽu+1|qu âˆ¼Ga(ÃŽu+1; aÃŽ, qu/aÃŽ).
Note that this construction leads to conditionally conjugate Markov blankets
that are given as
p(qu|ÃŽu, ÃŽu+1) âˆIGa(qu; aq + aÃŽ, aqÃŽu + aÃŽÃŽu+1)
p(ÃŽu|quâˆ’1, qu) âˆGa

ÃŽu; aÃŽ + aq, aÃŽq âˆ’1
uâˆ’1 + aqq âˆ’1
u

.
Moreover it can be shown that any pair of variables qi and q j are positively
correlated, and qi and ÃŽk are negatively correlated. Note that this is a particular
type of stochastic volatility model useful for characterization of non-stationary
behaviour observed in, for example, ï¬nancial time series (Shepard 2005).
We can represent a chain by a graphical model where the edge set is E =
{(u, u)} âˆª{(u, u + 1)}. Considering the Markov structure of the chain, we deï¬ne
a Gamma ï¬eld p(q, ÃŽ) as a bipartite undirected graphical model consisting of
the vertex set V = VÃŽ âˆªVq, where partitions VÃŽ and Vq denotes the collection of
variables ÃŽ and q that are conditionally distributed Ga and IGa respectively.

Bayesian Statistical Methods for Audio and Music Processing
735
Fig. 25.19 Possible model topologies for gamma ï¬elds. White and grey nodes correspond to Vq and
VÃŽ nodes respectively. The horizontal and vertical axis corresponds to frequency ÃŒ and frame index k.
Each model describes how the prior variances are coupled as a function of time â€“ frequency index.
For example, the ï¬rst model from the left corresponds to a source model with â€˜spectral continuityâ€™,
the energy content of a given frequency band changes only slowly. The second model is useful for
modelling impulsive sources where energy is concentrated in time but spread across frequencies.
We deï¬ne an edge set E where an edge (u, uâ€²) âˆˆE such that ÃŽu âˆˆVÃŽ and quâ€² âˆˆ
Vq, if the joint distribution admits the following factorization
p(ÃŽ, q) âˆ
âŽ›
âŽ
uâˆˆVÃŽ
ÃŽ
(
uâ€² au,uâ€²âˆ’1)
u
âŽž
âŽ 
âŽ›
âŽ
uâ€²âˆˆVq
q
âˆ’(
u au,uâ€²+1)
u
âŽž
âŽ 
âŽ›
âŽ
(u,uâ€²)âˆˆE
exp

âˆ’au,uâ€² ÃŽu
quâ€²
âŽž
âŽ .
Here, the shape parameters play the role of coupling strengths; when au,uâ€² is
large, adjacent nodes are correlated. Given, this construction, various signal
models can be developed â€“ see Figure 25.19.
25.3.6 Models based on latent variance/intensity factorization
The various Markov random ï¬eld priors of the previous section introduced
couplings between the latent variances qÃŒ,k. Another alternative and powerful
approach is to decompose the latent variances as a product. We deï¬ne the
following hierarchical model (see Figure 25.21)
sÃŒ,k âˆ¼N(sÃŒ,k; 0, qÃŒ,k)
qÃŒ,k = tÃŒvk
(25.7)
tÃŒ âˆ¼IGa

tÃŒ; at
ÃŒ, at
ÃŒbt
ÃŒ

vk âˆ¼IGa

vk; av
k, av
kbv
k

.
Such models are also particularly useful for modelling acoustic instruments.
Here, the tÃŒ variables can be interpreted as average expected energy template as
a function of frequency bin. At each time index this template is modulated by vÃŒ,
to adjust the overall volume. An example is given in Figure 25.20 to represent
a piano sound. The template gives the harmonic structure of the pitch and the
excitation characterises the time varying energy.
A simple factorial model that uses the Gamma chain prior models introduced
in Section 25.3.5 is constructed as follows:
xÃŒ,k =

i
sÃŒ,i,k
sÃŒ,i,k âˆ¼N(sÃŒ,i,k; 0, qÃŒ,i,k)
Q = {qÃŒ,i,k} âˆ¼p(Q|t). (25.8)
The computational advantage of this class of models is the conditional inde-
pendence of the latent sources given the latent variance variables. Given the

736
The Oxford Handbook of Applied Bayesian Analysis
10
20
30
40
50
60
20
40
60
80
100
120
v/Frame
v/Frequency
[MDCT] coefficients
Template tv
Excitation vk
Standard deviation qv,k
Template tv
Excitation vk
Intensity qv,k
Fig. 25.20 (Left) the spectrogram of a piano |sÃŒ,k|2. (Middle) estimated templates and excitations
using the conditionally Gaussian model deï¬ned in equation (25.7), where qÃŒ,k is the latent variance.
(Right) estimated templates and excitations using the conditionally Poisson model deï¬ned in the
next section.
vi,0
Â· Â· Â·
vi,k
Â· Â· Â·
tv,i
tv,i
sv
i,0
sv
i,0
Â· Â· Â·
sv
i,k
sv
i,Kâ€“1
sv
i,k
sv
i,Kâ€“1
vi,Kâ€“1
vi,0
vi,k
vi,Kâ€“1
xv,0
xv,k
xv,Kâ€“1
Â· Â· Â·
v=0 . . . W â€“1
y0
y0
yk
yk
yKâ€“1
yKâ€“1
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
i=1 . . . I
v=0 . . . Wâ€“1
Fig. 25.21 (Left) latent variance/intensity models in product form (equation 25.7). Hyperparameters
are not shown. (Right) factorial version of the same model, used for polyphonic estimation as used
in Section 25.3.7.1.
latent variances and data, the posterior of the sources is a product of Gaussian
distributions. In particular, the individual marginals are given in closed form as
p(sÃŒ,i,k|X, Q) = N(sÃŒ,i,k; ÃÃŒ,i,kxÃŒ,k, qÃŒ,i,k(1 âˆ’ÃÃŒ,i,k))
ÃÃŒ,i,k = qÃŒ,i,k
A 
iâ€²
qÃŒ,iâ€²,k.
This means that if the latent variances can be estimated, source separation can
be easily accomplished. The choice of prior structures on the latent variances
p(Q|Â·) is key here.
Below we illustrate this approach in single channel source separation
for transient/harmonic decomposition. Here, we assume that there are two
sources i = 1, 2. The prior variances of the ï¬rst source i = 1 are tied across
time frames using a Gamma chain and aims to model a source with harmonic
continuity. The prior has the form 	
ÃŒ p(qÃŒ,i=1,1:K ). This model simply assumes

Bayesian Statistical Methods for Audio and Music Processing
737
Time (Ï„)
Frequency bin (Î½)
Xorg
Shor
Sver
Fig. 25.22 Single channel source separation example, left to right, log-MDCT coefï¬cients of the
original signal and reconstruction with horizontal and vertical IGMRF models.
that for a given source the amount of energy in a frequency band stays roughly
constant. The second source i = 2 is tied across frequency bands and has
the form 	
k p(q1:W,i=2,k); this model tries to capture impulsive/percusive
structure (for example compare the piano and conga examples in Figure 25.1).
The model aims to separate the sources based on harmonic continuity and
impulsive structure.
We illustrate this approach to separate a piano sound into its constituent
components and drum separation. We assume that J = 2 components are
generated independently by two gamma chain models with vertical and hori-
zontal topology. In Figure 25.22, we observe that the model is able to separate
transients and harmonic components. The sound ï¬les of these results can be
downloaded and listened at the following url: http://www-sigproc.eng.cam.
ac.uk/sjg/haba, which is perhaps the best way assess the sound quality.
The variance/intensity factorization models described in equation 25.7 have
also straightforward factorial extensions
xÃŒ,k =

i
sÃŒ,i,k
sÃŒ,i,k âˆ¼N(sÃŒ,i,k; 0, qÃŒ,i,k)
qÃŒ,i,k = tÃŒ,ivi,k
(25.9)
T = {tÃŒ,i} âˆ¼p(T|t)
V = {vi,k} âˆ¼p(V|v).
(25.10)
If we integrate out the latent sources, the marginal is given as
xÃŒ,k âˆ¼N

xÃŒ,k; 0,

i
tÃŒ,ivi,k

.
Note that, as 
i tÃŒ,ivi,k = [TV]ÃŒ,k, the variance â€˜ï¬eldâ€™ Q is given compactly as
the matrix product Q = TV. This resembles closely a matrix factorisation and
is used extensively in audio modelling. In the next section, we discuss models
of this type.

738
The Oxford Handbook of Applied Bayesian Analysis
25.3.7 Non-negative matrix factorization models
Up to this point we have described conditionally Gaussian models. Recently,
a popular branch of source separation and analysis of musical audio literature
has focused on non-negativity of the magnitude spectrogram X = {xÃŒ,Ã™} with
xÃŒ,Ã™ â‰¡âˆ¥sÃŒ,kâˆ¥1/2
2 , where sÃŒ,k are expansion coefï¬cients obtained from a time â€“
frequency expansion. The basic idea of NMF is representing a spectrogram by
enforcing a factorization as X â‰ˆTV where both T and V are matrices with
positive entries (Smaragdis and Brown 2003; Abdallah and Plumbley 2006;
Virtanen 2006a; Kameoka 2007; Bertin, Badeau, and Richard 2007; Vincent,
Bertin, and Badeau 2008). In music signal analysis, T can be interpreted as a
codebook of templates, corresponding to spectral shapes of individual notes and
V is the matrix of activations, somewhat analogous to a musical score. Often,
the following objective is minimized:
(T, V)âˆ—= min
T,V D(X||TV)
(25.11)
where D is the information (Kullback â€“ Leibler) divergence, given by
D(X||) =

ÃŒ,Ã™

xÃŒ,Ã™ log xÃŒ,Ã™
ÃŽÃŒ,Ã™
âˆ’xÃŒ,Ã™ + ÃŽÃŒ,Ã™

.
(25.12)
Using Jensenâ€™s inequality (Cover and Thomas 1991) and concavity of log x, it
can be shown that D(Â·) is non-negative and D(X||) = 0 if and only if X = .
The objective in (25.11) could be minimized by any suitable optimization
algorithm. Lee and Seung (2000) have proposed an efï¬cient variational bound
minimization algorithm that has attractive convergence properties. that has
been since successfully applied to various applications in signal analysis and
source separation.
It can also be shown that the minimization algorithm is in fact an EM
algorithm with data augmentation (Cemgil 2008). More precisely, it can be
shown that minimizing D w.r.t. T and V is equivalent ï¬nding the ML solution
of the following hierarchical model
xÃŒ,k =

i
sÃŒ,i,k
sÃŒ,i,k âˆ¼Po(sÃŒ,i,k; 0, ÃŽÃŒ,i,k)
ÃŽÃŒ,i,k = tÃŒ,ivi,k
(25.13)
tÃŒ,i âˆ¼Ga

tÃŒ,i; at
ÃŒ,i, bt
ÃŒ,i/at
ÃŒ,i

vi,k âˆ¼Ga

vi,k; av
i,k, bv
i,k/av
i,k

. (25.14)
Note that this model is quite distinct from the Poisson point model used in
Section 25.3.2 since it models each time â€“ frequency coefï¬cient as a Poisson
random variable, while the previous approach models detected peaks in the
spectrum as a spatial point process.

Bayesian Statistical Methods for Audio and Music Processing
739
The computational advantage of this model is the conditional independence
of the latent sources given the variance variables. In particular, we have
p(sÃŒ,i,k|X, T, V) = Bi(sÃŒ,i,k; xÃŒ,k, ÃÃŒ,i,k)
ÃÃŒ,i,k = ÃŽÃŒ,i,k/

iâ€²
ÃŽÃŒ,iâ€²,k
This means that if the latent variances can be estimated somehow, source
separation can be easily accomplished as E(s)Bi(s;x,Ã) = Ãx. It is also possible to
estimate the marginal likelihood p(X) by integrating out all of the templates
and excitations. This can be done via Gibbs sampling or more efï¬ciently using
a variational approach that we outline in Appendix A.
25.3.7.1 Example: Polyphonic pitch estimation
In this section, we illustrate Bayesian NMF for polyphonic pitch detection. The
approach consists of two stages:
1. Estimation of hyperparameters given a corpus of piano notes.
2. Estimation of templates and excitations given new polyphonic data and
ï¬xed hyperparameters.
In the ï¬rst stage, we estimate the hyperparameters at
ÃŒ,i = at
i and bt
ÃŒ,i (see equa-
tion 25.14), via maximization of the variational bound given in equation 25.20.
Here, the observations are matrices Xi; a spectrogram computed given each
note i = 1 . . . I. In Figure 25.23, we show the estimated scale parameters bt
ÃŒ,i as
i/Key index
Î½/Frequency index
Estimated scale parameters of the template prior
10
20
30
40
50
60
70
80
50
100
150
200
250
300
350
400
450
500
Fig. 25.23 Estimated template hyperparameters bt
ÃŒ,i.

740
The Oxford Handbook of Applied Bayesian Analysis
Note index
Frame
Piano roll
Ï„/Frame
Î½/Frequency bin
Log [MDCT] coefficients
50
100
150
200
250
50
100
150
200
250
300
350
400
450
500
Fig. 25.24 The ground truth piano roll and the spectrum of the polyphonic data.
a function of frequency band ÃŒ and note index i. The harmonic structure of each
note is clearly visible.
To test the approach, we synthesize a music piece (here, a short segment
from the beginning of FÃ¼r Elise by Beethoven), given a MIDI piano roll and
recordings of isolated notes from a piano by simply appropriately shifting each
time series and adding. The piano roll and the the spectrogram of the synthe-
sized audio are shown in Figure 25.24. The pitch detection task is infering the
excitations given the hyperparameters and the spectrogram.
The results are shown in Figure 25.25. The top ï¬gure shows the exci-
tations estimated give the prior shown in equation 25.14. The notes are
visible here but there are some artifacts. The middle ï¬gure shows results
from a model where excitations are tied across time using a Gamma chain
introduced in Section 25.3.5. This prior is highly effective here and we
are able to get a more clearer picture. The bottom ï¬gure displays results

Bayesian Statistical Methods for Audio and Music Processing
741
Ï„/Frame index
Pitch
Excitations (weak coupling)
20
40
60
80
100
120
5
10
15
20
25
30
35
40
Pitch
Ï„/Frame index
20
40
60
80
100
120
5
10
15
20
25
30
35
40
50
100
150
200
250
5
10
15
20
25
30
35
40
Fig. 25.25 Polyphonic pitch detection. Estimated expected excitations. (Top) uncoupled excitations.
(Middle) tied excitations using a Gamma chain, ground truth shown in white. (Bottom) excitations
estimated from a guitar using the hyperparameters estimated from a piano â€“ ground truth shown in
black.
obtained from a real recording of FÃ¼r Elise, performed on electric guitar.
Interestingly, whilst we are still using the hyperparameters estimated from
a piano, the inferred excitations show signiï¬cant overlap with the original
score.

742
The Oxford Handbook of Applied Bayesian Analysis
25.4 Conclusions
In this chapter we have described recently proposed Bayesian methods for
analysis of audio signals. The Bayesian models exhibit complex statistical struc-
ture and in practice, highly adaptive and powerful computational techniques
are needed to perform inference. We have reviewed and developed some of
these statistical models and described how various problems in audio and
music processing can be cast into the Bayesian inference framework. We have
also illustrated inference methods based on Monte Carlo simulation or other
deterministic techniques (such as mean ï¬eld, variational Bayes) originating in
statistical physics to tackle computational problems posed by inference in these
models. We described models in both the time domain and transform domains,
the latter typically offering greater computational tractability and modelling
ï¬‚exibility at the expense of some accuracy in the models.
The Bayesian approach has two key advantages over more traditional engi-
neering solutions: it provides both a uniï¬ed methodology for probabilistic
model construction and a framework for algorithm development. Apart from
the pedagogical advantages (such as highlighting algorithmic similarities,
convergence characteristics and computational requirements), the framework
facilitates development of sophisticated models and the automation of code
generation procedures. We believe that the ï¬eld of computer hearing, which is
still in its infancy compared to topics such as computer vision and speech recog-
nition, has great potential for advancement in coming years, with the advent
of powerful Bayesian inference methodologies and accompanying increases in
computational power.
Appendix
A. Broader context and background
Audio processing applications require efï¬cient inference in fairly complex hier-
archical Bayesian models. In statistics, the fundamental computational tools to
such high dimensional integrals are based on Markov chain Monte Carlo strate-
gies such as the Gibbs sampler (Gilks, Richardson, and Spiegelhalter 1996). The
main advantage of MCMC is its generality, robustness and attractive theoretical
properties. However, the method comes at the price of heavy computational
burden which may render it impractical for data intensive applications.
An alternative approach for computing the required integrals is based on
deterministic ï¬xed point iterations (Variational Bayes â€“ Structured Mean ï¬eld;
Ghahramani and Beal 2000; Wainwright and Jordan 2003; Bishop 2006). This
set of methods have direct links with the well-known expectation-maximization
(EM) type of algorithms. Variational methods have been extensively applied

Bayesian Statistical Methods for Audio and Music Processing
743
to various models for source separation by a number of authors (Attias
1999; Valpola 2000; Girolami 2001; Miskin and Mackay 2001; Hojen-Sorensen,
Winther, and Hansen 2002; Winther and Petersen 2006).
From an algorithmic point of view, the VB method can be viewed as a
â€˜deterministicâ€™ counterpart of the Gibbs sampler. Especially for models where
a Gibbs sampler is easy to construct (e.g. in models with conjugate priors
leading to known full conditionals) the VB method is equally easy to apply. Like
the Gibbs sampler, the framework facilitates generalization to more complex
models and to automation of code generation procedure. Moreover, the method
directly provides an approximation (a lower bound) to the marginal likelihood.
Although in general not much is known about how tight the bound is, there
is empirical evidence that for many models the bound can provide a good
approximation to an estimate obtained from Gibbs sampling via Chibâ€™s method
(Chib 1995).
A.1 Bounding marginal likelihood via variational Bayes
We sketch here the Variational Bayes (VB) (Ghahramani and Beal 2000; Bishop
2006) as a method to bound the marginal loglikelihood
LX() â‰¡log p(X|) = log

dTdVp(X, T, V|).
(25.15)
We ï¬rst introduce an instrumental distribution q(T, V).
LX() â‰¥

dT, dVq log p(X, T, V|)
q
(25.16)
= E(log p(X, V, T|))q + H[q] â‰¡BV B[q].
(25.17)
Here, H[q] denotes the entropy of q. From the general theory of EM we know
that the bound is tight for the exact posterior q(T, V) = p(T, V|X, ). The VB
idea is to assume a simpler form for the instrumental distribution by ignoring
some of the couplings present in the exact posterior. A natural candidate is a
factorized distribution
q(T, V) = q(T)q(V) â‰¡

Â·âˆˆC
qÂ·.
In the last equation, we have formally written the q distribution as a product
over variables from disjoint clusters Â· âˆˆC and C = {{T}, {V}} denotes the set of
disjoint clusters. Since in general the family of q distributions wonâ€™t include the
exact posterior density, we are no longer guaranteed to attain the exact marginal
likelihood LX(). Yet, the bound property is preserved and the strategy of
VB is to optimize the bound. Although the best q distribution respecting the
factorization is not available in closed form, it turns out that a local optimum

744
The Oxford Handbook of Applied Bayesian Analysis
can be attained by the following ï¬xed point iteration:
q (n+1)
Â·
âˆexp

E(log p(X, T, V|))q (n)
Â¬Â·

(25.18)
where qÂ¬Â· = q/qÂ·. This iteration monotonically improves the individual factors
of the q distribution, i.e. B[q (n)] â‰¤B[q (n+1)] for n = 1, 2, . . . given an initialisa-
tion q (0). The order is not important for convergence â€“ one could visit blocks
in arbitrary order. However, in general, the attained ï¬xed point depends upon
the order of the updates as well as the starting point q (0)(Â·). This approach is
computationally rather attractive and is very easy to implement (Cemgil 2008).
B. Variational Bayesian NMF
In this section we derive a variational Bayes algorithm for the NMF model
described in equations (25.13) and (25.14). The marginal likelihood is
given as
LX() â‰¡log p(X|) â‰¥

S

d(T, V)q log p(X, S, T, V|)
q
(25.19)
= E(log p(X, S, V, T|))q + H[q] â‰¡BV B[q]
(25.20)
where q is deï¬ned as
q(S, T, V) = q(S)q(T)q(V)
=

ÃŒ,Ã™
q(ÃŒ, 1 : I, Ã™)
 
ÃŒ,i
q(tÃŒ,i)
 
i,Ã™
q(vi,Ã™)

â‰¡

Â·âˆˆC
qÂ·.
Here, Â· âˆˆC = {{S}, {T}, {V}} denotes a set of disjoint clusters. A local optimum
can be attained by the following ï¬xed point iteration:
q (n+1)
Â·
âˆexp

E(log p(X, S, T, V|))q (n)
Â¬Â·

(25.21)
where qÂ¬Â· = q/qÂ·.
The expectations of E(log p(X, S, T, V|)) are functions of the sufï¬cient
statistics of q. The ï¬xed point iteration for the latent sources S (where mÃŒ,Ã™ = 1),
and excitations V leads to the following
q(ÃŒ, 1 : I, Ã™) = M(ÃŒ, 1 : I, Ã™; xÃŒ,Ã™, pÃŒ,1:I,Ã™)
(25.22)
pÃŒ,i,Ã™ = exp(E(log tÃŒ,i) + E(log vi,Ã™))
A 
i
exp(E(log tÃŒ,i) + E(log vi,Ã™)) (25.23)
q(vi,Ã™) = Ga

vi,Ã™; Â·v
i,Ã™, â€šv
i,Ã™

(25.24)
Â·v
i,Ã™ = av
i,Ã™ +

ÃŒ
mÃŒ,Ã™E(ÃŒ, i, Ã™)
â€šv
i,Ã™ =

av
i,Ã™
bv
i,Ã™
+

ÃŒ
mÃŒ,Ã™E(tÃŒ,i)
âˆ’1
.
(25.25)

Bayesian Statistical Methods for Audio and Music Processing
745
The variational parameters of q(tÃŒ,i) = Ga

tÃŒ,i; Â·t
ÃŒ,i, â€št
ÃŒ,i

are found similarly.
The hyperparameters can be optimized by maximizing the variational bound
BV B[q]. While this does not guarantee to increase the true marginal likelihood,
it leads in this application to quite practical and fast algorithms and is very easy
to implement (Cemgil 2008).
For the same model, it is also straightforward to implement a Gibbs sam-
pler. A comparison showed that both algorithms give qualitatively very similar
results, both for inference as well as model order selection (Cemgil 2008). We
ï¬nd the variational approach somewhat more practical as it can be expressed as
simple matrix operations, where both the ï¬xed point equations as well as the
bound can be compactly and efï¬ciently implemented using matrix computation
software. In contrast, our Gibbs sampler is computationally more demanding
and the calculation of marginal likelihood is somewhat more tricky. With our
implementation of both algorithms the variational method is faster by a factor
of around 13.
In terms of computational requirements, the variational procedure has sev-
eral advantages. First, one circumvents sampling from multinomial variables,
which is the main computational bottleneck with a Gibbs sampler in this model.
Whilst efï¬cient algorithms are developed for multinomial sampling (Davis
1993), the procedure is time consuming when the number of latent sources I is
large. In contrast, the variational method computes the expected sufï¬cient sta-
tistics via elementary matrix operations. Another advantage is hyperparameter
estimation. In principle, it is possible to maximize the marginal likelihood via a
Monte Carlo EM procedure (Tanner 1996; Quintana, Liu, and del Pino 1999), yet
this potentially requires many more iterations of the Gibbs sampler. In contrast,
the evaluation of the derivatives of the lower bound is straightforward and can
be implemented without much additional computational cost.
Acknowledgements
We would like to thank Andrew Feldhaus for carefully proofreading the manu-
script.
References
Abdallah, S. A. and Plumbley, M. D. (2006). Unsupervised analysis of polyphonic music using
sparse coding. IEEE Transactions on Neural Networks, 17, 179â€“196.
Attias, H. (1999). Independent factor analysis. Neural Computation, 11, 803â€“851.
Bertin, N., Badeau, R. and Richard, G. (2007). Blind signal decompositions for automatic
transcription of polyphonic music: NMF and K-SVD on the benchmark. In Proceedings of
the International Conference on Audio, Speech and Signal Processing (ICASSP), Honolulu.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer, New York.

746
The Oxford Handbook of Applied Bayesian Analysis
Cemgil, A. T. (2004). Bayesian music transcription. Ph.D. thesis, Radboud University of
Nijmegen.
Cemgil, A. T. (2007). Strategies for sequential inference in factorial switching state space
models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP 07), Honolulu, Hawaii, pp. 513â€“516.
Cemgil, A. T. (2008). Bayesian inference in non-negative matrix factorisation models. Technical
Report CUED/F-INFENG/TR.609, University of Cambridge.
Cemgil, A. T. and Dikmen, O. (2007). Conjugate Gamma Markov random ï¬elds for modelling
nonstationary sources. In ICA 2007, 7th International Conference on Independent Compo-
nent Analysis and Signal Separation, London, UK.
Cemgil, A. T., Fevotte, C. and Godsill, S. J. (2007). Variational and stochastic inference for
Bayesian source separation. Digital Signal Processing, 17, 891â€“913. Special Issue on Bayesian
source separation.
Cemgil, A. T., Kappen, H. J. and Barber, D. (2006). A generative model for music transcription.
IEEE Transactions on Audio, Speech and Language Processing, 14, 679â€“694.
Cemgil, A. T., Peeling, P., Dikmen, O. and Godsill, S. J. (2007). Prior structures for time-
frequency energy distributions. In Proceedings of IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics, New Paltz, NY.
Chib, S. (1995). Marginal likelihood from the gibbs output. Journal of the Acoustical Society of
America, 90, 1313â€“1321.
Cover, T. M. and Thomas, J. A. (1991). Elements of Information Theory. John Wiley, New York.
Davis, C. S. (1993). The computer generation of multinomial random variates. Computational
Statistics and Data Analysis, 16, 205â€“217.
Davy, M., Godsill, S. and Idier, J. (2006). Bayesian analysis of polyphonic Western tonal music.
Journal of the Acoustical Society of America, 119, 2498â€“2517.
Davy, M. and Godsill, S. J. (2002). Detection of abrupt spectral changes using support vector
machines. An application to audio signal segmentation. In Proceedings IEEE International
Conference on Acoustics, Speech and Signal Processing, Orlando, FL.
FÃ©votte, C., Daudet, L., Godsill, S. J. and TorrÃ©sani, B. (2006). Sparse regression with structured
priors: Application to audio denoising. In Proceedings ICASSP, Toulouse, France.
FÃ©votte, C. and Godsill, S. (2006). A Bayesian approach for blind separation of sparse sources.
IEEE Transactions on Audio, Speech and Language Processing, 14, 2174â€“2188.
Fletcher, N. H. and Rossing, T. (1998). The Physics of Musical Instruments. Springer, Berlin.
Ghahramani, Z. and Beal, M. (2001). Propagation algorithms for variational Bayesian learning.
In Neural Information Processing Systems, (ed. T. Leen, T. Dietterich and V. Tresp, V.), Vol. 13,
507â€“513. The MIT Press, Cambridge, Massachusetts.
Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (eds.) (1996). Markov Chain Monte Carlo in
Practice. CRC Press, London.
Girolami, M. (2001). A variational method for learning sparse and overcomplete representa-
tions. Neural Computation, 13, 2517â€“2532.
Godsill, S. (2004). Computational modeling of musical signals. Chance Magazine, 17, 23â€“29.
Godsill, S. and Davy, M. (2005). Bayesian computational models for inharmonicity in musical
instruments. In Proceedings of IEEE Workshop on Applications of Signal Processing to
Audio and Acoustics, New Paltz, NY.
Godsill, S. J. and Davy, M. (2002). Bayesian harmonic models for musical pitch estimation and
analysis. In Proceedings IEEE International Conference on Acoustics, Speech and Signal
Processing. Orlando, FL.
Godsill, S. J. and Rayner, P. J. W. (1998). Digital Audio Restoration: A Statistical Model-Based
Approach. Springer, Berlin.

Bayesian Statistical Methods for Audio and Music Processing
747
Green, P. J. (1995). Reversible jump Markov-chain Monte Carlo computation and Bayesian
model determination. Biometrika, 82, 711â€“732.
Grimmett, G. and Stirzaker, D. (2001). Probability and Random Processes, (3rd edn). Oxford
University Press, Oxford.
Herrera-Boyer, P., Klapuri, A. and Davy, M. (2006). Automatic classiï¬cation of pitched musical
instrument sounds. See Klapuri, and Davy (2006), pp. 163â€“200.
Hojen-Sorensen, P., Winther, O. and Hansen, L. K. (2002). Mean-ï¬eld approaches to indepen-
dent component analysis. Neural Computation, 14, 889â€“918.
HyvÃ¤rinen, A., Karhunen, J. and Oja, E. (2001). Independent Component Analysis. John Wiley,
New York.
Kameoka, H. (2007). Statistical approach to multipitch analysis. Ph.D. thesis, University of
Tokyo.
Klapuri, A. and Davy, M. (Eds.) (2006). Signal Processing Methods for Music Transcription.
Springer, New York.
Knuth, K. H. (1998). Bayesian source separation and localization. In SPIEâ€™98: Bayesian Infer-
ence for Inverse Problems, San Diego, pp. 147â€“158.
Lee, D. D. and Seung, H. S. (2000). Algorithms for non-negative matrix factorization. In
Advances in Neural Information Processing Systems (NIPS), Volume 12, pp. 556â€“562. MIT
Press, Cambridge, MA.
McIntyre, M. E., Schumacher, R. T. and Woodhouse, J. (1983). On the oscillations of musical
instruments. Journal of the Acoustical Society of America, 74, 1325â€“1345.
Miskin, J. and Mackay, D. (2001). Ensemble learning for blind source separation. In Indepen-
dent Component Analysis, (ed. S. J. Roberts and R. M. Everson), pp. 209â€“233. Cambridge
University Press, Cambridge.
Mohammad-Djafari, A. (1997). A Bayesian estimation method for detection, localisation and
estimation of superposed sources in remote sensing. In SPIEâ€™97, San Diego.
Moore, B. (1997). An Introduction to the Psychology of Hearing, (4th edn). Academic Press,
New York.
Peeling, P. H., Li, C. and Godsill, S. J. (2007). Poisson point process modeling for polyphonic
music transcription. Journal of the Acoustical Society of America Express Letters, 121, EL168â€“
EL175.
Quintana, F. A., Liu, J. S. and del Pino, G. E. (1999). Monte Carlo EM with importance
reweighting and its applications in random effects models. Computational Statistics and Data
Analysis, 29, 429â€“444.
Reyes-Gomez, M., Jojic, N. and Ellis, D. (2005). Deformable spectrograms. In AI and Statistics
Conference, Barbados.
Rowe, D. B. (2003). Multivariate Bayesian Statistics: Models for Source Separation and Signal
Unmixing. Chapman & Hall/CRC, Boca Raton, Florida.
Shepard, N. (ed.) (2005). Stochastic Volatility, Selected Readings. Oxford University Press, Oxford.
Smaragdis, P. and Brown, J. (2003). Non-negative matrix factorization for polyphonic music
transcription. In IEEE Workshop on Applications of Signal Processing to Audio and
Acoustics (WASPAA). New Paltz, NY. IEEE.
Tanner, M. A. (1996). Tools for Statistical Inference: Methods for the Exploration of Posterior
Distributions and Likelihood Functions (3rd edn). Springer, New York.
Valpola, H. (2000). Nonlinear independent component analysis using ensemble learning:
Theory. In Proceedings of the Second International Workshop on Independent Component
Analysis and Blind Signal Separation, ICA 2000, Helsinki, Finland, pp. 251â€“256.
Vincent, E., Bertin, N. and Badeau, R. (2008). Harmonic and inharmonic nonneg-
ative matrix factorization for polyphonic pitch transcription. In Proceedings IEEE

748
The Oxford Handbook of Applied Bayesian Analysis
International Conference on Acoustics, Speech and Signal Processing (ICASSP), Las Vegas.
IEEE.
Virtanen, T. (2006a). Sound source separation in monaural music signals. Ph.D. thesis, Tam-
pere University of Technology.
Virtanen, T. (2006b). Unsupervised learning methods for source separation in monaural music
signals. See Klapuri and Davy (2006), pp. 267â€“296.
Wainwright, M. and Jordan, M. I. (2003). Graphical models, exponential families, and varia-
tional inference. Technical Report 649, Department of Statistics, UC Berkeley.
Walmsley, P., Godsill, S. J. and Rayner, P. J. W. (1998). Multidimensional optimisation of
harmonic signals. In Proceedings European Conference on Signal Processing. Rhodes,
Greece.
Walmsley, P. J., Godsill, S. J. and Rayner, P. J. W. (1999). Polyphonic pitch tracking using joint
Bayesian estimation of multiple frame parameters. In Proceedings IEEE Workshop on Audio
and Acoustics, Mohonk, NY State, Mohonk, NY State.
Wang, D. and Brown, G. J. (eds.) (2006). Computational Auditory Scene Analysis: Principles,
Algorithms, Applications. John Wiley, New York.
Winther, O. and Petersen, K. B. (2006). Flexible and efï¬cient implementations of Bayesian
Independent Component Analysis. Neuro Computing, 71, 221â€“233.
Wolfe, P. J., Godsill, S. J. and Ng, W. (2004). Bayesian variable selection and regularisation for
time-frequency surface estimation (with discussion). Journal of the Royal Statistical Society,
B, 66, 575â€“589.

Â·26Â·
Combining simulations and physical
observations to estimate cosmological
parameters
Dave Higdon, Katrin Heitmann, Charles Nakhleh
and Salman Habib
26.1 Introduction
Over the last three decades observational cosmology has made extraordinary
progress in determining the make-up of the universe and its evolution. Preci-
sion measurements from all-sky COsmic Background Explorer (COBE) (Smoot
et al., 1992) and, more recently, the Wilkinson Microwave Anisotropy Probe
(WMAP) (Spergel et al., 2007) have given very detailed measurements of the
cosmic microwave background. In addition, large galaxy surveys such as the
Sloan Digital Sky Survey (SDSS) (Adelman-McCarthy et al., 2006) advance our
understanding of structure formation and yield complementary information to
the CMB to determine the make-up of the universe.
The -cold dark matter (CDM) model is the simplest cosmological model
in agreement with the CMB and large scale structure measurements. This
model is determined by a small number of parameters which control the com-
position, expansion and ï¬‚uctuations of the universe. The precision measure-
ments from different cosmological probes reveal a highly unexpected result:
roughly 70% of the universe is made up of a mysterious dark energy that
accelerates the recent expansion of the universe.
In this chapter we combine computationally intensive simulation results with
measurements from the SDSS to infer about a subset of the parameters that
control the CDM model. We also describe a statistical framework adapted
from Kennedy and Oâ€™Hagan (2001) and Higdon et al. (2008) to determine a
posterior distribution for these cosmological parameters given the simulation
output and the physical observations. We then go on to demonstrate how this
formulation can be extended to combine information from simulations and
observations corresponding to both the large scale structure of the universe and
the CMB.

750
The Oxford Handbook of Applied Bayesian Analysis
26.1.1 Large scale structure of the universe
The SDSS maps out the spatial location of galaxies around our own Milky Way
Galaxy (Figure 26.1, right). Note that this spatial distribution of galaxies exhibits
a combination of voids and high density â€˜ï¬lamentsâ€™ of matter. This peculiar
spatial distribution is due to the cumulative effects of gravity (and other forces)
acting on slight matter density ï¬‚uctuations present shortly after the big bang,
as evidenced by the CMB.
Predicting the spatial distribution of matter at our current time in the uni-
verse, given the parameters of the CDM model, requires a substantial comput-
ing effort. For a given parameter setting, a very large-scale N-body simulation
is carried out that evolves tracer dark matter particles according to gravity and
other forces from an initial setting based on the CMB to our current time in
the universe. The result of one such simulation is shown in the left frame of
Figure 26.1. Under different cosmologies (i.e. cosmological parameter settings)
the spatial structure of the resulting simulation output differs. We would like
to determine which cosmologies are consistent with physical observations of
our universe â€“ such as those from the SDSS shown in the right hand frame of
Figure 26.1.
Direct comparison between the simulation output and the SDSS data is not
possible. The simulations evolve an idealized cube of dark matter particles,
while the SDSS data give a censored, local snapshot of the large scale structure
of the universe. Instead, we summarize the simulation output and physical
observations by their dark matter power spectra which describe the spatial
distribution of matter density at a wide range of length scales. Note that the
wave number k on the x-axis of these spectra is given in h Mpcâˆ’1. Mpc is a
120Â°
300Â°
60Â°
240Â°
180Â°
0Â°
0.25
0.2
0.15
0.1
Z
0.05
0
Fig. 26.1 Left: Simulation results from a large scale N-body simulation that evolves particles from an
early time in the universe to now. The large scale structure in the output depends on the cosmological
parameters Ã‹âˆ—under which the simulation was carried out. The goal of this analysis is to determine
which cosmologies are consistent with observations, such as the right hand ï¬gure from the Sloan
Digital Sky Survey (Credit: Sloan Digital Sky Survey).

Estimatation of Cosmological Parameters
751
â€“3.0
â€“2.5
â€“2.0
â€“1.5
â€“1.0
â€“0.5
2
3
4
5
6
Wavenumber (log10 k)
Log P(k)
â€“
â€“
â€“
â€“
â€“â€“
â€“â€“â€“
â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
Large scale structure data & simulations
Fig. 26.2
Twenty-two data points for the dark matter power spectrum derived by Tegmark et al.
(2004) from the Sloan Digital Sky Survey. The error bars denote two standard deviations. The grey
lines show the 128 simulated power spectra, discussed in Section 26.2.1.
length scale; two galaxies are separated by about 1 Mpc on average. The grey
lines in Figure 26.2 show a number of matter power spectra produced by
carrying out simulations using different cosmological parameter settings.
Computing the matter power spectrum is trivial for the simulation output
since it is deï¬ned on a periodic, cubic lattice. In contrast, determining matter
power spectrum from the SDSS data is a far more challenging task since one
must account for the many difï¬culties that accompany observational data: non-
standard survey geometry, redshift space distortions, luminosity bias and noise,
just to name a few. Because of these challenges, we use the published data and
likelihood of Tegmark et al. (2004) which is summarized in Figure 26.2. The
resulting data correspond to 22 pairs (yi, ki). The data vector y = (y1, . . . , y22)â€²
has a diagonal covariance y; the two standard deviation bars are shown in
Figure 26.2 for each data point.
For the N-body simulations, we consider ï¬ve CDM parameters
Ã‹ = (n, h, Ã›8, 	CDM, 	B),
where
n
is the spectral index,
h
is the Hubble constant,
Ã›8
is the galaxy ï¬‚uctuation amplitude,
	CDM
is the density of dark matter, and
	B
is the density of baryonic matter.

752
The Oxford Handbook of Applied Bayesian Analysis
Since we assume a ï¬‚at universe and a constant dark energy equation of state,
we expect that any variation in the remaining CDM parameters will not affect
the resulting matter power spectra.
The framework we use for carrying out this analysis is given in Higdon et al.
(2008), but gentler introductions to the topic of combining simulations and ï¬eld
data can be found in Kennedy and Oâ€™Hagan (2001) or Higdon et al. (2004). The
framework integrates the following concepts:
r simulation design â€“ the determination of the parameter settings at which
to carry out the simulations;
r emulation â€“ given simulation output at a ï¬xed set of input parameter
settings, how to estimate the output at new, untried settings;
r uncertainty and sensitivity analysis â€“ determining the variations in simula-
tion output due to uncertainty or changes in the input parameters;
r calibration â€“ combining observations (with known errors) and simulations
to estimate parameter values consistent with the observations;
r prediction â€“ given parameter uncertainties and uncertainties in other nui-
sance parameters, predict system behaviour with uncertainty.
In this paper we discuss the framework methodology in detail through a
speciï¬c application: Estimation of ï¬ve parameters from dark matter structure
formation simulations and SDSS measurements of the matter power spectrum
(Figure 26.2) in Section 26.2. We extend this framework in Section 26.3 to
include measurements of the CMB temperature power spectrum derived from
the WMAP data release. We perform a combined large scale structure and CMB
analysis and demonstrate how to extend our framework to include data from
diverse data sources.
26.2 The statistical framework
In this section we describe the statistical methodology to combine physical
observations with output from a simulation model to infer about unknown
model parameters. We use observations y from the matter power spectrum
(Figure 26.2) and matter power spectra derived from physical simulations.
Generally, the simulation model requires pÃ‹-vector Ã‹âˆ—of input parameter set-
tings in order to produce a matter power spectrum Ã(Ã‹âˆ—). The simplest possible
model one might postulate is that the vector of observations y is a noisy version
of the simulated spectrum Ã(Ã‹) at the true setting Ã‹
y = Ã(Ã‹) + Ã‚,
where the observation error vector is normal, with mean 0 and variance y.
Given a prior distribution ï£¿(Ã‹) for the true parameter vector Ã‹, the resulting

Estimatation of Cosmological Parameters
753
posterior distribution ï£¿(Ã‹|y) for Ã‹ is given by
ï£¿(Ã‹|y) âˆL(y|Ã(Ã‹)) Â· ï£¿(Ã‹),
where L(y|Ã(Ã‹)) comes from the normal sampling model for the data
L(y|Ã(Ã‹)) = exp
1
2(y âˆ’Ã(Ã‹))â€²âˆ’1
y (y âˆ’Ã(Ã‹))

.
In principle, this posterior distribution could be explored via MCMC. However,
if a single evaluation of Ã(Ã‹) requires hours (or days) of computation, a direct
MCMC-based approach is infeasible.
Our approach deals with this computational bottleneck by treating Ã(Â·) as
an unknown function to be estimated from a ï¬xed collection of simulations
Ã(Ã‹âˆ—
1), . . . , Ã(Ã‹âˆ—
m) carried out at input settings Ã‹âˆ—
1, . . . , Ã‹âˆ—
m. This approach requires
a prior distribution for the unknown function Ã(Â·), and treats the simulation
output Ãâˆ—= (Ã(Ã‹âˆ—
1), . . . , Ã(Ã‹âˆ—
m))â€² as additional data to be conditioned on for the
analysis. Hence there is an additional component of the likelihood obtained
from the sampling model for Ãâˆ—by L(Ãâˆ—|Ã(Â·)).
For this case, the resulting posterior distribution has the general form
ï£¿(Ã‹, Ã(Â·)|y, Ãâˆ—) âˆL(y|Ã(Ã‹)) Â· L(Ãâˆ—|Ã(Â·)) Â· ï£¿(Ã(Â·)) Â· ï£¿(Ã‹),
(26.1)
which has traded direct evaluations of the simulator model for a more compli-
cated form which depends strongly on the prior model for the function Ã(Â·).
Note that the marginal distribution for the cosmological parameters Ã‹ will be
affected by uncertainty regarding Ã(Â·).
In the following subsections, we describe in detail a particular formulation
of equation (26.1) in the context of this large scale structure application. This
formulation has proven fruitful in a variety of physics and engineering appli-
cations which combine ï¬eld observations with detailed simulation models for
inference. In particular we cover approaches for choosing the m parameter set-
tings at which to run the simulation model, and a (prior) model â€“ or emulator â€“
which describes how Ã(Â·) is modeled at untried parameter settings. Section
26.2.3 describes how the observed data is combined with the simulations and
the emulator to give the posterior distribution. In the following section we will
demonstrate how this formulation can be extended to combine information
from different data sets from galaxy surveys and cosmic microwave background
measurements.
26.2.1 The simulation design
The dark matter simulations are quite demanding since they must compute the
force interactions for over two million interacting particles. Simulation accuracy
is particularly important for the smaller length scales (k â‰¥0.2h Mpcâˆ’1), where
the gravitational effects become strongly nonlinear. For this application, we

754
The Oxford Handbook of Applied Bayesian Analysis
use m = 128 simulations. Future investigations which will make use of next
generation of large scale structure surveys will require far more resolution in
the simulations. With such large numbers of particles, we expect to be able to
carry out no more than 100 simulation runs.
Generally, the design speciï¬es m input settings which vary over predeï¬ned
ranges for each of the pÃ‹ input parameters:
âŽ›
âŽœâŽ
Ã‹âˆ—
1...
Ã‹âˆ—
m
âŽž
âŽŸâŽ =
âŽ›
âŽœâŽ
Ã‹âˆ—
11 Â· Â· Â· Ã‹âˆ—
1pÃ‹
...
...
...
Ã‹âˆ—
m1 Â· Â· Â· Ã‹âˆ—
mpÃ‹
âŽž
âŽŸâŽ .
(26.2)
We use Ã‹âˆ—to differentiate the design input settings from the true value of the
parameter vector Ã‹ which is what we are trying to estimate.
For our purposes, we would like a design that leads to an accurate, Gaussian
process (GP)-based emulator which is described in Section 26.2.2. Space-ï¬lling
Latin hypercube (LH) designs have proven to be well suited for building GP
models to estimate simulator output at untried settings (Sacks et al., 1989;
Currin et al., 1991). In particular, we have used orthogonal array-based LH
designs (Tang, 1993) as well as symmetric LH designs (Ye et al., 2000). Fig-
ure 26.3 shows the m = 128 point design over pÃ‹ = 5 dimensions used in this
analysis. This design was constructed by perturbing a ï¬ve-level orthogonal array
design so that each one-dimensional projection gives an equally spaced set of
points along the standardized parameter range [0,1]. See Santner et al. (2003),
Chapters 5 and 6, for a recent survey of the area.
The actual parameter ranges used for the m = 128 simulations are
0.8 â‰¤
n
â‰¤1.4,
0.5 â‰¤
h
â‰¤1.1,
0.6 â‰¤
Ã›8
â‰¤1.6,
0.0 â‰¤	CDM â‰¤0.6,
0.02 â‰¤
	B
â‰¤0.12.
(26.3)
These ranges are standardized to [0, 1]5 by shifting and scaling each interval.
26.2.2 Emulating simulator output
For a given input Ã‹, the simulator produces a matter power spectrum, as shown
in Figure 26.2. Each spectrum is a nÃ-vector of values. A key component of
our analysis is a probability model to describe this functional simulator output
at untried settings Ã‹. To do this, we use the output of the simulation runs to
construct a GP model that â€˜emulatesâ€™ the simulator at arbitrary input settings
over the (standardized) input space [0, 1]pÃ‹. The emulator models the simulation

Estimatation of Cosmological Parameters
755
n
h
Ïƒ8
Î©CDM
Î©B
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
0.0
0.4
0.8
Fig. 26.3
Lower triangle of plots: two-dimensional projections of a m = 128 point, ï¬ve-level, OA
design. Upper triangle of plots: an OA-based LH design obtained by spreading out the ï¬ve-level OA
design so that each one-dimensional projection gives an equally spaced set of points along [0,1].
output using a pÃ-dimensional basis representation:
Ã(Ã‹) =
pÃ

i=1
Ë†iwi(Ã‹) + Ã‚, Ã‹ âˆˆ[0, 1]pÃ‹,
(26.4)
where {Ë†1, . . . , Ë†pÃ} is a collection of orthogonal, nÃ-dimensional basis vectors,
the wi(Ã‹)s are GPs over the input space, and Ã‚ is an nÃ-dimensional error term.
This type of formulation reduces the problem of building an emulator that
maps [0, 1]pÃ‹ to RnÃ to building pÃ independent, univariate GP models for each
wi(Ã‹). The details of this model speciï¬cation are given below.
Output from each of the m simulation runs prescribed by the design results
in nÃ-dimensional vectors which we denote by Ã1, . . . , Ãm. Since the simulations
give complete output, the simulation output can be efï¬ciently represented
via principal components (Ramsay and Silverman, 1997). We ï¬rst centre the

756
The Oxford Handbook of Applied Bayesian Analysis
simulations about 0 by subtracting the mean ( 1
m
m
j=1 Ã j) from each output
vector. We note that, depending on the application, some alternative standard-
ization may be preferred. Whatever the choice of the standardization, the same
standardization is also applied to the experimental data.
We deï¬ne  to be the nÃ Ã— m matrix obtained by column-binding the (stan-
dardized) output vectors from the simulations
 = [Ã1; Â· Â· Â· ; Ãm].
The size of a given simulation output nÃ is much larger than the number of
simulations carried out m. We apply the singular value decomposition (SVD) to
the simulation output matrix  giving
 = UDV â€²,
where U is a nÃ Ã— m orthogonal matrix, D is a diagonal m Ã— m matrix holding
the singular values, and V is a m Ã— m orthonormal matrix. To construct a pÃ-
dimensional representation of the simulation output, we deï¬ne the principal
component (PC) basis matrix Ã to be the ï¬rst pÃ columns of [ 1
âˆšmUD]. The
resulting principal component loadings or weights is then given by [âˆšmV].
For the matter power spectrum application we take pÃ = 5 so that Ã =
[Ë†1; Ë†2; Ë†3; Ë†4; Ë†5]; the basis functions Ë†1, Ë†2, Ë†3, Ë†4 and Ë†5 are shown in
Figure 26.4. Note that the Ë†is are functions of log wave number.
We use the basis representation of equation (26.4) to model the nÃ-
dimensional simulator output over the input space. Each basis weight wi(Ã‹),
i = 1, . . . , pÃ, is then modeled as a mean 0 GP
wi(Ã‹) âˆ¼GP

0, ÃŽâˆ’1
wi R(Ã‹, Ã‹â€²; Ã’wi)

,
â€“3.0
â€“2.0
â€“1.0
Log wavenumber log(k)
Log spectrum
2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5
â€“3.0
â€“2.0
â€“1.0
Log wavenumber log(k)
Log spectrum
2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5
â€“3.0
â€“2.0
â€“1.0
â€“2.0
â€“1.0
0.0 0.5 1.0 1.5
Log wavenumber log(k)
Log spectrum
Fig. 26.4 Simulations (left), mean (centre), and the ï¬rst ï¬ve principal component bases (right)
derived from the simulation output.

Estimatation of Cosmological Parameters
757
where ÃŽwi is the marginal precision of the process and the correlation function
is given by
R(Ã‹, Ã‹â€²; Ã’wi) =
pÃ‹

k=1
Ã’
4(Ã‹kâˆ’Ã‹â€²
k)
2
wik
.
(26.5)
This is the Gaussian covariance function, which gives very smooth realizations,
and has been used previously by Kennedy and Oâ€™Hagan (2001) and Sacks et al.
(1989) to model computer simulation output. An advantage of this product
form is that only a single additional parameter is required per additional
input dimension, while the ï¬tted GP response still allows for rather general
interactions between inputs. We use this Gaussian form for the covariance
function because the simulators we work with tend to respond very smoothly to
changes in the inputs. The parameter Ã’wik controls the spatial range for the
kth input dimension of the process wi(Ã‹). Under this parametrization, Ã’wik
gives the correlation between wi(Ã‹) and wi(Ã‹â€²) when the input conditions Ã‹
and Ã‹â€² are identical, except for a difference of 0.5 in the kth component. Note
that this interpretation makes use of the standardization of the input space
to [0, 1]pÃ‹.
Restricting to the m input design settings given in (26.2), we deï¬ne the m-
vector wi to be
wi =

wi

Ã‹âˆ—
1

, . . . , wi

Ã‹âˆ—
m
â€² , i = 1, . . . , pÃ.
In addition we deï¬ne R(Ã‹âˆ—; Ã’wi) to be the m Ã— m correlation matrix resulting
from applying equation (26.5) to each pair of input settings in the design. The
pÃ‹-vector Ã’wi gives the correlation distances for each of the input dimensions.
At the m simulation input settings, the mpÃ-vector w = (wâ€²
1, . . . , wâ€²
pÃ)â€² then
has prior distribution
âŽ›
âŽœâŽœâŽ
w1
...
wpÃ
âŽž
âŽŸâŽŸâŽ âˆ¼N
âŽ›
âŽœâŽœâŽ
âŽ›
âŽœâŽœâŽ
0
...
0
âŽž
âŽŸâŽŸâŽ ,
âŽ›
âŽœâŽœâŽ
ÃŽâˆ’1
w1R(Ã‹âˆ—; Ã’w1) 0
0
0
...
0
0
0 ÃŽâˆ’1
wpÃ R(Ã‹âˆ—; Ã’wpÃ)
âŽž
âŽŸâŽŸâŽ 
âŽž
âŽŸâŽŸâŽ ,
(26.6)
which is controlled by pÃ precision parameters held in ÃŽw and pÃ Â· pÃ‹ spa-
tial correlation parameters held in Ã’w. The prior above can be written more
compactly as
w âˆ¼N(0, w),
where w, controlled by parameter vectors ÃŽw and Ã’w, is given in equation
(26.6).

758
The Oxford Handbook of Applied Bayesian Analysis
We specify independent Ga(aw, bw) priors for each ÃŽwi and independent
Be(aÃ’w, bÃ’w) priors for the Ã’wiks.
ï£¿(ÃŽwi) âˆÃŽawâˆ’1
wi
eâˆ’bwÃŽwi, i = 1, . . . , pÃ,
ï£¿(Ã’wik) âˆÃ’
aÃ’w âˆ’1
wik
(1 âˆ’Ã’wik)bÃ’w âˆ’1, i = 1, . . . , pÃ, k = 1, . . . , pÃ‹.
We expect the marginal variance for each wi(Â·) process to be close to one due to
the scaling of the basis functions. For this reason we specify that aw = bw = 5.
In addition, this informative prior helps stabilize the resulting posterior dis-
tribution for the correlation parameters which can trade off with the marginal
precision parameter.
Because we expect only a subset of the inputs to inï¬‚uence the simulator
response, our prior for the correlation parameters reï¬‚ects this expectation of
â€˜effect sparcity.â€™ Under the parametrization in equation (26.5), input k is inac-
tive for PC i if Ã’wik = 1. Choosing aÃ’w = 1 and 0 < bÃ’w < 1 will give a density
with substantial prior mass near 1. We take bÃ’w = 0.1, which makes Pr(Ã’wik <
0.98) â‰ˆ1
3 a priori. In general, the selection of these hyperparameters should
depend on how many of the pÃ‹ inputs are expected to be active. Alternatively,
the prior could be speciï¬ed to have some point mass at 1 as in Linkletter et al.
(2006).
If we take the error vector in the basis representation of equation (26.4) to
be i.i.d. normal, we can then develop the sampling model, or likelihood, for the
simulator output. We deï¬ne the nÃm-vector Ã to be the concatenation of all m
simulation output vectors
Ã = vec() = vec

Ã

Ã‹âˆ—
1

; Â· Â· Â· ; Ã

Ã‹âˆ—
m

,
where vec() produces a vector by stacking the columns of matrix . Given
precision ÃŽÃ of the errors the likelihood is then
L(Ã|w, ÃŽÃ) âˆÃŽ
mnÃ
2
Ã
exp

âˆ’1
2ÃŽÃ(Ã âˆ’w)â€²(Ã âˆ’w)

,
where the nÃ Ã— mpÃ matrix  is given by
 = [Im âŠ—Ë†1; Â· Â· Â· ; Im âŠ—Ë†pÃ],
and the Ë†is are the pÃ basis vectors previously computed via SVD. A Ga(aÃ, bÃ)
prior is speciï¬ed for the error precision ÃŽÃ.
Since the likelihood factors as shown below
L(Ã|w, ÃŽÃ) âˆÃŽ
mpÃ
2
Ã
exp

âˆ’1
2ÃŽÃ(w âˆ’Ë†w)â€²(â€²)(w âˆ’Ë†w)

Ã— ÃŽ
m(nÃâˆ’pÃ)
2
Ã
exp

âˆ’1
2ÃŽÃÃâ€²(I âˆ’(â€²)âˆ’1â€²)Ã

,

Estimatation of Cosmological Parameters
759
the formulation can be equivalently represented with a dimension reduced
likelihood and a modiï¬ed Ga(aâˆ—
Ã, bâˆ—
Ã) prior for ÃŽÃ:
L( Ë†w|w, ÃŽÃ) âˆÃŽ
mpÃ
2
Ã
exp

âˆ’1
2ÃŽÃ( Ë†w âˆ’w)â€²(â€²)( Ë†w âˆ’w)

,
where
aâˆ—
Ã = aÃ + m(nÃ âˆ’pÃ)
2
,
bâˆ—
Ã = bÃ + 1
2Ãâ€²(I âˆ’(â€²)âˆ’1â€²)Ã,
and
Ë†w = (â€²)âˆ’1â€²Ã.
(26.7)
Thus the Normal-Gamma model
Ã|w, ÃŽÃ âˆ¼N

w, ÃŽâˆ’1
Ã InÃ

, ÃŽÃ âˆ¼Ga(aÃ, bÃ)
is equivalent to the reduced form
Ë†w|w, ÃŽÃ âˆ¼N(w, (ÃŽÃâ€²)âˆ’1), ÃŽÃ âˆ¼Ga

aâˆ—
Ã, bâˆ—
Ã

since
L(Ã|w, ÃŽÃ) Ã— ï£¿(ÃŽÃ; aÃ, bÃ) âˆL( Ë†w|w, ÃŽÃ) Ã— ï£¿

ÃŽÃ; aâˆ—
Ã, bâˆ—
Ã

.
(26.8)
The likelihood depends on the simulations only through the computed PC
weights Ë†w. After integrating out w, the posterior distribution becomes
ï£¿(ÃŽÃ, ÃŽw, Ã’w| Ë†w) âˆ
(26.9)
(ÃŽÃâ€²)âˆ’1 + w
âˆ’1
2 exp

âˆ’1
2 Ë†wâ€²([ÃŽÃâ€²]âˆ’1 + w)âˆ’1 Ë†w

Ã—ÃŽ
aâˆ—
Ãâˆ’1
Ã
eâˆ’bâˆ—
ÃÃŽÃ Ã—
pÃ

i=1
ÃŽawâˆ’1
wi
eâˆ’bwÃŽwi Ã—
pÃ

i=1
pÃ‹

j=1
(1 âˆ’Ã’wi j)bÃ’âˆ’1.
This posterior distribution is a milepost on the way to the complete formulation,
which also incorporates experimental data. However, it is worth considering
this intermediate posterior distribution for the simulator response. It can be
explored via MCMC using standard Metropolis updates and we can view a
number of posterior quantities to illuminate features of the simulator. In Oakley
and Oâ€™Hagan (2004) the posterior of the simulator response is used to investi-
gate formal sensitivity measures of a univariate simulator; in Sacks et al. (1989)
this is done from a non-Bayesian perspective. For example, Figure 26.5 shows
boxplots of the posterior distributions for the components of Ã’w. From this
ï¬gure it is apparent that PCs 1 and 2 are most inï¬‚uenced by Ã›8 and 	CDM.

760
The Oxford Handbook of Applied Bayesian Analysis
0
0.5
1
n
h
Î©CDM
Î©B
2
3
4
5
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
1
2
3
k
4
5
1
0
0.5
1
0
0.5
1
0
0.5
1
0
0.5
1
Ïƒ8
PC1
Ïw1k
PC2
Ïw2k
PC3
Ïw3k
PC4
Ïw4k
PC5
Ïw5k
Fig. 26.5 Boxplots of posterior samples for each Ã’wik for the large scale structure application.
0
0.5
1
0
W1(Î¸)
0.5
1
â€“5
0
5
0
0
0
0.5
W2(Î¸)
0.5
1
â€“5
0
5
PC 1
Î¸4=Î©CDM
Î¸3= Ïƒ8
Î¸4= Î©CDM
Î¸3= Ïƒ8
PC 2
1
0
0.5
1
W3(Î¸)
0.5
1
â€“5
0
5
Î¸4=Î©CDM
Î¸3=Ïƒ8
PC 3
Fig. 26.6 Posterior mean surfaces for wi(Ã‹), i = 1, 2, 3. Here the other three parameters were held at
their midpoints as Ã›8 and 	CDM vary over the design range.
Figure 26.6 shows the resulting posterior mean surfaces for w1(Ã‹), w2(Ã‹) and
w3(Ã‹) as a function of Ã›8 and 	CDM.
Given the posterior realizations from equation (26.9), one can generate real-
izations from the process Ã(Ã‹) at any input setting Ã‹âˆ—. Since
Ã(Ã‹âˆ—) =
pÃ

i=1
Ë†iwi(Ã‹âˆ—),
realizations from the wi(Ã‹âˆ—) processes need to be drawn given the MCMC
output. For a given draw (ÃŽÃ, ÃŽw, Ã’w) a draw of wâˆ—= (w1(Ã‹âˆ—), . . . , wpÃ(Ã‹âˆ—))â€² can
be produced by making use of the fact

Ë†w
wâˆ—

âˆ¼N

0
0

,
6
(ÃŽÃâ€²)âˆ’1 0
0
0

+ w,wâˆ—(ÃŽw, Ã’w)
7
,

Estimatation of Cosmological Parameters
761
â€“3
â€“2
â€“1
â€“3
â€“2
â€“1
â€“3
â€“2
â€“1
3
4
5
Log P
n
h
Log10 k
Ïƒ8
Î©CDM
Î©B
Fig. 26.7 Changes to the posterior mean simulator predictions obtained by varying one input, while
holding others at their central values, i.e. at the midpoint of their range. The light to dark lines
correspond to the smallest parameter setting to the biggest, for each parameter.
where w,wâˆ—is obtained by applying the prior covariance rule to the augmented
input settings that include the original design and the new input setting (Ã‹âˆ—).
Recall Ë†w is deï¬ned in equation (26.7). Application of the conditional normal
rules then gives
wâˆ—| Ë†w âˆ¼N

V21V âˆ’1
11 Ë†w, V22 âˆ’V21V âˆ’1
11 V12

,
where
V =

V11 V12
V21 V22

=
6
(ÃŽÃâ€²)âˆ’1 0
0
0

+ w,wâˆ—(ÃŽw, Ã’w)
7
is a function of the parameters produced by the MCMC output. Hence, for each
posterior realization of (ÃŽÃ, ÃŽw, Ã’w), a realization of wâˆ—can be produced. The
above recipe easily generalizes to give predictions over many input settings at
once.
Figure 26.7 shows posterior means for the simulator response Ã where each
of the inputs is varied over its prior (standardized) range of [0, 1] while the
other four inputs are held at their nominal setting of 0.5. The posterior mean
response conveys an idea of how the different parameters affect the highly
multivariate simulation output. Other marginal functionals of the simulation
response can also be calculated such as sensitivity indicies or estimates of
the Sobol decomposition (Sacks et al., 1989; Oakley and Oâ€™Hagan, 2004). Note
that a simpliï¬ed emulator can be constructed by taking plug in estimates for
(ÃŽÃ, ÃŽw, Ã’w).
26.2.2.1 Assessing emulator ï¬t
The effectiveness of the GP emulator ï¬t depends in part on how smoothly
the output changes as inputs vary over their prior range, the effectiveness of
the PC basis representation, the number of â€˜activeâ€™ parameter inputs, and the
complexity of the simulator response. We assess the accuracy of this emulator
by predicting the simulated power spectrum over a 64 run holdout design.
This 64 run holdout design is also an OA-based LH design over the same

762
The Oxford Handbook of Applied Bayesian Analysis
2
4
6
2
4
6
Log spectrum
2
4
6
2
4
6
â€“3 â€“2 â€“1
â€“3 â€“2 â€“1
â€“3 â€“2 â€“1
â€“3 â€“2 â€“1
Log k
Log k
Log k
Log k
Fig. 26.8 Posterior mean estimates (light lines) and actual simulated spectra (black dots) for a 64 run
design that was not used to train the response surface model.
â€“3.0
â€“2.5
â€“2.0
â€“1.5
â€“1.0
â€“0.5
40
20
0
â€“20
â€“40
â€“3.0
â€“2.5
â€“2.0
â€“1.5
â€“1.0
â€“0.5
Log k
Percent difference from sim
Fig. 26.9 Emulator tests on a 64 run design. Left: emulator based on 128 runs, right: emulator based
on 32 runs. The central grey region contains the middle 50% of the residuals, the wider light grey
region, the middle 90%. The residuals outside the 90% bands are shown as dots.
parameter ranges. The resulting simulated and predicted power spectra are
shown in Figure 26.8. The green lines show the emulator predictions and
the black dots show the actual simulation output for the 64 runs. Overall,
the emulator performance is sufï¬cient for the task of estimating the cos-
mological parameters. A more detailed look at the emulator performance is

Estimatation of Cosmological Parameters
763
shown in Figure 26.9, left frame. Here we display the residuals of the emu-
lator prediction compared to the simulation runs. The dark gray band con-
tains the middle 50% of the residuals, the light gray band the middle 90%.
The overall accuracy of our emulator is over a wide range better than 5%.
Only on the edges of the parameter ranges investigated is the quality slightly
worse.
Next we investigate how the accuracy of the emulator changes with number
of simulations. This is important since large scale simulations are very costly,
therefore weâ€™d like to know what the minimal number of simulations required
might be. To this end, we create a design for 32 simulations using an orthogonal
Latin hypercube sampling design and test it on the same 64 design run we have
used for testing the 128 run emulator. The results for this limited run emulator
are shown in Figure 26.9, right frame. The overall quality of the emulator
is still sufï¬cient, with accuracy at the 10% level. Compared with the larger
design emulator, the predictions for the medium k ranges are not as good. Such
studies help us assess the relative tradeoff between a sparse set of high accuracy
simulations and a larger set of less accurate simulations.
26.2.3 Full statistical formulation
Given the model speciï¬cations for the simulator Ã(Ã‹), we can now consider the
sampling model for the experimentally observed data. The data are contained in
an n-vector y. For the matter power spectrum application n = 22, corresponding
to different wave numbers as shown in Figure 26.2. As stated in Section 26.1
the data are modelled as a noisy version of the simulated spectrum Ã(Ã‹) run at
the true, but unknown, parameter setting Ã‹. Thus
y = Ã(Ã‹) + Ã‚,
where the errors are assumed to be N(0, y). For notational convenience we
represent the precision âˆ’1
y
as ÃŽyWy, leaving open the option to estimate a
scaling of the error covariance with ÃŽâˆ’1
y . Using the basis representation for the
simulator this equation becomes
y = K yw(Ã‹) + Ã‚
where w(Ã‹) is the pÃ-vector (w1(Ã‹), . . . , wpÃ(Ã‹))â€². Because the wave number sup-
port of y is not necessarily contained in the support of the simulation output,
the basis vectors in K y may have to be interpolated over wave number from the
columns of KÃ. Since the simulation output over wave number is quite dense,
this interpolation is straightforward.

764
The Oxford Handbook of Applied Bayesian Analysis
We specify a Ga(ay, by) prior for the precision parameter ÃŽy resulting in a
Normal-Gamma form for the data model
y|w(Ã‹), ÃŽy âˆ¼N(K yw(Ã‹), (ÃŽyWy)âˆ’1), ÃŽy âˆ¼Ga(ay, by).
(26.10)
The observation precision Wy is fairly well-known for the SDSS data. Hence
we use an informative prior with ay = by = 5, encouraging ÃŽy to be near one.
Equivalently, equation (26.10) can be represented in terms of the basis
weights
Ë†wy|w(Ã‹), ÃŽy âˆ¼N

w(Ã‹),

ÃŽy K â€²
yWy K y
âˆ’1
, ÃŽy âˆ¼Ga

aâˆ—
y, bâˆ—
y

,
with
Ë†wy =

K â€²
yWy K y
âˆ’1 K â€²
yWy y,
aâˆ—
y = ay + 1
2(n âˆ’pÃ),
and
bâˆ—
y = by + 1
2(y âˆ’K y Ë†wy)â€²Wy(y âˆ’K y Ë†wy).
This equivalency follows from equation (26.8) given in Section 26.2.2.
The (marginal) distribution for the combined, reduced data obtained from
the experiments and simulations given the covariance parameters has the
form

Ë†wy
Ë†w

âˆ¼N

0
0

,

âˆ’1
y
0
0
âˆ’1
Ã

+

IpÃ
wyw
â€²
wyw w

,
(26.11)
where w is deï¬ned in (26.6),
y = ÃŽy K â€²
yWy K y,
Ã = ÃŽÃK â€²K,
IpÃ = pÃ Ã— pÃ identity matrix,
wyw =
âŽ›
âŽœâŽœâŽœâŽ
ÃŽâˆ’1
w1R(Ã‹, Ã‹âˆ—; Ã’w1) 0
0
0
...
0
0
0 ÃŽâˆ’1
wpÃ R(Ã‹, Ã‹âˆ—; Ã’wpÃ)
âŽž
âŽŸâŽŸâŽŸâŽ .
Above, R(Ã‹, Ã‹âˆ—; Ã’wi) denotes the 1 Ã— m correlation submatrix obtained by apply-
ing equation (26.5) to the observational setting Ã‹ crossed with the m simulator
input settings Ã‹âˆ—
1, . . . , Ã‹âˆ—
m.

Estimatation of Cosmological Parameters
765
26.2.3.1 Posterior distribution
If we take Ë†z to denote the reduced data ( Ë†wâ€²
y, Ë†wâ€²)â€², and Ë†z to be the covariance
matrix given in equation (26.11), the posterior distribution has the form
ï£¿(ÃŽÃ, ÃŽw, Ã’w, ÃŽy, Ã‹|Ë†z) âˆ|Ë†z|âˆ’1
2 exp

âˆ’1
2 Ë†zâ€²âˆ’1
Ë†z Ë†z

Ã— ÃŽ
aâˆ—
Ãâˆ’1
Ã
eâˆ’bâˆ—
ÃÃŽÃ Ã—
pÃ

i=1
ÃŽawâˆ’1
wi
eâˆ’bwÃŽwi
Ã—
pÃ

i=1
pÃ‹

k=1
Ã’
aÃ’w âˆ’1
wik
(1 âˆ’Ã’wik)bÃ’w âˆ’1 Ã— ÃŽ
aâˆ—
yâˆ’1
y
eâˆ’bâˆ—
yÃŽy Ã— I[Ã‹ âˆˆC],
where C denotes the pÃ‹-dimensional rectangle deï¬ned in (26.3).
Realizations from the posterior distribution are produced using standard,
single site MCMC. Metropolis updates (Metropolis et al., 1953) are used for
the components of Ã’w and Ã‹ with a uniform proposal distribution centred at
the current value of the parameter. The precision parameters ÃŽÃ, ÃŽw and ÃŽy
are sampled using Hastings updates (Hastings, 1970). Here the proposals are
uniform draws, centred at the current parameter values, with a width that is
proportional to the current parameter value. In a given application the candidate
proposal width can be tuned for optimal performance.
The resulting posterior distribution estimate for Ã‹ is shown in Figure 26.10
on the original scale. These estimates are consistent with the current best
estimates of these parameters.
26.3 Combined CMB and large scale structure analysis
So far we have focused our analysis on the matter power spectrum. A more com-
plete analysis of cosmological data should also include additional data sources,
such as the WMAP. Figure 26.11 shows a reconstruction of the temperature
ï¬eld for the CMB produced by the WMAP team. A reconstruction is required
since the observations from WMAP (and other sources) do not give a complete
picture of the cosmic sky. From these incomplete measurements, estimates
of the temperature spectrum (called the TT spectrum) are made at different
multipole moments â„“which index the spherical harmonics.
In this section we extend our analysis to include the TT spectrum data given
by the WMAP ï¬ve-year data release. We use the CAMB (Lewis et al., 2000) code
to produce a TT power spectrum given the CDM parameters. For modeling
the TT spectrum, we consider an additional parameter Ã™ which controls the
optical depth to reionization. Hence each CAMB simulation is determined by
the six-dimensional parameter vector
Ã‹ = (n, h, Ã›8, 	CDM, 	B, Ã™).
Different groups analysing different data sets (Spergel et al., 2007; Tegmark
et al., 2006) found that the model speciï¬ed by these six parameters consistently

766
The Oxford Handbook of Applied Bayesian Analysis
0.5
1
0.2
0.4
0.6
0.12
0.6
0.8
1
0.8
0.6
0.8
1
1.2
1.4
1.6
0.12
0.2
0.4
0.6
0.8
1
1.2
1.4
0.02
0.04
0.06
0.08
0.1
0.12
0.6 0.8 1 1.2 1.4 1.6
0.05
0.1
n
h
Ïƒ8
Î©CDM
Î©B
Fig. 26.10 Estimated posterior distribution of the parameters Ã‹ = (n, h, Ã›8, 	CDM, 	B). The diago-
nal shows the estimated marginal posterior pdf for each parameter; the off-diagonal images give
estimates of bivariate marginals; the contour lines show estimated 90% hpd regions.
0
200
400
600
800
1000
1
2
3
4
5
Multipole moment l
Log Power
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’
CMB TT power spectrum: WMAP data & simulations
Fig. 26.11 The TT temperature spectrum of the CMB as measured by WMAP. Left: a reconstruction
of the spherical CMB temperature ï¬eld from WMAP observations (Credit: NASA/WMAP Science
Team). Right: simulated and measured temperature spectra for the CMB using spherical harmonics.
The black vertical lines show two standard deviation uncertainties determined from the WMAP ï¬ve-
year data release.

Estimatation of Cosmological Parameters
767
ï¬ts all currently available data. We allow a prior range for Ã™ of (0, 0.3) in our
analysis below.
26.3.1 Constraints from the cosmic microwave background
Before we carry out a combined analysis of large scale structure and CMB data,
we investigate how well our framework works on the TT power spectrum alone.
We use a Gaussian approximation to the likelihood supplied by the Legacy
Archive for Microwave Background Data Analysis (LAMBDA) resulting in a
999-dimensional observed vector y, and error covariance matrix y. Figure
26.11 shows the data vector averaged over local bins, along with corresponding
uncertainties.
As with the matter power spectrum analysis, we create a design for 128 runs,
this time for a six-parameter space. In the right hand frame of Figure 26.11 the
grey lines show the TT power spectra produced by the 128 CAMB runs, and the
black +s show the data points with two standard deviation uncertainties derived
from the likelihood. Note that the data here do have a slight amount correlation
which is encoded in the error matrix y.
In Figure 26.12 we show the analog to Figure 26.9, demonstrating that the
TT emulator predicts 90% of the holdout runs to better than 10% and 50% of
the runs to better than 5%. This accuracy is impressive considering the dynamic
range and complexity of the TT spectra. However, this complexity requires more
0
100
200
300
400
500
600
700
800
900
1000
â€“20
â€“15
â€“10
â€“5
0
5
10
15
20
Multipole moment
Percent difference from sim
Fig. 26.12 Emulator performance on holdout test. The central grey region contains the middle 50%
of the residuals, the wider light grey region, the middle 90%. The lines show residuals extending
beyond the 90% bounds.

768
The Oxford Handbook of Applied Bayesian Analysis
0.5
1
0.2
0.4
0.6
0
0.1
0.2
0.3
0.5
1
0.6
0.8
1
1.2
1.4
1.6
0.2
0.4
0.6
0.02
0.8
0.04
0.06
0.08
0.1
0.12
0.8
1
1.2
1.4
0.6 0.8 1 1.2 1.4 1.6
0.10
0.08
0.00
0.12
n
h
Ï„
Ïƒ8
Î©CDM
Î©B
Fig. 26.13 Posterior distribution for the cosmological parameters using just the data from the
WMAP TT power spectrum. The TT analysis uses the additional parameter Ã™.
PCs for accurate emulation. We have kept six PCs for the TT analysis compared
to ï¬ve PCs for the matter power spectrum analysis. The bivariate marginal plot
summarizing the inference in the cosmological parameters taking into account
the TT spectrum alone is given in Figure 26.13 (compare to Figure 26.10).
Other groups who have developed interpolation schemes to predict the tem-
perature power spectrum (Jimenez et al., 2004; Kosowsky et al., 2002; Fendt and
Wandelt, 2007) choose much narrower priors than we have in this application.
The GP emulator affords us comparable accuracy over a much broader range of
parameter values.
26.3.2 Combined constraints
Now we combine the information contained in the matter power spectrum with
the TT spectrum. Given two sets of observed data, y1 and y2 â€“ corresponding to
the SDSS and WMAP observations respectively â€“ that inform on a common

Estimatation of Cosmological Parameters
769
0.5
1
0.2
0.4
0.6
0
0.1
0.2
0.3
0.5
1
0.6
0
0.8
1
1.2
1.4
1.6
0.2
0.4
0.6
0.02
0
0.04
0.06
0.08
0.1
0.12
1
1.2
1.4
0.8
0.6 0.8 1 1.2 1.4 1.6
0.02 0.06
0.10
0.08
0.12
n
h
Ï„
Ïƒ8
Î©CDM
Î©B
Fig. 26.14 Posterior distribution for the cosmological parameters using data from the matter power
spectrum and the CMB TT power spectrum.
set of cosmological parameters Ã‹ and statistical parameters Ã“, the posterior
density is
ï£¿(Ã‹, Ã“|y1, y2) âˆL(y1, y2|Ã‹) Â· ï£¿(Ã“) Â· ï£¿(Ã‹).
Assuming the two datasets are independent given the true cosmology Ã‹, the
likelihood factors into
ï£¿(Ã‹, Ã“|y1, y2) âˆL(y1|Ã‹, Ã“1) Â· ï£¿(Ã“1) Â· L(y2|Ã‹, Ã“2) Â· ï£¿(Ã“2) Â· ï£¿(Ã‹).
Our analysis code simply takes the product of the two separate posteriors, while
ensuring that the prior information for the common parameter vector Ã‹ is
counted only once.
The payoff from including both sets of observational data is illustrated in
Figure 26.14. The posterior volume of the cosmological parameter space is
signiï¬cantly reduced by the inclusion of the large scale structure information.

770
The Oxford Handbook of Applied Bayesian Analysis
There are two main reasons for this volumetric reduction. First is the expected
statistical increase due to the addition of two independent and consistent pieces
of data. Second, and more interesting, is the inï¬‚uence of posterior correlations
among the cosmological parameters induced by the two datasets. For example,
the degeneracy between n and Ã™ from the CMB analysis is signiï¬cantly reduced
when the large scale structure data are included.
The spectra produced from the SDSS and the WMAP data sources agree
very well with a common CDM cosmology. We feel the simulation models
are adequate for reproducing the matter and TT spectra relative to the accuracy
of the physical observations. Therefore our analysis did not explicitly account
for systematic discrepancy between the simulator Ã(Ã‹) and reality y. We did
investigate alternative formulations which included a term for this discrepancy,
but the magnitude of this additional error term was always estimated to be
quite small. For future studies which will include additional data sources (and
simulators), this model discrepancy term will likely play a more important
role. More generally, this is an important consideration if we suspect that the
simulation model is missing important physics to model a particular physical
observable. Kennedy and Oâ€™Hagan (2001) and Goldstein and Rougier (2008),
along with their accompanying discussions, give some background on the topic
of dealing with simulation model inadequacy.
Appendix
A. Broader context and background
A.1 GP-based calibration of computer models
In many settings, a model for physical reality is replaced by a computer model
which typically has a number of unknown, tunable parameters. The goal is
to use observations of the physical system to constrain, or calibrate, these
parameters. Given the resulting posterior distribution of these parameters, the
computer model can now be used to make predictions of the physical system at
new, untried conditions.
At various initial conditions x, observations y(x) are made of the physical
system ÃŠ(x)
y(xi) = ÃŠ(xi) + Ã‚i, i = 1, . . . , n,
where ÃŠ(xi) denotes the physical system at initial conditions xi, and Ã‚i denotes
the error in the ith physical observation. The n observations are modeled
statistically using the computer model Ã(x, Ã‹) where Ã‹ denotes the unknown
calibration parameters. The physical system is commonly modeled as ÃŠ(x) =
Ã(x, Ã‹) + â€°(x), so that â€°(x) accounts for inadequacies in the computer model
(Kennedy and Oâ€™Hagan, 2001; Higdon et al., 2004; Bayarri et al., 2007b). Thus

Estimatation of Cosmological Parameters
771
for the n observations we have
y(xi) = Ã(xi, Ã‹) + â€°(xi) + Ã‚i, i = 1, . . . , n.
This model discrepancy term can account for numerical error (Kaipio and
Somersalo, 2007) as well as missing physics. The discrepancy term is most
commonly modeled as a GP.
When the computer model can be evaluated very quickly, the resulting
posterior can be sampled directly using MCMC as is typically done in the
Bayesian solution to inverse problems (Kaipio and Somersalo, 2004; Higdon
et al., 2003). In applications where the evaluation of the computer code is
limited, an emulator of the computer is required. Most commonly, a GP model
is used to model Ã(x, Ã‹) at untried input settings, as is done in this chapter.
Figure 26.15 shows the posterior decomposition of the various model elements
in an example where both inputs x and Ã‹ are univariate. In this case, the
discrepancy â€°(x) is given a GP prior which ensures that â€°(x) changes smoothly
with x.
This simple example highlights the complications that arise when the dis-
crepancy term is clearly non-zero. First, the posterior distribution for the cal-
ibration parameters Ã‹ is typically not well determined, and is sensitive to the
prior for â€°(x). Second, the mere existence of the discrepancy means that our
computer model is not physical reality (even at the best Ã‹). Hence one must be
careful in attributing any physical meaning to the posterior for Ã‹. In general,
this indeterminacy does not vanish with additional simulations or physical
observations.
Predictions of the physical system ÃŠ(x) at â€˜nearbyâ€™ xs are less affected by this
indeterminacy resulting from the inclusion of model discrepancy. Intuitively
this makes sense since even an empirical model will be quite accurate given
sufï¬cient numbers of physical observations. However, the allure of the physics-
based computer model is to make extrapolative predictions, at initial conditions
x that may be far away from our observational experience. For extrapolations,
the prior model for discrepancy is quite important. For example, we may trust
our computational model to predict accurately for extrapolations in initial tem-
perature, but not extrapolations initial pressure. Ideally, such concepts should
be incorporated in the prior speciï¬cation of the model discrepancy. Exactly how
to utilize the various sources of available information to specify this model dis-
crepancy is an important area of ongoing investigation. Kennedy and Oâ€™Hagan
(2001), Goldstein and Rougier (2008) and their discussions are a good starting
points for the interested reader.
Finally we note that utilizing low-ï¬delity simulations may be very helpful
in predicting the output of a high-ï¬delity computer model run. In emulator-
free applications the low-ï¬delity simulators can be used to construct auxiliary
formulations which speed up the MCMC sampling. See Higdon et al. (2002)

772
The Oxford Handbook of Applied Bayesian Analysis
0
0.5
1
0
0.5
1
â€“2
0
2
Î¸*
(a) Model runs
x
Î·(x,Î¸*)
Î·(x,Î¸*)
0
0.5
1
â€“3
â€“2
â€“1
0
1
2
3
x
y(x), Î·(x,Î¸)
y(x), Î·(x,Î¸)
Î¸
0
0.5
1
Î¸
0
0.5
1
(b) Data & prior uncertainty
0
0.5
1
0
0.5
1
â€“1
0
1
2
3
Î¸*
(c) Posterior mean for Î·(x,Î¸*)
x
0
0.5
1
â€“3
â€“2
â€“1
0
1
2
3
x
(d) Calibrated simulator prediction
0
0.5
1
â€“3
â€“2
â€“1
0
1
2
3
x
Î´ (x)
(e) Posterior model discrepancy
0
0.5
1
â€“3
â€“2
â€“1
0
1
2
3
x
y(x), Î¶(x)
(f) Calibrated prediction
Fig. 26.15
Basic computer model calibration framework. (a) An initial set of simulation runs are
carried out over the input settings (xâˆ—
j , Ã‹âˆ—
j ), j = 1, . . . , m. (b) Experimental data are collected at n = 5
initial conditions; data are given by the black dots; 90% uncertainties are given by the black lines.
The light circles correspond to the m = 20 simulation output values. (c) Posterior mean estimate
for the simulator output Ã(x, Ã‹âˆ—). (d) Posterior distribution for the calibration parameter Ã‹ and
the resulting simulator-based predictions (lines). (e) Posterior mean estimate and pointwise 90%
prediction intervals for the model discrepancy term â€°(x). (f) Posterior mean estimate and pointwise
90% prediction intervals for the physical system ÃŠ(x) which incorporate parameter uncertainty,
observation error, emulator uncertainty and model discrepancy.
for such an example. An alternative is to use the delayed acceptance scheme
of Christen and Fox (2005). Both approaches use low-ï¬delity simulations to
speed up the exploration of the posterior based on the high-ï¬delity simulator.
In settings where an emulator is required, the work of Kennedy and Oâ€™Hagan
(2000) can be employed which constructs an emulator over multiple model
resolutions, resulting in improved accuracy at the highest resolution.
A.2 Emulating multivariate computer model output
When the output from the computer model is highly multivariate, the compu-
tational demands of ï¬tting a GP model to the output becomes burdensome.
For the CMB TT spectrum output in this chapterâ€™s application, the output
is a 999-vector. If the output were simply treated as just another dimension
in the GP model, the resulting covariance matrix in the likelihood would be

Estimatation of Cosmological Parameters
773
over 105 Ã— 105. A number of approaches have been developed to deal with the
computational bottleneck produced by multivariate output.
If one treats the output support dimension (log k) as just one more dimen-
sion in the covariance model in (26.5), then the resulting covariance matrix for
the GP has a kronecker form. Thus one only needs to compute two Cholesky
decompositions to evaluate the likelihood â€“ one of a 128 Ã— 128 matrix and
one of a 999 Ã— 999 matrix â€“ instead of a single decomposition of a 105 Ã— 105
matrix. Taking advantage of this Kronecker structure in the full posterior for the
calibration problem (26.12) is a bit more involved. See Williams et al. (2006) or
Bayarri et al. (2007b) for two examples. A detailed description of a multivariate
GP emulator which also exploits the kronecker structure of the mean function
can be found in Rougier (2007).
While very efï¬cient, the kronecker approach restricts every element of the
simulation output to have the same covariance model over the input space.
An alternative is to represent the multivariate output with a basis decom-
position as in (26.4) of this chapter. These basis approaches allow separate
GP models to control each basis element. In this chapterâ€™s application, the
basis was constructed using a principal component basis derived from the
128 simulation results. While this has worked well for a number of physics-
based applications, other approaches for constructing bases have also met with
success. For example Efendiev et al. (2009) use a Karhunen â€“ Loeve basis for an
oil reservoir representation and Bayarri et al. (2007a) use wavelets to represent
one dimensional time history of force.
Acknowledgments
We thank Adrian Pope for useful discussions and insight regarding the matter
power spectrum. Also, we would like to thank Antony Lewis for help with
CAMB and Licia Verde for clariï¬cation on the WMAP-III data sets. A special
acknowledgment is due to supercomputing time awarded to us under the LANL
Institutional Computing Initiative. This research is supported by the DOE
under contract W-7405-ENG-36, and with support from the LDRD program at
Los Alamos National Laboratory.
References
Adelman-McCarthy, J., Agueros, M., Allam, S., Anderson, K., Anderson, S., Annis, J., Bahcall,
N., Baldry, I., Barentine, J., Berlind, A., et al. (2006). The fourth data release of the Sloan
Digital Sky Survey. The Astrophysical Journal Supplement Series, 162, 38â€“48.
Bayarri, M., Berger, J., Cafeo, J., Garcia-Donato, G., Liu, F., Sacks, J., and Walsh, D. (2007a).
Computer model validation with functional output. Annals of Statistics, 35, 1874â€“1906.

774
The Oxford Handbook of Applied Bayesian Analysis
Bayarri, M., Berger, J., Paulo, R., Sacks, J., Cafeo, J., Cavendish, J., Lin, C., and Tu, J. (2007b).
A framework for validation of computer models. Technometrics, 49, 138â€“154.
Christen, J. and Fox, C. (2005). Markov chain Monte Carlo using an approximation. Journal of
Computational and Graphical Statistics, 14, 795â€“810.
Currin, C., Mitchell, T., Morris, M., and Ylvisaker, D. (1991). Bayesian prediction of determinis-
tic functions, with applications to the design and analysis of computer experiments. Journal
of the American Statistical Association, 86, 953â€“963.
Efendiev, Y., Datta-Gupta, A., Ginting, V., Ma, X., and Mallick, B. (2009). An efï¬cient two-
stage Markov chain Monte Carlo method for dynamic data integration. To appear in Water
Resources Research.
Fendt, W. and Wandelt, B. (2007). Pico: Parameters for the impatient cosmologist. The Astro-
physical Journal, 654, 2â€“11.
Frieman, J., Turner, M., and Huterer, D. (2008). Dark energy and the accelerating universe.
Arxiv preprint arXiv:0803.0982.
Goldstein, M. and Rougier, J. C. (2008). Reiï¬ed Bayesian modelling and inference for physical
systems (with discussion). Journal of Statistical Planning and Inference, 139, 1221â€“1239.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their appli-
cations. Biometrika, 57, 97â€“109.
Higdon, D., Gattiker, J. R., and Williams, B. J. (2008). Computer model calibration using high
dimensional output. Journal of the American Statistical Association, 103, 570â€“583.
Higdon, D., Kennedy, M., Cavendish, J., Cafeo, J., and Ryne, R. D. (2004). Combining ï¬eld
observations and simulations for calibration and prediction. SIAM Journal of Scientiï¬c Com-
puting, 26, 448â€“466.
Higdon, D., Lee, H., and Bi, Z. (2002). A Bayesian approach to characterizing uncertainty
in inverse problems using coarse and ï¬ne scale information. IEEE Transactions in Signal
Processing, 50, 389â€“399.
Higdon, D. M., Lee, H., and Holloman, C. (2003). Markov chain Monte Carlo-based approaches
for inference in computationally intensive inverse problems. In Bayesian Statistics 7 (eds.
J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith, and
M. West), pp. 181â€“197. Oxford University Press, Oxford.
Jimenez, R., Verde, L., Peiris, H., and Kosowsky, A. (2004). Fast cosmological parameter esti-
mation from microwave background temperature and polarization power spectra. Physical
Review D, 70, 23005.
Kaipio, J. and Somersalo, E. (2007). Statistical inverse problems: Discretization, model reduc-
tion and inverse crimes. Journal of Computational and Applied Mathematics, 198, 493â€“504.
Kaipio, J. P. and Somersalo, E. (2004). Statistical and Computational Inverse Problems. Springer,
New York.
Kennedy, M. and Oâ€™Hagan, A. (2000). Predicting the output from a complex computer code
when fast approximations are available. Biometrika, 87, 1â€“13.
Kennedy, M. and Oâ€™Hagan, A. (2001). Bayesian calibration of computer models (with discus-
sion). Journal of the Royal Statistical Society (Series B), 68, 425â€“464.
Kosowsky, A., Milosavljevic, M., and Jimenez, R. (2002). Efï¬cient cosmological parameter
estimation from microwave background anisotropies. Physical Review D, 66, 63007.
Lewis, A., Challinor, A., and Lasenby, A. (2000). Efï¬cient computation of cosmic microwave
background anisotropies in closed Friedmannâ€“Robertsonâ€“Walker models. The Astrophysical
Journal, 538, 473â€“476.
Linkletter, C., Bingham, D., Hengartner, N., Higdon, D., and Ye, K. (2006). Variable selection
for Gaussian process models in computer experiments. Technometrics, 48, 478â€“490.

Estimatation of Cosmological Parameters
775
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., and Teller, E. (1953). Equations of
state calculations by fast computing machines. Journal of Chemical Physics, 21, 1087â€“1091.
Oakley, J. and Oâ€™Hagan, A. (2004). Probabilistic sensitivity analysis of complex models. Journal
of the Royal Statistical Society (Series B), 66, 751â€“769.
Ramsay, J. O. and Silverman, B. W. (1997). Functional Data Analysis. Springer, New York.
Rougier, J. (2007). Efï¬cient emulators for multivariate deterministic functions. Journal of Com-
putational and Graphical Studies, 17, 827â€“843.
Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989). Design and analysis of computer
experiments (with discussion). Statistical Science, 4, 409â€“423.
Sahni, V. and Starobinsky, A. (2000). The case for a positive cosmological Lambda-term (2000).
International Journal of Modern Physics D, 9, 373.
Santner, T. J., Williams, B. J., and Notz, W. I. (2003). Design and Analysis of Computer Experi-
ments. Springer, New York.
Smoot, G., Bennett, C., Kogut, A., Wright, E., Aymon, J., Boggess, N., Cheng, E., de Amici, G.,
Gulkis, S., Hauser, M., et al. (1992). Structure in the COBE differential microwave radiometer
ï¬rst-year maps. Astrophysical Journal, 396, 1.
Spergel, D., Bean, R., Dore, O., Nolta, M., Bennett, C., Dunkley, J., Hinshaw, G., Jarosik,
N., Komatsu, E., Page, L., et al. (2007). Three-year Wilkinson Microwave Anisotropy Probe
(WMAP) observations: Implications for cosmology. The Astrophysical Journal Supplement
Series, 170, 377â€“408.
Tang, B. (1993). Orthogonal array-based Latin hypercubes. Journal of the American Statistical
Association, 88, 1392â€“1397.
Tegmark, M., Blanton, M., Strauss, M., Hoyle, F., Schlegel, D., Scoccimarro, R., Vogeley, M.,
Weinberg, D., Zehavi, I., Berlind, A., et al. (2004). The three-dimensional power spectrum of
galaxies from the Sloan Digital Sky Survey. The Astrophysical Journal, 606, 702â€“740.
Tegmark, M., Eisenstein, D., Strauss, M., Weinberg, D., Blanton, M., Frieman, J., Fukugita, M.,
Gunn, J., Hamilton, A., Knapp, G., et al. (2006). Cosmological constraints from the SDSS
luminous red galaxies. Physical Review D, 74, 123507.
Williams, B., Higdon, D., Gattiker, J., Moore, L., McKay, M., and Keller-McNulty, S. (2006).
Combining experimental data and computer simulations, with an application to ï¬‚yer plate
experiments. Bayesian Analysis, 1, 765â€“792.
Ye, K. Q., Li, W., and Sudjianto, A. (2000). Algorithmic construction of optimal symmetric Latin
hypercube designs. Journal of Statistical Planning and Inference, 90, 145â€“159.

Â·27Â·
Probabilistic grammars and hierarchical
Dirichlet processes
Percy Liang, Michael I. Jordan and Dan Klein
27.1 Introduction
The ï¬eld of natural language processing (NLP) aims to develop algorithms
that allow computers to understand and generate natural language. The ï¬eld
emerged from computational linguistics, a ï¬eld whose early history was shaped
in part by a rejection of statistical approaches to language, where â€˜statisticalâ€™
at the time generally referred to simplistic Markovian models on observed
sequences of words. Despite this unfavorable historical context, statistical
approaches to NLP have been in ascendancy over the past decade (Manning
and SchÃ¼tze, 1999), driven in part by the availability of large corpora of text
and other linguistic resources on the Internet, and driven in part by a growth
in sophistication among NLP researchers regarding the scope of statistical
modelling, particularly latent-variable modelling. The phenomenon of language
itself is also responsible: language is replete with ambiguity, so it is inevitable
that formal inferential methods should play a signiï¬cant role in managing this
ambiguity.
The majority of the work in statistical NLP has been non-Bayesian, but there
is reason to believe that this is a historical accident. Indeed, despite the large
corpora, sparse data problems abound to which hierarchical Bayesian methods
seem well suited. Also, the conditional perspective of Bayesian statistics seems
particularly appropriate for natural language â€“ conditioning on the current
sentence and the current context can provide precise inference despite the high
degree of ambiguity.
In the current chapter, we discuss a Bayesian approach to the problem
of syntactic parsing and the underlying problems of grammar induction and
grammar reï¬nement. The central object of study is the parse tree, an example of
which is shown in Figure 27.1. A substantial amount of the syntactic structure
and relational semantics of natural language sentences can be described using
parse trees. These trees play a central role in a range of activities in modern
NLP, including machine translation (Galley et al., 2004), semantic role extrac-
tion (Gildea and Jurafsky, 2002), and question answering (Hermjakob, 2001),

Probabilistic Grammars and Hierarchical Dirichlet Processes
777
Sentence
Noun-Phrase
Pronoun
They
Verb-Phrase
Verb
solved
Noun-Phrase
Determiner
the
Noun
problem
Prepositional-Phrase
Preposition
with
Noun-Phrase
Proper-Noun
Bayesian
Plural-Noun
statistics
Fig. 27.1 A parse tree for the sentence They solved the problem with Bayesian statistics.
just to name a few. From a statistical perspective, parse trees are an extremely
rich class of objects, and our approach to capturing this class probabilistically
will be to make use of tools from Bayesian nonparametric statistics.
It seems reasonable enough to model parse trees using context-free gram-
mars (CFGs); indeed, this goal was the original motivation behind the devel-
opment of the CFG formalism (Chomsky, 1956), and it remains a major
focus of research on parsing to this day. Early work on NLP parsing con-
centrated on efï¬cient algorithms for computing the set of all parses for a
sentence under a given CFG. Unfortunately, as we have alluded to, natural
language is highly ambiguous. In fact, the number of parses for a sentence
grows exponentially with its length. As a result, systems which enumer-
ated all possibilities were not useful in practice. Modern work on parsing
has therefore turned to probabilistic models which place distributions over
parse trees and probabilistic inference methods which focus on likely trees
(Lari and Young, 1990).
The workhorse model family for probabilistic parsing is the family of proba-
bilistic context-free grammars (PCFGs),1 which are probabilistic generalizations
of CFGs and structural generalizations of hidden Markov models (HMMs). A
PCFG (described formally in Section 27.1.1) is a branching process in which
nodes iteratively rewrite from top to bottom, eventually terminating in dedi-
cated lexical items, i.e. words. Each node is rewritten independently according
to a multinomial distribution speciï¬c to that nodeâ€™s symbol. For example, a
noun phrase frequently rewrites as a determiner followed by a noun (e.g. the
problem).
Early work focused on grammar induction (also known as grammatical infer-
ence): estimating grammars directly from raw sentences without any other
type of supervision (Carroll and Charniak, 1992). Grammar induction is an
important scientiï¬c problem connecting cognitive science, linguistics, statis-
tics, and even philosophy. A successful grammar induction system would
have important implications for human language learning, and it would
also be a valuable asset for being able to parse sentences with little human
1 Also known as stochastic context-free grammars (SCFGs).

778
The Oxford Handbook of Applied Bayesian Analysis
effort. However, a combination of model misspeciï¬cation and local optima
issues with the EM algorithm stymied these initial attempts. It turned out
that it was necessary to impose more constraints on the tree structure
(Pereira and Shabes, 1992).
Only with the advent of treebanks (Marcus et al., 1993) â€“ hand-labelled collec-
tions of parse trees â€“ were NLP researchers able to develop the ï¬rst successful
broad-coverage natural language parsers, but it still took a decade before the
performance of the best parsers started to level off. In this supervised setting,
maximum likelihood estimates for PCFGs have a simple closed-form solution:
the rule probabilities of the PCFG are proportional to the counts of the asso-
ciated grammar productions across all of the trees in the treebank (Charniak,
1996). However, such statistical grammars do not perform well for parsing. The
problem is that treebanks contain only a handful of very coarse symbols, such
as np (noun phrase) and vp (verb phrase), so the conditional independences
assumed by the PCFG over these coarse symbols are unrealistic. The true
syntactic process is vastly more complex. For example, noun phrases can be
subjects or objects, singular or plural, deï¬nite or indeï¬nite, and so on. Similarly,
verb phrases can be active or passive, transitive or intransitive, past or present,
and so on. For a PCFG to adequately capture the true syntactic process, ï¬ner-
grained grammar symbols are required. Much of the past decade of NLP pars-
ing work can be seen as trying to optimally learn such ï¬ne-grained grammars
from coarse treebanks.
Grammars can be reï¬ned in many ways. Some reï¬nements are syntactically
motivated. For example, if we augment each symbol with the symbol of its
parent in the tree, we get symbols such as np-vp, which represents direct
object noun phrases, distinct from np-s, which represents subject ones. This
strategy is called parent annotation (Johnson, 1998) and can be extended (Klein
and Manning, 2003). For the parse tree in Figure 27.1, if each nodeâ€™s symbol
is augmented with the symbols of its parent and grandparent, a maximum
likelihood grammar would only allow they to be produced under a noun phrase
in subject position, disallowing ungrammatical sentences such as The problem
solved they with Bayesian statistics.
Other reï¬nements are semantically motivated. In many linguistic theories,
each phrase is identiï¬ed with a head word, which characterizes many of the
important properties of the phrase. By augmenting the symbol of each node
with the head word of the phrase under that node, we allow some degree of
semantic plausibility to be captured by the parse tree. This process is called
lexicalization, and was the basis for the ï¬rst generation of practical treebank
parsers (Collins, 1999; Charniak, 2000). Consider the sentence in Figure 27.1.
It is actually ambiguous: did they use Bayesian statistics to solve the problem
at hand (A) or did Bayesian statistics itself have a fundamental ï¬‚aw which they
resolved (B)? Though both are perfectly valid syntactically, (B) is implausible

Probabilistic Grammars and Hierarchical Dirichlet Processes
779
semantically, and we would like our model to prefer (A) over (B). If we lexicalize
the verb phrase with solved (replace vp with vp-solved) and the preposition phrase
with statistics (replace pp with pp-statistics),2 we allow semantics to interact
through the tree, yielding a better model of language that could prefer (A)
over (B). Lexicalization produces a very rich model at the cost of multiplying
the number of parameters by millions. In order to cope with the resulting
problems of high-dimensionality, elaborate smoothing and parameter-tying
methods were employed (Collins, 1999; Charniak, 2000), a recurrent theme
in NLP.
Both parent annotation and lexicalization kept the grammar learning prob-
lem fully observed: given the coarse trees in the treebank, the potential
uncertainty resides in the choice of head words or parents, and these were
typically propagated up the tree in deterministic ways. The only inferential
problem remaining was to ï¬t the grammar parameters, which reduces (in the
point estimation setting generally adopted in statistical NLP) to counting and
smoothing.
More recently, latent-variable models have been successfully employed for
automatically reï¬ning treebank grammars (Matsuzaki et al., 2005; Petrov et al.,
2006). In such approaches, each symbol is augmented with a latent cluster
indicator variable and the marginal likelihood of the model is optimized. Rather
than manually specifying the reï¬nements, these latent-variable models let the
data speak, and, empirically, the resulting reï¬nements turn out to encode a mix-
ture of syntactic and semantic information not captured by the coarse treebank
symbols. The latent clusters allow for the modeling of long-range dependencies
while keeping the number of parameters modest.
In this chapter, we address a fundamental question which underlies all of
the previous work on parsing: what priors over PCFGs are appropriate? In
particular, we know that we must trade off grammar complexity (the number
of grammar symbols) against the amount of data present. As we get more data,
more of the underlying grammatical processes can be adequately modelled.
Past approaches to complexity control have considered minimum description
length (Stolcke and Omohundro, 1994) and procedures for growing the gram-
mar size heuristically (Petrov et al., 2006). While these procedures can be
quite effective, we would like to pursue a Bayesian nonparametric approach
to PCFGs so that we can state our assumptions about the problem of grammar
growth in a coherent way. In particular, we deï¬ne a nonparametric prior over
PCFGs which allocates an unbounded number of symbols to the grammar
through a Dirichlet process (Ferguson, 1973, 1974), then shares those symbols
throughout the grammar using a Bayesian hierarchy of Dirichlet processes
2 According to standard NLP head conventions, the preposition with would be the head word of a
prepositional phrase, but here we use a non-standard alternative to focus on the semantic properties of
the phrase.

780
The Oxford Handbook of Applied Bayesian Analysis
(Teh et al., 2006). We call the resulting model a hierarchical Dirichlet process
probabilistic context-free grammar (HDP-PCFG). In this chapter, we present the
formal probabilistic speciï¬cation of the HDP-PCFG, algorithms for posterior
inference under the HDP-PCFG, and experiments on grammar learning from
large-scale corpora.
27.1.1 Probabilistic context-free grammars (PCFGs)
Our HDP-PCFG model is based on probabilistic context-free grammars
(PCFGs), which have been a core modelling technique for many aspects of
syntactic structure (Charniak, 1996; Collins, 1999) as well as for problems in
domains outside of natural language processing, including computer vision
(Zhu and Mumford, 2006) and computational biology (Sakakibara, 2005; Dyrka
and Nebel, 2007).
Formally, a PCFG is speciï¬ed by the following:
r a set of terminal symbols  (the words in the sentence),
r a set of nonterminal symbols S,
r a designated root nonterminal symbol Root âˆˆS, and
r rule probabilities Ë† = (Ë†s(â€ž) : s âˆˆS, â€ž âˆˆ âˆª(S Ã— S)) with Ë†s(â€ž) â‰¥0 and

â€ž Ë†s(â€ž) = 1.
We restrict ourselves to rules s â†’â€ž that produce a right-hand side â€ž which is
either a single terminal symbol (â€ž âˆˆ) or a pair of non-terminal symbols (â€ž âˆˆ
S Ã— S). Such a PCFG is said to be in Chomsky normal form. The restriction to
Chomsky normal form is made without loss of generality; it is straightforward
to convert a rule with multiple children into a structure (e.g. a right-branching
chain) in which each rule has at most two children.
A PCFG deï¬nes a distribution over sentences and parse trees via the follow-
ing generative process: start at a root node with s = Root and choose to apply
rule s â†’â€ž with probability Ë†s(â€ž); â€ž speciï¬es the symbols of the children. For
children with nonterminal symbols, recursively generate their subtrees. The
process stops when all the leaves of the tree are terminals. Call the sequence of
terminals the yield. More formally, a parse tree has a set of nonterminal nodes
N along with the symbols corresponding to these nodes s = (si âˆˆS : i âˆˆN). Let
NE denote the nodes having one terminal child and NB denote the nodes having
two nonterminal children. The tree structure is represented by c = (c j(i) : i âˆˆ
NB, j = 1, 2), where c j(i) âˆˆN is the j-th child node of i from left to right. Let
z = (N, s, c) denote the parse tree and x = (xi : i âˆˆNE) denote the yield.
The joint probability of a parse tree z and its yield x is given by
p(x, z|Ë†) =

iâˆˆNB
Ë†si

sc1(i), sc2(i)
 
iâˆˆNE
Ë†si(xi).
(27.1)

Probabilistic Grammars and Hierarchical Dirichlet Processes
781
PCFGs are similar to hidden Markov models (HMMs) and these similarities
will guide our development of the HDP-PCFG. It is important to note at the out-
set, however, an important qualitative difference between HMMs and PCFGs.
While the HMM can be represented as a graphical model (a Markovian graph
in which the pattern of missing edges corresponds to assertions of conditional
independence), the PCFG cannot. Conditioned on the structure of the parse tree
(N, c), we have a graphical model over the symbols s, but the structure itself is
a random object â€“ we must run an algorithm to compute a probability distribu-
tion over these structures. As in the case of the forward-backward algorithm for
HMMs, this algorithm is an efï¬cient dynamic programming algorithm â€“ it is
referred to as the â€˜inside-outside algorithmâ€™ and it runs in time cubic in length
of the yield (Lari and Young, 1990). The inside-outside algorithm will play an
important role in the inner loop of our posterior inference algorithm for the
HDP-PCFG. Indeed, we will ï¬nd it essential to design our model such that it
can exploit the inside-outside algorithm.
Traditionally, PCFGs are deï¬ned with a ï¬xed, ï¬nite number of non-terminals
S, where the parameters Ë† are ï¬t using (smoothed) maximum likelihood. The
focus of this chapter is on developing a nonparametric version of the PCFG
which allows S to be countably inï¬nite and which performs approximate poste-
rior inference over the set of non-terminal symbols and the set of parse trees. To
deï¬ne the HDP-PCFG and develop effective posterior inference algorithms for
the HDP-PCFG, we need to bring several ingredients together â€“ most notably
the ability to generate new symbols and to tie together multiple usages of the
same symbol on a parse tree (provided by the HDP), and the ability to efï¬ciently
compute probability distributions over parse trees (provided by the PCFG).
Thus, the HDP-PCFG is a Bayesian nonparametric generalization of the PCFG,
but it can also be viewed as a generalization along the Chomsky hierarchy,
taking a nonparametric Markovian model (the HDP-HMM) to a nonparametric
probabilistic grammar (the HDP-PCFG).
The rest of this chapter is organized as follows. Section 27.2 provides the
probabilistic speciï¬cation of the HDP-PCFG for grammar induction, and Sec-
tion 27.3 extends this speciï¬cation to an architecture appropriate for grammar
reï¬nement (the HDP-PCFG-GR). Section 27.4 describes an efï¬cient variational
method for approximate Bayesian inference in these models. Section 27.5
presents experiments: using the HDP-PCFG to induce a small grammar from
raw text, and using the HDP-PCFG-GR to parse the Wall Street Journal, a
standard large-scale dataset. For supplementary information, see the appendix,
where we review DP-based models related to and leading up to the HDP-PCFG
(Appendix A.1), discuss general issues related to approximate inference for
these types of models (Appendix A.2), and provide some empirical intuition
regarding the interaction between model and inference (Appendix A.3). The
details of the variational inference algorithm are given in Appendix B.

782
The Oxford Handbook of Applied Bayesian Analysis
27.2 The hierarchical Dirichlet process PCFG (HDP-PCFG)
At the core of the HDP-PCFG sits the Dirichlet process (DP) mixture model
(Antoniak, 1974), a building block for a wide variety of nonparametric Bayesian
models. The DP mixture model captures the basic notion of clustering which
underlies symbol formation in the HDP-PCFG. From there, the HDP-PCFG
involves several structural extensions of the DP mixture model. As a ï¬rst step-
ping stone, consider hidden Markov models (HMMs), a dynamic generalization
of mixture models, where clusters are linked structurally according to a Markov
chain. To turn the HMM into a nonparametric Bayesian model, we use the
hierarchical Dirichlet process (HDP), yielding the HDP-HMM, an HMM with
a countably inï¬nite state space. The HDP-PCFG differs from the HDP-HMM in
that, roughly speaking, the HDP-HMM is a chain-structured model while HDP-
PCFG is a tree-structured model. But it is important to remember that the tree
structure in the HDP-PCFG is a random object over which inferences must be
made. Another distinction is that rules can rewrite to two nonterminal symbols
jointly. For this, we need to deï¬ne DPs with base measures which are products
of DPs, thereby adding another degree of complexity to the HDP machinery.
For a gradual introduction, see Appendix A.1, where we walk through the
intermediate steps leading up to the HDP-PCFG â€“ the Bayesian ï¬nite mixture
model (Appendix A.1.1), the DP mixture model (Appendix A.1.2), and the HDP-
HMM (Appendix A.1.3).
27.2.1 Model deï¬nition
Figure 27.2 deï¬nes the generative process for the HDP-PCFG, which consists
of two stages: we ï¬rst generate the grammar (which includes the rule probabil-
ities) speciï¬ed by (â€š, Ë†); then we generate a parse tree and its yield (z, x) using
that grammar. To generate the grammar, we ï¬rst draw a countably inï¬nite set of
stick-breaking probabilities, â€š âˆ¼GEM(Â·), which provides us with a base distrib-
ution over grammar symbols, represented by the positive integers (Figure 27.3a;
see Appendix A.1.2 for a formal deï¬nition of the stick-breaking distribution).
Next, for each symbol z = 1, 2, . . . , we generate the probabilities of the rules
of the form z â†’â€ž, i.e. those which condition on z as the left-hand side. The
emission probabilities Ë†E
z are drawn from a Dirichlet distribution, and provide
multinomial distributions over terminal symbols . For the binary production
probabilities, we ï¬rst form the product distribution â€šâ€šâŠ¤(Figure 27.3b), repre-
sented as a doubly-inï¬nite matrix. The binary production probabilities Ë†B
z are
then drawn from a Dirichlet process with base distribution â€šâ€šâŠ¤â€“ this provides
multinomial distributions over pairs of nonterminal symbols (Figure 27.3c).
The Bayesian hierarchy ties these distributions together through the base dis-
tribution over symbols, so that the grammar effectively has a globally shared

Probabilistic Grammars and Hierarchical Dirichlet Processes
783
HDP-PCFG
b ~ GEM(a) 
[draw top-level symbol probabilities]
For each grammar symbol z ÃŽ {1,2, ...}:
fz
T ~ Dir(aT) 
[draw rule type parameters]
fz
E ~ Dir(aE) 
[draw emission parameters]
fz
B ~ DP(aB, bbT) 
[draw binary production parameters]
For each node i in the parse tree:
  ti ~ Mult(fT
zi) 
[choose rule type]
If ti = EMISSION:
     xi ~ Mult(fE
zi) 
[emit terminal symbol]
If ti = BINARY-PRODUCTION:
(zc1(i), zc2(i)) ~ Mult(fB
zi) 
[generate child symbols]
b
fz
B
fz
T
fz
E
z
Â¥
z1
z2
x2
z3
x3
Fig. 27.2 The probabilistic speciï¬cation of the HDP-PCFG. We also present a graphical represen-
tation of the model. Drawing this graphical model assumes that the parse is known, which is not
our assumption, so this representation should be viewed as simply suggestive of the HDP-PCFG. In
particular, we show a simple ï¬xed tree in which node z1 has two children (z2 and z3), each of which
has one observed terminal child.
inventory of symbols. Note that the HDP-PCFG hierarchy ties distributions over
symbol pairs via distributions over single symbols, in contrast to the hierarchy
in the standard HDP-HMM, where the distributions being tied are deï¬ned over
the same space as that of the base distribution. Finally, we generate a â€˜switchingâ€™
distribution Ë†T
z over the two rule types {Emission, Binary-Production} from
a symmetric Dirichlet. The shapes of all the Dirichlet distributions and the
Dirichlet processes in our models are governed by concentration hyperpara-
meters: Â·, Â·T, Â·B, Â·E.
(a) b~GEM(a)
1
2
3
4
5
6
Â·Â·Â·
(b) bbT
left child
right child
(c) fB
z ~DP(aB, bbT)
left child
right child
Fig. 27.3 The generation of binary production probabilities given the top-level symbol probabilities
â€š. First, â€š is drawn from the stick-breaking prior, as in any DP-based model (a). Next, the outer-
product â€šâ€šâŠ¤is formed, resulting in a doubly-inï¬nite matrix (b). We use this as the base distribution
for generating the binary production distribution from a DP centered on â€šâ€šâŠ¤(c).

784
The Oxford Handbook of Applied Bayesian Analysis
Given a grammar, we generate a parse tree and sentence in the same manner
as for an ordinary PCFG: start with the root node having the designated root
symbol. For each non-terminal node i, we ï¬rst choose a rule type ti using Ë†T
zi and
then choose a rule with that type â€“ either an emission from Mult

Ë†E
zi

producing
xi or a binary production from Mult

Ë†T
zi

producing

zc1(i), zc2(i)

. We recursively
apply the procedure on any non-terminal children.
27.2.2 Partitioning the preterminal and constituent symbols
It is common to partition non-terminal grammar symbols into those that emit
only terminals (preterminal symbols) and those that produce only two child
grammar symbols (constituent symbols). An easy way to accomplish this is to
let Â·T = (0, 0). The resulting Dir(0, 0) prior on the rule type probabilities forces
any draw Ë†T
z âˆ¼Dir(Â·T) to put mass on only one rule type. Despite the simplicity
of this approach, is not suitable for our inference method, so let us make the
partitioning more explicit.
Deï¬ne two disjoint inventories, one for preterminal symbols (â€šP âˆ¼GEM(Â·))
and one for constituent symbols (â€šC âˆ¼GEM(Â·)). Then, we can deï¬ne the
following four rule types: preterminal-preterminal productions with rule
probabilities Ë†PP
z
âˆ¼DP

Â·PP, â€šP(â€šP)âŠ¤
, preterminal-constituent productions
with Ë†PC
z
âˆ¼DP

Â·PC, â€šP(â€šC)âŠ¤
, constituent-preterminal productions with Ë†CP
z
âˆ¼
DP

Â·CP, â€šC(â€šP)âŠ¤
,
and
constituent-constituent
productions
with
Ë†CC
z
âˆ¼
DP

Â·CC, â€šC(â€šC)âŠ¤
. Each node has a symbol which is either a preterminal (P, z)
or a constituent (C, z). In the former case, a terminal is produced from Ë†E
z . In
the latter case, one of the four rule types is ï¬rst chosen given Ë†T
z and then a rule
of that type is chosen.
27.2.3 Other nonparametric grammars
An alternative deï¬nition of an HDP-PCFG would be as follows: for each
symbol z, draw a distribution over left child symbols Ë†B1
z âˆ¼DP(Â·, â€š) and an
independent distribution over right child symbols Ë†B2
z âˆ¼DP(Â·, â€š). Then deï¬ne
the binary production distribution as the product Ë†B
z = Ë†B1
z Ë†B2âŠ¤
z
. This also yields
a distribution over symbol pairs and deï¬nes a different type of nonparamet-
ric PCFG. This model is simpler than the HDP-PCFG and does not require
any additional machinery beyond the HDP-HMM. However, the modelling
assumptions imposed by this alternative are unappealing as they assume the
left child and right child are independent given the parent, which is certainly
not the case in natural language.
Other alternatives to the HDP-PCFG include the adaptor grammar framework
of Johnson et al. (2006) and the inï¬nite tree framework of Finkel et al. (2007).
Both of these Bayesian nonparametric models have been developed using
the Chinese restaurant process rather than the stick-breaking representation.
In an adaptor grammar, the grammar symbols are divided into adapted and

Probabilistic Grammars and Hierarchical Dirichlet Processes
785
non-adapted symbols. Non-adapted symbols behave like parametric PCFG sym-
bols, while adapted symbols are associated with a Bayesian nonparametric prior
over subtrees rather than over pairs of child symbols as in PCFG. While this
gives adaptor grammars the ability to capture more global properties of a sen-
tence, recursion is not allowed on adaptor symbols (that is, coexistence of rules
of the form A â†’BC and B â†’DA is disallowed if A is an adaptor symbol).
Applications of adaptor grammars have mostly focused on the modelling of
word segmentation and collocation rather than full constituency syntax. The
inï¬nite tree framework is more closely related to the HDP-PCFG, differing
principally in that it has been developed for dependency parsing rather than
constituency parsing.
27.3 The HDP-PCFG for Grammar Reï¬nement (HDP-PCFG-GR)
While the HDP-PCFG is suitable for grammar induction, where we try to infer a
full grammar from raw sentences, in practice we often have access to treebanks,
which are collections of tens of thousands of sentences, each hand-labelled
with a syntactic parse tree. Not only do these observed trees help constrain the
model, they also declare the coarse symbol inventories which are assumed by
subsequent linguistic processing. As discussed in Section 27.1, these treebank
trees represent only the coarse syntactic structure. In order to obtain accurate
parsing performance, we need to learn a more reï¬ned grammar.
We introduce an extension of the HDP-PCFG appropriate for grammar
reï¬nement (the HDP-PCFG-GR). The HDP-PCFG-GR differs from the basic
HDP-PCFG in that the coarse symbols are ï¬xed and only their subsymbols
are modelled and controlled via the HDP machinery. This formulation makes
explicit the level of coarse, observed symbols and provides implicit control over
model complexity within those symbols using a single, uniï¬ed probabilistic
model.
27.3.1 Model deï¬nition
The essential difference in the grammar reï¬nement setting is that now we have
a collection of HDP-PCFG models, one for each symbol s âˆˆS, and each HDP-
PCFG operates at the subsymbol level. In practical applications we also need to
allow unary productions â€“ a symbol producing exactly one child symbol â€“ the
PCFG counterpart of transitions in HMMs. Finally, each non-terminal node
i âˆˆN in the parse tree has a symbol-subsymbol pair (si, zi), so each subsymbol
needs to specify a distribution over both child symbols and subsymbols. The
former is handled through a ï¬nite Dirichlet distribution since the ï¬nite set of
symbols is known. The latter is handled with the HDP machinery, since the
(possibly unbounded) set of subsymbols is unknown.
Despite the apparent complexity of the HDP-PCFG-GR model, it is fun-
damentally a PCFG where the symbols are (s, z) pairs with s âˆˆS and z âˆˆ

786
The Oxford Handbook of Applied Bayesian Analysis
HDP-PCFG for grammar reï¬nement (HDP-PCFG-GR)
For each symbol s âˆˆ S:
bs âˆ¼ GEM(a ) 
[draw subsymbol probabilities]
   For each subsymbol z âˆˆ {1,2,...}:
fT
sz âˆ¼ Dir(aT) 
[draw rule type parameters]
fE
sz âˆ¼ Dir(aE(s)) 
[draw emission parameters]
fu
sz âˆ¼ Dir(au(s)) 
[draw unary symbol production parameters]
For each child symbol sÂ¢ âˆˆ S:
fU
szsÂ¢ âˆ¼ DP(aU, bsÂ¢) 
[draw unary subsymbol production parameters]
fb
sz âˆ¼ Dir(ab(s)) 
[draw binary symbol production parameters]
For each pair of child symbols (sâ€², sâ€³) âˆˆ S âˆ‘ S:
fB
szsÂ¢sâ‰¤âˆ¼ DP(aB, bsâ€²bT
sâ€³) 
[draw binary subsymbol production parameters]
For each node i in the parse tree:
   ti âˆ¼ Mult(fT
sizi) 
[choose rule type]
   If ti = EMISSION:
      xi âˆ¼ Mult(fE
sizi) 
[emit terminal symbol]
   If ti = UNARY-PRODUCTION:
      sc1(i) âˆ¼ Mult(fu
sizi) 
[generate child symbol]
      zc1(i) âˆ¼ Mult(fU
sizisc1(i)) 
[generate child subsymbol]
   If ti = BINARY-PRODUCTION:
(sc1(i), sc2(i)) âˆ¼ Mult(fsizi) 
[generate child symbols]
      (zc1(i), zc2(i)) âˆ¼ Mult(fB
sizisc1(i)sc2(i)) 
[generate child subsymbols]
Fig. 27.4 The deï¬nition of the HDP-PCFG for grammar reï¬nement (HDP-PCFG-GR).
{1, 2, . . . }. The rules of this PCFG take one of three forms: (s, z) â†’x for
some terminal symbol x âˆˆ, unary productions (s, z) â†’(s â€², zâ€²) and binary
productions (s, z) â†’(s â€², zâ€²)(s â€²â€², zâ€²â€²). We can think of having a single (inï¬nite)
multinomial distribution over all right-hand sides given (s, z). Interestingly,
the prior distribution of these rule probabilities is no longer a Dirichlet or a
Dirichlet process, but rather a more general PÃ³lya tree over distributions on
{ âˆª(S Ã— Z) âˆª(S Ã— Z Ã— S Ã— Z)}.
Figure 27.4 provides the complete description of the generative process. As
in the HDP-PCFG, we ï¬rst generate the grammar: For each (coarse) sym-
bol s, we generate a distribution over its subsymbols â€šs, and then for each
subsymbol z, we generate a distribution over right-hand sides via a set of
conditional distributions. Next, we generate a distribution Ë†T
sz over the three
rule types {Emission, Unary-Production, Binary-Production}. The emis-
sion distribution Ë†T
sz is drawn from a Dirichlet as before. For the binary rules,
we generate from a Dirichlet a distribution Ë†b
sz over pairs of coarse symbols
and then for each coarse symbol s â€² âˆˆS, we generate from a Dirichlet process
a distribution Ë†b
szs â€²s â€²â€² over pairs of subsymbols (zâ€², zâ€²â€²). The unary probabilities

Probabilistic Grammars and Hierarchical Dirichlet Processes
787
are generated analogously. Given the grammar, we generate a parse tree and
a sentence via a recursive process similar to the one for the HDP-PCFG;
the main difference is that at each node, we ï¬rst generate the coarse sym-
bols of its children and then the child subsymbols conditioned on the coarse
symbols.
The HDP-PCFG-GR is a generalization of many of the DP-based models in
the literature. When there is exactly one symbol (|S| = 1), it reduces to the HDP-
PCFG, where the subsymbols in the HDP-PCFG-GR play the role of symbols
in the HDP-PCFG. Suppose we have two distinct symbols A and B and that
all trees are three node chains of the form A â†’B â†’x. Then the HDP-PCFG-
GR is equivalent to the nested Dirichlet process (Rodriguez et al., 2008). If the
subsymbols of A are observed for all data points, we have a plain hierarchical
Dirichlet process (Teh et al., 2006), where the subsymbol of A corresponds to
the group of data point x.
27.4 Bayesian inference
In this section, we describe an approximate posterior inference algorithm
for the HDP-PCFG based on variational inference. We present an algorithm
only for the HDP-PCFG; the extension to the HDP-PCFG-GR is straight-
forward, requiring only additional bookkeeping. For the basic HDP-PCFG,
Section 27.4.1 describes mean-ï¬eld approximations of the posterior. Sec-
tion 27.4.2 discusses the optimization of this approximation. Finally, Sec-
tion 27.4.3 addresses the use of this approximation for predicting parse trees
on new sentences. For a further discussion of the variational approach, see
Appendix A.2.1. Detailed derivations are presented in Appendix B.
27.4.1 Structured mean-ï¬eld approximation
The random variables of interest in our model are the parameters Ã‹ = (â€š, Ë†), the
parse tree z, and the yield x (which we observe). Our goal is thus to compute the
posterior p(Ã‹, z | x). We can express this posterior variationally as the solution
to an optimization problem:
argmin
qâˆˆQ
KL(q(Ã‹, z) || p(Ã‹, z | x)).
(27.2)
Indeed, if we let Q be the family of all distributions over (Ã‹, z), the solution to
the optimization problem is the exact posterior p(Ã‹, z | x), since KL divergence
is minimized exactly when its two arguments are equal. Of course, solving this
optimization problem is just as intractable as directly computing the posterior.
Though it may appear that no progress has been made, having a variational
formulation allows us to consider tractable choices of Q in order to obtain
principled approximate solutions.

788
The Oxford Handbook of Applied Bayesian Analysis
b
fzB
fzT
fzE
z
âˆž
z1
z2
z3
Fig. 27.5 We approximate the posterior over parameters and parse trees using structured mean-ï¬eld.
Note that the posterior over parameters is completely factorized, but the posterior over parse trees is
not constrained.
For the HDP-PCFG, we deï¬ne the set of approximate distributions Q to be
those that factor as follows:
Q =

q :

q(â€š)
K

z=1
q

Ë†T
z

q

Ë†E
z

q

Ë†B
z
 
q(z)

.
(27.3)
Figure 27.5 shows the graphical model corresponding to the family of approxi-
mate distributions we consider. Furthermore, we impose additional parametric
forms on the factors as follows:
r q(â€š)
is
degenerate
(q(â€š) = â€°â€šâˆ—(â€š)
for
some
â€šâˆ—)
and
truncated

â€šâˆ—
z = 0 for z > K

.
The truncation is typical of inference algorithms for DP mixtures that are
formulated using the stick-breaking representation (Blei and Jordan, 2005).
It can be justiï¬ed by the fact that a truncated stick-breaking distribution
well approximates the original stick-breaking distribution in the following
sense: Let G = âˆž
z=1 â€šzâ€°Ë†z and let GK = K
z=1 â€šâ€²
zâ€°Ë†z denote the truncated
version, where â€šâ€²
z = â€šz for z < K , â€šâ€²
K = 1 âˆ’K âˆ’1
z=1 â€šz and â€šâ€²
z = 0 for z > K .
The variational distance between G and GK decreases exponentially as a
function of the truncation level K (Ishwaran and James, 2001).
We also require q(â€š) to be degenerate to avoid the computational difï¬culties
due to the nonconjugacy of â€š and Ë†B
z .
r q

Ë†T
z

, q

Ë†E
z

, and q

Ë†B
z

are Dirichlet distributions. Although the binary
production parameters Ë†B
z specify a distribution over {1, 2, . . . }2, the K -
component truncation on â€š forces Ë†B
z to assign zero probability to all
but a ï¬nite K Ã— K subset. Therefore, only a ï¬nite Dirichlet distribu-
tion and not a general DP is needed to characterize the distribution
over Ë†B
z .
r q(z) is any multinomial distribution (this encompasses all possible dis-
tributions over the discrete space of parse trees). Though the number of
parse trees is exponential in the sentence length, it will turn out that the

Probabilistic Grammars and Hierarchical Dirichlet Processes
789
optimal q(z) always has a factored form which can be found efï¬ciently
using dynamic programming.
Note that if we had restricted all the parameter distributions to be degenerate
(q(Ë†) = â€°Ë†âˆ—(Ë†) for some Ë†âˆ—) and ï¬xed â€šâˆ—to be uniform, then we would get the
objective function optimized by the EM algorithm. If we further restrict q(z) =
â€°zâˆ—(z) for some zâˆ—, we would obtain the objective optimized by Viterbi EM.
27.4.2 Coordinate ascent
We now present an algorithm for solving the optimization problem described
in (27.2)â€“(27.3). Unfortunately, the optimization problem is non-convex, and
it is intractable to ï¬nd the global optimum. However, we can use a simple
coordinate ascent algorithm to ï¬nd a local optimum. The algorithm optimizes
one factor in the mean-ï¬eld approximation of the posterior at a time while ï¬xing
all other factors. Optimizing q(z) is the analog of the E-step in ï¬tting an ordinary
PCFG, and optimizing q(Ë†) is the analog of the M-step. Optimizing q(â€š) has no
analog in EM. Note that Kurihara and Sato (2004) proposed a structured mean-
ï¬eld algorithm for Bayesian PCFGs; like ours, their algorithm optimizes q(z)
and q(Ë†), but it does not optimize q(â€š).
Although mathematically our algorithm performs coordinate ascent on the
mean-ï¬eld factors shown in (27.3), the actual implementation of the algo-
rithm does not actually require storing each factor explicitly; it stores only
summaries sufï¬cient for updating the other factors. For example, to update
q(Ë†), we only need the expected counts derived from q(z); to update q(z),
we only need the multinomial weights derived from q(Ë†) (see below for
details).
27.4.2.1 Updating posterior over parse trees q(z) (â€˜E-stepâ€™)
A sentence is a sequence of terminals x = (x1, . . . , xn); conditioned on the
sentence length n, a parse tree can represented by the following set of variables:
z = {z[i, j] : 1 â‰¤i â‰¤j â‰¤n}, where the variable z[i, j] indicates whether there is
a node in the parse tree whose terminal span is (xi, . . . , xj), and, if so, what
the grammar symbol at that node is. In the ï¬rst case, the value of z[i, j] is a
grammar symbol, in which case we call [i, j] a constituent; otherwise, z[i, j] takes
on a special Non-Node value. In order for z to specify a valid parse tree, two
conditions must hold: (1) [1, n] is a constituent, and (2) for each constituent [i, j]
with i < j, there exists exactly one k âˆˆ[i, j âˆ’1] for which [i, k] and [k + 1, j] are
constituents.
Before deriving the update for q(z), we introduce some notation. Let B(z) =
{[i, j] : i < j and z[i, j] =/ Non-Node} be the set of binary constituents. Let

790
The Oxford Handbook of Applied Bayesian Analysis
c1([i, j]) = [i, k] and c2([i, j]) = [k + 1, j], where k is the unique integer such that
[i, k] and [k + 1, j] are constituents.
The conditional distribution over the parse tree is given by the following:
p(z | Ã‹, x) âˆp(x, z | Ã‹)
=
n

i=1
Ë†T
z[i,i](E) Ë†E
z[i,i](xi)

[i, j]âˆˆB(z)
Ë†T
z[i, j](B) Ë†B
z[i, j](zc1([i, j])zc2([i, j])),
(27.4)
where
we
have
abbreviated
the
rule
types:
E = Emission
and
B =
Binary-Production. The ï¬rst product is over the emission probabilities
of generating each terminal symbol xi, and the second product is over the
binary production probabilities used to produce the parse tree. Using this
conditional distribution, we can form the mean-ï¬eld update by applying the
general mean-ï¬eld update rule (see (27.35) in Appendix B.1):
q(z) âˆ
n

i=1
WT
z[i,i](E) WE
z[i,i](xi)

[i, j]âˆˆB(z)
WT
z[i, j](B) WB
z[i, j](zc1([i, j]), zc2([i, j])),
(27.5)
where the multinomial weights are deï¬ned as follows:
WE
z (x) def= exp

Eq(Ë†) log Ë†E
z (x)

,
(27.6)
WB
z (z â€², z â€²â€²) def= exp

Eq(Ë†) log Ë†B
z (z â€²z â€²â€²)

,
(27.7)
WT
z (t) def= exp

Eq(Ë†) log Ë†T
z (t)

.
(27.8)
These multinomial weights for a d-dimensional Dirichlet are simply d numbers
that â€˜summarizeâ€™ that distribution. Note that if q(Ë†) were degenerate, then
WT, WE, WB would be equal to their non-random counterparts Ë†T, Ë†E, Ë†B.
In this case, the mean-ï¬eld update (27.5) is the same as conditional distri-
bution (27.4). Even when q(Ë†) is not degenerate, note that (27.4) and (27.5)
factor in the same way, which has important implications for computational
efï¬ciency.
To compute the normalization constant of (27.5), we can use using dynamic
programming. In the context of PCFGs, this amounts to using the standard
inside-outside algorithm (Lari and Young, 1990); see Manning and SchÃ¼tze
(1999) for details. This algorithm runs in O(n3K 3) time, where n is the length
of the sentence and K is the truncation level. However, if the tree structure is
ï¬xed as in the case of grammar reï¬nement, then we can use a variant of the
forward-backward algorithm which runs in just O(nK 3) time (Matsuzaki et al.,
2005; Petrov et al., 2006).

Probabilistic Grammars and Hierarchical Dirichlet Processes
791
A common perception is that Bayesian inference is slow because one needs
to compute integrals. Our mean-ï¬eld inference algorithm is a counterexam-
ple: because we can represent uncertainty over rule probabilities with sin-
gle numbers, much of the existing PCFG machinery based on EM can be
imported into the Bayesian framework in a modular way. In Section A.3.2,
we give an empirical interpretation of the multinomial weights, showing how
they take into account the uncertainty of the random rule probabilities they
represent.
27.4.2.2 Updating posterior over rule probabilities q(Ë†) (â€˜M-stepâ€™)
The mean-ï¬eld update for q(Ë†) can be broken up into independent updates for
the rule type parameters q

Ë†T
z

the emission parameters q

Ë†E
z

and the binary
production parameters q

Ë†B
z

for each symbol z âˆˆS.
Just as optimizing q(z) only required the multinomial weights of q(Ë†), opti-
mizing q(Ë†) only requires a â€˜summaryâ€™ of q(z). In particular, this summary
consists of the expected counts of emissions, rule types, and binary productions,
which can be computed by the inside-outside algorithm:
C E(z, x) def= Eq(z)

1â‰¤iâ‰¤n
I[z[i,i] = z, xi = x],
(27.9)
C B(z, zâ€² zâ€²â€²) def= Eq(z)

1â‰¤iâ‰¤k< jâ‰¤n
I[z[i, j] = z, z[i,k] = zâ€², z[k+1, j] = zâ€²â€²],
(27.10)
C T(z, t) def=

â€ž
Ct(z, â€ž).
(27.11)
Applying (27.35), we obtain the following updates (see Appendix B.3 for a
derivation):
q

Ë†E
z

= Dir

Ë†E
z ; Â·E + C E(z)

,
q

Ë†B
z

= Dir

Ë†B
z ; Â·B + C B(z)

,
q

Ë†T
z

= Dir

Ë†T
z ; Â·T + CT(z)

.
27.4.2.3 Updating the top-level component weights q(â€š)
Finally, we need to update the parameters in the top level of our Bayesian hierar-
chy q(â€š) = â€°â€šâˆ—(â€š). Unlike the other updates, there is no closed-form expression
for the optimal â€šâˆ—. In fact, the objective function (27.2) is not even convex in
â€šâˆ—. Nonetheless, we can use a projected gradient algorithm (Bertsekas, 1999) to

792
The Oxford Handbook of Applied Bayesian Analysis
improve â€šâˆ—to a local optimum. The optimization problem is as follows:
min
â€šâˆ—KL(q(Ã‹, z)âˆ¥p(Ã‹, z | x))
(27.12)
= max
â€šâˆ—
Eq log p(Ã‹, z | x) + H(q(Ã‹, z))
(27.13)
= max
â€šâˆ—log GEM(â€šâˆ—; Â·) +
K

z=1
Eq log Dir

Ë†B
z ; Â·Bâ€šâˆ—â€šâˆ—T
+ constant
(27.14)
def= max
â€šâˆ—
L(â€šâˆ—) + constant.
(27.15)
We have absorbed all terms that do not depend on â€šâˆ—into the constant, includ-
ing the entropy, since â€°â€šâˆ—is always degenerate. See Appendix B.4 for the details
of the gradient projection algorithm and the derivation of L(â€š) and âˆ‡L(â€š).
27.4.3 Prediction: Parsing new sentences
After having found an approximate posterior over parameters of the HDP-
PCFG, we would like to be able to use it to parse new sentences, that is, predict
their parse trees. Given a loss function â„“(z, zâ€²) between parse trees, the Bayes
optimal parse tree for a new sentence xnew is given as follows:
zâˆ—
new = argminzâ€²new E p(znew|xnew,x,z)â„“

znew, zâ€²
new

(27.16)
= argminzâ€²new E p(znew|Ã‹,xnew)p(Ã‹,z|x)â„“

znew, zâ€²
new

.
(27.17)
If we use the 0-1 loss â„“

znew, zâ€²
new

= 1 âˆ’I

znew = zâ€²
new

, then (27.16) is equiv-
alent to ï¬nding the maximum marginal likelihood parse, integrating out the
parameters Ã‹:
zâˆ—
new = argmax
znew
E p(Ã‹,z|x) p(znew | Ã‹, xnew).
(27.18)
We can substitute in place of the true posterior p(Ã‹, z | x) our approximate
posterior q(Ã‹, z) = q(Ã‹)q(z) to get an approximate solution:
zâˆ—
new = argmax
znew
Eq(Ã‹) p(znew | Ã‹, xnew).
(27.19)
If q(Ã‹) were degenerate, then we could evaluate the argmax efï¬ciently using
dynamic programming (the Viterbi version of the inside algorithm). However,
even for a fully-factorized q(Ã‹), the argmax cannot be computed efï¬ciently,
as noted by MacKay (1997) in the context of variational HMMs. The reason
behind this difï¬culty is that the integration over that rule probabilities couples
distant parts of the parse tree which use the same rule, thus destroying the
Markov property necessary for dynamic programming. We are thus left with
the following options:

Probabilistic Grammars and Hierarchical Dirichlet Processes
793
1. Maximize the expected log probability instead of the expected probability:
zâˆ—
new = argmax
znew
Eq(Ã‹) log p(znew | xnew, Ã‹).
(27.20)
Concretely, this amounts to running the Viterbi algorithm with the same
multinomial weights that were used while ï¬tting the HDP-PCFG.
2. Extract the mode Ã‹âˆ—= q(Ã‹) and parse using Ã‹âˆ—with dynamic programming.
One problem with this approach is that the mode is not always well
deï¬ned, for example, when the Dirichlet posteriors have concentration
parameters less than 1.
3. Use options 1 or 2 to obtain a list of good candidates, and then choose the
best candidate according to the true objective (27.19).
In practice, we used the ï¬rst option.
27.4.3.1 Grammar reï¬nement
Given a new sentence xnew, recall that the HDP-PCFG-GR (Section 27.3.1)
deï¬nes a joint distribution over a coarse tree snew of symbols and a reï¬nement
znew described by subsymbols. Finding the best reï¬ned tree (snew, znew) can be
carried out using the same methods as for the HDP-PCFG described above.
However, in practical parsing applications, we are interested in predicting
coarse trees for use in subsequent processing. For example, we may wish to
minimize the 0â€“1 loss with respect to the coarse tree

1 âˆ’I

snew = sâ€²
new

. In
this case, we need to integrate out znew:
sâˆ—
new = argmax
snew
Eq(Ã‹)

znew
p(snew, znew | Ã‹, xnew).
(27.21)
Note that even if q(Ã‹) is degenerate, this expression is difï¬cult to compute
because the sum over znew induces long-range dependencies (which is the
whole point of grammar reï¬nement), and as a result, p(snew | xnew, Ã‹) does not
decompose and cannot be handled via dynamic programming.
Instead, we adopt the following two-stage strategy, which works quite well in
practice (cf. Matsuzaki et al., 2005). We ï¬rst construct an approximate distri-
bution Ëœp(s | x) which does decompose and then ï¬nd the best coarse tree with
respect to Ëœp(s | x) using the Viterbi algorithm:
sâˆ—
new = argmax
s
Ëœp(s | x).
(27.22)
We consider only tractable distributions Ëœp which permit dynamic program-
ming. These distributions can be interpreted as PCFGs where the new symbols
S Ã— {(i, j) : 1 â‰¤i â‰¤j â‰¤n} are annotated with the span. The new rule probabil-
ities must be consistent with the spans; that is, the only rules allowed to have
non-zero probability are of the form (a, [i, j]) â†’(b, [i, k]) (c, [k + 1, j]) (binary)

794
The Oxford Handbook of Applied Bayesian Analysis
and (a, [i, j]) â†’(b, [i, j]) (unary), where a, b, c âˆˆS and x âˆˆ. The â€˜bestâ€™ such
distribution is found according to a KL-projection:
Ëœp(s | x) = argmin
Ëœp â€² tractable
KL

exp Eq(Ã‹) log p(snew | xnew, Ã‹)âˆ¥Ëœp â€²(s | x)

.
(27.23)
This KL-projection can be done by simple moment-matching, where the
moments we need to compute are of the form I[(s[i, j] = a, s[i,k] = b, s[k+1, j] =
c)] and I[s[i, j] = a, s[i, j] = b] for a, b, c âˆˆS, which can be computed using the
dynamic programming algorithm described in Section 27.4.2.1.
Petrov and Klein (2007) discuss several other alternatives, but show that the
two-step strategy described above performs the best.
27.5 Experiments
We now present an empirical evaluation of the HDP-PCFG and HDP-PCFG-
GR models for grammar induction and grammar reï¬nement, respectively. We
ï¬rst show that the HDP-PCFG and HDP-PCFG-GR are able to recover a known
grammar more accurately than a standard PCFG estimated with maximum
likelihood (Section 27.5.1â€“Section 27.5.2). We then present results on a large-
scale parsing task (Section 27.5.3).
27.5.1 Inducing a synthetic grammar using the HDP-PCFG
In the ï¬rst experiment, the goal was to recover a PCFG grammar given only
sentences generated from that grammar. Consider the leftmost grammar in
Figure 27.6; this is the â€˜true grammarâ€™ that we wish to recover. It has a total of
eight nonterminal symbols, four of which are left-hand sides of only emission
rules (these are the preterminal symbols) and four of which are left-hand sides
of only binary production rules (constituent symbols). The probability of each
rule is given above the appropriate arrow. Though very simple, this grammar
still captures some of the basic phenomena of natural language such as noun
phrases (NPs), verb phrases (VPs), determiners (DTs), adjectives (JJs), and
so on.
From this grammar, we sampled 1000 sentences. Then, from these sentences
alone, we attempted to recover the grammar using the standard PCFG and
the HDP-PCFG. For the standard PCFG, we allocated 20 latent symbols, 10
for preterminal symbols and 10 for constituent symbols. For the HDP-PCFG
(using the version described in Section 27.2.2), we set the stick-breaking trun-
cation level to K = 10 for the preterminal and constituent symbols. All Dirichlet
hyperparameters were set to 0.01. We ran 300 iterations of EM for the standard
PCFG and variational inference for the HDP-PCFG. Since both algorithms
are prone to local optima, we ran the algorithms 30 times with different ran-
dom initializations. We also encouraged the use of constituent symbols by

Probabilistic Grammars and Hierarchical Dirichlet Processes
795
True grammar
Recovered grammar 1
Recovered grammar 2
Recovered grammar 3
S 1.0
â†’
1.0
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.5
â†’
0.33
â†’
0.33
â†’
0.33
â†’
NP VP
NP
DT NN
NP
DT NPBAR
NPBAR
JJ NN
NPBAR
JJ NPBAR
VP
VB NP
DT
the
DT
a
JJ
big
JJ
black
NN
mouse
NN
cat
NN
dog
VB
chased
VB
ate
the cat ate a mouse
the cat ate the black mouse
a big big black cat chased the black mouse
a black big dog chased a cat
the mouse chased a mouse
the mouse ate a black dog
...
S
1.0
â†’NP VP
NP
0.5
â†’DT NN
NP
0.5
â†’DT NPBAR
VP
1.0
â†’VB NP
NPBAR 0.11
â†’JJ2 NPBAR
NPBAR 0.38
â†’JJ1 NPBAR
NPBAR 0.07
â†’JJ-big NN
NPBAR 0.42
â†’JJ2 NN
NPBAR 0.02
â†’JJ2 NN-cat/dog
DT 0.51
â†’a
DT
0.5
â†’the
JJ1 0.52
â†’big
JJ1 0.48
â†’black
JJ-big
1.0
â†’big
JJ2 0.59
â†’black
JJ2 0.42
â†’big
NN 0.35
â†’mouse
NN 0.32
â†’dog
NN 0.33
â†’cat
NN-cat/dog 0.47
â†’cat
NN-cat/dog 0.53
â†’dog
VB 0.49
â†’chased
VB 0.51
â†’ate
S
1.0
â†’NP-V NP
NP-V
1.0
â†’NP VB
NP
0.47
â†’DT2 NPBAR
NP
0.04
â†’DT1 NN-mouse
NP
0.29
â†’DT2 NN
NP
0.17
â†’DT1 NN
NP
0.03
â†’DT1 NPBAR
NPBAR
0.02
â†’JJ NN-mouse
NPBAR
0.01
â†’JJ-big NN-mouse
NPBAR
0.48
â†’JJ NN
NPBAR
0.49
â†’JJ NPBAR
DT1
0.45
â†’a
DT1
0.55
â†’the
DT2
0.52
â†’a
DT2
0.48
â†’the
JJ
0.51
â†’black
JJ
0.49
â†’big
JJ-big
1.0
â†’big
NN
0.35
â†’dog
NN
0.36
â†’cat
NN
0.29
â†’mouse
NN-mouse
1.0
â†’mouse
VB
0.49
â†’chased
VB
0.51
â†’ate
S
1.0
â†’NP-VB NP
NP-VB
1.0
â†’NP VB
NP
0.5
â†’DT NN
NP 0.25
â†’DT-JJ+1 NN
NP 0.25
â†’DT-JJ+2 NN
DT-JJ+1 0.79
â†’DT JJ1
DT-JJ+1 0.18
â†’DT JJ2
DT-JJ+1 0.03
â†’DT-a JJ2
DT-JJ+2 0.49
â†’DT-JJ+2 JJ1
DT-JJ+2 0.37
â†’DT-JJ+1 JJ1
DT-JJ+2 0.15
â†’DT-JJ+1 JJ2
DT
0.5
â†’the
DT
0.5
â†’a
DT-a
1.0
â†’a
JJ1 0.52
â†’black
JJ1 0.48
â†’big
JJ2 0.56
â†’big
JJ2 0.44
â†’black
NN 0.33
â†’cat
NN 0.34
â†’mouse
NN 0.32
â†’dog
VB 0.49
â†’chased
VB 0.51
â†’ate
Recovered parse 3
Recovered parse 2
Recovered parse 1
True parse
S
NP
DT
the
NN
cat
VP
VB
ate
NP
DT
the
NPBAR
JJ
black
NN
mouse
S
NP
DT1
the
NN
cat
VP
VB
ate
NP
DT2
the
NPBAR
JJ2
black
NN
mouse
S
NP-VB
NP
DT2
the
NN
cat
VB
ate
NP
DT2
the
NPBAR
JJ
black
NN
mouse
S
NP-VB
NP
DT
the
NN
cat
VB
ate
NP
DT-JJ+1
DT
the
JJ1
black
NN
mouse
Fig. 27.6 We generated 1000 sentences from the true grammar. The HDP-PCFG model recovered
the three grammars (obtained from different initializations of the variational inference algorithm).
The parse trees of a sentence under the various grammars are also shown. The ï¬rst is essentially
the same as that of the true grammar, while the second and third contain various left-branching
alternatives.
placing slightly more prior weight on rule type probabilities corresponding to
production-production rules.
In none of the 30 trials did the standard PCFG manage to recover the true
grammar. We say a rule is active if its multinomial weight (see Section 27.4.2.1)
is at least 10âˆ’6 and its left-hand side has total posterior probability also at least
10âˆ’6. In general, rules with weight smaller than 10âˆ’6 can be safely ignored
without affect parsing results. In a typical run of the standard PCFG, all
20 symbols were used and about 150 of the rules were active (in contrast,
there are only 15 rules in the true grammar). The HDP-PCFG managed to
do much better. Figure 27.6 shows three grammars (of the 30 trials) which
had the highest variational objective values. The symbols are numbered 1 to
K in the model, but we have manually labeled them with to suggest their
actual role in the syntax. Each grammar has around 4â€“5 constituent symbols
(there were four in the true grammar) and 6â€“7 preterminal symbols (four

796
The Oxford Handbook of Applied Bayesian Analysis
in the true grammar), which yields around 25 active rules (15 in the true
grammar).
While the HDP-PCFG did not recover the exact grammar, it was able to
produce grammars that were sensible. Interestingly, each of the three gram-
mars used a slightly different type of syntactic structure to model the data.
The ï¬rst one is the closest to the true grammar and differs only in that it
allocated three subsymbols for jj and two for nn instead of one. Generally, extra
subsymbols provide a better ï¬t of the noisy observations, and here, the sparsity-
inducing DP prior was only partially successful in producing a parsimonious
model.
The second grammar differs fundamentally from the true grammar in that
the subject and the verb are grouped together as a constituent (np-vb) rather
than the verb and the object (vp). This is a problem with non-identiï¬ability;
in this simple example, both grammars describe the data equally well and
have the same grammar complexity. One way to break this symmetry is to use
an informative prior â€“ e.g., that natural language structures tend to be right-
branching.
The third grammar differs from the true grammar in one additional way,
namely that noun phrases are also left-branching; that is, for the black mouse,
the and black are grouped together rather than black and mouse. Intuitively, this
grammar suggests the determiner as the head word of the phrase rather than
the noun. This head choice is not entirely unreasonable; indeed there is an
ongoing debate in the linguistics community about whether the determiner
(the DP hypothesis) or the noun (the NP hypothesis) should be the head
(though even in a DP analysis the determiner and adjective should not be
grouped).
Given that our variational algorithm converged to various modes depending
on initialization, it is evident that the true Bayesian posterior over grammars is
multimodal. A fully Bayesian inference procedure would explore and accurately
represent these modes, but this is computationally intractable. Starting our
variational algorithm from various initializations provides a cheap, if imperfect,
way of exploring some of the modes.
Inducing grammars from raw text alone is an extremely difï¬cult problem.
Statistical approaches to grammar induction have been studied since the early
1990s (Carroll and Charniak, 1992). As these early experiments were discour-
aging, people turned to alternative algorithms (Stolcke and Omohundro, 1994)
and models (Klein and Manning, 2004; Smith and Eisner, 2005). Though we
have demonstrated partial success with the HDP-PCFG on synthetic examples,
it is unlikely that this method alone will solve grammar induction problems
for large-scale corpora. Thus, for the remainder of this section, we turn to the
more practical problem of grammar reï¬nement, where the learning of symbols
is constrained by a coarse treebank.

Probabilistic Grammars and Hierarchical Dirichlet Processes
797
27.5.2 Reï¬ning a synthetic grammar with the HDP-PCFG-GR
We ï¬rst conduct a simple experiment, similar in spirit to the grammar induc-
tion experiment, to show that the HDP-PCFG-GR can recover a simple gram-
mar while a standard PCFG-GR (a PCFG adapted for grammar reï¬nement)
cannot. From the grammar in Figure 27.7(a), we generated 2000 trees of the
form shown in Figure 27.7(b). We then replaced all Xis with X in the training
data to yield a coarse grammar. Note that the two terminal symbols always have
the same subscript, a correlation not captured by their parent X. We trained
both the standard PCFG-GR and the HDP-PCFG-GR using the modiï¬ed trees
as the input data, hoping to estimate the proper reï¬nement of X. We used a
truncation of K = 20 for both S and X, set all hyperparameters to 1, and ran EM
and the variational algorithm for 100 iterations.
Figure 27.8 shows the posterior probabilities of the subsymbols in the gram-
mars produced by the PCFG-GR (a) and HDP-PCFG-GR (b). The PCFG-GR
used all 20 subsymbols of both S and X to ï¬t the noisy co-occurrence statistics
of left and right terminals, resulting in 8320 active rules (with multinomial
weight larger than 10âˆ’6). On the other hand, for the HDP-PCFG-GR, only four
subsymbols of X and one subsymbol of S had non-negligible posterior mass;
68 rules were active. If the threshold is relaxed from 10âˆ’6 to 10âˆ’3, then only 20
rules are active, which corresponds exactly to the true grammar.
S 
â†’ X1X1 | X2X2 | X3X3 | X4X4
X1 â†’ a1 | b1 | c1 | d1
X2 â†’ a2 | b2 | c2 | d2
X3 â†’ a3 | b3 | c3 | d3
X4 â†’ a4 | b4 | c4 | d4
S
Xi
Xi
{ai, bi, ci, di}
(b)
(a)
{ai, bi, ci, di}
Fig. 27.7 (a) A synthetic grammar with a uniform distribution over rules. (b) The grammar generates
trees of the form shown on the right.
1 2 3 4 5 6 7 8 9 1011121314151617181920
Subsymbol
0.25
Posterior
1 2 3 4 5 6 7 8 9 10 11121314151617181920
Subsymbol
0.25
Posterior
(b) HDP-PCFG-GR
(a) PCFG-GR
Fig. 27.8 The posterior probabilities over the subsymbols of the grammar produced by the PCFG-
GR are roughly uniform, whereas the posteriors for the HDP-PCFG-GR are concentrated on four
subsymbols, the true number in the original grammar.

798
The Oxford Handbook of Applied Bayesian Analysis
27.5.3 Parsing the Penn Treebank
In this section, we show that the variational HDP-PCFG-GR can scale up to
real-world datasets. We truncated the HDP-PCFG-GR at K subsymbols, and
compared its performance with a standard PCFG-GR with K subsymbols esti-
mated using maximum likelihood (Matsuzaki et al., 2005).
27.5.3.1 Dataset and preprocessing
We ran experiments on the Wall Street Journal (WSJ) portion of the Penn Tree-
bank, a standard dataset used in the natural language processing community
for evaluating constituency parsers. The dataset is divided into 24 sections and
consists of approximately 40,000 sentences. As is standard, we ï¬t a model on
sections 2â€“21, used section 24 for tuning the hyperparameters of our model,
and evaluated parsing performance on section 22.
The HDP-PCFG-GR is deï¬ned only for grammars with unary and binary
production rules, but the Penn Treebank contains trees in which a node
has more than two children. Therefore, we use a standard binarization
procedure to transform our trees. Speciï¬cally, for each non-terminal node
with symbol X, we introduce a right-branching cascade of new nodes with
symbol X.
Another issue that arises when dealing with a large-scale dataset of this kind
is that words have a Zipï¬an distribution and in parsing new sentences we
invariably encounter new words that did not appear in the training data. We
could let the HDP-PCFG-GRâ€™s generic Dirichlet prior manage this uncertainty,
but would like to use prior knowledge such as the fact that a capitalized word
that we have not seen before is most likely a proper noun. We use a simple
method to inject this prior knowledge: replace any word appearing fewer than
ï¬ve times in the training set with one of 50 special â€˜unknown wordâ€™ tokens,
which are added to . These tokens can be thought of as representing manually
constructed word clusters.
We evaluated our predicted parse trees using F1 score, deï¬ned as follows.
Given a (coarse) parse tree s, let the labelled brackets be deï¬ned as
LB(s) = {(s[i, j], [i, j]) : s[i, j] =/ Non-Node, 1 â‰¤i â‰¤j â‰¤n}.
Given the correct parse tree s and a predicted parse tree sâ€², the precision, recall
and F1 scores are deï¬ned as follows:
Precision(s, sâ€²) = |LB(s) âˆ©LB(sâ€²)|
|LB(s)|
Recall(s, sâ€²) = |LB(s) âˆ©LB(sâ€²)|
|LB(sâ€²)|
(27.24)
F1(s, sâ€²)âˆ’1 = 1
2

Precision(s, sâ€²)âˆ’1 + Recall(s, sâ€²)âˆ’1
. (27.25)

Probabilistic Grammars and Hierarchical Dirichlet Processes
799
Table 27.1 For each truncation level, we report
the Â·B that yielded the highest F1 score on the
development set.
truncation K
2
4
8
12
16
20
best Â·B
16
12
20
28
48
80
uniform Â·B
4
16
64
144
256
400
27.5.3.2 Hyperparameters
There are six hyperparameters in the HDP-PCFG-GR model, which we set
in the following manner: Â· = 1, Â·T = 1 (uniform distribution over unaries
versus binaries), Â·E = 1 (uniform distribution over terminal words), Â·u(s) =
Â·b(s) =
1
N(s), where N(s) is the number of different unary (binary) right-
hand sides of rules with left-hand side s in the treebank grammar. The two
most important hyperparameters are Â·U and Â·B, which govern the sparsity
of the right-hand side for unary and binary rules. We set Â·U = Â·B although
greater accuracy could probably be gained by tuning these individually. It
turns out that there is not a single Â·B that works for all truncation levels,
as shown in Table 27.1. If Â·B is too small, the grammars are overly sparse,
but if Â·B is too large, the extra smoothing makes the grammars too uniform
(see Figure 27.9).
If the top-level distribution â€š is uniform, the value of Â·B corresponding to a
uniform prior over pairs of child subsymbols is K 2. Interestingly, the best Â·B
appears to be superlinear but subquadratic in K . We used these best values of
Â·B in the following experiments.
1
2
4
6
8
10
12
14
16
20
24
28
32
40
48
Concentration parameter aB
72
74
76
78
80
F1
Fig. 27.9 Development F1 performance on the development set for various values of Â·B, training on
only section 2 with truncation K = 8.

800
The Oxford Handbook of Applied Bayesian Analysis
27.5.3.3 Results
The regime in which Bayesian inference is most important is when the
number of training data points is small relative to the complexity of the
model. To explore this regime, we conducted a ï¬rst experiment where we
trained the PCFG-GR and HDP-PCFG-GR on only section 2 of the Penn
Treebank.
Table 27.2 shows the results of this experiment. Without smoothing, the
PCFG-GR trained using EM improves as K increases but starts to overï¬t
around K = 4. If we smooth the PCFG-GR by adding 0.01 pseudo-counts
(which corresponds using a Dirichlet prior with concentration parameters
1.01, ..., 1.01), we see that the performance does not degrade even as K
increases to 20, but the number of active grammar rules needed is substantially
more. Finally, we see that the HDP-PCFG-GR yields performance compara-
ble to the smoothed PCFG-GR, but the number of grammar rules needed is
smaller.
We also conducted a second experiment to demonstrate that our methods
can scale up to realistically large corpora. In particular, in this experiment we
trained on all of sections 2â€“21. When using a truncation level of K = 16, the
standard PCFG-GR with smoothing yielded an F1 score of 88.36 using 706,157
active rules. The HDP-PCFG-GR yielded an F1 score of 87.08 using 428,375
active rules. We thus see that the HDP-PCFG-GR achieves broadly comparable
performance compared to existing state-of-the-art parsers, while requiring a
substantially smaller number of rules. Finally, note that K = 16 is a relatively
stringent truncation and we would expect to see improved performance from
the HDP-PCFG-GR with a larger truncation level.
Table 27.2 Shows the development F1 scores and grammar sizes (the
number of active rules) as we increase the truncation K . The ordinary
PCFG-GR overï¬ts around K = 4. Smoothing with 0.01 pseudo-counts
prevents this, but produces much larger grammars. The HDP-PCFG-GR
attains comparable performance with smaller grammars.
K
PCFG-GR
PCFG-GR (smoothed)
HDP-PCFG-GR
F1
Size
F1
Size
F1
Size
1
60.47
2558
60.36
2597
60.50
2557
2
69.53
3788
69.38
4614
71.08
4264
4
75.98
3141
77.11
12436
77.17
9710
8
74.32
4262
79.26
120598
79.15
50629
12
70.99
7297
78.80
160403
78.94
86386
16
66.99
19616
79.20
261444
78.24
131377
20
64.44
27593
79.27
369699
77.81
202767

Probabilistic Grammars and Hierarchical Dirichlet Processes
801
27.6 Discussion
The HDP-PCFG represents a marriage of grammars and Bayesian nonpara-
metrics. We view this marriage as a particularly natural one, given the need
for syntactic models to have large, open-ended numbers of grammar symbols.
Moreover, given that the most successful methods that have been developed for
grammar reï¬nement have been based on clustering of grammar symbols, we
view the Dirichlet process as providing a natural starting place for approaching
this problem from a Bayesian point of view. The main problem that we have had
to face has been that of tying of clusters across parse trees, and this problem is
readily solved via the hierarchical Dirichlet process.
We have also presented an efï¬cient variational inference algorithm to approx-
imate the Bayesian posterior over grammars. Although more work is needed to
demonstrate the capabilities and limitations of the variational approach, it is
important to emphasize the need for fast inference algorithms in the domain
of natural language processing. A slow parser is unlikely to make inroads in the
applied NLP community. Note also that the mean-ï¬eld variational algorithm
that we presented is closely linked to the EM algorithm, and the familiarity of
the latter in the NLP community may help engender interest in the Bayesian
approach.
The NLP community is constantly exploring new problem domains that
provide fodder for Bayesian modeling. Currently, one very active direction of
research is the modeling of multilingual data â€“ for example, synchronous
grammars which jointly model the parse trees of pairs of sentences which
are translations of each other (Wu, 1997). These models have also received
a initial Bayesian treatment (Blunsom et al., 2009). Also, many of the mod-
els used in machine translation are essentially already nonparametric, as the
parameters of these models are deï¬ned on large subtrees rather than indi-
vidual rules as in the case of the PCFG. Here, having a Bayesian model
that integrates over uncertainty can provide more accurate results (DeNero
et al., 2008). A major open challenge for these models is again computa-
tional; Bayesian inference must be fast if its application to NLP is likely to
succeed.
Appendix
A. Broader context and background
In this section, we present some of the background in Bayesian nonpara-
metric modelling and inference needed for understanding the HDP-PCFG.
Appendix A.1 describes various simpler models that lead up to the HDP-
PCFG. Appendix A.2 overviews some general issues in variational inference and

802
The Oxford Handbook of Applied Bayesian Analysis
Appendix A.3 provides a more detailed discussion of the properties of mean-
ï¬eld variational inference in the nonparametric setting.
A.1 DP-based models
We ï¬rst review the Dirichlet process (DP) mixture model (Antoniak, 1974), a
building block for a wide variety of nonparametric Bayesian models, including
the HDP-PCFG. The DP mixture model captures the basic notion of clustering
which underlies symbol formation in the HDP-PCFG. We then consider the
generalization of mixture models to hidden Markov models (HMMs), where
clusters are linked structurally according to a Markov chain. To turn the HMM
into a nonparametric Bayesian model, we introduce the hierarchical Dirichlet
process (HDP). Combining the HDP with the Markov chain structure yields the
HDP-HMM, an HMM with a countably inï¬nite state space. This provides the
background and context for the HDP-PCFG (Section 27.2).
A.1.1 Bayesian ï¬nite mixture models We begin our tour with the familiar Bayesian
ï¬nite mixture model in order to establish notation which will carry over to
more complex models. The model structure is summarized in two ways in
Figure 27.10: symbolically in the diagram on the left side of the ï¬gure and
graphically in the diagram on the right side of the ï¬gure. As shown in the ï¬gure,
we consider a ï¬nite mixture with K mixture components, where each component
z âˆˆ{1, . . . , K } is associated with a parameter vector Ë†z drawn from some prior
distribution G0 on a parameter space . We let the vector â€š = (â€š1, . . . , â€šK )
denote the mixing proportions; thus the probability of choosing the mixture
component indexed by Ë†z is given by â€šz. This vector is assumed to be drawn
from a symmetric Dirichlet distribution with hyperparameter Â·.
Given the parameters â€š and Ë† = (Ë†z : z = 1, . . . , K ), the data points x =
(x1, . . . , xn) are assumed to be generated conditionally i.i.d. as follows: ï¬rst
choose a component zi with probabilities given by the multinomial probability
vector â€š and then choose xi from the distribution indexed by zi; i.e. from
F(Â·; Ë†zi).
Bayesian finite mixture model
b âˆ¼ Dir(a, ... ,a) 
[draw component probabilities]
For each component z âˆˆ {1, ... ,K}:
fz âˆ¼ G0 
[draw component parameters]
For each data point i âˆˆ {1, ... ,n}:
   zi âˆ¼ Mult(b) 
[choose component]
   xi âˆ¼ F(Â·; fzi) 
[generate data point]
b
fz
zi
xi
i
z
K
n
Fig. 27.10 The deï¬nition and graphical model of the Bayesian ï¬nite mixture model.

Probabilistic Grammars and Hierarchical Dirichlet Processes
803
There is another way to write the ï¬nite mixture model that will be helpful
in developing Bayesian nonparametric extensions. In particular, instead of
expressing the choice of a mixture component as a choice of a label from a
multinomial distribution, let us instead combine the mixing proportions and
the parameters associated with the mixture components into a single object â€“ a
random measure G â€“ and let the choice of a mixture component be expressed as
a draw from G. In particular, write
G =
K

z=1
â€šzâ€°Ë†z,
(27.26)
where â€°Ë†z is a delta function at location Ë†z. Clearly G is random, because
both the coefï¬cients {â€šz} and the locations {Ë†z} are random. It is also a
measure, assigning a nonnegative real number to any Borel subset B of :
G(B) = 
z:Ë†zâˆˆB â€šz. In particular, as desired, G assigns probability â€šz to the atom
Ë†z. To summarize, we can equivalently express the ï¬nite mixture model in
Figure 27.10 as the draw of a random measure G (from a stochastic process
that has hyperparameters G0 and Â·), followed by n conditionally independent
draws of parameter vectors from G, one for each data point. Data points are then
drawn from the mixture component indexed by the corresponding parameter
vector.
A.1.2 Dirichlet process mixture models The Dirichlet process (DP) mixture model
extends the Bayesian ï¬nite mixture model to a mixture model having a count-
ably inï¬nite number of mixture components. Since we have an inï¬nite number
of mixture components, it no longer makes sense to consider a symmetric prior
over the component probabilities as we did in the ï¬nite case; the prior over
component probabilities must decay in some way. This is achieved via a so-
called stick-breaking distribution (Sethuraman, 1994).
The stick-breaking distribution that underlies the DP mixture is deï¬ned as
follows. First deï¬ne a countably inï¬nite collection of stick-breaking proportions
V1, V2, . . . , where the Vz are drawn independently from a one-parameter Beta
distribution: Vz âˆ¼Beta(1, Â·), where Â· > 0. Then deï¬ne an inï¬nite random
sequence â€š as follows:
â€šz = Vz

zâ€²<z
(1 âˆ’Vzâ€²).
(27.27)
As shown in Figure 27.11, the values of â€šz deï¬ned this procedure can be inter-
preted as portions of a unit-length stick. In particular, the product 	
zâ€²<z(1 âˆ’Vzâ€²)
is the currently remaining portion of the stick and multiplication by Vz breaks
off a proportion of the remaining stick length. It is not difï¬cult to show that
the values â€šz sum to one (with probability one) and thus â€š can be viewed as an
inï¬nite-dimensional random probability vector.

804
The Oxford Handbook of Applied Bayesian Analysis
0
1
Î²1
Î²2
Î²3
...
Fig. 27.11 A sample â€š âˆ¼GEM(Â·) from the stick-breaking distribution with Â· = 1.
DP mixture model
b âˆ¼ GEM(a) 
[draw component stick probabilities]
For each component z âˆˆ {1,2, ...}:
fz âˆ¼ G0 
[draw component parameters]
For each data point i âˆˆ {1, ... ,n}:
   zi âˆ¼ Mult(b) 
[choose component]
   xi âˆ¼ F(Â·; fzi) 
[generate data point]
b
fz
zi
xi
i
z
Â¥
n
Fig. 27.12 The deï¬nition and graphical model of the DP mixture model.
We write â€š âˆ¼GEM(Â·) to mean that â€š = (â€š1, â€š2, . . . ) is distributed according
to the stick-breaking distribution. The hyperparameter Â· determines the rate of
decay of the â€šz; a larger value of Â· implies a slower rate of decay.
We present a full speciï¬cation of the DP mixture model in Figure 27.12. This
speciï¬cation emphasizes the similarity with the Bayesian ï¬nite mixture; all that
has changed is that the vectors â€š and Ë† are inï¬nite dimensional (note that â€˜Multâ€™
in Figure 27.12 is a multinomial distribution in an extended sense, its meaning
is simply that index z is chosen with probability â€šz).
Let us also consider the alternative speciï¬cation of the DP mixture model
using random measures. Proceeding by analogy to the ï¬nite mixture model,
we deï¬ne a random measure G as follows:
G =
âˆž

z=1
â€šzâ€°Ë†z,
(27.28)
where â€°Ë†z is a delta function at location Ë†z. As before, the randomness has two
sources: the random choice of the Ë†z (which are drawn independently from G0)
and the random choice of the coefï¬cients {â€šz}, which are drawn from GEM(Â·).
We say that such a random measure G is a draw from a Dirichlet process, denoted
G âˆ¼DP(Â·, G0). This random measure G plays the same role in the DP mixture
as the corresponding G played in the ï¬nite mixture; in particular, given G the
parameter vectors are independent draws from G, one for each data point.
The term â€˜Dirichlet processâ€™ is appropriate because the random measure
G turns out to have ï¬nite-dimensional Dirichlet marginals: for an arbitrary
partition (B1, B2, . . . , Br) of the parameter space  (for an arbitrary integer r),
Sethuraman (1994) showed that

G(B1), G(B2), . . . , G(Br)

âˆ¼Dir

Â·G0(B1), Â·G0(B2), . . . , Â·G0(Br)

.

Probabilistic Grammars and Hierarchical Dirichlet Processes
805
A.1.3 Hierarchical Dirichlet Process Hidden Markov Models (HDP-HMMs) The hidden
Markov model (HMM) can be viewed as a dynamic version of a ï¬nite mixture
model. As in the ï¬nite mixture model, an HMM has a set of K mixture
components, referred to as states in the HMM context. Associated to each state,
z âˆˆ{1, 2, . . . , K }, is a parameter vector Ë†E
z ; this vector parametrizes a family of
emission distributions, F

Â·; Ë†E
z

, from which data points are drawn. The HMM
differs from the ï¬nite mixture in that states are not selected independently,
but are linked according to a Markov chain. In particular, the parametrization
for the HMM includes a transition matrix, whose rows Ë†T
z are the conditional
probabilities of transitioning to a next state zâ€² given that the current state is z.
Bayesian versions of the HMM place priors on the parameter vectors

Ë†E
z , Ë†T
z

.
Without loss of generality, assume that the initial state distribution is ï¬xed and
degenerate.
A nonparametric version of the HMM can be developed by analogy to the
extension of the Bayesian ï¬nite mixture to the DP mixture. This has been
done by Teh et al. (2006), following on from earlier work by Beal et al. (2002).
The resulting model is referred to as the hierarchical Dirichlet process HMM
(HDP-HMM). In this section we review the HDP-HMM, focusing on the new
ingredient, which is the need for a hierarchical DP.
Recall that in our presentation of the DP mixture, we showed that the choice
of the mixture component (i.e. the state) could be conceived in terms of a draw
from a random measure G. Recall also that the HMM generalizes the mixture
model by making the choice of the state conditional on the previous state. This
suggests that in extending the HMM to the nonparametric setting we should
consider a set of random measures, {Gz}, one measure for each value of the
current state.
A difï¬culty arises, however, if we simply proceed by letting each Gz be
drawn independently from a DP. In this case, the atoms forming Gz and
those forming Gzâ€², for z =/ zâ€², will be distinct with probability one (assum-
ing that the distribution G0 is continuous). This means that the set of next
states available from z will be entirely disjoint from the set of states available
from zâ€².
To develop a nonparametric version of the HMM, we thus require a notion
of hierarchical Dirichlet process (HDP), in which the random measures {Gz} are
tied. The HDP of Teh et al. (2006) does this by making a single global choice
of the atoms underlying each of the random measures. Each of the individual
measures Gz then weights these atoms differently. The general framework of
the HDP achieves this as follows:
G0 âˆ¼DP(â€ž, H)
(27.29)
Gz âˆ¼DP(Â·, G0),
z = 1, . . . , K,
(27.30)

806
The Oxford Handbook of Applied Bayesian Analysis
where â€ž and Â· are concentration hyperparameters, where H is a measure and
where K is the number of measures in the collection {Gz}. The key to this
hierarchy is the presence of G0 as the base measure used to draw the random
measures {Gz}. Because G0 is discrete, only the atoms in G0 can be chosen as
atoms in the {Gz}. Moreover, because the stick-breaking weights in G0 decay,
it is only a subset of the atoms â€“ the highly weighted atoms â€“ that will tend to
occur frequently in the measures {Gz}. Thus, as desired, we share atoms among
the {Gz}.
To apply these ideas to the HMM and to the PCFG, it will prove helpful to
streamline our notation. Note in particular that in the HDP speciï¬cation, the
same atoms are used for all of the random measures. What changes among
the measures G0 and the {Gz} is not the atoms but the stick-breaking weights.
Thus we can replace with the atoms with the positive integers and focus
only on the stick-breaking weights. In particular, we re-express the HDP as
follows:
â€š âˆ¼GEM(â€ž)
(27.31)
Ë†T
z âˆ¼DP(Â·, â€š),
z = 1, . . . , K.
(27.32)
In writing the hierarchy this way, we are abusing notation. In (27.31), the vector
â€š is a vector with an inï¬nite number of components, but in (27.32), the symbol
â€š refers to the measure that has atoms at the integers with weights given by
the components of the vector. Similarly, the symbol Ë†T
z refers technically to a
measure on the integers, but we will also abuse notation and refer to Ë†T
z as a
vector.
It is important not to lose sight of the simple idea that is expressed by
(27.32): the stick-breaking weights corresponding to the measures {Gz} are
reweightings of the global stick-breaking weights given by â€š. (An explicit for-
mula relating Ë†T
z and â€š can be found in Teh et al. (2006).)
Returning to the HDP-HMM, the basic idea is to use (27.31) and (27.32) to
ï¬ll the rows of an inï¬nite-dimensional transition matrix. In this application of
the HDP, K is equal to inï¬nity, and the vectors generated by (27.31) and (27.32)
form the rows of a transition matrix for an HMM with a countably inï¬nite state
space.
We provide a full probabilistic speciï¬cation of the HDP-HMM in Figure
27.13. Each state z is associated with transition parameters Ë†T
z and emission
parameters Ë†E
z . Note that we have specialized to multinomial observations in
this ï¬gure, so that Ë†E
z is a ï¬nite-dimensional vector that we endow with a Dirich-
let distribution. Given the parameters

Ë†T
z , Ë†E
z

, a state sequence (z1, . . . , zn)
and an associated observation sequence (x1, . . . , xn) are generated as in the
classical HMM. For simplicity we assume that z1 is always ï¬xed to a designated
Start state. Given the state zt at time t, the next state zt+1 is obtained by drawing

Probabilistic Grammars and Hierarchical Dirichlet Processes
807
HDP-HMM
b âˆ¼ GEM(g) 
[draw top-level state probabilities]
For each state z âˆˆ {1,2, . . .}:
fzE âˆ¼ Dir(g) 
[draw emission parameters]
fzT âˆ¼ DP(a,b) 
[draw transition parameters]
For each time step i âˆˆ {1, . . . ,n}:
   xi âˆ¼ Mult(fEzi) 
[emit current observation]
   zi+1 âˆ¼ Mult(fTzi) 
[choose next state]
b
fTz
fEz
z
Â¥
z1
z2
z3
Â· Â· Â·
x1
x2
x3
Â· Â· Â·
Fig. 27.13 The deï¬nition and graphical model of the HDP-HMM.
an integer from Ë†T
zt, and the observed data point at time t is obtained by a draw
from Ë†E
zt.
A.2 Bayesian inference
We would like to compute the full Bayesian posterior p(Ã‹, z | x) over the HDP-
PCFG grammar Ã‹ and latent parse trees z given observed sentences x. Exact
computation of this posterior is intractable, so we must resort to an approximate
inference algorithm.
A.2.1 Sampling versus variational inference The two major classes of methodology
available for posterior inference in the HDP-PCFG are Markov chain Monte
Carlo (MCMC) sampling (Robert and Casella, 2004) or variational infer-
ence (Wainwright and Jordan, 2008). MCMC sampling is based on forming
a Markov chain that has the posterior as its stationary distribution, while varia-
tional inference is based on treating the posterior distribution as the solution to
an optimization problem and then relaxing that optimization problem. The two
methods have complementary strengths: under appropriate conditions MCMC
is guaranteed to converge to a sample from the true posterior, and its stochastic
nature makes it less prone to local optima than deterministic approaches.
Moreover, the sampling paradigm also provides substantial ï¬‚exibility; a variety
of sampling moves can be combined via Metropolis â€“ Hastings. On the other
hand, variational inference can provide a computationally efï¬cient approxima-
tion to the posterior. While the simplest variational methods can have substan-
tial bias, improved approximations can be obtained (at increased computational
cost) via improved relaxations. Moreover, storing and manipulating a variational
approximation to a posterior can be easier than working with a collection of
samples.
In our development of the HDP-PCFG, we have chosen to work within the
variational inference paradigm, in part because computationally efï¬cient infer-
ence is essential in parsing applications, and in part because of the familiarity
of variational inference ideas within NLP. Indeed, the EM algorithm can be

808
The Oxford Handbook of Applied Bayesian Analysis
viewed as coordinate ascent on a variational approximation that is based on
a degenerate posterior, and the EM algorithm (the inside-outside algorithm
in the case of PCFGs; Lari and Young, 1990) has proven to be quite effective
in NLP applications to ï¬nite non-Bayesian versions of the HMM and PCFG.
Our variational algorithm generalizes EM by using non-degenerate posterior
distributions, allowing us to capture uncertainty in the parameters. Also, the
variational algorithm is able to incorporate the DP prior, while EM cannot in a
meaningful way (see Section A.3.3 for further discussion). On the other hand,
the variational algorithm is procedurally very similar to EM and incurs very little
additional computational overhead.
A.2.2 Representation used by the inference algorithm Variational inference is based on
an approximate representation of the posterior distribution. For DP mix-
ture models (Figure 27.12), there are several possible choices of represen-
tation. In particular, one can use a stick-breaking representation (Blei and
Jordan, 2005) or a collapsed representation based on the Chinese restau-
rant process, where the parameters â€š and Ë† have been marginalized out
(Kurihara et al., 2007).
Algorithms that work in the collapsed representation have the advantage
that they only need to work in the ï¬nite space of clusterings rather than the
inï¬nite-dimensional parameter space. They have also been observed to perform
slightly better in some applications (Teh et al., 2007). In the stick-breaking rep-
resentation, one must deal with the inï¬nite-dimensional parameters by either
truncating the stick (Ishwaran and James, 2001), introducing auxiliary variables
that effectively provide an adaptive truncation (Walker, 2004), or adaptively
allocating more memory for new components (Papaspiliopoulos and Roberts,
2008).
While collapsed samplers have been effective for the DP mixture model, there
is a major drawback to using them for structured models such as the HDP-
HMM and HDP-PCFG. Consider the HDP-HMM. Conditioned on the parame-
ters, the computation of the posterior probability z can be done efï¬ciently using
the forward-backward algorithm, a dynamic program that exploits the Markov
structure of the sequence. However, when parameters have been marginalizing
out, the hidden states z sequence are coupled, making dynamic programming
impossible.3 As a result, collapsed samplers for the HDP-HMM generally end
up sampling one state zi at a time conditioned on the rest. This sacriï¬ce can be
especially problematic when there are strong dependencies along the Markov
chain, which we would expect in natural language. For example, in an HMM
model for part-of-speech tagging where the hidden states represent parts-of-
speech, consider a sentence containing a two-word fragment heads turn. Two
3 However, it is still possible to sample in the collapsed representation and still exploit dynamic
programming via Metropolis â€“ Hastings (Johnson et al., 2007).

Probabilistic Grammars and Hierarchical Dirichlet Processes
809
possible tag sequences might be Noun Verb or Verb Noun. However, in order
for the sampler to go from one to the other, it would have to go through Noun
Noun or Verb Verb, both of which are very low probability conï¬gurations.
An additional advantage of the non-collapsed representation is that condi-
tioned on the parameters, inference on the parse trees decouples and can be
easily parallelized, which is convenient for large datasets. Thus, we chose to use
the stick-breaking representation for inference.
A.3 Mean-ï¬eld variational inference for DP-based models
Since mean-ï¬eld inference is only an approximation to the HDP-PCFG poste-
rior, it is important to check that the approximation is a sensible one â€“ that we
have not entirely lost the beneï¬ts of having a nonparametric Bayesian model.
For example, EM, which approximates the posterior over parameters with a
single point estimate, is a poor approximation, which lacks the model selection
capabilities of mean-ï¬eld variational inference, as we will see in Section A.3.3.
In this section, we evaluate the mean-ï¬eld approximation with some illustra-
tive examples of simple mixture models. Section A.3.1 discusses the qualitative
nature of the approximated posterior, Section A.3.2 shows how the DP prior
manifests itself in the mean-ï¬eld update equations, and Section A.3.3 discusses
the long term effect of the DP prior over multiple mean-ï¬eld iterations.
A.3.1 Mean-ï¬eld approximation of the true posterior Consider simple mixture model in
Figure 27.14. Suppose we observe n = 3 data points drawn from the model:
(1, 4), (1, 4), and (4, 1), where each data point consists of ï¬ve binomial trials.
Figure 27.15 shows how the true posterior over parameters compares with EM
and mean-ï¬eld approximations. We make two remarks:
1. The true posterior is symmetrically bimodal, which reï¬‚ects the non-
identiï¬ability of the mixture components: Ë†1 and Ë†2 can be interchanged
without affecting the posterior probability. The mean-ï¬eld approximation
can approximate only one of those modes, but does so quite well in this
simple example. In general, the mean-ï¬eld posterior tends to underesti-
mate the variance of the true posterior.
Two-component mixture model
f1 âˆ¼ Beta(1,1) 
[draw parameter for component 1]
f2 âˆ¼ Beta(1,1) 
[draw parameter for component 2]
For each data point i âˆˆ {1, . . . ,n}:
   zi âˆ¼ Mult(       ) 
[choose component]
   xi âˆ¼ Bin(5, fzi) 
[generate data point]
1
2
1
2
,
Fig. 27.14 A two-component mixture model.

810
The Oxford Handbook of Applied Bayesian Analysis
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
f1
f2
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
f1
f2
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
f1
f2
(a) True posterior
(b) Mean-ï¬eld
(c) Degenerate (EM)
Fig. 27.15 Shows (a) the true posterior p(Ë†1, Ë†2 | x), (b) the optimal mean-ï¬eld posterior, and (c) the
optimal degenerate posterior (found by EM).
2. The mode found by mean-ï¬eld has higher variance in the Ë†1 coordinate
than the Ë†2. This is caused by the fact that there component 2 has two data
points ((1, 4) and (1, 4)) supporting the estimates whereas component 1
has only one ((4, 1)). EM (using MAP estimation) represents the posterior
as a single point at the mode, which ignores the varying amounts of
uncertainty in the parameters.
A.3.2 EM and mean-ï¬eld updates in the E-step In this section we explore the difference
between EM and mean-ï¬eld updates as reï¬‚ected in the E-step. Recall that in
the E-step, we optimize a multinomial distribution q(z) over the latent discrete
variables z; for the HDP-PCFG, the optimal multinomial distribution is given
by (27.5) and is proportional to a product of multinomial weights (27.36). The
only difference between EM and mean-ï¬eld is that EM uses the maximum
likelihood estimate of the multinomial parameters whereas mean-ï¬eld uses the
multinomial weights. Unlike the maximum likelihood solution, the multino-
mial weights do not have to sum to one; this provides mean-ï¬eld with an
additional degree of freedom that the algorithm can use to capture some of
the uncertainty. This difference is manifested in two ways, via a local tradeoff
and a global tradeoff between components.
Local
tradeoff Suppose
that
â€š âˆ¼Dir(Â·, . . . , Â·),
and
we
observe
counts
(c1, . . . , c K ) âˆ¼Mult(â€š). Think of â€š as a distribution over the K mixture
components and c1, . . . , c K as the expected counts computed in the E-step. In
the M-step, we compute the posterior over â€š, and compute the multinomial
weights needed for the next E-step.
For MAP estimation (equivalent to assuming a degenerate q(â€š)), the multino-
mial weights would be
Wi =
ci + Â· âˆ’1
K
j=1(c j + Â· âˆ’1)
.

Probabilistic Grammars and Hierarchical Dirichlet Processes
811
0.4
0.8
1.2
1.6
2.0
x
0.4
0.8
1.2
1.6
2.0
x
exp(Y(Â·))
Fig. 27.16 The exp(
(Â·)) function, which is used in computing the multinomial weights for mean-
ï¬eld inference. It has the effect of adversely impacting small counts more than large counts.
When Â· = 1, maximum likelihood is equivalent to MAP, which corresponds to
simply normalizing the counts (Wi âˆci).
Using a mean-ï¬eld approximation for q(â€š) yields the following multinomial
weights:
Wi = exp{Eâ€šâˆ¼Dir(c1+Â·,...,c K +Â·) log â€ši} =
exp(
(ci + Â·))
exp


K
j=1(c j + Â·)
.
(27.33)
Let Â· = Â·â€²/K and recall from that for large K , Dir(Â·â€²/K, . . . , Â·â€²/K ) behaves
approximately like a Dirichlet process prior with concentration Â·â€² (Theorem 2
of Ishwaran and Zarepour 2002).
When K is large, Â· = Â·â€²/K â‰ˆ0, so, crudely, Wi âˆexp(
(ci)) (although the
weights need not sum to 1). The exp(
(Â·)) function is shown in Figure 27.16.
We see that exp(
(Â·)) has the effect of reducing the weights; for example, when
c > 1
2, exp(
(c)) â‰ˆc âˆ’1
2. However, the relative reduction 1 âˆ’exp(
(ci))/ci is
much more for small values of ci than for large values of ci.
This induces a rich-gets-richer effect characteristic of Dirichlet processes,
where larger counts get further boosted and small counts get further dimin-
ished. As c increases, however, the relative reduction tends to zero. This asymp-
totic behavior is in agreement with the general fact that the Bayesian posterior
over parameters converges to a degenerate distribution at the maximum likeli-
hood estimate.
Global tradeoff Thus far we have considered the numerator of the multinomial
weight (27.33), which determines how the various components of the multino-
mial are traded off locally. In addition, there is a global tradeoff that occurs
between different multinomial distributions due to the denominator. Consider

812
The Oxford Handbook of Applied Bayesian Analysis
the two-component mixture model from Section A.3.1. Suppose that after the E-
step, the expected sufï¬cient statistics are (20, 20) for component 1 and (0.5, 0.2)
for component 2. In the M-step, we compute the multinomial weights Wzj for
components z = 1, 2 and dimensions j = 1, 2. For another observation (1, 0) âˆ¼
Bin1(Ë†z), where z âˆ¼Mult
 1
2, 1
2

, the optimal posterior q(z) is proportional to the
multinomial weights Wz1.
For MAP (q(Ë†1, Ë†2) is degenerate), these weights are
W11 =
20
20 + 20 = 0.5
and
W21 =
0.5
0.5 + 0.2 â‰ˆ0.714,
and thus component 2 has higher probability. For mean-ï¬eld (q(Ë†1, Ë†2) is a
product of two Dirichlets), these weights are
W11 =
e
20+1
e
20+20+1 â‰ˆ0.494
and
W21 =
e
0.5+1
e
0.5+0.2+1 â‰ˆ0.468,
and thus component 1 has higher probability. Note that mean-ï¬eld is sensitive
to the uncertainty over parameters: even though the mode of component 2
favors the observation (1, 0), component 2 has sufï¬ciently higher uncertainty
that the optimal posterior q(z) will favor component 1.
This global tradeoff is another way in which mean-ï¬eld penalizes the use of
many components. Components with more data supporting its parameters will
be preferred over their meager counterparts.
A.3.3 Interaction between prior and inference In this section we consider some of the
interactions between the choice of prior (Dirichlet or Dirichlet process) and the
choice of inference algorithm (EM or mean-ï¬eld). Consider a K -component
mixture model with component probabilities â€š with a â€˜nullâ€™ data-generating
distribution, meaning that F(Â·; Â·) â‰¡1 (see Section A.1.1). Note that this is not
the same as having zero data points, since each data point still has an unknown
assignment variable zi. Because of this, the approximate posterior q(â€š) does not
converge to the prior p(â€š).
Figure 27.17 shows the impact of three priors using both EM and mean-ï¬eld.
Each plot shows how the posterior over components probabilities change over
time. In general, the component probabilities would be governed by both the
inï¬‚uence of the prior shown here and the inï¬‚uence of a likelihood.
With a Dir(1, . . . , 1) prior and EM, the component probabilities do not
change over iterations since the uniform prior over component probabilities
does not prefer one component over the other (a). If we use mean-ï¬eld with
the same prior, the weights converge to the uniform distribution since Â· = 1
roughly has the effect of adding 0.5 pseudo-counts (d) (see Section A.3.2). The
same asymptotic behaviour can be obtained by using Dir(1.1, . . . , 1.1) and EM,
which is equivalent to adding 0.1 pseudo-counts (b).

Probabilistic Grammars and Hierarchical Dirichlet Processes
813
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
2
4
6
8
10
12
14
posterior component weight
iteration
(a) Dirichlet(1) prior; EM
component 0
component 1
component 2
component 3
component 4
component 5
component 6
component 7
0
2
4
6
8
10
12
14
iteration
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
posterior component weight
(b) Dirichlet(1.1) prior; EM
component 0
component 1
component 2
component 3
component 4
component 5
component 6
component 7
0
2
4
6
8
10
12
14
iteration
0
0.05
0.1
0.15
0.2
0.25
0.3
posterior component weight
(c) truncated GEM(1) prior; EM
component 0
component 1
component 2
component 3
component 4
component 5
component6
component 7
0
2
4
6
8
10
12
14
iteration
0.115
0.12
0.125
0.13
0.135
posterior component weight
(d) Dirichlet(1) prior; mean-field
component 0
component 1
component 2
component 3
component 4
component 5
component 6
component 7
0
2
4
6
8
10
12
14
iteration
0.1
0.2
0.3
0.4
0.5
posterior component weight
(e) Dirichlet(1/K) prior; mean-field
component 0
component 1
component 2
component 3
component 4
component 5
component 6
component 7
0
2
4
6
8
10
12
14
iteration
0
0.1
0.2
0.3
0.4
0.5
posterior component weight
(f) truncated GEM(1) prior; mean-field
component 0
component 1
component 2
component 3
component 4
component 5
component 6
component 7
Fig. 27.17 The evolution of the posterior mean of the component probabilities under the null data-
generating distribution for three priors p(â€š) using either EM or mean-ï¬eld.
Since the data-generating distribution is the same for all components, one
component should be sufï¬cient and desirable, but so far, none of these priors
encourage sparsity in the components. The Dir(1/K, . . . , 1/K ) prior (e), which
approximates a DP prior (Ishwaran and Zarepour, 2002), does provide this
sparsity if we use mean-ï¬eld. With Â· = 1/K near zero, components with more
counts are favoured. Therefore, the probability of the largest initial component
increases and all others are driven close to zero. Note that EM is not well deï¬ned
because the mode is not unique.
Finally, plots (c) and (f) provide two important insights:

814
The Oxford Handbook of Applied Bayesian Analysis
1. The Dirichlet process is deï¬ned by using the GEM prior (Section A.1.2),
but if we use EM for inference the prior does not produce the desired
sparsity in the posterior (c). This is an instance where the inference algo-
rithm is too weak to leverage the prior. The reason behind this failure is
that the truncation of GEM(1) places a uniform distribution on each of the
stick-breaking proportions, so there is no pressure to move away from the
initial set of component probabilities.
Note that while the truncation of GEM(1) places a uniform distribution
over stick-breaking proportions, it does not induce a uniform distribution
over the stick-breaking probabilities. If we were to compute the MAP
stick-breaking probabilities under the induced distribution, we would
obtain a different result. The reason because this discrepancy is that
although the likelihood is independent of the parametrization, the prior
is not. Note that full Bayesian posterior inference is independent of the
parametrization.
2. If we perform approximate Bayesian inference using mean-ï¬eld (f), then
we can avoid the stagnation problem that we had with EM. However,
the effect is not as dramatic as in (e). This shows that although the
Dir(1/K, . . . , 1/K ) prior and the truncated GEM(1) prior both converge
to the DP, their effect in the context of mean-ï¬eld is quite different. It is
perhaps surprising that the stick-breaking prior, which places emphasis
on decreasing component sizes, actually does not result in a mean-ï¬eld
posterior that decays as quickly as the posterior produced by the symmet-
ric Dirichlet.
B. Derivations
This section derives the update equations for the coordinate-wise ascent algo-
rithm presented in Section 27.4. We begin by deriving the form of the general
update (Section B.1). Then we apply this result to multinomial distributions
(Section B.2) for the posterior over parse trees q(z) and Dirichlet distributions
(Section B.3) for the posterior over rule probabilities q(Ë†). Finally, we describe
the optimization of the degenerate posterior over the top-level component
weights q(â€š) (Section B.4).
B.1 General updates
In a general setting, we have a ï¬xed distribution p(y) over the variables y =
(y1, . . . , yn), which can be computed up to a normalization constant. (For the
HDP-PCFG, p(y) = p(z, Ã‹ | x).) We wish to choose the best mean-ï¬eld approx-
imation q(y) = 	n
i=1 q(yi), where the q(yi) are arbitrary distributions. Recall
that the objective of mean-ï¬eld variational inference (27.2) is to minimize the
KL-divergence between q(y) and p(y). Suppose we ï¬x q(yâˆ’i) def= 	
j=/i q(y j) and

Probabilistic Grammars and Hierarchical Dirichlet Processes
815
wish to optimize q(yi). We rewrite the variational objective with the intent of
optimizing it with respect to q(yi):
KL(q(y)||p(y)) = âˆ’Eq(y)[log p(y)] âˆ’H(q(y))
= âˆ’Eq(y)[log p(yâˆ’i)p(yi | yâˆ’i)] âˆ’
n

j=1
H(q(y j))
=

âˆ’Eq(yi)[Eq(yâˆ’i) log p(yi | yâˆ’i)] âˆ’H(q(yi))

+

âˆ’Eq(yâˆ’i) log p(yâˆ’i) âˆ’

j=/i
H(q(y j))

= KL(q(yi)|| exp{Eq(yâˆ’i) log p(yi | yâˆ’i)}) + constant,
(27.34)
where the constant does not depend on q(yi). We have exploited the fact that
the entropy H(q(y)) decomposes into a sum of individual entropies because q
is fully factorized.
The KL-divergence is minimized when its two arguments are equal, so if
there are no constraints on q(yi), the optimal update is given by
q(yi) âˆexp{Eq(yâˆ’i) log p(yi | yâˆ’i)}.
(27.35)
In fact, we only need to compute p(yi | yâˆ’i) up to a normalization constant,
because q(yi) must be normalized anyway.
From (27.35), we can see a strong connection between Gibbs sampling and
mean-ï¬eld. Gibbs sampling draws yi âˆ¼p(yi | yâˆ’i). Both algorithms iteratively
update the approximation to the posterior distribution one variable at a time
using its conditional distribution.
B.2 Multinomial updates
This general update takes us from (27.4) to (27.5) in Section 27.4.2.1, but we
must still compute the multinomial weights deï¬ned in (27.6)â€“(27.7). We will
show that the multinomial weights for a general Dirichlet are as follows:
Wi
def= exp{EË†âˆ¼Dir(â€ž) log Ë†i} =
exp{
(â€ži)}
exp{
(
j â€ž j)},
(27.36)
where 
(x) =
d
dx log (x) is the digamma function.
The core of this computation is the computation of the mean value of log Ë†i
with respect to a Dirichlet distribution. Write the Dirichlet distribution in expo-
nential family form:
p(Ë† | â€ž) = exp
 
i
â€ži
log Ë†i
H IJ K
sufï¬cient
statistics
âˆ’
 
i
log (â€ži) âˆ’log 
 
i
â€ži

H
IJ
K
log-partition function A(â€ž)

.

816
The Oxford Handbook of Applied Bayesian Analysis
Since the expected sufï¬cient statistics of an exponential family are equal to the
derivatives of the cumulant function, we have
E log Ë†i = âˆ‚A(â€ž)
âˆ‚â€ži
= 
(â€ži) âˆ’
 
j
â€ž j

.
Exponentiating both sides yields (27.36).
Finally, in the context of the HDP-PCFG, (27.36) can be applied with Dir(â€ž)
as q(Ë†E), q(Ë†B), or q(Ë†T).
B.3 Dirichlet updates
Now we consider updating the Dirichlet components of the mean-ï¬eld approx-
imation, which include q(Ë†E), q(Ë†B), and q(Ë†T). We will only derive q(Ë†E) since
the others are analogous. The general mean-ï¬eld update (27.35) gives us
q

Ë†E
z

âˆexp

Eq(z) log p

Ë†E
z | Ã‹\Ë†E
z , z, x
 
(27.37)
âˆexp

Eq(z) log

xâˆˆ
Ë†E
z (x)Â·E(x) 
xâˆˆ
Ë†E
z (x)
n
i=1 I[z[i,i]=z,xi=x]
(27.38)
= exp
 
xâˆˆ
Eq(z)

Â·E(x) +
n

i=1
I[z[i,i] = z, xi = x]

log Ë†E
z (x)

(27.39)
= exp
 
xâˆˆ
(Â·E(x) + C E(z, x)) log Ë†E
z (x)

(27.40)
=

xâˆˆ
Ë†E
z (x)Â·E(x)+C E (z,x)
(27.41)
âˆDir

Ë†E
z ; Â·E(Â·) + C E(z, Â·)

.
(27.42)
B.4 Updating top-level component weights
In this section, we discuss the computation of the objective function L(â€š) and
its gradient âˆ‡L(â€š), which are required for updating the top-level component
weights (Section 27.4.2.3).
We adapt the projected gradient algorithm (Bertsekas, 1999). At each iteration
t, given the current point â€š(t), we compute the gradient âˆ‡L(â€š(t)) and update the
parameters as follows:
â€š(t+1) â†(â€š(t) + Ãtâˆ‡L(â€š(t))),
(27.43)
where Ãt is the step size, which is chosen based on approximate line search,
and  projects its argument onto the K -dimensional simplex, where K is the
truncation level.
Although â€š speciï¬es a distribution over the positive integers, due to trunca-
tion, â€šz = 0 for z > K and â€šK = 1 âˆ’K âˆ’1
z=1 â€šz. Thus, slightly abusing notation,

Probabilistic Grammars and Hierarchical Dirichlet Processes
817
we write â€š = (â€š1, . . . , â€šK âˆ’1) as the optimization variables, which must reside in
the set
T =

(â€š1, . . . , â€šK âˆ’1) : âˆ€1 â‰¤z < K, â€šz â‰¥0 and
K âˆ’1

z=1
â€šz â‰¤1

.
(27.44)
The objective function is the sum of two terms, which can be handled
separately:
L(â€š) = log GEM(â€š; Â·)
H
IJ
K
â€˜prior termâ€™ L prior
+

z
Eq log Dir

Ë†B
z ; Â·Bâ€šâ€šâŠ¤
H
IJ
K
â€˜rules termâ€™ L rules
.
(27.45)
Prior term Let ï£¿: [0, 1]K âˆ’1 $â†’T be the mapping from stick-breaking proportions
u = (u1, . . . , uK âˆ’1) to stick-breaking weights â€š = (â€š1, . . . , â€šK âˆ’1) as deï¬ned in
(27.27). The inverse map is given by
ï£¿âˆ’1(â€š) = (â€š1/T1, â€š2/T2, . . . , â€šK âˆ’1/TK âˆ’1)âŠ¤,
where
Tz
def= 1 âˆ’
zâˆ’1

z â€²=1
â€šz â€²
are the tail sums of the stick-breaking weights.
The density for the GEM prior is naturally deï¬ned in terms of the densities
of the independent Beta-distributed stick-breaking proportions u. However, to
produce a density on T , we need to perform a change of variables. First, we
compute the Jacobian of ï£¿âˆ’1:
âˆ‡ï£¿âˆ’1 =
âŽ›
âŽœâŽœâŽœâŽœâŽœâŽœâŽœâŽ
1/T1
0
0
Â· Â· Â·
0
âˆ—
1/T2
0
Â· Â· Â·
0
âˆ—
âˆ—
1/T3
Â· Â· Â·
0
...
...
...
...
...
âˆ—
âˆ—
âˆ—
âˆ—
1/TK âˆ’1
âŽž
âŽŸâŽŸâŽŸâŽŸâŽŸâŽŸâŽŸâŽ 
.
(27.46)
The determinant of a lower triangular matrix is the product of the diagonal
entries, so we have that
log det âˆ‡ï£¿âˆ’1(â€š) = âˆ’
K âˆ’1

z=1
log Tz,
(27.47)

818
The Oxford Handbook of Applied Bayesian Analysis
which yields
Lprior(â€š) = log GEM(â€š; Â·)
= log
K âˆ’1

z=1
Beta(â€šz/Tz; 1, Â·) + log det âˆ‡ï£¿âˆ’1(â€š)
= log
K âˆ’1

z=1
(Â· + 1)
(Â·)(1)(1 âˆ’â€šz/Tz)Â·âˆ’1 + log det âˆ‡ï£¿âˆ’1(â€š)
= (K âˆ’1) log Â· +
K âˆ’1

z=1
(Â· âˆ’1) log(Tz+1/Tz) + log det âˆ‡ï£¿âˆ’1(â€š)
= (Â· âˆ’1) log TK âˆ’
K âˆ’1

z=1
log Tz + constant.
Note that the Beta prior is only applied to the ï¬rst K âˆ’1 stick-breaking propor-
tions, since the K -th proportion is always ï¬xed to 1. The terms from the log
densities reduce to (Â· âˆ’1) log TK via a telescoping sum, leaving the terms from
the Jacobian as the main source of regularization.
Observing that
âˆ‚Tz
âˆ‚â€šk = âˆ’I[z > k], we can differentiate the prior term with
respect to â€šk:
âˆ‚Lprior(â€š)
âˆ‚â€šk
= âˆ’Â· âˆ’1
TK
âˆ’
K âˆ’1

z=1
âˆ’I[z > k]
Tz
(27.48)
=
K âˆ’1

z=k+1
1
Tz
âˆ’Â· âˆ’1
TK
.
(27.49)
To compute the effect of â€šk on the objective, we only need to look at tail sums
of the component weights that follow it.
For comparison, let us compute the derivative when using a Dirichlet instead
of a GEM prior:
ËœLprior(â€š) = log Dir(â€š; Â·) = (Â· âˆ’1)
K

z=1
log â€šk + constant.
(27.50)
Differentiating with respect to â€šk for k = 1, . . . , K âˆ’1 (â€šK is a deterministic
function of â€š1, . . . , â€šK âˆ’1) yields:
âˆ‚ËœLprior(â€š)
âˆ‚â€šk
= (Â· âˆ’1)
 1
â€šk
âˆ’1
â€šK

= Â· âˆ’1
â€šk
âˆ’Â· âˆ’1
TK
.
(27.51)

Probabilistic Grammars and Hierarchical Dirichlet Processes
819
Rules term We compute the remaining term in the objective function:
Lrules(â€š) =
K

z=1
Eq(Ë†) log Dir

Ë†B
z ; Â·Bâ€šâ€šâŠ¤
(27.52)
=
K

z=1
âŽ›
âŽlog (Â·B) âˆ’
K

i=1
K

j=1
log (Â·Bâ€šiâ€š j)
+
K

i=1
K

j=1
(Â·Bâ€šiâ€š j âˆ’1)Eq(Ë†) log Ë†B
z (i, j)
âŽž
âŽ .
Before differentiating L rules(â€š), let us ï¬rst differentiate a related function
Lrulesâˆ’K (â€š, â€šK ), deï¬ned on K arguments instead of K âˆ’1:
âˆ‚Lrulesâˆ’K (â€š, â€šK )
âˆ‚â€šk
=
K

z=1

âˆ’2
K

i=1
Â·Bâ€ši
(Â·Bâ€šiâ€šk)
+
K

i=1
Â·Bâ€ši Eq(Ë†) log Ë†B(k, i)Ë†B
z (i, k)

(27.53)
= Â·B
K

z=1
K

i=1
â€ši

Eq(Ë†) log Ë†B
z (k, i)Ë†B
z (i, k) âˆ’2
(Â·Bâ€šiâ€šk)

.
Now we can apply the chain rule using the fact that âˆ‚â€šK
âˆ‚â€šk = âˆ’1 for k = 1, . . . ,
K âˆ’1:
âˆ‚L rules(â€š)
âˆ‚â€šk
= âˆ‚L rulesâˆ’K(â€š, â€šK )
âˆ‚â€šk
+ âˆ‚L rulesâˆ’K(â€š, â€šK )
âˆ‚â€šK
Â· âˆ‚â€šK
âˆ‚â€šk
= Â·B
K

z=1
K

i=1
â€ši(Eq(Ë†) log Ë†B
z (k, i)Ë†B
z (i, k) âˆ’2
(Â·Bâ€šiâ€šk)) âˆ’
Â·B
K

z=1
K

i=1
â€ši(Eq(Ë†) log Ë†B
z (K, i)Ë†B
z (i, K ) âˆ’2
(Â·Bâ€šiâ€šK )). (27.54)
References
Antoniak, C. E. (1974). Mixtures of Dirichlet processes with applications to Bayesian nonpara-
metric problems. Annals of Statistics, 2, 1152â€“1174.
Beal, M., Ghahramani, Z. and Rasmussen, C. (2002). The inï¬nite hidden Markov model. In
Advances in Neural Information Processing Systems (NIPS), Volume 14, pp. 577â€“584. MIT
Press, Cambridge, MA.

820
The Oxford Handbook of Applied Bayesian Analysis
Bertsekas, D. (1999). Nonlinear Programming. Athena Scientiï¬c, Belmont, MA.
Blei, D. and Jordan, M. I. (2005). Variational inference for Dirichlet process mixtures. Bayesian
Analysis, 1, 121â€“144.
Blunsom, P., Cohn, T. and Osborne, M. (2009). Bayesian synchronous grammar induction.
In Advances in Neural Information Processing Systems (NIPS), Volume 21, pp. 161â€“168. MIT
Press, Cambridge, MA.
Carroll, G. and Charniak, E. (1992). Two experiments on learning probabilistic dependency
grammars from corpora. In Workshop Notes for Statistically-Based NLP Techniques, pp.
1â€“13. American Association for Artiï¬cial Intelligence (AAAI).
Charniak, E. (1996). Tree-bank grammars. In Proceedings of the Thirteenth National Conference on
Artiï¬cial Intelligence, American Association for Artiï¬cial Intelligence (AAAI), pp. 1031â€“1036.
MIT Press, Cambridge, MA.
Charniak, E. (2000). A maximum-entropy-inspired parser. In Applied Natural Language
Processing and North American Association for Computational Linguistics (ANLP/NAACL),
Seattle, Washington, pp. 132â€“139. Association for Computational Linguistics.
Chomsky, N. (1956). Three models for the description of language. IRE Transactions on Infor-
mation Theory, 2, 113â€“124.
Collins, M. (1999). Head-driven statistical models for natural language parsing. Ph.D. thesis,
University of Pennsylvania.
DeNero, J., Bouchard-CÃ´tÃ©, A. and Klein, D. (2008). Sampling alignment structure under
a Bayesian translation model. In Empirical Methods in Natural Language Processing
(EMNLP), Honolulu, HI, pp. 314â€“323.
Dyrka, W. and Nebel, J. (2007). A probabilistic context-free grammar for the detection of
binding sites from a protein sequence. Systems Biology, Bioinformatics and Synthetic Biology, 1,
78â€“79.
Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. Annals of Statis-
tics, 1, 209â€“230.
Ferguson, T. S. (1974). Prior distributions on spaces of probability measures. Annals of Statis-
tics, 2, 615â€“629.
Finkel, J. R., Grenager, T. and Manning, C. (2007). The inï¬nite tree. In Association for
Computational Linguistics (ACL), Prague, Czech Republic, pp. 272â€“279. Association for
Computational Linguistics.
Galley, M., Hopkins, M., Knight, K. and Marcu, D. (2004). Whatâ€™s in a translation rule? In
Human Language Technology and North American Association for Computational Linguis-
tics (HLT/NAACL), Boston, MA, pp. 273â€“280.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguis-
tics, 28, 245â€“288.
Hermjakob, U. (2001). Parsing and question classiï¬cation for question answering. In Work-
shop on Open-domain Question Answering, ACL, Toulouse, France, pp. 1â€“6.
Ishwaran, H. and James, L. F. (2001). Gibbs sampling methods for stick-breaking priors.
Journal of the American Statistical Association, 96, 161â€“173.
Ishwaran, H. and Zarepour, M. (2002). Exact and approximate sum-representations for the
Dirichlet process. Canadian Journal of Statististics, 30, 269â€“284.
Johnson, M. (1998). PCFG models of linguistic tree representations. Computational Linguis-
tics, 24, 613â€“632.
Johnson, M., Grifï¬ths, T. and Goldwater, S. (2006). Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian models. In Advances in Neural Information
Processing Systems (NIPS), Volume 19, pp. 641â€“648. MIT Press, Cambridge, MA.

Probabilistic Grammars and Hierarchical Dirichlet Processes
821
Johnson, M., Grifï¬ths, T. and Goldwater, S. (2007). Bayesian inference for PCFGs
via Markov chain Monte Carlo. In Human Language Technology and North Amer-
ican Association for Computational Linguistics (HLT/NAACL), Rochester, New York,
pp. 139â€“146.
Klein, D. and Manning, C. (2003). Accurate unlexicalized parsing. In Association for Com-
putational Linguistics (ACL), Sapporo, Japan, pp. 423â€“430. Association for Computational
Linguistics.
Klein, D. and Manning, C. D. (2004). Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Association for Computational Linguistics (ACL),
Barcelona, Spain, pp. 478â€“485. Association for Computational Linguistics.
Kurihara, K. and Sato, T. (2004). An application of the variational Bayesian approach to prob-
abilistic context-free grammars. In International Joint Conference on Natural Language
Processing Workshop Beyond Shallow Analyses, Japan.
Kurihara, K., Welling, M. and Teh, Y. W. (2007). Collapsed variational Dirichlet process mixture
models. In International Joint Conference on Artiï¬cial Intelligence (IJCAI), Hyderabad,
India.
Lari, K. and Young, S. J. (1990). The estimation of stochastic context-free grammars using the
inside-outside algorithm. Computer Speech and Language, 4, 35â€“56.
MacKay, D. (1997). Ensemble learning for hidden Markov models. Technical report, University
of Cambridge.
Manning, C. and SchÃ¼tze, H. (1999). Foundations of Statistical Natural Language Processing. MIT
Press, Cambridge, MA.
Marcus, M. P., Marcinkiewicz, M. A. and Santorini, B. (1993). Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics, 19, 313â€“330.
Matsuzaki, T., Miyao, Y. and Tsujii, J. (2005). Probabilistic CFG with latent annotations. In Asso-
ciation for Computational Linguistics (ACL), Ann Arbor, Michigan, pp. 75â€“82. Association
for Computational Linguistics.
Papaspiliopoulos, O. and Roberts, G. O. (2008). Retrospective MCMC for Dirichlet process
hierarchical models. Biometrika, 95, 169â€“186.
Pereira, F. and Shabes, Y. (1992). Inside-outside reestimation from partially bracketed corpora.
In Association for Computational Linguistics (ACL), Newark, Delaware, pp. 128â€“135. Asso-
ciation for Computational Linguistics.
Petrov, S., Barrett, L., Thibaux, R. and Klein, D. (2006). Learning accurate, compact, and
interpretable tree annotation. In International Conference on Computational Linguistics
and Association for Computational Linguistics (COLING/ACL), pp. 433â€“440. Association
for Computational Linguistics.
Petrov, S. and Klein, D. (2007). Learning and inference for hierarchically split PCFGs. In
Human Language Technology and North American Association for Computational Linguis-
tics (HLT/NAACL), Rochester, New York, pp. 404â€“411.
Robert, C. P. and Casella, G. (2004). Monte Carlo Statistical Methods. Springer, New York.
Rodriguez, A., Dunson, D. B. and Gelfand, A. E. (2008). The nested Dirichlet process. Journal
of the American Statistical Association, 103, 1131â€“1144.
Sakakibara, Y. (2005). Grammatical inference in bioinformatics. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 27, 1051â€“1062.
Sethuraman, J. (1994). A constructive deï¬nition of Dirichlet priors. Statistica Sinica, 4, 639â€“650.
Smith, N. and Eisner, J. (2005). Contrastive estimation: Training log-linear models on unlabeled
data. In Association for Computational Linguistics (ACL), Ann Arbor, Michigan, pp. 354â€“
362. Association for Computational Linguistics.

822
The Oxford Handbook of Applied Bayesian Analysis
Stolcke, A. and Omohundro, S. (1994). Inducing probabilistic grammars by Bayesian model
merging. In International Colloquium on Grammatical Inference and Applications, London,
UK, pp. 106â€“118. Springer-Verlag, Berlin.
Teh, Y. W., Jordan, M. I., Beal, M. and Blei, D. (2006). Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101, 1566â€“1581.
Teh, Y. W., Newman, D. and Welling, M. (2007). A collapsed variational Bayesian inference
algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems
(NIPS), Volume 19, pp. 1353â€“1360. MIT Press, Cambridge, MA.
Wainwright, M. and Jordan, M. I. (2008). Graphical models, exponential families, and varia-
tional inference. Foundations and Trends in Machine Learning, 1, 1â€“307.
Walker, S. G. (2004). Sampling the Dirichlet mixture model with slices. Communications in
Statistics â€“ Simulation and Computation, 36, 45â€“54.
Wu, D. (1997). Stochastic inversion transduction grammars and bilingual parsing of parallel
corpora. Computational Linguistics, 23, 377â€“404.
Zhu, S. C. and Mumford, D. (2006). A stochastic grammar of images. Foundations and Trends
in Computer Graphics and Vision, 2, 259â€“362.

Â·28Â·
Designing and analysing a circuit
device experiment using treed
Gaussian processes
Herbert K. H. Lee, Matthew Taddy, Robert B. Gramacy
and Genetha A. Gray
28.1 Introduction
This chapter describes work in the development of a new circuit device, and
is part of a collaboration with scientists at Sandia National Laboratories. Cir-
cuit devices need to be tested for various capabilities during the development
phase, in order to eventually create a device that will be effective in a range
of operational environments. Both physical and computer simulation experi-
ments are used. Our work here focuses on two parts of this process: creation
of the design for the physical experiment, and optimization during the cali-
bration of the computer model. Our key statistical tool is the treed Gaussian
process.
The goal of our collaborators was to both â€˜calibrateâ€™ and â€˜validateâ€™ computer
models of the electrical circuit devices. These models can then be used for
the next stage of the design process. In this context, calibration uses data
from physical experiments to inform upon uncertain input parameters of a
computer simulation model (Kennedy and Oâ€™Hagan, 2001, for example). Model
parameters are tuned so that the code calculations in the simulator most closely
match the observed behaviour of the experimental data. Accurate calibration
both improves predictive capabilities and minimizes the information lost by
using a numerical model instead of the actual system.
In contrast, the validation process is applied in order to quantify the degree
to which a computational model is an accurate representation of the real
world phenomena it seeks to represent (Oberkampf et al., 2003). Validation
is critical to ensure some level of predictive capability of the simulation
codes so that these codes can be subsequently used to study or predict sit-
uations for which experimental data are unavailable due to environmental
or economic limitations. Model validation is a major effort regularly under-
taken by numerous government agencies and private companies. Our role as

824
The Oxford Handbook of Applied Bayesian Analysis
statisticians focused on the calibration aspects, and did not deal directly with
validation.
28.1.1 Circuit experiments
The circuit devices under study are bipolar junction transistors (bjt), which
are used to amplify electrical current. They exist in both PNP and NPN con-
structions (Sedra and Smith, 1997). Twenty different such devices were stud-
ied; in this chapter we present results only for a few of them. For example,
we will consider the bft92a, which is a PNP, and the bfs17a which is an
NPN. The primary interest is in understanding current output as a function
of the intensity of a pulse of gamma radiation applied to the devices, where
the current output is characterized by the peak amplitude reached during
the experiment. Because of the physical setup of the experiment, a particular
testing facility is capable of only a rather limited range of possible radiation
doses, so experiments were run at three different facilities to span a broader
range of possible dose rates. Although in principle the relationship between
dose rate and peak amplitude should not depend on the facility, some of the
results do appear to show a facility effect. Experimental runs were done at
three different temperature settings. The scientists do allow for the results to
depend on temperature, and they have separate computer simulation models
for each temperature. Further details on the physical experiment are available in
Gray et al. (2007).
The physical experiment is accompanied by a collection of computer simula-
tion models. These radiation-aware models are built on a Xyce implementation
of the Tor Fjeldy photocurrent model for the bjt. Xyce is an electrical circuit
simulator developed at Sandia National Laboratories (Keiter, 2004), and the
Tor Fjeldy photocurrent model is described in detail in Fjeldy et al. (1997).
The model input is a radiation pulse expressed as a dose rate over time. The
corresponding output is a curve of current value over time which reï¬‚ects the
response of the electrical device. This curve is summarized by its maximum
value. The model also involves 38 user-deï¬ned tuning parameters. It is these
parameters that need to be calibrated using the physical data so that the simu-
lator output is as close as possible to the results from the physical experiments.
All bjts share a basic underlying model and the same computer simulator can
be used to approximate their behavior, with only changes to the parameters
required.
28.1.2 Treed Gaussian processes
Treed Gaussian processes are a highly ï¬‚exible and computationally efï¬cient
model for spatially correlated data, as well as for more general functions.
They combine standard Gaussian processes with treed partition models,

Designing and Analysing a Circuit Device Experiment
825
producing an effective semi-parametric non-stationary model. In this section,
we start with a brief review of Gaussian processes, cover the basics of treed
Gaussian processes, then discuss their use for adaptive sampling in com-
puter experiments and for optimization. Additional details are available in the
appendix.
Gaussian processes are the standard model for creating a statistical emulator
of the output of a computer simulator (Sacks et al., 1989; Kennedy and Oâ€™Hagan,
2001; Santner et al., 2003). A Gaussian process (GP) speciï¬es that the set of
responses z(x1, . . . , xm) at any ï¬nite collection of locations x (which could be
spatial locations or multivariate inputs to a computer model) has a multivariate
Gaussian distribution. In general, we can write z(x) = Ã(x) + w(x) where Ã is
a mean function and w(x) is a zero mean random process with covariance
C(xj, xk). Typically we will assume stationarity for the correlation structure, and
thus we have C(xj, xk) = Ã›2K (xj, xk) where K is a correlation matrix whose
entries depend only on the difference vectors xj âˆ’xk. Sometimes a further
assumption of isotropy is made, and then the correlations depend only on the
distance between xj and xk, typically speciï¬ed with a simple parametric form
(Abrahamsen, 1997). We take the mean function to be linear in the inputs,
Ã(x) = â€šâ€²x. We also use a small nugget to allow for noisy data, for smoothing,
or for numerical stability. For more details on GPs, we refer the reader to
Cressie (1993) or Stein (1999). Herein we use the separable power family for
the correlation structure, with a separate range parameter di in each dimension
(i = 1, . . . , mX), and power p0 = 2 to give smooth realizations:
K (xj, xk|d) = exp

âˆ’
mX

i=1
|xi j âˆ’xik|p0
di

.
(28.1)
Standard GPs have three drawbacks that affect us here. First, they are compu-
tationally intensive to ï¬t, with effort growing with the cube of the sample size
due to the need to invert the covariance matrix. Second, GP models usually
assume stationarity, in that the same covariance structure is used throughout
the entire input space, which may be too strong of a modelling assumption,
or else they are fully nonstationary and too computationally expensive to be
used for more than a relatively small number of datapoints. Third, the esti-
mated predictive error of a stationary model does not directly depend on the
locally observed response values. Rather, the predictive error at a point depends
only on the locations of the nearby observations and on a global measure of
error that uses all of the discrepancies between observations and predictions
without regard for their distance from the point of interest (because of the
stationarity assumption). Thus there is no ready local measure of lack-of-ï¬t or
uncertainty, which will be needed for sequential experimental design (adaptive
sampling).

826
The Oxford Handbook of Applied Bayesian Analysis
All of these issues can be addressed by combining the stationary GP model
with treed partitioning, resulting in what we call a treed Gaussian process
(TGP). The input space is partitioned into disjoint regions, with an independent
stationary GP ï¬t in each region. This approach provides a computationally
efï¬cient way of creating a non-stationary model. It reduces the overall com-
putational demands by ï¬tting separate GP models to smaller data sets (the
individual partitions). The partitioning approach is based on that of Chipman
et al. (1998, 2002), who used it to develop the Bayesian classiï¬cation and
regression trees. Reversible jump Markov chain Monte Carlo (Green, 1995)
with tree proposal operations (prune, grow, swap, change, and rotate) allows
simultaneous ï¬tting of the tree and the parameters of the individual GP mod-
els. In this way, all parts of the model can be learned automatically from the
data, and Bayesian model averaging through reversible jump allows for explicit
estimation of predictive uncertainty. It also provides a smooth mean ï¬t when
appropriate (as the partition locations are also averaged over, so the mean
function does not exhibit discontinuities unless the data call for such a ï¬t). We
provide more details in the appendix, and also point the reader to Gramacy and
Lee (2008a). Software is available in the tgp library for the statistical package
R
at
http://www.cran.r-project.org/web/packages/tgp/index.html
(Gramacy and Taddy, 2008; Gramacy, 2007).
28.1.2.1 Adaptive sampling
When each datapoint is difï¬cult or expensive to collect, either because a physical
experiment is involved or because the computer simulator takes a long time to
run, it is important to choose the sample points with great care. One needs to
select the set of design points that will provide maximum information about
the problem. Traditional methods for experimental design involve starting with
a speciï¬c model then creating a design of all of the planned runs. If a simple
enough model is chosen, or enough assumptions about parameter values are
made, then optimal designs can be found in closed form. For more complex
cases, optimal designs are found numerically. Various optimality criteria lead to
designs such as A-optimal, D-optimal, maximin, maximum entropy, and Latin
hypercube designs. A good review of the Bayesian literature on optimal designs
is provided by Chaloner and Verdinelli (1995). However, such an approach is
not readily amenable to learning about the process as the data are collected.
The concept of sequential experimental design is less well-developed, but work
has progressed in some areas, such as computer experiments (Sacks et al., 1989;
Santner et al., 2003). By updating the design after learning from each new obser-
vation (or each new batch), one can better deal with lack of knowledge about
model parameters, or even about the model itself. Here we are concerned about
learning both the tree structure and the GP parameters during the experiment,

Designing and Analysing a Circuit Device Experiment
827
and so direct application of optimality criteria is not feasible. Some additional
background on experimental design is in the appendix.
In selecting design points sequentially, we want those which will provide the
most additional information, conditional on the data we have already collected.
Because we are ï¬tting a fully Bayesian model, we can consider the predictive
variance as a measure of our uncertainty. Thus one way to deï¬ne the amount of
information we expect to learn from a new datapoint is to look at the expected
reduction in squared error averaged over the input space. This approach was
developed in the active learning literature in computer science by Cohn (1996).
This expected reduction in global error for a proposed new sample Ëœx under a
GP model can be expressed as:
Ë†Ã›2(Ëœx) =

Ë†Ã›2
Ëœx(y)dy â‰¡

Ë†Ã›2(y) âˆ’Ë†Ã›2
Ëœx(y)dy =
 Ã›2 
q â€²(y)Câˆ’1q(x) âˆ’Ã(x, y)
2
Ã(x, x) âˆ’q â€²(x)Câˆ’1q(x)
dy
(28.2)
where y represents the input space, Ã›2 is the overall variance, Ë†Ã›2
Ëœx(y) is the
estimated (posterior) predictive variance at y when Ëœx is added into the design,
and
Câˆ’1 = (K + Ã™2FWFâ€²)âˆ’1
q(x) = k(x) + Ã™2FW f (x)
Ã(x, y) = K (x, y) + Ã™2 f â€²(x)Wf (y)
with f â€²(x) = (1, xâ€²), and k(x) an n-vector with kÃŒ, j(x) = K (x, xj), for all xj âˆˆX.
Refer to the appendix for deï¬nitions of the other quantities, e.g. W and K .
For a treed GP, this expression is evaluated within partitions each MCMC
iteration and the result averaged across the whole MCMC run. For points in
separate partitions, there is no change in predictive variance. More details on
this derivation are given by Gramacy and Lee (2009). In practice the integral is
evaluated as a sum over a grid of locations.
Searching for the optimal design point over a multidimensional continuous
space is quite difï¬cult and computationally expensive, so we restrict our atten-
tion to a smaller set of candidate points that are well-spaced out relative to each
other, and then choose the best point from our candidate set. To obtain a well-
spaced set, we return to standard optimal designs, such as maximum entropy
designs, maximin designs, and/or Latin hypercubes. This approach is then
easily extensible to choosing more than one point, either because a physical
experiment is being done in batches or because an asynchronous parallel com-
puting environment is being used for a computer experiment. Since the points
are spread apart, a simple approach to choosing nb design points for the next
batch is to just select the nb points in the candidate set with the highest Ë†Ã›2(Ëœx)
as per equation (28.2). A better approach, if computational resources allow, is

828
The Oxford Handbook of Applied Bayesian Analysis
to choose the ï¬rst design point as the one with the highest Ë†Ã›2(Ëœx), then to
add a pseudo-datapoint at this chosen location with value equal to its predictive
mean value and to recompute equation (28.2), choosing the second point as the
one that maximizes this among the remaining points in the candidate set. This
approximation has the effect of reducing uncertainty in the local region of the
ï¬rst point, so that the second point will typically be selected from a different
part of the input space. This process is iterated until nb design points are
obtained.
28.1.2.2 Sensitivity analysis
Global sensitivity analysis (SA; not to be confused with local derivative based
analysis) is a resolving of the sources of output variability by apportioning
elements of this variation to different sets of input variables (Saltelli et al., 2000).
In large engineering problems there can be a huge number of input variables
over which the objective is to be optimized, but only a small subset will be
inï¬‚uential within the conï¬nes of their uncertainty distribution. Thus global SA
is an important (but often overlooked) aspect of efï¬cient optimization and it
may be performed, at relatively little additional cost, on the basis of a statistical
model ï¬t to the initial sample. Variance-based SA methods decompose the
variance of the objective function, with respect to an uncertainty distribution
placed on the inputs, into variances of conditional expectations. These provide
a natural measure of the output association with speciï¬c sets of variables
and provide a basis upon which the importance of individual inputs may be
judged.
We will concentrate on two inï¬‚uential sensitivity indices: the ï¬rst order for
the jth input variable, Sj = var

E

f (x)|xj

/var( f (x)), and the total sensitivity
for input j, Tj = E

var

f (x)|xâˆ’j

/var( f (x)). Here, f denotes the objective
function, xâˆ’j is the input vector excluding the jth input, and all expectations
and variances are taken with respect to the uncertainty distribution placed
on x. The uncertainty distribution may be any probability function deï¬ned over
the input space, but we will assume that it consists of independent uniform
distributions over each (bounded) dimension of the input space. The ï¬rst
order indices measure the portion of variability that is due to variation in the
main effects for each input variable, while the total effect indices measure
the portion of variability that is due to total variation in each input. Thus,
the difference between Tj and Sj provides a measure of the variability in the
objective function due to interaction between input j and the other input
variables. A large difference may lead the investigator to consider other sen-
sitivity indices to determine where this interaction is most inï¬‚uential, and
this is often a key aspect of the dimension-reduction that SA provides for
optimization problems. Refer to Sobolâ€™ (1993) and Homma and Saltelli (1996)

Designing and Analysing a Circuit Device Experiment
829
for a complete discussion of the properties and derivation of variance-based SA
indices.
The inï¬‚uential paper by Oakley and Oâ€™Hagan (2004) describes an empirical
Bayes estimation procedure for the sensitivity indices; however, some variability
in the indices is lost due to plug-in estimation of GP model parameters and,
more worryingly, the variance ratios are only possible in the form of a ratio
of expected values. Likelihood based approaches are proposed by Welch et al.
(1992) and in Morris et al. (2008). The technique proposed here is, in contrast,
fully Bayesian and provides a complete accounting of the uncertainty involved.
Brieï¬‚y, at each iteration of an MCMC sampler that is taking draws from the
TGP posterior, output is predicted over a large (carefully chosen) set of input
locations. Conditional on this predicted output, the sensitivity indices can be
calculated via Monte Carlo integration. In particular, Saltelli (2002) describes an
efï¬cient LHS based scheme for estimation of both ï¬rst order and total effect
indices in such situations, and we follow this technique exactly. That is, the
locations chosen for TGP prediction are precisely those prescribed in Saltelliâ€™s
approach. At each MCMC iteration, after calculating Monte Carlo estimates of
the integrals involved conditional on the TGP predicted response, we obtain a
posterior realization of the variance indices. The resultant full posterior sample
then incorporates variability from both the integral estimation and uncertainty
about the function output.
Apart from the variance-related quantities, another common component of
global SA is an accounting of the main effects for each input variable, E[ f (x)|xj]
as a function of xj. These can easily be obtained as a byproduct of the above
variance analysis procedure, again through Monte Carlo integration conditional
upon the TGP predicted response.
28.1.2.3 Optimization
In Section 28.1.2.1, TGP prediction was used to guide the intelligent collection
of data, with the goal being to minimize the predictive variance. An alternative
goal would be one of optimization â€“ it is not the entire response surface which
is of interest, but rather only the minimum (or maximum) response point on
this surface. In this case, although TGP prediction still forms the backbone of
our inference, we need a different objective function and a different sampling
approach.
Statistical methods are useful in optimization problems, particularly where
the function being optimized is best treated as an expensive black-box, i.e. each
function evaluation is relatively costly to obtain (physically or in computing
time), and no additional information about the function (such as parametric
form or gradient evaluations) is available. Creation of a statistical surrogate
model, or emulator, of the black-box then allows optimization on the cheaper

830
The Oxford Handbook of Applied Bayesian Analysis
statistical model, reducing the need for expensive full evaluations (Booker et al.
1999). The most direct application of this approach would have statistical pre-
diction fully determine the search for an optimum input conï¬guration, and
the most prominent example of this strategy is the Expected Global Optimizer
(EGO) algorithm of Jones et al. (1998). The method is designed to search the
input space and converge towards the global minimum. At each iteration, a GP
is ï¬t to the current set of function evaluations and a new location for data col-
lection is chosen based upon the GP posterior expectation for the improvement
statistics:
I(x) = max{( fbest âˆ’f (x)), 0},
(28.3)
where fbest is response corresponding to the current best point in the search.
The location with maximum expected improvement is chosen for evaluation,
and following evaluation the GP is then ï¬t anew to the data set augmented by
these results. Schonlau et al. (1998) provide an extensive discussion of improve-
ment statistics. The key to success for this algorithm is that, in the expecta-
tion, candidate locations are rewarded both for a near-optimal mean predicted
response as well as for high response uncertainty (indicating a poorly explored
region of the input space). Hence, the posterior expectation for improvement
provides an ideal statistic to inform intelligent optimization.
For this particular project, our collaborators were interested in not simple
global point optimization, but rather robust local optimization. Cost and time
constraints often make it infeasible to execute a search of the magnitude
required to guarantee global convergence (such as EGO). On the other hand,
purely local optimization algorithms will fail on highly multimodal problems
where one can easily get stuck in poor local optima. Furthermore, in engineer-
ing applications (such as that discussed herein) it is essential to avoid local
solutions on a knifeâ€™s edge portion of the response surface, where small changes
in the input lead to large changes in the response.
Thus, the problem at hand may be characterized as robust local optimiza-
tion â€“ we need to ï¬nd a solution with a response that is close to the global
optimum, while using many fewer iterations than a truly global search would
require. The proposed solution is to combine existing local optimization meth-
ods, for quick convergence, with TGP statistical prediction, to give the algo-
rithm a global scope. Brieï¬‚y, a local optimization search pattern is periodically
augmented with locations chosen to maximize the TGP predicted expected
improvement.
The TGP generated search pattern consists of m locations that maximize
(over a discrete candidate set chosen through some space-ï¬lling design) the
expected multilocation improvement, E

I(x1, . . . , xm)

, where
I(x1, . . . , xm) = max{( fbest âˆ’f (x1)), . . . , ( fbest âˆ’f (xm)), 0}
(28.4)

Designing and Analysing a Circuit Device Experiment
831
(Schonlau et al., 1998). Taking a fully Bayesian approach, the improvement
I(Ëœx) is drawn for each Ëœx in the candidate set at each iteration of MCMC
sampling from the TGP posterior. This offers an improvement over compet-
ing algorithms that use only point estimates of the parameters governing the
probability distribution around the response surface. Our Bayesian analysis
results in a full posterior predictive distribution for the response (and, hence,
the improvement) at any desired location in the input space. This full posterior
predictive sample is essential to the maximization of the multivariate expected
improvement in (28.4): locations x1 through xm may be chosen iteratively,
such that each xi maximizes the expected i-location improvement conditional
on x1 through xiâˆ’1 already having been selected. Full posterior samples for
the improvement statistics at each Ëœx are required to recalculate the expected
improvement conditional on the iteratively expanding list of selected locations.
This simpliï¬es what would have itself been a complex optimization prob-
lem, and has the added beneï¬t of deï¬ning an order to the list of m search
locations.
Finally, although our general hybrid optimization approach will work with
any local pattern search algorithm, the local optimization scheme used here
is the asynchronous parallel pattern search (APPS) (Kolda, 2005; Gray and
Kolda, 2006) developed at Sandia National Laboratories. APPS is a derivative-
free optimization method that works by evaluating points in a search pattern of
decreasing radius around the present best point. Software is publicly available at
http://software.sandia.gov/appspack/. The primary stopping criterion
is based on step length, and APPS is locally convergent under mild condi-
tions. In addition, APPS is an efï¬cient method for ï¬nding a local optimum
when already in the neighbourhood of this optimum. By combining it with
a TGP emulator, we can more quickly ï¬nd the correct neighbourhood. Thus
we use more points chosen by TGP early in the optimization process, and
more points chosen by APPS later in the process. More details of optimization
through posterior expected improvement via TGP, as well as the hybrid opti-
mization algorithm with APPS and a parallel implementation, are provided by
Taddy et al. (2009).
28.2 Experimental design
Calibration of the circuit devices under study consists of a minimization of
a loss function for the distance between simulated current amplitude curves
and experimental data. Before turning to the optimization problem in the
following section, it is necessary to physically test the devices. The goal of
this calibration is to obtain a single simulator parametrization for each device
that will provide accurate current amplitude curve predictions under a variety

832
The Oxford Handbook of Applied Bayesian Analysis
of different physical situations. In particular, simulation will be required at
different temperatures, for different radiation dosages, and at different DC
voltage bias levels for the relevant complex system composed of the circuit
devices. Thus the experiment design should provide as full a picture as possible
of the device performance over the space deï¬ned by these three physical vari-
ables. Although device performance is completely characterized as a current
amplitude curve, the peak amplitude provided a univariate output which the
experimentalists felt would be representative of the device behaviour at different
input variable values.
For each of 20 circuit devices, historical testing had yielded a bank (approxi-
mately 50 observations per device) of existing data for device current amplitude
behaviour at various levels of temperature, radiation dosage, and bias. The
experimentalists requested a list of 150 additional variable location vectors for
testing. Although, in each case, the underlying input variables are continuous
(and unbounded over the region of interest), the limitations of physical exper-
imentation reduced the possible input values to three temperature levels, six
dosage levels, and ï¬ve bias levels. Hence, the space of potential experiment
input locations consisted of a 3 Ã— 6 Ã— 5 = 90 point grid. These factor levels
correspond only to the set-up conï¬guration; actual temperature, dosage, and
bias amounts for each experiment are measurable and will be in the neighbour-
hood of the speciï¬ed input conï¬guration, but will not be exactly the same as
designed.
The experiments are performed in batches, with ï¬ve circuit devices grouped
together on each of four different boards. Due to the difï¬culties inherent in
ï¬nding a design that is optimal for each batch of ï¬ve circuits, one device was
chosen to be representative of each board and the experiment was designed
around this single device. There is random noise in the results, and the data
already include replication at individual input vectors. The list for additional
testing locations should include replication where necessary to reduce the over-
all variance of output (peak amplitude) predictions.
Our approach to design is an iterative application of the adaptive sampling
procedure outlined in Section 28.1.2.1. The expected reduction in global error
shown in (28.2) is not easily extended to an analogous criterion for combina-
tions of multiple locations. This issue is resolved through the implementation
of a greedy algorithm which repeatedly chooses a single new location for testing
to be added to an existing list. A ï¬rst new input vector is chosen exactly as
proposed above in Section 28.1.2.1, through adaptive sampling based on the
TGP prior for peak amplitude conditional on temperature, dosage, and usage.
A value for realized peak amplitude at this input location is then drawn from
the conditional posterior predictive distribution, and this value is used as the
imputed output corresponding to a future test at this location. Thus in search-
ing for the second location, we treat the existing data as the combination of

Designing and Analysing a Circuit Device Experiment
833
the observed data and the one new imputed point, and now look to maximize
the expected reduction in global error of this updated dataset. The process
is repeated, and at each iteration the treed GP model is ï¬t to the existing
data augmented by randomly imputed output values at all of the locations
already chosen for future testing. All of the existing imputed output values
are redrawn from their conditional posterior predictive distributions at each
iteration. This additional variability helps to account for the variability in the
physical observations (in contrast to the typically deterministic behaviour of a
computer simulator). This iterative adaptive sampling algorithm is used to pro-
vide an ordered list of 150 locations (including repetition) for additional testing
of each of the 20 devices. The prioritization implied by the ordering of the list
is especially valuable in the motivating example, as the expense of individual
experiments is unknown in advance and experimentation is terminated once
the study has reached a predetermined budget constraint. At some point, a
batch of experimental data becomes available, and all of the placeholder values
are replaced by the new real data, and the iterative process continues for the
next round of physical experiments.
Results for two of the devices, bft92a and bfs17a, are shown in Figure 28.1.
The predictions shown in the left hand ï¬gure expose a discontinuity in the
log peak amplitude surface which occurs for log dosages between 20 and 21.
Although the exhibited results are conditional on a temperature of 75 celsius
and a bias of zero, similar behaviour was discovered at other parameter conï¬g-
urations. The posterior mean for error reduction, Ë†Ã›2(x) as deï¬ned in (28.2),
from additional testing at a single new point x, is plotted in the right hand
panel. We see that Ë†Ã›2(x) is substantially reduced following the additional
testing, with signiï¬cant room for variance reduction only for x in the zone of
discontinuity and at the boundaries. Finally, we note that the complex surfaces
shown in the left hand ï¬gure mean that signiï¬cant modeling gains are avail-
able by using treed Gaussian processes instead of more traditional stationary
models.
28.3 Calibrating the computer model
The second part of this project focused on calibration and validation of a
computer simulation model for the circuits. Numerical simulation is increas-
ingly used because of advances in computing capabilities and the rising costs
associated with physical experiments. In this context the computer models are
often treated as an objective function to be optimized, and this is how they
were treated by our collaborators. The challenges inherent in implementing this
optimization are characterized by an inability to calculate derivatives and by the
expense of obtaining a realization from the objective function. Fast convergence

834
The Oxford Handbook of Applied Bayesian Analysis
â€“10
â€“9
â€“8
â€“7
â€“6
â€“10
â€“9
â€“8
â€“7
â€“6
log Peak Amp
17
18
19
20
21
22
23
24
log Dosage
â€“10
â€“9
â€“8
â€“7
â€“6
â€“10
â€“9
â€“8
â€“7
â€“6
log Peak Amp
17
18
19
20
21
22
23
24
log Dosage
0
5eâ€“5
1eâ€“4
1.5eâ€“4
0
5eâ€“5
1eâ€“4
1.5eâ€“4
Î”Ïƒ^ 2
Fig. 28.1 Experiment results for bft92a on the top and bfs17a on the bottom, given a temperature of
75 celsius and zero bias. The left hand ï¬gures show mean (solid lines) and 90% interval predicted
log peak amplitude; grey lines correspond to a TGP ï¬t for the original observations, and black
corresponds to a TGP ï¬t to all observations. The right hand ï¬gure shows the original data in black
and the data obtained through additional testing in grey. Plotted over the data we see the expected
posterior reduction in error variance, Ë†Ã›2 for this temperature and bias, as a function of log dosage.
The solid line refers to the posterior expectation after the original testing, and the dashed line
corresponds to posterior expectation conditional on the completed dataset.
of the optimizer is needed because of the cost of simulation, and a search of the
magnitude required to guarantee global convergence is not feasible. However,
it is important to be aware that these large engineering problems typically have
multimodal objective functions, so we want to avoid converging to low quality
solutions. Thus we combine the local optimization method APPS for quick
convergence with TGP to help provide a more robust solution, as described
in Section 28.1.2.3.
The goal here is to ï¬nd tuning parameter values for the Xyce simulator
that lead to predicted current output that is as close as possible to the results
obtained in the physical experiments. Here â€˜closeâ€™ is deï¬ned through a squared-
error objective function:
f (x) =
N

i=1
1
Ti
Ti

t=1

(Si(t; x) âˆ’Ei(t))2
,
(28.5)
where N is the number of physical experimental runs (each corresponding
to a unique radiation pulse), Ti is the total number of time observations for

Designing and Analysing a Circuit Device Experiment
835
experiment i, Ei(t) is the amount of electrical current observed during exper-
iment i at time t, and Si(t; x) is the amount of electrical current at time t
as computed by the simulator with tuning parameters x and radiation pulse
corresponding to experiment i. Since each physical experiment may result in a
different number of usable time observations, the weights of the errors are stan-
dardized so that each experimental run is counted equivalently. We note that the
traditional statistical approach would also include a term for model discrepancy.
However, our collaborators did not want such a term, as the mathematical
modellers want to know what the best ï¬t is for their model, and then they
intend to address any remaining discrepancies by updating the physics in their
model.
Because of the need to do both calibration and validation, only six experimen-
tal runs were used for each calibration, with the remaining datapoints saved for
the validation stage. For each circuit device and each temperature setting, the six
points were chosen to be representative of the whole set of data collected, and
were selected by ï¬tting a six-component mixture model. The selection details
are not needed herein, we just treat the six points as the available data, but if
the reader is interested, the details are given by Lee et al. (2009).
The simulator involves 38 user-deï¬ned tuning parameters for modeling cur-
rent output as a function of radiation pulse input. Through discussion with
experimentalists and researchers familiar with the simulator, 30 of the tuning
parameters were ï¬xed in advance to values either well known in the semicon-
ductor industry or determined through analysis of the device construction. The
semiconductor engineers also provided informative bounds for the remaining
eight parameters. It is these eight parameters which are the inputs for our
objective function (28.5). These parameters include those that are believed to
have both a large overall effect on the output of the model and a high level
of uncertainty with respect to their ideal values. Figure 28.2 shows the results
of an MCMC sensitivity analysis, as described in Section 28.1.2.2, based on a
TGP model ï¬t to an initial Latin Hypercube Sample (LHS) of 160 input loca-
tions and with respect to a uniform uncertainty distribution over the bounded
parameter space. All of the eight parameters appear to have signiï¬cant effect
on the objective function variability, although the main effect and ï¬rst-order
plots indicate that some variables are only effective in interaction with the other
inputs. These higher-order interactions create challenges for optimization. In
addition, the posterior mean main effect plots alerted the researchers to the
possibility of optimal solutions on the boundaries of the input space (especially
for x2 and x7) and will provide valuable guidance for checking the validity of the
calibrated simulator.
The objective function (28.5) was optimized using both APPS by itself and
the hybrid algorithm TGP-APPS. In the case of the hybrid algorithm, a LHS
of 160 points was used to provide and initial ï¬t for the TGP. The wall clock

836
The Oxford Handbook of Applied Bayesian Analysis
0
0.2
0.4
0.6
0.8
1
Firstâ€“order
Total effect
x1
x2
x3
x4
x5
x6
x7
x8
0
0.2
0.4
0.6
0.8
1
x1
x2
x3
x4
x5
x6
x7
x8
â€“0.2
â€“0.1
0.0
0.1
0.2
x1
x5
x2
x6
x3
x7
x4
x8
â€“0.2
â€“0.1
0.0
0.1
0.2
â€“0.4
â€“0.2
0.0
0.2
0.4
Objective
Scaled input
Fig. 28.2 Sensitivity analysis for bft92a (top) and bfs17a (bottom) optimization objective functions,
summarized by posterior distributions for the ï¬rst order (left), total (middle) sensitivity indices, and
posterior mean main effects (right).

Designing and Analysing a Circuit Device Experiment
837
Table 28.1 For each bjt device and each opti-
mization algorithm, the number of objective
function evaluations and total wall clock time
required to ï¬nd a solution.
Device
Method
Evaluations
Time (hours)
bft92a
APPS
6823
94.8
bft92a
APPS-TGP
962
13.8
bfs17a
APPS
811
10.3
bfs17a
APPS-TGP
1389
18.1
time and the number of objective function evaluations corresponding to each
device and each optimization algorithm are shown in Table 28.1. Figure 28.3
shows simulated current response curves corresponding to each solution and
to the initial guess for tuning parameter values, as well as the data, for a single
radiation pulse input to each device. Results for the other radiation pulse input
values exhibit similar properties.
In the case of bft92a, the solutions produced by the two optimization algo-
rithms are practically indistinguishable (they appear on top of each other in
the ï¬gure). However, the APPS solution required over seven times as may
functional evaluations, leading to much additional computational expense and
elapsed time. The gain of the hybrid algorithm here is in its ability to move the
search pattern quickly into better areas of the input space. We note that even
if we started with the hybrid algorithm without an initial LHS (and starting
0
10
20
30
40
50
â€“4
â€“3
â€“2
â€“1
0
0
10
20
30
40
50
0
50
100
150
200
Time in 10eâ€“6 seconds
Current in 10eâ€“4 amps
Fig. 28.3 Simulated response current curves for the bft92a (left) and bfs17a (right) devices. The solid
line shows the response for parameters found using TGP-APPS, the dashed line for parameters
found through APPS alone, and the dotted line for the initial parameter vector guess. The experi-
mental current response curves for the radiation impulse used in these simulations are shown in
grey.

838
The Oxford Handbook of Applied Bayesian Analysis
from the same initial parameter vector as for APPS), it only takes about two
more hours (a total of 15.8 hours) to obtain an equivalent solution through
TGP-APPS â€“ still a huge gain over APPS alone.
For the bfs17a device, the difference in the resulting response curves is
striking and illustrates the desirable robustness of our hybrid algorithm. The
response curve created using the parameter values obtained by APPS alone
differs signiï¬cantly from the data in overall shape. In contrast, the curve result-
ing from the parameters found by TGP-APPS is a reasonable match to the
experimental data. These results suggest that the APPS algorithm was unable to
overcome a weak local minimum while the inclusion of TGP allowed for a more
comprehensive search of the design space. Note that the time results support
this, as they show that the APPS algorithm converged relatively quickly. The
extra computational cost of TGP-APPS is well justiï¬ed by the improvement
in ï¬t. Of course, it is still clear that the simulator is not completely matching
the data, and at this point we suspect that there is an inherent bias in the
simulator. A complete statistical calibration would thus require the modelling
of a bias term, as in the work of Kennedy and Oâ€™Hagan (2001). However, for our
purposes in this project on optimum control, both our collaborating modellers
and experimentalists were quite happy with the robust solution with respect
to minimization of the provided objective function that TGP-APPS was able to
ï¬nd.
28.4 Further discussion
We have illustrated how the treed Gaussian process (TGP) model can be useful
for spatial data and semiparametric regression in the context of a computer
experiment for designing a circuit device. We have seen how the model can be
used towards sequential design of (computer) experiments (via Bayesian adap-
tive sampling), sequential robust local optimization (with the help of APPS),
validation, calibration, and sensitivity analysis all by simply sampling from the
posterior distribution. In both optimization and experiment design, full poste-
rior sampling combined with recursive iteration allowed us to use univariate
prediction to optimize multivariate criteria.
Some of the models and methods described herein have also been used
to design a computational ï¬‚uid dynamics computer experiment for a rocket
booster at NASA (Gramacy and Lee, 2009), and have been validated as compet-
itive regression and spatial models on numerous synthetic and real data sets
(Gramacy and Lee, 2008a,b; Gramacy, 2007). TGP is indeed a ï¬‚exible model
with many potential applications. However, one limitation is that the current
methodology only supports real-valued inputs and responses. An adaptation

Designing and Analysing a Circuit Device Experiment
839
of these methods to support categorical inputs and outputs promises to be a
fruitful future direction of research.
Allowing categorical inputs will widen the scope of regression and design
applications that can be addressed by the model. While the GP part of the model
can easily handle binary-encoded categorical inputs on its own, it represents
a sort of overkill. For example, a separable correlation function with width
parameter di will tend to inï¬nity if the output does not vary with binary input xi,
and will tend to zero if it does. Clearly, this functionality is more parsimoniously
served by partitioning, e.g. using a tree. However, in a TGP implementation the
tree will never partition on the binary inputs because doing so would cause
the resulting design matrices at the leaves to be rank deï¬cient. So without
special care, any beneï¬ts of a divide-and-conquer approach (e.g. speed) to a
nonparametric (TGP) regression with categorical inputs are lost. Once a careful
implementation has been realized, one can imagine many further extensions.
For example, including latent variable categorical inputs could enable the model
to be used for clustering.
Extending the methodology to handle categorical responses will allow TGP
to be applied to problems in classiï¬cation. Separately, treed models and GP
models have enjoyed great success in classiï¬cation problems. Adapting treed
models for classiï¬cation (e.g. CART) is straightforward, whereas adapting
GP models is a bit more complicated, requiring the introduction of O(nk) latent
variables where k is the number of classes. A combined modelling approach via
TGP has the potential to be as fruitful for classiï¬cation as it is for regression.
It will be exciting to see how this extension develops, as well as accompany-
ing methods for adaptive sampling, optimization, validation, calibration, and
sensitivity that can be developed alongside.
Appendix
A. Broader context and background
A.1 Treed Gaussian processes
Here we provide more background on treed Gaussian processes, with further
details available in Gramacy and Lee (2008a). First, the structure of the tree.
We partition the input space using a tree, along the lines of models such as
CART Breiman et al. (1984). Such a tree is constructed with a series of binary
recursive partitions. For example, in two dimensions, one might consider an
input space on [0,1] X [0,1]. The ï¬rst split could be at X1 = .4 separating the
space into two rectangles, [0,0.4] X [0,1] and [0.4,1] X [0,1]. The second split
might be at X2 = .3 in the ï¬rst partition, thus creating a third partition. Note that
this second split does not affect the [0.4,1] X [0,1] region. By allowing multiple

840
The Oxford Handbook of Applied Bayesian Analysis
splits on the same variable, any arbitrary axis-aligned partitioning structure can
be achieved. The restriction of axis-alignment allows us to build models in a
computationally efï¬cient manner, without losing too much modeling ï¬‚exibility.
Arbitrary partitions would require signiï¬cantly more computing resources. We
denote the whole tree structure by T and the leaf nodes by Ã âˆˆT , each of
which represents a region of the input space. A prior for the tree is deï¬ned
through a growth process. We start with a null tree (no partitions, all of the
data is together in a single leaf node). Each leaf node Ã splits with probability
a(1 + qÃ)âˆ’b, where qÃ is the depth of Ã âˆˆT and a and b are parameters chosen
to give an appropriate size and spread to the distribution of trees. Further
details are available in Chipman et al. (1998, 2002). Here we use the default
values of a = 0.5 and b = 2 from the R package (Gramacy and Taddy, 2008). The
tree recursively partitions the input space into into R non-overlapping regions
{rÃŒ}R
ÃŒ=1. Each region rÃŒ contains data DÃŒ = {xÃŒ, ZÃŒ}, consisting of nÃŒ observations.
Let m â‰¡mX + 1 be number of covariates in the design (input) matrix X plus
an intercept. For each region rÃŒ, the hierarchical generative GP model is
ZÃŒ|â€šÃŒ, Ã›2
ÃŒ, KÃŒ âˆ¼NnÃŒ

fÃŒâ€šÃŒ, Ã›2
ÃŒ KÃŒ

â€š0 âˆ¼Nm(Ã, B)
â€šÃŒ|Ã›2
ÃŒ, Ã™2
ÃŒ, W, â€š0 âˆ¼Nm

â€š0, Ã›2
ÃŒÃ™2
ÃŒ W

Ã™2
ÃŒ âˆ¼IG(Â·Ã™/2, qÃ™/2),
Ã›2
ÃŒ âˆ¼IG(Â·Ã›/2, qÃ›/2)
Wâˆ’1 âˆ¼W((Ã’V)âˆ’1, Ã’)
with FÃŒ = (1, XÃŒ), and W is an m Ã— m matrix. The N, IG, and W are the
(Multivariate) Normal, Inverse-Gamma, and Wishart distributions, respectively.
Hyperparameters Ã, B, V, Ã’, Â·Ë†Ã›, qÃ›, Â·Ã™, qÃ™ are treated as known, and we use the
default values in the R package. This model (28.4) speciï¬es a multivariate
normal likelihood with linear trend coefï¬cients â€šÃŒ, which are also modeled
hierarchically. Each region is ï¬t independently (conditional on the hierarchical
structure), which gives this approach some similarity to change-point models.
The GP correlation structure within each region is given by KÃŒ(xj, xk) =
K âˆ—
ÃŒ (xj, xk|d) + gÃŒâ€° j,k, where K âˆ—
ÃŒ is the separable power family given in equa-
tion (28.1) and g is the nugget term. Our choice of priors encodes a preference
for a model with a nonstationary global covariance structure, giving roughly
equal mass to small d representing a population of GP parameterizations for
wavy surfaces, and a separate population for those which are quite smooth or
approximately linear:
p(d, g) = p(d) Ã— p(g) = p(g) Ã— 1
2[Ga(d|Â· = 1, â€š = 20) + Ga(d|Â· = 10, â€š = 10)].
(28.6)
We take the prior for g to be exponential with rate ÃŽ.
In some cases, a full GP may not be needed within a partition; instead a
simple linear model may sufï¬ce. Because of the linear mean function in our

Designing and Analysing a Circuit Device Experiment
841
implementation of the GP, the standard linear model can be seen as a limiting
case. The linear model is more parsimonious, as well as much more compu-
tationally efï¬cient. We augment the parameter space with indicator variables
b = {b}mX
i=1 âˆˆ{0, 1}mX. The boolean bi selects either the GP (bi = 1) or its limiting
linear model for the ith dimension. The prior for bi speciï¬es that smoother GPs
(those larger range parameters di) are more likely to jump to the limiting linear
model:
pâ€ž,Ã‹1,Ã‹2(bi = 0|di) = Ã‹1 + (Ã‹2 âˆ’Ã‹1)/(1 + exp{âˆ’â€ž(di âˆ’0.5)})
and we use the R package default values of (â€ž, Ã‹1, Ã‹2) = (10, 0.2, 0.95). More
details are available in Gramacy and Lee (2008b).
A.2 Experimental design
The basic ideas for experimental design in computer experiments follow those
of standard experimental design, i.e. one wants a relatively small set of points
that are expected to provide maximal information about parameters for a
particular choice of model. For a GP, as with most models, one generally
wants to spread out the points, as each observation gives a fair amount of
local information because of the smoothness properties of the GP model.
Approaches include maximin distance, Latin hypercube, D-optimal, maximum
entropy, and orthogonal array designs (McKay et al., 1979; Santner et al.,
2003). In general, no replications are planned when the computer simulator is
deterministic.
With computer experiments, it is natural to move on to sequential collection
of data, i.e. Sequential Design of Experiments (DOE) or Sequential Design and
Analysis of Computer Experiments (SDACE) (Sacks et al., 1989; Currin et al.,
1991; Welch et al., 1992). Depending on whether the goal of the experiment
is inference or prediction, the choice of utility function will lead to different
algorithms for obtaining optimal designs (Shewry and Wynn, 1987; Santner
et al., 2003).
In the machine learning literature, sequential design of experiments is often
referred to as active learning. Two approaches applied to Gaussian processes
are that of Cohn (1996) described in the main text (maximizing the expected
reduction in average squared error) and that of MacKay (1992), which chooses
the new point as that with the largest standard deviation in predicted output.
While the Mackay approach is simpler, it is also more localized, and less useful
in the presence of heteroskedasticity.
B. Computations
We ï¬t these models using the tgp package in R (Gramacy and Taddy, 2008). A
tutorial is provided by Gramacy (2007). The core of the package is based on C++

842
The Oxford Handbook of Applied Bayesian Analysis
code that employs reversible jump Markov chain Monte Carlo to ï¬t both the tree
structure and the GPs in each of the partitions. By averaging across the Markov
chain realizations, estimates of the posterior mean and of predictive intervals
are obtained. This averaging includes the tree structures, and as a result, we
typically obtain smooth posterior mean ï¬ts because of mixing over the location
of partitions.
Conditional on a tree structure, most parameters can be updated via Gibbs
sampling. The linear regression parameters â€šÃŒ and their prior mean â€š0 all have
multivariate normal full conditionals. The data variance parameter Ã›2 and the
linear variance parameter Ã™2 are both conditionally inverse-Gamma, and the
linear model covariance matrix W is conditionally inverse-Wishart. Correlation
parameters d and g require Metropolis â€“ Hastings updates. The tree structure
itself is updated with reversible jump steps: grow, prune, change, swap, and
rotate. The ï¬rst two require care in accounting for the change of dimension,
while the latter three are straightforward Metropolis â€“ Hastings steps. More
details on estimation and prediction are available in Gramacy and Lee (2008a).
It can be helpful to standardize the data before running the R code, so that the
default parameter values are reasonable.
Acknowledgements
This work was partially supported by NASA awards 08008-002-011-000 and SC
2003028 NAS2-03144, Sandia National Laboratories grants 496420 and 673400,
and National Science Foundation grants DMS 0233710 and 0504851. Any
opinions, ï¬ndings, and conclusions expressed in this material are those of the
authors and do not necessarily reï¬‚ect the views of the funding organizations.
References
Abrahamsen, P. (1997). A review of Gaussian random ï¬elds and correlation functions.
Technical Report 917, Norwegian Computing Center, Box 114 Blindern, N-0314 Oslo,
Norway.
Booker, A. J., Dennis, J. E. Frank, P. D., Seraï¬ni, D. B., Torczon, V. and Trosset, M. W. (1999).
A rigorous framework for optimization of expensive functions by surrogates. Structural and
Multidisciplinary Optimization, 17, 1â€“13.
Breiman, L., Friedman, J. H., Olshen, R. and Stone, C. (1984). Classiï¬cation and Regression Trees.
Wadsworth, Belmont, CA.
Chaloner, K. and Verdinelli, I. (1995). Bayesian experimental design, a review. Statistical
Science, 10, 273â€“304.
Chipman, H., George, E. and McCulloch, R. (1998). Bayesian CART model search (with
discussion). Journal of the American Statistical Association, 93, 935â€“960.
Chipman, H., George, E. and McCulloch, R. (2002). Bayesian treed models. Machine Learn-
ing, 48, 303â€“324.

Designing and Analysing a Circuit Device Experiment
843
Cohn, D. A. (1996). Neural network exploration using optimal experimental design. In Advances
in Neural Information Processing Systems (NIPS), Volume 6(9), pp. 679â€“686. Morgan Kauf-
mann Publishers. San Mateo, CA.
Cressie, N. A. C. (1993). Statistics for Spatial Data, revised edition. John Wiley, New York.
Currin, C., Mitchell, T., Morris, M. and Ylvisaker, D. (1991). Bayesian prediction of determinis-
tic functions, with applications to the design and analysis of computer experiments. Journal
of the American Statistical Association, 86, 953â€“963.
Fjeldy, T. A., Ytterdal, T. and Shur, M. S. (1997). Introduction to Device Modeling and Circuit
Simulation. Wiley-Interscience, New York..
Gramacy, R. B. (2007). tgp: An R package for Bayesian nonstationary, semiparametric non-
linear regression and design by treed gaussian process models. Journal of Statistical Soft-
ware, 19, 1â€“46.
Gramacy, R. B. and Lee, H. K. H. (2008a). Bayesian treed Gaussian process models with
an application to computer modeling. Journal of the American Statistical Association, 103,
1119â€“1130.
Gramacy, R. B. and Lee, H. K. H. (2008b). Gaussian processes and limiting linear models.
Computational Statistics and Data Analysis, 53, 123â€“136.
Gramacy, R. B. and Lee, H. K. H. (2009). Adaptive design and analysis of supercomputer
experiments. Technometrics, 51, 130â€“145.
Gramacy, R. B. and Taddy, M. A. (2008). tgp: Bayesian treed Gaussian process models. R
package version 2.1-2. http://www.ams.ucsc.edu/âˆ¼rbgramacy/tgp.html.
Gray, G. A. and Kolda, T. G. (2006). Algorithm 856: APPSPACK 4.0: Asynchronous parallel pat-
tern search for derivative-free optimization. ACM Transactions on Mathematical Software, 32,
485â€“507.
Gray, G. A., Martinez-Canales, M., Lam, C., Owens, B. E., Hembree, C., Beutler, D. and
Coverdale, C. (2007). Designing dedicated experiments to support validation and calibration
activities for the qualiï¬cation of weapons electronics. In Proceedings of the 14th NECDC.
Also available as Sandia National Labs Technical Report SAND2007-0553C.
Green, P. (1995). Reversible jump Markov chain monte carlo computation and Bayesian model
determination. Biometrika, 82, 711â€“732.
Homma, T. and Saltelli, A. (1996). Importance measures in global sensitivity analysis of
nonlinear models. Reliability Engineering and System Safety, 52, 1â€“17.
Jones, D., Schonlau, M. and Welch, W. J. (1998). Efï¬cient global optimization of expensive black
box functions. Journal of Global Optimization, 13, 455â€“492.
Keiter, E. R. (2004). Xyce parallel elctronic simulator design: mathematical formulation. Tech-
nical Report SAND2004-2283, Sandia National Labs, Albuquerque, NM.
Kennedy, M. and Oâ€™Hagan, A. (2001). Bayesian calibration of computer models (with discus-
sion). Journal of the Royal Statistical Society, Series B, 63, 425â€“464.
Kolda, T. G. (2005). Revisiting asynchronous parallel pattern search for nonlinear optimization.
SIAM Journal of Optimization, 16, 563â€“586.
Lee, H. K. H., Taddy, M. and Gray, G. A. (2009). Selection of a representative sample. Journal of
Classiï¬cation. To appear.
MacKay, D. J. C. (1992). Information-based objective functions for active data selection. Neural
Computation, 4, 589â€“603.
McKay, M. D., Conover, W. J. and Beckman, R. J. (1979). A comparison of three methods
for selecting values of input variables in the analysis of output from a computer code.
Technometrics, 21, 239â€“245.
Morris, R. D., Kottas, A., Taddy, M., Furfaro, R. and Ganapol, B. (2008). A sta-
tistical framework for the sensitivity analysis of radiative transfer models used in

844
The Oxford Handbook of Applied Bayesian Analysis
remote sensed data product generation. IEEE Transactions on Geoscience and Remote
Sensing, 12, 4062â€“4074.
Oakley, J. and Oâ€™Hagan, A. (2004). Probabilistic sensitivity analysis of complex models: a
Bayesian approach. Journal of the Royal Statistical Society, Series B, 66, 751â€“769.
Oberkampf, W. L., Trucano, T. G. and Hirsch, C. (2003). Veriï¬cation, validation, and predictive
capability. Technical Report SAND2003-3769, Sandia National Labs, Albuquerque, NM.
Sacks, J., Welch, W. J., Mitchell, T. J. and Wynn, H. P. (1989). Design and analysis of computer
experiments. Statistical Science, 4, 409â€“435.
Saltelli, A. (2002). Making best use of model evaluations to compute sensitivity indices. Com-
puter Physics Communications, 145, 280â€“297.
Saltelli, A., Chan, K. and Scott, E. (eds.) (2000). Sensitivity Analysis. John Wiley, New York.
Santner, T. J., Williams, B. J. and Notz, W. I. (2003). The Design and Analysis of Computer
Experiments. Springer-Verlag, New York.
Schonlau, M., Jones, D. and Welch, W. (1998). Global versus local search in constrained opti-
mization of computer models. In New Developments and Applications in Experimental Design,
(ed. N. Flournoy, W. F. Rosenberger and W. K. Wong) IMS Lecture Notes â€“ Monograph
Series, Vol. 34, pp. 11â€“25. Institute of Mathematical Statistics, USA.
Sedra, A. S. and Smith, K. C. (1997). Microelectronic Circuits, (4th edn). Oxford University Press,
Oxford.
Shewry, M. and Wynn, H. (1987). Maximum entropy sampling. Journal of Applied Statistics, 14,
165â€“170.
Sobol, I. M. (1993). Sensitivity analysis for nonlinear mathematical models. Mathematical
Modeling and Computational Experiment, 1, 407â€“414.
Stein, M. L. (1999). Interpolation of Spatial Data. Springer, New York.
Taddy, M., Lee, H. K. H., Gray, G. A. and Grifï¬n, J. D. (2009). Bayesian guided pattern search
for robust local optimization. Technometrics, 51, 389â€“401.
Welch, W. J., Buck, R. J., Sacks, J., Wynn, H. P., Mitchell, T. and Morris, M. D. (1992).
Screening, predicting, and computer experiments. Technometrics, 34, 15â€“25.

Â·29Â·
Multistate models for mental fatigue
Raquel Prado
29.1 Goals and challenges in the analysis of brain signals:
The EEG case
Electroencephalographic signals or EEGs are recordings of waves that represent
electrical activity in the brain over time. An EEG measures changes of voltage
levels at the scalp surface in cycles per second (Hz). In clinical settings, the
analysis of EEG signals has been useful in the diagnostic of seizures and other
neurological disorders (e.g. Le Van Quyen et al. 2001), and in assessing the
efï¬cacy of treatments for major depression such as electroconvulsive therapy
or ECT (Krystal, Prado, and West 1999). In non-clinical settings EEGs have
been used in the characterization of cognitive fatigue (Trejo et al. 2007) and
in automatic classiï¬cation of cognitive overload (Trejo, Matthews, and Allison
2007).
Here, we present analyses of EEG data recorded on a subject who performed
continuous arithmetic operations for a period of three hours. The most relevant
aspects of the experimental setting that led to these electroencephalogram
recordings are described below. Other analyses of these data, as well as analyses
of EEG data from other subjects who participated in the same experiment,
appear in Trejo et al. (2006) and Prado (2009). The main objectives of EEG
monitoring in this area include detection, monitoring and prediction of mental
fatigue in awake subjects, which is deï¬ned as the willingness of alert, moti-
vated subjects to continue performing mental work (Trejo et al. 2007). More
speciï¬c goals and some of the challenges of dealing with these signals are also
discussed.
EEG monitoring of brain activity usually involves the recording of EEG traces
at multiple channels located over a subjectâ€™s scalp. For instance, in the analysis
of EEG signals measured after ECT was administered to a patient, Prado, West,
and Krystal (2001) studied traces from 19 channels recorded at a sampling rate
of 256 Hz. The monitoring time depended on the subject and typically varied
from 1 to 3 minutes. For the patient considered in Prado, West, and Krystal
(2001), the recording time was a bit longer than one and a half minutes and
so, the full data set consisted of 19 time series with about 26,000 observations
per series. In the cognitive fatigue EEG application presented below, we have

846
The Oxford Handbook of Applied Bayesian Analysis
traces recorded at 30 channels at a sampling rate of 128 Hz. The recording time
in this study was quite long, ranging from 1 to 3 hours depending on whether
or not a given subject completed the experiment. For subjects who performed
continuous arithmetic operations for 180 minutes, the EEG traces consist of
1,382,400 observations per channel. Consequently, one of the difï¬culties faced
in an analysis of this type of signals is the size of the data sets.
The EEG can be broken up into four main frequency bands referred to as
the delta (0â€“4 Hz), theta (4â€“8 Hz), alpha (8â€“13 Hz) and Beta (above 13 Hz)
bands. Various types of activity/behaviour may induce the appearance of brain
waves in one or more of these frequency bands. For example, the normal resting
EEG usually consists of alpha and beta rhythms (Dyro 1989), while anesthesia
effects in electrically induced seizures may be characterized by a mixture of
slow and fast frequency activity (Weiner, Coffey, and Krystal 1991). In the area
of monitoring and detection of mental fatigue, prior EEG studies have sug-
gested an association of mental fatigue with an increase in the theta band power
at channels located in mid-line frontal areas, and with an increase in the alpha
band power at one or more parietal locations (Trejo et al. 2006). The fact that
different mental states can lead to changes in the spectral characteristics of
the EEGs over time results in signals that are typically non-stationary when
relatively long recording time periods are considered â€“ and often also during
very short intervals due to the nature of the process being measured. From the
modelling viewpoint this represents a challenge, as many of the time domain
and frequency domain methods used in time series analysis are based on the
assumption of stationarity. Extending such methods to deal with non-stationary
time series can be a difï¬cult task analytically and computationally, as it is often
hard to develop efï¬cient algorithms for inference and prediction given and
that the models considered may be non-linear and/or non-Gaussian, and the
data sets are large. In addition, in many EEG applications, such as the one
considered here, it is necessary to perform on-line inference and prediction,
which further complicates the analysis.
We focus on the study of EEG signals for characterization of cognitive fatigue,
however, we emphasize that many of the models and methods discussed here
can also be applied to other kinds of EEG signals and, in general, to time series
that can be described by means of multiple autoregressive processes such as
some biomedical signals, speech signals and seismic recordings. In Section
29.1.1, we describe the data and brieï¬‚y explain the experimental setting that led
to such data. In Section 29.2.1 we discuss analyses, based on autoregressive
models and related time series decompositions, of EEG signals recorded in
one of the subjects who participated in the experiment. The results obtained
with these analyses motivate the general class of mixtures of autoregressive
models, or multi-AR(p) processes, proposed in Section 29.2.2. Due to the com-
putational complexity of achieving on-line inference within this general model

Multistate Models for Mental Fatigue
847
class, we consider approximate inference in low order multi-AR processes
(with p = 1 or p = 2). In spite of their limitations, these low order models
are useful for on-line characterization of cognitive fatigue as shown in Section
29.2.4. Finally, Section 29.3 discusses current and future trends in the develop-
ment of methods for on-line analysis and classiï¬cation of multichannel EEG
signals.
29.1.1 Data description
The EEG data analysed here were recorded at the NASA Ames Research Center
in Moffett Field, California by L. Trejo and collaborators. We present a study of
EEG traces recorded in one of 16 subjects who participated in the experiment.
We refer to this subject as skh. A detailed description of the experiment and
previous data analyses can be found in Trejo et al. (2006, 2007). We now sum-
marize the key aspects of the experimental setting and describe the format of
the data.
The subjects who participated in this study were asked to solve simple
arithmetic operations continuously for up to 3 hours. More speciï¬cally, the
participants sat in front of a computer with their right hand resting on a key pad,
and were asked to solve the summation problems that appeared on the screen
as quickly as possible without sacriï¬cing accuracy. The summations consisted
on four randomly generated single digits, three operators and a target sum,
e.g. 5 + 2 âˆ’1 + 1 = 8. The participants had to solve each problem and decide
whether the summation on the left was less than, equal to, or greater than
the target sum on the right by pressing the appropriate button on the keypad
(<,=,>). Once an answer was received, the monitor went blank for one second
and after this a new summation appeared on the screen. Each participant
performed the task until 180 minutes had elapsed or they quit from exhaustion.
Electroencephalograms were recorded from each of the participants at 30
channels locations using 32 Ag/AgCl electrodes (two of the electrodes were
used to record vertical and horizontal electrooculograms). A schematic rep-
resentation with the approximate location of the channels over the scalp is
shown in Figure 29.1. The EEGs were ampliï¬ed, digitized and submitted to
algorithms for elimination of eye-artifacts, as well as visually inspected. Blocks
of data containing artifacts were removed as detailed in Trejo et al. (2006). The
EEGs were also epoched around the times of the stimuli, which were the times
at which the summations appeared on the computer screen. In particular, the
data of subject skh consist of 864 consecutive epochs per channel. Each of these
epochs corresponds to the EEG recording that goes from 5 seconds prior to
a given stimulus, to 8 seconds after such stimulus. As mentioned before, the
sampling rate was 128 Hz, and so, each epoch has a total number of 1,664
observations.

848
The Oxford Handbook of Applied Bayesian Analysis
FP1
FP2
F3
F4
C3
C4
O1
O2
F7
F8
T7
T8
P7
P8
CZ
FZ
FCZ
CP3
FC3
FC4
TP7
TP8
OZ
FT7
FT8
P3
P4
PZ
CPZ
CP4
Fig. 29.1 Approximate location of the 30 EEG channels over the scalp. The channels in bold are those
for which it is possible to discriminate epochs recorded at the beginning of the experiment â€“ when
the individual is alert â€“ from those recorded towards the end â€“ when the individual is fatigued â€“
based on a multi-AR(1) analysis.
The ultimate goal of this study is the development of automatic methods
for detection and prediction of cognitive fatigue. In this chapter we focus
on models for characterizing fatigue in terms of the changes in the spectral
content of some of the latent processes underlying the EEG signals over time.
Modelling these signals is a balancing act, as we need to use models that are
sophisticated enough to capture the possible â€“ and often very subtle â€“ changes
in the signals over time, and yet such models must be simple enough so that
on-line posterior estimation and prediction are feasible. It is also known from
preliminary analyses that the changes in the EEG spectral characteristics that
may be associated to cognitive fatigue vary across subjects and that, within
a given subject, such effects may also vary across channels. Therefore, the
ideal models would be those that allow us to incorporate relevant subject and
channel-speciï¬c prior information about the mental states â€“ based for instance
on previous experiments involving the same subject or other subjects â€“ and can
also adapt over time.
29.2 Modelling and data analysis
29.2.1 Discovering EEG features associated with mental fatigue via AR models
We begin by showing how autoregressions (AR models) and related time series
decompositions can be used to discover spectral features in the EEG signals that
may be associated with cognitive fatigue. This approach was followed in Trejo
et al. (2007) and Prado (2009) to estimate EEG frequencies in the alpha range, as

Multistate Models for Mental Fatigue
849
well as their associated peaks, in the power spectra for epochs recorded during
the ï¬rst 15 min. and the last 15 min. of the experiment.
Let yt,q, j be the t-th observation of epoch q for channel j, with t = 1 : T,
q = 1 : Q, and j = 1 : J. For subject skh we have that T = 1, 664, Q = 864 and
J = 30. Following Trejo et al. (2006), the AR-based approach presented in Trejo
et al. (2007) and Prado (2009) divides the EEG recordings into intervals, each
interval corresponding to 15 minutes of recording. Data from the ï¬rst and the
last intervals are then used to determine if there are changes in the spectral
characteristics of the EEG signals that allow us to discriminate epochs recorded
when the individual was supposed to be alert â€“ e.g., during the ï¬rst 15 min.
of the experiment â€“ from those recorded when the individual was supposed to
be fatigued â€“ e.g., during the last 15 min. of the experiment. This was done
by ï¬tting AR models to the epochs from the ï¬rst and last intervals, and then
looking at their estimated AR-based time series decompositions as detailed
below.
For subject skh we have 70 epochs in the ï¬rst interval and 92 epochs in the
last interval. An AR(p) model is ï¬tted to each of these epochs and each of the
30 channels, i.e.
yt,q, j =
p

i=1
Ã‹i,q, j ytâˆ’i,q, j + Ã‚t,q, j,
Ã‚t,q, j, âˆ¼N(0, vq, j).
(29.1)
Bayesian inference is achieved assuming a normal prior on the AR coefï¬cients
and a Gamma prior on Ë†q, j = 1/vq, j, as detailed in Appendix A. For each
channel, inference can be summarized in terms of the posterior means of
the AR coefï¬cients for all the epochs in the ï¬rst and last intervals, this is,
Ë†Ã‹q, j = (Ë†Ã‹1,q, j, . . . , Ë†Ã‹p,q, j)â€² for q = 1 : 70 and q = 773 : 864. Using these posterior
estimates, AR-based decompositions of each time series (or EEG epoch) can be
computed following the results of West (1997) (see Appendix A for details), and
so we can write
yt,q, j =
nc
q, j

i=1
zi,t,q, j +
nr
q, j

i=1
xi,t,q, j,
(29.2)
where the zi,t,q, js are real process associated with the nc
q, j pairs of complex
reciprocal roots of the AR characteristic polynomial Ë†q, j(u) = 1 âˆ’Ë†Ã‹1,q, ju âˆ’
Â· Â· Â· âˆ’Ë†Ã‹p,q, jup, where u is a complex number, and the xi,t,q, js are real processes
associated with the nr
q, j real reciprocal roots of the same AR characteristic
polynomial. More speciï¬cally, if the pairs (rl,q, j, ÃŽl,q, j) denote the moduli and
periods of the complex reciprocal roots of Ë†q, j(u), for l = 1 : nc
q, j, and rl,q, j
for l =

2nc
q, j + 1

: p are the moduli of the real reciprocal roots, then, each
zi,t,q, j is a quasi-periodic AR(2, 1) process with modulus ri,q, j, and period

850
The Oxford Handbook of Applied Bayesian Analysis
ÃŽi,q, j, while each xi,t,q, j is an AR(1) process with modulus r2nc
q, j +i,q, j. The
numbers of real and complex pairs of reciprocal roots, nr
q, j and nc
q, j with
p = nr
q, j + 2nc
q, j, do vary slightly across epochs and channels for a given p.
However, for the AR(10)-based analyses discussed here we found that most
epochs in subject skh displayed exactly four pairs of complex reciprocal roots
and two distinct real reciprocal roots. The moduli of these roots were always
below one, indicating stationarity within each epoch, however, some of the roots
were rather persistent: typically one of the real roots and at least one pair of
complex roots had estimated moduli higher than 0.8, and very often close to
one.
In Prado (2009) models with the same order p were considered for all the
epochs and all the channels. The model order was chosen as follows. For each
channel, AR(p) models with 8 â‰¤p â‰¤20 were ï¬tted to the epochs in the ï¬rst
and last intervals. Then, the model order with the best average â€˜leave-one-
epoch-outâ€™ predictive performance, averaging across channels, was chosen as
the optimal order. The predictive performance was measured as the percentage
of the epochs that were correctly classiï¬ed as epochs from the ï¬rst or the last
intervals. For subject skh such model order was found to be p = 10. The details
of this approach will not be discussed here. Other criteria that have been used
for choosing optimal model orders in EEG analyses include BIC and AIC. When
these criteria were used, model orders p = 9, 10 and p = 11 were found to be
optimal for the various epochs and channels.
More general model classes can also be considered to describe the epochs,
such as time-varying AR models (TVARs). TVARs were successfully used in
the analyses of EEG traces presented in West, Prado, and Krystal (1999). TVAR
models have the form given in (29.1) but with AR coefï¬cients changing over
time. West, Prado, and Krystal (1999) considered models in which the AR
coefï¬cients vary according to a random walk. In such models, the smoothness
of the changes in time is controlled by a parameter â€°, with â€° âˆˆ(0, 1], referred
to as a discount factor (West and Harrison, 1997). Low values of â€° are related to
rapid changes in the AR parameters over time, while high values are consistent
with slow changes. The case of â€° = 1 corresponds to the standard AR model,
since no changes of the AR coefï¬cients over time are allowed when â€° = 1.
Optimal values of p and â€° can be chosen by maximum likelihood as explained
in West, Prado, and Krystal (1999). We considered the class of TVAR models
in the analysis of the single epoch EEG data of subject skh. We found that, for
almost all the epochs and all the model orders considered, the optimal value of â€°
was â€° = 1, indicating that there is no advantage in using TVAR models, instead
of simpler AR models, to describe individual epochs.
The AR(10)-based analyses presented in Prado (2009) show that some of the
channels in subject skh display signiï¬cant differences in the alpha frequency
band between the epochs recorded in the ï¬rst 15 min. of the experiment,

Multistate Models for Mental Fatigue
851
P3
Mod interval 1
Density
Mod interval 12
Density
0
20
40
0
20 40 60
0.93
0.94
0.95
0.96
0.97
0.98
0.99
0.93
0.94
0.95
0.96
0.97
0.98
0.99
0.80
0.85
0.90
0.95
10.0
10.5
11.0
11.5
12.0
12.5
Moduli
Frequencies
T8
Fig. 29.2 Left panel: histograms of the real reciprocal roots with the highest estimated modulus for
epochs recorded in the ï¬rst 15 min. of the experiment (top graph) and those for epochs recorded in
the last 15 min (bottom graph). Right panel: Estimated frequencies and corresponding moduli in the
alpha band in channel T8 for epochs in the ï¬rst interval (light points) and epochs in the last intervals
(dark points).
and those recorded in the last 15 min. In particular, it was found that chan-
nels Oz, T8, F T8 and F4 showed lower estimated frequency values and higher
estimated moduli values of the component in the alpha frequency band for
most (but not all) the epochs in the last interval. Other latent processes in
the decomposition also show differences in their spectral characteristics. More
speciï¬cally, for some of the channels, the estimated values of the moduli of the
real characteristic roots with the highest moduli are larger for epochs recorded
in the last 15 min. of the experiment than those for epochs recorded during
the ï¬rst 15 min. Some of these ï¬ndings are illustrated in Figure 29.2. The
left panel depicts histograms of the estimated real characteristic roots with
the highest moduli (as mentioned before, there are typically two real roots for
each epoch) in channel P3. The top histogram shows the estimated moduli
for epochs in the ï¬rst interval, while the histogram at the bottom corresponds
to epochs recorded in the last 15 min. The right panel in Figure 29.2 shows
that most epochs in the last interval in channel T8 have estimated frequency
values â€“ in the alpha band â€“ that are lower than the estimated frequency values
(in the same band) for epochs in the ï¬rst interval, while the corresponding
estimated moduli are higher for epochs in the last intervals than those for early
epochs.
The AR-based results summarized here motivate the class of multi-AR(p)
models with structured priors developed in the next section. The idea behind
these models is to describe the mental process as a mixture of states such that,
at any given time, we can compute the probability that an individual is in a
particular mental state, e.g. alert or fatigue. Each of the states is characterized
in terms of the spectral content of the EEG signals. For instance, fatigue may be

852
The Oxford Handbook of Applied Bayesian Analysis
deï¬ned as a state for which the AR component that lies in the alpha band has
a modulus that is above a certain threshold, say xf , while for the alert state the
modulus of such component is below some other threshold xa, with xa â‰¤x f .
Our approach assumes that the number of states is known a priori. The models
allow us to incorporate prior information in a structured and interpretable
manner, and so, EEG data from previous studies can be used to build the priors
on the model parameters that characterize each mental state.
29.2.2 Multi-AR models: General case and computational difï¬culties
In the equations below we drop the index j, that indicates the speciï¬c EEG
channel, in order to simplify the notation as much as possible.
For each epoch q, consider a collection of K models {Mq(1), . . . , Mq(K )},
representing K different states, with
Mq(k) :
yq = XqÃ‹(k) + Ã‚(k)
q ,
Ã‚(k)
q
âˆ¼N(0, Ë†âˆ’1I),
(29.3)
for k = 1 : K. Here yq = (yp+1,q, . . . , yT,q)â€² is a vector of dimension n = (T âˆ’p),
Ã‹(k) is a p-dimensional parameter vector Ã‹(k) =

Ã‹(k)
1 , . . . , Ã‹(k)
p
â€²
, Ã‚(k)
q
is a vector of
innovations of dimension n, Ë†âˆ’1 is the precision and
Xq =
âŽ›
âŽœâŽœâŽœâŽœâŽ
yp,q
. . .
y1,q
yp+1,q
. . .
y2,q
...
...
...
yTâˆ’1,q
. . .
yTâˆ’p,q
âŽž
âŽŸâŽŸâŽŸâŽŸâŽ 
.
We assume that for each epoch q, the observed series yq follows model Mq(k)
with some probability. Each of these models is an AR(p) with AR coefï¬cients
Ã‹(k). In order to specify the mechanisms by which the models are chosen, we
follow a probabilistic procedure that provides a multiprocess based on discrete
probability mixtures of AR models, as detailed later in Section 29.2.2.2.
A key aspect of this modeling approach is the choice of the prior distribu-
tions. We propose the use of structured priors on the AR coefï¬cients of each
model, following the developments of Huerta and West (1999) for standard
AR models. The main reason for using structured priors in the context of
EEG monitoring and characterization of cognitive fatigue, is that they allow
us to incorporate relevant information on the mental states in terms of inter-
pretable parameters, such as those deï¬ning the spectral features of the EEG
signals.
We describe the priors for the general case in Section 29.2.2.1 and discuss the
computational difï¬culties for posterior inference under these priors in Section
29.2.2.2. In Section 29.2.3 we consider the speciï¬c cases of multiprocesses for
which all the models are either AR(2)s or AR(1)s, and summarize methods for

Multistate Models for Mental Fatigue
853
approximate posterior inference in this framework. Finally, in Section 29.2.4,
we illustrate the use of multi-AR(1) and multi-AR(2) processes in the character-
ization of cognitive fatigue for subject skh.
29.2.2.1 Prior structure
Let Â·(k) =

Â·(k)
1 , . . . , Â·(k)
p

denote the reciprocal roots of the AR(p) characteristic
polynomial given by
(k)(u) = 1 âˆ’Ã‹(k)
1 u âˆ’Ã‹(k)
2 u2 âˆ’. . . âˆ’Ã‹(k)
p up,
(29.4)
where u is a complex number. Therefore, model Mq(k) in (29.3) is stationary
if the roots of (29.4) lie outside the unit circle, or equivalently, if |Â·(k)
l | < 1
for all l = 1 : p. Some of these reciprocal roots will be real and the rest will
appear in complex conjugate pairs. Assume that for all models we have nc
pairs of complex reciprocal roots, each of them deï¬ned in terms of its modulus
and wavelength, i.e., Â·(k)
2 jâˆ’1 = r (k)
j exp

âˆ’2ï£¿i/ÃŽ(k)
j

and Â·(k)
2 j = r (k)
j exp

+2ï£¿i/ÃŽ(k)
j

,
for j = 1 : nc, and nr real reciprocal roots Â·(k)
j
= r (k)
j
for j = (2nc + 1) : p, where
the r (k)
j s denote the moduli and the ÃŽ(k)
j s are the wavelengths or periods. The
assumption of having the same values of nc and nr across q is a reasonable one
for the EEG signals recorded during the cognitive fatigue experiment. Analyses
of EEG traces in some of the subjects who participated in this experiment
indicate that the numbers of complex reciprocal pairs and real reciprocal roots
generally remain unchanged over the course of the experiment, and so, we
assume that the AR models Mq(k) have the same model order p = 2nc + nr
for q = 1 : Q and k = 1 : K .
We propose the following prior structure on the moduli and periods of
the characteristic reciprocal roots of each of the K models. For the complex
reciprocal roots we have that

r (k)
j |D0

âˆ¼f j,k

r (k)
j

,
and

ÃŽ(k)
j |D0

âˆ¼g j,k

ÃŽ(k)
j

,
j = 1 : nc,
with f j,k

r (k)
j

a continuous distribution on lc
j,k(1) â‰¤r (k)
j
â‰¤uc
j,k(1), with 0 <
l c
j,r(1) â‰¤uc
j,k(1) < 1, and g j,k

ÃŽ(k)
j

a continuous distribution on l c
j,k(2) â‰¤ÃŽ(k)
j
â‰¤
uc
j,k(2), with 2 â‰¤l j,k(2) â‰¤u j,k(2) â‰¤ÃŽâˆ—
j,k for some ï¬xed value ÃŽâˆ—
j,k â‰¥2. In addi-
tion, for the real reciprocal roots we have that

r (k)
j |D0

âˆ¼h j,k

r (k)
j

,
j = (2nc + 1) : p,
with h j,k

r (k)
j

a continuous distribution on lr
j,k â‰¤r (k)
j
â‰¤ur
j,k, with âˆ’1 < lr
j,k â‰¤
ur
j,k < 1. D0 denotes all the information available initially.

854
The Oxford Handbook of Applied Bayesian Analysis
Finally, if the precision parameter Ë† is unknown, a Gamma prior distribution
is used, with (Ë†|D0) âˆ¼Ga(n0/2, d0/2). Some examples follow. In these exam-
ples we assume that Ë† is known. We illustrate how truncated normal priors
can be useful in specifying which spectral characteristics of the signals deï¬ne a
particular mental state.
Example 29.1 (Multi-AR(1) process with truncated normal priors) Assume that
we have two possible states and so, K = 2. Suppose that it is known that each
of these states can be represented by an AR(1) such that, when k = 1 we have
a stationary but very persistent process, with

r (1)|D0

âˆ¼TN(0.95, 0.001, R(1)),
and R(1) = (0.8, 1.0). Here, TN(x|Ã, Ã›2, R) denotes a truncated normal dis-
tribution on x with location parameter Ã, scale parameter Ã› and trunca-
tion region R. In addition, when k = 2, the process is much closer to white
noise and

r (2)|D0

âˆ¼TN(0.1, 0.01, R(2)), where R(2) = (0, 0.3). In this multi-
AR(1) setting we have that, before observing any data, E

r (1)|D0

= 0.946,
E

r (2)|D0

= 0.123, V

r (1)|D0

= 0.0008 and V

r (2)|D0

= 0.0052. As data yq
arrives, the distributions of

r (k)|Dq

need to be sequentially updated, as well as
Pr(Mq(k)|Dq).
Example 29.2 (Multi-AR(2) processes) Take K = 2, p = 2 and assume that the
two states can be characterized a priori by

r (1)|D0

âˆ¼TN

0.95, 0.001, R(1)
1

,

ÃŽ(1)|D0

âˆ¼TN

10, 4, R(1)
2

,

r (2)|D0

âˆ¼TN

0.95, 0.001, R(2)
1

,

ÃŽ(2)|D0

âˆ¼TN

17, 4, R(2)
2

,
with R(1)
1 = R(2)
1 = (0.8, 1), R(1)
2 = (8, 12) and R(2)
2 = (14, 20). Here, E

ÃŽ(1)|D0

=
10, E

ÃŽ(2)|D0

= 17, and V

ÃŽ(1)|D0

= 1.1645 and V

ÃŽ(2)|D0

= 2.206. So, both
states have the same prior distribution on the moduli of the complex reciprocal
roots that characterize each process in the mixture. In particular, these roots are
assumed to be rather persistent, given that their moduli are constrained to be
above 0.8 by the prior structure. State k = 1 has a period in the (8, 12) interval,
while the second state has larger period in the (14, 20) interval. Then, via the
prior structure, the two states are differentiated in terms of their frequency
content.
29.2.2.2 Posterior inference
In this section we follow the multiprocess notation of West and Harrison (1997,
Chapter 12). Speciï¬cally, let ï£¿q(k) = Pr

Mq(k)|Dqâˆ’1

, be the prior probability
of model Mq(k) before observing the data of epoch q, where Dqâˆ’1 denotes all
the information available up to q âˆ’1. We assume that D0 = {y1,1, . . . , yp,1} and
Dq = {Dqâˆ’1, yq}. Similarly, let pq(k) = Pr(Mq(k)|Dq) be the posterior probability

Multistate Models for Mental Fatigue
855
of model Mq(k) after epoch q has been observed. In addition, a ï¬rst-order
Markov assumption relates each pqâˆ’1(i), for i = 1 : K, with ï£¿q(k) via ï¬xed and
known transition probabilities denoted by
ï£¿(k|i) = Pr

Mq(k)|Mqâˆ’1(i), Dqâˆ’1

= Pr

Mq(k)|Mqâˆ’1(i), D0

,
for all q and so, ï£¿q(k) = K
i=1 ï£¿(k|i)pqâˆ’1(i). Now, for each q and h such that
0 â‰¤h < q, deï¬ne
pq(kq, kqâˆ’1, . . . , kqâˆ’h) = Pr

Mq(kq), . . . , Mqâˆ’h(kqâˆ’h)|Dq

,
and consequently, the posterior probability of model Mq(kq) after observing the
data of epoch q is pq(kq).
In our EEG application, we want to update p

Ã‹(1), . . . , Ã‹(K ), Ë†|Dq

and pq(kq)
sequentially as the epoched EEG data arrives. We begin with q = 1, and so,
p

Ã‹(1), . . . , Ã‹(K ), Ë†|D1

=
K

k=1
p

Ã‹(1), . . . , Ã‹(K ), Ë†|M1(k1), D1

p1(k1), (29.5)
where
p

Ã‹(1), . . . , Ã‹(K ), Ë†|M1(k1), D1

= p

y1|M1(k1), Ã‹(1), . . . , Ã‹(K ), Ë†

Ã— p

Ã‹(1), . . . , Ã‹(K ), Ë†|D0

p(y1|M1(k1), D0)
,
(29.6)
and
p(y1|M1(k1), D0) =

Â· Â· Â·

1
(2ï£¿|Ë†âˆ’1I|)1/2 exp

âˆ’Ë†

y1 âˆ’X1Ã‹(k1)â€² 
y1 âˆ’X1Ã‹(k1)
2

Ã—p

Ã‹(1), . . . , Ã‹(K ), Ë†|D0

dÃ‹(1) Â· Â· Â· dÃ‹(K )dË†.
(29.7)
In addition, p1(k1) in equation (29.5) is given by
p1(k1) = Pr(M1(k1)|D1) âˆp(y1|M1(k1), D0)Pr(M1(k1)|D0),
(29.8)
with p1(k1) normalized such that K
k1=1 p1(k1) = 1.
The ï¬rst difï¬culty in computing (29.5) arises because the priors proposed
in Section 29.2.2.1 do not lead to closed form expressions of the posterior.
Furthermore, the expression in (29.7) is generally not available analytically.
Huerta and West (1999) develop a Markov chain Monte Carlo approach to
compute the posterior distribution of the parameters of a single AR model when
structured priors on the characteristic reciprocal roots are used. Such approach
is not useful in this setting since it does not allow us to directly compute (29.7)
for each k1 with k1 = 1 : K . More importantly, an MCMC scheme is not compu-
tationally efï¬cient to achieve on-line posterior inference, which is a requirement

856
The Oxford Handbook of Applied Bayesian Analysis
in our EEG application. Even if normal-Gamma prior distributions were set on
(Ã‹(k)|Ë†) and Ë†, computing the posterior distribution at steps q > 1 would be
highly computationally demanding, particularly for K large, as it would involve
a mixture of K q components. This is
p

Ã‹(1), . . . , Ã‹(K ), Ë†|Dq

=
K

kq =1
. . .
K

k1=1
p

Ã‹(1), . . . , Ã‹(K ), Ë†|Mq(kq), . . . , M1(k1), Dq

Ã—pq(kq, . . . , k1).
(29.9)
West and Harrison (1997) obtain approximate posterior inference for
multiprocess models of class II â€“ i.e. mixture models in which each model
component has a dynamic linear model structure with conjugate normal-
Gamma priors. Such inference is obtained by approximating the posterior
distribution at each time period by a ï¬xed number of mixtures, say K h+1 for
some h, with h typically small. We follow a similar approach here for the
speciï¬c cases of multi-AR(1) and multi-AR(2) models with structured priors,
as detailed below. The approximations used in these low order mixture mod-
els are not helpful in obtaining approximate posterior inference for multi-AR
models with p > 2 if structured priors are considered. An alternative solu-
tion for achieving real-time inference in higher order multi-AR(p) processes
with structured priors is to consider sequential Monte Carlo methods. We
revisit this topic in Section 29.3 when discussing current and future research
directions.
29.2.3 The multi-AR(1) and multi-AR(2) cases: Approximate posterior inference
We summarize the methodology for approximate posterior inference in multi-
AR(1) and multi-AR(2) models. We present results for cases in which the prior
distributions on the AR coefï¬cients â€“ implied by impossing the structured
priors on the AR characteristic reciprocal roots proposed in Section 29.2.2.1 â€“
are truncated normal priors, or can be approximated by truncated normal
priors.
29.2.3.1 The multi-AR(1) case
We discuss some features of the methodology to approximate the posteriors
sequentially when Ë† is known. Details on approximations for multi-AR(1) mod-
els in which Ë† is unknown are given in Appendix B.
When p = 1 we have K AR coefï¬cients â€“ one per mixture component â€“ or
equivalently, K real reciprocal roots, each with modulus r (k) and so, Ã‹(k) = r (k),

Multistate Models for Mental Fatigue
857
for k = 1 : K . Assume that fk(r (k)) = TN

r (k)|m0(k), C0(k), R(k)
. Then,
p

Ã‹(1:K )|Dq

=
K

kq =1
K

kqâˆ’1=1
Â· Â· Â·
K

k1=1
p

Ã‹(1:K )|Mq(kq), Mqâˆ’1(kq), . . . , M1(k1), Dq

Ã—pq(kq, kqâˆ’1, . . . , k1)
â‰ˆ
K

kq =1
K

kqâˆ’1=1
p

Ã‹(1:K )|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq

Ã— pq(kq, kqâˆ’1).
(29.10)
In other words, we are using the approximation
p

Ã‹(1:K )|Mq(kq), Mqâˆ’1(kqâˆ’1), . . . , M1(k1), Dq

â‰ˆp

Ã‹(1:K )|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq

,
suggested in West and Harrison (1997) in the context of multiprocess models
of class II. This implies that the number of components in the mixture at any
given time period will be at most K 2. Then, when epoch q arrives, we only
need to consider the models for the previous epoch to perform the analysis,
instead of looking at all the models for all the epochs, starting from the previous
one down to the ï¬rst one. As mentioned before, approximations can use K (h+1)
components in the mixture for h any integer with h â‰¥1. In such cases we would
need to consider the models in epochs (q âˆ’h) : (q âˆ’1) when performing the
analysis for epoch q.
Using
the
approximation
with
h = 1
it
can
be
shown
that,
if

Ã‹(k)|Mqâˆ’1(kqâˆ’1), Dqâˆ’1

is approximated by a single truncated normal,
the distribution of

Ã‹(k)|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq

can be approximated by

Ã‹(k)|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq

â‰ˆTN

Ã‹(k)|m(k)
q (kq, kqâˆ’1), C(k)
q (kq, kqâˆ’1), R(k)
.
(29.11)
The expressions to compute m(k)
q (Â·, Â·), and C(k)
q (Â·, Â·) in (29.11), as well as pq(Â·, Â·)
in (29.10) are given in Appendix B.
Finally, the distribution of

Ã‹(k)|Mq(kq), Dq

, which can be written as a
mixture of K components, can be approximated by a single truncated normal
distribution, i.e.

Ã‹(k)|Mq(kq), Dq

â‰ˆTN

Ã‹(k) m(k)
q (kq), C(k)
q (kq), R(k)
,
(29.12)
by further collapsing the K components in such mixture. Then, we can proceed
with approximate on-line inference by sequentially computing the moments of
the distributions in (29.11) and (29.12), as well as pq(kq, kqâˆ’1) and pq(kq), with

858
The Oxford Handbook of Applied Bayesian Analysis
pq(kq) = K
kqâˆ’1=1 pq(kq, kqâˆ’1). Once again, the expressions to compute m(k)
q (Â·)
and C(k)
q (Â·) are given in Appendix B.
When Ë† is unknown, a Gamma prior distribution (Ë†|D0) âˆ¼Ga(n0/2, d0/2) is
assumed and so, the posterior distributions in (29.11) and (29.12) can be approx-
imated by truncated Student-t distributions instead of truncated normals, while
the distribution of (Ë†|Mq(kq), Dq) is approximated by a Gamma distribution
(see Appendix B).
29.2.3.2 The multi-AR(2) case
For p = 2 we can either have two real reciprocal roots per model, or one pair of
complex reciprocal roots. We describe the later case as it is relevant in the EEG
framework.
Let r (k) and ÃŽ(k) denote the modulus and wavelength of the k-th AR(2)
component in the mixture. In AR(2) models, the relationship between the
AR coefï¬cients and the reciprocal roots of the characteristic polynomial is
given by
Ã‹(k)
2 = âˆ’

r (k)2
,
and
Ã‹(k)
1 = 2r (k) cos

2ï£¿/ÃŽ(k)
.
Then, if r (k) âˆˆ(lk(1), uk(1)) and ÃŽ(k) âˆˆ(lk(2), uk(2)), with lk(Â·) and uk(Â·) such that
0 < lk(1) â‰¤uk(1) < 1 and 2 < lk(2) â‰¤uk(2) < ÃŽâˆ—
k, we have that
Ã‹(k)
2 âˆˆ

âˆ’(uk(1))2, âˆ’(lk(1))2
and
Ã‹(k)
1 âˆˆ

2
;
âˆ’Ã‹(k)
2 cos(2ï£¿/lk(2)), 2
;
âˆ’Ã‹(k)
2 cos(2ï£¿/uk(2))

.
(29.13)
Now, assume that the prior structure on r (k) and ÃŽ(k) is such that

r (k)|D0

âˆ¼
fk(r (k)) and

ÃŽ(k)|D0

âˆ¼gk(ÃŽ(k)), with
fk(r (k)) = TN

r (k)|m0,1(k), C0,1(k), R(k)
1

, and
gk(ÃŽ(k)) = TN

ÃŽ(k)|m0,2(k), C0,2(k), R(k)
2

,
(29.14)
where R(k)
1 = (lk(1), uk(1)) and R(k)
2 = (lk(2), uk(2)). Such prior imposes the
restrictions (29.13) on the prior (and the posterior) of Ã‹(k)
1
and Ã‹(k)
2
and, when
combined with (29.3) for p = 2, it does not lead to closed form posterior distri-
butions on Ã‹(k)
1
and Ã‹(k)
2 . In order to perform approximate on-line inference in
multi-AR(2) models, we propose a prior on Ã‹(k) of the form

Ã‹(k)|D0

âˆ¼TN

Ã‹(k)|m0(k), C0(k), R(k)
,
(29.15)

Multistate Models for Mental Fatigue
859
with
R(k) = (a1(k), b1(k)) Ã— (a2(k), b2(k)),
where
a2(k) = âˆ’(uk(1))2,
b2(k) =
âˆ’(lk(1))2, a1(k) = 2lk(1) cos(2ï£¿/lk(2)) and b1(k) = 2uk(1) cos(2ï£¿/uk(2)). This prior
structure guarantees that r (k) âˆˆ(lk(1), uk(1)), but its support includes regions
of the parameter space that lead to ÃŽ(k) values that are not in (lk(2), uk(2)). This
could be a problem in situations where we want to discriminate between two
mental states characterized only in terms of the frequency of a given latent
component of the EEG signal â€“ instead of the frequency and the modulus of
such component â€“ and the frequency values for those two states are relatively
close. In our EEG application we choose m0(k) and C0(k) in (29.15) that result
in a prior structure as similar as possible to the implied prior of Ã‹(k) that would
be obtained if a structure of the form (29.14) was used on r (k) and ÃŽ(k). We
illustrate this issue in the example below.
The priors (29.15) lead to the following approximate results, when Ë† is known

Ã‹(k)|Mq(kq), Mq(kqâˆ’1), Dq

â‰ˆTN

Ã‹(k) m(k)
q

kq, kqâˆ’1

, C(k)
q (kq, kqâˆ’1), R(k)
,

Ã‹(k)|Mq(kq), Dq

â‰ˆTN

Ã‹(k) m(k)
q (kq), C(k)
q (kq), R(k)
.
(29.16)
Then, as in the multi-AR(1) case, approximate on-line posterior inference
is achieved by computing the moments of the distributions in (29.16),
as well as pq(kq, kqâˆ’1) and pq(kq). In addition, when Ë† is unknown and
modelled with a Gamma prior, the distributions used in the approxima-
tions (29.16) will be truncated Student-t distributions (see Appendix B for
details).
Before proceeding with the EEG analysis, we show how the approximate
inference works in the analysis of data simulated from two AR(2) processes,
each with a pair of complex conjugate characteristic roots with similar moduli
values but different periods.
Example 29.3 (Simulated data) A time series with 9,000 data points was sim-
ulated from two AR(2) processes as follows. The ï¬rst 2,000 observations were
simulated, in batches of 100 observations, from an AR(2) process with modulus
r1 = 0.95 and wavelength ÃŽ1 = 16. The following 3,000 data points were simu-
lated from an AR(2) process with modulus r2 = 0.99 and wavelength ÃŽ2 = 6 (also
in batches of 100 observations). Then, the next 2,000 observations were again
simulated from the AR(2) process with reciprocal roots r1eÂ±2ï£¿i/ÃŽ1 and ï¬nally, the
last 2,000 observations were simulated from the AR(2) model with reciprocal
roots r2eÂ±2ï£¿i/ÃŽ2. The innovations for both types of processes followed indepen-
dent Gaussian distributions centered at zero with variance v = 1/Ë† = 100. In
order to mimic the structure of our EEG data, we assume that an epoch consists
of 100 observations and so, the simulated data has a total number of 90 epochs.

860
The Oxford Handbook of Applied Bayesian Analysis
We ï¬tted multi-AR(2) models with K = 2, and the following prior structure

Ã‹(1)|D0

âˆ¼TN

m0(1), C0(1), R(1)
,
R(1) =

1.4 cos
2ï£¿
4

, 2 cos
2ï£¿
10

Ã— (âˆ’12, âˆ’0.72),

Ã‹(2)|D0

âˆ¼TN

m0(2), C0(2), R(2)
,
R(2) =

1.4 cos
2ï£¿
12

, 2 cos
2ï£¿
20

Ã— (âˆ’12, âˆ’0.72),
(29.17)
(Ë†|D0) âˆ¼Ga(1/2, 10/2).
The values of m0(k) and C0(k) for k = 1, 2 were chosen to approximate a prior
with the following structure on r (k) and ÃŽ(k)
r (1) âˆ¼TN

r (1)|0.8, 1, R(1)
1

, ÃŽ(1) âˆ¼TN

ÃŽ(1)|5, 1, R(1)
2

,
(29.18)
r (2) âˆ¼TN

r (2)|0.8, 1, R(2)
1

, ÃŽ(2) âˆ¼TN

ÃŽ(2)|13, 1, R(2)
2

,
and with R(1)
1 = R(2)
1 = (0.7, 1), R(1)
2 = (4, 10) and R(2)
2 = (12, 20). The graphs (a)
and (c) in Figure 29.3 show 1,000 simulated values from the implied prior
on Ã‹(k) =

Ã‹(k)
1 , Ã‹(k)
2
â€²
if the priors in (29.18) are chosen for r (k) and ÃŽ(k), while
the graphs (b) and (d) correspond to 1,000 values simulated from the prior
in (29.17). For these graphs we used m0(1) = (1.6 cos(2ï£¿/5), âˆ’0.82), m0(2) =
(1.6 cos(2ï£¿/13), âˆ’0.83) and C0(1) and C0(2) such that C0(1)[1, 1] = C0(1)[2, 2] =
1, C0(1)[1, 2] = C0(1)[2, 1] = âˆ’0.4, C0(2)[1, 1] = C0(2)[1, 1] = 1 and C0(2)[1, 2] =
C0(2)[2, 1] = âˆ’0.9. As mentioned before, the priors in (29.17) and (29.18) are
not the same, with (29.17) giving non-zero probability to regions of Ã‹(k) that
result in values of ÃŽ(k) that do not lie in R(k)
2 . This may be a problem in models
for which different states are characterized by the same modulus and distinct
but very close wavelength values. In such cases, as expected, it will be typically
hard to discriminate between states. In the EEG data for subject skh channels
that show discrepancies in the EEG signals recorded at the beginning of the
experiment, and those recorded towards the end of the experiment are typically
characterized by relatively different moduli values and often by different wave-
length values as well.
Finally, to complete the model structure in this analysis we set ï£¿(1|1) =
ï£¿(2|2) = 0.9 and ï£¿0(1) = 0.9. The left panel in Figure 29.4 shows the approximate
values of pq(1) = Pr(Mq(1)|Dq) for q = 1 : 90. We have that Ë†p1(1) â‰ˆ0.2, and
then Ë†pq(1) = 0 for q = 2 : 20 and q = 51 : 70, while Ë†pq(1) = 1 for q = 21 : 50 and
q = 71 : 90. This corresponds precisely to the structure used to simulate the
data. The right panel shows approximate values of E

Ë†âˆ’1|Dq

for q = 1 : 90,
indicating that the approximations also work well in terms of the posterior

Multistate Models for Mental Fatigue
861
Î¸(1)
(a)
Î¸1
Î¸2
0.0
0.5
1.0
1.5
â€“1.0
â€“0.8
â€“0.6
Î¸(1)
(b)
Î¸1
Î¸2
0.0
0.5
1.0
1.5
â€“1.0
â€“0.8
â€“0.6
Î¸(2)
(c)
Î¸1
Î¸2
1.2
1.4
1.6
1.8
â€“1.0
â€“0.8
â€“0.6
Î¸(2)
(d)
Î¸1
Î¸2
1.2
1.4
1.6
1.8
â€“1.0
â€“0.8
â€“0.6
Fig. 29.3 (a) Simulated values from the prior (29.18) for Ã‹(1); (b) simulated values from the prior
(29.17) for Ã‹(1); (c) simulated values from the prior (29.18) for Ã‹(2); (d) simulated values from the prior
(29.17) for Ã‹(1).
posterior inference for Ë†. In addition, we obtain that E(r (1)|D90) â‰ˆ0.9909,
E(r (2)|D90) â‰ˆ0.9557, E(ÃŽ(1)|D90) â‰ˆ5.9919, and E(ÃŽ(2)|D90) â‰ˆ16.1103. Then,
Mq(1) captures the AR(2) structure whose characteristic roots have moduli
r2 = 0.99 and period ÃŽ2 = 6, while Mq(2) captures the AR(2) structure whose
characteristic roots have moduli r1 = 0.95 and period ÃŽ1 = 16.
29.2.4 Analysis of EEG data via multi-AR(1) and multi-AR(2)
We use multi-AR(1) and multi-AR(2) models to study changes in the latent
components of the EEG epochs over time. We begin by computing the decom-
positions in (29.2) for each channel and each epoch and then extract some of
the latent processes as follows. For each channel j, we ï¬t AR(10) models to each
of the 864 epochs and estimate the AR reciprocal roots based on the posterior
means of the AR coefï¬cients. Most epochs display four pairs of complex roots
and two distinct real roots. We order the reciprocal roots by moduli and extract
the latent process with the highest modulus for each epoch and each channel.
Therefore, for each channel we obtain a collection of 864 time series (one per
epoch). For subject skh, each of these latent processes is associated with a real
reciprocal root for all the epochs and all the channels. We denote each time

862
The Oxford Handbook of Applied Bayesian Analysis
(a)
Epochs
Prob of M(1)
0
20
40
60
80
(b)
Epochs
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
1.0
85
90
95
100
E(1/Ï†)
Fig. 29.4 (a) Approximate values of pq(1) = Pr(Mq(1)|Dq), with q = 1 : 90 for the simulated data; (b)
E

Ë†âˆ’1|Dq

based on approximations.
series in this collection by x1:T,q, j. Similarly, we extract the quasi-periodic latent
components that lie in the alpha frequency band for all the epochs and all
channels. Each of those latent processes is a time series, denoted by z1:T,1:Q, j,
with an ARMA(2, 1) structure. Then, for each channel j we proceed to analyse
the collection of time series x1:T,1:Q, j with multi-AR(1) models, and z1:T,1:Q, j
with multi-AR(2) models.
Based on analyses of EEG data for another subject who participated in
the same study (labelled as ess), we use a model with two states to describe
x1:T,1:Q, j. One of the two states is characterized by a very persistent modulus
in (0.975, 1.0), while the other state is characterized by a modulus below 0.975.
Speciï¬cally, for each channel j we model the series x1:T,q, j for q = 1 : 864 as a
mixture of two AR(1) process with the following prior structure

Ã‹(1)|D0

âˆ¼TN

Ã‹(1)|0.94, C0(1), R(1)
,

Ã‹(2)|D0

âˆ¼TN

Ã‹(2)|0.98, C0(2), R(2)
,
(Ë†|D0) âˆ¼Ga(1/2, 10/2),
with R(1) = (0.9, 0.975) and R(2) = (0.975, 1.0). Values of C0(1) = C0(2) = c, with
c = 0.01, 1, 10 were used, leading to similar results in terms of the posterior
inference. In addition, we set ï£¿(1|1) = ï£¿(2|2) = 0.999 and ï£¿0(1) = 0.9.
Figure 29.5 displays the estimated values of pq(1) = Pr(Mq(1)|Dq) (light
dots) and pq(2) = Pr(Mq(2)|Dq) (dark squares) for channels Pz, P4, C P4, P3
and C Pz. For these channels, the process in the mixture with AR coefï¬cient
restricted to (0.9, 0.975) dominates the ï¬rst epochs of the experiment, while
that with AR coefï¬cient above 0.975 dominates the last epochs of the experi-
ment. There is a fair amount of variability across channels. Channels Pz, P4
and C P4 behave similarly, displaying large values of pq(1) for most epochs
before q = 600 (i.e. before approx. 106 min.), and large values of pq(2) for

Multistate Models for Mental Fatigue
863
800
600
400
200
0
0.2
Epoch
800
600
400
200
0
Epoch
800
600
400
200
0
Epoch
800
600
400
200
0
Epoch
800
600
400
200
0
Epoch
PZ
0.2
P4
0.2
CP4
0.2
P3
0.2
CPZ
Fig. 29.5 Estimates of pq(1) (light dots) and pq(2) (dark dots) for the latent processes with the highest
moduli in channels Pz, P4, CP4, P3 and CPz.
most epochs after q = 600. These three channels are located close to each other
in left-parietal and left-middle areas, as shown in Figure 29.1. Channel P3
displays large values of pq(2) for most epochs after about q = 250 (i.e. after Â±45
min. had elapsed). Pictures for the remaining channels (not displayed) showed
pq(1) â‰ˆ1 for almost all the epochs in the experiment, and so, no multistate
evidence for the EEG latent process with the largest modulus was found in such
channels.
The same type of analyses were carried out to describe the quasi-periodic
latent processes z1:T,q, j, for each j and all q with q = 1 : 864. Figure 29.6 shows
the estimates of pq(1) (light dots) and pq(2) (dark squares) for channel T8, based
on a multi-AR(2) model with K = 2 and truncated normal priors on Ã‹(1) and Ã‹(2)
with truncation regions
R(1) = (1.2 cos(2ï£¿/9.5), 1.6 cos(2ï£¿/11.5)), Ã— (âˆ’0.852, âˆ’0.62),
R(2) = (1.6 cos(2ï£¿/11.5), 1.9 cos(2ï£¿/13.5)) Ã— (âˆ’1.02, âˆ’0.852).

864
The Oxford Handbook of Applied Bayesian Analysis
0
200
400
600
800
0.0
0.2
0.4
0.6
0.8
1.0
T8
Epoch
p(k)
Fig. 29.6 Estimates of pq(1) (light dots) and pq(2) (dark squares) for channel T8 based on a multi-
AR(2) analysis.
Different values of m0(1) âˆˆR(1) and m0(2) âˆˆR(2) and C0(k) for k = 1, 2 were
chosen, leading to similar posterior estimates. Given that the sampling rate in
the data is 128 Hz, the prior for k = 1 aims to restrict the frequency approxi-
mately to the 11â€“13.5 Hz range, and with corresponding modulus in the 0.6â€“
0.85 range while for k = 2, the range of the frequency is approximately (9.5, 11)
Hz and its corresponding modulus is in the 0.85â€“1.0 range. We ï¬tted this model
to the collection of series z1:T,q, j for q = 1 : 864 and for each channel. We found
evidence in favor of a process with two states as deï¬ned above in channels T8
and F T8. For channel T8, pq(2) â‰ˆ1 for all the epochs after q â‰ˆ500 (â‰ˆ80 min.)
as seen in Figure 29.6. Channel F T8 has pq(2) â‰ˆ1 for most (but not all) epochs
after q = 650 (not shown). For the remaining channels pq(1) â‰ˆ1 for almost all
the epochs, indicating no evidence in support of more than one state based on
this two-state multi-AR(2) process.
29.3 Further discussion
In this chapter we discuss the challenges of developing models for EEG data
when the main goal is automatic detection and prediction of cognitive fatigue.
We ï¬rst show how AR models and related decompositions can be used as a
descriptive tool for discovering EEG features that may be associated with one or
more mental states. We have used these models in the analyses of EEG epochs
from subjects who participated in the experiment described in Section 29.1.1
(Trejo et al. 2007). For some of these individuals, such as subject skh, the AR-
based inference points towards differences in the spectral characteristics of the
signals recorded during the ï¬rst 15 min. of the experiment, and those recorded

Multistate Models for Mental Fatigue
865
during the last 15 min. Some of these ï¬ndings are consistent with the results
shown in Trejo et al. (2006) obtained using a very different modelling approach.
Given that the individuals were rested prior to the experiment and clearly
fatigued after the experiment ended â€“ as conï¬rmed by measures of subject
performance and pre and post-task moods (Trejo et al. 2006) â€“ it is possible
to hypothesize that the observed differences in the spectral characteristics of
the signals recorded at the beginning of the experiment, and those recorded at
the end, are associated with cognitive fatigue.
If a particular mental state can be characterized in terms of the frequencies
and their corresponding peaks in the power spectra of the EEG signals for
different channels â€“ as seems to be feasible based on the results obtained using
AR models in the cognitive fatigue study â€“ the class of multi-AR(p) processes
provides a modeling framework that allows practicioners to perform on-line
detection of multiple mental states in a probabilistic fashion, as illustrated with
the multi-AR(1) and multi-AR(2) analyses shown in Section 29.2.4. We empha-
size that our intention with the analyses presented here was not to provide a
precise deï¬nition of cognitive fatigue in terms EEG features, but to show that,
if hypotheses about which EEG spectral features deï¬ne a particular cognitive
state are available, multi-AR processes can be used to determine if and when an
individual enters such mental state.
Due to the computational difï¬culties for on-line posterior inference in multi-
AR models, we present an approximate analysis for models with p = 1 and
p = 2. Because the EEG signals are composed by several latent processes, most
of which are quasi-periodic, low order models cannot be directly applied to the
actual data and so, we ï¬rst extracted relevant latent processes from the signals
using the AR decompositions and then analyzed such processes with low order
multi-AR models. In spite of their limitations, these low order multi-AR models
provide useful insights in the description of the EEG data of subject skh.
Approximate posterior inference can be achieved in a computationally efï¬cient
manner, allowing practioners to use these methods descriptively to study EEG
data and hypothesize about which EEG features are associated with a particular
mental state.
Current and future research directions include developing efï¬cient algo-
rithms for on-line posterior estimation in general multi-AR processes with
structured priors for cases with p â‰«2. We are currently exploring sequential
Monte Carlo (SMC) approaches for this purpose. In particular, we are consid-
ering SMC algorithms such as those proposed in Liu and West (2001) and
Carvalho et al. (2008). One of the challenges in connection with the use of
sequential MC approaches in this framework is dealing with relatively large
parameter spaces, specially when K is large. Other areas of future research
include developing models that can simultaneously handle data from multiple
channels. Mixtures of vector autoregressions and factor models are two of

866
The Oxford Handbook of Applied Bayesian Analysis
the model classes that will be considered in the analysis of mulvariate EEG
data.
Appendix
A. Broader context and background
A.1 Model speciï¬cation for TVARs and ARs
A time-varying autoregressive model of order p, or TVAR(p), is a model of the
form
yt =
p

i=1
Ã‹t,i ytâˆ’i + Ã‚t,
(29.19)
where Ã‹t = (Ã‹t,1, . . . , Ã‹t,p)â€² is the vector or AR coefï¬cients at time t and Ã‚t is
a zero-mean innovation, typically assumed Gaussian with variance vt (West,
Prado, and Krystal 1999). Two additional equations are used to describe the
evolution of the model parameters over time. One of such equations describes
the changes in Ã‹t while the other one models the variance vt. Typically, a random
walk is used to model Ã‹t and the multiplicative version of a random walk
equation is used to describe the changes in vt. This is,
Ã‹t = Ã‹tâˆ’1 + Â¯t, Â¯t âˆ¼N(0, Wt),
(29.20)
and
vt = vtâˆ’1(â€š/Ãt), Ãt âˆ¼Be(at, bt),
(29.21)
where the stochastic terms Â¯t and Ãt are independent and mutually indepen-
dent, as well as independent of Ã‚t. The matrix Wt can be deï¬ned using a
discount factor â€° âˆˆ(0, 1], as described in West and Harrison (1997). Low values
of â€° are consistent with rapid changes in Ã‹t over time, while large values describe
smooth changes. A value of â€° = 1 leads to the standard autoregressive model,
AR(p), i.e.
yt =
p

i=1
Ã‹i ytâˆ’i + Ã‚t,
(29.22)
when â€° = 1. Similarly, in equation (29.21), â€š acts as a discount factor taking
values in (0, 1]. â€š = 1 leads to a model for which vt = v for all t. The model
deï¬ned by equations (29.19), (29.20) and (29.21) is completed by specifying a
prior distribution on (Ã‹1, v1|D0), where D0 denotes all the information available
initially. Conjugate normal-inverse-Gamma distributions are generally used,
this is, (Ã‹1|v1, D0) âˆ¼N(Ã‹1|m0, v1C0) and

vâˆ’1
1 |D0

âˆ¼Ga(n0/2, d0/2).

Multistate Models for Mental Fatigue
867
A.2 Time series decompositions
West (1997) and West, Prado, and Krystal (1999) provide the basic methodology
for obtaining the time series decompositions for AR and TVAR models. Here
we summarize such results.
Consider the following stucture
yt = F â€²Ã‹t, Ã‹t = GtÃ‹tâˆ’1 + Ãt,
Ãt âˆ¼N(0, Wt).
Assume that at each time t the eigenvalues of Gt are all distinct, with c pairs
of complex eigenvalues denoted by rt, j exp(Â±2ï£¿i/ÃŽt, j) for j = 1 : c, and r real
eigenvalues rt, j with j = 1 : r. Then, it is possible to show that
yt =
c

j=1
zt, j +
r
j=1
xt, j,
where zt, j is a real process associated to rt, j exp(Â±2ï£¿i/ÃŽt, j) and xt, j is a
real process associated to r j. In the particular case of TVAR(p) models, F =
(1, 0, . . . , 0)â€², Ãt = (Ã‚t, 0, . . . , 0)â€² and Gt is given by
âŽ›
âŽœâŽœâŽœâŽœâŽœâŽœâŽœâŽ
Ã‹t,1
Ã‹t,2
. . .
Ã‹tâˆ’1,p
Ã‹t,p
1
0
Â· Â· Â·
0
0
0
1
Â· Â· Â·
0
0
...
...
Â· Â· Â·
...
0
0
Â· Â· Â·
1
0
âŽž
âŽŸâŽŸâŽŸâŽŸâŽŸâŽŸâŽŸâŽ 
,
and its eigenvalues correspond precisely to the reciprocal roots of the charac-
teristic polynomial Ã‹t(u) = 1 âˆ’Ã‹t,1u âˆ’. . . âˆ’Ã‹t,pup. Each of the zt, j processes
follows approximately a TVARMA(2, 1), with modulus rt, j and wavelength ÃŽt, j.
Similarly, each of the xt, j processes follows approximately a TVAR(1), with
modulus rt, j. In the case of AR(p) models Ã‹t = Ã‹ for all t and so, zt, j and xt, j
follow ARMA(2, 1) and AR(1) processes, respectively.
B. Computation
B.1 Bayesian inference in AR and TVAR models
Inference in the TVAR models described above can be easily achieved using
standard Dynamic Linear Model (DLM) theory, as detailed in West and Harri-
son (1997) for general DLMs, and in West, Prado, and Krystal (1999) for the
particular case of TVAR models with the structure described above.
The software tvar, available on-line at www.stat.duke.edu, performs
Bayesian inference and computes the time series decompositions for TVAR
and AR models with conjugate normal-inverse-Gamma priors on Ã‹t and vt.
This software allows the user to choose optimal values for the model order

868
The Oxford Handbook of Applied Bayesian Analysis
p and the discount factors â€° and â€š. In particular, when â€° = 1 and â€š = 1 the
tvar software provides posterior inference for standard AR models based on
conjugate normal-Gamma priors on Ã‹|Ë† and Ë† = 1/v.
A related software for Bayesian inference in AR models is arcomp, also
available on-line at www.stat.duke.edu. This software implements the AR
models with structured priors developed in Huerta and West (1999). There are
two interesting features in the models implemented in arcomp. One of them
is that it allows for unit roots. The other one is that it handles model order
uncertainty.
B.2 Approximate posterior inference in multi-AR models
We refer the reader to West and Harrison (1997) for general theory of multi-
process models. Such models are mixtures in which each model component
has a dynamic linear model structure. West and Harrison (1997) develop
methodology for approximate on-line posterior updating when normal-Gamma
priors are used on the parameters of each DLM considered in the mixture. We
summarize the steps to obtain approximate on-line posterior inference in multi-
AR processes below, assuming that a truncated normal prior is set on the AR
coefï¬cients of each model component, and a Gamma prior is set on Ë†. In our
multi-AR setting the AR coefï¬cients do not change over time, and our priors
are not conjugate within each mixture component. Therefore, the steps needed
to sequentially update the approximate posterior distributions are not exactly
the same as those used in the DLM multi-process framework, however they are
very similar, and so, the reader would beneï¬t from reading Chapter 12 of West
and Harrison (1997).
Case with Ë† known
(a) Updating the posteriors at q = 1. After observing the data of the ï¬rst epoch,
we have that
p

Ã‹(1), . . . , Ã‹(K )|D1

=
K

k1=1
p

Ã‹(1), . . . , Ã‹(K )|M1(k1), D1

p1(k1),
where
p

Ã‹(1), . . . , Ã‹(K )|M1(k1), D1

= p

y1|M1(k1), Ã‹(1), . . . , Ã‹(K )
p

Ã‹(1), . . . , Ã‹(K )|D0

p(y1|M1(k1), D0)
,
and with
p(y1|M1(k1), D0) =

p

y1|M1(k1), Ã‹(1), . . . , Ã‹(K )
p

Ã‹(1), . . . , Ã‹(K )|D0

dÃ‹(1) . . . dÃ‹(K ).

Multistate Models for Mental Fatigue
869
In addition,
p1(k1) âˆp(y1|M1(k1), D0)ï£¿1(k1).
Now, if

Ã‹(k)|D0

âˆ¼TN

m0(k), C0(k), R(k)
, we have that

Ã‹(k)|M1(k1), D1

= TN

Ã‹(k) m(k)
1 (k1), C(k)
1 (k1), R(k)
,
and
p1(k1)âˆ
Ã1(k1)ï£¿1(k1)
C(k1)
1
(k1)

1/2
Ãâˆ—
0(k1)
C0(k1)
1/2
exp

âˆ’1
2

Ë†yâ€²
1y1 + [m0(k1)]â€²[C0(k1)]âˆ’1[m0(k1)]

Ã— exp
1
2

m(k1)
1 (k1)
â€² 
C(k1)
1
(k1)
âˆ’1 
m(k1)
1 (k1)

,
with
C(k)
1 (k1) =
C0(k)
if
k =/k1,

Câˆ’1
0 (k) + Ë†Xâ€²
1X1
âˆ’1
if
k = k1,
and
m(k)
1 (k1) =
m0(k)
if
k =/k1,
C(k1)
1
(k1)

C0(k)âˆ’1m0(k) + Ë†Xâ€²
1y1

if
k = k1.
In addition,
Ãâˆ—
0(k1) =

R(k1) N

Ã‹(k1)|m0(k1), C0(k1)

dÃ‹(k1),
Ã1(k1) =

R(k1) N

Ã‹(k1) m(k1)
1 (k1), C(k1)
1
(k1)

dÃ‹(k1) .
(b) Updating the posteriors when q > 1. Assume that the distribution of

Ã‹(k)|Mqâˆ’1(kqâˆ’1), Dqâˆ’1

can be approximated by a truncated normal distribu-
tion and that approximations for pqâˆ’1(kqâˆ’1) with kqâˆ’1 = 1 : K are also available.
Then, after observing the data of epoch q for q > 1, we have

Ã‹(k)|Mq(kq), Mqâˆ’1(kq), Dq

â‰ˆTN

Ã‹(k) m(k)
q (kq, kqâˆ’1), C(k)
q (kq, kqâˆ’1)

,

870
The Oxford Handbook of Applied Bayesian Analysis
and
pq(kq, kqâˆ’1) âˆ
ï£¿(kq|kqâˆ’1)pqâˆ’1(kqâˆ’1)Ãq(kq, kqâˆ’1)
C
(kq )
q
(kq, kqâˆ’1)

1/2
Ãâˆ—
qâˆ’1(kq, kqâˆ’1)
C
(kq)
qâˆ’1(kqâˆ’1)

1/2
Ã—exp

âˆ’1
2

Ë†yâ€²
q yq +

m
(kq )
qâˆ’1(kqâˆ’1)
â€²
C
(kq)
qâˆ’1(kqâˆ’1)
âˆ’1
m
(kq )
qâˆ’1(kqâˆ’1)

Ã— exp
1
2

m
(kq )
q
(kq, kqâˆ’1)
â€² 
C
(kq )
q
(kq, kqâˆ’1)
âˆ’1 
m
(kq )
q
(kq, kqâˆ’1)

,
with pq(kq, kqâˆ’1) normalized such that K
kq =1
K
kqâˆ’1=1 pq(kq, kqâˆ’1) = 1 and with
C(k)
q (kq, kqâˆ’1) =
âŽ§
âŽªâŽ¨
âŽªâŽ©
C(k)
qâˆ’1(kqâˆ’1)
if
k =/kq,

C(k)
qâˆ’1(kqâˆ’1)
âˆ’1
+ Ë†Xâ€²
q Xq
âˆ’1
if
k = kq,
m
(kq )
q
(kq, kqâˆ’1) =
âŽ§
âŽªâŽ¨
âŽªâŽ©
m(k)
qâˆ’1(kqâˆ’1)
if k =/kq,
C(k)
q (kq, kqâˆ’1)

C(k)
qâˆ’1(kqâˆ’1)
âˆ’1
m(k)
qâˆ’1(kqâˆ’1) + Ë†Xâ€²
q yq

if k = kq.
In addition,
Ãâˆ—
qâˆ’1(kq, kqâˆ’1) =

R(kq ) N

Ã‹(kq ) m
(kq )
qâˆ’1(kqâˆ’1), C
(kq )
qâˆ’1(kqâˆ’1)

dÃ‹(kq )
Ãq(kq, kqâˆ’1) =

R(kq ) N

Ã‹(kq ) m
(kq )
q
(kq, kqâˆ’1), C
(kq )
q
(kq, kqâˆ’1)

dÃ‹(kq ) .
Now,
taking
pq(kq) = K
kqâˆ’1=1 pq(kq, kqâˆ’1),
we
have
that,
if
pq(kq) =/ 0,
p

Ã‹(k)|Mq(kq), Dq

can be written as follows
p

Ã‹(k)|Mq(kq), Dq

=
K

kqâˆ’1=1
p

Ã‹(k)|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq

pq(kq, kqâˆ’1)/pq(kq),
and so, we can approximate this mixture as

Ã‹(k)|Mq(kq), Dq

â‰ˆTN

Ã‹(k) m(k)
q (kq), C(k)
q (kq), R(k)
,
with
m(k)
q (kq) =
K

kqâˆ’1=1
m(k)
q (kq, kqâˆ’1)pq(kq, kqâˆ’1)/pq(kq),

Multistate Models for Mental Fatigue
871
and
C(k)
q (kq) =
K

kqâˆ’1=1
 
C(k)
q (kq, kqâˆ’1) +

m(k)
q (kq) âˆ’m(k)
q (kq, kqâˆ’1)
â€²

m(k)
q (kq) âˆ’m(k)
q (kq, kqâˆ’1)

Ã— pq(kq, kqâˆ’1)
pq(kq)

.
Case with Ë† unknown
(a) Updating the posteriors at q = 1. When Ë† is unknown, having a prior of the
form
p

Ã‹(1), . . . , Ã‹(k), Ë†|D0

=
K

k=1
TN

Ã‹(k)|m0(k), Ë†âˆ’1Câˆ—
0(k), R(k)
Ã— Ga(Ë†|n0/2, d0/2),
leads to the following results

Ã‹(k)|M1(k1), D1, Ë†

= TN

Ã‹(k) m(k)
1 (k1), Ë†âˆ’1Câˆ—,(k)
1
(k1), R(k)
,
(Ë†|M1(k1), D1) â‰ˆGa(n1/2, d1(k1)/2),
with
Câˆ—,(k)
1
(k1) =
Câˆ—
0(k)
if
k =/k1,

Câˆ—
0(k)âˆ’1 + Xâ€²
1X1
âˆ’1
if
k = k1,
m(k)
1 (k1) =
m0(k)
if
k =/k1,
Câˆ—,(k1)
1
(k1)

Câˆ—
0(k1)âˆ’1m0(k1) + Xâ€²
1y1

if
k = k1,
n1 = n0 + n, where n is the dimension of the yq vectors, and
d1(k1) = d0 + (y1 âˆ’X1m0(k1))â€² 
Qâˆ—
1(k1)
âˆ’1 (y1 âˆ’X1m0(k1)),
where Qâˆ—
1(k1) =

X1Câˆ—
0(k1)Xâ€²
1 + I

. We can also obtain the approximation

Ã‹(k)|M1(k1), D1

â‰ˆTTnq

Ã‹(k) m(k)
1 (k1), C(k)
1 (k1), R(k)
,
where TTÃŒ(Â·|m, C, R) denotes a truncated Student-t distribution with ÃŒ degrees
of freedom, location m, scale C and truncation region R. The value of C(k)
1 (k1) in
the equation above is given by C(k)
1 (k1) = Câˆ—,( j)
1
(k1)S1(k1), with S1(k1) = d1(k1)/n1.
In addition, writing
Ëœp1(k1) âˆÃ1(k1)ï£¿1(k1)
Ãâˆ—
0(k1)
Ã—
C(k)
1 (k1)

1/2
d(n0/2)
0
(n1/2)
|C0(k1)|1/2(n0/2)[d1(k1)]n1/2 ,

872
The Oxford Handbook of Applied Bayesian Analysis
where the Ëœp1(Â·)s are normalized such that K
k1=1 Ëœp1(k1) = 1, we have that
p1(k1) â‰ˆËœp1(k1). Finally, Ãâˆ—
0(Â·) and Ã1(Â·) are deï¬ned as
Ãâˆ—
0(k1) =

R(k1) TTn0

Ã‹(k1)|m0(k1), C0(k1)

dÃ‹(k1),
Ã1(k1) =

R(k1) TTn1

Ã‹(k1) m(k1)
1 (k1), C(k1)
1
(k1)

dÃ‹(k1).
(b) Updating the posteriors when q > 1. After yq is observed, we have the follow-
ing approximations

Ã‹(k)|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq, Ë†

â‰ˆTN

Ã‹(k) m(k)
q (kq, kqâˆ’1),
Câˆ—,(k)
q
(kq, kqâˆ’1)/Ë†, R(k)
,
with
Câˆ—,(k)
q
(kq, kqâˆ’1) =
âŽ§
âŽªâŽ¨
âŽªâŽ©
Câˆ—,(k)
qâˆ’1 (kqâˆ’1)
if
k =/kq,

Câˆ—,(k)
q
(kq, kqâˆ’1)
âˆ’1
+ Xâ€²
q Xq
âˆ’1
if
k = kq,
m(k)
q (kq, kqâˆ’1) =
âŽ§
âŽªâŽ¨
âŽªâŽ©
m(k)
qâˆ’1(kqâˆ’1)
if
k =/kq,
Câˆ—,(k)
q
(kq, kqâˆ’1)

Câˆ—,(k)
qâˆ’1 (kqâˆ’1)
âˆ’1
m(k)
q (kqâˆ’1) + Xâ€²
q yq

if k = kq,
and
(Ë†|Mq(kq), Mqâˆ’1(kqâˆ’1), Dq) â‰ˆGa(nq/2, dq(kq, kqâˆ’1)/2),
with nq = nqâˆ’1 + n, and
dq(kq, kqâˆ’1) = dqâˆ’1(kqâˆ’1) +

yq âˆ’Xqm
(kq )
qâˆ’1(kqâˆ’1)
â€²

Qâˆ—
q(kq, kqâˆ’1)
âˆ’1 
yq âˆ’Xqm
(kq)
qâˆ’1(kqâˆ’1)

,
where Qâˆ—
q(kq, kqâˆ’1) =

XqC
(kq )
qâˆ’1(kqâˆ’1)Xâ€²
q + I

. In addition, we approximate
pq(kq, kqâˆ’1) by Ëœpq(kq, kqâˆ’1), where
Ëœpq(kq, kqâˆ’1) âˆï£¿(kq
kqâˆ’1)Ãq(kq, kqâˆ’1)
Ãâˆ—
qâˆ’1(kq, kqâˆ’1)
Ã—
C
(kq )
q
(kq, kqâˆ’1)

1/2
(dqâˆ’1(kqâˆ’1))nqâˆ’1/2(nq/2)
C
(kq)
qâˆ’1(kqâˆ’1)

1/2
(dq(kq, kqâˆ’1))nq/2(nqâˆ’1/2)
,

Multistate Models for Mental Fatigue
873
with Ëœpq(kq, kqâˆ’1) normalized such that K
kq =1
K
kqâˆ’1=1 pq(kq, kqâˆ’1) = 1 and
Ãâˆ—
qâˆ’1(kq, kqâˆ’1) =

R(kq ) Tnqâˆ’1

Ã‹(kq )
m
(kq)
qâˆ’1(kqâˆ’1), C
âˆ—,(kq )
qâˆ’1 (kqâˆ’1)dqâˆ’1(kqâˆ’1)
nqâˆ’1

dÃ‹(kq )
Ãq(kq, kqâˆ’1) =

R(kq ) Tnq

Ã‹(kq )
m
(kq )
q
(kq, kqâˆ’1), C
âˆ—,(kq )
qâˆ’1 (kq, kqâˆ’1)dq(kq)
nq

dÃ‹(kq ) .
Then, we approximate pq(kq, kqâˆ’1) by Ëœpq(kq, kqâˆ’1). Finally, using the fact that
p

Ã‹(k)|Mq(kq), Dq, Ë†

can be written as a mixture of K components, we can
obtain the following approximations by collapsing the K components into a
single component (see West and Harrison, 1997, Chapter 12), and so
r (Ë†|Mq(kq), Dq) â‰ˆGa(nq/2, dq(kq)/2), with dq(kq) = nq Sq(kq) and
Sâˆ’1
q (kq) =
K

kqâˆ’1=1
Sâˆ’1
q (kq, kqâˆ’1)pq(kq, kqâˆ’1)/pq(kq).
r 
Ã‹(k)|Mq(kq), Dq, Ë†âˆ’1
â‰ˆTN

Ã‹(k) m(k)
q (kq), Câˆ—,(k)
q
(kq)/Ë†, R(k)
, with Câˆ—,( j)
q
(kq) = C(k)
q (kq )
Sq (kq) . The values of m(k)
q (kq) and C(k)
q (kq) are computed as follows
m(k)
q (kq) =
K

kqâˆ’1=1
m(k)
q (kq, kqâˆ’1)pâˆ—
q(kq, kqâˆ’1),
C(k)
q (kq) =
K

kqâˆ’1=1

C(k)
q (kq, kqâˆ’1) +

m(k)
q (kq) âˆ’m(k)
q (kq, kqâˆ’1)
â€²

m(k)
q (kq) âˆ’m(k)
q (kq, kqâˆ’1

Ã— pâˆ—
q(kq, kqâˆ’1)

,
where
pâˆ—
q(kq, kqâˆ’1) = Sq(kq)Sâˆ’1
q (kq, kqâˆ’1)pq(kq, kqâˆ’1)/pq(kq),
normalized
such that, for all kq, K
kqâˆ’1=1 pâˆ—
q(kq, kqâˆ’1) = 1.
Acknowledgements
We are grateful to L. Trejo for useful discussions and for providing access to the
data.
References
Carvalho, C., Johannes, M., Lopes, H. F. and Polson, N. (2008). Particle learning and smooth-
ing. Technical report, Graduate School of Business, University of Chicago.
Dyro, F. (1989). The EEG Handbook. Little, Brown and Company, Boston, Massachusetts.

874
The Oxford Handbook of Applied Bayesian Analysis
Huerta, G. and West, M. (1999). Priors and component structures in autoregressive time series
models. Journal of the Royal Statistical Society B, 61, 881â€“899.
Krystal, A., Prado, R. and West, M. (1999). New methods of time series analysis of non-
stationary EEG data: eigenstructure decompositions of time-varying autoregressions. Clin-
ical Neurophysiology, 110, 2197â€“2206.
Le Van Quyen, M., Martinerie, J., Navarro, V., Boon, P., Dâ€™Have, M., Adam, C., Renault, B.,
Varela, F. and Baulac, M. (2001). Anticipation of epileptic seizures from standard EEG
recordings. Lancet, 357, 183â€“8.
Liu, J. and West, M. (2001). Combined parameter and state estimation in simulation-based
ï¬ltering. In Sequential Monte Carlo Methods in Practice, (ed. A. Doucet, N. de Freitas, and N.
Gordon), pp. 197â€“223. Springer, New York.
Prado, R. (2009). Characterization of latent structure in brain signals. In Statistical Methods for
Modeling Human Dynamics (ed. S. Chow, E. Ferrer, and F. Hsieh). Psychology Press, Taylor
and Francis Group, New York.
Prado, R., West, M. and Krystal, A. (2001). Multi-channel EEG analyses via dynamic regression
models with time-varying lag/lead structure. Journal of the Royal Statistical Society, Series C
(Applied Statistics), 50, 95â€“109.
Trejo, L., Knuth, K., Prado, R., Rosipal, R., Kubitz, K., Kochavi, R., Matthews, B. and Zhang, Y.
(2007). EEG-based estimation of mental fatigue: Convergent evidence for a three-state model.
In Augmented Cognition, HCII 2007, LNAI 4565, (ed. D. Schmorrow and L. Reeves), New
York, pp. 201â€“211. Springer, New York.
Trejo, L., Kochavi, R., Kubitz, K., Montgomery, L., Rosipal, R. and Matthews, B. (2006). EEG-
based estimation of mental fatigue. Technical report, Available at http://publications.
neurodia.com/Trejo-et-al-EEG-Fatigue2006-Manuscript.pdf.
Trejo, L., Matthews, R. and Allison, B. (2007). Experimental design and testing of a multi-
modal cognitive overload classiï¬er. In Foundations of Augmented Cognition (4th edn), (ed.
D. Schmorrow, D. Nicholson, J. Drexler, and L. Reeves), pp. 13â€“22. Strategic Analysis, Inc,
Arlington, VA.
Weiner, R. D., Coffey, E. and Krystal, A. D. (1991). The monitoring and management of
electrically induced seizures. Psychiatric Clinics of North America, 14, 845â€“69.
West, M. (1997). Time series decompositions. Biometrika, 84, 489â€“94.
West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models, (2nd edn). Springer-
Verlag, New York.
West, M., Prado, R. and Krystal, A. (1999). Evaluation and comparison of EEG traces: Latent
structure in nonstationary time series. Journal of the American Statistical Association, 94,
1083â€“1095.

Index
The index entries appear in letter-by-letter alphabetical order.
Index entries displayed in bold text indicate entries from â€˜Aâ€™ Appendices.
ability estimation 649
adaptive sampling 826â€“8, 832â€“3
adjusted expectation 268â€“9
agricultural production analysis see multiscale
spatio-temporal model (agricultural
production analysis)
Air Pollutants Exposure Model (APEX) 485
air pollution assessment see environmental
exposure assessment
alcohol dehydrogenase 29, 36â€“8
aleatory uncertainty 73â€“4, 86â€“7, 408
amino acid sequences see protein bioinformatics
Andersen, T. 604â€“5
APEX (Air Pollutants Exposure) Model 485
approximate simulation algorithms 166â€“7
APPS see asynchronous parallel pattern search
(APPS)
ARF 157, 180
AR models see autoregressions (AR models)
assimilation, sequential 487; see also fusion
modelling
assimilation-contrast effect 601
asynchronous parallel pattern search
(APPS) 831, 834â€“5, 837â€“8
treed Gaussian process (TGP-APPS) 835,
837â€“8
ATM kinase 156â€“7, 158, 180
audio and music processing
applications 712â€“13
computational context and
background 742â€“5
conclusions 742
basic time-domain note and chord
models 721â€“8
Bayesian approachâ€™s advantages 711â€“12, 742
fundamental tasks 712â€“13, 717â€“20
Gamma chains and ï¬elds 734â€“5
Gaussian frequency-domain model 729â€“30,
731â€“2
inference in 717â€“20, 731â€“3,
742
introduction xxvi, 711â€“21
latent variance/intensity factorization
models 733â€“4, 735â€“8
musical transient analysis example 726â€“8
non-negative matrix factorization (NMF)
models 738â€“41, 744â€“5
point process frequency-domain model 730,
732â€“3
polyphonic music transcription 713, 718â€“20
polyphonic pitch estimation example
739â€“41
prior structures 722â€“6, 733â€“4
properties of musical audio signals 713â€“16
source separation 713, 718, 743
superposition 715, 716â€“17
time-domain state-space models 728
tone complex analysis example 731â€“3
auditing
concepts and terminology 654â€“5
previous work review 656â€“7
sequential multilocation see sequential
multilocation auditing (New York food
stamps program application)
autoregressive (AR) models 848â€“52, 864â€“5,
866â€“7, 867â€“8
application example see cognitive fatigue EEG
characterization
approximate posterior inference for multi-AR
models 856â€“61, 865, 867â€“73
moving average (ARMA) model 112
multi-AR model details 851â€“6, 865
time-varying (TVAR) models 850, 866â€“7,
867â€“8
auxiliary feedwater system (AF) 225â€“6, 230â€“1,
233
Bayes factors 331â€“3, 334, 336â€“7
Bayesian Factor Regression Modelling
(BFRM) software 123, 127, 128, 148, 150â€“1
Bayesian analysis
applicational diversity xv
scientiï¬c philosophy changes xv
Bayes inference see variational/Bayes inference

876
Index
Bayesâ€™ theorem, audio-processing context 712
Bernoulli probability 442, 468
BFRM (Bayesian Factor Regression Modelling)
software 123, 127, 128, 148, 150â€“1
BIC (Bayesian information criterion) 10
Bilog-mg 639â€“40
binary bet 346â€“7
biodiversity, demographic rates and see
demographic rate analysis (tree study)
biomarkers
clinical 129, 133
genomic see oncogene pathway deregulation
and breast cancer
biomolecule matching and alignment
advantages of Bayesian modelling
approach 40
data analysis 36â€“40
data sources 28â€“9, 47â€“8
EM approach 41
future directions 42â€“3
geometrical transformations 29â€“30
introduction xvii, 27â€“30
methodology reï¬nements 40â€“1
model formulation and inference 44â€“6
model implementation 46â€“7
multiconï¬guration alignment model 33â€“6
pairwise matching model 30â€“3, 44â€“5
Procrustes type approaches 41â€“2, 44
protein data and alignment problems 27â€“9
shape analysis background 43â€“4
bipolar junction transistors (bjt) study see circuit
device experiment design and analysis
bivariate inï¬nite mixtures of experts
models 11â€“12
Blackâ€“Scholes model 338
BLAST (Basic Local Alignment Search Tool) 48
breast cancer, and oncogene pathway
deregulation see oncogene pathway
deregulation and breast cancer
Brownian motion 339
calibration
climate models 410
concept of 752
environmental models 483â€“4
GP-based 770â€“2
history matching and 257â€“63
treed Gaussian processes 823, 833â€“8
Calvo pricing 374â€“5, 376, 378, 385, 388
cancer studies see causal inference (cancer
survival and treating hospital type
analysis); oncogene pathway deregulation
and breast cancer; stochastic kinetic model
of p53 and Mdm2 oscillations
carbonyl reductase 28â€“9, 36, 37
cardiovascular thrombotic adverse events, Vioxx
and see Vioxx trials
CATH 48
causal inference (cancer survival and treating
hospital type analysis)
assignment mechanism 684â€“5, 689â€“90
as-treated analysesâ€™ problems 697
causal effect of large versus small treating
hospitals 689â€“705
computation 701â€“2
exclusion restriction 680, 698â€“9
introduction xxvâ€“xxvi, 679â€“80
model details 700â€“1
monotonicity/no-deï¬er assumption 680,
695â€“7
per-protocol analysesâ€™ problems 697â€“8
posterior predictive causal inference 685â€“6
principal stratiï¬cation 680, 689, 694â€“7, 706
Rubinâ€™s causal model (RCM) 680â€“9, 705â€“6
science assumptions 687â€“9
stable unit treatment value assumption
(SUTVA) 682â€“3, 691
change point problem 289
chemical Langevin equation (CLE) 167
chemotherapy development 119
Chinese Restaurant Process
representation 146, 784, 808; see also Polya
urn representation
Cholesky decomposition 773
Chomsky normal form 780
circuit device experiment design and analysis
calibration of computer model 823, 833â€“8
circuit experiments 824
computations 841â€“2
experimental design 831â€“3, 841
introduction xxvii, 823â€“31
treed Gaussian processes 824â€“31, 838â€“9,
839â€“41
validation of computer model 823â€“4, 833
classiï¬cation problems 839
climate change projectionsâ€™ uncertainty
characterization
application of latest model
projections 562â€“72
computational details 583â€“92
conjugate priors context and
background 574â€“6
current state and future scenarios 545â€“8
diagnostics context and background
582â€“3
discussion 572â€“3
extent of data 548â€“9
future directions 573
hierarchical modelsâ€™ context and
background 576â€“7

Index
877
climate change projectionsâ€™ (cont.)
hierarchy of models 551â€“8
Markov chain Monte Carlo (MCMC)
methodsâ€™ context and background 577â€“83
Metropolisâ€“Hastings algorithm
background 580â€“2
model validation 558â€“62
overview xxiv
simpliï¬ed datasets 550â€“1
climate models
application examples see climate change
predictionsâ€™ uncertainty characterization;
meridional overturning circulation (MOC)
collapse probability analysis
components of 406
climate scenarios and 408
climate system and 406â€“7
inference from 408â€“11
quantity and diversity of 545â€“7
clinical trials
missing data/dropout problems 53, 58â€“66,
66â€“7
sequential meta-analysis 53â€“8, 66
Vioxx trial history xviiâ€“xviii, 51â€“3
CMAQ (Community Multi-Scale Air Quality)
Eta forecast model 483, 484, 489â€“98
code uncertainty 411
cognitive fatigue EEG characterization
approximate posterior inference for multi-AR
models 856â€“61, 865, 867â€“73
autoregressions (AR models) 848â€“52, 864-5,
866â€“7, 867â€“8
computation details 867â€“73
data description 847â€“8
EEG data analysis via multi-AR
models 861â€“4
experimental setting 847â€“8
future directions 865â€“6
introduction xxvii, 845â€“8
multi-AR model details 851â€“6, 865
time series decompositions 848â€“51, 867
time-varying AR (TVAR) models 850, 866â€“7,
867â€“8
-cold dark matter (CDM) model see
cosmological parametersâ€™ estimation
(-cold dark matter (CDM) model)
Collaborative Perinatal Project, substudy of 4â€“6
CoMFA (Comparative Molecular Field Analysis)
database xvii, 29
Community Multi-Scale Air Quality (CMAQ)
Eta forecast model 483, 484, 489â€“98
complete data likelihood function 59
conditional autoregressive (CAR) model 90, 91,
97, 101â€“2
conjugacy, inducement of 22
conjugate priors 574â€“6
continuous time models
ï¬nancial modelling context and
background 338â€“9
hidden Markov model (CTHMM) 182
coordinate ascent algorithm 789â€“92, 814â€“19
Cosmic Background Explorer (COBE) 749
cosmological parametersâ€™ estimation (-cold
dark matter (CDM) model)
combined CMB and large scale structure
analysis 765â€“70
cosmic microwave background (CMB)
measurement 749
future directions 770
GP-based calibration context and
background 770â€“2
introduction xxviâ€“xxvii, 749â€“52
matter power spectrum analysis 752â€“65
multivariate output emulation context and
background 772â€“3
simulation design 753â€“4, 755
simulation model overview 752â€“3
simulator output emulation 754â€“63, 764,
767â€“8, 771â€“3
statistical formulation 763â€“5
TT spectrum modelling 765â€“8
covariance structures 91, 97â€“8, 100â€“9
Cox proportional hazard regression model 228
credit crisis, 2007â€“2008 see ï¬nancial market
volatility analysis (2007â€“2008 credit crisis)
cultural differences, educational assessment
and see differential item functioning (DIF)
(in educational assessment)
Dali database 48
data augmentation (DA) 701â€“2, 738
DDE, maternal exposure and pregnancy
outcome 5â€“7, 13â€“18
DDT 13â€“14; see also DDE
decision theory 371
decomposition
AR models 848â€“51, 867
Cholesky 773
multiscale for economic data analysis 295,
296â€“7, 298, 313, 314â€“15
singular value (SVD) 756
de Finetti, B. 344, 686
delayed acceptance scheme 772
demographic models 467â€“8
application example see demographic rate
analysis (tree study)
current limitations 432
multistage model advantages 432â€“3
demography rate analysis (tree study)
complexity summarizing 463â€“7

878
Index
demography rate analysis (tree study) (cont.)
computation 452â€“3, 469â€“79
current demographic model limitations 432
data prediction 461â€“3, 464
demographic data 433â€“40
demographic model context and
background 467â€“8
diameter growth and fecundity
modelling 444â€“7
exposed canopy area modelling 447
gender and maturation modelling 441â€“3
hierarchically structured generalized linear
model (GLM) background 468â€“9
incomplete information challenges 431â€“2
introduction xxiiâ€“xxiii, 431â€“3
multistage model advantages 432â€“3
prior distributions 448â€“52
prior/posterior comparisons 453â€“61
seed data and fecundity modelling 443â€“4
survival probability modelling 448
density regression, pregnancy outcomes
13â€“18
deterministic ï¬xed point iterations 742â€“5
differential item functioning (DIF)
(educational assessment application)
Bayesian methods in item response theory
background 649â€“50
computation context and background
641â€“5
conclusions 641
DIF analysis of PISA 2003 633â€“41
DIF overview 624â€“5, 627â€“9
introduction xxv, 624â€“5
model computation details 633, 645â€“9
model description 629â€“33
Programme for International Student
Assessment (PISA) overview 624â€“7, 628,
650â€“1
Dirichlet models 659, 660
context and background for HDP-PFCG and
HDP-PCFG-GR 802â€“7
HDP-PCFG for grammar reï¬nement
(HDP-PCFG-GR) 785â€“7, 793â€“4,
797â€“800
hierarchical Dirichlet process probabilistic
context-free grammar (HDP-PCFG)
780â€“5, 787â€“96, 801
see also Dirichlet process mixture (DPM)
models
Dirichlet process (DP) 146â€“7
dependent 20â€“1
hierarchical (HDP) 805â€“6
local 11â€“12, 21
order-based dependent 11, 21
stick-breaking representation of 10â€“11,
12, 23
Dirichlet process mixture (DPM) models 802,
803â€“5
deï¬nition and graphical model of 804
dependent (DDP) 20â€“1
ï¬nite mixture model background 802â€“3
latent factor models 127
nonparametric Bayes mixture models context
and background 20â€“1
of normals 10â€“11
posterior distribution representations for
inference 808â€“9
stick-breaking distribution 803â€“4,
808â€“9
Dirichlet process priors 146â€“7
discrepancy ratio 255
disease mapping see malaria mapping
(spatio-temporal mixture model)
DNA damage 155â€“8
DNA proï¬ling xix, 188â€“9
genetic background 209â€“10
simple paternity testing 190â€“3
see also paternity testing and mutation rate
uncertainty
dropout, non-ignorable 53, 58â€“66, 66â€“7
Drudge report 606â€“7
drug development programmes 53, 58, 66;
see also Vioxx trials
DSGE models see dynamic stochastic general
equilibrium (DSGE) models
dynamic/state-space models 313
for audio and music processing 728
auditing context and background 676â€“7
computational details of generic 360â€“1
for demographic modelling 444â€“5, 470â€“1
in ï¬nancial futures marketsâ€™
forecasting 350â€“1
functional network of generic 351
posterior computation in 617â€“19
dynamic linear models 109â€“11, 313â€“14
class of 111â€“12
future developments in 115
for futures marketsâ€™ forecasting 351â€“2, 354,
355â€“7, 361â€“2, 363
general model building approach 111â€“12
Markov chain Monte Carlo (MCMC)
for 112â€“14, 115, 618
multi-process model approximate posterior
distributions 568
in multiscale spatio-temporal model 297,
304â€“5, 313â€“14
in multivariate Poisson-lognormal model 90,
97, 99, 109â€“15
non-normal 113â€“14
similarities in forecasting and auditing
models 676â€“7
spatio-temporal 114

Index
879
dynamic stochastic general equilibrium (DSGE)
models
application example see new
macroeconometrics (DSGE model analysis
of US economy dynamics)
policy-making and 370, 371, 391
sequential Monte Carlo (SMC) methods 369,
391â€“3
Earth system models of intermediate
complexity (EMICs) 407, 411â€“12
GENIE-1 application see meridional
overturning circulation (MOC) collapse
probability analysis
econometrics 366
economics see multiscale spatio-temporal model
(for agricultural production analysis); new
macroeconometrics (DSGE model analysis
of US economy dynamics)
educational assessment, cultural differences
and see differential item functioning (DIF)
(educational assessment application)
Educational Testing Service (ETS) 628
EEG data analyses see cognitive fatigue EEG
characterization
Efï¬cients Markets Hypothesis (EMH) 347â€“9,
351
electro-hydraulic control (EHC) system 226â€“7,
230â€“2
elicitation
designing 534â€“7
direct versus indirect 515, 538â€“40
ecological application see rock-wallaby habitat
modelling and mapping (indirect
elicitation from experts)
for regression 514â€“20
in sequential multilocation auditing 676
software tool for 518, 520â€“6, 532â€“3, 534
Elicitator 518, 520â€“6, 532â€“3, 534
EM (expectation-maximization) algorithm
in audio and music processing 738
in biomolecule matching and alignment 41
in ï¬‚exible Bayesian regression 8
in IRT models 649â€“50
in natural language processing (NLP) 789,
791, 794â€“801, 807â€“8, 809â€“14
variational methods and 742â€“3, 794â€“801,
807â€“8, 809â€“14
empirical Bayes estimation 304â€“5
emulation
linear uncertainty analysis 246, 249â€“53,
253â€“7, 260, 261â€“3
multiscale method 253â€“7
multivariate computer model output 772â€“3
re-emulation 260, 261â€“3
stochastic kinetic model 167â€“8, 182â€“4
treed Gaussian processes 829â€“31
energy balance models 407
ensembles of opportunity 547â€“8
environmental exposure assessment
algorithmic and pseudo-statistical weather
prediction 486â€“8
background and context 503â€“4
downscaling fusion modelling 490â€“508
dry deposition estimation 498, 503
environmental computer models 483â€“6
Gibbs sampling distributions 504â€“8
history of pollutant space-time
modelling 482â€“3
introduction xxiii, 482â€“6
National Atmospheric Deposition Program
(NADP) data illustration 495â€“502
stochastic integration fusion
modelling 488â€“9
upscaling fusion modelling 489â€“90
epidemiologic data
ï¬‚exible Bayes regression of see pregnancy
outcome study
spatial-temporal modelling see malaria
mapping (spatio-temporal mixture model)
epistemic uncertainty 73â€“4, 86â€“7, 408â€“9
equilibrium, economic deï¬nition of 366â€“7
error classesâ€™ modelling 660â€“1
application example see sequential
multilocation auditing (New York food
stamps program application)
error ratesâ€™ modelling 659â€“60
application example see sequential
multilocation auditing (New York food
stamps program application)
Euler discretization 325â€“6, 327, 338â€“9
event futures see prediction market volatility
(political campaign information ï¬‚ow
measurement)
evolutionary factor models search 149
exact simulation algorithms 167
exchangeability, second order 268â€“9
exotic preferences, in DSGE models 390
expectation, adjusted 268â€“9
expectation-maximization (EM) algorithm see
EM (expectation-maximization) algorithm
Expected A-Posteriori (EAP) 649
Expected Global Optimizer (EGO)
algorithm 830
experiment design see circuit device experiment
design and analysis
exposureâ€“disease relationships
misinterpretation of 3â€“4
pregnancy outcomes 4â€“24
factor modelling 144â€“5
latent factor models 125â€“7, 146â€“7, 148â€“51

880
Index
failure rate 222â€“30
false consensus effect 601
ï¬ltering theory 368â€“9
ï¬nancial futures markets
computations 363
dynamic and dynamic linear models 354,
355â€“7, 360â€“2, 363
forecasting 350â€“2
futures markets 344â€“8, 358
introduction xxiâ€“xxii, 343
portfolio optimization 349â€“50, 357,
360
probability axioms derivation 359â€“60
risk modelling 352â€“8, 359
speculation 348â€“50, 358â€“9
subjective expectations 343â€“4
varianceâ€“covariance graphical model 354â€“7,
362â€“3
ï¬nancial markets
political effect on 603
see also ï¬nancial futures markets; ï¬nancial
market volatility analysis (2007â€“2008 credit
crisis)
ï¬nancial market volatility analysis (2007â€“2008
credit crisis)
conclusions 337â€“8
continuous-time model context and
background 338â€“9
Garch model 328â€“9
introduction xxi, 319â€“24
option pricesâ€™ and returnsâ€™ informational
content 339â€“40
option prices and volatility 321â€“4
particle ï¬ltering for sequential
learning 329â€“3
problem context and goals 321
results 333â€“7
stochastic volatility (SV) model 325â€“6
stochastic volatility jump (SVJ) model
326â€“8
ï¬nite mixture models 7â€“10, 22, 802â€“3
ï¬‚exible Bayes regression, of epidemiologic
data 3â€“24
food safety risk assessment sensitivity analysis
see microbial risk assessment sensitivity
analysis (VTEC 0157 study)
forecasting
DSGE models and 370
dynamic linear models 767
failure 272, 281â€“5
ï¬nancial future markets 350â€“2,
in oil reservoir simulation uncertainty
analysis 260, 262, 263â€“8
prior information and 676
forensic genetics see paternity testing and
mutation rate uncertainty
forward ï¬lter backward sampler algorithm
(FFBS)
Markov-switching stochastic volatility
model 619
multiscale spatio-temporal model 298, 305,
314
singular (SFFBS) 298, 305, 316
spatio-temporal mixture model 99â€“100,
112â€“13
state-space models 618
frequency/transform domain models
Gaussian 729â€“30, 731â€“2
latent variance/intensity factorization
735â€“7
non-negative matrix factorization 738â€“41,
744â€“5
point process 730â€“1, 732â€“3
prior structures 733â€“5
frequentist analysis 6â€“7, 15â€“16
full conditional distributions, derivation of
315
fusion modelling
downscaling approach 490â€“508
stochastic integration approach 488â€“9
upscaling approach 489â€“90
Gabor regression 734
Gamma chains 734â€“5
Gamma ï¬eld 734â€“5
Garch model 320, 328â€“9, 333â€“8
gas pipeline reliability 289â€“91
Gaussian covariance function 757
Gaussian process models
calibration 770â€“3
computer experiment design and 841
drawbacks of 825
emulation 754â€“63, 771â€“3, 825
frequency-domain 729â€“30, 731â€“2
spatial 503â€“4; see also environmental
exposure assessment
treed 824â€“31
Gelman â€“ Rubin procedure 582â€“3
general circulation models (GCMs) 407
application example see climate change
projectionsâ€™ uncertainty characterization
bivariate 556â€“8
computational details 583â€“92
future possibilities 572â€“3
Hadley Center 573
multivariate 554â€“6
simpliï¬ed datasets for 550â€“1
univariate 551â€“4
validation of 558â€“62
generalized linear models (GLMs) 113â€“14,
468â€“9, 514
generalized method of moments (GMM) 369

Index
881
GENIE-1 (Grid Enabled Integrated Earth system
model) 411â€“12
application of see meridional overturning
circulation (MOC) collapse probability
analysis
Geo-R 504
geostatical modelling 90â€“1
Gibbs sampling 643
for climate models 556, 558, 560â€“1, 578â€“80,
583â€“92
collapsed/marginal/Polya Urn 22â€“3, 146â€“7;
see also Chinese Restaurant representation
conditional/blocked 23â€“4
for demographic model 452â€“3, 469â€“79
for environmental exposure assessment
504â€“8
for IRT models 650
for latent factor regression models 148
mean ï¬eld and 815
for multiscale spatio-temporal model 305
for multivariate Poisson-lognormal model 99
non-parametric Bayes analysis 9â€“10, 22, 22â€“4
for state-space models 618
for stochastic kinetic model 168
for treed Gaussian processes 842
variational method alternative to 742â€“3, 745
Gillespie algorithm 161, 166â€“7
graph theoretic technique 36, 38
graphical models
in audio and music processing 720, 734â€“5
in natural language processing (NLP) 781,
788
varianceâ€“covariance 354â€“7, 362â€“3
greenhouse gases emission scenarios
(SRES) 414, 546â€“7, 548â€“9, 550
Grid Enabled Integrated Earth system model
(GENIE-1) 411â€“12
application example see meridional
overturning circulation (MOC) collapse
probability analysis
habitat modelling/mapping see rock-wallaby
habitat modelling and mapping (indirect
elicitation from experts)
Hadley Center GCM 573
Hansen, L. 369
Hastings step 765; see also Metropolis â€“
Hastings step
Health Maintenance Organizations
(HMOs) 675
health outcomes
causal inference see causal inference (cancer
survival and treating hospital type analysis)
exposureâ€“disease relationship
misinterpretation 3â€“4, 19
pregnancy study see pregnancy outcome study
Heidelberger â€“ Welch procedures 583â€“4
hidden Markov models (HMMs) 181, 792,
805
continuous-time (CTHMM) 182
hierarchical Dirichlet process
(HDP-HMM) 782, 783, 805â€“7
probability context-free grammars (PCFGs)
and 781
hierarchically-structured generalized linear
model (GLM) 468â€“9
hierarchical models 576â€“7
application examples see audio and music
processing; climate change projectionsâ€™
uncertainty characterization; demographic
rate analysis (tree study); differential item
functioning (DIF) (educational assessment
application); environmental exposure
assessment; natural language processing
(syntactic parsing problem); sequential
multilocation auditing (New York food
stamps program application)
history matching 257â€“63
17â€“â€š hydroxysteroid-dehydrogenase 28â€“9,
36, 37
hypoxia-related pathways 143
idea futures see prediction market volatility
(political campaign information ï¬‚ow
measurement)
identiï¬ability, model 633
implausibility 257â€“61
independent components analysis 418
industrial maintenance applications see nuclear
power plant maintenance analysis and
decisions; train door reliability analysis
(bivariate Poisson process model)
information criterion, Bayesian (BIC) 10
information ï¬‚ow measurement see prediction
market volatility (political campaign
information ï¬‚ow measurement)
information markets see prediction market
volatility (political campaign information
ï¬‚ow measurement)
information ratio 349, 352
inside-outside algorithm 781
instrumental variables models 706
intensity factorization models 735â€“8
Intergovernmental Panel on Climate Change
(IPCC) 408, 414, 546
interpolation, spatial see spatial interpolation
Intrade/Tradesports 598, 600â€“1, 606
in-vitro to in-vivo translation problem 118,
119â€“20
Iowa Electronic Markets (IEM) 598, 600â€“2
item response theory (IRT)
Bayesian methods in 649â€“50

882
Index
item response theory (IRT) (cont.)
in educational assessment 629â€“33; see also
differential item functioning (DIF)
(educational assessment application)
Kalman ï¬lter
for DSGE models 369, 378, 381, 394â€“5
in particle ï¬ltering 330
sequential assimilation and 487â€“8
Kaplanâ€“Meier curves 135â€“7
kernel continuation ratio probit model 22
kernel stick-breaking process (KSBP) 11â€“13,
14, 15, 21
KL-divergence 814â€“15
KL-projection 794
Kronecker structure 773
Kolmogorovâ€“Smirnov statistics 292
Kydland, F. 367â€“8
label ambiguity 8â€“9
Lagrangian function 373
latent class model 7â€“10
latent factor models 125â€“7
Dirichlet process priors in 146â€“7
models and computations 148â€“51
latent mean process reconstruction
305â€“6
latent point process model 30â€“1
latent variance models 733â€“4, 735â€“7
latin hypercube sampling
climate models 412â€“13
cosmological models 754, 755, 761â€“2, 763
stochastic kinetic model 183
treed Gaussian processes 835â€“7
Legacy Archive for Microwave Background Data
Analysis (LAMBDA) 767
legal evidence issues 208â€“9
likelihood
joint 210â€“13
in legal evidence 208â€“9
linear methods 263â€“8, 268â€“9
local partition process 21
logistic regression, elicitation for 512, 514â€“20
Lucas, R. 367â€“8
Lucasâ€™ critique 368
macroeconomics 366
main effect 80â€“1, 82â€“3
main effect index 80, 85â€“6
maintenance model 222â€“3
application example see nuclear power plant
maintenance analysis and decisions
malaria
DDT use 13â€“14
disease mapping see malaria mapping
prevalence and impact of 92
malaria mapping (spatio-temporal mixture
model)
dynamic linear model (DLM) background
and context 109â€“15
introduction xviii, 90â€“1
model extension possibilities 109
motivation for 92â€“6
multivariate Poisson-lognormal
model 96â€“102
results 102â€“9
Markov chain Monte Carlo (MCMC) methods
577â€“80, 641â€“5
in biomolecule matching and
alignment 37â€“40, 41, 45â€“7
block-move 618
for causal inference 701â€“2
for climate models 577â€“83, 583â€“92
in clinical trialsâ€™ analysis 62, 64, 65
data augmentation (DA) 701â€“2
for DIF models 645â€“9
in disease mapping 97, 99, 100â€“1, 102
for dynamic linear models (DLMs) 112â€“14,
115, 314, 618
for dynamic stochastic general equilibrium
(DSGE) models 369, 370
for ï¬‚exible Bayesian regression 8â€“9,
13, 15
for latent factor regression models 127, 148
for Markov-switching stochastic volatility
model 606, 617â€“19
for multiscale spatio-temporal model 305â€“6
in reliability analysis 279â€“81
reversible jump see reversible jump Markov
chain Monte Carlo (RJMCMC)
for semiparametric model 229
single-move 618
in sparse multivariate regression
analysis 123
for stochastic kinetic model 155, 168â€“70,
171, 174, 181, 182
for stochastic volatility models 326
for treed Gaussian processes 827, 829
variational alternatives and 742â€“5, 807â€“8,
809â€“14
see also EM algorithm; Gibbs sampling;
Metropolis â€“ Hastings step; Metropolis step
Markovian process models
classiï¬cation 181
Markov-switching stochastic volatility
model 604â€“6; see also prediction market
volatility (political campaign information
ï¬‚ow measurement)
parameter inference and time course
data 181â€“2
see also hidden Markov models (HMMs)
Markov random ï¬eld (MRF) 734

Index
883
Mdm2 see stochastic kinetic model of p53 and
Mdm2 oscillations
mean ï¬eld 787â€“9, 742â€“3
for DP-based models 809â€“14
update equation derivation 814â€“19
measurement error model (MEM) 490
mental fatigue see cognitive fatigue EEG
characterization
meridional overturning circulation (MOC)
collapse probability analysis
21st century MOC simulations 415â€“17
climate models 406â€“8
climate simulator uncertainty 408â€“9
context and background 425â€“7
data and climate prediction 409â€“10
emulators 410â€“11, 417â€“24, 425â€“7
experimental structure 412â€“14
future directions 424â€“5
GENIE-1 411â€“12
input parameter boundsâ€™ elicitation
414â€“15
introduction xxii, 403â€“6
Monte Carlo methods 410
results summary 424
uncertainty analysis 419â€“24
method-of-moments analysis 696, 697, 698
generalized 369
Metropolis step 453, 470â€“9, 759, 765; see also
Metropolis â€“ Hastings step
Metropolis â€“ Hastings step 47, 229, 580â€“2,
643â€“5
biomolecule matching and alignment 47
climate models 554, 556, 558, 580â€“2,
583â€“92
DIF models 645â€“8
DSGE model 382, 394
mixture of Polya trees (MPT) model 229
multivariate Poisson-lognormal model 99,
113
non-normal DLMs 113
state-space models 618
treed Gaussian processes 842
Metropolis-within-Gibbs algorithm 644â€“5, 650
microbial risk assessment sensitivity analysis
(VTEC 0157 study)
contamination assessment model 72â€“3
epistemic and aleatory uncertainty context
and background 86â€“7
future directions 84
introduction xviii, 69
microbial risk assessment background 69â€“70
model input distributions 73â€“9
model output analysis 79â€“84
problem of Vero-cytotoxic E.coli 0157 (VTEC
0157) in pasteurized milk 70â€“2
variance-based sensitivity background 84â€“6
missing data problems
in causal inference 681â€“2
in clinical trials 53, 58â€“66, 66â€“7
mixture of Polya trees models (MPTs) 228â€“31,
233â€“5
Model A-Posteriori (MAP) 649
model identiï¬ability 633
modern portfolio theory (MPT) 349â€“50
Molecular Signatures Database (MSigDB) 142
monetary unit sampling 654, 656â€“7
monotonicity/no-deï¬er assumption 680, 695â€“7
Monte Carlo (MC) estimators 642
Monte Carlo (MC) methods
for climate models 410, 420
in microbial risk assessment 70, 79â€“82, 86
for sequential multilocation auditing 662
for treed Gaussian processes 829
see also Markov chain Monte Carlo (MCMC)
methods; sequential Monte Carlo (SMC)
methods
multi-AR models 851â€“6, 865
application example see cognitive fatigue EEG
characterization
approximate posterior inference for 856â€“61,
865, 867â€“73
multiscale modelling
application exampe see multiscale
spatio-temporal model (agricultural
production analysis
background 313
multiscale spatio-temporal model (agricultural
production analysis)
agricultural production data analysis 300â€“2,
306â€“11, 312
computing background 315â€“16
dynamic linear model context and
background 313â€“14
dynamic multiscale modelling 302â€“4
estimation 304â€“6
introduction xxi, 295â€“8
massive data set capability 311, 313
model summary 304
multiscale decomposition 295, 296â€“7, 298,
313, 314â€“15
multiscale factorization 298â€“300
multiscale model background 313
music processing see audio and music
processing
mutation models 194â€“6
mutation rates, uncertainty and paternity
testing see paternity testing and mutation
rate uncertainty
MySQL 522â€“3
NASDAQ NDX100 see ï¬nancial market
volatility analysis (2007â€“2008 credit crisis)

884
Index
National Ambient Air Quality Standards
(NAAQS) Exposure Model 485
national and oncome product accounts
(NIPA) 396
National Atmospheric Deposition Program
(NADP) 495â€“502
natural language processing (syntactic parsing
problem)
adaptor grammar framework 784â€“5
approximate posterior inference algorithm
for HDP-PCFG 787â€“94, 801, 807â€“8,
814â€“19
Bayesian approachâ€™s advantages 776
DP-based modelsâ€™ context and
background 802â€“7
experiments 794â€“800
future directions 801
grammar induction 777â€“8, 794â€“6
grammar reï¬nement 778â€“9, 785â€“7, 793â€“4,
797â€“801
HDP-PCFG for grammar reï¬nement
(HDP-PCFG-GR) 785â€“7, 793â€“4,
797â€“800
hidden Markov models (HMMs) and 777
hierarchical Dirichlet process probabilistic
context-free grammar
(HDP-PCFG) model 780â€“5, 787â€“96, 801
inï¬nite tree framework 784, 785
introduction xxvii, 776â€“81
latent-variable models 779
lexicalization 778â€“9
machine translation 801
multilingual data modelling 801
parent annotation 778, 779
parse trees and treebanks 776â€“7, 778â€“9,
789â€“91, 792â€“4, 798â€“800
Penn treebank parsing 798â€“800
probabilistic context-free grammars
(PCFGs) 777â€“8, 780â€“81
update equationsâ€™ derivations 814â€“19
variational inference context and
background 807â€“14
net payoff 349
New Keynesian DSGE models 369,
371â€“2
aggregation 377â€“8
equilibrium 378â€“80
ï¬nal good producer 375
future directions 390â€“1
the government 376â€“7
households 372â€“5
intermediate good producers 375â€“6
likelihood function 380â€“2
US economy data estimation 382â€“90
see also dynamic stochastic general
equilibrium (DSGE) models
new macroeconometric US economy dynamics
analysis (DSGE model)
data construction 396
empirical analysis 382â€“90
future directions 390â€“1
introduction xxii, 366â€“71
Kalman ï¬lter 369, 378, 381, 394â€“5
model computation 393â€“4
new Keynesian model 371â€“82
policy-making 370, 371, 391
sequential Monte Carlo (SMC) methods 369,
391â€“3
Newtonian relaxation 487
New York food stamps program, auditing of
see sequential multilocation auditing
(New York food stamps program
application)
NIPA (national and oncome product accounts)
396
no-deï¬er/monotonicity assumption 680,
695â€“7
non-negative matrix factorization (NMF)
models 738â€“41, 744â€“5
nonparametric models 10â€“12, 20â€“2
Normal-Gamma model 759, 764
NPACI/NBCR resource 48
nuclear power plant maintenance analysis and
decisions
data 224â€“7
introduction xx, 219â€“22
maintenance model 222â€“3
mixture of Polya trees models (MPTs)
228â€“31, 233â€“5
optimization algorithm 238
optimization results 223â€“4
parametric model 224â€“5, 227â€“8, 230â€“3
proof of propositions 235â€“9
semiparametric model 225, 228â€“31
nudging 487
oestrogen receptor (ER) pathways 143â€“4
Ofï¬ce of Air Quality and Planning 485
oil reservoir simulation uncertainty analysis
analysis overview 246â€“7
Bayes linear approach context and
background 268â€“9
coarse model emulation 249â€“53
forecasting 263â€“8
general formulation of approach 244â€“6
history matching and calibration 257â€“63
initial model runs and screening 247â€“9
introduction xx, 241â€“2
model description 242â€“4
multiscale emulation 253â€“7
oncogene pathway deregulation and breast
cancer

Index
885
BFRM (Bayesian Factor Regression
Modelling) software 123, 127, 128, 148,
150â€“1
clinical outcome prediction 133â€“40
Dirichlet process priors context and
background 146â€“7
evaluation and pathway annotation
analysis 140â€“4, 150â€“1
evolutionary factor models search 149
factor modelsâ€™ context and
background 144â€“5
introduction xviiiâ€“xix, 118
latent factor models for tumour gene
expression 125â€“7, 144â€“7, 148â€“51
latent factor projection from in-vivo to
in-vitro 131â€“3, 134
latent factor structure exploration 128â€“31
MCMC for 148, 149, 151
pathway activity quantiï¬cation 123â€“5
pathway signature generation 122â€“3
previous studies 120â€“1
probabilistic pathway annotation (PROPA)
analysis 142â€“4, 150â€“1
problem context and goals 118â€“20
shotgun stochastic search (SSS) 133â€“5, 150
sparsity modelling in multivariate analysis
background 147
Weibull survival regression models 133â€“40,
150
opinion polls 598
optimization 787, 789
asynchronous parallel pattern search
(APPS) 831, 834â€“5, 837â€“8
climate model 427
Expected Global Optimizer algorithm 830
global sensitivity analysis and 828
maintenance analysis 219, 221, 222â€“4, 238
Pareto 349, 357
portfolio 349â€“50, 357, 360
treed Gaussian processes 829â€“31, 834â€“5,
837â€“8
option prices 319
informational content 339â€“40
and volatility 321â€“4
Organization for Economic Co-Operation and
Development (OECD)
Programme for International Student
Assessment (PISA) see differential item
functioning (DIF) (in educational
assessment)
p53 and Mdm2 oscillations see stochastic kinetic
model of p53 and Mdm2 oscillations
palaeoclimate data, use of 405â€“6, 409
parameter relationship modelling 658â€“9
parametric model 224â€“5, 227â€“8, 230â€“3
Pareto optimization 349, 357
particle ï¬ltering
for DSGE models 392â€“3
in volatility analysis 319â€“21, 329â€“33, 337
paternity testing and mutation rate uncertainty
allele frequency uncertainty 207â€“8
analytical allowance for mutation 193â€“7
assumed mutation rate case analysis 197â€“8
Bayesian networks 207â€“8
case analysis example 191â€“2, 197â€“8, 206
casework data 200â€“1
DNA proï¬ling context and
background 209â€“10
introduction xix, 188â€“90
joint likelihood 210â€“13
legal evidence context and background 208â€“9
mutation rate data analysis 204â€“5
mutation rate likelihood 201â€“4
mutation rate uncertainty 198â€“200
null alleles 207
simple cases 190â€“3
unobtainable DNA proï¬les 207
pCNEM 485
Penn Treebank 798â€“800
perturbation methods 378â€“9, 381â€“2, 394
pharmaceutical development/testing see drug
development programmes; Vioxx trials
PISA (Programme for International Student
Assessment), cultural differences and see
differential item functioning (DIF) (in
educational assessment)
plant population demography analysis see
demography rate analysis (tree study)
point process frequency-domain model 730,
732â€“3
Poisson process models 272
bivariate application example see train door
reliability analysis (bivariate Poisson
process model)
Poisson-lognormal model (multivariate)
application example see malaria mapping
for repairable systems 286â€“92
zero-inï¬‚ated Poisson (ZIPo) model 469
policy-making, DSGE models and 370, 371, 391
pollution exposure assessment see
environmental exposure assessment
Polya trees 233â€“5
mixtures of (MPT) 228â€“31, 233â€“5
Polya urn representation 22â€“3, 146â€“7; see also
Chinese Restaurant representation
portfolio optimization 349â€“50, 360
power law process (PLP) 278, 288â€“9, 293
prediction market volatility (political campaign
information ï¬‚ow measurement)
computation context and background 617â€“20
conclusions 615â€“16

886
Index
prediction market volatility (cont.)
future directions 617
introduction xxiv, 597â€“8
model description 604â€“6
political prediction markets 599â€“602
US presidential election 2004 xxiv, 597â€“9,
606â€“16
volatility, trading volume and information
ï¬‚ow 602â€“8
pregnancy outcome study
density regression 33â€“8
ï¬nite mixture models 7â€“10
ï¬‚exible Bayesian regression 7â€“24
introduction xvii, 3â€“7
Kernel stick-breaking process 12â€“13
nonparametric Bayes methods 10â€“12
nonparametric Bayes mixure modelsâ€™ context
and background 20â€“2
posterior computation 22â€“3
standard regression approachâ€™s
limitations 4â€“7, 19
Prescott, E. 367â€“8
principal components analysis 418
principal stratiï¬cation 680, 689, 694â€“7, 706
principal variables 247â€“9
prior information 676
probabilistic context-free grammars
(PCFGs) 777â€“8, 780â€“81
hierarchical Dirichlet processes and see
natural language processing (syntactic
parsing problem)
probabilistic pathway annotation (PROPA)
analysis 142â€“4, 150â€“1
probability, Bayes deï¬nition of 346
probability axioms, derivation of 359â€“60
probability theory, origins of 343
probit stick-breaking process (PSBP) 22
Procrustes analysis 41â€“2, 44
Program for Climate Model Diagnosis and
Intercomparison (PCMDI) 547, 548â€“9, 562
Programme for International Student
Assessment (PISA), cultural differences
and see differential item functioning (DIF)
(in educational assessment)
propositions, proof of 235â€“8
protein bioinformatics study
active sites 28â€“9
advantages of Bayesian approach 40
alternative approaches 41â€“2
data analysis 36â€“40
data sources 28â€“9, 47â€“8
future directions 42â€“3
introduction to xvii, 27â€“30
methodology reï¬nements 40â€“1
model formulation and inference 44â€“6
model implementation 46â€“7
multiconï¬guration alignment model 33â€“6
pairwise matching model 30â€“3
shape analysis context and background
43â€“4
protein level oscillations see stochastic kinetic
model of p53 and Mdm2 oscillations
PSB databank 47â€“8
quantile regression 5
random effect terms 90, 96, 109, 503
Rayleigh quotient 353
regression
density 33â€“8
dynamic linear models 112
elicitation for 514â€“20
logistic 3â€“7, 8, 19
sparse multivariate application example see
oncogene pathway deregulation and breast
cancer
reliability analysis
gas pipelines 289â€“91
train door see train door reliability analysis
repairable systems
Poisson process models for 286â€“92
reliability analysis see train door reliability
analysis
replication 682
resample-propagate ï¬lter 330â€“1
response and missingness, joint model
59â€“61
reversible jump Markov chain Monte Carlo
(RJMCMC)
audio and music processing 722, 725â€“8,
731â€“3
ï¬nite mixture models 10
Poisson process models 289
treed Gaussian processes and 826, 842
risk modelling, ï¬nancial futures markets xxi,
352â€“8, 359
rock-wallaby habitat modelling and mapping
(indirect elicitation from experts)
elicitation design 523, 534â€“7
elicitation method 514â€“23, 538â€“40
encoding 523â€“7, 537
introduction xxiiiâ€“xxiv, 511â€“12
opinion collation 527â€“31
outcome discussion 531â€“4
problem introduction 511â€“14
root mean square deviations (RMSD) 32, 44
Rubinâ€™s causal model (RCM)
application example see causal inference
(cancer survival and treating hospital type
analysis)
framework 680â€“9
observational studies background 705â€“6

Index
887
sampling, adaptive 826â€“8, 832â€“3
Savageâ€“Dickey density ratio 332
scientiï¬c philosophy, Bayesian analysis and
changes in xv
SCOP 48
seasonal models 112
selection model 60
semiparametric model 225, 228â€“31
sensitivity analysis
microbial risk assessment see microbial risk
assessment sensitivity analysis (VTEC 0157
study)
paternity testing 197â€“8
sequential multilocation auditing 671â€“4, 675
treed Gaussian processes 828â€“9, 835, 836
of univariate simulator 759â€“60
variance-based 80â€“1, 82, 84â€“6, 828â€“9
Sequential Design and Analysis of Computer
Experiments (SDACE) 841
Sequential Design of Experiments (DOE) 841
sequential estimation 329â€“33, 336
sequential meta-analysis 53â€“8, 66
sequential Monte Carlo (SMC) methods
for DSGE models 369, 391â€“3
for Markov process models 181, 182
for multi-AR models 856, 865
see also particle ï¬ltering
sequential multilocation auditing (New York
food stamps program application)
auditing concepts and terminology 654â€“5
dynamic model context and
background 676â€“7
error classes modelling 660â€“1
error rates modelling 659â€“60
introduction xxv, 653â€“8
New York food stamps program 653â€“4, 655,
666â€“75
notation and outline 657â€“8
previous work review 656â€“7
prior information context and
background 676
projection 662â€“5
sensitivity analysis 671â€“4, 675
updating 661â€“2
wider applicability 675â€“6
serum ï¬broblast cell cycle pathway gene
list 142â€“3
shape analysis xvii, 28, 43â€“4; see also protein
bioinformatics
short tandem repeat (STR) markers 193, 210
shotgun stochastic search (SSS) 133â€“5, 150
signal-to-noise ratio 303, 306â€“9
similarity index 31
singular forward ï¬lter backward sample
(SFFBS) 298, 305, 316
SitesBase 48
Sloan Digital Sky Survey (SDSS) 749â€“52
South Texas Project (STP) Electric Generation
Station 219â€“20
sparse multivariate regression analysis 122â€“3,
124, 147
sparsity priors 123, 147
spatial correlation 90â€“1, 107â€“9
spatial data Bayesian software 504
spatial Gaussian process models 503â€“4
application example see environmental
exposure assessment
spatial interpolation
environmental exposure assessment 494â€“5,
496, 498â€“9, 501
malaria mapping 100â€“1, 104, 106
spatial random effects 503
spatio-temporal models
application example see malaria mapping
dynamic linear 114
malaria mapping with see malaria mapping
(spatio-temporal mixture model)
multiscale see multiscale spatio-temporal
model (for agricultural production
analysis)
SpBayes 504
Special Report on Emissions Scenarios
(SRES) 414, 547, 548â€“9, 550
speculation 348â€“50, 358â€“9
stable unit treatment value assumption
(SUTVA) 682â€“3, 691
Standard and Poorâ€™s S&P500 see ï¬nancial
market volatility analysis (2007â€“2008 credit
crisis)
standardization, internal 94, 109
standardized mortality ratio (SMR) 94â€“6
state-space models see dynamic/state-space
models
steady model 111
steroid molecules, matching multiple
conï¬gurations of 29, 38â€“40
Stochastic Human Exposure and Dose
Simulation (SHEDS) model 485
stochastic kinetic model of p53 and Mdm2
oscillations
data 161â€“2
emulator construction 167, 182â€“4
inference from multiple cells 173â€“80
inference from single cell data 170â€“3, 174
introduction xix, 155â€“60
linking model to data 162â€“6
Markov process modelsâ€™ context and
background 181â€“2
model 160â€“1
model extension possibilities 180â€“1
parameter inference difï¬culties 181â€“2
posterior computation 166â€“70

888
Index
stochastic singularity problem 380
stochastic volatility (SV) model 320, 324, 325â€“6,
331, 332â€“8, 339
in audio and music processing 734
see also Markovian process models,
Markov-switching stochastic volatility
model
stochastic volatility with jumps (SVJ)
model 320, 324, 326â€“8, 331, 332â€“8
STP Nuclear Operating Company
(STPNOC) 219â€“20
successive corrections method (SCM) 486â€“7
survival
causal inference and see causal inference
(cancer survival and treating hospital type
analysis)
models 133â€“40, 150, 448
systems biology application example see
stochastic kinetic model of p53 and Mdm2
oscillations
Systems Biology Markup Language
(SBML) 158â€“9
Tamoxifen 144
Taylor series representation 111
temporal prediction 101, 103â€“4, 105
temporal sure preference principle 269
three-parameter logistic model 630â€“3
time domain audio models 721â€“8
Time to Build and Aggregate Fluctuations 367â€“8
Tor Fjeldy photocurrent model 824
total effect variances 85â€“6
Total Risk Integrated Methodology (TRIM)
model framework 485
Tradesports/Intrade 598, 600â€“1, 606
train door reliability analysis (bivariate Poisson
process model)
data quality problems 272, 285â€“6
exploratory data analysis 273â€“6
failure forecasting 272, 281â€“5
intensity function 275â€“8, 281, 285,
287â€“92
introduction xxâ€“xxi, 271â€“3
likelihood, prior and parameter
estimation 278â€“81
model development possibilities 285
Poisson process models for repairable
systems 286â€“92, 293
warranty parameter assessment 272â€“3,
281â€“5
transform analysis 339â€“40
transform domain models see
frequency/transform domain models
translation problem, in-vitro to in-vivo 118,
119â€“20
treatment effect 62â€“6
tree demography rate analysis see demography
rate analysis (tree study)
treed Gaussian processes (TGP) 824â€“6, 839â€“41
adaptive sampling 826â€“8
application example see circuit device
experiment design and analysis
future directions 839
optimization 829â€“31, 834â€“5, 837â€“8
potential applications 838â€“9
sensitivity analysis 828â€“9, 835, 836
uncertainty, aleatory and epistemic 73â€“4, 86â€“7,
408
uncertainty analysis
in complex computer models see microbial
risk assessment sensitivity analysis (VTEC
0157 study)
for complex physical system based on system
model see oil reservoir simulation
uncertainty analysis
extreme outcome risk quantiï¬cation
see meridional overturning circulation
(MOC) collapse probability analysis
linear see oil reservoir simulation uncertainty
analysis;
universe, analysis of see cosmological
parametersâ€™ estimation (-cold dark matter
(CDM) model)
US economy analysis see new
macroeconometric US economy dynamics
analysis (DSGE model)
US Environmental Protection Agency
(EPA) 485
US presidential election (2004), information
ï¬‚ow measurement see prediction market
volatility (political campaign information
ï¬‚ow measurement)
variational/Bayes inference 742â€“5
approximate posterior inference algorithm
for HDP-PCFG 787â€“801, 807â€“14,
814â€“19
AR and TVAR models 867â€“8
comparison with Markov chain Monte Carlo
sampling 807â€“8
DP mixture models representation
and 808â€“9
mean-ï¬eld inference and DP-based
models 809â€“12
and prior interaction 812â€“14
Vero-cytotoxigenic E. coli (VTEC) 0157 study see
microbial risk assessment sensitivity
analysis (VTEC 0157 study)
Vioxx trials study
APPROVe study 52, 54â€“5, 58â€“66
background to xviiâ€“xviii, 51

Index
889
cardiovascular adverse eventsâ€™ deï¬nition
54
non-ignorable dropout 58â€“66, 66â€“7
placebo-controlled trials list 52
sequential meta-analysis 53â€“8, 66
VIGOR trial 55, 56
Viterbi algorithm 793
VIX index 319, 321, 322â€“5
volatility see ï¬nancial market volatility analysis
(2007â€“2008 credit crisis); prediction
market volatility (political campaign
information ï¬‚ow measurement)
VXN index 319, 321, 322â€“5
Wall Street Journal, Penn Treebank 798â€“800
warranty parameter assessment 272â€“3,
281â€“5
wavelet decomposition 296, 298, 313
weather prediction
climate forecasting and 559
numerical approaches 486â€“8
Weibull survival models 133â€“40, 150
Wilkinson Microwave Anisotropy Probe
(WMAP) 749, 765â€“6, 768â€“80
WinBUGS 504, 522
wish fulï¬llment 601, 602
XLF index see ï¬nancial market volatility analysis
(2007â€“2008 credit crisis)
Xyce 824, 834
zero-inï¬‚ated Poisson (ZIPo) model 469
âˆ’cold dark matter (CDM) model see
cosmological parametersâ€™ estimation
(-cold dark matter (CDM) model)

