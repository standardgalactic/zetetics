Regulative development as a model for origin of life and artificial life studies
Chris Fieldsa 1 and Michael Levina,b 
2
aAllen Discovery Center at Tufts University, Medford, MA 02155 USA
bWyss Institute for Biologically Inspired Engineering at Harvard University, Boston, MA 02115, USA
Abstract:  Using the formal framework of the Free Energy Principle, we show how generic 
thermodynamic requirements on bidirectional information exchange between a system and its 
environment can generate complexity.  This leads to the emergence of hierarchical computational 
architectures in systems that operate sufficiently far from thermal equilibrium.  In this setting, the 
environment of any system increases its ability to predict system behavior by “engineering” the system 
towards increased morphological complexity and hence larger-scale, more macroscopic behaviors.  
When seen in this light, regulative development becomes an environmentally-driven process in which 
“parts” are assembled to produce a system with predictable behavior.  We suggest on this basis that life 
is thermodynamically favorable and that human engineers are acting like a generic “environment” 
when designing artificial living systems.
Keywords:  Free energy principle; Kinematic replication; Learning; Multicellularity; Multiscale 
competency architecture; Target morphology
1. Introduction
Living systems are hierarchical arrangements of components that, critically, exhibit a multiscale 
competency architecture (MCA; Levin, 2022; Clawson and Levin, 2022).  In an MCA, components at 
each scale are competent to perform the functions appropriate to that scale without explicit top-down 
instructions.  Human cells, for example, do not have to be told how to divide by the brain, or by any 
larger-scale system; they are competent to divide on their own.  At any scale j in an MCA, the 
behaviors of the components at the lower scale j-1 provide enabling mechanisms for j-appropriate 
behaviors, while the behaviors of the components at the higher scale j+1 impose boundary conditions.  
As the boundary conditions effective constrain the possibility space at scale j, state changes at scale j+1
1
ORCID: 0000-0002-4812-0744
2
ORCID: 0000-0001-7292-8084

can modulate or deform the possibility space at scale j, providing a source of top-down novelty.  Cell 
division, for example, is enabled by gene expression within the cell, and constrained by signaling from,
or transmitted via, the surrounding multicellular tissue.  Top-down “instructions” from the surrounding 
or even distal (McMillan et al., 2021) tissue can increase or decrease the rate of cell division, e.g. 
during wound healing; other examples include the modulation of cancer cell properties by the 
microenvironment (Bissell, 2002; Ingber, 2008; Bizzarri and Cucina, 2014), control of stem cell fate by
large-scale axial patterning cues during regeneration (Durant et al. 2017), and local remodeling of a 
structure based on its larger anatomical context (Farinella-Ferruzza, 1956).  Such upward flows of 
enabling mechanisms and downward flows of constraining boundary conditions have been identified as
characteristic of living systems by Polanyi (1968), Rosen (1986), Pezzulo and Levin (2016), and Fields 
and Levin (2020a) among others.  In the language of cognitive or computer science, an MCA 
encapsulates the competencies required for scale-appropriate behaviors at the scale where they are 
appropriate.  This encapsulation of competencies allows processes specific to each scale to function as 
virtual machines (Smith and Nair, 2005) independently of the implementations of either lower- or 
higher-scale components.  Such implementation-independence opens up the possibility of “mix and 
match” systems that combine evolved biological, engineered biological, and artificial components in 
almost arbitrary ways (Levin, 2022; Clawson and Levin, 2022).
Making the idea of an MCA precise requires having a precise formulation of what counts as a 
“competence.”  A fully-precise formulation is provided by the variational Free Energy Principle (FEP): 
a system is “competent” to the extent that it minimizes the variational free energy (VFE) of its 
environment as measured at the Markov blanket (MB) that separates it statistically from (i.e. renders its
states conditionally independent from the states of) its environment. The VFE measured by a system is 
an upper bound on surprisal, and therefore on the unpredictability of the actions of its environment on 
its MB.  As a modeling framework, the FEP has the advantage of provable generality for physical 
systems with well-defined states spaces and states that are conditionally-independent of the states of 
their environments, i.e. systems that have MBs.  Since its introduction as a theory of brain function 
(Friston, 2005; Friston, Kilner, and Harrison, 2006; Friston, 2010), the FEP has developed into a 
general theory of living systems (Friston, 2013; Friston et al., 2017; Ramstead, Badcock and Friston, 
2018; Ramstead, Constant, Badcock and Friston, 2019; Kuchling, Friston, Georgiev and Levin, 2020), 
and has more recently been formulated as a general theory of classical (Friston, 2019; Ramstead et al., 
2022) and quantum (Fields, Friston, Glazebrook, and Levin, 2022) physical systems.  Key to this 
generality is that the FEP does not specify how a system minimizes its detected VFE, consistent with 
the general idea that “competence” involves not just an ability to solve problems, but an ability to solve
the same problem in different ways or by different means in different contexts.  Minimizing VFE is 
maximizing the effectiveness, in terms of future predictability, of its actions on its environment via its 
MB; such effective action is called “active inference” in the FEP literature.  Hence a “competent” 
system is one that acts on its environment in a way that increases its future predictive power.  As 
discussed in more detail below, actions that maintain homeo/allostasis are examples of such effective 
action, as are exploratory actions that gain useful information.
By providing a scale-free, thermodynamic definition of competence, the FEP enables a scale-free 
biology that treats evolution and development as manifestations of a single process – VFE 
minimization – at different levels of organization (Fields and Levin, 2020a; b).  It is natural to extend 
this conception of “biology” to encompass prebiotic, abiotic, or exobiotic scenarios of biological 
relevance.  Here we propose that development and regeneration, the regulative morphogenetic 
processes that enable multicellular systems to reach “normal” target morphology despite significant 
perturbations or altered starting conditions (Birnbaum and Alvarado, 2008; Levin, 2011; Vandenberg, 
Adams and Levin, 2012; Lobo, Solano, Bubenik and Levin, 2014; Pezzulo and Levin, 2016; Pinet and 

McLaughlin, 2019; Fields and Levin, 2020b) provides a model for origin of life and artificial life 
studies.  
Here we investigate a particular instance of scale-free biological explanation using the definition of 
“competence’’ provided by the FEP.  Consider two examples of biological self-organization:
Regulative development: Using information available from their surrounding environment(s), some 
number of cells self-organize into a multicellular organism.  The result could also be a facultative 
multicellular system like a microbial mat.
Ab initio self organization: Some number of molecules self-organize into a cell.  The result could also 
be a self-sustaining proto-cell as in some origin-of-life models.
Regulative development has been studied for over two centuries, and many model systems are now 
understood at a substantial level of detail including, in several cases, cell-specific gene expression 
profiles (Tintori et al., 2016; Farnsworth, Saunders and Miller, 2020; Wang et al., 2022).  Examples of 
ab initio self organization, on the other hand, remain hypothetical.  While a variety of models have 
been proposed, mostly in an origin-of-life context, none have been fully implemented experimentally 
and there is little consensus even about biological plausibility.  The goal of any scale-free explanation is
to tell exactly the same kind of story about these two examples.  Constructing such an explanation by 
requiring examples of ab initio self organization to be both formal and mechanistic analogs of 
regulative development raises a number of issues, mainly concerning the structure and role of the 
“environment” in ab initio self organization, that have been relatively neglected by previous 
approaches.
We will see, in particular, that treating ab initio self organization as an analog of regulative 
development challenges two deeply-entrenched ideas.  First, it questions the near-universal assumption 
that any ab initio model must result in a self-replicating system.  Evolutionary models from Darwin 
onwards have strongly coupled variation with inheritance through reproduction (Monod, 1972; 
Szathmáry and Maynard Smith, 1995; Michod, 1999).  Natural selection acts on variants, and hence 
depends in Darwinian models on reproduction.  Such models suggest ab initio processes that involve 
self-replicating molecules immediately.  We will suggest that models in which the needed “parts” are 
generated by environmental processes that are at most weakly coupled to the systems of interest are 
also worth consideration.  Variation generated by weakly coupled processes is consistent with evolution
at the global scale, but does not depend on natural selection at any any single local scale; it thus 
provides a “Non-Darwinian” source of order.  Such weak coupling is exemplified by situations 
involving self-organizing systems that include engineered and manufactured components; we suggest 
that weak-coupling models may be realistic in other settings as well.
Second, treating ab initio self organization as an analog of regulative development challenges the very 
idea of self organization.  In its purest form, the idea of self-organization suggests that the information 
needed for organization is present in the self.  This immediately raises the essentially unanswerable 
question of how this critical information got there to begin with.  Directly relating ab initio self 
organization to regulative development forces us to ask, at each step of the process, what systems count
as “selves,” how the environment of each “self” is defined, and how, in each case, the exchange of 
information between “self” and environment is implemented (Levin, 2021).  
Our analysis suggests two broad conclusions;

1.  Reproduction is an efficiency hack, which evolution “froze in” by selecting strong self/other 
recognition systems starting in bacteria.  Decoupling replication from self-organization generalizes the 
systems of interest to developmental studies toward ``egalitarian’’ (Strassmann and Queller, 2010) 
assemblages of unrelated, or in the evolved case only distantly related, components that “just happen” 
to work well together.  This allows simpler origin of life stories.  It acknowledges the possibility of 
kinematic replication in biological systems, as observed experimentally in “xenobots” constructed from
dissociated Xenopus laevis skin cells (Kriegman, Blackiston, Levin and Bongard, 2021).   It suggests 
that symbiotic systems such as microbial mats are “canonical” to at least the same same extent as 
obligately sexual multicellulars.  The focus of both ab initio and developmental studies becomes, in 
this case, how components interact once they are placed in mutual proximity, independently of their 
origins.
2.  "Self-organization" is always environment dependent, so we can view it as at least in part 
environment-directed: some of the instructive information is initially in the environment, not in the self 
that assembles.  From the perspective of physical interaction (Fields and Marcianò, 2020; Fields, 
Glazebrook and Marcianò, 2021) or of the FEP (Friston, 2019; Ramstead et al., 2022; Fields, Friston, 
Glazebrook, and Levin, 2022) this is obvious.  It is also obvious in many origin of life models, in 
microbial mat assembly, and in embryogenesis, even in “European plan” organisms like C. elegans 
(Barrière and Bertrand, 2020).  It is, nonetheless, often neglected or de-emphasized, particularly in 
theoretical work.
In what follows, we first characterize in §2 the interaction between any system and its environment 
from the general perspective employed in Fields, Friston, Glazebrook, and Levin (2022).  While any 
such interaction is informationally symmetric in principle – equal quantities of information flow in both
directions – what the two parties do with the information they receive may be radically different.  
Symmetric information flows, in other words, are consistent with “cognitive light cones’’ (Levin, 2021;
2022) of different widths and depths, and hence different active inference capabilities, on the two sides 
of the system-environment boundary.  We turn in §3 to an explicit comparison between regulative 
development and ab initio self organization, focusing first on characterizing the environments of each 
active component as information sources, and hence as themselves active agents, at each step in the 
process.  We investigate, in particular, how both regulative development and ab initio self-organization 
decrease the VFE detected by the environment, and hence are thermodynamically-driven processes 
under the FEP.  We consider in §4 how the developing “self” represents itself, and how this process 
depends on memory and computational resources.  We show that the complexity of the self 
representation as a data structure scales with the volume of the system’s cognitive light cone, discuss 
the energy and memory resource tradeoffs between maintaining this data structure and engaging in 
active inference.  We distinguish in §5 between passive training and active learning, and point out how 
informational symmetry renders all realistic cases mixtures of the two.  We conclude by outlining a 
“mix and match” experimental strategy suggested by these results in §6.
2. Informational symmetry is consistent with cognitive asymmetry
2.1 Physical interactions are informationally symmetric
The FEP applies to all systems, classical or quantum, that can be distinguished from their environments
over macroscopic times (Friston, 2019; Ramstead et al., 2022; Fields, Friston, Glazebrook, and Levin, 
2022).  Note that “environment” here refers to everything that is not the system of interest. 
Distinguishability or separability of system from environment, and vice-versa, requires the presence of 

a boundary in the joint system-environment state space that assures conditional statistical independence
(and hence zero entanglement entropy) between system states and environment states.  This boundary 
functions as an MB, which may, but is not necessarily, implemented by a spatial boundary such as a 
cell membrane (Kirchhoff et al., 2018).  The system-environment interaction is defined at this 
boundary.  The (binary) dimension of this interaction is the number N of bits in the minimal-length 
binary encoding of its largest eigenvalue.  The dimension N of the interaction is also the dimension of 
the state space of the boundary; indeed, any such boundary can be represented as an array of N 
quantum bits (Fields and Marcianò, 2020; Fields, Glazebrook and Marcianò, 2021; Fields, Friston, 
Glazebrook, and Levin, 2022).   The system-environment interaction can, therefore, always be regarded
as transferring N bits bidirectionally between system and environment; these N bits specify, at any 
microscopic (with respect to the characteristic time of the interaction) time interval dt, the eigenvalue 
of the interaction during the interval dt.  All physical interactions are, in other words, informationally 
symmetric; there are no physically-implemented one-way information flows.
Information and thermodynamic energy are interconvertable by Landauer’s principle (Landauer, 1961; 
1999; Bennett, 1982), and direct information-to-energy conversion has been demonstrated 
experimentally (Toyabe et al. 2010; Bérut et al, 2012; see Parrondo, Horowitz and Sagawa, 2015 for 
review).  Any physical interaction that transfers information, therefore, transfers thermodynamic 
energy.  Processing information to obtain an answer that has an effect on subsequent behavior, i.e. 
irreversible classical computation (Horsman, Stepney, Wagner and Kendon, 2014), has a finite free 
energy cost, again given by Landauer’s principle.  Hence any system that uses information from its 
environment to alter its behavior – including, clearly, any living system – must devote some of the 
information/energy obtained from its environment to the free-energy cost of classical computation.  The
amount of energy obtained from the environment thus provides a strict upper bound on the amount of 
classical computation (in bits per unit time) a system can perform (Fields and Levin, 2021).  We can, 
therefore, represent any system-environment interaction as a bidirectional exchange of information 
across a boundary, in which some of the information is devoted, by each party, to the free-energy 
requirements of classical computation, as illustrated in Fig. 1.

Fig. 1:  a) A system S interacts with its environment E; the Hamiltonian operator H
SE
 
representing the interaction is defined at the boundary B that functions as an MB 
statistically separating S from E.  The environment E comprises everything that is not 
the system S.  b) When the boundary B is viewed from S’s perspective, in can be divided
into incoming (red) and outgoing (blue) bits.  Some of these bits (gray shading) must be 
allocated to the thermodynamic functions of free-energy input and waste-heat 
dissipation; these bits are uninformative, i.e. unavailable for computational processing.  
The remaining bits are available as computational inputs (sensations) and outputs 
(actions).  The situation is the same from E’s perspective, though the allocation of bits to
informative and uninformative sectors may be different.  Adapted from Kuchling, Fields
and Levin (2022), CC-BY license.
The informational symmetry of physical interactions has an important consequence in the context of 
the FEP: the environment E of any system S must itself be considered an agent.  The FEP requires that 
this agent E engages in active inference, i.e. that it acts on its environment – the system of interest S – 
so as to minimize the VFE that it, E measures at its MB.  As the MB of each system is just the (state 
space) boundary between them, the FEP requires that any pair of interacting systems behave in such a 
way that both minimize the VFE that they measured at their mutual boundary.  In the limit in which the 
S-E interaction is purely thermodynamic, i.e. neither S nor E engages in any classical computation, both
minimize measured VFE by approaching thermodynamic equilibrium.  If either S or E engages in 
classical computation, VFE minimizing solutions are in general not at thermodynamic equilibrium, but 
are rather nonequilibrium steady state (NESS) solutions for both S and E (see Friston, 2019 for 
extensive details).  Such solutions respect the 2nd Law from the perspectives of both component 
systems: S and E each absorb free energy from and exhaust waste heat into their interaction partner, i.e.
E and S respectively.  Hence each system “sees” the entropy of its interaction partner increase.  The 
entropy of the total system SE is its total information content, which is strictly conserved and hence can
be rescaled to zero (cf. Tegmark, 2012 for a discussion of entropy in this bipartite-decomposition 
setting).
2.2 Informational symmetry is consistent with thermodynamic asymmetry
Let us now assume that S is capable of nontrivial information processing.  In this case, S implements 
some nontrivial function f: s → a, where s is the instantaneous state of its informative (unshaded in Fig.
1) input sector and a is the instantaneous state of its informative output sector.  The FEP requires this 
function to be, on average, either a gradient descent or a solenoidal flow on the statistical manifold that 
defines S’s measured VFE as a function of E’s state (Friston, 2019; Ramstead et al., 2022).  The 
function f must, in other words, on average be VFE non-increasing for S. 
The function f is implemented by the internal dynamics of S, which we can represent as a Hamiltonian 
operator H
S
 on the state space of S.  The internal dynamics H
S
 must, therefore, respect the 
thermodynamic asymmetry, for S, between the informative (s, a) sector of B and the uninformative, 
thermodynamic sector.  The internal dynamics H
E
 of E, however, are by construction conditionally 
independent of H
S
.  Hence if E also implements nontrivial information processing, there is no 
requirement, and indeed no expectation, that the boundary between informative and uninformative 

sectors for E aligns with that assigned by H
S
.  Nor is there any requirement or expectation that S and E 
have the same, or even similar, efficiencies of free energy usage or waste heat dissipation.  What one 
interaction partner treats as an informative signal, either incoming or outgoing, can be treated by the 
other as uninformative “noise,” i.e. as waste heat output or free energy input.  Meaningful 
communication across B requires, first and foremost, overcoming this thermodynamic asymmetry.  As 
we will see, morphology – effectively, geometrization of B – provides one way of doing this.
2.3 State measurements are reference-frame dependent
To understand what S can measure, and hence what S can “know” about E, it is useful to reflect on how
we make measurements, as scientists in laboratories.  Consider measuring lengths with a meter stick 
that has 1 mm resolution.  The meter stick physically encodes a length standard – the meter – and also 
physically encodes, by means of permanent, unmovable tick marks, the possible outcome lengths that it
can measure, each separated from the others by half of the measurement resolution.  The standard 
encoded by the meter stick is arbitrary, but by encoding it, the meter stick allows measurements of 
different objects at different times to be compared.  The comparison process depends on two 
assumptions: 1) that the length of the meter stick, and hence what “1 meter” means, does not change, 
and 2) that the number and positions of the tick marks, traditionally called “pointer positions” do not 
change.  Physically encoding these two assumptions makes the meter stick a reference frame (RF); it is 
useful as a reference to which other lengths may be meaningfully compared.  An RF is simultaneously 
a physical object and a semantic object: it assigns to a measurement not just an outcome value, but also 
a meaning.
Human observers make ubiquitous use of reference frames, including laboratory equipment but also 
such non-artifacts as the diurnal cycle and the Earth’s gravitational field.  Making use of an external 
RF, however, always requires observation, and these observations must be mutually comparable to be 
meaningful.  Hence employing an external RF requires encoding an internal RF.  Humans and other 
organisms encode internal spatial, temporal, vibrational, chemical, and electromagnetic RFs, with RFs 
for the cell cycle, membrane voltage, and various chemical concentrations being some of the most 
ancient (Fields and Levin, 2020c; Fields, Glazebrook and Levin, 2021).  Internal RFs solve the 
“problem of meaning” (Froese and Taguchi, 2019) for organisms.
Internal RFs are, clearly, embedded in complex systems, e.g. organisms.  In the language used above, 
any internal RFs of a system S are implemented by H
S
.  This suggests that internal RFs may not be 
fully characterizable by external observations.  This is, in fact, the case in principle: all RFs, whether 
internal or external, are physically implemented by systems that must, at least at microscopic scales, be 
considered quantum systems.  They are, therefore, quantum RFs (QRFs; Aharonov and Kaufherr, 1984;
Bartlett, Rudolph and Spekkens, 2007) and encode “nonfungible” information, that is, information that 
cannot be fully specified by any finite string of classical bits (Bartlett, Rudolph and Spekkens, 2007).  
This has the consequence that QRFs cannot be shared by sharing bit strings, i.e. by classical 
communication, but only by being transferred as unique physical systems (Fields and Marcianò, 2019). 
Internal QRFs are, therefore, not sharable in principle.  Indeed in the limit in which two distinct 
systems approach sharing an internal QRF, they become entangled and lose their identities as distinct 
systems (Fields, Friston, Glazebrook, and Levin, 2022). 

With this language, we can distinguish three types of measurements that a system S could, in principle, 
make.   The first is to implement a QRF Q that acts on the entire input component B
inf
(in) of the 
informative sector B
inf
 of its boundary, as shown in Fig. 2a.  The action of Q yields an encoding q of 
the instantaneous state of B
inf
(in).  As the number of bits encoded on B, or on any sector of B, is 
proportional to its area, the free energy cost of implementing Q on B
inf
(in) can be written as:
                                                             E
Q
 =  A
inf
(in) β
Q
 k
B
T                                                               (1)
where A
inf
(in) is the area of B
inf
(in), β
Q
 ≥ ln2 is a factor measuring the energetic efficiency of Q, k
B
 is 
Boltzmann’s constant and T is temperature (Fields and Marcianò, 2019).
Fig. 2: a) A system S implements a QRF Q that acts on the entire input component 
B
inf
(in) of the informative sector B
inf
 of its boundary, yielding an encoding q of the state 
of B
inf
(in).  b) S implements QRFs Q
1
 and Q
2
 that act only on sectors Q
1
 and Q
2
 of 
B
inf
(in), yielding encodings q
1
 and q
2
 of the states of those sectors.  c)  S implements 
QRFs P (“pointer”) and R (“reference”) that act on sectors P and R of B
inf
(in), yielding 
encodings p and r of the states of those sectors, subject to the restriction that r remains 
fixed.

Assuming fixed efficiency β = β
Q
, S can save free energy by measuring only particular sectors of 
B
inf
(in), as shown in Fig. 2b.  The state information obtained is, in this case, limited to the sector states 
q
1
 and q
2
; this limitation reflects the inevitable trade-off between energy expenditure and information 
gain.  From a computational perspective, however, S can now measure, using further specialized QRFs,
the correlation < q
1
 , q
2
 > and the conditional dependencies ( q
1
 | q
2
 ) and ( q
2
 | q
1
 ).  These further 
computations enable basic logic functions (AND, OR, NOT) and, given a memory for prior 
probabilities, Bayesian inference (Fields and Glazebrook, 2022).  Hence sector measurements enable 
explicitly hierarchical computation.
A specialized form of sector measurement is shown in Fig. 2c.  Here the sector R is required to 
maintain a fixed state r; hence measured states of the sector P have the form p = ( p | r).  The disjoint 
union PR, in this case, functions as an external RF with fixed reference component R and pointer 
component P; the “state of interest” of PR is its “pointer” state p.  All perceivable objects, or in the 
language of Friston (2019), all time-persistent things, have this composite form: the invariant reference 
sector R allows identification of the object as “the same thing” that was measured earlier, while the 
variable pointer sector P displays the state of interest.  Again items of laboratory apparatus, which must
be distinguished from their surroundings by measuring such observables as size, shape, color, and 
position before the informative “pointer” readouts can be determined, provide a canonical example.
2.4 Hierarchical measurements are ubiquitous at all scales
Turning now to biology, it is clear that measurements of the forms shown in Fig. 2b and 2c are 
ubiquitous, while measurements of the form shown in Fig. 2a are rare and possibly nonexistent.  
Measuring and processing the entire detectable state of the environment at every instant is 
computationally challenging and energetically expensive; organisms instead focus their resources on 
what is salient and significant, deploying different sector-specific QRFs at different times.  Allocating 
resources to the salient and significant is the function of attention systems (Fields, Glazebrook and 
Levin, 2021).
While both object perception and comparative measurements are familiar in animals with large brains, 
they are less well characterized in smaller-brained or aneural animals, plants, and unicellulars.  All 
organisms capable of identifying and behaving in state-specific ways toward mates or conspecifics, 
however, are capable of some level of object identification, and hence of Fig. 2c measurements.  This 
capability extends, therefore, into the microbial world.  All organisms capable of conditioning their 
behavior on the values of two variables simultaneously, e.g. on sugar and salt concentration, or on 
ambient light and the availability of water, are capable of Fig. 2b measurements.  This capability 
extends, therefore, even deeper into the microbial world, and to our knowledge characterizes all of life.
The same considerations apply at the intracellular scales.  Enzymes that require cofactors or 
phosphorylation as well as a substrate are performing Fig. 2b measurements on their environments, as 
are membrane-bound channels, pumps, and  receptors that are sensitive to both local membrane voltage
and binding of GTP or other regulatory molecules.  RNA polymerases that bind DNA and then scan for 
specific transcription-initiation sites (Kuehner and Brow, 2006) are performing Fig. 2c measurements 
on their environments: first identifying a DNA molecule as an object, and the detecting a particular 
state – a particular base-sequence motif – of that object.  Spatially-organized intracellular pathways are 

increasingly recognized as performing complex, multi-input computations, both in neurons (Gidon et 
al., 2020) and in non-neural cells (Kramer, del Castillo and Pelkmans, 2022), as long argued by the 
basal cognition movement on the basis of both biochemical and behavioral data (Maturana and Varela, 
1980; Pattee, 1982; Stewart, 1996; di Primio, Müller and Lengeler, 2000; Lyon, 2015; Baluška and 
Levin, 2016; Baluška & Reber, 2019; Levin, 2019; Lyon, 2020).
2.5 Hierarchies of QRFs are MCAs
We have, in the above, focused on measurements, i.e. on processing the bits encoded on the incoming 
side of the informative sector B
inf
(in) of the S-E boundary.  Actions on E can, however, be viewed as 
“preparations” of the bits encoded on the outgoing side of the informative sector B
inf
(out).  Preparing or
setting a bit value is dual, as a process, to measuring its value (Pegg, Barnett and Jeffers, 2002).  These 
processes are, therefore, informationally symmetric; indeed any QRF can be viewed as preparing, as 
well as measuring, the values of some degree of freedom.  A meter stick, for example, can be used to 
mark 30 cm lengths as well as to measure them.  This informational symmetry is manifest when QRFs 
are represented as category-theoretic structures – formally, limits and colimits – of bidirectional 
operators on single strings of bits, a representation that is provably completely general (Fields, 
Glazebrook and Marcianò, 2022). 
If all QRFs are informationally symmetric, the mapping f: s → a implemented by the internal dynamics
H
S
 must be informationally symmetric.  The informational symmetry of H
SE
 then requires that the 
thermodynamic functions of free energy acquisition and waste heat dissipation must be informationally
symmetric.  The dynamics H
S
 can, therefore, be divided into two informationally symmetric flows, one
of which transfers free energy to and dissipates waste heat from the other.  We can, therefore, redraw 
Fig. 1b as Fig. 3.  The analogy with information and free energy flows in chemical reactions is obvious.
Fig. 3: The mapping f: s → a implemented by S’s QRFs is supported by an 
informationally-symmetric thermodynamic flow from the uninformative 
(thermodynamic) sector B
th
(in) to the corresponding output sector B
th
(out).

The mapping f: s → a is, at each instant, implemented by some hierarchy of QRFs on both input and 
output sides; we have previously showed, as an example, how cortical neurons implement QRF 
hierarchies (Fields, Glazebrook, and Levin, 2022).  Which QRFs are deployed at any instant determines
a measurement context (Fields and Glazebrook, 2022; Fields, Friston, Glazebrook, and Levin, 2022); 
switching between contexts is a metaprocessing function which, as a component of f: s → a must itself 
be implemented by a (fixed) QRF hierarchy (Kuchling, Fields and Levin, 2022).  Each QRF in the 
hierarchy is a self-contained computational system with its own inputs, outputs, power supply, and 
semantics.  The QRF hierarchy is, therefore, a canonical MCA.
3. Self organization is thermodynamically driven
3.1 The “environment” is an active agent
We are now in a position to use these conceptual tools to understand self organization both within a 
given scale and, more interestingly, as a thermodynamically-driven process that generates complexity 
at progressively larger scales.  This will allow us to connect origin-of-life models to evolutionary and 
developmental models within a single formal framework, and to understand how the transition from 
protocellular to cellular and then multicellular systems is driven by the FEP.
The FEP has already been shown to provide a generic model of within-scale self organization 
(Kirchhoff et al., 2018; Friston, 2019).  To briefly summarize, VFE minimization corresponds to 
maintaining a state close to the NESS.  This is achieved when internal processes – i.e. the internal 
dynamics H
S
 – predicts the environment’s actions on the MB sufficiently accurately to maintain the 
integrity of the MB and hence the conditional independence of internal states.  This is referred to as 
“self-evidencing” (Friston, 2019) and corresponds, in biological systems, to the maintenance of 
homeo/allostasis (Friston, 2013).  The internal dynamics H
S
, and in particular, the QRF hierarchy that it
implements, can in this case be regarded as a generative model of the behavior of E, specifically, of the 
action of E on the MB.
We have also shown previously how producing “copies” of itself that cluster in the local environment is
a viable strategy for a system to reduce VFE (Fields and Levin, 2019).  The copies shield the system 
from the open environment, reducing its unpredictability.  The behavior of the copies is similar to the 
behavior of the system, thus increasing predictability.  While this model was formulated for biological 
cells and shows how multicellularity can be advantageous from the perspective of the FEP, it applies at 
larger scales as well, with ethnic, linguistic, or religious communities and social-media “echo 
chambers” as obvious examples.
Here we take a different, but complementary perspective.  The “environment” surrounding an open 
system is typically treated as passive: as a thermodynamic or material-exchange resource, an ambient 
field, or simply a heat bath.  In the FEP literature, the environment is often just a source of uncertainty. 
For biological systems, this is clearly unrealistic: the environment of an organism consists largely of 
other organisms, both conspecifics and others.  Similarly, the environment of a biomolecule consists 
largely of other biomolecules, the environment of a population consists largely of other populations, 
etc.  These “others” are active agents pursuing their own agendas, as made explicit in game-theoretic 
models.

The FEP not only allows, but when viewed in full generality requires this game-theoretic perspective.  
The FEP applies to all systems with MBs, and describes all such systems as VFE-minimizing agents.  
The environment of any system shares an MB with that system, as made explicit in Fig. 1a.  The 
environment E of any system S is, therefore, a VFE-minimizing agent.  The sole source of VFE for E is
S; any generative model implemented by E is, therefore, a generative model of S’s actions on its MB.  
To assume that E’s generative model is random – that E functions only as a resource – is thus to assume
a very special case, one that is largely irrelevant to biology.  Realistic environments are active agents, 
just as more typical systems of interest are.
3.2 The environment acts so as to increase the system’s predictability
When we consider E to be an active agent, the goal of its actions becomes clear: E acts to decrease its 
measured VFE; hence it acts to increase the predictability, by its generative model, of S’s behavior.  In 
particular, E acts to increase the predictability of S’s actions on its MB.  Note that from E’s perspective,
S’s behavior becoming increasingly predictable corresponds to S “losing freedom” and hence to S’s 
entropy, relative to E, increasing as discussed in § 2.2 above.  The considerations of the previous 
section apply equally to S and to E; to understand E’s actions on S, we must consider E’s information-
processing capabilities, i.e. its hierarchy of QRFs.  It is, in particular, important to understand whether 
E processes S’s actions as informative inputs and vice versa (Fields, Friston, Glazebrook, and Levin, 
2022).
A model in which S inserts “copies” of itself into E (Fields and Levin, 2019) is effectively a model in 
which E deploys QRFs that “make sense” of S’s actions, as illustrated in Fig. 4a – 4c.  From S’s 
perspective, the copies are components of E – or more precisely, the behaviors of the copies are 
components of the behavior of E, as measured by S at its MB – that are at least partially predictable and
hence “make sense” to S.  As the number of copies increases from zero (Fig. 4a) to one (Fig. 4b) to 
many (Fig. 4c), the behavior of E as a whole becomes progressively more predictable by S.  Hence it is 
advantageous, from an FEP perspective, for S to insert copies of itself into E (Fields and Levin, 2019). 
The same, however, is true for E: as E gains copies of S and hence incorporates their QRFs, the 
behavior of S becomes more predictable.  As noted earlier, this symmetric dynamic becomes obvious 
when we consider the QRFs that implement human language understanding or other sociocultural 
practices.  A company E can predict the behavior of its customer S much better if it has employees C 
that speak S’s language.
We have thus far considered the interaction H
SE
 and the boundary B between S and E.  We are, 
however, free to pick any boundary in the joint system SE we like, provided only that it functions as an 
MB in rendering the states of two systems that it separates mutually conditionally independent.  Let us 
suppose, therefore, that the boundary B’ shown in Fig. 4d meets this condition.  Drawing this boundary 
defines a system S’ – which includes S – that is inside B’ and an environment E’ that is outside B’.  The 
two interact, at B’, via an interaction H
S’E’
.  All of the previous considerations apply to this new 
interaction.
We can now consider the consequences of S inserting copies of itself into S’, i.e. into the interior 
defined by the boundary B’, as shown in Fig. 4e and 4f.  Doing this has the consequences for S’s 
predictive capability discussed above.  Here, however, we will be interested in its consequences for the 
predictive capabilities of S’ and E’.

When the boundary is moved from B to B’, some degrees of freedom of E become degrees of freedom 
of S’; hence E’ has fewer degree of freedom, and therefore less computational power, than E.  Any 
QRFs of E that directly measured bits encoded on B, in particular, are lost in the transition from E to 
E’.  However, because B’ by construction functions as an MB, the states of S’ are conditionally 
independent of the states of E’.  In this case, E factors as E = E’ · (S’\S), where “\” denotes set, or more 
properly state-space subtraction.  Any QRFs implemented by S’\S that measure states of B, therefore, 
can be viewed as writing their outcome values on B’.  We can, therefore, view B’ as a coarse-graining 
of B.  Indeed, this construction implements the idea of “MBs within MBs” (Kirchhoff et al., 2018).
Fig. 4: a) A system S interacts with its environment E as in Fig. 1a.  b) S inserts a “copy”
C of itself into E.  The copy brings new QRFs to E.  c) S inserts multiple copies of itself 
into E, as in the model of Fields and Levin (2019).  d) A boundary B’ can be drawn 
anywhere in E.  Provided the states inside and outside B’ are mutually conditionally 
independent (i.e. B’ functions as an MB), drawing this new boundary defines new 

systems S’ (inside B’, containing S) and E’ (the remainder of E).  e) S adds a copy C of 
itself to S’.  f) S adds multiple copies of itself to S’.
A particular example of the above construction is familiar, and has been studied in detail.  Suppose S is 
a system of interest, E’ is an observer, e.g. a human observer, and S’\S is an ambient field, e.g. the 
ambient visible-frequency photon field.  In this case, the QRFs implemented by S’\S that measure B are
light-scattering interactions that encode properties of S, e.g. size or shape.  The scattered light impacts 
E’ at the boundary B’, transferring the encoded information about S to E’.  The resolution of the 
encoding is decreased by a factor proportional to the ratio of the areas of B and B’; the transfer process 
thus implements a coarse-graining.  This coarse-graining information transfer process is, clearly, simply
the standard classical-optics mechanism of visual perception.  Such environment-mediated 
measurement has been studied in general under the rubric of “quantum Darwinism” (Zurek, 2003; 
2009).  It depends critically on boundaries that impose conditional independence at both B and B’.
Given this conditional independence condition, we can write H
E
 = H
E’
 + H
S’\S
, where H
S’\S
 implements
QRFs transferring information between B and B’ and H
E’
 implements QRFs outside B’.  Drawing the 
boundary B’ in E, in other words, has no effect on H
E
 if B’ functions as an MB that “cuts” E’s QRF 
hierarchy at a set of vertices of E’s QRFs.  The component E’ therefore “knows” exactly the same 
things about S in Fig. 4a and Fig. 4d; Fig. 4d simply makes the boundary B’ explicit.  The same 
equality must, in this case, hold between what E’ knows about S in Fig. 4b and 4e, and in Fig. 4C and 
4f.  What is going on inside B’ does not change in any of these pairs of situations.
What, then, is the effect of the copies C filling up S’\S from the point of view of E’?  What is the 
difference, for E’, between Fig. 4d and 4f, or between Fig. 4a and 4c?  The copies C add their own 
QRFs to S’\S.  These QRFs transfer information about S to E’, and also transfer information about E’ to
S as discussed in Fields and Levin (2019).  The area, and hence the coding capacity, of B’ does not 
change as these additional QRFs are inserted into S’\S.  The boundary B’ must, therefore, encode a 
convolution, which for simplicity we can take to simply be an average, of the outcomes written by the 
QRFs contributed by the multiple copies C.  These outcomes are likely to be similar, since the C are all 
copies of S, but they in general will not be identical.  Hence in general, B’ implements a further coarse-
graining of information about S in Fig. 4f compared to 4d, or between Fig. 4a and 4c.  
Coarse-graining decreases uncertainty because it decreases the resolution of measurements; it encodes 
relatively low-resolution “macrostates” in place of high-resolution microstates (Hoel, Albantakis and 
Tononi, 2013; Hoel, 2017).  From the perspective of E’, therefore, filling up S’\S with copies C of S 
decreases VFE, and therefore increases predictive power.  It increases predictive power more if the 
copies C are diverse, as the coarse-graining imposed by B’ will “wash out” the details written by the C 
more if they are writing different outcomes.  Hence we have a rather surprising conclusion:
The FEP will drive the peripheral environment E’ around any system S to act on S 
so as to enable or facilitate the insertion of diversified “copies” of S into S’s 
immediate environment.

The FEP, in other words, drives the environment around any “interesting” system to enable both the 
replication of that system and the self-organization of the replicates into a “body” surrounding the 
system.  Self-organization is, therefore, environmentally-driven under the FEP.  
3.3 Environmental driving facilitates both the origin of life and its diversification
The surprising conclusion above becomes unsurprising when we consider it in the case of either animal
or plant reproduction: germ calls are intentionally enclosed in microenvironments – seeds, eggs, a 
uterus – that facilitate their replication and the self-organization of an embryo.  The “correct” 
microenvironment is generally essentially to the success of the self-organization process.
The same is obviously true in bioengineering and artificial life contexts; here the “environment” in the 
form of the human experimenter provides both the materials and the microenvironment required for 
self organization.  Here as in all curiosity-driven experimentation, the goal of the experimenter-as-
environment is to increase future predictive power, i.e. to decrease future VFE.
Bioengineering and artificial life contexts highlight the environment’s ability to “take over” the task of 
replication, providing the copies C that are needed and sometimes actually inserting them into the 
immediate microenvironment of the system.  In the case of kinematic replication of xenobots, for 
example, the environment provides the needed supply of dissociated X. laevis skin cells (Kriegman, 
Blackiston, Levin and Bongard, 2021).  The environment also provides the “parts” in naturally-
occurring cases of affiliative aggregation, from Dictyostelium sporulation or the formation of 
multispecies microbial mats to the replication of symbiotic systems, including all holobionts (Guerrero,
Margulis and Berlanga, 2013; Gilbert, 2014; Bordenstein and Theis, 2015).
Viewed more broadly, the environment provides the parts in every case of biological or biochemical 
replication, in the form of molecular subunits to be assembled by an essentially kinematic process.  
Replication of DNA – the fundamental “replicator” in the gene-centric neo-Darwinist view of both 
evolution and development (Dawkins, 1984) – is an environmentally-driven process: the environment 
provides the nucleic acids, the enzymes, the free energy, and the biochemically and thermodynamically 
stabilized compartment required for the kinematic process.  Why?  From an FEP perspective, the 
environment does these things to increase future predictability.  Making more of the same kind of 
molecule generates a more predictable future state than making a random assortment of molecules.
The environment similarly provides both parts and stabilized microenvironment in origin-of-life 
models (Cornish-Bowden and Cárdenas, 2017; Bartlett and Wong, 2020).  The FEP suggests that it 
does this for the same reason that it does this in the case of DNA replication or organismal 
reproduction: to increase its future predictive power.  Living systems localize, organize, and coarse-
grain information.  From the point of view of the environment, this compartmentalization reduces VFE.
4. Active inference and self representation compete for energy and memory resources
4.1 Temporally-extended cognitive light cones require memory for events
We have thus far discussed intersystem information exchange and intrasystem information processing 
without explicitly considering the memory resources required for robust agential behavior.  We can 
distinguish, in particular, between the memory required to process current observations or execute 
current actions, and the memory required to maintain records of previous observations.  The former is, 

effectively, encoded by the structures of the deployed QRFs; in the limit of purely quantum 
computation, it has zero free-energy cost (Bennett, 1982; Parrondo, Horowitz and Sagawa, 2015).  It 
can thus be considered implicit, procedural, or semantic memory.  The latter may be implemented as an
explicit, declarative record: a literal copy of a previously-obtained outcome value.  It may also be 
implemented procedurally, as human episodic memory appears to be (Nadel et al., 2012; Schwabe, 
Nader and Pruessner, 2014), but the result of executing this procedure is an encoding of a reconstructed
outcome value.  Either way, each remembered event has a free energy cost of: 
                                                             E
M
 =  A
M
 β
M
 k
B
T                                                               (2)
where A
M
 is the number of bits and hence the effective area of the encoding and β
M
 ≥ ln2 is the 
efficiency of the memory.  When S is treated as a quantum system, the only locus on which classical 
information can be encoded is the boundary B (Fields, Glazebrook and Marcianò, 2021; Fields, Friston,
Glazebrook, and Levin, 2022); hence A
M
 is the area on B devoted to encoding event memories.  Note 
that encoding on B exposes event memories to the environment E; additional free energy may be 
expended (corresponding to a larger value of β
M
) to protect the memories from environmental 
disruption, e.g. “forgetting” or modification.  Both of these processes are familiar in the case of human 
episodic memories.
Planning future actions requires event representation and hence explicit memory resources with costs 
given by Eq. 2 (Boyer, 2008).  Representation of distant, unobserved events has similar requirements.  
Hence any organism with a spatially or temporally extended cognitive light cone (Levin, 2021; 2022) 
must devote free energy resources to maintaining event memories.  As all free energy must be sourced 
through B
th
(in), this requirement imposes a trade-off between event memory and real-time perception 
and action processing, i.e. active inference.  Coarse-graining event memories to decrease their energetic
cost provides a partial solution to this trade-off (Fields, Friston, Glazebrook, and Levin, 2022), one that
is clearly employed by human episodic memory.
4.2 An explicit self representation depends on event memory
Implicit in the notion of an agent is the idea of a self representation.  Here again, we can distinguish a 
purely implicit, procedural representation of the self from an explicit representation.  Restriction 
enzymes in bacteria or immune systems in mammals are examples of the former, the commonplace 
psychological notion of the “self” is an example of the latter.  The psychological self is widely regarded
as a construct (Blackmore, 2002; Metzinger, 2004; Graziano & Webb, 2014), a view for which there is 
now significant neurofunctional evidence (Craig, 2002; 2010; Seth, 2013; Seth and Tsakiris, 2018).  As 
in the case of constructed memories, encoding a constructed self representation requires free energy as 
specified by Eq. 2.
In humans, the self is represented as both observing and enacting remembered and planned events, and 
also as both being affected by, and affecting other via distant unobserved events.  The cognitive light 
cone is, in other words, the domain of the represented self.  The volume of the cognitive light cone 
places an upper limit on the number of past, present, and future event memories, which in turn place an 
upper limit on the number of distinct contexts in which the self can be represented.  Hence a human-

like self – the kind of self on which our human intuitions of selfhood and agency are based – depends 
critically on event memory, though see Strikwerda-Brown et al. (2019) for a discussion of 
compensation mechanisms between different representations of current, past, and future events.  
The free-energy cost of maintaining the self representation imposes a trade-off between maintaining the
self as a “character” in multiple remembered contexts and other uses of memory, including event-
memory access by generative models subserving real-time active inference.  Hence high real-time task 
demands can be expected to “turn off” the self representation, as in fact observed in “flow” states 
(Csikzentmihalyi, 2014) and other automated expert performance (Bargh and Ferguson, 2000), and at 
the neurofunctional level in default-mode suppression by the external task network (Chen et al., 2013), 
particularly in tasks that do not require self-relevant processing (Andrews-Hanna, Smallwood and 
Spreng, 2014).
These memory-associated tradeoffs are summarized in Fig. 5, with familiar systems as examples.  The 
question of greater interest for the present purposes, however, is where the environments, or even the 
total proximal environments, of cells, organisms, or other systems of biological interest would fall in 
these tradeoff spaces.  The environment of a typical human, for example, has tremendous event-
memory capacity, a capacity that steadily increases as a the built environment, and particularly the built
information environment, continues to expand.  How much of this memory is encoded explicitly on the 
boundary between a typical human and her environment?  To what extent are the environment’s 
memories exposed to modification?  How much of this capacity is devoted to representing the 
environment to itself as a self?  While are used to asking and answering such questions when they 
concern organisms, they sound unfamiliar or even meaningless when asked about environments.  The 
informational symmetry of the FEP requires us to take them seriously.
Fig. 5: a) Trade-off space for low versus high areal ratio of self-representation to non-
self event memory sectors (A
S
 /A
M
, vertical axis) and low versus high areal ratio of 
event memory to real-time processing (A
M
 /A
RT
, horizontal axis).  Artificial systems are
generally less energy-efficient than living systems.  Mammalian brains devote more 
resources to event memories than do individual cells.  ANN = Artificial Neural Network.

b) Trade-off space for low versus high areal ratio of informative to thermodynamic 
sectors (A
inf
 /A
th
), i.e. low versus high thermodynamic efficiency (vertical axis) and low 
versus high areal ratio of event memory to real-time processing (A
M
 /A
RT
, horizontal 
axis).  The upper-left quadrant devotes insufficient resources to non-self aspects of 
context in event memory to support a robust self representation.  
4.3 All minds are flat
Based on data from both cognitive psychology and cognitive neuroscience, Chater (2018) concludes 
that the human mind is “flat” – that there is only one kind of percept, motivation, plan, or belief, the 
ordinary experienced kind.  All such “mental states” include interpretative semantic content.  The mind,
in Chater’s model, is a cyclic process that generates semantic interpretations from perceptual or 
memory inputs.  The interpretations are available to consciousness; the processes that generate them are
not.
The present model is completely consistent with this “flat” view of minds, or more generally, of 
cognition (Fields and Glazebrook, 2020).  Indeed, it requires it on first principles.  No finite system, i.e.
no system that consumes only finite energy, can allocate sufficient classical memory to record, during 
some time interval, a complete execution trace of its internal processes during that time interval.  
Digital computers, for example, record only abstractions – hence coarse-grainings – of their execution 
traces.  This limitation on self representation can be seen as a consequence of Gödel’s (1931) 1st 
incompleteness theorem, which rules out complete self-description by any formal system with the 
expressive power of basic arithmetic.  For quantum systems, this limitation is even stricter, as every 
state transition, in principle, transfers nonfungible information.  Any system, therefore, operates on a 
Chater-like cycle in which generative processes cannot be fully recorded, and hence cannot be accessed
as classical information.  We can summarize this fundamental limitation as: no system can fully 
characterize its own QRFs.
Working in the context of the FEP, it is natural to regard all of the information of which a system is 
aware as being recorded on it boundary B (Fields, Glazebrook and Levin, 2021; Fields, Friston, 
Glazebrook and Levin, 2022).  In this case, Chater’s flat conception of cognition follows immediately.  
A system with sufficient resources can record and report a metacognitive abstraction – by definition a 
coarse-graining – of its internal processes, but any such report is an interpretation dependent on the 
capabilities of the metaprocessor (Kuchling, Fields and Levin, 2022).  This abstraction is part of the 
instantaneous self representation.  These considerations apply equally both to any system and to its 
environment.
The flat conception of mind, with its in-principle limitations on the self representation, has an 
important consequence:
No system can fully predict is own future behavior.
No system can, in particular, predict its own response to a novel input, or to a familiar input in a novel 
context.  This result has significant consequences for real-time cognition, and more broadly, for 
learning, regulative development, and evolution.
4.4 Thermodynamics generates the Frame Problem

The existence of an MB, i.e. a boundary B separating S from E prevents S, by definition, from directly 
measuring E’s internal dynamics.  Any generative model G that S implements is, therefore, in principle 
only a model of E’s actions on B, not a model of E’s internal dynamics.  By using G to predict E’s 
future actions on B, S is, effectively, using G to predict E’s internal dynamics.  The pair (B, G) thus 
constitutes an interface on E for S in the sense defined by Hoffman, Singh and Prakash (2015); the 
model G, in particular, specifies the “icons” that appear on the “screen” defined by B.  Replacing G with
a different model G’ puts different icons, which may have more or less manipulative capability and 
confer more or less predictive power, on the interface.
A central result of the Interface Theory of Perception ( Hoffman, Singh and Prakash, 2015) is that 
models G that results from processes of natural selection generically fail to fully reproduce either the 
structure or the dynamics – e.g. the underlying symmetries – of the environment.  While this result was 
originally demonstrated within a game-theoretic setting involving resource competition (Prakash et al., 
2020; 2021), here we can see it from a deeper perspective.  The existence of B prevents S from 
measuring the dynamics of E, so S can learn these dynamics, effectively, only if E communicates them 
via its behavior.  We have seen above, however, that E cannot encode its own dynamics; hence a 
fortiori it cannot encode them on B.  Therefore S cannot learn E’s dynamics by being told, or indeed by 
any learning process mediated by B.
This result can be recast in terms of the Frame Problem (FP, McCarthy and Hayes, 1969), a canonical 
problem in Artificial Intelligence (AI).  The FP is standardly stated as the problem of specifying, using 
some representation more compact than an exhaustive, explicit list, the properties of the environment 
that will not change as a result on an action.  This is, clearly, equivalent to the problem of predicting 
what will not change by a generative model G.  We see from the above, however, not only that S cannot
achieve such a prediction, but that E cannot achieve such a prediction either.  The FP is known to be 
unsolvable, i.e. finite Turing undecidable; an FP solution would imply a solution to the unsolvable 
Halting Problem (Dietrich and Fields, 2020).  We see here that it follows from the existence of an MB 
separating S from E.  It is, therefore, a fundamentally thermodynamic result.
5. Learning is neither completely passive nor fully autonomous
Supervised learning via algorithms such as error back-propagation (Rumelhart, Hinton and Williams, 
1986) remains the dominant paradigm in machine learning (ML), particularly for deep learning systems
in applied settings.  Organisms in the wild, in contrast, are generally regarded as learning by 
unsupervised, autonomous exploration.  Social learning and training in domesticated settings are 
viewed as lying somewhere between these extremes.
The informational symmetry of the FEP suggests that both fully-passive training and fully-autonomous 
exploration are unrealistic as models of systems embedded in and physically interacting with real, as 
opposed to merely formal, environments.  The objective of training is to produce predictable behavior 
by an initially unpredictable system.  Training is, in other words, a method of reducing VFE.  In 
practice, however, it is never completely successful.  Even in the case of fully-specified formal systems
such as ANNs, training yields unpredictable generative models; this unpredictability is the source of the
explanation problem in AI (XAI, Samek et al., 2021; Taylor and Taylor, 2021).  While this failure is 
generally considered technological, we can see from the above that it has a deep, thermodynamic 
source.  Training an ANN by supervised learning involves evaluating its responses to items in the 

training set as either correct or incorrect, and then back-propagating an error signal.  This evaluation 
employs a generative model G of the experimenter/trainer, who is the effective environment of the 
ANN being trained.  The behavior of the system after training is then tested against this same G.  Hence
even given perfect knowledge of the post-training algorithm implemented by the ANN, its correlation 
with G and hence its test behavior will remain unpredictable because G itself is unpredictable.  The XAI
problem is hard, in part, because the problem of predicting our own future behavior is hard.
We mention these AI problems not only because they carry over to engineered systems in general, but 
because they characterize natural biological processes as well.  The environment can be viewed as 
“training” developing or evolving systems via selection processes as described in §3 above.  This 
process is not fully predictable, even by the environment itself.  All systems, regardless of internal 
structure or dynamics and regardless of the environment in which they are embedded, remain 
unpredictable both by their environments (including by us an environmentally-embedded observers) 
and by themselves.  Hence all systems have some degree of autonomy, if autonomy is defined as lack 
of predictability.  No systems, however, have full autonomy, in the sense of independence from or 
imperviousness to training by their environments.  This result is reminiscent of the “free will theorem” 
in physics, which states that under broadly plausible assumptions, if any systems are unpredictable 
given all information in their past light cones (here their physical, not cognitive light cones), then all 
systems have this property (Conway and Kochen, 2009).  This is, at bottom, a consequence of 
conditional independence, i.e. a thermodynamic consequence of the existence of the separating 
boundary B.
6. Conclusion
We have seen here how the FEP provides a generic model of MCAs that applies equally to natural and 
engineered systems and equally to short and long timescales.  Indeed the FEP erases the distinction 
between natural and engineered systems.  Because the FEP characterizes the environment of any 
system of interest as an agent, the environment can always be regarded as “training” or “engineering” 
the system.  The ubiquitous role of the environment in providing the parts required for replication, 
whether of DNA molecules, cells, evolved or constructed organisms, or completely artificial, abiotic 
systems, demonstrates this engineering aspect.  When the environment is seen as an engineer, it 
becomes clear that “self organization” is always environment-assisted self organization.  The product of
any such process, the FEP tells us, serves to decrease the environment’s measured VFE, and hence in 
an important sense serves the environment’s goals.
This view of the environment and its interactions with living systems significantly broadens the usual 
concept of what is “normal” or “typical” in biology.  Xenobots and chimeras become exemplars, not 
oddities.  It also becomes clear that the environment encodes “target morphology” in the form of VFE 
reduction criteria all the way down.  In both evolution and development, and in origin of life scenarios, 
the environment assembles a bunch of likely parts to see what happens.  Life is a successful outcome of
an experiment performed by the environment.
Why would an environment assemble parts to create a living system, and then assist in its replication?  
The FEP suggests a simple answer: the environment is an agent that creates novelty in order to see 
what information it can get in return.  The environment is a typical active inference agent.  It behaves 
like any such agent behaves.

This way of thinking suggests an experimental strategy that has been pursued, but never systematically:
it suggests dissociating embryos or other collections of cells, of various different kinds, mixing them 
together in diverse, "multicultural" populations, and seeing how they behave in various environments.  
Can we make xenobot-like systems, for example, that are multi-origin chimeras?  Can we make de 
novo symbiotic complexes, analogous to lichens, that have parts from very different lineages?  
Experiments along these lines would, in effect, be a kind of “recombinant biology” analogous to 
standard genetic engineering, but carried out with cells, not genes.  The success of recombinant 
genetics suggests that recombinant biology may work for cells in some “right” kinds of environments.  
The outcomes of such experiments could substantially increase the diversity of life beyond that 
supplied thus far by evolution.
Conflict of Interest: The authors state that they have no conflicts of interest relevant to the reported 
results.
Funding:  The work of CF is partially supported by the John Templeton Foundation (grant # TBA).   
ML gratefully acknowledges support by the John Templeton Foundation (grant # 62212).  
References
Aharonov, Y. and Kaufherr, T. (1984) Quantum frames of reference. Phys. Rev. D 30, 368-385.
Andrews-Hanna, J. R., Smallwood, J. and Spreng, R. N. (2014) The default network and self-generated
thought: Component processes, dynamic control, and clinical relevance.  Ann. N. Y. Acad. Sci. 1316(1),
29-52. https://doi.org/10.1111/nyas.12360
Baluška, F. and Levin, M. (2016) On having no head: Cognition throughout biological systems. Front. 
Psychol. 7: 902.
Bargh, J. A. and Ferguson, M. J. (2000) Beyond behaviorism: On the automaticity of higher
mental processes. Psychol. Bull. 126, 925-945.
Baluška, F. and Reber, A. (2019) Sentience and consciousness in single cells: How the first minds 
emerged in unicellular species. BioEssays 41, 1800229.
Barrière, A. and Bertrand, V. (2020) Neuronal specification in C. elegans: combining lineage 
inheritance with intercellular signaling.  J. Neurogenet. 34, 273-281.  
https://doi.org/10.1080/01677063.2020.1781850
Bartlett, S. D., Rudolph, T. and Spekkens, R. W. (2007) Reference frames, super-selection rules, and
quantum information. Rev. Mod. Phys. 79, 555-609.
Bartlett, S. and Wong, M. L. (2020) Defining Lyfe in the Universe: From three privileged functions to 
four pillars. Life 10, 42.

Bennett, C. H. (1982) The thermodynamics of computation. Int. J. Theor. Phys. 121, 905-940.
Bérut, A., Arakelyan, A., Petrosyan, A., Clberto, S., Dllenschneider, R. and Lutz, E. (2012) 
Experimental verification of Landauer’s principle linking information and thermodynamics.  Nature 
483, 187-189.  https://doi.org/10.1038/nature10872
Birnbaum, K. D. and Alvarado, A. S. (2008)  Slicing across kingdoms: Regeneration in plants and 
animals.  Cell 132, 697-710.
Bissell, M. J.; Radisky, D. C., Rizki, A., Weaver, D. M. and Peterson, O. W. (2002) The organizing 
principle: Microenvironmental influences in the normal and malignant breast.  Differentiation 70, 537-
546.
Bizzarri, M. and Cucina, A. (2014) Tumor and the microenvironment: A chance to reframe the 
paradigm of carcinogenesis?  Biomed. Res. Int. 2014, 934038. 
Blackmore, S. (2002) There is no stream of consciousness. J. Cons. Stud. 9, 17-28.
Bordenstein, S. R. and Theis, K. R. (2015) Host biology in light of the microbiome: Ten principles of 
holobionts and hologenomes. PLoS Biol. 13(8), e1002226.
Boyer, P. (2008) Evolutionary economics of mental time travel. Trends Cogn. Sci. 12, 219-224.
Chater, N. (2018) The Mind Is Flat.  Allen Lane, London.
Chen, A. C., Oathes, D. J., Chang, C. and Etkin, A. (2013) Causal interactions between fronto-parietal 
central executive and default-mode networks in humans. Proc. Natl. Acad. Sc. USA 110, 19944-19949. 
https://doi.org/10.1073/pnas.1311772110
Clawson, W. and Levin, M. (2022) Endless forms most beautiful 2.0: Teleonomy and the 
bioengineering of chimaeric and synthetic organisms, Biol. J. Linnean Soc. 2022, blac073. 
https://doi.org/10.1093/biolinnean/blac073
Conway, J. H. and Kochen, S. (2009) The strong free will theorem. Notices AMS 56, 226-232.
Cornish-Bowden, A. and Cárdenas, M. (2017) Life before LUCA. J. Theor. Biol. 434, 68-74.
Csikzentmihalyi, M. (2014) Flow and the Foundations of Positive Psychology. Springer,
Dordrecht.
Craig, A. D. (2002) How do you feel? Interoception: the sense of the physiological condition of the 
body. Nat. Rev. Neurosci. 3, 655-666.
Craig, A. D. (2010) The sentient self. Brain Struct. Funct. 214, 563-577.
Dawkins, R. (1984) Replicators and vehicles. In: Brandon, R. N. and Burian, R.M. (eds)  Genes, 
Organisms, Populations: Controversies Over the Units of Selection. Cambridge, MA: The MIT Press, 
pp. 161-180.

di Primio, F., Müller, B. F. and Lengeler, J. W. (2000) Minimal cognition in unicellular organisms. In: 
Meyer, J.A., Berthoz, A., Floreano, D., Roitblat, H. L. and Wilson, S. W. (Eds), From Animals to 
Animats. International Society for Adaptive Behavior: Honolulu, HI, USA, pp. 3-12.
Dietrich, E. and Fields, C. (2020) Equivalence of the Frame and Halting problems. Algorithms 13, 175. 
https://doi.org/10.3390/a13070175
Durant, F., Morokuma, J., Fields, C. Williams, K., Adams, D. S. and Levin, M. (2017) Long-term, 
stochastic editing of regenerative anatomy via targeting endogenous bioelectric gradients. Biophys. J. 
112, 2231-2243.
Fan, X., Tang, D., Liao, Y., Li, P., Zhang, Y., Wang, M. et al. (2020) Single-cell RNA-seq analysis of 
mouse preimplantation embryos by third-generation sequencing.  PLoS Biol. 18, e3001017. 
https://doi.org/10.1371/journal.pbio.3001017
Farinella-Ferruzza, N. (1956) The transformation of a tail into limb after xenoplastic transplantation. 
Experientia 12, 304-305.
Farnsworth, D. R., Saunders, L. M. and Miller, A. C. (2020) A single-cell transcriptome atlas for 
zebrafish development.  Devel. Biol. 459, 100-108. https://doi.org/10.1016/j.ydbio.2019.11.008
Fields, C., Friston, K., Glazebrook, J. F., Levin, M. (2022) A free energy principle for generic quantum 
systems. Prog. Biophys. Mol. Biol. 173, 36-59.  https://doi.org/10.1016/j.pbiomolbio.2022.05.006
Fields, C. and Glazebrook, J. F. (2020) Do Process-1 simulations generate the epistemic feelings that 
drive Process-2 decision making? Cogn. Proc. 21, 533-553.  https://doi.org/10.1007/s10339-020-
00981-9
Fields, C. and Glazebrook, J. F. (2022) Information flow in context-dependent hierarchical Bayesian 
inference. J. Expt. Theor. Artif. Intell. 34, 111-142 https://doi.org/10.1080/0952813X.2020.1836034
Fields, C., Glazebrook, J. F. and Levin, M. (2021) Minimal physicalism as a scale-free substrate for 
cognition and consciousness. Neurosci. Cons. 2021, niab013.  https://doi.org/10.1093/nc/niab013
Fields, C., Glazebrook, J. F. and Levin, M. (2022) Neurons as hierarchies of quantum reference frames.
BioSystems 219, 104714. https://doi.org/10.1016/j.biosystems.2022.104714
Fields, C., Glazebrook, J. F. and Marcianò, A. (2021) Reference frame induced symmetry breaking on 
holographic screens. Symmetry 13, 408. https://doi.org/10.3390/sym13030408
Fields, C., Glazebrook, J. F. and Marcianò, A. (2022) Sequential measurements, topological quantum 
field theories, and topological quantum neural networks.  Fortschr. Physik 70, in press.  
https://doi.org/10.1002/prop.202200104
Fields, C. and Levin, M. (2019) Somatic multicellularity as a satisficing solution to the prediction-error
minimization problem. Commun. Integr. Biol. 12, 119-132. 
https://doi.org/10.1080/19420889.2019.1643666

Fields, C. and Levin, M. (2020a) Scale-free biology: Integrating evolutionary and developmental 
thinking.  BioEssays 2020, 1900228.  https://doi.org/10.1002/bies.201900228
Fields, C. and Levin, M. (2020b) Does evolution have a target morphology? Organisms 4, 57-76. 
https://doi.org/10.13133/2532-5876/16961
Fields, C. and Levin, M. (2020c) How do living systems create meaning? Philosophies 5, 36.  
https://doi.org/10.3390/philosophies5040036
Fields, C. and Levin, M. (2021) Metabolic limits on classical information processing by biological 
cells. BioSystems 209, 104513. https://doi.org/10.1016/j.biosystems.2021.104513
Fields, C. and Marcianò, A. (2019)  Sharing nonfungible information requires shared nonfungible 
information. Quant. Rep. 1, 252-259.  https://doi.org/10.3390/quantum1020022
Fields, C. and Marcianò, A. (2020) Holographic screens are classical information channels. Quant. Rep.
2, 326-336.  https://doi.org/10.3390/quantum2020022
Friston, K. J. (2005) A theory of cortical responses. Philos. Trans. R. Soc. Lond. B, Biol. Sci.
360, 815-836.
Friston, K. J. (2010) The free-energy principle: A unified brain theory? Nature Rev. Neurosci. 11, 127-
138.
Friston, K. J. (2013) Life as we know it. J. R. Soc. Interface 10, 20130475.
Friston, K. J. (2019) A free energy principle for a particular physics. Preprint arXiv:1906.10184 [q-
bio.NC].
Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P. and Pezzulo, G. (2017) Active inference: A
process theory. Neural Comput. 29, 1-49.
Friston, K. J., Kilner, J. and Harrison, L. (2006) A free energy principle for the brain. J. Physiol. (Paris)
100, 70-87.
Froese, T. and Taguchi, S. (2019)  The problem of meaning in AI and robotics: Still with us after all 
these years.  Philosophies 4, 14. https://doi.org/10.3390/philosophies4020014
Gidon, A., Zolnik, T. A., Fidzinski, P., Bolduan, F., Papoutsi, A., Poirazi, P., Holtkamp, M., Vida, I. and 
Larkum, M. E. (2020) Dendritic action potentials and computation in human layer 2/3 cortical neurons.
Science 367, 83-87.
Gilbert, S. F. (2014) Symbiosis as the way of eukaryotic life: The dependent co-origination of the body.
J. Biosci. 39, 201-209.
Gödel, K. (1931) Über formal unentscheidbare sätze der Principia Mathematica und verwandter 
systeme. I. Monatsh. Math. Phys. 38(1), 173-198.

Graziano, M. S. A. and Webb, T. W. (2014) A mechanistic theory of consciousness. Int. J. Mach. Cons. 
6(2), 1-14.
Guerrero, R., Margulis, L. and Berlanga, M. (2013) Symbiogenesis: The holobiont as a unit of
evolution. Int. Microbiol. 16, 133-143.
Hoel, E. P. (2017) When the map is better than the territory. Entropy 19, 188.
Hoel, E. P., Albantakis, L., Tononi, G. (2013) Quantifying causal emergence shows that macro can beat 
micro. Proc. Natl. Acad. Sci. USA 110, 19790-19795.
Hoffman, D. D., Singh, M. and Prakash, C. (2015) The interface theory of perception. Psychon. Bull. 
Rev. 22, 1480-506.
Horsman, C., Stepney, S., Wagner, R. C. and Kendon, V. (2014) When does a physical system 
compute? Proc. R. Soc. A 470, 20140182.
Ingber, D. E. (2008) Can cancer be reversed by engineering the tumor microenvironment? Semin. 
Cancer Biol. 18, 356-364. 
Kirchhoff, M., Parr, T., Palacios, E., Friston, K. and Kiverstein, J. (2018) The Markov blankets of life: 
autonomy, active inference and the free energy principle. J. R. Soc. Interface 15, 2017.0792.  
https://doi.org/10.1098/rsif.2017.0792
Kramer, B. A., del Castillo, J. S. and Pelkmans, L. (2022) Multimodal perception links cellular state to
decision-making in single cells.  Science 377, 642-648.  https://doi.org/10.1126/science.abf4062
Kriegman, S., Blackiston, D., Levin, M. and Bongard, J. (2021) Kinematic self-replication in 
reconfigurable organisms.  Proc. Natl. Acad. Sci. USA 118, e2112672118. 
https://doi.org/10.1073/pnas.2112672118
Kuchling, F., Fields, C. and Levin, M. (2022) Metacognition as a consequence of competing 
evolutionary time scales. Entropy 24, 601.
Kuchling, F., Friston, K., Georgiev, G. and Levin, M. (2020) Morphogenesis as Bayesian inference: A 
variational approach to pattern formation and control in complex biological systems. Phys. Life Rev. 33,
88-108.
Kuehner, J. N. and Brow, D. A. (2006) Quantitative analysis of in vivo initiator selection by yeast RNA 
Polymerase II supports a scanning model.  J. Biol. Chem. 281, 14119-14128.  
https://doi.org/10.1074/jbc.M601937200
Landauer, R. (1961) Irreversibility and heat generation in the computing process. IBM J. Res. Devel. 5, 
183-195.
Landauer, R. (1999) Information is a physical entity. Physica A 263, 63-67.
Levin, M. (2011) The wisdom of the body: Future techniques and approaches to morphogenetic
fields in regenerative medicine, developmental biology and cancer. Regen. Med. 6, 667-673.

Levin, M. (2019) The computational boundary of a “self”: Developmental bioelectricity drives 
multicellularity and scale-free cognition. Front. Psychol. 10, 2688.
Levin, M. (2021) Life, death, and self: Fundamental questions of primitive cognition viewed through 
the lens of body plasticity and synthetic organisms.  Biochem. Biophys. Res. Comm. 564, 114-133.  
https://doi.org/10.1016/j.bbrc.2020.10.077
Levin, M. (2022) Technological approach to mind everywhere: An experimentally-grounded 
framework for understanding diverse bodies and minds.  Front. Syst. Neurosci. 16, 768201.
https://doi.org/10.3389/fnsys.2022.768201
Lobo, D., Solano, M., Bubenik, G. A. and Levin, M. (2014) A linear-encoding model explains the 
variability of the target morphology in regeneration.  J. R. Soc. Interface 11, 20130918.
Lyon, P (2015) The cognitive cell: Bacterial behavior reconsidered. Front. Microbiol. 6, 264.
Lyon, P. (2020) Of what is ‘‘minimal cognition’’ the half-baked version? Adapt. Behav. 28: 407-428.
Maturana, H. R. and Varela, F. J. (1980) Autopoesis and Cognition: The Realization of the Living. 
Dordrecht, D. Reidel.
McCarthy, J. and Hayes, P. (1969) Some philosophical problems from the standpoint of artificial 
intelligence.  In Meltzer, B. and Michie, D. (eds.) Machine Intelligence, Vol. 4. Edinburgh University 
Press, Edinburgh, pp. 463-502.
McMillan, P., Oudin, M. J., Levin, M. and Payne, S. L. (2021) Beyond neurons: Long distance 
communication in development and cancer. Front. Cell Dev. Biol. 9, 739024. 
Metzinger, T. (2004) Being No One: The Self-Model Theory of Subjectivity. MIT Press, Cambridge, 
MA.
Michod, R. E. (1999) Darwinian Dynamics. Princeton University Press, Princeton, NJ.
Monod, J. (1972) Chance and Necessity. Random House, New York.
Nadel, L., Hupbach, A., Gomez, R. and Newman-Smith, K. (2012) Memory formation, consolidation 
and transformation. Neurosci. Biobehav. Rev. 36, 1640-1645.
Parrondo, J. M. R., Horowitz, J. M. and Sagawa, T. (2015) Thermodynamics of information.  Nat. 
Phys. 11, 131-193. https://doi.org/10.1038/NPHYS3230
Pattee, H. H. (1982) Cell psychology. Cognit. Brain Theory 5, 325-341.
Pegg, D. T., Barnett, S. M. and Jeffers, J. (2002) Quantum theory of preparation and measurement.  J. 
Mod. Optics 49, 913-924.  https://doi.org/10.1080/09500340110109412
Pezzulo, G. and Levin, M. (2016) Top-down models in biology: Explanation and control of complex 
living systems above the molecular level. J. R. Soc. Interface 13, 20160555.

https://doi.org/10.1098/rsif.2016.0555
Pinet, K. and McLaughlin, K. A. (2019) Mechanisms of physiological tissue remodeling in animals: 
Manipulating tissue, organ, and organism morphology.  Dev. Biol. 451, 134-145. 
Polanyi, M. (1968) Life’s irreducible structure. Live mechanisms and information in DNA are 
boundary conditions with a sequence of boundaries above them. Science 160, 1308-1312.
https://doi.org/10.1126/science.160.3834.1308
Prakash, C., Fields, C., Hoffman, D. D., Prentner, R. and Singh, M. (2020) Fact, fiction, and fitness. 
Entropy 22, 514.  https://doi.org/10.3390/e22050514
Prakash, C., Stephens, K. D., Hoffman, D. D., Singh, M. and Fields, C. (2021) Fitness beats truth in the
evolution of perception. Acta Biotheor. 69, 319-341.  https://doi.org/10.1007/s10441-020-09400-0
Ramstead, M. J., Constant, A., Badcock, P. B. and Friston, K. J. (2019) Variational ecology and the
physics of sentient systems. Phys. Life Rev. 31, 188-205.
Ramstead, M. J., Sakthivadivel, D. A. R., Heins, C., Koudahl, M., Millidge, B., Da Costa. L.,
Klein, B., and Friston, K. J. (2022) On Bayesian mechanics: A physics of and by beliefs. Preprint
arXiv:2205.11543 [cond-mat.stat-mech].
Rosen, R. (1986) On information and complexity. In: J. L. Casti, A. and Karlqvist, A. (Eds.) 
Complexity, Language, and Life: Mathematical Approaches. Springer, Berlin, pp. 174-196.
Rumelhart, D. E., Hinton, G. E and Williams, R. J. (1986) Learning representations by back-
propagating errors. Nature 323, 533-536.
Samek, W. Montavon, G., Lapuschkin, S., Anders, C. J. and Müller, K.-R. (2021) Explaining deep 
neural networks and beyond: A review of methods and applications. Proc. IEEE 109, 247-278.
Schwabe, L., Nader, K. and Pruessner, J. C. (2014) Reconsolidation of human memory: Brain 
mechanisms and clinical relevance. Biol. Psych. 76, 274-280.
Seth, A. K. (2013) Interoceptive inference, emotion, and the embodied self. Trends Cogn. Sci. 17, 565-
573.
Seth, A. K. and Tsakiris, M. (2018) Being a beast machine: The somatic basis of selfhood. Trends 
Cogn. Sci. 22, 969-981.
Smith, J. E. and Nair, R. (2005) The architecture of virtual machines. IEEE Computer 38(5), 32-38. 
https://doi.org/10.1109/MC.2005.173
Stewart, J. (1996) Cognition = Life: Implications for higher-level cognition. Behav. Process. 35, 311-
326.
Strassmann, J. E. and Queller, D. C. (2010) The social organism: Congresses, parties and committees.
Evolution 64, 605-616. https://doi.org/10.1111/j.1558-5646.2009.00929.x

Strikwerda-Brown, C., Grill, M. D., Andrews-Hanna, J. and Irish, M. (2019) “All is not lost” – 
Rethinking the nature of memory and the self in dementia.  Ageing Res. Rev. 54, 100932. 
https://doi.org/10.1016/j.arr.2019.100932
Szathmáry, E. and Maynard Smith, J .(1995) The major evolutionary transitions. Nature 374, 227-232.
Taylor, J. E. T. and Taylor, G. W. (2021) Artificial cognition: How experimental psychology can help 
generate explainable artificial intelligence. Psychon. Bull. Rev. 28, 454-475.
Tegmark, M. (2012) How unitary cosmology generalizes thermodynamics and solves the inflationary 
entropy problem.  Phys. Rev. D 85, 123517.
Tintori, S. C., Nishimura, E. O., Golden, P., Lieb, J. D. and Goldstein, B. (2016) A transcriptional 
lineage of the early C. elegans embryo.   Devel. Cell 38, 430-444.  
https://doi.org/10.1016/j.devcel.2016.07.025
Toyabe, S., Sagawa, T., Ueda, M., Muneyuki, E. and Sano, M. (2010) Experimental demonstration of 
information-to-energy conversion and validation of the generalized Jarzynski equality.  Nat. Phys. 6, 
988-992. https://doi.org/10.1038/nphys1821
Vandenberg, L. N., Adams, D. S. and Levin, M. (2012)  Normalized shape and location of perturbed 
craniofacial structures in the Xenopus tadpole reveal an innate ability to achieve correct morphology.  
Dev. Dyn. 241, 863-878.
Wang, M., Hu, Q., Lv, T., Wang, Y., Lan, Q., Xiang, R. et al. (2022) High-resolution 3D spatiotemporal 
transcriptomic maps of developing Drosophila embryos and larvae.  Devel. Cell 57, 1271-1283.  
https://doi.org/10.1016/j.devcel.2022.04.006
Zurek, W. H. (2003) Decoherence, einselection, and the quantum origins of the classical. Rev. Mod. 
Phys. 75, 715-775.
Zurek, W.H. (2009) Quantum Darwinism. Nat. Phys. 5, 181-188.

