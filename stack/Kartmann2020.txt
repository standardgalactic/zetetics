Representing Spatial Object Relations as Parametric Polar Distribution
for Scene Manipulation Based on Verbal Commands
Rainer Kartmann, You Zhou, Danqing Liu, Fabian Paus and Tamim Asfour
Abstractâ€” Understanding spatial relations is a key element
for natural human-robot interaction. Especially, a robot must be
able to manipulate a given scene according to a human verbal
command specifying desired spatial relations between objects.
To endow robots with this ability, a suitable representation of
spatial relations is necessary, which should be derivable from
human demonstrations. We claim that polar coordinates can
capture the underlying structure of spatial relations better than
Cartesian coordinates and propose a parametric probability
distribution deï¬ned in polar coordinates to represent spatial
relations. We consider static spatial relations such as left of,
behind, and near, as well as dynamic ones such as closer to
and other side of, and take into account verbal modiï¬ers such
as roughly and a lot. We show that adequate distributions can
be derived for various combinations of spatial relations and
modiï¬ers in a sample-efï¬cient way using Maximum Likelihood
Estimation, evaluate the effects of modiï¬ers on the distribution
parameters, and demonstrate our representationâ€™s usefulness in
a pick-and-place task on a real robot.
I. INTRODUCTION
Humans use spatial relations in natural language to describe
tasks to other people, e. g., put the cup to the left of the
plate. When such a description is given to a robot, the
robot needs to understand and reason about spatial object
relations, i. e. what does being left of another object mean.
Such understanding of spatial object relations in robotics
must also consider the actions that are needed to transfer a
given object conï¬guration in the scene to a goal conï¬guration
using executable robot skills. These object-action relations
are essential for task execution in robotics and research have
been conducted to derive representations that bridge the gap
between sensorimotor experience and symbolic planning and
reasoning. An example for such representations are Object-
Action Complexes (OACs), which have been proposed to
provide a unifying representation that has the ability to rep-
resent and reason about low-level sensorimotor experience as
well as high-level symbolic information in a robot architecture
and thus to bridge the gap between low-level sensorimotor
representations, required for robot perception and control,
and high-level representations supporting abstract reasoning,
planning and natural language understanding [1]. Other works
address speciï¬cally the problem of spatial relations learning to
endow a robot with the ability to describe a sceneâ€™s structure
by classifying the displacement between two objects [2],
to compare spatial relations by learning a distance metric
This work has been supported by the German Federal Ministry of
Education and Research (BMBF) under the project OML (01IS18040A).
The authors are with the Institute for Anthropomatics and Robotics, Karl-
sruhe Institute of Technology, Karlsruhe, Germany. {rainer.kartmann,
asfour}@kit.edu
â€œPlace the apple tea
on the other side
of the corny.â€
Fig. 1: Scene manipulation based on verbal commands specifying
spatial object relations.
between scenes [3], as well as to encode and recognize
actions based on changes of spatial relations using similarity
scores [4] and graph networks [5]. Instead of describing or
classifying a scene, our goal is the realization of a given
spatial relation by manipulating the current scene [6].
When using natural language for scene manipulation,
considerable related work addresses the grounding of referring
expressions involving spatial relations to physical entities in
the current scene. Tellex et al. [7] propose a probabilistic
graphical model to map constituents of a natural language
command to objects and places as well as robot actions.
Shridhar et al. [8] present a framework for grounding reference
expressions to objects in an image and resolving ambiguities
by using spatial relations. However, instead of limiting
possible action parameters to the objects and landmarks
present in the scene, we propose a subsymbolic representation
of spatial relations able to generate arbitrary placing positions.
Due to the imprecise and subjective nature of spatial rela-
tions and their natural language description, a representation
of spatial relations needs to be intuitively speciï¬ed by the
human. Since a human can only give a limited number of ex-
amples, a sample-efï¬cient way to learn the parameters of said
representation is required. This learned representation should
generalize to new scene states and objects. Furthermore, the
representation should be linked to the motion generation
enabling goal-directed scene manipulation.
We consider the representation and derivation of spatial
relations of one object to another in a two-dimensional plane
for a robot pick-and-place task. In this task, a robot is
commanded to move a target object relative to a reference
object according to a spatial relation speciï¬ed in natural
language. For instance, in the command â€œMove the tea

closer to the cup,â€ â€œteaâ€ is the target object and â€œcupâ€ is
the reference object. The goal is to execute a motion that
results in repositioning the target object according to the given
command. We consider both static and dynamic relations,
where static relations are relative only to the reference objectâ€™s
location while dynamic relations involve both objectsâ€™ relative
position [4]. In addition, we take into account verbal modiï¬ers,
which are words that affect the quantitative meaning of a
spatial relation in an imprecise manner [9], such as in roughly
left, a bit closer or not in front of. We consider spatial relations
only from the robotâ€™s point of view.
We require a representation of spatial relations to be
generative, i. e. allow the generation of multiple target position
candidates. To achieve this, we follow the approach of
representing spatial relations as probability distributions over
valid positions of the target object. However, instead of
predicting these distributions over pixels of an image as
done in [6], we propose to represent spatial relations using a
parametric probability distribution. A parametric distribution
offers the beneï¬t of being characterized by a limited number
of parameters which can be intuitively interpreted.
The main contribution of this work is a novel, under-
standable representation of spatial relations as parametric
probability distributions based on polar coordinates. This
representation can be effectively estimated from demonstra-
tion in a sample-efï¬cient manner. We show that the derived
representation is suitable to generate target positions which
can be used to adapt a movement primitive in a robot pick-
and-place task.
II. RELATED WORK
A. Spatial Relations for Scene Understanding
Rosman et al. [2] propose to use the spatial relations on
and adjacent as symbolic scene descriptions, which are
determined by classifying the displacement between two
objects. SjÂ¨oÂ¨o et al. [10] learn to classify functional relations
such as support force from geometric object features in
simulation. In [11], spatial object relations are extracted
from 3D vision using histograms. These works focus on
extracting object relations from a given scene, but our goal is
to manipulate the scene according to a verbal command.
Considerable work has been conducted on using spatial
relations for action recognition. In [12], spatial relations are
extracted by partitioning the space around an object relative
to auxiliary coordinate frames aligned to the workspace.
This information is used to classify actions in an RGB-D
image sequence. Similar approaches are described in [4],
[13], [14] and [15], where Semantic Event Chains (SECs) are
derived from human demonstrations to describe and recognize
manipulation actions and tasks. Recently, graph networks have
been used to segment actions in RGB-D videos based on
frame-wise extracted spatial relations [5].
Another part of the literature uses spatial relations to
resolve referential expressions in natural language utterances.
In an early work, Stopp et al. [16] introduce the applicability
of different spatial relations as potential ï¬elds around an
object based on user-deï¬ned cubic splines. In [17], spatial
relations are represented as fuzzy memberships to nine
image regions around an object to identify an object referred
to in a command. Spatial relations are used to enrich a
robotâ€™s knowledge from daily human instructions in [18]. In
these works, no actions were executed on a robot. In the
context of robot programming by demonstration, referential
expressions are used e. g. in [19] to parametrize primitive
actions. However, distances are ï¬xed at all times and action
parameters have a limited number of values. In [20], spatial
relations are used as a basis to ground abstract spatial concepts
such as columns or groups of objects for natural language
interaction. Shridhar et al. [21] use spatial relations to resolve
ambiguities when referring to similar objects.
The spatial relations used in these works are mainly
discriminative and used to classify scenes and actions or
to identify referred objects, while we require a generative
representation of spatial relations to execute manipulation
actions to realize a given spatial relation.
B. Realizing Spatial Relations
The following work deals with the question of how to
change a given scene to fulï¬ll given spatial relations. In [3],
Mees et al. learn a distance metric to ï¬nd a relative object
pose representing the same relation as known scene examples
with potentially different objects. The authors build upon this
work in [22], where they describe how gradient descent can
be performed on object poses to change a given scene to
represent the same relation as a reference scene. Prepositions
from natural language commands are incorporated as action
parameters in a task representation in [23]. However, ï¬xed
positions or offsets are used in action execution. Placement
positions are found based on spatial prepositions in [24] by
training a multi-class logistic regression to classify random
positions on a table. In contrast, our goal is to ï¬nd generative
representations of spatial relations abstracting from examples,
which can be directly used to generate target positions. Also,
these works do not consider dynamic relations and verbal
modiï¬ers. In a work very similar to ours, Mees et al. [6] train
a neural network to predict pixel-wise probability maps of
placement positions for different spatial relations given an
input image of the scene to place an object according to a
verbal command. We, however, derive parametric probability
distributions representing spatial relations, which can be
intuitively interpreted and estimated from few examples. In
addition, our representation is not restricted to an input image.
While the methods used in [6] could be extended to include
dynamic relations and verbal modiï¬ers, these aspects are
currently not taken into account.
C. Uncertainty of Spatial Relations and Natural Language
Spatial relations between objects involve some degree of
uncertainty. For instance, there is no exact boundary between
regions described by left of and in front of. To cope with
uncertainties in spatial relations, a continuous measure of ap-
plicability based on user-deï¬ned cubic splines is used in [16],
while Tan et al. [17] use fuzzy membership of locations to
deï¬ned regions around the object. Skubic et al. [25] divide

directions around the robot into 16 sub-directions to represent
utterances such as â€œmostly in front but somewhat to the leftâ€.
Instead, uncertainty and fuzziness of spatial relations are
naturally embedded in our probabilistic representations in the
form of variance parameters.
As mentioned above, verbal instructions can contain verbal
modiï¬ers introducing more uncertainty. For example, a human
might prefer saying â€œMove it a bit closer.â€ rather than â€œMove
it 5 cm closer.â€ In addition, one could instruct the robot to
place an object â€œroughly to the left ofâ€ or â€œnot in front ofâ€
another. A comprehensive analysis of uncertain information
in natural language instructions for robotic applications is
provided by [9]. Instead of modeling such modiï¬ers in
a special way, we take them into account by estimating
probability distributions separately for each combination of
spatial relation and modiï¬er and show how this affects the
estimated distribution parameters.
D. Learning Spatial Arrangements of Objects
Spatial relations also play a role in the generation of
typical or natural arrangements of objects. Jiang et al. [26]
predict the correct placements of objects in a 3D scene
based on their affordances and potential human poses,
i. e. by modeling human-object relations instead of object-
object relations. The authors address ï¬nding good placement
locations with respect to stability and support, i. e. based
on relations between objects and the environment, in [27].
However, we are interested in arranging objects based on a
spatial relation between them speciï¬ed by a verbal command.
Chang et al. [28] build scene templates from example scenes
to generate 3D scenes according to a textual description. They
consider typical relative arrangements and support surfaces
of objects based on their categories as well as objects which
are likely to be present despite not being mentioned explicitly.
Although taking into account typical arrangements can be
useful, we focus on manipulating a concrete scene to realize
a speciï¬c spatial relation.
III. SPATIAL RELATIONS AS POLAR DISTRIBUTIONS
A. Cartesian vs. Polar Space
For the reasons explained above, we aim at modeling spatial
relations as parametric probability distributions. The ï¬rst
choice might be to represent a spatial relation as a multivariate
Gaussian distribution in Cartesian space. However, this is not
suitable for relations such as close to and far away from, which
should cover positions in any direction from the reference
object oref. For these relations, the best option for a Cartesian
Gaussian distribution would involve placing the mean inside
oref and adjusting the variances to ï¬t the examples. As a result,
positions inside oref would be considered most likely, which is
not desired. In essence, a Cartesian Gaussian distribution fails
to represent a â€œringâ€ around the reference object. A solution
might be to use a Gaussian Mixture Model to construct this
ring around the reference object oref. However, this would
require specifying the number of components beforehand.
Instead, we propose to represent a spatial relation not in
Cartesian space, but in polar space. As polar coordinates
represent positions as direction and distance, we claim that
they capture the underlying structure of spatial relations better
than Cartesian coordinates. To this end, we deï¬ne a polar
coordinate system (PCS) centered at the reference object, and
deï¬ne a two-dimensional distribution in this polar coordinate
space. This way, far away from can be represented by covering
all angles, but only sufï¬ciently large distances.
B. Polar Distribution
We denote a polar coordinate with distance d âˆˆRâ‰¥0 and
angle Ï† âˆˆ[âˆ’Ï€, Ï€] as
q =
 d
Ï†âŠ¤âˆˆRâ‰¥0 Ã— [âˆ’Ï€, Ï€] .
(1)
We assume that d follows a Gaussian distribution
d âˆ¼N
 Âµd, Ïƒ2
d

,
(2)
with mean Âµd and variance Ïƒ2
d. As the angle Ï† is periodic
over [âˆ’Ï€, Ï€], we utilize the von Mises distribution
Ï† âˆ¼M(ÂµÏ†, ÎºÏ†) ,
(3)
with mean ÂµÏ† and concentration ÎºÏ†, which is a circular
distribution behaving similarly to a Gaussian distribution but
wrapping over the interval [âˆ’Ï€, Ï€]. For easier comparison, we
will refer to the concentrationâ€™s inverse as variance Ïƒ2
Ï† := Îºâˆ’1
Ï†
and use it instead of ÎºÏ†.
We deï¬ne a polar distribution as joint probability distri-
bution d, Ï† âˆ¼P(Î¸) with Î¸ = (Âµd, Ïƒ2
d, ÂµÏ†, Ïƒ2
Ï†) being the
distributionâ€™s parameters. In our model, we assume that d and
Ï† are independent. Therefore, the joint probability density
function (p.d.f.) is
p(d, Ï†) = p(d) Â· p(Ï†) .
(4)
To evaluate a polar distribution in Cartesian space, global
Cartesian positions must be transformed to polar space. A
local Cartesian position ploc = (x, y)âŠ¤âˆˆR2 is transformed
to polar coordinates by
polar(ploc) =
d
Ï†

=
 p
x2 + y2
atan2(y, x)

.
(5)
The resulting coordinate system is visualized in Fig. 2a. A
global Cartesian position pglob = (x, y)âŠ¤âˆˆR2 is translated
to the reference objectâ€™s local coordinate system before
transforming it to polar coordinates:
q = polar(pglob âˆ’pref)
(6)
C. Static vs. Dynamic Relations
As noted in [4], there are two types of spatial relations
with respect to time: static and dynamic spatial relations.
Static relations (e. g. left of, close to) are independent of the
target objectâ€™s current position. In contrast, dynamic relations
(e. g. closer to, on other side of) depend on both the target
and reference objectsâ€™ positions.
When estimating polar distributions to represent dynamic
relations without respecting this dependency, the resulting
distributions will cover all angles and distances encountered
in the examples, not learning a meaningful representation for

x
y
d = 1
d = 2
Ï† = 0
Ï† = Ï€/4
Ï† = Ï€
Ï† = âˆ’Ï€
2
(a) Original
x
y
d = 0
d = 1
Ï† = 0
Ï† = Ï€/4
Ï† = Ï€
Ï† = âˆ’Ï€
2
(b) Static
d = 0.5
d = 1
x
y
Ï† = 0
Ï† = Ï€/4
Ï† = Ï€
Ï† = âˆ’Ï€
2
d = 1.5
(c) Dynamic
Fig. 2: Polar coordinate systems (PCS). In all cases, the origin of the
PCS is the reference objectâ€™s position (orange). Its local Cartesian
coordinate system (x, y) is shown in gray. The PCS is shown in blue
(distance d) and green (angle Ï†). (a) Original deï¬nition of PCS. (b)
PCS for static relations. Distance d = 0 is translated to the objectâ€™s
size rref. (c) PCS for dynamic relations. Distance d = 1 is scaled
to the current distance to the target object (shown in purple). Angle
Ï† = 0 is translated to the current direction to the target object.
a speciï¬c relation. Therefore, we explicitly incorporate this
dependency by aligning the polar coordinate system around
the reference object to fulï¬ll two constraints:
1) The distance d = 1 corresponds to the current distance
between reference and target object.
2) The angle Ï† = 0 corresponds to the current direction
from reference to target object.
This idea is shown in Fig. 2c. Formally, let pref and ptrg0 be
the reference and target objectsâ€™ initial positions, the dynamic
polar coordinate of a global position p is deï¬ned by
polardyn
 p, pref, ptrg0
 :=

d Â· dâˆ’1
trg
Ï† âˆ’Ï†trg

(7)
with
(dtrg, Ï†trg)âŠ¤= polar
 ptrg0 âˆ’pref

,
(8)
(d, Ï†)âŠ¤= polar(p âˆ’pref) .
(9)
Note that Ï† âˆ’Ï†trg in eq. (7) is wrapped over [âˆ’Ï€, Ï€].
As to static relations, while using the original polar
coordinates as deï¬ned in eq. (6) yielded good results in
our experiments, we found that, especially for the inside
relation, it is useful to let the distance d = 0 correspond to
the reference objectâ€™s size rref âˆˆR+:
polarsta(p, pref, rref) :=
d âˆ’rref
Ï†

(10)
with d, Ï† deï¬ned as in eq. (9). Note that in this case, the
resulting distance d âˆˆ[âˆ’rref, âˆ). As a size measure rref,
we use the radius of the objectâ€™s bounding circle. Fig. 2b
illustrates this transformation. With this convention, we found
that the estimated distributions generalize better over reference
objects of different size. Table I speciï¬es the respective type
of each relation.
D. Data Collection and Distribution Estimation
To estimate polar distribution for different relations and
modiï¬ers, we collected examples using an interactive data
collection tool. The tool generates random verbal commands
based on sentence templates as well as random scenes
containing two objects. The user is presented with both
command and scene and can move the target object to valid
positions according to the command.
TABLE I
GENERATED SPATIAL RELATIONS, THEIR TYPES AND
COMBINATIONS WITH MODIFIERS
Type
Relations
Modiï¬ers
â€“
not
roughly, exactly
a bit, a lot
Static
left, right
âœ“
âœ“
âœ“
in front, behind
âœ“
âœ“
âœ“
near / close to
âœ“
âœ“
away / far from
âœ“
inside
âœ“
âœ“
Dynam.
closer to
âœ“
âœ“
farther away
âœ“
âœ“
opposite side
âœ“
âœ“
âœ“
The generated commands include the relations left, right,
in front, behind, near, far, inside, closer, farther and opposite
side and the modiï¬ers not, roughly, exactly, a bit and a lot.
Table I shows the generated combinations. Each relation and
modiï¬er can be instantiated by different words. For example,
the relation near can also be expressed as close to and around.
Similarly, the modiï¬er not can as well be instantiated as
anywhere other than. Examples of generated commands are
â€œPlace the milk so that it stands not left of the rusk.â€ and
â€œMove salt roughly to the other side of the popcorn.â€
The objects are taken from a list of household objects
containing their names, shapes and sizes. The shapes include
rectangular, circular and elliptic objects. The objectsâ€™ sizes
are randomly varied. The reference object is always placed in
the center of the screen; its orientation and the target objectâ€™s
pose are chosen randomly.
Each entered target object position creates one sample.
Each sample i consists of the command sentence and the
reference and target objectsâ€™ names, their initial positions
pi
ref, pi
trg0 and orientations, as well as the target objectâ€™s
ï¬nal position pi
trg1 and orientation.
Using this tool, we generated data to estimate polar distri-
butions representing spatial relations. In total, we collected
5275 samples from three participants. We partitioned the
samples according to relation and modiï¬er in the command
using keyword matching. For each combination of relation
and modiï¬er, we estimate one single polar distribution. To this
end, we rely on established Maximum Likelihood Estimation
(MLE). Given the samples of a combination of relation and
modiï¬er, we transform the target objectâ€™s ï¬nal positions pi
trg1
to polar coordinates qi according to eq. (7) and eq. (10) and
the respective relationâ€™s type:
 d i, Ï†iâŠ¤â†
ï£±
ï£²
ï£³
polarsta

pi
trg1, pi
ref, ri
ref

,
static
polardyn

pi
trg1, pi
ref, pi
trg0

,
dynamic
for i = 1 . . . N, with N being the respective number of
samples. For static relations, pi
trg0 is not used. We perform
MLE separately for distances and angles:
Âµd, Ïƒ2
d â†MLEN
 
d i	
i=1...N

(11)
ÂµÏ†, Ïƒ2
Ï† â†MLEM
 
Ï†i	
i=1...N

(12)
The resulting polar distribution is then speciï¬ed by its
parameters Î¸ = (Âµd, Ïƒ2
d, ÂµÏ†, Ïƒ2
Ï†) as described in Section III-B.

Fig. 3: Using estimated polar distributions in pick-and-place task.
IV. RELATIONAL PICK-AND-PLACE TASK
Now, we describe how we utilize polar distributions
representing spatial relations to place objects according to a
verbal command. Fig. 3 shows an overview of the approach.
A. Retrieval and Grounding of Spatial Relation
In our experiments, we only consider known objects which
we can localize based on visual features using our previous
work in [29]. For each object, we store the class label, relevant
geometric information and the visual features needed for
localization in the robotâ€™s prior knowledge memory. Before
receiving a verbal command, we scan the robotâ€™s workspace
for known objects, and store detected objects and their poses
in the robotâ€™s working memory.
When given a verbal command, as natural language
understanding is not our focus, we perform simple keyword
matching to extract the referred objects and desired relation
(including a potential modiï¬er). To this end, we search in the
verbal command for known object names. We assume that
the ï¬rst object name speciï¬es the target object and the second
refers to the reference object. For relations and modiï¬ers, we
search for the same keywords as used in Section III. We then
retrieve the desired relationâ€™s polar distribution parameters Î¸
from the set of previously estimated distributions. Given the
referred object entities and relation, we ground the relationâ€™s
polar distribution to the current scene by aligning the polar
coordinate system to the involved objects according to the
relationâ€™s type as explained in Section III-C.
B. Generation and Selection of Target Position
In order to facilitate collision-free object placing, we exploit
the representation by sampling multiple placing positions from
the probability distribution. In the following, let P(Î¸) be the
used polar distribution, p(d, Ï†) its probability density function
(p.d.f.), and pi and qi (i = 1 . . . n) the n sampled Cartesian
and polar candidate positions, respectively. An example is
shown in Fig. 4.
Sampling enables ï¬ltering infeasible placing positions,
i. e. those which would collide with other objects already
present in the scene or which are not on the table. Let
Ifeas âŠ†{1, . . . , n} be the feasible samples. As sampling
can produce positions with low likelihood, which will be less
representative for the relation, we evaluate the probability
density function at each position and select the candidates
Fig. 4: Sampling, ï¬ltering and selection of target position for the
command â€œPut the wild berry tea roughly to the left of the milk.â€
The initial samples drawn from the distribution are shown as small
spheres. The red samples have been discarded due to potential
collisions. From the remaining samples, the blue and green samples
have the highest p.d.f. values. From these two, the green sample,
highlighted by an arrow, is selected as it is closer to the target
objectâ€™s current position.
with the highest values:
Ibest =

i âˆˆIfeas
 p
 qi
â‰¥0.9 Â· max
jâˆˆIfeas p
 qj
(13)
From the remaining positions we select the position piâˆ—
closest to the target objectâ€™s current position to avoid
unnecessary movement:
iâˆ—= arg min
iâˆˆIbest
pi âˆ’ptrg

(14)
C. Adaption of Movement Primitive
We use via-points movement primitive (VMP), described
in our previous work [30], to represent robot actions as they
provide a ï¬‚exible way to adapt robot actions to different start
and goal positions and allow integrating arbitrary via-points.
A VMP consists of two parts: an elementary trajectory h(x)
and a shape modulation term f(x):
y(x) = h(x) + f(x)
(15)
The elementary trajectory can be any polynomial. In this
work, we consider a simple form:
h(x) = (y0 âˆ’g)x + g ,
(16)
where y0 is the start and g is the goal. x is the canonical
variable that goes from 1 to 0. The shape modulation term
is a linear regression model f(x) = Ïˆ(x)âŠ¤w, where Ïˆ(Â·) is
the squared exponential kernels. We learn the weights vector
w from demonstrations. After learning, VMP can adapt to
different goals by changing the hyper-parameters g in the
elementary trajectory.
We learn VMP using kinesthetic teaching. After learning,
we adapt the VMP to different target positions given by the
sampling process. In the meantime, we use several via-points
to meet some task requirements. As an example, in the pick-
and-place task, we use one via-point to avoid collisions with
the table while approaching the target object. In this way,

collision-free execution is guaranteed, no matter where the
target object is.
V. EVALUATION
In this section, we test the following hypotheses: 1) polar
distributions are suitable generative representations of spatial
relations, 2) the estimated distributions differ meaningfully
for different verbal modiï¬ers, 3) estimating polar distributions
from examples is sample efï¬cient and 4) the generated spatial
relations can be used to change a scene through pick and place
tasks using verbal commands on a real robot. To this end,
we will provide qualitative results and quantitative analysis
of the polar distributions estimated from collected data and
demonstrate their usefulness in a real robot experiment.
A. Estimated Distributions
First, we provide a qualitative evaluation of the estimated
distributions for all relations and modiï¬ers. Table II shows all
estimated polar distributions for all considered combinations
of relations and modiï¬ers. The images show a top view on an
example scene in Cartesian space, where the green milk box
is the reference object and the red coffee ï¬lters represent the
target object. Yellow regions have a high p.d.f. value, while
purple regions have a p.d.f. close to zero. For the relations
left of, right of, in front of, behind and other side of it is
visible how modiï¬ers exactly and roughly change the angle
variance. Interestingly, for these relations, the distributions for
modiï¬er not seem to complement the afï¬rmative exemplars
mainly in terms of direction, but not in terms of distance.
As a consequence, these distributions still represent locations
in the area around the reference object. The relations near,
far from and inside feature very high angle variance but
distinct distance distributions. Especially, it appears that the
distributions for not near and far from are very similar1. For
the dynamic relations closer to and further from which take
the modiï¬ers a bit and a lot, it can be seen that the mean
direction points to the target object and that the modiï¬ers
strongly affect the distance means and variances. Overall, the
results indicate that meaningful distributions were estimated
for all relations and modiï¬ers.
B. Effects of Modiï¬ers
We conducted a quantitative analysis of the differences
between distributions for different modiï¬ers. First, we in-
vestigated the estimated angle variances Ïƒ2
Ï† for exactly and
roughly. To make them comparable for different relations,
we normalize each Ïƒ2
Ï† by the value for the respective relation
without a modiï¬er. The results are shown in Fig. 5a. As can
be seen, angle variances for exactly are lower, and those for
roughly are higher when compared to no modiï¬er. We found
that distance variance Ïƒ2
d behaves similarly, but there the
effect is less strong. We performed a similar analysis for the
effects of a bit and a lot on the distance mean of relations
closer and farther. We did not normalize the values to allow
direct comparison between the relations and report the results
1In fact, their Bhattacharyya distance DB is 0.146. For details on DB
see Section V-C.
(a)
(b)
Fig. 5: Effects of modiï¬ers on distribution. (a) Relative angle
variance for modiï¬ers exactly and roughly. (b) Distance mean for
modiï¬ers a little and much.
(including no modiï¬er) in Fig. 5b. It can be seen that a lot
closer results in a distance mean closer to the reference object
than closer and a bit closer. For farther, the inverse behavior
is observable.
C. Sample Efï¬ciency
To evaluate the sample efï¬ciency of polar distribution
estimation, we conducted an experiment as follows: First,
for each combination of relation r and modiï¬er m, we
randomly draw 100 samples Dr,m from our data set to
avoid effects of imbalances. Second, we take k samples from
Dr,m and use them to estimate a polar distribution. We do
this for k âˆˆK = {2, 3, 5, 10, 25, 50, 75, 100} and repeat each
sampling and estimation R = 10 times.
Let P(k,i)
r,m
be the distribution estimated using k âˆˆK
samples in repetition i âˆˆ{1, . . . , R}, and Pâˆ—
r,m be the
distribution estimated using k = 100 (i. e. all samples). We
compared the P(k,i)
r,m with Pâˆ—
r,m using two measures:
1) The dataâ€™s mean likelihood given P(k,i)
r,m relative to its
likelihood given Pâˆ—
r,m.
2) The Bhattacharyya distance of P(k,i)
r,m to Pâˆ—
r,m.
In the following, each measure is explained and the results
are discussed.
1) Relative Mean Likelihood: We deï¬ne the mean likeli-
hood Â¯L(D, P) of data D given a distribution P with p.d.f. p
as exponential of its mean log-likelihood LL(D, P):
Â¯L(D, P) = exp
 LL(D, P)

= exp
ï£«
ï£­1
|D|
X
qâˆˆD
log p(q)
ï£¶
ï£¸
We compute the mean likelihoods of Dr,m given each P(k,i)
r,m .
In this experiment, Â¯L
 Dr,m, Pâˆ—
r,m

constitutes the optimal
performance on Dr,m, which is different for each combination
of r and m. Therefore, we compute the relative mean
likelihood of Dr,m given P as:
Â¯Lrel(Dr,m, P) =
Â¯L(Dr,m, P)
Â¯L
 Dr,m, Pâˆ—r,m

(17)
Consequently, Â¯Lrel(Dr,m, P) = 1 indicates an optimal
estimate, while values close to zero indicate worst estimates.
We report the relative mean likelihood as mean with standard

TABLE II
ESTIMATED DISTRIBUTIONS FOR EACH COMBINATION OF RELATION AND MODIFIER IN CARTESIAN SPACE.
(TOP VIEW OF SCENE, ROBOTâ€™S VIEW IS FROM BOTTOM OF IMAGES.)
Static
Dynamic
left of
right of
in front of
behind
near
far from
inside
closer to
further from other side of
exactly /
a bit
â€“
roughly /
a lot
not
(a) Relative mean likelihood
(b) Bhattacharyya distance
Fig. 6: Evaluation of sample efï¬ciency as comparison of estimated
distributions using k samples versus 100 samples.
deviation aggregated over all relations r, modiï¬ers m and
repetitions i in Fig. 6a.
2) Bhattacharyya Distance: The Bhattacharyya distance
measures the distance between two probability distributions
with density functions p and q, see [31]. It is deï¬ned as
DB(p, q) = âˆ’log CB(p, q)
(18)
where Bhattacharyya coefï¬cient CB measures the overlap
between two distributions and is deï¬ned as
CB(p, q) =
Z
xâˆˆX
p
p(x) Â· q(x) dx .
(19)
CB is close to 1 for distributions with high overlap and 0 if
there is no overlap. For two polar distributions with density
functions p, q as in eq. (4), the Bhattacharyya coefï¬cient and
distance can be written as:
CB(p, q) = CB(pd, qd) Â· CB(pÏ†, qÏ†) ,
(20)
DB(p, q) = DB(pd, qd) + DB(pÏ†, qÏ†) ,
(21)
where pd = p(d), qd = q(d) describe Gaussian distributions
and pÏ† = p(Ï†), qÏ† = q(Ï†) describe von Mises distributions.
To compute DB(pd, qd), we use a closed-form solution for
Gaussian distributions given in [32]. For DB(pÏ†, qÏ†), we
numerically compute the integral in eq. (19) over the interval
[âˆ’Ï€, Ï€] and apply eq. (18). Using eq. (21) we then obtain
the Bhattacharyya distance between two polar distributions.
Similar to relative mean likelihoods, we compute the
Bhattacharyya distances between each P(k,i)
r,m and Pâˆ—
r,m. The
results are shown in Fig. 6b as mean and standard deviation
aggregated over all relations, modiï¬ers and repetitions. Note
that two identical distributions have a DB of 0.
3) Discussion of the Results: Both the relative mean
likelihood and Bhattacharyya distance quickly approach
their respective optimum with increasing number of samples
for all combinations of relations and modiï¬ers. Using
k = 10 samples already yields a relative mean likeli-
hood of 76.27 Â± 0.20 %, and a Bhattacharyya distance of
0.063 Â± 0.059 (with a CB of 0.941 Â± 0.052) compared to us-
ing 100 samples. When using k = 25 samples, relative mean
likelihood achieves 93.02 Â± 0.06 % and the Bhattacharyya
distance decreases to 0.018 Â± 0.015 (CB = 0.983 Â± 0.015).
Using more samples has only marginal effects on the
similarity to using all samples. These ï¬ndings indicate that
representing spatial relations as polar distributions introduces
a suitable inductive bias which allows to generalize from few
samples.
D. Robot Pick-And-Place Task
We demonstrate the usefulness of the generated spatial
relation representations in a robot pick-and-place task. We
place different, known objects on a table, enter a verbal
command as text and follow the procedure described in
Section IV to retrieve the desired relation, ground it to the
referred objects, sample and select a suitable target position
and execute the grasping and placing motions.
The experiments show that the estimated polar distributions
can be used to generate suitable target positions. Especially,

in scenes where the distribution mean is blocked by other
objects, their generative and fuzzy nature allow us to still
ï¬nd feasible placing locations.
VI. CONCLUSION
We have presented a parametric probability distribution
deï¬ned in polar coordinate space representing spatial relations,
which can be estimated from examples and used to manipulate
a scene in order to fulï¬ll spatial relations speciï¬ed in verbal
commands. We have shown that estimation is sample-efï¬cient
and that the estimated parameters differ between verbal
modiï¬ers. In future work, we will extend this representation
to three dimensions and orientations and investigate how
such representations can be derived from few examples
shown in real-world, how they can generalize to objects
of different shapes and sizes, and how relations can be
mapped to and from speech. In addition, we will work on the
integration of this work in our work regarding programming
by demonstration.
REFERENCES
[1] N. KrÂ¨uger, C. Geib, J. Piater, R. Petrick, M. Steedman, F. WÂ¨orgÂ¨otter,
A. Ude, T. Asfour, D. Kraft, D. OmrË‡cen, A. Agostini, and R. Dillmann,
â€œObject-Action Complexes: Grounded Abstractions of Sensorimotor
Processes,â€ Robotics and Autonomous Systems, vol. 59, pp. 740â€“757,
2011.
[2] B. Rosman and S. Ramamoorthy, â€œLearning spatial relationships
between objects,â€ International Journal of Robotics Research, vol. 30,
no. 11, pp. 1328â€“1342, 2011.
[3] O. Mees, N. Abdo, M. Mazuran, and W. Burgard, â€œMetric Learning
for Generalizing Spatial Relations to New Objects,â€ in IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
Sept. 2017, pp. 3175â€“3182.
[4] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, and F. WÂ¨orgÂ¨otter,
â€œRecognition and Prediction of Manipulation Actions Using Enriched
Semantic Event Chains,â€ Robotics and Autonomous Systems (RAS),
vol. 110, pp. 173â€“188, Dec. 2018.
[5] C. R. G. Dreher, M. WÂ¨achter, and T. Asfour, â€œLearning Object-
Action Relations from Bimanual Human Demonstration Using Graph
Networks,â€ IEEE Robotics and Automation Letters, Oct. 2019.
[6] O. Mees, A. Emek, J. Vertens, and W. Burgard, â€œLearning Object
Placements For Relational Instructions by Hallucinating Scene Rep-
resentations,â€ in IEEE International Conference on Robotics and
Automation (ICRA), 2020.
[7] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee,
S. Teller, and N. Roy, â€œUnderstanding Natural Language Commands
for Robotic Navigation and Mobile Manipulation,â€ in Twenty-Fifth
AAAI Conference on Artiï¬cial Intelligence, Aug. 2011.
[8] M. Shridhar, D. Mittal, and D. Hsu, â€œINGRESS: Interactive visual
grounding of referring expressions,â€ The International Journal of
Robotics Research, vol. 39, no. 2-3, pp. 217â€“232, 2020.
[9] M. A. V. J. Muthugala and A. G. B. P. Jayasekara, â€œA Review
of Service Robots Coping With Uncertain Information in Natural
Language Instructions,â€ IEEE Access, vol. 6, pp. 12 913â€“12 928, 2018.
[10] K. SjÂ¨oÂ¨o and P. Jensfelt, â€œLearning spatial relations from functional
simulation,â€ in IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), Sept. 2011, pp. 1513â€“1519.
[11] S. Fichtl, A. McManus, W. Mustafa, D. Kraft, N. KrÂ¨uger, and F. Guerin,
â€œLearning spatial relationships from 3D vision using histograms,â€ in
IEEE International Conference on Robotics and Automation (ICRA),
May 2014, pp. 501â€“508.
[12] K. Zampogiannis, Y. Yang, C. Fermuller, and Y. Aloimonos, â€œLearning
the Spatial Semantics of Manipulation Actions Through Preposition
Grounding,â€ in IEEE International Conference on Robotics and
Automation (ICRA), 2015, pp. 1389â€“1396.
[13] E. E. Aksoy, Y. Zhou, M. WÂ¨achter, and T. Asfour, â€œEnriched Manip-
ulation Action Semantics for Robot Execution of Time Constrained
Tasks,â€ in IEEE/RAS International Conference on Humanoid Robots
(Humanoids), Nov. 2016, pp. 109â€“116.
[14] F. Ziaeetabar, T. Kulvicius, M. Tamosiunaite, and F. WÂ¨orgÂ¨otter,
â€œPrediction of Manipulation Action Classes Using Semantic Spatial
Reasoning,â€ in IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), Oct. 2018, pp. 3350â€“3357.
[15] T. R. Savarimuthu, A. G. Buch, C. Schlette, N. Wantia, J. RoÃŸmann,
D. MartÂ´Ä±nez, G. Aleny`a, C. Torras, A. Ude, B. Nemec, A. Kramberger,
F. WÂ¨orgÂ¨otter, E. E. Aksoy, J. Papon, S. Haller, J. Piater, and
N. KrÂ¨uger, â€œTeaching a Robot the Semantics of Assembly Tasks,â€ IEEE
Transactions on Systems, Man, and Cybernetics: Systems, vol. 48, no. 5,
pp. 670â€“692, May 2018.
[16] E. Stopp, K.-P. Gapp, G. Herzog, T. Laengle, and T. C. Lueth, â€œUtilizing
Spatial Relations for Natural Language Access to an Autonomous
Mobile Robot,â€ in KI-94: Advances in Artiï¬cial Intelligence, ser.
Lecture Notes in Computer Science.
Springer, 1994, pp. 39â€“50.
[17] J. Tan, Z. Ju, and H. Liu, â€œGrounding Spatial Relations in Natural
Language by Fuzzy Representation for Human-Robot Interaction,â€ in
IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), July
2014, pp. 1743â€“1750.
[18] J. Bao, Z. Hong, H. Tang, Y. Cheng, Y. Jia, and N. Xi, â€œTeach robots
understanding new object types and attributes through natural language
instructions,â€ in International Conference on Sensing Technology
(ICST), vol. 10, Nov. 2016, pp. 1â€“6.
[19] M. Forbes, R. P. N. Rao, L. Zettlemoyer, and M. Cakmak, â€œRobot
Programming by Demonstration with Situated Spatial Language
Understanding,â€ in IEEE International Conference on Robotics and
Automation (ICRA), May 2015, pp. 2014â€“2020.
[20] R. Paul, J. Arkin, N. Roy, and T. M. Howard, â€œEfï¬cient Grounding
of Abstract Spatial Concepts for Natural Language Interaction with
Robot Manipulators,â€ in Robotics: Science and Systems (RSS), vol. 12,
June 2016.
[21] M. Shridhar and D. Hsu, â€œInteractive Visual Grounding of Referring
Expressions for Human-Robot Interaction,â€ in Robotics: Science &
Systems (RSS), 2018.
[22] P. Jund, A. Eitel, N. Abdo, and W. Burgard, â€œOptimization Beyond
the Convolution: Generalizing Spatial Relations with End-to-End
Metric Learning,â€ in IEEE International Conference on Robotics and
Automation (ICRA), May 2018, pp. 4510â€“4516.
[23] M. Nicolescu, N. Arnold, J. Blankenburg, D. Feil-Seifer, S. B. Banisetty,
M. Nicolescu, A. Palmer, and T. Monteverde, â€œLearning of Complex-
Structured Tasks from Verbal Instruction,â€ in IEEE/RAS International
Conference on Humanoid Robots (Humanoids), Oct. 2019, p. 8.
[24] S. Guadarrama, L. Riano, D. Golland, D. GÂ¨ohring, Y. Jia, D. Klein,
P. Abbeel, and T. Darrell, â€œGrounding Spatial Relations for Human-
Robot Interaction,â€ in IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), Nov. 2013, pp. 1640â€“1647.
[25] M. Skubic, D. Perzanowski, S. Blisard, A. Schultz, W. Adams,
M. Bugajska, and D. Brock, â€œSpatial Language for Humanâ€“Robot
Dialogs,â€ IEEE Transactions on Systems, Man, and Cybernetics, Part
C (Applications and Reviews), vol. 34, no. 2, pp. 154â€“167, May 2004.
[26] Y. Jiang, C. Zheng, M. Lim, and A. Saxena, â€œLearning to Place New
Objects,â€ in IEEE International Conference on Robotics and Automation
(ICRA), May 2012, pp. 3088â€“3095.
[27] Y. Jiang, M. Lim, and A. Saxena, â€œLearning Object Arrangements
in 3D Scenes using Human Context,â€ in International Conference on
Machine Learning (ICML), June 2012.
[28] A. Chang, M. Savva, and C. D. Manning, â€œLearning Spatial Knowledge
for Text to 3D Scene Generation,â€ in Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing
(EMNLP).
Association for Computational Linguistics, Oct. 2014, pp.
2028â€“2038.
[29] P. Azad, T. Asfour, and R. Dillmann, â€œCombining Harris interest points
and the SIFT descriptor for fast scale-invariant object recognition,â€ in
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Oct. 2009, pp. 4275â€“4280.
[30] Y. Zhou, J. Gao, and T. Asfour, â€œLearning Via-Point Movement
Primitives with Inter- and Extrapolation Capabilities,â€ in IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
2019.
[31] A. K. Bhattacharyya, â€œOn a measure of divergence between two statis-
tical populations deï¬ned by their probability distributions,â€ Bulletin of
the Calcutta Mathematical Society, vol. 35, pp. 99â€“109, 1943.
[32] G. B. Coleman and H. C. Andrews, â€œImage Segmentation by Clustering,â€
Proceedings of the IEEE, vol. 67, no. 5, pp. 773â€“785, May 1979.

